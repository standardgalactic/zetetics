Online Nonsubmodular Minimization with Delayed
Costs: From Full Information to Bandit Feedback
Tianyi Lin⋆,⋄
Aldo Pacchiano⋆,‡
Yaodong Yu⋆,⋄
Michael I. Jordan⋄,†
Department of Electrical Engineering and Computer Sciences⋄
Department of Statistics†
University of California, Berkeley
Microsoft Research, NYC‡
June 2, 2022
Abstract
Motivated by applications to online learning in sparse estimation and Bayesian opti-
mization, we consider the problem of online unconstrained nonsubmodular minimization
with delayed costs in both full information and bandit feedback settings. In contrast to
previous works on online unconstrained submodular minimization, we focus on a class of
nonsubmodular functions with special structure, and prove regret guarantees for several
variants of the online and approximate online bandit gradient descent algorithms in static
and delayed scenarios. We derive bounds for the agent’s regret in the full information and
bandit feedback setting, even if the delay between choosing a decision and receiving the
incurred cost is unbounded. Key to our approach is the notion of (α, β)-regret and the
extension of the generic convex relaxation model from El Halabi and Jegelka [2020], the
analysis of which is of independent interest. We conduct and showcase several simulation
studies to demonstrate the eﬃcacy of our algorithms.
1
Introduction
With machine learning systems increasingly being deployed in real-world settings, there is
an urgent need for online learning algorithms that can minimize cumulative costs over the
long run, even in the face of complete uncertainty about future outcomes. There exist a
myriad of works that deal with this setting, most prominently in the area of online learning
and bandits [Cesa-Bianchi and Lugosi, 2006, Lattimore and Szepesv´ari, 2020]. The majority
of this literature deals with problems where the decisions are taken from either a small set
(such as in the multi armed bandit framework [Auer, 2002]), a continuous decision space (as in
linear bandits [Auer, 2002, Dani et al., 2008]) or in the case the decision set is combinatorial
in nature, the response is often assumed to maintain a simple functional relationship with the
input (e.g., linear Cesa-Bianchi and Lugosi [2012]).
In this paper, we depart from these assumptions and explore what we believe is a more
realistic type of model for the setting where the actions can be encoded as selecting a subset
of a universe of size n. We study a sequential interaction between an agent and the world that
takes place in rounds. At the beginning of round t, the agent chooses a subset St ⊆[n] (e.g.,
selecting the set of products in a factory [McCormick, 2005]), after which the agent suﬀers
cost ft(St) such that ft is an α−weakly DR-submodular and β−weakly DL-supermodular
function [Lehmann et al., 2006]. The agent then may receive extra information about ft as
⋆Tianyi Lin, Aldo Pacchiano and Yaodong Yu contributed equally to this work.
1
arXiv:2205.07217v2  [cs.LG]  1 Jun 2022

feedback, for example in the full information setting the agent observes the whole function ft
and in the bandit feedback scenario the learner does not receive any extra information about
ft beyond the value of ft(St). The standard metric to measure an online learning algorithm is
regret [Blum and Mansour, 2007]: the regret at time T is the diﬀerence between PT
t=1 ft(St)
that is the total cost achieved by the algorithm and minx∈A
PT
t=1 ft(x) that is the total cost
achieved by the best ﬁxed action in hindsight. A no-regret learning algorithm is one that
achieves sublinear regret (as a function of T). Many no-regret learning algorithms have been
developed based on online convex optimization toolbox [Zinkevich, 2003, Kalai and Vempala,
2005, Shalev-Shwartz and Singer, 2006, Hazan et al., 2007, Shalev-Shwartz, 2011, Arora et al.,
2012, Hazan, 2016] many of them achieving minimax-optimal regret bounds for diﬀerent cost
functions even when these are produced by the world in an adversarial fashion. However, many
online decision-making problems remain open, for example when the decision space is discrete
and large (e.g., exponential in the number of problem parameters) and the cost functions are
nonlinear [Hazan and Kale, 2012].
To the best of our knowledge, Hazan and Kale [2012] were the ﬁrst to investigate non-
parametric online learning in combinatorial domains by considering the setting where the
costs ft are all submodular functions. In this formulation the decision space is the set of
all subsets of a set of n elements; and the cost functions are submodular. They provided
no-regret algorithms for both the full information and bandit settings. Their chief innovation
was to propose a computationally eﬃcient algorithm for online submodular learning that
resolved the exponential computational and statistical dependence on n suﬀered by all previous
approaches [Hazan and Kale, 2012]. These results served as a catalyst for a rich and expanding
research area [Streeter and Golovin, 2008, Jegelka and Bilmes, 2011, Buchbinder et al., 2014,
Chen et al., 2018c, Roughgarden and Wang, 2018, Chen et al., 2018b, Cardoso and Cummings,
2019, Anari et al., 2019, Harvey et al., 2020, Thang and Srivastav, 2021, Matsuoka et al., 2021].
Even though submodularity can be used to model a few important typical cost functions
that arise in machine learning problems [Boykov et al., 2001, Boykov and Kolmogorov, 2004,
Narasimhan et al., 2005, Bach, 2010], it is an insuﬃcient assumption for many other applications
where the cost functions do not satisfy submodularity, e.g., structured sparse learning [El Halabi
and Cevher, 2015], batch Bayesian optimization [Gonz´alez et al., 2016, Bogunovic et al., 2016],
Bayesian A-optimal experimental design [Bian et al., 2017], column subset selection [Sviridenko
et al., 2017] and so on. In this work we aim to ﬁll in this gap. In view of all this, we consider
the following question:
Can we design online learning algorithms when the cost functions are
nonsubmodular?
This paper provides an aﬃrmative answer to this question by demonstrating that online/bandit
approximate gradient descent algorithm can be directly extended from online submodular
minimization [Hazan and Kale, 2012] to online nonsubmodular minimization when each cost
functions ft satisfy the regularity condition in El Halabi and Jegelka [2020].
Moreover, in online decision-making there is often a signiﬁcant delay between decision and
feedback. This delay has an adverse eﬀect on the characterization between marketing feedback
and an agent’s decision [Quanrud and Khashabi, 2015, H´eliou et al., 2020]. For example, a
click on an ad can be observed within seconds of the ad being displayed, but the corresponding
sale can take hours or days to occur. We extend all of our algorithms to the delayed feedback
setting by leveraging a pooling strategy recently introduced by H´eliou et al. [2020] into the
framework of online/bandit approximate gradient descent.
2

Contribution.
First, we introduce a new notion of (α, β)-regret which allows for analyzing
no-regret online learning algorithms when the loss functions are nonsubmodular. We then
propose two randomized algorithms for both the full-information and bandit feedback settings
respectively with the regret bounds in expectation and high-probability sense. We then
combine the aforementioned algorithms with the pooling strategy found in [H´eliou et al., 2020]
and prove that the resulting algorithms are no-regret even when the delays are unbounded (cf.
Assumption 5.1). Speciﬁcally, when the delay dt satisﬁes dt = o(tγ), we establish a O(
√
nT 1+γ)
regret bound in full-information setting and a O(nT
2+γ
3 ) regret bound in bandit feedback
setting. To our knowledge, this is the ﬁrst theoretical guarantee for no-regret learning in online
nonsubmodular minimization with delayed costs. Experimental results on sparse learning with
synthetic data conﬁrm our theoretical ﬁndings.
It is worth comparing our results with that in the existing works [El Halabi and Jegelka,
2020, Hazan and Kale, 2012, H´eliou et al., 2020]. First of all, the results concerning online
nonsubmodular minimization are not a straightforward consequence of El Halabi and Jegelka
[2020]. Indeed, it is natural yet nontrivial to identify the notion of (α, β)-regret under which
formal guarantees can be established for the nonsubmodular case. This notion does not appear
before and appears to be a novel idea and an interesting conceptual contribution. Further,
our results provide the ﬁrst theoretical guarantee for no-regret learning in online and bandit
nonsubmodular minimization and generalize the results in Hazan and Kale [2012]. Even though
the online and bandit learning algorithms and regret analysis share the similar spirits with the
context of Hazan and Kale [2012], the proof techniques are diﬀerent since we need to deal with
the nonsubmodular case with (α, β)-regret. Finally, we are not aware of any results on online
and bandit combinatorial optimization with delayed costs. H´eliou et al. [2020] focused on the
gradient-free game-theoretical learning with delayed costs where the action sets are continuous
and bounded. Thus, their results can not imply ours. The only component that two works
share is the pooling strategy which has been a common algorithmic component to handle the
delays. Even though the pooling strategy is crucial to our delayed algorithms, we make much
eﬀorts to combine them properly and prove (α, β)-regret bound of our new algorithms.
Notation.
We let [n] be the set {1, 2, . . . , n} and Rn
+ be the set of all vectors in Rn with
nonnegative components. We denote 2[n] as the set of all subsets of [n]. For a set S ⊆[n],
we let χS ∈{0, 1}n be the characteristic vector satisfying that χS(i) = 1 for each i ∈S and
χS(i) = 0 for each i /∈S. For a function f : 2[n] 7→R, we denote the marginal gain of adding
an element i to S by f(i | S) = f(S ∪{i}) −f(S). In addition, f is normalized if f(∅) = 0 and
nondecreasing if f(A) ≤f(B) for A ⊆B. For a vector x ∈Rn, its Euclidean norm refers to ∥x∥
and its i-th entry refers to xi. We denote the support set of x by supp(x) = {i ∈[n] : xi ̸= 0}
and, by abuse of notation, we let x deﬁne a set function x(S) = P
i∈S xi. We let PS be the
projection onto a closed set S and dist(x, S) = infy∈S ∥x −y∥denotes the distance between x
and S. A pair of parameters (α, β) ∈R+ × R+ in the regret refer to approximation factors
of the corresponding oﬄine setting. Lastly, a = O(b(α, β, n, T)) refers to an upper bound
a ≤C · b(α, β, n, T) where C > 0 is independent of α, β, n and T.
2
Related Work
The oﬄine nonsubmodular optimization with diﬀerent notions of approximate submodularity
has recently received a lot of attention.
Most research focused on the maximization of
nonsubmodular set functions, emerging as an important paradigm for studying real-world
3

application problems [Das and Kempe, 2011, Horel and Singer, 2016, Chen et al., 2018a,
Kuhnle et al., 2018, Hassidim and Singer, 2018, Elenberg et al., 2018, Harshaw et al., 2019]. In
contrast, we are aware of relatively few investigations into the minimization of nonsubmodular
set functions. An interesting example is the ratio problem [Bai et al., 2016] where the objective
function to be minimized is the ratio of two set functions and is thus nonsubmodular in
general. Note that the ratio problem does not admit a constant factor approximation even
when two set functions are submodular [Svitkina and Fleischer, 2011]. However, if the objective
function to be minimized is approximately modular with bounded curvature, the optimal
approximation algorithms exist even when the constrain sets are assumed [Iyer et al., 2013].
Another typical example is the minimization of the diﬀerence of two submodular functions,
where some approximation algorithms were proposed in Iyer and Bilmes [2012] and Kawahara
et al. [2015] but without any approximation guarantee. Very recently, El Halabi and Jegelka
[2020] provided a comprehensive treatment of optimal approximation guarantees for minimizing
nonsubmodular set functions, characterized by how close the function is to submodular. Our
work is close to theirs and our results can be interpreted as the extension of El Halabi and
Jegelka [2020] to online learning with delayed feedback.
Another line of relevant works comes from online learning literature and focuses on no-
regret algorithms in diﬀerent settings with delayed costs. In the context of online convex
optimization, Quanrud and Khashabi [2015] proposed an extension of online gradient descent
(OGD) where the agent performs a batched gradient update the moment gradients are received
and proved that OGD achieved a regret bound of O(√T + DT ) where DT is the total delay
over a horizon T. However, their batch update approach can not be extended to bandit
convex optimization since it does not work with stochastic estimates of the received gradient
information (or when attempting to infer such information from realized costs). This issue was
posted by Zhou et al. [2017] and recently resolved by H´eliou et al. [2020] who proposed a new
pooling strategy based on a priority queue. The eﬀect of delay was also discussed in the multi-
armed bandit (MAB) literature under diﬀerent assumptions [Joulani et al., 2013, 2016, Vernade
et al., 2017, Pike-Burke et al., 2018, Thune et al., 2019, Bistritz et al., 2019, Zhou et al., 2019,
Zimmert and Seldin, 2020, Gyorgy and Joulani, 2021]. In particular, Thune et al. [2019] proved
the regret bound in adversarial MABs with the cumulative delay and Gyorgy and Joulani [2021]
studied the adaptive tuning to delays and data in this setting. Further, Joulani et al. [2016]
and Zimmert and Seldin [2020] also investigated adaptive tuning to the unknown sum of delays
while Bistritz et al. [2019] and Zhou et al. [2019] gave further results in adversarial and linear
contextual bandits respectively. However, the algorithms developed in the aforementioned
works have little to do with online nonsubmodular minimization with delayed costs.
3
Preliminaries and Technical Background
We present the basic setup for minimizing structured nonsubmodular functions, including
motivating examples and convex relaxation based on Lov´asz extension. We extend the oﬄine
setting to online setting and (α, β)-regret which is important to the subsequent analysis.
3.1
Structured nonsubmodular function
Minimizing a set function f : 2[n] 7→R is NP-hard in general but is solved exactly with
submodular structure in polynomial time [Iwata, 2003, Gr¨otschel et al., 2012, Lee et al., 2015]
and in strongly polynomial time [Schrijver, 2000, Iwata et al., 2001, Iwata and Orlin, 2009,
Orlin, 2009, Lee et al., 2015]. More speciﬁcally, f is submodular if it satisﬁes the diminishing
4

returns (DR) property as follows,
f(i | A) ≥f(i | B),
for all A ⊆B, i ∈[n] \ B.
(1)
Further, f is modular if the inequality in Eq. (1) holds as an equality and is supermodular if
f(i | B) ≥f(i | A),
for all A ⊆B, i ∈[n] \ B.
Relaxing these inequalities will bring us the notions of weak DR-submodularity/supermodularity
that were introduced by Lehmann et al. [2006] and revisited in the machine learning litera-
ture [Bian et al., 2017]. Formally, we have
Deﬁnition 3.1. A set function f : 2[n] 7→R is α-weakly DR-submodular with α > 0 if
f(i | A) ≥αf(i | B),
for all A ⊆B, i ∈[n] \ B.
Similarly, f is β-weakly DR-supermodular with β > 0 if
f(i | B) ≥βf(i | A),
for all A ⊆B, i ∈[n] \ B.
We say that f is (α, β)-weakly DR-modular if both of the above two inequalities hold true.
The above notions of weak DR-submodularity (or weak DR-supermodularity) generalize
the notions of submodularity (or supermodularity); indeed, we have f is submodular (or
supermodular) if and only if α = 1 (or β = 1). They are also special cases of more general
notions of weak submodularity (or weak supermodularity) [Das and Kempe, 2011] and we refer
to Bogunovic et al. [2018, Proposition 1] and El Halabi et al. [2018, Proposition 8] for the details.
For an overview of the approximate submodularity, we refer to Bian et al. [2017, Section 6]
and El Halabi and Jegelka [2020, Figure 1]. In addition, the parameters 1 −α and 1 −β are
referred to as generalized inverse curvature and generalized curvature respectively [Bian et al.,
2017, Bogunovic et al., 2018] and can be interpreted as the extension of inverse curvature
and curvature [Conforti and Cornu´ejols, 1984] for submodular and supermodular functions.
Intuitively, these parameters quantify how far the function f is from being a submodular (or
supermodular) function.
Recently, El Halabi and Jegelka [2020] have proposed and studied the problem of minimizing
a class of structured nonsubmodular functions as follows,
min
S⊆[n] f(S) := ¯f(S) −f(S),
(2)
where ¯f and f are both normalized (i.e., ¯f(∅) = f(∅) = 0)1 and nondecreasing, ¯f is α-weakly
DR-submodular and f is β-weakly DR-supermodular. Note that the problem in Eq. (2) is
challenging; indeed, f is neither weakly DR-submodular nor weakly DR-supermodular in
general since the weak DR-submodularity (or weak DR-supermodularity) are only valid for
monotone functions.
It is worth mentioning that Eq. (2) is not necessarily theoretically artiﬁcial but encompasses
a wide range of applications. We present two typical examples which can be formulated in the
form of Eq. (2) and refer to El Halabi and Jegelka [2020, Section 4] for more details.
1In general, we can let ¯f(S) ←¯f(S) −¯f(∅) and f(S) ←f(S) −f(∅) which will not change the minimization
problem.
5

Example 3.1 (Structured Sparse Learning). We aim to estimate a sparse parameter vector
whose support satisﬁes a particular structure and commonly formulate such problems as
minx∈Rn ℓ(x) + λf(supp(x)), where ℓ: Rn 7→R is a loss function and f : 2[n] 7→R is a set
function favoring the desirable supports. Existing approaches such as [Bach, 2010] proposed
to replace the discrete regularization function f(supp(x)) by its closest convex relaxation
and is computationally tractable only when f is submodular. However, this problem is often
better modeled by a nonsubmodular regularizer in practice [El Halabi and Cevher, 2015]. An
alternative formulation of structured sparse learning problems is
min
S⊆[n] λf(S) −h(S),
(3)
where h(S) = ℓ(0) −minsupp(x)⊆S ℓ(x). Note that Eq. (3) can be reformulated into the form
of Eq. (2) under certain conditions; indeed, h is a normalized and nondecreasing function
and El Halabi and Jegelka [2020, Proposition 5] has shown that h is weakly DR-modular if
ℓis smooth, strongly convex and is generated from random data. Examples of weakly DR-
submodular regularizers f include the ones used in time-series and cancer diagnosis [Rapaport
et al., 2008] and healthcare [Sakaue, 2019].
Example 3.2 (Batch Bayesian Optimization). We aim to optimize an unknown expensive-
to-evaluate noisy function ℓwith as few batches of function evaluations as possible. The
evaluation points are chosen to maximize an acquisition function – the variance reduction
function [Gonz´alez et al., 2016] – subject to a cardinality constraint. Maximizing the variance
reduction may be phrased as a special instance of the problems in Eq. (2) in the form of
minS⊆[n] λ|S|−G(S), where G : 2[n] 7→R is the variance reduction function deﬁned accordingly
and El Halabi and Jegelka [2020, Proposition 6] has shown that it is also non-decreasing and
weakly DR-modular. This formulation allows to include nonlinear costs with (weak) decrease
in marginal costs (economies of scale) with some applications in the sensor placement.
3.2
Convex relaxation based on the Lov´asz extension
The Lov´asz extension [Lov´asz, 1983] is a toolbox commonly used for minimizing a submodular
set function f : 2[n] 7→R. It is a continuous interpolation of f on the unit hypercube [0, 1]n
and can be minimized eﬃciently since it is convex if and only if f is submodular. The minima
of the Lov´asz extension also recover the minima of f.
Before the formal argument, we deﬁne a maximal chain of [n]; that is, {A0, . . . , An} is a
maximal chain if ∅= A0 ⊆A1 ⊆. . . ⊆An = [n]. Formally, we have
Deﬁnition 3.2. Given a submodular function f, the Lov´asz extension is the function fL :
[0, 1]n 7→R given by fL(x) = Pn
i=0 λif(Ai) where {A0, . . . , An} is a maximal chain2 of [n]
so that Pn
i=0 λiχAi = x and Pn
i=0 λi = 1 where χAi(j) = 1 for ∀j ∈Ai and χAi(j) = 0 for
∀j /∈S.
Even though Deﬁnition 3.2 implies that fL(χS) = f(S) for all S ⊆[n], it remains unclear
how to ﬁnd the chain or the coeﬃcients. The preceding discussion deﬁnes the Lov´asz extension
in an equivalent way that is more amenable for computing the subgradient of fL.
Let x = (x1, x2, . . . , xn) ∈[0, 1]n and we deﬁne that π : [n] 7→[n] is the sorting permutation
of {x1, x2, . . . , xn} where π(i) = j implies that xj is the i-th largest element. By deﬁnition,
we have 1 ≥xπ(1) ≥. . . ≥xπ(n) ≥0 and let xπ(0) = 1 and xπ(n+1) = 0 for simplicity. Then,
2Both the chain and the set of λi may depend on the input x.
6

we set λi = xπ(i) −xπ(i+1) for all 0 ≤i ≤n and let A0 = ∅and Ai = {π(1), . . . , π(i)} for all
i ∈[n]. We also have
n
X
i=0
λiχAi =
n
X
i=0
(xπ(i) −xπ(i+1))(χAi−1 + eπ(i))
=
n
X
i=1
eπ(i)
n
X
j=i
(xπ(j) −xπ(j+1)) = x.
As such, we obtain that fL(x) = Pn
i=1 xπ(i)f(π(i) | Ai−1) where xπ(1) ≥xπ(2) ≥. . . ≥xπ(n)
are the sorted entries in decreasing order, A0 = ∅and Ai = {π(1), . . . , π(i)} for all i ∈[n].
Then, the classical results [Edmonds, 2003, Fujishige, 2005] suggest that the subgradient g of
fL at any x ∈[0, 1]n can be computed by simply sorting the entries in decreasing order and
taking
gπ(i) = f(Ai) −f(Ai−1), for all i ∈[n].
(4)
Since fL is convex if and only if f is submodular, we can apply the convex optimization toolbox
here. Recently, El Halabi and Jegelka [2020] have shown that the similar idea can be extended
to nonsubmodular optimization in Eq. (2).
More speciﬁcally, we can deﬁne the convex closure fC for any nonsubmodular function f;
indeed, fC : [0, 1]n 7→R is the point-wise largest convex function which always lower bounds f.
By deﬁnition, fC is the tightest convex extension of f and minS⊆[n] f(S) = minx∈[0,1]n fC(x).
In general, it is NP-hard to evaluate and optimize fC [Vondr´ak, 2007]. Fortunately, El Halabi
and Jegelka [2020] demonstrated that the Lov´asz extension fL approximates fC such that the
vector computed using the approach in Edmonds [2003] and Fujishige [2005] approximates the
subgradient of fC. We summarize their results in the following proposition and provide the
proofs in Appendix A for completeness.
Proposition 3.1. Focusing on Eq. (2), we let x ∈[0, 1]n with xπ(1) ≥. . . ≥xπ(n) and
gπ(i) = f(Ai) −f(Ai−1) for all i ∈[n] where A0 = ∅and Ai = {π(1), . . . , π(i)} for all i ∈[n].
Then, we have
fL(x) = g⊤x ≥fC(x),
(5)
and
g(A) =
X
i∈A
gi ≤1
α ¯f(A) −βf(A), for all A ⊆[n],
(6)
and
g⊤z ≤1
α ¯fC(z) + β(−f)C(z), for all z ∈[0, 1]n.
(7)
Proposition 3.1 highlights how fL approximates fC; indeed, we see from Eq. (5) and Eq. (7)
that fC(x) ≤fL(x) ≤1
α ¯fC(x) + β(−f)C(x) for all x ∈[0, 1]n. As such, it gives the key insight
for analyzing the oﬄine algorithms in El Halabi and Jegelka [2020] and will play an important
role in the subsequent analysis of our paper.
3.3
Online nonsubmodular minimization
We consider online nonsubmodular minimization which extends the oﬄine problem in Eq. (2)
to the online setting. In particular, an adversary ﬁrst chooses structured nonsubmodular
functions f1, f2, . . . , fT : 2[n] 7→R given by
ft(S) := ¯ft(S) −ft(S), for all S ⊆[n], t ∈[T],
(8)
7

where ¯ft and ft are normalized and non-decreasing, ¯ft is α-weakly DR-submodular and ft is
β-weakly DR-supermodular. In each round t = 1, 2, . . . , T, the agent chooses St and observes
the incurred loss ft(St) after committing to her decision. Throughout the horizon [0, T], one
aims to minimize the regret – the diﬀerence between PT
t=1 ft(St) and the loss at the best ﬁxed
solution in hindsight, i.e., ST
⋆= argminS⊆[n]
PT
t=1 ft(S) – which is deﬁned by3
R(T) =
T
X
t=1
ft(St) −
T
X
t=1
ft(ST
⋆).
(9)
An algorithm is no-regret if R(T)/T →0 as T →+∞and eﬃcient if it computes each decision
set St in polynomial time. In the context, the regret is used when the minimization for a
known cost, i.e., minS⊆[n] f(S), can be solved exactly. However, solving the optimization
problem in Eq. (2) with nonsubmodular costs is NP-hard regardless of any multiplicative
constant factor [Iyer and Bilmes, 2012, Trevisan, 2014]. Thus, it is necessary to consider a
bicriteria-like approximation guarantee with the factors α, β > 0 as El Halabi and Jegelka
[2020] suggested. In particular, (α, β) are bounds on the quality of a solution S returned by a
given oﬄine algorithm compared to the optimal solution ¯S; that is, f(S) ≤1
α ¯f(S⋆) −βf(S⋆).
Such bicriteria-like approximation is optimal: El Halabi and Jegelka [2020, Theorem 2] has
shown that no algorithm with subexponential number of value queries can improve on it in
the oracle model.
Our goal is to analyze online approximate gradient descent algorithm and its bandit variant
for online nonsubmodular minimization. Let (α, β) be the approximation factors attained
by an oﬄine algorithm that solves minS⊆[n] f(S) for a known nonsubmodular function f in
Eq. (2). The (α, β)-regret compares to the best solution that can be expected in polynomial
time and is deﬁned by
Rα,β(T) =
T
X
t=1
ft(St) −
T
X
t=1
( 1
α ¯ft(ST
⋆) −βft(ST
⋆)),
(10)
where ST
⋆= argminS⊆[n]
PT
t=1 ft(S). It is analogous to the α-regret which is widely used in
online constrained submodular minimization [Jegelka and Bilmes, 2011] and online submodular
maximization [Streeter and Golovin, 2008].
As mentioned before, we consider the algorithmic design in both full information and
bandit feedback settings. In the former one, the agent is allowed to have unlimited access to
the value oracles of ft(·) after choosing St in each round t. In the latter one, the agent only
observes the incurred loss at the point that she has chosen in each round t, i.e., ft(St), and
receives no other information.
4
Online Approximation Algorithm
We analyze online approximate gradient descent algorithm and its bandit variant for regret
minimization when the nonsubmodular cost functions are in the form of Eq (8). Due to space
limit, we defer the proofs to Appendix B and C.
4.1
Full information setting
Let [0, 1]n be the unit hypercube and the cost function on [0, 1]n corresponding to ft is the
function (ft)C that is the convex closure of ft. Equipped with Proposition 3.1, we can compute
3If the sets St are chosen by a randomized algorithm, we consider the expected regret over the randomness.
8

Algorithm 1: Online Approximate Gradient Descent
1: Initialization: the point x1 ∈[0, 1]n and the stepsize η > 0;
2: for t = 1, 2, . . . do
3:
Let xt
π(1) ≥. . . xt
π(n) be the sorted entries in the decreasing order with
At
i = {π(1), . . . , π(i)} for all i ∈[n] and At
0 = ∅. Let xt
π(0) = 1 and xt
π(n+1) = 0.
4:
Let λt
i = xt
π(i) −xt
π(i+1) for all 0 ≤i ≤n.
5:
Sample St from the distribution P(St = At
i) = λt
i for all 0 ≤i ≤n and observe the new
loss function ft.
6:
Compute gt
π(i) = ft(At
i) −ft(At
i−1) for all i ∈[n].
7:
Compute xt+1 = P[0,1]n(xt −ηgt).
8: end for
approximate subgradients of (ft)C such that the online gradient descent [Zinkevich, 2003] is
applicable.
This leads to Algorithm 1 which performs one-step projected gradient descent that yields
xt and then samples St from the distribution λ over {Ai}n
i=0 encoded by xt. It is worth
mentioning that λt
i = xt
π(i) −xt
π(i+1) for all 0 ≤i ≤n and λ is thus completely independent of
ft. This guarantees that Algorithm 1 is valid in online manner since ft is realized after the
decision maker chooses St. One of the advantages of Algorithm 1 is that it does not require
the value of α and β which can be hard to compute in practice. We summarize our results for
Algorithm 1 in the following theorem.
Theorem 4.1. Suppose the adversary chooses nonsubmodular functions in Eq. (8) satisfying
¯ft([n]) + ft([n]) ≤L. Fixing T ≥1 and letting η =
√n
L
√
T in Algorithm 1, we have E[Rα,β(T)] =
O(
√
nT) and Rα,β(T) = O(
√
nT +
p
T log(1/δ)) with probability 1 −δ.
Remark 4.2. Theorem 4.1 demonstrates that Algorithm 1 is regret-optimal for our setting;
indeed, our setting includes online unconstrained submodular minimization as a special case
where (α, β)-regret becomes standard regret in Eq. (9) and Hazan and Kale [2012] shows that
Algorithm 1 is optimal up to constants. Our theoretical result also extends the results in Hazan
and Kale [2012] from submodular cost functions to nonsubmodular cost functions in Eq. (8)
using the (α, β)-regret instead of the standard regret in Eq. (9).
4.2
Bandit feedback setting
In contrast with the full-information setting, the agent only observes the loss function ft at
her action St, i.e., ft(St), in bandit feedback setting. This is a more challenging setup since
the agent does not have full access to the new loss function ft at each round t yet.
Despite the bandit feedback, we can compute an unbiased estimator of the gradient gt in
Algorithm 1 using the technique of importance weighting and try to implement a stochastic
version of Algorithm 1. More speciﬁcally, we notice that ˆft
i = 1(St=At
i)
λt
i
ft(St) is unbiased for
estimating ft(At
i) for all 0 ≤i ≤n. Thus, ˆgt
π(i) = ˆft
i −ˆft
i−1 for all i ∈[n] gives us an unbiased
estimator of the gradient gt. However, the variance of the estimator ˆg could be undesirably
large since the values of λt
i may be arbitrarily small.
To resolve this issue, we can sample St from a mixture distribution that combines (with
probability 1 −µ) samples from λt and (with probability µ) samples from the uniform
9

Algorithm 2: Bandit Approximate Gradient Descent
1: Initialization: the point x1 ∈[0, 1]n and the stepsize η > 0; the exploration probability
µ ∈(0, 1).
2: for t = 1, 2, . . . , T do
3:
Let xt
π(1) ≥. . . xt
π(n) be the sorted entries in decreasing order with
At
i = {π(1), . . . , π(i)} for all i ∈[n] and At
0 = ∅. Let xt
π(0) = 1 and xt
π(n+1) = 0.
4:
Let λt
i = xt
π(i) −xt
π(i+1) for all 0 ≤i ≤n.
5:
Sample St from the distribution P(St = At
i) = (1 −µ)λt
i +
µ
n+1 for all 0 ≤i ≤n and
observe the loss ft(St).
6:
Compute ˆft
i =
1(St=At
i)
(1−µ)λt
i+µ/(n+1)ft(St) for all 0 ≤i ≤n.
7:
Compute ˆgt
π(i) = ˆft
i −ˆft
i−1 for all i ∈[n].
8:
Compute xt+1 xt+1 = P[0,1]n(xt −ηgt).
9: end for
distribution over {At
i}n
i=0. This guarantees that the variance of ˆft
i is upper bounded by
O(n2/µ). The similar idea has been employed in Hazan and Kale [2012] for online submodular
minimization. Then, we conduct the careful analysis for the estimators ˆgt such that the scale
of the variance is taken into account. Note that our analysis is diﬀerent from the standard
analysis in Flaxman et al. [2005] which seems oversimpliﬁed for our setting and results in
worse regret of O(T 3/4) compared to our result in the following theorem.
Theorem 4.3. Suppose the adversary chooses nonsubmodular functions ft in Eq. (8) satisfying
¯ft([n]) + ft([n]) ≤L. Fixing T ≥1 and letting (η, µ) = (
1
LT 2/3 ,
n
T 1/3 ) in Algorithm 2, we have
E[Rα,β(T)] = O(nT
2
3 ) and Rα,β(T) = O(nT
2
3 +
p
n log(1/δ)T
2
3 ) with probability 1 −δ.
Remark 4.4. Theorem 4.3 demonstrates that Algorithm 2 is no-regret for our setting even
when only the bandit feedback is available, further extending the results in Hazan and Kale
[2012] from submodular cost functions to nonsubmodular cost functions in Eq. (8) using the
(α, β)-regret instead of the standard regret in Eq. (9).
5
Online Delayed Approximation Algorithm
We investigate Algorithm 1 and 2 for regret minimization even when the delay between choosing
an action and receiving the incurred cost exists and can be unbounded.
5.1
The general framework
The general online learning framework with large delay that we consider can be represented as
follows. In each round t = 1, . . . , T, the agent chooses the decision St ⊆[n] and this generates
a loss ft(St). Simultaneously, St triggers a delay dt ≥0 which determines the round t + dt at
which the information about ft will be received. Finally, the agent receives the information
about ft from all previous rounds Rt = {s : s + ds = t}.
The above model has been stated in an abstract way as the basis for the regret analysis.
The information about ft is determined by whether the setting is full information or bandit
feedback. Our blanket assumptions for the stream of the delays encountered will be:
Assumption 5.1. The delays dt = o(tγ) for some γ < 1.
10

Algorithm 3: Delay Online Approximate Gradient Descent
1: Initialization: the point x1 ∈[0, 1]n and the stepsize ηt > 0; P0 ←∅and f∞= 0.
2: for t = 1, 2, . . . do
3:
Let xt
π(1) ≥. . . xt
π(n) be the sorted entries in the decreasing order with
At
i = {π(1), . . . , π(i)} for all i ∈[n] and At
0 = ∅. Let xt
π(0) = 1 and xt
π(n+1) = 0.
4:
Let λt
i = xt
π(i) −xt
π(i+1) for all 0 ≤i ≤n.
5:
Sample St from the distribution P(St = At
i) = λt
i for 0 ≤i ≤n and observe the new
loss function ft.
6:
Compute gt
π(i) = ft(At
i) −ft(At
i−1) for all i ∈[n] and then trigger a delay dt ≥0.
7:
Let Rt = {s : s + ds = t} and Pt ←Pt−1 ∪Rt. Take qt = min Pt and set Pt ←Pt \ {qt}.
8:
Compute xt+1 using Eq. (11).
9: end for
Assumption 5.1 is not theoretically artiﬁcial but uncovers that long delays are observed in
practice [Chapelle, 2014]; indeed, the data statistics from real-time bidding company suggested
that more than 10% of the conversions were ≥2 weeks old. More speciﬁcally, Chapelle [2014]
showed that the delays in online advertising have long-tail distributions when conditioning
on context and feature variables available to the advertiser, thus justifying the existence of
unbounded delays. Note that Assumption 5.1 is mild and the delays can even be adversarial
as in Quanrud and Khashabi [2015].
5.2
Full information setting
At the round t, the agent receives the loss function fs(·) for Rt = {s : s + ds = t} after
committing her decision, i,e., gets to observe fs(At
i) for all s ∈Rt and all 0 ≤i ≤n. To let
Algorithm 1 handle these delays, the ﬁrst thing to note is that the set Rt received at a given
round might be empty, i.e., we could have Rt = ∅for some t ≥1. Following up the pooling
strategy in H´eliou et al. [2020], we assume that, as information is received over time, the agent
adds it to an information pool Pt and then uses the oldest information available in the pool
(where “oldest” stands for the time at which the information was generated).
Since no information is available at t = 0, we have P0 = ∅and update the agent’s
information pool recursively: Pt = Pt−1 ∪Rt \ {qt} where qt = min(Pt−1 ∪Rt) denotes the
oldest round from which the agent has unused information at round t. As H´eliou et al. [2020]
pointed out, this scheme can be seen as a priority queue where {fs(·), s ∈Rt} arrives at
time t and is assigned in order; subsequently, the oldest information is utilized at ﬁrst. An
important issue that arises in the above computation is that, it may well happen that the
agent’s information pool Pt is empty at time t (e.g., if we have d1 > 0 at time t = 1). Following
the convention that inf ∅= +∞, we set qt = +∞and g∞= 0 (since it is impossible to have
information at time t = +∞). Under this convection, the computation of a new iterate xt+1
at time t can be written more explicitly form as follows,
xt+1 =
 xt
if Pt = ∅,
P[0,1]n(xt −ηtgqt),
otherwise.
(11)
We present a delayed variant of Algorithm 1 in Algorithm 3. There is no information aggregation
here but the updates of xt+1 follows the pooling policy induced by a priority queue. We
summarize our results in the following theorem.
11

Algorithm 4: Delay Bandit Approximate Gradient Descent
1: Initialization: the point x1 ∈[0, 1]n and the stepsize ηt > 0; P0 ←∅and f∞= 0; the
exploration probability µt ∈(0, 1).
2: for t = 1, 2, . . . do
3:
Let xt
π(1) ≥. . . xt
π(n) be the sorted entries in the decreasing order with
At
i = {π(1), . . . , π(i)} for all i ∈[n] and At
0 = ∅. Let xt
π(0) = 1 and xt
π(n+1) = 0.
4:
Let λt
i = xt
π(i) −xt
π(i+1) for all 0 ≤i ≤n.
5:
Sample St from the distribution P(St = At
i) = (1 −µt)λt
i +
µt
n+1 for 0 ≤i ≤n and
observe the loss ft(St).
6:
Compute ˆft
i =
1(St=At
i)
(1−µt)λt
i+µt/(n+1)ft(St).
7:
Compute ˆgt
π(i) = ˆft
i −ˆft
i−1 for all i ∈[n] and then trigger a delay dt ≥0.
8:
Let Rt = {s : s + ds = t} and Pt ←Pt−1 ∪Rt. Take qt = min Pt and set Pt ←Pt \ {qt}.
9:
Compute xt+1 using Eq. (12).
10: end for
Theorem 5.2. Suppose the adversary chooses nonsubmodular functions in Eq. (8) satisfying
¯ft([n]) + ft([n]) ≤L and let the delays satisfy Assumption 5.1. Fixing T ≥1 and letting
ηt =
√n
L
√
t1+γ in Algorithm 3, we have E[Rα,β(T)] = O(
√
nT 1+γ) and Rα,β(T) = O(
√
nT 1+γ +
p
T log(1/δ)) with probability 1 −δ.
Remark 5.3. Theorem 5.2 demonstrates that Algorithm 3 is no-regret if Assumption 5.1
hold. To our knowledge, this is the ﬁrst theoretical guarantee for no-regret learning in online
nonsubmodular minimization with delayed costs and also complement similar results for online
convex optimization with delayed costs [Quanrud and Khashabi, 2015].
5.3
Bandit feedback setting
As we have done in the previous section, we will make use of an unbiased estimator ˆg of the
gradient for the bandit feedback setting. However, we only receive the old estimator ˆgqt at the
round t due to the delay dt. Following the same reasoning as in the full information setting,
the computation of a new iterate xt+1 at time t can be written more explicitly form as follows,
xt+1 =
 xt
if Pt = ∅,
P[0,1]n(xt −ηtˆgqt),
otherwise.
(12)
Algorithm 4 follows the same template as Algorithm 3 but substituting the exact gradients
with the gradient estimator. We summarize our results in the following theorem.
Theorem 5.4. Suppose the adversary chooses nonsubmodular functions in Eq. (8) satisfying
¯ft([n]) + ft([n]) ≤L and let the delays satisfy Assumption 5.1. Fixing T ≥1 and letting
(ηt, µt) = (
1
Lt(2+γ)/3 ,
n
t(1−γ)/3 ) in Algorithm 4, we have E[Rα,β(T)] = O(nT
2+γ
3 ) and Rα,β(T) =
O(nT
2+γ
3
+
p
n log(1/δ)T
4−γ
6 ) with probability 1 −δ.
Remark 5.5. Theorem 5.4 demonstrates that Algorithm 4 attains the regret of nT
2+γ
3
which
is worse that that of
√
nT 1+γ for Algorithm 3 and reduces to that of nT
2
3 for Algorithm 2.
Since γ < 1 is assumed, Algorithm 4 is the ﬁrst no-regret bandit learning algorithm for online
nonsubmodular minimization with delayed costs to our knowledge.
12

0
2000
4000
6000
8000
10000
Round
0
200
400
600
Regret
(Full-information) OAGD v.s. DOAGD
OAGD
DOAGD: max=500
DOAGD: max=1000
DOAGD: max=2000
(a) Eﬀect of delay under the full-
information setting.
0
2000
4000
6000
8000
10000
Round
0
1000
2000
3000
Regret
(Bandit) BAGD v.s. DBAGD
BAGD
DBAGD: max=500
DBAGD: max=1000
DBAGD: max=2000
(b) Eﬀect of delay under the bandit
feedback setting.
0
2000
4000
6000
8000
10000
Round
0
1000
2000
3000
Regret
Full-information v.s. Bandit
(Full-information) OAGD
(Full-information) DOAGD
(Bandit) BAGD
(Bandit) DBAGD
(c) Eﬀect of bandit feedback and
delay.
Figure 1: Comparison of our algorithms on sparse learning with delayed costs. In (a) and (b),
we examine the eﬀect of delay in the full-information and bandit settings respectively where
the maximum delay d ∈{500, 1000, 2000}. In (c), we examine the eﬀect of bandit feedback by
comparing the online algorithm with its bandit version where the maximum delay d = 500.
6
Experiments
We conduct the numerical experiments on structured sparse learning problems and include
Algorithm 1-4, which we refer to as OAGD, BAGD, DOAGD, and DBAGD. All the experiments
are implemented in Python 3.7 with a 2.6 GHz Intel Core i7 and 16GB of memory. For all
our experiments, we set total number of rounds T = 10, 000, dimension d = 10, number of
samples (for round t) n = 100, and sparse parameter k = 2. For OAGD and DOAGD, we set
the default step size ηo = √n/(L
√
T) (as described in Theorem 4.1). For BAGD and DBAGD,
we set the default step size ηb = 1/(LT 2/3) (as described in Theorem 4.3).
Our goal is to estimate the sparse vector x⋆∈Rd using the structured nonsubmodular
model (see Example 3.1). Following the setup in El Halabi and Jegelka [2020], we let the
function fr be the regularization in Eq. (3) such that f(S) = fr(S) = max(S) −min(S) + 1
for all S ̸= ∅and fr(∅) = 0. We generate true solution x⋆∈Rd with k consecutive 1’s and
other n −k elements are zeros. We deﬁne the function ht(S) for the round t as follows: let
yt = Atx⋆+ ϵt where each row of At ∈Rn×d is an i.i.d. Gaussian vector and each entry of
ϵt ∈Rn is sampled from a normal distribution with standard deviation equals to 0.01. Then,
we deﬁne the square loss ℓt(x) = ∥Atx −yt∥2
2 and let ht(S) = ℓt(0) −minsupp(x)⊆S ℓt(x). We
consider the constant delays in our experiments, i.e., the delay maxt dt ≤d for all t ≥1 where
d > 0 is a constant.
Figure 1 summarizes some of experimental results. Indeed, we see from Figure 1(a) that the
bigger delays lead to worse regret for the full-information setting which conﬁrms Theorem 4.1
and 5.2. The result in Figure 1(b) demonstrates the similar phenomenon for the bandit
feedback setting which conﬁrms Theorem 4.3 and 5.4. Further, Figure 1(c) demonstrates
the eﬀect of bandit feedback and delay simultaneously; indeed, OAGD and DOAGD perform
better than BAGD and DBAGD since the regret will increase if only the bandit feedback is
available. We implement all the algorithms with varying step sizes and summarize the results
in Figure 2 and 3. In the former one, we use step sizes ηo/2 = √n/(2L
√
T) for OAGD and
DOAGD and ηb/2 = 1/(2LT 2/3) for BAGD and DBAGD. In the latter one, we use step sizes
ηo/5 = √n/(5L
√
T) for OAGD and DOAGD, and ηb/5 = 1/(5LT 2/3) for BAGD and DBAGD.
Figure 1-3 demonstrate that our proposed algorithms are not sensitive to the step size choice.
13

0
2000
4000
6000
8000
10000
Round
0
200
400
600
800
Regret
(Full-information) OAGD v.s. DOAGD
OAGD
DOAGD: max=500
DOAGD: max=1000
DOAGD: max=2000
(a) Eﬀect of delay under the full-
information setting.
0
2000
4000
6000
8000
10000
Round
0
1000
2000
3000
4000
5000
Regret
(Bandit) BAGD v.s. DBAGD
BAGD
DBAGD: max=500
DBAGD: max=1000
DBAGD: max=2000
(b) Eﬀect of delay under the bandit
feedback setting.
0
2000
4000
6000
8000
10000
Round
0
1000
2000
3000
4000
Regret
Full-information v.s. Bandit
(Full-information) OAGD
(Full-information) DOAGD
(Bandit) BAGD
(Bandit) DBAGD
(c) Eﬀect of bandit feedback and
delay.
Figure 2: Comparison of our algorithms on sparse learning with delayed costs and step size
η = √n/(2L
√
T) for OAGD and DOAGD, and η = 1/(2LT 2/3) for BAGD and DBAGD. Note
that (a), (b) and (c) follow the same setup as Figure 1.
0
2000
4000
6000
8000
10000
Round
0
250
500
750
1000
1250
Regret
(Full-information) OAGD v.s. DOAGD
OAGD
DOAGD: max=500
DOAGD: max=1000
DOAGD: max=2000
(a) Eﬀect of delay under the full-
information setting.
0
2000
4000
6000
8000
10000
Round
0
2000
4000
6000
Regret
(Bandit) BAGD v.s. DBAGD
BAGD
DBAGD: max=500
DBAGD: max=1000
DBAGD: max=2000
(b) Eﬀect of delay under the bandit
feedback setting.
0
2000
4000
6000
8000
10000
Round
0
2000
4000
6000
Regret
Full-information v.s. Bandit
(Full-information) OAGD
(Full-information) DOAGD
(Bandit) BAGD
(Bandit) DBAGD
(c) Eﬀect of bandit feedback and
delay.
Figure 3: Comparison of our algorithms on sparse learning with delayed costs and step size
η = √n/(5L
√
T) for OAGD and DOAGD, and η = 1/(5LT 2/3) for BAGD and DBAGD. Note
that (a), (b) and (c) follow the same setup as Figure 1.
7
Concluding Remarks
This paper studied online nonsubmodular minimization with special structure through the
lens of (α, β)-regret and the extension of generic convex relaxation model. We proved that
online approximate gradient descent algorithm and its bandit variant adapted for the convex
relaxation model could achieve the bounds of O(
√
nT) and O(nT
2
3 ) in terms of (α, β)-regret
respectively. We also investigated the delayed variants of two algorithms and proved new regret
bounds where the delays can even be unbounded. More speciﬁcally, if delays satisfy dt = o(tγ)
with γ < 1, we showed that our proposed algorithms achieve the regret bound of O(
√
nT 1+γ)
and O(nT
2+γ
3 ) for full-information setting and bandit setting respectively. Simulation studies
validate our theoretical ﬁndings in practice.
8
Acknowledgments
This work was supported in part by the Mathematical Data Science program of the Oﬃce
of Naval Research under grant number N00014-18-1-2764 and by the Vannevar Bush Faculty
Fellowship program under grant number N00014-21-1-2941. The work of Michael Jordan is
also partially supported by NSF Grant IIS-1901252.
14

References
N. Anari, N. Haghtalab, S. Naor, S. Pokutta, M. Singh, and A. Torrico. Structured robust
submodular maximization: Oﬄine and online algorithms. In AISTATS, pages 3128–3137.
PMLR, 2019. (Cited on page 2.)
S. Arora, E. Hazan, and S. Kale. The multiplicative weights update method: A meta-algorithm
and applications. Theory of Computing, 8(1):121–164, 2012. (Cited on page 2.)
P. Auer. Using conﬁdence bounds for exploitation-exploration trade-oﬀs. Journal of Machine
Learning Research, 3(Nov):397–422, 2002. (Cited on page 1.)
F. Bach. Structured sparsity-inducing norms through submodular functions. In NeurIPS,
pages 118–126, 2010. (Cited on pages 2 and 6.)
W. Bai, R. Iyer, K. Wei, and J. Bilmes. Algorithms for optimizing the ratio of submodular
functions. In ICML, pages 2751–2759. PMLR, 2016. (Cited on page 4.)
A. A. Bian, J. M. Buhmann, A. Krause, and S. Tschiatschek. Guarantees for greedy maxi-
mization of non-submodular functions with applications. In ICML, pages 498–507. PMLR,
2017. (Cited on pages 2 and 5.)
I. Bistritz, Z. Zhou, X. Chen, N. Bambos, and J. Blanchet. Online EXP3 learning in adversarial
bandits with delayed feedback. In NeurIPS, page 11349–11358, 2019. (Cited on page 4.)
A. Blum and Y. Mansour. From external to internal regret. Journal of Machine Learning
Research, 8(6), 2007. (Cited on page 2.)
I. Bogunovic, J. Scarlett, A. Krause, and V. Cevher. Truncated variance reduction: A uniﬁed
approach to Bayesian optimization and level-set estimation. In NeurIPS, pages 1507–1515,
2016. (Cited on page 2.)
I. Bogunovic, J. Zhao, and V. Cevher. Robust maximization of non-submodular objectives. In
AISTATS, pages 890–899. PMLR, 2018. (Cited on page 5.)
Y. Boykov and V. Kolmogorov. An experimental comparison of min-cut/max-ﬂow algorithms
for energy minimization in vision. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 26(9):1124–1137, 2004. (Cited on page 2.)
Y. Boykov, O. Veksler, and R. Zabih. Fast approximate energy minimization via graph cuts.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(11):1222–1239, 2001.
(Cited on page 2.)
N. Buchbinder, M. Feldman, and R. Schwartz. Online submodular maximization with preemp-
tion. In SODA, pages 1202–1216. SIAM, 2014. (Cited on page 2.)
A. R. Cardoso and R. Cummings. Diﬀerentially private online submodular minimization. In
AISTATS, pages 1650–1658. PMLR, 2019. (Cited on page 2.)
N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University
Press, 2006. (Cited on pages 1, 23, and 25.)
N. Cesa-Bianchi and G. Lugosi. Combinatorial bandits. Journal of Computer and System
Sciences, 78(5):1404–1422, 2012. (Cited on page 1.)
15

O. Chapelle. Modeling delayed feedback in display advertising. In KDD, pages 1097–1105,
2014. (Cited on page 11.)
L. Chen, M. Feldman, and A. Karbasi. Weakly submodular maximization beyond cardinality
constraints: Does randomization help greedy? In ICML, pages 804–813. PMLR, 2018a.
(Cited on page 4.)
L. Chen, C. Harshaw, H. Hassani, and A. Karbasi. Projection-free online optimization with
stochastic gradient: From convexity to submodularity. In ICML, pages 814–823. PMLR,
2018b. (Cited on page 2.)
L. Chen, H. Hassani, and A. Karbasi. Online continuous submodular maximization. In
AISTATS, pages 1896–1905. PMLR, 2018c. (Cited on page 2.)
M. Conforti and G. Cornu´ejols. Submodular set functions, matroids and the greedy algorithm:
tight worst-case bounds and some generalizations of the Rado-Edmonds theorem. Discrete
Applied Mathematics, 7(3):251–274, 1984. (Cited on page 5.)
V. Dani, T. P. Hayes, and S. M. Kakade. Stochastic linear optimization under bandit feedback.
In COLT, pages 355–366. Omnipress, 2008. (Cited on page 1.)
A. Das and D. Kempe. Submodular meets spectral: Greedy algorithms for subset selection,
sparse approximation and dictionary selection. In ICML, pages 1057–1064, 2011. (Cited on
pages 4 and 5.)
S. Dughmi. Submodular functions: Extensions, distributions, and algorithms. a survey. ArXiv
Preprint: 0912.0322, 2009. (Cited on pages 22, 26, and 33.)
J. Edmonds. Submodular functions, matroids, and certain polyhedra. In Combinatorial
Optimization—Eureka, You Shrink!, pages 11–26. Springer, 2003. (Cited on page 7.)
M. El Halabi and V. Cevher. A totally unimodular view of structured sparsity. In AISTATS,
pages 223–231. PMLR, 2015. (Cited on pages 2 and 6.)
M. El Halabi and S. Jegelka. Optimal approximation for unconstrained non-submodular
minimization. In ICML, pages 3961–3972. PMLR, 2020. (Cited on pages 1, 2, 3, 4, 5, 6, 7, 8,
and 13.)
M. El Halabi, F. Bach, and V. Cevher. Combinatorial penalties: Which structures are preserved
by convex relaxations? In AISTATS, pages 1551–1560. PMLR, 2018. (Cited on page 5.)
E. R. Elenberg, R. Khanna, A. G. Dimakis, and S. Negahban. Restricted strong convexity
implies weak submodularity. The Annals of Statistics, 46(6B):3539–3568, 2018. (Cited on
page 4.)
A. D. Flaxman, A. T. Kalai, and H. B. McMahan. Online convex optimization in the bandit
setting: gradient descent without a gradient. In SODA, pages 385–394, 2005. (Cited on
page 10.)
D. A. Freedman. On tail probabilities for martingales. The Annals of Probability, pages
100–118, 1975. (Cited on page 25.)
S. Fujishige. Submodular Functions and Optimization. Elsevier, 2005. (Cited on page 7.)
16

J. Gonz´alez, Z. Dai, P. Hennig, and N. Lawrence. Batch Bayesian optimization via local
penalization. In AISTATS, pages 648–657. PMLR, 2016. (Cited on pages 2 and 6.)
M. Gr¨otschel, L. Lov´asz, and A. Schrijver. Geometric algorithms and combinatorial optimization,
volume 2. Springer Science & Business Media, 2012. (Cited on page 4.)
A. Gyorgy and P. Joulani. Adapting to delays and data in adversarial multi-armed bandits.
In ICML, pages 3988–3997. PMLR, 2021. (Cited on page 4.)
C. Harshaw, M. Feldman, J. Ward, and A. Karbasi. Submodular maximization beyond
non-negativity: Guarantees, fast algorithms, and applications. In ICML, pages 2634–2643.
PMLR, 2019. (Cited on page 4.)
N. Harvey, C. Liaw, and T. Soma. Improved algorithms for online submodular maximization
via ﬁrst-order regret bounds. In NeurIPS, pages 123–133. Curran Associates, Inc., 2020.
(Cited on page 2.)
A. Hassidim and Y. Singer. Optimization for approximate submodularity. In NeurIPS, pages
394–405, 2018. (Cited on page 4.)
E. Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization,
2(3-4):157–325, 2016. (Cited on page 2.)
E. Hazan and S. Kale. Online submodular minimization. Journal of Machine Learning
Research, 13(10), 2012. (Cited on pages 2, 3, 9, and 10.)
E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex optimiza-
tion. Machine Learning, 69(2-3):169–192, 2007. (Cited on page 2.)
A. H´eliou, P. Mertikopoulos, and Z. Zhou. Gradient-free online learning in continuous games
with delayed rewards. In ICML, pages 4172–4181. PMLR, 2020. (Cited on pages 2, 3, 4, 11, 30,
and 35.)
W. Hoeﬀding. Probability inequalities for sums of bounded random variables. Journal of the
American Statistical Association, 58(301):13–30, 1963. (Cited on page 23.)
T. Horel and Y. Singer. Maximization of approximately submodular functions. In NeurIPS,
pages 3045–3053, 2016. (Cited on page 4.)
S. Iwata. A faster scaling algorithm for minimizing submodular functions. SIAM Journal on
Computing, 32(4):833–840, 2003. (Cited on page 4.)
S. Iwata and J. B. Orlin. A simple combinatorial algorithm for submodular function minimiza-
tion. In SODA, pages 1230–1237, 2009. (Cited on page 4.)
S. Iwata, L. Fleischer, and S. Fujishige. A combinatorial strongly polynomial algorithm for
minimizing submodular functions. Journal of the ACM, 48(4):761–777, 2001. (Cited on page 4.)
R. Iyer and J. Bilmes. Algorithms for approximate minimization of the diﬀerence between
submodular functions, with applications. In UAI, pages 407–417, 2012. (Cited on pages 4
and 8.)
R. K. Iyer, S. Jegelka, and J. A. Bilmes. Curvature and optimal algorithms for learning and
minimizing submodular functions. In NeurIPS, pages 2742–2750, 2013. (Cited on page 4.)
17

S. Jegelka and J. Bilmes. Online submodular minimization for combinatorial structures. In
ICML, pages 345–352, 2011. (Cited on pages 2 and 8.)
P. Joulani, A. Gyorgy, and C. Szepesv´ari. Online learning under delayed feedback. In ICML,
pages 1453–1461. PMLR, 2013. (Cited on page 4.)
P. Joulani, A. Gyorgy, and C. Szepesv´ari. Delay-tolerant online convex optimization: Uniﬁed
analysis and adaptive-gradient algorithms. In AAAI, pages 1744–1750, 2016. (Cited on page 4.)
A. Kalai and S. Vempala. Eﬃcient algorithms for online decision problems. Journal of
Computer and System Sciences, 71(3):291–307, 2005. (Cited on page 2.)
Y. Kawahara, R. Iyer, and J. Bilmes. On approximate non-submodular minimization via
tree-structured supermodularity. In AISTATS, pages 444–452. PMLR, 2015. (Cited on page 4.)
A. Kuhnle, J. D. Smith, V. Crawford, and M. Thai. Fast maximization of non-submodular,
monotonic functions on the integer lattice. In ICML, pages 2786–2795. PMLR, 2018. (Cited
on page 4.)
T. Lattimore and C. Szepesv´ari. Bandit Algorithms. Cambridge University Press, 2020. (Cited
on page 1.)
Y. T. Lee, A. Sidford, and S. C-W. Wong. A faster cutting plane method and its implications
for combinatorial and convex optimization. In FOCS, pages 1049–1065. IEEE, 2015. (Cited
on page 4.)
B. Lehmann, D. Lehmann, and N. Nisan. Combinatorial auctions with decreasing marginal
utilities. Games and Economic Behavior, 55(2):270–296, 2006. (Cited on pages 1 and 5.)
L. Lov´asz. Submodular functions and convexity. In Mathematical Programming The State of
The Art, pages 235–257. Springer, 1983. (Cited on page 6.)
T. Matsuoka, S. Ito, and N. Ohsaka. Tracking regret bounds for online submodular optimization.
In AISTATS, pages 3421–3429. PMLR, 2021. (Cited on page 2.)
S. T. McCormick. Submodular function minimization. Handbooks in Operations Research and
Management Science, 12:321–391, 2005. (Cited on page 1.)
M. Narasimhan, N. Jojic, and J. Bilmes. Q-Clustering. In NeurIPS, pages 979–986, 2005.
(Cited on page 2.)
J. B. Orlin. A faster strongly polynomial time algorithm for submodular function minimization.
Mathematical Programming, 118(2):237–251, 2009. (Cited on page 4.)
C. Pike-Burke, S. Agrawal, C. Szepesvari, and S. Grunewalder. Bandits with delayed, aggre-
gated anonymous feedback. In ICML, pages 4105–4113. PMLR, 2018. (Cited on page 4.)
K. Quanrud and D. Khashabi. Online learning with adversarial delays. In NeurIPS, pages
1270–1278, 2015. (Cited on pages 2, 4, 11, and 12.)
F. Rapaport, E. Barillot, and J-P. Vert. Classiﬁcation of arrayCGH data using fused SVM.
Bioinformatics, 24(13):i375–i382, 2008. (Cited on page 6.)
18

T. Roughgarden and J. R. Wang. An optimal learning algorithm for online unconstrained
submodular maximization. In COLT, pages 1307–1325. PMLR, 2018. (Cited on page 2.)
S. Sakaue. Greedy and IHT algorithms for non-convex optimization with monotone costs of
non-zeros. In AISTATS, pages 206–215. PMLR, 2019. (Cited on page 6.)
A. Schrijver. A combinatorial algorithm minimizing submodular functions in strongly poly-
nomial time. Journal of Combinatorial Theory, Series B, 80(2):346–355, 2000. (Cited on
page 4.)
S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends
in Machine Learning, 4(2):107–194, 2011. (Cited on page 2.)
S. Shalev-Shwartz and Y. Singer. Convex repeated games and Fenchel duality. In NIPS, pages
1265–1272, 2006. (Cited on page 2.)
M. Streeter and D. Golovin. An online algorithm for maximizing submodular functions. In
NeurIPS, pages 1577–1584, 2008. (Cited on pages 2 and 8.)
M. Sviridenko, J. Vondr´ak, and J. Ward.
Optimal approximation for submodular and
supermodular optimization with bounded curvature. Mathematics of Operations Research,
42(4):1197–1218, 2017. (Cited on page 2.)
Z. Svitkina and L. Fleischer. Submodular approximation: Sampling-based algorithms and
lower bounds. SIAM Journal on Computing, 40(6):1715–1737, 2011. (Cited on page 4.)
N. K. Thang and A. Srivastav. Online non-monotone DR-submodular maximization. In AAAI,
pages 9868–9876, 2021. (Cited on page 2.)
T. S. Thune, N. Cesa-Bianchi, and Y. Seldin. Nonstochastic multiarmed bandits with unre-
stricted delays. In NeurIPS, pages 6538–6547, 2019. (Cited on page 4.)
L. Trevisan. Inapproximability of combinatorial optimization problems. Paradigms of Com-
binatorial Optimization: Problems and New Approaches, pages 381–434, 2014. (Cited on
page 8.)
C. Vernade, O. Capp´e, and V. Perchet. Stochastic bandit models for delayed conversions. In
UAI. AUAI Press, 2017. (Cited on page 4.)
J. Vondr´ak. Submodularity in Combinatorial Optimization. PhD thesis, Charles University,
2007. (Cited on page 7.)
Z. Zhou, P. Mertikopoulos, N. Bambos, P. Glynn, and C. Tomlin. Countering feedback delays
in multi-agent learning. In NeurIPS, pages 6172–6182, 2017. (Cited on page 4.)
Z. Zhou, R. Xu, and J. Blanchet. Learning in generalized linear contextual bandits with
stochastic delays. In NeurIPS, pages 5197–5208, 2019. (Cited on page 4.)
J. Zimmert and Y. Seldin. An optimal algorithm for adversarial bandits with arbitrary delays.
In AISTATS, pages 3285–3294. PMLR, 2020. (Cited on page 4.)
M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In
ICML, pages 928–936, 2003. (Cited on pages 2 and 9.)
19

A
Proof of Proposition 3.1
We have
fC(x) =
max
g∈Rd,ρ∈R
n
g⊤x + ρ : g(A) + ρ ≤f(A), ∀A ⊆[n]
o
.
(13)
First, we prove Eq. (5) using the deﬁnition of fL and Eq. (13). Indeed, we have fL(x) = g⊤x
where we let x ∈[0, 1]n with xπ(1) ≥. . . ≥xπ(n) and gπ(i) = f(π(i) | Ai−1) for all i ∈[n]. Then,
it suﬃces to show that g⊤x ≥˜g⊤x + ˜ρ in which ˜g(A) + ˜ρ ≤f(A) for all A ⊆[n]. We have
g⊤x −(˜g⊤x + ˜ρ) =
n
X
i=1
xπ(i)
 f(π(i) | Ai−1) −˜gπ(i)

−˜ρ
=
n−1
X
i=1
 xπ(i) −xπ(i+1)

(f(Ai) −˜g(Ai)) + xπ(n) (f([n]) −˜g([n])) −˜ρ.
Since ˜g(A) + ˜ρ ≤f(A) for all A ⊆[n], we have
f([n]) −˜g([n]) ≥˜ρ,
f(Ai) −˜g(Ai) ≥˜ρ,
for all i ∈[n].
Putting these pieces together with xπ(1) ≥. . . ≥xπ(n) yields that
g⊤x −(˜g⊤x + ˜ρ) ≥
n−1
X
i=1
 xπ(i) −xπ(i+1)

˜ρ + xπ(n)˜ρ −˜ρ = (xπ(1) −1)˜ρ.
Since x ∈[0, 1]n, we have xπ(1) ≤1. Since ˜g(A) + ˜ρ ≤f(A) for all A ⊆[n] and f(∅) = 0, we
derive by letting A = ∅that ˜ρ ≤f(∅) −˜g(∅) ≤0. This implies the desired result.
Further, we prove Eq. (6) using the deﬁnition of weak DR-submodularity. Indeed, we have
g(A) = P
i∈A gi. Since gπ(i) = f(π(i) | Ai−1) for all i ∈[n], we have
g(A) =
X
π(i)∈A
f(π(i) | Ai−1) =
X
π(i)∈A
  ¯f(π(i) | Ai−1) −f(π(i) | Ai−1)

.
Since ¯f is α-weakly DR-submodular, f is β-weakly DR-supermodular and A ∩Ai−1 ⊆Ai−1,
we have
¯f(π(i) | A ∩Ai−1) ≥α ¯f(π(i) | Ai−1),
f(π(i) | Ai−1) ≥βf(π(i) | A ∩Ai−1).
(14)
Putting these pieces together yields that
g(A) ≤
X
π(i)∈A
  1
α ¯f(π(i) | A ∩Ai−1) −βf(π(i) | A ∩Ai−1)

.
Then, we have
g(A)
≤
1
α
 n
X
i=1
  ¯f(A ∩Ai) −¯f(A ∩Ai−1)

!
−β
 n
X
i=1
 f(A ∩Ai) −f(A ∩Ai−1)

!
=
1
α ¯f(A) −βf(A),
for all A ⊆[n].
This implies the desired result.
20

Finally, we prove Eq. (7) using Eq. (13). Indeed, we have g = ¯g −g where ¯gπ(i) = ¯f(π(i) |
Ai−1) and gπ(i) = f(π(i) | Ai−1) for all i ∈[n]. For any A ⊆[n], we obtain by using Eq. (14)
that
¯g(A) ≤1
α

X
π(i)∈A
¯f(π(i) | A ∩Ai−1)

= 1
α
 n
X
i=1
  ¯f(A ∩Ai) −¯f(A ∩Ai−1)

!
= 1
α ¯f(A),
−g(A) ≤−β

X
π(i)∈A
f(π(i) | A ∩Ai−1)

= −β
 n
X
i=1
 f(A ∩Ai) −f(A ∩Ai−1)

!
= −βf(A).
Equivalently, we have α¯g(A) + 0 ≤¯f(A) and 1
β(−g(A)) + 0 ≤−f(A) for any A ⊆[n]. Using
Eq. (13), we have
α¯g⊤z + 0 ≤¯fC(z),
1
β(−g)⊤z + 0 ≤(−f)C(z),
for all z ∈[0, 1]n.
Since g = ¯g −g and α, β > 0, we have g⊤z ≤1
α ¯fC(z) + β(−f)C(z). This implies the desired
result.
B
Regret Analysis for Algorithm 1
In this section, we present several technical lemmas for analyzing the regret minimization
property of Algorithm 1. We also give the missing proof of Theorem 4.1.
B.1
Technical lemmas
We provide two technical lemmas for Algorithm 1. The ﬁrst lemma gives a bound on the
vector gt and the diﬀerence between xt and any ﬁxed x ∈[0, 1]n.
Lemma B.1. Suppose that the iterates {xt}t≥1 and the vectors {gt}t≥1 be generated by
Algorithm 1 and x ∈[0, 1]n and let ft = ¯ft −ft satisfy that ¯ft([n]) + ft([n]) ≤L for all t ≥1
and both ¯ft and ft are nondecreasing. Then, we have ∥xt −x∥≤√n and ∥gt∥≤L for all
t ≥1.
Proof. Since xt ∈[0, 1]n and x ∈[0, 1]n is ﬁxed, we have ∥xt −x∥≤
pPn
i=1 1 = √n for
all t ≥1. By the deﬁnition of gt, we have gt
π(i) = ft(At
i) −ft(At
i−1) for all i ∈[n] where
At
i = {π(1), . . . , π(i)} for all i ∈[n]. Then, we have
∥gt∥≤
n
X
i=1
|ft(At
i) −ft(At
i−1)| ≤
n
X
i=1
| ¯ft(At
i) −¯ft(At
i−1)| +
n
X
i=1
|ft(At
i) −ft(At
i−1)|.
Since ¯ft and ft are both normalized and non-decreasing, we have
n
X
i=1
| ¯ft(At
i) −¯ft(At
i−1)| +
n
X
i=1
|ft(At
i) −ft(At
i−1)| = ¯ft([n]) + ft([n]) ≤L.
Putting these pieces together yields that ∥gt∥≤L for all t = 1, 2, . . . , T.
□
The second lemma gives a key inequality for analyzing Algorithm 1.
21

Lemma B.2. Suppose that the iterates {xt}t≥1 are generated by Algorithm 1 and x ∈[0, 1]n
and let ft = ¯ft −ft satisfy that ¯ft([n]) + ft([n]) ≤L for all t ≥1. Then, we have
T
X
t=1
E[(ft)L(xt)] ≤
 T
X
t=1
1
α( ¯ft)C(x) + β(−ft)C(x)
!
+ n
2η + ηL2T
2
.
Proof. Since xt+1 = P[0,1]n(xt −ηgt), we have
(x −xt+1)⊤(xt+1 −xt + ηgt) ≥0,
for all x ∈[0, 1]n.
Rearranging the above inequality and using the fact that η > 0, we have
(xt+1−x)⊤gt ≤1
η(x−xt+1)⊤(xt+1−xt) =
1
2η
 ∥x −xt∥2 −∥x −xt+1∥2 −∥xt −xt+1∥2
. (15)
Using Young’s inequality, we have
(xt −xt+1)⊤gt ≤
1
2η∥xt −xt+1∥2 + η
2∥gt∥2.
(16)
Combining Eq. (15) and Eq. (16) yields that
(xt −x)⊤gt ≤
1
2η
 ∥x −xt∥2 −∥x −xt+1∥2
+ η
2∥gt∥2.
Since ft = ¯ft −ft where ¯ft and ft are both non-decreasing, ¯ft is α-weakly DR-submodular
and ft is β-weakly DR-supermodular, Proposition 3.1 implies that
(xt −x)⊤gt ≥(ft)L(xt) −
  1
α( ¯ft)C(x) + β(−ft)C(x)

.
By Lemma B.1, we have ∥gt∥≤L for all t = 1, 2, . . . , T. Then, we have
(ft)L(xt) ≤1
α( ¯ft)C(x) + β(−ft)C(x) + 1
2η
 ∥x −xt∥2 −∥x −xt+1∥2
+ ηL2
2 .
Summing up the above inequality over t = 1, 2, . . . , T and using ∥x1−x∥≤√n (cf. Lemma B.1),
we have
T
X
t=1
(ft)L(xt) ≤
 T
X
t=1
1
α( ¯ft)C(x) + β(−ft)C(x)
!
+ n
2η + ηL2T
2
.
Taking the expectation of both sides yields the desired inequality.
□
B.2
Proof of Theorem 4.1
By the deﬁnition of the Lov´asz extension, we have
(ft)L(xt) =
n−1
X
i=1
(xt
π(i) −xt
π(i+1))ft(At
i) + (1 −xt
π(1))ft(At
0) + xt
π(n)ft(At
n) =
n
X
i=0
λt
ift(At
i).
By the update formula, we have E[ft(St) | xt] = (ft)L(xt) which implies that E[ft(St)] =
E[(ft)L(xt)]. By the deﬁnition of the convex closure, we obtain that the convex closure of a
set function f agrees with f on all the integer points [Dughmi, 2009, Page 4, Proposition 3.3].
Letting ST
⋆= argminS⊆[n]
PT
t=1 ft(S), we have ST
⋆is an integer point and
( ¯ft)C(χST
⋆) = ¯ft(ST
⋆),
(−ft)C(χST
⋆) = −βft(ST
⋆),
22

which implies that
1
α( ¯ft)C(χST
⋆) + β(−ft)C(χST
⋆) = 1
α ¯ft(ST
⋆) −βft(ST
⋆).
Putting these pieces together and letting x = χST
⋆in the inequality of Lemma B.2 yields that
T
X
t=1
E[ft(St)] ≤
 T
X
t=1
1
α ¯ft(ST
⋆) −βft(ST
⋆)
!
+ n
2η + ηL2T
2
.
Plugging the choice of η =
√n
L
√
T into the above inequality yields that E[Rα,β(T)] = O(
√
nT)
as desired.
We proceed to derive a high probability bound using the concentration inequality. In
particular, we review the Hoeﬀding inequality [Hoeﬀding, 1963] and refer to Cesa-Bianchi
and Lugosi [2006, Appendix A] for a proof.
The following proposition is a restatement
of Cesa-Bianchi and Lugosi [2006, Corollary A.1].
Proposition B.3. Let X1, . . . , Xn be independent real-valued random variables such that for
each i = 1, . . . , n, there exist some ai ≤bi such that P(ai ≤Xi ≤bi) = 1. Then for every
ϵ > 0, we have
P
 n
X
i=1
Xi −E
" n
X
i=1
Xi
#
> +ϵ
!
≤
exp

−
2ϵ2
Pn
i=1(bi −ai)2

,
P
 n
X
i=1
Xi −E
" n
X
i=1
Xi
#
< −ϵ
!
≤
exp

−
2ϵ2
Pn
i=1(bi −ai)2

.
Since the sequence of points x1, x2, . . . , xT is obtained by several deterministic gradient
descent steps, we have this sequence is purely deterministic. Since each of St is obtained by
independent randomized rounding on the point xt, we have the sequence of random variables
Xt = ft(St) is independent. By deﬁnition of ft, we have
|Xt| = | ¯ft(St) −ft(St)| ≤¯ft(St) + ft(St).
Since ¯ft and ft are non-decreasing and ¯ft([n]) + ft([n]) ≤L for all t ≥1, we have P(−L ≤
Xi ≤L) = 1 for all t ≥1. Then, by Proposition B.3, we have
P
 n
X
i=1
ft(St) −E
" n
X
i=1
ft(St)
#
> ϵ
!
≤exp

−ϵ2
2nL2

.
Equivalently, we have Pn
i=1 ft(St)−E[Pn
i=1 ft(St)] ≤L
p
2T log(1/δ) with probability at least
1−δ. This together with E[Rα,β(T)] = O(
√
nT) yields that Rα,β(T) = O(
√
nT +
p
T log(1/δ))
with probability at least 1 −δ as desired.
C
Regret Analysis for Algorithm 2
In this section, we present several technical lemmas for analyzing the regret minimization
property of Algorithm 2. We also give the missing proofs of Theorem 4.3.
23

C.1
Technical lemmas
We provide several technical lemmas for Algorithm 2.
The ﬁrst lemma is analogous to
Lemma B.1 and gives a bound on the vector ˆgt (in expectation) and the diﬀerence between xt
and any ﬁxed x ∈[0, 1]n.
Lemma C.1. Suppose that the iterates {xt}t≥1 and the vectors {ˆgt}t≥1 be generated by
Algorithm 2 and x ∈[0, 1]n and let ft = ¯ft −ft satisfy that ¯ft([n]) + ft([n]) ≤L for all t ≥1
and both ¯ft and ft are nondecreasing. Then, we have ∥xt −x∥≤√n for all t ≥1 and
E[ˆgt | xt] = gt,
E[∥ˆgt∥2 | xt] ≤8n2L2
µ
,
∥ˆgt∥2 ≤2(n+1)2L2
µ2
.
where we have gt
π(i) = ft(At
i) −ft(At
i−1) for all i ∈[n].
Proof. Using the same argument as in Lemma B.1, we have ∥xt −x∥≤√n for all t ≥1. By
the deﬁnition of ˆgt, we have
ˆgt
π(i) =

1(St=At
i)
(1−µ)λt
i+
µ
n+1 −
1(St=At
i−1)
(1−µ)λt
i−1+
µ
n+1

ft(St),
for all i ∈[n].
This together with the sampling scheme for St implies that
E[ˆgt
π(i) | xt] = ft(At
i) −ft(At
i−1),
for all i ∈[n],
Since gt
π(i) = ft(At
i) −ft(At
i−1) for all i ∈[n], we have E[ˆgt | xt] = gt. Since ft = ¯ft −ft satisfy
that ¯ft([n]) + ft([n]) ≤L for all t ≥1 and ¯ft and ft are both normalized and non-decreasing,
we have
E[∥ˆgt∥2 | xt] ≤
n
X
i=0
2(ft(At
i))2
(1−µ)λt
i+
µ
n+1 ≤2(n+1)2L2
µ
≤8n2L2
µ
.
Further, let St = Ait in the round t, we can apply the same argument and obtain that
∥ˆgt∥2 ≤2

ft(At
it)
(1−µ)λt
it+
µ
n+1
2
≤2(n+1)2L2
µ2
.
This completes the proof.
□
The second lemma is analogous to Lemma B.2 and gives a key inequality for analyzing
Algorithm 2.
Lemma C.2. Suppose that the iterates {xt}t≥1 are generated by Algorithm 2 and x ∈[0, 1]n
and let ft = ¯ft −ft satisfy that ¯ft([n]) + ft([n]) ≤L for all t ≥1. Then, we have
T
X
t=1
E[(ft)L(xt)] ≤
 T
X
t=1
1
α( ¯ft)C(x) + β(−ft)C(x)
!
+ n
2η + 4n2L2ηT
µ
.
Proof. Using the same argument as in Lemma B.2, we have
(xt −x)⊤ˆgt ≤
1
2η
 ∥x −xt∥2 −∥x −xt+1∥2
+ η
2∥ˆgt∥2.
By Lemma C.1, we have E[ˆgt | xt] = gt and E[∥ˆgt∥2 | xt] ≤8n2L2
µ
for all t ≥1. This implies
that
(xt −x)⊤gt ≤
1
2η
 ∥x −xt∥2 −E[∥x −xt+1∥2 | xt]

+ 4n2L2η
µ
.
24

Since ft = ¯ft −ft where ¯ft and ft are both non-decreasing, ¯ft is α-weakly DR-submodular
and ft is β-weakly DR-supermodular, Proposition 3.1 implies that
(xt −x)⊤gt ≥(ft)L(xt) −
  1
α( ¯ft)C(x) + β(−ft)C(x)

.
By Lemma B.1, we have ∥gt∥≤L for all t = 1, 2, . . . , T. Then, we have
(ft)L(xt) ≤1
α( ¯ft)C(x) + β(−ft)C(x) + 1
2η
 ∥x −xt∥2 −E[∥x −xt+1∥2 | xt]

+ 4n2L2η
µ
.
Taking the expectation of both sides and summing up the resulting inequality over t =
1, 2, . . . , T, we have
T
X
t=1
E[(ft)L(xt)] ≤
 T
X
t=1
1
α( ¯ft)C(x) + β(−ft)C(x)
!
+ 1
2η∥x −x1∥2 + 4n2L2ηT
µ
.
Using ∥x1 −x∥≤√n (cf. Lemma C.1) yields the desired inequality.
□
To prove the high probability bound, we require the following concentration inequality.
In particular, we review the Bernstein inequality for martingales [Freedman, 1975] and refer
to Cesa-Bianchi and Lugosi [2006, Appendix A] for a proof. The following proposition is a
consequence of Cesa-Bianchi and Lugosi [2006, Lemma A.8].
Proposition C.3. Let X1, . . . , Xn be a bounded martingale diﬀerence sequence with respect
to the ﬁltration F = (Fi)1≤i≤n such that |Xi| ≤K for each i = 1, . . . , n. We also assume that
E[∥Xi+1∥2 | Fi] ≤V for each i = 1, . . . , n −1. Then for every δ > 0, we have
P
 
n
X
i=1
Xi −E[Xi | Fi−1]
 >
p
2TV log(1/δ) +
√
2
3 K log(1/δ)
!
≤δ.
Then we provide our last lemma which signiﬁcantly generalizes Lemma C.2 for deriving
the high-probability bounds.
Lemma C.4. Suppose that the iterates {xt}t≥1 are generated by Algorithm 2 with µ =
n
T 1/3
and x ∈[0, 1]n and let ft = ¯ft −ft satisfy that ¯ft([n]) + ft([n]) ≤L for all t ≥1. Fixing a
suﬃciently small δ ∈(0, 1) and letting T > log
3
2 (1/δ). Then, we have
T
X
t=1
(ft)L(xt) ≤
 T
X
t=1
1
α ¯ft(S) −βft(S)
!
+ n
2η+4n2L2ηT
µ
+12LT
2
3 p
n2 + n log(1/δ)+6ηL2T
p
n log(1/δ),
with probability at least 1 −3δ.
Proof. Using the same argument as in Lemma C.2, we have
(xt −x)⊤ˆgt ≤
1
2η
 ∥x −xt∥2 −∥x −xt+1∥2
+ η
2∥ˆgt∥2,
and
(xt −x)⊤gt ≥(ft)L(xt) −
  1
α( ¯ft)C(x) + β(−ft)C(x)

.
For simplicity, we deﬁne et = ˆgt −gt. Then, we have
(ft)L(xt) −
  1
α( ¯ft)C(x) + β(−ft)C(x)

≤(x −xt)⊤et + 1
2η
 ∥x −xt∥2 −∥x −xt+1∥2
+ η
2∥ˆgt∥2
(17)
25

Summing up Eq. (17) over t = 1, 2, . . . , T and using ∥x1 −x∥≤√n and E[∥ˆgt∥2 | xt] ≤8n2L2
µ
for all t ≥1 (cf. Lemma C.1), we have
T
X
t=1
(ft)L(xt)
≤
 T
X
t=1
1
α( ¯ft)C(x) + β(−ft)C(x) + (x −xt)⊤et + η
2(∥ˆgt∥2 −E[∥ˆgt∥2 | xt])
!
+ n
2η + 4n2L2ηT
µ
.
By the deﬁnition of the convex closure, we obtain that the convex closure of a set function
f agrees with f on all the integer points [Dughmi, 2009, Page 4, Proposition 3.3]. Letting
S ⊆[n], we have ( ¯ft)C(χS) = ft(S) and (−ft)C(χS) = −βft(S) which implies that
1
α( ¯ft)C(χS) + β(−ft)C(χS) = 1
αft(S) −βft(S).
Letting x = χS, we have
T
X
t=1
(ft)L(xt) ≤
 T
X
t=1
1
α ¯ft(S) −βft(S)
!
+ n
2η+4n2L2ηT
µ
+
T
X
t=1
(χS −xt)⊤et
|
{z
}
I
+η
2
 T
X
t=1
∥ˆgt∥2 −E[∥ˆgt∥2 | xt]
!
|
{z
}
II
.
(18)
In what follows, we prove the high probability bounds for the terms I and II in the above
inequality.
Bounding I.
Consider the random variables Xt = (xt)⊤ˆgt for all 1 ≤t ≤T that are adapted
to the natural ﬁltration generated by the iterates {xt}t≥1. By Lemma C.1 and the H¨older’s
inequality, we have
|Xt| ≤∥ˆgt∥1∥xt∥∞≤2

ft(At
it)
(1−µ)λt
it+
µ
n+1
 ≤2(n+1)L
µ
Since µ =
n
T 1/3 , we have |Xt| ≤4LT
1
3 . Further, we have
E[X2
t | xt] ≤E[∥ˆgt∥2
1∥xt∥2
∞| xt] =
n
X
i=0
4(ft(At
i))2
(1−µ)λt
i+
µ
n+1 ≤2(n+1)2L2
µ
≤8nL2T
1
3 .
Since E[ˆgt | xt] = gt and et = ˆgt −gt, Proposition C.3 implies that
P
 
T
X
t=1
(xt)⊤et
 > 4LT
2
3 p
n log(1/δ) + 2LT
1
3 log(1/δ)
!
≤δ.
Since T > log
3
2 (1/δ), we have T
2
3 p
log(1/δ) ≥T
1
3 log(1/δ). This implies that
P
 
T
X
t=1
(xt)⊤et
 > 6LT
2
3 p
n log(1/δ)
!
≤δ.
Similarly, we ﬁx a set S ⊆[n] and consider the random variable Xt = (χS)⊤ˆgt for all 1 ≤t ≤T
that are adapted to the natural ﬁltration generated by the iterates {xt}t≥1. By repeating the
above argument with
δ
2n , we have
P
 
T
X
t=1
(χS)⊤et
 > 6LT
2
3 p
n log(2n/δ)
!
≤
δ
2n .
26

By taking a union bound over the 2n choices of S, we obtain that
P
 
T
X
t=1
(χS)⊤et
 > 6LT
2
3 p
n log(2n/δ)
!
≤δ,
for any S ⊆[n].
Since
p
n log(2n/δ) ≤
p
n2 + n log(1/δ), we have I ≤12LT
2
3 p
n2 + n log(1/δ) with probabil-
ity at least 1 −2δ.
Bounding II.
Consider the random variables Xt = ∥ˆgt∥2 for all 1 ≤t ≤T that are
adapted to the natural ﬁltration generated by the iterates {xt}t≥1. By Lemma C.1, we have
|Xt| ≤2(n+1)2L2
µ2
. Since µ =
n
T 1/3 , we have |Xt| ≤8L2T 2/3. Further, we have
E[X2
t | xt] ≤
n
X
i=0
2(ft(At
i))4
((1−µ)λt
i+
µ
n+1 )3 ≤2(n+1)4L4
µ3
≤32nL4T.
Applying Proposition C.3, we have
P
 
T
X
t=1
∥ˆgt∥2 −E[∥ˆgt∥2 | xt]
 > 8L2T
p
n log(1/δ) + 4L2T
2
3 log(1/δ)
!
≤δ.
Since T > log
3
2 (1/δ), we have T
p
log(1/δ) ≥T
2
3 log(1/δ). This implies that
P
 
T
X
t=1
∥ˆgt∥2 −E[∥ˆgt∥2 | xt]
 > 12L2T
p
n log(1/δ)
!
≤δ.
Therefore, we conclude that II ≤12L2T
p
n log(1/δ) with probability at least 1 −δ.
Putting these pieces together with Eq. (18) yields that
T
X
t=1
(ft)L(xt) ≤
 T
X
t=1
1
α ¯ft(S) −βft(S)
!
+ n
2η+4n2L2ηT
µ
+12LT
2
3 p
n2 + n log(1/δ)+6ηL2T
p
n log(1/δ),
with probability at least 1 −3δ.
□
C.2
Proof of Theorem 4.3
By the deﬁnition of the Lov´asz extension and λt, we have
(ft)L(xt) =
n−1
X
i=1
(xt
π(i) −xt
π(i+1))ft(At
i) + (1 −xt
π(1))ft(At
0) + xt
π(n)ft(At
n) =
n
X
i=0
λt
ift(At
i).
By the update formula of St, we have
E[ft(St) | xt] −(ft)L(xt) = µ
n
X
i=0

1
n + 1 −λt
i

ft(At
i) ≤µ
n
X
i=0

1
n + 1 + λt
i

|ft(At
i)|.
Since ft = ¯ft −ft satisfy that ¯ft([n]) + ft([n]) ≤L for all t ≥1 and ¯ft and ft are both
normalized and non-decreasing, we have
E[ft(St) | xt] −(ft)L(xt) ≤Lµ
n
X
i=0

1
n + 1 + λt
i

= 2Lµ.
(19)
27

which implies that
E[ft(St)] −E[(ft)L(xt)] ≤2Lµ.
Using the same argument as in Theorem 4.1, we have
1
α( ¯ft)C(χST
⋆) + β(−ft)C(χST
⋆) = 1
αft(ST
⋆) −βft(ST
⋆),
where ST
⋆= argmin
S⊆[n]
T
X
t=1
ft(S).
Putting these pieces together and letting x = χST
⋆in the inequality of Lemma C.2 yields that
T
X
t=1
E[ft(St)] ≤
 T
X
t=1
1
α ¯ft(ST
⋆) −βft(ST
⋆)
!
+ n
2η + 4n2L2ηT
µ
+ 2LTµ.
Plugging the choice of η =
1
LT 2/3 and µ =
n
T 1/3 into the above inequality yields that
E[Rα,β(T)] = O(nT
2
3 ) as desired.
We proceed to derive a high probability bound using Lemma C.4. Indeed, we ﬁrst consider
the case of T < 2 log
3
2 (1/δ). Since ft = ¯ft −ft satisfy that ¯ft([n]) + ft([n]) ≤L for all t ≥1,
we have
Rα,β(T) ≤
T
X
t=1
ft(St) −
T
X
t=1
( 1
α ¯ft(ST
⋆) −βft(ST
⋆)) ≤
 1 + 1
α + β

LT = O(T
2
3 p
log(1/δ)).
For the case of T ≥2 log
3
2 (1/δ), we obtain by combining Lemma C.4 with Eq. (19) that
T
X
t=1
E[ft(St) | xt] ≤
 T
X
t=1
1
α ¯ft(S) −βft(S)
!
+ n
2η + 2nLT
2
3 + 4nL2ηT
4
3 + 12LT
2
3 p
n2 + n log(1/δ) + 6ηL2T
p
n log(1/δ),
with probability at least 1−3δ. Then, it suﬃces to bound the term PT
t=1 ft(St)−PT
t=1 E[ft(St) |
xt] using Proposition C.3. Consider the random variables Xt = ft(St) for all 1 ≤t ≤T that
are adapted to the natural ﬁltration generated by the iterates {xt}t≥1. Since ft = ¯ft −ft satisfy
that ¯ft([n]) + ft([n]) ≤L for all t ≥1, we have |Xt| ≤L. Further, we have E[X2
t | xt] ≤L2.
Applying Proposition C.3, we have
P
 
T
X
t=1
ft(St) −E[ft(St) | xt]
 > L
p
2T log(1/δ) + L
2 log(1/δ)
!
≤δ.
Since T > log
3
2 (1/δ), we have
p
2T log(1/δ) ≥1
2 log(1/δ). This implies that
P
 
T
X
t=1
ft(St) −E[ft(St) | xt]
 > 3L
p
T log(1/δ)
!
≤δ.
Therefore, we conclude that PT
t=1 ft(St) −PT
t=1 E[ft(St) | xt] ≤3L
p
T log(1/δ) with proba-
bility at least 1 −δ. Putting these pieces together yields that
T
X
t=1
ft(St) ≤
 T
X
t=1
1
α ¯ft(S) −βft(S)
!
+ n
2η + 3L
p
T log(1/δ) + 2nLT
2
3
+4nL2ηT
4
3 + 12nLT
2
3 + 12LT
2
3 p
n log(1/δ) + 6ηL2T
p
n log(1/δ),
28

with probability at least 1 −4δ. Plugging the choice of η =
1
LT 2/3 yields that
T
X
t=1
ft(St) ≤
 T
X
t=1
1
α ¯ft(S) −βft(S)
!
+ 37
2 nLT
2
3 + 21LT
2
3 p
n log(1/δ),
with probability at least 1 −4δ. Letting S = ST
⋆= argminS⊆[n]
PT
t=1 ft(S) and changing δ to
δ
4 yields that Rα,β(T) = O(nT
2
3 +
p
n log(1/δ)T
2
3 ) with probability at least 1 −δ as desired.
D
Regret Analysis for Algorithm 3
In this section, we present several technical lemmas for analyzing the regret minimization
property of Algorithm 3. We also give the missing proofs of Theorem 5.2.
D.1
Technical lemmas
We provide one technical lemma for Algorithm 3 which is analogues to Lemma B.2. It gives
a key inequality for analyzing the regret minimization property of Algorithm 3. Note that
the results in Lemma B.1 still hold true for the iterates {xt}t≥1 and {gt}t≥1 generated by
Algorithm 3.
Lemma D.1. Suppose that the iterates {xt}t≥1 are generated by Algorithm 3 and x ∈[0, 1]n
and let ft = ¯ft −ft satisfy that ¯ft([n]) + ft([n]) ≤L for all t ≥1. Then, we have
T
X
t=1
E[(ft)L(xt)] ≤
 T
X
t=1
1
α( ¯ft)C(x) + β(−ft)C(x)
!
+
n
2η ¯
T + L2
2


¯T
X
t=1
ηt

+ L2


¯T
X
t=1
t−1
X
s=qt
ηs

,
where ¯T > 0 in the above inequality satisﬁes that q ¯T = T.
Proof. Using the same argument as in Lemma B.2, we have
(xt −x)⊤gqt ≤
1
2ηt
 ∥x −xt∥2 −∥x −xt+1∥2
+ ηt
2 ∥gqt∥2.
Since ft = ¯ft −ft where ¯ft and ft are both normalized and non-decreasing, ¯ft is α-weakly
DR-submodular and ft is β-weakly DR-supermodular, Proposition 3.1 implies that
(xqt −x)⊤gqt ≥(fqt)L(xqt) −
  1
α( ¯fqt)C(x) + β(−fqt)C(x)

.
By Lemma B.1, we have ∥gt∥≤L for all t ≥1. Then, we have
(fqt)L(xqt) ≤1
α( ¯fqt)C(x) + β(−fqt)C(x) +
1
2ηt
 ∥x −xt∥2 −∥x −xt+1∥2
+ L∥xqt −xt∥+ ηtL2
2 .
(20)
Further, we have
∥xqt −xt∥≤
t−1
X
s=qt
ηs∥gs∥≤L
 t−1
X
s=qt
ηs
!
.
(21)
Plugging Eq. (21) into Eq. (20) yields that
(fqt)L(xqt) ≤1
α( ¯fqt)C(x)+β(−fqt)C(x)+ 1
2ηt
 ∥x −xt∥2 −∥x −xt+1∥2
+ ηtL2
2
+L2
 t−1
X
s=qt
ηs
!
.
(22)
29

For a ﬁxed horizon T ≥1, we have q ¯T = T for some ¯T ≥T. Then, by summing up Eq. (22)
over t = 1, 2, . . . , ¯T and using ∥xt −x∥≤√n for all t ≥1 (cf. Lemma B.1) and that {ηt}t≥1 is
nonincreasing, we have
¯T
X
t=1
(fqt)L(xqt) ≤


¯T
X
t=1
1
α( ¯fqt)C(x) + β(−fqt)C(x)

+
n
2η ¯
T + L2
2


¯T
X
t=1
ηt

+ L2


¯T
X
t=1
t−1
X
s=qt
ηs

.
Since q ¯T = T and our pooling policy is induced by a priority queue (note that fqt = ¯fqt = fqt = 0
if Pt = ∅), we have
¯T
X
t=1
(fqt)L(xqt)
=
T
X
t=1
(ft)L(xt),
¯T
X
t=1
1
α( ¯fqt)C(x) + β(−fqt)C(x)
=
T
X
t=1
1
α( ¯ft)C(x) + β(−ft)C(x).
Therefore, we conclude that
T
X
t=1
(ft)L(xt) ≤
 T
X
t=1
1
α( ¯ft)C(x) + β(−ft)C(x)
!
+
n
2ηT + L2
2


¯T
X
t=1
ηt

+ L2


¯T
X
t=1
t−1
X
s=qt
ηs

.
Taking the expectation of both sides yields the desired inequality.
□
D.2
Proof of Theorem 5.2
By H´eliou et al. [2020, Corollary 1], we have t −qt = o(tγ) under Assumption 5.1; in particular,
we have t −qt = o(t) and qt = Θ(t). Since q ¯T = T, we have T = Θ( ¯T) which implies that
¯T = Θ(T). Recall that ηt =
√n
L
√
t1+γ , we have
n
2η ¯
T
=
L
√
n ¯T 1+γ
2
= O(
√
nT 1+γ),
L2
2


¯T
X
t=1
ηt


=
√nL
2


¯T
X
t=1
1
√
t1+γ

≤
√nL
1−γ
p
¯T 1−γ = O(
√
nT 1−γ),
L2


¯T
X
t=1
t−1
X
s=qt
ηs


≤
L2


¯T
X
t=1
(t −qt)ηqt

= O

√nL
¯T
X
t=1
1
√
t1−γ

= O(L
p
n ¯T 1+γ) = O(
√
nT 1+γ),
Putting these pieces together with Lemma D.1 yields that
T
X
t=1
E[(ft)L(xt)] −
 T
X
t=1
1
α( ¯ft)C(x) + β(−ft)C(x)
!
= O(
√
nT 1+γ).
(23)
By the deﬁnition of the Lov´asz extension, we have
(ft)L(xt) =
n−1
X
i=1
(xt
π(i) −xt
π(i+1))ft(At
i) + (1 −xt
π(1))ft(At
0) + xt
π(n)ft(At
n).
30

By the update formula, we have E[ft(St) | xt] = (ft)L(xt) which implies that E[ft(St)] =
E[(ft)L(xt)]. Further, by using the same argument as in Theorem 4.1, we have
1
α( ¯ft)C(χST
⋆) + β(−ft)C(χST
⋆) = 1
α ¯ft(ST
⋆) −βft(ST
⋆).
Putting these pieces together and letting x = χST
⋆in Eq. (23) yields that
T
X
t=1
E[ft(St)] −
 T
X
t=1
1
α ¯ft(ST
⋆) −βft(ST
⋆)
!
= O(
√
nT 1+γ).
which implies that E[Rα,β(T)] = O(
√
nT 1+γ) as desired.
We proceed to derive a high probability bound using the concentration inequality in
Proposition B.3. Indeed, we have
P
 n
X
i=1
ft(St) −E
" n
X
i=1
ft(St)
#
> ϵ
!
≤exp

−ϵ2
2nL2

.
Equivalently, we have Pn
i=1 ft(St)−E[Pn
i=1 ft(St)] ≤L
p
2T log(1/δ) with probability at least
1 −δ. This together with E[Rα,β(T)] = O(
√
nT 1+γ) yields that Rα,β(T) = O(
√
nT 1+γ +
p
T log(1/δ)) with probability at least 1 −δ.
E
Regret Analysis for Algorithm 4
In this section, we present several technical lemmas for analyzing the regret minimization
property of Algorithm 4. We also give the missing proofs of Theorem 5.4.
E.1
Technical lemmas
We provide two technical lemmas for Algorithm 4 which are analogues to Lemma C.2 and C.4.
It gives a key inequality for analyzing the regret minimization property of Algorithm 3. Note
that the results in Lemma C.1 still hold true for the iterates {xt}t≥1 and {ˆgt}t≥1 generated by
Algorithm 4.
Lemma E.1. Suppose that the iterates {xt}t≥1 are generated by Algorithm 4 and x ∈[0, 1]n
and let ft = ¯ft −ft satisfy that ¯ft([n]) + ft([n]) ≤L for all t ≥1. Then, we have
T
X
t=1
E[(ft)L(xt)] ≤
 T
X
t=1
1
α( ¯ft)C(x) + β(−ft)C(x)
!
+ n
2ηT +4n2L2


¯T
X
t=1
ηt
µqt

+4nL2


¯T
X
t=1
t−1
X
s=qt
ηs
µs

,
where ¯T > 0 in the above inequality satisﬁes that q ¯T = T.
Proof. Using the same argument as in Lemma B.2, we have
(xt −x)⊤ˆgqt ≤
1
2ηt
 ∥x −xt∥2 −∥x −xt+1∥2
+ ηt
2 ∥ˆgqt∥2.
Since our pooling policy is induced by a priority queue, ˆgqt has never been used before
updating xt+1. Thus, we have E[ˆgqt | xt] = E[ˆgqt | xqt] and E[∥ˆgqt∥2 | xt] = E[∥ˆgqt∥2 | xqt]. By
31

Lemma C.1, we have E[ˆgqt | xqt] = gqt and E[∥ˆgqt∥2 | xqt] ≤8n2L2
µqt
for all t ≥1. Putting these
pieces together yields that
(xt −x)⊤gqt ≤
1
2ηt
 ∥x −xt∥2 −E[∥x −xt+1∥2 | xt]

+ 4n2L2ηt
µqt
.
Since ft = ¯ft −ft where ¯ft and ft are both normalized and non-decreasing, ¯ft is α-weakly
DR-submodular and ft is β-weakly DR-supermodular, Proposition 3.1 implies that
(xqt −x)⊤gqt ≥(fqt)L(xqt) −
  1
α( ¯fqt)C(x) + β(−fqt)C(x)

.
By Lemma B.1, we have ∥gt∥≤L for all t ≥1. Then, we have
(fqt)L(xqt) ≤1
α( ¯fqt)C(x)+β(−fqt)C(x)+ 1
2ηt
 ∥x −xt∥2 −E[∥x −xt+1∥2 | xt]

+L∥xqt−xt∥+4n2L2ηt
µqt
.
(24)
Further, by Lemma C.1, we have
∥xqt −xt∥≤
t−1
X
s=qt
ηs∥ˆgs∥≤2(n + 1)L
 t−1
X
s=qt
ηs
µs
!
.
(25)
Plugging Eq. (25) into Eq. (24) yields that
(fqt)L(xqt) ≤1
α( ¯fqt)C(x) + β(−fqt)C(x)
+ 1
2ηt
 ∥x −xt∥2 −E[∥x −xt+1∥2 | xt]

+ 4n2L2ηt
µqt
+ 4nL2
 t−1
X
s=qt
ηs
µs
!
.
By using the same argument as in Lemma D.1, we have
T
X
t=1
(ft)L(xt)
≤
 T
X
t=1
1
α( ¯ft)C(x) + β(−ft)C(x)
!
+
¯T
X
t=1
1
2ηt
 ∥x −xt∥2 −E[∥x −xt+1∥2 | xt]

+4n2L2


¯T
X
t=1
ηt
µqt

+ 4nL2


¯T
X
t=1
t−1
X
s=qt
ηs
µs

.
Taking the expectation of both sides of the above inequality and using ∥xt −x∥≤√n for all
t ≥1 (cf. Lemma C.1) and that {ηt}t≥1 is nonincreasing yields the desired inequality.
□
Then, we provide our second lemma which signiﬁcantly generalizes Lemma E.1 for deriving
the high-probability bounds.
Lemma E.2. Suppose that the iterates {xt}t≥1 are generated by Algorithm 4 with ηt =
1
Lt(2+γ)/3 ,
µt =
n
t(1−γ)/3 and x ∈[0, 1]n and let ft = ¯ft −ft satisfy that ¯ft([n]) + ft([n]) ≤L for all t ≥1.
Fixing a suﬃciently small δ ∈(0, 1) and letting T > log
3
2+γ (1/δ). Then, we have
T
X
t=1
(ft)L(xt)
≤
 T
X
t=1
1
α ¯ft(S) −βft(S)
!
+
n
2η ¯
T + 4n2L2


¯T
X
t=1
ηt
µqt

+ 4nL2


¯T
X
t=1
t−1
X
s=qt
ηs
µs


+12L ¯T
4−γ
6 p
n2 + n log(1/δ) + 6L
q
n ¯T log(1/δ),
with probability at least 1 −3δ where ¯T > 0 in the above inequality satisﬁes that q ¯T = T.
32

Proof. Using the same argument as in Lemma E.1, we have
(xt −x)⊤ˆgqt ≤
1
2ηt
 ∥x −xt∥2 −∥x −xt+1∥2
+ ηt
2 ∥ˆgqt∥2,
and
(xqt −x)⊤gqt ≥(fqt)L(xqt) −
  1
α( ¯fqt)C(x) + β(−fqt)C(x)

.
For simplicity, we deﬁne et = ˆgt −gt. By Lemma B.1, we have ∥gt∥≤L for all t ≥1. Then,
we have
(fqt)L(xqt) −
  1
α( ¯fqt)C(x) + β(−fqt)C(x)

(26)
≤
(x −xt)⊤eqt +
1
2ηt
 ∥x −xt∥2 −∥x −xt+1∥2
+ L∥xqt −xt∥+ ηt
2 ∥ˆgqt∥2.
Plugging Eq. (25) into Eq. (26) yields that
(fqt)L(xqt) −
  1
α( ¯fqt)C(x) + β(−fqt)C(x)

≤
(x −xt)⊤eqt +
1
2ηt
 ∥x −xt∥2 −∥x −xt+1∥2
+ ηt
2 (∥ˆgt∥2 −E[∥ˆgt∥2 | xt]) + 4n2L2ηt
µqt
+ 4nL2
 t−1
X
s=qt
ηs
µs
!
.
By using the same argument as in Lemma D.1, we have
T
X
t=1
(ft)L(xt)
≤
 T
X
t=1
1
α( ¯ft)C(x) + β(−ft)C(x)
!
+
¯T
X
t=1
(x −xt)⊤eqt +
¯T
X
t=1
ηt
2 (∥ˆgt∥2 −E[∥ˆgt∥2 | xt])
+ n
2η ¯
T + 4n2L2


¯T
X
t=1
ηt
µqt

+ 4nL2


¯T
X
t=1
t−1
X
s=qt
ηs
µs

.
By the deﬁnition of the convex closure, we obtain that the convex closure of a set function
f agrees with f on all the integer points [Dughmi, 2009, Page 4, Proposition 3.3]. Letting
S ⊆[n], we have ( ¯ft)C(χS) = ft(S) and (−ft)C(χS) = −βft(S) which implies that
1
α( ¯ft)C(χS) + β(−ft)C(χS) = 1
αft(S) −βft(S).
Letting x = χS, we have
T
X
t=1
(ft)L(xt)
≤
 T
X
t=1
1
α ¯ft(S) −βft(S)
!
+
¯T
X
t=1
(χS −xt)⊤eqt
|
{z
}
I
+


¯T
X
t=1
ηt
2 (∥ˆgt∥2 −E[∥ˆgt∥2 | xt])


|
{z
}
II
+ n
2η ¯
T + 4n2L2


¯T
X
t=1
ηt
µqt

+ 4nL2


¯T
X
t=1
t−1
X
s=qt
ηs
µs

.
(27)
In what follows, we prove the high probability bounds for the terms I and II in the above
inequality.
33

Bounding I.
Consider the random variables Xt = (xt)⊤ˆgqt for all 1 ≤t ≤¯T that are
adapted to the natural ﬁltration generated by the iterates {xt}t≥1. By Lemma C.1 and the
H¨older’s inequality, we have
|Xt| ≤∥ˆgqt∥1∥xt∥∞≤2(n+1)L
µt
.
Since µ =
n
t(1−γ)/3 , we have |Xt| ≤4L ¯T
1−γ
3
for all 1 ≤t ≤¯T. Further, we have
E[X2
t | xt] ≤E[∥ˆgt∥2
1∥xt∥2
∞| xt] ≤2(n+1)2L2
µt
≤8nL2 ¯T
1−γ
3 .
Since E[ˆgqt | xt] = gqt and eqt = ˆgqt −gqt, Proposition C.3 implies that
P



¯T
X
t=1
(xt)⊤eqt

> 4L ¯T
4−γ
6 p
n log(1/δ) + 2L ¯T
1−γ
3 log(1/δ)

≤δ.
Since ¯T ≥T > log
3
2+γ (1/δ), we have ¯T
4−γ
6 p
log(1/δ) ≥¯T
1−γ
3 log(1/δ). This implies that
P



¯T
X
t=1
(xt)⊤eqt

> 6L ¯T
4−γ
6 p
n log(1/δ)

≤δ.
Similarly, we ﬁx a set S ⊆[n] and consider the random variable Xt = (χS)⊤ˆgt for all 1 ≤t ≤¯T
that are adapted to the natural ﬁltration generated by the iterates {xt}t≥1. By repeating the
above argument with
δ
2n , we have
P



¯T
X
t=1
(χS)⊤eqt

> 6L ¯T
4−γ
6 p
n log(2n/δ)

≤
δ
2n .
By taking a union bound over the 2n choices of S, we obtain that
P



¯T
X
t=1
(χS)⊤eqt

> 6L ¯T
4−γ
6 p
n log(2n/δ)

≤δ,
for any S ⊆[n].
Since
p
n log(2n/δ) ≤
p
n2 + n log(1/δ), we have I ≤12L ¯T
4−γ
6 p
n2 + n log(1/δ) with proba-
bility at least 1 −2δ.
Bounding II.
Consider the random variables Xt = ηt
2 ∥ˆgqt∥2 for all 1 ≤t ≤¯T that are
adapted to the natural ﬁltration generated by the iterates {xt}t≥1. By Lemma C.1, we have
|Xt| ≤(n+1)2L2ηt
µ2
t
. Since ηt =
1
Lt(2+γ)/3 and µt =
n
t(1−γ)/3 , we have |Xt| ≤4L. Further, we have
E[X2
t | xt] ≤(n+1)4L4η2
t
2µ3
t
≤8nL2.
Applying Proposition C.3, we have
P



¯T
X
t=1
ηt
2 (∥ˆgqt∥2 −E[∥ˆgqt∥2 | xt])

> 4L
q
n ¯T log(1/δ) + 2L log(1/δ)

≤δ.
34

Since ¯T ≥T > log
3
2+γ (1/δ), we have
p ¯T log(1/δ) ≥log(1/δ). This implies that
P



¯T
X
t=1
ηt
2 (∥ˆgqt∥2 −E[∥ˆgqt∥2 | xt])

> 6L
q
n ¯T log(1/δ)

≤δ.
Therefore, we conclude that II ≤6L
p
n ¯T log(1/δ) with probability at least 1 −δ. Putting
these pieces together with Eq. (27) yields that
T
X
t=1
(ft)L(xt)
≤
 T
X
t=1
1
α ¯ft(S) −βft(S)
!
+
n
2η ¯
T + 4n2L2


¯T
X
t=1
ηt
µqt

+ 4nL2


¯T
X
t=1
t−1
X
s=qt
ηs
µs


+12L ¯T
4−γ
6 p
n2 + n log(1/δ) + 6L
q
n ¯T log(1/δ),
with probability at least 1 −3δ.
□
E.2
Proof of Theorem 5.4
By H´eliou et al. [2020, Corollary 1], we have t −qt = o(tγ) under Assumption 5.1; in particular,
we have t −qt = o(t) and qt = Θ(t). Since q ¯T = T, we have T = Θ( ¯T) which implies that
¯T = Θ(T). Recall that ηt =
1
Lt(2+γ)/3 and µt =
n
t(1−γ)/3 , we have
n
2η ¯
T
=
nL ¯T
2+γ
3
2
= O(nT
2+γ
3 ),
4n2L2


¯T
X
t=1
ηt
µqt


=
4nL


¯T
X
t=1
(qt)
1−γ
3
t
2+γ
3

= O

nL
¯T
X
t=1
1
t
1+2γ
3

= O(nL ¯T
2−2γ
3 ) = O(nT
2−2γ
3 ),
4nL2


¯T
X
t=1
t−1
X
s=qt
ηs
µs


≤
4L


¯T
X
t=1
(t −qt)ηqt
µt

= O

L
¯T
X
t=1
1
t
1−γ
3

= O(L ¯T
2+γ
3 ) = O(T
2+γ
3 ),
Putting these pieces together with Lemma E.1 yields that
T
X
t=1
E[(ft)L(xt)] −
 T
X
t=1
1
α( ¯ft)C(x) + β(−ft)C(x)
!
= O(nT
2+γ
3 ).
(28)
By using the similar argument as in Theorem 4.3, we have
E[ft(St) | xt] −(ft)L(xt) ≤Lµt
n
X
i=0

1
n+1 + λt
i

= 2Lµt.
(29)
which implies that
T
X
t=1
E[ft(St)] −E[(ft)L(xt)] ≤2L
T
X
t=1
µt = O(nT
2+γ
3 ).
Using the same argument as in Theorem 4.1, we have
1
α( ¯ft)C(χST
⋆) + β(−ft)C(χST
⋆) = 1
αft(ST
⋆) −βft(ST
⋆),
where ST
⋆= argmin
S⊆[n]
T
X
t=1
ft(S).
35

Putting these pieces together and letting x = χST
⋆in Eq. (28) yields that
T
X
t=1
E[ft(St)] −
 T
X
t=1
1
α ¯ft(ST
⋆) −βft(ST
⋆)
!
= O(nT
2+γ
3 ).
which implies that E[Rα,β(T)] = O(nT
2+γ
3 ) as desired.
We proceed to derive a high probability bound using Lemma E.2. Indeed, we ﬁrst consider
the case of T < 2 log
3
2+γ (1/δ). Since ft = ¯ft −ft satisfy that ¯ft([n]) + ft([n]) ≤L for all t ≥1,
we have
Rα,β(T) ≤
T
X
t=1
ft(St) −
T
X
t=1
( 1
α ¯ft(ST
⋆) −βft(ST
⋆)) ≤
 1 + 1
α + β

LT = O(T
4−γ
6 p
log(1/δ)).
For the case of T ≥2 log
3
2+γ (1/δ), we obtain by combining Lemma E.2 with Eq. (29) that
T
X
t=1
E[ft(St) | xt] ≤
 T
X
t=1
1
α ¯ft(S) −βft(S)
!
+
n
2η ¯
T + 2L
 T
X
t=1
µt
!
+ 4n2L2


¯T
X
t=1
ηt
µqt


+4nL2


¯T
X
t=1
t−1
X
s=qt
ηs
µs

+ 12L ¯T
4−γ
6 p
n2 + n log(1/δ) + 6L
q
n ¯T log(1/δ),
with probability at least 1−3δ. Then, it suﬃces to bound the term PT
t=1 ft(St)−PT
t=1 E[ft(St) |
xt] using Proposition C.3. By using the same argument as in Theorem 4.3, we have
P
 
T
X
t=1
ft(St) −E[ft(St) | xt]
 > 3L
p
T log(1/δ)
!
≤δ,
which implies that PT
t=1 ft(St) −PT
t=1 E[ft(St) | xt] ≤3L
p
T log(1/δ) with probability at
least 1 −δ. Putting these pieces together yields that
T
X
t=1
ft(St) ≤
 T
X
t=1
1
α ¯ft(S) −βft(S)
!
+
n
2η ¯
T + 2L
 T
X
t=1
µt
!
+ 4n2L2


¯T
X
t=1
ηt
µqt


+4nL2


¯T
X
t=1
t−1
X
s=qt
ηs
µs

+ 3L
p
T log(1/δ) + 12L ¯T
4−γ
6 p
n2 + n log(1/δ) + 6L
q
n ¯T log(1/δ),
with probability at least 1 −4δ. Plugging the choices of ηt =
1
Lt(2+γ)/3 and µt =
n
t(1−γ)/3 and
¯T = Θ(T) yields that
T
X
t=1
ft(St) −
 T
X
t=1
1
α ¯ft(S) −βft(S)
!
= O

nT
2+γ
3
+
p
n log(1/δ)T
4−γ
6

,
with probability at least 1 −4δ. Letting S = ST
⋆= argminS⊆[n]
PT
t=1 ft(S) and changing δ
to δ
4 yields that Rα,β(T) = O(nT
2+γ
3
+
p
n log(1/δ)T
4−γ
6 ) with probability at least 1 −δ as
desired.
36

