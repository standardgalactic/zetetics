
Advisory Board: S. Amari G. Brassard K.A. De Jong
C.C.A.M. Gielen T. Head L. Kari L. Landweber T. Martinetz
Z. Michalewicz M.C. Mozer E. Oja G. P˘aun J. Reif H. Rubin
A. Salomaa M. Schoenauer H.-P. Schwefel C. Torras
D. Whitley E. Winfree
E.J.M. Zurada
Natural Computing Series
Series Editors: G. Rozenberg
Th. Bäck A.E. Eiben J.N. Kok H.P. Spaink
Leiden Center for Natural Computing

Philip F. Hingston · Luigi C. Barone ·
Zbigniew Michalewicz (Eds.)
Design by Evolution
Advances in Evolutionary Design
Foreword by David B. Fogel
123

Philip F. Hingston
School of Computer and Information
Sciences
Edith Cowan University
2 Bradford St
Mt. Lawley, WA 6020, Australia
p.hingston@ecu.edu.au
Luigi C. Barone
School of Computer Science and
Software Engineering
The University of Western Australia
35 Stirling Highway
Crawley, WA 6009, Australia
luigi@csse.uwa.edu.au
Zbigniew Michalewicz
School of Computer Science
University of Adelaide
Adelaide, SA 5005, Australia
zbyszek@cs.adelaide.edu.au
ISBN: 978-3-540-74109-1
e-ISBN: 978-3-540-74111-4
Library of Congress Control Number: 2008932894
ACM Computing Classiﬁcation (1998): I.2, F.1, J.2, J.3, J.5
c⃝2008 Springer-Verlag Berlin Heidelberg
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is
concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting,
reproduction on microﬁlm or in any other way, and storage in data banks. Duplication of this publication
or parts thereof is permitted only under the provisions of the German Copyright Law of September 9,
1965, in its current version, and permission for use must always be obtained from Springer. Violations are
liable to prosecution under the German Copyright Law.
The use of general descriptive names, registered names, trademarks, etc. in this publication does not imply,
even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective laws
and regulations and therefore free for general use.
Cover design: KuenkelLopka GmbH
Printed on acid-free paper
9 8 7 6 5 4 3 2 1
springer.com

Foreword
When we think of design, it is usually in the context of solving some sort of
problem, such as the design of a bridge, a city, or a song. To be eﬀective, the
design must address a purpose to be achieved. In the case of a bridge or a
city, the parameters of interest can be quantiﬁed. In the case of a bridge, they
might include facets such as structural integrity, maximum carrying capacity,
and cost, or in the case of a city, traﬃc ﬂow, resource utilization, safety, and
also again cost. In the case of a song, things are not as clear. The time-honored
saying of beauty residing in the eye (or in this case, the ear) of the beholder
remains true. Yet, a person can at least oﬀer guidance about whether or not a
song is to his or her liking, or perhaps more enjoyable than some other song.
Thus, eﬀective design requires some feedback mechanism to the designer.
As has been argued many times for more than 150 years, evolution, a de-
sign process that is ancient to the extreme, serves as a designer – the blind
watchmaker – by using the process of random variation and natural selection,
iterated over generations. The concept of design is usually connected with
intelligence, and yet evolution – a simple process resulting from reproducing
organisms with variation competing for ﬁnite resources – can itself eﬃciently
design wondrous creatures, each of which ﬁnds diﬀerent solutions to its pri-
mary problem: the problem of survival. It is natural to look to evolution for
inspiration on how to create algorithms that can design solutions to our own
challenging problems.
It should be no surprise then that computer scientists, engineers, mathe-
maticians, roboticists, and biologists alike have made considerable eﬀorts to
program evolution into algorithms. Such eﬀorts now span at least 55 years,
dating back to Nils Barricelli’s pioneering work on artiﬁcial life algorithms
at von Neumann’s laboratory at Princeton University in 1953. Even in the
initial phases of this body of research into what is now called evolutionary
computation there were examples of evolutionary algorithms used to create
artiﬁcial intelligence, design mechanical devices, discover optimal strategies in
games, and many other tasks that require the act of designing a solution to a
problem.

VI
Foreword
Today, with computers that are over 10 billion times faster than the com-
puters of the early 1950s, the range of design problems that can be explored
with this approach is much broader. Design by Evolution – Advances in Evo-
lutionary Design highlights some recent contributions in this ﬁeld in the areas
of biology, art, embryogeny, and engineering. You will ﬁnd the chapters to be
timely, sometimes controversial, and always thought-provoking.
Design and creativity are strongly connected concepts, but this is not a
novel recognition. Over 40 years ago, Fogel et al. [2] argued that the process
of evolution is analogous to the process of the scientiﬁc method, a process
that can be simulated as an evolutionary program, in which “the process of
induction [is] reduced to a routine procedure. If ‘creativity’ and ‘imagination’
are requisite attributes of this process, then these too have been realized.”
Even now, this concept may seem provocative to some. It was more than
provocative in the 1960s, eliciting dismissive comments that such ideas were
nothing more than “fustian [that] may unfortunately alienate many ... from
the important work being done in artiﬁcial intelligence” [3]. Yet, what words
of attribution then would be appropriate for an antenna designed for a space
application by an evolutionary algorithm [4] that featured a brand new conﬁg-
uration and comments from traditional antenna designers of “we were afraid
this was going to happen”? What words of attribution would be appropriate
for an evolutionary program – Blondie24 – that taught itself to play checkers
at a level commensurate with human experts without using human expertise
about how to play checkers [1], when human opponents routinely praised the
program’s “good moves” and even observed that the program was “clamping
down on mobility”? Blondie24 had no preprogrammed concept of mobility.
Evidently, it created the concept of mobility.
Computers can become creative designers by using evolutionary processes.
Random variation is a key component of this creativity, for random variation
is what provides the element of surprise that can generate something truly
novel, something beyond just shifting and sifting through combinations of
pieces of existing solutions. It is ironic, perhaps, that so much of engineering
is aimed at removing noise and yet noise is an integral part of the ingenuity
that engineering requires!
Evolution is a creative process in every sense of the word creative. I ex-
pect that you will ﬁnd the contributions in this book to provide a current
accounting of the potential for using an evolutionary paradigm for creating
novel designs in the four main topic areas discussed. I also expect that you will
ﬁnd these contributions creative in their own right, and illustrative of what
we may expect in terms of automating the process of design in the future.
San Diego,
August 2007
David B. Fogel

Foreword
VII
References
1. Fogel, D.: Blondie24: Playing at the Edge of AI. Morgan Kaufmann, San Fran-
cisco, CA (2001)
2. Fogel, L., Owens, A., Walsh, M.: Artiﬁcial Intelligence through Simulated Evo-
lution. John Wiley (1966)
3. Lindsay, R.: Artiﬁcial evolution of intelligence. Contemporary Psychology 13(3),
113–116 (1968)
4. Lohn, J., Linden, D., Hornby, G., Kraus, B., Rodriguez, A., Seufert, S.: Evolu-
tionary design of an X-band antenna for NASA’s Space Technology 5 Mission.
In: Proc. 2004 IEEE Antenna and Propagation Society International Symposium
and USNC/URSI National Radio Science Meeting, vol. 3, pp. 2313–2316 (2004)

Preface
It is often stated that the 19th century was the century of chemistry, the 20th
century was the century of physics and that the 21st century will be the cen-
tury of biology. We wonder just who ﬁrst said this. Perhaps Polykarp Kusch
should be credited with the phrase “century of physics”, from his 1955 accep-
tance speech for the Nobel Prize in Physics (though the quote was actually
“We live, I think, in the century of science and, perhaps, even in the century
of physics” [2]. It may have been US President Bill Clinton who ﬁrst made the
observation and coined the phrase “century of biology” [1], or maybe Kenneth
Shine, President of the Institute of Medicine, who actually said “... the 20th
century will be known as the century of physics and astronomy ... the 21st
century will be the century of the life sciences in all their ramiﬁcations” [3].
Many people seem to have appropriated the idea and oﬀer it as their own.
But, whoever said it ﬁrst, it is widely accepted now that advances in biol-
ogy will lead to many great marvels in our lifetimes. One of the keys to this
progress is our new and ever improving understanding of genetics and the
processes of natural evolution. And much of this new understanding is driven
by computation, which is where we, as computer scientists, come in.
Our own interests are in Artiﬁcial Intelligence (AI) – the attempt to
model and understand intelligence through computational models. Evolution-
ary Computation (EC) is one way to do this using models based on the prin-
ciples of natural evolution. We feel very fortunate to be working in EC, which
allows us to indulge our fascination with both AI and evolution. This work
has taken us into evolutionary design: two of us on evolving designs for ore
processing plants, and the other on a wide range of real world applications.
This is what motivated us to organise a special session on evolutionary design
at the IEEE World Congress on Evolutionary Computation in Edinburgh in
2005 (CEC’05), and, partly as a response to the interesting ideas presented at
that session, to embark on collecting together the contributions that make up
this book. Along the way, we have realised that there are deep and beautiful
connections between work done by scientists, engineers and artists, each with

X
Preface
their own speciﬁc knowledge and culture, all striving to understand evolution
as a design process or as a tool.
We would like to thank all our authors, our reviewers, Lucas Bradstreet
and Mark Wittkamp, for their help in proof-reading the book, and, most
especially, our area leaders for biology (Kay Wiese), art (Jon McCormack),
embryogeny (Daniel Ashlock), and engineering (Kalyanmoy Deb), whose cre-
ative suggestions and diligence have made an immense contribution to the
quality of the ﬁnal result.
In this book, the reader will ﬁnd chapters drawing on many diverse, yet
interconnected topics. There are chapters on creating evolved art using EC
along with L-systems, cellular automata and Mandelbrot sets, making images
of biological life or beguiling abstract patterns. Chapters about simulations
of biological processes that produce images so beautiful they must be called
art. Chapters describing our progress from understanding how morphogene-
sis works in biology, to using such processes to create art or other physical
artefacts, towards designing systems to control the process of morphogenesis
itself. Chapters about using what we have learned to solve practical engineer-
ing design problems and even some more philosophical discussions about what
this all means and how it relates to our place in the universe.
Whatever the reader’s penchant, we hope there is something new here for
everyone, that will educate, inspire, and delight.
Philip Hingston
Luigi Barone
Zbigniew Michalewicz
References
1. Clinton, W.: Remarks at the Morgan State University commencement ceremony
(1997). http://avac.org/pdf/hvad_clinton_speech.pdf
2. Kusch, P.: Nobel banquet speech (1955).
http://nobelprize.org/nobel_
prizes/physics/laureates/1955/kusch-speech.html
3. Shine, K.: Welcome. In: Serving Science and Society Into the New Millennium.
The National Academies Press (1998)

Contents
Part I Biology
Evolutionary Design in Biology
Kay C. Wiese . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1 Intelligent Design and Evolutionary Computation
Thomas English, Garrison W. Greenwood . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2 Inference of Genetic Networks Using an Evolutionary
Algorithm
Shuhei Kimura . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
3 Synthetic Biology: Life, Jim, but Not As We Know It
Jennifer Hallinan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
4 Dancing with Swarms: Utilizing Swarm Intelligence to
Build, Investigate, and Control Complex Systems
Christian Jacob . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
Part II Art
Evolutionary Design in Art
Jon McCormack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
5 Natural Processes and Artiﬁcial Procedures
Erwin Driessens, Maria Verstappen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
6 Evolutionary Exploration of Complex Fractals
Daniel Ashlock, Brooke Jamieson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
7 Evolving the Mandelbrot Set to Imitate Figurative Art
JJ Ventrella . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145

XII
Contents
8 Evolutionary L-systems
Jon McCormack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
Part III Embryogeny
Evolutionary Design in Embryogeny
Daniel Ashlock . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
9 Embryogenesis of Artiﬁcial Landscapes
Daniel Ashlock, Stephen Gent, Kenneth Bryden . . . . . . . . . . . . . . . . . . . . . . 203
10 On Form and Function: The Evolution of Developmental
Control
Sanjeev Kumar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
11 Modularity in a Computational Model of Embryogeny
Chris P. Bowers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
Part IV Engineering
Evolutionary Design in Engineering
Kalyanmoy Deb . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
12 Engineering Optimization Using Evolutionary Algorithms:
A Case Study on Hydro-thermal Power Scheduling
Kalyanmoy Deb . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
13 Multimodal Function Optimization of Varied-Line-Spacing
Holographic Grating
Qing Ling, Gang Wu, Qiuping Wang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
14 GPBG: A Framework for Evolutionary Design of Multi-
domain Engineering Systems Using Genetic Programming
and Bond Graphs
Jianjun Hu, Zhun Fan, Jiachuan Wang, Shaobo Li, Kisung Seo,
Xiangdong Peng, Janis Terpenny, Ronald Rosenberg, Erik Goodman . . . . 319
Index
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347
. . . . . . .
.

Part I
Biology

Evolutionary Design in Biology
Kay C. Wiese
Simon Fraser University, BC, Canada wiese@cs.sfu.ca
Much progress has been achieved in recent years in molecular biology and
genetics. The sheer volume of data in the form of biological sequences has
been enormous and eﬃcient methods for dealing with these huge amounts of
data are needed. In addition, the data alone does not provide information on
the workings of biological systems; hence much research eﬀort has focused on
designing mathematical and computational models to address problems from
molecular biology. Often, the terms bioinformatics and computational biology
are used to refer to the research ﬁelds concerning themselves with designing so-
lutions to molecular problems in biology. However, there is a slight distinction
between bioinformatics and computational biology: the former is concerned
with managing the enormous amounts of biological data and extracting in-
formation from it, while the latter is more concerned with the design and
development of new algorithms to address problems such as protein or RNA
folding. However, the boundary is blurry, and there is no consistent usage of
the terms. We will use the term bioinformatics to encompass both ﬁelds. To
cover all areas of research in bioinformatics is beyond the scope of this section
and we refer the interested reader to [2] for a general introduction. A large
part of what bioinformatics is concerned about is evolution and function of
biological systems on a molecular level. Evolutionary computation and evo-
lutionary design are concerned with developing computational systems that
“mimic” certain aspects of natural evolution (mutation, crossover, selection,
ﬁtness). Much of the inner workings of natural evolutionary systems have been
copied, sometimes in modiﬁed format into evolutionary computation systems.
Artiﬁcial neural networks mimic the functioning of simple brain cell clusters.
Fuzzy systems are concerned with the “fuzzyness” in decision making, similar
to a human expert. These three computational paradigms fall into the cate-
gory of computational intelligence (CI). While biological systems have helped
to develop many of the computational paradigms in CI, CI is now returning
the favor to help solve some of the most challenging biological mysteries itself.
In many cases these probabilistic methods can produce biologically relevant
results where exact deterministic methods fail. For an extensive overview of

4
Wiese
successful applications of CI algorithms to problems in bioinformatics please
refer to [1].
The work presented in this section covers four chapters.
The ﬁrst chapter by Tom English and Garrison Greenwood covers a discus-
sion of intelligent design (ID) and evolutionary computation. The proponents
of intelligent design try to establish that the complexities inherent in many
biological systems are such that there is no chance they may have “evolved”.
Rather, it is proposed that elements of design are evident, and hence these
biological systems were designed, not evolved. The chapter investigates the
general claims made by ID and shows the logical ﬂaws in this reasoning on
a number of examples. Also, it highlights how some of the mathematics and
statistical reasoning is ﬂawed. In addition, it provides examples of how evo-
lutionary algorithms can evolve complex systems that according to ID would
exhibit elements of design, but obviously this is not the case since the system
evolved from a few very basic rules without human intelligence.
The second chapter by Jennifer Hallinan is a review chapter on the topic
of synthetic biology. The diﬀerences and commonalities between genetic en-
gineering and synthetic biology are explored. While genetic engineering is a
reality today and involves the modiﬁcation of genetic or biological systems,
synthetic biology aims to construct an organism from ﬁrst principles. A note-
worthy example of a successful synthetic biology experiment is that of the
creation of a poliovirus from the knowledge of its DNA sequence alone. Af-
ter discussing some minimal requirements for the successful practice of syn-
thetic biology the chapter explores the topic of biological networks and mo-
tifs. Understanding gene networks and gene expression is essential towards
understanding how cellular systems work. The chapter advises us that a more
data-driven approach to synthetic biology will prove useful and it explores the
potential of evolutionary computation to inform the ﬁeld of synthetic biology.
Chapter 3 by Shuhei Kimura discusses the inference of genetic networks
with an evolutionary algorithm. Two inference methods, the problem decom-
position approach and the cooperative co-evolution approach, are presented.
Both of these methods are evaluated and it is demonstrated that they can infer
genetic networks of dozens of genes. The analysis of actual DNA microarray
data usually requires genetic networks of hundreds of genes. To address this
Kimura proposes a method to combine the cooperative co-evolution approach
with a clustering technique.
Chapter 4 by Christian Jacob discusses examples of massively parallel,
decentralized information processing systems and explores how swarm intel-
ligence can be used to build and investigate complex systems. The examples
studied include gene regulation inside a bacterial cell, bacteria and cell popu-
lation simulations including chemotaxis, cellular orchestration and army ant
raiding, herd behavior (predator prey), and human behavior (sales table rush).
All of these examples are beautifully illustrated with graphics that show the
evolution of the systems or the swarm behavior. This is very useful for un-
derstanding these complex, dynamic systems. Some insightful comments on

Evolutionary Design in Biology
5
swarm intelligence, evolutionary computation, and the re-use of software li-
braries are provided as well.
These chapters provide an interesting mix of problems and methods to
show the diversity of biological applications that can be solved with evolu-
tionary techniques. While biological systems have inspired new computational
paradigms such as evolutionary computation or swarm intelligence, these tech-
niques can now be used to further our understanding of biological systems
themselves. We hope that you will enjoy these chapters.
Kay C. Wiese
Biology Area Leader
References
1. Fogel, G., Corne, D., Pan, Y. (eds.): Computational Intelligence in Bioinformat-
ics. IEEE Press, Piscataway, NJ (2007)
2. Jones, N., Pevzner, P.: An Introduction to Bioinformatics Algorithms. The MIT
Press, Cambridge, Massachusetts (2004)

1
Intelligent Design and Evolutionary
Computation
Thomas English1 and Garrison W. Greenwood2
1 Bounded Theoretics, Lubbock, Texas, USA Thom.English@gmail.com
2 Portland State University, Portland, Oregon, USA greenwd@ece.pdx.edu
We ought, it seems to me, to consider it likely that the formation of
elementary living organisms, and the evolution of those organisms, are
also governed by elementary properties of matter that we do not un-
derstand perfectly but whose existence we ought nevertheless admit.
´Emile Borel, Probability and Certainty
1.1 Introduction
In the United States, a succession of lost legal battles forced opponents of
public education in evolution to downgrade their goals repeatedly. By the
1980s, evolution was ensconced in the biology curricula of public schools,
and references to the creator of life were illegal. The question of the day
was whether instruction in creation, without reference to the creator, as an
alternative explanation of life violated the constitutional separation of church
and state. In 1987, the U.S. Supreme Court decided that it did, and intelligent
design (ID) rose from the ashes of creation science. ID may be seen as a
downgraded form of creation. While the creation science movement sought
to have biology students introduced to the notion that creation is evident in
the complexity of living things, the ID movement sought to have students
introduced to the notion that design, intelligence, and purpose are evident.3
ID preserves everything in the notion of creation but the making.
Although intellectual endeavor is secondary to sociopolitical action in the
ID movement, the objective here is to assess the intellectual component. Sepa-
rating the two is not always possible. Sometimes ID advocates formulate their
3 The ID movement, led by the Discovery Institute, has downgraded its goals, and
presently does not advocate teaching ID in public schools. The Discovery Institute
continues, however, to advocate teaching the shortcomings of evolutionary theory.
It bears mention also that the ID movement now distinguishes biological ID and
cosmological ID. Here we focus on biological ID.

8
English and Greenwood
ideas in ways that make sense only in light of their sociopolitical objectives.
The main intellectual oﬀering of ID is the design inference, an ostensibly sci-
entiﬁc adaptation of the classical argument from design. While the classical
argument might indicate that a natural entity is too complex to have arisen
unless created by an intelligent and purposive agent, and that the agent could
only be God, a design inference eschews creation and declines to identify the
agent, concluding that a non-natural and purposive intelligence designed the
natural entity.
The sociopolitical ingenuity of the design inference is that, if taught as an
alternative to evolution in public-school science classes, it would leave identiﬁ-
cation of the designer to schoolchildren. The faithful would conclude that sci-
ence supports belief in God the Designer and disbelief in evolutionary theory.
Whether ID’s evasion of direct reference to God eventually will pass judicial
scrutiny in the U.S. is unknown. The design inference has a legal vulnerability
arising from the fact that non-natural intelligence is supernatural, and the su-
pernatural is clearly linked with the religious in case law [24, p. 67]. In recent
years, the ID movement has shifted to saying that intelligence is natural, but
not material. (For an example of earlier usage, see [23].) Given that scientists
conventionally regard nature to be material, the ID movement has changed
the meaning of natural to suit itself, and for no apparent reason but to gain
better legal footing. Similarly, many ID advocates call themselves evolution-
ists, falling back on a dictionary meaning of the term (a process of change
in a given direction), rather than scientists’ conventional interpretation (an
undirected process of change deriving from random variation of oﬀspring and
natural selection). This chapter will use conventional scientiﬁc terminology.
There are two basic approaches to design inference: the argument from
irreducible complexity, and the argument from speciﬁed complexity. The argu-
ment from irreducible complexity is a demonstration that a biological system
with several or more parts could not serve any useful function if any of its
parts were removed. This in eﬀect shows that the system cannot have evolved
directly. The argument from speciﬁed complexity demonstrates that an entity
matches some pattern recognized by an intelligent agent, and that it is im-
probable that any match of the pattern would arise by natural (materialistic)
causes in the history of the universe.
The ID movement has for some years considered evolutionary computation
(EC) a threat. EC is the proving ground of ideas in evolutionary theory, ac-
cording to William A. Dembski, billed by the ID movement as the “Isaac New-
ton of Information Theory” [13]. Positive results in EC apparently contradict
Dembski’s claims about “conservation of information” in chance-and-necessity
processes. He and other ID advocates acknowledge this, and are at pains to
show that the contradiction is only apparent [13]. Thus EC investigators have
a potent means of challenging ID. In fact, the main biological claims of ID
advocates such as Michael Behe [3] and Steven Meyer [28] are that evolution
cannot account for certain biological innovations that occurred hundreds of
millions of years ago, and it is easier to challenge the information-theoretic

1 Intelligent Design and Evolutionary Computation
9
claims of Dembski with simulation and analysis than it is to challenge the
speciﬁc biological claims of other ID advocates with new data on ancient
events.
Research in artiﬁcial life is closely related to that in EC, and in the present
context the artiﬁcial life program Avida will be considered an example of EC.
Research with Avida has been reported in Nature [25], and it apparently
contradicts the central claim Behe makes in Darwin’s Black Box [3], namely
that gradual evolutionary processes cannot generate irreducible complexity
(deﬁned below). In a brief intended for submission to a federal judge, Dembski
does not deny that Avida generated irreducible complexity, but instead argues
that it lacks biological relevance [16, p. 19].
1.2 Historical Background
The current arguments in favor of ID follow a common theme. In one way or
another they all state that the universe we observe is so complex that it simply
could not have developed by mere chance; some intelligent agent had to have
been responsible. Today ID is the primary opponent of Darwin’s theories of
evolution and disputes often become heated.
Is this controversy – some would call it a war – of competing ideas some-
thing new? Surprisingly, no. In fact, this war began centuries ago and the
battles conducted today are not all that diﬀerent from those waged in the
past. It is therefore instructive to see exactly what the arguments were in fa-
vor of creationism then, and how they compare to the arguments being made
by the ID community today. That is the purpose of this section.4
Darwin (1809–1882) was among the ﬁrst to oﬀer a plausible explanation
of nature’s development [9]. His theory was accompanied by a large amount
of empirical evidence, which strongly contributed to its initial acceptance.
In spite of its critics, Darwin’s theory was largely responsible for the demise
of creationism in the late 19th century. It therefore seems somewhat ironic
that the tables have turned – today ID is the major threat to the teaching of
evolution!
The earliest ID arguments were overtly religious with no attempts to hide
the intelligent designer’s identity. Archbishop James Ussher (1581–1656) is
best known for declaring the world was created in late October 4004 BC. [43].
In the beginning God created Heaven and Earth, Gen. I. V. I. Which
beginning of time, according to our Chronologie, fell upon the entrance
of the night proceeding the twenty third day of October, in the year
of the Julian Calendar, 710 [4004 BC].
4 Some quotes appearing in this section have unusual grammar, spelling and punc-
tuation. They were purposely kept that way to preserve the original wording of
papers written in the 17th through 19th centuries.

10
English and Greenwood
Others interjected anatomy into the debate while still acknowledging God as
the designer. John Ray (1627–1705) used the eye [37]:
For ﬁrst, Seeing, for instance, That the Eye is employed by Man and
Animals for the sue of Vision, which, as they are framed, is so neces-
sary for them, that they could not live without it; and God Almighty
knew that it would be so; and seeing it is so admirably ﬁtted and
adapted this use, that all the Wit and Art of men and Angels could
not have contrived it better is so well; it must needs be highly absurd
and unreasonable to aﬃrm, either that it was not designed at all for
this use, or that it is impossible for man to know whether it was or
not.
Of course early arguments were not restricted to just human anatomy. William
Paley (1743–1805) compared the eye of a ﬁsh, which must process light re-
fracted by water, with the eye of land animals, which must process light
passing through air [35]. He believed such a subtle physical diﬀerence, while
still performing the same function, was conclusive proof of a designer.
Accordingly we ﬁnd that the eye of a ﬁsh. . . is much rounder than the
eye of terrestrial animals. What plainer manifestation of design can
there be than this diﬀerence?
Remarkably the eye example is still used today, although the arguments are
now more sophisticated with decades of biochemical research for support.
Michael Behe attempts to discredit Darwin in the following way [4]:
Neither of Darwin’s black boxes – the origin of life or the origin of
vision (or other complex biochemical systems) – has been accounted
for by his theory.
Many early ID arguments were philosophical and had no real scientiﬁc content.
This is not unexpected because science had made little progress by today’s
standards and the general populace had no background in it anyway. Perhaps
the most famous argument made along this line is William Paley’s story about
ﬁnding a watch [35].
In crossing a heath,5 suppose I pitched my foot against a stone, and
were asked how the stone came to be there; I might possibly answer
that, for anything I knew to the contrary, it might have been there
for ever: nor would it perhaps be very easy to show the absurdity of
this answer. But suppose I had found a watch on the ground, and
it should be inquired how the watch happened to be in that place; I
should hardly think of the answer I had given before, that for anything
I knew, the watch might have always been there. Yet why should not
this answer serve for the watch as well as for the stone? . . . when
5 A tract of open wasteland.

1 Intelligent Design and Evolutionary Computation
11
we come to inspect the watch, we perceive . . . that the watch must
have had a maker: that there must have existed, at some time, and at
some place or other, and artiﬁcer or artiﬁcers, who formed it for the
purpose which we ﬁnd it actually to answer; who comprehended its
construction, and designed its use.
It should come as no surprise that such a widely referenced anecdote from the
past would be the basis for a philosophical counter-argument of today [10].
All appearances to the contrary, the only watchmaker in nature is the
blind forces of physics, albeit deployed in a very special way. A true
watchmaker has foresight: he designs his cogs and springs, and plans
their interconnections, with a future purpose in mind. Natural selec-
tion, the blind, unconscious automatic process Darwin discovered. . .
has no purpose in mind. It has no vision, no foresight, no sight at all.
If it can be said to play the role of watchmaker in nature, it is the
blind watchmaker.
Prior to the 20th century science had not progressed to a point where it could
provide much support to the proponents of evolution. Nevertheless, that did
not prevent some from disparaging scientiﬁc arguments anyway. For instance,
John Ray even went so far as to say scientiﬁc principles are meaningless with-
out belief in a designer [37].
In particular I am diﬃcult to believe, that the Bodies of Animals can
be formed by Matter divided and moved by that Laws you will or can
imagine, without the immediate Presidency, Direction and Regulation
of some Intelligent Being.
The last quote from Ray also demonstrates a new and signiﬁcant change
in the creationist’s arguments. Up to this point there were no misgivings
about identifying who the intelligent designer was: it was God from the Old
Testament of the Bible. Notice in Ray’s quote there was no speciﬁc reference
to a Judeo-Christian God. Instead, the designer’s identity was purposely kept
vague. The ID community today is very careful to avoid any references to a
Judeo-Christian deity. William Dembski, one of the leaders in the present day
ID movement, recently put it this way [12]:
Intelligent design is theologically minimalist. It detects intelligence
without speculating about the nature of the intelligence.
Another change in tactics along this same line was to make secular arguments
based on what a reasonable man would (or at least should) believe. What gave
some of these new ID arguments credibility was highly respected scientists
were making them and not theologians. For instance, the renowned naturalist
Louis Agassiz (1807–1873) took believers of evolution to task [1].

12
English and Greenwood
The most advanced Darwinians seem reluctant to acknowledge the
intervention of an intellectual power in the diversity which obtains
in nature, under that plea that such an admission implies distinct
creative acts for every species. What of it, if it were true?
The goal here was to put the Darwinist on the defensive. The argument went
something like this: since there was no irrefutable proof that evolution can
explain how the world developed, then any “reasonable person” should be
open to alternative ideas – and of course creationism just happens to be one
of those alternative ideas. These arguments were meant to intimidate, if not
outright mock, those who wouldn’t seriously consider creationist ideas. For
example, in [1] Agassiz also said
Have those who object to repeated acts of creation ever considered
that no progress can be made in knowledge without repeated acts of
thinking? And what are thoughts but speciﬁc acts of the mind? Why
should it then be unscientiﬁc to infer that the facts of nature are the
result of a similar process, since there is no evidence of any other
cause? The world has arisen in some way or other.
This tactic of mocking unbelievers is still used today. For example, in 1990
Phillip Johnson (who was a lawyer and not a scientist) wrote the following [23]:
What the science educators propose to teach as evolution and label as
fact, is based not upon any incontrovertible empirical evidence, but
upon a highly controversial philosophical presupposition. The contro-
versy over evolution is therefore not going to go away as people become
better educated on the subject.
Until the late 19th century science and religion were strongly intertwined. It is
therefore not unexpected that creationist thought permeated early discussions
about the origin of life. Even Darwin was not immune and recognized the role
of a Creator. In his treatise On the Origin of the Species he stated
To my mind, it accords better with what we know of the laws im-
pressed upon matter by the Creator, that the production and extinc-
tion of the past inhabitants of the world should have been due to
secondary causes, like those determining the birth and death of the
individual.
Nowadays science and religion are at odds with each other over the evolution
issue. Without exaggeration, ID advocates seek to return science to the days of
natural philosophy, when scientiﬁc beliefs were tested against higher religious
truths. ID supporters talk about “design theory” as science while mainstream
scientists counter that it is really no science at all. Unfortunately, this debate
has moved to the public arena where emotions often trump objective dialog.

1 Intelligent Design and Evolutionary Computation
13
1.3 What Is Intelligent Design?
Advocates of ID use the term intelligent design to name both their ﬁeld of
inquiry and a putative cause of certain natural phenomena. They refer to their
body of beliefs as intelligent design theory. Note that the sense of theory here
is not scientiﬁc theory.
In ID theory, information is a physical primitive, like matter and energy,
which may enter the natural universe from without.6 An intelligence is a non-
natural source of information – i.e., it changes probabilities of events in the
natural universe. When an intelligence increases the probability of an event
that is in some sense meaningful or functional, it is goal-directed or telic. The
central thesis of ID is that some natural entities exhibit such complex organi-
zation that the processes giving rise to them cannot have been entirely natural,
but instead must have been directed (informed) to some degree by telic in-
telligence. The type of organization of interest to ID theorists is known as
speciﬁed complexity (or complex speciﬁed information). An entity with speci-
ﬁed complexity higher than ID theory says could have arisen by purely natural
processes is said to be intelligently designed. ID theorists consider irreducible
complexity as an indicator of high speciﬁed complexity.
ID theory says something outside of nature may cause an event within
nature. In contrast, mainstream scientists embrace methodological naturalism,
the working assumption that all natural phenomena have natural causes. ID
theory allows theists to associate the intelligent causation of humans with
that of one or more deities. In particular, the Biblical notion that humans, as
intelligent entities with free will, are created in the image of God, and thus
stand apart from nature in some aspects, is supported by the philosophy of
intelligent design.
Many ID sites on the Internet (e.g., [21]) oﬀer the deﬁnition, attributed to
Dembski, “Intelligent Design is the study of patterns in nature that are best
explained as the result of intelligence.” Although this is a casual deﬁnition, its
shortcomings are worth examining. First, ID theory permits an event to be
explained in terms of both non-natural intelligence and natural antecedents.
Design is not all-or-nothing. Second, ID does not study the patterns per se,
but which patterns indicate that intelligence has contributed information.
The following sections discuss the two main approaches to design inference:
argument from irreducible complexity and argument from speciﬁed complexity.
1.4 Irreducible Complexity
The champion of irreducible complexity in the ID community is Michael Behe,
a biochemist. He has given two deﬁnitions of the term. William Dembski, a
6 Recall that ID advocates say that intelligence is natural but not material, and
that mainstream science holds that anything natural is material.

14
English and Greenwood
mathematician and philosopher, followed with a related deﬁnition. The arti-
ﬁcial life program Avida has posed a challenge to the claim that evolution
cannot give rise to irreducible complexity.
1.4.1 Behe’s Deﬁnitions
In The Origin of Species, Charles Darwin wrote, “If it could be demonstrated
that any complex organ existed, which could not possibly have been formed
by numerous, successive, slight modiﬁcations, my theory would absolutely
break down” [9]. In Darwin’s Black Box, biochemist and intelligent design
advocate Michael Behe responds with the claim that some biological systems
are irreducibly complex:
By irreducibly complex I mean a single system which is composed of
several interacting parts that contribute to the basic function, and
where the removal of any one of the parts causes the system to ef-
fectively cease functioning. An irreducibly complex system cannot be
produced directly (that is, by continuously improving the initial func-
tion, which continues to work by the same mechanism) by slight, suc-
cessive modiﬁcations of a precursor system, because any precursor to
an irreducibly complex system that is missing a part is by deﬁnition
nonfunctional. [...] Even if a system is irreducibly complex (and thus
cannot have been produced directly), however, one can not deﬁnitely
rule out the possibility of an indirect, circuitous route. As the com-
plexity of an interacting system increases, though, the likelihood of
such an indirect route drops precipitously. [3, pp. 39–40].
Behe holds that irreducible complexity is evidence for intelligent design. There
are two logical ﬂaws here, however, both of which Behe has acknowledged [24].
First, to treat evidence against Darwinian gradualism as evidence for intelli-
gent design is to set up a false dichotomy. For instance, there could be some
biological structure for which both explanations are wrong. Second, a biolog-
ical system that is irreducibly complex may have precursors that were not
irreducibly complex.
To help understand this idea, think of an irreducibly complex biological
structure as a stone arch. The argument from irreducible complexity is that
all the stones could not have been put in place simultaneously by evolutionary
processes, and that the arch must be the product of intelligent design. But this
ignores the possibility that preexisting structures were used opportunistically
as “scaﬀolding” in gradual assembly of the arch. If the scaﬀolding is removed
by evolution after the arch is complete, then the arch is irreducibly complex,
but arguments that it could not have emerged gradually are wrong.
Interestingly, H. J. Muller, a geneticist who went on to win a Nobel prize,
held in 1918 that interlocking complexity (identical to irreducible complexity)
arose through evolution:

1 Intelligent Design and Evolutionary Computation
15
Most present-day animals are the result of a long process of evolu-
tion, in which at least thousands of mutations must have taken place.
Each new mutant in turn must have derived its survival value from
the eﬀect which it produced upon the “reaction system” that had
been brought into being by the many previously formed factors in co-
operation; thus a complicated machine was gradually built up whose
eﬀective working was dependent upon the interlocking action of very
numerous diﬀerent elementary parts or factors, and many of the char-
acters and factors which, when new, were originally merely an asset
ﬁnally became necessary because other necessary characters and fac-
tors had subsequently become changed so as to be dependent on the
former. It must result, in consequence, that a dropping out of, or even
a slight change in any one of these parts is very likely to disturb fatally
the whole machinery... [32]
Muller worked out interlocking complexity in more detail in a 1939 paper [33].
According to Orr [34], “Muller gives reasons for thinking that genes which at
ﬁrst improved function will routinely become essential parts of a pathway. So
the gradual evolution of irreducibly complex systems is not only possible, it’s
expected.”
Parts of the pathway may also arise from neutral mutations and gene
duplication. A duplicate gene is available for mutation into a gene that serves
a diﬀerent function from the original. The mutated duplicate may serve a
function similar to the original, and may come to be required by the organism.
For instance, the genes for myoglobin, which stores oxygen in muscles, and
hemoglobin, which stores oxygen in blood, are closely related, and there is
strong evidence that one or both arose through duplication. Both are necessary
to humans [34]. Another way in which parts of the pathway may arise is
through co-optation (also known as cooption), the adaptation of an existing
biological system to serve a new function. The role of neutral mutations, gene
duplication, and co-optation in the evolution of systems deemed irreducibly
complex by Behe will be discussed in the following.
Behe’s examples of irreducible complexity have generally not stood up to
scrutiny. He gives considerable attention to biochemical cascades – in particu-
lar, the blood-clotting cascade and the complementary cascade of the immune
system. Here we focus on blood clotting. Behe says of cascades [3, p. 87]:
Because of the nature of a cascade, a new protein would immediately
have to be regulated. From the beginning, a new step in the cascade
would require both a proenzyme and also an activating enzyme to
switch on the proenzyme at the correct time and place. Since each step
necessarily requires several parts, not only is the entire blood-clotting
system irreducibly complex, but so is each step in the pathway.
To this Orr responds [34]:

16
English and Greenwood
[Behe] even admits that some genes in his favorite pathway – blood
clotting – are similar. But he refuses to draw the obvious conclusion:
some genes are copies of others. [...] But this implies that such systems
can arise step by step. Behe avoids this conclusion only by sheer eva-
sion: he brands gene duplication a “hypothesis,” leaves the similarity
of his favorite genes unexplained. . .
Miller [30] argues that the gene for ﬁbrinogen has as its ancestor a duplicate
of a gene that had nothing to do with blood clotting. A genetic sequence
similar to that of the ﬁbrinogen gene has been identiﬁed in the sea cucum-
ber, an echinoderm [45]. Furthermore, the blood clotting cascade is not irre-
ducibly complex, because a major component, the Hageman factor, is missing
in whales and dolphins [42], and three major components are missing in puﬀer
ﬁsh [22].
Behe also introduced what has since become the most widely cited example
of irreducible complexity, the bacterial ﬂagellum. However, the base of the
ﬂagellum is structurally similar to the type-3 secretory system (TTSS) of
some bacteria [6]. Furthermore, with 42 distinct proteins in the ﬂagellum, and
25 in the TTSS, there are 10 homologous proteins in the two structures. This
constitutes evidence that the TTSS was co-opted in evolution of the ﬂagellum.
But ID proponents contend “the other thirty proteins in the ﬂagellar motor
(that are not present in the TTSS) are unique to the motor and are not found
in any other living system. From whence, then, were these protein parts co-
opted?” [31]
A simple response to this challenge is that non-TTSS homologs have been
identiﬁed for 17 of the 42 ﬂagellar proteins, leaving only 42 −10 −17 = 15
proteins without known homologs. A more subtle response is that only 20
proteins appear to be structurally indispensable to modern ﬂagella (i.e., 22
are not), and only two of them have no known homologs [36]. Thus most
proteins of the ﬂagellum are not unique to the ﬂagellum, and the notion that
the structure arose through co-optation is at least plausible. That half of the
ﬂagellar proteins are not structurally necessary suggests the ﬂagellum is not
irreducibly complex, but this ignores issues in the evolution of regulation [36].
ID advocates insist that such indirect evidence of co-optation is insuﬃcient.
As Miller [29] has pointed out, demanding direct evidence of the evolution of
biochemical systems has advantages for the ID movement:
Behe demands that evolutionary biologists should tell us exactly “how”
evolution can produce a complex biochemical system. This is a good strate-
gic choice on his part, because the systems he cites, being common to most
eukaryotic cells, are literally hundreds of millions of years old. And, being
biochemical, they leave no fossils.
In contrast, ID advocates might emphasize that the system of ossicles
(small bones transmitting sound from the tympanic membrane to the cochlea)
in the middle ear is irreducibly complex if not for direct fossil evidence of its
evolution from a reptilian jawbone [39].

1 Intelligent Design and Evolutionary Computation
17
Responding to critics of Darwin’s Black Box, Behe [5] points out limita-
tions in his original deﬁnition of irreducible complexity:
It focuses on already-completed systems, rather than on the process
of trying to build a system, as natural selection would have to do.
[...] What’s more, the deﬁnition doesn’t allow for degree of irreducible
complexity [...] irreducible complexity could be better formulated in
evolutionary terms by focusing on a proposed pathway, and on whether
each step that would be necessary to build a certain system using that
pathway was selected or unselected.
Here he acknowledges that neutral mutations (which he refers to as “unse-
lected”) can give rise to irreducible complexity. He observes “if a mutation is
not selected, the probability of its being ﬁxed in a population is independent
of the probability of the next mutation.” And this motivates his “evolution-
ary” deﬁnition of irreducible complexity: “An irreducibly complex evolution-
ary pathway is one that contains one or more unselected steps (that is, one
or more necessary-but-unselected mutations). The degree of irreducible com-
plexity is the number of unselected steps in the pathway” [5].
Behe relates the degree of irreducible complexity directly to the improba-
bility that evolution followed the pathway. “If the improbability of the path-
way exceeds the available probabilistic resources (roughly the number of or-
ganisms over the relevant time in the relevant phylogenetic branch) then Dar-
winism is deemed an unlikely explanation and intelligent design a likely one.”
There are two serious errors in logic here. First, there is the fallacy of the false
dichotomy, with a forced choice between Darwinism and ID when a third al-
ternative might explain the pathway. The improbability of one explanation in
terms of natural causation does not lend credence to an explanation in terms of
non-natural causation. Second, there is a mathematical fallacy long exploited
by creationists. When an evolutionist speciﬁes a particular evolutionary, Behe
proceeds as though evolution could have taken no other path, and computes
an absurdly low probability that the system arose by evolution [5]:
To get a ﬂavor of the diﬃculties [my adversary’s] scenario faces, note
that standard population genetics says that the rate at which neutral
mutations become ﬁxed in the population is equal to the mutation
rate. Although the neutral mutation rate is usually stated as about
10−6 per gene per generation, that is for any random mutation in
the gene. When one is looking at particular mutations such as the
duplication of a certain gene or the mutation of one certain amino acid
residue in the duplicated gene, the mutation rate is likely about 10−10.
Thus the ﬁxation of just one step in the population for the scenario
would be expected to occur only once every ten billion generations.
Yet [my adversary’s] scenario postulates multiple such events.
A quantity more relevant to falsifying evolutionary theory is the probability
that no evolutionary pathway arrives at the system. (However, even this is

18
English and Greenwood
ad hoc.) Behe and others in the ID movement essentially take a divide-and-
conquer approach, dispensing with evolutionary pathways individually rather
than collectively to discredit evolutionary theory.
1.4.2 Dembski’s Deﬁnition
William Dembski, better known in ID circles for his notion of speciﬁed com-
plexity, claims to have generalized Behe’s notion of irreducible complexity, but
in fact has greatly restricted the class of irreducibly complex systems [15]. The
salient point of his modiﬁcation of Behe’s deﬁnition of irreducible complexity
is that “we need to establish that no simpler system achieves the same basic
function.” For instance, a three-legged stool is irreducibly complex for Behe,
but not for Dembski, because a block serves the same function as the stool.
The import of the “no simpler system” requirement is that evolution cannot
obtain an irreducibly complex biological system through successive improve-
ments of simpler precursors performing the “same basic function.” That is,
Dembski rules out direct evolution of irreducibly complex systems by deﬁni-
tion. If a putatively irreducibly complex system turns out to have emerged by
a direct evolutionary pathway, his ready response is that the system was not
irreducibly complex in the ﬁrst place.
Turning to indirect evolution of irreducibly complex systems, Dembski
falls back on argument from ignorance [15]:
Here the point at issue is no longer logical but empirical. The fact
is that for irreducibly complex biochemical systems, no indirect Dar-
winian pathways are known. [...] What’s needed is a seamless Dar-
winian account that’s both detailed and testable of how subsystems
undergoing coevolution could gradually transform into an irreducibly
complex system. No such accounts are available or have so far been
forthcoming.
Thus Dembski adopts Behe’s tactic of limiting the domain of investigation to
that of maximum biological ignorance, suggesting that evolutionary ﬁndings
do not generalize to biochemical systems. Given that the biochemical systems
of interest to ID advocates originated hundreds of millions of years ago and
left no fossil traces, Dembski does not risk much in demanding seamless and
detailed evolutionary accounts.
Why should our relative ignorance of the evolution of irreducibly complex
biochemical systems lead us to believe something other than that we are igno-
rant? “[W]ithout the bias of speculative Darwinism coloring our conclusions,
we are naturally inclined to see such irreducibly complex systems as the prod-
ucts of intelligent design.” Dembski claims, in other words, that evolutionists
have had their native perception of the truth educated out of them.

1 Intelligent Design and Evolutionary Computation
19
1.4.3 Evolution of Complexity in Avida
The artiﬁcial life program Avida has provided evidence that irreducible com-
plexity can evolve [25]. In the Avida environment, a digital organism is a
virtual computer with an assembly language program as its genotype. An or-
ganism must code to replicate itself in order to generate oﬀspring. The only
logical operation provided by the assembly language is NAND (“not and”).
Multiple instructions are required to compute other logical functions. In [25],
the ﬁtness of an organism is the sum of ﬁtness values for distinct logical func-
tions it computes in its lifetime. Nine logical functions are associated with
positive ﬁtness – the greater the inherent “complexity” of computing the func-
tion, the greater the contribution to ﬁtness. The logical function contributing
the most to ﬁtness is EQU (“equals”). In 50 runs with a population of 3600
organisms, 23 gave rise to EQU.
The focus of [25] is on the dominant genotype in the ﬁnal population of
a particular run giving rise to EQU. A step in the evolution of the genotype
comes when an ancestor has a genotype diﬀerent from that of its parent. The
ﬁnal dominant genotype, which computes all nine logical functions, has 83
instructions, and is 344 steps removed from its ﬁrst ancestor, which had 50
instructions.
The EQU function ﬁrst appeared at step 111 (update 27,450). There
were 103 single mutations, six double mutations, and two triple mu-
tations among these steps. Forty-ﬁve of the steps increased overall
ﬁtness, 48 were neutral and 18 were deleterious relative to the imme-
diate parent. [25]
The step giving rise to EQU was highly deleterious. Thus the “evolution of
a complex feature, such as EQU, is not always an inexorably upward climb
toward a ﬁtness peak, but instead may involve sideways and even backward
steps, some of which are important.”
The evolved code includes a component that is irreducibly complex in the
sense of Behe [3].
The genome of the ﬁrst EQU-performing organism had 60 instruc-
tions; eliminating any of 35 of them destroyed that function. Although
the mutation of only one instruction produced this innovation when it
originated, the EQU function evidently depends on many interacting
components. [25]
The code is not irreducibly complex in the sense of Dembski [15], because it has
been established that 19 Avida instructions suﬃce to compute EQU. However,
in another run there was a genotype that computed EQU unless any of 17
instructions were eliminated. The researchers determined by inspection that
there was redundant computation of some critical operations. Thus Dembski’s
stringent deﬁnition of irreducible complexity, which requires a near-minimalist
implementation, appears to have been satisﬁed by one of the 50 runs.

20
English and Greenwood
Lenski et al. [25] realize that critics of the study will complain that they
“‘stacked the deck’ by studying the evolution of a complex feature that could
be built on simpler functions that were also useful.” In fact, EQU did not
emerge when all simpler logical functions were assigned zero ﬁtness. They
contend that this “is precisely what evolutionary theory requires.” That is,
evolutionary theory holds that complex features emerge through successive
steps, not by saltation, and that intermediate forms persist in a population
only if they imbue the individuals that possess them with some advantage.
1.5 Speciﬁed Complexity
For centuries, design advocates, though not the present-day ID advocates,
have advanced the argument from improbability [40]. The approach is to show
some event in nature is very unlikely to have occurred by chance, and to
therefore conclude God caused it. But this argument is fallacious. For one
thing, low probability does not necessarily justify rejection of chance. When
numbers are drawn in a lottery, for instance, it is certain that the chance
outcome will be one that was highly improbable a priori [40]. Another problem
with the argument is that assigning an identity to the cause of the event is
unwarranted [14].
William Dembski has developed an analogous argument from speciﬁed
complexity, which concludes that a natural event reﬂects the intervention of
intelligence [11, 13, 18]. That is, a natural event contains information that
was introduced purposely by an unidentiﬁed source outside of nature. This
statement is not quite the same thing as saying intelligence caused the event,
because the event may have resulted from a combination of natural causes
and intelligent intervention. For instance, one may argue that natural evolu-
tionary mechanisms, while operating as claimed by mainstream scientists, do
not account fully for life on earth, and that intelligence has guided (added
information to) evolutionary processes. Note that information may have en-
tered continually, and that there may have never have been a discrete design
event.
To conclude that a natural event reﬂects intelligent design, one must
demonstrate that the event is improbable. Dembski describes improbable
events as complex. One must also demonstrate that the event is speciﬁed in
the sense that it exhibits a pattern that exists independently of itself [11,13].
To be more precise, there must exist some “semiotic agent” that describes the
event with a sequence of signs [18]. Speciﬁed complexity, or complex speciﬁed
information (CSI), is a quantity that “factors in” the improbability of the
event and the cost of describing it. Dembski claims that when the CSI of an
event exceeds a threshold value, inference that intelligence contributed to the
event is warranted.

1 Intelligent Design and Evolutionary Computation
21
1.5.1 Design Inference as Statistical Hypothesis Testing
In unpublished work [18], Dembski has reduced the argument from speciﬁed
complexity [11, 13] to statistical hypothesis testing. The approach is derived
from that of Fisher, with a null (or chance) hypothesis possibly rejected in
favor of an alternative hypothesis. The chance hypothesis says natural causes
account entirely for an event in nature, and the alternative hypothesis says
the event reﬂects design (contains information that could only have come from
without nature). The Fisherian approach requires speciﬁcation of the rejection
region prior to sampling. But the argument from speciﬁed complexity entails
selection of a past event and subsequent deﬁnition of a rejection region in
terms of a pattern found in the event. Dembski claims to have corrected for
“data dredging” and a posteriori ﬁtting of the rejection region to the event
by including replicational resources and speciﬁcational resources as penalty
factors in a test statistic [18].
Dembski’s argument goes something like this. Suppose H is the chance
hypothesis, and let E be an event in the sample space of H. For any pattern
describable by semiotic agent S, there is a corresponding event T containing
all matches of the pattern. Dembski uses T to denote both the event and the
pattern [18]. The probability (under the chance hypothesis) of matching the
pattern is
P(T | H).
T serves as a rejection region, and it is possible to make the probability low
enough to ensure rejection by choosing a very speciﬁc pattern that matches
few events, or perhaps no event but E. A penalty factor counters such a
“rigged” selection of the pattern.
The speciﬁcational resources used by S in identifying pattern T are
ϕS(T),
which gives the rank-complexity of the semiotic agent’s description of the
pattern. In essence, the agent enumerates its pattern descriptions from less
complex (e.g., shorter) to more complex (longer), looking for matches of E.
The rank-complexity is the least index of a description of pattern T in the
enumeration. It is a count of how many descriptions the agent processed to
obtain the description of matching pattern T. Dembski [18] considers
ϕS(T) · P(T | H)
“an upper bound on the probability (with respect to the chance hypothesis
H) for the chance occurrence of an event that matches any pattern whose
descriptive complexity is no more than T and whose probability is no more
than P(T | H)” for a ﬁxed agent S and a ﬁxed event E. The negative logarithm
of this quantity,

22
English and Greenwood
σ = −log2[ϕS(T) · P(T | H)] bits,
is speciﬁcity, a type of information [18]. As the probability of matching the
pattern goes down, speciﬁcity goes up. As the number of patterns “dredged”
by the semiotic agent goes up, speciﬁcity goes down. Maximizing speciﬁcity
– and ultimately inferring design in event E – is a matter of ﬁnding in the
event a simple pattern that is matched with low probability under the chance
hypothesis.
Not only is the pattern chosen to obtain high speciﬁcity, but the event
E and the semiotic agent S, and another penalty is required. The number
of replicational resources is bounded above by the product of the number
of semiotic agents available and the number of events that might have been
considered. In applications to biology, Dembski uses as an upper bound Seth
Lloyd’s estimate of the number of elementary logical operations in the history
of the universe, 10120 [27]. Dembski claims that if
10120 · ϕS(T) · P(T | H) < 0.5,
then the chance hypothesis is less likely to account for event E than the
alternative hypothesis of intelligent design [18]. The CSI of event E is the
penalized speciﬁcity,
χ = −log2[10120 · ϕS(T) · P(T | H)] ≈σ −399 bits.
Intelligent design is inferred for event H if χ > 1 or, equivalently, speciﬁcity
in excess of 400 bits [18]. Note that Dembski sometimes invokes the argument
from speciﬁed complexity to reject chance in favor of human intelligence, and
in these cases he sets the number of replicational resources smaller [18].7
1.5.2 Some Criticisms of Speciﬁed Complexity
It seems that Dembski, as a mathematician and philosopher, thinks more
analytically than algorithmically. Most of the following addresses aspects of
computation of CSI. It is important to keep in mind the adversarial aspect of
the argument from speciﬁed complexity. Chance hypotheses should come from
mainstream scientists, not ID advocates. They often will be analytically in-
tractable, and design inferences will require direct computation of CSI. Below
are listed some major criticisms of Dembski’s arguments.
A Model of Nature Is Conﬂated with Nature Itself
Recall that the chance hypothesis is essentially that natural causes account
entirely for a natural event. The design inference is a claim that natural causes
7 ID advocates hold that human intelligence is not natural (materialistic). Thus
humans can cause events with high levels of CSI.

1 Intelligent Design and Evolutionary Computation
23
alone do not suﬃce to explain the event. But in practice the chance hypothesis
that is likely to be derived from a scientiﬁc model, and what is subject to
rejection is not natural causation itself, but the model of natural causation.
The distinction is of vital importance. If scientists do not understand some
class of events, a chance hypothesis derived from their best model may be
rejected in favor of design. The inability of the model to account for the event
is treated as the inability of natural causation to account for the event. This
constitutes a logically fallacious argument from ignorance. And as described
above, ID advocates indeed focus on biological entities with histories that are
very diﬃcult to determine.
Key Aspects of CSI Are Not Explicit in Dembski’s Treatment
In conventional mathematical terms, a “pattern” described by a semiotic agent
is a property. A property T is a subset of some set U, and saying that x ∈U
has property T is equivalent to saying that x ∈T. Let DS denote the set of
all descriptions that semiotic agent S may emit. For all descriptions d in DS,
let ϕS(d) be the rank-complexity of d described above. Let DS(E) ⊆DS be
the set of all descriptions associated with event E by S. Finally, let
TS(d) = {ω ∈Ω | ω has the property S describes with d},
where Ω is the sample space. This glosses over semantic interpretation of the
descriptions in DS. Nonetheless, it should convey that there is no way to
determine the rejection region without knowing both its description and the
semantics of the semiotic agent that generated the description. Then for all
semiotic agents S and for all descriptions d in DS(E) the CSI is
χS(d) = −log2[10120 · ϕS(d) · P(TS(d) | H)].
This appropriately indicates that CSI is associated with descriptions of event
E. For completeness, one may deﬁne χ(E) as the maximum of χS(d) over all S
and d, but maximization is infeasible in practice, and design inference requires
only χS(d) > 1 for some S and d.
“Divide-and-Conquer” Rejection of Disjunctive Hypotheses Is
Permitted
When there are multiple chance hypotheses {Hi}, they must be rejected
jointly to infer intelligent design. Dembski fails to point out that the semiotic
agent S and the description d in DS(E) must be held constant while rejecting
all hypotheses [18]. This requirement is captured by generalizing the deﬁnition
of χS to
χS(d) = −log2[10120 · ϕS(d) · max
i
P(TS(d) | Hi)].

24
English and Greenwood
CSI Is Not Computable
For Dembski, the physical (material) universe is discrete and ﬁnite, and so
is Ω [11, 13, 18]. This would seem to bode well for computation of CSI, but
problems arise from the fact that a semiotic agent may associate with event
E a description of a property deﬁned on an inﬁnite set. Many ﬁnitely de-
scribable properties are not algorithmically decidable [26], irrespective of the
nonexistence of inﬁnite sets in the physical universe.
The value of P(TS(d) | H) is the sum of P(ω | H) over all points ω
in rejection region TS(d). Its computation generally requires conversion of
description d into an algorithm that decides which points in Ω have the de-
scribed property. But if the described property is not decidable, P(TS(d) | H)
is computable only under special circumstances. This holds even if the initial
“translation” of d into an algorithm is non-algorithmic.
Incomputable properties are especially likely to arise in the important case
that Ω is a set of entities that describe or compute partial (not always total)
recursive functions. An example is the set of all LISP programs of length
not exceeding some large bound. A semiotic agent’s description of program
E will commonly refer to a nontrivial property of the function computed by
E. But a key result in the theory of computation, Rice’s theorem, implies
that no algorithm decides whether other LISP programs compute functions
with that property [26]. In other words, there is generally no algorithm to
say whether programs in Ω belong to the rejection region. This indicates
that for a wide range of computational entities CSI may be computed only
for the form (e.g., the source code), and not the function. Note that some
philosophers and scientists believe that brains compute partial (not total)
recursive functions [19].
Some Design Hypotheses Call for Nonexistent Chance Hypotheses
In conventional statistical hypothesis testing, one begins with an alternative
hypothesis and then selects a chance hypothesis. This does not carry over to
the argument from speciﬁed complexity. An ID advocate may believe an event
is designed, but mainstream scientists may not have provided an appropriate
chance hypothesis to reject. The non-existence of the hypothesis (scientiﬁc
model) may be due to scientiﬁc indiﬀerence or scientiﬁc ignorance.
As an example of scientiﬁc indiﬀerence, consider what is required to com-
pute the CSI of the bacterial ﬂagellum, which Dembski qua semiotic agent
describes as a “bidirectional rotary motor-driven propeller” [18]. The sample
space contains biological structures detached from whole phenotypes, and the
chance hypothesis must associate probabilities of evolution with them. But
nothing in evolutionary theory leads to such a hypothesis, and it is absurd to
insist that scientists to supply one.
Ignorance is ubiquitous in science, and some phenomena (e.g., gravity)
have resisted explanation for centuries. The inability of science to explain

1 Intelligent Design and Evolutionary Computation
25
a class of events does not constitute the least evidence for ID. To suggest
otherwise is to engage in a logical fallacy known as argument from ignorance.
Computation of CSI May Be Infeasible When Theoretically
Possible
If Ω is the set of all biological structures (begging the question of how to
deﬁne “biological structure”) that have existed (begging the question of how
to determine all structures of entities that have ever lived) or might have
existed (begging the question of how to determine what might have lived),
how will an algorithm eﬃciently locate the points in the sample space with
the property “bidirectional rotary motor-driven propeller”? No approach other
than exhaustive exploration of the sample space for points with the property
is evident. The time required for such a computation makes it infeasible.
Furthermore, the practicality of deﬁning the sample space for an algorithm to
operate upon is highly dubious.
Another feasibility issue is the cost of computing P(ω | H) for a single
ω in Ω. Suppose P(ω | H) is, loosely speaking, the probability of evolution
of ω, and that H is derived from a simulation model supplied by a scientist.
The results of a simulation run usually depend upon initial conditions and
parameter settings. There will virtually always be uncertainty as to how to
set these values, and the consequence is that many runs of the simulation
model (with various settings) will be required to obtain P(ω | H).
Putative Innovations in Statistical Hypothesis Testing Have Not
Passed Peer Review
Dembski’s approach to design inference [18] is correct only if he has made
monumental contributions to statistical hypothesis testing. There is nothing
precluding publication of his statistical work in a peer-reviewed journal of
mathematics or statistics. At the time of this writing, Dembski has not pub-
lished any of his work. Consequently, one must regard his statistical reasoning
with skepticism.
1.5.3 The Law of Conservation of Information
In earlier work [13], Dembski argues informally for a law of conservation of
information, which does not specify that complex speciﬁed information is
strictly conserved in natural processes, but that gain of CSI is bounded above
by 500 bits. That is, a closed physical system may go from a state of lower CSI
to a state of higher CSI, but the increase cannot exceed 500 bits. The bound
corresponds to a putative limit on the improbability of events in the physi-
cal universe, as described below. Dembski regards evolutionary computations
(ECs) as closed systems, and if an EC produces an apparent gain of more than

26
English and Greenwood
500 bits of CSI in its population, he argues that humans have surreptitiously
(perhaps haplessly) added CSI to the process [13].
The 500-bit bound on CSI gain is the negative logarithm of the universal
probability bound Dembski advocates in earlier work, 10−150 [11,13]. He con-
siders events with probability below the bound to be eﬀectively impossible.
Dembski [11] cites ´Emile Borel, who is quoted in the epigraph of this chapter,
as a famous proponent of a universal probability bound. In fact Borel selects
diﬀerent bounds for diﬀerent applications – they are hardly “universal” [7].
Some are much smaller, and some much larger, than Dembski’s bound. In
the work detailed above, Dembski indicates that “instead of a static universal
probability bound of 10−150 we now have a dynamic one of 10−120/ϕS(d)” [18].
That is, the bound is adapted to the observer of an event and the observer’s
description of the event. This is in marked contrast with Borel’s approach.
Dembski does not indicate in [18] how to rescue the law of “conservation”
of information. He states, however, that ϕS(d) should not exceed 1030 in
practice, and observes that his old static bound of 10−150 is a lower bound
on the dynamic bound. This suggests that Dembski may renew his claim that
CSI gain cannot exceed 500 bits in a natural process. With the dependence
of CSI upon observers and their descriptions of events, what it means to gain
CSI is hardly obvious.
1.6 ID and Evolutionary Computation
Dembski has been at pains to argue, particularly in Chapter 4 of No Free
Lunch [13], that the results of evolutionary computation violate his law of
conservation of information, and that human investigators must be injecting
their own intelligence into the EC programs under investigation. In particular,
he has attacked Chellapilla and Fogel’s study of co-evolution of checkers play-
ers [8], Ray’s Tierra program for artiﬁcial life [38], Schneider’s demonstration
of gain of Shannon information in an evolutionary program [41], and Altshuler
and Linden’s evolutionary optimization of bent-wire antenna designs [2].
Dembski often cites the main “no free lunch” (NFL) theorem for optimiza-
tion, which says in essence that if all objective functions are equally likely, then
all optimizers that do not revisit points have identically distributed perfor-
mance [44]. He takes this as an indication that performance is generally bad.
Ironically, English [20] showed six years prior to the publication of Dembski’s
book that NFL arises as a consequence of (absolute) conservation of Shannon
information in optimization, and that average performance is very good when
test functions are uniformly distributed. In other words, NFL does not bode
as poorly for EC as Dembski has thought.
Dembski has since responded [17] by analyzing search of “needles-in-a-
haystack” functions, in which a few points in the domain are categorically
good and the remainder are categorically bad [20]. He motivates the analysis
by alluding to proteins as needles in the haystack of all sequences of amino

1 Intelligent Design and Evolutionary Computation
27
acids. For each function, it is highly improbable that an arbitrarily selected
search algorithm locates a good solution in feasible time. Dembski holds that
successful search requires a prior “search for a search” [17]. This amounts to
displacement of the search problem from the original solution space to the
space of search algorithms. He argues that a search problem cannot be solved
more rapidly with displacement than in the original space. Thus from his
perspective, if an EC ﬁnds a good solution in feasible time, the choice of the
EC was necessarily informed by intelligence [17].
There is nothing novel in the notion that it is sometimes necessary to
“align” the search algorithm with the problem [44], but there is in the idea
that alignment requires search [17]. How does one search for a search algo-
rithm? Dembski is very vague about this. All one can possibly do, in black-box
optimization, is to examine the value of at least one point in the search space
(domain) and use the information to select an algorithm. But then one has
initiated a search of the function. It follows that any search for a search may
be embedded in an algorithm for search of the original solution space.
“Displacement” is a construct that makes it appear that intelligence cre-
ates information by selecting an eﬀective search algorithm to locate a solution.
In reality, humans are able to tune an EC to a ﬁtness function only when the
ﬁtness function is not a black box. Only when one knows some property or
properties of the ﬁtness function can one select an EC that is expected to
outperform random sampling. And how does one recognize properties of a
function? Does one’s intelligence create information? No, it seems much more
reasonable to say that one has learned (acquired information) about functions
and algorithms in the past, and that one uses this repository of information
to match algorithms to functions. There is a great deal of empirical research
aimed at learning which forms of EC handle which classes of functions well.
1.7 Conclusion
We have criticized ID theory for its intrinsic faults. But in the end the only
way to understand the theory is as a veiled apologetic. Jews, Christians, and
Muslims agree that the God of Abraham created the diverse forms of life
on earth, imbuing only humans with a capacity to create ex nihilo. Although
some of the faithful accept that religion and science are diﬀerent belief systems
leading to diﬀerent beliefs, others insist that science must never contradict
religion. ID theorists begin with religious beliefs about life and humanity, and
attempt to show that contradictory beliefs held by almost all mainstream
scientists are wrong. They hide their religious motivation because they hope
their theory will ﬁnd its way into science classes of public schools.
Irreducible complexity is the weakest part of the apologetics. Behe has
had to concede what Muller pointed out decades before he was born, namely
that indirect evolutionary pathways may give rise to irreducible complexity.

28
English and Greenwood
And there is good fossil evidence that the interconnected bones of the mam-
malian middle ear evolved from a reptilian jawbone. The Avida simulation is
reasonably interpreted as generating irreducibly complex programs. ID advo-
cates continue, however, to focus on irreducibly complex biosystems for which
there are few historical data (e.g., the ﬂagellum). They argue that evolution-
ary theory fails to account for the emergence of these systems when in fact
there are few hard data.
The argument from speciﬁed complexity rests on an approach to statisti-
cal hypothesis testing that has not passed peer review. Even if the statistical
foundation is sound, the argument is logically ﬂawed. When it claims to reject
purely natural causation in favor of design, it actually rejects a model. That
is, if there is no good model of a phenomenon, then the argument from spec-
iﬁed complexity reduces to argument from ignorance. Even with an excellent
model, speciﬁed complexity is in some cases impractical to compute, or even
incomputable.
Dembski’s claim that all entities with high speciﬁed complexity are intel-
ligently designed seems to have been falsiﬁed by various evolutionary com-
putations. But Dembski argues constantly that experimenters have smuggled
intelligence into the computations. Accumulating further computational evi-
dence should be valuable, but in the end formal mathematical analysis may
be required to settle the dispute.
References
1. Agassiz, L.: Evolution and permanence of type. Atlantic Monthly (1874)
2. Altshuler, E., Linden, D.: Wire-antenna designs using genetic algorithms. IEEE
Antennas and Propagation Magazine 39, 33–43 (1997)
3. Behe, M.: Darwin’s Black Box: The Biochemical Challenge to Evolution. Free
Press, New York (1996)
4. Behe, M.: Molecular machines: experimental support for the design inference.
Cosmic Pursuit 1(2), 27–35 (1998)
5. Behe, M.: A response to critics of Darwin’s Black Box. Progress in Complexity,
Information, and Design 1 (2002)
6. Blocker, A., Komoriya, K., Aizawa, S.: Type III secretion systems and bacterial
ﬂagella: insights into their function from structural similarities. Proceedings of
the National Academy of Science USA 100, 3027–3030 (2003)
7. Borel, E.: Probability and Life. Dover, New York (1962)
8. Chellapilla, K., Fogel, D.: Evolving an expert checkers playing program without
using human expertise. IEEE Transactions on Evolutionary Computation 5,
422–428 (2001)
9. Darwin, C.: On the Origin of Species by Means of Natural Selection – the
Preservation of Favoured Races in the Struggle for Life. John Murray, London
(1859)
10. Dawkins, R.: The Blind Watchmaker. W. W. Norton and Co., London (1986)
11. Dembski, W.: The Design Inference: Eliminating Chance Through Small Prob-
abilities. Cambridge University Press, Cambridge (1998)

1 Intelligent Design and Evolutionary Computation
29
12. Dembski, W.: The intelligent design movement. In: J. Miller (ed.) An Evolving
Dialogue: Theological and Scientiﬁc Perspectives on Evolution, pp. 439–443.
Trinity Press International, Harrisburg, PA (2001)
13. Dembski, W.: No Free Lunch: Why Speciﬁed Complexity Cannot be Purchased
Without Intelligence. Rowman and Littleﬁeld, Lanham, MD (2002)
14. Dembski, W.: The Design Revolution. InterVarsity, Downers Grove, IL (2004)
15. Dembski, W.: Irreducible complexity revisited. Tech. rep. (2004). http://www.
designinference.com/documents/2004.01.Irred_Compl_Revisited.pdf
16. Dembski, W.: Rebuttal to reports by opposing expert witnesses (2005).
http://www.designinference.com/documents/2005.09.Expert_Rebuttal_
Dembski.pdf
17. Dembski, W.: Searching large spaces.
Tech. rep. (2005).
http://www.
designinference.com/documents/2005.03.Searching_Large_Spaces.pdf
18. Dembski, W.: Speciﬁcation: the pattern that signiﬁes intelligence.
Tech.
rep.
(2005).
http://www.designinference.com/documents/2005.06.
Specification.pdf
19. Dietrich, E.: The ubiquity of computation. Think 2, 12–78 (1993)
20. English, T.: Evaluation of evolutionary and genetic optimizers: no free lunch. In:
L. Fogel, P. Angeline, T. B¨ack (eds.) Evolutionary Programming V: Proceedings
of the Fifth Annual Conference on Evolutionary Programming, pp. 163–169.
MIT Press, Cambridge, Mass. (1996)
21. Hartwig, M.: What is intelligent design? (2003). http://www.arn.org/idfaq/
Whatisintelligentdesign.htm
22. Jiang, Y., Doolittle, R.: The evolution of vertebrate blood coagulation as viewed
from a comparison of puﬀer ﬁsh and sea squirt genomes. Proceedings of the
National Academy of Science USA 100(13), 7527–32 (2003)
23. Johnson, P.: Evolution as dogma: the establishment of naturalism. In: W. Demb-
ski (ed.) Uncommon Descent: Intellectuals Who Find Darwinism Unconvincing.
ISI Books, Wilmington, Delaware (2004)
24. Jones III, J.: Tammy Kitzmiller et al. v. Dover Area School District et al.
Memorandum opinion in case no. 04cv2688, United States District Court for
the Middle District of Pennsylvania (2005). http://www.pamd.uscourts.gov/
kitzmiller/kitzmiller_342.pdf
25. Lenski, R., Ofria, C., Pennock, R., Adami, C.: The evolutionary origin of com-
plex features. Nature 423, 129–144 (2003)
26. Lewis, H., Papadimitriou, C.: Elements of the Theory of Computation, 2nd edn.
Prentice-Hall, Upper Saddle River, NJ (1998)
27. Lloyd, S.: Computational capacity of the universe. Physical Review Letters 88,
7901–7904 (2002)
28. Meyer, S.: The origin of biological information and the higher taxonomic cate-
gories. Proceedings of the Biological Society of Washington 117, 213–239 (2004)
29. Miller, K.: A review of Darwin’s Black Box.
Creation/Evolution pp. 36–40
(1996)
30. Miller, K.: The evolution of vertebrate blood clotting (no date). http://www.
millerandlevine.com/km/evol/DI/clot/Clotting.html
31. Minnich, S., Meyer, S.: Genetic analysis of coordinate ﬂagellar and type III
regulatory circuits in pathogenic bacteria. In: Second International Conference
on Design & Nature (2004)
32. Muller, H.: Genetic variability, twin hybrids and constant hybrids, in a case of
balanced lethal factors. Genetics 3(5), 422–499 (1918)

30
English and Greenwood
33. Muller, H.: Reversibility in evolution considered from the standpoint of genetics.
Biological Reviews of the Cambridge Philosophical Society 14, 261–280 (1939)
34. Orr, H.: Darwin v. Intelligent Design (again). Boston Review (1996)
35. Paley, W.: Natural Theology – Evidences of the Existence and Attributes of the
Deity: Collected from the Appearances of Nature (1802)
36. Pallen, M., Matzke, N.: From The Origin of Species to the origin of bacterial
ﬂagella. Nature Reviews Microbiology 4, 784–790 (2006)
37. Ray, J.: The Wisdom of God Manifested in the Works of the Creation (1691)
38. Ray, T.: An approach to the synthesis of life.
In: C. Langton, C. Taylor,
J. Farmer, S. Rasmussen (eds.) Artiﬁcial Life II, pp. 371–408. Addison-Wesley,
Reading, MA (1992)
39. Rich, T., Hopson, J., Musser, A., Flannery, T., Vickers-Rich, P.: Independent
origins of middle ear bones in monotremes and therians. Science 307, 910–914
(2005)
40. Rosenhouse, J.: How anti-evolutionists abuse mathematics. The Mathematical
Intelligencer 23, 3–8 (2001)
41. Schneider, T.: Evolution of biological information. Nucleic Acids Research 28,
2794–2799 (2000)
42. Semba, U., Shibuya, Y., Okabe, H., Yamamoto, T.: Whale Hageman factor
(factor XII): prevented production due to pseudogene conversion. Thrombosis
Research 90(1), 31–37 (1998)
43. Ussher, J.: The Annals of the Old Testament, From the Beginning of the World
(1654)
44. Wolpert, D., Macready, W.: No free lunch theorems for optimization.
IEEE
Transactions on Evolutionary Computation 1, 67–82 (1997)
45. Xu, X., Doolittle, R.: Presence of a vertebrate ﬁbrinogen-like sequence in an
echinoderm. Proceedings of the National Academy of Science USA 87, 2097–
2101 (1990)

2
Inference of Genetic Networks Using an
Evolutionary Algorithm
Shuhei Kimura
Tottori University, 4-101, Koyama-Minami, Tottori, Japan
kimura@ike.tottori-u.ac.jp
2.1 Introduction
Genes control cellular behavior. Most genes play biological roles when they
are translated into proteins via mRNA transcription. The process by which
genes are converted into proteins is called gene expression, and the analysis
of gene expression is one means by which to understand biological systems.
A DNA microarray is a collection of microscopic DNA spots attached to a
solid surface forming an array. As each spot can measure an expression level
of a diﬀerent gene, this technology allows us to monitor expression patterns of
multiple genes simultaneously. With recent advances in these technologies, it
has become possible to measure gene expression patterns on a genomic scale.
To exploit these technologies, however, we must ﬁnd ways to extract useful
information from massive amounts of data. One of the promising ways to ex-
tract useful information from voluminous data is to infer genetic networks.
The inference of genetic networks is a problem in which mutual interactions
among genes are deduced using gene expression patterns. The inferred model
of the genetic network is conceived as an ideal tool to help biologists generate
hypotheses and facilitate the design of their experiments. Many researchers
have taken an interest in the inference of genetic networks, and the devel-
opment of this methodology has become a major topic in the bioinformatics
ﬁeld.
Many models to describe genetic networks have been proposed. The
Boolean network model is one of the more abstract models among them [1,
10, 14, 22]. This model classiﬁes each gene into one of two states, ON (ex-
press) or OFF (not-express), then applies gene regulation rules as Boolean
functions. The simplicity of inference methods based on the Boolean network
model makes it possible to analyze genetic networks of thousands of genes.
However, as gene expression levels are not binary but continuous in general, it
is diﬃcult for this approach to predict the eﬀect of a change in the expression
level of one gene upon the other genes [6].

32
Kimura
The observed data on gene expression are normally polluted by noise aris-
ing from the measurement technologies, the experimental procedures, and the
underlying stochastic biological processes. The Bayesian network model has
often been used to handle these noisy data [9,12,26,27,40]. As the Bayesian
network model treats the expression level of each gene as a random variable
and represents interactions among genes as conditional probability distribu-
tion functions, this model can easily handle noisy data. Another advantage
of this model is an ability to infer genetic networks consisting of hundreds of
genes [9,27]. These features have prompted the successful application of the
Bayesian approach to the analysis of actual DNA microarray data [9,12,26,27].
Several groups have attempted to exploit the quantitative feature of gene
expression data by models based on sets of diﬀerential equations. The fol-
lowing equations describe a genetic network in a genetic network inference
problem based on a set of diﬀerential equations.
dXi
dt = Gi(X1, X2, · · · , XN),
(i = 1, 2, · · · , N),
(2.1)
where Xi is the expression level of the i-th gene, N is the number of genes in
the network, and Gi is a function of an arbitrary form. The purpose of the
genetic network inference problem based on the set of diﬀerential equations is
to identify the functions Gi from the observed gene expression data.
In order to model a genetic network, Sakamoto and Iba [30] have proposed
to use a set of diﬀerential equations of an arbitrary form. They tried to iden-
tify the functions Gi using genetic programming. The scope of their method
has been limited, however, as the diﬃculty of identifying arbitrary functions
restricts application to small-scale genetic network inference problems of less
than ﬁve genes. It generally works better to approximate the functions Gi
than to attempt to identify them. In many of the earlier studies, Gi has been
approximated using models based on sets of diﬀerential equations of a ﬁxed
form [2,3,7,34,39]. When we use a model of ﬁxed form to describe a genetic
network, the genetic network inference problem becomes a model parameter
estimation problem.
The S-system model [36], a set of diﬀerential equations of ﬁxed form, is one
of several well-studied models for describing biochemical networks. A number
of researchers have proposed genetic network inference methods based on the
S-system model [4,11,15–18,24,25,33–35,37]. Some of these inference methods,
however, must estimate a large number of model parameters simultaneously
if they are to be used to infer large-scale genetic networks [4, 11, 15, 33–35].
Because of the high-dimensionality of the genetic network inference problem
based on the S-system model, these inference methods have been applied only
to small-scale networks of a few genes. To resolve this high-dimensionality, a
problem decomposition strategy, that divides the original problem into sev-
eral subproblems, has been proposed [25]. This strategy enables us to infer
S-system models of larger-scale genetic networks, and the author and his col-
leagues have proposed two inference methods based on this strategy, i.e., the

2 Inference of Genetic Networks Using an Evolutionary Algorithm
33
problem decomposition approach [16,17] and the cooperative coevolutionary
approach [18]. As the strategy deﬁnes the inference of the S-system model
of the genetic network as non-linear function optimization problems, their
methods use evolutionary algorithms as the function optimizer. The rest of
this chapter presents these two inference methods. Then, through the appli-
cation of these methods to genetic network inference problems consisting of
dozens of genes, some observations on and limitations of these methods are
discussed.
2.2 S-system Model
The S-system model [36] is a set of non-linear diﬀerential equations of the
form
dXi
dt = αi
N

j=1
Xgi,j
j
−βi
N

j=1
Xhi,j
j
,
(i = 1, 2, · · · , N),
(2.2)
where αi and βi are multiplicative parameters called rate constants, and gi,j
and hi,j are exponential parameters called kinetic orders. Xi is the state vari-
able and N is the number of components in the network. In genetic network
inference problems, Xi is the expression level of the i-th gene and N is the
number of genes in the network. The ﬁrst and second terms on the right-
hand side of (2.2) represent processes that contribute to the increase and the
decrease, respectively, in Xi.
The term S of the S-system refers to the synergism and saturation of
the investigated system. Synergism is the phenomenon in which two or more
objects produce an eﬀect greater than the total of their individual eﬀects.
Saturation is the condition in which, after a suﬃcient increase in a causal
force, no further increase in the resultant eﬀect is possible. Synergism and
saturation, fundamental properties of biochemical systems, are both inherent
in the S-system model. Moreover, steady-state evaluation, control analysis and
sensitivity analysis of this model have been established mathematically [36].
These properties have led to a number of applications of the S-system model
to the analysis of biochemical networks (e.g. [31,38]).
The genetic network inference methods described in this chapter use this
model to approximate the functions Gi given in Sect. 2.1. Thus, the inference
of genetic networks is recast as the estimation problem for the model param-
eters αi, βi, gi,j, and hi,j. As the numbers of the rate constants (αi and βi)
and kinetic orders (gi,j and hi,j) are 2N and 2N 2, respectively, the number
of S-system parameters to be estimated is 2N(N + 1).

34
Kimura
2.3 Genetic Network Inference Problem
2.3.1 Canonical Problem Deﬁnition
The purpose of the genetic network inference problem based on the S-system
model is to estimate the model parameters from the time-series of observed
DNA microarray data. This problem is generally formulated as a function
optimization problem to minimize the following sum of the squared relative
error:
f =
N

i=1
T

t=1
Xi,cal,t −Xi,exp,t
Xi,exp,t
2
,
(2.3)
where Xi,exp,t is an experimentally observed gene expression level at time
t of the i-th gene, Xi,cal,t is a numerically computed gene expression level
acquired by solving a system of diﬀerential equations (2.2), N is the number
of components in the network, and T is the number of sampling points of
observed data.
The number of S-system parameters to be determined in order to solve
the set of diﬀerential equations (2.2) is 2N(N + 1), hence this function opti-
mization problem is 2N(N +1) dimensional. Several inference methods based
on this problem deﬁnition have been proposed [11, 15, 34]. If we try to infer
the S-system models of large-scale genetic networks made up of many network
components, however, this problem has too many dimensions for non-linear
function optimizers [25].
2.3.2 Problem Decomposition
Given the high-dimensionality of the canonical problem deﬁnition, it can be
diﬃcult to use function optimizers for inferring S-system models of large-scale
genetic networks. Maki and his colleagues [25] proposed a strategy for resolv-
ing the high-dimensionality by dividing the genetic network inference problem
into several subproblems. In their strategy, each subproblem corresponds to
a gene. The objective function of the subproblem corresponding to the i-th
gene is
fi =
T

t=1
Xi,cal,t −Xi,exp,t
Xi,exp,t
2
,
(2.4)
where Xi,cal,t is a numerically computed gene expression level at time t of the
i-th gene, as described in Sect. 2.3.1. In contrast to Sect. 2.3.1, however, we
obtain Xi,cal,t by solving the following diﬀerential equation:
dXi
dt = αi
N

j=1
Y gi,j
j
−βi
N

j=1
Y hi,j
j
,
(2.5)
where

2 Inference of Genetic Networks Using an Evolutionary Algorithm
35
Yj =
 Xj, if j = i,
ˆ
Xj, otherwise.
(2.6)
ˆ
Xj is an estimated time-course of the j-th gene’s expression level acquired not
by solving a diﬀerential equation, but by direct estimation from the observed
time-series data. We can obtain ˆ
Xj’s using a spline interpolation [29], a local
linear regression [5], or other interpolation techniques.
Equation (2.5) is solvable when 2(N +1) S-system parameters (i.e., αi, βi,
gi,1, · · · , gi,N, hi,1, · · · , hi,N) are given. Thus, the problem decomposition
strategy divides a 2N(N + 1) dimensional network inference problem into N
subproblems that are 2(N + 1) dimensional.
2.3.3 Use of a Priori Knowledge
The genetic network inference problem based on the S-system model may have
multiple optima, as the model has a high degree-of-freedom and the observed
time-series data are usually polluted by noise. To increase the probability of
inferring a reasonable S-system model, we introduced a priori knowledge of
the genetic network into the objective function (2.4) [16].
Genetic networks are known to be sparsely connected [32]. When an in-
teraction between two genes is clearly absent, the S-system parameter values
corresponding to the interaction (i.e., kinetic orders; gi,j and hi,j) are zero.
We incorporated the knowledge into the objective function (2.4) by applying
a penalty term, as shown below:
Fi =
T

t=1
Xi,cal,t −Xi,exp,t
Xi,exp,t
2
+ c
N−I

j=1
(|Gi,j| + |Hi,j|) ,
(2.7)
where Gi,j and Hi,j are given by rearranging gi,j and hi,j, respectively, in
descending order of their absolute values (i.e., |Gi,1| ≤|Gi,2| ≤· · · ≤|Gi,N|
and |Hi,1| ≤|Hi,2| ≤· · · ≤|Hi,N|). The variable c is a penalty coeﬃcient and
I is a maximum indegree. The maximum indegree determines the maximum
number of genes that directly aﬀect the i-th gene.
The penalty term is the second term on the right-hand side of (2.7). This
term forces most of the kinetic orders down to zero. When the penalty term
is applied, most of the genes are thus disconnected from each other. The term
does not penalize, however, when the number of genes that directly aﬀect
the i-th gene is lower than the maximum indegree I. Thus, the optimum
solutions to the objective functions (2.4) and (2.7) are identical when the
number of interactions that aﬀect the focused (i-th) gene is lower than the
maximum indegree. Our inference methods use (2.7) as an objective function
to be minimized.

36
Kimura
2.4 Problem Decomposition Approach
2.4.1 Estimation of S-system Parameters
In order to estimate the S-system parameters, we must minimize the objec-
tive function (2.7). While any type of function optimizer can be used for
this purpose, we used GLSDC (a Genetic Local Search with distance inde-
pendent Diversity Control) [19], a method based on the real-coded genetic
algorithm [16–18]. GLSDC has two features important for the inference of
large-scale genetic networks; it works well on multimodal function optimiza-
tion problems with high-dimensionality and it can be suitably applied to par-
allel computers. The following is a pseudocode description of GLSDC.
1. Initialization
As an initial population, create np individuals randomly. Set Generation
= 0 and the iteration number of the converging operations Niter = N0.
2. Local Search Phase
Apply a local search method to all individuals in the population. As a
sophisticated local search algorithm, the modiﬁed Powell’s method [29]
was used.
3. Adaptation of Niter
If the best individual in the population shows no improvement over the
previous generation, set Niter ←Niter + N0. Otherwise, set Niter = N0.
4. Converging Phase
Execute the exchange of individuals according to the genetic operators
Niter times.
5. Termination
Stop if the halting criteria are satisﬁed. Otherwise, Generation ←Gener-
ation + 1 and go to step 2.
Readers can ﬁnd more detailed information on GLSDC in the paper [19].
Recently, however, several groups have proposed useful techniques to solve
the genetic network inference problem based on the S-system model. Voit and
Almeida [37] reformulate the estimation of the S-system parameters as a set
of algebraic equations. Similarly, Tsai and Wang [35] converted a set of dif-
ferential equations (2.2) into a set of algebraic equations. The computational
costs of these inference methods are lower since they do not require solving
any diﬀerential equation. Imade and his colleagues [11], on the other hand,
have developed an inference system running on a grid computing environment.
Grid computing is the pooling of computational resources into a single set of
shared services. As the grid computing environment provides us with powerful
computational resources, this method has an ability to infer larger-scale ge-
netic networks. The use of these new techniques should improve the inference
ability of the method presented in this chapter.

2 Inference of Genetic Networks Using an Evolutionary Algorithm
37
2.4.2 Estimation of Initial Gene Expression Level
To solve the decomposed subproblem, we ﬁrst need to solve the diﬀerential
equation (2.5). This can only be accomplished by inputting the initial ex-
pression level of the gene (the initial state value for the diﬀerential equation)
in addition to the S-system parameters. We can obtain the initial gene ex-
pression level from the observed time-series data, but only when the data are
unpolluted by noise. Given that this condition generally cannot be met, the
initial gene expression level must be estimated together with the S-system
parameters.
Again, we need to estimate the initial gene expression level when applying
an inference algorithm to a realistic genetic network inference problem. How-
ever, the simultaneous estimation of the initial gene expression level and the
S-system parameters makes the function optimization problem higher dimen-
sional, and this is inconvenient for function optimizers. To skirt this problem,
we proposed the method of estimating the initial gene expression level and
the set of S-system parameters alternately [17]. When we use this method
to estimate the initial expression level of the i-th gene, we ﬁx the S-system
parameters to the values of some candidate solution for the i-th subproblem.
Because the initial expression level of the i-th gene is a unique variable and
the rest of the model parameters are ﬁxed, the estimation of the initial ex-
pression level of the i-th gene is formulated as a one-dimensional function
minimization problem. The objective function of this estimation problem is
F initial
i
=
T

t=1
γt−1
Xi,cal,t −Xi,exp,t
Xi,exp,t
2
,
(2.8)
where Xi,cal,t is acquired by solving (2.5), and γ (0 ≤γ ≤1) is a discount
parameter. As the ﬁxed S-system parameters are not always optimal, the cal-
culated time-course of gene expression may diﬀer greatly from the actual time-
course. When the calculated time-course is incorrect, the algorithm should not
ﬁt the time-course, especially the latter half of it, into the observed data. The
discount parameter γ was introduced for this reason.
A golden section search [29] was used to solve this one-dimensional function
minimization problem. (Golden section search is a technique for ﬁnding an
optimum of a single-dimensional objective function by successively narrowing
the interval inside which the optimum seems to exist.) When multiple sets of
time-series data are given as the observed data, the one-dimensional search
was applied to all of the sets.
2.4.3 Algorithm
As mentioned above, we can only solve a decomposed subproblem by esti-
mating both the S-system parameters and the initial gene expression level.

38
Kimura
Fig. 2.1. Framework of the problem decomposition approach
The algorithm presented here handles these calculations alternately, estimat-
ing the initial gene expression level at the end of the every cycle (generation)
of GLSDC (see Fig. 2.1) [17]. When the algorithm estimates the initial gene
expression level, the S-system parameters are ﬁxed to the values of the best
candidate solution. In contrast, the initial gene expression level is ﬁxed to the
value obtained by optimizing the objective function (2.8) when the algorithm
estimates the S-system parameters. This approach fails to estimate the initial
expression level at the ﬁrst generation, however, as the calculation only be-
comes possible after the estimation of the S-system parameters. We overcome
this obstacle in the algorithm by using the value obtained from the observed
time-series data as the initial gene expression level at the ﬁrst generation.
This inference method is referred to as the problem decomposition approach.
One run of this approach solves one subproblem corresponding to one
gene. As N subproblems must be solved to infer a genetic network consisting
of N genes, we need to perform N runs. A parallel computer can assist us in
this task, however, as it can be used to execute multiple runs of the problem
decomposition approach in parallel.
2.5 Cooperative Coevolutionary Approach
2.5.1 Concept
The problem decomposition approach described above can be used to infer
large-scale genetic networks. When the time-series data are noisy, however,

2 Inference of Genetic Networks Using an Evolutionary Algorithm
39
Estimated time-courses of gene expression levels
Subpopulation 1
Subpopulation 2
Subpopulation N
Subproblem 1
Subproblem 2
Subproblem N
Calculated time-course
of expression level of gene 1
Calculated time-course
of expression level of gene 2
Calculated time-course
of expression level of gene N
Input
Output
Update
Fig. 2.2. The cooperative coevolutionary model for the inference of genetic networks
the model inferred by this approach fails in the computational simulation of
genetic networks.
Although the computational simulation is performed by solving the set of
diﬀerential equations (2.2), the problem decomposition approach estimates the
model parameters without solving it. While the canonical (non-decomposed)
approach uses the set of diﬀerential equations (2.2) in order to estimate the
parameters, the problem decomposition approach uses the diﬀerential equa-
tions (2.5) for this purpose. As explained earlier, we cannot solve (2.5) without
ﬁrst obtaining the estimated time-courses of the gene expression levels, ˆ
Xj’s.
The problem decomposition approach estimates the time-courses,
ˆ
Xj’s, di-
rectly from the observed time-series data using some interpolation method.
When ˆ
Xj’s are estimated correctly, optimum solutions obtained by the prob-
lem decomposition approach and the canonical approach will completely co-
incide with each other. The reason for this coincidence is that the optimum
parameter values should make the solutions of the set of diﬀerential equa-
tions (2.2) and the diﬀerential equations (2.5) identical. If, on the other hand,
the given time-series data are noisy, ˆ
Xj’s will often be diﬃcult to estimate
correctly. When incorrect ˆ
Xj’s are applied, the optimum solutions of the de-
composed subproblems do not always coincide with the optimum solution
of the non-decomposed problem. This means that the parameters obtained
by solving the subproblems do not always provide a model [i.e., the set of
diﬀerential equations (2.2)] that ﬁts into the observed data. As such, the in-
ferred model in the problem decomposition approach is not yet suitable for
the computational simulation of genetic networks.
In the subproblem corresponding to the i-th gene, the time-course of the
i-th gene’s expression level is calculated by solving the diﬀerential equa-
tion (2.5). When optimizing the i-th subproblem, the function optimizer
searches for the S-system parameters which lead to the best ﬁt of the cal-
culated expression time-course of the i-th gene into the observed data. Thus,
the calculated time-courses obtained by solving the subproblems are the most
suitable for ˆ
Xj’s. If we can always use the calculated time-courses of the gene

40
Kimura
Initialization
Termination
Yes
No
Estimation of S-system parameters
Estimation of initial gene expression level
Update of estimated gene expression time-courses
Fig. 2.3. Framework of the cooperative coevolutionary approach
expression levels as ˆ
Xj’s, optimizing the subproblems should provide a model
that ﬁts into the observed data.
A cooperative coevolutionary model (see, e.g. [23, 28]) can use the time-
courses of the gene expression levels obtained by solving the subproblems as
ˆ
Xj’s. The cooperative coevolutionary algorithm consists of several subpopula-
tions, each of which contains competing individuals in each subproblem. The
subpopulations are genetically isolated, i.e., the individuals within a subpopu-
lation only mate with each other. The only interactions to take place between
the subpopulations occur when the ﬁtness values are calculated. When the
coevolutionary model is applied to the genetic network inference method, the
subpopulations only interact with each other through the gene expression
time-courses. When the i-th subproblem is solved, the calculated expression
time-courses of the other genes, that are obtained from the best individuals
of the other subproblems at the previous generation, are used as ˆ
Xj’s (see
Fig. 2.2).
2.5.2 Algorithm
Our group has proposed a cooperative coevolutionary algorithm for infer-
ring genetic networks, what we call the cooperative coevolutionary approach,
based on the foregoing concept [18]. The cooperative coevolutionary approach
simultaneously solves all of the decomposed subproblems, which weakly inter-
act with each other. The following is a description of this algorithm (see also
Fig. 2.3).
1. Initialization
N subpopulations, each corresponding to one subproblem, are generated.
Each subpopulation consists of np randomly created individuals. The ini-
tial estimations of the time-courses of the gene expression levels, ˆXj’s,

2 Inference of Genetic Networks Using an Evolutionary Algorithm
41
meanwhile, are directly calculated from the observed time-series data. Set
Generation = 0.
2. Estimation of S-system parameters
One generation of GLSDC [19] is performed to optimize the objective func-
tion (2.7) for each subpopulation. When the algorithm calculates the ﬁt-
ness value of each individual in each subpopulation, the diﬀerential equa-
tion (2.5) is solved using the estimated time-courses of the gene expression
levels, ˆXj’s. At this point, we require an initial level of gene expression (an
initial state value for the diﬀerential equation) together with the S-system
parameters. The initial expression level of the i-th gene is obtained from
its estimated gene expression time-course, hence the value of ˆXi(0) is used
for Xi,cal,0.
3. Estimation of initial gene expression level
The objective function (2.8) is minimized on each subpopulation. The
golden section search is used as the function optimizer. When the initial
expression level of the i-th gene is estimated in this step, the S-system
parameters are ﬁxed to the best candidate solution of the i-th subproblem.
4. Update of estimated gene expression time-courses
In this step, we calculate the time-courses of the gene expression levels
obtained from the best individuals of the subpopulations, each of which is
given as a solution of the diﬀerential equation (2.5). When the algorithm
solves the diﬀerential equation, the new initial gene expression level esti-
mated in the previous step is used as the initial state value. The old gene
expression time-courses are then updated to the calculated time-courses.
The updated gene expression time-courses are used as ˆXj’s in the next
generation.
5. Termination
Stop if the halting criteria are satisﬁed. Otherwise, Generation ←Gener-
ation +1 and return to the step 2.
The cooperative coevolutionary approach is suitable for parallel implemen-
tation. Accordingly, we ran the calculations on a PC cluster.
2.6 Experiment on an Artiﬁcial Genetic Network
Next, we applied the problem decomposition approach and the cooperative
coevolutionary approach to an artiﬁcial genetic network inference problem of
30 genes. We used the S-system model to describe the target network for the
experiment. Figure 2.4 shows the network structure and model parameters
of the target [24]. The purpose of this artiﬁcial problem is to estimate the
S-system parameters solely from the time-series data obtained by solving the
set of diﬀerential equations (2.2) on the target model.
Fifteen sets of noise-free time-series data, each covering all 30 genes, were
given as the observed data in this case. Eleven sampling points for the time-

42
Kimura
1
5
9
14
24
27
30
18
g5,1=1.0
g6,1=1.0
h1,1=1.0
6
2
3
7
15
10
11
16
20
21
19
25
26
28
29
12
22
4
8
g8,4=0.2
13
17
23
h8,8=1.0
g1,14=-0.1
g14,9=1.0
g24,19=0.3
h24,24=1.0
g27,24=0.6
h27,27=1.0
g27,30=-0.2 g30,27=0.6
h30,30=1.0
h28,28=1.0
g27,25=0.3
h25,25=1.0
g25,20=0.4
g24,15=-0.2
g20,15=0.7
g20,26=0.3
h20,20=1.0
g28,25=0.5 g26,28=0.1
g29,26=0.4
h29,29=1.0
h26,26=1.0
g21,16=0.6
g16,12=-0.2
h16,16=1.0
g15,10=0.2
h11,11=1.0
g11,7=-0.2
g13,8=0.6
h13,13=1.0
g17,13=0.5
h17,17=1.0
g23,17=0.2
h23,23=1.0
g12,23=0.1
h12,12=1.0
g16,11=0.5
g22,16=0.5 h22,22=1.0
g11,22=0.4
g8,17=-0.2
h19,19=1.0
h15,15=1.0
g19,14=0.1
h18,18=1.0
g24,18=-0.1
g9,5=1.0
g9,6=-0.1
h9,9=1.0
h14,14=1.0
h10,10=1.0
h6,6=1.0
g7,2=0.5
h 5,5=1.0
g
g7,3=0.4
g10,7=0.3
h7,7=1.0
h2,2=1.0
h3,3=1.0
g11,4=0.4
h21,21=1.0
h4,4=1.0
: positive regulation
: negative regulation
α i =1.0  ( i = 1, 2,       , 30 )
.....
β i =1.0  ( i = 1, 2,       , 30 )
.....
g26,21=-0.2
other gi,j’s and hi,j’s are 0
Fig. 2.4. The target network model. The S-system parameters and the network
structure are shown
series data were assigned on each gene in each set. While these sets of time-
series data could be obtained by actual biological experiments under diﬀerent
experimental conditions in a practical application, this experiment obtained
them by solving the set of diﬀerential equations (2.2) on the target model. In
this experiment, we estimated 2 × 30 × (30 + 1) = 1860 S-system parameters
and 30 × 15 = 450 levels of initial gene expression. The penalty coeﬃcient c
was 1.0, the maximum indegree I was 5 and the discount coeﬃcient γ was 0.75.
The following parameters were used in GLSDC applied here; the population
size np is 3n, where n is the dimension of the search space of each subproblem;
and the number of children generated by the crossover per selection nc is 10.
Five runs were carried out by changing the seed for the pseudo random number
generator. Each run was continued until the number of generations reached
75.

2 Inference of Genetic Networks Using an Evolutionary Algorithm
43
1
5
9
14
24
27
30
18
h1,1=1.017
6
2
3
7
15
10
11
16
20
21
19
25
26
28
29
12
22
4
8
13
17
23
h3,3=1.059
g5,1=1.022
h 5,5 =1.030
g
g6,1=1.017
h6,6=1.014
g7,2=0.516
g7,3=0.408
h7,7=1.066
g8,4=0.208
g8,17=-0.213
h8,8=1.048
g9,5 =1.077
g
=-0.102
1,14
g9,6=-0.120
g9,9=-0.066
h9,9=1.062
g10,7=0.315
g11,4=0.403
g11,7=-0.218
g11,22=0.427
h11,11=1.065
g12,23=0.113
h 12,12=1.062
g13,8=0.625
h13,13=1.078
g14,9=1.017
h14,14=1.019
g10,10=-0.066
h10,10=1.110
g15,10=0.202
h15,15=1.060
g16,11=0.516
g16,12=-0.201
h16,16=1.047
g17,13=0.510
h17,17=1.048
g18,18=-0.083
h18,18=1.074
g19,14=0.107
h19,19=1.043
g20,15=0.717
g20,26=0.307
h20,20=1.040
g21,16=0.645
g21,21=-0.061
h21,21=1.091
g22,16=0.607
h22,16=-0.071
h23,23=0.995
g23,17=0.198
g24,15=-0.202
g24,18=-0.100
g24,19=0.300
h24,24=1.010
g25,20=0.428
g25,25=-0.055
h25,25=1.076
g26,26=-0.060
h26,26=1.099
g27,24=0.649
g27,25=0.327
g27,27=-0.066
g27,30=-0.215
h27,27=1.070
g28,25=0.514
h28,28=1.040
g29,26=0.509
g26,28=0.119
g29,29=-0.298
h29,29=1.254
h29,26=-0.107
g30,27=0.625
h30,30=1.047
h9,5 =-0.051
g2,2=-0.077
h2,2=1.115
α 1=0.975 α 2=0.836 α 3=0.912 α 4=0.936 α 5=0.945 α 6=0.972 α 7=0.909 α 8=0.920 α 9=0.879 α 10=0.855
α 11=0.900 α 12=0.914 α 13=0.885 α 14=0.966 α 15=0.911 α 16=0.939 α 17=0.914 α 18=0.866 α 19=0.930 α 20=0.941
α 21=0.870 α 22=0.731 α 23=1.006 α 24=0.977 α 25=0.882 α 26=0.861 α 27=0.877 α 28=0.942 α 29=0.639 α 30=0.926
β 1=0.975 β 2=0.835 β 3=0.911 β 4=0.934 β 5=0.943 β 6=0.972 β 7=0.909 β 8=0.920 β 9=0.878 β 10=0.855
β 11=0.899 β 12=0.913 β 13=0.885 β 14=0.966 β 15=0.909 β 16=0.939 β 17=0.913 β 18=0.865 β 19=0.929 β 20=0.940
β 21=0.869 β 22=0.753 β 23=1.005 β 24=0.977 β 25=0.882 β 26=0.860 β 27=0.878 β 28=0.942 β 29=0.639 β 30=0.925
absolute values of other gi,j’s and hi,j’s are less than 0.05
h4,4=1.036
g22,22=-0.168
h22,22=1.188
g26,21=-0.214
Fig. 2.5. A sample of the model inferred by the problem decomposition approach
When suﬃcient amounts of noise-free data were given as observed data,
the models obtained by the two inference approaches presented in this chapter
were almost identical. Figure 2.5 shows a sample model inferred by the prob-
lem decomposition approach. When the j-th gene positively regulates the i-th
gene in the S-system model, gi,j is positive and/or hi,j is negative. Similarly,
the negative value of gi,j and/or the positive value of hi,j reﬂect the negative
regulation from the j-th gene to the i-th gene. The values of gi,j and hi,j, on
the contrary, are zero when the j-th gene does not regulate the i-th gene. In
this experiment, we assume that, when the absolute values of gi,j and hi,j are
less than 0.05, the i-th gene is unaﬀected by the j-th gene. Thus, although
the inference methods were unable to estimate the parameter values with
perfect precision, they succeeded in inferring the interactions between genes

44
Kimura
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
1.4
0
0.5
1
1.5
2
gene expression level
time
observed data
(a) cooperative coevolutionary approach
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
1.4
0
0.5
1
1.5
2
gene expression level
time
solid line: time course obtained 
by solving the equations (2)
dotted line: time course obtained
by solving the equation (5)
(b) problem decomposition approach
Fig. 2.6. Samples of calculated time-courses obtained from (a) the cooperative co-
evolutionary approach, and (b) the problem decomposition approach, in the exper-
iment with noisy data. Solid line: solution for the set of diﬀerential equations (2.2),
where the estimated values are used as the model parameters. Dotted line: time
course obtained at the end of the search, i.e., the solution of the diﬀerential equa-
tion (2.5). +: noisy time-series data given as the observed data
correctly (see Fig. 2.5). When we focus only on the structure of the inferred
network, the averaged numbers of false-positive and false-negative regulations
of the problem decomposition approach were 4.8 ± 5.1 and 0.4 ± 0.8, respec-
tively, and those of the cooperative coevolutionary approach were 0.6 ± 1.2
and 0.0 ± 0.0, respectively. When used to solve the genetic network inference
problem consisting of 30 genes, the problem decomposition approach required
about 49.1×30 hours on a single-CPU personal computer (Pentium III 1GHz)
and the cooperative coevolutionary approach required about 46.5 hours on a
PC cluster (Pentium III 933MHz × 32 CPUs).
As mentioned above, when the given data were unpolluted by noise, the
models obtained by the two inference methods were almost identical. In real-
life application, however, DNA microarray data are usually noisy. When noisy
data are given, the models inferred by the two methods slightly diﬀer. To con-

2 Inference of Genetic Networks Using an Evolutionary Algorithm
45
ﬁrm the diﬀerence, we tested both inference approaches using noisy time-series
data generated by adding 10% Gaussian noise to the time-series data com-
puted by solving the diﬀerential equations on the target model. Figure 2.6
gives samples of the calculated gene expression time-courses obtained from
the two approaches in the experiment with noisy data. Speciﬁcally, the ﬁgure
shows the calculated time-courses obtained by solving the set of diﬀerential
equations (2.2) and the diﬀerential equation (2.5), respectively. When using
the coevolutionary approach, the time-course obtained by solving the set of
diﬀerential equations (2.2) was almost identical to that obtained by solving
the diﬀerential equation (2.5) (see Fig. 2.6a). When using the problem de-
composition approach, the time-courses calculated by these equations greatly
diﬀered (see Fig. 2.6b).
Both approaches use diﬀerential equation (2.5) to calculate time-courses
of gene expression levels when inferring S-system models of genetic networks.
In (2.5), however, the perturbation in the i-th gene does not aﬀect the expres-
sion levels of the other genes. While (2.5) remains useful for inferring genetic
networks, it is of no help to biologists who need a model to generate hy-
potheses or facilitate the design of experiments. When attempting to analyze
an inferred genetic network, we thus must use the set of diﬀerential equa-
tions (2.2) as our model for computational simulation. We therefore conclude
that the problem decomposition approach will not always produce a suitable
model for computational simulation, as the model sometimes ﬁts poorly into
the observed data. Given that the time-courses obtained from the diﬀeren-
tial equation set (2.2) are almost identical to those obtained from (2.5), the
cooperative coevolutionary approach provides us with a suitable model.
As just mentioned, the models inferred by the cooperative coevolutionary
approach are suitable for the computational simulation. On the other hand,
when the observed data are noisy, both inference methods erroneously infer
many interactions absent from the target network (false-positive interactions)
and fail to infer some interactions present in the target model (false-negative
interactions). Table 2.1 and Table 2.2 show the averaged numbers of the false-
positive and false-negative regulations of the two methods in the experiments
using diﬀerent numbers of sets of noisy time-series data. The tables show that,
although the addition of more data decreased the numbers of the erroneous
regulations, the inferred models still contained many false-positive regulations.
The number of the false-positive regulations is, however, diﬃcult to reduce be-
cause of the maximum indegree I introduced into the objective function (2.7).
In order to make the obtained models more reasonable, we must ﬁnd ways to
reduce the number of the erroneous regulations. The use of further a priori
knowledge about the genetic network should be one of promising means for
this purpose.

46
Kimura
Table 2.1. The numbers of the false-positive and false-negative regulations of the
models inferred by the problem decomposition approach in the experiments with
diﬀerent sets of noisy time-series data given
The number of
The number of
The number of
time-series sets
false-positive regulations
false-negative regulations
10
216.8 ± 1.5
17.0 ± 2.3
15
205.2 ± 3.3
9.4 ± 1.2
20
194.8 ± 2.3
4.2 ± 2.2
25
191.8 ± 3.6
2.6 ± 1.5
Table 2.2. The numbers of the false-positive and false-negative regulations of the
models inferred by the cooperative coevolutionary approach
The number of
The number of
The number of
time-series sets
false-positive regulations
false-negative regulations
10
218.4 ± 2.8
18.8 ± 1.3
15
205.8 ± 2.4
8.4 ± 1.4
20
201.2 ± 3.4
4.2 ± 1.5
25
195.6 ± 5.6
3.2 ± 1.0
2.7 Inference of an Actual Genetic Network
The inference methods described in this chapter can be used to infer large-
scale genetic networks consisting of dozens of genes. The analysis of actual
DNA microarray data, on the other hand, requires the handling of many
hundreds or even thousands of genes. This task lies far beyond the powers of
current inference methods based on the S-system model. One way to improve
these inference capabilities may be to use a clustering technique to identify
genes with similar expression patterns and group them together [6, 8]. By
treating groups of similar genes as single network components, the inference
approaches can analyze systems made up of many hundreds of genes. Based
on this idea, the author and his colleagues proposed a method to combine
the cooperative coevolutionary approach with the clustering technique [18].
In this section, we present the results of our attempt to apply the combined
method for the analysis of cDNA microarray data on Thermus thermophilus
HB8 strains.
Two sets of cDNA microarray time-series data, i.e., wild type and UvrA
gene disruptant, were observed. Each data set was measured at 14 time points.
Our clustering technique [13] grouped the 612 putative open reading frames
(ORFs) included in the data into 24 clusters. An ORF is a sequence locating
between the start-code sequence and the stop-code sequence of a gene. As we
treated the disrupted gene, UvrA, as single network component, the target
system consisted of 24 + 1 = 25 network components. The time-series data

2 Inference of Genetic Networks Using an Evolutionary Algorithm
47
cluster
1
cluster
23
cluster
16
cluster
19
cluster
6
cluster
15
cluster
24
cluster
18
cluster
17
cluster
8
cluster
20
cluster
2
cluster
21
cluster
5
cluster
3
cluster
4
UvrA
cluster
11
cluster
9
cluster
13
cluster
14
cluster
7
cluster
12
cluster
22
cluster
10
Fig. 2.7. The core network structure of the models inferred in the experiment using
the actual DNA microarray data
of each cluster were given by averaging the expression patterns of the ORFs
included in the cluster.
Figure 2.7 shows the core network structure when the interactions are in-
ferred by the combined method more than nine times within 10 runs. The
inferred network model seems to include many erroneous interactions due to
a lack of suﬃcient data. The method also infers a number of reasonable in-
teractions, however. Many of the ORFs in clusters 6, 7, 10, 15, 16, 19 and 22
are annotated to be concerned with “energy metabolism”, and most of these
clusters are located relatively near to one another in the inferred model. We
also see from the ﬁgure that clusters 12 and 23 are located nearby the clus-
ters of “energy metabolism”. Only a few of the ORFs in the clusters 12 and
23, however, are annotated to be concerned with “energy metabolism”. This
suggests that some of the hypothetical and unknown ORFs included in the
clusters 12 and 23 may work for “energy metabolism” or related functions. In
this experiment, as the amount of the measured time-series data was insuﬃ-
cient, it is hard to extract many suggestions from the inferred network. We
should obtain more meaningful results in either of two ways: by using more
sets of time-series data obtained from additional biological experiments, or by
using further a priori knowledge about the genetic network.
This section presented a method to combine the cooperative coevolution-
ary approach with a clustering technique in order to analyze a system con-
sisting of hundreds of genes. The combined use of the inference method and
clustering technique is a less than perfect solution for analyzing actual genetic
networks, however, as few genes are likely to have the same expression pattern
at any one time. A target of future research will therefore be to develop an
inference method capable of handling hundreds of components.

48
Kimura
2.8 Conclusion
In this chapter, we formulated the inference of the S-system model of the ge-
netic network as several function optimization problems. Then, on the basis
of this problem deﬁnition, two inference methods, i.e., the problem decom-
position approach and the cooperative coevolution approach, were presented.
Experimental results showed that, while the inference abilities of these meth-
ods are almost identical, the models inferred by the cooperative coevolutionary
approach are suitable for computational simulation. These methods can infer
S-system models of genetic networks made up of dozens of genes. However,
the analysis of actual DNA microarray data generally requires the handling of
hundreds of genes. In order to resolve this problem, this chapter also presented
a method to combine the cooperative coevolution approach with a clustering
technique. As the S-system model has an ability to exploit the quantitative
feature of the gene expression data, it should be a promising model for the
inference of the genetic network. However, as mentioned before, the methods
described in this chapter are still insuﬃcient for analyzing actual genetic net-
works since they have several drawbacks we must settle. A target of future
research will be to develop an inference method based on the S-system model,
that can handle hundreds of network components and can infer few erroneous
regulations.
Sets of diﬀerential equations can capture the dynamics of target systems,
and, with this capability, they show promise as models to describe genetic
networks. The purpose of the genetic network inference problem based on the
set of diﬀerential equations is to obtain a good approximation of the functions
Gi given in (2.1), as described above. Models which accurately represent the
dynamics of target systems may provide us with good approximations of Gi.
In most cases, however, the parameters of these complicated models are diﬃ-
cult to estimate. To easily obtain a better approximation than the S-system
model, the author and his colleagues proposed an approach to deﬁne the ge-
netic network inference problem as a function approximation task [20]. Based
on this new problem deﬁnition, they also proposed the inference methods de-
rived from the neural network model [20] and the NGnet (Normalized Gaus-
sian network) model [21], respectively. However, none of the proposed models
yet seem suﬃcient. Another target for future research will be to develop an
appropriate model for describing genetic networks and its inference method.
Acknowledgments
The problem decomposition approach and the cooperative coevolutionary ap-
proach were published in references [17] and [18], respectively. The method
to combine the cooperative coevolutionary approach with the clustering tech-
nique and the analysis of Thermus thermophilus HB8 strains were also pre-
sented in reference [18].

2 Inference of Genetic Networks Using an Evolutionary Algorithm
49
The author thanks Dr. Mariko Hatakeyama of RIKEN Genomic Sciences
Center for her useful suggestions and comments, and the reviewers for review-
ing the draft of this chapter.
References
1. Akutsu, T., Miyano, S., Kuhara, S.: Inferring qualitative relations in genetic
networks and metabolic pathways. Bioinformatics 16, 727–734 (2000)
2. Bourque, G., Sankoﬀ, D.: Improving gene network inference by comparing ex-
pression time-series across species, developmental stages or tissues. Journal of
Bioinformatics and Computational Biology 2, 765–783 (2004)
3. Chen, T., He, H., Church, G.: Modeling gene expression with diﬀerential equa-
tions. In: Proceedings of the Paciﬁc Symposium on Biocomputing 4, pp. 29–40
(1999)
4. Cho, D., Cho, K., Zhang, B.: Identiﬁcation of biochemical networks by S-tree
based genetic programming. Bioinformatics 22, 1631–1640 (2006)
5. Cleveland, W.: Robust locally weight regression and smoothing scatterplots.
Journal of American Statistical Association 79, 829–836 (1979)
6. D’haeseleer, P., Liang, S., Somogyi, R.: Genetic network inference: from co-
expression clustering to reverse engineering. Bioinformatics 16, 707–726 (2000)
7. D’haeseleer, P., Wen, X., Fuhrman, S., Somogyi, R.: Linear modeling of mRNA
expression levels during CNS development and injury. In: Proceedings of the
Paciﬁc Symposium on Biocomputing 4, pp. 42–52 (1999)
8. Eisen, M., Spellman, P., Brown, P., Botstein, D.: Cluster analysis and display
of genome-wide expression patterns. Proceedings of the National Academy of
Sciences of the United States of America 95, 14,863–14,868 (1998)
9. Friedman, N., Linial, M., Nachman, I., Pe’er, D.: Using Bayesian networks to
analyze expression data. Journal of Computational Biology 7, 601–620 (2000)
10. Ideker, T., Thorsson, V., Karp, R.: Discovery of regulatory interactions through
perturbation: inference and experimental design. In: Proceedings of the Paciﬁc
Symposium on Biocomputing 5, pp. 302–313 (2000)
11. Imade, H., Mizuguchi, N., Ono, I., Ono, N., Okamoto, M.: Gridifying: An evolu-
tionary algorithm for inference of genetic networks using the improved GOGA
framework and its performance evaluation on OBI grid. Lecture Notes in Bioin-
formatics 3370, 171–186 (2005)
12. Imoto, S., Goto, T., Miyano, S.: Estimation of genetic networks and functional
structures between genes by using Bayesian network and nonparametric regres-
sion. In: Proceedings of the Paciﬁc Symposium on Biocomputing 7, pp. 175–186
(2002)
13. Kano, M., Nishimura, K., Tsutsumi, S., Aburatani, H., Hirota, K., Hirose, M.:
Cluster overlap distribution map: visualization for gene expression analysis using
immersive projection technology. Presence: Teleoperators and Virtual Environ-
ments 12, 96–109 (2003)
14. Kauﬀman, S.: Metabolic stability and epigenesis in randomly constructed ge-
netic nets. Journal of Theoretical Biology 22, 437–467 (1969)
15. Kikuchi, S., Tominaga, D., Arita, M., Takahashi, K., Tomita, M.: Dynamic mod-
eling of genetic networks using genetic algorithm and S-system. Bioinformatics
19, 643–650 (2003)

50
Kimura
16. Kimura, S., Hatakeyama, M., Konagaya, A.: Inference of S-system models of ge-
netic networks using a genetic local search. In: Proceedings of the 2003 Congress
on Evolutionary Computation, pp. 631–638 (2003)
17. Kimura, S., Hatakeyama, M., Konagaya, A.: Inference of S-system models of
genetic networks from noisy time-series data. Chem-Bio Informatics Journal 4,
1–14 (2004)
18. Kimura, S., Ide, K., Kashihara, A., Kano, M., Hatakeyama, M., Masui, R., Nak-
agawa, N., Yokoyama, S., Kuramitsu, S., Konagaya, A.: Inference of S-system
models of genetic networks using a cooperative coevolutionary algorithm. Bioin-
formatics 21, 1154–1163 (2005)
19. Kimura, S., Konagaya, A.: High dimensional function optimization using a new
genetic local search suitable for parallel computers. In: Proceedings of the 2003
Conference on Systems, Man & Cybernetics, pp. 335–342 (2003)
20. Kimura, S., Sonoda, K., Yamane, S., Matsumura, K., Hatakeyama, M.: Inference
of genetic networks using neural network models. In: Proceedings of the 2005
Congress on Evolutionary Computation, pp. 1738–1745 (2005)
21. Kimura, S., Sonoda, K., Yamane, S., Matsumura, K., Hatakeyama, M.: Func-
tion approximation approach to the inference of normalized Gaussian network
models of genetic networks.
In: Proceedings of the 2006 International Joint
Conference on Neural Networks, pp. 4525–4532 (2006)
22. Liang, R., Fuhrman, S., Somogyi, R.: Reveal, a general reverse engineering al-
gorithm for inference of genetic network architectures. In: Proceedings of the
Paciﬁc Symposium on Biocomputing 3, pp. 18–29 (1998)
23. Liu, Y., Yao, X., Zhao, Q., Higuchi, T.: Scaling up fast evolutionary program-
ming with cooperative coevolution. In: Proceedings of the 2001 Congress on
Evolutionary Computation, pp. 1101–1108 (2001)
24. Maki, Y., Tominaga, D., Okamoto, M., Watanabe, S., Eguchi, Y.: Development
of a system for the inference of large scale genetic networks. In: Proceedings of
the Paciﬁc Symposium on Biocomputing 6, pp. 446–458 (2001)
25. Maki, Y., Ueda, T., Okamoto, M., Uematsu, N., Inamura, Y., Eguchi, Y.: Infer-
ence of genetic network using the expression proﬁle time course data of mouse
P19 cells. Genome Informatics 13, 382–383 (2002)
26. Ong, I., Glasner, J., Page, D.: Modelling regulatory pathways in Escherichia coli
from time series expression proﬁles. Bioinformatics 18, S241–S248 (2002)
27. Pe’er, D., Regev, A., Elidan, G., Friedman, N.: Inferring subnetworks from per-
turbed expression proﬁles. Bioinformatics 17(S215–S224) (2001)
28. Potter, M., De Jong, K.: Cooperative coevolution: an architecture for evolving
coadapted subcomponents. Evolutionary Computation 8, 1–29 (2000)
29. Press, W., Teukolsky, S., Vetterling, W., Flannery, B.: Numerical Recipes in C,
2nd edn. Cambridge University Press, Cambridge (1995)
30. Sakamoto, E., Iba, H.: Inferring a system of diﬀerential equations for a gene
regulatory network by using genetic programming. In: Proceedings of the 2001
Congress on Evolutionary Computation, pp. 720–726 (2001)
31. Shiraishi, F., Savageau, M.: The tricarboxylic acid cycle in Dictyostelium dis-
coideum. Journal of Biological Chemistry 267, 22,912–22,918 (1992)
32. Thieﬀry, D., Huerta, A., P´erez-Rueda, E., Collado-Vides, J.: From speciﬁc gene
regulation to genomic networks: a global analysis of transcriptional regulation
in Escherichia coli. BioEssays 20, 433–440 (1998)

2 Inference of Genetic Networks Using an Evolutionary Algorithm
51
33. Tominaga, D., Horton, P.: Inference of scale-free networks from gene expression
time series. Journal of Bioinformatics and Computational Biology 4, 503–514
(2006)
34. Tominaga, D., Koga, N., Okamoto, M.: Eﬃcient numerical optimization algo-
rithm based on genetic algorithm for inverse problem. In: Proceedings of the
Genetic and Evolutionary Computation Conference, pp. 251–258 (2000)
35. Tsai, K., Wang, F.: Evolutionary optimization with data collocation for reverse
engineering of biological networks. Bioinformatics 21, 1180–1188 (2005)
36. Voit, E.: Computational Analysis of Biochemical Systems. Cambridge Univer-
sity Press, Cambridge (2000)
37. Voit, E., Almeida, J.: Decoupling dynamical systems for pathway identiﬁcation
from metabolic proﬁles. Bioinformatics 20, 1670–1681 (2004)
38. Voit, E., Radivoyevitch, T.: Biochemical systems analysis of genome-wide ex-
pression data. Bioinformatics 16, 1023–1037 (2000)
39. Yeung, M., Tegn´er, J., Collins, J.: Reverse engineering gene networks using
singular value decomposition and robust regression. Proceedings of the National
Academy of Sciences of the United States of America 99, 6163–6168 (2002)
40. Yu, J., Smith, V., Wang, P., Hartemink, A., Jarvis, E.: Advances to Bayesian
network inference for generating causal networks from observational biological
data. Bioinformatics 20, 3594–3603 (2004)

3
Synthetic Biology: Life, Jim, but Not As We
Know It
Jennifer Hallinan
Newcastle University, Newcastle upon Tyne, UK J.S.Hallinan@ncl.ac.uk
3.1 Introduction
Frankenstein, Mary Shelley’s classic tale of horror, warns of the perils of
hubris; of the terrible fate that awaits when Man plays God and attempts
to create life. Molecular biologists are clearly not listening. Not content with
merely inserting the occasional gene into the genome of an existing organ-
ism, they are developing a whole new ﬁeld, Synthetic Biology, which aims to
engineer from ﬁrst principles organisms with desirable, controllable qualities.
Synthetic biology is not a new idea. Ever since the nature of the genetic
code was elucidated by Watson and Crick in 1953, the imagination of biolo-
gists has leapt ahead of their technology. By the late 1970s, that technology
had advanced to the point where Szybalski and Skalka [57] could suggest “The
work on restriction nucleases not only permits us easily to construct recom-
binant DNA molecules and to analyse individual genes, but also has led us
into the new era of ‘synthetic biology’ where not only existing genes are de-
scribed and analysed but also new gene arrangements can be constructed and
evaluated.” (cited in [14]).
The diﬀerence between the existing, well established ﬁeld of genetic en-
gineering and that of synthetic biology is largely one of attitude. Counter-
intuitively, molecular biologists who engage in genetic engineering are think-
ing as biologists, taking an existing complex system and tweaking it slightly
by adding a gene or modifying a couple of regulatory processes, but essen-
tially respecting the integrity of the existing system. Synthetic biologists, in
comparison, think as engineers, with the aim of developing a toolbox of bi-
ological devices and modules which can be combined in a multitude of ways
to produce a system with predictable properties. Synthetic biology takes the
classical engineering strategies of standardization of components, decoupling
of processes and abstraction of theory and implementation, and attempts to
apply them to biological systems [1].
Synthetic biology is a ﬁeld which has attracted considerable interest within
the molecular biology community for the last decade or so, and there are a

54
Hallinan
number of recent reviews addressing diﬀerent aspects of the ﬁeld, which to-
gether provide a broader perspective than is covered here [1,6,14,20,35,42,54].
In this review we address speciﬁcally the potential of computational intelli-
gence approaches, in particular evolutionary computation (EC), to inform the
ﬁeld of synthetic biology, which is currently dominated by molecular biologists
and engineers. Excellent overviews of EC are given in [19, 26, 45], and with
speciﬁc reference to its use in bioinformatics in [18].
3.2 The Basics
All living organisms are constructed from a genome made up of DNA, which
is a deceptively simple string of four molecules: adenine (A), guanine (G),
thymine (T) and cytosine (C). Nucleotides are read by the cellular machinery
in groups of three, known as codons. Each codon codes for a speciﬁc amino
acid, so a length of DNA codes for a string of amino acids (via an intermediary
called RNA, a similar, but chemically slightly diﬀerent, molecule to DNA). A
string of amino acids folds up to form a three-dimensional protein.
The “Central Dogma” of molecular biology, proposed in the late 1950s by
one of the discoverers of the structure of DNA, Francis Crick, was that genetic
information ﬂows from DNA through RNA to protein, and that proteins do the
work of building cells and bodies. This straightforward, mechanistic view rev-
olutionized biology; from being a primarily observational discipline (natural
history) it became an empirical science. Genetic engineering – the mechanis-
tic manipulation of the genetic material – became feasible, and over the past
half century it has been an extraordinarily successful and productive ﬁeld of
endeavour. Although genetically modiﬁed organisms (GMOs) have acquired
an aura of menace in the public eye due to the large agricultural companies’
engineering of crops and the subsequent media backlash (the “Frankenfoods”
debate), genetic engineering has been hugely beneﬁcial to society in somewhat
less well-publicised applications, such as the cheap commercial production of
large amounts of insulin for the treatment of diabetes [12].
Genetic technologies are improving all the time, and there is widespread
belief that the goal of synthetic biology – the engineering of artiﬁcial organ-
isms with desirable characteristics – is within reach. The basic technological
requirements for engineering organisms were identiﬁed by Heinemann and
Panke [32] as: standardized cloning; de novo DNA synthesis; and providing
what they refer to as an “engineering chassis” for biology. The ﬁrst two require-
ments are already standard in every biology laboratory around the world, and
have well and truly demonstrated their utility, whilst the latter is the subject
of considerable research.
Synthetic biology does not necessarily aim to construct organisms from
scratch; much of the work currently dubbed “synthetic biology” is merely ge-
netic engineering rebadged. However, the ultimate aim, in the eyes of many,

3 Synthetic Biology: Life, Jim, but Not As We Know It
55
would be to design and build an organism from ﬁrst principles. And a min-
imal organism has, in fact, been constructed more-or-less from scratch: the
poliovirus [9]. Poliovirus has a small RNA genome; it is only 7.5 kilobases
in length and codes for ﬁve diﬀerent macromolecules. It has been completely
sequenced, and the sequence (the “recipe” for polio) is freely available from
the large online databases which are the lifeblood of bioinformatics. Because
RNA is chemically less stable than DNA, the researchers who re-engineered
polio downloaded the RNA sequence information, translated it into its DNA
equivalent, and then used a commercial DNA synthesis service to produce the
required DNA. A naturally occurring enzyme known as reverse transcriptase
was then used to convert the DNA back into RNA, which was added to a
mixture of mouse cellular components. The RNA copied itself and produced
new, infectious, disease-causing viruses.
Although the poliovirus synthesis is a major advance – the ﬁrst time an
organism has been built from scratch using only information about its genome
sequence – there are a couple of caveats. Firstly, polio is a very small, simple
organism. Active only when it is within a host cell, it has all the parasite’s
advantages of being able to outsource a lot of its cellular machinery to the
host cell. Free-living organisms are generally much larger, with more complex
genomes, and are consequently harder to construct. Further, it is now appar-
ent that the Central Dogma is woefully incomplete; cellular metabolism relies
not just upon the synthesis of proteins, but upon a bewildering variety of
non-protein-coding RNAs [41]. There are also a whole suite of modiﬁcations
to DNA, RNA and proteins, collectively known as epigenetic modiﬁcations,
which are equally important to gene regulation and expression [34]. Moreover,
the poliovirus was not constructed completely without the assistance of an ex-
isting biological system; the medium in which it was grown, although cell-free,
consisted of components of mammalian host cells, without which replication
would have been impossible. And, of course, the polio was not engineered to
do anything except be a polio virus.
Some idea of the magnitude of the task of creating a viable organism
from scratch has been provided by investigations into the minimal genome
needed by an independently living cell. A number of workers have addressed
this issue, and estimates for the minimal number of genes needed to sustain
life tend to be around the 200 range (see, e.g. [46]), although Forster and
Church [20] theorise that only 151 genes are essential. The organism1 with
the smallest known genome is Mycoplasma genitalium. Its genome contains
482 protein coding genes (along with 43 genes which produce RNA but not
protein). Every genome appears to carry some redundancy, however, Glass
et al. [25], using mutation to disrupt M. genitalium genes in a systematic
1 I refer here only to free-living organisms. Parasites may have smaller genomes,
because they can hijack some of the metabolic functions of their hosts. Viruses
are, of course, the extreme example of this, to the point where some people query
whether they’re alive at all.

56
Hallinan
manner, estimate that only 382 of the protein-coding genes are essential to
life. Interestingly, it appears that at least some of the RNA-coding genes are
absolutely essential [56].
Synthetic biologists rely upon the assumption that a complex system such
as an organism can be deconstructed into a manageable number of compo-
nents, the behaviour of which can be understood, and which can be put to-
gether in a variety of ways to produce organisms with diﬀerent behaviours,
in the same way in which electronic components can be combined in diﬀer-
ent ways to make a microwave oven or a computer. Endy [14] provides the
following list of minimal requirements for the successful practice of synthetic
biology:
1. the existence of a limited set of predeﬁned, reﬁned materials that can be
delivered on demand and that behave as expected;
2. a set of generally useful rules (that is, simple models) that describe how
materials can be used in combination (or alone);
3. a community of skilled individuals with a working knowledge of and means
to apply these rules.
Although this list provides a comforting illusion of engineering control over
biological processes, Endy does, however, concede that so far this has not
proven to be a straightforward process, commenting, somewhat drily, that “it
is possible that the designs of natural biological systems are not optimized by
evolution for the purposes of human understanding and engineering” [14]. The
complex system of interactions within the cell must somehow be simpliﬁed to
the point where biologists can understand it, comprehend the relationship
between the network topology and its dynamics, and use this understanding
to predict ways in which the networks can be engineered to produce reliably
repeatable behaviours. The most popular approach to this task is currently
that of network motifs.
3.3 Networks and Motifs
Biological systems consist of networks of interactions. At a cellular level, these
interactions are between genes, gene products (protein and RNA), metabolites
and biomolecules. Gene expression, and hence cellular phenotype, is controlled
by a complex network of intracellular interactions. Gene networks are not only
complex, they tend to be very large. Even the 482-gene genome of Mycoplasma
genitalium has the potential to produce a network of over 200,000 interactions,
even if we assume that each gene only produces one product (an assumption
which is certainly false in most eukaryotes). Understanding gene networks is
essential to understanding the workings of cells, but their size and complexity
make this a challenging undertaking.
Networks are usually modelled computationally as graphs. A graph is a
collection of nodes and links. In gene networks nodes usually represent genes

3 Synthetic Biology: Life, Jim, but Not As We Know It
57
(or proteins, or some similar biological agent). Links can be undirected, when
the relationship between the agents is symmetrical, as with protein-protein
interaction networks; in this case they are known as edges. Directed links,
known as arcs, occur in networks such as genetic regulatory networks, where
the fact that gene A regulates gene B does not necessarily mean that gene
B regulates gene A. Graph theory is a ﬁeld of mathematics with a long his-
tory (see, for example, [16]) and graph analysis has been applied to networks
in ﬁelds as diverse as sociology, economics, computer science, physics and,
of course, biology (see, e.g. [27]). There is a large body of literature on the
computational modelling of genetic networks. For reviews of this broader lit-
erature, see [7,10,15,23,50,52].
Endy’s point 1 – the existence of a limited set of predeﬁned, reﬁned mate-
rials that can be delivered on demand – can be translated, in network terms,
into the existence of a set of genetic components whose behaviour is under-
stood and which can be combined at will. One approach to the identiﬁcation
of these components is the study of network motifs; small subsets of nodes
with a given pattern of connectivity. The idea that networks might be decom-
posable into more-or-less distinct motifs with comprehensible functions was
suggested by Milo et al. [44], and has since been the subject of considerable
research (e.g. [4,48]). This research is predicated upon the assumption that if
enough motifs can be identiﬁed and characterized in terms of their dynamic
behaviour, the behaviour of an entire network can be reconstructed. Indeed,
several authors believe that the analysis of network motifs is not just the best
but the only way in which large, complex networks can be analysed [31,47,58].
It has been asserted that “Identifying and characterizing the design principles
which underlie such feedback paths is essential to understanding sub-cellular
systems” [47].
Small network motifs have, indeed, been found to be over-represented in
the gene networks of model organisms such as the gut bacterium E. coli [13,53]
and the Bakers’ yeast Saccharomyces cerevisiae [36, 60]. The assumption is
that these motifs occur more often than would be expected by chance, which
must imply that they are under positive selection pressure, and hence must be
performing a useful, and hopefully comprehensible biological function. How-
ever, not all researchers accept that network motifs are the key to under-
standing the behaviour of complex networks, arguing that complex behaviour
is likely to be an emergent network property rather than an additive function
of individual modules [30].
If we assume that we can understand the behaviour of individual network
motifs, it would appear to be a relatively straightforward matter to engineer
them into genomes for the purpose of introducing new behaviour into organ-
isms. However, naturally occurring modules are not optimized for operation
within a cellular context that is not their own, and outside of their evolu-
tionary context may not be functional at all. In addition, they are diﬃcult to
modify and it may be impossible to ﬁnd an appropriate natural module that
performs a desired task [1].

58
Hallinan
Although the practical signiﬁcance of network motifs in general remains
unproven, there is a class of network motifs likely, on theoretical grounds, to
be of particular interest to those interested in network dynamics; feedback
loops [39, 40]. A feedback loop may be deﬁned as a path through a network
which begins and ends at the same node; in gene network terms this is a
set of regulatory interactions in which the expression of one gene ultimately
feeds back to aﬀect the activity of the gene itself. Feedback loops may be
positive or negative, depending upon the parity of the number of negative
interactions in the loop. Negative feedback loops tend to act within biological
systems to maintain homeostasis, the maintenance of internal stability in the
face of environmental ﬂuctuations [5], whereas positive feedback loops tend to
produce ampliﬁcation of existing trends, or even runaway increases in dynamic
behaviour [17].
Systems involving negative feedback loops tend to settle to a steady state,
which may be stable or unstable. If it is stable, the result is generally a damped
oscillation, tending towards the stable state; if it is unstable the result is a
sustained oscillation. Oscillatory systems are crucial to all organisms, from
cyanobacteria to elephants. The most obvious biological oscillatory system (or
set of systems) is that underlying circadian rhythms in gene expression. The
expression of many genes in almost all light-sensitive organisms varies in a pe-
riodic manner over the course of a day, aﬀecting physical traits such as sleep–
wake cycles, cardiovascular activity, endocrinology, body temperature, renal
activity, physiology of the gastro-intestinal tract, and hepatic metabolism [22].
Negative feedback loops amongst the genes involved in circadian rhythms
have been modelled in organisms as diverse as the fungus Neurospora [55], the
plant Arabidopsis thaliana [38] and the fruit ﬂy Drosophila [51]. The behaviour
of a negative feedback loop depends partly upon its length. A one element
loop generates a single, stable, steady state; a two element loop produces a
single steady state which is approached to and departed from in a periodic
way; while a loop with three or more elements can generate damped or stable
oscillations depending upon parameter values. In addition, negative feedback
tends to suppress the noise in biological systems which arises from the small
number of molecules which may be available to take part in the processes of
transcription and translation [47].
In contrast, positive feedback loops promote multistationarity; that is, the
existence of a number of diﬀerent stable states [17, 58]. Multistationarity is
essential to development, since diﬀerent cell types represent diﬀerent stable
states in the gene expression space of the organism. Multistationarity is fre-
quently studied in the context of bistable switches in regulatory networks. In
a bistable switch there are two stable states, between which the system can be
moved by an external stimulus. Bistable switches are essentially a memory for
the cell, since the state in which it ﬁnds itself is dependent upon the history
of the system. Such switches have been the subject of both computational
modelling (e.g. [43]) and in vivo investigation.

3 Synthetic Biology: Life, Jim, but Not As We Know It
59
The theory-based approach described above is one way in which genetic
circuits can be designed, but it has not so far been completely successful.
As Sprinzak and Elowitz [54] point out, the study of existing gene circuits is
complicated by our incomplete knowledge of the organisms in question. The
circuits identiﬁed as important may be either incompletely understood or
may contain extraneous, confusing interactions. Successful synthetic biology
requires a more hands-on approach.
The ﬁrst Intercollegiate Genetically Engineered Machine (iGEM) compe-
tition was held at the Massachusetts Institute of Technology in November
1995, with the stated aim of increasing interest and expertise in the synthetic
biology ﬁeld amongst students of biology and engineering. Although a wide
variety of reasonably ambitious projects were proposed, the actual achieve-
ments were, although promising, more modest. Perhaps the most impressive
outcome of the competition was a bacterial circuit that is switched between
diﬀerent states, producing diﬀerently coloured biochemicals, by red light. The
research team grew a lawn of bacteria on an agar plate, and projected a pat-
tern of light onto it to generate a high-deﬁnition “photograph” [37]. iGEM
is an engaging way to recruit the synthetic biologists of the future, but the
“engineer from scratch” approach will probably not be feasible in the imme-
diate future. What, then, is the best way to do synthetic biology with current
technology?
The most obvious way in which to engineer cells to produce a desired
behaviour is to work with the already highly-optimized products of natural
evolution. An existing system can be modiﬁed according to a careful design
based upon an understanding of the system of interest. This is the most widely
used approach to date, and has been successfully applied to a number of well-
characterised biological circuits.
3.4 Model Biological Circuits
There are several biological control systems which are relatively simple and
occur in easily-cultured bacteria, and hence have been studied in considerable
detail for several decades. Circuits which respond to changes in their envi-
ronment are obviously of interest to synthetic biologists. Fortunately, several
such systems have been studied in detail, one of which is the lac operon in
bacteria.
The lac operon is one of the all-time classics of molecular biology. An
operon is a collection of genes whose functions are related, and which are
usually located close together on a chromosome, under the control of a single
transcriptional promoter. The lac operon was the ﬁrst to be identiﬁed, and
consists of a set of genes which can be switched on or oﬀas needed, to deal
with the sugar lactose as an energy source in the medium in which bacteria
grow [33]. The lac operon is a natural target for synthetic biologists. For ex-
ample, the engineering approach to synthetic biology was successfully applied

60
Hallinan
to the lac operon in the gut bacterium E. coli by Atkinson et al. [2], who pro-
duced both bistable switch and oscillatory behaviour by engineering carefully
designed modiﬁcations into the already well-understood genetic circuit. The
major problem with this approach is that it relies upon a detailed understand-
ing of the system to be modiﬁed. The lac operon has been intensively studied
and manipulated for nearly half a century, and so is ideal for engineering, but
very few other circuits are understood in anything like such detail.
Another type of circuit which has been the subject of much experimen-
tation is the bistable switch. This is a system which can exist in either of
two stable states, between which it can be ﬂipped by a speciﬁc trigger. Such
switches are of interest because they are widespread in genetic systems, are
extremely important in determining the behaviour of the system, and are,
in principle at least, relatively easy to understand. The simplest version of
a bistable switch involve a system of two genes, each of which produces a
protein which inhibits the transcription of the other. So if gene a is being
actively transcribed, intracellular levels of its protein, A will be high, gene
b will be inhibited and intracellular level of protein B will be low. If some
event represses a or activates b, the switch will ﬂip and the cell will enter
the alternate state. A bistable switch is the simplest form of cellular memory,
since once in a particular state the system will tend to stay there, even after
the stimulus that ﬂipped it into that state is gone. Understandably, bistable
switches are ubiquitous in biological systems.
In real systems a simple two-repressor switch as described above is unlikely
to work, given the complex genetic context in which it is embedded. Biological
bistable switches tend to be somewhat more complex. A classic example is the
lysis/lysogeny switch of bacteriophage (phage) λ.
Bacteriophages are viruses which infect bacteria. Being viruses, they have
small genomes, and since they grow in bacterial cells they are easy to culture
and collect. Phage λ has therefore been intensively studied, and much is known
about its regulatory interactions [49]. Hard upon its entry into a bacterial cell,
λ must decide which of two pathways it will take: lysis or lysogeny. Under the
lytic pathway the viral DNA is replicated and packed into around 100 new
virions, which are then released into the environment by rupture (lysis) of the
host cell membrane. In contrast, if the lysogenic pathway is followed, the viral
DNA becomes integrated into the host DNA and its lytic genes are repressed
by a repressor known as CI. Although the lysogenic state is very stable, it can
be switched to the lytic state. This occurs if an enzyme called RecA becomes
activated, at which point it cleaves CI and hence inactivates it. The decision as
to whether to enter the lytic or the lysogenic pathway depends upon levels of
another viral protein, CII, which in turn is controlled by the physiological state
of the cell. Once one of the states is entered it is maintained by a network of
interactions: for the lysogenic state this network of interactions is dominated
by CI and in the lytic state it is dominated by an enzyme called Cro, which
also acts as a repressor.

3 Synthetic Biology: Life, Jim, but Not As We Know It
61
The phage λ lysis/lysogeny switch is one of the most intensively studied
of biological circuits, and hence is a tempting target for synthetic biologists.
Because it is so well understood, it is possible to manipulate it in a wide variety
of ways. Although more complex than the lac operon, it is entirely possible to
engineer desired changes into the phage λ switch, but an intriguing alternative
to designed engineering is to harness the power of chance.
3.5 Evolutionary Approaches to Synthetic Biology
The simplest way in which to incorporate stochasticity is to genetically en-
gineer small networks of transcription factor encoding genes with varying
topologies in an existing organism, and then to screen them for interesting
behaviours. Guet et al. [28] did exactly this using three transcriptional reg-
ulators in the gut bacterium Escherichia coli and found that simple binary
logical circuits, analogous to computational “and” and “or” gates could be
produced by networks with a variety of diﬀerent topologies.
A further reﬁnement to the in vivo approach is the addition of selection.
Applying artiﬁcial selection pressure to existing cells can lead to the evolu-
tion of solutions which would not necessarily be apparent to human ingenuity.
This approach is particularly valuable for generating behaviours in which the
nonlinearity and feedback in the system make its behaviour hard to predict
in advance. Artiﬁcial evolution in vivo involves generating an initial pool of
mutants which are then selected for their ability to exhibit a desired be-
haviour. This was proposed, in the context of genetic regulatory circuits, by
Yokobayashi, Weiss and Arnold [61], who demonstrated that artiﬁcial evolu-
tion could rapidly convert a non-functional circuit in phage λ into a functional
one, and that circuits consisting of linked logic gates could be evolved.
Atsumi and Little [3] extended this approach to modify the existing genetic
circuitry controlling the λ lysis / lysogeny switch, replacing Cro with an alter-
native repressor with very diﬀerent properties. They evolved several diﬀerent
types of mutants with diﬀerent regulatory behaviour which they proceeded to
analyse in excruciating biochemical detail, demonstrating that this approach
is valuable not only from a pragmatic point of view, but also as a generator
of interesting experimental subjects. Circuits generated by artiﬁcial evolution
vary in the details of their organization and behaviour; studying them in detail
increases our understanding of the diﬀerent ways in which similar behaviour
can be generated.
From a computational point of view artiﬁcial evolution in vivo is a fasci-
nating but remote topic. But the same sort of approach can be taken in silico,
generating potential network designs which can then be implemented in real
cells. A computational evolutionary approach is potentially much more rapid,
controllable and inexpensive than in vivo experiments. An EC approach could
also help us deal with the problem of incomplete knowledge; Andrianantoan-
dro, Basu, Karig and Weiss [1] suggest that many of the problems currently

62
Hallinan
encountered in synthetic biology, arising from our lack of knowledge about
the details of the systems we are trying to manipulate, can be overcome by
using populations of cells, so that ﬂuctuations in the behaviour of individual
cells are buﬀered by the overall behaviour of the cell population as a whole.
This approach clearly lends itself to modelling via a computational evolution
approach such as the classic genetic algorithm, in which evolution involves a
population of competing (or cooperating) individuals.
Several researchers have applied EC approaches to problems related to, al-
though not directly relevant to, synthetic biology. For example, Deckard and
Sauro [11] used an evolutionary strategy to evolve “signalling networks”, mod-
elled as systems of ODEs, which could perform mathematical operations such
as multiplication by constants, square roots and natural logarithms. EC has
also been used to study the regulation of ﬂux in simple metabolic models [24],
and to optimise parameter values in reaction rates in signal transduction mod-
els [8,59]. While such models provide valuable insights into the topology and
dynamics of abstract network models, they are not immediately relevant to
the hands-on engineering of organisms.
EC has to date been underused in synthetic biology. Most of the work has
involved deliberate design, or artiﬁcial evolution in vivo, as described above.
But the potential utility of computational intelligence is starting to be appre-
ciated. Simple networks exhibiting bistable switch behaviour were produced
using computational evolution by Francois and Hakim [21]. They used a sim-
ple evolutionary algorithm operating upon a population of gene networks. As
well as the promotion and repression of transcription, proteins in their model
could interact to form complexes, and could be post-transcriptionally modi-
ﬁed. Post-transcriptional modiﬁcation is known to be an important component
of genetic regulatory systems, but it is still relatively poorly understood, and
hence is rarely included in gene network models. Networks were subjected to a
range of “mutation operators” including modiﬁcation of kinetic constants and
addition and deletion of genes, and were scored according to how closely their
behaviour matched the target behaviour. The researchers found that bistable
switches could be evolved in less than 100 generations, while oscillatory cir-
cuits could be achieved in a few hundred generations.
Francois and Hakim’s results are particularly interesting because they re-
veal the variety of network topologies which can produce bistable behaviour,
few of which had the “classical” two-repressor design described above. This ob-
servation was also made by Deckard and Sauro with reference to their evolved
mathematical networks. Some of the bistable switch network designs were very
similar to known biological switches such as the lac operon [33]. The networks
also tended to be relatively robust to variations in parameter values and to
noise, a good indication that the network topology is the most important
factor in producing the observed behaviour.
The relationship between network topology and dynamics is not well un-
derstood, and Francois and Hakim demonstrate how EC approaches can pro-
duce insights into this relationship. In order to advance synthetic biology,

3 Synthetic Biology: Life, Jim, but Not As We Know It
63
these insights must be translated into useful biological systems. Computa-
tional and laboratory approaches to the design of living systems are essential
complements to each other.
Computational modelling was tightly integrated with laboratory experi-
mentation in an elegant set of experiments carried out by Guido et al. [29]
using phage λ promoters inserted into the lac operon in E. coli. They started
by engineering the system in such a way that no regulation occurred, in or-
der to examine the behaviour of the unregulated system. They used data on
the transcriptional activity of this system to build a least-squares mathemat-
ical model of the system, parameterized using the best of 2000 random initial
guesses for the model parameter values. The biological system was then modi-
ﬁed to incorporate various combinations of gene activation and repression and
the model parameters were ﬁtted to these new data sets. The deterministic
model was then modiﬁed to incorporate stochastic noise, and it was observed
that ﬂuctuations in the concentrations of transcription factors have very mi-
nor eﬀects on the variability of the expression levels. The model was then used
to generate testable predictions as to how changes in the system would aﬀect
expression levels. The researchers conclude that their simple model had cap-
tured many, but not all, of the sources of variability in the biological system.
The work of Guido et al. is interesting for several reasons. Their tightly
integrated and iterative modelling and experimental processes maximize the
information gained from both the computational and the laboratory experi-
ments, and this is almost certainly a good model for future progress in syn-
thetic biology. They also demonstrate the potential of using computational
models of regulatory subsystems to predict the behaviour of larger, more com-
plex networks, an approach which is considerably more sophisticated than the
search for network motifs discussed above, which has been the focus of most
research in this area.
3.6 Conclusions
Synthetic biology is the culmination of the essentially engineering approach
to biology which started with the deciphering of the molecular structure of
DNA in 1953. In little more than half a century we have gone from won-
dering whether the informational molecule of life was protein or DNA, to
manipulating the characteristics of existing organisms, to a point where the
construction of entirely new life forms is a realistic ambition. Advances in
technology have played an essential role in this progression. While biologists
tend to lay emphasis upon the development of automated systems for DNA
analysis and synthesis, it is becoming increasingly apparent that the math-
ematical and simulation capacities of computers will be equally essential for
the achievement of this goal.
Synthetic biology to date has been based upon what biologists think they
know about the organisms with which they work. This approach has been

64
Hallinan
moderately successful, but as Endy puts it “At present, we do not have a
practical theory that supports the design of reproducing biological machines,
despite great progress in understanding how natural biological systems couple
and tune error detection and correction during machine replication to organ-
ism ﬁtness” [14]. Part of the problem may be an over-reliance on theory, with
a concomitant failure to extract full value from the large amounts of data and
sophisticated computational algorithms currently available.
The need for data-driven, as opposed to theory-driven, approaches to syn-
thetic biology is starting to be acknowledged. In a recent review, Heinemann
and Panke [32] stress the need for quantitative analysis, which they interpret
as mathematical models, for understanding and hence engineering complex
biological systems. This is a step in the right direction; however, the very
factors which make biological systems diﬃcult to understand and predict also
make them hard to model using classical mathematical approaches such as
systems of ordinary diﬀerential equations, which describe the rates of change
of continuously changing quantities modelled by functions. The more ﬂexible,
data-driven and heuristic approach of computational intelligence has enor-
mous potential to contribute to synthetic biology.
For a start, EC algorithms mimic the processes by which life was originally
created and shaped. Biological evolution is, after all, the only force which has
produced life so far. From a more pragmatic point of view, EC is uniquely
ﬁtted to deal with complex, incompletely speciﬁed problems in which the goal
may be clear, although the means by which to achieve it is not. Although
our understanding of biological systems is constantly and rapidly improving,
the ﬁeld of synthetic biology is likely to be in this situation for many years
yet. Until we really understand how living organisms work, the “engineering
design” approach will only be successful for small, simple problems. And even
when (or if!) we do have a full understanding of biological complexity, our
designs will be limited by human imaginations. Evolution, real or computa-
tional, is not so limited, and may produce novel and eﬀective solutions which
are completely unintuitive to the human mind. EC algorithms, being stochas-
tic, also provide multiple solutions to any given problem, many of which will
be equally good. EC thus provides the opportunity to study the “whys” as
well as the “hows” of complex biological behaviour.
The ﬁeld of systems biology is in its very early stages, and new tools and
techniques are under active investigation. Evolutionary computation has been
employed, and found useful, but undoubtedly has more to oﬀer, particularly
if used in close conjunction with laboratory experiments. Although EC is
never guaranteed to produce the optimum solution to any problem, it may
well provide “good-enough” solutions to many of the problems of synthetic
biology. And sometimes, as Victor Frankenstein would undoubtedly agree,
good enough is the best you can hope for, particularly when trying to create
life.

3 Synthetic Biology: Life, Jim, but Not As We Know It
65
References
1. Andrianantoandro, A., Basu, S., Karig, D., Weiss, R.: Synthetic biology: New
engineering rules for an emerging discipline.
Molecular Systems Biology
2(2006.0028) (2006)
2. Atkinson, M., Savageau, M., Myers, J., Ninfa, A.: Development of genetic cir-
cuitry exhibiting toggle switch or oscillatory behaviour in Escherichia coli. Cell
113, 597–607 (2003)
3. Atsumi, S., Little, J.: Regulatory circuit design and evolution using phage
lambda. Genes and Development 18, 2086–2094 (2004)
4. Banzhaf, W., Kuo, P.: Network motifs in natural and artiﬁcial transcriptional
regulatory networks. Journal of Biological Physics and Chemistry 4(2), 85–92
(2004)
5. Becskei, A., Serrano, L.: Engineering stability in gene networks by autoregula-
tion. Nature 405, 590–593 (2000)
6. Benner, S., Sismour, A.: Synthetic biology. Nature Reviews Genetics 6, 533–543
(2005)
7. Bolouri, H., Davidson, E.: Modelling transcriptional regulatory networks. BioEs-
says 24, 1118–1129 (2002)
8. Bray, D., Lay, S.: Computer simulated evolution of a network of cell-signalling
molecules. Biophysical Journal 66(4), 972–977 (1994)
9. Cello, J., Paul, A., Wimmer, E.: Chemical synthesis of poliovirus cDNA: gen-
eration of infectious virus in the absence of natural template.
Science 297,
1016–1018 (2002)
10. de Jong, H.: Modelling and simulation of genetic regulatory systems: a literature
review. Journal of Computational Biology 9(1), 67–103 (2002)
11. Deckard, A., Sauro, H.: Preliminary studies on the in silico evolution of bio-
chemical networks. ChemBioChem 5, 1423–1431 (2004)
12. Dickman, S.: Production of recombinant insulin begins. Nature 329(6136), 193
(1987)
13. Dobrin, R., Beqand, Q., Barabasi, A., Oltvai, S.: Aggregation of topological
motifs in the Escherichia coli transcriptional regulatory network. BMC Bioin-
formatics 5(10) (2004)
14. Endy, D.: Foundations for engineering biology. Nature 438, 449–453 (2005)
15. Endy, D., Brent, R.: Modelling cellular behaviour. Nature 409, 391–395 (2001)
16. Erd¨os, P., R´enyi, A.: On random graphs. Publicationes Mathematicae 6, 290
(1959)
17. Ferrell Jr, J.: Self-perpetuating states in signal transduction: positive feedback,
double-negative feedback and bistability. Current Opinion in Cell Biology 14,
140–148 (2002)
18. Fogel, G., Corne, D.: Evolutionary Computation in Bioinformatics.
Morgan
Kaufmann, Boston (2003)
19. Fogel, L.: Intelligence Through Simulated Evolution: Four Decades of Evolu-
tionary Programming. Wiley (1999)
20. Forster, A., Church, G.: Toward synthesis of a minimal cell. Molecular Systems
Biology 2, 1–10 (2006)
21. Francois, P., Hakim, V.: Design of genetic networks with speciﬁed functions by
evolution in silico. Proceedings of the National Academy of Sciences of the USA
101(2), 580–585 (2004)

66
Hallinan
22. Gachon, F., Nagoshi, E., Brown, S., Ripperger, J., Schibler, U.: The mammalian
circadian timing system: from gene expression to physiology.
Chromosoma
113(3), 103–112 (2004)
23. Gilman, A., Larkin, A.: Genetic “code”: Representations and dynamical models
of genetic components and networks. Annual Reviews of Genomics and Human
Genetics 3, 341–369 (2002)
24. Gilman, A., Ross, J.: Genetic-algorithm selection of a regulatory structure that
directs ﬂux in a simple metabolic model. Biophysical Journal 69(4), 1321–1333
(1995)
25. Glass, J., Assad-Garcia, N., Alperovitch, N., Yooseph, S., Lewis, M., Maruf, M.,
Hutchinson, C., Smith, H., Venter, J.: Essential genes of a minimal bacterium.
Proceedings of the National Academy of Sciences of the USA (2006)
26. Goldberg, D.: Genetic Algorithms in Search, Optimization and Machine Learn-
ing. Addison-Wesley, Boston (1989)
27. Gross, J.: Graph Theory and its Applications. Chapman and Hall/CRC, Boca
Raton (2006)
28. Guet, C., Elowitz, M., Hsing, W., Leibler, S.: Combinatorial synthesis of gene
networks. Science 296(5572), 1466–1470 (2002)
29. Guido, N., Wang, X., Adalsteinsson, D., McMillen, D., Hasty, J., Cantor, C.,
Elston, T., Collins, J.: A bottom-up approach to gene regulation. Nature 439,
856–860 (2006)
30. Hallinan, J., Jackway, P.: Network motifs, feedback loops and the dynamics of
genetic regulatory networks. In: Proceedings of the 2005 IEEE Symposium on
Computational Intelligence in Bioinformatics and Computational Biology, pp.
90–96. IEEE Press (2005)
31. Hasty, J., McMillen, D., Collins, J.: Engineered gene circuits. Nature 420, 224–
230 (2002)
32. Heinemann, M., Panke, S.: Synthetic biology – putting engineering into biology.
Bioinformatics 22(22), 2790–2799 (2006)
33. Jacob, F., Monod, J.: Genetic regulatory mechanisms in the synthesis of pro-
teins. Journal of Molecular Biology 3, 318–356 (1961)
34. Jaenisch, R., Bird, A.: Epigenetic regulation of gene expression: how the genome
integrates intrinsic and environmental signals.
Nature Genetics 33, 245–254
(2003)
35. Kaern, M., Blake, W., Collins, J.: The engineering of gene regulatory networks.
Annual Review of Biomedical Engineering 5, 179–206 (2003)
36. Lee, T., Rinaldi, N., Robert, F., Odom, D., Bar-Joseph, Z., Gerber, G.: Tran-
scriptional regulatory networks in Saccharomyces cerevisiae. Science 298, 799–
805 (2002)
37. Levskaya, A., Chevalier, A., Tabor, J., Simpson, Z., Lavery, L., Levy, M., David-
son, E., Scouras, A., Ellington, A., Marcotte, E., Voigt, C.: Synthetic biology:
engineering Escherichia coli to see light. Nature 438, 441–442 (2005)
38. Locke, J., Millar, A., Turner, M.: Modelling genetic networks with noisy and
varied experimental data: the circadian clock in Arabidopsis thaliana. Journal
of Theoretical Biology 234, 383–393 (2005)
39. Mangan, S., Alon, U.: Structure and function of the feed-forward loop network
motif. Proceedings of the National Academy of Sciences of the USA (2003)
40. Mangan, S., Zaslaver, A., Alon, U.: The coherent feedforward loop serves as
a sign-sensitive delay element in transcription networks. Journal of Molecular
Biology 334(2), 197–204 (2003)

3 Synthetic Biology: Life, Jim, but Not As We Know It
67
41. Mattick, J.: Non-coding RNAs: the architects of molecular complexity. EMBO
Reports 2(11), 986–991 (2001)
42. McDaniel, R., Weiss, R.: Advances in synthetic biology: on the path from pro-
totypes to applications. Current Opinion in Biology 16, 476–483 (2005)
43. Michael, D., Oren, M.: The p53-Mdm2 module and the ubiquitin system. Sem-
inars in Cancer Biology 13(1), 49–58 (2003)
44. Milo, R., Shen-Orr, S., Itzkovitz, S., Kashtan, N., Chklovskii, D., Alon, U.:
Network motifs: simple building blocks of complex networks. Science 298, 824–
827 (2002)
45. Mitchell, M.: An Introduction to Genetic Algorithms. MIT Press, Cambridge,
MA (1996)
46. Mushegian, A., Koonin, E.: A minimal gene set for cellular life derived by com-
parison of complete bacterial genomes. Proceedings of the National Academy
of Sciences of the USA 93(19), 10,268–10,273 (1996)
47. Orrell, D., Bolouri, H.: Control of internal and external noise in genetic regula-
tory networks. Journal of Theoretical Biology 230, 301–312 (2004)
48. Przulj, N., Wigle, D., Jurisica, I.: Functional topology in a network of protein
interactions. Bioinformatics 20(3), 340–348 (2004)
49. Ptashne, M.: A Genetic Switch: Phage Lambda and Higher Organisms. Cell
Press and Blackwell Scientiﬁc Publications, Cambridge MA (1992)
50. Reil, T.: Models of gene regulation – a review. In: C. Maley, E. Boudreau (eds.)
Artiﬁcial Life 7 Workshop Proceedings, pp. 107–113. MIT Press, Cambridge,
MA (2000)
51. Ruoﬀ, P., Christensen, M., Sharma, V.: PER/TIM-mediated ampliﬁcation, gene
dosage eﬀects and temperature compensation in an interlocking-feedback loop
model of the Drosophila circadian clock. Journal of Theoretical Biology 237,
41–57 (2005)
52. Schilling, C., Schuster, S., Palsson, B., Heinrich, R.: Metabolic pathway analysis:
basic concepts and scientiﬁc applications. Biotechnology Progress 15(3), 296–
303 (1999)
53. Shen-Orr, S., Milo, R., Mangan, S., Alon, U.: Network motifs in the transcrip-
tional network of Escherichia coli. Nature Genetics 31, 64–68 (2002)
54. Sprinzak, D., Elowitz, M.: Reconstruction of genetic circuits. Nature 438, 443–
448 (2005)
55. Sriram, K., Gopinathan, M.: A two variable delay model for the circadian
rhythm of Neurospora crassa. Journal of Theoretical Biology 231, 23–38 (2004)
56. Szathmary, E.: Life: In search of the simplest cell. Nature 433, 469–470 (2006)
57. Szybalski, W., Skalka, A.: Nobel prizes and restriction enzymes. Gene 4, 181–
182 (1978)
58. Thomas, R., Thieﬀry, D., Kaufman, M.: Dynamical behaviour of biological reg-
ulatory networks I – biological role of feedback loops and practical use of the
concept of the loop-characteristic state. Bulletin of Mathematical Biology 57(2),
247–276 (1995)
59. Tsuchiya, M., Ross, J.: Application of genetic algorithm to chemical kinetics:
Systematic determination of reaction mechanism and rate coeﬃcients for a com-
plex reaction network. Journal of Physical Chemistry A 105(16), 4052–4058
(2001)
60. Wuchty, S., Oltvai, Z., Barabasi, A.: Evolutionary conservation of motif con-
stituents in the yeast protein interaction network. Nature Genetics 35(2), 176–
179 (2003)

68
Hallinan
61. Yokobayashi, Y., Weiss, R., Arnold, F.: Directed evolution of a genetic circuit.
Proceedings of the National Academy of Sciences of the USA 99(26), 16,587–
16,591 (2002)

4
Dancing with Swarms: Utilizing Swarm
Intelligence to Build, Investigate, and Control
Complex Systems
Christian Jacob
Department of Computer Science, Faculty of Science and Department of
Biochemistry & Molecular Biology, Faculty of Medicine, University of Calgary,
Calgary, Alberta, Canada cjacob@ucalgary.ca
We are surrounded by a natural world of massively parallel, decentralized
biological “information processing” systems, a world that exhibits fascinat-
ing emergent properties in many ways. In fact, our very own bodies are the
result of emergent patterns, as the development of any multi-cellular organ-
ism is determined by localized interactions among an enormous number of
cells, carefully orchestrated by enzymes, signalling proteins and other molec-
ular “agents”. What is particularly striking about these highly distributed
developmental processes is that a centralized control agency is completely ab-
sent. This is also the case for many other biological systems, such as termites
which build their nests – without an architect that draws a plan, or brain cells
evolving into a complex ‘mind machine’ – without an explicit blueprint of a
network layout.
Obviously, being able to understand, build and harness the emergent prop-
erties of such systems would be highly beneﬁcial. Designers of complex sys-
tems could utilize their adaptability and robustness. Such systems would con-
struct themselves through self-organization. However, system designers and
programmers are facing an enormous challenge. How can we actually build
highly distributed systems of which we have only limited understanding?
Would we have to invent new ways of building, maintaining, and controlling
such systems? It seems to be necessary to explore a completely new mindset
for programming and system control:
“It is no longer possible to use traditional, centralized, hierarchical
command and control techniques to deal with systems that have thou-
sands or even millions of dynamically changing, communicating, het-
erogeneous entities. [...] the type of solution swarm intelligence of-
fers is the only way of moving forward, [we] have to rethink the way
[we] control complex distributed systems.”
Eric Bonabeau, 2003, co-author of Swarm Intelligence [7]

70
Jacob
4.1 The Emergence of Complexity
Despite the fact that we have become so used to living with machines and de-
vices, which seem to be relatively easy to handle from a user perspective, these
mostly represent extremely complex systems, built by carefully engineered
top-down design processes. Cybernetics and systems sciences are experienc-
ing a revival, mostly in combination with the sciences of complexity. Studying
designs from an integrated systems perspective has become increasingly im-
portant as we recognize that the understanding of emergent properties of com-
plex systems that are built and designed from a bottom-up perspective prove
to be more and more advantageous [3,42,68]. Artiﬁcial neural networks and
artiﬁcial immune systems as adaptive units that learn from training data and
do not rely on large-scale programming are just one aspect of the success story
of the integrative systems paradigm [13,39,45,55,72]. Advances in small-scale
technologies and manufacturing enable us to build systems in a self-organizing
manner – with a large number of interacting entities [12,54]. Engineers, physi-
cists, computer scientists, biologists, and life scientists are also getting a better
understanding of complex systems as a whole, and how observations and be-
haviors on higher levels are eﬀects of system-speciﬁc, underlying emergent
properties with bottom-up and top-down feedback loops [8,23,40,51,63,65].
We also want to know increasingly more details about vastly complex
systems that we can either currently build on our own – such as nano-devices
for medication and minimally invasive diagnostics – or natural systems that we
do have to understand – such as regulatory processes inside our human bodies,
or complex inter-relationships within eco-, economic or social systems [27,38,
50]. Medicine has made major advances. However, too many of all the intricate
details to make our human bodies work are still mysteries to us. Will we ever
get deﬁnite answers to questions such as: How does the immune system work?
How does the human brain work? How can we re-program cells that had a
‘program crash’ and have become cancerous? What do we understand about
gene expression and the complex regulatory mechanisms involved? And from
an educational point of view: Do we adequately prepare our next generations
of medical doctors, biologists, software developers, and engineers to equip
them with these new mindsets necessary to cope with the challenges that
come with the complexities of natural systems? How do we make them reveal
their secrets and utilize these for our engineered systems, which we want to
build, understand, and control?
4.2 Emergent and Agent-Based Computing
Agent-based computing and simulation approaches to study emergent phe-
nomena are starting to become more and more prominent within computa-
tional and mathematical modeling [1,20,22,73]. It seems necessary to explore

4 Dancing with Swarms
71
a completely new mindset for programming, system control, and design. En-
gineers, physicists, computer scientists and biologists already beneﬁt from
simulation tools that are being developed to investigate agent interactions
in virtual laboratories such as StarLogo [62], REPAST [2], Swarm [49], and
Breve [67].
Furthermore, some of these programming, exploration and simulation tools
are now entering the classrooms – from kindergarten to high school to uni-
versity. The latest release of StarLogo TNG [44], for example, comes with a
graphical programming interface, a terrain editor, an agent shape customizer
and many other features to build complex scenes of interacting agents, ob-
serve the emergent patterns (such as the spreading of an epidemic triggered
by person-to-person contact), and investigate the inﬂuences of system param-
eters. Using tools like these, our current generation of students acquires a
better appreciation of how observable eﬀects on a global level are the conse-
quence of an intricate ensemble of rather simple but local interactions among
a multitude of “agents”. Hence, our next-generation of researchers and en-
gineers will gain a more in-depth comprehension of the ﬁne balances within
ecosystems, how traﬃc control systems based on collective intelligence can
help avoid gridlocks on our city roads, how a people-centered architectural
perspective may change the way we design our buildings, or how nano devices
and swarm-based medical simulations will make true personalized medication
possible.
4.3 Chapter Overview: An Emergent Computing Tour
This chapter will take us on a virtual tour of emergent computing systems –
from very small scale to large scale. We approach the question of design from
a swarm intelligence perspective, in order to explore bottom-up designs of
dynamic systems with emergence properties. We will discuss example imple-
mentations of agent-based systems that have been designed and investigated
over the last few years in the Evolutionary & Swarm Design Laboratory at
the University of Calgary.
We start with a demonstration of a gene regulatory system within a bac-
terial cell in Sect. 4.4, where we show how a seemingly simple on/oﬀswitch
has a rather intriguing realization, which – despite (or rather because of) its
swarm-based implementation – is functional, reliable, and highly robust.
From this example of a metabolic process inside a single bacterium, we go
one level higher in Sect. 4.5, where we look at interaction patterns among a
population of bacterial cells in a simulated petri dish. We demonstrate that
even simple interaction rules can quickly lead to intricate modes of interaction.
The focus in Sect. 4.6 is on how human cells of diﬀerent types as well as
messenger molecules shape an orchestrated network to control blood clotting.
How a blood clot is formed is understood to some extent, but all its details

72
Jacob
are still not completely revealed, which turns this complex interaction pro-
cess into yet another challenge and useful testbed for swarm-based computer
simulations.
From an orchestra of cells within the human body we proceed to the world
of social insects, which have so far provided a rich reservoir of inspirations
for novel computer algorithms in optimization, traﬃc engineering and swarm
intelligence. Section 4.7 shows computer models of raiding patterns within
diﬀerent species of army ants. The actual ways of exploration, food attack
and collection back to the nest depends on speciﬁc ant characteristics as well
as on the food distribution.
Section 4.8 takes us from insect societies to predator–prey systems in a spa-
tial, two-dimensional world. We use this example to demonstrate how seem-
ingly ‘intelligent’ strategies might not result from conscious strategic planning
at all, but could as well be eﬀects from collective interactions driven by simple
rules. This predator–prey scenario also shows the role that the environmental
setup plays when one tries to understand and interpret agent behaviors.
Looking at interactions in human societies, it is interesting to see how
observations made among social insects, such as army ants, have parallels in
the interaction patterns of human crowds. In Sect. 4.9 we explore this aspect
in a simulation of a sales event, with customers rushing to and from sales
desks.
Finally, swarm intelligence systems have also entered the artistic scene.
Interactive computer installations that promote emergent systems behaviors
engage the user in a scientiﬁc exploration, but at the same time push the
envelope for modern multi-media art. Section 4.10 gives a brief overview of the
SwarmArt.com initiative, which resulted from a close collaboration between a
Calgarian artist (Gerald Hushlak) and computer scientists from the University
of Calgary.
4.4 Inside a Cell: Gene Regulation
What distinguishes a neuron cell from a skin cell, or a liver cell from a heart
cell? As all cells in our body share the same genetic information encoded in
the DNA (deoxyribonucleic acid) and all cells have an identical copy of this
DNA in their nucleus, how is it that there are diﬀerent cell types, which not
only can have a wide variety of shapes and functional designs but also attend
to diﬀerent tasks within our bodies? Cell diﬀerentiation is all about a cell’s
genome and its proteome, the set of proteins that it manufactures depending
on the set of genes that are currently switched on.1 Diﬀerent cell types have
diﬀerent sets of genes switched on and oﬀ. But how do these on/oﬀswitches
actually work?
It turns out that even the simplest procaryotic (bacterial) cells without a
nucleus have evolved intriguing mechanisms for regulating the expression of
1 Biologists talk about the ‘expression’ of genes in the form of proteins.

4 Dancing with Swarms
73
(a)
(b)
(c)
(d)
(e)
(f)
Fig. 4.1. A swarm-based model of gene regulation processes around the lactose
operon in the bacterium E. coli (a) Looking at the outside of the cell immersed
in lactose (b) Zooming inside the cell reveals the bio-molecular agents in the cy-
toplasm (c) Free polymerases (in brown) eventually dock onto the DNA and start
scanning (in pink) one of the strands (d) The processes of transcription (from DNA
to the single-strand copy of messenger RNA/mRNA) and translation (from mRNA
to amino acid chains and folded proteins) are simulated. The yellow spheres represent
lactose permeases, which – once close to the cell wall – enable lactose to enter the cy-
toplasm (e) The green repressors gather around the promoter site which marks the
starting point of the β-galactosidase gene. The ‘cloud’ of these regulatory repressors
prevents polymerases from docking onto the DNA at this location, hence keeping
the subsequent genes switched oﬀ(f) More bio-molecular agents are added into the
simulation, such as allolactose, acetylase, glucose, galactose, phosphate, ATP, and
cAMP

74
Jacob
Experiment 1
Experiment 2
Experiment 3
Fig. 4.2. The concentrations of bio-molecular agents during a lactose operon simu-
lation for three independent experiments as in Fig. 4.1. Note that there is inherent
noise, which is a natural consequence of the agent-based spatial system. The switch-
ing behavior also shows a high degree of robustness. Each experiment plot shows
the concentrations of mRNA, ribosomes, repressors, repressor-lactose complexes,
β-galactosidase, lactose, glucose, and galactose (from left to right, top to bottom)
genes in the form of proteins as key building blocks of cells. The bacterium
Escherichia coli (E. coli) has been studied for almost a century as a model
organism to shed light on various designs of gene regulatory mechanisms [5,
37,52,59,60].
Over the last few years, our lab has constructed 3D agent-based models
of the so-called Lactose Operon [11, 31, 32, 70]. This system constitutes an
active inhibiting switch as a result of a swarm of proteins interacting with
a regulatory section (the promoter and operator regions) on the DNA. The
bacterium E. coli is one of the main micro-organism species living in the
lower intestines of mammals (including humans). One of the energy sources
for these bacteria is lactose. That is, when you drink milk, these bacteria
are getting prepared for a boost of energy. Figures 4.1(a) and (b) show a
virtual bacterium immersed in lactose. Particles of lactose enter the cell by
help of lactose-permease enzymes, which are represented as yellow spheres in
Fig. 4.1(d).
Although the bacterium is enveloped by lactose, there is one crucial prob-
lem. E. coli can not use lactose in its original form, but has to break it down
into two components: glucose and galactose. Glucose constitutes the actual
energy-carrying components the bacterium is able to utilize in its metabolism.
Now here comes a very neat design cue: In order to break down lactose a par-
ticular enzyme, β-galactosidase, has to be present inside the cell. However,
it would be a waste of energy for the cell to produce this helpful enzyme
all the time. The expression of β-galactosidase is therefore normally stopped,
i.e., its gene is switched oﬀ. This inactivation is achieved by so-called re-

4 Dancing with Swarms
75
pressors, which attach onto a region (the so-called operator) just before the
β-galactosidase gene section on the DNA. Figure 4.1(e) shows a swarm of re-
pressors (in green) that hover around the regulatory region for β-galactosidase
and block the reading machinery – in the form of polymerases – from getting
to the genome section encoding for β-galactosidase, lactose permease, and
acetylase.2
At this point the cutting enzymes are almost completely inactivated. This
actually is another intriguing aspect of the switch. As it turns out, repressors
that have grabbed onto the DNA tend to fall oﬀevery now and then. This
‘design ﬂaw’ is compensated for by the cloud of repressors that have high
aﬃnity for the same region. Once one repressor falls oﬀthe DNA, another
one is quickly around to take its position. Therefore, by keeping a relatively
low number (usually less than twenty) of repressors around, the cell manages
to reliably keep the expression of the operon inactivated.
So when is the gene for β-galactosidase switched on? Once lactose enters
the cytoplasm (the inside of the cell) and comes in contact with a repressor,
they form a repressor-lactose complex, which completely changes the shape
of the repressor. As a consequence, the repressor is no longer able to grab
onto the DNA, hence looses its regulatory (suppressive) function. Eventually,
with more lactose being distributed inside the cell, all repressors become dis-
functional, do not block polymerases from accessing the operon and therefore
start expressing β-galactosidase,3 which, in turn, breaks down lactose into
glucose and galactose.
Of course, this creates a negative feedback loop. As soon as all lactose is
broken down and there is no more lactose inﬂux into the cell, repressors can
resume their task of deactivating β-galactosidase expression. The cell is then
back to its initial state again. Figure 4.2 illustrates both the stochasticity and
robustness of this simulated switch. Details regarding the actual interaction
rules are described in [31,32]. Our lab has also constructed a similar swarm-
based model of the more complicated λ-switch involving a bacteriophage [30],
which shows related properties and is also driven by emergent behaviors from
local interactions of biomolecular agents.
4.5 Bacteria and Cell Populations: Chemotaxis
From examples of metabolic and regulatory processes inside a single bac-
terium, we go one level higher, where we look at interaction patterns among
2 This is the actual operon, a set of genes that is switched on and oﬀtogether
as one module. The permease brings lactose into the cell. The acetylase is be-
lieved to detoxify thiogalactosides, which are transported into the cell along with
lactose [60].
3 At the same time, as part of the operon, lactose permeases and acetylases are
also produced.

76
Jacob
(a)
(b) t = 100
(c) t = 200
(d) t = 1200
(e) t = 2200
(f) t = 3200
(g) t = 4200
(h) t = 6200
(i) t = 9900
Fig. 4.3. Evolution of a bacterial culture with color genes. (a) 100 ‘color’ bacteria
(represented by yellow and blue color) are placed around the center of a simulated
agar plate. The bacteria grow and multiply, and radial patterns emerge as food is
depleted in the centre which drives bacteria outward. These patterns are similar to
in vivo observations [48] (b)–(i) An analogous simulation, now with four types of
bacteria distributed on an agar plate of size 1000 × 1000. The emergent patterns
also resemble those found in cellular automaton simulations of excitable media [21]

4 Dancing with Swarms
77
Nutrient
MCP
CheA
CheW
CheB
CheR
CheY
FliM
CheZ
Tumble
+P
+P
+P
Outside of Cell
Inside of Cell
+P
Fig. 4.4. The simpliﬁed E. coli chemotaxis signal transduction pathway (derived
from [41]) used in our model [26,58]. A nutrient gradient is sensed by the bacterium
that triggers an internal cascade of signals and molecular interactions, which result in
an action such as tumbling by controlling the rotational movements of their ﬂagella
tails
a population of bacterial cells on a simulated petri dish. Bacteria are remark-
ably adaptive and can therefore survive in a wide variety of conditions, from
extreme temperature environments of volcanoes to soil to the inside of bodies
of higher organisms. Bacteria such as E. coli are relatively easy to handle in
wet labs, thus provide perfect model organisms, and – due to their relative
simplicity (which nevertheless poses enormous challenges for computational
modeling) – are good testbeds for computer simulations. Apart from the inter-
esting dynamics inside such single-cell organisms (as described in Sect. 4.4),
bacteria display intriguing evolutionary patterns when they interact in large
quantities (which is almost always the case).4 The resulting emergent dynam-
ics are not unlike the ones observed in social insects, such as ants and bees
(see Sect. 4.7), where local interactions result in collective intelligence [24,28].
Such swarm- and agent-based paradigms have been used to model decentral-
ized, massively-parallel systems – from vehicular traﬃc to ant colonies, bird
ﬂocks, and bacterial ecosystems [25,26,46,57,58,61].
Figure 4.3 shows the evolution among three types of simulated bacteria
over time. This system (described in more detail in [26,58]) is an application
of in silico simulation and modeling to the study of signal transduction path-
ways [56]. A signal transduction pathway (STP) describes the proteins, DNA,
regulatory sites (operons) and other molecules involved in a series of intra-
cellular chemical reactions, which eventually result in a physical action of the
cell (Fig. 4.4). In this case, metabolism and chemotaxis in the bacterium E.
coli is modeled. Through chemotaxis, bacteria navigate within their environ-
4 As an illustrative example of a high-density eﬀect, recent photographic techniques
developed with E. coli bacteria result in a resolution of 100 megapixels per square
inch [47].

78
Jacob
ment and seek out food by following nutrient gradients. Signal transduction is
modeled through an editable, evolvable rule-based artiﬁcial chemistry [16,71]
that deﬁnes all bindings and productions of proteins.
By encoding the genotype of each bacterium as a virtual DNA sequence
we are also able to study how mutation and inheritance propagate through
generations of bacteria. Intra-cellular complexes and molecules that match
(pre-deﬁned) behavioral patterns trigger observable behaviors, such as tum-
bling, swimming or cell-division. This approach allows for the construction
and investigation of arbitrary STPs, which result in a large diversity of bac-
terial behavior patterns.
For the evolutionary studies described in this section (Fig. 4.3), each bac-
terium, n, carries characteristic color genes gn = (rn, gn, bn), which are ex-
pressed as the red, green, and blue (RGB) color components. Each time a
bacterium splits into two, the RGB genes are mutated within a radius r, re-
sulting in new gene settings g′
n = (rn + χ(r), gn + χ(r), bn + χ(r)). Here χ(r)
generates a uniformly distributed random number from the interval [−r, r].
The bacteria circle (Fig. 4.3a) is an emergent phenomenon observed in
vivo [48]. When a single bacterium is placed into agar and allowed to grow and
multiply, a circle emerges as food is depleted in the centre and bacteria move
outward. An interesting pattern forms in simulations when this initial setup is
combined with bacteria which mutate only one of their three color components
as they evolve. The culture in Fig. 4.3(a) begins with 100 bacteria in the
centre, all with the same color gene, and a uniform food distribution. After
360, 000 iterations the culture has reached the edge of the square plate with a
population peak of about 1, 000, 000 bacteria. There are distinct groupings of
dark and light colored segments as well as mixed regions where bacteria with
a variety of colors aggregate.
The triangular ‘slices’ radiating from the center of the circle show how
an early subset of the population is able to propagate its genetic color over
time. Some segments of the population exist within large pockets of similar
individuals while others mix more thoroughly with non-similar bacteria. This
visualization illustrates how locality and inheritance are related. Furthermore,
it shows the non-uniformity of an evolving bacterial environment, an impor-
tant ﬁtness consideration.
Figure 4.3(b) shows 100 small, independent cultures distributed through-
out the environment, with diﬀerent initial color genes, mutation rates and
swimming speeds. Figures 4.3(c) through (e) show a smoothing eﬀect over the
emitted colors, which merge into larger colored regions with bright boundaries,
where bacteria with high mutation rates are located. The remaining images
illustrate how the movement of bacteria and the decay of colors begin to form
seemingly coordinated patterns. The bacteria continue to emit colors and more
intricate patterns emerge, which we revisit from an art perspective in a later
section (Fig. 4.12).

4 Dancing with Swarms
79
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
Fig. 4.5. Blood clotting: (a) Partial view of the simulated blood vessel with a
trauma site at the bottom. Red blood cells, platelets (white) and ﬁbrinogens (blue)
are moving through the vessel section (b) Close-up of the damaged tissue site with
collagen lining (c) Platelets (white) and activated platelets (transparent wire-frame)
as well as activating factors (d) Agglomerations of activated platelets (e) Fibrino-
gens (blue) and ﬁbrins (green) (f) An initial mesh formed by sticky ﬁbrins (g) Red
blood cells get caught in ﬁbrin network (h) View from inside the blood vessel to-
wards the wound site (i) Final clotting with red blood cells caught in a network of
ﬁbrins and activated platelets
4.6 The Swarming Body: Cellular Orchestration
In the previous section, we looked at cells of equal type and functionality,
with clear, externally observable behaviors, such as swimming, tumbling or
cell division. So, from a design and modeling point of view, these systems and
their interaction dynamics seem to be less complicated than those biological
systems in which an extensive cascade of orchestrated actions is necessary
to achieve a ﬁnal outcome – such as in blood clotting. The coagulation of
blood is a highly complex process during which bio-molecular agents form
solid clots and stop blood ﬂow in a damaged vessel. The focus in this section

80
Jacob
Serotonin
Platelet
(unactivated)
ADP
Platelet
(activated)
Collagen
Cell
cAMP
Prostacyclin
Thrombin
Endothelial
Cell
Tissue Factor
Fibrin
(unactivated)
Fibrin
(activated)
Trauma Message
becomes
secretes
activates
secretes
destroys
secretes
secretes
prompts cAMP secretion
destroys
prompts
Prostacyclin
secretion
activates
becomes
activates
secretes
creates
adheres to
adheres to
adheres to
adheres to
activates
Fig. 4.6. Overview of bio-molecular agents and their interactions involved in the
blood clotting cascade
is on how human blood-related cells of diﬀerent types as well as messenger
molecules form an orchestrated network to achieve blood clotting. How a blood
clot is formed is understood to a large extent, but all its details are still not
completely revealed [64].
A three-dimensional model of a section of a human blood vessel is shown in
Fig. 4.5(a). Blood-related agents ﬂow through the vessel section. At the bot-
tom of the vessel is a trauma site, through which blood particles can escape.
Figure 4.5(b) shows a close-up of the damaged endothelial blood vessel lining,
around which the tips of collagen ﬁbers are visible. Collagens are long, ﬁbrous
structural proteins and most abundant within connective tissue in mammals.
When donut-shaped platelets come into contact with collagen they become
activated (compare Fig. 4.6). Active platelets release messenger molecules in
the form of several types of coagulation factors and platelet activating fac-
tors (Fig. 4.5c). After platelets have become activated, they tend to become
sticky and adhere to collagen. Figure 4.5(d) shows several agglomerations of
platelets. These platelets alone, however, are not enough to stop the escape
of blood particles through the wound site. One of the proteins released by
activated platelets is thrombin, which converts soluble ﬁbrinogen into insolu-
ble strands of ﬁbrins which attach to collagen and form a weblike structure

4 Dancing with Swarms
81
(Fig. 4.5e,f). Red blood cells get caught in these ﬁbrin networks (Fig. 4.5g),
an eﬀect that reinforces the actual clotting in combination with the activated
platelets (Fig. 4.5i).
Again, what is interesting about these processes involved in coagulation
is that all interactions among the bio-molecular and cellular agents can be
described and implemented as activation rules based on local collisions among
the agents involved (Fig. 4.6). The overall outcome leading to the formation of
a web that manages to stop the escape of red blood cells is an emergent eﬀect
of this orchestration. Contrary to an orchestra, however, there is no central
conductor. All actions are triggered locally; all system components work on
a completely independent and distributed basis. This is what agent-based
systems like this one can facilitate. Our lab has built similarly complicated
swarm-based models of the processes within lymph nodes and at blood vessel-
tissue interfaces involved in immune system responses to viral and bacterial
infections [34,36].
4.7 Colony Swarms: Army Ant Raids
From an ‘orchestra’ of cells within the human body we proceed to the world
of social insects, which have so far provided a rich reservoir of inspirations for
novel computer algorithms in optimization [13,17,43], traﬃc engineering [25,
57] and swarm intelligence [7]. We show computer models of raiding patterns
within diﬀerent species of army ants. The actual ways of exploration, food
attack and collection back to the nest depends on speciﬁc ant characteristics
as well as on the food distribution.
Army ants are one of the most ferocious predators in the insect world [28].
Colonies of new world army ants may contain upwards of 600,000 members.
In order to feed this veritable horde, army ants conduct raids almost daily,
netting over 30,000 prey items. Raids may contain up to 200,000 attacking
worker ants, all of which are almost completely blind. As with all swarm raid-
ing phenomena, army ant raids are ordered complex organizational structures
conducted without any centralized control.
While raiding in army ants is not completely understood, many models of
these collective intelligence systems have been constructed [15,18,66]. A more
recent model, which we follow in our simulations, suggest that the rates at
which ants are able to turn act as control mechanisms for the formation of
traﬃc lanes during raids [14], a phenomenon also observed in human crowds
(see Sect. 4.9 for details). Individual ants within our simulation move on a
discrete grid. They are oriented either along the vertical or horizontal axis,
but can assume any position within a grid square. Grid sites are assumed to
hold dozens of ants and are therefore much larger than a single ant. Each grid
site is attributed with diﬀerent kinds of pheromones, that are deposited by
ants and decay over time. While moving from grid to grid, ants make their

82
Jacob
(a)
(b)
(c)
(d)
(e) E. burchelli
(f) E. hamatum
(g) E. rapax
Fig. 4.7. Army ants raiding patterns: a swarm-based simulation models diﬀerent
species of Eciton army ants [69]. (a)–(d) Diﬀerent stages during raiding visualize
the ants’ behaviors during raiding. Ants are color-coded dependent on their roles
as follows: pioneers (red), followers (blue), primary recruiters (yellow), secondary
recruiters (green), attackers (cyan), transporters (yellow). Cyan squares contain
pheromone. Pink squares mark combat sites (e)–(g) Three diﬀerent raiding pat-
terns emerge dependent on food distribution and ant species

4 Dancing with Swarms
83
decisions depending on the amount of pheromone sensed in the three grid sites
ahead of the ant’s current orientation (Fig. 4.7).5
The simulation incorporates the following types of ants: Pioneer ants ven-
ture into unexplored territory at the front of the swarm. Follower ants de-
tect and follow a trail of pheromones. Pioneers turn into primary recruiters
upon their ﬁrst encounter with a potential prey. Recruiters then initiate mass
recruitment by signaling followers to change into secondary recruiters. Also
drawn by pheromone signals, attack ants move towards a prey site and initiate
prey capture. Finally, transport ants retrieve prey back to the nest.
Figure 4.7(a) shows a swarm of ants leaving the nest, which is located at
the top left corner. Pioneer ants are in red, which leave pheromone trails (cyan
squares in Fig. 4.7b) behind for other ants to follow. Pink squares indicate
sites of combat with prey, which is distributed in a quasi-uniform fashion. Prey
capture is introduced into the model by applying the principle of Lanchester’s
theory of combat [19]. Once the ants succeeded in overwhelming their prey,
transport ants bring prey items back to the nest. As these transporting ants
have reduced maneuverability due to their heavy loads, other ants tend to
get out of their ways, which leads to the formation of distinct lanes to and
from the nest site (Fig. 4.7d). The last row of images in Fig. 4.7 show typical
exploration patterns of the ants that result from diﬀerent ant parameters (e.g.,
speed and turning rate) and from varying the distribution of food items.
4.8 Herd Behaviors: Predators and Prey
This section takes us from insect societies to predator–prey systems in a spa-
tial, two-dimensional world. We use this example to demonstrate how seem-
ingly ‘intelligent’ strategies might not result from conscious strategic planning
at all, but could as well be eﬀects from collective interactions driven by simple
rules. This predator–prey scenario also shows the role that the environmental
setup plays when one tries to understand and interpret agent behaviors.
Figure 4.8(a) presents a simple predator–prey scenario. Seven predator
agents (in red) are going to start oﬀwith random walks, until they encounter
any of the prey agents (in cyan), which are arranged in a cluster near the
top left corner. Both predators and prey are equipped with distance sensors.
This way, predators become aware of prey, in which case they will go after
the detected prey agent. On the other hand, prey agents are able to sense
approaching predators and will turn away from any approaching predator.
Prey agents also have an urge to stay close to their ﬂockmates. Following
these simple rules, once predators approach the group of prey agents, they
retreat into a corner as if acting as a herd. In Fig. 4.8(b) the white prey agents
are the ones ‘seeing’ the approaching predators. Notice that the prey group
5 These sites are the one straight ahead of the ant’s current site and the two sites
diagonally ahead to the left and right.

84
Jacob
(a)
(b)
(c)
(d)
Simulation in a ﬂat environment.
(e)
(f)
(g)
(h)
Simulation in an environment with trees (green cones).
Fig. 4.8. Predators and Prey: environmental cues inﬂuence system behaviors. Ini-
tially, prey are cyan and predators are red. Prey agents turn white when they become
aware of a predator. Light green cones represent trees (bottom row)
sticks together (Fig. 4.8c). Eventually, as more predators happen to discover
more prey agents and start following them, groups of predators become more
scattered over the landscape (Fig. 4.8d). Eventually, most of the prey agents
get caught and eliminated by the predators; this is built into the system, as
our prey agents are slightly slower than the predators.
Now imagine the same scenario, but this time the landscape also contains
a number of trees, which appear as green cones in Fig. 4.8(e). Both predator
and prey agents follow exactly the same rules as previously. Predators will
approach the prey agents, which tend to become more and more scattered into
smaller groups (Fig. 4.8f). However, something interesting happens. Many of
the prey agents seem to use the trees as hiding places. Both Figs 4.8(g) and
(h) show examples of prey agents taking refuge from predators behind trees.
In many cases, predators and prey engage in a chasing game around trees. Not
knowing about the same underlying agent rules as in the scenario with trees,
one might be misled to think that prey agents have discovered a successful
survival strategy. Yet this is another example of emergent eﬀects – this time,
however, environmental factors play a key role, where interactions among
agents and their surroundings combine into seemingly ‘intelligent strategies.’

4 Dancing with Swarms
85
(a)
(b)
Fig. 4.9. Human crowds: sales table rush. (a) Low separation urge. (b) High sep-
aration urge. The corresponding snapshots are taken at the same simulation time
steps

86
Jacob
4.9 Human Crowds: Sales Table Rush
Looking at interactions in human societies, it is interesting to see how obser-
vations made among social insects, such as army ants, have parallels in the
interaction patterns of human crowds. Here we explore this particular aspect
in a simulation of a sales event, with customers rushing to and from a sales
desk.
Figure 4.9 shows the scenario. One hundred agents are spread across an
area conﬁned by four walls. An exit door is located at the bottom left, marked
by a blue square. The sales table area is the green square near the top corner.
Agents in blue are still waiting to get into the sales area. Yellow agents have
made their purchase recently. Once these agents are heading towards the exit
door, they are displayed in red.
Two experiments are shown along the two columns in Fig. 4.9. On the left
side, agents approach the sales area (yellow) and then move almost straight
away from the sales table (yellow and red, Fig. 4.9a). On their way they
encounter the blue agents that still have to get into the sales area. A downward
stream of returning agents (yellow and red) seams to separate the blue agents.
This is similar to the trail formation we have observed in the army ant raiding
model (compare Fig. 4.7d).
In the second experiment (Fig. 4.9b) the separation urge of agents going to
and leaving from the sales area is increased ﬁve-fold. This means that agents
tend to separate from each other; they need more ‘personal space’. This has
the consequence that agents are pushing towards the sales table within a wider
area. In fact, as agents approach the sales area from a wider range of directions
– as they are pushed apart – they get their purchases faster and can return
earlier. Eventually, only a lane of approaching (blue) and returning (yellow)
agents in the centre remains.
4.10 SwarmArt.com: The Art of Swarm Intelligence
Swarm intelligence systems have also entered the artistic scene. Over the last
few years, we have been working on interactive computer installations that
promote emergent systems behaviors [9,10,53]. These exhibitions engage the
user in a scientiﬁc exploration, but at the same time push the envelope for
modern multi-media art. This section gives a brief overview of our Swarm-
Art.com initiative, which resulted from a close collaboration between a Cal-
garian artist and computer scientists from the University of Calgary.6
6 Gerald Hushlak, Department of Art; Jeﬀrey Boyd and Christian Jacob, Depart-
ment of Computer Science.

4 Dancing with Swarms
87
Fig. 4.10. Swarm art with BoidLand: Illustration of a swarm of paint brushes
following a user-controlled red dot. An obstacle is illustrated as a grey square
The SwarmArt.com project7 began in 2002 with a simple idea. Instead of
having an artist direct his/her paint strokes with a single brush, we wanted to
explore how a ‘swarm of paint brushes’ could be used to create art. We ﬁrst
started with a simulated two-dimensional canvas in a system called BoidLand,
which later got extended to 3D canvas spaces [35, 46]. Figure 4.10 gives an
illustration of the swarm brushes. A collection of swarming agents moves on a
2D canvas and leaves trails of paint behind. A user-controlled cursor, visual-
ized by a red dot, acts as a focal point for the swarm agents. Hence, keeping
the dot in the center of the canvas for a while, makes the swarm congregate
in the middle. Continuing to move the cursor, the swarm follows. Obstacles
7 The name is derived from the utilization of swarm intelligence techniques to art,
implemented through computer and information processing technologies (dot-
com).

88
Jacob
are enveloped by the swarm agents. Once a former obstacle disappears, the
canvas space is reclaimed by the swarms.
Fig. 4.11. BoidLand II: Examples of swarm paintings on a 2D canvas
Fig. 4.12. BoidPainter: van Gogh-style swarm recreations of still images
Figure 4.11 shows a few examples of swarm art created with the second
version of BoidLand. Here a user can either be present to direct the swarm
brushes, or the system will create its own designs. In 2002, this system was
extended to use live video input to direct the swarms, and was exhibited at

4 Dancing with Swarms
89
the Nickle Arts Museum in Calgary (http://www.ucalgary.ca/~nickle/).
An implementation of 3D canvas spaces and swarm choreography [46] led to
another installation at the same gallery in 2003.
The following year, we not only included video-controlled, swarming agents
but also aspects of bacterial chemotaxis (compare Sect. 4.5 and Fig. 4.3, in
particular). We illustrate the basic idea of BoidPainter in Fig. 4.12. Again,
a swarm of painter agents inhabits the canvas space. The canvas now con-
sists of either a still image or a frame from a video stream. The agents pick
up color information from each pixel they come across during their random
walks and interactions on the canvas. Each agent remembers the color for a
certain amount of time. Similar to pheromones, the agents then paint a single
pixel square that they are visiting with that same color they have stored. At
the beginning, color pheromones evaporate quickly, so have to be replenished
often by the agents. Over time the color pheromones persist longer and thus
the actual underlying image becomes more and more visible. After a pre-
deﬁned time, the cycle starts again with high pheromone evaporation rates.
As these processes can be simulated quite fast, this technique can also be
eﬀectively applied to video streams. This allows one to directly interact with
the swarm system, direct the agents, and also inﬂuence the composition and
decomposition patterns created. An exhibition which applied the BoidPainter
technique to live video streams was implemented both at the Nickle Arts Mu-
seum and in The Other Gallery at the BanﬀCentre (Banﬀ, Alberta, Canada;
http://www.banffcentre.ca/). More recent versions of our continued Swar-
mArt.com projects as well as a more detailed overview can be found in [33]
and [10].
4.11 Conclusion and Outlook
In this chapter, we have given a glimpse into the world of massively parallel,
decentralized information processing systems – with emergent properties from
agents that determine the outcome of the overall system through their local
interactions, and with essentially no central controller in charge. What have
we learnt by looking at these example simulations which covered a wide range
of scales? We went from gene regulation inside a bacterial cell, populations of
cells engaged in competition for nutrients, and orchestrated blood-related bio-
molecular agents to army ant raiding patterns, seemingly strategic predator–
prey interaction scenarios, and human crowd interaction around a sales event.
A small excursion into the arts concluded our journey.
From a programming perspective, it turns out that many code libraries
developed for one emergent computing application can be re-used in many
others, regardless of the scale of the simulated system. Nature’s systems ex-
hibit analogous properties. Natural evolution is not possible without inter-
actions of entities that scale from molecules to ecosystems [4]. Evolutionary
computation [29] as well as more sophisticated in silico implementations of

90
Jacob
evolutionary processes (computational evolution) can be very beneﬁcial for
the design and ﬁne tuning of swarm intelligence models [35,46].
Our examples seem to be in strong support of a purely reductionist view,
but it should be noted that we will only succeed with a more holistic perspec-
tive that integrates bottom-up design with downward causation [54]. Mas-
sively parallel, decentralized systems require a diﬀerent approach to design,
with feedback loops in both directions up and down the scales across all levels
of organizational structures, following the design and control principles in liv-
ing systems. Once we better understand swarm-based systems in combination
with self-organization, regulatory and evolutionary mechanisms within this
broader context, we are prepared to implement one of the next revolutions in
engineering: self-assembling and self-adjusting structures that follow Nature’s
inspiring examples [6]. As mentioned in the introductory section, program-
ming tools to explore such systems based on a large number of interacting
agents are readily available, even for our youngest students [44]. Among those
is the next generation of engineers and scientists that will look at our world
and our technological systems from a diﬀerent perspective: the world as a big
swarm!
4.12 ESD Resources and Examples on the Web
Programming code and tools, as well as movies and further details about the
models presented in this chapter are available online through our Evolutionary
& Swarm Design Laboratory web site: http://www.swarm-design.org/.
Acknowledgments
Many students have come through my lab over my last few years at the Univer-
sity of Calgary. I am grateful to all of them for their enthusiasm and contribu-
tions. In particular I want to acknowledge the following students: Ian Burleigh
(lac operon), Joanne Penner and Ricardo Hoar (bacterial chemotaxis), Garret
Suen and Patrick King (army ants), Euan Forrester (BoidLand), Paul Nuytten
(BoidLand II and BoidPainter). Some of the projects discussed here are based
on class projects implemented by student groups. Especially I want to thank
Sonny Chan, Matthew Templeton, Garth Rowe, Rennie Degraaf, Aarti Punj
(blood clotting), Wilson Hui, Michael Lemczyk, David Robertson, Lia Rogers,
Leslie Schneider, Vaughan van der Merwe (predator–prey), Brett Hardin, Linh
Lam, Lillian Tran, Phil Tzeng, and Oscar Yeung (human crowds).
I am also grateful to my colleagues Jeﬀrey Boyd and Gerald Hushlak for
their participation in SwarmArt.

4 Dancing with Swarms
91
References
1. Adamatzky, A., Komosinski, M. (eds.): Artiﬁcial Life Models in Software.
Springer (2005)
2. Altaweel, M., Collier, N., Howe, T., Najlis, R., North, M., Parker, M., Tatara,
E., Vos, J.R.: REPAST: Recursive porous agent simulation toolkit (2006). URL:
http://repast.sourceforge.net/
3. Bak, P.: How Nature Works: The Science of Self-organized Criticality. Coper-
nicus, Springer, New York (1996)
4. Banzhaf, W., Beslon, G., Christensen, S., Foster, J., K´ep`es, F., Lefort, V., Miller,
J., Radman, M., Ramsden, J.: From artiﬁcial evolution to computational evo-
lution: a research agenda. Nature Reviews: Genetics 7 (2006)
5. Beckwith, J., Zipser, D. (eds.): The Lactose Operon. Cold Spring Harbor Lab-
oratory Press, Cold Spring Harbor, NY (1970)
6. Benyus, J.: Biomimicry: Innovation Inspired by Nature. William Morrow, New
York (1997)
7. Bonabeau, E., Dorigo, M., Theraulaz, G.: Swarm Intelligence: From Natural to
Artiﬁcial Systems. Santa Fe Institute Studies in the Sciences of Complexity.
Oxford University Press, New York (1999)
8. Bonner, J.: The Evolution of Complexity by Means of Natural Selection. Prince-
ton University Press, Princeton, NJ (1988)
9. Boyd, J., Hushlak, G., Jacob, C.: SwarmArt: interactive art from swarm intel-
ligence. In: ACM Multimedia. ACM (2004)
10. Boyd, J., Jacob, C., Hushlak, G., Nuytten, P., Sayles, M., Pilat, M.: SwarmArt:
interactive art from swarm intelligence. Leonardo (2007)
11. Burleigh, I., Suen, G., Jacob, C.: DNA in action! A 3D swarm-based model of
a gene regulatory system. In: First Australian Conference on Artiﬁcial Life.
Canberra, Australia (2003)
12. Camazine, S., Deneubourg, J., Franks, N., Sneyd, J., Theraulaz, G., Bonabeau,
E.: Self-Organization in Biological Systems. Princeton Studies in Complexity.
Princeton University Press, Princeton (2003)
13. Corne, D., Dorigo, M., Glover, F. (eds.): New Ideas in Optimization. Advanced
Topics in Computer Science. McGraw-Hill, London (1999)
14. Couzin, I., Franks, N.: Self-organized lane formation and optimized traﬃc ﬂow
in army ants.
Proceedings of the Royal Society of London B(270), 139–146
(2002)
15. Deneubourg, J., Goss, S., Franks, N., Pasteels, J.: The blind leading the blind:
modeling chemically mediated army ant raid patterns. Journal of Insect Behav-
ior 2, 719–725 (1989)
16. Dittrich, P., Ziegler, J., Banzhaf, W.: Artiﬁcial chemistries: a review. Artiﬁcial
Life 7(3), 225–275 (2001)
17. Dorigo, M., Gambardella, L.: Ant colony system: a cooperative learning ap-
proach to the traveling salesman problem. IEEE Transactions on Evolutionary
Computation 1(1), 53–66 (1997)
18. Franks, N.: Army ants: collective intelligence. American Scientist 77, 139–145
(1989)
19. Franks, N., Partridge, L.: Lanchester battles and the evolution of combat in
ants. Animal Behaviour 45, 197–199 (1993)
20. Gaylord, R., D’Andria, L.: Simulating Society: A Mathematica Toolkit for Mod-
eling Socioeconomic Behavior. TELOS/Springer, New York (1999)

92
Jacob
21. Gaylord, R., Wellin, P.: Computer Simulations with Mathematica: Explorations
in Complex Physical and Biological Systems. Springer, New York (1995)
22. Gershenfeld, N.: The Nature of Mathematical Modeling. Cambridge University
Press, Cambridge (1999)
23. Goodwin, B.: How the Leopard Changed Its Spots: The Evolution of Complex-
ity. Touchstone Book, Simon & Schuster, New York (1994)
24. Gordon, D.: Ants at Work: How an Insect Society is Organized. The Free Press,
New York (1999)
25. Hoar, R., Penner, J., Jacob, C.: Evolutionary swarm traﬃc: if ant roads had
traﬃc lights. In: Proceedings of the IEEE World Congress on Computational
Intelligence. IEEE Press, Honolulu, Hawaii (2002)
26. Hoar, R., Penner, J., Jacob, C.: Transcription and evolution of a virtual bacteria
culture. In: Proceedings of the IEEE Congress on Evolutionary Computation.
IEEE Press, Canberra, Australia (2003)
27. Holland, J.: Emergence: From Chaos to Order. Helix Books, Addison-Wesley,
Reading, MA (1998)
28. H¨olldobler, B., Wilson, E.: The Ants. Harvard University Press, Cambridge,
MA (1990)
29. Jacob, C.: Illustrating Evolutionary Computation with Mathematica. Morgan
Kaufmann, San Francisco, CA (2001)
30. Jacob, C., Barbasiewicz, A., Tsui, G.: Swarms and genes: exploring λ-switch
gene regulation through swarm intelligence.
In: Proceedings of the IEEE
Congress on Evolutionary Computation (2006)
31. Jacob, C., Burleigh, I.: Biomolecular swarms: an agent-based model of the lac-
tose operon. Natural Computing 3(4), 361–376 (2004)
32. Jacob, C., Burleigh, I.: Genetic programming inside a cell. In: T. Yu, R. Riolo,
B. Worzel (eds.) Genetic Programming Theory and Practice III, pp. 191–206.
Springer (2006)
33. Jacob, C., Hushlak, G.: Evolutionary and swarm design in science, art, and
music. In: P. Machado, J. Romero (eds.) The Art of Artiﬁcial Evolution, Natural
Computing Series. Springer-Verlag (2007)
34. Jacob, C., Litorco, J., Lee, L.: Immunity through swarms: agent-based simula-
tions of the human immune system. In: Artiﬁcial Immune Systems, ICARIS
2004, Third International Conference. LNCS 3239, Springer, Catania, Italy
(2004)
35. Jacob, C., von Mammen, S.: Swarm grammars: growing dynamic structures in
3D agent spaces. Digital Creativity 18(1) (2007)
36. Jacob, C., Steil, S., Bergmann, K.: The swarming body: simulating the decen-
tralized defenses of immunity. In: Artiﬁcial Immune Systems, ICARIS 2006, 5th
International Conference. Springer, Oeiras, Portugal (2006)
37. Jacob, F., Monod, J.: Genetic regulatory mechanisms in the synthesis of pro-
teins. Molecular Biology 3, 318–356 (1961)
38. Johnson, S.: Emergence: The Connected Lives of Ants, Brains, Cities, and Soft-
ware. Scribner, New York (2001)
39. Kasabov, N.: Foundations of Neural Networks, Fuzzy Systems, and Knowledge
Engineering. MIT Press, Cambridge, MA (1998)
40. Kauﬀman, S.: At Home in the Universe: The Search for Laws of Self-
Organization and Complexity. Oxford University Press, Oxford (1995)
41. KEGG: Bacterial chemotaxis – Escherichia coli K-12 MG1655 (path:eco02030)
(2003). URL: http://www.genome.ad.jp/kegg/expression

4 Dancing with Swarms
93
42. Kelly, K.: Out of Control. Perseus Books, Cambridge, MA (1994)
43. Kennedy, J., Eberhart, R.: Swarm Intelligence. The Morgan Kaufmann Series
in Evolutionary Computation. Morgan Kaufmann, San Francisco (2001)
44. Kloper, E., Begel, A.: StarLogo TNG (2006). URL: http://education.mit.
edu/starlogo-tng/
45. K¨uhn, R., Menzel, R., Menzel, W., Ratsch, U., Richter, M., Stamatescu, I. (eds.):
Adaptivity and Learning: An Interdisciplinary Debate. Springer (2003)
46. Kwong, H., Jacob, C.: Evolutionary exploration of dynamic swarm behaviour.
In: Proceedings of the IEEE Congress on Evolutionary Computation. IEEE
Press, Canberra, Australia (2003)
47. Levskaya, A., Chevalier, A., Tabor, J., Simpson, Z., Lavery, L., Levy, M., David-
son, E., Scouras, A., Ellington, A., Marcotte, E., Voigt, C.: Engineering Es-
cherichia coli to see light. Nature 438, 441–442 (2005)
48. Madigan, M., Martinko, J., Parker, J.: Brock Biology of Microorganisms, 10th
edn. Prentice-Hall, Upper Saddle River, NJ (2003)
49. Minar, N., Burkhart, R., Langton, C., Askenazi, M.: The swarm simulation
system: a toolkit for building multi-agent simulations. Working paper 96-06-
042, Santa Fe Institute, Santa Fe, New Mexico, USA (1996). URL: http://
www.swarm.org
50. Morowitz, H.: The Emergence of Everything: How the World Became Complex.
Oxford University Press, Oxford (2002)
51. Morris, R.: Artiﬁcial Worlds: Computers, Complexity, and the Riddle of Life.
Harper Collins Canada/Perseus Books (2003)
52. M¨uller-Hill, B.: The Lac Operon – A Short History of a Genetic Paradigm.
Walter de Gruyter, Berlin (1996)
53. Nguyen, Q., Novakowski, S., Boyd, J., Jacob, C., Hushlak, G.: Motion swarms:
video interaction for art in complex environments. In: ACM Multimedia. ACM,
ACM (2006)
54. Noble, D.: The Music of Life. Oxford University Press (2006). URL: http:
//www.oup.com
55. Novartis-Foundation (ed.): Immunoinformatics: Bioinformatics Strategies for
Better Understanding of Immune Function, vol. 254.
Wiley (2003).
URL:
http://www.novartisfound.org.uk
56. Palsson, B.: The challenges of in silico biology. Nature Biotechnology 18, 1147–
1150 (2000)
57. Penner, J., Hoar, R., Jacob, C.: Swarm-based traﬃc simulation with evolution-
ary traﬃc light adaptation. In: L. Ubertini (ed.) Applied Simulation and Mod-
elling, International Association of Science and Technology for Development,
pp. 289–294. ACTA Press, Zurich, Crete, Greece (2002)
58. Penner, J., Hoar, R., Jacob, C.: Bacterial chemotaxis in silico. In: ACAL 2003,
First Australian Conference on Artiﬁcial Life. Canberra, Australia (2003)
59. Ptashne, M.: A Genetic Switch: Phage λ and Higher Organisms, 2nd edn. Cell
Press & Blackwell Scientiﬁc Publications, Palo Alto, CA (1992)
60. Ptashne, M., Gann, A.: Genes & Signals. Cold Spring Harbor Laboratory Press,
Cold Spring Harbor, NY (2002)
61. Resnick, M.: Turtles, Termites, and Traﬃc Jams: Explorations in Massively
Parallel Microworlds. Complex Adaptive Systems. MIT Press, Cambridge, MA
(1997)
62. Resnick, M.: StarLogo (2006). URL: http://education.mit.edu/starlogo/

94
Jacob
63. Schlosser, G., Wagner, G. (eds.): Modularity in Development and Evolution.
University of Chicago Press, Chicago (2004)
64. Sherwood, L.: Human Physiology, 5th edn. Thomson (2004)
65. Simon, H.: Modularity: Understanding the Development and Evolution of Nat-
ural Complex Systems. MIT Press (2005)
66. Sol´e, R., Bonabeau, E., Delgado, J., Fern´andez, P., Marin, J.: Pattern formation
and optimization in army ant raids. Artiﬁcial Life 6, 216–227 (2000)
67. Spector, L., Klein, J.: Evolutionary dynamics discovered via visualization in
the BREVE simulation environment. In: Artiﬁcial Life VIII. Addison-Wesley,
Reading, MA (2002)
68. Strogatz, S.: Sync – The Emerging Science of Spontaneous Order. Theia Books,
New York (2003)
69. Suen, G.: Modelling and simulating army ant raids. M.Sc. thesis, University of
Calgary, Dept. of Computer Science, Calgary, Canada (2004)
70. Suen, G., Jacob, C.: A symbolic and graphical gene regulation model of the lac
operon. In: Fifth International Mathematica Symposium, pp. 73–80. Imperial
College Press, London, England (2003)
71. Suzuki, H.: An approach to biological computation: unicellular core-memory
creatures evolved using genetic algorithms. Artiﬁcial Life 5(4), 367–386 (1999)
72. Vertosick, F.: The Genius Within: Discovering the Intelligence of Every Living
Thing. Harcourt, New York (2002)
73. Wolfram, S.: A New Kind of Science. Wolfram Media, Champaign, IL (2002)

Part II
Art

Evolutionary Design in Art
Jon McCormack
Centre for Electronic Media Art, Faculty of Information Technology, Monash
University, Clayton 3800, Victoria, Australia
Jon.McCormack@infotech.monash.edu.au
Evolution is one of the most interesting and creative processes we currently
understand, so it should come as no surprise that artists and designers are
embracing the use of evolution in problems of artistic creativity. The material
in this section illustrates the diversity of approaches being used by artists
and designers in relation to evolution at the boundary of art and science.
While conceptualising human creativity as an evolutionary process in itself
may be controversial, what is clear is that evolutionary processes can be used
to complement, even enhance human creativity, as the chapters in this section
aptly demonstrate.
When it comes to using evolutionary methods for art and creative design,
there are two related technical problems:
(i) the choice and speciﬁcation of the underlying systems used to generate
the artistic work;
(ii) how to assign ﬁtness to individuals in an evolutionary population based
on aesthetic or creative criteria.
In the case of (i) there are a multitude of possibilities, but a recurring
theme seems to be the use of complex dynamical systems, probably due to
the easy mapping of their complexity to visual or sonic output, combined with
their ability to generate a large variety of aesthetically unusual output. These
systems are, in computational terms, relatively simple. Yet they are able to
produce highly detailed and complex images or sounds.
Two of the chapters that follow deal with fractal-based images as their un-
derlying generative mechanism. Fractals have become a perennial obsession
for many who work in evolutionary art. Mandelbrot’s exploration of the ‘frac-
tal geometry of nature’, as espoused in the book of the same name, triggered
a revolution in thinking about the mathematical description of natural shape
and form from the time it was ﬁrst published in the 1970s. Fractal images
seem to have a rich ‘nature’ of their own; one that is beguiling, cosmic, and
endless. Fractals permit a vast phase-space of possibilities well-suited to evo-
lutionary exploration. The chapter by Daniel Ashlock and Brooke Jamieson

98
McCormack
introduces the reader to this topic and illustrates ways in which evolution
can be used to explore the complexity of the Mandelbrot set. The chapter by
Jeﬀrey Ventrella uses an automated ﬁtness function based on likeness to real
images (including Mandelbrot himself!) to evolve fractal shapes reminiscent
of real things, yet with their own unique aesthetic qualities – what might be
termed ‘a fractal reality’.
For problem (ii) listed above (evaluating ﬁtness according to aesthetic cri-
teria), the paradigm-setting breakthrough was the concept of the interactive
genetic algorithm (iga), ﬁrst appearing in Richard Dawkins’ Biomorph soft-
ware, described in his 1986 book, The Blind Watchmaker. In this method the
role of aesthetic ﬁtness evaluation – a problem diﬃcult to represent in algo-
rithmic form – is performed by the user of the interactive evolutionary system,
who selects individuals based on their own aesthetic preferences. Selected indi-
viduals become the parents for the next generation and the process is repeated
until a satisfactory result is obtained or the user runs out of patience.
Following the pioneering work of artists such as Karl Sims and William
Latham in the early 1990s, the iga, or aesthetic selection as it is sometimes
called, established the signiﬁcant idea that creativity was no longer limited
by the imagination of the creator. Artistic designs could be evolved through
the simple act of selection, even if the artist doing the selecting had no un-
derlying knowledge of how the system was producing the design! Indeed, a
common observation for evolved complex systems is that the designer does
not understand explicitly how a particular genotype produces the individual
outcomes in the phenotype. Unlike engineering or biological applications, re-
verse engineering genotypes seems to oﬀer little insight into the nature of what
constitutes aesthetically pleasing images.
The use of the iga in evolutionary art applications continues today as
evidenced in my own chapter, where L-system grammars are interactively
evolved using the iga to create strange and bizarre hybrids of real plants. In
the E-volver system described in the chapter by Erwin Driessens and Maria
Verstappen, drawing agents are evolved based on their ability to modify their
environment, which happens to be an image. Multiple interacting agents form
a self-generating image that constantly undergoes change and renewal. The
idea of conceptualising the artwork as an ‘evolutionary ecosystem’ seems to
hold much promise for evolutionary art applications.
In artistic applications, we observe the importance of ‘strange ontologies’,
whereby the artist forgoes conventional ontological mappings between sim-
ulation and reality in order to realise new creative possibilities. In scientiﬁc
studies, the conceptualisation of models is based on our understanding of re-
ality and we selectively try to preserve as much isomorphic mapping between
reality and model as possible. Moreover, in implementing the model as a com-
puter program, the programmer is forced to deﬁne basic ontological structures
(or at least symbolic representations of them). We talk about ‘individuals’ and
‘environments’ interacting through well-deﬁned channels of possibility. These

Evolutionary Design in Art
99
concepts and interactions represent the ontology of the system, and must be
explicitly mapped to data structures and algorithms.
In the most interesting creative works, however, conventional ontologies
are changed and perverted in order to open new creative possibilities: an
agent becomes a pixel in a image that is its environment; a plant grows in
a way that deﬁes biological convention, yet still maintains its own topologi-
cal logic – two simple examples of the strange ontologies that appear in the
chapters that follow. In this mode of artistic enquiry we are no longer try-
ing to model reality, but to use human creativity to devise new relationships
and interactions between components. These new relationships and interac-
tions may be impossible, even illogical, yet through their impossibility they
expose implicit thinking about the way we conceptualise our models in the
ﬁrst place. This kind of thinking may shed light on previously unconsidered
assumptions in scientiﬁc models. Indeed, if we are lucky, it might even result
in some enlightening artwork.
Jon McCormack
Art Area Leader

5
Natural Processes and Artiﬁcial Procedures
Erwin Driessens and Maria Verstappen
http://www.xs4all.nl/~notnot/
notnot@xs4all.nl
5.1 Introduction
The boundaries between nature, culture and technology are increasingly fad-
ing. Nature has become increasingly less natural in everyday life and tech-
nological applications have become more and more natural. Our relation to
nature has rested on technical control and scientiﬁc and economic analysis for
many years. Technology is at a stage where it is virtually invisible, having be-
come a natural part of our life. New worlds open up as boundaries disappear.
If technology itself sets the preconditions for survival of existing species and
ecosystems, as well as the development of new and artiﬁcial ones, then this
becomes an inseparable part of our experience of nature.
In addition to this shaping role, another aspect of technology is now coming
to the fore, for it is deployed in a manner allowing for chance and stimulating
self-organisation. Our work shows developments in the ﬁeld of Artiﬁcial Life
and Artiﬁcial Evolution in the form of machine generative systems, aiming for
unpredictable behaviour and variety of form. This revives a century old desire:
“to bring a work of art to life”. These developments in art, encompassing tech-
nology, provide the contemporary phase of our culture with new possibilities
of experiencing nature. As artists this challenges us to enlist vivid, artiﬁcial
worlds in alliance with technology, and to design them in so reﬁned a manner
that they may awaken new experiences of beauty. Not merely in relation to
the external forms themselves, but also in terms of the underlying processes
which create it all.
5.2 A Brief Review
To sketch the background to our interest in Artiﬁcial Life and Artiﬁcial Evo-
lution we will pay a brief visit to the end of the eighties and the beginning
of the nineties. At the time, still studying at Rijksakademie Amsterdam, we
found ourselves in a situation that forced us to question art practice. We were

102
Driessens and Verstappen
under the impression that a work of art seemed to be primarily a strategic
instrument guaranteeing the continuity of the institutionalised art establish-
ments. New artworks had to be shown every month, and production had to be
kept up. The journals gave the most extensive and favourable reports to those
galleries and art institutions buying big expensive pages of advertisements in
their magazine. The so-called new and interesting seemed to be strongly inter-
twined with mutual commercial interest. We concluded that the art world was
a generative system maintaining itself. In addition, post modernism stripped
us of illusions, leaving us with the view that nothing is possible beyond the
appropriation of images from other cultures and from the past: repetition, re-
combining, eclecticism, the end of history [2]. At the same time, we became
interested in the theories of chaos and complexity. Scientists had discovered
that through complex matter-energy ﬂows, order could arise spontaneously
from chaos. Under the right circumstances this dynamism allows new wholes
to arise, which are more than just the sum of their various parts. Small modi-
ﬁcations may sometimes lead to a dramatic turn around that is unpredictable.
In spite of the fact that chaos research attempts to explain the origin of what
is new, it remains uncertain what is going to occur in the future [10,13]. As
artists, we experienced that the associative tendency of our human mind may
disturb the spontaneous development of new possibilities. By following our
intuition, we often close oﬀavenues that might have led to interesting results.
In this context the idea of automating art production arose. This would
expose the underlying generative mechanism of the art world, and it would cir-
cumvent the cultural and the biological limitations of human art at the same
time. Initially this idea was a rather nihilistic response to the powerless situ-
ation in which we seemed to ﬁnd ourselves. However, this point of departure
rapidly became an adventure when we realised how diﬃcult and at the same
time challenging it is to attempt an art in which spontaneous phenomena are
created systematically, art that is not entirely determined by the subjective
choices of a human being, but instead is generated by autonomously operating
processes. These are self-organising processes able to create an endless ﬂow of
surprising results.
With the above in mind, we began to develop generative systems in which
we attempted to formalise processes of creation. In addition to working with
physical and chemical processes generating morphological transformations, we
use the computer for the development of artiﬁcial worlds with self-organising
properties. We wish to see what happens if you describe all the aspects of the
development and growth processes. Not simulations of laws that are valid in
our physical world, but instead the deﬁnition of an artiﬁcial nature,1 with ﬁc-
1 The concept “Nature” is often used in the meaning of unspoiled land, an area
that has not been touched or cultivated by Man. In a broader sense, however,
nature means “disposition” or “character”. From this point of view, Nature is
the expression of the underlying laws that shape a world or entity. These funda-
mental laws of nature determine which development, growth and transformation
processes are possible, and which are not.

5 Natural Processes and Artiﬁcial Procedures
103
tional laws constituting a complete world of its own: a parallel universe with
intrinsic creative principles and spontaneous expressions. These generative
systems are able to create very detailed images and forms, without precisely
determining a priori how they will look and without attributing a speciﬁc
meaning to them. We discovered that our approach ﬁts the broader research
ﬁeld of Artiﬁcial Life. But whereas A-life science emphasises “knowing and
understanding” of complex behaviour and self-organisation, we are more in-
terested in “making and becoming”, to give expression to the desire for an
activity exploring the unseen, the unimagined and the unknown [16].
The speciﬁc properties of the computer enable it to be equipped with an
autonomous, non-human way of “thinking”. Unlike the associative tendency
of the human mind, most software to date has no built-in preference for one
solution or another. It is able to research the whole spectrum of possibilities
consistently and systematically. Yet not all solutions within this inﬁnite uni-
verse of possibility are equally interesting to human beings. Just as in the
world around us, these artiﬁcial worlds have fascinating, strange but also te-
dious phenomena. We therefore need a powerful mechanism to enable us to
explore the ﬁeld of possibilities and which will enable further development of
areas potentially interesting to us. In the world around us the process of evo-
lution takes care of the multiformity of biological life and the creation of new
species from old ones. The species become increasingly better adapted to the
changing ecosystem by a process of mutation and selection. This is a mech-
anism that we can use. If we can link a mechanism of artiﬁcial evolution to
a morphogenetic system (morphogenesis is the structural development of an
organism) then we have an eﬀective manner of giving direction to the image
generating properties, without pre-determination of how the results deriving
from this are going to look.
In the following, we will go into a few projects and describe the changing
role of artiﬁcial evolution in relation to various morphogenetic processes. Two
works that are more involved with the observation of existing processes intro-
duce these projects. The theme of artiﬁcial evolution will emerge in another
manner here.
5.3 Carrots and Tubers
Nowadays, science and industry are able to develop new and more attrac-
tive crops via direct intervention in genetic material, but the role of industry
and technology have inﬂuenced the development of new biological species and
variants for far longer than this. In particular, the species used for large-scale
food production have been subject to an evolutionary process spurred on by
industry for a long time, with far-reaching control of environmental factors.
While the natural process of evolution usually engenders multiformity and
diversity so that species remain strong and adaptive, industry manoeuvres
the process as much as possible in a direction of eﬃciency, uniformity and

104
Driessens and Verstappen
(a) Morphotheque#8
(b) Morphotheque#9
Fig. 5.1. Morphotheque: collection Anne Marie and S¨oren Mygind, Copenhagen
homogeneity. This results in many current species being extremely sensitive
to disease and degeneration; science then subsequently develops technologi-
cal solutions for this. In short, species development and species preservation
(including humanity) are strongly correlated with technological management
and monitoring. This technology driven reciprocal dependency has become
our natural condition [12]. Morphotheque #8 (1994) and Morphotheque #9
(1997) (see Fig. 5.1) are two works reﬂecting this topic. Morphotheque #8 is
a collection of 28 elements, 1:1 copies of potatoes (cultivar Jaerla) executed
in painted aluminium. Morphotheque #9 is a collection of 32 elements, exact
copies of carrots executed in painted plaster.
Wild potatoes have relatively large diversity of form. Form characteristics
are in part genetically determined, but environmental factors (climate and
soil structure) inﬂuence the ultimate form at least as strongly. The indus-
trial potato has become rather uniform due to Man’s continual selection and
cultivation regarding form. That is also the intention, for peeling irregular
potatoes is impractical and even more important: it is very uneconomical.
Potatoes for consumption are reproduced via clones to ensure product char-
acteristics remain constant. In the northern European industrial agriculture
there are strict checks on the growth quality of the seed-potatoes. The soil
structure is also maintained as homogeneously as possible and artiﬁcial fer-
tiliser ensures suﬃcient nutrients. Pesticides and chemicals ensure that no
other species can inhibit the culture of the potato plant and its tubers [19]. In
spite of that control, some tubers or roots do escape this imposed uniformity.
These amusing and suggestive forms stimulate our imagination. Local irregu-
larities in soil structure among other things (stones, roots or other tubers) or
sudden climate change (including a period of too much or too little moisture)
may cause these irregular forms of growth. Other causes are possible mutation
in the genes or accidents in the process of growth, which is not yet fully un-
der control. Because these diﬀerently formed products do not meet standards,
they cannot be sold to consumers, and we are therefore never allowed to see

5 Natural Processes and Artiﬁcial Procedures
105
these variants in form. If by accident this does in fact happen, then they are
associated with pathological deviation, disease, degeneration and ugliness. A
farmer selling carrots and potatoes at an Amsterdam market experienced this
as provocation when we asked him if he could put the irregular, strangely
formed variants aside for us: “How can you even think that I would grow
inferior and malformed products!” Normally speaking, these deviant growths
disappear into potato starch or they serve as cattle fodder. Sorting takes place
in large distribution centres and on location, we made a selection out of a great
number of rejected products, representing the variety and diversity in form
within the species. We thought it was important to record the forms in their
three-dimensionality, and not only by means of photography. For this reason a
copy was made of each form in a durable material and it was then painted with
acrylic paint so that the colour impressions would also match the original.
We also researched whether the diversity in shape of the Morphotheque #8
potatoes was in fact determined by external factors. The following year we
planted the potatoes as seeds in a piece of land, where they grew under circum-
stances comparable to those in the industry. Without exception, this yielded
“normal”, round progeny. We suspect that a varied environment and variable
circumstances may stimulate diversity in form, but we did not carry out the
experiment in reverse (planting uniform seeds in an uneven soil, no removal
of weeds, no spraying and fertilising, etc.). In Morphotheque #9, one of the
carrots stands out immediately. Here it is the straight form that is diﬀerent,
and in fact it is the only carrot from the supermarket.
5.4 Breed
Is it possible to breed industrial products via a technological route whereby
multiformity is achieved instead of uniformity? In 1995, we began the project
titled Breed with this question as a starting point, which ultimately led
to a system that automates both the design and the production of three-
dimensional sculptures. Breed makes a “mass production of unique artefacts”
possible, with a minimum of human intervention.
To attain multiformity without designing each individual form in detail, we
focussed on a system – in fact a recursive cellular automaton – able to generate
a large number of morphogenetic rules independently. The rules prescribe
how a detailed spatial form arises from a simple elementary form, or rather
“grows”. However, not all rules lead to a spatial object that is executable as
sculpture. The system therefore comprises an evolutionary component that
seeks solutions for this problem completely independently.

106
Driessens and Verstappen
Fig. 5.2. Breed, stages of morphogenic development
5.4.1 Morphogenesis
The underlying principle of the Breed morphogenesis is division of cells.2 One
initial cell, a cube, engenders throughout successive stages of cell division a
complex, multi-cellular “body”. Morphogenetic rules determine how the divi-
sion of a cell occurs, dependent on its situation between the cells surrounding
it. Every potential situation has a separate rule, so a cell surrounded on all
sides by other cells may divide diﬀerently from a cell that only has a neigh-
bour on the left and underneath, or a cell with nothing at all in the vicinity,
et cetera. Each rule is coded in a gene, and the complete set of rules forms
the genotype of the growth.
A parent cell divides by halving the cubic space that it takes up according
to length, width and depth. This creates eight smaller spaces, each of which
either contain a child cell or remain empty, according to the rule applicable to
the parent cell. The new child cells again function as parent cells during the
following division stage, the empty spaces remain empty. Each division reﬁnes
the cells and diﬀerentiates the form further, until growth ceases after a number
of stages (see Fig. 5.2). The ultimate form, the phenotype, is not speciﬁed at
a high level in its genotype, but is the result of the recursive application
of simple rules on the lowest organisation level, that of the cell. A spatial
characteristic is thus not directly speciﬁed, but is the result of a local chain
reaction of events. The ultimate appearance of such a virtual Breed object
may take many forms (see Fig. 5.3). The majority of these are comprised of
many parts ﬂoating separately from each other in space. This is no problem
while it remains a computer model on the screen where gravity does not count,
but turned into real material, subject to gravity, such an incoherent structure
would collapse. If we wish to produce such a computer designed object, then
it must form a physical whole. If the model is to be feasible as sculpture
there can be no loose cells, or groups of cells that do not connect to the
rest. When the morphogenetic system was eventually operational, we made
protracted experiments with randomly composed genotypes, in order to see
which phenotypes this would produce. By doing a large number of samples
we realised how huge the room for possible forms of manifestation was, and
2 We often borrow terms from the domain of biology such as organism, cell, geno-
type, phenotype, ecosystem, morphogenesis and evolution, but we use these con-
cepts in an abstract manner. This concerns three-dimensional forms in the case
of Breed, thus volume-element or voxel is intended by the term cell, and voxel-
multiplication process by cell division.

5 Natural Processes and Artiﬁcial Procedures
107
that only a smaller section of that space met the conditions of constructibility
we considered necessary. Yet this smaller section would still be able to run to
an enormous number, far too great to generate within a human life, let alone
examine.
(a) Breed 1.2 #e234
(b) Breed 1.2 #e365
(c) Breed 1.2 #f315
Fig. 5.3. ProMetal models, approx. 80 × 80 × 80 mm, stainless steel/bronze.
Courtesy gallery VOUS ETES ICI, Amsterdam
5.4.2 Selection and Mutation
To automate the search for constructable results, it is necessary to establish
objective and measurable preconditions for constructibility. One crucial con-
dition is that the phenotype should be completely coherent, i.e. consisting
of a single part. The computer can compare two models with each other by
counting the number of separate parts the model contains. The model with
fewer parts satisﬁes the criterion of “coherence” more, or rather is “ﬁtter”
than the model with the higher number of parts. We are now able to imple-
ment a computerised process of trial and error that incrementally evolves in
the direction of potential solutions. The simplest method is already eﬀective:
take a randomly composed genotype as base, generate the phenotype and test
it for ﬁtness; mutate the base genotype, generate the phenotype and test it
for ﬁtness; compare both results with each other, and take the result with the
highest ﬁtness as the new base. Repeat the mutations until the result satis-
ﬁes the stated requirements. It is not of course certain that this process of
evolution will always lead to a solution of the constructibility problem. This
minimal mechanism (the two-membered evolution strategy) has the character-
istic of ﬁrmly clinging to a turning it has already taken. Favourable mutations
in general become rarer, and sometimes even impossible, to the extent that
ﬁtness increases. As Mitchell Whitelaw put it: “... the structure of the evolu-
tionary process mirrors the morphogenetic process; rather than searching for

108
Driessens and Verstappen
a single absolute goal, this simple stepwise evolution uses only a local compar-
ison. As it forms a sequence of incrementally ﬁtter forms, the process paints
itself into a corner; the ﬁnal optimal form is in fact only the most optimal
form that the speciﬁc sequence of random alterations has produced.” [17]
Breed carries out one evolution after the other, always from a diﬀerent
initial genotype. The number of mutations per evolution is limited; if no suit-
able solution presents itself within the maximum number of mutations allowed,
Breed begins a new evolution from a subsequent starting point. By approach-
ing the problem from diﬀerent sides, all sorts of solutions reveal themselves.
The ﬁrst evolution experiments had a surprising result. It was apparent that
virtually all constructable solutions consisted merely of one single cell (the
path of least resistance also seems to occur here). This unexpectedly simple
result, however valid, did not meet our attempt at multiformity! We therefore
added a “volume” and “surface area” criterion alongside the “coherence” cri-
terion. All these criteria determine the objective characteristics of the object,
but at the same time leave open how the form is going to turn out. We only
ﬁx essential limits, yet there are billions of possibilities within this.
5.4.3 Selection and Execution
Breed evolves forms based on a ﬁxed set of selection criteria, that remain the
same during the run of the process or in the subsequent breeding sessions.
Innumerable forms of manifestation may show up. Some forms appeal more
to the imagination than others, but this plays no role at all within the system.
Evolution has the role of constructor here, taking no account of the aesthetic
quality of the result. Ultimately, we speciﬁed which models should or should
not be produced based on our personal preferences. We paid attention to
signiﬁcant diﬀerences in form, style and structure in the selection – just as for
the Morphotheques – in order to reﬂect the form diversity of the system.
An initial series of six sculptures was executed manually in plywood. The
tracing, sawing and gluing was a time-consuming technique and in any case,
we wanted an industrial procedure. It was not possible to produce such com-
plex forms via computer controlled Rapid Prototyping techniques until the
end of the nineties. Only with the arrival of the SLS (Selected Laser Sinter-
ing) technology did it become possible to computerise the whole line from
design to execution. A second series of nine samples was realised with this
technique in DuraForm nylon. Recently, a series of six sculptures were pro-
duced with the ProMetal technique. These objects are made out of stainless
steel, inﬁltrated with bronze. The results became smaller and more detailed
so that the separate cells of the objects began to be absorbed into the total
form.

5 Natural Processes and Artiﬁcial Procedures
109
5.5 E-volver
We wondered if we could design an artiﬁcial evolutionary process that would
not be evaluated based on a set of objective criteria hard-coded within the
system, but on the basis of diﬀerent subjective selection criteria introduced by
user interaction. In this way one obtains a very open system, where one can
explore a great many diﬀerent paths within the gigantic realm of possibilities.
The project E-volver has expanded this idea into a working system. E-volver
encompasses an alliance between a breeding machine on the one hand and a
human breeder on the other. The combination of human and machine prop-
erties leads to results that could not have been created by either alone. The
cultivated images show that it is possible to generate lively worlds with a high
degree of detail and often of great beauty.
In this system, a population of eight virtual creatures is active within the
screen to construct an ever-changing picture. Each pixel-sized creature is an
individual with its own characteristic behaviour that literally leaves its visi-
ble traces on the screen. Because each creature is incessantly interacting with
its environment, the picture remains continually changing. Figure 5.4 shows
snapshots of such pictures. Artiﬁcial evolution is the means used to develop
the characteristics of the individual creatures and the composition of the pop-
ulation as a whole. The user directs the process based on visual preferences,
but they can never specify the image in detail, due to the patterns and unpre-
dictabilities within the system. We will now give a detailed description of the
system, ﬁrst of the morphogenetic and then of the evolutionary component.
5.5.1 Generative Process
The E-volver software generates a colourful dynamic ﬂow of images in real-
time. The successive images are cohesive because each new image only diﬀers
a little from the previous one. Due to this rapid succession of separate images,
like frames in a ﬁlm, you experience continuity of movement. In contrast to
a ﬁlm (where the images once ﬁlmed and recorded can be repeated over and
over again), there is no question of ﬁlming and recording E-volver – for the
images are created at the moment of presentation. E-volver will never repeat
earlier results; it always exists in the present and gives a immediate, life-like
experience. In order to generate images that are coherent in space and time,
we use a development process that is also active in space and time. In this case,
the space consists of a right-angled grid of “picture elements” corresponding
to the pixels of the screen. The essential characteristics of the picture elements
are their individual colours and positions in the grid. Time, just like space, is
divided up into equal pieces. Time passes in discrete steps, each as short as
the other. Thousands of such time steps pass per second.
The pixel grid forms the environment – the habitat – of eight artiﬁcial
“monocellular” creatures, who each have the size of a pixel. They are simple,

110
Driessens and Verstappen
Fig. 5.4. E-volved culture, two stills, inkjet on canvas, 600 × 300 cm. Courtesy
LUMC, Leiden and SKOR, Amsterdam
autonomous operating machines, mobile cellular automata, which operate ac-
cording to a ﬁxed, built-in program. The creatures are able to change the
colour of the pixel they stand on, and move to a neighbouring pixel. Their
program describes how they do this, dependent on local circumstances: colours
from the immediate vicinity. Each creature is provided with its own control
program, endowing it with a speciﬁc and individual behaviour, reminiscent of
Langton’s work [8]. During each successive time step all the creatures execute
their program, one after the other; they change their environment by leav-
ing visible traces in their habitat. Innumerable scenarios for interaction and
emergence are possible because one creature’s traces can inﬂuence the other
creature. The habitat functions here as a common buﬀer, which holds history
and shapes the present with it. The collective activity of all the creatures in

5 Natural Processes and Artiﬁcial Procedures
111
the habitat forms an artiﬁcial ecosystem, developing gradually in time, and
constantly brought into view. In principle, all the actions of the creatures are
visible; E-volver does not employ any complicated visualisation techniques or
hidden layers; the image plane is the world itself and not a representation of
something else. The image plane, the habitat, is unlimited in the eyes of the
creatures. When a creature bursts through the edge of the plane, it reappears
on the opposite side. All the picture elements are equal to the creature in
the sense that they all have a complete environment: each pixel is surrounded
by eight neighbouring pixels (Moore neighbourhood). There are no edge or
corner pixels impeding its perception and movement – its notion of space is
continuous.
At a superﬁcial glance, pixels on a screen each have a colour. On closer
inspection, however, we see that each pixel is actually constructed from three
smaller sub-pixels, a red and a green, and a blue one. By controlling the
intensity of the three sub-pixels, a pixel can arouse the impression of any
colour at all. RGB (Red, Green, Blue) is therefore the standard for machines
producing coloured light, such as monitors and projectors, and a standard for
colour speciﬁcations in computers. It is not so much the RGB components that
are important to human perception and the artiﬁcial beings in E-volver, as
the interpretation in the components Hue, Saturation, Lightness (HSL). These
elementary properties of colour are perceptually more relevant, because they
are more revealing than the RGB values. E-volver deploys both colour models:
HSL for the actual habitat, RGB for its reproduction in coloured light. Both
colour models can easily be converted into each other arithmetically. In this
habitat of coloured pixels the creature exists. The creature is able to perceive
its immediate environment – in the form of HSL colour components. The
creature is able to alter the colour components at its present standpoint, and
take a step towards a neighbouring pixel. It is a simple creature, its state is
only given by a colour, a position, and a direction of looking and moving. The
memory of the creature does not extend further than the colour and direction
it had during the previous time step. Its control program propels the creature;
this determines what the creature’s following state will be, given the present
state and the local context. The control program converts input into output
within a single time step. The input is the creature’s present colour, position
and direction, and the colour of the underlying and bordering pixels. The
output is the creature’s new colour and direction. A control program is thus
constructed of two parts, one part taking care of the colour determination,
and one part deciding on the direction to be taken. One random example of
a control program in pseudo-code follows:
•
Colour determination:
–
view the light values of the surrounding eight pixels;
–
choose the value that most resembles your current light value, and
adopt it;

112
Driessens and Verstappen
–
if the value is greater than the light value of the pixel you are on, then
increase your own light value;
–
mix your hue and saturation with the hue and saturation of the pixel
you are on.
•
Direction determination:
–
view the saturation values of the surrounding pixels;
–
choose the least saturated pixel, and turn in its direction;
–
if this least saturated pixel is as saturated as your own colour, then
make a 90 degree turn to the left.
The creature subsequently colours the pixel it is on in its new colour,
and jumps to the following position in its new direction. This winds down all
activities in this time step, and the creature is ready for the following time
step, that only begins when all other creatures in the habitat have had their
turn.
We do not write the control programs for E-volver creatures ourselves, the
computer automatically designs and assembles them from building blocks. The
building blocks are code fragments forming a working whole by linking them
up together, somewhat like Koza’s genetic programming [6]. The building
blocks are simple for they encode basic mathematical operations and ﬂow
control. There are diﬀerent types of building blocks, and each type in its
turn has diﬀerent sub-types. For example, there is a type of building block
to extract a colour component value from the environment. Each sub-type
of this “extractor” performs this operation in a speciﬁc way: one taking the
minimum, one taking the maximum, one taking the average, one taking the
median, one taking a value randomly, one taking the value most similar to
the current creatures component value, one taking the least similar, et cetera.
Each creature has a set of thirteen genes that prescribe which building blocks
are to be used, and how these will be combined. This whole set of thirteen
genes, the genotype, is a creature’s compact symbolic building scheme.
Two pipelines are constructed, one for the speciﬁcation of the new colour,
and one for the speciﬁcation of the new direction. In the present version of
E-volver each pipeline is constructed out of four building blocks at most, each
responsible for a stage of processing. Further, a pipeline is accompanied by a
building block that speciﬁes which of the colour components (hue, saturation
or lightness) serve as the input for the processing stages. Finally, there are
three more building blocks in eﬀect, which potentially modify the operation
of every building block forming the pipeline. In the present version, E-volver
uses more than a hundred diﬀerent sorts of building blocks when compiling
the artiﬁcial creatures’ control programs. In total, there are 2859,378,278,400
diﬀerent possible programs for controlling a creature. E-volver therefore has a
wide choice when creating a single creature; when putting together an ecosys-
tem where several creatures are present next to each other the choice is even
considerably greater. For a community of eight creatures the number of pos-
sibilities increases to 4.46861e+83, a number composed of 84 digits.

5 Natural Processes and Artiﬁcial Procedures
113
5.5.2 Evolutionary Component
Evolution is in essence a stochastic process of trial and error – nothing has
been decided beforehand about where it leads to. Evolution is not necessarily
progressive and does not lead per se to ever greater complexity. Evolution is,
however, accumulative. A favourable step taken is a basis for further devel-
opment. As long as variants are still to hand of which the “worse ones” (less
adapted) are suppressed, or rather the “better ones” (more adapted) are stim-
ulated and as long as variants shape more variants of themselves, a directed
development will occur – an evolution.
The E-volver software is divided into a generative component, that in prin-
ciple oﬀers as large a space of potential cultures as possible, and an evolution-
ary component that within that space concentrates attention around ﬁelds of
special interest. The human being standing outside the software system forms
the link between the two components. He/she is the one who evaluates and
selects the output of the generative component, and thus furnishes the evolu-
tionary component with input. The human being functions as a breeder who
continually makes a selection from a diverse supply of results. The system
itself does not prefer speciﬁc cultures – it remains impartial here. In contrast,
a spectator has the tendency to prefer speciﬁc cultures above other ones and
is thus able to make personal choices from the supply of various cultures.
The reasons for these choices are of no signiﬁcance to the system. We only
posit that cultures similar to cultures previously chosen have more chance of
re-selection than random other cultures. This point of departure oﬀers the
possibility of exploring the enormous potential of cultures in an individual
manner, without tracing demarcated paths beforehand. Repeatedly selecting
from a varied supply puts an evolution into motion that can lead to the devel-
opment of cultures increasingly ﬁtting the preferences, interests, sensitivities,
aﬃnities and peculiarities of the spectator/user. In this way, the system is
reminiscent of Dawkins’ Blind Watchmaker program [3].
Four separate cultures are presented on the screen at the same time, as
in Fig. 5.5, over and over again. Each culture consists of a habitat containing
a community of eight creatures. By picturing the cultures next to each other
an immediate visual comparison is possible. You can end a culture at any
moment, after which a new culture is initiated automatically. The ending
of one culture, also signiﬁes the “survival” of the three other growths. Each
time that a culture survives a selection it earns a point, and its ﬁtness score
increases. Growths that are terminated quickly have a low score, cultures that
keep going for a long time collect many points. Each culture tried out is added
to a database that records the culture’s name, the ﬁnal ﬁtness score, and the
genotype of each creature in that culture. The evolution component uses this
data when it puts together new cultures. Characteristics of higher scoring
cultures have a greater chance of being deployed than characteristics from
lower scoring cultures.

114
Driessens and Verstappen
Fig. 5.5. E-volver, touch screen interface
The properties of a culture are spread over two levels. The higher level, of
the culture as a whole, is characterised by the speciﬁc collection of creatures
that form a community. The lower level, of the individual creature, is charac-
terised by the speciﬁc composition of its genotype. Properties of both levels
may be involved when composing a new culture, thus variation can arise both
in the composition of the community, and in the composition of the genotypes
of the creatures that form a community. In order to come to a new culture,
E-volver uses three sorts of evolutionary operators: mixing, crossing and mu-
tation. “Mixing” composes a new culture from creatures that have already
appeared in previous cultures. It chooses a number of cultures, proportional
to their ﬁtness score, and makes a random selection of creatures from these
cultures. These are then assembled in the new culture. “Crossing” composes a
new culture from an earlier culture, chosen in proportion to its ﬁtness score. It
crosses a small number of creatures from the culture by cutting their genome
in two pieces, exchanging the parts with each other and assembling them
again into a complete genome. “Mutating” composes a new culture from the
creatures of an earlier culture, whereby the genome from some creatures is
subject to variation. Each gene from the genome from a mutating creature
has a small chance of randomly changing into another possible variant of this
gene. By applying these three evolutionary operations to already evaluated
cultures, E-volver is able to constantly supply new variants for evaluation and

5 Natural Processes and Artiﬁcial Procedures
115
selection, variants that continue to build on selected properties but at the
same time also introduce new properties.
At the start of a new E-volver breeding session no evaluations are known
yet, and so no evolutionary operations can be performed. A breeding session
therefore begins with a “primordial soup” phase. Cultures are formed out
of creatures that each have a completely randomly composed genome. Only
after a large number of these “primordial soup cultures” have been developed
and evaluated, and the E-volver database is ﬁlled with a range of cultures
with varying ﬁtness scores, does the system begin to apply the evolutionary
operators in order to evolve further on the basis of the above.
5.5.3 Cultures and Interpretation
The E-volver creatures are embedded in their habitat, which inﬂuences the
creature’s behaviour; the behaviour of the creatures in turn inﬂuences the
habitat. The creatures have no notion of each other’s existence, and do not
enter into direct relationships with each other. Nevertheless, because all eight
creatures share their habitat with each other and give it form, a stigmergic
network of indirect relationships arises. Here the habitat functions as the re-
ﬂection of a shared history and as an implicit communications medium for
all the creatures present. Each event that leaves its traces may later lead to
other events [4]. The habitat, which is homogeneously grey at the start of
each new culture, develops in the course of time into a heterogeneous whole.
It is a dynamic and diﬀerentiated landscape that, in spite of its complexity,
still forms an organic unity. The evolution is not restricted to the evolution
of separate artiﬁcial life forms, but particularly includes the evolution of the
culture as a whole. It is not an evolution of one individual in isolation, but
a co-evolution of individuals within their mutual connection. So when we
evaluate a culture, we do not so much evaluate the individual creatures, the
habitat at a speciﬁc moment of development, but rather the continuing in-
terplay between them. The selection criteria applied have a bearing on how
the dynamic of the culture allows itself to be read and interpreted as a visual
performance. The colourful abstract animations arouse associations with mi-
croscopic observations, rasters, geological processes, noise, cloud formations,
modern art, fungus cultures, organ tissues, geometry, satellite photos etc., but
ultimately they still avoid any deﬁnitive identiﬁcation. The pixel creatures are
in fact elementary image processors. Klaas Kuitenbrouwer puts it thus: “The
system can also be interpreted as a machine which does nothing more than
present the process of representation itself. Imaging without an object: we see
a mutating process of visualisation, in which local image features operate and
feedback on themselves. . . . Another way to appreciate the work is as a system,
an entirety of relationships between forces and processes. The beauty is then
in the amazing connection between the simplicity of the pixel creatures and
the complexity and endless variation that they create. The images therefore
look like living processes because similar processes are actually taking place

116
Driessens and Verstappen
in the work! . . . The work as a system automatically includes the viewers, who
have a very important role to play in the whole thing. It is they who give
the work its direction, who appreciate the diﬀerences that occur, and thus
give it meaning. . . . However you view the work, whatever way you choose to
interpret it, there will be diﬀerent reasons for deleting the images. The choices
made by the viewer will also select the type of meaning and these meanings
are made manifest in the work by the viewer’s choice.” [7]
5.6 Directed Versus Open
While we applied artiﬁcial evolution in Breed as an optimising technique, in
E-volver it has become an inherent content aspect of the work. The aim of the
evolutionary process is not described, but any imaginable ﬁtness criterion can
make surprising and signiﬁcant properties of the system visible. Through the
evolution process the users of the software become conscious of the inﬂuence
they have on the development of the generative system, because personal pref-
erences are progressively reﬂected in the changing visual patterns. This in itself
is not new (for example Richard Dawkins’ Biomorphs [3], Karl Sims’ Genetic
Images [14] and William Latham’s Evolutionary Art [15]). It is particularly
the linking of artiﬁcial evolution to a powerful and elegant morphogenetic
process that throws new light on our quest for a system of self-organisation
that is as open as possible. A user of E-volver confronted with the continually
changing patterns several times a day over a long period, builds up a profound
relationship with the work. In the ﬁrst instance, the spectator is inclined to
impose his personal taste on the system. Gradually we see that fathoming of
the generative system more and more interferes with taste and judgement.
We discover which sort of images occur more generally and which are rarer.
It can indeed become a challenge to develop and sublimate a more unusual
phenomenon, by adapting the evolutionary strategy that was used. However,
what was initially exceptional becomes ordinary again after a little while, and
thus attention keeps moving, always in the direction of the unseen and the
unknown. The selection criteria themselves are thus subject to change, they
co-evolve in interaction with the E-volver system.
The initial idea of automating art production, “art that is not made or de-
veloped by the subjective choices of the human being but is instead generated
by independently operating processes”, does not have to exclude human sub-
jectivity completely. We have established a collaboration of typically human
characteristics and the speciﬁc qualities of the machine.
5.7 Generative Art and Present Day Experience of
Nature
Ultimately, it is important that the results of our research be presented in
order to communicate with the public. When presenting Breed we prefer to

5 Natural Processes and Artiﬁcial Procedures
117
show a number of materialised objects next to each other. This enables the
objects to be compared with each other, seeing the similarities and diﬀer-
ences. They clearly form a close family with their own characteristic style
features. In the ﬁrst instance, the constructions remind one of enlarged mod-
els of complicated molecules or crystals. Or the reverse – scaled down models
of a grotesque futuristic architecture. But usually, it becomes progressively
clearer that a process that is not immediately obvious, must have been at
work here. For a human being would not be easily inclined to think up some-
thing so complex, and neither does the Nature we know produce this sort of
technical construction as a matter of course. So there is conjecture, or at least
it sets people thinking. Even if the creation process cannot be perceived in
detail in the static ﬁnal results, it does give rise to speculations about the pro-
cess underlying it all. A small series of objects is often suﬃcient to summon
the potential of possibilities indirectly.
Fig. 5.6. E-volver image breeding unit installed at LUMC Research Labs, Leiden.
Photography Gert Jan van Rooij
In 2006, E-volver was installed in the form of an interactive artwork in the
new building of the LUMC Research Labs in Leiden (see Fig. 5.6). Research
within the laboratories is concentrated on Human Genetics, Clinical Genetics,
Anatomy, Embryology, Molecular Cell Biology, Stem Cells and many other ar-
eas of medical DNA research. Four interactive image-breeding-machines are
to be found in the common areas of the laboratories, which the scientists use
to direct a collective evolution process via a touch-screen. If desired, one can
log on to a personal culture environment and perform an individual evolution

118
Driessens and Verstappen
process. Each breeding unit has a large wide-screen monitor, where you can
see the cultures that score highly. From time to time, these top cultures are
replaced, as the process produces even higher scores. The evolution is termi-
nated after a ﬁxed number of selections, and the whole process starts again
from the beginning – a primordial soup once again, where new possibilities lie
enshrined which may subsequently reach expression. Five large prints hang in
other places in the building, random snapshots from the most unusual growth
processes that have been developed at high resolution. The permanent instal-
lation E-volver in the LUMC allows the scientists to experience by means of
interaction how intriguing images can evolve out of a primordial soup of dis-
connected lines and spots. The underlying generative processes have full scope,
with the scientists in the role of force of nature practising pressure of selection.
Wilfried Lentz, director SKOR has said “While the scientists are concerned
with the biochemistry, genetics and evolution of biological life, the artists ad-
dress the potential oﬀered by the underlying mechanisms of these processes for
art, by implementing them as a purely visual and image-generating system. In
this way, E-volver mirrors the complexity of the biochemical universe and hu-
mankind’s desire for knowledge and understanding. Self-organising processes
such as growth and evolution – which from a theoretical point of view may
be well understood, but which in daily life are never directly observable – can
with E-volver be experienced at a sensory level. It is at the same time a source
of great aesthetic pleasure.” [9]
5.7.1 Concrete Approach
The visual structures of Breed and E-volver do not represent anything but
they arise from a logical and direct use of the medium and its formal visual
means. This testiﬁes to an approach that is related to concrete art, which has
its origin in early Modernism. The artwork itself is the reality. Jean Arp said
“We do not wish to copy Nature; we do not wish to reproduce, but to produce.
We want to produce as a plant produces its fruit. We wish to produce directly,
and no longer via interpretation. . . . Artists should not sign their works of con-
crete art. Those paintings, statues and objects ought to remain anonymous;
they form a part of Nature’s great workshop as do trees and clouds, animals
and people . . . ” [1] Concrete art does not reject the increasing prevalence
of technology and industrialisation, but it wishes to provide new times with
a ﬁtting visual language. We share this approach but in contrast to mod-
ernistic artwork – that attempts to reveal the underlying harmony of reality
by rational ordering and reduction of visual means – we are actually striving
for complexity and multiformity in the ﬁnal result. In our case the reduc-
tionist approach is not manifest in the visible results but on the other hand,
we do use a form of minimalism in the design of the underlying generative
process. The harmony model has been replaced in our case by the convic-
tion that chance, self-organisation and evolution order and transform reality.
Thus, new scientiﬁc insights and technologies contribute to a continuation

5 Natural Processes and Artiﬁcial Procedures
119
and adjustment of older modernistic and expressionistic ideals. The concrete
and formal approach can now enter into a union with new possibilities of viv-
iﬁcation – viviﬁcation in the sense of bringing a work of art literally to life.
These new possibilities are enshrined in the procedural character of the com-
puter. By developing artiﬁcial life/evolution programs we unlock worlds each
having their own generative principles and spontaneous expressions. These
expressions make visible the internal structures of an artiﬁcial nature in all
its variation, with a high degree of detail and often of great beauty.
5.7.2 Computational Sublime
Our A-life/evolution projects contribute to a new possibility of experiencing
Nature. Jon McCormack introduced the term “the computational sublime”,
during the Second Iteration conference in Melbourne in 2001 [11]. It is an
important concept in relationship to our work. He states that an important
aspect of the sublime is the tension between pleasure and fear. Here, the plea-
sure is that we can be conscious of that which we cannot experience; fear
is that things exist too great and too powerful for us to experience. In the
dynamic sublime it is about the incomprehensible power of Nature [5], in the
computational sublime such feelings are aroused by an uncontrollable process
taking place in the computer. The underlying generative process is not directly
comprehensible but we are in fact able to experience it through the machine.
By real time generation and visualising of the internal processes, the spectator
can become overwhelmed by a sense of losing control, and at the same time
enjoy the spectacle of this artiﬁcial Nature. What Romantic Painting in the
nineteenth century could only arouse via ﬁgurative representation, can now in
fact be generated and experienced live with Artiﬁcial Life and Artiﬁcial Evo-
lution techniques. To quote Mitchell Whitelaw again: “Evolution, an idea that
has become the most powerful organising narrative of contemporary culture,
appears to unfold on a screen. A-life proposes not a slavish imitation of this
or that living thing but, at it strongest, an abstract distillation of aliveness,
life itself, re-embodied in voltage and silicon” [18, p. 5].
Acknowledgments
Thanks to The Netherlands Foundation for Visual Arts, Design and Archi-
tecture.
References
1. Arp, J.: Abstract art, concrete art. In: Art of This Century. New York (1942)
2. Baudrillard, J.: Les Strat´egies Fatales. Grasset & Fasquelle, Paris (1983)
3. Dawkins, R.: The Blind Watchmaker. W. W. Norton & Company (1986)

120
Driessens and Verstappen
4. Grass´e, P.: La r´econstruction du nid et les co¨ordinations interindividuelles. La
th´eorie de la stigmergie. Insectes Sociaux 6, 41–84 (1959)
5. Kant, I.: Kritiek der Urteilskraft (Critique of Judgement).
Lagarde und
Friederich, Berlin (1790)
6. Koza, J.: Genetic Programming – On the Programming of Computers by Means
of Natural Selection. MIT Press (1992)
7. Kuitenbrouwer, K.: E-volver. In: E-volver, SKOR, Foundation Art and Public
Space, pp. 7–22. Amsterdam (2006)
8. Langton, C.: Studying artiﬁcial life with cellular automata. Physica 22D, 120–
149 (1986)
9. Lentz, W.: Art and scientiﬁc research. In: E-volver, SKOR, Foundation Art and
Public Space, pp. 4–6. Amsterdam (2006)
10. Lewin, R.: Complexity: Life at the Edge of Chaos. Macmillan, New York (1992)
11. McCormack, J., Dorin, A.: Art, emergence, and the computational sublime. In:
Second Iteration – Proceedings of the Second International Conference on Gen-
erative Systems in the Electronic Arts, pp. 67–81. CEMA, Monash University,
Melbourne (2001)
12. Pollan, M.: The Botany of Desire. Random House (2001)
13. Prigogine, I., Stengers, I.: Order out of Chaos – Man’s New Discourse with
Nature. Flamingo (1984)
14. Sims, K.: Artiﬁcial evolution for computer graphics. In: Proceedings of SIG-
GRAPH, pp. 319–328. ACM Press, Las Vegas, Nevada (1991)
15. Todd, S., Latham, W.: Evolutionary Art and Computers. Academic Press (1992)
16. Whitelaw, M.: Tom Ray’s hammer: emergence and excess in A-life art. Leonardo
31(5), 377–381 (1998)
17. Whitelaw, M.: Morphogenetics: generative processes in the work of Driessens &
Verstappen. Digital Creativity 14(1), 45–53 (2003). Swets & Zeitlinger
18. Whitelaw, M.: Metacreation, Art and Artiﬁcial Life. MIT Press (2004)
19. van der Zaag, D.: Die Gewone Aardappel (That Ordinary Potato). Wageningen
University (1996)

6
Evolutionary Exploration of Complex Fractals
Daniel Ashlock1 and Brooke Jamieson2
1 University of Guelph, Department of Mathematics and Statistics, 50 Stone Road
East, Guelph, Ontario N1G 2W1, Canada dashlock@uoguelph.ca
2 University of Guelph, Department of Mathematics and Statistics, 50 Stone Road
East, Guelph, Ontario N1G 2W1, Canada bjamieson@uoguelph.ca
6.1 Introduction
A fractal is an object with a dimension that is not a whole number. Imagine
that you must cover a line segment by placing centers of circles, all the same
size, on the line segment so that the circles just cover the segment. If you
make the circles smaller, then the number of additional circles you require
will vary as the ﬁrst power of the degree the circles were shrunk by. If the
circle’s diameter is reduced by half, then twice as many circles are needed; if
the circles are one-third as large, then three times as many are needed. Do
the same thing with ﬁlling a square and the number of circles will vary as
the second power of the amount the individual circles were shrunk by. If the
circles are half as large, then four times as many will be required; dividing the
circle’s diameter by three will cause nine times as many circles to be needed.
In both cases the way the number of circles required varies is as the degree
to which the circles shrink to the power of the dimension of the ﬁgure being
covered. We can thus use shrinking circles to measure dimension.
For a ﬁgure whose dimension is to be computed, count how many circles
of several diﬀerent sizes are required to just cover the ﬁgure. Fit a line to the
log of the number of circles required as a function of their diameter. The slope
of this line is the dimension of the ﬁgure. For a line segment the value will be
close to 1, while for a square the value will be close to two. This procedure
approximates the Hausdorﬀdimension [9] of the objects. For Mandelbrot and
Julia sets, the fractals that are the focus of this chapter, the boundaries of
the sets will have dimensions strictly between 1 and 2. Examples of these
fractals appear in Figs 6.1 and 6.2. Objects as diverse as clouds, broccoli, and
coastlines exhibit the property of having non-integer Hausdorﬀdimension.
Fractals appeared in art before their deﬁning property of fractal dimension
was formalized. A nineteenth century painting, taken from a series of 36 views
of Mt. Fuji, depicts a great wave threatening an open boat. This picture
appears in [9] as an example of a depiction of a natural fractal. The wave
in the picture is an approximation to a self-similar fractal. This picture was

122
Ashlock and Jamieson
Fig. 6.1. An example of a view of the Mandelbrot set
painted long before the 1868 birth of Felix Hausdorﬀ, who is credited with
deﬁning the notion of fractal dimension.
Fig. 6.2. An example of a Julia set

6 Evolutionary Exploration of Complex Fractals
123
Fractals are sometimes taken as a type of art in their own right [1,3,4,6,7].
They may also be incorporated as elements of a piece of visual art [2,12,13].
Fractals are also used as models that permit nature to be incorporated more
easily into artistic eﬀorts [5,9,11]. Examples of evolution of fractals including
systems that use genetic programming [14] and which optimize parameters
of (generalized) Mandelbrot sets to generate biomorphs [16]. Fractals that
are located by evolutionary real parameter optimization to match pictures
of faces appear in [17]. Finally, fractals can be used as a way of making the
beauty of math apparent to an audience whose last contact with math may
have been in the context of a less-than-enjoyed class [8, 10]. The American
Mathematical Society’s web page on mathematics and art (http://www.ams.
org/mathimagery/) displays several eﬀorts in this direction.
This chapter explores fractals as a form of art using an evolutionary algo-
rithm to search for interesting subsets of the Mandelbrot set and for interesting
generalized Julia sets. The key problem is to numerically encode the notion of
“interesting” fractals. Using an evolutionary algorithm automates the search
for interesting fractal subsets of the complex plane. The search proceeds from
a simple form of arithmetic directions given by the artist. After this direction
is given, the process runs entirely automatically.
The quadratic Mandelbrot set is a subset of the complex plane consisting
of those complex numbers z for which the Mandelbrot sequence
z, z2 + z, (z2 + z)2 + z, ((z2 + z)2 + z)2 + z, . . .
(6.1)
does not diverge in absolute value. A quadratic Julia set with parameter μ ∈C
is a subset of the complex plane consisting of those complex numbers z for
which the Julia sequence:
z, z2 + μ, (z2 + μ)2 + μ, ((z2 + μ)2 + μ)2 + μ, . . .
(6.2)
fails to diverge in absolute value. These two fractals are related. The deﬁning
sequences for both fractals involve iterated squaring. The diﬀerence is that,
when checking a point z in the complex plane for membership in the fractal,
the Mandelbrot sequence adds the value z under consideration after each
squaring, while the Julia sequence adds the ﬁxed parameter μ. The overall
appearance of a Julia set matches the local appearance of the Mandelbrot set
around μ. Good Julia set parameters μ are thus drawn from near interesting
regions of the Mandelbrot set.
The indexing of Julia sets by the Mandelbrot set is demonstrated in
Fig. 6.3. Notice in the lower picture how the Julia sets changes as their pa-
rameter moves across a small region of the Mandelbrot set. This indexing
property of the Mandelbrot set means that the search for interesting Julia
sets can be undertaken as a search of the Mandelbrot set. Because of this we
use a generalization of the Julia set we discovered.
We tested two versions of the evolutionary algorithm. The ﬁrst searches
the Mandelbrot set for interesting views. A view of the Mandelbrot set is a

124
Ashlock and Jamieson
Fig. 6.3. Indexing of Julia sets by the Mandelbrot set. Julia sets are displayed
as small insets in the Mandelbrot set (above) and a subset of the Mandelbrot set
(below). Julia set parameters are drawn from the location of the red dots connected
to the inset thumbnails

6 Evolutionary Exploration of Complex Fractals
125
square subset of the complex plane speciﬁed by three real parameters: x, y,
and r. The square has the complex number x + iy as its upper left corner and
has side length r. The second version of the evolutionary algorithm searches
for the parameters of a generalized Julia set. The generalization of Julia sets is
indexed by a four-dimensional form of the Mandelbrot set and so is in greater
need of automatic search methods than the standard two-dimensional (with
one complex parameter) space of Julia sets. The generalization of the Julia
set used in this study requires two complex parameters, ω1 and ω2. As with
the Mandelbrot set and standard Julia sets, the members of the generalized
Julia set are those complex numbers z for which a sequence fails to diverge.
The sequence is:
z
(6.3)
z2 + ω1
(6.4)
(z2 + ω1)2 + ω2
(6.5)
((z2 + ω1)2 + ω2)2 + ω1
(6.6)
(((z2 + ω1)2 + ω2)2 + ω1)2 + ω2
(6.7)
· · ·
(6.8)
Rather than adding a single complex parameter after each squaring, the two
complex parameters are added alternately. This generalization originated as
a programming error a number of years ago. It encompasses a large number
of fractal appearances that do not arise in standard Julia sets. One clear
diﬀerence is this. A standard Julia set is a connected set if its parameter is a
member of the Mandelbrot set and is a fractal dust of separate points if its
parameter is outside of the Mandelbrot set. The generalized Julia sets include
fractals that have multiply connected regions.
The remainder of the chapter is structured as follows. Section 6.2 reminds
those readers who have not used complex arithmetic recently of the details as
well as explaining how the fractals are rendered for display. Section 6.3 deﬁnes
the ﬁtness function used to drive the evolutionary search for Mandelbrot and
generalized Julia sets. Section 6.4 gives the experimental design, specifying
the evolutionary algorithm and its parameters. Section 6.5 gives the results,
including visualizations. Section 6.6 gives possible next steps.
6.2 Complex Arithmetic and Fractals
The complex numbers are an extension of the familiar real numbers (those that
represent distances or the negative of distances) achieved by including one
“missing” number, i = √−1, and then closing under addition, subtraction,
multiplication, and division by non-zero values. The number i is called the
imaginary unit. A complex number z is of the form z = x + iy where x and y

126
Ashlock and Jamieson
are real values. The number x is called the real part of z, and y is called the
imaginary part of z. The arithmetic operations for complex numbers work as
follows:
(a + bi) + (c + di) = (a + c) + (b + d)i
(6.9)
(a + bi) −(c + di) = (a −c) + (b −d)i
(6.10)
(a + bi) × (c + di) = (ac −bd) + (ad + bc)i
(6.11)
a + bi
c + di =
ac + bd
√
c2 + d2 + bc −ad
√
c2 + d2 i.
(6.12)
One of the properties of the complex numbers is that they place an arith-
metic structure on points (x, y) in the Cartesian plane so that arithmetic
functions over the complex numbers can be thought of as taking points (x, y)
(represented by x + yi) in the plane to other points in the plane. Because the
complex numbers have this one-to-one correspondence with the points of the
Cartesian plane, they are also sometimes referred to as the complex plane.
Complex fractals are easier to deﬁne when thought of as consisting of points
in the complex plane. The absolute value of a complex number z = x + yi is
denoted in the usual fashion |z| and has as its value the distance

x2 + y2
from the origin.
Suppose we are examining a point z in the plane for membership in a
Mandelbrot or a generalized Julia set. The iteration number for a point is the
number of terms in the relevant sequence (Equation 6.1 for the Mandelbrot
set, Equation 6.2 for standard Julia sets, or Equations 6.3–6.7 for generalized
Julia sets) before the point grows to an absolute value of 2 or more. The
reason for using an absolute value of 2 is that the point in the Mandelbrot
set with the largest absolute value is z = −2 + 0i. For any value z with an
absolute value greater than 2, its square has an absolute value exceeding 4 and
so the absolute value of the sequence must increase. Points not in the fractal
thus have ﬁnite iteration numbers, while points in the fractal have inﬁnite
iteration numbers. Iteration numbers are used for rendering visualizations of
the fractal.
This chapter uses a coloring scheme with a cyclic continuously varying
palette for rendering divergence behaviors. The software uses an RGB repre-
sentation for color, with each color represented by a red, green, and blue value
in the range 0 ≤r, g, b ≤255. The palettes used are of the form:
R(k) = 127 + 128 · cos (AR · k + BR)
(6.13)
G(k) = 127 + 128 · cos (AG · k + BG)
(6.14)
B(k) = 127 + 128 · cos (AB · k + BB)
(6.15)
where the A∗values control how fast each of the three color values cycle, the
B∗values control the phase of their cycle, and k is the iteration number of
the point being colored. The palette used in this chapter uses the color-cycle

6 Evolutionary Exploration of Complex Fractals
127
values: Ar = 0.019, AG = 0.015, AB = 0.011 (a slowly changing palette). The
phase-shift values were set to BR = BG = BB = 0.8 so that the ﬁrst color,
for k = 0, is a uniform gray. Thus, when the cosine waves that control each
color are in phase, the color is some shade of gray. Since the parameters A∗
controlling speed diﬀer, the hue changes with k. The apparent diﬀerences in
palettes between pictures are caused by the fact that the scale of the views is
diﬀerent (some have far smaller side lengths than others), and so the colors
displayed in the view are sampled from diﬀerent parts of the periodic palette.
When checking for membership in a complex fractal, it is necessary to set
an upper limit Mmax on the number of terms of the sequence checked. In this
chapter, Mmax = 200. Points with Mmax terms of the sequence with absolute
value less than two are taken to be within the set. The color value for these
points is calculated using k = Mmax in the periodic palette.
6.3 Fitness Function Design
The ﬁtness function we used to locate interesting complex fractals transforms
the three real parameters deﬁning a view of the Mandelbrot set or the four
parameters that specify a particular generalized Julia set into a scalar ﬁtness
value. The ﬁtness function gives the artist control over the appearance of
the fractals located by the algorithm. A ﬁtness function that can do this is
presented in [1]. A grid of points, either 11×11 or 15×15 is placed on a square
subset of the complex plane. When searching for views into the Mandelbrot
set, this square is the square deﬁned by the view, while for generalized Julia
sets it is the square with corners (0.8, 0.8) and (−0.8, −0.8).
For a given set of parameters deﬁning a complex fractal, the iteration
values for the points in the grid are computed. A mask is used to specify
desired iteration values on the grid points. Fitness is the average, over the
grid points, of the squared error of the true iteration value and the desired
value speciﬁed by the mask. The average is used, rather than the total, to
permit comparison between masks with diﬀerent numbers of points.
The mask speciﬁes where points with various approximate iteration values
are to be in the evolved images. This ability to specify the behavior of the
fractal on the grid gives the artist control over the appearance of the resulting
complex fractals. The masks used for the generalized Julia sets are shown
in Fig. 6.4, while those used to search for Mandelbrot views are shown in
Figs 6.5 and 6.4. The evaluation square of (−0.8, −0.8) to (0.8, 0.8) used for
the generalized Julia sets was chosen by experimentation. When too large a
square is used, the behavior far from the origin of the complex plane dominates
the ﬁtness function, and the fractals found look like simple lagoons. When too
small a square is used, the overall appearance of the Julia set becomes visually
unrelated to the mask. The rendered views of the Julia sets have a side length
of 3.6 units with the square used for ﬁtness evaluation centered in the rendered

128
Ashlock and Jamieson
Continuous Times
Continuous Plus
Two Hill
Strict Plus
Inverse Plus
Constant
Fig. 6.4. The ﬁve masks used in the search for generalized Julia sets and the
constant mask used in the Mandelbrot experiments. White represents an iteration
value of zero, black a value of 200
view. The squares used for rendering Mandelbrot views are exactly the square
associated with the view.

6 Evolutionary Exploration of Complex Fractals
129
Single Hill
Trench
Fig. 6.5. Two of the masks used in the search for views into the Mandelbrot set. The
constant mask appearing in Fig. 6.4 was also used in the Mandelbrot experiments.
White represents an iteration value of zero, black a value of 200
6.4 Speciﬁcation of Experiments
The evolutionary algorithm for both sorts of complex fractal operates on a
population of 800 structures. The initial population for the Mandelbrot ex-
periments is generated by placing (x, y) uniformly at random in the square
region with corners (−2, −1.5) and (1, 1.5). This is a relatively small square
including all of the Mandelbrot set. The value of r is initialized in the range
0.0001 ≤r ≤0.01 using a distribution whose natural log is uniformly dis-
tributed. Variation operators for the Mandelbrot views are as follows. The
y-values are exchanged, and the r-values are exchanged 50% of the time. Af-
ter the exchange of y-values and r-values, the new structures are mutated.
Mutation has two parts. The ﬁrst consists of adding a displacement in the
plane to (x, y) in a direction selected uniformly at random and with magni-
tude given by a normal random variable with mean zero and standard de-
viation σ = 0.002. For the small range of zooms used in this chapter, this
constant value for σ was acceptable but a greater range of potential zoom
would require that this parameter be made to adapt to the current level of
zoom. After the corner of a view has been displaced in this fashion, the side
length is multiplied by 1.1eN(0,1) where N(0, 1) is a standard normal random
variable. The eﬀect of this is to multiply r by a value near 1. These mutation
operators were chosen to permit incremental exploration of view-space with as
little mutational bias as possible. The multiplicative mutation operator used
for r makes small adjustments whose scale does not depend on the current
value of the side length. Twenty independent runs were performed for each
mask.
Parameters for the initial population of generalized Julia sets are stored
as an array (a, b, c, d) where ω1 = a + bi and ω2 = c + di. These parameters
are initially chosen uniformly at random in the range −2 ≤a, b, c, d ≤2. The

130
Ashlock and Jamieson
Fig. 6.6. Selected thumbnails of best-of-run Mandelbrot views for the ﬁtness func-
tion using the constant mask
variation operators for the Julia sets consist of two-point crossover operating
on the array of four reals and a single point mutation that adds a Gaussian
with a variance of 0.1 to one of the four real parameters selected uniformly at

6 Evolutionary Exploration of Complex Fractals
131
Fig. 6.7. Selected thumbnails of best-of-run Mandelbrot views for the ﬁtness func-
tion using the single hill mask
random. Thirty-six independent runs were performed for each mask for the
Julia sets.
The model of evolution for all experiments uses size seven single tourna-
ment selection. A group of seven distinct population members is selected at

132
Ashlock and Jamieson
Fig. 6.8. Selected thumbnails of best-of-run Mandelbrot views for the ﬁtness func-
tion using the trench mask
random without replacement. The two best are copied over the two worst,
and then the copies are subjected to the variation operators deﬁned for the
respective sorts of fractals. The evolutionary algorithm is a steady state [15]
algorithm, proceeding by mating events in which a single tournament is pro-

6 Evolutionary Exploration of Complex Fractals
133
Fig. 6.9. Selected thumbnails of the best-of-run generalized Julia sets for the ﬁtness
function using the continuous plus mask
cessed. Evolution continues for 100,000 mating events for the Mandelbrot
views and for 50,000 mating events for the generalized Julia sets. The most
ﬁt (lowest ﬁtness) individual is saved in each run. The ﬁtness functions used
are described in Section 6.3.

134
Ashlock and Jamieson
Fig. 6.10. Selected thumbnails of the best-of-run generalized Julia sets for the
ﬁtness function using the continuous times mask
6.5 Results and Discussion
For all the complex fractals studied in this chapter, ﬁnal ﬁtnesses varied con-
siderably within the runs done for each of the masks. This suggests that
all eight ﬁtness landscapes used are pretty rugged. The thumbnails of the

6 Evolutionary Exploration of Complex Fractals
135
fractals located also show a substantial diversity of appearance within each
ﬁtness function. The ﬁtness landscapes are themselves fractal. In the search
for Mandelbrot views the ﬁtness function is a smoothed version of the iter-
ation number function for the Mandelbrot set. The mask deﬁnes an average
of the iteration numbers over its sample points, acting as a smoothing ﬁlter.
Since this iteration function makes arbitrarily large jumps in arbitrarily small
spaces, a ﬁnite amount of smoothing will not erase its rugged character.
The ﬁtness landscape for the generalized Julia sets is fractal for a similar
reason. The parameters represent a point in a four-dimensional Mandelbrot
set that speciﬁes the complexity of the resulting generalized Julia set. This
four-dimensional Mandelbrot set, alluded to earlier in the manuscript, can be
understood as follows. Each set of four parameters (a, b, c, d) yields a gener-
alized Julia set. If we ﬁx ω1 = a + bi and then check the iteration value of
ω2 = c + di using ω2 as both the point being tested and the second Julia pa-
rameter, then the points that fail to diverge are a (nonstandard) Mandelbrot
set. The points tested depend on all the parameters a, b, c and d, and so the
set is four dimensional. The role of ω1 and ω2 can be interchanged, but this
yields a diﬀerent parameterization of the same four-dimensional Mandelbrot
set. Much as the ﬁtness function for Mandelbrot views ﬁlters the Mandel-
brot set, the Julia parameters are a ﬁltered image of the four-dimensional
Mandelbrot set.
Figs 6.6–6.13 give selected thumbnails for the runs performed for the eight
diﬀerent masks. One of the goals in the design of the mask-based ﬁtness
function was to give the artist control over the appearance of the fractals
located. Comparing the within-ﬁtness-function variation in appearance with
the between-ﬁtness-function variance, it is clear that the diﬀerent masks grant
substantial control over the character of the fractals located. Each ﬁtness
function found similar but not identical fractals (with the exception of the
constant mask). The thumbnails shown in the ﬁgures show the diversity of
fractals found after eliminating fractals with substantially similar appearance.
The results presented represent a successful controlled search of both the
Mandelbrot set and the 4-space of generalized Julia sets. The success lies in
providing an automated search tool that, via the use of the mask-based ﬁtness
function, is still controlled by the user. The mask ﬁtness functions give the
artist input while still leaving a rich space of fractals that are near optima
(probably local optima) of the ﬁtness function. The diﬀerence between the
continuous plus mask, Fig. 6.9 and the strict plus, Fig. 6.12, is substantial.
The softer plus-shaped mask locates “softer” fractals. All eight of the masks
used in the study yield markedly diﬀerent appearances.
The most diverse collection of fractals located were the Mandelbrot views
associated with the constant mask. The location of these views on the Mandel-
brot set is scattered in space and their appearances have less in common than
those found using any of the other masks. The constant mask was intended as
a control, and for the generalized Julia sets it serves as one, locating two Julia
sets that are both quite simple and mirror images of one another (data not

136
Ashlock and Jamieson
Fig. 6.11. Selected thumbnails of the best-of-run generalized Julia sets for the
ﬁtness function using the twin hill mask
shown). In retrospect it is not surprising that the Mandelbrot views located
with the constant mask are interesting. The constant mask tries for iteration
numbers that are 75% of the maximum number of iterations (150 out of 200
in this case). This requires that a ﬁt view have high iteration numbers on its

6 Evolutionary Exploration of Complex Fractals
137
Fig. 6.12. Selected thumbnails of selected best-of-run generalized Julia sets for the
ﬁtness function using the strict plus mask
sample points, and that it not contain points in the interior of the Mandelbrot
set. Thus, the constant mask looks for action near the boundary of the set
where most of the interesting fractals reside. The choice of 75% of maximum

138
Ashlock and Jamieson
iteration value for the constant mask was made arbitrarily. Other constant
masks might also yield interesting appearances.
The single hill mask for Mandelbrot views turned out to be good at locating
a well-known feature of the Mandelbrot set – the minibrot. A minibrot (see
Fig. 6.7) is a smaller copy of the Mandelbrot set within the Mandelbrot set.
There are an inﬁnite number of minibrots within the set. Since a minibrot
gives a central collection of high iteration number, it is intuitive that the
single hill mask would tend to locate minibrots. The second most common
type of view located by the single hill mask is a radiation collection of strips
of high iteration value. Several examples of these appear in Fig. 6.7.
The continuous plus and continuous times masks used for the Julia set
evolution are quite similar; the continuous plus mask is slightly lower reso-
lution, but both are twin intersecting ridges when viewed as functions. The
images in Figs 6.9 and 6.10 form two clearly distinct groups. The twin hill
mask (Fig. 6.11) provides a collection of superﬁcially similar fractals – show-
ing the inﬂuence of the mask – but close examination reveals that, in terms of
spiral structure, connectivity, branching factor, and other properties of Julia
sets, these fractals are actually quite diverse. The masks provide some control
but yield a substantial diversity of appearances.
An interesting aspect of the type of evolutionary search presented here is
the new role of local optima. The mask-based ﬁtness functions do not clearly
specify the exact, desired outcome. They are more like “guidelines.” This
means that the local optima of these ﬁtness functions may have greater appeal
than the global optimum. This is very diﬀerent from a standard evolutionary
optimization in which local optima can be an absolute bane of eﬀective search.
This is good news given the complexity of the ﬁtness landscape; locating a
global optimum would be very diﬃcult.
The search for interesting complex fractals presented here is an example
of an automatic ﬁtness function for locating artistically interesting images.
Enlargements of some of the thumbnail views appear in Figs 6.14–6.16. This
contrasts with the process of using a human being as the ﬁtness function. It
gives the artist a tool to guide the search, in the form of the masks, but does
not require the artist’s attention throughout evolution.
6.6 Next Steps
The fractal evolution techniques used here are intended as a technique for use
by artists. Various evolved art contests held at the Congress on Evolutionary
Computation and the EvoMusArt conferences have stimulated a good deal
of interest in evolved art as a technique. With this in mind several possible
directions for the extension of this research follow. Potential collaborators
are welcome to contact the ﬁrst author to discuss possible variations and
extensions of this work.

6 Evolutionary Exploration of Complex Fractals
139
Fig. 6.13. Selected thumbnails of the best-of-run generalized Julia sets for the
ﬁtness function using the inverse plus mask
The time to run the evolutionary search for complex fractals grows in direct
proportion to the number of sample points. This is why the number of sample
points used here have been restricted to an 11 × 11 or 15 × 15 grid. Larger
grids could cause tighter groupings of the appearances than occurred here. In

140
Ashlock and Jamieson
Fig. 6.14. An enlargement of the third thumbnail from the Mandelbrot views lo-
cated with the constant mask
addition, there is the potential for sparse masks that have an indiﬀerence value
at some locations. The minimization of squared error would simply ignore
those points, permitting more rapid search on what are, in eﬀect, non-square
masks.
Another obvious extension of this line of research lies in the fact that
quadratic Julia sets are simply one of an inﬁnite number of types of Julia
sets. Each possible power (cubic, quartic, etc.) yields its own Mandelbrot set,
indexing a corresponding collection of Julia sets. Similarly, Mandelbrot and
Julia sets can be derived from transcendental functions such as ez, sin(z), et
cetera. Each of these is its own domain for evolutionary search.
An obvious extension comes from continuing the generalization process for
Julia sets. The generalized Julia sets presented here use, alternately, two com-
plex parameters when generating the sequence used to test for set membership;
a standard Julia set uses one. An n-fold, rather than two-fold, alternation of
constants could be used. Likewise a set of constants could be added into terms
of the series in a pattern rather than in strict alternation or rotation. Each
of these variations has a simple realization as a real-parameter optimization
problem using the mask ﬁtness functions.
The masks tested in this study are a small set, designed using the intuition
of the authors. A vast number of other masks are possible. One possibility is
to used an existing complex fractal as a source for a mask. The artist would

6 Evolutionary Exploration of Complex Fractals
141
Fig. 6.15. An enlargement of the tenth thumbnail from the Mandelbrot views
located with the single hill mask
Fig. 6.16. An enlargement of the tenth thumbnail from the generalized Julia sets
located with the twin hill mask

142
Ashlock and Jamieson
hand select a set of parameters that yield an interesting complex fractal. The
iteration values for this fractal would then be used as a mask, permitting
evolutionary generalization of the hand-selected fractal. This proposed tech-
nique would require a parameter study on the number of sample points used
in the mask. Too few points and the resulting fractals will likely bear no re-
semblance to the original one. Too many sample points and the search will
locate collections of nearly identical Julia sets.
Finally we feel that automating the selection of a coloring algorithm is a
possibly diﬃcult but rewarding avenue for future research. This might be a
natural place for evolution with a human serving as the evaluator, or it might
be possible to select a feature set and make chromatic analogies with existing
pictures thought to be chromatically balanced.
Acknowledgments
The authors would like to thank the University of Guelph Department of
Mathematics and Statistics for its support of this research.
References
1. Ashlock, D.: Evolutionary exploration of the Mandelbrot set. In: Proceedings
of the 2006 Congress on Evolutionary Computation, pp. 7432–7439 (2006)
2. Ashlock, D., Bryden, K., Gent, S.: Creating spatially constrained virtual plants
using L-systems. In: Smart Engineering System Design: Neural Networks, Evo-
lutionary Programming, and Artiﬁcial Life, pp. 185–192. ASME Press (2005)
3. Ashlock, D., Jamieson, B.: Evolutionary exploration of generalized Julia sets.
In: Proceedings of the 2007 IEEE Symposium on Computational Intelligence in
Signal Processing, pp. 163–170. IEEE Press, Piscataway NJ (2007)
4. Barrallo, J., Sanchez, S.: Fractals and multi-layer coloring algorithm. In: Bridges
Proceedings 2001, pp. 89–94 (2001)
5. Castro, M., P´erez-Luque, M.: Fractal geometry describes the beauty of inﬁnity
in nature. In: Bridges Proceedings 2003, pp. 407–414 (2003)
6. Fathauer, R.: Fractal patterns and pseudo-tilings based on spirals. In: Bridges
Proceedings 2004, pp. 203–210 (2004)
7. Fathauer, R.: Fractal tilings based on dissections of polyhexes.
In: Bridges
Proceedings 2005, pp. 427–434 (2005)
8. Ibrahim, M., Krawczyk, R.: Exploring the eﬀect of direction on vector-based
fractals. In: Bridges Proceedings 2002, pp. 213–219 (2002)
9. Mandelbrot, B.: The Fractal Geometry of Nature. W. H. Freeman and Company,
New York (1983)
10. Mitchell, L.: Fractal tessellations from proofs of the Pythagorean theorem. In:
Bridges Proceedings 2004, pp. 335–336 (2004)
11. Musgrave, F., Mandelbrot, B.: The art of fractal landscapes. IBM J. Res. Dev.
35(4), 535–540 (1991)

6 Evolutionary Exploration of Complex Fractals
143
12. Parke, J.: Layering fractal elements to create works of art. In: Bridges Proceed-
ings 2002, pp. 99–108 (2002)
13. Parke, J.: Fractal art – a comparison of styles. In: Bridges Proceedings 2004,
pp. 19–26 (2004)
14. Rooke, S.: Eons of genetically evolved algorithmic images.
In: P. Bentley,
D. Corne (eds.) Creative Evolutionary Systems, pp. 339–365. Academic Press,
London, UK (2002)
15. Syswerda, G.: A study of reproduction in generational and steady state genetic
algorithms. In: Foundations of Genetic Algorithms, pp. 94–101. Morgan Kauf-
mann (1991)
16. Ventrella, J.: Creatures in the complex plane. IRIS Universe (1988)
17. Ventrella, J.: Self portraits in fractal space. In: La 17 Exposicion de Audiovi-
suales. Bilbao, Spain (2004)

7
Evolving the Mandelbrot Set to Imitate
Figurative Art
JJ Ventrella
http://www.ventrella.com/
Jeffrey@Ventrella.com
7.1 Introduction
This chapter describes a technique for generating semi-abstract ﬁgurative im-
agery using variations on the Mandelbrot Set equation, evolved with a genetic
algorithm. The Mandelbrot Set oﬀers an inﬁnite supply of complex fractal im-
agery, but its expressive ability is limited, as far as being material for visual
manipulation by artists. The technique described here achieves diverse im-
agery by manipulating the mathematical function that generates the Set.
The art of portraiture includes many art media and many styles. In the
case of self-portraiture, an artist’s choice of medium is sometimes the most
important aspect of the work. A love for math and art, and an irreverence
concerning the massacring of math equations for visual eﬀect, inspired the
medium described in this chapter. It evolves manipulations of the Mandelbrot
equation to mimic the gross forms in ﬁgurative imagery, including digital
photographs of human and animal ﬁgures. To exploit the Mandelbrot Set’s
potential for this, the technique emphasizes shading the “ﬂesh” (the interior
of the Set), rather than the outside, as is normally done. No image has been
generated that looks exactly like a speciﬁc ﬁgure in detail, but nor is this the
goal. The images have an essence that they are “trying” to imitate something,
enough to evoke a response in the viewer. As an example, Fig. 7.1 shows
evolved images that are all based on a single digital image of the author’s
head. The technique for generating these images will be explained near the
end of the chapter.
The technique is partly inspired by two movements in Modernist paint-
ing: abstract expressionism and surrealism. But in this case, the eﬀect is not
achieved by the physics of paint on canvas and an artist’s dialog with an
emerging image. Instead, the eﬀect is achieved by dynamics in the complex
plane (the canvas upon which the Mandelbrot Set is painted) and an artist’s
algorithmic searching methods.
After developing many artworks by painstakingly adjusting numerical pa-
rameters, an interactive interface was added to help automate the selective

146
Ventrella
Fig. 7.1. Four examples based on an image of the author’s head
process and to eliminate the need to control the numbers directly. Then a
question came up: is it possible to evolve these kinds of images automatically
using a genetic algorithm and an image as the objective ﬁtness function? Also,
given a speciﬁc image as a ﬁtness function, what are the limits of the Mandel-
brot Set – and the array of parameters added to increase its dimensionality
and plasticity – to imitate images? How evolvable are these mathematically-
deﬁned forms? And for the sake of image-making, are not other parametric
schemes more evolvable – such as Koch fractal construction, IFS, chaotic
plots, L-systems, cellular automata, et cetera? General questions like this will
be threaded throughout this chapter, touching upon the nature of the Man-
delbrot Set, and the notion of evolutionary art as imitation of natural form.
The Mandelbrot Set has a special place in the universe of visual materi-
als available to artists for manipulation – it has a peculiar complexity all its
own. And the fascination with its arbitrary symmetry and beauty can reach
near-religious levels. Roger Penrose believes, as do many mathematicians, that
there is a Platonic truth and universality to mathematics, and that the Man-
delbrot Set demonstrates this. It was not, and could never have been, invented
by a single human mind [13]. It could only have been discovered. A diﬀerent
approach to mathematics is presented by Lakoﬀand Nunez [9] who claim
that mathematics springs out of the embodied mind. It expresses our physical
relationship with the world, and the subsequent metaphors that have evolved.
Math is not universal truth, but a language of precision, contingent upon the
nature of the human brain and the ecology from which it evolved.
We will not try to address this debate in this chapter. Only to say that this
technique takes a rather un-Platonic approach to manipulation of the math,
which may be unsavory from a mathematical standpoint, but from an artistic
standpoint it breaks open the canvas of the complex plane to a larger visual
vocabulary.
Even when pulling the function out of the realm of complex analysis,
a manipulated Mandelbrot Set still possesses remarkable properties, and it
appears not to be as plastic and malleable as a lump of sculptor’s clay. It is
more like an organism whose entire morphology, at every scale, is determined
by a speciﬁc genetic code and the constraints of its iterative expression. For
instance, rotational behavior is characteristic of the complex plane (from the
multiplication of two complex numbers). Curvilinear, rotational, and spiral-

7 Evolving the Mandelbrot Set to Imitate Figurative Art
147
like features are common, and they are still present when the Mandelbrot
function has been manipulated, as seen in the detail at right of Fig. 7.2.
Fig. 7.2. Curvilinear forms remain upon manipulation of the Mandelbrot equation
And so, while this approach may be irreverent mathematically, what can be
learned about the nature of this particular artistic canvas elevates admiration
for the magic of the Mandelbrot Set, and its extended family of related fractals.
Like the playful and evocative variations on complex plane fractals created
by Pickover [14] and others, the technique described here is heavy on the
“tweaking”. It is not focused on ﬁnding interesting regions in the pure un-
altered Set (as in “Mandelzoom” [4]). It is more like sculpting than nature
photography. Or perhaps it is more like genetic engineering than painting. For
this reason, the technique is called “Mandeltweak”. Images produced by this
technique are also called “Mandeltweaks”.
7.1.1 Genetic Space
Inspired by a metaphor which Dawkins uses to describe the genes of species
as points existing within a vast multi-dimensional “genetic space” [3], Man-
deltweaks are likewise considered as existing within a genetic space. Figure 7.3
illustrates this. In each image, two genes vary in even increments, one in the
horizontal dimension, and the other in the vertical dimension. The values are
default in the middle of each space, and this is where the Mandelbrot Set lies.
The image on the left is of a large nine-panel photo series shown in various
gallery settings.
7.1.2 Genetic Parameters
The number of ways one can alter a digital image is practically inﬁnite. Con-
sider the number of plug-in ﬁlters available for software tools like Photoshop.
But while an arbitrary number of ﬁlters, distortions, layerings, etc. could
have been applied with parameters for manipulating Mandelbrot images in
the pixel domain, the choice was to keep all variability to within the conﬁnes
of the mathematical function. The game is to try to optimize the parameters

148
Ventrella
Fig. 7.3. Mandelbrot Set in the middle of two 2D genetic spaces
of the manipulated Mandelbrot equation so that the resulting images resemble
an ideal form (or in some cases an explicit target image). This is a challenge
because the mapping of genetic parameters to image attributes (genotype to
phenotype) is non-trivial, and unpredictable.
7.2 Background
The Mandelbrot Set has been called “the most complex object in mathe-
matics” [4]. It is like the mascot of the chaos and complexity renaissance –
replicated in popular science books like a celebrity. When looking at it in its
whole, it looks like a squashed bug – not pretty. But its deep remote recesses
reveal amazing patterns that provoke an aesthetic response. Is the Mandelbrot
Set a form of abstract art? No. Not if you consider abstractionism as human-
made art that is “abstracted” from nature, with human interpretation. The
Platonic stance claims that the Mandelbrot Set “just is”. It had been hiding
in the folds of complex mathematics until humans and computers revealed it.
But consider the canvas upon which the Mandelbrot Set is painted. We can
alter our mathematical paint brush from z = z2 +c to something else which is
not so clearly deﬁned, and make a departure from its Platonic purity. We can
render images with the kind of interpretation, imprecision, and poetry that
distinguishes art from pure mathematics.
It is possible that the Mandelbrot Set was discovered a handful of times
by separate explorers, apparently ﬁrst rendered in crude form by Brooks and
Matelski [13]. Benoit Mandelbrot discovered it in the process of developing his
theory of fractals, based on earlier work on complex dynamics by Fatou and
Julia [10]. His work, and subsequent ﬁndings by Douady [6], helped popularize
the Set, which now bears Mandelbrot’s name.
Brieﬂy described, the Set is a portrait of the complex function, z = z2 +c,
when iterated in the two-dimensional space known as the complex plane, the
space of all complex numbers. When the function is applied repeatedly, using
its own output as input for each iteration, the value of z changes in interesting
ways, characteristically diﬀerent depending on c (i.e., where it is being applied
in the plane). The dynamics of the function as it is mapped determines the

7 Evolving the Mandelbrot Set to Imitate Figurative Art
149
colors that are plotted as pixels, to make images of the Set. Speciﬁcally, if
the magnitude of z grows large enough (> 2) and escapes to inﬁnity, it is
considered outside of the Set. The inside is shown as the black shape on the
left of Fig. 7.4.
The Set has become a favorite subject for computer art. On its boundary
is an inﬁnite amount of provocative imagery. The most common visual explo-
rations involve zooming into remote regions and applying color gradations on
the outside of the Set near the boundary. This is somewhat like simple point-
and-shoot photography, with a bit of darkroom craft added. Some techniques
have been developed to search-out interesting remote regions, including an
evolutionary algorithm developed by Ashlock [1]. A particle swarm for con-
verging on the boundary, under diﬀerent magniﬁcations, was developed by
Ventrella [21].
Deeper exploration into the nature of the Set is achieved by manipulating
the mathematics, to reveal hidden structures. Pickover [14] has ﬁshed out a
great wealth of imagery by using varieties of similar math functions. Peitgen et
al. [12] describe a variety of complex plane fractals, with ample mathematical
explanations. Dickerson [5], and others, have explored higher-order variations
of the function to generate other “Mandelbrot Sets”. According to Dickerson,
the equation can be generalized to: z = a × f(z) + c, where a is a scale
constant and f(z) is one of a broad range of functions, such as higher powers
of z, polynomials, exponentials and trigonometric functions. As an example
of a higher-order function: z = z3 + c creates the shape shown at the right
in Fig. 7.4. This “Mandelbrot cubed” function is used in some experiments
described below.
Fig. 7.4. Mandelbrot Set (left), and Mandelbrot “cubed” (right)
The algorithms used in Mandeltweak can also be considered as a general-
ization from z = z2 + c to z = a × f(z) + c, only in this case, the complex
nature of the number z is violated: the real and imaginary components of the
number, as exposed in the software implementation, are manipulated. A col-
lection of fractals created by Shigehiro Ushiki [18], are generated in a similar
manner. More examples of separate treatment of the real and imaginary parts
of the equation are emerging. These are described in various web sites on the
Internet, including a technique developed by eNZed Blue [2].

150
Ventrella
7.2.1 Evolutionary Art
One way an artist can approach mathematically-based image-making is to
identify a number of variables that determine visual variations and to tweak
them to suit his/her own aesthetic style. The more variables available for
tweaking, the more the artist can potentially tweak to reach some level of
personal expression. The problem is that in most cases the variables are in-
terdependent, and it is hard to predict the eﬀects of variables in combination.
Besides, most artists would rather not use numbers to manipulate visual lan-
guage. Evolutionary computation addresses this problem.
In evolutionary art, a computer software program becomes a creative col-
laborator to the artist. The most common process is interactive evolution (also
called “aesthetic evolution” or “aesthetic selection”). In contrast to the stan-
dard genetic algorithm (GA) [7], the selection agent is not determined by an
objective ﬁtness function, but rather by a human observer/participant (the
artist), whose aesthetic choices guide the direction of evolution in a population
of variations of an artwork. McCormack [11] outlines a number of problems
that remain open as we articulate and reﬁne the tools for evolutionary art.
Among these are the problem of ﬁnding interesting and meaningful pheno-
types, which are capable of enough variation to allow for artistic freedom.
Among the earliest examples of evolutionary art are the work of La-
tham [17]. Sims [16] has applied genetic programming [8] to various visual
realms. Rooke [15], Ventrella [19], and others have developed evolutionary
techniques for generating visual art and animation.
7.3 Technique
The standard black-and-white ﬁgure of the Mandelbrot Set is created as fol-
lows: on a rectangular pixel array (i, j), determine whether each pixel lies
inside or outside of the Set. If inside, color the pixel black, otherwise, color
it white. This 2D array of pixels maps to a mathematical space (x, y) lying
within the range of −2 to 1 in x, and −1 to 1 in y. The function z = z2 + c is
iterated. The value c is a complex number (x + yi) – (the 2D location in the
complex plane corresponding to the pixel), and z is a complex number which
starts at (0+0i) and is repeatedly fed back into the function. This is repeated
until either the number of iterations reaches a maximum limit, or the magni-
tude of z exceeds 2. If it exceeds 2, it is destined to increase towards inﬁnity,
and this signiﬁes that it is outside of the Set, as expressed in Algorithm 1.
This is not implemented as optimally as it could be, but it exposes more
variables for manipulation – which is part of the Mandeltweak technique.
maxIterations could be any positive number, but higher numbers are
needed to resolve a detailed deﬁnition of the boundary, which is especially
important for high magniﬁcations. Mandeltweak does not require high mag-
niﬁcations, and so this value ranges from 30 to 50. The mapping of (i, j) to

7 Evolving the Mandelbrot Set to Imitate Figurative Art
151
Algorithm 1 Plot Mandelbrot
For each pixel (i, j)
map screen pixel values (i, j) to real number values (x, y)
zm = 0
zx = 0
zy = 0
timer = 0
while ( zm < outsideTest AND timer < maxIterations )
z1 = y + zy × zy + zx × −zx
z2 = x + 2.0 × zx × zy
zm = z1 × z1 + z2 × z2
zx = z2
zy = z1
end while
if ( zm < outsideTest )
set pixel color black
else
set pixel color white
end if
plot pixel (i, j)
end for loop
(x, y) determines translation, rotation, and scaling in the complex plane. Com-
plex number z is represented by zx and zy. The variable zm is the squared
magnitude of z. The variable outsideTest is set to 4 for normal Mandelbrot
plotting, but it could be set to other values, as explained below.
Note that swapping x and y rotates the Set 90 degrees. The Mandeltweak
approach is to make the real number axis vertical, orienting the Set as if it
were a fellow entity with left-right bilateral symmetry, as shown in Fig. 7.5.
7.3.1 Coloration
The typical coloration scheme for Mandelbrot Set images applies a color gradi-
ent on the outside of the Set which maps to the value of timer after iteration.
A smoother version of the outside gradient, described by Peitgen [12] can
be achieved by replacing the value timer with: 0.5 × log(z)/2timer. Setting
outsideTest to higher values, such as 1000, makes it smoother. Mandeltweak
uses this technique, and in most cases, the background is rendered with a light
color, which shifts to a dark color very close to the boundary. The eﬀect is a
mostly-light colored background, which is darker near the complex boundary,
as seen in Fig. 7.5(c) and (d).
The inside of the Set is colorized with gradient (zm/outsideTest + za)/2,
where za is a special value determined by analyzing the orbit of z during
iteration. As the value of z jumps around the complex plane, the change in
angle between vectors traced by each consecutive pair of jumps is saved, and

152
Ventrella
when iteration is complete, the average angle is calculated. This is normalized
to create za. In addition, both zm and za are modulated by sine waves whose
frequencies and phases are evolvable. The result is that a variety of coloration
scenarios are possible, sometimes accentuating hidden features in the ﬂesh.
Fig. 7.5. (a) The default set, (b) rotated, (c) colorized, (d) tweaked
One artistic post-process is added to give the images a sepia tone quality
with a subtle blue shadow eﬀect. While keeping pure black and white at the
extremes, a slight blue shift in the darker range, and a slight orange shift in
the lighter range, are applied.
7.3.2 Morphological Tweakers
The kernel of the Mandelbrot equation is in the two lines in Algorithm 1 that
express the complex number equation z = z2 + c in real number terms:
z1 = y + zy × zy + zx × −zx
(7.1)
z2 = x + 2.0 × zx × zy.
(7.2)
Arbitrary morphological tweakers are inserted. Following is an example of a
typical set of tweakers:
z1 = y + (zy + p1) × (zy × p2) + (zx × p3) × ((zx × p4) + p5) × p6(7.3)
z2 = x + p7 × (zx + p8) × (zy + p9)
(7.4)
where p1 through p9 are real number variables. Their default values are as
follows: p2, p3, and p4 are set to 1; p6 is set to −1; and p7 is set to 2. The rest
are set to 0. Each tweaker can deviate from its default value within a range
(extending in both the negative and positive directions). These ranges are
unique for each tweaker – most of them are around 1 or 2. Each tweaker con-
trols a unique visual behavior. For instance, p2 is responsible for the distortion
shown in Fig. 7.5(d).
In addition to these morphological tweakers, the following lines:
zm = z1 × z1 + z2 × z2
zx = z2
zy = z1

7 Evolving the Mandelbrot Set to Imitate Figurative Art
153
are expanded as follows:
zm = z1 × z1 + z2 × z2
zm = zm × (1 −p10) + z2 × p10
zx = z2 × (1 −p11) + z1 × p11
zy = z1 × (1 −p12) + z2 × p12
where p10, p11, and p12 are 0 by default. Also, before the iterative loop, the
lines:
zx = 0
zy = 0
are expanded to:
zx = p13
zy = p14
where p13 and p14 are set to 0 as default.
Since the kernel of the Mandelbrot equation could also be expressed as:
z1 = y + zy × zx + zx × zy
(7.5)
z2 = x + zx × zx −zy × zy
(7.6)
a diﬀerent set of tweakers could be applied as follows:
z1 = y + (zy × t1 + t9) × zx × t2 + t10)
+ (zx × t3 + t11) × zy × t4 + t12)
(7.7)
z2 = x + (zx × t5 + t13) × zx × t6 + t14)
−(zy × t7 + t15) × zy × t8 + t16).
(7.8)
This is a more orderly expansion, and it provides a larger genetic space than
the p1–p9 shown in the original expression (Equations 7.1 and 7.2). New and
intriguing forms exist in this space as well.
The example of tweakers just shown is not the whole story. To describe all
the variations that have been tried could take potentially many more pages.
These examples should give a general sense of how the technique is applied.
The reader, if interested in trying out variations, is encouraged to choose a
unique set of tweakers appropriate to the speciﬁc goal. Mandeltweaking is
more an Art than a Science.
These tweakers are stored in an array as real number values within varying
phenotype-speciﬁc ranges (the phenotype array). They are generated from an
array of genes (the genotype array). The genotype holds normalized represen-
tations of all the tweakers, in the range 0–1. Before an image is generated,
the genotype is mapped to the phenotype array, taking into consideration the

154
Ventrella
tweakers’ default values and ranges. This normalized genotype representation
will come in handy as explained later when a genetic algorithm is applied to
the technique.
Besides morphology tweaking, the entire image can be transformed. There
are four tweakers for this which determine:
1. angle of rotation;
2. magniﬁcation (uniform scaling in both x and y);
3. translation in x; and
4. translation in y.
These can be thought of as transformations of the digital microscope that
views the complex plane.
When these tweakers are set to their default values, the function produces
the Mandelbrot Set. Any oﬀset from a default value will push it out to a
superset of the Mandelbrot Set, called Mandeltweak, a dimension that does not
obey the normal rules of complex numbers. All experiments can be considered
as deviations from the true Set – deviations from its home in the complex
plane. This is the control point – the point of registration from which to build
a visual vocabulary describing this multidimensional genetic space.
7.3.3 Interactive Evolution for Artistic Breeding
Originally, the Mandeltweak software was given an interface to generate im-
ages which could be repeatedly reviewed and altered, until interesting and
provocative forms were resolved. These sessions sometimes involved hundreds
of adjustments. Considering the accumulated memory in the artist’s mind
of the variations being explored, you could say that a sort of wetware ge-
netic algorithm was being run, resulting in convergence towards a desired im-
age. These sessions were sometimes very long. These ﬁnal images were stored
mostly as photographs, and exhibited in art shows and on the web [20]. Fig-
ure 7.6 shows six examples of images created with this process.
This experience set the stage for developing a modiﬁed genetic algorithm
with an interactive evolution interface, whereby the artistic choices could be
stored in a population of tweak settings, and re-circulated within the popula-
tion to oﬀer up combinations of favorite images. This was a great improvement
– a natural application of evolutionary programming to the problem domain.
This interactive evolution scheme is described below.
1. Initialization
A population of genotypes is generated, with their genes initialized randomly,
distributed evenly in the range (0–1). Population size is usually set to around
100. The genotypes are associated with initially random ﬁtness values rang-
ing from 0 to 1 (which is meaningless at ﬁrst, but as explained below, ﬁtness

7 Evolving the Mandelbrot Set to Imitate Figurative Art
155
Fig. 7.6. Some early examples of Mandeltweaks
values will gradually change to take on meaning).
2. Iteration
The iterative loop has three basic components: (a) mating, (b) evaluation,
and (c) death. This is explained below.
(a) Mating via Tournament Selection
Two random, relatively ﬁt genotypes are chosen as parents, each by way of a
competition for relative ﬁtness, as follows:
1. Two genotypes are randomly chosen, and their ﬁtness values are com-
pared. The one with the highest ﬁtness is labeled “parent 1”.
2. Another competition is run with two other randomly-chosen genotypes,
and the winner is chosen as “parent 2”.
The two resulting parent genotypes mate to produce one oﬀspring genotype
using crossover, with some chance of mutation. During mating, standard
crossover and mutation techniques are used, with crossover rate C = 0.2,
and mutation rate m ranging from 0.01 to 0.1 in most experiments. While
parent genotypes are being read to generate the oﬀspring genotype, gene-by-
gene, there is C chance of the parent genotype which is being copied to the
oﬀspring to swap to the other parent. And there is m chance that the gene
being read will mutate. If mutated, a random number is added to the gene
value, which ranges between −1 and 1, and is weighted towards zero. If af-
ter mutation, the gene value falls out of the normal interval 0 to 1, it wraps

156
Ventrella
around to keep the value normalized.
(b) Evaluation
The resulting oﬀspring genotype is then mapped to a phenotype array to
determine the tweakers for generating a new Mandeltweak image. The user
evaluates this image by giving it a value in the range of (0–1). Diﬀerent ver-
sions have been explored as far as inputting this value, including binary (0 =
bad vs. 1 = good); a three-choice scheme (bad-medium-good); and continuous
(clicking the mouse on the screen, with the location from left to right or bot-
tom to top determining a value from 0 to 1). There are pros and cons to each
of these input techniques, which will not be covered here – what’s important
is that a value ranging from 0–1 is provided by the user.
(c) Death
Once a ﬁtness value has been provided for this image, the associated genotype
replaces the least-ﬁt genotype in the population.
This process is iterated indeﬁnitely. In the beginning, the user experiences
no progress, especially in the case of large populations (like more than 100),
but in time, the choices that the user has been making start to eﬀect the
population, and images begin to come up that are preferable, having visual
qualities that the user has been responding positively to.
In some experiments, the selection of parent genotypes is set to not take
relative ﬁtness into consideration, and so any two genotypes can become par-
ents. In this case, the only driving force for evolution is the fact that the
least-ﬁt genotype is killed oﬀto make room for the oﬀspring. This causes
slower convergence, but more thorough exploration of the genetic space. It
appears not to have a major impact on the outcome.
7.3.4 One Image at a Time, One Mating at a Time
A common design in interactive evolution schemes is to present the user with a
collection of images with variation, and for the user to compare these and make
some selection based on that comparison. A major diﬀerence in the technique
described here is that the user is presented with only one image at a time,
and uses visual memory to compare with other images seen. This interaction
design is intended to enhance the experience of perceiving an individual image
as a work of art. It allows the aesthetic sense to operate more like viewing
art than shopping for a product. This interface is meant to allow the process
of interactive evolution to be pure and direct – an evolving dialog between
the artist’s visual memory and a progression of fresh images, with an arc of
aesthetic convergence that threads through the experience.
Many genetic algorithm schemes use generational selection: all the geno-
types in the population are sized-up for ﬁtness in one step, and then the

7 Evolving the Mandelbrot Set to Imitate Figurative Art
157
entire population is updated to create a new generation of genotypes, which
selectively inherit the genetic building blocks from the previous generation.
For software implementation, a backup population is required in order cre-
ate each new generation. In contrast, this scheme uses steady-state selection:
it keeps only one population in computer memory, and genetic evolution is
performed on that population one genotype at a time. This is slower than gen-
erational selection, but it has the eﬀect of preserving the most ﬁt genotypes
at all times. And the grim reaper only visits the least ﬁt.
7.3.5 Fitness Decay
At the start of the process, ﬁtness values are randomly distributed from 0 to
1. Since relatively-ﬁt genotypes are chosen to mate and oﬀer up Mandeltweak
images for evaluation, the ﬁrst images reviewed are essentially random in their
relative quality. But this soon begins to change as the user provides meaningful
ﬁtness values. In addition to this, a global decay scalar d is applied to all ﬁtness
values at each iteration (d = just under 1, typically 0.99). The eﬀect of d at
the beginning of the process is that genotypes that were randomly initialized
with high ﬁtness values decrease as new genotypes rise to the top of the ﬁtness
range as a result of positive user selection. The distribution of ﬁtness values
begins to reﬂect user choice.
As the initial random distribution of ﬁtness values gives way to meaningful
values from user interaction, the decay operator then begins to serve a diﬀerent
purpose: that of allowing for meandering aesthetic goals. Genotypes that were
once considered ﬁt are allowed to decay. The decay eﬀect roughly corresponds
to memory. This avoids having the highest ﬁt genotypes continually dominate
the population and prohibit new discoveries to take the lead. If there were no
ﬁtness decay, the population would lose any ﬂexibility to respond to changes
in the aesthetic trajectory. The ﬁtness decay operator is like the evaporation
of ant pheromones – chemicals released by ants for communicating which
build up in the environment and permit ants to establish trails for foraging.
If pheromone scent never decayed, ant colonies would not be able to adapt
to changing distributions of food, and their collective behavior would become
rigid. Same with this ﬁtness decay: it gives the user a chance to push the
population in new directions when an aesthetic dead-end is reached.
The value d is sensitive. If it is set too low (like, 0.9 – causing ﬁtness
to decay too quickly), then the results of the user’s choices will not stay
around long enough to have an eﬀect on the general direction of evolution.
If it is too weak (like 0.999 – decreasing too slowly) then inertia sets in:
user selections that were either “mistakes” (or choices that are no longer
relevant) will stick around too long and make evolution inﬂexible. This value
is sensitive to population size, user psychology, the nature of the evolvable
imagery, mutation rate, and other factors.
The interactive evolution technique just described has a few notable prop-
erties:

158
Ventrella
1. it always preserves the most ﬁt genotypes (imagine looking at a picture,
liking it, and choosing to keep it in a box for future use – you can rely on
it being there for a long time);
2. it always overwrites the least-ﬁt genotype, which has the eﬀect of increas-
ing average ﬁtness over time (note that the least-ﬁt genotype fell to its
place either because the user put it there directly by selection, or else it
slowly “faded into the remote past” as a result of ﬁtness decay – in both
cases, it is appropriate to replace it);
3. it allows user aesthetics to change direction over time, and to redirect the
population.
7.3.6 Using a Digital Image as a Fitness Function
The latest stage in this progression towards automating the process is to use
an image as an objective ﬁtness function. Instead of a user providing the ﬁtness
of a Mandeltweak based on aesthetics, the Mandeltweak is compared to an
ideal image to determine similarity. The genetic algorithm for this scheme is
the same as the interactive evolutionary scheme described above, except for
three important diﬀerences:
1. The human user is replaced by an image-comparison algorithm, which
uses a single ideal image.
2. There is no ﬁtness decay operator. Fitness decay is a psychological mech-
anism, and is not needed in this case. Since the ideal image is static (as
opposed aesthetic whim), there is no need for the ﬂexibility that decay
aﬀords.
3. Instead of setting all ﬁtness values randomly at initialization, the initial
genotypes are used to generate an initial population of Mandeltweaks to
establish meaningful ﬁtness values. This becomes the starting point for
the iterative loop.
The ideal image is either painted in Photoshop, pulled oﬀof a website, or
snapped with a digital camera and then post-processed with Photoshop. Only
gray-scale images are used, for three reasons:
1. it reduces the complexity of the experiment to fewer variables;
2. the addition of color was not found to contribute signiﬁcantly to the per-
ception of ﬁgurative form in the images; and
3. it was an artistic choice: to encourage the resulting images to resemble
black-and-white portrait photography.
All ideal images have a white background, to simplify the technique and to
disambiguate ﬁgure vs. ground.

7 Evolving the Mandelbrot Set to Imitate Figurative Art
159
7.3.7 Image Resolution
It was found that comparing a Mandeltweak image with the ideal image could
be done adequately using a resolution r of only 50 pixels. So, the ideal image
consists of r2 (50 × 50 = 2500) pixels, where each pixel color is a shade of
gray ranging from black to white in g = 256 possible values (0 ≤g ≤1255).
When a Mandeltweak is generated so as to compare with the ideal image to
calculate a ﬁtness value, it is rendered at the same resolution. The images are
compared, pixel-by-pixel, and so there are r2 pixel value diﬀerences used to
determine the diﬀerence between the images, and thus the ﬁtness.
Note that even though the Mandeltweak is rendered at a speciﬁc resolution
for comparison, this does not mean that it could not be re-rendered at a higher
resolution. In fact, since the genotype is what constitutes the representation
of the image (not the pixel values), it could be re-rendered at any arbitrary
resolution. What r represents, then, is the amount of image detail that could
potentially be compared. And since the mimicking ability of Mandeltweak
is limited only to general forms and approximate gray-scale values, it is not
necessary to present it with high-resolution ideal images – the extra detail
would be wasted.
7.3.8 Image Comparison
To illustrate how the comparison scheme works, consider the ideal image in
Fig. 7.7 of a black disk against a white background.
Fig. 7.7. Five examples showing images compared to an ideal image (black disk)
Compare each image to the right of the black disks in examples (a) through
(e). In example (a), every pixel value is as diﬀerent as possible (covering the
complete range of pixel diﬀerence: 255), and so the resulting ﬁtness is 0. In
example (b), half of the pixel values are identical, and the other half are as
diﬀerent as possible, and so ﬁtness is 0.5. In example (c), the image is ﬁlled
entirely with gray value 128. In this case, ﬁtness is 0.5, since the diﬀerence
between a gray pixel and either black or white is 128. In example (d), all
pixels are identical, and so ﬁtness is 1.0. Example (e) shows a Mandeltweak
that resulted from evolution in a population using the black disk as the ﬁtness
function. It was able to approach the ideal, reaching a ﬁtness value of 0.8517.
Let’s deﬁne p, (−255 ≤p ≤255) as the diﬀerence in value of a pixel
in the Mandeltweak image and its corresponding pixel in the ideal image.
The normalized pixel diﬀerence is |p/g|. P is the sum of all normalized pixel

160
Ventrella
diﬀerences. Thus, the ﬁtness f of a Mandeltweak image is f = 1 −P/r2,
(0 ≤f ≤1).
This technique uses a simple pixel-wise comparison. A few variations have
been explored in order to encourage sensitivity to certain features. But nothing
conclusive has come of this. There is certainly a lot of research, and many
techniques, for feature-based image comparison, and it would make for an
interesting enhancement to this technique. But for the preliminary purposes
of these experiments, this simple scheme is suﬃcient.
7.4 Experiments
To help visualize the evolution of a population of Mandeltweaks using an ideal
image as the ﬁtness function, each genotype is plotted as a row of rectangles.
The gray value of a rectangle corresponds to the value of its associated gene,
with the range 0–1 mapped to a gray scale from black to white. An example
is illustrated in Fig. 7.8.
Fig. 7.8. Visualization of a genotype with gene values mapped to grayscale
This genotype visualization in used in Fig. 7.9, which shows a population
of 1000 genotypes evolving to imitate an ideal image of the author’s face
(upper left). Four stages of the evolution are plotted, at times 0, 1000, 10,000,
and 50,000. Fitness is visualized as the vertical height of the genotype, and
convergence is revealed as similarity in genotype coloration. The Mandeltweak
with the highest ﬁtness in the population is shown at the top of each plot,
and its ﬁtness value is shown at the left of the plot. The ﬁnal Mandeltweak is
enlarged at right.
At initialization genotypes are randomized and their associated images are
compared to the ideal image to determine ﬁtness. In this particular experi-
ment, the ﬁtness values range from just under 0.5 to 0.827 at initialization.
This distribution of ﬁtness values is believed to be due to the three following
factors:
1. the characteristics of the genotype-to-phenotype mapping, and thus the
resulting Mandeltweak images;
2. the ideal image; and
3. the nature of the ﬁtness comparison scheme.
The graph shows that after 1000 iterations the lower end of the ﬁtness
range has raised. This is because the least-ﬁt genotype is always replaced

7 Evolving the Mandelbrot Set to Imitate Figurative Art
161
Fig. 7.9. Plotting ﬁtness and genetic convergence in a population of 1000 genotypes
with the newly-created genotype for each step, and since each new genotype
is the product of two relatively-ﬁt genotypes, it usually has higher ﬁtness.
This is especially the case in the beginning. By time 50,000 we see that the
highest ﬁtness is 0.915. Notice also that the genotypes cover a much smaller
range of ﬁtness and that they have converged considerably (revealing visible
bands of similar colors across the population).
Figure 7.10 shows the results of six experiments, each using a diﬀerent
ideal image as the ﬁtness function.
Fig. 7.10. Six examples of ideal images and evolved Mandeltweaks
In these examples, the ideal images are shown to the left of their associated
Mandeltweaks. The ideal images may appear jagged or pixelated because of
their low resolution. The Mandeltweaks, in contrast, are shown at a higher

162
Ventrella
resolution – recall that pixel resolution is arbitrary in Mandeltweaks, as the
encoding of the image is parametric.
In all of these experiments, population size was set to 1000, and mutation
rate was set to 0.05 except for examples (a) and (c), in which population size
was set to 100 and mutation rate was set to 0.1. In all cases, the highest ﬁtness
achieved was in the approximate range of 0.95. The number of iterations in
each case ranged, averaging around 2000.
7.4.1 Range of Genetic Variation
Each tweaker has a unique range within which it can deviate from its default
value, as explained earlier. To manipulate this range, a global range scale s
was created so that the whole array of range values could be scaled at once.
In most experiments, s is set to 1 (resulting in the normal tweak ranges as
originally designed). But s can be varied to explore Mandeltweak’s imitative
performance over diﬀerent genetic ranges. Figure 7.11 shows the results of 11
experiments with the ideal image set to a portrait of painter Francis Bacon.
Fig. 7.11. Varying tweak ranges in a series of experiments
In each experiment, population size was set to 1000, and mutation rate was
set to 0.05. When s is set to 0.0, the result is that when genotypes are mapped
to phenotypes, the values of the tweakers are clamped to their defaults. And
so the image at the left-most side of Fig. 7.11 has the Mandelbrot morphology.
As s is increased by increments of 0.2, we see that ﬁtness increases on average,
because there is an increasingly larger genetic space available for searching.
The reason the default Mandelbrot image has a ﬁtness of 0.78 has to do with
the nature of the ideal image, the nature of the default Mandeltweak settings,
and the comparison technique. What is of interest, though, is not this value,

7 Evolving the Mandelbrot Set to Imitate Figurative Art
163
but the rate at which ﬁtness increases, and then reaches what appears to be
an upper limit. All experiments indicate a similar limitation, including when
they are run for many more iterations and with much larger populations.
Artistically-speaking, one might ﬁnd the visual “sweet-spot” to occur be-
fore ﬁtness has reached its maximum. In fact, the images in Fig. 7.1 were
intentionally evolved using a slightly smaller range. They also used smaller
populations and were run for fewer iterations – this allowed the peculiar ves-
tiges of Mandelbrot features to remain, making the head-like shapes more
intriguing and ambiguous.
7.4.2 Imitating ... The Mandelbrot Set?
Since the vast genetic space of Mandeltweak contains the Mandelbrot Set at
the point in the space where all values are at their default settings, it is pos-
sible that an initial population of random Mandeltweaks can converge on the
Mandelbrot Set. But in a number of experiments with diﬀerent population
sizes and mutation rates, this was not achieved. Instead, the population con-
verged on other regions of the space which are similar to the shape of the
Set. Figure 7.12 shows the most ﬁt Mandeltweak in a population in multiple
stages of evolution, starting at time 0, then 2000, and then doubling the time
intervals, up to 128,000. It reached a maximum ﬁtness of 0.895. The image
at right is the Mandelbrot Set (a high-res version of the ideal image used) to
show what it was trying to imitate.
Fig. 7.12. Mandeltweak imitates the Mandelbrot Set, only backwards
In this and other similar experiments, the Mandeltweak got stuck on a
local hill in the ﬁtness landscape, which corresponds roughly to the shape,
but it is rotated almost 180 degrees! The ﬁtness landscape is very large, and
the shapes in the initial random population are too varied from the original
shape – and so a common protrusion resulting from tweaking (such as the
one shown in Fig. 7.5d), ends up being a proxy for the main bulb. As a test,
a critical gene, the “angle” gene (responsible for varying the rotation of the
shape in the complex plane), was not allowed to vary. The population was then
able to more easily converge on the Mandelbrot Set. This supports intuition
that the angle gene enlarges the ﬁtness landscape considerably.

164
Ventrella
7.4.3 Using Higher-Order Mandelbrot Functions
The shape created by the higher-order function, z = z3+c, is shown in Fig. 7.4.
This uses more variables in the software implementation, to which tweakers
can be attached, and so it was considered as another base function to explore.
It can be expressed as follows: replace the kernel of the expanded Mandelbrot
function shown in (7.7) and (7.8) with:
a = zx
b = zy
z2 = a × zx −b × zy
z1 = b × zx + a × zy
zx = z2
zy = z1
z2 = a × zx −b × zy
z1 = b × zx + a × zy
z1 = z1 + x
z2 = z2 + y
zx = z2
zy = z1
and tweak like this:
a = zx
b = zy
z2 = (a × p1 + p2) × zx × p3 + p4) −(b × p5 + p6) × zy × p7 + p8)
z1 = (b × p9 + p10) × zx × p11 + p12) + (a × p13 + p14) × zy × p15 + p16)
zx = z2
zy = z1
z2 = (a × p17 + p18) × zx × p19 + p20) −(b × p21 + p22) × zy × p23 + p24)
z1 = (b × p25 + p26) × zx × p27 + p28) + (a × p29 + p30) × zy × p31 + p32)
z1 = z1 + x
z2 = z2 + y
zx = z2
zy = z1.
Figure 7.13 shows the results of ﬁve experiments in which the normal Man-
deltweak algorithm is compared to the one which uses the cubed algorithm
just described. Population was set to 1000, and mutation rate was set to 0.05.
Each experiment was run until the population converged signiﬁcantly, and the

7 Evolving the Mandelbrot Set to Imitate Figurative Art
165
Fig. 7.13. Comparing normal versus cubed algorithm
number of iterations, while varying among pairs of tests, was kept constant
for each algorithm in the pair.
The cubed algorithm doesn’t appear to be much better at imitating the
ideal image, and it might even be inferior, if the stars placed next to the ﬁtness
values in the ﬁgure are any indication. In the case of both algorithms, there
is an inability to imitate local features – notice that the ﬁngers of the hand
and the legs of the horse are not picked up very well using either algorithm.
The reasons for this are not clear. A few possibilities are:
1. the image comparison scheme is not feature-based;
2. the population is too small;
3. the genetic algorithm is not designed appropriately; or
4. these mathematical equations are simply not able to conjure up these
particular shapes, even though the genetic space is very large.
There is more exploration to be done with higher-order Mandelbrot func-
tions, as well as the other varieties of fractal equations.
7.5 Conclusion
Math-based computer art created using evolutionary computation is often
non-objective or abstract – not meant to represent anything in particular.
The Platonic Mandelbrot Set and its kin are neither art, nor are they ab-
stract art. But the curious animal-like nature of Mandeltweak, as far as how
it behaves upon manipulation, invites one to read it as an organic entity,
and thus it enters into an interpretive space, making it more pregnant as an
art medium. This was part of the initial motivation behind the technique

166
Ventrella
described. Its ability to imitate explicit images is limited. But this tension
– the tension between being the Mandelbrot Set and being coerced into a
representational form – is part of the game. It is a kind of conceptual art.
The question of how evolvable an image-making scheme can be is a common
problem in evolutionary art: is the phenotype space large enough? – and can
a subset of it map to the artist’s aesthetic space? The Mandeltweak technique
was created to take on this question. In the process of asking these questions,
and to better understand its limits, the artist’s aesthetic (and mathematical)
vocabulary has grown larger.
References
1. Ashlock, D.: Evolutionary exploration of the Mandelbrot set. In: Proceedings
of the 2006 Congress On Evolutionary Computation, pp. 7432–7439 (2006)
2. eNZed
Blue:
The
Koruandelbrot
(2005).
http://www.enzedblue.com/
Fractals/Fractals.html
3. Dawkins, R.: The Blind Watchmaker – Why the Evidence of Evolution Reveals
a Universe Without Design. W. W. Norton and Company (1986)
4. Dewdney, A.: Computer recreations: a computer microscope zooms in for a look
at the most complex object in mathematics.
Scientiﬁc American pp. 16–25
(1985)
5. Dickerson, R.: Higher-order Mandelbrot fractals: experiments in nanogeometry
(2006). http://mathforum.org/library/view/65021.html
6. Douady, A., Hubbard, J.: Etude dynamique des polynomes complexes, I and II.
Publ. Math. Orsay (1984, 1985)
7. Goldberg, D.: Genetic Algorithms in Search, Optimization, and Machine Learn-
ing. Addison-Wesley (1989)
8. Koza, J.: Genetic Programming: On the Programming of Computers by Means
of Natural Selection. MIT Press (1992)
9. Lakoﬀ, G., N´unez, R.: Where Mathematics Comes From – How the Embodied
Mind Brings Mathematics Into Being. Basic Books (2000)
10. Mandelbrot, B.: The Fractal Geometry of Nature. W. H. Freeman and Company
(1977)
11. McCormack, J.: Open problems in evolutionary music and art. Lecture Notes
in Computer Science 3449, 428–436 (2005)
12. Peitgen, H., Saupe, D. (eds.): The Science of Fractal Images. Springer-Verlag
(1988)
13. Penrose, R.: The Road to Reality, A Complete Guide to the Laws of the Uni-
verse. Knopf (2004)
14. Pickover, C.: Computers, Pattern, Chaos, and Beauty – Graphics From an Un-
seen World. St. Martins Press (1990)
15. Rooke, S.: Eons of genetically evolved algorithmic images.
In: P. Bentley,
D. Corne (eds.) Creative Evolutionary Systems. Morgan Kaufmann, San Fran-
cisco, CA (2001)
16. Sims, K.: Artiﬁcial evolution for computer graphics.
In: Proceedings of the
18th annual conference on Computer graphics and interactive techniques, pp.
319–328 (1991)

7 Evolving the Mandelbrot Set to Imitate Figurative Art
167
17. Todd, S., Latham, W.: Evolutionary Art and Computers. Academic Press (1992)
18. Ushiki, S.: Phoenix. IEEE Transactions on Circuits and Systems 35(7), 788
(1988)
19. Ventrella, J.: Explorations in the emergence of morphology and locomotion be-
haviour in animated characters. In: R. Brooks, P. Maes (eds.) Artiﬁcial Life IV –
Proceedings of the 4th International Workshop on the Synthesis and Simulation
of Living Systems, pp. 436–441. MIT Press (1994)
20. Ventrella, J.: Mandeltweaks (2004).
http://www.ventrella.com/Tweaks/
MandelTweaks/tweaks.html
21. Ventrella, J.: Mandelswarm – particle swarm seeks the boundary of the
Mandelbrot Set (2005).
http://www.ventrella.com/Tweaks/MandelTweaks/
MandelSwarm/MandelSwarm.html

8
Evolutionary L-systems
Jon McCormack
Centre for Electronic Media Art, Faculty of Information Technology, Monash
University, Clayton 3800, Victoria, Australia
Jon.McCormack@infotech.monash.edu.au
Any “novelty,” . . . will be tested before all else for its compatibility
with the whole of the system already bound by the innumerable con-
trols commanding the execution of the organism’s projective purpose.
Hence the only acceptable mutations are those which, at the very least,
do not lessen the coherence of the teleonomic apparatus, but rather
further strengthen it in its already assumed orientation or (probably
more rarely) open the way to new possibilities.
Jacques Monod, Chance and Necessity [17, p. 119]
8.1 Introduction
The problem confronting any contemporary artist wishing to use technology
is in the relationship between algorithmic and creative processes. This rela-
tionship is traditionally a conﬂicting one, with the artist trying to bend and
adapt to the rigour and exactness of the computational process, while aspir-
ing for an unbounded freedom of expression. Software for creative applications
has typically looked to artforms and processes from non-computational me-
dia as its primary source of inspiration and metaphor (e.g. the photographic
darkroom, cinema and theatre, multi-track tape recording, etc.).
The process implicitly to be advanced in this chapter is that we should
turn to natural processes, including computation itself as a creative source of
inspiration. Adopting this approach removes the traditional conﬂict between
algorithmic and creative processes that typiﬁes most software used in creative
applications, liberating the artist to explore computational processes as a
source of inspiration and artistic creativity.
This chapter details the interactive evolution1 of string rewriting gram-
mars, known as L-systems. The method is a powerful way to create complex,
1 Also known as aesthetic evolution, interactive aesthetic selection, or the interactive
genetic algorithm (iga).

170
McCormack
organic computer graphics and animations. This chapter describes an inter-
active modelling system for computer graphics in which the user is able to
evolve grammatical rules. This system has been used by the author for more
than 15 years to produce a number of creative works (see [14] for details).
Starting from any initial timed, parametric D0L-system grammar (deﬁned in
Section 8.2.4), evolution proceeds via repeated random mutation and user
selection. Sub-classes of the mutation process depend on the context of the
current symbol or rule being mutated and include mutation of: parametric
equations and expressions, development functions, rules, and productions. As
the grammar allows importation of parametric surfaces, these surfaces can be
mutated and selected as well. The mutated rules are then interpreted to cre-
ate a three-dimensional, time-dependent model composed of parametric and
polygonal geometry. L-system evolution allows a novice user, with minimal
knowledge of L-systems, to create complex, ‘life-like’ images and animations
that would be diﬃcult and far more time-consuming to achieve by writing
rules and equations explicitly.
8.1.1 L-systems and Graphics
Modern computer graphics and computer aided design (cad) systems allow
for the creation of three-dimensional geometric models with a high degree of
user interaction. Such systems provide a reasonable paradigm for modelling
the geometric objects made by humans. Many organic and natural objects,
however, have a great deal of complexity that proves diﬃcult or even impos-
sible to model with surface or csg based modelling systems. Moreover, many
natural objects are statistically similar, that is they appear approximately the
same but no two of the same species are identical in their proportions.
L-systems have a demonstrated ability to model natural objects, particu-
larly branching structures, botanical and cellular models. However, despite the
ﬂexibility and potential of L-systems for organic modelling (and procedural
models in general), they are diﬃcult for the non-expert to use and control. To
create speciﬁc models requires much experimentation and analysis of the ob-
ject to be modelled. Even an experienced user can only create models that are
understood and designed based on their knowledge of how the system works.
This may be a limitation if the designer seeks to explore an open-ended design
space. The method presented here gives the artist or designer a way of ex-
ploring the “phase space” of possibilities oﬀered by L-system models without
an explicit understanding of how to construct the grammar that generates a
given model.
8.1.2 Related Work
Interactive evolution is a well-explored method to search through combina-
torial computational spaces using subjective criteria. As with all Evolution-
ary Computing (ec) methods, it is loosely based on a Darwinian evolution

8 Evolutionary L-systems
171
metaphor, with the ﬁtness of each generation explicitly selected by the user.
The method was ﬁrst described by Richard Dawkins in his 1986 book The
Blind Watchmaker [4]. Dawkins demonstrated simulated evolution by evolv-
ing biomorphs – simple two-dimensional structures with a lateral symme-
try resembling organic creatures in form. The survival of each generation of
biomorphs is selected by the user who evolves features according to their per-
sonal selection.
In the 20 years since The Blind Watchmaker was published, a number of
applications of the technique have been published. An extensive overview can
be found in [24]. Applying evolutionary methods to generate L-systems has
also been extensively researched, [12] provides a summary.
The next section provides a brief overview of the interactive evolution pro-
cess. Section 8.2 looks at the syntax and structure of L-systems used for in-
teractive evolution. Section 8.3 shows how the strings produced by L-systems
may be interpreted into complex geometric structures, suited to the auto-
mated modelling of organic form. Section 8.4 explains the mutation process
necessary for the evolutionary process to ﬁnd new genotypes. Finally, Sect. 8.6
summarises the results of the system, including a case study of a recent com-
missioned artwork. The artwork was produced using software that implements
the methods described in this chapter.
8.1.3 Overview of the Interactive Evolution Process
In nature, genetic variation is achieved through two key processes: mutation of
the parent genotype; and crossing over of two parental genotypes in the case of
sexual species. In the system described here, ‘child’ genotypes are created by
mutating a single parent genotype. Such mutations cause diﬀerent phenotypes
to result. Changes may aﬀect structure, size, topology and growth. Through
a repeated process of selection by the user and mutation by the computer,
aesthetic characteristics of the resultant forms can be optimised (the genetic
algorithm can be thought of as an optimisation process).
Typically, the user of the system begins with an existing L-system that
they wish to evolve. This germinal genotype becomes the ﬁrst parent from
which oﬀspring are mutated. A library of existing L-systems is available for
the user to select from to begin the evolutionary process. It is also possible to
evolve ‘from scratch’, i.e. with only implicitly deﬁned identity productions in
the germinal genotype.
At each generation, 11–15 mutated oﬀspring from the parent genotype are
created. Each phenotype is a time-variant, three-dimensional structure that
can be manipulated in real-time on screen. The user examines each oﬀspring,
selecting the child with the most appealing traits to become the new parent.
The process is repeated for as long as necessary.2
2 Normally until a satisfactory form is found, or the user runs out of patience.

172
McCormack
Following the evolutionary process, the user has the option of saving
the mutated genotype in the library. Alternatively, the phenotype (three-
dimensional geometric data, shading and lighting information) can be ex-
ported to software or hardware based rendering systems for further visualisa-
tion and processing. The dataﬂow of the system is outlined in Fig. 8.1.
Initial germinal
genotype
Evolution by
mutation and
selection.
L-system
genotype
library
Select initial genotype
Evolved Model
(genotype)
Genotype may be stored in library
Phenotype
(geometric
model)
Generate 3D model
from genotype
Geometric, lighting &
shading data
External
surfaces
library
Surface Evolution
by mutation and
selection.
Pre-defined surfaces as needed
Implicit references
Software
Renderer
Hardware
Renderer
Images &
Animation
Real time
display
Fig. 8.1. The initial genome is selected from an existing library to undergo the evo-
lutionary processes described in this chapter. Following evolution the genotype can
be (optionally) saved into the library or output to rendering systems for visualisation
The interactive evolution process is a useful technique for the aesthetic
exploration of parameterised computational systems, however is not without
problems, both technical and creative [5,15].
8.2 L-systems
Since the original formulation by Lindenmayer in 1968 [10], L-systems have
been adapted to modelling a diverse range of phenomena, including herba-
ceous plants [19], neural networks [9] and the procedural design of cities [18].
This has necessarily involved many diﬀerent formulations and extensions be-
yond Lindenmayer’s original formalism. Fundamentally however, all L-systems
are iterative symbol rewriting systems, where strings of symbols are replaced
in parallel according to a set of rules, known as productions. For detailed
descriptions, the reader is referred to [19,21,22].
In the following sections, I will brieﬂy review a number of L-system vari-
ants: deterministic, stochastic, parametric and timed. Only context-free L-
systems (0L-systems) will be considered here. The variants described can be
subject to evolutionary algorithms, detailed in Sect. 8.4.

8 Evolutionary L-systems
173
8.2.1 0L-systems
0L-systems are the simplest class of L-system, being context-free, interaction-
less, or zero-sided. Following the terminology and presentations found in [7,
8,19,21], a formal description is presented below.
A context-free, 0L-System is deﬁned as the ordered triple G = ⟨V, ω, P⟩,
where:
•
V = {s1, s2, . . . , sn} is an alphabet composed of a set of distinct symbols,
si;
•
ω ∈V +, a non-empty word (sequence of symbols) over V , known as the
axiom;
•
P ⊂V × V ∗an endomorphism deﬁned on V ∗, known as the ﬁnite set of
productions.
A production (s, χ) ∈P is written in the form s →χ, where the symbol
s is known as the predecessor and the word χ ∈V ∗the successor of the
production. Where there is no speciﬁc production for a symbol s, the identity
production s →s is assumed.
Deterministic L-systems (D0L-systems) have at most one production for
each symbol. A 0L-system is deterministic if and only if for each s ∈V there
is exactly one χ ∈V ∗such that s ∈χ.
The L-system, G, applies a production to a symbol, s, in a word when
the predecessor for that production matches s. The notation s →χ means
module s produces a word χ as a result of applying a production in G.
Let μ = s1s2 . . . sk be an arbitrary word over V . The word ν = χ1χ2 . . . χm
is generated by G from μ, denoted μ ⇒ν, iﬀsi →χi ∀i : i = 1, 2, . . . , k.
G generates a developmental sequence of words μ0μ1 . . . μn by applying the
matching productions from P at each iteration, beginning with the axiom, ω.
That is μ0 = ω and μ0 ⇒μ1 ⇒. . . ⇒μn. A word ν is a derivation of length
n if there exists a developmental sequence such that ν = μn.
Here is a simple D0L-system example:
V = {F, R, L, [, ]}
ω : F
p1 : F →FFR[RFLFLF]L[LFRFRF]
(8.1)
which generates the sequence of words:
ω = μ0 = F
μ1 = FFR[RFLFLF]L[LFRFRF]
μ2 = FFR[RFLFLF]L[LFRFRF]FFR[RFLFLF]L[LFRFRF]R
[RFFR[RFLFLF]L[LFRFRF]LFFR[RFLFLF]L[LFRFRF]
LFFR[RFLFLF]L[LFRFRF]]L[LFFR[RFLFLF]L[LFRFRF]
RFFR[RFLFLF]L[LFRFRF]RFFR[RFLFLF]L[LFRFRF]].

174
McCormack
As can be seen, the size of the string increases rapidly under such a production,
illustrating the generative nature of productions. Moreover, it is easy to see
recursive patterns developing in the produced strings, leading to self-similarity
in the visualisations of the string (Fig. 8.5 below).
8.2.2 Stochastic L-systems
In the D0L-system deﬁned in the previous section, application of productions
was deterministic, meaning the L-system will always generate an identical de-
velopmental sequence. One way of introducing variation into produced strings
is to incorporate a probabilistic application of productions.
A stochastic 0L-system is an ordered quadruplet Gπ = ⟨V, ω, P, π⟩where:
•
the alphabet, V , axiom ω and set of productions, P are as deﬁned for
D0L-systems in Sect. 8.2.1;
•
the function π : P →(0, 1] is called the probability distribution and maps
the set of productions to a set of production probabilities.
Let ˆP(s) ⊂P be the subset of productions with s as the predecessor
symbol. If no production for s is speciﬁed the identity production, s →s is
assumed. Each production pi ∈ˆP(s) has with it an associated probability
π(pi), where

pi∈ˆ
P (s)
π(pi) = 1.
(8.2)
The derivation μ ⇒ν is known as a stochastic derivation in Gπ if for each
occurrence of s in the word μ, the probability of applying production pi is equal
to π(pi). In a single word, diﬀerent productions with the same predecessor
may be applied in a single derivation step. Selection is weighted according
to the probabilities associated with each production with the appropriate
predecessor.
A production in a stochastic L-system is notated pi : s
π(pi)
−−−→χ.
8.2.3 Parametric L-systems
Parametric L-systems were proposed to address a number of shortcomings in
discrete L-system models, particularly for the realistic modelling of plants.
As string rewriting systems, L-systems are fundamentally discrete in both
production application and in relation to the symbolic representation of the
strings themselves. This discrete nature makes it diﬃcult to model many
continuous phenomena or accurately represent irrational ratios. The appli-
cation of parametric L-systems to plant modelling was extensively developed
by Hanan [7], the deﬁnition below is based on this work.
Parametric L-systems associate a vector of real-valued parameters with
each symbol, collectively forming a parametric module. A module with symbol
s ∈V and parameters a1, a2, . . . , an ∈R is written s(a1, a2, . . . , an). Strings

8 Evolutionary L-systems
175
of parametric modules form parametric words. It is important to diﬀerentiate
the real-valued actual parameters of modules, from the formal parameters
speciﬁed in productions. In practice, formal parameters are given unique3
identiﬁer names when specifying productions.
A parametric 0L-system (p0L-system) is deﬁned as an ordered quadruplet
G = ⟨V, Σ, ω, P⟩where
•
V = {s1, s2, . . . , sn} is an alphabet composed of a set of distinct symbols,
si;
•
Σ the set of formal parameters;
•
ω ∈(V × R∗)+, a non-empty parametric word known as the axiom; and
•
P ⊂(V × Σ∗) × C(Σ) × (V × E(Σ)∗)∗the ﬁnite set of productions.
C(Σ) and E(Σ) are the sets of correctly constructed logical and arithmetic
expressions with parameters from Σ. Logical expressions evaluate to Boolean
values of TRUE and FALSE, arithmetic expressions evaluate to real numbers.
Expressions are composed of relational operators (e.g. <, >, ≤, ≥, =, ̸=), logi-
cal operators !(not), &(and), |(or), arithmetic operators (e.g. +, −, ÷, ×, unary
−, etc.), trigonometric, stochastic and other functions, and parenthesis ‘(’, ‘)’.
Productions (s, C, χ) are denoted s : C →χ where the formal module
s ∈V ×Σ∗is the predecessor, the logical expression C ∈C(Σ) is the condition
and χ ∈(V × E(Σ)∗)∗is the formal parametric word known as the successor.
A formal parameter appears once in the predecessor. The number of formal
parameters must be consistent for any given symbol and match the number
of actual parameters for the same symbol. A production without a condition
has an implicit condition constant value of TRUE.
A production is applied to a module in a parametric word if the produc-
tion matches that module. The necessary conditions for matching are: if the
module and production predecessor symbols and parameter counts match;
the condition statement, C, evaluates to TRUE when the module’s actual pa-
rameters are bound to the formal parameters as speciﬁed in the predecessor
module. When a module is matched it is replaced by the successor word, χ,
whose formal parameters are evaluated and bound to the corresponding actual
parameters.
Parametric L-system Examples
This parametric L-system
ω : A(1, 1)
p1 : A(x, y) →A(y, y + x)
(8.3)
generates the derivation sequence
A(1, 1) ⇒A(1, 2) ⇒A(2, 3) ⇒A(3, 5) ⇒· · ·
3 Within the scope of the associated production.

176
McCormack
calculating the Fibonacci sequence. This parametric L-system
ω : A(1, 1)
p1 : A(x, c) : x × x ̸= c →A
(x + c
x)
2
, c

(8.4)
uses the conditional application of p1 to compute the square root of the second
parameter of A (labelled c) using Newton’s method:
A(1, 2) ⇒A(1.5, 2) ⇒A(1.416667, 2) ⇒A(1.414216, 2) ⇒· · ·
For parametric L-systems to be deterministic no two productions can
match the same module in a derivation word by the rules above. However,
ensuring determinism by these criteria can be diﬃcult to prove for all possible
combinations of parameter values, hence a practical solution is to order the
set of productions and apply the ﬁrst production that matches in the list. If
there are no matching productions, the identity production is assumed and
the parameters for that module remain unchanged.
8.2.4 Timed L-systems
The development of parametric L-systems addressed problems with D0L-
systems in representing irrational ratios and adding continuous components
to individual modules. However, development still proceeds in discrete, incre-
mental steps. While it is possible to simulate continuous development using
very ﬁne increments, this complicates the grammar, requiring the developer
to ﬁght against the elegance and simplicity of the basic L-system formalism.
The symbolic and discrete nature of an L-system alphabet is inherently
suited to a stepwise topological and structural description, whereas the devel-
opmental aspects may be more suited to a continuous model. It is conceptually
elegant to separate model development (which is continuous) from model ob-
servation (which is discrete). With these points in mind, the concept of timed
L-systems was introduced [19, Chap. 6], the authors seeing an analogy with
the theory of morphogenesis advanced by Thom [25] where development is a
piecewise continuous process, punctuated by catastrophes.
The description that follows is based on [11]. We assume the deﬁnition
of parametric 0L-systems in Sect. 8.2.3. Timed L-systems include continu-
ously variable module ages that permit continuous temporal and spatial de-
velopment that is diﬃcult or impossible to achieve with conventional 0L- or
nL-systems.
In this case, modules consist of timed symbols with associated parame-
ters. For each module, the symbol also carries with it an age – a continuous,
real variable, representing the amount of time the module has been active
in the derivation string. Strings of modules form timed, parametric words,
which can be interpreted to represent modelled structures. As with paramet-
ric L-systems, it is important to diﬀerentiate between formal modules used in

8 Evolutionary L-systems
177
production speciﬁcation, and actual modules that contain real-valued param-
eters and a real-valued age.
Let V be an alphabet, R the set of real numbers and R+ the set of positive
real numbers, including 0. The triple (s, λ, τ) ∈V × R∗× R+ is referred to as
a timed parametric module (hereafter shortened to module). It consists of the
symbol, s ∈V , its associated parameters, λ = a1, a2, . . . , an ∈R and the age
of s, τ ∈R+. A sequence of modules, x = (s1, λ1, τ1) · · · (sn, λn, τn) ∈(V ×
R∗× R+)∗is called a timed, parametric word. A module with symbol s ∈V ,
parameters a1, a2, . . . , an ∈R and age τ is denoted by (s(a1, a2, . . . , an), τ).
A timed, parametric 0L-system (tp0L-system) is an ordered quadruplet
G = ⟨V, Σ, ω, P⟩where:
•
V is the non-empty set of symbols called the alphabet;
•
Σ is the set of formal parameters;
•
ω ∈(V ×R∗×R+)+ is a non-empty, timed, parametric word over V called
the axiom; and
•
P ⊂(V × Σ∗× R+) × C(Σ) × (V × E(Σ)∗× E(Σ))∗is a ﬁnite set of
productions.
A production (a, C, χ) is denoted a : C →χ, where the formal module
a ∈V × Σ∗× R+ is the predecessor, the logical expression C ∈C(Σ) is the
condition, and the formal timed parametric word χ ∈(V × E(Σ)∗× E(Σ))∗
is called the successor.
Let (s, λ, β) be a predecessor module in a production pi ∈P and
(s1, λ1, α1) · · · (sn, λn, αn) the successor word of the same production. The
parameter β ∈R+ of the predecessor module represents the terminal age of
s. The expressions, αi ∈E(Σ), i = 1, . . . , n sets the initial or birth age. Birth
age expressions are evaluated when the module is created in the derivation
string. This nomenclature is illustrated in Fig. 8.2.
( A(p1,p2,...pn ),  β )
( B(e1,e2,...en ),  α )
(formal) module
symbol
symbol
terminal age
birth age
(expression)
parameter list
parameter list
(expressions)
predecessor module
successor module
Fig. 8.2. Nomenclature for predecessor and successor modules in a timed L-system

178
McCormack
As with parametric 0L-systems, if the condition is empty the production
can be written s →χ. Formal and actual parameter counts must be the same
for any given symbol.
Here are some example productions:

A(j, k), 3.0
	
: j < k →

B(j × k), 0.0
	
C(j + 1, k −1), 0.5
	
(8.5)

A(t), 5.0
	
→

A(t + 1), 5.0/t
	
.
(8.6)
It is assumed:
•
For each symbol s ∈V there exists as most one value of β ∈R+ for any
production pi ∈P where (s, λ, β) is the predecessor in pi. If s does not
appear in any production predecessor then the terminal age of s, βs = ∞
is used (eﬀectively the module never dies).
•
If (s, λ, β) is a production predecessor in pi and (s, λi, αi) any module that
appears in a successor word of P for s, then β > αi when αi is evaluated
and its value bound to αi (i.e. the lifetime of the module, β −αi > 0).
Development proceeds according to some global time, t, common to the
entire word under consideration. Local times are maintained by each mod-
ule’s age variable, τ (providing the relative age of the module). A derivation
function is used to obtain the derivation string at some global time, t (see [11]
for details of this function). The derivation function guarantees a derivation
string for any value of t, thus elegantly providing a continuous development
that can be discretely sampled at arbitrary time intervals. This sampling has
no eﬀect on module or derivation string development.
Growth Functions
Any module in the development string may have associated with it a growth
function, gs : (R∗× R+) →R. The purpose of the growth function is to
relate a module’s age to changes in its parameters. For example a module
may represent a developing sub-branch of a tree, whose radius and length
change as the segment ages. The growth function for this module relates the
module’s age with parameters representing these attributes.
The growth function may involve any of the module’s parameters, the
current age, τ, and the terminal age β of s (determined by the predecessor
of the production acting on s, the unique symbol of the module under con-
sideration). Thus gs is a real valued function that can be composed of any
arithmetic expression E(λs, τs, βs). In addition to the formal parameters sup-
plied, expressions can include the operators, numeric constants, and a variety
of functions, some of which are illustrated in Fig. 8.3. The development func-
tion returns a real value, which is then used as a scaling factor for the actual
parameter vector λ. That is:
λ′ = gs · [λ].
(8.7)

8 Evolutionary L-systems
179
1
0
a
step(a,x)
1
1
0
0
min
max
pulse(min,max,x)
max
min
min
max
clamp(x,min,max)
1
0
min
linearstep(min,max,x)
max
1
0
min
smoothstep(min,max,x)
max
k0
k1
k2
k3
k4
k5
spline(x,k0,k1,k2,k3,k4,k5)
Fig. 8.3. Sample functions used for composing growth functions
The growth function is evaluated whenever a module requires turtle inter-
pretation (explained in Sect. 8.3), with parameter vector λ′ sent to the turtle,
rather than λ as is the case with parametric L-systems. No constraints are
placed on the range or continuity of gs, however if continuity is required when
a production is applied (such as the accurate modelling of cellular develop-
ment), gs must be monotonic and continuous. These constraints extend to the
development functions for those symbols that are related as being part of the
successor deﬁnition of s.
8.3 Turtle Interpretation
Having deﬁned the mechanics of various types of L-systems, the reader might
be curious as to how such formalisms may be turned into pictures. In the case
of the system described in this chapter, we are interested in building three-
dimensional geometric models, which develop over time and may be rendered
as still images or animated sequences. This is achieved by a process known as
turtle interpretation.
The derivation strings of developing L-systems can be interpreted as a
linear sequence of instructions (with real-valued parameters in the case of
parametric L-systems) to a ‘turtle’, which interprets the instructions as move-
ment and geometry building actions. The historical term turtle interpretation
comes from the early days of computer graphics, where a mechanical robot
turtle (either real or simulated), capable of simple movement and carrying a
pen, would respond to instructions such as ‘move forward’, ‘turn left’, ‘pen
up’ and ‘pen down’. Each command modiﬁes the turtle’s current position, ori-
entation and pen position on the drawing surface [1]. The cumulative product
of commands creates the drawing.
In the system described in this chapter, the simulated turtle operates in
three-dimensions and maintains an extensive state, which includes:

180
McCormack
•
the current position, t, a position vector in three-space;
•
the current orientation, an orthogonal basis matrix, composed of three
vectors [H, L, U] representing the turtle’s heading, left and up directions
respectively;
•
a homogeneous transformation matrix, T, used for local transformations
on geometry created by the turtle;
•
the current drawing material, a context dependent value determined by
the type of geometric output;
•
current drawing parameters such as line width, cylinder radius, generalised
cylinder proﬁle shape (see Sect. 8.3.1);
•
a normalised level-of-detail value, used to control the complexity of output
geometry.
In addition, the turtle maintains a ﬁrst-in-last-out (filo) stack. The cur-
rent turtle state may be pushed onto and popped from the stack, permitting
the easy construction of complex structures, such a branches. The turtle po-
sition and orientation co-ordinate system is shown in Fig. 8.4.
H
U
L
–
&
^
+
\
/
t=(tx,ty,tz)
Fig. 8.4. Turtle position and orientation
The turtle reads the derivation string from left to right. Speciﬁc symbols
from the alphabet, V , are interpreted by the turtle. If the symbol has asso-
ciated parameters, the value(s) of the parameter(s) may be used to provide
continuous control of the command. For example the symbol f is interpreted
as ‘move forward by the default move distance’ by the turtle. The parametric
module f(1.5) is interpreted as ‘move forward 1.5 units’. In the case of tp0L-
systems, the module’s growth function would ﬁrst be applied (Sect. 8.2.4). A
subset of the turtle commands is summarised in Table 8.1. Other commands
instance geometric primitives (e.g. spheres, discs, boxes), or change the turtle
state (e.g. modifying the transformation matrix, T).
Recalling the simple D0L-system (8.1) and its derivation (Sect. 8.2.1): the
turtle interpretation of this L-system for μ4 is shown in Fig. 8.5. Note that,
even though this image looks ‘tree like’ it is only two-dimensional. Three-
dimensional examples will be given in the sections that follow.

8 Evolutionary L-systems
181
Table 8.1. Summary of turtle commands. ld and θd are global default values
Command Default Description
f(l)
l = ld
Move l units in the current heading direction, H.
F(l)
Draw a line (2D) or cylinder (3D) of length l, in the
direction of H, updating the current turtle position to
be l units in the direction of H
!(r)
r = ld/10 Set line width (2D) or cylinder radius (3D) to r.
+(θ)
θ = θd
Turn left θ degrees.
-(θ)
Turn right θ degrees.
&(θ)
Pitch down θ degrees.
∧(θ)
Pitch up θ degrees.
\(θ)
Roll left θ degrees.
/(θ)
Roll right θ degrees.
|(n)
n = 1
Turn around n times – equivalent to +(180n).
[
Save the current turtle state onto a ﬁrst-in, last-out stack.
]
Restore the turtle state from the top of the stack.
%
If this is the ﬁrst time this particular symbol has been
encountered: interpret current derivation string, μ, up
to the position of this symbol without generating any
output. Store turtle reference frame with this symbol.
Subsequent readings of the symbol set the turtle reference
frame to the value stored with the symbol.
Fig. 8.5. Turtle interpretation of L-system (8.1) in Sect. 8.2.1

182
McCormack
8.3.1 Advanced Geometry with Generalised Cylinders
The turtle commands discussed in the previous section enable the generation
of simple two- and three-dimensional shapes, constructed with lines (2D) or
cylinders (3D). This is suitable for generating crude ‘tree like’ forms, but
unsuited to more complex organic shapes such as horns, limbs, leaves, stems
or tentacles. Previous solutions to this problem involved designing particular
shapes in an external modelling system and reading them in as pre-deﬁned
surfaces [19]. Special symbols representing the surfaces were included in the
grammar and the geometry instanced by the turtle when the symbol was
encountered in the derivation string. This limits the complexity and variety
of possible forms and is less elegant and compact than a grammar speciﬁcation
that generates all the geometry, rather than relying on external pre-computed
surfaces.
#deﬁne N
10
#deﬁne L
10
#deﬁne R
5
#deﬁne A
10
#deﬁne kl
0.8
#deﬁne kr
0.8
ω : horn(N, L, R, A)
p1 : horn(n, l, r, a) →seg(n, l, r, a)
p2 : seg(n, l, r, a) : n > 0 →!(r) F(l)
∧(a) seg(n −1, l × kl, r × kr, a × 1.1)
#deﬁne N
10
#deﬁne L
10
#deﬁne R
5
#deﬁne A
10
#deﬁne kl
0.8
#deﬁne kr
0.8
ω : horn(N, L, R, A)
p1 : horn(n, l, r, a) →c(1) cseg(n, l, r, a)
p2 : cseg(n, l, r, a) : n > 0 →!(r) F(l)
∧(a) cseg(n −1, l × kl, r × kr, a × 1.1)
A
B
Fig. 8.6. A simple horn deﬁned (A) using cylinders, which leaves noticeable gaps
where the radius and angle of the cylinder changes. In B, this problem is ﬁxed with
the use of generalised cylinders. The parametric L-system generating each model is
shown below the image
The Biologist Stephen Wainwright suggests that the cylinder has found
general application as a structural element in plants and animals [26]. He

8 Evolutionary L-systems
183
sees the cylinder as a logical consequence of the functional morphology of
organisms, with the dynamic physiological processes (function) considered
dependent on form and structure over time. Wainwright distinguishes the
cylinder as a natural consequence of evolutionary design based on the physical
and mechanical properties of cylindrical structures.
The problem of describing more complex cylindrical shapes can be solved
using generalised cylinders, originally developed by Agin for applications in
computer vision [2]. Generalised cylinders have found wide application in
botanical and biological visual modelling [3,13,16,20]. An example is shown
in Fig. 8.6.
The basic principle for creating a generalised cylinder is to deﬁne a series
of cross-sections, possibly of varying shape and size, distributed over some
continuous curve, known as the carrier curve. The cross-sections are connected
to form a continuous surface. This is illustrated in Fig. 8.7.
H
U
L
A
B
C
Fig. 8.7. Construction of a generalised cylinder deﬁned by a turtle. A shows the
cross-sectional elements and how they are oriented according to the turtle reference
frame at cross-section instantiations. The path traced out through the cross sections
represents the path taken by the turtle as it moves between cross-sections, thus
creating a carrier curve. B shows the polygonalisation of the cylinder, and C a
rendered version

184
McCormack
As the turtle moves through space, it receives commands to ‘drop’ par-
ticular cross-section curves (also deﬁned by the turtle and stored in an array
of curves which forms part of the turtle state). Each cross-section is trans-
formed to the turtle reference frame at the time it was dropped. The path of
the turtle as it moves through space forms an implicit carrier curve for the
cross-sections. Cross-sections are automatically swept along the carrier curve,
undergoing possible interpolation, and forming a solid generalised cylinder as
shown in Fig. 8.7. The method of automated turtle instantiation of generalised
cylinders provides a powerful modelling capability for modelling organic form
(Fig. 8.8).
#deﬁne N
10
#deﬁne R
10
#deﬁne L
50
θd = π/4
ω : tree
p1 : tree →seg(1)
p2 : seg(n) : n < N →!(R/n)
/ F(L/n) [ + seg(n + 1) ]
[ −seg(n + 1) ]
p3 : seg(n) : n ≥N →F(L/n)
#deﬁne N
10
#deﬁne R
10
#deﬁne L
50
θd = π/4
ω : tree
p1 : tree →!(R) cseg(1)
p2 : cseg(n) : n < N →!(R/n)
/ C(L/n) [ + cseg(n + 1) ]
[ −cseg(n + 1) ]
p3 : cseg(n) : n ≥N →C(L/n)
A
B
Fig. 8.8. A simple tree-like structure with texture (A) using isolated, simple cylin-
ders, and (B) using generalised cylinders. The L-system generating each form is
shown. The generalised cylinder method requires only one extra symbol and the
same number of productions as the isolated cylinder version

8 Evolutionary L-systems
185
Having now covered a variety of L-systems, turtle interpretation of pro-
duced strings, and modelling of complex shapes using generalised cylinders,
let us now turn to the mutation of L-system rules. These mutations form the
basis of an evolutionary system to explore the possibilities for form generation
using L-systems.
8.4 Mutation of L-system Rules
In order for the structure and form of an L-system generated model to change,
its productions and module parameters must be changed. Small changes in
genotype are usually preferred so as not to completely alter the form on which
it is based. However, for radical change larger amounts of mutation are re-
quired.
There are three principal components of an L-system grammar to which
mutation can be applied:
•
mutation of rules (productions) and successor sequences (Sect. 8.4.1 and
8.4.2);
•
mutation of parameters and parametric expressions (Sect. 8.4.3 and 8.4.4);
•
mutation of growth functions and symbol ages (Sect. 8.4.5).
In addition, predeﬁned surfaces, included as part of the turtle interpreta-
tion may themselves be mutated, using, for example, techniques described by
Sims [23]. The connection between surface mutation and L-system mutation
is illustrated in Fig. 8.1. The two processes proceed independently of each
other, coming together at the generation of the geometric model.
8.4.1 Rule Mutation
For each production, symbols and successors have the possibility of being mu-
tated. For the moment, we assume a D0L-system G = ⟨V, ω, P⟩as deﬁned in
Sect. 8.2.1. Mutation of a single production can be represented pn
⇒
p∗
n,
where pn ∈P is the original production and p∗
n the mutated version. The
probability of each type of mutation is speciﬁed separately. The types of mu-
tations possible are listed below:
(A.1) A symbol from a production successor may be removed; e.g.
pn = {a →ab} ⇒p∗
n = {a →b}.
(A.2) A symbol α, where α ∈χpn may be added to the production successor;
e.g. a →ab ⇒a →abb, where α is b.
(A.3) A symbol α, where α ∈χpn may change to another symbol β, where
β ̸= α; e.g. a →ab ⇒a →bb, where successor a has changed to b.
The above rules work on a previously deﬁned set of symbols, typically
a subset of the L-systems alphabet, ψ : ψ ⊆V . This subset can be
speciﬁed by the user. In addition:

186
McCormack
(A.4) A new symbol, α, where α ∈ψ may be added to the production;
e.g. a →ab ⇒a →abc, where α is c in this example.
In addition, it is sometimes necessary to disallow mutation of certain sym-
bols whose purpose is some kind of control (such as output resolution), or to
limit a search space. Special symbols, such as the brackets (‘[’ and ‘]’) repre-
senting turtle state push and pop operations, need to be matched to prevent
a stack overﬂow or underﬂow. Here there are two options:
•
ignore stack underﬂows and kill any genotypes that reach an upper limit
of stack size when the phenotype is generated;
•
execute a bracket balancing operation following the ﬁnal mutation of a
given production. That is for any given production ensure the number of
‘[’ and ‘]’ symbols is equal. This can be achieved by adding or deleting
bracket symbols as necessary. This is the preferred option.
8.4.2 Production Set Mutation
In addition to individual symbol changes within successors, productions may
be created and deleted. Here we assume a stochastic 0L-system, Gπ =
⟨V, P, ω, π⟩as deﬁned in Sect. 8.2.2:
(B.1) A stochastic production pn with associated probability π(pn) may split
into two stochastic productions, p′
n and p′∗
n , i.e.: pn ⇒{p′
n, p′∗
n }, where
π(p′
n) + π(p′∗
n ) = π(pn). For example: a
0.5
−−→ab ⇒{a
0.3
−−→ab, a
0.2
−−→bb}
The successor of p′
n is the same as that of pn . The successor of p′∗
n is a
mutated version of p′
n, created using the mutations A.1–A.4 as speciﬁed
in the previous section.
(B.2) A new production can be created. Essentially, this is for identity produc-
tions of the form a →a, which are implicitly assumed for all members
of V with no explicit productions. If all members of V have non-identity
productions then this mutation can’t take place. If an identity produc-
tion does exist then the successor is assigned to be a randomly selected
element from the set ψ (deﬁned in the previous section). Note that this
may result in the identity production if the predecessor is an element of
ψ. This rule is superﬂuous to a degree, since if we assume the existence
of identity productions they could be subject to the mutations A.1–A.4
as speciﬁed in the previous section. It is introduced as a convenience
due to the internal representation of productions in the system.
(B.3) An existing production may be deleted. If the production is stochastic
then the productions with the same predecessor gain in probability in
equal amounts totalling to the probability of the deleted rule. e.g.
{a
0.4
−−→ab, a
0.3
−−→bb, a
0.3
−−→abc} ⇒{a
0.5
−−→bb, a
0.5
−−→abc}.

8 Evolutionary L-systems
187
(B.4) The probability of a stochastic production may change. This change is
selected from the interval (−π(pn), 1 −π(pn)), where pn ∈P, is the
production selected for this mutation. The addition or diﬀerence redis-
tributes probabilities over all the other stochastic productions involving
that successor, e.g.
{a
0.5
−−→ab, a
0.5
−−→bb} ⇒{a
0.2
−−→ab, a
0.8
−−→bb}.
8.4.3 Parametric Mutation
We now assume parametric 0L-systems, ⟨V, Σ, P, ω⟩as deﬁned in Sect. 8.2.3.
For the sake of eﬃciency and ease of implementation, symbols may not gain or
lose parameters during mutation. However, new modules4 may be created with
the default number of parameters, or if no default exists, a random number of
parameters up to a ﬁxed limit. Productions involving predecessor parametric
module may split as follows:
(C.1) Productions involving modules with no conditions may split, and gain
conditions. Some examples:
a(l) →a(2l) b(l2) ⇒

a(l) : l ≤10 →a(2l) b(l2),
a(l) : l < 10 →a(2l) c(l/2)
d(s1, x2) →d(x1 + x2, x2 + 1) ⇒
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
d(x1, x2) :
x2 > 8 →
d(x1 + x2, x2 + 1),
d(x1, x2) :
x1 ≤x2 | x2 ≥8 →
d(x1 −x2, x1 + 1).
(C.2) For productions involving modules with conditions, the conditions
themselves may mutate according to the rules speciﬁed in the next
section.
8.4.4 Mutation of Expressions
Parameters of modules on the successor side of productions are expressions.
They are parsed into a tree structure and executed during the application of
productions. For example, the expression (x1 + x2)/x3 can be represented:
x1
x2
x3
+
/
4 A module is a symbol and its associated parameters, and a current age if timed.

188
McCormack
The mutation operations outlined here equate to manipulation of such tree
structures. Let us assume a predecessor module α from a given production
pn ∈P, is undergoing parametric mutation. We also assume α has a series
of associated parameters x1, x2, . . . , xk. We assume a set of binary operators
{+, −, ÷, ×, exp} and the unary operator {−}. Each node on the expression
tree can be recursively subject to mutation by the following rules:
(D.1) If the node is a variable it can mutate to another variable. xi ⇒xj,
where 1 ≤j ≤k and i ̸= j.
The set of possible variables xi and xj are drawn from the set of pa-
rameters associated with the predecessor symbol under consideration
(i, j →[1, n] , i ̸= j).
(D.2) If the node is a constant, it is adjusted by the addition of some normally
distributed random amount.
(D.3) If the node is an operator it can mutate to another operator. Operators
must be of the same ‘arity’ (i.e. unary or binary), e.g. x1 + 5 ⇒x1 × 5.
(D.4) Nodes may become the arguments to a new expression, e.g. x1 + 5 ⇒
x2 × (x1 + 5). This example is illustrated graphically below:
x1
5
x2
+

x1
5
+

(D.5) An expression may reduce to one of its operands, e.g. x1 + 5 ⇒x1.
In the current implementation, only simple arithmetic operators are
supported (addition, subtraction, division, multiplication, negation and
power). Other functions (such as trigonometric functions) could be
added if required. Formal parameters on the left side of productions
do not mutate, as this serves no useful purpose.
8.4.5 Growth Function and Module Age Mutation
If a module is timed it must have a birth age (α) and terminal age (β).
Both these values must be constants.5 Both constants can be mutated by the
addition of a normally-distributed random value, with mean 0. The variance
is proportional to the constant’s current value.
The growth function, g : Rn →R (deﬁned in Sect. 8.2.4) for a timed
module controls the behaviour of that module over its growth period. It is
expressed in the form gs(λs, τs, βs), where s is the symbol associated with
5 In non-evolutionary versions of the system, birth ages can be expressions.

8 Evolutionary L-systems
189
the function, λs the parameter vector for s, τs is the current age and βs the
terminal age. A simple linear growth function would be gs = τ/β, increasing
from α/β to 1 from birth to terminal age. However, such a simple function
is not typical in nature. More common is some form of exponential function
with damping increasing with age such as the Bertalanﬀy equation:
gs = k1(1 −e−k2τ)k3,
(8.8)
where k1, k2 and k3 are constants. Many diﬀerent growth functions have been
studied in the biological literature, the exact choice of function dependent on
the particular feature or phenomena being modelled6 (e.g. tree trunk diam-
eter, length or height of the organism, population size, etc.). Uniﬁcation of
the major growth functions has been proposed, but this results in far more
complex equations with a large number of constants [27].
Since growth functions are expressions, their internal and external con-
struction is the same as for a module’s parameter expressions. Thus the mu-
tations are identical to those described for expressions in the previous section.
8.4.6 Mutation Probabilities
Diﬀerent kinds of mutations occur with diﬀering probabilities. The term mu-
tation probability means the probability a particular kind of mutation will
occur at a given time. Mutation probabilities are expressed as ﬂoating-point
numbers with the interval [0, 1]. A probability of 0 for a particular mutation
means that it will never occur (‘always oﬀ’); 1 means it will always occur (‘al-
ways on’); any value in between sets the overall frequency of occurrence (e.g.
0.1 means, on average, 1 mutation every 10 times the mutation is considered).
Some mutation probabilities are dependent on the size of the genotype being
mutated – with ﬁxed mutation probabilities, more mutations will occur on
larger genotypes than on shorter ones.
An important consideration for successfully evolving structures is correctly
adjusting mutation probabilities as the evolution progresses. For example, it is
better to set production mutation probabilities to maintain or slightly shrink
current production size. If rule mutation is biased towards adding productions
and/or modules then genotypes tend to become larger without necessarily
producing phenotypes of greater ﬁtness. Large sets of productions take longer
to parse and in general, take longer to generate.7 This does not stop rules
evolving complexity by selection. An upper limit is set for any newly mutated
rule’s resource consumption (i.e. using too much space or time). Rules that
6 It is important to consider when modelling growth using L-systems that a large
number of modules may contribute to the total growth of the ‘organism’. A bi-
ological growth function, such as (8.8) represents the growth of the organism in
total, not necessarily the individual components modelled as growing modules.
7 Although a production as simple as a →aa doubles the size of the produced
string at each derivation (assuming an axiom of a).

190
McCormack
consume above this limit during derivation are automatically stopped by the
system and removed.
The user can change mutation probabilities interactively during the evolu-
tionary process. A wide variety of mutation probability controls are provided
in the system developed, as this aﬀords a useful aid when one appears to be
approaching the desired result and wishes to limit mutations to speciﬁc areas.
Hierarchical controls also permit changing groups of associated parameters as
a whole, while maintaining individual ratios between the associated parts.
8.5 The Interactive Process
To evolve forms interactively we begin with a germinal genotype as described
in Sect. 8.1.3. This genotype may be drawn from an existing library of L-
systems, deﬁned by the user, or an ‘empty’ L-system consisting only of identity
productions can be used. The subset of the alphabet of the current L-system
suitable for mutation, ψ (Sect. 8.4.1), can be speciﬁed at this time. A list of
external surfaces to be used is also supplied. The parent production set is then
mutated according to probabilities speciﬁed. The axiom is mutated in a man-
ner similar to that of productions. After mutation, the L-system is parsed and
derived to a speciﬁed level, or time-period in the case of timed models. The
user may interrupt this process at any time. The software will automatically
interrupt the process if the computation time or space requirements exceeds
a speciﬁed limit. In traditional uses of the genetic algorithm, population sizes
can be large, as the selection process is automatic. In the case of this type of
system, selection is based on the subjective notion of aesthetics. This is one
reason why the population size in this case is limited. A much more overpow-
ering reason is the limitation of space on the screen to display phenotypes and
the computation time involved in generating large populations.
Eleven to ﬁfteen mutations per generation are performed. This provides
a trade oﬀbetween generation time and variety. With increased computa-
tional power (or more patience) larger populations can be created. The screen
resolution also limits the number of phenotypes that can be displayed and
manipulated simultaneously. The parent phenotype is displayed in the upper
left-hand corner of the screen followed by its mutated children. The user ad-
justs the sliders to control the mutation probabilities and selects a child to
act as the new parent. Selecting the current parent is also possible, normally
indicating that the user is unsatisﬁed with any of the current child mutations.
Each phenotype may have a temporal dimension (in the case of timed
L-systems) and three spatial dimensions. Thus the system allows real-time
rotation, translation and scaling in three-dimensions along with control over
playback (play, pause, rewind) for observing the development over time.

8 Evolutionary L-systems
191
A
B
surface stamen;
surface floret;
surface floret2;
surface leaf2;
productions:
A(n) -> +(137.5)[f(n^0.5) C(n)]
A(n+1);
C(n) : n <= 440 -> floret
: n > 440 & n <= 565 ->
floret2
: n > 565 & n <= 610 ->
^(90) S(0.3) leaf2;
axiom:
stamen A(0);
surface stamen;
surface floret;
surface floret2;
surface ball;
productions:
M1(l,s) : l < 2 ->
[S(s)[f(-1.0*s)stamen]
F(1.60*s)&(90)A(0)]!(s*0.07)
F(4.14*s)M1(l+1,s*0.67);
: l >= 2 -> [ballh];
A(n) : n < 1000 -> +(137.5)
[f(1.10*n^0.5)^(90-(n/1000*90))
C(n)]A(n+1)
: n > 1000 -> ;
C(n) : n >= 10 & n < 600 : ->
floret2
: n >= 600 & n < 900 : ->
floret
: n >= 900 : -> [
M2(n,0,1.76,90.40,0.05,3)];
M2(p0,p1,p2,p3,p4,p5): p0<8 : ->
/(p5,p0)^(p5,p0)!(p4)
f(-p2/9)F(p2,p0 + 0.07)^(p3,p0)
M2(p0,p1+1,p2/1.4,p3/2.0,p4*0.8*
p5,p5*1.8);
axiom:
M1(0,2.11);
Fig. 8.9. (A) Original form (a Sunﬂower) and (B) the form after many generations
of aesthetic evolution using the system described in this chapter. The L-systems
generating each model are shown below the image

192
McCormack
8.6 Results
The system, in various forms, has been used over a number of years to produce
both still and animated works. Figure 8.9 is an example of the results achieved
with the system. Starting with the ﬁgure shown on the left (A, a common
sunﬂower head), after more than 50 generations of aesthetic selection the
resultant form is shown on the right (B). The key productions that generate
each form are shown underneath the corresponding image. In the interests of
clarity, some control and rendering support symbols have been removed from
the productions shown.
The model also includes morphogenic growth, and the resulting animation
is smooth and continuous from birth to the fully developed model (Fig. 8.10).
The emphasis on the system has been one of a sculptural tool for modelling
and evolving growth, form and behaviour (Fig. 8.11).
Fig. 8.10. Sequence showing the temporal development (left to right) of the evolved
model shown in Fig. 8.9. Each element in the sequence is approximately 1 second
apart
8.6.1 A Case Study: Bloom
Thus far, only technical and descriptive details have been covered in this
chapter. Technical details are interesting, however the aesthetic, narrative
and production methodologies are also a vital part of practically using any
creative system [14]. This section brieﬂy looks at the process involved in using
the system described in this chapter to create a commissioned artwork, titled
Bloom.
Bloom is an artwork commissioned for the largest public billboard in Aus-
tralia, located in the ‘Creative Industries Precinct’ of the Queensland Uni-
versity of Technology in Brisbane. The billboard image is approximately 45
m × 9.5 m. Figure 8.12 shows the artwork in situ. The basic concept for the
work was to use artiﬁcial evolution methods to evolve ﬁve plant-like forms,
reminiscent of the ﬁve Platonic solids, but reinterpreted in a diﬀerent sense
of the concept of ‘form’.
The original source for the plant forms was derived from native species,
local to the area. An L-system corresponding to each original plant was ﬁrst

8 Evolutionary L-systems
193
Fig. 8.11. A walking creature. The gaits of the legs, body movements and geometry
are all created and controlled by timed L-systems.
Fig. 8.12. Bloom on display at QUT, Brisbane, Queensland (photo by Peter Lavery)

194
McCormack
Fig. 8.13. Deniﬂex: detail of one of the ﬁve Bloom forms
developed by hand-coding the L-system. These hand-coded L-systems rep-
resented the germinal genotypes for evolution. Each genome was subject to
the interactive evolutionary process outlined in this chapter. In addition, por-
tions of each of the genomes were spliced and intermixed, resulting in parts of
one plant’s structure appearing in a diﬀerent plant. After many generations,
ﬁve ﬁnal hybrid forms were evolved and rendered at high-resolution. Each of
the ﬁve ‘new’ species8 is presented in isolation, starkly displayed on a black
background, giving a clinical, formalist feel. Each form has a certain softness
and synthetic beauty, drawing on the dualistic nature of synthetic biology
(Fig. 8.13 shows the detail of one of the forms). This dualism promises im-
8 Of course, this method of mixing species would not necessarily be possible in real
biology.

8 Evolutionary L-systems
195
mense new possibilities, yet these possibilities also indicate a cost to our own
nature and environment of potentially dangerous proportions.
The billboard is located on a busy freeway in a developed city area. Na-
tive species that might have once been proliﬁc in the area have largely been
destroyed or removed to make way for human ‘progress’. The giant billboard
of evolved plants reminds viewers of a displaced nature. Viewers of the art-
work may recognise, for example, elements of the Bunya pine and a variety
of Banksia forms. These iconic forms remind us of native species, yet their
appearance is clearly strange and unwieldy. The work also conjures a sense
of irony, as the best views of nature are now synthetic ones, large, bright and
easily viewed from the comfort of any of the more than 70,000 cars that drive
past the billboard each weekday morning, relentlessly spewing emissions into
the atmosphere.
The deliberate use of decay and mutation in these models plays against
the ‘super-real’ feel of most computer graphics, suggesting a conﬂict between
the ideal of perfection in computer graphics and a diminishing biosphere.
The concepts of mutation, cross-breeding and genetic manipulation used to
create these images signal a growing concern over the consequences of recent
scientiﬁc research in genetically modiﬁed plants and animals. We humans have
always manipulated nature to our advantage, but the pace and possibilities
now possible with genetic manipulation are unprecedented. As the philosopher
John Gray reminds us, while science has enabled us to satisfy our desires, it
has done nothing to change them [6]. Bloom uses the generative evolutionary
computational models of morphogenesis, discussed in this chapter, to remind
us of the cost of those desires.
References
1. Abelson, H., DiSessa, A.: Turtle geometry: the computer as a medium for ex-
ploring mathematics. The MIT Press series in artiﬁcial intelligence. MIT Press,
Cambridge, Mass. (1982)
2. Agin, G.: Representation and description of curved objects. Stanford Artiﬁcial
Intelligence Report: Technical Memo AIM-173, Stanford, California (1972)
3. Bloomenthal, J., Barsky, B.: Modeling the mighty maple. In: Proceedings of the
12th annual conference on Computer graphics and interactive techniques, pp.
305–311. ACM, New York (1985)
4. Dawkins, R.: The Blind Watchmaker. Longman Scientiﬁc & Technical, Essex,
UK (1986)
5. Dorin, A.: Aesthetic ﬁtness and artiﬁcial evolution for the selection of imagery
from the mythical inﬁnite library.
In: J. Kelemen, P. Sos´ık (eds.) Advances
in Artiﬁcial Life. Lecture Notes in Artiﬁcial Intelligence 2159, pp. 659–668.
Springer-Verlag (2001)
6. Gray, J.: Straw dogs: thoughts on humans and other animals. Granta Books,
London (2002)
7. Hanan, J.: Parametric L-Systems and their application to the modelling and
visualization of plants. Ph.D. thesis, University of Regina, Saskatchewan (1992)

196
McCormack
8. Herman, G., Rozenberg, G.: Developmental Systems and Languages. North-
Holland, Amsterdam (1975)
9. Kitano, H.: Designing neural networks using genetic algorithms with graph gen-
eration system. Complex Systems 4, 461–476 (1990)
10. Lindenmayer, A.: Mathematical models for cellular interactions in development,
I and II. Journal of Theoretical Biology 18, 280–315 (1968)
11. McCormack, J.: The application of L-systems and developmental models to
computer art, animation, and music synthesis. Ph.D. thesis, Monash University,
Clayton (2003)
12. McCormack, J.: Aesthetic evolution of L-systems revisited. In: G. Raidl et al.
(ed.) Applications of Evolutionary Computing (EvoWorkshops 2004). Lecture
Notes in Computer Science 3005, pp. 477–488. Springer-Verlag, Berlin (2004)
13. McCormack, J.: Generative modelling with timed L-systems. In: J. Gero (ed.)
Design Computing and Cognition ’04, pp. 157–175. Kluwer Academic Publish-
ers, Dordrecht (2004)
14. McCormack, J.: Impossible nature: the art of Jon McCormack. Australian Cen-
tre for the Moving Image, Melbourne (2004)
15. McCormack, J.: Open problems in evolutionary music and art. In: F. Rothlauf et
al. (ed.) Applications of Evolutionary Computing (EvoWorkshops 2005) Lecture
Notes in Computer Science 3449, pp. 428–436. Springer-Verlag, Berlin (2005)
16. Mech, R., Prusinkiewicz, P., Hanan, J.: Extensions to the graphical interpre-
tation of L-systems based on turtle geometry. Technical report 1997-599-01,
University of Calgary, Alberta, Canada (1997)
17. Monod, J.: Chance and necessity – an essay on the natural philosophy of modern
biology. Penguin, London (1971)
18. Parish, Y., M¨uller, P.: Procedural modeling of cities. In: Proceedings of the
28th annual conference on Computer graphics and interactive techniques, pp.
301–308. ACM, New York (2001)
19. Prusinkiewicz, P., Lindenmayer, A.: The algorithmic beauty of plants. Springer-
Verlag, New York (1990)
20. Prusinkiewicz, P., M¨undermann, L., Karwowski, R., Lane, B.: The use of posi-
tional information in the modeling of plants. In: Proceedings of the 28th annual
conference on Computer graphics and interactive techniques, pp. 289–300. ACM,
New York (2001)
21. Rozenberg, G., Salomaa, A.: The Mathematical Theory of L-systems. Academic
Press, New York (1980)
22. Salomaa, A.: Formal Languages. Academic Press, New York (1973)
23. Sims, K.: Artiﬁcial evolution for computer graphics.
In: Proceedings of the
18th annual conference on Computer graphics and interactive techniques, pp.
319–328. ACM, New York (1991)
24. Takagi, H.: Interactive evolutionary computation: fusion of the capabilities of
EC optimization and human evaluation. Proceedings of the IEEE 89, 1275–1296
(2001)
25. Thom, R.: Structural stability and morphogenesis: an outline of a general theory
of models, 1st English edn. W. A. Benjamin, Reading, Mass. (1975)
26. Wainwright, S.: Axis and circumference: the cylindrical shape of plants and
animals. Harvard University Press, Cambridge, Mass. (1988)
27. Zeide, B.: Analysis of growth equations. Forest Science 39, 594–616 (1993)

Part III
Embryogeny

Evolutionary Design in Embryogeny
Daniel Ashlock
University of Guelph, Department of Mathematics and Statistics, 50 Stone Road
East, Guelph, Ontario N1G 2W1, Canada dashlock@uoguelph.ca
In biology texts embryogeny is deﬁned as “the development or production of
an embryo.” An embryo is a living creature in its ﬁrst stage of life, from the
fertilized egg cell through the initial development of its morphology and its
chemical networks. The study of embryogeny is part of developmental biol-
ogy [1,2]. The reader may wonder why a book on evolutionary design should
have a section on embryogeny. Computational embryogeny is the study of rep-
resentations for evolutionary computation that mimic biological embryogeny.
These representations contain analogs to the complex biological processes that
steer a single cell to become a rose, a mouse, or a man. The advantage of us-
ing embryogenic representations is their richness of expression. A small seed
of information can be expanded, through a developmental process, into a
complex and potentially useful object. This richness of expression comes at
a substantial price: the developmental process is suﬃciently complex to be
unpredictable.
Let us consider a pseudo-random number generator in analogy to embryo-
geny. This is an algorithm that takes an initial number, called a seed, and
generates a long sequence of numbers whose value lies in the diﬃculty of
telling that the sequence is not, in fact, random. While the algorithm driving
a pseudo-random number generator is typically both short and fast, it exploits
subtle aspects of number theory to achieve a high degree of unpredictability.
Computational embryogeny seeks to take a small data object and expand it
into a large, complex, and potentially useful object such as a robot controller
or a detailed image. In order to be useful, the expression algorithm that trans-
forms an artiﬁcial embryo into a ﬁnished structure must run rapidly enough
to be used in ﬁtness evaluations in evolutionary computation. It must also
not be so complex that direct design of the ﬁnal object by the usual methods
of design in engineering is a compelling alternative. An expression algorithm
must thus mimic a pseudo-random number generation in being both short and
relatively fast. If the expression algorithm were also predictable, the design
problem being solved would be simple. We can deduce that useful expres-

200
Ashlock
sion algorithms have a behavior that is not easy to predict from the artiﬁcial
embryos they act upon.
Unpredictable expression algorithms that transform a small collection of
parameters or a simple initial geometric state into a complex object are natural
targets for evolutionary computation. Getting an unpredictable process to
exhibit a desired behavior cannot be done with optimization algorithms like
gradient search. Either there is no simple notion of gradient or the gradient
is extremely rough (unpredictable). The representation of the computational
embryogeny cannot itself be completely random, but any sort of heritability
in the interaction of the representation and the variation operators chosen by
the designer will permit evolution to search for useful structures. It remains
to motivate why computational embryogeny is a useful alternative to direct
representations.
The famous no free lunch theorem proves that no technique is universally
useful. The correct question is therefore “when is computational embryogeny
useful?” The three chapters in this section form a set of three case studies
of when computational embryogeny is useful and their references point to
many more. Using an expression algorithm to expand a small informational
seed into a complex structure can be viewed as mapping the space of possible
seeds into a subspace of the enormously larger space of structures. If the
subspace of structure space selected by the expression algorithm contains no
viable structures, then the computational embryogeny fails. If, on the other
hand, the subspace selected by the expression algorithm is highly enriched in
viable structures, then the computational embryogeny succeeds. One can view
computational embryogeny as a technique for radically reducing the size of
the search space via the selection of a subspace of structure space. In Chap. 10
of this section, Chris Bowers quotes Altenberg and Williams as noting that a
monkey, given a typewriter, is far more likely to produce a verse of Shakespeare
than a monkey given a pencil and paper. This is because the typewriter selects
a tiny subspace of the space of possible black-on-white paper documents.
Another point of view one can take is that the expression algorithm is a
point at which expert knowledge and known constraints can be embedded into
the representation. Rather than using penalty functions to encourage search
to take place in an admissible portion of structure space, a well designed ex-
pression algorithm constructively forces search to take place in a subspace
containing only viable structures. Computational embryogeny is thus useful
when it is possible to construct an expression algorithm that embeds known
constraints and design principles. This makes the use of computational em-
bryogeny a somewhat opportunistic enterprise. It is important to keep this in
mind as a way of avoiding a nail hunt in celebration of the acquisition of a
new methodological hammer. Computational embryogeny can be remarkably
eﬀective for some problems but is inappropriate for others. A recent survey
classifying applications of computational embryogeny appears in [4].

Evolutionary Design in Embryogeny
201
The Chapters
When creating a computational embryogeny system the designer may choose
to create a highly speciﬁc system for a single task or a general framework.
Likewise, the designer can make a weak or strong biological analogy. At its
weakest any indirect representation using a form of expression algorithm, even
one as simple as a greedy algorithm for completing a partial structure, can be
considered an instance of computational embryogeny. Incorporating analogies
of genetic control networks, cellular developmental paths, and biochemical
signaling networks strengthens the biological analogy to the point where the
computational embryogeny becomes an interesting model of biological em-
bryogeny.
Chapter 9
Embryogenisis of Artiﬁcial Landscapes, is an example of a highly task-speciﬁc
embryogenesis system that generates artiﬁcial landscapes using an expression
algorithm derived from Lindenmayer systems [3]. Lindenmayer systems are
developmental models originally devised for plants. They appear in the section
of this book on evolved art as well. The landscape evolution in Chap. 9 is at the
weaker extreme of the biological analogy. An indirect representation is used,
and the expression algorithm creates a large image from a small informational
seed. There is, however, no structure analogous to the biological signaling that
drives the embryogeny of living plants.
Chapter 10
Modularity in a Computational Model of Embryogeny gives a model of em-
bryogeny that is novel in incorporating a type of modularity. Modularity is
a key feature of biological systems. Diﬀerent subsystems in a living organ-
ism manage the development of diﬀerent organs, overall morphology, skeletal
development, and the management of basal metabolism. Incorporating mod-
ularity into models of embryogenesis is thus key for biological plausibility.
Modularity is also a key feature of successful engineering and design tech-
niques, and so its incorporation into computational embryogeny is likely to
enhance its usefulness as a design tool. The new model is applied to a pattern
formation task and compared with other techniques.
Chapter 11
On Form and Function: The Evolution of Developmental Control outlines and
demonstrates a general framework called the Evolutionary Development Sys-
tem (EDS) for computational embryogeny. This system makes the strongest
biological analogy of the three chapters and provides a software framework
for exploring the utility of computational embryogeny. EDS is demonstrated

202
Ashlock
on two types of tasks. In the ﬁrst, an artiﬁcial organism is evolved to ﬁll a
cube or a planar region as best it can. The second EDS task is to evolve robot
controllers for memorizing a path, eﬀectively using sensors to navigate, and
avoiding obstacles.
Daniel Ashlock
Embryogeny Area Leader
References
1. Davidson, E.: Genomic Regulatory Systems: Development and Evolution. Aca-
demic Press, New York (2003)
2. Gilbert, S.: Developmental Biology, 7th edn. Sinauer Associates (2003)
3. Lindenmayer, A.: Mathematical models for cellular interaction in development, I
and II. Journal of Theoretical Biology 16, 280–315 (1968)
4. Stanley, K., Miikkulainen, R.: A taxonomy for artiﬁcial embryology. Artiﬁcial
Life 9, 93–130 (2003)

9
Embryogenesis of Artiﬁcial Landscapes
Daniel Ashlock1, Stephen Gent2, and Kenneth Bryden3
1 University of Guelph, Department of Mathematics and Statistics, 50 Stone Road
East, Guelph, Ontario N1G 2W1, Canada dashlock@uoguelph.ca
2 Iowa State University, Department of Mechanical Engineering, Ames, Iowa
50011, USA sgent@iastate.edu
3 Iowa State University, Department of Mechanical Engineering, Ames, Iowa
50011, USA kmbryden@iastate.edu
9.1 Introduction
This chapter examines the artiﬁcial embryogeny of landscapes intended for
use with virtual reality which consist of collections of polygons encoded using
L-systems. Artiﬁcial Embryogeny is the study of indirect representations. A
recent survey that attempts to classify diﬀerent types of artiﬁcial embryogeny
appears in [18]. A representation is a way of encoding a model or a solution to
a problem for use in computation. For example, an array of n real numbers is a
representation of the value of a function in n variables. A representation is in-
direct if it gives a set of directions for constructing the thing it speciﬁes rather
than encoding the object directly. The process of following the directions given
in the indirect representation to obtain the ﬁnal object is called expression.
Indirect representations require an interpreter to express them and, because
of this, are more diﬃcult to understand at the speciﬁcation or genetic level.
There are a number of advantages to indirect representations that more than
balance this genetic obscurity in many situations. The most general of these
advantages is that the transformation from the indirect speciﬁcation to the
ﬁnal model or solution can incorporate heuristics and domain knowledge. This
permits a search of a genetic space that is far smaller than the space in which
the expressed objects reside and has a much higher average quality. Another
advantage, showcased in this chapter, is compactness of representation. The
indirect representations we evolve in this chapter use a few hundred bytes to
specify megabyte-sized collections of polygons.
One kind of indirect representations speciﬁes initial conditions for a dy-
namical system that generates the desired ﬁnal state. Dynamical systems
based on reaction–diﬀusion equations were proposed by Turing [20] as a pos-
sible expression mechanism for permitting biochemistry to create biological
morphology. Beautiful examples of pattern formation, in the form of mam-
malian coat patterns, using reaction–diﬀusion-based dynamical systems can

204
Ashlock et al.
be found in [16]. Discrete dynamical systems such as cellular automata [7] can
also be used as indirect representations of this type.
Cellular representations are an indirect representation that use a grammar
as the basis of their encoding. The representation consists of a speciﬁcation
of strings of production rules in the grammar that, when applied, yield the
ﬁnal structure. These representations were ﬁrst applied to artiﬁcial neural
nets [8,9] but have also been applied to ﬁnite state game-playing agents [6].
Lindenmayer systems or (L-systems) [13] which were devised as computational
models of plants [17] are an older type of grammatical indirect representation
that share many features with cellular representations. L-systems have been
applied to targets as diverse as music [4], error correcting codes [3], and the
morphology of virtual creatures [11]. In this chapter, L-systems will be used
to evolve a diverse collection of virtual landscapes. This encoding of virtual
landscapes uses a few hundred bytes of data to specify a collection of large,
complex virtual images of the same landscape at diﬀerent resolutions.
Rules:
A
A
A
A
B
B
B
Axiom:
A
B
A
A B
B A
B
A
Two expansions:
A
A
B
B
B
A
A
A
A
B
A
B
B
B
B
A
Fig. 9.1. The axiom and rules for a simple two-dimensional L-system, together with
two expansion of the axiom

9 Embryogenesis of Artiﬁcial Landscapes
205
An L-system has two parts. The ﬁrst is a grammar which speciﬁes an
axiom and a collection of replacement rules. The L-system creates a sequence
of objects, starting with the axiom, by applying the rules. This application
is called an expansion. The replacement rules used in this chapter operate
on a two-dimensional array of characters. Rules are applied simultaneously
to every symbol in a two-dimensional array to create a new, larger array. An
example of this type of two-dimensional L-system is shown in Fig. 9.1.
The second part of an L-system is the interpreter. The interpreter turns
the expression of the L-system into the object it speciﬁes. In this chapter,
the interpreter’s task is to render the symbols into the polygons of a virtual
landscape. The L-systems used here are called midpoint L-systems and create
midpoint fractals. For midpoint L-systems, interpretation (or expression) is
integrated with the expansion process.
The simplest sort of midpoint fractal is generated starting with a horizontal
line segment. Take the midpoint, displace it vertically by a number selected
from a probability distribution, creating two line segments that meet. Repeat
the process on each of the resulting line segments possibly using a diﬀerent
probability distribution at each level of the process. Continue until you have
the desired appearance, cycling through a sequence of probability distributions
D1, D2, D3, . . . as you go. Such a fractal is shown in Fig. 9.2. You can also
Fig. 9.2. An example of a midpoint fractal generated from a line segment
generate midpoint fractals starting with a square. This is done just as it was
for the line segment, except that you displace the center of the square instead
of the midpoint of the line segment. Each time you displace the center of a
square, you create four new squares. The choice of Di controls the character of
the resulting fractal. For example, if the Di are Gaussian with a small positive
mean and variance that decreases with i, the resulting midpoint fractal will
look like part of a mountain range [14].
For midpoint L-systems the Di are replaced with ﬁxed numbers associated
with the symbols in the L-system together with a decay parameter ω. Let
{S1, S2, . . . , Sk} be the symbols of the L-system (A and B in Fig. 9.1). Let hi
be the number associated with symbol Si. The interpretation of the midpoint
L-system starts with the speciﬁcation of the heights of four corners of a square.
In each expansion the value ωk−1·hi is added to the center of the square where
symbol Si is being expanded. The value k is the index of the expansion (ﬁrst,

206
Ashlock et al.
Fig. 9.3. The result of six expansions of the midpoint L-system given in Fig. 9.1
with h1 = 0.8 h2 = 0.2 and ω = 0.7
second, etc.). Thus, on the ﬁrst expansion, the value added to the center of
the initial square when it is partitioned is the appropriate hi for the axiom.
In the next expansion, ω · hi will be added to the heights of the centers of the
four squares the initial square was divided into. In the third expansion, the
values added will have the form ω2 ·hi for the centers of all 16 squares present
in that expansion, and so on. The value ω controls the degree to which the
heights associated with symbols decay with expansion. In the experiments
in this chapter, values for ω between 0 and 1 are used. A rendering of the
example midpoint L-system given in Fig. 9.1 is shown in Fig. 9.3.
An application of evolving L-system grammars appears in [12, 15] where
the L-system provided the connection topology of an artiﬁcial neural net.
The parameters of the L-system interpreter were ﬁxed in that study, not
evolved. Evolutionary algorithms that set the parameters of an L-system ap-
pear in [1, 2]. In [10] L-systems are used to specify a body for an artiﬁcial
agent that is co-evolved together with a control system. The type of L-system
presented in this chapter is unique in co-evolving the parameters used in in-
terpretation together with the L-system grammar. The current application is
intended to generate rugged versions of idealized smooth landscapes for use in
virtual reality applications. Ideally, the algorithm will generate a selection of

9 Embryogenesis of Artiﬁcial Landscapes
207
diﬀerent landscapes, none too diﬀerent from the original idealized landscape
but diﬀering in details of appearance.
The remainder of this chapter is structured as follows. Section 9.2 speciﬁes
the evolvable representation for midpoint L-systems. Section 9.3 gives the
experimental design for the evolutionary algorithm used to locate midpoint
L-systems that specify virtual landscapes. Section 9.4 summarizes the results
of the evolutionary runs. Section 9.5 discusses the results and suggests possible
variations. Section 9.6 suggests some possible next steps for the creation and
application of midpoint L-systems.
9.2 The L-system Representation
It is straightforward to put midpoint L-systems into an evolvable form. Spec-
ifying a midpoint L-system requires the following parameters:
1. the initial heights of the corners of the ﬁrst square;
2. the number of symbols used;
3. the 2 × 2 grids of symbols used to expand each symbol;
4. the midpoint displacement associated with each symbol; and
5. the decay parameter ω.
In this study the decay parameter ω is speciﬁed globally and thus has a ﬁxed
value in any given experiment. Likewise, the initial square heights have a ﬁxed,
global value of zero. The axiom for all midpoint L-systems in this study is the
single symbol A. (Since expansion rules for symbols are generated at random
when initializing populations of L-systems, this choice of a ﬁxed axiom has no
cost in expressiveness of the system.)
The representation used to evolve L-systems contains two linear structures.
For an n-symbol L-system there is a string of 4n symbols that speciﬁes the
replacement rules, in adjacent groups of four, for each of the symbols. There
is also an array of n real values that specify the midpoint displacements asso-
ciated with each symbol. An example of a midpoint L-system speciﬁed in this
fashion is given in Fig. 9.4. The upper-case Roman alphabet symbols (followed
by the lower-case if necessary) in alphabetical order expand into the groups
of four. (In this case, A–H.)
The variation operators we deﬁned for this representation are as follows.
Two crossover operator are used: two-point crossover of the string specify-
ing the expansion rules and two-point crossover of the array of weights. Both
crossover operators are used 100% of the time. Two mutation operators are
used. The displacement mutation picks one of the midpoint displacement val-
ues uniformly at random and adds a number uniformly distributed in the
range [−0.05, 0.05] to that displacement value. The symbol mutation picks
one of the symbols in the string that speciﬁes the replacement rules and re-
places it with a symbol chosen uniformly at random from those available.
Both of these mutations are used, once each, on each new structure.

208
Ashlock et al.
Expansion rules:
CCCC.AAGF.DGDG.GDGD.FBHC.GEDA.GGGG.CEEB
Displacement values:
(0.790, −0.002, −0.102, 0.010, 0.116, −0.058, 0.036, 0.143)
Fig. 9.4. An example of an eight-symbol midpoint L-system evolved to ﬁt a hill.
Dots in the string of symbols that specify the expansion rules are inserted for read-
ability. The four symbols in each expansion rule are places in the 2 × 2 array in
reading order
Midpoint L-systems are evolved to ﬁt a speciﬁed landscape. All landscapes
are placed on the unit square. The landscapes used in this chapter (shown in
Fig. 9.5) are: a hill:
H(x, y) =
1
1 + (6x −3)2 + (6y −3)2
(9.1)
and a crater:
C(x, y) =
1
1 + (r −2)2
(9.2)
where r(x, y) =

(12x −6)2 + (12y −6)2. Both these shapes are radially
symmetric about the center of the unit square making them somewhat chal-
lenging for the midpoint L-systems which ﬁnd shapes that divide naturally
into squares easiest to ﬁt.
9.3 Experimental Design
The ﬁtness of a given midpoint L-system is evaluated by expanding the L-
system seven times to generate a 128×128 grid of height values. These values
are placed on a regular grid covering the unit square. The RMS error of the
agreement of the height values with the desired landscape is computed. This
error value, to be minimized, is the ﬁtness function used to drive evolution.
The evolutionary algorithm operates on a population of 120 midpoint L-
systems. The initial population is created with symbols for expansion rules
chosen uniformly at random from those available. The initial values for the
midpoint displacements are chosen uniformly at random in the range [0.1,
1] but are permitted to leave the interval later via mutation. The algorithm
is a steady-state algorithm [19] using size seven tournament selection. For
each landscape and set of parameters tested, we performed a collection of 100
evolutionary runs. Each of these runs consisted of 10,000 mating events. For
each mating event, a tournament of size seven is selected, and the two most
ﬁt members of the tournament are bred to produce children that replace the
two least ﬁt members. Breeding consists of copying the parents, performing

9 Embryogenesis of Artiﬁcial Landscapes
209
Hill Landscape
 0
 0.2
 0.4
 0.6
 0.8
 1  0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
Crater Landscape
 0
 0.2
 0.4
 0.6
 0.8
 1  0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
Fig. 9.5. Surface plots of the hill and crater landscapes
both kinds of crossover on the copies, and then performing one displacement
mutation and one symbol mutation on each of the resulting new structures.
For the hill landscape, given by (9.1), nine experiments were performed
to explore the impact of changing the decay parameter ω and the number
of symbols. These experiments used the nine possible combinations available
when ω ∈{0.8, 0.9, 0.95} and n ∈{4, 8, 16}. Visual inspection of the resulting
landscapes led to the choice of ω = 0.8 and n = 16 for the subsequent set
of experiments performed on the crater landscape. An additional experiment
was performed using ω = 0.8 and n = 32 for the crater landscape to study
the impact of allowing more symbols in the L-system.

210
Ashlock et al.
Fig. 9.6. Four diﬀerent rendered hills derived from midpoint L-systems. These hills
are all developed from ω = 0.8, n = 16 L-systems. This rendering uses six expansions
of the midpoint L-system for each hill. Note the diversity of appearances
9.4 Results
The parameter study of the impact of varying the number of symbols and
the decay parameter is summarized in Table 9.1. When ω was 0.8 or 0.9,
the average RMS error of the L-system to the target surface improved as the
number of symbols increased. This improvement was statistically signiﬁcant,
as documented by the disjoint conﬁdence intervals, for both increases of sym-
bol number for ω = 0.8 and for the change from eight to 16 symbols when
ω = 0.9. When ω = 0.95 the RMS error becomes steadily worse as the num-
ber of symbols increases with a signiﬁcant diﬀerence between four and eight
symbols. This initial exploration shows that the impact of varying ω and n
is not independent. Both visual inspection of landscapes rendered from the

9 Embryogenesis of Artiﬁcial Landscapes
211
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
Mean RMS error
Hundreds of Mating Events
Mean population fitness
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0
 20
 40
 60
 80
 100
Mean RMS error
Hundreds of Mating Events
Mean population fitness
"run1.dat" using 0:1
Fig. 9.7. The behavior of ﬁtness as evolution progresses for two typical runs of
the midpoint L-system training algorithm. The upper plot shows ﬁtness on the hill
landscape given by (9.1), while the lower plot shows ﬁtness on the crater landscape
given by (9.2). The much higher RMS error values for the crater run reﬂect the
relative diﬃculty of ﬁtting the crater
L-systems and the ﬁtness results supported the choice of ω = 0.8 and n = 16
for subsequent experiments. Examples of the types of hills evolved with this
system are given in Fig. 9.6.
Fitting the crater landscape is a considerably more challenging problem
than ﬁtting the hill landscape (as illustrated in Fig. 9.7). The conﬁdence
interval for the craters using ω = 0.8 and n = 16 symbols, analogous to the
ones in Table 9.1, is (0.1661, 0.1710). Many of the evolved craters took the

212
Ashlock et al.
Table 9.1. 95% conﬁdence intervals on the mean RMS error (ﬁtness) for agreement
between the best midpoint L-systems in each run and the target landscape for each
of the nine sets of 100 evolutionary runs performed to study the impact of changing
the decay parameter ω and number of symbols n
Mean RMS error, 95% conﬁdence intervals
Value of ω
n
0.8
0.9
0.95
4
(0.0490, 0.0500) (0.0487, 0.0497) (0.0487, 0.0479)
8
(0.0466, 0.0475) (0.0489, 0.0499) (0.0511, 0.0523)
16 (0.0426, 0.0434) (0.0459, 0.0469) (0.0514, 0.0527)
form of a collection of spikes at various positions along the crater’s rim. One
of the better looking craters, also the most ﬁt, is shown in Fig. 9.8. The 95%
conﬁdence interval for mean RMS error for the craters evolved with n = 32
symbols is (0.1397, 0.1478).
Fig. 9.8. The most ﬁt crater located in the evolutionary runs on the crater land-
scape. This crater uses n = 32 symbols in its midpoint L-system

9 Embryogenesis of Artiﬁcial Landscapes
213
9.5 Discussion and Conclusions
The evolutionary algorithm used for locating midpoint L-systems worked well
for the hill landscape and for some runs for the crater landscape. It is a novel
example of the techniques of artiﬁcial embryogenesis. The RMS error for the
best set of parameters (n = 16, ω = 0.8) has hills deviating 4.3%, on average,
from the height proﬁle of the idealized hill surface. The midpoint L-system
approximations of the crater with n = 16 symbols varied roughly 17% from
the ideal crater, and even the best evolved crater deviated 14% on average.
The parameter study on the hill suggests that increasing n or decreasing ω
might yield a better approximation of the ideal landscape. This hypothesis was
tested in the runs that evolved craters with n = 32 symbols. The conﬁdence
interval on the mean RMS error of end-of-run best ﬁts to the crater dropped
substantially with the best ﬁtness found having 12.1% average error.
The pattern of ﬁtness values in Table 9.1 is a little startling. Initially we
conjectured that lowering ω and raising n would make independent, positive
contributions to ﬁtness. This is not the case when ω is near 1. This is probably
due to problems with ﬁtting both the ﬂat and curved portions of the landscape
accurately. The parameter ﬁtting study demonstrated that the parameters
interact, and so a parameter sweep should be performed in any other work
done with this system.
The algorithm using both trial surfaces located a variety of diﬀerent mid-
point L-systems that produce landscapes with distinct appearances. This
meets the goal of supplying a palette of landscape features for a virtual land-
scape designer. The polygons speciﬁed by a midpoint L-system at the evolved
resolution of 128 × 128 form a data object with a size a little over 800 kilo-
bytes; this grows to megabytes when rendered as an uncompressed image.
The midpoint L-system itself, with 16 symbols, requires 48 characters and
16 real number midpoint displacements, for a total of 144 bytes of storage.
The midpoint L-systems can thus specify a virtual landscape in far less space
than direct storage of the polygon models. Since the midpoint L-systems, once
evolved, use no random numbers, they can rapidly be expanded to whatever
resolution is required.
9.5.1 Level of Detail
The level of detail (LOD) for a midpoint L-system is determined by the num-
ber of expansions performed. The evolved midpoint L-systems in this chapter
were all evolved with a ﬁtness evaluation using seven expansions of the L-
system which yielded a grid of 128 × 128 = 16384 height values.
A beneﬁt of modeling landscapes with a midpoint L-system is that the
LOD can be varied by simply running diﬀerent numbers of expansions of
the L-system. The time to expand a midpoint L-system is linear in the ﬁnal
number of polygons, and the expansion is almost trivial to parallelize since

214
Ashlock et al.
Fig. 9.9. Varying the level of detail for a hill. The same midpoint L-system for
the hill landscape is rendered on N × N grids for N=16, 32, 64, and 128. This
corresponds to four, ﬁve, six, and seven expansions of the L-system which yield
increasing levels of detail
diﬀerent quadrants of the expansion share information only at their bound-
aries, and that information does not change during the expression process.
Figure 9.9 shows an example of a single midpoint L-system for the hill land-
scape expanded to four diﬀerent levels of detail. The highest level of detail
shown, seven expansions of the L-system, is the level of detail at which the
L-system was evolved. Figure 9.10 shows a crater with ﬁve and six expansions.
If the level of detail of the evolved midpoint L-systems is increased beyond
the level at which it was evolved, the behavior can become unpredictable.
Expansions performed beyond the evolved level of detail with the L-systems
located in this study produced remarkably spiky pictures, and so it is probably
a good idea to evolve midpoint L-systems to the maximum level of detail
required. The most expensive part of the midpoint L-system training software
is ﬁtness evaluation. The time for a ﬁtness evaluation is proportional to 2n

9 Embryogenesis of Artiﬁcial Landscapes
215
Fig. 9.10. Varying the level of detail in a crater. The same midpoint L-system for
the hill landscape is rendered on a 32 × 32 and a 64 × 64 grid representing ﬁve and
six expansions of the L-system
where n is the number of expansions of the L-system used during ﬁtness
evaluation.

216
Ashlock et al.
Fig. 9.11. Varying the value of ω. The above crater was evolved using ω = 0.8 but
then rendered for ω = 0.7, 0.75, 0.8, and 0.85. The smaller ω the ﬂatter the landscape
A compromise between the conﬂicting constraints of training time and ﬁ-
nal level of detail may be possible. As the ﬁtness plots in Fig. 9.7 demonstrate,
the initial population is relatively unﬁt. Using a small number of expansions
initially and increasing the number as evolution progresses could well result
in a substantial savings in training time. The schedule for increasing the pa-
rameter n during the course of evolution would have to be experimentally
determined. We conjecture that eﬀective patterns for increasing n will be
diﬀerent for diﬀerent landscapes.
9.5.2 Varying the Decay Parameter
The decay parameter controls the relative importance to the landscape’s ﬁnal
appearance of the earlier and later applications of the expansion rules. A
high value of ω permits later expansions to have more impact. Figure 9.11
shows what happens when ω is varied for a crater evolved with ω = 0.8. This

9 Embryogenesis of Artiﬁcial Landscapes
217
suggests that, if evolved land-forms of the sort demonstrated here are needed
to build a complex landscape, then varying ω will permit an inexpensive source
of additional diversity. If ω varied functionally with position in the virtual
landscape, it could be used as a parameter to set the tone for various regions.
The more rugged the area, the higher the ω.
While all the values of ω used in this chapter are in the range [0.7, 0.95],
other values yield interesting eﬀects. If ω > 1, then later expansions have more
of an eﬀect. Evolved landscapes from this chapter rendered in this fashion are
remarkably spiky; if rules were evolved with ω > 1, then the results would
probably be quite diﬀerent from those we present.
9.6 Next Steps
The parameter study performed for the hill landscape tested a small num-
ber of parameters on a single landscape. The characterization of the system
for evolving midpoint L-systems as an evolutionary algorithm is thus barely
begun. Larger parameter sweeps may yield unexpected results in the part of
the parameter space not yet explored, and many more landscapes should be
tested. The rate of application of the crossover and mutation operators was
set to the defaults used in the authors’ own in-house evolutionary algorithm
code; exploration here might also improve performance.
The unexpected interaction of the number n of symbols and the decay
parameter ω suggest that letting ω become an evolvable parameter could also
help. A step that was not taken was to normalize the height values of the
expanded midpoint L-system to have the correct average height. All such nor-
malization was left for evolution. Permitting ω to ﬂoat during evolution would
give evolution a far more eﬀective tool, an overall normalization parameter, for
adjusting the height of the virtual landscape. In retrospect, it seems obvious
that an evolvable ω would improve performance. “Obvious” and “correct” are
not always the same in evolutionary computation, and so experiments with
evolvable ω are an early goal of additional research on this system.
9.6.1 Haptic and Visual L-textures
The goal of this study was to create compact descriptions, in the form of
midpoint L-systems, for large features of a virtual landscape. This is not the
only potential application. If midpoint L-systems were ﬁt to a relatively ﬂat
landscape they could be used to supply tactile texture. Virtual reality systems
are starting to implement haptic (touch) feedback devices and the height data
supplied by a midpoint L-system could be used to create any desired degree
of roughness by simply modifying the vertical scale. In eﬀect, the ω parameter
could be used to control the rugosity of a haptically perceived surface.
While the systems evolved in this chapter used a square area for ﬁtness
evaluation and rendering, it is not diﬃcult to create non-square midpoint L-
systems. Midpoint L-systems can be used to “paint” any rectangle. If the

218
Ashlock et al.
Fig. 9.12. Examples of rendering midpoint L-systems as gray-scale values rather
than heights. The L-systems rendered above are hand designed, not evolved

9 Embryogenesis of Artiﬁcial Landscapes
219
height values are instead displayed as colors or shades of gray, then midpoint
L-systems can be used to generate the more traditional visual textures used in
virtual reality as well as interesting pictures. Examples of gray-scale renderings
of two hand-designed midpoint L-systems appear in Fig. 9.12.
The images shown in Fig. 9.12 suggest that colored midpoint L-systems
could be used to design interesting, symmetric objects. Rather than mimicking
natural land-forms, these L-systems could be used to create three dimensional
crystals, complex patterns for ﬂoor tiles or rugs, and other dressing of a virtual
environment. In this case the ability of midpoint L-systems to create complex
forms that use modest amounts of storage and which can be rendered quickly
would be quite valuable. If symmetry and a requirement to stay within a
particular volume were imposed, then an evolutionary algorithm could be
used to generate thousands of crystalline forms or non-repeating ﬂoor-tile
designs for an entire virtual city.
9.6.2 Recursive Application
The initial height values for the squares in which our L-systems were expanded
were set to 0 throughout the chapter. The ability to set these initial values
to any height means that a land-form speciﬁed by a midpoint L-system can
be expanded into any four-sided polygon within the landscape. This, in turn,
yields the potential for recursive application of midpoint L-systems. At its
simplest, one might render a broad ﬂat hill (one of the hills evolved in this
chapter with ω = 0.4) at a low LOD and then place an evolved crater in
several of the resulting squares. A more complex application would place the
crater L-system in the expansion process at diﬀerent levels of the expansion
of the hill. This would permit craters of varying size to populated the virtual
hill.
A more complex form of this idea would use existing midpoint L-systems
as tiles of a new one. The evolutionary algorithm presented in this chapter
would generate a palette of land-forms that would then be available to a higher
level system that creates virtual landscapes that satisfy various strategic goals
such as lines of sight or lowland connectivity but which are otherwise diverse.
This representation would decompose the problem of ﬁtting complex land-
scapes into pieces represented by the lower and higher level L-systems. This
application might be valuable for providing an automatic source of variety in
a video game.
Acknowledgments
This work expands and illuminates a paper presented at the 2005 Congress
on Evolutionary Computation [5]. The authors would like to thank the Iowa
State University Virtual Reality Applications Center, particularly Douglas
McCorkle and Jared Abodeely, who helped evaluate over 1000 renderings of

220
Ashlock et al.
midpoint L-systems produced during the parameter study. The authors also
thank the Department of Mathematics and Statistics at the University of
Guelph for its support of the ﬁrst author.
References
1. Ashlock, D., Bryden, K.: Evolutionary control of L-system interpretation. In:
Proceedings of the 2004 Congress on Evolutionary Computation, vol. 2, pp.
2273–2279. IEEE Press, Piscataway, NJ (2004)
2. Ashlock, D., Bryden, K., Gent, S.: Evolutionary control of bracketed L-system
interpretation. In: Intelligent Engineering Systems Through Artiﬁcial Neural
Networks, vol. 14, pp. 271–276 (2004)
3. Ashlock, D., Bryden, K., Gent, S.: Evolving L-systems to locate edit metric
codes. In: C. Dagli et al. (ed.) Smart Engineering System Design: Neural Net-
works, Fuzzy, Evolutionary Programming, and Artiﬁcial Life, vol. 15, pp. 201–
209. ASME Press (2005)
4. Ashlock, D., Bryden, K., Meinert, K., Bryden, K.: Transforming data into music
using fractal algorithms. In: Intelligent Engineering Systems Through Artiﬁcial
Neural Networks, vol. 13, pp. 665–670 (2003)
5. Ashlock, D., Gent, S., Bryden, K.: Evolution of L-systems for compact virtual
landscape generation. In: Proceedings of the 2005 Congress on Evolutionary
Computation, vol. 3, pp. 2760–2767. IEEE Press (2005)
6. Ashlock, D., Kim, E.: The impact of cellular representation on ﬁnite state agents
for Prisoner’s Dilemma. In: Proceedings of the 2005 Genetic and Evolutionary
Computation Conference, pp. 59–66. ACM Press, New York (2005)
7. Ball, P.: The Self-made Tapestry, Pattern Formation in Nature. Oxford Univer-
sity Press, New York (1999)
8. Gruau, F.: Genetic synthesis of modular neural networks. In: Proceedings of
the Fifth International Conference on Genetic Algorithms, pp. 318–325. Morgan
Kaufmann, San Francisco (1993)
9. Gruau, F., Whitley, D., Pyeatt, L.: A comparison between between cellular
encodings and direct codings for genetic neural networks. In: Genetic Program-
ming 1996, pp. 81–89. MIT Press, Cambridge MA (1996)
10. Hornby, G., Pollack, J.: Body-brain co-evolution using L-systems as a generative
encoding. In: L. Spector et al. (ed.) Proceedings of the Genetic and Evolutionary
Computation Conference (GECCO-2001), pp. 868–875. Morgan Kaufmann, San
Francisco, California, USA (2001)
11. Hornby, G., Pollack, J.: Evolving L-systems to generate virtual creatures. Com-
puters and Graphics 25(6), 1041–1048 (2001)
12. Kitano, H.: Designing neural networks using genetic algorithms with graph gen-
eration system. Complex Systems 4, 461–476 (1990)
13. Lindenmayer, A.: Mathematical models for cellular interaction in development,
parts I and II. Journal of Theoretical Biology 16, 280–315 (1968)
14. Mandelbrot, B.: The fractal geometry of nature. W. H. Freeman and Company,
New York (1983)
15. Miller, G., Todd, P., Hegde, S.: Designing neural networks using genetic algo-
rithms. In: Proceedings of the 3rd International Conference on Genetic Algo-
rithms, pp. 379–384. Morgan Kaufmann, San Mateo, CA (1989)

9 Embryogenesis of Artiﬁcial Landscapes
221
16. Murray, J.: Mathematical Biology II. Springer-Verlag, New York (2003)
17. Prusinkiewicz, P., Lindenmayer, A., Hanan, J.: The algorithmic beauty of plants.
Springer-Verlag, New York (1990)
18. Stanley, K., Miikkulainen, R.: A taxonomy for artiﬁcial embryology. Artiﬁcial
Life 9, 93–130 (2003)
19. Syswerda, G.: A study of reproduction in generational and steady state Ge-
netic Algorithms. In: Foundations of Genetic Algorithms, pp. 94–101. Morgan
Kaufmann (1991)
20. Turing, A.: The chemical basis of morphogenesis. Philosophical Transactions of
the Royal Society(B) 237, 37–72 (1952)

10
On Form and Function: The Evolution
of Developmental Control
Sanjeev Kumar
Sibley School of Mechanical & Aerospace Engineering, Cornell University, Ithaca,
New York, 14853, USA sanjeev.kumar8@gmail.com
10.1 Introduction
How does the genome control the construction of a complex multi-cellular
system with well deﬁned form and structures from a single cell? Artiﬁcial
life and developmental biology overlap on some quite important topics. The
question above reveals two such pivotal topics: construction and control.
Construction of complex adaptive systems, or indeed intricate forms, in
a robust, self-organizing manner is a notoriously diﬃcult problem that high-
lights fundamental issues of scalability, modularity, and self-organization. The
need to build such complex technology has, over recent years, sparked renewed
interest in using approaches inspired by developmental biology [4,8,12,14,19].
For example, the problem domain seeing the most success with the application
of the developmental metaphor for construction-related tasks is Evolutionary
Design [1]. Examples of typical tasks include the synthesis and use of models
of development that incorporate the use of embryos, cells, proteins, diﬀusion,
receptors, diﬀerentiation, cell signaling, and other cell and developmental pro-
cesses to build forms ranging from buildings [11] to geometric shapes [14] to
creatures [2,5].
Control is an important function. Be it a robot controller or the control of
the construction process, devising novel techniques to implement robust con-
trol has been an important research motivation for many years and has only
recently begun taking inspiration from non-neural developmental biology. For
example, in the ﬁeld of robotics aspects of developmental biology, such as
hormone control, have been modeled with much success for distributed robot
control [9, 22]. In Evolutionary Robotics (ER), robot control using develop-
mental biology inspired approaches has typically been achieved indirectly, for
example, through the development of a controller such as a neural network.
Several researchers have investigated such an approach [4, 8]. The focus of
such investigations, however, is on evolving a developmental process which
speciﬁes how the topology of a neural network (NN) is to be grown using

224
Kumar
concepts from developmental biology in which cells are able to divide, diﬀer-
entiate, and form connections with other cells. After a period of growth, the
resulting network of cells is interpreted as a neural network used to control
robot behavior and navigation. Common to all such works is that once the
neural network is constructed the genome is discarded [20].
Yet nature has solved problems of construction, control, scalability, mod-
ularity, self-organization, and even self-repair through the evolution of devel-
opment: the process or set of processes responsible for constructing organisms
[24], and the complex Genetic Regulatory Networks (GRNs) that are found in
the genome. Furthermore, nature does not discard the genome once the brain
is built; quite the contrary, developmental processes and mechanisms continue
to operate long after embryonic development has ceased in order to maintain
the highly dynamic state of organisms.
In nature the ability to react fast to changing situations and circumstances
is crucial to the success of an organism, for example, ﬂeeing a predator. As
Marcus [17] points out, neurons react on a faster time-scale than genes, typ-
ically on the order of milliseconds, whereas genes are relatively slower. How-
ever, this should not detract from the fact that the genome is an immensely
complex, real-time control system that builds bodies, brains, and immune
systems capable of remarkable abilities.
Development oﬀers an oft-forgotten alternative route to tackling problems
of construction and control. Yet despite all its abilities, the process of devel-
opment remains a relatively untapped, and frequently overlooked source of
inspiration, mechanisms, and understandings. Consequently, inspired by bi-
ology, the ﬁeld of computational development is seen as a potential solution
to such construction and control related problems [15]. An important point
worth mentioning is that as computer scientists and engineers, we are not
constrained by the same issues as biology. As a result, genetic regulatory net-
works need not operate on a slower time-scale than Neural Networks (NNs);
they could operate at the same time-scale. Furthermore, in using GRNs as the
control system for generating, say, reactive robot behaviors in which current
sensor readings are mapped into actions; or as, say, controllers of a process
that speciﬁes the construction of form, future research can begin to explore
how one might harness their powerful robust, fault-tolerant, regenerative, and
dynamical abilities.
Work by Kumar [13] has shown GRNs to be a viable control architecture
for reactive robot control for the problem domain of evolutionary robotics,
along with other works such as [1,20,23]. Alternative, and related, approaches
to robot control and behavior generation that have seen much success are rule
systems [21]. Still, problems with rule systems exist: although they can be
made to have state, traditionally state was lacking.
In this chapter, I describe previous works that address the dual problems
of form and function. In particular, this chapter addresses the ability of de-
velopment to produce both form (morphology of geometric shapes that have
proven useful in, for example, computer graphics such as planes and cubes)

10 On Form and Function
225
and function (robot control, i.e. the ability to specify reactive robot behaviors
through the evolution of real-time GRN robot controllers).
The chapter is structured as follows: ﬁrst a brief description is given of
the model of development, called the Evolutionary Developmental System
(EDS), and the underlying genetic regulatory network model that lies at the
core of the EDS. This is followed by two self-contained sections detailing
experiments that investigate the use of development for form and function
respectively. The ﬁrst of these sections documents experiments that not only
demonstrate how the EDS and its GRN model are able to construct – in a
self-organizing manner – geometric forms (shapes), but also provide detailed
evolved developmental mechanisms that specify the self-construction of these
forms. Two sets of experiments, which diﬀer only in the target form evolved,
are detailed. The ﬁrst set of experiments explores the construction of a plane,
while the second experiment examines the construction of a cube.
The second of these sections demonstrates how an adapted version of the
GRN model within the EDS can be evolved for function, i.e. the problem of
reactive robot control. The function experiments are broken up into three sets
of experiments. The ﬁrst set of experiments shows controllers that are able to
memorize routes through the environment using no sensors. The second set
shows controllers that are able to interact with the environment through sen-
sors for both navigation and behavior generation. However, analysis revealed
that these controllers, although appearing to display obstacle avoidance be-
havior, memorized routes through the environment. In order to address this,
a third set of experiments is detailed that evolves GRN controllers to perform
general purpose obstacle avoidance while being subjected to small amounts of
sensory noise.
10.2 The Evolutionary Developmental System (EDS)
The Evolutionary Developmental System is an object oriented computer
model of many of the natural processes of development [14]. At the heart
of the EDS lies the developmental core. This implements concepts such as
embryos, cells, cell cytoplasm, cell wall, proteins, receptors, transcription fac-
tors (TFs), genes, and cis-regulatory regions. Genes and proteins form the
atomic elements of the system. A cell stores proteins within its cytoplasm
and its genome (which comprises rules that collectively deﬁne the develop-
mental program) in the nucleus. The overall embryo is the entire collection
of cells (and proteins emitted by them) in some ﬁnal conformation attained
after a period of development. A genetic algorithm is wrapped around the
developmental core. This provides the system with the ability to evolve
genomes for the developmental machinery to execute. The following sections
describe the main components of the developmental model: proteins, genes,
and cells.

226
Kumar
10.2.1 Proteins
In the EDS, the concept of a protein is captured as an object. In total there
are 40 proteins (see [14] for more details), each protein having four member
variables:
•
an ID tag (simply an integer number denoting one of 46 pre-deﬁned pro-
teins the EDS uses to control cellular behavior);
•
source concentration (storing the concentration of the protein);
•
two sets of coordinates (isospatial [6] and Cartesian);
•
a bound variable (storing whether or not a receptor has bound a protein).
(The latter is only used in receptor proteins.)
10.2.2 Genes
In nature, genes can be viewed as comprising two main regions: the cis-
regulatory region [3] and the coding region [16]. Cis-regulatory regions are
located just before (upstream of) their associated coding regions and eﬀec-
tively serve as switches that integrate signals received (in the form of pro-
teins) from both the extra-cellular environment and the cytoplasm. Coding
regions specify a protein to be transcribed upon successful occupation of the
cis-regulatory region by assembling transcription machinery.
The EDS uses a novel genetic representation termed the cis-trans architec-
ture (Fig. 10.1), based on empirical genetics data emerging from experimental
biology labs [3]. The ﬁrst portion of the genome contains protein speciﬁc val-
ues (e.g. protein production, decay, diﬀusion rates). These are encoded as
ﬂoating-point numbers.
Transcription direction
1
4
7
4
Gene 1
2
6
1
7
Gene 2
1
4
4
1
Gene 3
3
7
2
1
Gene 4
Cis−sites
Cis−sites
Cis−sites
Cis−sites
cis−reg region
cis−reg region
cis−reg region
cis−reg region
Fig. 10.1. An arbitrary genome created by hand. Genes consist of two objects: a
cis-regulatory region and a coding region. Each number denotes a protein
The remaining portion of the genome describes the architecture of the
genome to be used for development. It describes which proteins are to play a
part in the regulation of diﬀerent genes. It is this latter portion of the genome
that is employed by each cell for development.
Currently, the EDS’s underlying genetic model assumes a “one gene, one
protein” simpliﬁcation rule [16] to aid in the analysis of resulting genetic

10 On Form and Function
227
regulatory networks. The genome is represented as an array of gene objects
(Fig. 10.1). Each gene object contains two members: a cis-regulatory region
and a protein coding region. The cis-regulatory region contains an array of
TF target sites. These sites bind TFs in order to regulate the activity of the
gene. The gene then integrates these TFs and either switches the gene “on”
or “oﬀ”.
Details of the equations used to calculate activity of a single gene are
given in Table 10.1. Integration is performed by summing the products of
the concentration and interaction strength (weight) of each TF to ﬁnd the
total activity of all TFs occupying a single gene’s cis-regulatory region; see
(10.1). This sum provides the input to (10.3), yielding a probability of the
gene ﬁring [10,14].
10.2.3 Cells
Cell objects in the EDS have two state objects: current and new. During
development, the system examines the current state of each cell, depositing
the results of the protein interactions on the cell’s genome in that time step
into the new state of the cell. After each developmental cycle the current and
new states of each cell are swapped, ready for the next cycle.
The EDS supports a range of diﬀerent cell behaviors, triggered by the
expression of certain genes. The behaviors used for the experiments described
in this work are:
Table 10.1. Equations used to calculate the activity of a single gene by summing
the weighted product of all transcription factors regulating a single structural gene
inputj =
d

i=1
conci × weightij
(10.1)
where inputj = total input of all TFs assembling upon the jth gene’s cis regulatory
region; i = current TF; d = total number of TF proteins visible to the current
gene; conci = concentration of the ith TF at the centre of the current cell; and
weightij = interaction strength between TF i and gene j;
activityj = inputj −THRESHOLD CONSTANT
SHARPNESS CONSTANT
(10.2)
where activityj = total activity of the jth gene; inputj = total input to the jth
gene; and SHARPNESS CONSTANT = a constant taken from the range 0.1–
0.001 and is typically set to 0.01;
activation probabilityj = 1 + tanh(activityj)
2
(10.3)
where activation probabilityj = activation probability for the jth gene; and
activityj = total activity of the jth gene.

228
Kumar
•
division (when an existing cell “divides”, a new cell object is created and
placed in a neighboring position);
•
the creation and destruction of cell surface receptors; and
•
apoptosis (programmed cell death).
The EDS uses an n-ary tree data structure to store the cells of the embryo,
the root of which is the zygote (initial cell). As development proceeds, cell
multiplication occurs. The resulting cells are stored as child nodes of parents’
nodes in the tree. Proteins are stored within each cell. When a cell needs to
examine its local environment to determine which signals it is receiving, it
traverses the tree, checks the state of the proteins in each cell against its own
and integrates the information.
The decision for a cell to divide in the EDS is governed by the ratio of
division activator protein to repressor. The direction (or isospatial axis) the
daughter cell is to be placed in is non-random and is speciﬁed by the position
of the mitotic spindle within the cell, see [14] for more details.
10.3 Experiments
10.3.1 Form: Multi-cellular Development and Diﬀerentiation
This section details experiments to investigate the evolution of GRNs for
multi-cellular development. GRNs were evolved for the task of constructing
multi-cellular 3D geometric form and structure in a robust, self-organizing
manner. The target forms are a plane and cube.
10.3.2 System Setup
The experiments in this section used the following parameter settings: 100
runs with a population size of 100 evolving for 250 generations. One-point
crossover is applied all the time, while Gaussian mutation was applied at a
rate of 0.01 per gene. Tournament selection was used with a tournament size of
33 (although 33 is regarded as high, informal experimentation with the system,
not reported here, provided this value). Thirty developmental iterations, 100
generations, six proteins and two cis-sites per gene were used for the multi-
cellular cube experiment. Fitness for multicellular cubes was determined using
the equation for a cube. The number of cells inside and outside the enclosed
cube was determined, resulting in a function to be minimized (10.4):
ﬁtness =

1
cells inside

+
cells inside
SCALE

(10.4)
where SCALE refers to a shape dependant constant deﬁning total number of
cells in shape.

10 On Form and Function
229
10.4 Results and Analysis
10.4.1 Multi-cellular 3D Morphology
Plane
Figure 10.2(c) shows a crucial change in development from the ﬁve-cell stage
(not shown) – two divisions occur in iteration 9. These divisions are due to the
zygote and the ﬁfth cell. The zygote manages to increase its level of protein
6 above a threshold and change division direction to 1, while the ﬁfth cell
(shown in the lower left corner of Fig. 10.2d) also divides but in direction 0,
giving rise to this seven cell embryo. The new cell in the lower left corner is
fated to die.
Figure 10.2(d) shows the state at iteration 10 in which the new cell in the
lower left corner of the image ceases dividing. Instead, the upper right portion
of the image shows division has occurred again in direction 1. The cell in the
(a) Zygote, iteration 1
(b) Iteration 2 – zygote di-
vides in direction 1
(c) Iteration 9 – seven-cell
stage, growth in two direc-
tions
(d) Iteration 10 – eight-
cells, growth occurs in up-
per right corner, this cell is
fated to die
(e) Iteration 11 – 10-cell
T-shape embryo, with di-
vision in directions 0 and
11
(f) Final state of plane –
with all proteins removed
[ 27, 11 | 36 ] [ 15, 4 | 20 ] [ 15, 36 | 17 ] [ 7, 37 | 6 ] [ 11, 27 | 33 ]
[ 7, 15 | 30 ] [ 17, 36 | 2 ] [ 22, 5 | 22 ] [ 8, 13 | 10 ] [ 28, 7 | 0 ]
Fig. 10.2. The development of the best plane (top) and its evolved genome (below)

230
Kumar
lower left and upper right corners of Fig. 10.2(d) are ephemeral additions and
are ultimately fated to commit suicide (or apoptosis).
Figure 10.2(e) shows the zygote’s ﬁrst daughter cell has managed to divide
in direction 11 resulting in a T-shape. Thereafter, cells eventually start to
divide in direction 11 and others in direction 10, thus giving rise to the main
body of the plane.
Cube
Figure 10.3 shows snapshots of the development of the best cube, which at-
tained a ﬁtness score of 0.006173. Figure 10.3 also shows the evolved genome.
Evolution has only evolved a single gene for directional control of cell division
through gene 7, which emits protein 4. Protein 4 rotates the division spindle
anti-clockwise by 1 direction (full analysis is beyond the scope of this chapter,
see [14]).
In the zygote, for example, two proteins control (more or less) the activa-
tion of gene 3: proteins 0 and 4, conferring inhibitory and excitatory stimuli,
respectively. In the daughter cell, levels of both proteins 0 and 4 are low due to
division, and so do not provide suﬃcient inhibition or activation (not shown,
see [14]).
Instead, it falls not only to other proteins, such as proteins 24 and 37
(not shown), to provide inhibition, but also to cell signaling, which initially
delivers large inhibitory stimuli through the receptor 13-proteins 4 and 31
signal transduction pathways from the ﬁrst division in iteration 3.
Over time, as receptor 13 decays so does the inhibitory stimulus received
through that pathway. Leaving the job of inhibiting gene 3 in the daughter
cell to an alternative pathway. Note a full analysis is beyond the scope of this
chapter; see [14].
It must be noted that both cells by virtue of expressing a diﬀerent subset
of genes also have a diﬀerent subset of active receptors. The zygote begins
development with an assortment of receptors, while the daughter cell (and
later progeny) inherit their state including receptors from their parent, and
then begin to express diﬀerent genes and consequently diﬀerent receptors.
In addition, the behaviour of certain receptors (for example, 9 and 10)
reﬂects the fact that diﬀerent receptors may interact with exactly the same
proteins, but the outcome of the interaction may be diﬀerent. This type of
diﬀerential receptor–protein interaction is important in development.
The gene expression plots of Fig. 10.4 reveal important diﬀerential gene
expression patterns between the two cells, i.e. the cells have diﬀerentiated.
Noticeably, genes 3 and 9 are expressed, albeit sparingly, in the zygote, but
not at all in the daughter cell. Other important diﬀerences in gene expression
between the two cells are the expression of genes 7 and 8, which are both
increasingly activated, in the zygote, over time, but are seldom activated in
the daughter cell.

10 On Form and Function
231
(a) Zygote
(b) Iteration 4 – three-cell
stage
(c) Iteration 5 – four-cell
stage with new cell placed
in the lower front of the
embryo
(d) Iteration 10 – ﬁve it-
erations later and the em-
bryo remains at the four-
cell stage
(e) Iteration 15 – 11-cell
stage
with
many
long-
range proteins removed for
clarity
(f) Iteration 20 – the core
of the cube begins to take
form
(g) Iteration 25 – approxi-
mate cube structure is es-
tablished
(h) Final
state
of
cube
with all proteins removed
from view for clarity
(i) Final state of cube with
all proteins present
[ 32, 3 | 28 ] [ 0, 35 | 20 ] [ 4, 30 | 28 ] [ 26, 28 | 0 ] [ 26, 36 | 30 ]
[ 33, 32 | 31 ] [ 25, 8 | 4 ] [ 27, 23 | 21 ] [ 22, 1 | 27 ] [ 37, 8 | 14 ]
Fig. 10.3. The development of the best cube (top) and its evolved genome (below)

232
Kumar
(a)
(b)
Fig. 10.4. Gene expression plot for the (a) zygote and (b) the ﬁrst daughter cell
of the best cube. Vertical axis denotes time, horizontal axis denotes gene ID, white
and black squares denote activated and inactivated genes, respectively
10.4.2 Function Experiments: Reactive Robot Control
Having successfully explored the evolution of form using developmental bi-
ology-inspired methods, via the EDS, this section explores the evolution of
function (i.e. reactive robot control) using developmental methods. This work
had two main objectives:
•
show that GRNs are a plausible technology for real-time reactive robot
control; and
•
generate reactive behaviors such as traversing an obstacle laden world.
However, during the course of the experiments, analysis of evolved solu-
tions generated an additional objective:
•
to evolve general purpose obstacle avoidance irrespective of the robot’s
position in the environment.

10 On Form and Function
233
Due to the inherent noise in the GRN’s gene activation mechanism and
the addition of Gaussian noise to the sensory inputs, reactive robot control
would be achieved in the face of noise at both levels. The number of proteins
was changed between the experiments due to problem requirements such as
the use of sonar proteins, for example.
Noise
The GRNs used in this work employed noise at two diﬀerent levels:
•
gene activation (rule ﬁring); and
•
sensory input (in the third experiment).
At the gene activation level, the ﬁring of genes (rules) was performed
probabilistically, thus introducing noise at the level of rule ﬁring making the
task of evolving robot controllers more diﬃcult. The incorporation of noise at
this level is justiﬁed on the basis that it enables the system to evolve robust
rule sets. Noise at the level of sensory input (only employed in experiment 3)
was provided by adding a small degree of Gaussian noise (σ in the small range
[0.01–0.08]) to the sonar values before being input to the GRN controller.
10.4.3 Robots
This work employed two diﬀerent robots – a Pioneer 2DX (see Fig. 10.5a), and
an Amigobot (see Fig. 10.5b) – in simulation using the University of Southern
California’s robot simulator, player-stage [7]. This simulator was selected for
many reasons of which the most salient was the wide range of robots sup-
ported, and the inherent noise in the system. This element of stochasticity
aides the transition of the controller over, what has now become known as,
the “reality gap” from simulation to real hardware [8].
(a)
(b)
Fig. 10.5. Robot images

234
Kumar
Three sets of experiments were performed. In the ﬁrst and second set
of experiments, both robots were permitted to rotate either left or right and
move forwards, but were prevented from moving backwards so as to encourage
clear paths to emerge while avoiding obstacles. In the third set of experiments,
however, the robots were allowed to reverse.
Fitness Function
All experiments involved using Euclidean distance as part of the ﬁtness func-
tion. For the ﬁrst set of experiments, the task set for the robots was to maxi-
mize Euclidean distance – to move as far as possible – within a set period of
time, approximately 12 seconds with no sensory information. In the second
experiment, the robot was allowed to use sensors.
The third set of experiments used Euclidean distance as one of the terms,
the full ﬁtness function is shown in (10.5). This equation is an adapted version
of the obstacle avoidance ﬁtness function used by [18], it deﬁnes an equation to
be maximized. It is, however, lacking an additional term, which [18] included:
a term to measure whether or not the robot is moving in a straight line. This
was included in Mondada’s work to prevent evolution awarding high ﬁtness
values to controllers that cause the robot to traverse a circle as fast as it can.
In place of this term, the Euclidean distance was used.
Avoidance =

(e + s + SonarValueMAX)
(10.5)
where e is the Euclidean distance, s the speed of the robot, and SonarValueMAX
the reading from the sonar with the most activity.
Experiment 1: Memorizing a Path
The ﬁrst experiment requires the memorization of a path through an environ-
ment with obstacles using no sensors or start seeds (i.e. no maternal factors
– solutions are evolved completely from scratch). Although this experiment
does not class as “reactive” robot control, a solution to this problem does
re-quire internal state; thus the experiment was deemed necessary in order to
ascertain the level of state embodied within GRNs. The GRN could only use
the ﬁrst four proteins shown in Table 10.2. Note that this experiment was only
performed with the Pioneer 2DX in simulation. This task implicitly requires
internal state in order to solve the problem. This set of experiment used four
proteins, a population size of 100 evolving for 100 generations, with 10 genes,
two cis-sites, and a simulation lifetime of 12 seconds per ﬁtness function trial.
Experiment 1: Results
As can be seen from Fig. 10.6, the EDS evolved genetic regulatory network
controllers that were successfully able to memorize a path through the environ-
ment without any sensors. The ﬁgures illustrate two diﬀerent paths discovered

10 On Form and Function
235
Table 10.2. List of proteins and their purpose
Protein ID
Purpose
0
Move forwards
1
Move backwards
2
Rotate counter-clockwise
3
Rotate clockwise
4
No purpose
5
Rear left sensor
6
Left sensor
7
Front-left sensor 1
8
Front-left sensor 2
9
Front-right sensor 1
10
Front-right sensor 2
11
Right sensor
12
Rear-right sensor
by two diﬀerent controllers evolved from scratch. Figure 10.6(a) provides an
example of the robot traversing a circular trajectory with minor deviations.
To encourage a more intricate path to be found, the wall in the upper left
corner of Fig. 10.6(a) was brought down, thus blocking the robot’s path in
the upper left direction, see Fig. 10.6(b). With the wall now blocking the
robot’s path Fig. 10.6(b) shows a diﬀerent evolved GRN resulting in a more
intricate path with no environment interaction. In this example, the robot is
able to navigate quickly through the environment negotiating left and right
turns past obstacles. This example reﬂects internal state in GRNs.
Using no sensors, a population size of 100 individuals and 100 generations,
good solutions capable of discovering clear paths through the environment
emerged at around the 54th generation.
(a)
(b)
Fig. 10.6. No sensors. The robot has no sensory feedback whatsoever. It learned
a route through the obstacle course. Moving objects in its path would cause the
controller to break

236
Kumar
Experiment 2: System–Environment Interaction
While memorizing paths through an environment using no sensory feedback
may seem to be of limited use, it does demonstrate that GRNs are capable
of encoding such paths and that these kinds of solutions are evolvable. In
the second experiment, both the Pioneer 2DX and the Amigobot were used,
again in simulation, but with diﬀerent types of sensors. In the case of the
Pioneer, SICK LMS 200 laser-range ﬁnder sensors were employed, while the
Amigobot used its eight sonar sensors. The laser values were compiled into
two values: one which corresponded to a left side sensor and the other, which
corresponded to a right side sensor. The Amigobot has sonar emitters and
receivers distributed around the robot with six arranged around the sides and
front, while two are located at the rear of the robot. Since the robot was
not permitted to reverse, the two sonars at the rear of the robot were not
used (only in experiments 1 and 2). Consequently, the remaining six sonars
were split in two and provided left and right sonar sensing. The settings used
were: seven proteins, 15 genes, six cis-sites and a population size of only 25
individuals evolving for 50 generations.
Experiment 2: Results
Figures 10.7 and 10.8 show that despite noisy gene transcription (noisy rule
ﬁring) GRNs are evolved that are able to control both a Pioneer 2DX robot
equipped with SICK LMS lasers, and an Amigobot equipped with sonars
through the same environment more convincingly.
Additionally, coupling the system and environment through sensors en-
ables faster evolution of successful solutions, for example, all experiments
using sensors resulted in good solutions, able to cope with noisy gene tran-
scription, emerging within just four generations using 25 individuals. Compare
this to the performance of GRNs evolved with no sensors in experiment 1.
Fig. 10.7. Amigobot sonar. Only sonar sensors are used, which immediately provide
suﬃcient information for the robot to take intricate paths through the obstacle
course. This solution was found much more quickly than without sensors

10 On Form and Function
237
(a)
(b)
Fig. 10.8. Laser avoidance. Sensory feedback in the form of laser sensors allowed
the robot to navigate intricate paths through the obstacle course. These solutions
were found much more quickly than without sensors
Experiment 3: General Purpose Obstacle Avoidance (GPOA)
Analysis of the results in experiment two revealed that evolution had managed
to cheat: the controllers had learned routes through the world just as in the
ﬁrst experiment. Except this time, solutions were found much sooner since
evolution was able to exploit sensory information from the laser and sonars.
With this in mind a third objective was set: to evolve a general purpose
obstacle avoidance GRN controller.
In this experiment, in order to ensure the controllers were using sensory
information and not simply using additional proteins to memorize the route,
only sensor information – in the form of protein concentrations – was used in
the GRN. In other words, the GRNs could only use sensor proteins (proteins
5 through 12, see Table 10.2) to trigger gene expression. This was achieved
by forcing cis-site inputs to genes to be sonar proteins. The total number of
cis-sites per gene was increased for this experiment to six. Thus, out of a total
of eight sonar proteins only six were used per gene, note the particular sonar
proteins used were set by evolution. A total of 13 proteins were used for this
set of experiments: four proteins for movement, eight for sonar readings, and
one protein with no purpose (evolution can use this protein as it sees ﬁt).
Noise was added at the level of gene activation in the form of probabilistic
gene activation. In addition, Gaussian noise was added to all sonar values
with a small standard deviation, σ = 0.02, (note experiments with σ = 0.2
were also performed successfully) arbitrarily selected. A single ﬁtness evalua-
tion consisted of controlling the robot for twelve seconds using the currently
evolved GRN. The parameter settings for this experiment are: seven genes, six
cis-sites, 13 proteins, a simulation lifetime of 12 seconds per ﬁtness function
trial, and 25 individuals evolving for 25 generations. As Fig. 10.9(a) shows, a
new world was created with corridors, open spaces, and obstacles in the form
of walls.

238
Kumar
Experiment 3: Results
A solution displaying general purpose obstacle avoidance behavior was found
in generation ﬁve and gradually optimized over the subsequent twenty gen-
erations, see Fig. 10.9(b). As the robot moved around the world it tended to
display a general anti-clockwise turning motion while avoiding obstacles.
(a)
(b)
Fig. 10.9. General purpose obstacle avoidance controller
This particular solution’s approach to negotiating obstacles was as follows:
on approaching an obstacle head-on, and fast, the controller caused the robot
to reverse. On a slower approach the robot got within a certain distance of the
object and either reversed or very slowly bumped into the object, upon which
reverse is triggered immediately. An obstacle sensed behind the robot, how-
ever, always resulted in immediate forward movement away from the object.
It is worthwhile noting that despite being evolved in a static environment,
where each ﬁtness assessment consisted of a single trial, the Amigobot can
be moved to diﬀerent areas of the environment and still maintains general
purpose obstacle avoidance behavior. Additionally, informal experiments have
shown that evolved GRN controllers are quite robust with respect to the level
of noise added, for example, this GPOA controller evolved using sonar noise
with a standard deviation σ = 0.02 (very small), yet as the noise is increased
(to a maximum of σ = 0.2) the controller manages to cope with no adverse
eﬀects.
10.5 Discussion
This chapter has demonstrated the application of the developmental metaphor
for the dual tasks of generating form and function. Form was explored through
the evolution of robust controllers for the construction of 3D, multi-cellular
form and structure in a self-organizing manner, akin to the development of a
single cell into a complex multicellular system. While function, in the form of

10 On Form and Function
239
reactive robot control, was evolved through genetic regulatory network robot
controllers, which replace the function of a neural network.
This chapter has summarized successful experiments to evolve processes
of development controlled by genetic regulatory networks. This was achieved
through the use of a bespoke software testbed, the Evolutionary Developmen-
tal System (EDS), which represents an object-oriented model of biological
development.
The successful evolution of genetic regulatory networks that are able to
specify the construction of 3D, multi-cellular forms that exhibit phenomena
such as cell diﬀerentiation and subroutining was shown.
In addition, the successful application of the developmental metaphor to
the ﬁeld of evolutionary robotics was demonstrated by exploring the ability of
GRNs to specify reactive robot behaviors. Through the successful evolution of
GRN robot controllers that provide general purpose obstacle avoidance, the
following was shown:
•
the developmental biology metaphor can be productively applied to the
problem domain of evolutionary robotics;
•
GRNs are a plausible technology for real-time reactive robot control;
•
GRNs have internal state and are capable of generating reactive behaviors
such as traversing an obstacle laden world; and
•
GRN controllers can be evolved for general purpose obstacle avoidance
that are robust to small amounts of varying noise at two important levels:
rule ﬁring (or gene expression) and sensor noise.
In biology, the developmental and genetic machinery combine to provide
a method by which nature can construct, in real-time, robust, complex forms
capable of complex functions.
In order to ensure GRNs remain a viable option for the ﬁeld of evolution-
ary robotics, further research is required into the ability of GRNs to specify,
multiple low-level reactive robot behaviors in a modular manner. The ability
of GRNs to construct modular networks while being robust to damage makes
GRNs very well suited to the task of generating form and function. The pre-
liminary results shown here oﬀer encouraging potential for future research
into developmental-based controllers, and indeed developmental biology in-
spired approaches to evolutionary design, evolutionary robotics and robotics
in general.
Acknowledgments
Many thanks to Peter Bentley, Ken De Jong, Michel Kerszberg, Sean Luke,
Paul Wiegand, Lewis Wolpert, and Binita Kumar for helpful advice and crit-
icism. This research was funded by Science Applications International Corpo-
ration.

240
Kumar
References
1. Bentley, P.: Adaptive fractal gene regulatory networks for robot control. In:
J. Miller (ed.) Genetic and Evolutionary Computation Conference Workshop
Proceedings – Workshop on Regeneration and Learning in Developmental Sys-
tems (2004)
2. Bongard, J.: Evolving modular genetic regulatory networks. In: Proceedings
of the 2002 IEEE Congress on Evolutionary Computation (CEC 2002), pp.
1872–1877 (2002)
3. Davidson, E.: Genomic Regulatory Systems: Development and Evolution. Aca-
demic Press (2001)
4. Dellaert, F.: Towards a biologically defensible model of development. Master’s
thesis, Case Western Reserve University (1995)
5. Eggenberger, P.: Evolving morphologies of simulated 3D organisms based on
diﬀerential gene expression. In: Proceedings of the Fourth European Conference
on Artiﬁcial Life, pp. 205–213 (1997)
6. Frazer, J.: An Evolutionary Architecture. Architectural Association Publica-
tions (1995)
7. Gerkey, B., Vaughan, R., Howard, A.: The player/stage project: tools for multi-
robot and distributed sensor systems. In: Proceedings of the 11th International
Conference on Advanced Robotics (2003)
8. Jakobi, N.: Harnessing morphogenesis.
In: Proceedings of the International
Conference on Information Processing in Cells and Tissues (1995)
9. Jiang, T., Wideltz, R., Shen, W., Will, P., Wu, D., Lin, C., Jung, J., Chuong,
C.: Integument pattern formation involves genetic and epigenetic controls op-
erated at diﬀerent levels: feather arrays simulated by a digital hormone model.
International Journal on Developmental Biology (2004)
10. Kerszberg, M., Changeux, J.: A simple molecular model of neurulation. BioEs-
says 20, 758–770 (1998)
11. Kicinger, R., Arciszewski, T., De Jong, K.: Evolutionary design of steel struc-
tures in tall buildings. Journal of Computing in Civil Engineering 19(3), 223–238
(2005)
12. Kitano, H.: Designing neural networks using genetic algorithms with graph gen-
eration system. Complex Systems 4(4), 461–476 (1990)
13. Kumar, S.: A developmental biology inspired approach to robot control. In:
Proceedings of the Artiﬁcial Life 9 Conference (2004)
14. Kumar, S.: Investigating computational models of development for the construc-
tion of shape and form. Ph.D. thesis, University College London (2004)
15. Kumar, S., Bentley, P.: On Growth, Form and Computers. Elsevier Academic
Press (2004)
16. Lewin, B.: Genes VI. Oxford University Press (1999)
17. Marcus, G.: The Birth of the Mind. Basic Books (2004)
18. Mondada, F., Floreano, D.: Evolution of neural control structures: Some exper-
iments on mobile robots. Robotics and Autonomous Systems 16(2–4), 183–195
(1995)
19. Prusinkiewicz, P., Lindenmeyer, A., Hanan, J.: Developmental models of herba-
ceous plants for computer imagery purposes. In: Proceedings of the 15th Annual
Conference on Computer Graphics and Interactive Techniques (1988)

10 On Form and Function
241
20. Quick, T., Dautenhahn, K., Nehaniv, C., Roberts, G.: Evolving embodied ge-
netic regulatory network-driven control systems.
In: Proceedings of the 7th
European Conference on Artiﬁcial Life (ECAL 2003) (2003)
21. Schultz, A., Grefenstette, J.: Evolving robot behaviors. In: Proceedings of the
1994 Artiﬁcial Life Conference (1994)
22. Shen, W., Salemi, B., Will, P.: Hormone-inspired adaptive communication and
distributed control for CONRO self-reconﬁgurable robots. IEEE Transactions
on Robotics and Automation 18(5) (2002)
23. Taylor, T.: A genetic regulatory network-inspired real-time controller for a group
of underwater robots. In: Proceedings of Intelligent Autonomous Systems 8, pp.
403–412 (2004)
24. Wolpert, L., Beddington, R., Brockes, J., Meyerowitz, E.: Principles of Devel-
opment. Oxford University Press (2002)

11
Modularity in a Computational Model of
Embryogeny
Chris P. Bowers
School of Computer Science, University of Birmingham, UK
C.P.Bowers@cs.bham.ac.uk
11.1 Introduction
Natural evolution is about searching for adaptations in order to survive in a
constantly changing environment. Of course, in nature these adaptations can
occur at many levels from the single gene to entire populations of individuals.
Adaptation can consist of rapid successful genetic changes, common in many
forms of bacteria, to changes in phenotypic behaviour, such as the ability of
humans to develop new skills and technologies.
Nature has evolved techniques, such as multi-cellular organisms and the
complex unfolding of genetic information, in order to provide new, and more
successful, ways of adapting. One could argue that the evolutionary search
process is not the really interesting part of evolution. After all, it is just an-
other form of search comparable in many ways to other search processes. What
is really interesting about evolution is the systems and behaviours that have
been observed to have built up around it in both natural and simulated evo-
lutionary systems. On the other hand, simulations of evolutionary processes
are typically very constrained, in how they adapt, by the representations that
are used.
We are becoming increasingly constrained by the approaches we take to en-
gineering and developing new technologies as our technological requirements
become more demanding [25]. Nature can be of great inspiration to those sci-
entists and engineers who strive to create a new generation of technologies that
overcome the limitations of existing ones. Such approaches are often termed
“nature inspired” and the beneﬁts are widely displayed. Biological systems are
capable of adaptation, self-organisation, self-repair and levels of complexity
against which our current engineering endeavours pale in comparison.
One approach is to try to integrate some of the adaptations that nature has
produced into a simulated evolutionary framework. This requires careful, and
often extremely diﬃcult, untangling of these adaptations from the biological
and physical framework in which they developed. However, striving to achieve

244
Bowers
these same capabilities through simulated evolution may be an extremely
fruitful direction of research.
One such adaptation is embryogeny which embodies the natural process by
which the genetic representation within the single initial cell (zygote) controls
the development of that cell to form a functioning multi-cellular organism.
This process allows an extraordinarily complex organism (phenotype) to be
produced from a relatively simple set of genetic instructions (genotype). This
can be achieved because the zygote cell divides to produce additional cells that
continue to proliferate to eventually produce every cell within the organism.
All of these cells originated from the zygote cell and all contain the same
genetic information. Therefore, diﬀerences in the behaviour of cells within an
organism, as a result of this genetic information, must be derived from the
state of the cell and its environment. However, the state of the cell and its
environment can also depend upon the actions of cells. This forms a complex
interaction between the phenotype and genotype and results in an extremely
complex mapping between the two representations.
11.2 Why a Computational Model of Embryogeny?
A commonly used argument to justify the simulation of an embryogeny-
like process is to point to the issue of scalability in relation to phenotype
size [6,24,30]. In a typical evolutionary algorithm the representation used to
transform the genotype into a phenotype is either a direct (one-to-one) map-
ping or an indirect (many-to-one) mapping. This may lead to an association
of size between the genotype and phenotype. In order for simulated evolution
to tackle increasingly complex engineering and research problems, the search
space of a direct encoding would also become increasingly large and diﬃcult
to navigate. This has been a major inﬂuence upon the movement to more indi-
rect encoding approaches [5,6,15,30]. Since modelling embryological processes
introduces a complex (indirect) mapping between the genotype and pheno-
type, this association of size no longer exists. However, it is important to note
that this eﬀectively creates two diﬀerent spaces between which neighbourhood
relationships are not necessarily preserved.
A second argument is that nature is very good at maintaining a system
that is undergoing perpetual change with cells constantly dying and being
replaced. These natural systems demonstrate high levels of robustness and
self-repair and work has begun to try to build these characteristics into man-
made systems [23,24].
With the rapid advancement of computational resources, investigation util-
ising simulation of such processes has become a rapidly growing ﬁeld. It is a
subject that has been widely recognised as being of fundamental importance
by researchers in various strands of evolutionary computation and evolution-
ary biology. However, even with ever improving resources, progress has proven
to be slow in coming and signiﬁcant results rare and diﬃcult to obtain.

11 Modularity in a Computational Model of Embryogeny
245
A further side eﬀect of a computational model of embryogeny, due to the
complex nature and potential redundancy within the mapping, is the existence
of many individuals which, whilst genetically diverse, perform identically when
evaluated. These individuals are considered to be “neutral” relative to each
other. Such neutrality has important implications for computational search
processes because it changes the nature of the search space. Local optima
become much rarer with most individuals belonging to networks of individuals
of equal ﬁtness (neutral networks) which can be vastly distributed across the
entire search space.
What happens to a population, which inhabits a neutral network, is largely
dependent upon how the evolutionary process has been deﬁned. Typically, the
result is a random drifting (neutral drift) of the population until a path to
a ﬁtter individual is found and then adaptive evolution recommences. This
enables the population to appear converged on local optima at the phenotypic
level whilst neutral drift allows for genetic divergence.
During neutral evolution, by deﬁnition, there can be no directed selection
pressure. However selection is still ongoing, albeit random. Wilke [34] argued
that some of these randomly selected parents will be more likely to produce
viable oﬀspring than others. The more densely connected the neutral network
an individual occupies the more likely that its oﬀspring will be neutral rather
than deleterious. This will place a bias on evolution toward more densely
connected neutral networks.
However, the eﬀect of neutrality on a search process is still an open ques-
tion. Miglino et al. [22] argued that neutral changes could be functional in the
sense that they enable pre-adaptations which subsequently become the basis
for an adaptive change. Hogeweg [16] showed, that using a model of mor-
phogenesis, the behaviour of mutated individuals during neutral evolution,
which was termed the mutational shadow of the neutral path, was noticeably
diﬀerent than during adaptive evolution.
There have been many arguments for how neutrality may result in an
improved evolvability. Ebner et al. [13] suggested that the redundancy causing
the neutrality also increases the likelihood that evolution can ﬁnd a smooth
and amiable search path and that neutral networks help to prevent early
convergence.
Introducing such complex mappings to the genotype-phenotype map is of-
ten viewed as an added complication that makes simulating and understanding
evolution even more diﬃcult. However, it is important to recognise that this
mapping process can eﬀectively be used to direct the evolutionary search path
and without it, there may be no possibility of truly understanding evolution-
ary systems. Wagner and Altenberg [33] used the analogy of a monkey and
a verse of Shakespeare to describe the importance of a mapping process. If a
monkey randomly presses keys on a typewriter it is far more likely to produce
a verse of Shakespeare than if it was given a pencil and paper. This is due to
mechanical constraints of the typewriter to produce only viable text charac-

246
Bowers
ters. In the same fashion, genetic variation can have a constrained eﬀect on
the phenotype via the genotype-phenotype map.
The hypothesis upon which this work is based is that an evolutionary
search will bias towards individuals which are more densely connected within
a neutral network. Since each neutral network will have in common a set of
genes which generate the shared ﬁtness of that network, a modularity within
the mapping between genotype and phenotype is created around these genes.
11.3 A New Computational Model of Embryogeny
Biological systems are subjected to, and are highly entrenched in, the physics
of the environment in which they develop. This makes accurate simulation of
biological systems incredibly diﬃcult since many of the underlying features
are heavily entangled with processes dependent upon the precise physics of
the environment.
The model presented here is an attempt to simulate the dynamics achieved
within biological systems between the genetic and phenotypic representations.
The model strikes a balance between being both computationally feasible
and expressing the growth dynamics observed in real biological systems. The
purpose of this model is to provide a tool for investigating the consequences
of such a distinction between genotype and phenotype. In this sense, the
biological references are, to a large extent, only inspirational.
11.3.1 Cell Position
Models of cell growth often use grid-based or iso-spatial coordinate systems [7,
20, 24]. This makes the simulation of chemical diﬀusion and choice of data
structure much easier. However, it introduces problems for cell division and
physical cell interactions due to the positional limitations enforced by the
coordinate systems on the cells. For example, cell division becomes awkward
if all neighbouring elements of a dividing cell are already occupied. The most
common solution is to simply over-write an existing cell if a new cell is to be
placed in that location [9,24].
In our new model, a diﬀerent approach is taken which removes these limita-
tions and retains the ability to model chemical diﬀusion whilst allowing more
rational physical interactions between cells. Each cell exists in a real valued
position with coordinates stored as a two-dimensional vector. This allows a
much greater variety of structural forms than models using grid or iso-spatial
coordinates. The position of a cell is measured from its geometrical centre and
can be placed anywhere within a unit square, [0, 1]2, based upon a standard
Cartesian coordinate system.

11 Modularity in a Computational Model of Embryogeny
247
11.3.2 Physical Body
Cells are structurally discrete with a deﬁnitive boundary between the internal
state of the cell and the outside environment. The cell membrane is a semiper-
meable layer, which allows some molecules to pass through whilst preventing
others. Transport mechanisms exist which allow the exchange of molecules
that cannot permeate through the cell membrane, under certain conditions.
In this model, chemicals from external sources are freely allowed to enter
the cell. Restrictions only apply to chemicals that exist within the cell. The
rate at which chemicals from an internal source can leave a cell is controlled
by a diﬀusion rate.
The choice of physical cell shape for many previous cell growth models was
based on a grid or iso-spatial approach, where the cell is modelled as a polygon
such as a square or hexagon. Since this model uses real-valued positions there
is no simple constraint in terms of the form or shape of a cell. For simplicity,
cells in this work are considered to be circular in shape, requiring only a single
variable for their deﬁnition, the radius.
11.3.3 Cell State and Diﬀerentiation
The cell state is deﬁned by a set of real-valued protein concentrations and
diﬀusion rates. The concentration represents the amount of a given chemical
that exists within a cell. The diﬀusion rate deﬁnes the amount of that chemical
which is free to diﬀuse out of the cell and into the environment. In both this
model and in natural systems, the cell state is highly dependent upon proteins,
since these are almost the only products of the genetic representation stored
within the cell. The genes act as protein factories and it is the proteins that
they produce which deﬁne the state of the cell and thus determines the speciﬁc
cell behaviour and function.
Cell diﬀerentiation is the process by which a cell becomes specialised in
form or function. Essentially a cell is considered to have diﬀerentiated if its
form or function has changed in some manner. Cell diﬀerentiation is an impor-
tant process in enabling multi-cellular organisms to consist of a large variety
of highly functionally specialised cells. The ability to rediﬀerentiate is also a
key aspect of multi-cellular growth.
This has been shown to be critical to the regenerative capabilities of some
creatures such as the newt which can regenerate limbs which have been subject
to damage [11], since the cells have enough plasticity to allow them to re-
diﬀerentiate into stem-like forms which then go on to re-grow the damaged
component. This capability is, of course, of vast interest to those aspiring
to enhance these biological traits or implement similar capabilities into man-
made structures such as electronic hardware [23,24].
The cell type is a result of the cell being in a speciﬁc cell state. Since the
model represents the cell state as a set of real-valued chemical concentrations
there are no obviously identiﬁable discrete cell types. In order to identify a cell

248
Bowers
as belonging to a speciﬁc cell type, a deﬁnition of that cell type must be given
as a set, or range, of values to which the cell state must correspond in order
to be identiﬁed as a cell of that given type. The simplest way to achieve this is
to allocate a single protein concentration to a cell type such that an increase
in that protein concentration indicates a closer aﬃnity to the corresponding
cell type. There are no externally enforced restrictions on cell types with any
cell able to rediﬀerentiate freely.
11.3.4 Division
Cell division consists of, ﬁrstly, the duplication of the genetic material held
within the cell and then physical partitioning of the cell into two. This whole
process is triggered as a consequence of the cell state. In most cases, cell
mass is doubled and vital components duplicated before division. In this case,
daughter and parent cells can be considered almost identical. There are ex-
ceptions, such as cleavage, where division occurs without increase in mass.
In some cells, especially those that form in layers, there is a polarity in the
internal structure of the cell. In these cases, it is common for the division
process to be orientated relative to this polarisation [1].
In this work, division is considered to be a replication of a cell. When a
cell divides it creates an almost identical copy of itself such that the only
diﬀerence between parent and daughter cell is position. The daughter cell is
placed alongside the parent cell and a vector stored within the parent cell,
the cell spindle, determines the direction of this placement. Each cell stores
its own independent spindle that it is free to re-orientate and enables the cell
to store directional information.
Cell division in biological systems must be regulated to ensure that growth
ceases at some point. This is dependent upon many factors associated with
the cell development process including energy constraints and physical forces.
This ability for a simulated cell growth process to regulate its own size has
been identiﬁed as a very diﬃcult problem [24]. In order to try to avoid such
a nontrivial issue, without detriment to the computational model of embryo-
geny, it is appropriate to directly control the regulation of phenotype size by
restricting both the cell radius and environment size such that the growth pro-
cess is terminated once the phenotypic environment is ﬁlled with cells. This
is easily achieved since the boundaries to the growth environment are a ﬁxed
unit square, and the cell radius is deﬁned as the ratio between the size of a
cell and this growth environment. Since all the cells have the same radius the
hexagonal closest circle packing ratio, π/
√
12, can be utilised since it gives the
optimal ratio between the area of the circles and the space in which they are
packed. Given the most proliﬁc rate of division will result in 2g cells, where
g is the number of iterations of cell division (growth steps), (11.1) approxi-
mately relates the cell radius, r, to the number of iterations of cell division
required to ﬁll the environment.

11 Modularity in a Computational Model of Embryogeny
249
g ≈log2
√
3 (1 + 2r)2
2πr2

: g ∈N.
(11.1)
It can only be an approximation since the curved surface of the cell does
not allow perfect tessellation but it does provide a suﬃcient upper bound on
cell numbers. The number of growth steps must clearly be a non-negative
integer value.
Although this removes provision for self-regulation of overall phenotypic
size there is still the ability to regulate the size of internal structures within
the phenotype relative to each other.
11.3.5 Cell Death
Cells can die from direct external inﬂuences that cause damage to the cell
structure (necrosis), but can also undergo a self-inﬂicted death for a purposeful
means. Programmed cell death refers to the case where it forms an essential
part of the multi-cellular growth process. Examples include the separation
from a single mass of the ﬁngers and toes in the embryo state and the death
of a cell as an immune system response.
In this model, cells are capable of programmed cell death, at which point
they are removed from the phenotype environment, such that they could be
considered never to have existed. Obviously, in natural systems, the dead cells
remain and must be physically dealt with accordingly.
11.3.6 Cell Signalling
One can argue that, in order for cells to form a multi-cellular structure with
behaviour consistent with a single organism, there must exist the capability
for cells to communicate. Cell signalling provides the ability to communicate
and there are two primary methods through which it is conducted.
Direct cell–cell communication describes the case in which a cell directs
the form of communication to a speciﬁc cell. Since the extra-cellular environ-
ment has no means to support this, direct contact between cells is required.
Information is then either passed directly through the cell wall to an internal
signal receptor or via signal receptors on the surface of the cell.
Indirect cell–cell communication prevents direct control over which cells
receive the signal, but allows longer-range communication. Cells secrete cell-
signalling molecules that diﬀuse in the extra-cellular environment. Cells can
receive these diﬀused signalling molecules in the same way as direct cell–cell
communication either at the cell surface or within the body of the cell.
Of course, as in all areas of biology, there are exceptions to these cases.
For example, neuronal cells are specialised for direct cell–cell interactions over
relatively larger distances by using neuritic protrusions from the cells that use
a mixture of chemical and electrical signals.

250
Bowers
For the purposes of this model, direct cell signalling is considered to be a
further adaptation of the cell whilst indirect cell signalling is a fundamental
feature of the underlying physics. For this reason only indirect communication,
through the diﬀusion of chemicals, is represented.
11.3.7 Chemical Diﬀusion
The capability of cells to diﬀerentiate based on positional information high-
lights the capability of multi-cellular organisms to organise their internal
shape, form and structure. This process is known as morphogenesis and is
highly dependent on the presence of diﬀusible chemicals, called morphogens.
These morphogens control the topological structuring of cells based on chem-
ical concentration patterns.
Diﬀusion is the spontaneous spreading of a physical property and is usu-
ally observed as a lowering in concentration often demonstrated using an ink
drop in a glass of water. Typically diﬀusion is modelled using a partial diﬀer-
ential equation which describes the changes in the density, ρ, of some physical
property, with diﬀusion rate c, over time t:
δρ
δt = c∇2ρ.
(11.2)
In the case of cell growth models based on a grid layout, this equation can
be adapted to a discrete grid based environment. If each element of the grid
has a neighbourhood, N, and a discrete Cartesian density at time t of ρx,y(t),
then:
ρx,y(t) = (1 −c)ρx,y(t −1) +
c
|N|

i,j∈N
ρi,j(t −1).
(11.3)
Another approach is to approximate the long term eﬀects of diﬀusion from
a point source using some localised function. Such instantaneous diﬀusion can
be calculated for any given point from a number of sources using a set of lo-
calised functions. This assumes that the long term behaviour of the diﬀusion
process will be stable with a constant rate of release and absorption of mor-
phogen. Kumar [20] chose to use a Gaussian function centred over the protein
source, s, which assumes the morphogen has diﬀused equally and consistently
in all directions:
ρ = se−d2/2c2
(11.4)
where c is the width of the Gaussian and replicates the behaviour of the
diﬀusion rate and s is the height of the Gaussian which corresponds to the
strength of the chemical source or release rate. The distance, d, is measured
between the point at which the diﬀusion is being calculated, ρ, and the location
of the chemical source.

11 Modularity in a Computational Model of Embryogeny
251
Although it makes vast assumptions about the diﬀusion characteristics of a
chemical, instantaneous diﬀusion is by far the simplest and quickest approach
to take. It removes the need for iterative calculations that other diﬀusion mod-
els require and there is no need to worry about boundary conditions. Also,
calculating diﬀusion using an iterative process requires a decision about the
rate at which chemicals diﬀuse in relation to the rate of cell growth. In this
sense, instantaneous modelling of chemical diﬀusion suggests that cell devel-
opment is occurring on a much longer time scale than chemical diﬀusion. So
any dynamics between the diﬀusion process and cell development are lost,
as is the ability to model reaction–diﬀusion systems. What remains is the
ability for diﬀusion to represent global information for cells such as direction,
magnitude and distance. This provides all the information that a real diﬀu-
sive system would provide, given a much faster rate of diﬀusion than rate of
cell growth and where morphogens are considered to be chemically inert to
each other. This also means that any interaction between chemicals is strictly
through processes within the cell. Therefore, the dynamics of the system are
solely a result of cell states such that any emergent behaviour is a result of
the embryogeny model alone.
In natural systems there is evidence that the growing organism can con-
struct orthogonal gradients in order to provide more detailed positional infor-
mation. This ability seems to be particularly important in the early growth of
the brain [31]. The model supports diﬀusion from predeﬁned external sources
such as from a boundary edge or source positioned outside of the boundary
to support such positional information.
11.3.8 Intercellular Forces
The choice of real valued positioning of cells solves the problem of cell over-
writing by removing the constraints of a grid layout, but this also creates the
potential for cells to overlap. In real organisms cells cannot overlap, but they
can exert forces upon each other.
By modelling cells as particles, which exert forces based upon a spring
model, overlap can be reduced since overlapping cells result in spring com-
pression that exerts an opposing force to increase the distance between the
cells. The trajectory of these particles due to these forces can be modelled
using Verlet integration [32]:
x(t+Δt) = 2x(t) −x(t−Δt) + a(t)Δt2
(11.5)
where x(t) is the position at time t, Δt is the time step and a the accelera-
tion vector resulting from the application of forces upon the cell. The great
advantage of the Verlet integration method is its improved stability and eﬃ-
ciency when compared with other methods for modelling the trajectories of
particles [19].
One situation, which can easily occur, is the constant division of cells with
division spindles in the same direction, resulting in a one-dimensional chain

252
Bowers
of cells. Since the forces applied by the Verlet algorithm would all be acting in
the same direction there is no possibility for cells to move out of the direction
of the chain. To overcome this problem, a small amount of random noise is
applied to the direction of division which enables the Verlet algorithm to apply
forces in directions slightly out of line with the other cells in order to push
the cell out of this chain. Generating random numbers with a random seed
derived from the cell state enables the deterministic nature of the process to
be retained.
11.3.9 Genes and the Genetic Regulatory Network
Sequencing the genes from an organism alone will tell you very little about
how that organism functions and how it came to be. In order to achieve a
more functional understanding, the nature of the interaction between genes
is required [26]. Genetic Regulatory Networks (GRNs) are a fundamental fea-
ture of natural developmental processes, and we are just beginning to unravel
how these networks of genes interact with evolution. In essence, a genetic reg-
ulatory network is a complex protein factory. It produces proteins that can
interact with and alter the surrounding environment, but it is also dependent
upon proteins for its function.
Enabling the representation of GRNs requires that genes can support reg-
ulatory dependencies upon each other. In this work, genes are modelled using
the concept of an Operon which describes how genes can interact and control
the expression of functional groupings of genes. Since the only function of a
gene, if expressed, is to specify some given protein, and the transcription and
thus expression of these genes is dependent upon proteins, then proteins are
how the genetic regulatory network must interact within itself and the external
environment. In this model two methods of gene regulation are supported.
The ﬁrst is through the chain of expression created by the genes. This
chain allows a string of genes to regulate each other in sequence until a gene
that ignores the expression from the previous gene breaks the sequence. This
form of regulation is dependent upon the gene’s position in the genotype.
The second is through protein regulation, whereby one gene controls the
characteristics of a protein that aﬀects the performance of another gene. With
this form of regulation there is less dependence upon position of the genes
within the genotype.
Each gene consists of the following elements:
•
An expression strength which is obtained from the previous gene in the
genome and unless altered by the gene is passed on to the next gene in
the genome.
•
A protein dependency which can be used by the gene for evaluative or
functional purposes.
•
An output which may either be in the form of some functional operation
on the cell or to control the expression of the next gene in the genome by
altering its expression strength.

11 Modularity in a Computational Model of Embryogeny
253
Table 11.1. Genes and their functions
Gene ID Dependent
Function
Express
Terminal
0
-
-
e = 0.0
1
-
-
e = 1.0
Expressive
2
-
-
ρ(p)
3
-
-
1.0 −ρ(p)
4
-
-
r(p)
5
-
-
1.0 −r(p)
Evaluative
6
if e > ρ(p)
-
e = 1.0
7
if e < ρ(p)
-
e = 1.0
8
if e > ρ(p)
-
e = 0.0
9
if e < ρ(p)
-
e = 0.0
Functional
10
-
r(p) = e
-
11
-
r(p) = 1.0 −e
-
12
if e = 1.0
spindle towards protein
-
13
if e = 1.0 spindle opposite protein
-
14
if e = 1.0
divide
-
15
if e = 1.0
die
-
From this basic set of features, several classes of gene type can be deﬁned.
•
Terminal genes have no dependents or function. They simply override the
expression chain to ensure that the next gene is either expressed or not.
•
Expressive genes determine the expression of the next gene based on the
concentration of the protein dependency.
•
Evaluative genes are similar to expressive genes except that they are not
only dependent upon the protein dependency concentration but also on
the expression strength through some form of evaluation.
•
Functional genes are dependent upon expression strength. If they are ex-
pressed then they perform some function utilising a protein dependency.
These functions can directly alter the cell state or cause the cell to divide
or die.
Table 11.1 lists 16 gene types used in the model, each having a unique
identiﬁer or gene ID. For each gene, the manner of the dependency on the
dependency protein and any eﬀects on the cell state, must be deﬁned. In
addition, any ability to control the current strength of the expression chain,
e, must also be deﬁned.
The protein dependency, p, can be used to determine the diﬀused concen-
tration of the dependency protein, ρ(p) or the release rate of the dependency
protein from with the cell, r(p).

254
Bowers
Fig. 11.1. The French ﬂag model of spatial diﬀerentiation
A genome consists of a ﬁnite list of genes, 20 in this case. Each gene is
represented by two integer values, the ﬁrst value represents the gene ID and
the second identiﬁes the genes protein dependency, p. Each gene in the genome
can be of any one of the 16 gene types with any combination of protein.
11.4 The Pattern Formation Problem
One key advance in understanding the processes of diﬀerentiation and mor-
phogenesis was to highlight its relationship with position. Wolpert [35] noted
that cells can use positional information in order to determine their cell type.
He used the analogy of a French ﬂag to explain how the diﬀusion of a chemical
from one side of the ﬂag could determine in what section of the French ﬂag a
cell resided using a threshold as outlined in Fig. 11.1.
The use of Wolpert’s French ﬂag analogy has become widespread when
discussing the self-organisation of cells into some form of structure. For this
reason many previous attempts to model cell growth within a computational
system have often used the French ﬂag as a target structure for self-organising
pattern formation for both reasons of continuity and appropriateness [8,9,24].
The simulated evolution of a complex mapping such as that introduced
by a computational model of embryogeny faces a number of issues. Firstly,
neighbourhood structure is not preserved which may result in an extremely
noisy and discontinuous search space. Secondly, such a complex mapping pro-
cess has the potential for much redundancy which means that neutrality is
prevalent. It is important to ensure that the population is still able to move
through these neutral areas when a selection pressure is non-existent.
Bearing in mind these issues, the following evolutionary algorithm is de-
ﬁned: selection is performed by choosing the best 50% of the population. Each
of these selected individuals is subjected to a random two point crossover op-
eration with another selected individual. This is performed by replacing all
the values between the two randomly selected points from the ﬁrst selected
parent with the values between those same two points from the second. The
product of this crossover is then subjected to a further random single point

11 Modularity in a Computational Model of Embryogeny
255
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0
500
1000
1500
2000
2500
3000
3500
4000
Evolutionary Generations
Optimal Fitness
Average Population Fitness
Population Diversity
Fig. 11.2. Averaged evolutionary behaviour for the French ﬂag pattern problem
mutation. Mutation is conducted by replacing a value, chosen at random, with
a randomly generated value.
The resulting oﬀspring then replace the worst 50% of the population. This
ensures a consistent generational selection pressure during adaptive evolution
since the population is eﬀectively ﬁtness ranked. The selection method also
supports a suﬃcient level of random drift during neutral evolution by ensuring
that at least 50% of the population is replaced with a set of newly generated
oﬀspring.
Figure 11.2 shows a typical individual, and the template against which
it was evaluated. The averaged behaviour of this evolutionary approach is
also shown in the same ﬁgure with the average population ﬁtness following
just below the current best ﬁtness suggesting that the population is fairly
converged in terms of ﬁtness. Fitness is measured as the percentage of cells
which are identiﬁed as being in the correct cell state given their position. The
genetic diversity is also plotted and is measured between two genotypes as
the percentage of genes which diﬀer. This is averaged over every every possi-
ble combination of individuals in the population. Genetic diversity, although
noisy, is generally maintained during evolution.
11.5 Modularity in a Computational Model of
Embryogeny
11.5.1 Deﬁnitions of Modularity
The concept of “modularity” or “module” is often used in a very ﬂuid way.
In software engineering, modularity is used to refer to software components

256
Bowers
which are developed or used in a largely independent manner whilst interact-
ing to produce some overall function such as object orientated design [27] and
functional programming [17]. Cognitive scientists often refer to the modular-
ity of mind [14], arguing that the mind is composed of a set of independent,
domain speciﬁc, processing modules, whilst evolutionary psychologists believe
this modularity occurred due to selection pressures at transitional points in
the evolution of the mind. The hardware production industry uses modularity
to describe how sets of product characteristics are bundled together. For ex-
ample, the automotive industry is considered to be becoming more modular
by enabling common automotive components to be easily transferred between
various models of vehicles in a bid to reduce production and maintenance
costs [28].
Such ubiquitous and often vague usage of the term often makes its use
very easy to misinterpret and diﬃcult to defend. For this reason it is often
advantageous to refer to the more fundamental deﬁnition of a module as a
set of sub-components of a system which are greatly inter-dependent whilst
remaining largely independent of the rest of the system.
11.5.2 Modularity in Evolution and Development
Modularity is an important aspect of natural evolution. It is clear from the
physiology of animals that they are inherently modular. This is perhaps most
obviously apparent in the symmetry of appendages such as legs and arms and
in the organs of the body which have very speciﬁc but necessary functions,
largely independent of each other, such as the heart, lungs and liver. It is a
widely held belief amongst evolutionary biologists that this apparent modular
architecture of animals is generated at every hierarchy of the developmental
process right down to the interactions between genes or even molecules [29].
Modularity in evolution occurs in many forms and one of the greatest chal-
lenges is to clearly identify prospective modularity from within such incred-
ibly complex systems. This is becoming increasingly important as we gather
a greater understanding of biological systems. Much of the discussion raised
by evolutionary biologists on this topic is highly relevant to the work pre-
sented here since it consists of observing modularity in various structures and
determining how these modules interact and inﬂuence each other [3, 4, 29].
Also, in the same fashion, insights into aspects of modularity in cognitive and
neurological systems can be as equally useful [12,18,21].
The importance of modularity can be no better highlighted than by its
numerous and extensive practical beneﬁts. Modularity can reduce disruptive
interference, provide a mechanism for robustness and enable complex systems
to be built from simple subsystems. These are just a few of the arguments for
why a modular approach is useful.

11 Modularity in a Computational Model of Embryogeny
257
11.5.3 Modularity in the Genotype-Phenotype Mapping
Measuring modularity in the genotype and phenotype independently makes
little sense in a system where the two entities are so highly inter-dependent.
Here, the occurrence of modularity within the mapping process between geno-
type and phenotype is investigated.
The term “pleiotropy” is used to refer to the situation when a single gene
inﬂuences multiple phenotypic traits. This has important ramiﬁcations for the
mapping between genotype and phenotype. Altenberg [4] considers modular-
ity to exist when the genotype–phenotype map can be decomposed into the
product of independent genotype–phenotype maps of smaller dimension. He
adds:
The extreme example of modularity would be the idealized model of a
genome in which each locus maps to one phenotypic trait. For the con-
verse, the extreme example of non-modularity would be a genotype–
phenotype map with uniform pleiotropy in which every gene has an
eﬀect on every phenotypic variable. Real organisms, one could ar-
gue, have genotype–phenotype maps that range somewhere in between
these extremes.
If such an association were to exist, then identifying deﬁnitive relationships
between elements of the genotype with certain elements of the phenotype
would allow the identiﬁcation of such modules and indicate that modularity
exists.
Identifying modules is not always easy. For the French ﬂag pattern there
are three obvious choices, the three diﬀerently coloured stripes, and previous
work has shown such modularity to be evident [10]. However, this form of
modularity measure is limited since it would discount other potential sources
of modularity, within a solution, which are not intuitively obvious. A better
way to identify the existence of modularity in the embryogeny mapping would
be to measure the alignment of structure in the genotype with structure in
the phenotype without needing to deﬁne what those structures might be. This
requires representations of structure from the genotype and phenotype which
are comparable. There is already a clearly deﬁned topology for the genotype
in the form of the genetic regulatory network. A similar network of gene–gene
relationships can be derived from the phenotype.
For each cell within a phenotype, the dependency upon the internal cell
state for each gene in the genome can be measured. To do this, each gene
in turn is ignored when mapping the genotype to the phenotype. Diﬀerences
for each cell are measured against the original phenotype. All cells that have
an altered state due to ignoring a gene have a shared dependency upon that
gene. Figure 11.3 shows such dependencies for each of the 20 genes in a sample
genome where cells dependent upon a gene are marked with a cross. In this
example case, a large number of the genes have absolutely no impact on the
phenotype when they are removed, evidenced by the lack of crosses in the

258
Bowers
Genes -1
Genes -2
Genes -3
Genes –4,6,7,8,9,12,14,15,16,19,20
Genes -5
Genes –10,11
Genes -13
Genes –17,18
Fig. 11.3. Phenotypic dependencies for each gene in a genome
phenotype. However, a number of the genes are clearly very speciﬁc in the
cells that they aﬀect.
The relationship between genes can be determined through their interac-
tions with the phenotype. This can be achieved by measuring the similarity
between the phenotypic dependencies of any given pair of genes. If this is done
for every combination of genes in the genotype, then a network based upon
these gene–gene relationships can be formed where nodes represent the genes
and edges represent the similarity of the respective gene’s impact upon the
phenotype. The similarity between the phenotypic dependencies of two genes
is measured as the percentage of the total cells which are aﬀected by both
genes.
The alignment between genotypic and phenotypic structures can be deter-
mined by comparing the genetic network derived from phenotypic interactions
to the network deﬁned by the genetic structure itself, the GRN. For each gene
there is a corresponding node in each of the two comparative networks, and
each of these nodes has a set of edges. Since these edges are weighted then
the similarity between a given node in the two comparative networks can be
measured by using the Euclidean distance between the set of edges belonging
to those nodes. A smaller Euclidean distance represents a greater similarity.
This is then averaged over each node to form a measure of similarity for the
entire network.
Figure 11.4 shows the result of measuring the similarity between the GRN
and the genetic network derived from the phenotype. This is measured over
evolutionary time for the best individual in the population. The results are
averaged over 10 evolutionary runs and shown with the standard deviation.
There is a clear progressive alignment between these two networks during
evolution.

11 Modularity in a Computational Model of Embryogeny
259
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
100
200
300
400
500
600
700
800
900
1000
Evolutionary Generations
Alignment
Alignment
Standard Deviation
Fig. 11.4. Alignment of structures derived from genotype and phenotype
Throughout this work, results have been obtained using a speciﬁed evo-
lutionary algorithm. However, if this trait of modularity is truly dependent
upon the interaction between simulated evolution and a computational model
of embryogeny then there should be a discernible diﬀerence in the measure of
module alignment between the individuals obtained through simulated evo-
lution and other search techniques. Here, the module alignment is compared
for simulated evolution, a hill climbing local search and random search.
For random search, each iteration of the search algorithm consists of the
creation, at random, of a new population of individuals. The best individual
found so far is retained.
A local search is conducted using a hill-climbing algorithm. From the cur-
rent point of search, a subset of the neighbourhood is generated using single
point mutations from the current position. If any neighbouring individual in
the generated sample has an improved or equal ﬁtness to the current point of
search it becomes the new point of search.
To ensure the comparisons between the three search algorithms are as fair
as possible, the number of evaluations required at each generation is consis-
tent across each search algorithm. However, simply comparing this measure
of alignment over the number of iterations of the search process is not mean-
ingful since, especially in the case of random search, the performance of these
algorithms can vary greatly. A means of comparison is required that is less
dependent upon the performance of the search process over time. The time
eﬃciency of the search algorithms can essentially be ignored by plotting the
measure of alignment directly against the ﬁtness of individuals found by the
various search processes. Figure 11.5 shows the alignment, between genotypic

260
Bowers
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
50%
55%
60%
65%
70%
75%
80%
85%
90%
95%
100%
Fitness
Alignment
Simulated Evolution
Random Search
Hill Climber
Fig. 11.5. Alignment of genotypic and phenotypic structures against their relative
ﬁtness
and phenotypic structures, plotted against the ﬁtness of the individual for all
three forms of search.
Although the results for random search do not reach the same levels of
ﬁtness as those of local and evolutionary search, the results suggest that all
the search processes investigated here result in an increasing alignment be-
tween genotypic and phenotypic structures with increasing ﬁtness and that
they follow a very similar trend. Therefore, each of the search algorithms is
capable of producing individuals with modular alignment and that this level
of alignment is dependent upon the ﬁtness of an individual. Since a random
search is sampling uniformly across the entire search space, thus is not depen-
dent upon a search trajectory through the search space, it can be established
that the modular behaviour must be independent of the search process. This
would suggest that an inherent property of the embryogeny mapping process
is that ﬁtter individuals will have a greater inclination to modular structures.
11.6 Conclusion
Results of this work suggest that the observed modular behaviour is an inher-
ent property of the computational model of embryogeny and, as such, is likely
to be obtained from any form of search algorithm. This does not necessarily

11 Modularity in a Computational Model of Embryogeny
261
discredit an evolutionary search as a good choice of search process. In com-
parison to local and random search, simulated evolution is certainly the more
eﬃcient optimiser. This could be simply due to the noisy nature of the search
landscape, and the presence of neutrality, to which an evolutionary approach
is the better suited of the three search algorithms.
However, there are some interesting observations that can be made about
the nature of the alignment between genotypic and phenotypic structures and
what this means for a search process. Altenberg’s hypothesis of Constructional
Selection [2,3] states:
Selection during the origin of genes provides a ﬁlter on the construc-
tion of the genotype-phenotype map that naturally produces evolv-
ability.
Altenberg suggests that a search process that incrementally changes the
degrees of freedom of a genetic representation will do so to more closely match
the degrees of freedom of the phenotypic domain. This is because such indi-
viduals are more evolvable, since a change in a variable in such a genotype is
more likely to result in some measurable response, in the associated variables,
in the phenotype.
Another way of viewing this argument is to say that an evolutionary search
process will bias towards individuals that, once mapped from genotypic space
to phenotypic space, will form a relatively smooth search landscape that is
easier to traverse. The increasing alignment between genotypic and pheno-
typic structure shown in the results of Fig. 11.5 are symptomatic of such an
alignment of the degrees of freedom.
Therefore, one could argue that an algorithm which utilises iterative adap-
tations to existing solutions would be more suited to navigating the search
space embodied by the model of computational embryogeny introduced in this
work.
References
1. Alberts, B., Bray, D., Lewis, J., Ra., M., Roberts, K., Watson, J.: Molecular
Biology of the Cell, 3rd edn. Garland Publishing (1994)
2. Altenberg, L.: The evolution of evolvability in genetic programming. In: J. Kin-
near (ed.) Advances in Genetic Programming, pp. 47–74. MIT Press (1994)
3. Altenberg, L.: Genome growth and the evolution of the genotype-phenotype
map. In: W. Banzhaf, F. Eeckman (eds.) Evolution and Biocomputation: Com-
putational Models of Evolution, pp. 205–259. Springer (1995)
4. Altenberg, L.: Modularity in evolution: Some low-level questions. In: W. Calle-
baut, D. Rasskin-Gutman (eds.) Modularity: Understanding the Development
and Evolution of Complex Natural Systems, pp. 99–131. MIT Press (2004)
5. Banzhaf, W., Miller, J.: The challenge of complexity. In: A. Menon (ed.) Fron-
tiers of Evolutionary Computation, pp. 243–260. Kluwer Academic (2004)

262
Bowers
6. Bentley, P., Gordon, T., Kim, J., Kumar, S.: New trends in evolutionary com-
putation. In: Proceedings of the 2001 Congress on Evolutionary Computation,
pp. 162–169. IEEE Press (2001)
7. Bentley, P., Kumar, S.: Three ways to grow designs: a comparison of embryoge-
nies for an evolutionary design problem. In: Proceedings of the 1999 Genetic and
Evolutionary Computation Conference, pp. 35–43. Morgan Kaufmann (1999)
8. Beurier, G., Michel, F., Ferber, J.: Towards an evolution model of multiagent or-
ganisms. In: Multi-Agents for Modeling Complex Systems (MA4CS’05): Satellite
Workshop of the European Conference on Complex Systems (ECCS’05) (2005)
9. Bowers, C.: Evolving robust solutions with a computational embryogeny. In:
J. Rossiter, T. Martin (eds.) Proceedings of the UK Workshop on Computational
Intelligence (UKCI-03), pp. 181–188 (2003)
10. Bowers, C.: Formation of modules in a computational model of embryogeny. In:
Proceeding of 2005 Congress on Evolutionary Computation, pp. 537–542. IEEE
Press (2005)
11. Brockes, J., Kumar, A.: Plasticity and reprogramming of diﬀerentiated cells in
amphibian regeneration.
Nature Reviews Molecular Cell Biology 3, 566–574
(2002)
12. Bullinaria, J.: To modularize or not to modularize. In: J. Bullinaria (ed.) Pro-
ceedings of the 2002 UK Workshop on Computational Intelligence (UKCI-02),
pp. 3–10 (2002)
13. Ebner, M., Shackleton, M., Shipman, R.: How neutral networks inﬂuence evolv-
ability. Complexity 7(2), 19–33 (2001)
14. Fodor, J.: Modularity of Mind. MIT Press (1983)
15. Gordon, T., Bentley, P.: Development brings scalability to hardware evolution.
In: Proceedings of the 2005 NASA/DoD Conference on Evolvable Hardware, pp.
272–279. IEEE Computer Society (2005)
16. Hogeweg, P.: Shapes in the shadows: Evolutionary dynamics of morphogenesis.
Artiﬁcial Life 6, 85–101 (2000)
17. Hughes, J.: Why functional programming matters.
The Computer Journal
32(2), 98–107 (1989)
18. Jacobs, R.: Computational studies of the development of functionally specialized
neural modules. Trends in Cognitive Sciences 3(1), 31–38 (1999)
19. Jakobsen, T.: Advanced character physics. In: Proceedings of the 2001 Game
Developer’s Conference (2001)
20. Kumar, S.: Investigating computational models of development for the con-
struction of shape and form. Ph.D. thesis, Department of Computer Science,
University College London, UK (2004)
21. Lam, C., Shin, F.: Formation and dynamics of modules in a dual-tasking multi-
layer feed-forward neural network. Physical Review E 58(3), 3673–3677 (1998)
22. Miglino, O., Nolﬁ, S., Parisi, D.: Discontinuity in evolution: How diﬀerent levels
of organization imply pre-adaption.
In: R. Belew, M. Mitchell (eds.) Adap-
tive Individuals in Evolving Populations: Models and Algorithms, pp. 399–415.
Addison-Wesley (1996)
23. Miller, J.: Evolving developmental programs for adaptation, morphogenesis, and
self-repair. Lecture Notes in Artiﬁcial Intelligence 2801, 256–265 (2003)
24. Miller, J.: Evolving a self-repairing, self-regulating, French ﬂag organism. In:
Proceedings of the Genetic and Evolutionary Computation Conference (GECCO
2004), pp. 129–139. Springer (2004)

11 Modularity in a Computational Model of Embryogeny
263
25. Miller, J., Thomson, P.: Beyond the complexity ceiling: evolution, emergence and
regeneration. In: Proceedings of the Workshop on Regeneration and Learning in
Developmental Systems – Genetic and Evolutionary Computation Conference
(GECCO 2004) (2004)
26. Monk, N.: Unravelling nature’s networks. Biochemical Society Transactions 31,
1457–1461 (2003)
27. Pressman, R.: Software Engineering: A Practitioner’s Approach, 5th edn.
McGraw-Hill (2000)
28. Ro, Y., Liker, J., Fixson, S.: Modularity as a strategy for supply chain coordi-
nation: The case of U.S. auto. IEEE Transactions on Engineering Management
54(1), 172–189 (2007)
29. Schlosser, G., Wagner, G.: Modularity in Development and Evolution. Univer-
sity of Chicago Press (2004)
30. Stanley, K., Miikkulainen, R.: A taxonomy for artiﬁcial embryogeny. Artiﬁcial
Life (2003)
31. Trisler, D.: Cell recognition and pattern formation in the developing nervous
system. Journal of Experimental Biology 153(1) (1990)
32. Verlet, L.: Computer experiments on classical ﬂuids I – thermodynamical prop-
erties of Lennard-Jones molecules. Physical Review 159(1) (1967)
33. Wagner, G., Altenberg, L.: Complex adaptations and the evolution of evolvabil-
ity. Evolution 50(3), 967–976 (1996)
34. Wilke, C.: Adaptive evolution on neutral networks. Bulletin of Mathematical
Biology 63(4), 715–730 (2001)
35. Wolpert, L.: Positional information and the spatial pattern of cellular diﬀeren-
tiation. Theoretical Biology 25(1), 1–47 (1969)

Part IV
Engineering

Evolutionary Design in Engineering
Kalyanmoy Deb
Department of Mechanical Engineering, Indian Institute of Technology Kanpur,
PIN 208016, India deb@iitk.ac.in
Engineering design is an age-old yet important topic, taught in most engi-
neering schools around the world and practiced in all engineering disciplines.
In most scenarios of engineering design, depending on whether there is a need
for performing the particular design task, a number of well laid-out steps are
followed: (i) conceptual design in which one or more concepts are formulated
and evaluated, (ii) embodiment design in which more concrete shapes and
material of components are decided, and (iii) detail design in which all di-
mensions, parameters, tolerancing, etc. of the components are made usually
with an optimization framework and with a view to manufacturing aspects
of the design. These steps are usually repeated a few times before ﬁnalizing
the design and making a blueprint for manufacturing the product. Although
optimization is an integral part of the engineering design process at every
step mentioned above, often such a task is overshadowed by the experience of
designers and the race for achieving a record turn-over design time.
Evolutionary design, as we describe in this book, is a task which can be
used in any of the above design tasks to come up with optimal or innova-
tive design solutions by using evolutionary computing principles. Evolution-
ary computing algorithms provide a ﬂexible platform for coming up with new
and innovative solutions. In the conceptual design stage, driven by interac-
tive or a fuzzy evaluation scheme, diﬀerent concepts can be evolved. In the
embodiment and detail design stages, a ﬁner optimization problem can be
formulated and solved using an evolutionary optimization (EO) procedure.
There are, however, a number of reasons for the lukewarm interest in using
optimization procedures routinely in an engineering design task:
•
The objective functions and constraints associated with the optimization
problem in engineering design tasks are usually non-linear, non-convex,
discrete, and non-diﬀerentiable, thereby making it diﬃcult to use gradient
methods and to use optimality theorems which are developed mainly for
well-behaved problems.

268
Deb
•
There can be more than one optimal solution where an algorithm can get
stuck, thereby failing to ﬁnd a global optimal solution. In multi-modal
problems, more than one global optimal solution exists, thereby requiring
a global-optimization method to be applied many times, so as to get an
idea of diﬀerent global optimal solutions.
•
Objective functions can be many and conﬂicting with each other, resulting
in multiple trade-oﬀoptimal solutions. Again, a repeated application of a
classical method is needed in such multi-objective optimization problems.
•
Objective functions and constraints can be noisy and involve manufactur-
ing or modeling errors, thereby causing the classical methods to get caught
in the details and not ﬁnd desired optimal solutions.
•
Objective functions and constraints can be computationally daunting to
evaluate, thereby causing an ineﬃcient search due to the serial algorithmic
nature of classical methods.
•
The decision variables are usually mixed: some of them can be real-valued,
some others can be discrete (integer, for example), yet some others can
take one or a handful of options (such as the choice of a material), and
some others can be a permutation or a code representing some chain of
events et cetera. The solution can be a graph or a tree representing the
architecture of a system built in a hierarchical manner from sub-systems
et cetera. Classical methods need to have ﬁx-ups to handle such mixed-
variable problems.
•
The number of decision variables, objectives and constraints may be large,
causing “curse of dimensionality” problems.
•
Some parameters and decision variables may not be considered as a de-
terministic quantity. Due to manufacturing uncertainties and the use of
ﬁnite-precision manufacturing processes, these parameters must ideally be
considered as stochastic parameters with a known distribution.
•
Knowing the uncertainties involved in parameters, decision variables and
the objective/constraint evaluation, an upper limit on the proportion of
unacceptable products may be imposed, leading to reliability-based design
optimization.
Classical optimization methods were developed mainly by following theoretical
optimality conditions and are often too idealized to be applied to such vagaries
of engineering design optimization tasks.
However, for the past two decades or so, evolutionary optimization (EO)
procedures are becoming increasingly popular in engineering design tasks due
to their ﬂexibility, self-adaptive properties, and parallel search abilities. But
all these advantages do not come free; the user of an EO must use appropri-
ate and customized EO operators to the problem at hand, which requires a
good understanding of how EO algorithms work, a thorough understanding
of optimization basics, and good knowledge about the problem being solved.
In this section on Engineering Design, we have three chapters which bring out
diﬀerent aspects of EO applications pertinent to engineering design.

Evolutionary Design in Engineering
269
Hu et al.’s chapter discusses a genetic programming based EO approach
with a bond graph based strategy to evolve new designs in three diﬀerent
engineering applications. In some cases, human-competitive designs are found
(designs better than what a well-skilled human would develop). Since the sug-
gested methodology can, in practice, evolve diﬀerent topologies, the method-
ology is appropriate for a conceptual design task.
Deb’s chapter discusses a number of matters related to engineering opti-
mization: (i) single versus multiple objective optimization, (ii) static versus
dynamic optimization, and (iii) deterministic versus stochastic optimization
through uncertainties in decision variables. On a hydro-thermal power dis-
patch problem involving time-varying quantities, uncertainties in power gen-
eration units, non-diﬀerentiable objectives, and non-linear objectives and con-
straints, the suggested EO based strategies ﬁnd theoretically correct Pareto-
optimal solutions and suggest ways to choose a single optimal solution for
implementation in a dynamically changed optimization problem.
Finally, Ling et al.’s chapter suggests an EO based multi-modal optimiza-
tion procedure which is capable of ﬁnding multiple optimal solutions in a
single simulation run. On a holographic grating optimization problem, the
proposed EO has found four diﬀerent optical conﬁgurations. The authors also
discuss a way to choose a particular solution from the four obtained solutions
by keeping in mind the practicalities of implementing the solutions.
The examples portrayed in this book on engineering design are just three
application studies taken from a plethora of diﬀerent engineering problems
which are being routinely solved using EO methodologies. The number of
such applications and their areas are so vast that it would be a foolish act
to even make a list of them. It is our humble understanding that the case
studies discussed here will spur the reader on to become interested in the
EO literature (conference proceedings, journals, books, etc.). However, before
closing these introductory remarks, the author would like to present his ﬁve-
step view on a systematic application of an EO methodology to a real-world
engineering design optimization problem:
1. Prepare the Case: Before applying an EO methodology, the problem
must be attempted using one or more standard classical methodologies. It
may be found that the classical method cannot solve the problem at hand,
or that it takes enormous computational time, or that it ﬁnds a solution
which is not acceptable, or that it does not even ﬁnd any solution, or that
the solution is sensitive to the supplied initial solution, or others. Such a
study will build up a case in favor of using an EO methodology for solving
the problem at hand.
2. Develop an EO Methodology: After studying the problem thoroughly,
an EO methodology appropriate to the problem can be developed. This
is where some experience with the EO literature and the EO simulation
studies are needed. The choice of an appropriate EO can be motivated
by the nature of decision variables, objective functions and constraints

270
Deb
the problem has. For example, if all variables are real-valued, it is better
to choose a real-parameter EO, such as an evolution strategy, diﬀerential
evolution or a real-parameter genetic algorithm with real-valued recombi-
nation and mutation operators. Some other important issues in developing
a methodology are as follows:
Handling Constraints: In the presence of constraints, an important
step is to choose an appropriate constraint-handling strategy. It would
not be desirable to use penalty-based methodologies in the ﬁrst in-
stance. If constraints can be handled by using a repair mechanism, this
would be an ideal choice. For example, a linear or quadratic equality
constraint can be handled by simply ﬁnding the root(s) of the equation
and replacing the variable values with the root(s).
Customization: While designing the EO algorithm, as much customiza-
tion as possible must be utilized in creating the initial population and
developing EO operators. In most problems, it is possible to develop
seed solutions in terms of building blocks or partial good sub-solutions.
Sometimes, the creation of an initial population satisfying one or more
constraints is easier to achieve. In many instances, the knowledge of
one or more existing solutions is already available and these solutions
can be used as seed solutions to create an initial EO population by
perturbing (or mutating) parts of these seed solutions. Recombination
and mutation operators can also be customized to suit the creation of
feasible or good solutions from feasible parent solutions. One generic
strategy would be to consider recombination as a primary creation
operator of new solutions by combining better aspects of a number of
parent population members and the mutation to repair these solutions
to make them as feasible as possible.
Hybridization: It is always a good idea to use a hybrid strategy involv-
ing EO and a local search method. Although there are several ways to
couple the two approaches, a common approach is to start the search
with an EO and then ﬁnish with a local search method.
3. Perform a Parametric Study: It is also imperative to perform a para-
metric study of EO parameters by clearly demonstrating the range of
parameter values where the resulting algorithm would work well. The use
of design of experiments concepts or other statistical methodologies would
be useful here. To reduce the eﬀorts of a parametric study, as few param-
eters as possible must be used. Some of the unavoidable parameters in an
EO procedure are population size, crossover probability, mutation prob-
ability, tournament size, and niching parameters. Any other parameters,
if they need to be included, should be as few as possible and may well be
ﬁxed to some previously suggested values.
In no case should the outcome of the developed EO procedure depend
on the random number generator or its seed value. If this happens even
after the parametric study, a major change in the representation scheme
and/or operators is called for.

Evolutionary Design in Engineering
271
4. Apply to Obtain Optimal Solutions: After a suitable EO (hybrid
or autonomous) is developed through parametric studies, suitable GUI-
based software can be developed to ﬁnd optimal solution(s). If the code
is to be supplied to a user, the recommended EO parameter values must
be provided.
5. Verify and Perform Post-optimality Analysis: After a set of solu-
tions has been found, the onus still remains on the part of the devel-
oper/user to analyze the solutions to explain why the obtained solutions
are good solutions for the problem. This step is necessary to establish
that the obtained solutions from the developed EO methodology are not
a ﬂuke. Thus, this may require multiple applications of the developed
EO procedure using diﬀerent (biased or unbiased, whichever is used in
the procedure) initial populations. This may require a veriﬁcation of ob-
tained solutions by means of other optimization algorithms or through
the solution of a variant of the original optimization problem leading to
a sensitivity analysis. In case of any discrepancies between the results of
these diﬀerent studies, further investigations must be made by repeating
the above steps till a satisfactory explanation of the results are obtained.
This is a step often by-passed in many optimization studies. Often, an
analysis of obtained solutions may bring out important insights about
properties which make a solution optimal. Such information may also
provide important clues about creating a better initial population, recom-
bination, and mutation operators so as to launch another, yet a more
eﬃcient, EO application.
The suggestions made above not only allow a systematic development and
application of an EO procedure, but also generate important problem knowl-
edge which is valuable in understanding the problem and the trade-oﬀs among
various problem parameters which constitute the region of optimality. None
of these tasks is easy, and by no means universal to all problems. Every design
optimization problem in practice is diﬀerent in terms of diﬀerent complexities
associated with variables, objectives, and constraints. Systematic applications
of the above procedures may one day unveil salient knowledge about ‘What
procedure is apt to which problems?’ – an ever-pertinent question asked most
often in the ﬁeld of optimization.
Kalyanmoy Deb
Engineering Area Leader

12
Engineering Optimization Using Evolutionary
Algorithms: A Case Study on Hydro-thermal
Power Scheduling
Kalyanmoy Deb∗
Kanpur Genetic Algorithms Laboratory (KanGAL), Indian Institute of Technology
Kanpur, PIN 208016, India deb@iitk.ac.in
12.1 Introduction
Many engineering design and developmental activities ﬁnally resort to an op-
timization task which must be solved to get an eﬃcient solution. These opti-
mization problems involve a variety of complexities:
•
Objectives and constraints can be non-linear, non-diﬀerentiable and dis-
crete.
•
Objectives and constraints can be non-stationary.
•
Objectives and constraints can be sensitive to parameter uncertainties near
the optimum.
•
The number of objectives and constraints can be large.
•
Objectives and constraints can be expensive to compute.
•
Decision or design variables can be of mixed type involving continuous,
discrete, Boolean, and permutations.
Although classical optimization algorithms have been around for more than
ﬁve decades, they face diﬃculties in handling most of the above problems
alone. When faced with a particular diﬃculty, an existing algorithm is mod-
iﬁed to suit it to apply to the problem. One of the main reasons for this
diﬃculty is that most of the classical methodologies use a point-by-point ap-
proach and are derivative-based. With one point to search a complex search
space, algorithms seem to be inﬂexible in improving from a prematurely-stuck
solution.
Evolutionary algorithms (EAs), suggested around the 1960s and applied
to engineering design optimization problems from around the 1980s, are pop-
ulation based procedures which are increasingly being found to be more suited
to diﬀerent vagaries of practical problems. In this chapter, we consider a
∗Currently occupying the Finnish Distinguished Professor position at Helsinki
School of Economics, Finland (Kalyanmoy.Deb@hse.fi).

274
Deb
case study of the hydro-thermal power dispatch problem and illustrate how
a population-based EA can handle diﬀerent complexities and produce use-
ful solutions for practice. Particularly, we address the ﬁrst three complexities
mentioned above in this chapter.
In the remainder of the chapter, we ﬁrst introduce the hydro-thermal power
scheduling problem and through this problem discuss various complexities
which can arise in solving practical optimization problems.
12.2 Hydro-thermal Power Scheduling Problem
In hydro-thermal power generation systems, both hydroelectric and thermal
generating units are utilized to meet the total power demand. A proper
scheduling of the power units is an important task in a power system de-
sign. The optimum power scheduling problem involves the allocation of power
to all concerned units, so that the total fuel cost of thermal generation is
minimized, while satisfying all constraints in the hydraulic and power system
networks [19]. To solve such a single-objective hydro-thermal scheduling prob-
lem, many diﬀerent conventional methods such as Newton’s method [21], the
Lagrange multiplier method [14], dynamic programming [20] and soft com-
puting methodologies such as genetic algorithms [12], evolutionary program-
ming [16], simulated annealing [18], etc. have been tried. However, the thermal
power generation process produces harmful emission which must also be min-
imized for environmental safety. Unfortunately, an optimal economic power
generation is not optimal for its emission properties and vice versa. Due to
the conﬂicting nature of minimizing power generation cost and emission char-
acteristics, a multi-objective treatment of the problem seems to be the most
suitable way to handle this problem [1]. Such an optimization task ﬁnds a
set of Pareto-optimal solutions with diﬀerent trade-oﬀconditions between the
two diﬀerent objectives in a single simulation run.
12.2.1 Optimization Problem Formulation
The original formulation of the problem was given in Basu [1]. The hydro-
thermal power generation system is optimized for a total scheduling period
of T. However, the system is assumed to remain ﬁxed for a period of tT so
that there are a total of M = T/tT changes in the problem during the total
scheduling period. In this oﬀ-line optimization problem, we assume that the
demand in all M time intervals is known a priori and an optimization needs
to be made to ﬁnd the overall schedule before starting the operation.
Let us also assume that the system consists of Ns number of thermal (Ps)
and Nh number of hydroelectric (Ph) generating units sharing the total power
demand. The fuel cost function of each thermal unit considering valve-point
eﬀects is expressed as the sum of a quadratic and a sinusoidal function and the

12 Engineering Optimization Using Evolutionary Algorithms
275
total fuel cost in terms of real power output for the whole scheduling period
can be expressed as follows:
f1(Ph, Ps) =
M

m=1
Ns

s=1
tm[as + bsPsm + csP 2
sm + |ds sin{es(P min
s
−Psm)}|].
(12.1)
Here, parameters as, bs, cs, ds, and es are related to the power generation units
and their values are given in the appendix. The parameter P min
s
is the lower
bound of the s-th thermal power generation unit (speciﬁed in the appendix).
This objective is non-diﬀerentiable and periodic to the decision variables. No-
tice that the fuel cost is involved only with thermal power generation units
and hydroelectric units do not contribute in the cost objective. Minimization
of this objective is of interest to the power companies. A conﬂicting objective
appears from environmental consideration of minimizing the emission of ni-
trogen oxides for the whole scheduling period T from all thermal generation
units:
f2(Ph, Ps) =
M

m=1
Ns

s=1
tm[αs + βsPsm + γsP 2
sm + ηs exp(δsPsm)].
(12.2)
Here again, ﬁxed values of all parameters except the decision variables are
given in the appendix. Like the cost objective, the emission objective does
not involve hydroelectric power generation units.
The optimization problem has several constraints involving power balance
for both thermal and hydroelectric units and water availability for hydroelec-
tric units. In the power balance constraint, the demand term is time depen-
dent, which makes the problem dynamic:
Ns

s=1
Psm +
Nh

h=1
Phm −PDm −PLm = 0,
m = 1, 2, . . . , M,
(12.3)
where the transmission loss PLm term at the m-th interval is given as follows:
PLm =
Nh+Ns

i=1
Nh+Ns

j=1
PimBijPjm.
(12.4)
This constraint involves both thermal and hydroelectric power generation
units. In this stationary optimization problem, it is assumed that the power
demand values PDm at each time period (m = 1, 2, . . . , M) is known a priori.
The water availability constraint can be written as follows:
M

m=1
tm(a0h + a1hPhm + a2hP 2
hm) −Wh = 0,
h = 1, 2, . . . , Nh,
(12.5)
where Wh is the water head of the h-th hydroelectric unit and is given in the
appendix for the problem chosen in this study.

276
Deb
Finally, the variable bounds are expressed as follows:
P min
s
≤Psm ≤P max
s
,
s = 1, 2, . . . , Ns, m = 1, 2, . . . , M,
(12.6)
P min
h
≤Phm ≤P max
h
,
h = 1, 2, . . . , Nh, m = 1, 2, . . . , M.
(12.7)
Thus, the two-objective problem involves (M(Ns +Nh)) variables, two ob-
jectives, (M +Nh) quadratic equality constraints and (2M(Ns+Nh)) variable
bounds. The speciﬁc stationary case considered here involves four (M = 4)
changes in demand over T = 48 hours having a time window of statis of
tT = 12 hours. We use this value to compare our results with another study [1]
which also used the same parameter values. The corresponding problem has
six (two hydroelectric (Nh = 2) and four thermal (Ns = 4)) power units. For
the above data, the optimization problem has 24 variables, two objectives,
six equality constraints, and 48 variable bounds. All the above parameter val-
ues for this case are given in the appendix. Four ﬁxed power demand values
of 900, 1100, 1000 and 1300 MW are considered for the four time periods,
respectively.
12.3 Solution Procedure for the Stationary Problem
Using an EA
A solution is simply represented as a vector of 24 real-parameter variables.
An EA procedure for solving an optimization problem helps to reduce the
complexity of the problem in a number of ways. We discuss these here.
12.3.1 Handling Multiple Objectives
Multi-objective optimization problems give rise to a set of trade-oﬀoptimal
solutions. Each such solution is a potential candidate for implementation, but
exhibits a trade-oﬀbetween two objectives. Classical methods require prefer-
ence information a priori and then optimize a preferred single objective version
of the problem [11]. If diﬀerent trade-oﬀsolutions are needed to investigate
the eﬀect of diﬀerent preference values before choosing a ﬁnal solution, such
a classical approach is required to be applied again and again. A recent study
has shown that such independent optimizations may be computationally ex-
pensive in complex problems [15].
However, EAs are ideal for such problem solving tasks. This is because
the EA population can be used to store diﬀerent trade-oﬀoptimal solutions
obtained in a single simulation run. A number of eﬃcient methodologies ex-
ist for this purpose [3], here we use a commonly-used procedure (NSGA-
II [5]), which uses a non-domination sorting of population members to em-
phasize non-dominated solutions systematically, an elite preserving procedure
for faster convergence, and a diversity-preserving mechanism for maintaining
a widely distributed set of solutions in the objective space. More about the
NSGA-II procedure can be found in the original study [5].

12 Engineering Optimization Using Evolutionary Algorithms
277
12.3.2 Handling Variable Bounds
The current optimization problem involves a large number (48) of variable
bounds, which must be treated as constraints using classical methods. How-
ever, in an EA, they all can be easily taken care of directly in the initialization
procedure and in the subsequent generation of new solutions. Initial solutions
are created randomly within these bounds and subsequent crossover and mu-
tation operators are modiﬁed to create solutions only within these limits [3,7],
thereby automatically satisfying all variable bounds.
12.3.3 Handling Quadratic Equality Constraints
Since all constraints in this problem are quadratic, we can use them to repair
an infeasible solution. First, we discuss the procedure for the water availabil-
ity constraints (12.5), as they involve hydroelectric power generation variables
alone. Each equality constraint (for a hydroelectric unit h) can be used to re-
place one of the M hydroelectric power values Phμ) by ﬁnding the roots of the
corresponding quadratic equation, thereby satisfying the equality constraints
exactly. For a ﬁxed h, the equality constraint can be written for μ-th time
period as follows:
P 2
hμ+a1h
a2h
Phμ+
1
tμa2h
⎛
⎜
⎝−Wh + a0hT +
M

m=1
m̸=μ
tma1hPhm +
M

m=1
m̸=μ
tma2hP 2
hm
⎞
⎟
⎠= 0.
(12.8)
Since a1h/a2h is always positive, only one root of this equation is positive
and it represents the desired hydroelectric power value. Other hydroelectric
power values at diﬀerent time steps are calculated by maintaining the original
ratio between them and Phμ. For a solution specifying (Nh + Ns) variables, a
systematic procedure is used for this purpose.
Step 1: Start with m = 1 (time step) and h = 1 (hydroelectric unit).
Step 2: Equation (12.5) is written as a quadratic root-ﬁnding problem for
variable Phm by ﬁxing other hydroelectric power values at diﬀerent time
steps as they are in the original solution.
Step 3: Find two roots and consider the positive root, if any, and replace
original Phm with the positive root. Update other Phk values (for k ̸= m as
discussed above. If all units satisfy their variable bounds given in (12.7),
we accept this solution. We increment h by one and move to the next
hydroelectric unit and go to Step 2. We continue till all Nh units are
considered and all constraints are satisﬁed. Then, we go to Step 4.
If none of the roots satisfy the variable bounds, we increment m by one
and go to Step 2 for trying with the h-th hydroelectric unit for the next
time. We continue till all M variables are tried to ﬁnd at least one case in
which the constraint and the corresponding variable bounds are satisﬁed.
Then, we go to Step 4.

278
Deb
Step 4: If any of the Nh equality constraints are not satisﬁed by the above
procedure, we compute the overall constraint violation (which is the ab-
solute value of the left side of the original equality constraint) and declare
the solution infeasible and do not proceed with any further constraint
evaluation. Else the solution is declared feasible with zero penalty.
If the above repair mechanism for all Nh water availability constraints is suc-
cessful, we proceed to repair the solution for thermal power units using power
balance equality constraints, else the solution is declared infeasible and no
further computation of power balance constraints nor the computation of ob-
jective functions are performed. But if the solution is repaired successfully, we
proceed to repair the thermal power generation variables by using the power
balance equality constraints. Equation (12.3) involves M quadratic equality
constraints, each for one time period. We follow a similar procedure as above
and repair a particular thermal unit Psm for each time slot. Thus, the above
procedures attempt to make a solution feasible. If successful, it computes the
objective function values, else it computes a penalty corresponding to the
amount of violation of the ﬁrst constraint it cannot repair successfully and
refrain from computing objective values.
The above constraint handling procedure is suitable to be used with the
NSGA-II framework, in which a penalty-parameter-less procedure [2] is em-
ployed to handle feasible and infeasible solutions. In a tournament selection
involving two solutions taken from the NSGA-II population, three scenarios
are possible. If one is feasible and other is not, we simply choose the feasible
one, If both solutions are feasible, the one dominating the other is chosen.
Finally, if both solutions are infeasible, the one with smaller constraint vio-
lation is chosen. Thus, it is interesting to note that if a solution is infeasible,
objective function computation is not necessary with the above procedure,
thereby requiring no penalty parameters. Since this constraint handling pro-
cedure involves more than one solution, such a penalty-parameter-less strategy
is possible to implement with a population-based optimization procedure. For
single-objective optimization, the second condition favors the solution having
better objective value [2].
12.4 Simulation Results on Stationary Hydro-thermal
Power Scheduling
NSGA-II is combined with the above-discussed constraint handling method
for solving the hydro-thermal scheduling problem. NSGA-II parameters used
in this study are as follows: Population size is 100, SBX crossover probability
is 0.9, polynomial mutation probability is 1/n (where n is the number of
variables), and distribution indices for crossover and mutation are 10 and 20,
respectively. More about these operators can be found in [3,4].
To investigate the optimality of the obtained NSGA-II frontier, each objec-
tive is also optimized independently by a single-objective EA (with identical

12 Engineering Optimization Using Evolutionary Algorithms
279
 22000
 26000
 30000
 34000
 38000
 65000
 70000
 75000
 80000
 85000
 90000
Cost
Emission
NSGA−II
Single−obj.
eps−constraint
Basu (2005)
Fig. 12.1. Pareto-optimal front obtained by NSGA-II, veriﬁed by single-objective
methods, and by a previous study
EA selection, crossover and mutation operators, constraint-handling strat-
egy, and parameter setting) and two individually best objective solutions are
plotted in Fig. 12.1. One of these individual-best solutions (the minimum-
cost (f1)) solution gets dominated by a NSGA-II solution and the minimum-
emission solution is matched by a NSGA-II solution. To validate the optimality
of some other NSGA-II solutions, we also employ the same single-objective
EA to solve several ϵ-constraint problems [11] by ﬁxing f1 value at diﬀerent
values:
Minimize f2(Ph, Ps),
Subject to f1(Ph, Ps) ≤ϵ,
(Ph, Ps) ∈S,
(12.9)
where S is the feasible search region satisfying all constraints and variable
bounds. The obtained solutions are shown in Fig. 12.1 and it is observed that
all these solutions more or less match with those obtained by NSGA-II. It is
unlikely that so many diﬀerent independent optimizations will result in one
trade-oﬀfrontier, unless it is close to the optimal frontier. These multiple op-
timization procedures give us conﬁdence about the optimality of the obtained
NSGA-II frontier. We believe that in the absence of an EA’s proof of con-
vergence for any arbitrary problem, such a procedure of ﬁnding solutions and
verifying them with various optimization techniques is a reliable approach for
practical optimization.
Basu [1] used a simulated annealing (SA) procedure to solve the same
problem. That study used a naive penalty function approach, in which if any
SA solution if found infeasible, it is simply penalized. For diﬀerent weight
vectors scalarizing both objectives, the study presented a set of optimized so-
lutions. A comparison of these results is made with our NSGA-II approach in
Fig. 12.1. It is observed that the front obtained by NSGA-II dominate that

280
Deb
 0
 50
 100
 150
 200
 250
 65000
 70000
 75000
 80000
 85000
 90000
0−12 hrs.
12−24 hrs.
24−36 hrs.
36−48 hrs.
Hydro Power 1
Cost
Fig. 12.2. Hydroelectric unit Ph1 versus
f1 for the original problem
 0
 100
 200
 300
 400
 500
 65000
 70000
 75000
 80000
 85000
 90000
0−12 hrs.
12−24 hrs.
24−36 hrs.
36−48 hrs.
Cost
Hydro Power 1
Fig. 12.3. Hydroelectric unit Ph2 versus
f1 for the original problem
obtained the previous study. This is mainly due to the fact that the previous
study used a naive constraint handling method despite the constraints being
quadratic. Since we used a simple repair mechanism in our approach, most so-
lutions created during the optimization process are feasible and the algorithm
is able to ﬁnd a better front.
12.4.1 Extending Variable Boundaries
Figures 12.2 and 12.3 show the variation of two hydroelectric power units for
diﬀerent trade-oﬀsolutions obtained by NSGA-II. These values are plotted
against the corresponding cost objective value. The ﬁgure shows that for all
solutions Ph1 in the fourth time period (36–48 hrs.) needs to be set at its upper
limit of 250 MW and for most solutions Ph2 needs to be set at its upper limit
of 500 MW. These suggest that there is scope of improving cost and emission
values if the upper bounds considered in the original study are allowed to
extend for these two variables. In the conceptual stage of design of such a
power system when no decision about parameters are ﬁxed, such information
is useful. An another study addresses this issue in greater details [22]. Based on
this observation and for investigation purposes, we increase the upper limits
of these two variables to 350 and 600 MW, respectively. We have also observed
that one of the thermal generator Ts2 almost reaches its upper limit of 175
MW in the fourth time period and hence we also increase its upper limit to
200 MW. Figure 12.4 shows the obtained front using the extended variable
boundaries. Individual optima and a number of ϵ-constraint single-objective
minimizations suggest that the obtained front is close to the true Pareto-
optimal front. To compare, the previous NSGA-II frontier is also shown as a
solid line. It is clear that the extension of boundaries allowed a better front to
be achieved. The whole new frontier dominates the previous frontier. In both
objectives, better individual optimal solutions are obtained.

12 Engineering Optimization Using Evolutionary Algorithms
281
Original
bounds
29000
28000
27000
26000
25000
24000
23000
22000
95000
90000
85000
80000
75000
70000
65000
Extended bounds
Single−obj.
eps−constraint
Cost
Emission
Fig. 12.4. NSGA-II frontier veriﬁed by single-objective optimizations for the
extended-boundary problem. Original frontier is also shown as a solid line
12.4.2 Detecting Robust Regions of Optimality
The single objective optimization problems solved near the minimum-cost so-
lution (in Fig. 12.1) indicates that this region is diﬃcult to optimize. The cost
objective involves a periodic term with the thermal generating units Psm.
This objective along with quadratic equality constraint makes the optimiza-
tion task diﬃcult. To investigate the sensitivities of solutions in this region, we
perform a robust optimization [8] in which the variables are perturbed within
±5 MW around each variable using a uniform distribution and 50 diﬀerent
scenarios are considered. Each solution is then evaluated for 50 such perturba-
tions and a normalized change in each function value (Δfi/fi) is noted. If the
maximum normalized change in function between two objectives is more than
a threshold value (η), the solution is declared infeasible. Thus, we add a new
constraint to the original two-objective optimization problem and optimize to
ﬁnd the robust frontier. Such a constraint is non-diﬀerentiable and may cause
diﬃculty, if used with a classical optimization method.
We use two diﬀerent values of η and plot the obtained robust frontiers in
Fig. 12.5 along with the original NSGA-II frontier (from Fig. 12.1). It can be
clearly seen that as the threshold η is reduced, thereby restraining the normal-
ized change to a small value, the robust front deviates more from the original
non-robust frontier in the area of minimum-cost solution. The ﬁgure shows
that solutions costing 75,000 or less is sensitive to the variable uncertain-
ties. However, the minimum-emission region remains relatively unperturbed,
meaning the near minimum-emission region is already robust. A similar anal-
ysis on the modiﬁed problem with extended boundaries is performed and the
results are shown in Fig. 12.6. It can be clearly seen from the ﬁgure that

282
Deb
the trade-oﬀfrontier for the extended problem is fairly robust except a small
portion near the minimum-cost region.
Original
η=0.01
η=0.05
region
Sensitive 
Robust
region
 23000
 24000
 25000
 26000
 27000
 28000
 65000
 70000
 75000
 80000
 85000
 90000
Emission
Cost
Fig. 12.5. Robust frontier for the orig-
inal problem
 22500
 23000
 23500
 24000
 24500
 25000
 25500
 65000  70000  75000  80000  85000  90000  95000
NSGA−II
Robust (eta=0.05)
Robust (eta=0.01)
Cost
Emission
Fig. 12.6. Robust frontier for the mod-
iﬁed problem with extended boundaries
Such a robust optimization strategy is of utmost importance in practice,
as in the event of uncertainties in achieving decision variables and problems,
engineers and designers are in most situations interested in ﬁnding a robust
solution which is relatively insensitive to such uncertainties. The above study
also reveals that the minimum-cost solution is relatively more sensitive to
such uncertainties than the minimum-emission solution for the chosen power
dispatch problem.
12.4.3 Unveiling Common Principles of Operation
To understand the operating condition of the hydro-thermal power system,
we analyze the obtained solutions in the trade-oﬀfrontier (Fig. 12.4) obtained
using extended boundaries. This process of deciphering important design or
operating principles in a problem is discussed in detail in another study [10]
and is called the innovization – innovation through optimization – task. Fig-
ures 12.7 through 12.12 show the variation of output levels with cost objective.
Several interesting behaviors (innovizations) can be observed from these plots:
1. Output powers for both hydroelectric units are more or less constant for
all trade-oﬀoptimal solutions for all time slots. The average values are
shown below:
0–12 Hrs. 12–24 Hrs. 24–36 Hrs. 36–48 Hrs.
Demand PD (MW)
900.00
1100.00
1000.00
1300.00
Ph1 (MW)
155.45
232.70
189.29
293.39
Ph2 (MW)
311.68
406.30
364.37
498.69

12 Engineering Optimization Using Evolutionary Algorithms
283
 0
 50
 100
 150
 200
 250
 300
 350
 65000
 70000
 75000
 80000
 85000
 90000
 95000
0−12 hrs.
12−24 hrs.
24−36 hrs.
36−48 hrs.
Cost
Hydro Power 1
Fig. 12.7. Hydroelectric unit Ph1 ver-
sus f1
 0
 100
 200
 300
 400
 500
 600
 65000
 70000
 75000
 80000
 85000
 90000
 95000
0−12 hrs.
12−24 hrs.
24−36 hrs.
36−48 hrs.
Hydro Power 2
Cost
Fig. 12.8. Hydroelectric unit Ph2 ver-
sus f1
 20
 40
 60
 80
 100
 120
 65000
 70000
 75000
 80000
 85000
 90000
 95000
0−12 hrs.
12−24 hrs.
24−36 hrs.
36−48 hrs.
Thermal Power 1
Cost
Fig. 12.9. Thermal unit Ps1 versus f1
 40
 60
 80
 100
 120
 140
 160
 180
 200
 65000
 70000
 75000
 80000
 85000
 90000
 95000
0−12 hrs.
12−24 hrs.
24−36 hrs.
36−48 hrs.
Cost
Thermal Power 2
Fig. 12.10. Thermal unit Ps2 versus f1
 50
 100
 150
 200
 250
 65000
 70000
 75000
 80000
 85000
 90000
 95000
0−12 hrs.
12−24 hrs.
24−36 hrs.
36−48 hrs.
Thermal Power 3
Cost
Fig. 12.11. Thermal unit Ps3 versus f1
 50
 100
 150
 200
 250
 300
 65000
 70000
 75000
 80000
 85000
 90000
 95000
0−12 hrs.
12−24 hrs.
24−36 hrs.
36−48 hrs.
Thermal Power 4
Cost
Fig. 12.12. Thermal unit Ps4 versus f1
2. These hydroelectric power output values are related to the variation in
power demand, as can be seen from the above table. If the demand is
more in a time period, the optimal strategy is to increase the hydroelectric
power generation for that time period and vice versa.
3. The combined hydroelectric power generation (with only two units) takes
care of more than 50% of the total demand in all time periods.

284
Deb
4. The power output of both hydroelectric units for a particular time period
reﬂects the amount of water availability. For the second hydroelectric unit
the water availability is more, hence the power generation is also more.
5. For all four thermal units, the power generation must be increased with
the demand in that interval.
6. For better cost solutions, the thermal power generation of units 1 and 4
must be increased. This is because as and bs values for these units are lower
compared to other two thermal units, thereby causing a comparatively
smaller increase in the fuel cost with an increase in power generation in
these two units. However, the power generation of units 2 and 3 must be
decreased (with an exception at the fourth time period for unit 2) due to
the opposite reason to that above.
7. For smaller cost solutions, the thermal power generation is more or less
the same for all time periods, except the fourth time period in unit 2.
This is because due to handling a large demand in this time period, the
emission-eﬀective way of reducing the cost is to increase the thermal power
generation in unit 2 (due to the large negative βs value (see appendix)
associated with this unit).
8. Although a large range of power generation is allowed, the optimal values
of these power generation units are concentrated within a small region in
the search space.
The above properties of the optimized trade-oﬀsolutions are interesting and
are, by any means, not trivial. Finding the optimized trade-oﬀsolutions and
then analyzing the solutions for discovering common properties (the innoviza-
tion process [10]) is a viable procedure for deciphering such important informa-
tion. Having solved the stationary problem to our satisfaction and comparing
the optimized solutions with the existing study, we are now conﬁdent and
venture to solve the same problem as a non-stationary problem.
12.5 Dynamic Power Scheduling Problem
The hydro-thermal power dispatch problem is truly dynamic in nature due
to the changing nature of power demand. An optimal power scheduling for a
particular power demand proﬁle is not necessarily optimal for another power
demand. In such cases, new optimal solutions must be found as and when
there is a change in the power demand. Many search and optimization prob-
lems in practice change with time and therefore must be treated as on-line
optimization problems. The change in the problem with time t can be ei-
ther in its objective functions or in its constraint functions or in its variable
boundaries or in a combination of the above:
Minimize f(x, t) = (f1(x, t), f2(x, t), . . . , fM(x, t)) ,
Subject to gj(x, t) ≥0,
∀j,
hk(x, t) = 0,
∀k,
x(L)
i
(t) ≤xi(t) ≤x(U)
i
(t),
∀i.
(12.10)

12 Engineering Optimization Using Evolutionary Algorithms
285
Such an optimization problem ideally must be solved at every time instant t
or whenever there is a change in any of the above functions with t. In such
optimization problems, the time parameter can be mapped with the iteration
counter τ of the optimization algorithm. One diﬃculty which may arise in
solving the above on-line optimization task is that the underlying optimization
algorithm may not get too many iterations to ﬁnd the optimal solutions before
there is a change in the problem. If the change is too frequent, the best hope
of an optimization task is to track the optimal solutions as closely as possible
within the time span allowed to iterate. However, for steady changes in a
problem (which is usually the case in practice), there lies an interesting trade-
oﬀwhich we discuss next.
Let us assume that the change in the optimization problem is gradual in
t. Let us also assume that each optimization iteration requires a ﬁnite time
G to execute and that τT iterations are needed (or allowed) to track the
optimal frontier. In this chapter, we assume that problem does not change
(or is assumed to be constant) within a time interval tT and τT G < tT . Here,
initial τT G time is taken up by the optimization algorithm to track the new
trade-oﬀfrontier and to make a decision for implementing a particular solution
from the frontier. Here, we choose α = τT G/tT to be a small value (say 0.25),
such that after the optimal frontier is tracked, (1 −α)tT time is spent on
using the outcome for the time period. Figure 12.13 illustrates this dynamic
procedure. Thus, if we allow a large value of tT (allowing a large number of
f(t)
window for
optimization
assumed f(t) for
time period of length t
computed at A
A
tT
GτT
GτT
tT
0
2tT
T
Time, t
Fig. 12.13. The on-line optimization procedure adopted in this study. For simplicity,
only one objective function is shown here
optimization iterations τT ), a large change in the problem is expected, but
the change occurs only after a large number of iterations of the optimization
algorithm. Thus, despite the large change in the problem, the optimization
algorithm may have enough iterations to track the trade-oﬀoptimal solutions.
However, this scenario will produce a grossly approximate solution to the real
problem, which changes with t continuously. On the other hand, if we choose

286
Deb
a small τT , the change in the problem is frequent (which approximates the
real scenario more closely), but a lesser number of iterations are allowed to
track new optimal solutions for a problem which has also undergone a small
change. Obviously, there lies a lower limit to τT below which, albeit a small
change, the number of iterations are not enough for an algorithm to track
the new optimal solutions adequately. Such a limiting τT will depend on the
nature of the dynamic problem and the chosen algorithm, but importantly
allows the best scenario (and closest approximation to the original problem)
which a particular algorithm can achieve. Here, we investigate this aspect in
the context of dynamic multi-objective optimization problem and ﬁnd such
a limiting τT for two variants of NSGA-II on a test problem and the real-
world hydro-thermal power dispatch optimization problem discussed above.
However, the procedure adopted in this study is generic and can be applied
to other dynamic optimization problems as well.
The procedure can be applied to its extreme as well. If we allow the prob-
lem to change as frequently as the time needed to complete one iteration of the
optimization algorithm (that is, tT = τT = 1, yielding G = 1), we have a true
on-line optimization procedure in which the problem changes continuously
with generation counter.
12.5.1 Proposed Modiﬁcations to NSGA-II
NSGA-II was developed for solving stationary multi-objective optimization
problems and may not be suitable on its own to be applied to dynamic prob-
lems. We make minor changes to the original NSGA-II.
First, we introduce a test to identify whether there is a change in the
problem. The mathematical formulation of the problem remains the same,
but due to change in power demand the evaluation of constraints will give
rise to a change in power generation values, thereby causing a change in the
objective values. For this purpose, we randomly pick a few solutions from
the parent population (10% population members considered here) and re-
evaluate them. If there is a change in any of the objectives and constraint
functions, we establish that there is a change in the problem. In the event
of a change, all parent solutions are re-evaluated before merging the parent
and child population into a bigger pool. This process allows both oﬀspring and
parent solutions to be evaluated using the changed objectives and constraints.
In the ﬁrst version (DNSGA-II-A) of the proposed dynamic NSGA-II, we
introduce new solutions whenever there is a change in the problem. A ζ% of
the new population is replaced with randomly created solutions. This helps
to introduce new (random) solutions whenever there is a change in the prob-
lem. This method may perform better in problems with a large change in
the objectives and constraints. In the second version (DNSGA-II-B), instead
of introducing random solutions, ζ% of the population is replaced with mu-
tated versions of existing solutions (chosen randomly). This way, the new

12 Engineering Optimization Using Evolutionary Algorithms
287
solutions introduced in the population are related to the existing population.
This method may work well in problems with a small change in problem.
The dynamic version of the problem involves more frequent changes in
the demand PDm. The objective and constraint functions are the same as
those in the stationary problem. We keep the overall time window of T = 48
hours, but increase the frequency of changes (that is, increase M from four
to 192, so that the time window tT for each demand level varies from 12
hours to 48/192 hours or 15 minutes). It will then be an interesting task
to ﬁnd the smallest time window of statis which a speciﬁc multi-objective
optimization algorithm can solve successfully. Any change more frequent than
this smallest time limit would be too fast a change in the problem for the
algorithm to track the optimal trade-oﬀfront. For this purpose, we use the
same overall four-step changing pattern in demand as we considered in the
stationary case, but make a step-wise linear interpolation between any two
consecutive changes to ascertain the intermediate level of demand. Fig. 12.14
explains the interpolation procedure for M = 16. The same procedure is
followed to calculate the demands for other M values.
 0
 200
 400
 600
 800
 1000
 1200
 1400
 0  3  6  9  12  15  18  21  24  27  30  33  36  39  42  45  48
Time, t (Hrs.)
Power Demand (MW)
Fig. 12.14. Change in power de-
mand with time for M = 16 (3-
hourly change)
36−48 hrs.
0−12 hrs.
12−24 hrs.
24−36 hrs.
900 MW
1300 MW
1100 MW
1000 MW
 2000
 4000
 6000
 8000
 10000
 12000
 14000
 16000
 10000
 14000
 18000
 22000
 26000
 30000
Ideal Frontiers
DNSGA−II−A
Emission
Cost
Fig. 12.15. Four fronts, each change in
demand, obtained using DNSGA-II-A with
ζ = 0.2
Diﬃculty in Handling Water Availability Constraint
Equation (12.5) requires hydroelectric power generation units from diﬀerent
time intervals to be used together to satisfy the equation. In a dynamic op-
timization problem, this is a diﬃculty, as this means that information about
all hydroelectric units is needed right in the ﬁrst generation. This constraint
equates the total required water head to be identical to the available value for
each hydroelectric system.
It is interesting to note from Figs 12.7 and 12.8 that the optimized hydro-
electric power unit is almost proportional to the demand. A proportionate law

288
Deb
can be used to allocate Ph1 and Ph2 values with time and in such a manner
as to satisfy the water availability constraints. We are currently pursuing this
proportionate concept and will report results in the due course of time. In this
study, we use a simple principle of allocating an identical water head Wh/M
for each time interval.
12.5.2 Simulation Results Using Dynamic NSGA-II
To make the time-span of the scheduling problem in tune with the NSGA-II
generations, we assume that the demand value PDm changes with generation
counter (as in Fig. 12.14) and the mapping is such that the NSGA-II run
reaches its maximum generation number at the end of the scheduling time-
span (T = 48 hours). Since the proposed procedures have a way to detect
if there is a change in the problem, such an optimization algorithm can be
directly used for on-line optimization applications.
We apply the two dynamic NSGA-II procedures discussed above (DNSGA-
II-A and DNSGA-II-B) to solve the dynamic optimization problem. The pa-
rameters used are the same as in the oﬀ-line optimization case presented
before. First, we consider the problem with a 12-hour statis. To compare the
dynamic NSGA-II procedures, we ﬁrst treat each problem as a static opti-
mization problem and apply the original NSGA-II procedure [5] for a large
number (500) of generations so that no further improvement is likely. We call
these ‘ideal’ fronts and compute the hypervolume measure using a reference
point which is the nadir point of the ideal front. Thereafter, we apply each
dynamic NSGA-II and ﬁnd an optimized non-dominated front. Then for each
front, we compute the hypervolume using the same reference point and then
compute the ratio of this hypervolume value with that of the ideal front. This
way, the maximum value of the ratio of hypervolume for an algorithm is one
and as the ratio becomes smaller than one, the performance of the algorithm
gets poorer.
In the dynamic NSGA-II procedures, we run for 960/M (M is the number
of changes in the problem) generations for each change in the problem. Thus,
if a large number changes are to be accommodated, the number of generations
needed for each optimization run gets reduced. First, we consider the problem
in which we consider a change after every 12 hours (M = 4). Figure 12.15
shows the four Pareto-optimal fronts obtained using DNSGA-II-A with 20%
addition of random solutions every time there is a change in the problem.
The DNSGA-II-A procedure is able to ﬁnd a set of solutions very close to the
ideal frontiers in all four time periods. The ﬁgure makes one aspect clear. As
the demand is more, the power production demands larger cost and emission
values.
Increasing Number of Changes in the Problem
Next, we increase the number of changes from eight to 192, but present results
for M = 16, 48, 96 and 192. Figures 12.16 – 12.19 show the hypervolume ratio

12 Engineering Optimization Using Evolutionary Algorithms
289
for diﬀerent number of changes in the problem for the DNSGA-II-A procedure
with diﬀerent proportion of addition of random solutions, ζ. The ﬁgures also
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
50%ile
90%ile
95%ile
99%ile
Proportion of addition
Hypervolume ratio
Fig. 12.16. 3-hourly change (M = 16)
with DNSGA-II-A
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
50%ile
90%ile
95%ile
99%ile
Proportion of addition
Hypervolume ratio
Fig. 12.17. 1-hourly change (M = 48)
with DNSGA-II-A
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
50%ile
90%ile
95%ile
99%ile
Hypervolume ratio
Proportion of addition
Fig. 12.18. 30-minute change (M =
96) with DNSGA-II-A
50%ile
90%ile
95%ile
99%ile
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
Hypervolume ratio
Proportion of addition
Fig. 12.19. 15-minute change (M =
192) with DNSGA-II-A
mark the 50th, 90th, 95th and 99th percentile of hypervolume ratio, meaning
the cut-oﬀhypervolume ratio which is obtained by the best 50, 90, 95, and
99 percent of M frontiers in a problem with M changes. The ﬁgures show
that as M increases, the performance of the algorithm gets poorer due to the
fact that a smaller number of generations (960/M) was allowed to meet the
time constraint. If a 90% hypervolume ratio is assumed to be the minimum
required hypervolume ratio for a reasonable performance of an algorithm and
if we consider 95 percentile performance is adequate, the ﬁgures show that we
can allow a maximum of 96 changes in the problem, meaning a change in the
problem in every 48/96 or 0.5 hours (or 30 minutes). With 96 changes in the
problem, about 20 – 70% random solutions can be added whenever there is a
change in the problem to start the next optimization, to achieve a successful
run. This result is interesting and suggests the robustness of the DNSGA-II

290
Deb
procedure for this problem. It is also clear that no addition of any random
solution (ζ = 0) is not a good strategy.
Next, we consider DNSGA-II-B procedure in which mutated solutions are
added instead of random solutions. Mutations are performed with double the
mutation probability used in NSGA-II and with ηm = 2. Figures 12.20 –
12.23 show the performance plots. Here, the eﬀect is somewhat diﬀerent. In
general, with an increase in addition of mutated solutions, the performance
is better. Once again, with 96 changes in the problem within 48 hours of
operation seem to be the largest number of changes allowed for the algorithm
to perform reasonably well. However, mutated solutions more than ζ = 40%
of the population seem to perform well. Once again, DNSGA-II-B procedure
is also found to work well with a wide variety of ζ values (40 – 100%).
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
50%ile
90%ile
95%ile
99%ile
Hypervolume ratio
Proportion of addition
Fig. 12.20. 3-hourly change (M = 16)
with DNSGA-II-B
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
50%ile
90%ile
95%ile
99%ile
Hypervolume ratio
Proportion of addition
Fig. 12.21. 1-hourly change (M = 48)
with DNSGA-II-B
50%ile
90%ile
95%ile
99%ile
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
Proportion of addition
Hypervolume ratio
Fig. 12.22. 30-minute change (M =
96) with DNSGA-II-B
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
50%ile
90%ile
95%ile
99%ile
Hypervolume ratio
Proportion of addition
Fig. 12.23. 15-minute change (M =
192) with DNSGA-II-B

12 Engineering Optimization Using Evolutionary Algorithms
291
12.6 Decision-making in a Dynamic Multi-objective
Optimization
One of the issues which is not discussed enough in the EMO literature is the
decision-making aspect after a set of trade-oﬀsolutions is found. In a static
multi-objective optimization problem, the overall task is to come up with one
solution which would be ﬁnally implemented in practice. Thus, choosing a so-
lution from the Pareto-optimal set is as important as ﬁnding a set of optimized
trade-oﬀsolutions. Some studies in this direction have just begun for station-
ary problems [6,9,13,17] and more such studies are called for. However, in a
dynamic multi-objective optimization problem, there is an additional problem
with the decision-making task. A solution is to be chosen and implemented
as quickly as the trade-oﬀfrontier is found, and in most situations before the
next change in the problem has taken place. This deﬁnitely calls for an auto-
matic procedure for decision-making with some pre-speciﬁed utility function
or some other procedure. This way, in an on-line implementation, as soon as a
dynamic EMO ﬁnds a frontier, a quick analysis of the solutions can be made
to ﬁnd a particular solution which optimizes or satisﬁes the decision-making
procedure maximally.
In this chapter, we choose a utility measure which is related to the relative
importance given to both cost and emission objectives. First, we consider a
dynamically operating decision-making procedure in which we are interested
in choosing a solution having almost an equal importance to both cost and
emission. As soon as the frontier is found for the forthcoming time period, we
compute the pseudo-weight w1 (for cost objective) for every solution x in the
frontier using the following term:
w1(x) =
(f max
1
−f1(x))/(f max
1
−f min
1
)
(f max
1
−f1(x))/(f max
1
−f min
1
) + (f max
2
−f2(x))/(f max
2
−f min
2
).
(12.11)
Thereafter, we choose the solution (x) with w1(x) closest to 0.5.
To demonstrate the utility of this dynamic decision-making procedure, we
consider the hydro-thermal problem with 48 time periods (meaning a change
of the problem at every hour). Figure 12.24 shows the obtained frontiers in
solid lines and the corresponding preferred (operating) solution with a circle.
It can be observed that due to the preferred importance of 50–50% to cost
and emission, the solution comes nearly in the middle of each frontier. The
ﬁgure also marks the time period on some of the frontiers to give an idea how
the frontiers change with time. Since hydroelectric units are assumed to be
constant over time in this study, to satisfy two water availability constraints,
Th1 = 219.76 MW and Th2 = 398.11 MW must be used. However, four thermal
power units must produce power to meet the remaining demand and these
values for the 50–50% cost-emission solution for all 48 time periods are shown
in Fig. 12.25. It is interesting to note that how the thermal units must produce
diﬀerent power values to meet the remaining overall changing demand (shown
by a solid line). The changing pattern in overall thermal power generation

292
Deb
t=1
48
2
47
41
318
131115
20
3
46
5
6
7
t=12
Operating point
 0
 200
 400
 600
 800
 1000
 1200
 1400
 800  1000  1200  1400  1600  1800  2000  2200  2400  2600
Cost
Emission
Fig. 12.24. Operating solution for 50–
50% cost-emission case on the trade-oﬀ
frontiers for the 48 time-period problem
 0
 100
 200
 300
 400
 500
 600
 700
 800
 1
 5
 9
 13  17  21  25  29  33  37  41  45  48
Thermal Demand
Ts1
Ts2
Ts3
Ts4
Fig. 12.25. Variation of thermal power
production for 50–50% cost-emission
case for the 48 time-period problem
varies similar to that in the remaining demand in power. The ﬁgure also
shows a slight over-generation of power to meet the loss term given in (12.4).
50−50
100−0
0−100
Demand
 800
 1000
 1200
 1400
 1600
 1800
 2000
 2200
 2400
 2600
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
Time (Hrs.)
Cost
Fig. 12.26. Variation of cost of oper-
ation with time for the 48 time-period
problem
100−0
0−100
Demand
50−50
 0
 200
 400
 600
 800
 1000
 1200
 1400
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
Time (Hrs.)
Emission
Fig. 12.27. Variation of emission value
with time for the 48 time-period prob-
lem
Table 12.1. Cost-emission trade-oﬀ
Case
Cost
Emission
50-50% 74239.07 25314.44
100-0% 69354.73 27689.08
0-100% 87196.50 23916.09

12 Engineering Optimization Using Evolutionary Algorithms
293
Next, we compare the above operating schedule of power generation with
two other extreme cases: (i) 100–0% importance to cost and emission and
(ii) 0–100% importance to cost and emission. Figures 12.26 and 12.27 show
the variation of cost and emission, respectively, for these two cases and the
50–50% case discussed above. First, it is interesting to note that the optimal
cost and emission values ﬂuctuate the way the power demand varies. Second,
the case with 100% importance to cost requires minimum-cost, but causes
large emission values and the case with 100% importance to emission causes
minimum-emission values, but with large costs. A comparison of overall cost
and emission values for the entire 48-hour operation for these three cases are
summarized in Table 12.1. The results agree with above argument. Third, the
change in emission values is not as much as in cost values, as was also observed
in the computation of the robust frontiers in the stationary case (Fig. 12.5).
12.7 Conclusions
In this chapter, we have demonstrated the power of population-based evolu-
tionary algorithms for handling a complex hydro-thermal dispatch optimiza-
tion problem involving non-linear objective and constraint functions, uncer-
tainties in decision variables, the dynamic nature of the problem due to the
change in power demand, and multiple objectives. Following important con-
clusions can be drawn from this study:
1. The study shows how multi-objective EA results can be found and ver-
iﬁed with a number of other single-objective optimization tasks to build
conﬁdence in obtained solutions.
2. The study shows how multiple trade-oﬀsolutions can be used to deci-
pher important insights about the problem which give a clear idea of the
properties needed for a solution to become an optimal solution. This is
possible only by ﬁnding and analyzing a set of optimal solutions, instead
of a single optimal solution.
3. The study also shows how robust solutions (which are relatively insensitive
to parameter uncertainties) can be found and good regions of robustness
in a problem can be identiﬁed.
4. The study also shows how a dynamic optimization task can be executed
using an EA and optimal solutions can be tracked, whenever there is a
change in the problem.
5. Finally, the study suggests an automatic decision-making procedure for
choosing a preferred solution from the multiple trade-oﬀfrontier, as and
when the problem is changed.
All these tasks have tremendous practical importance and this study is a
testimony to some of the possibilities of evolutionary optimization procedures
in making a computational optimization task a big step closer to practice.

294
Deb
Acknowledgments
The author would like to thank Ms. Barnali Kar and Mr. Uday Bhaskara Rao
for discussions about the hydro-thermal power dispatch optimization problem.
References
1. Basu, M.: A simulated annealing-based goal-attainment method for economic
emission load dispatch of ﬁxed head hydrothermal power systems.
Electric
Power and Energy Systems 27(2), 147–153 (2005)
2. Deb, K.: An eﬃcient constraint handling method for genetic algorithms. Com-
puter Methods in Applied Mechanics and Engineering 186(2–4), 311–338 (2000)
3. Deb, K.: Multi-objective optimization using evolutionary algorithms.
Chich-
ester, UK: Wiley (2001)
4. Deb, K., Agrawal, R.: Simulated binary crossover for continuous search space.
Complex Systems 9(2), 115–148 (1995)
5. Deb, K., Agrawal, S., Pratap, A., Meyarivan, T.: A fast and elitist multi-
objective genetic algorithm: NSGA-II.
IEEE Transactions on Evolutionary
Computation 6(2), 182–197 (2002)
6. Deb, K., Chaudhuri, S.: I-MODE: an interactive multi-objective optimization
and decision-making using evolutionary methods.
In: Proceedings of Fourth
International Conference on Evolutionary Multi-Criteria Optimization (EMO
2007), pp. 788–802 (2007)
7. Deb, K., Goyal, M.: A combined genetic adaptive search (GeneAS) for engineer-
ing design. Computer Science and Informatics 26(4), 30–45 (1996)
8. Deb, K., Gupta, H.: Searching for robust Pareto-optimal solutions in multi-
objective optimization. In: Proceedings of the Third Evolutionary Multi-Criteria
Optimization (EMO-05) Conference (Also Lecture Notes on Computer Science
3410), pp. 150–164 (2005)
9. Deb, K., Kumar, A.: Interactive evolutionary multi-objective optimization and
decision-making using reference direction method. In: Proceedings of the Ge-
netic and Evolutionary Computation Conference (GECCO-2007), pp. 781–788.
New York: The Association for Computing Machinery (ACM) (2007)
10. Deb, K., Srinivasan, A.: Innovization: innovating design principles through opti-
mization. In: Proceedings of the Genetic and Evolutionary Computation Confer-
ence (GECCO-2006), pp. 1629–1636. New York: The Association for Computing
Machinery (ACM) (2006)
11. Miettinen, K.: Nonlinear Multiobjective Optimization. Kluwer, Boston (1999)
12. Orero, S., Irving, M.: A genetic algorithm modeling framework and solution
technique for short term optimal hydrothermal scheduling. IEEE Transactions
on Power Systems 13(2) (1998)
13. Phelps, S., Koksalan, M.: An interactive evolutionary metaheuristic for multi-
objective combinatorial optimization. Management Science 49(12), 1726–1738
(2003)
14. Rashid, A., Nor, K.: An eﬃcient method for optimal scheduling of ﬁxed head
hydro and thermal plants. IEEE Transactions Power Systems 6(2) (1991)

12 Engineering Optimization Using Evolutionary Algorithms
295
15. Shukla, P., Deb, K.: Comparing classical generating methods with an evolution-
ary multi-objective optimization method. In: Proceedings of the Third Interna-
tional Conference on Evolutionary Multi-Criterion Optimization (EMO-2005),
pp. 311–325 (2005). Lecture Notes on Computer Science 3410
16. Sinha, N., Chakraborty, R., Chattopadhyay, P.: Fast evolutionary programming
techniques for short-term hydrothermal scheduling.
Electric Power Systems
Research 66, 97–103 (2003)
17. Thiele, L., Miettinen, K., Korhonen, P., Molina, J.: A preference-based interac-
tive evolutionary algorithm for multiobjective optimization. Tech. Rep. Working
Paper Number W-412, Helsinki School of Economics, Helsingin Kauppakorkeak-
oulu, Finland (2007)
18. Wong, K., Wong, Y.: Short-term hydrothermal scheduling I – simulated anneal-
ing approach. IEE Proc-C Gener., Trans., Distrib 141(5) (1994)
19. Wood, A., Woolenberg, B.: Power Generation, Operation and Control. John-
Wiley & Sons (1986)
20. Yang, J., Chen, N.: Short-term hydrothermal co-ordination using multipass dy-
namic programming. IEEE Transactions Power Systems 4(3) (1989)
21. Zaghlool, M., Trutt, F.: Eﬃcient methods for optimal scheduling of ﬁxed head
hydrothermal power systems. IEEE Transactions Power Systems 3(1) (1988)
22. Zeleny, M.: Multiple Criteria Decision Making. McGraw-Hill, New York (1982)
Appendix A: Parameters for Hydro-thermal Power
Dispatch Problem
Table 12.2. Hydroelectric system data
Unit
a0h
a1h
a2h
Wh
P min
h
P max
h
(acre-ft/h) (acre-ft/MWh) (acre-ft/(MW)2h) (acre-ft) (MW) (MW)
1
260
8.5
0.00986
125000
0
250
2
250
9.8
0.01140
286000
0
500
Table 12.3. Cost related thermal system data
Unit
as
bs
cs
ds
es
P min
s
P max
s
($/h) ($/MWh) ($/(MW)2h) ($/h) (1/MW) (MW) (MW)
3
60.0
1.8
0.0030
140
0.040
20
125
4
100.0
2.1
0.0012
160
0.038
30
175
5
120.0
2.1
0.0010
180
0.037
40
250
6
40.0
1.8
0.0015
200
0.035
50
300

296
Deb
Table 12.4. Emission related thermal system data
Unit αs(Lb/h) βs(Lb/MWh) γs(Lb/(MW)2h) ηs(Lb/h) δs(1/MW)
3
50
−0.555
0.0150
0.5773
0.02446
4
60
−1.355
0.0105
0.4968
0.02270
5
45
−0.600
0.0080
0.4860
0.01948
6
30
−0.555
0.0120
0.5035
0.02075
B =
⎡
⎢⎢⎢⎢⎢⎣
0.000049
0.000014
0.000015
0.000015
0.000020
0.000017
0.000014
0.000045
0.000016
0.000020
0.000018
0.000015
0.000015
0.000016
0.000039
0.000010
0.000012
0.000012
0.000015
0.000020
0.000010
0.000040
0.000014
0.000010
0.000020
0.000018
0.000012
0.000014
0.000035
0.000011
0.000017
0.000015
0.000012
0.000010
0.000011
0.000036
⎤
⎥⎥⎥⎥⎥⎦
per MW.

13
Multimodal Function Optimization of
Varied-Line-Spacing Holographic Grating
Qing Ling1, Gang Wu2, and Qiuping Wang3
1 Department of Automation, University of Science and Technology of China,
Hefei, China qingling@mail.ustc.edu.cn
2 Department of Automation, University of Science and Technology of China,
Hefei, China wug@ustc.edu.cn
3 National Synchrotron Radiation Laboratory, University of Science and
Technology of China, Hefei, China qiuping@ustc.edu.cn
13.1 Introduction
In practical optimal design problems, objective functions often lead to multi-
modal domains. There are generally two basic requirements in the multimodal
function optimization problem:
1. to ﬁnd the global optimum, i.e. global search ability, and
2. to locate several local optima that might be good alternatives to the global
optimum, i.e. multi-optimum search ability.
The ﬁrst one comes from the requirement of optimality, while the second
one reﬂects the needs of practical engineering design. Multiple solutions per-
mit designers to choose the best one in terms of ease of manufacture, ease
of maintenance, reliability, etc. which cannot be simply represented by the
objective function [2].
Evolutionary algorithms (EAs), such as genetic algorithms (GAs), evolu-
tion strategies (ESs), and diﬀerential evolution (DE), are powerful tools for
global optimization. To locate the global optimum eﬃciently, we need to im-
prove the diversity of the population to prevent the premature convergence
phenomenon. Self-adaptation strategies are helpful in preserving the diversity
and achieving global search ability [13,31].
Intuitively, EAs are also proper choices for multi-optimum search problems
because of their intrinsic parallel search mechanism. But in multi-optimum
search, it is not enough to preserve the diversity of the population only. We
need to promote the formation of stable subpopulations in the neighborhood
of optimal solutions, both global and local [32]. In this way, multiple solutions
are identiﬁed at the end of the optimization, rather than only one global
optimum. In EAs, niching techniques have been introduced to achieve this
goal.

298
Ling et al.
The concept of niche comes from ecosystems. A niche can be viewed as
a subspace in the environment that can support diﬀerent types of life. For
each niche, resources are ﬁnite and must be shared among the population of
that niche. By analogy, in EAs, a niche is commonly referred to as a neigh-
boring area of an optimum of the parameter space. The corresponding ﬁtness
function of the optimum represents resources of that niche [32]. Niching tech-
niques reallocate resources among individuals in a niche, thus preventing the
population from converging to few optima, and promoting the formation of
stable subpopulations in niches.
Niching techniques have achieved successful applications in many engi-
neering design areas. Liu proposed a hybrid niching genetic algorithm in the
optimal design of a ultra-broadband ampliﬁer [21] and ﬁber Raman ampli-
ﬁer [22]. Crutchley searched the quiescent operating point of nonlinear cir-
cuits with the aid of a niching EA [4]. Other applications include electrode
shape optimization [34], electromagnetics optimization [33], and the planning
of multiple paths of robots [15].
Theoretical analysis of niching techniques is very diﬃcult because of the
complicated ﬁtness landscape of multimodal functions. Therefore, the analysis
is limited to simple trap functions and simpliﬁed algorithms [3, 25]. A pow-
erful tool for theoretical analysis is the Markov chain model. Nijssen studied
convergence velocity and reliability with the Markov chain [29]. Another ex-
ample is [7], where relationships between convergence properties, algorithm
parameters, and the ﬁtness landscape were investigated.
In this chapter, we mainly focus on the multi-optimum search problem
under the framework of multimodal function optimization. Various niching
techniques, such as sharing, crowding, clearing, sequential niche sharing, dy-
namic niche clustering, etc. are discussed. Among the main niching techniques,
the restricted evolution evolution strategy (REES) is selected to demonstrate
the usefulness of multi-optimum search in practical engineering design.
The rest of this chapter is arranged as follows. Section 13.2 reviews the
main niching techniques in multimodal function optimization. REES is de-
scribed in detail in Sect. 13.3. In Sect. 13.4, the proposed algorithm is com-
pared with standard crowding and deterministic crowding in several multi-
modal functions. REES is applied to the design of the varied-line-spacing
holographic grating (VLSHG) for the National Synchrotron Radiation Labo-
ratory (NSRL) in Sect. 13.5. Section 13.6 concludes the chapter.
13.2 Niching Techniques
A comparison of some existing niching techniques is made in a recent study
by Singh and Deb [35]. The key point in niching techniques is how to decide
the range of a speciﬁed niche, i.e. the niche radius problem. The choice of the
niche radius reﬂects an estimation of the shape of the ﬁtness landscape. There

13 Multimodal Function Optimization of VLSHG
299
are three classes of methods to deal with the niche radius problem: explicit,
implicit, and adaptive methods.
13.2.1 Explicit Niche Radius
Explicit niching methods set a predetermined niche radius before the op-
timization process. The choice of niche radius inﬂuences the performance of
multi-optimum search greatly. But the proper value of niche radius is strongly
related to the ﬁtness landscape, which is generally diﬃcult to analyze in prac-
tical optimization problems.
Sharing
The most important explicit niching method is ﬁtness sharing [5,11,12]. The
basic idea is to punish individuals for occupying the same regions in the search
space by scaling the ﬁtness of each individual. Thus, by penalizing individuals
that cluster together, the sharing scheme prevents individuals from converging
to a single optimum. The shared ﬁtness f ′(Pi) of individual Pi is calculated
as follows from the original ﬁtness f(Pi):
f ′(Pi) =
f(Pi)
j=P opNum
j=1
sh(D(Pi, Pj))
(13.1)
where PopNum is the population size of the EA, D is the distance metric,
and sh is the sharing function. Typically, a triangular sharing function is used:
sh(D(Pi, Pj)) =
1 −(D(Pi, Pj)/σ)α
D(Pi, Pj) < σ
0
D(Pi, Pj) ≥σ.
(13.2)
The constant σ is the predetermined niche radius, and α is the scaling factor
to regulate the shape of the sharing function.
The main limitation of the sharing method lies in the choice of niche radius
σ. Firstly, setting of σ requires a priori knowledge of the ﬁtness landscape. But
for practical optimization problems, we generally do not have enough informa-
tion about the shape of the objective function and the distance between opti-
mal solutions. Secondly, σ is the same for all individuals. This supposes that
all optima must be nearly equidistant in the domain. Sareni [32] proved that
sharing failed to maintain desired optimal solutions if they are not equidis-
tant or if the estimated distance between two optima is incorrect. Thirdly, the
sharing scheme is very expensive regarding computational complexity. Thus
clustering analysis [39] and dynamic niche sharing [27] have been proposed to
reduce the computation overhead.

300
Ling et al.
Clearing
Clearing is a similar niching technique to sharing [30]. In a given niche radius
σ, the basic clearing algorithm preserves the ﬁtness of the dominant individual
while it resets the ﬁtness of all other individuals to zero. Thus, the clearing
procedure fully attributes the whole resources of a niche to a single winner –
the winner takes all rather than sharing resources with other individuals in
the same niche as is done in the sharing method.
Sequential Niche
The sequential niche method executes the EA program with a modiﬁed ﬁtness
function repeatedly [2]. First, we set the modiﬁed ﬁtness function to the raw
ﬁtness function, run the EA program, and record the best individual. Second,
we update the modiﬁed ﬁtness function to give a depression in the region
near the best individual. Then the new best individual of the new modiﬁed
ﬁtness function is recorded by using EA again. By sequentially using EA and
sequentially modifying the ﬁtness function, multiple solutions are ﬁnally iden-
tiﬁed. In the sequential niche method, the modiﬁcation of the ﬁtness function
is related to the estimation of niche radius, as in sharing and clearing.
Species Conserving
The species conserving genetic algorithm [18] is based on the concept of di-
viding the population into several species according to their similarity. Each
of these species is built around a dominating individual called the species
seed. Species seeds found in the current generation are conserved by moving
them into the next generation. Similarity of species is also decided through a
predetermined niche radius σ, for which an experimental formula is provided.
Restricted Competition Selection
As in the clearing method, restricted competition selection sets the loser’s
ﬁtness to zero in a given niche radius. At the end of each generation, M
individuals are selected into an elite set. Therefore, the restricted competition
selection algorithm is a combination of the clearing method and the elite
strategy. By introducing the elite set with size M, we may expect that at
least M local optimal or near-optimal solutions are obtained at the end [17].
13.2.2 Implicit Niche Radius
Implicit niche radius methods do not require a predeﬁned niche radius. The
information on niches is embodied in the population itself. There are two
classes of implicit niching methods: crowding and multi-population.

13 Multimodal Function Optimization of VLSHG
301
Crowding methods include standard crowding, deterministic crowding and
restricted tournament selection. The main disadvantages of crowding methods
are selection error and genetic drift. Selection error means that individuals fail
to compete with elements in the same niche. For example, an unsatisfactory
individual may not be chosen to compare with other neighboring individu-
als during the optimization process, and thus survives at the end. Though
inevitable for a ﬁnite population, selection error will lead to much depressed
optimization performance if too many unsatisfactory individuals survive in
evolution. On the contrary, genetic drift means that good candidate solutions
are chosen by error to compare with dominating individuals, thus individuals
are inclined to converge to a few eminent solutions. If we fail to protect the
formation of stable subpopulations in the neighborhood of optimal solutions,
genetic drift will occur.
Multi-population EAs divide the whole population into some subpopula-
tions, and permit information exchange between subpopulations. The main
diﬃculty in multi-population EAs is how to control information exchange,
both maintaining a steady exploitation and achieving an eﬃcient exploration.
Standard Crowding
Standard crowding updates the population through replacing similar par-
ents [36]. For each child individual C, randomly select CF (crowding factor)
individuals in the parent population, choose the nearest parent P under some
distance metric. If C is better than P, use C to replace P, else preserve P.
When CF is small, each individual will only have a small chance to compare
with other individuals in the same niche and to improve the quality of solu-
tion. Therefore, for small CF, standard crowding introduces great selection
error. On the other hand, individuals nearing a dominating solutions may be
replaced with high probability, even though they are potential local optima.
Thus the standard crowding method will result in noticeable genetic drift.
Deterministic Crowding
In deterministic crowding [24], two parents P1 and P2 generate two children C1
and C2. For some distance metric D, if D(P1, C1) + D(P2, C2) is smaller than
D(P1, C2)+D(P2, C1), then introduce competition between P1 and C1, P2 and
C2, else compete between P1 and C2, P2 and C1. The better solutions survive
in the competitions. Deterministic crowding is a special standard crowding
strategy where CF equals 2.
Restricted Tournament Selection
Restricted tournament selection is a variant of tournament selection [14]. For
each child C, choose WS (window size) individuals from the parent population

302
Ling et al.
and select the nearest parent P for competition according to some distance
metric. If P is better than C, preserve it, else preserve C. This tournament
prevents an individual from competing with individuals of diﬀerent niches.
Multi-population
In multi-population EAs, the population is partitioned into a set of islands
where isolated EAs are executed. Sparse exchanges of individuals are per-
formed among these islands with the goal of introducing some diversity into
the subpopulations. Common operators to link subpopulations are exchange
and migration [23].
The multinational EA borrows the concept of international relationships.
Each subpopulation is regarded as a nation. Operations to link nations are
migration and merging [38].
Forking and Dynamic Division
The Forking Operator is an extension of multi-population EAs [37]. For the
population of EAs, if certain convergence conditions are satisﬁed, fork the
population into a parent population and a child population. In the parent
population, evolution continues in one part of the search space. In the child
population, evolution continues inside the other part of the search space.
Dynamic division is similar to the forking method. In the dynamic di-
vision algorithm, subpopulations are divided under given criteria, until the
convergence of all subpopulations is achieved [6].
Cellular
Compared with the multi-population method, the concept of neighborhood
is intensively used in cellular EAs. This means that an individual may only
interact with its nearby neighbors. The induced slow migration of solutions
through the population provides a kind of exploration, while exploitation takes
place inside each neighborhood by genetic operations [1,28].
13.2.3 Adaptive Niche Radius
To tackle the niche radius problem, several adaptive algorithms have been
proposed to adjust the niche radius dynamically. Adaptive niching methods
estimate landscape characteristics from population data. The main problem
in adaptive niching methods is how to set adaptive niching rules. Complicated
rules are diﬃcult to realize while simple rules are unable to extract enough
information to form stable subpopulations.

13 Multimodal Function Optimization of VLSHG
303
Dynamic Niche Clustering
Gan combined clustering and ﬁtness sharing and proposed a dynamic niche
clustering algorithm [8–10]. For each generation, a clustering operation cal-
culates the adaptive niche radius with a dynamically estimated number of
clusters. Within a cluster, ﬁtness sharing is implemented with the niche ra-
dius provided by clustering. Dynamic niche clustering does not require any
prior knowledge of the objective function. But the estimation of number of
clusters will aﬀect the quantity and quality of identiﬁed optimal solutions
greatly.
Restricted Evolution
Restricted evolution (RE) is an adaptive version of restricted competition
selection [16]. In each generation, the evolution range of each individual is
dynamically expanded or shrunk with a scaling coeﬃcient. The maximum,
minimum, and initial evolution ranges are predetermined, as well as the scal-
ing coeﬃcient. Strictly speaking, restricted evolution is just a half-adaptive
method. But it is favored for practical optimization problems because of its
relatively simple control rules [20].
13.3 Restricted Evolution Evolution Strategy
The procedures of the restricted evolution evolution strategy (REES) are in-
troduced in this section. A consequent local search operator is applied based
on identiﬁed solutions, as a modiﬁcation to improve ﬁne search ability.
13.3.1 Procedures of REES
The main ideas in REES are:
1. separating the search space to several subspaces;
2. adapting the radii of subspaces to accommodate the ﬁtness landscape;
3. introducing competition between subspaces to locate better optima;
4. using an ES as the local search operator within each subspace.
The procedures of REES are stated as follows [16]:
1. Initialization of evolution range: ai is the evolution range for the ith design
variable. If the ith design variable is vi, child generation of the ES is
generated within [vi −ai, vi + ai]. amin,j, amax,j and ainit,j are minimum,
maximum and initial value of ai, respectively.
2. Initialization of elite set: select μ solutions as the initial elite set, assuring
that each solution is not in the evolution range of any other solutions.
Given two solutions P1 and P2, P2 is in the evolution range of P1 means
that the Euclidean distance of each design variable of P1 and P2 is smaller
than the evolution range of the corresponding design variable of P1.

304
Ling et al.
3. Restricted evolution with ES: for each individual in the elite set, run a
(1+λ) ES [26] in its evolution range to ﬁnd μ solutions.
4. Adaptation of evolution range: for each individual in the elite set, if the
solution of (1+λ) ES improves its ﬁtness value, then increase all evolution
ranges of its design variables with a factor PRE; else decrease all evolution
range of its design variables with a factor 1/PRE, where PRE > 1.
5. Validation of new solutions: delete new solutions which are in the other
solutions’ evolution range and with worse ﬁtness value. The number of
deleted solutions is denoted by ξ.
6. Competition of solutions: generate ξ +ρ new solutions in the whole search
space randomly, assuring that they are outside of evolution ranges of ex-
isting elite solutions. During the random generation process, they should
not invade evolution ranges of other solutions too. Then select the best μ
individuals to construct the new elite set, from the remainder individuals
in the elite set and the new generated ξ + ρ solutions.
7. Stopping criterion: repeat 3–6 until the maximum generation MaxGenRE
is reached.
13.3.2 Procedures of ES
The procedures of a (1+λ) ES are stated as follows [26]:
1. Initialization of the elite individual: initialize a random individual in the
search space as the elite.
2. Mutation of the elite individual: generate λ children from the elite using
Gaussian mutation, with mutation variance MutV ar.
3. Adaptation of the mutation variance: if the proportion of improved chil-
dren is higher than a given threshold, for example, 1/5, then increase
MutV ar with a factor PES; else decrease MutV ar with a factor 1/PES,
PES > 1.
4. Update of the elite individual: choose the best one as the new elite among
the original elite and generated children.
5. Stopping criterion: repeat 2–4, until the maximum generation MaxGenES
reaches.
13.3.3 Complexity of REES
The order of complexity is O(μ2) in Step 5 and O((μ+ρ)2) in Step 6 of REES.
It is not large for small μ and ρ. Main runtime comes from the evaluation
of objective function. In each generation, the algorithm evaluates objective
function for μ + ρ + ξ + μ × λ × MaxGenES times. Setting MaxGenES as a
small number, the consumption of function evaluation is limited, especially
when μ, i.e. the number of solutions we want to obtain, is not too large.

13 Multimodal Function Optimization of VLSHG
305
13.3.4 Modiﬁcation of REES
REES works well in locating multiple niches. If λ and MaxGenES of the ES
are set to a large number, the accuracy of located optima will be satisfactory
while evaluation times of the objective function will increase sharply, as seen
from the complexity analysis of the algorithm. Thus we introduce a consequent
simplex method as the local search operator which uses identiﬁed solutions
as starting points. It is expected that with the local search heuristic, each
solution converges to the nearest local optimum. Modiﬁed REES maintains
the ability of locating multiple niches, and improves the accuracy of solutions.
For comparison convenience, niching techniques discussed in Sect. 13.4
are all followed by the simplex method as a consequent local search operator.
Therefore, accuracy of the multi-optimum optimization is not the issue.
13.3.5 Parameter Settings of REES
According to the discussion above, we conclude several experimental rules for
the parameter settings of REES:
1. The minimum, maximum, and initial value of evolution range amin,j,
amax,j and ainit,j for each design variable ai aﬀect the formation of sub-
populations. Proper settings of these parameters are related to the prior
knowledge of ﬁtness landscape. Generally speaking, small minimum evolu-
tion range leads to extensive ﬁne search, thus is unnecessary if the conse-
quent local search operator is appended. Maximum evolution range should
be large enough to cover the whole search space, but too large maximum
evolution range results in too much competition between niches, thus de-
stroy stable evolution of the population. Initial evolution range is not im-
portant because of the half-adaptive mechanism of REES. For the same
reason, settings of evolution range parameters are not stringent, which is
diﬀerent from the niche radius problem.
2. Size of the elite set μ denotes the expected number of optima. Trade-oﬀ
should be made between the multimodal optimization performance and
the computation overload.
3. Appropriate choice of the number of extra competition individuals ρ will
improve the quality of the whole population. But too large ρ may obstacle
stable convergence of the population.
4. Setting of λ in the (1+λ) ES also needs a trade-oﬀ. Large λ improves the
quality of solutions, but increases the computation overhead.
5. The mutation variance MutV ar denotes the expected search space for
each (1+λ) ES. We set them as the current evolution range of design
variables.
6. The adaptive parameter PRE in RE reﬂects the expected convergence
rate of solutions. Large PRE, i.e. large expected convergence rate, results
in unstable population, while small PRE leads to low quality population.

306
Ling et al.
Similarly, the adaptive parameter PES in ES reﬂects the expected conver-
gence rate of each ES search process. We set both PRE and PES as 1.2
through this chapter.
7. Generation numbers MaxGenRE and MaxGenES are almost proportional
to the computation time. But preliminary experiments indicate that large
MaxGenES is unnecessary, because the REES framework may tolerate the
relatively low accuracy of each ES search process.
13.4 Numerical Experiments
The modiﬁed REES algorithm is compared with other niching techniques in
two test functions, according to the quality and quantity of eﬀective optimal
solutions.
13.4.1 Performance Criteria
Evaluation of the performance of multi-optimum search depends on the en-
gineering requirements. Here we use two performance criteria: niche number
(NN) and niche objective (NO). In this chapter, a local optimum is considered
to be identiﬁed if at least one solution converges to it after the consequent
local search operator.
Niche Number NN
At the end of the optimization process, the niche number is deﬁned as the
number of identiﬁed local optima which reside in the feasibility region. NN
characterizes the quantity of eﬀective solutions.
Niche Objective NO
The niche objective is the sum of the objective value of local optima identiﬁed
by the niching technique. NO characterizes the quality of eﬀective solutions.
13.4.2 Test Functions
Two two-dimensional test functions are used in this chapter. The ﬁrst one has
a simple ﬁtness landscape [20]:
min
x1,x2
f1(x1, x2) =
2
i=1
−xi × cos(2π(xi −5))
s.t.
0 ≤xi ≤9.2,
i = 1, 2.
The second test function has a relatively rugged ﬁtness landscape, known as
the Ripple function [36]:

13 Multimodal Function Optimization of VLSHG
307
min
x1,x2
f2(x1, x2) =
2
i=1
{( 1
4)( xi−0.1
0.8
)2 × [sin(5πxi)6 + cos(500πxi)2
10
]}
s.t.
0 ≤xi ≤1,
i = 1, 2
13.4.3 Comparison in Test Functions
REES is compared with three niching methods: standard crowding where
crowding factor CF equals to 2 (SCA), standard crowding where crowding
factor CF equals to population number (SCB), and deterministic crowding
(DC). For the convenience of comparison, these methods are all followed by
a consequent simplex search. Parameters of each algorithm are listed in Ta-
ble 13.1.
Table 13.1. Algorithm parameters
Algorithm Parameters
REES
MaxGenRE = 15, μ = 40, ρ = 10, λ = 5, MaxGenES = 2
SCA
maximum generation = 100, population number = 40, CF = 2
SCB
maximum generation = 100, population number = 40, CF = 40
DC
maximum generation = 100, population number = 40
Execute the four algorithms 100 times for each test function, compute
the mean value and standard variance of performance criteria, as shown in
Table 13.2 and Table 13.3. For both test functions, REES and SCB can identify
nearly 40 local minima, which is close to the expected number of identiﬁed
optima. In the simple ﬁtness landscape of f1, REES and SCB have similar
search ability. REES is better than SCB in niche objective NO, and slightly
worse than SCB in niche number NN. In the complicated ﬁtness landscape
of f2, REES outperforms SCB in both niche number NN and niche objective
NO. The main reason is that the standard crowding algorithm has diﬃculty
overcoming genetic drift in rugged ﬁtness landscapes. Therefore individuals
converge to a few eminent solutions along with generations. The other two
algorithms, SCA and DC, both fail in identifying enough local optima, because
of the inﬂuence of selection error and genetic drift.
Table 13.2. Mean value (MEAN) and standard variance (STD) of niche number
NN and niche objective NO of four algorithms for f1
Group MEAN of NN STD of NN MEAN of NO STD of NO
1
38.1500
1.1315
−449.0278
16.4659
2
2.2400
0.4522
−28.7402
6.8227
3
39.3600
0.8229
−394.4150
24.3628
4
11.6100
2.6927
−153.3006
33.8637

308
Ling et al.
Table 13.3. Mean value (MEAN) and standard variance (STD) of niche number
NN and niche objective NO of four algorithms for f2
Group MEAN of NN STD of NN MEAN of NO STD of NO
1
31.8300
2.0521
−41.8183
2.0168
2
2.0600
0.3712
−3.5912
0.6755
3
24.5100
1.6361
−35.7460
2.1747
4
5.5200
1.3961
−10.5463
2.6811
13.5 Practical Engineering Optimization
In this section, the modiﬁed REES algorithm is applied to the practical opti-
mal design of recording optics of the varied-line-spacing holographic grating
(VLSHG) in the National Synchrotron Radiation Laboratory (NSRL).
13.5.1 Recording Optics of VLSHG
Holographic gratings are fabricated by recording interference fringes of two
coherent sources in the photoresist coated on grating blanks. Given the shape
of blanks, their focal properties can be adjusted by altering the properties
of two recording light sources. Ling [19] introduced auxiliary uniform-line-
spacing gratings to generate aspherical wave-fronts to fabricate varied-line-
spacing holographic gratings. VLSHGs are able to correct high order aber-
rations in diﬀractive optical systems and are widely used in high resolution
spectrometers and monochromators.
The recording optical system is shown in Fig. 13.1. It consists of two
coherent point light sources of wavelength λ0, C and D, and two uniform-
line-spacing gratings, G1 and G2. Four distance parameters and four angle
parameters, pC, qC, pD, qD, γ, ηC, δ, ηD, are known as recording parame-
ters [19].
The aim of optimal design is to ﬁnd a set of recording parameters to form
the expected groove shape, which is generally represented by the expected
groove density nE(w) in the Y axis:
nE(w) = n0E(1 + b2Ew + b3Ew2 + b4Ew3),
−w0 ≤w ≤w0.
(13.3)
Here n0E, b2E, b3E, b4E are expected groove density parameters, w is the
coordinate in the Y axis, and w0 is the half-width of the grating to record.
For a given set of recording parameters, the practical groove density nP (w)
can be calculated from a complicated function g of recording parameters [19]:
nP (w) = g(pC, qC, pD, qD, γ, ηC, δ, ηD).
(13.4)
We formulate the objective function as minimizing the integral of square errors
of the expected groove density and the expected groove density in the Y
axis [20]:

13 Multimodal Function Optimization of VLSHG
309
min
f =
w0

−w0
(nP (w) −nE(w))2dw.
(13.5)
We evaluate (7) by numerical integral with proper step size. The objective
function for minimization is multidimensional and multimodal, and its evalu-
ation is time-consuming.
G
G1
Y1
Z1
O1
G2
Y2
Z2
O2
Z
Y
X
C(xC, yC, 0) 
PC
X1
ȘC
O
ȗC
Ȗ
X2
D(xD, yD, 0) 
į
ȗC
ȘC
Ȝ0
Ȝ0
Q2(0, w2, t2)
Q1(0, w1, t1)
P(0, w, t) 
Fig. 13.1. Schematic diagram of the recording system consists of two coherent point
sources, C and D, two auxiliary uniform-line-spacing gratings, G1 and G2, and a
plane grating blank G. Recording parameters are four distance parameters and four
angle parameters
13.5.2 Multimodal Optimal Design of VLSHG
The optimal design of VLSHG is a typical multi-optimum search problem
with a multimodal function. The main reason for multi-optimum search is
the requirement to handle implicit constraints. There are two kinds of con-
straints, explicit and implicit, in the optimization process. Firstly, recording
parameters must satisfy the upper and lower bound constraints. These kinds
of explicit constraints come from the restriction of the size of recording ta-
ble. Secondly, in the practical recording system, there are many other optical
elements not shown in the schematic diagram, such as beam splitter, light
ﬁlter, et cetera. All these optical elements must do not disturb each other.
For the ease of maintenance, it is also expected that they are evenly placed in

310
Ling et al.
the recording table. Furthermore, improper recording parameters may bring
on extra demand of auxiliary optical elements, or extra requirement of ﬁne
calibration. These kinds of implicit constraints are diﬃcult or inconvenient to
be described in a closed mathematical formulation.
To tackle the implicit constraint problem, we adopt the following optimal
design procedures:
1. Construct the objective function and explicit constraints, and search for
multiple solutions with the multimodal optimization algorithm.
2. Draw schematic diagrams for solutions, and exclude those possibly vio-
lating implicit constraints. Notice that with the consequent local search
operator, some solutions may violate explicit constraints, and must be
excluded too.
3. Evaluate concrete design schemes for the qualiﬁed solutions with the aid
of optical manufacturing engineers, select a ﬁnal solution, and keep several
proper solutions as alternatives in case of unexpected diﬃculties.
The multimodal optimization algorithm plays an important role in our op-
timal design procedures for both satisfying implicit constraints and providing
backup solutions. Considering the complexity of the ﬁtness landscape for the
VLSGH design, ﬁnding the global optimum is almost an impossible task. Thus
the objective of optimization is to ﬁnd several groups of acceptable solutions,
and the main concern of the multi-optimum optimization algorithm is the
diversity of solutions. According to numerical experiments in Sect. 13.4, the
modiﬁed REES algorithm is a proper choice for the VLSHG design because
of its capability of multi-optimum search in a complicated ﬁtness landscape.
13.5.3 Modiﬁed REES for VLSHG Design
The VLSHG to design in NSRL has desired groove density parameters:
n0E
= 1.4000 × 103 groove/mm, b2E
= 8.2453 × 10−4 mm−1, b3E
=
3.0015×10−7 mm−2, b4E = 0.0000×10−10 mm−3. Half width of the grating is
w0 = 90 mm and the recording wavelength is λ0 = 413.1 nm. Auxiliary grating
G2 has groove density n2 = 1.0000×103 groove/mm and the diﬀraction order
equals to +1. Auxiliary grating G1 has groove density n1 = 0 groove/mm,
i.e. a plane mirror. Upper bounds and lower bounds of recording parameters
are from the physical constraints of recording table. For distance parameters,
upper bounds are 2000 mm, lower bounds are 100 mm. For angle parameters,
upper bounds are π/2, lower bounds are −π/2.
To optimize the objective function with modiﬁed REES, the algorithm pa-
rameters are: MaxGenRE = 30, μ = 40, ρ = 20, λ = 20, MaxGenES = 2. Four
sets of feasible recording parameters are shown in Table 13.4. Corresponding
groove parameters are shown in Table 13.5, denoted as n0, b2, b3, and b4.
Corresponding objective functions are also provided, denoted as f. Schemes
of the solutions are shown in Fig. 13.2.

13 Multimodal Function Optimization of VLSHG
311
Table 13.4. Four sets of feasible recording parameters
Group
γ(rad) ηC(rad) δ(rad) ηD(rad) pC(mm) qC(mm) pD(mm) qD(mm)
1 −0.0999
0.0999 0.4990
1.1427
574.4
752.0
427.8
989.9
2 −0.5970
0.5970 0.0156
1.2673
510.4
316.4
254.7
808.1
3
0.3865 −0.3865 1.2709
0.6494
782.5
710.6
112.4
1451.4
4 −0.1748
0.1748 0.4164
0.9876
510.0
518.0
520.9
380.8
Table 13.5. Corresponding groove parameters and objective functions of the opti-
mized recording parameters
Grp n0(groove/mm)
b2(mm−1)
b3(mm−2)
b4(mm−3)
f
1
1.3999 × 103 8.2457 × 10−4 3.0028 × 10−7 −0.0005 × 10−10 9.9202 × 10−3
2
1.3999 × 103 8.2472 × 10−4 3.0047 × 10−7 −0.0007 × 10−10 9.8671 × 10−3
3
1.4000 × 103 8.2440 × 10−4 3.0017 × 10−7 −0.0002 × 10−10 8.5979 × 10−5
4
1.4001 × 103 8.2453 × 10−4 3.0017 × 10−7
0.0002 × 10−10 1.0050 × 10−2
13.5.4 Discussion of Solutions
Before discussion of the optimized solutions, we need to clarify several issues
regarding the practical recording optics:
1. Though we refer to two light sources C and D in the schematic dia-
gram in Fig. 13.1 and previous discussions, it is obligatory to use a sin-
gle laser generator to generate the two light sources. Because the coher-
ence of light sources can not be guaranteed by using two laser generators.
Beam splitters and light ﬁlters are applied to generate two coherent light
sources, as shown in the schematic diagram in Fig. 13.3. A plane wave
from light source S reaches the beam splitter BS, i.e. a half-transparent-
half-reﬂecting mirror, and is split to two plane waves. The two plane waves
are ﬁltered by light ﬁlters LF1 and LF2, and transformed to two spherical
waves, i.e. point light sources C and D. Furthermore, directions of the
principle rays from C and D are adjusted by using extra plane mirrors.
Therefore, a proper solution of the recording optics needs to leave appro-
priate room for the placement of beam splitters, light ﬁlters, and other
auxiliary optical elements. On the other hand, large distance between C
and D requires long light distance before the plane waves reach the light
ﬁlters, thus we need to adjust beam splitters and light ﬁlters carefully to
guarantee the coherence and parallelism of the two plane waves.
2. Adjustment of the distance and angle parameters are always challenging in
the recording optics. Generally we require the largest tolerance is 1 mm for
distance parameters and 0.001 rad for angle parameters. Proper recording
parameters are able to mitigate the burden of system adjustment. For
example, if the lengths of the incident arm and the emergent arm are not

312
Ling et al.
G
C
G2
D
(a)
G
C
G2
D
(b)
G
C
G2
D
(c)
G
C
G2
D
(d)
Fig. 13.2. Schemes of the optimized solutions, with only the +1 order diﬀractive
light shown. Here (a)–(d) are corresponding to Groups 1–4 of Table 13.2
in the same magnitude, adjustment error in the shorter arm will introduce
a much larger system error than that of the longer arm. Regarding the
angle parameters, grazing incident ray, i.e. incidence angle near to π/2,
should be avoided.
3. The auxiliary plane gratings will generate multiple orders of diﬀractive
rays. The orders of diﬀractive rays depend on the recording parameters.
Generally diﬀractive rays from order −5 to order +5 are observable. Note
that we only need order +1 for the recording, thus other diﬀractive rays

13 Multimodal Function Optimization of VLSHG
313
may interfere the recording process. One solution to this problem is to use
screens to absorb other lights. Another and more preferred solution is to
choose a proper set of recording parameters such that the lights of other
orders will not reach the grating G.
S
C
LF1
D
BS
LF2
Fig. 13.3. A plane wave from light source S reaches the beam splitter BS, and is
split to two plane waves. The two plane waves are ﬁltered by light ﬁlters LF1 and
LF2, and transformed to two spherical waves C and D
The recording parameters Group 1 and Group 2 of Table 13.2 are both
proper solutions. Group 2 is slightly inferior to Group 1 because of the small
imbalance between the incident arm pD and emergent arm qD, with a ratio
of 1/3.
In Group 3, the incident arm pD is much smaller than the other arms.
This magnitude of imbalance is intolerable for optical system calibration. We
can use a predetermined threshold to penalize this kind of solution, but it will
introduce twelve extra inequality constraints for four distance parameters, and
result in a more complicated optimization problem. For recording parameters
in Group 4, light sources C and D are too far, thus the plane waves have
to travel a long distance before reach the light ﬁlters. Therefore the laser
generator should have a high stability to guarantee the parallelism of light
sources. Furthermore, the optics are exposed in the air, thus the coherence is
vulnerable to stray lights and vibrations. Large light distance will increase the
risk of incoherence of the two plane waves. Hence Group 4 sacriﬁces the system
reliability comparing to Group 1 and Group 2. Though Group 3 and Group
4 are feasible solutions, they are not proper solutions from the viewpoint
of practical fabrication. Noting that Group 3 is the best one regarding the
objective function value, this fact underlines the necessity of the multimodal
optimization algorithm and the usefulness of our optimal design procedures.

314
Ling et al.
1326.4mm
5.72°
X
Y
G
C
989.9mm
X2
Y2
G2
427.8mm
1st
0th
2nd
29.78°
65.47°
D
Fig. 13.4. Final recording optics adopts the optimized recording parameters in
Group 1 of Table 13.2. Light originated from D is diﬀracted at auxiliary grating
G2, and the +1 order diﬀractive light reaches G. Light originated from C reaches G
directly because the auxiliary plane mirror G1 can be omitted. The other diﬀractive
lights will not disturb the recording process when recording parameters are properly
selected
We ﬁnally leave Group 2 as a backup solution, and select Group 1 as the
practical recording parameters, as shown in Fig. 13.4. Note that by properly
selecting recording parameters, the other diﬀractive lights will not disturb the
recording process while only the +1 order diﬀractive light reaches G.
13.6 Conclusion
In this chapter, we focus on an important kind of optimization problem for
practical applications: multimodal function optimization, i.e. a search for mul-
tiple optimal solutions. A survey of the main niching techniques for multi-
optimum search is given. According to the method of dealing with the niche
radius problem, niche techniques are classiﬁed as explicit, implicit, and adap-
tive methods.
A speciﬁed adaptive niche radius method, REES, is selected to demon-
strate the usefulness of multi-optimum search in a practical engineering de-

13 Multimodal Function Optimization of VLSHG
315
sign. To improve the local search ability, a consequent local search operator
is applied to modify the original REES. Numerical experiments indicate that
REES outperforms the standard crowding algorithm and the deterministic
crowding algorithm, regarding both quantity and quality of solutions.
The characteristics of REES are ﬁt for our practical multi-optimum design
problem, i.e. VLSHG design. In the VLSHG design problem, various implicit
constraints, which are diﬃcult or inconvenient to be described in a closed
mathematical formulation, emerge from the requirement of practical fabrica-
tion. To cope with implicit constraints, multiple solutions are expected for
trials by optical engineers. The modiﬁed REES algorithm is applied to the
practical design of a VLSHG in NSRL, and achieves satisfactory results.
References
1. Alba, E., Dorronsoro, B.: The exploration/exploitation tradeoﬀin dynamic cel-
lular genetic algorithms. IEEE Transactions on Evolutionary Computation 9,
126–142 (2005)
2. Beasley, D., Bull, D., Martin, R.: A sequential niche technique for multimodal
function optimization. Evolutionary Computation 1, 101–125 (1993)
3. Cedeno, W., Vemuri, V.: Analysis of speciation and niching in the multi-niche
crowding GA. Theoretical Computer Science 299, 177–197 (1999)
4. Crutchley, D., Zwolinski, M.: Globally convergent algorithm for DC operating
point analysis of nonlinear circuits. IEEE Transactions on Evolutionary Com-
putation 7, 2–10 (2003)
5. Deb, K., Goldberg, D.: An investigation of niche and species formation in genetic
function optimization. In: Proceedings of International Conference on Genetic
Algorithms, pp. 42–50 (1989)
6. Elo, S.: A parallel genetic algorithm on the CM-2 for multi-modal optimization.
In: Proceedings of International Conference on Evolutionary Computation, pp.
818–822 (1994)
7. Francois, O.: An evolutionary strategy for global minimization and its Markov
chain analysis.
IEEE Transactions on Evolutionary Computation 2, 77–90
(1998)
8. Gan, J., Warwick, K.: A genetic algorithm with dynamic niche clustering for
multimodal function optimization. In: Proceedings of International Conference
on Artiﬁcial Neural Network and Genetic Algorithms, pp. 248–255 (1999)
9. Gan, J., Warwick, K.: Dynamic niche clustering: a fuzzy variable radius nich-
ing technique for multimodal optimisation in GAs. In: Proceedings of IEEE
Congress on Evolutionary Computation, pp. 215–222 (2001)
10. Gan, J., Warwick, K.: Modelling niches of arbitrary shape in genetic algorithms
using niche linkage in the dynamic niche clustering framework. In: Proceedings
of IEEE Congress on Evolutionary Computation, pp. 43–48 (2002)
11. Goldberg, D.: Sizing populations for serial and parallel genetic algorithms.
In: Proceedings of International Conference on Genetic Algorithms, pp. 70–79
(1989)
12. Goldberg, D., Richardson, I.: Genetic algorithms with sharing for multimodal
function optimization. In: Proceedings of International Conference on Genetic
Algorithms, pp. 41–49 (1987)

316
Ling et al.
13. Gomez, J.: Self adaptation of operator rates for multimodal optimization. In:
Proceedings of IEEE Congress on Evolutionary Computation, pp. 1720–1726
(2004)
14. Harik, G.: Finding multimodal solutions using restricted tournament selection.
In: Proceedings of International Conference on Genetic Algorithms, pp. 24–31
(1995)
15. Hocaoglu, C., Sanderson, A.: Planning multiple paths with evolutionary speci-
ation. IEEE Transactions on Evolutionary Computation 5, 169–191 (2001)
16. Im, C., Kim, H., Jung, H., Choi, K.: A novel algorithm for multimodal function
optimization based on evolution strategy. IEEE Transactions on Magnetics 40,
1224–1227 (2004)
17. Lee, C., Cho, D., Jun, H., Lee, C.: Niching genetic algorithm with restricted
competition selection for multimodal function optimization. IEEE Transactions
on Magnetics 34, 1722–1725 (1999)
18. Li, J., Blazs, M., Parks, G., Clarkson, P.: A species conserving genetic algorithm
for multimodal function optimization. Evolutionary Computation 10, 207–234
(2002)
19. Ling, Q., Wu, G., Liu, B., Wang, Q.: Varied line spacing plane holographic
grating recorded by using uniform line spacing plane gratings. Applied Optics
45, 5059–5065 (2006)
20. Ling, Q., Wu, G., Wang, Q.: Restricted evolution based multimodal function
optimization in holographic grating design. In: Proceedings of IEEE Congress
on Evolutionary Computation, pp. 789–794 (2005)
21. Liu, X., Lee, B.: Optimal design for ultra-broad-band ampliﬁer.
Journal of
Lightwave Technology 21, 3446–3454 (2003)
22. Liu, X., Lee, B.: Optimal design of ﬁber Raman ampliﬁer based on hybrid genetic
algorithm. IEEE Photonics Technology Letters 16, 428–430 (2004)
23. Logot, G., Walter, M.: Computing robot conﬁgurations using a genetic algorithm
for multimodal optimization. In: Proceedings of International Conference on
Evolutionary Computation, pp. 312–317 (1998)
24. Mahfoud, S.: Crossover interactions among niches. In: Proceedings of Interna-
tional Conference on Evolutionary Computation, pp. 188–193 (1994)
25. Mahfoud, S.: Genetic drift in sharing methods. In: Proceedings of International
Conference on Evolutionary Computation, pp. 67–71 (1994)
26. Michalewicz, Z.: Genetic Algorithms + Data Structures = Evolution Programs.
Springer, Berlin (1996)
27. Miller, B., Shaw, M.: Genetic algorithms with dynamic niche sharing for mul-
timodal function optimization. In: Proceedings of International Conference on
Evolutionary Computation, pp. 786–791 (1996)
28. Nakashima, T., Ariyama, T., Yoshida, T., Ishibuchi, H.: Performance evalua-
tion of combined cellular genetic algorithms for function optimization problems.
In: Proceedings of International Symposium on Computational Intelligence in
Robotics and Automation, pp. 295–299 (2003)
29. Nijssen, S., B¨ack, T.: An analysis of the behavior of simpliﬁed evolutionary
algorithms on trap functions. IEEE Transactions on Evolutionary Computation
7, 11–22 (2003)
30. Petrowski, A.: A clearing procedure as a niching method for genetic algorithms.
In: Proceedings of International Conference on Evolutionary Computation, pp.
798–803 (1996)

13 Multimodal Function Optimization of VLSHG
317
31. Rudolph, G.: Self-adaptive mutations may lead to premature convergence. IEEE
Transactions on Evolutionary Computation 5, 410–414 (2001)
32. Sareni, B., Krahenbuhl, L.: Fitness sharing and niching methods revisited. IEEE
Transactions on Evolutionary Computation 2, 97–106 (1998)
33. Sareni, B., Krahenbuhl, L., Nicolas, A.: Niching genetic algorithms for optimiza-
tion in electromagnetics I – fundamentals. IEEE Transactions on Magnetics 34,
2984–2991 (1998)
34. Sareni, B., Krahenbuhl, L., Nicolas, A.: Eﬃcient genetic algorithms for solving
hard constrained optimization problems. IEEE Transactions on Magnetics 36,
1027–1030 (2000)
35. Singh, G., Deb, K.: Comparison of multi-modal optimization algorithms based
on evolutionary methodologies. In: Proceedings of the Genetic and Evolutionary
Computation Conference (GECCO-2006), pp. 1305–1312. The Association for
Computing Machinery (ACM), New York (2006)
36. Thomsen, R.: Multimodal optimization using crowding based diﬀerential evo-
lution. In: Proceedings of IEEE Congress on Evolutionary Computation, pp.
1382–1389 (2004)
37. Tsutsui, S., Fujimoto, Y., Hayashi, I.: Extended forking genetic algorithm for
order representation (o-fGA). In: Proceedings of International Conference on
Computational Intelligence, pp. 639–644 (1994)
38. Ursem, R.: Multinational evolutionary algorithms.
In: Proceedings of IEEE
congress on Evolutionary Computation, pp. 1633–1640 (1999)
39. Yin, X., Germay, N.: A fast genetic algorithm with sharing scheme using cluster
analysis methods in multimodal function optimization. In: Proceedings of In-
ternational Conference on Artiﬁcial Neural Networks and Genetic Algorithms,
pp. 450–457 (1993)

14
GPBG: A Framework for Evolutionary Design
of Multi-domain Engineering Systems Using
Genetic Programming and Bond Graphs
Jianjun Hu1, Zhun Fan2, Jiachuan Wang3, Shaobo Li4, Kisung Seo5,
Xiangdong Peng6, Janis Terpenny7, Ronald Rosenberg8, and Erik
Goodman9
1 Department of Computer Science and Engineering, University of South Carolina,
SC 29208, USA jianjunh@cse.sc.edu
2 Department of Mechanical Engineering, Technical University of Denmark,
Building 404, DK-2800 Lyngby, Denmark zf@mek.dtu.dk
3 United Technologies Research Center, Systems Department, East Hartford, CT
06108, USA wangj2@utrc.utc.com
4 CAD/CIMS Institute, Guizhou University, Guiyang, Guizhou 550003, China
lishaobo@gzu.edu.cn
5 Department of Electronics Engineering, Seokyeong University, Seoul 136-704,
Korea ksseo@skuniv.ac.kr
6 Department of Electrical and Computer Engineering, Michigan State University,
East Lansing, MI 48824, USA pengxian@egr.msu.edu
7 Department of Engineering Education, Virginia Tech, Blacksburg, VA 24061,
USA terpenny@vt.edu
8 Department of Mechanical Engineering, Michigan State University, East
Lansing, MI 48824, USA rosenber@egr.msu.edu
9 Department of Electrical and Computer Engineering, Michigan State University,
East Lansing, MI 48824, USA goodman@egr.msu.edu
14.1 Introduction
Current engineering design is a multi-step process proceeding from conceptual
design to detailed design and to evaluation and testing. It is estimated that
60–70% of design decisions and most innovation occur in the conceptual design
stage, which may include conceptual design of function, operating principles,
layout, shape, and structure. However, few computational tools are available
to help designers to explore the design space and stimulate the product in-
novation process. As a result, product innovation is strongly constrained by
the designer’s ingenuity and experience, and a systmatic approach to product
innovation is strongly needed.
Many engineering design problems, such as mechatronic systems, can be
abstracted as a design space exploration problems in which a set of building

320
Hu et al.
blocks or modules need to be assembled/connected together to compose a
system satisfying a set of given functional requirements. In many cases, such as
analog circuit design, it is relatively easy to simulate a product design model to
evaluate its functional performance via simulation software such as P-SPICE,
while it is extremely hard to come up with an innovative design solution out
of the almost unlimited number of design candidates of the topological design
space. An eﬃcient topological search technique is needed to help to improve
this process.
In recent decades, evolutionary computation has emerged as an eﬀective
and promising search/optimization technique that is suitable for large-scale
non-linear multi-modal engineering optimization problems. In particular, ge-
netic programming (GP) has been used as an attractive approach for engineer-
ing design innovation in a variety of domains, including design of analog cir-
cuits, digital circuits, chemical molecules, control systems, et cetera [12]. Such
work employs GP as a topologically open-ended search technique for func-
tional design innovation – achieving given behaviors without pre-specifying
the design topology – and has achieved considerable success. While electrical
circuits and block diagrams are well suited for the design problems in ana-
log circuit design and controller synthesis, many engineering design problems
cover multiple domains, including, for example, mechanical, electrical, and
hydraulic subsystems. Since 2001, we have been developing a new framework,
called GPBG [19], for automated synthesis of multi-domain systems using ge-
netic programming and bond graphs [8], which are a well-established modeling
tool for multi-domain systems.
In this chapter, we will detail how an engineering design problem can be
solved under the GPBG framework using several design synthesis problems:
a vibration absorber, a MEMS ﬁlter design, and a controller design in sus-
pension systems. The rest of the chapter is organized as follows. Section 14.2
presents a survey of applications of evolutionary algorithms in engineering
design synthesis that are more than just parameter optimization. Section 14.3
introduces the GPBG framework, which exploits genetic programming and
bond graphs for automated synthesis of dynamic systems. In Sect. 14.4, ﬁrst,
the vibration absorber design problem is used to illustrate the basic approach
to mapping the engineering design problem into a topology space search prob-
lem using genetic programming. We then use a MEMS ﬁlter design problem
to show how expert domain knowledge can be incorporated into the evolution-
ary synthesis and greatly improve the eﬃciency of this approach. The third
application uses a diﬀerent, direct-encoding GPBG approach for synthesizing
controllers of suspension systems. A brief summary of evolving robust designs
is then presented. Finally, the conclusions and future research are highlighted
in Sect. 14.5.

14 GPBG: A Framework for Evolutionary Synthesis
321
14.2 Related Work
Automated synthesis of dynamic systems has been investigated intensively in
the past ten years. Most of that work is related to analog circuit synthesis,
as pioneered by Koza and his colleagues [10, 12]. Their work in automated
analog circuit synthesis, including low-pass, high-pass, and asymmetric band-
pass ﬁlters, is described in [10,11]. Lohn and Colombano [14] proposed a linear
representation approach to evolve analog circuits. Ando and Iba [1] suggested
another simple linear genome method to evolve low-pass and band-pass ﬁlters
with small numbers (<50) of components. In our previous work, we applied
GP to the lowpass analog ﬁlter design problem [2], MEMS [3], the printer
mechanism design, active-passive dynamic system design [22], all using bond
graphs as the modeling and simulation tool. Controllers, or dynamic systems
represented as block diagrams have also been synthesized automatically using
genetic programming by Koza et al. [13]. This work has led to the invention
of a patentable controller having better performance than a standard PID
controller.
14.3 The GPBG Framework for Evolutionary Design
In this section, we present a generic methodology for open-ended computa-
tional synthesis of multi-domain dynamic systems based on bond graphs [9]
and genetic programming – the GPBG approach = Genetic Programming +
Bond Graphs.
14.3.1 Genetic Programming
Genetic programming is a derivative of genetic algorithms that is charac-
terized by its capability to evolve programs. In typical GP, an individual is
represented as a syntax/GP tree composed of functions and terminals de-
ﬁned by the user according to the problem. Each function has one or more
inputs, while terminals have no inputs. Both functions and terminals can be
executed to generate some output or do some processing such as inserting a
new component into a developing/growing analog circuit. Genetic program-
ming’s open-ended topological search capability has been widely applied to
computational synthesis of analog circuits, controllers, mechatronic systems,
quantum circuits, et cetera.
14.3.2 Bond Graphs
The bond graph is a multi-domain modeling tool for analysis and design of
dynamic systems, especially hybrid multi-domain systems, including mechani-
cal, electrical, pneumatic, hydraulic, etc., components. Details of notation and

322
Hu et al.
methods of systems analysis related to the bond graph representation can be
found in [9]. Figure 14.1 illustrates a small bond graph that represents the
accompanying electrical system.
A typical simple bond graph model is composed of inductors (I), resis-
tors (R), capacitors (C), transformers (TF), gyrators (GY), 0-Junctions (J0),
1-junctions (J1), sources of eﬀort (SE), and sources of ﬂow (SF). In this chap-
ter, we are only concerned with linear dynamic systems and do not include
transformers and gyrators as components.
1
RS
RL
(1)
Se
0
AC
GND
RS
RL
evolved 
circuit
Fig. 14.1. A bond graph and its equivalent circuit. The dotted boxes in the left
bond graph indicate modiﬁable sites at which further topological manipulations can
be applied (to be explained in the next section)
In the context of electric circuit design, a bond graph consists of the fol-
lowing types of elements:
•
C, I, and R elements, which are passive one-port elements that contain no
sources of power, and represent capacitors, inductors, and resistors.
•
Power source elements including Se and Sf, which are active one-port el-
ements representing sources of voltage and current, respectively. In addi-
tion, when the current of a current source is ﬁxed as zero, it can serve as
an ideal voltage gauge. Similarly, when the voltage of a voltage source is
ﬁxed as zero, it can serve as an ideal current gauge.
•
Transformer (TF) and gyrator (GY), which are two-port elements. Power
(i.e., product of voltage and current) is conserved in these elements, but
the values of voltage and current may be changed in the elements.
•
0-junctions and 1-junctions, which are multi-port elements for representing
series and parallel relationships among elements. They serve to intercon-
nect elements into subsystem or system models.
•
Bonds, which are used to connect any two elements in the bond graph.
A unique characteristic of bond graphs is their use of 0- and 1-junctions
to represent the series and parallel relationships among components in cir-
cuits. In fact, it is this concept that led to the foundation of the bond graph
ﬁeld [15]. Junctions transform common circuits into a very clean structure with

14 GPBG: A Framework for Evolutionary Synthesis
323
few loops, which can otherwise make circuits appear very complicated. Fig-
ure 14.1 shows the comparison of a circuit diagram and a corresponding bond
graph. The evaluation eﬃciency of the bond graph model is further improved
due to the fact that analysis of causal relationships and power ﬂow between
elements and subsystems can reveal certain system properties and inherent
characteristics. This makes it possible to discard infeasible design candidates
even before numerically evaluating them, thus reducing time of evaluation to
a large degree. In addition, as virtually all of the circuit topologies created
are valid, our system does not need to check validity conditions of individ-
ual circuits to avoid singular situations that could interrupt the running of a
program evaluating them.
14.3.3 GPBG = GP + Bond Graphs
By combining the topological search capability of GP and the multi-domain
representation feature of bond graphs, GPBG provides an appealing approach
for open-ended synthesis of multi-domain systems. To map an engineering
design problem into the GPBG framework, the design space of target de-
sign solutions must ﬁrst be identiﬁed, including all component types, com-
ponent interfaces, and connection types. Then, depending on how we encode
the bond graphs using the GP tree, two types of approaches have been used
within GPBG framework. One is the developmental GPBG approach, similar
to Koza’s work on evolving analog ﬁlters, in which the bond graph pheno-
types are grown from an embryo bond graph by executing the GP tree pro-
gram to manipulate the topology. The other approach is the direct encoding
GPBG, in which the GP trees directly encode the bond graph topology. This
method shares some similarity to the GP-based controller synthesis approach
by Koza [13].
Developmental GPBG
The problem of automated synthesis of bond graphs involves two basic
searches – the search for a good topology and the search for good param-
eters for each topology – in order to be able to evaluate its performance.
Based on Koza’s work [10] on automated synthesis of electronic circuits, we
created a developmental GPBG system for synthesizing mechatronic systems,
including:
1. An embryo bond graph with modiﬁable sites at which further topological
operations can be applied to grow the embryo into a functional system.
2. A GP function set, composed of a set of topology manipulation and other
primitive instructions which will be assembled into a GP tree by the evo-
lutionary process (execution of this GP program leads to topological and
parametric manipulation of the developing embryo bond graph).
3. A ﬁtness function to evaluate the performance of candidate solutions.

324
Hu et al.
We use the analog ﬁlter synthesis problem as an example to illustrate
the developmental GPBG approach [2]. In this problem, the design space
consists of bond graphs composed of capacitors (C), inductors (I), resistors
(R), 1-junctions (J1) and 0-junctions (J0) (we omitted transformers and gy-
rators for the sake of simplicity). We have two types of elements in a bond
graph. One is the elements including C/I/R/J1/J0 which have one or more
interface ports. The second type of elements are two-port bonds. The applica-
ble topological operations on node elements include replacing the component
type (on C/I/R/J1/J0), and adding a new C/I/R component (on J1/J0). The
topological operations on bonds include inserting a new J1/J0. This operator
set will enable the GPBG to evolve a large number of bond graphs. Then
we develop an alternative, “basic” GP function set for this problem, includ-
ing {Insert J0/J1, Add C/I/R, and Replace C/I/R}. Figure 14.2 and 14.3
shows how these topological manipulation GP functions work. Note that for
Add C/I/R functions, we can have one or more branches that accept numeric
subtrees to set the component parameters. More examples of developmental
GP are presented in Sect. 14.4.1 and Sect. 14.4.2.
The developmental GPBG framework enables us to do simultaneous topol-
ogy and parameter search. Compared to the direct encoding GPBG below, it
can evolve much more diverse topology types including those with loops and
has more ﬂexibility during the evolutionary search process, but at the cost of
a less intuitive GP tree.
 
 
ERC or 
+ / -
add_R 
(1) 
(2) 
(3) 
(1) 
1 
Modifiable Site (1) 
Modifiable Site  (3)   
R 
1 
Modifiable Site  (2)  
Modifiable Site  (1)  
Fig. 14.2. The add R function adds a resistor to a junction
GPBG with Direct Encoding
One interesting observation of typical bond graphs is that most bond graphs
do not have loops and are themselves tree-structured. This makes it natural
to use the GP trees themselves to represent/encode the bond graph structure.
We thus proposed the direct-encoding GPBG approach for evolving tree-type
bond graph models. In this approach, 1-junction and 0-junction are used as GP
functions with two input variables (ports). The capacitors/resistors/inductors

14 GPBG: A Framework for Evolutionary Synthesis
325
 
(3) 
(2)
(1)
   insert_J0
(1)
Modifiable Site  (2)  
 Modifiable Site  (1) 
Modifiable Site  (3) 
0 
R
1 
Modifiable Site 
1 
R 
Fig. 14.3. The Insert J0 inserts a new 0-junction into a bond
are all GP functions with one input that connects to a numeric subtree to es-
tablish the component size (parameter value). We also have plus/minus arith-
metic operators in addition to ERC random terminals. One such function set
is shown in Table 14.1, which is used to synthesize controllers for a suspension
system in Sect. 14.4.3. Since there is a one-to-one correspondence of the GP
tree and bond graph topology, one can easily build a bond graph by following
the topologies in the GP trees.
Table 14.1. GP function set for suspension controller synthesis
Name and description
Function arity
J0 – Junction (0)
2
J1 – Junction (1)
2
R Element (R)
1
C Element (C)
1
I Element (I)
1
Arithmetic + (+): add two ERCs
2
Arithmetic – (–): Subtract two ERCs
2
Ephemeral Random Constant (ERC)
0
With this encoding approach, each GP tree is a bond graph. Actually,
here, all GP trees are binary trees. Clearly, this can only represent a subset
of bond graphs compared to the developmental GPBG approach, but enjoys
simplicity in implementation. Since the standard GP needs to specify the arity
of the GP functions, this limits the number of ports of the junctions to be
three, which is a shortcoming of this approach. However, this disadvantage
can be ameliorated by deﬁning a larger arity (e.g., 8) and then deﬁning a
null-element terminal. In this way, we can evolve 6-arity GP trees in which
many of the ports are simply empty. Most real-world bond graph models have
fewer than 8 ports. One possible disadvantage maybe that this may greatly
increase the search space.

326
Hu et al.
14.4 Case Studies of Evolutionary Synthesis Using
GPBG
In this section, we applied GPBG to three real-world design problems, includ-
ing synthesis of a passive vibration absorber, a MEMS ﬁlter, a robust analog
ﬁlter, and a controller for a suspension system. These examples are used to
illustrate the following unique advantages of GPBG based evolutionary design
compared to conventional design approaches:
•
topologically open-ended exploration of design space (the vibration ab-
sorber design problem);
•
easy and natural incorporation of domain knowledge into the design pro-
cess (the MEMS ﬁlter design problem);
•
the capability to evolve novel and unconventional design solutions (the
suspension system design problem).
Each problem will start with the description of the design space and the
conﬁguration of GPBG, including design embryo, GP function set, and ﬁtness
function. We also discuss the strategies to evolve robust designs using the
GPBG framework.
14.4.1 Synthesis of Mechanical Vibration Absorber
Problem Description
Vibration absorbers are a class of dynamic systems, and can be modeled as
analog circuits, block diagrams, bond graphs, et cetera. One special charac-
teristic of these particular dynamic systems is that the building blocks usually
have a ﬁxed number interface ports and may not be connected arbitrarily.
In this section, we are mainly interested in synthesizing passive vibration
absorbers to reduce the vibration response of primary systems of various con-
ﬁgurations. Figure 14.4 shows a primary system and its corresponding bond
graph model. The design task is to attach some new components to the pri-
mary system such that the frequency response at the excitation frequency
ω be minimized. Figure 14.5 shows the ﬁrst vibration absorber, invented by
H. Frahm in 1911, and its bond graph model. The frequency response of the
stand-alone primary system and the primary system with vibration absorber
is shown in Fig. 14.6. It can be seen that the vibration absorber can signiﬁcant
quench the response of the primary systems at the excitation frequency.
In this design problem, the objective is to synthesize a vibration absorber
such that the frequency response
fraw = |TF(jω)|ω=ω0
(14.1)
of the primary system mass (displacement) at the frequency ω of excitation
force f = f0×sinωt is minimized. This problem is extracted from [7]. We want

14 GPBG: A Framework for Evolutionary Synthesis
327
 
 
(a)
(b)
Fig. 14.4. The bond graph structure of a primary system and its bond graph model
(a) The primary system under perturbation of excitation force F(t). (b) The bond
graph model of the embryo system
Fig. 14.5. The bond graph structure of the ﬁrst patented vibration absorber and
its bond graph model
to see if the GPBG system can reinvent the ﬁrst patented vibration absorber,
shown in Fig. 14.5. The parameters of the primary system are as follows: mp
= 5.77 kg; kp=251.132 × 1e6 N/m; cp= 192.92 kg/s. The parameters of the
standard passive absorber solution are as follows: ma = 0.227 kg; ka=9.81e6
N/m; ca= 355.6 kg/s.
GPBG Conﬁguration
The design space of passive vibration absorbers is composed of masses (R),
springs (C), and dampers, corresponding to Resistor(R), Capacitor (C) and

328
Hu et al.
(a)
0
500
1000
1500
2000
−450
−400
−350
−300
−250
Amplitude (dB)
frequency (Hz)
(b)
0
500
1000
1500
2000
−450
−400
−350
−300
−250
Amplitude
frequency (Hz)
Fig. 14.6. Frequency responses of the primary system under perturbation of ex-
citation force F(t), without and with a vibration absorber: (a) without vibration
absorber; (b) with vibration absorber
Inductor (I), respectively. Following the GPBG framework outlined before, we
used the bond graph embryo in Fig. 14.1 for this design problem. The mod-
iﬁable site is the 1-junction. Since it is not physically realistic to have many
masses attached to the primary structures, we limit the maximum number of
masses to two in all the experiments.
In our earliest work [2], a “basic” GP function set was used for evolutionary
synthesis of analog ﬁlters. In that approach, the GP functions for topological
operation included {Insert J0/J1, Add C/I/R, and Replace C/I/R}, which
allowed evolution of a large variety of bond graph topologies. The shortcoming
of this approach is that it tended to evolve redundant and sometimes causally
ill-posed bond graphs [18]. Later, we used a causally well-posed modular GP
function set to evolve more concise bond graphs with much less redundancy [6].
However, that encoding had a strong bias toward a chain-type topology and

14 GPBG: A Framework for Evolutionary Synthesis
329
thus may have limited the scope of topology search [5]. Here we have improved
the basic function set in [2] and developed the following hybrid function set
approach to reduce redundancy while enjoying the ﬂexibility of topological
exploration:
F={Insert J0E, Insert J1E, Add C/I/R, EndNode, EndBond, ERC}
where the Insert J0E, Insert J1E functions insert a new 0/1-junction into a
bond while attaching at least one and at most three elements (from among
C/I/R). Figure 14.7 illustrates the operation of the Insert J0E function.
EndNode and EndBond terminate the development (further topology manip-
ulation) at junction modiﬁable sites and bond modiﬁable sites, respectively;
ERC represents a real number (Ephemeral Random Constant) that can be
changed by Gaussian mutation. In addition, the number and type of elements
attached to the inserted junctions are controlled by three “ﬂag” bits. A ﬂag
mutation operator is used to evolve these ﬂag bits, each representing the pres-
ence or absence of the corresponding C/I/R component. Compared with the
basic function set approach, this hybrid approach can eﬀectively avoid adding
many bare (and redundant) junctions. At the same time, Add C/I/R (illus-
trated in Fig. 14.8) still provides the ﬂexibility needed for broad topology
search. For any of the three C/I/R components attached to each junction,
there is a corresponding parameter to represent the component’s value, which
is evolved by a Gaussian mutation operator in the modiﬁed genetic program-
ming system used here. This is diﬀerent from our previous work in which the
“classical” numeric subtree approach was used to evolve parameters of com-
ponents. Figure 14.9 shows a GP tree that develops an embryo bond graph
into a complete bond graph solution. Our comparison experiments [5] showed
that this function set was more eﬀective on both an eigenvalue and an analog
ﬁlter test problem.
Insert_J0E
OB: Old bond modifiable site
NJ1 NB
OB
NJ: New Junction modifiable site
NB:New bond modifiable site
OB
V1 V2 V3
Vi: ERC values for  I/R/C
1
0
OB
1
0
0
OB NJ1 NB
I
R C
V1V2 V3
Fig. 14.7. The Insert J0E GP function inserts a new junction into a bond along
with a certain number of attached components

330
Hu et al.
J
Add_C/I/R
OJ: old junction modifiable site
OJ
NB
NB: new bond modifiable site
OJ
ERC
ERC: numeric value for C/I/R
OJ
C/I/R
J
OJ NB
(12.0)
 
Fig. 14.8. The Add C/I/R GP function adds a C/I/R component to a junction
1
I
R
Tree_Root
EndNode
Insert_J1E
Add_C
EndBond
EndNode
Add_I
EndBond
(1)
(1)
010
Insert_J0E
001
0
RL
(1)
1
Se
Rs
0
RL
(1)
1
Se
Rs
C
I
0
Fig. 14.9. An example of a GP tree, composed of topology operators applied to an
embryo, generating a bond graph after depth-ﬁrst execution (numeric ERC nodes
are omitted). Note that the 010 and 001 are the ﬂag bit sets showing the presence
or absence of attached C/I/R components
The ﬁtness function for candidate design evaluation is deﬁned as:
fnorm =
NORM
NORM + fraw
(14.2)
where fraw is the frequency response as deﬁned in (14.1). NORM is a normal-
ization term aimed at adjusting the fnorm into the range of [0,1]. This process
transforms the minimization of deviation from target frequency response into
a maximization of ﬁtness process as used in our GP system. Since tournament
selection is used as the selection operator, the normalization term can be an

14 GPBG: A Framework for Evolutionary Synthesis
331
arbitrary positive number. Here NORM is set to 10, and the ﬁtness range is
[0,1].
According to (14.1), we calculate the frequency response X1(s)/F(s) where
X1 is the displacement of the primary mass. However, we can only extract from
a bond graph the source eﬀort signal
.
X (s). We use the following procedure
to get the fraw:
1. calculate A, B, C, D matrices from a given bond graph;
2. convert A, B, C, D into transfer function TF raw;
3. TF norm = TF raw × 1/s is equal to X1(s)/F(s);
4. convert TF norm back to A′, B′, C′, D′ matrices and simulate its frequency
response with MATLAB.
Design Experiments
Compared to the evolutionary synthesis of electrical circuits, a mechanical
vibration absorber usually has a much smaller number of components. So the
topological and parameter search can be greatly decreased. We used a bond
graph simulation engine and developed the GPBG platform based on the
Open beagle GP framework by Christian Gagne [4]. Most of the experiments
are ﬁnished in less than an hour. Some of them take only a few minutes.
Here we set the maximum number of components to be 7. Other standard GP
parameters are summarized in Table 14.2.
Table 14.2. Experimental parameters for vibration absorber synthesis
Parameter
Value Parameter
Value
No. of subpopulations
5
Tournament Selection Size
7
Sub population size
400
pCrossover
0.4
Maximum evaluation 100000 pMutationStandard
0.05
Migration Interval
5 gen MutateMaxDepth
3
Migration Size
40
pMutationParameter
0.3
Init.MaxDepth
3
pSwitchBit
0.2
Init.MinDepth
2
pSwapSubtree
0.05
StronglyTyped
True
TreeMaxDepth
7
Figure 14.10 shows an evolved single frequency vibration absorber and
its frequency response compared to the responses of the primary structure
without any absorber and with a standard passive absorber invented in 1912.
It is very interesting that the frequency response of the evolved vibration
absorber has a very deep spike at the excitation frequency to minimize the
frequency response at that single frequency. If the excitation frequency is
relatively constant with little shifting, our evolved absorber will achieve better
performance at that speciﬁc frequency. Our evolved vibration absorber utilizes

332
Hu et al.
one damper (I) and several springs (C), sharing similarity to the original
absorber invention of 1912. We found the GPBG framework is very ﬂexible
for vibration absorber synthesis. In addition to this single-frequency vibration
absorber, we have also synthesized novel dual frequency and band-pass passive
vibrators, which will be reported elsewhere.
Input Signal
Se
R
I
C
C
I
1
0
C
1
C
0
C
 
0
500
1000
1500
2000
−450
−400
−350
−300
−250
Amplitude (dB)
frequency (Hz)
GP−VA
Primary system
1911 VA invention
Fig. 14.10. The evolved single-frequency vibration absorber and its performance
compared to a standard vibration absorber
14.4.2 Synthesis of MEMS Filters: Knowledge Incorporation in
GPBG
Due to the complexity of real-world engineering design problems, hands-free
automated synthesis system can rarely provide entirely satisfactory solutions.
It may be that the computational demand is too high for current inexpensive
computing hardware or the design solutions are hard to implement using phys-
ical components or they violate some design constraints. It is thus strongly
desirable to incorporate expert/human knowledge into the evolutionary syn-
thesis process to create some kind of interactive evolutionary synthesis tools
that help human designers make better decisions and explore under-explored
design spaces.
Problem Description
In this section, we try to synthesize MEMS (micro-electro-mechanical systems)
band-pass ﬁlters to examine how domain knowledge can be conveniently in-
cluded into the evolutionary synthesis process. Due to its multi-domain and
intrinsically three-dimensional nature, MEMS design and analysis is very com-
plicated. However, the multi-domain property of MEMS models makes them

14 GPBG: A Framework for Evolutionary Synthesis
333
suitable for representation as bond graphs. In this MEMS ﬁlter synthesis prob-
lem, the goal is to automatically generate bond graph models of MEMS ﬁlters
to meet particular design speciﬁcations.
One distinct characteristic of MEMS ﬁlter design with vibration absorber
synthesis and analog ﬁlter design is that, due to manufacturing constraints,
MEMS ﬁlters are usually composed of a restricted and specialized set of com-
ponents. Two popular topologies for micromechanical band-pass ﬁlters, built
using surface micromachining, are topologically composed of a series or con-
catenation of Resonant Units (RUs) and Bridging Units (BUs), or RUs and
Coupling Units (CUs). Figure 14.11 illustrates the layouts and corresponding
bond graph representations of two such ﬁlter topologies, labeled I and II.
 
 
(a)
(b)
Fig. 14.11. MEMS ﬁlter topologies. (a) Layout of ﬁlter topology I (b) Layout of
ﬁlter topology II
From this ﬁgure, it is clear that here the building blocks of MEMS ﬁl-
ter design are high-level modules that are tailored to a speciﬁc fabrication
process. Design solutions composed of arbitrary topologies of basic primitive
components will be diﬃcult to manufacture.
GPBG Conﬁguration
For this band-pass ﬁlter design problem, we use the bond graph model shown
in Fig. 14.12 as the design embryo of the GPBG framework. The accompany-
ing block diagram indicates that this implementation will accept an electrical
voltage signal as input and produce a voltage signal as output, but the interior
components will be implemented as micromechanical elements.
To incorporate the domain knowledge of MEMS ﬁlters, we propose the
realizable GP function set concept, which manipulates topologies composed

334
Hu et al.
 
Fig. 14.12. MEMS ﬁlter design embryo in bond graph and block diagram forms
Table 14.3. Realizable function set for MEMS ﬁlter synthesis
Function
Description
insert RU Insert a Resonator Unit
insert CU Insert a Coupling Unit
insert BU Insert a Bridging Unit
add RU
Add a Resonator Unit
insert J01 Insert a 0-1-junction compound with elements
insert CIR Insert a special CIR compound
insert CR Insert a special CR compound
Add J
Add a junction compound
+
Sum two ERCs
–
Subtract two ERCs
endn
End terminal for add functions
endb
End terminal for insert functions
endr
End terminal for replace functions
erc
Ephemeral Random Constant (ERC)
of manufacturable modules by adding, removing, or replacing these modules
as units. We use the following GP function set in Table 14.3 as our modular
GP function set, which can impose domain knowledge of design constraints on
the ﬁnal synthesis results for guaranteed manufacturability of the design under
current or anticipated manufacturing technology. By using only operators in
a realizable function set, we seek to guarantee that the evolved design is
physically realizable and has the potential to be manufactured. This concept
of realizability may include stringent fabrication constraints to be fulﬁlled in
some speciﬁc application domain.

14 GPBG: A Framework for Evolutionary Synthesis
335
Examples of operators, namely insert CU and insert RU, are illustrated in
Fig. 14.13. Examples of basic operators are available in our earlier work [2].
Figure 14.13(a) explains how the insert BU function works. A Bridging Unit
(BU) is a subsystem composed of three capacitors with the same parameters,
attached together with a 0-junction in the center and 1-junctions at the left
and right ends. After execution of the insert BU function, an additional mod-
iﬁable site (2) appears at the rightmost newly created bond. As illustrated
in Fig. 14.13(b), a resonator unit (RU), composed of one I, R, and C com-
ponent all attached to a 1-junction, is inserted in an original bond with a
modiﬁable site through the insert RU function. After the insert RU function
is executed, a new RU is created and one additional modiﬁable site, namely
bond (3), appears in the resulting phenotype bond graph, along with the
original modiﬁable site bond (1). The newly-added 1-junction also has an ad-
ditional modiﬁable site (2). As components C, I, and R all have parameters to
be evolved, the insert RU function has three corresponding ERC-typed sites,
(4), (5), and (6), for numerical evolution of parameters.
 
 
(a)
(b)
Fig. 14.13. Two realizable GP functions for MEMS ﬁlter design. (a) Insert BU
(b) Insert RU
Filter performance is measured by the magnitude ratio of the frequency re-
sponse for the voltage across RL to the input voltage us. The desired frequency
response has unity magnitude ratio in the pass band (316–1000 Hz), and zero
magnitude ratio outside the pass band. The frequency range of interest is
0.1Hz–100KHz. To evaluate ﬁtness within the frequency range of interest, 100
points are sampled at equal intervals on a log scale. The magnitudes of the
frequency response at the sample points are compared with their desired mag-
nitudes. The diﬀerences are computed and the sum of all squared diﬀerences is
taken as raw ﬁtness. The normalized ﬁtness is calculated according to (14.2).

336
Hu et al.
Design Experiments
We used a strongly-typed version of lilgp [17] to generate bond graph models.
The major GP parameters were as shown in Table 14.4.
Table 14.4. Experimental parameters for vibration absorber synthesis
Parameter
Value
Population size
500 in each of 13 subpopulations
Initial population half and half
Initial depth
4–6
Max depth
50
Max nodes
5000
Selection
Tournament (size=7)
Crossover
0.9
Mutation
0.3
Results of the experiments show the capability of the GPBG approach for
ﬁnding realizable designs for micro-electro-mechanical ﬁlters. Figure 14.14(a)
shows the ﬁtness improvement curve of a typical genetic programming run,
in which K is deﬁned as the number of resonator units used in the MEM
ﬁlter design. It is shown that as evolution progresses, the ﬁtness value under-
goes continual improvement. It is also observed that as ﬁtness improves, the
value of K also becomes larger. This observation is supported by the reason-
ing that a higher-order system with more resonator units has the potential
of having better system performance than its lower-order counterpart. The
system frequency responses at generations 27, 52, 117 and 183 are shown in
Fig. 14.14(b), with increased K value and performance evaluation.
The use of realizable function sets can be made less rigid to assist the
designer in exploring more novel topologies for MEMS ﬁlter design. The de-
signer may use a function set in which not all elements are guaranteed to be
strictly realizable. Instead, a diﬀerent set of design knowledge is incorporated
in the evolutionary process – i.e., a semi-realizable function set may be used
to relax the topological constraints with the purpose of ﬁnding new topologies
not discovered before but still usually realizable after careful interpretation.
Figure 14.15 gives an example of a novel topology evolved for a MEM ﬁl-
ter design by incorporating a special CIR component into the semi-realizable
function set.
The work presented in this section analyzes the promise of MEMS de-
sign synthesis at the system level using the GPBG approach. The basic GP
function set imposes very few constraints on design, while the realizable func-
tion set used for MEMS design features relatively few but structurally more
complex devices in the component library. The use of a realizable function set
guarantees that the phenotypes generated can be built using existing or antic-
ipated manufacturing technology. Large-scale component reuse and assembly

14 GPBG: A Framework for Evolutionary Synthesis
337
 
 
 
(a)
(b)
Fig. 14.14. (a) Fitness progress over generations (b) Frequency responses of design
candidates at diﬀerent generations
 
 
Fig. 14.15. A novel design topology using a semi-realizable function set
of MEMS is expected to show more applicability and promise of this method
for MEMS design.

338
Hu et al.
14.4.3 Synthesis of Suspension System Controllers
Problem Description
Suspension systems are important subsystems of most wheeled vehicles. From
a system design point of view, there are two main types of disturbances act-
ing on a vehicle, namely road and load disturbances. Road disturbances have
the characteristics of large magnitude in low-frequency disturbances (such
as hills) and small magnitude in high-frequency disturbances (such as road
roughness). Load disturbances include the variations of loads induced by ac-
celerating, braking and cornering. A good suspension design is concerned with
disturbance rejection from these disturbances to the outputs (e.g. vertical po-
sition of vehicle mass), the basis for evaluating performance. In general, a
suspension system needs to be “soft” to insulate against road disturbances
and “hard” to insulate against load disturbances.
 
zu 
ms 
mu 
kt 
Fs 
u 
Fr 
zs 
zr 
 
 
(a)
(b)
Fig. 14.16. (a) Quarter-car model in iconic diagram (b) quarter-car suspension
control with both road and load disturbances
A quarter-car iconic model is illustrated in Fig. 14.16(a). The sprung mass
ms (kg), consists of the main vehicle body supported by the suspension. The
unsprung mass mu (kg), consists of hub, wheel and tire. The tire is modeled as
a spring with stiﬀness k t (N/m). z s, z u, and z r are the vertical positions of the
sprung mass, the unsprung mass and the road disturbance input, respectively.
Force Fs is the load force disturbance input. Force u represents any possible
suspension force.

14 GPBG: A Framework for Evolutionary Synthesis
339
From the point of view of a multi-port mechatronics network, the quarter-
car suspension system can be viewed externally as a two-port network [21],
with its corresponding mixed immittance matrix G deﬁned as:
 Fr
˙zs
!
=
 G11(s) G12(s)
G21(s) G22(s)
!  ˙zr
Fs
!
(14.3)
where F r represents the applied force from the tire to the road. The matrix
G can be obtained from the following equations of system motion, together
with speciﬁed suspension force u:
ms¨zs = −u + Fs
(14.4)
mu¨zu = u + kt(zr −zu).
(14.5)
When load disturbance is also considered, the suspension system needs to be
stiﬀto loads acting on the sprung mass. This requires in (14.3), G11(s) and
G21(s) be set “soft” for road disturbance rejection while G12(s) and G22(s)
be set “hard” for load disturbance rejection. For such design requirements,
the matrix G fails to be positive-real, which implies active energy input is
necessary for such suspension implementation [21].
There is one degree-of-freedom available for the response to each of the
road and load disturbances. They can be determined independently if two suit-
able measurements are available for feedback (e.g. suspension deﬂection and
sprung mass velocity). The suspension design with two measurements is shown
in Fig. 14.16(b), with the control law taken to be: u =
"
k1(s) k2(s)
#  
zs −zu
szs
!
,
where k 1(s) is collocated control, while k 2(s) is non-collocated control.
In order to synthesize controller k 1(s) and k 2(s), desired performance re-
quirements for road and load disturbance rejection are speciﬁed. The desired
frequency response for road disturbance H1(s) is speciﬁed in (14.6). The de-
sired load disturbance frequency response H 2(s) is the frequency response
speciﬁcation for G22(s) in the immittance matrix in (14.7). It is obtained by
choosing certain suitable parameters in another double skyhook conﬁguration:
u = ks(zs −zu) + c1 ˙zs −c2 ˙zu, with a hard damper and spring conﬁguration
with ks = 150000 N/m, c1 = 12000 Ns/m, c2 = 6000 Ns/m. The desired H2(s)
is calculated as:
H1(s) = ˙zs
˙zr
=
c2kts + kskt
msmus4 + (c1mu + c2ms)s3 + (ksmu + ksms + ktms)s2 + c1kts + kskt
(14.6)
H2(s) = ˙zs
Fs
=
(mus2 + c2s + kt + ks)s
msmus4 + (c1mu + c2ms)s3 + (ksmu + ksms + ktms)s2 + c1kts + kskt
.
(14.7)

340
Hu et al.
GPBG Conﬁguration
The design space of the controllers is bond graphs composed of C/I/R compo-
nents and 1/0 junctions. We use the direct encoding formulation of the GPBG
framework as speciﬁed in Sect. 14.3.3. There need be no embryo or modiﬁ-
able site. The bond graphs are directly encoded by the GP trees. We use the
following GP function set in Table 14.1. In this function set, the J0 and J1
functions have two inputs, meaning that in this encoding, the 1/0 junctions in
the represented bond graphs can only have three ports: two input ports and
one output port.
The ﬁtness of a GP individual is evaluated by how accurately it approxi-
mates the desired frequency domain speciﬁcation, minimizing the value of the
expression ∥dTF(jω) −tTF(jω)∥2, where dTF(jω) is the desired frequency
response as speciﬁed by H1(s) and H2(s), and tTF(jω) is the theoretical
frequency response of an evolved individual bond graph structure to be eval-
uated.
Design Experiments
Taking the desired road and load disturbance rejection responses H1(s) and
H2(s) as evaluation criteria, we used the settings listed in Table 14.5 for the
experiments.
The best run of genetic programming using the basic function set in Ta-
ble 14.1 produced the results shown in Fig. 14.17 for k 1(s), and Fig. 14.18 for
k 2(s).
k1(s) = 2128s3 + 46680s2 + 1137000s + 4792000
s2 + 16.08s + 32.45
= 2128(s + 5.011)(s2 + 16.93s + 449.4)
(s + 2.366)(s + 13.71)
 
Fig. 14.17. Controller structure in bond graph form for k1(s)

14 GPBG: A Framework for Evolutionary Synthesis
341
Table 14.5. Experiment settings
Objective:
Design a suspension system composed of two con-
trollers.
Test ﬁxture and embryo: Two-input, two-output initial suspension system with
a sprung mass, an unsprung mass, and a spring.
Program architecture:
Two result-producing GP species, k1 and k2, sharing
the following attributes.
Function set:
For construction-continuing subtrees: Fccs−rpb−initial
= {f0, f1, R, C, I}.
For arithmetic-performing subtrees: Faps = {ADD,
SUB}.
Terminal set:
For arithmetic-performing subtrees: Taps = {E}.
Fitness Cases:
41 frequency values in an interval of four decades of
frequency values between 0.1 Hz and 1000 Hz.
Raw Fitness:
Taking the desired road and load disturbance re-
jection responses as evaluation criteria, the raw ﬁt-
ness of a combined solution including individuals
from both species is calculated as: Fitnessraw
=
$
n

i=1
(err1+err2)2
n
n is the number of logarithmically sampled frequency
points; err 1 and err 2 are the absolute diﬀerences of
magnitude between the evolved and the desired road
and load disturbance rejection frequency responses, re-
spectively.
err1 = ∥G12(jω) −Gs
12(jω)∥2;
err2 =
%%G11(jω) −Gh
11(jω)
%%
2
Normalized Fitness:
Fitnessnorm =
1.0
F itnessraw+1.0
Parameters:
Each species: 10 subpopulations of 100 individuals;
Migration interval: 10 generations; Migration size: 2
individuals
Crossover rate: 0.85; Mutation rate: 0.15; initializing
tree depth: 2–4; maximum tree depth: 10–17
Result designation:
Best-so-far individual from max ﬁtness species and
matching individual from another species.
Termination:
When either species reaches max ﬁtness value 0.99.
The degree of a system can be determined by counting independent storage
elements present in the bond graph. The controllers obtained here are of lower
order than the controllers obtained using conventional approaches based on
the standard controller design theory [20]. The limitation of this approach
is that the design space are usually constrained by the limited number of
component conﬁgurations used in the suspension systems. On the other hand,
the GPBG approach can exploit the open-ended topology search capability
to evolve novel structure of the suspension systems as well as its controller.

342
Hu et al.
k2(s) = 10320s3 + 453300s2 + 40260000s + 437000000
s3 + 172s2 + 5799s + 15890
= 10320(s + 12.04)(s2 + 31.89s + 3517)
(s + 3)(s + 41.5)(s + 127.5)
 
Fig. 14.18. Controller structure in bond graph form for k2(s)
The simultaneous design of structures and controllers distinguishes the GPBG
approach from conventional controller design methods.
Figure 14.19 shows the simulation results as MATLAB Bode diagrams
comparing desired responses (solid lines) with actual responses (dashed lines)
realized by active suspension control evolved from evolutionary computation.
The left-hand side shows the road disturbance rejection responses, and the
right-hand side shows the load disturbance rejection responses. It demon-
strates that the actual responses approximate the desired responses very well.
In summary, using the GPBG framework, we have evolved an active sus-
pension system that has the ability to store, dissipate and to introduce energy
to the system, with extra ﬂexibility to achieve improved design performance.
It should be noted that in this work, we have assumed that the sensor and the
actuator have perfect dynamics. The suspension design will be considerably
modiﬁed if such assumptions do not hold well.
14.4.4 Automatic Generation of Robust Designs
Although the topic of design for robustness cannot be addressed in detail in
this chapter, application of GPBG for robust design has already been demon-
strated. In [16], three strategies for using GPBG to synthesize robust passive
analog ﬁlters were explored. The broadest conclusion was that ﬁlters of high
robustness to variation in the values of their parameters could be evolved
under GPBG, by introducing appropriate stochasticity during evolution of
the topology of the ﬁlters. It did not require many more ﬁlter evaluations to
evolve robust structures than to evolve those of similar nominal performance
without stochasticity. It was also shown that robustness of designs with com-
ponent values chosen from small, discrete sets could be improved by using only

14 GPBG: A Framework for Evolutionary Synthesis
343
 
Fig. 14.19. Desired and actual responses of evolved suspension design. Left (road
disturbance response). Right (load disturbance response)
the “catalog” values during the evolutionary process, but adding stochastic
variation about their nominal values.
14.5 Conclusions and Future Work
This chapter has applied genetic programming and bond-graph simulation –
the GPBG approach – to the design synthesis problem in engineering. Three
real-world design problems have been examined in detail, including mechan-
ical vibration absorbers, MEMS ﬁlters, and suspension system controllers.
Experimental results illustrate that the GPBG framework is an eﬀective tool
for exploring design space and evolving innovative designs that diﬀer from
those produced by human designers.
References
1. Ando, S., Iba, H.: Linear genome methodology for analog circuit design. Tech.
rep., Information and Communication Department, School of Engineering, Uni-
versity of Tokyo (2000)
2. Fan, Z., Hu, J., Seo, K., Goodman, E., Rosenberg, R., Zhang, B.: Bond graph
representation and GP for automated analog ﬁlter design. In: E. Goodman (ed.)

344
Hu et al.
2001 Genetic and Evolutionary Computation Conference Late Breaking Papers,
pp. 81–86. San Francisco, California, USA (2001)
3. Fan, Z., Seo, K., Hu, J., Rosenberg, R., Goodman, E.: System-level synthesis of
MEMS via genetic programming and bond graphs. In: E. Cant´u-Paz et al. (ed.)
Genetic and Evolutionary Computation (GECCO-2003), LNCS, vol. 2724, pp.
2058–2071. Springer-Verlag, Chicago (2003)
4. Gagn´e, C., Parizeau, M.: Open BEAGLE: a new versatile C++ framework for
evolutionary computation. In: E. Cant´u-Paz (ed.) Late Breaking Papers at the
Genetic and Evolutionary Computation Conference (GECCO-2002), pp. 161–
168. AAAI, New York, NY (2002)
5. Hu, J., Goodman, E.: Robust and eﬃcient genetic algorithms with hierarchical
niching and sustainable evolutionary computation model. In: Proceedings of
the 2004 Genetic and Evolutionary Computing Conference. Springer, Chicago
(2004)
6. Hu, J., Goodman, E., Rosenberg, R.: Topological search in automated mecha-
tronic system synthesis using bond graphs and genetic programming. In: Proc.
of American Control Conference ACC 2004. Boston (2004)
7. Jalili, N.: A comparative study and analysis of semi-active vibration-control
systems. Journal of Vibration and Acoustics 124, 593 (2002)
8. Karnopp, D., Margolis, D., Rosenberg, R.: System Dynamics: Modeling and
Simulation of Mechatronic Systems, 3rd edn. John Wiley & Sons, Inc., New
York (2000)
9. Karnopp, D., Margolis, D., Rosenberg, R.: System Dynamics: Modeling and
Simulation of Mechatronic Systems. John Wiley & Sons, Inc., New York (2000)
10. Koza, J., Andre, D., Bennett III, F., Keane, M.: Genetic Programming 3: Dar-
winian Invention and Problem Solving. Morgan Kaufmann (1999)
11. Koza, J., Bennett III, F., Andre, D., Keane, M., Dunlap, F.: Automated syn-
thesis of analog electrical circuits by means of genetic programming.
IEEE
Transactions on Evolutionary Computation 1(2), 109–128 (1997)
12. Koza, J., Keane, M., Streeter, M., Mydlowec, W., Yu, J., Lanza, G.: Genetic
Programming IV: Routine Human-Competitive Machine Intelligence. Kluwer
Academic Publishers (2003)
13. Koza, J., Keane, M., Yu, J., Bennett III, F., Mydlowec, W.: Automatic creation
of human-competitive programs and controllers by means of genetic program-
ming. Genetic Programming and Evolvable Machines 1(1/2), 121–164 (2000)
14. Lohn, J., Colombano, S.: A circuit representation technique for automated cir-
cuit design. IEEE Transactions on Evolutionary Computation 3(3), 205–219
(1999)
15. Paynter, H.: An epistemic prehistory of bond graphs.
In: P. Breedveld,
G. Dauphin-Tanguy (eds.) Bond Graphs for Engineers. Elsevier Science Pub-
lishers, Amsterdam (1991)
16. Peng, X., Goodman, E., Rosenberg, R.: Comparison of robustness of three ﬁl-
ter design strategies using genetic programming and bond graphs. In: R. Ri-
olo, T. Soule, B. Worzel (eds.) Genetic Programming Theory and Practice IV.
Springer (2006)
17. Punch, W., Zongker, D.: lilgp – a C system for genetic programming (1995).
URL: http://garage.cse.msu.edu/software/lil-gp
18. Seo, K., Fan, Z., Hu, J., Goodman, E., Rosenberg, R.: Dense and switched
modular primitives for bond graph model design. In: E. Cant´u-Paz et al. (ed.)

14 GPBG: A Framework for Evolutionary Synthesis
345
Genetic and Evolutionary Computation (GECCO-2003), LNCS, vol. 2724, pp.
1764–1775. Springer-Verlag, Chicago (2003)
19. Seo, K., Fan, Z., Hu, J., Goodman, E., Rosenberg, R.: Toward an automated
design method for multi-domain dynamic systems using bond graph and genetic
programming. Mechatronics 13(8-9), 851–885 (2003)
20. Smith, M.: Achievable dynamic response for automotive active suspension. Ve-
hicle System Dynamics 24, 1–33 (1995)
21. Smith, M., Walker, G.: Performance limitations and constraints for active and
passive suspensions: a mechanical multi-port approach. Vehicle System Dynam-
ics 33, 137–168 (2000)
22. Wang, J., Terpenny, J.: Integrated active and passive mechatronic system design
using bond graphs and genetic programming. In: B. Rylander (ed.) Genetic
and Evolutionary Computation Conference Late Breaking Papers, pp. 322–329.
Chicago, USA (2003)

Index
A-life, see artiﬁcial life
artiﬁcial life, V, 14, 19, 26, 101–103,
116, 119, 223
Ashlock, Dan, 98, 121, 149, 199, 203
Avida, 9, 14, 19, 28
Behe, Michael, 8, 10, 13–18, 27
blind watchmaker, V, 11
bond graph, 269, 318, 320–326, 328,
329, 331, 333, 335, 336, 340, 341
Bowers, Chris, 200, 242
Bryden, Kenneth, 203
carrot, 103–105
cell diﬀerentiation, 72, 239, 247, 250
cellular automata, 75, 146
cellular EA, see evolutionary algorithm:
cellular, 302
central dogma, 54, 55
Chellapilla and Fogel, 26
coevolution, 18, 26, 33, 38, 40, 41, 44–48
cooperative coevolution, 33, 38, 40,
41, 44–48
complex adaptive system, 4, 69, 98, 223,
256
complex system, see complex adaptive
system
computational development, 224
creation science, 7
Crick, Francis, 53, 54
Darwin, Charles, 9–12, 14, 171
Dawkins, Richard, 98, 147, 171
biomorph, 98, 116, 123, 171
The Blind Watchmaker, 98, 113, 171
Deb, Kalyanmoy, 267, 269, 273
NSGA-II, 276, 278–281, 286, 288, 290
Dembski, William, 8, 9, 11, 13, 14, 18,
20–26, 28
design, VI, X, 98, 199–201, 219
conceptual design, 267, 319
controller design, 320, 342
design inference, 8, 21–23, 25
design space, 170, 319, 320, 324, 326,
332, 340, 342, 343
evolutionary design, IX, 183, 199,
223, 239, 267, 318, 321, 326
functional design, 72, 320
Intelligent Design, see Intelligent
Design
real-world design, 326, 343
real-world engineering design, 269,
332
development, 199, 201, 222–230, 232,
239, 244, 248, 251, 252, 256, 323
diﬀusion, 223, 246, 247, 249–251, 254
diﬀusion rate, 226, 247, 250, 251
reaction–diﬀusion, 203, 251
DNA, 4, 31, 32, 34, 44, 46, 48, 53–55,
60, 63, 72, 74, 75, 78, 118
Driessens and Verstappen, 100
Breed, 105–108, 117, 119
E-volver, 98, 109, 111–113, 115–119
Morphotheque, 103–105, 108
E. coli, 57, 60, 61, 63, 74, 75
EA, see evolutionary algorithm

348
Index
embryo, 199, 200, 223–225, 228, 229,
249, 324, 326, 328, 333, 340, 341
embryogenesis, see embryogeny
embryogeny, 118, 199–201, 203, 213,
242, 244, 248, 251, 254, 257,
259–261, 324
artiﬁcial embryogeny, 203
emergent phenomena, 69, 70, 78, 89
emergent properties, see emergent
phenomena
EMO, see multi-objective: evolutionary
algorithm
English, Tom, 4, 6
ES, see evolutionary algorithm:
evolution strategy
evolutionary algorithm, V, 4, 31, 40, 62,
123, 125, 129, 132, 172, 199, 200,
206, 208, 213, 217, 219, 244, 255,
259, 261, 273, 276, 297, 300, 302
cellular, 302
evolution strategy, 108, 270, 297, 304
genetic algorithm, 36, 62, 98, 145,
150, 154, 156, 170, 171, 190, 225,
270, 274, 297, 298, 300
genetic programming, 32, 112, 123,
150, 269, 318, 320, 321, 323–326,
328, 331, 333, 334, 336, 337, 340,
343
evolutionary art, 98, 116, 146, 150, 166
Evolutionary Development System, 201,
225–228, 232, 235, 239
Fan, Zhun, 318
ﬁtness landscape, 135, 138, 298, 299,
303, 305–307, 310
Fogel, David, VI, 26
form, 192, 222–225, 232, 239, 244, 247,
250
fractal, 98, 121, 123, 125–127, 129, 134,
135, 138, 139, 145–149, 205
French Flag, 254, 257
GA, see evolutionary algorithm: genetic
algorithm
gene expression, 4, 31, 32, 37, 39–42,
45, 56, 58, 230, 232, 237, 239
gene network, see genetic regulatory
network
gene regulatory system, see genetic
regulatory network
genetic drift, 245, 301, 307
genetic engineering, 4, 53, 55
genetic network, see genetic regulatory
network
genetic regulatory network, 4, 31–41,
44–48, 56–58, 62, 71, 224, 225,
227, 228, 232–239, 252, 257, 258
genetically modiﬁed organism, 54
Gent, Stephen, 203
GMO, see genetically modiﬁed organism
Goodman, Erik, 318
GP, see evolutionary algorithm: genetic
programming
GPBG, see bond graph
Greenwood, Garrison, 4, 6
GRN, see genetic regulatory network
Hallinan, Jennifer, 4, 52
holographic grating, 269, 297
varied-line-spacing holographic
grating, 297, 298, 308–310, 315
Hu, Jianjun, 269, 318
ID, see Intelligent Design
Intelligent Design, 4, 6–9, 11–14, 16–18,
20, 22, 24, 26
irreducible complexity, 8, 9, 13–18, 28
law of conservation of information, 25
speciﬁed complexity, 8, 13, 20–22, 24,
28
interactive evolution, 98, 150, 154,
156–158, 170–172, 195, 332
irreducible complexity, see Intelligent
Design: irreducible complexity
Jacob, Christian, 4, 69
Jamieson, Brooke, 98, 121
Julia, Gaston, 148
Julia set, 121, 123, 125–127, 129, 135,
138, 140
Kimura, Shuhei, 4, 31
Kumar, Sanjeev, 222, 224, 250
L-system, X, 98, 146, 168, 170–176, 179,
180, 184, 185, 187, 189–191, 195,
201, 203–211, 213, 214, 217, 219

Index
349
timed L-system, 176, 190, 193
parametric L-system, 174–176, 179,
182
stochastic L-system, 174
lac operon, see operon: lactose operon
lactose operon, see operon: lactose
operon
Latham, William, 98, 116, 150
law of conservation of information,
see Intelligent Design: law of
conservation of information
Li, Shaobo, 318
Lindenmayer system, see L-system
Ling, Qing, 269, 297
local optimum, 138, 245, 297, 300, 306,
307
Mandelbrot, 98, 148
Mandelbrot set, X, 98, 121, 123,
125–127, 129, 135, 137, 138, 140,
144–150, 154, 163
McCormack, Jon, 97, 119, 150, 168
Bloom, 192, 195
computational sublime, 119
MEMS, see micro-electro-mechanical
systems
micro-electro-mechanical systems, 320,
321, 324, 326, 337
MEMS ﬁlter, 320, 326, 332–336, 343
morphogenesis, X, 103, 106, 195, 245,
250, 254
Muller, Hermann, 14, 15, 27
multi-modal, 36, 268, 269, 297, 298,
305, 309, 310, 313, 314, 320
multi-objective, 274, 287
dynamic multi-objective, 286, 291
dynamic multi-objective optimiza-
tion, 291
evolutionary algorithm, 276, 277, 286,
288, 291, 293
multi-objective optimization, 268,
276, 286, 287
multi-optimum, 297–299, 309, 310, 314,
315
multinational EA, 302
Nature, 11–13, 90, 98, 101, 103, 116,
117, 119, 148, 171, 189, 195, 224,
226, 239, 243, 244
network inference, 32–37, 41, 44, 48
neural network, 3, 48, 70, 172, 224, 239
neutral drift, see genetic drift
NFL, see no free lunch theorem
niche/niching, 270, 297–300, 302, 303,
305–307, 314
adaptive niching, 302
crowding, 298, 300, 301, 307, 315
ﬁtness sharing, 299, 303
species conserving genetic algorithm,
300
no free lunch theorem, 26, 200
operon, 75, 252
lactose operon, 59–63, 72, 74, 75, 90
Paley, William, 10
Peng, Xiangdong, 318
phage λ, 61
potato, 104, 105
power scheduling, 272, 274, 284
Ray, John, 10, 11
Ray, T., 26
RE, see restricted evolution
REES, see restricted evolution:
restricted evolution evolution
strategy
regulatory network, see genetic
regulatory network
restricted evolution, 303
restricted evolution evolution strat-
egy, 298, 303–308, 310, 314,
315
restricted tournament selection, 301
robotics, 179, 199, 202, 224, 225,
232–239, 298
Rosenberg, Ronald, 318
S-system, 32–37, 39, 41–43, 45, 46, 48
Seo, Kisung, 318
speciﬁed complexity, see Intelligent
Design: speciﬁed complexity
suspension design, 342
suspension system, 320, 325, 326, 338,
339, 341–343
swarm intelligence, 4, 69, 71, 72, 81, 86,
90
synthetic biology, 4, 52, 54, 56, 59–64

350
Index
Terpenny, Janis, 318
The Origin of Species, 14
theories of evolution, 9
trade-oﬀ, 271, 274, 276, 279, 282, 291
Pareto optimal, 269, 279, 280, 288
trade-oﬀsolutions, 268, 276, 284, 291
tuber, 103, 105
Ussher, James, 9
Ventrella, Jeﬀrey, 98, 144, 149, 150
Mandeltweak, 147, 149, 150, 153, 154,
156–163
vibration absorber, 320, 326, 331–333,
336, 343
VLSHG, see holographic grating:
varied-line-spacing
Wang, Jiachuan, 318
Wang, Qiuping, 297
Watson and Crick, 53
Wun, Gang, 297

Natural Computing Series
A.A. Freitas: Data Mining and Knowledge Discovery with Evolutionary Algorithms.
XIV, pages, gs., tables. 
H.-P. Schwefel, I. Wegener, K. Weinert (Eds.): Advances in Computational Intelligence.
Āe ory and Practice. VIII, pages. 
A. Ghosh, S. Tsutsui (Eds.): Advances in Evolutionary Computing. Ā eory and
Applications. XVI, pages. 
L.F. Landweber, E. Winfree (Eds.): Evolution as Computation. DIMACS Workshop,
Princeton, January . XV, pages. 
M. Hirvensalo: Quantum Computing. nd ed., XI, pages. (rst edition
published in the series)
A.E. Eiben, J.E. Smith: Introduction to Evolutionary Computing. XV, pages. 
A. Ehrenfeucht, T. Harju, I. Petre, D.M. Prescott, G. Rozenberg: Computation in Living
Cells. Gene Assembly in Ciliates. XIV, pages. 
L. Sekanina: Evolvable Components. From Ā eory to Hardware Implementations.
XVI, pages. 
G. Ciobanu, G. Rozenberg (Eds.): Modelling in Molecular Biology. X, pages. 
R.W. Morrison: Designing Evolutionary Algorithms for Dynamic Environments.
XII, pages, gs. 
R. Paton†, H. Bolouri, M. Holcombe, J.H. Parish, R. Tateson (Eds.): Computation in Cells
and Tissues. Perspectives and Tools of Ā ought. XIV, pages, gs. 
M. Amos: Āe oretical and Experimental DNA Computation. XIV, pages, gs. 
M. Tomassini: Spatially Structured Evolutionary Algorithms. XIV, pages, gs.,
tables. 
G. Ciobanu, G. P˘aun, M.J. Pérez-Jiménez (Eds.): Applications of Membrane Computing.
X, pages, gs., tables. 
K.V. Price, R.M. Storn, J.A. Lampinen: Diﬀerential Evolution. XX, pages,
gs., tables and CD-ROM. 
J. Chen, N. Jonoska, G. Rozenberg: Nanotechnology: Science and Computation.
XII, pages, gs., tables. 
A. Brabazon, M. O’Neill: Biologically Inspired Algorithms for Financial Modelling.
XVI, pages, gs., tables. 
T. Bartz-Beielstein: Experimental Research in Evolutionary Computation.
XIV, pages, gs., tables. 
S. Bandyopadhyay, S.K. Pal: Classication and Learning Using Genetic Algorithms.
XVI, pages, gs., tables. 
H.-J. Böckenhauer, D. Bongartz: Algorithmic Aspects of Bioinformatics.
X, pages, gs., tables. 
P. Siarry, Z. Michalewicz (Eds.): Advances in Metaheuristics for Hard Optimization.
Multiobjective Problem Solving from Nature.
J. Knowles, D. Corne, K. Deb (Eds.):
From Concepts to Applications. XVI, 412 pages, 7gs., 53 tables. 8
XII, 362 pages, 43 gs., 20 tables. 8
P.F. Hingston, L.C. Barone, Z. Michalewicz (Eds.). Design by Evolution. 
XVI, 81 pages, gs., tables. 

