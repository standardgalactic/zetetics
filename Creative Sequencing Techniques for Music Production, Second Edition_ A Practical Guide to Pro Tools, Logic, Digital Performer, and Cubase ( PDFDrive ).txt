
Creative Sequencing 
Techniques for Music 
Production
A Practical Guide to Pro Tools, Logic, 
Digital Performer, and Cubase

To my father.

Creative Sequencing 
Techniques for Music 
Production
A Practical Guide to Pro Tools, Logic, 
Digital Performer, and Cubase
Second edition
Dr. Andrea Pejrolo
AMSTERDAM • BOSTON • HEIDELBERG • LONDON • NEW YORK • OXFORD
PARIS • SAN DIEGO • SAN FRANCISCO • SINGAPORE • SYDNEY • TOKYO
Focal Press is an imprint of Elsevier

To my father.

Focal Press is an imprint of Elsevier
The Boulevard, Langford Lane, Kidlington, Oxford, OX5 1GB
225 Wyman Street, Waltham, MA 02451, USA
First edition 2005
Second edition 2011
Copyright © 2011, 2005 Andrea Pejrolo. Published by Elsevier Ltd. All rights reserved
The right of Andrea Pejrolo to be identified as the author of this work has been asserted in accordance with the 
Copyright, Designs and Patents Act 1988
No part of this publication may be reproduced or transmitted in any form or by any means, electronic or 
mechanical, including photocopying, recording, or any information storage and retrieval system, without 
permission in writing from the publisher. Details on how to seek permission, further information about the 
Publisher’s permissions policies and our arrangement with organizations such as the Copyright Clearance 
Center and the Copyright Licensing Agency, can be found at our website: www.elsevier.com/permissions
This book and the individual contributions contained in it are protected under copyright by the Publisher 
(other than as may be noted herein).
Notices
Knowledge and best practice in this field are constantly changing. As new research and experience broaden 
our understanding, changes in research methods, professional practices, or medical treatment may become necessary.
Practitioners and researchers must always rely on their own experience and knowledge in evaluating and using any 
information, methods, compounds, or experiments described herein. In using such information or methods they 
should be mindful of their own safety and the safety of others, including parties for whom they have a professional 
responsibility.
To the fullest extent of the law, neither the Publisher nor the authors, contributors, or editors, assume any liability 
for any injury and/or damage to persons or property as a matter of products liability, negligence or otherwise, or 
from any use or operation of any methods, products, instructions, or ideas contained in the material herein.
British Library Cataloguing in Publication Data
A catalogue record for this book is available from the British Library
Library of Congress Number: 2011924207
ISBN: 978-0-240-52216-6
For information on all Focal Press publications 
visit our website at www.focalpress.com
Trademarks/Registered Trademarks
Brand names mentioned in this book are protected by their 
respective trademarks and are acknowledged.
Printed and bound in the United Kingdom
11 12 13 14    10 9 8 7 6 5 4 3 2 1

xi
Acknowledgments
I am extremely lucky to be able to write about my passions: composing, producing, and educating. 
This new edition of Creative Sequencing Techniques is a combined effort by many people who for the 
last year have been supportive, inspiring, and encouraging. First of all, my dear wife Irache and my 
wonderful daughter Alessandra. Without them there would be no music in me. To my parents Rosalba 
and Gianni, my brothers Luca and Marco, and my dear friend Nella, whose support, education, and con­
stant attention have always helped me to achieve my goals, thank you!
A special thank you goes to Catharine Steers and Carlin Reagan from Elsevier UK for the interest 
they showed in the idea for this new edition and for their precious support.
And finally a big thanks to my friends, students, and colleagues, a constant source of learning and 
growing.


xiii
Introduction
The main reason why I decided to update this book for a new second edition resides not only in the 
need to update the original content for the new versions of the digital audio workstation (DAW) software 
but also, and mainly, because new techniques, tools, and options have become available since the book 
was first published. Therefore, here you will find many new topics such as a brand new chapter dedi­
cated entirely to working with video, which has become a real must for any contemporary commercial 
composer and producer, in addition to new studio setups, MIDI and audio hybrid production techniques, 
and much more. You might also have noticed that the bundled CD has been removed from this edition. 
A new website: www.CreativeSequencingTechniques.com now hosts all the audio examples and support­
ing files. On the website you will find not only all the audio examples and supporting files but also a 
comprehensive list of video tutorials, articles, forums, and a live chat with me to discuss the techniques 
presented in the book. Having the website as a companion to this book gives you the possibility of more 
frequent updates and multimedia experience.
This book covers the four main DAWs used in professional production environments: Digital 
Performer, Cubase, Pro Tools, and Logic Pro, and how to get the most out of them by explaining and 
revealing advanced techniques such as groove quantization, sounds layering, tap tempo, creative meter 
and tempo changes, advanced use of plug-ins automation, synchronization of linear and non-linear 
machines, to mention a few. The subjects are approached from both a technical and a creative point 
of view in order to provide the modern composer/producer with tools and inside views on how to treat 
MIDI and DAWs as the orchestra and musicians of the twenty-first century.
This new and updated edition not only fills in techniques and tools for the new revisions of the soft­
ware, but also adds several crucial new topics such as working with video and software synthesizers. The 
main reason that inspired me to write this book was the incredible potential concealed in the modern 
production tools and in the existing software applications that are available to contemporary composers 
and producers. Unfortunately, most of the time, these tools are used in very mechanical and non-musical 
ways, therefore reducing and limiting not only the potential of the technology involved but also (and 
mainly) the potential of the composers who use these incredibly powerful tools for their productions. In 
this book I bridge the two worlds, trying to return the term “music technology” to its original connota­
tion, meaning a way to produce music with the help of technology. I want to stress the “help” factor that 
technology plays in the music production process since this is what technology is, a tool to help expand 
and improve the creation process on which the composer relies. Technology applied to music is not the 
goal of the production process but can be seen as the thread that guides and joins the inspirational proc­
ess with the final product. In this book you will learn sequencing techniques that always relate to practi­
cal aspects of music production and they are explained as much as possible in a simple yet thorough 
way. Following this concept I will refer often to the MIDI/audio computer workstation as the “orches­
tra of the twenty-first century”, since modern composers find themselves increasingly treating the MIDI 
and audio setup as the virtual musicians for whom they are writing. The MIDI standard, along with 
professional sequencing programs and software synthesizers/libraries, represent the modern score paper 
and they provide an extremely flexible medium both for sketching ideas and for full-scale productions. 
I don’t see this approach as limiting in terms of flexibility and sonorities, in fact I believe the opposite. 
The use of new sounds, techniques, and tools can only expand and improve the palette of contemporary 
composers, orchestrators, and producers. I decided to take on the four main sequencers at the same time 
because these days both professional and beginner musicians need to be able to master and program all 

xiv
Introduction
of them in order to have an edge over the competition. It is not enough any more to be familiar with only 
one or two applications. It is crucial to be comfortable with all of them, not only to expand one’s oppor­
tunities, but also to be able to take advantage of specific features that are available only in certain appli­
cations. This approach will help you to enhance considerably your palette and tools when it comes to 
sequencing and music production. Each technique explained in this book is presented first on a general 
level and then further developed with examples and practical applications for each sequencer.
This book was written with mainly four categories of readers in mind: the professional acoustic 
composer, the professional MIDI composer, the college educator along with his or her students, and the 
beginner. The professional acoustic composer who so far has been afraid to approach the digital MIDI 
and audio workstation, or who has been using only basic sequencing techniques, will be able to greatly 
improve his or her skills and will find a familiar environment reading this book since references are 
made keeping in mind a musical approach to sequencing. Seasoned MIDI programmers and produc­
ers can take advantage of the multi-sequencer environment on which this book is based. Through the 
examples and techniques for each of the four main DAWs used in the industry, they can quickly learn 
the same tools with which they are already familiar within a certain sequencer for all the other applica­
tions, giving them an advantage over the competition. College educators and students can use this man­
ual not only as an introduction to intermediate MIDI and sequencing classes, but also for more advanced 
MIDI orchestration and production courses. The summaries and the exercise sections at the end of each 
chapter were specially designed for educational applications. The beginner readers will be amazed by 
the improvement in their sequencing skills brought about by reading just few chapters and using the 
included exercises to improve their techniques.
In Chapter 1, I will cover the needs and solutions for a problem-free project studio, to enhance the 
creative flow involved in a production session. In Chapters 2, 3, and 4, I will guide the readers through 
basic, intermediate, and advanced sequencing techniques targeted to improve the overall quality of their 
productions. These chapters will help the reader to reach a professional level in terms of MIDI orches­
tration and programming using the leading and most advanced digital audio sequencers available on the 
market.
Chapter 5 is dedicated to MIDI orchestration. Here you will learn how to orchestrate for the MIDI 
ensemble and how to get the most out of your gear. This chapter covers not only the acoustic instruments 
but also the synthesizers and some of the most common synthesis techniques available at the moment. 
Chapter 6 focuses on the final mix and on the premastering process. Here you will learn mixing tech­
niques that take advantage of plug-in technology. Maximizing the use of effects such as reverb, compres­
sor, limiter, equalizer, and many others is crucial to bring your productions to the next level. Finally, 
Chapter 7 focuses on dealing with projects that are based on video. Here you will learn the techniques 
and tools for successfully writing music to video.
At the end of each chapter you will find a comprehensive summary of the concepts and techniques 
explained within it and a series of exercises oriented to provide practical applications and to develop the 
notions learned. These two sections are particularly helpful for both the professional and the student. 
They provide the former with a quick reference for several techniques and ideas, while the latter can take 
advantage of the concise layout to familiarize themselves with the concepts just learned.
The book is supported by several video tutorials, examples of arrangements, and sequencing tech­
niques accessible through the companion website. These examples demonstrate how to avoid common 
mistakes and how to fix them. Here you can find loops, templates, and comprehensive audio/video 
examples that you can use as a starting point for your productions.
Learn the technology in every detail but let always the creative flow guide your music! Now let’s 
begin.

1
Creative Sequencing Techniques for Music Production.
© 
  2005 Andrea Pejrolo. Published by Elsevier Ltd. All rights reserved.
CHAPTER
2011,
Setting up Your Creative 
Environment: The Studio
1
1.1  BASIC STUDIO INFORMATION
There is nothing more exciting for me than to sit in my studio, my creative environment, and write, pro­
duce, create music. I am sure that if you are reading these pages you share the same excitement and pas­
sion. I strongly believe that having this enthusiasm as the main motivation to write and produce music 
is a key element in obtaining successful results. Now you are ready to take the next step in sequencing 
and production techniques in order to improve the quality of your work. Remember that the final qual­
ity of your music depends on many variables, including your skills with and knowledge of sequencing 
techniques, the equipment you use, the software, and the environment (meaning essentially the studio) in 
which you work. In fact, the studio is one of the most important elements involved in the creative proc­
ess of composing music. I am not talking just in terms of equipment, machines, and software (which I 
will discuss in detail in a moment), but also in terms of comfort, ease of use of the working environment 
and accessibility of the various functions that your studio provides. These are all qualities that are essen­
tial if you are going to spend many hours composing and sequencing your projects. Your studio should 
have good illumination, both natural and artificial. If you are going to use electric light as a main source 
for illumination, try to avoid lights with dimmer switches, since they are known for causing interfer­
ence with studio recording equipment. I particularly recommend having as much natural light as pos­
sible. You would be amazed how much an extra window can improve the overall working experience 
in your studio, particularly when having to spend several hours in a row sitting in front of a computer 
and a mixer.
Acoustic isolation and acoustic treatment of the room are also important elements that will help 
avoid external noises and create well-balanced mixes. Even though the subject of acoustic isolation and 
treatment goes beyond the scope of this manual, here are some basic rules to follow when building your 
studio. First of all, try to avoid (if possible) perfectly square or rectangular rooms. These are the most 
problematic because the parallel walls can create unwanted phasing effects and standing waves. You will 
soon realize that, unless you build an environment designed specifically to host a studio, most rooms are 
in fact rectangular. Therefore, I recommend the use of absorption panels to reduce excessive reverbera­
tion caused by reflective and parallel surfaces, such as flat and smooth walls. Absorption panels (Figure 
1.1) help reduce excessive reverberation, their main function being to stop the reflection of high fre­
quencies. As a rule of thumb, try to avoid covering your entire studio with absorption panels since this 
would make your room a very acoustically dry listening environment, which not only would cause hear­
ing fatigue but also would mislead your ears during your final mixes.
In order to reduce standing waves, you should use diffusers (Figure 1.2) on the walls and ceiling of 
the room. The main purpose of diffusers is to reflect the sound waves at angles that are different (mostly 
wider) than the original angle of incidence and thereby to limit the audio artifacts caused by parallel walls.

2
CHAPTER 1  Setting up Your Creative Environment: The Studio
The use of bass-traps (Figure 1.3) will help to reduce low-frequency standing waves. By placing 
them in the top corners of the room you will avoid annoying bass buildup frequencies.
If you have a tight budget or construction limitations you can achieve similar acoustic treatment 
effects by making sure to have the walls of your studio covered with a variety of surfaces and materials. 
For example, accurately placed bookshelves and heavy curtains are excellent low-budget solutions to 
improve the acoustic response of a room. For more detailed information on studio acoustics and studio 
design I highly recommend Recording Studio Design, by Philip Newell.
1.2  SETTING UP YOUR PRODUCTION SPACE: THE PROJECT STUDIO
Although up to four or five years ago the distinction between a home studio, a project studio, and a 
recording studio was very clear, these days things are much more blurred. A home studio can often be all 
FIGURE 1.2
Example of diffusers.
(© Primacoustic, a division of Radial Engineering Ltd).
FIGURE 1.3
Example of bass-traps.
(© Primacoustic, a division of Radial Engineering Ltd).
FIGURE 1.1
Example of absorption panels.
(© Primacoustic, a division of Radial Engineering Ltd).

3
1.3  The Music Equipment
you need to record and produce your projects (particularly if the use of acoustic instruments is limited). 
In addition, the size and sophistication of what we used to consider a home studio have increased, blur­
ring even further the line between home and project studio; therefore, from now on I will refer to your 
working environment as the project studio, independently of space size or complexity of the equipment. 
A project studio originally meant a studio designed and built specifically for a particular project. More 
recently, the term has shifted to indicate a studio that is smaller than a fully-fledged recording studio 
(and not designed to handle large live recording sessions). A project studio is built around a medium-
sized control room that serves as the main writing room and that can also be used to track electric 
instruments, such as electric guitars or basses, if necessary. A small or medium-sized iso-booth is often 
included in order to allow the recording of vocalists, voiceovers, or solo instruments. The size and layout 
can change drastically, depending on the location and budget, but it is important to understand what the 
main elements are that are indispensable to create an efficient, powerful, and flexible studio for the mod­
ern composer. In my project studio (Figures 1.4 and 1.5) I have a large control/writing room where all 
my gear (computers, synthesizers, controllers, etc.) resides. Adjacent to the control room I have a small 
room that serves as a recording booth for vocal and acoustic instruments tracks. I used little acoustic 
treatment in the control room and the booth since a series of bookshelves provides good natural diffu­
sion, but I would recommend paying particular attention to your specific situation.
The equipment around which your project studio is going to be built can be divided into three main 
general categories: music equipment, computer equipment, and software. All these elements are indis­
pensable for composers to achieve the best results for their productions. In the modern project studio the 
music equipment can be further divided into three subgroups: electronic instruments, acoustic instru­
ments, and sound/audio equipment. Remember that every element plays a very important and essential 
role in the music-making process. Let’s take a closer look at each category to help you make the right 
decision when building your composing environment.
1.3  THE MUSIC EQUIPMENT
The music equipment constitutes the “muscles” of your studio. This category includes everything 
related to the actual writing/sequencing, playing, and mixing of your production. In general, the acoustic 
FIGURE 1.4
My project studio setup: the gear.

4
CHAPTER 1  Setting up Your Creative Environment: The Studio
instruments’ setup can vary from studio to studio and from composer to composer. If you are a composer 
who plans to produce in a variety of styles and genres, and for a wide assortment of ensembles, I recom­
mend having available a good selection of acoustic instruments. Therefore, besides your main instru­
ments you should have a series of other ones that you will use to produce and record. Having a wider 
acoustic palette available when writing, arranging, and producing will increase exponentially the quality 
of your final productions. Being a bass player, I have available in my studio different basses (acoustic, 
electric, and stick) that give me a good starting palette for creating convincing bass tracks in a variety 
of styles. In addition, I have two guitars (acoustic and electric) and several percussion instruments. You 
don’t need to have very expensive or extremely rare acoustic instruments; as long as they sound good 
and have a nice feel you are all set. The bottom line is that you should have everything you need (and 
can afford) to have as much flexibility as possible when composing and producing.
While any acoustic instrument interfaces easily with any other component of a studio, the electronic 
instrumentation and the digital/analog audio equipment need to be accurately coordinated and integrated, 
in order to achieve the most efficient and trouble-free production environment. The modern project stu­
dio is based on two different signal paths: musical instrument digital interface (MIDI) and audio (analog 
and digital). These two paths interact with one another during the entire production, integrating with 
and complementing each other. The electronic instruments are connected to each other through the 
MIDI network, while the audio components of your studio are connected through the audio network. 
At the most basic level you can think of the MIDI devices in your studio as the interface with which 
you will interact with the other electronic instruments. Take a look at Figure 1.6. The MIDI network 
includes all devices that exchange data and information using the MIDI standard. The audio network 
includes devices connected using audio cables (either analog/digital and balanced/unbalanced connec­
tions, depending on the type of device). As you can see, at the center of both networks is the computer 
(or computers) that among other very important tasks such as recording/editing MIDI and audio, serving 
as a virtual mixer and a virtual sound generator (software synthesizers), has the crucial role of being the 
main hub for all the data in your studio.
FIGURE 1.5
My project studio setup: acoustic treatment.

5
1.3  The Music Equipment
1.3.1  MIDI Equipment and MIDI Messages
Before moving on to analyze the details of each signal path, it is useful to know more about the MIDI 
standard, its evolution, and its characteristics. While some believe (incorrectly) that MIDI is not used any­
more in the modern digital audio workstation (DAW) era, this could not be further from the truth. For the 
past 25 years MIDI has been the only valid and widely accepted standard to allow controllers, hardware 
synthesizers, and computers to exchange data. Even though the standard is fairly old it was originally con­
ceived with a very open structure that would be future proof and that is why the modern composer is still 
using it constantly. MIDI has naturally evolved and improved over the years and while some of its origi­
nal features have been less useful others have been proved essential to today’s writing and production.
In order to understand better such an important tool let’s take a look at the evolution of MIDI and at its 
future. MIDI was established in 1983 as a protocol to allow different devices to exchange data. In particu­
lar, the major manufacturers of electronic musical instruments were interested in adopting a standard that 
would allow keyboards and synthesizers from different companies to interact with each other. The answer 
was the MIDI standard. With the MIDI protocol, the general concept of interfacing (meaning to establish 
FIGURE 1.6
MIDI and audio network in your studio.
(Courtesy of Roland Corporation US, Avid Technology, © 2010, Apple Inc., Akai Professional).

6
CHAPTER 1  Setting up Your Creative Environment: The Studio
a connection between two or more components of a system) is applied to electronic musical instruments. 
As long as two components (synthesizers, sound modules, computers, etc.) have a MIDI interface, they 
are able to exchange data. In early synthesizers the data were mainly notes played on keyboards that could 
be sent to another synthesizer. This allowed the keyboard players to layer two sounds without having to 
play simultaneously the same part with both hands on two different synthesizers. Nowadays the specifica­
tions of MIDI data have been extended considerably, ranging from notes to Control Changes (CCs), from 
System Exclusive messages to synchronization messages (i.e., MTC, MIDI Clock, etc.).
The MIDI standard is based on 16 independent channels on which MIDI data are sent to and received 
from the devices. On each channel a device can transmit messages that are independent from the other chan­
nels. When sending MIDI data the transmitting device stamps on each message the channel on which the 
information is sent so that the receiving device will assign it to the right receiving channel. One aspect of 
MIDI that is important to understand and remember is that MIDI messages do not contain any information 
about audio. MIDI and audio signals are always kept separate. Think of MIDI messages as the notes that a 
composer would write on paper; when you record a melody as MIDI data, for example, you write the notes 
in a sequencer but you do not actually record their sound. While the sequencer records the notes, it is up to 
the synthesizers and sound modules connected to the MIDI system to play back the notes received through 
their MIDI interfaces. The role of the sequencer in the modern music production process is very similar to 
that of the paper score in the more traditional compositional process. You sketch and write (sequence) the 
notes of your composition on a sequencer, then you have your virtual musicians (synthesizers, samplers, 
etc.) play back your composition. This is the main feature that makes MIDI such an amazing and versatile 
tool for music production. If you are dealing only with notes and events instead of sound files, the editing 
power available is much greater, meaning that you are much freer to experiment with your music.
Here is a quick example to illustrate this concept. In Figure 1.7 we see a simple melody that was 
sequenced using a MIDI keyboard controller. The sequencer (in this example, Logic Pro, LP), after 
recording the notes played on the MIDI keyboard, shows the part as notation (there are many other ways 
of looking at MIDI data; we will learn other editing techniques later). Since the sequencer (LP) is deal­
ing with performance data only, you are free to change any aspect of the music; for example, the pitch 
of the notes, their position in time, and the tempo of the piece. In Figure 1.8 I have changed the key, the 
upbeat of bars 60–66, and the tempo (from 120 to 110 beats per minute, or BPM).
FIGURE 1.7
MIDI data shown in notation format in Logic Pro.

7
1.3  The Music Equipment
Every device that needs to be connected to a MIDI studio or system needs to have a MIDI interface. 
The MIDI standard uses three ports to control the data flow: IN, OUT, and THRU. The connectors for 
the three ports are all the same: a five-pin DIN female port on the device (Figure 1.9) and a correspond­
ing male connector on the cable.
While the OUT port sends out MIDI data generated from a device, the IN port receives the data. The 
THRU port is used to send out an exact copy of the messages received from the IN port, and it can be 
utilized in a particular setup called Daisy Chain, which I will describe in a moment. Of course, as I men­
tioned earlier, in order to be connected to the MIDI network a device needs to be equipped with a MIDI 
interface. Nowadays all the professional electronic music instruments, such as synthesizers, sound mod­
ules, and hardware sequencers, have a built-in MIDI interface. It is important to note that in the mod­
ern digital studio setup the presence of external hardware gear has been reduced to a minimum owing 
to the increasing power, flexibility, and cost-effectiveness of personal computers. This has changed how 
we design and conceive the modern home and project studio. Whereas in the past a complicated MIDI 
network was almost an essential part of the working environment, owing to the large use of hardware syn­
thesizers, nowadays things have changed drastically. The explosion of the software synthesizers market 
(sound generator engines that run on your computer instead of on dedicated hardware platforms) has the 
main advantage, among many others, of reducing cable clutter and simplifying the setup immensely (more 
on this later). Since the computer is usually not equipped with a built-in MIDI interface, if you want to 
have it connected to a MIDI device through the IN and OUT ports, you will need to expand its I/O with 
an external MIDI interface, which usually connects to the computer through a universal serial bus (USB) 
interface. The MIDI data can eventually be recorded by a device called a sequencer. Such a device records, 
FIGURE 1.8
MIDI data edited.
FIGURE 1.9
Standard MIDI ports.
(Courtesy of Roland Corporation US).

8
CHAPTER 1  Setting up Your Creative Environment: The Studio
stores, edits, and plays back MIDI data. In simple words, a sequencer acts as a digital tape recorder for 
MIDI data; we can record the data on a MIDI track, edit them as we want, and then play them back.
Before we go on to learn how to connect and setup the production studio and work on your creative 
sequencing skills, you need to understand thoroughly the structure, potential and strengths of the MIDI 
protocol. Because the number of messages that constitute the MIDI standard is very high, it is practical 
to separate them into two main categories: Channel messages and System messages. Channel messages 
are further subdivided into Channel Voice and Channel Mode messages, while System messages are sub­
divided into Real-Time, Common, and Exclusive. Table 1.1 illustrates how they are organized.
1.3.2  Channel Voice Messages
Channel Voice messages are probably the most used and are the most important in terms of sequencing 
and production, because they carry the information about the performance, meaning, for example, which 
notes we played and how hard we pressed the trigger on the controller.
Note On message: This message is sent every time you press a key on a MIDI controller; as soon as 
you press it, a MIDI message (in the form of binary code) is sent to the MIDI OUT of the transmit­
ting device. The Note On message includes information about the note you pressed (the note number 
ranges from 0 to 127, or C-2 to G-8), the MIDI channel on which the note was sent (1 to 16), and the 
velocity-on parameter, which describes how hard you press the key (this ranges from 0 to 127).
Note Off message: This message is sent when you release the key of the controller. Its function is to 
terminate the note that was triggered with a Note On message. The same result can be achieved by 
sending a Note On message with its velocity set to 0, a technique that can help to reduce the stream 
of MIDI data. It contains the velocity-off parameter, which registers how hard you released the key 
(note that this particular information is not used by most of the MIDI devices at the moment).
Aftertouch: This is a specific MIDI message sent after the Note message. When you press a key of 
a controller a Note On message is generated and sent to the MIDI OUT port; this is the message that 
triggers the sound on the receiving device. If you push a little bit harder on the key, after hitting it, an 
extra message called Aftertouch is sent to the MIDI OUT of the controller. The Aftertouch message is 
usually assigned to control the vibrato effect of a sound. But, depending on the patch that is receiving 
it, it can also affect other parameters, such as volume, pan and more.
There are two types of Aftertouch: polyphonic and monophonic. Monophonic Aftertouch affects 
the entire range of the keyboard no matter which key or keys triggered it. This is the most common 
type of Aftertouch, and it is implemented on most (but not all) controllers and MIDI synthesizers 
available on the market. Polyphonic Aftertouch allows you to send an independent message for each 
key. It is more flexible since only the intended notes will be affected by the effect.
Table 1.1  List of MIDI Messages Organized by Category.
Channel Messages
System Messages
Channel Voice: Note On, Note Off, Monophonic 
Aftertouch, Polyphonic Aftertouch, Pitch Bend, 
Program Change, Control Changes
System Real-Time: Timing Clock, Start, Stop, 
Continue, Active Sensing, System Reset
Channel Mode: All Notes Off, Local Control (On/Off), 
Poly On/Mono On, Omni On, Omni Off, All Sound Off, 
Reset All Controllers
System Common: MIDI Time Code (MTC), Song 
Position Pointer, Song Select, Tune Request, End of 
System Exclusive
System Exclusive

9
1.3  The Music Equipment
Pitch Bend: This message is controlled by the Pitch Bend wheel on a keyboard controller. It allows 
you to raise or lower the pitch of the notes being played. It is one of the few MIDI data that does not 
have a range of 128 steps. In order to allow a more detailed and accurate tracking of the transposi­
tion, the range of this MIDI message extends from 0 to 16,383. Usually, a sequencer would display 0 
as the center position (non-transposed), 8191 fully raised, and 8192 fully lowered.
Program Change: This message is used to change the patch assigned to a certain MIDI channel. Each 
synthesizer has a series of programs (also called patches, presets, instruments or, more generically, 
sounds) stored in its internal memory; for each MIDI channel we need to assign a patch that will play 
back all the MIDI data sent to that particular channel. This operation can be done by manually chang­
ing the patch from the front panel of the synthesizer, or by sending a program change message from a 
controller or a sequencer. The range of this message is 0 to 127. As modern synthesizers can store many 
more than 128 sounds, nowadays programs are organized into banks, where each bank stores a maximum 
of 128 patches. In order to change a patch through MIDI messages it is, therefore, necessary to combine a 
bank change message and a program change message. While the latter is part of the MIDI standard speci­
fication, the former changes depending on the brand and model of MIDI device. Most devices use CC 0 
or CC 32 to change bank (or sometimes a combination of both), but you should refer to the synthesizer’s 
manual to find out which MIDI message is assigned to bank change for that particular model and brand.
Control Changes (CCs): These messages allow you to control certain parameters of a MIDI chan­
nel. There are 128 CCs (0–127); the range of each controller goes from 0 to 127. Some of these 
controllers are standard and are recognized by all MIDI devices. Among these the most important 
(mainly because they are used more often in sequencing) are CCs 1, 7, 10, and 64. CC 1 is assigned to 
Modulation. It is activated by moving the Modulation wheel on a keyboard controller (Figure 1.10). 
It is usually associated with a slow vibrato effect but it can be assigned to control pretty much any 
FIGURE 1.10
The Pitch Bend (left) and the Modulation wheels (right) on a MIDI controller.
(Courtesy of Avid Technology, © 2010).

10
CHAPTER 1  Setting up Your Creative Environment: The Studio
parameter of a software synthesizer. CC 7 controls the volume of a MIDI channel from 0 to 127. 
CC 10 controls its pan. Value 0 is pan hard left, 127 is hard right, and 64 is centered.
Controller number 64 is assigned to the Sustain pedal (the notes played are held until the pedal is 
released). This controller has only two positions: on (values  64) and off (values  63). While the 
four controllers mentioned above are the most basic ones, there are other controllers (such as CC 2 
Breath, CC 5 Portamento Value, and CC 11 Expression, for example) that can considerably enhance 
your sequences and the control that you have over the sound of your MIDI devices. Table 1.2 lists all 
128 controllers with their specifications and their most common uses in sequencing situations.
Table 1.2  MIDI Control Change (CC) Messages.
Controller no.
Function
Use
  0
Bank Select
Allows you to switch Bank for Patch selection. Sometimes 
used in conjunction with CC 32 to send Bank number 
higher than 128
  1
Modulation
Sets the Modulation Wheel to the specified value. Usually 
this parameter controls a Vibrato effect generated through 
an LFO. It can also be used to control other sound 
parameters such as volume in certain sound libraries
  2
Breath Controller
Can be set to affect several parameters but usually is 
associated with Aftertouch messages
  3
Undefined
  4
Foot Controller
Can be set to affect several parameters but usually is 
associated with Aftertouch messages
  5
Portamento Value
Controls the rate used by Portamento to slide between two 
subsequent notes
  6
Data Entry (MSB)
Controls the value of either RPN or NRPN parameters
  7
Volume
Controls the Volume level of a MIDI channel
  8
Balance
Controls the Balance (Left and Right) of a MIDI channel. 
Mostly used on patches that contain stereo elements (such 
as stereo patches). 64    Centre, 127    100% Right, 
0    100% Left
  9
Undefined
10
Pan
Controls the Pan of a MIDI channel. 64    Centre, 
127    100% Right, 0    100% Left
11
Expression
Controls a percentage of Volume (CC 7)
12
Effect Controller 1
Mostly used to control the effect parameter of one of the 
internal effects of a synthesizer (e.g., the Decay Time of a 
Reverb)
13
Effect Controller 2
Mostly used to control the effect parameter of one of the 
internal effects of a synthesizer
14–15
Undefined
16–19
General Purpose
These controllers are open and they can be assigned to 
Aftertouch or similar messages
20–31
Undefined
(Continued)

11
1.3  The Music Equipment
Table 1.2  (Continued) 
Controller no.
Function
Use
32–63
LSB for Control 0–31
These controllers allow you to have a finer scale for the 
corresponding controllers 0–31
64
Sustain Pedal
Controls the sustain function of a MIDI channel. It has 
only two positions: Off (values between 0 and 63) and On 
(values between 64 and 127)
65
Portamento On/Off
Controls whether the Portamento effect (slide between two 
subsequent notes) is On or Off. It has only two positions: 
Off (values between 0 and 63) and On (values between 64 
and 127)
66
Sostenuto On/Off
Similar to the Sustain controller, but holds only the notes 
that are already turned On when the pedal was pressed. 
Ideal for the “Chord Hold” function, where you can have 
one chord holding while playing a melody on top. It has 
only two positions: Off (values between 0 and 63) and On 
(values between 64 and 127)
67
Soft Pedal On/Off
Lowers the volume of the notes that are played. It has 
only two positions: Off (values between 0 and 63) and On 
(values between 64 and 127)
68
Legato Footswitch
Produces a Legato effect (two subsequent notes without 
pause in between). It has only two positions: Off (values 
between 0 and 63) and On (values between 64 and 127)
69
Hold 2
Prolongs the release of the note (or notes) playing while the 
controller is On. Unlike the Sustain controller (CC 64), the 
notes will not sustain until you release the pedal but instead 
they will fade out according to their release parameter
70
Sound Controller 1
Usually associated with the way the synthesizer produces 
the sound. It can control, for example, the sample rate of a 
waveform in a wave table synthesizer
71
Sound Controller 2
Controls the envelope over time of the VCF of a sound, 
allowing you to change over time the shape of the filter. 
Also referred to as Resonance
72
Sound Controller 3
Controls the release stage of the VCA of a sound, allowing 
you to adjust the sustain time of each note
73
Sound Controller 4
Controls the attack stage of the VCA of a sound, allowing 
you to adjust the time that the wave form takes to reach its 
maximum amplitude
74
Sound Controller 5
Controls the filter cutoff frequency of the VCF, allowing you 
to change the brightness of the sound
75–79
Sound Controller 6–10
Generic controllers that can be assigned by a manufacturer 
to control non-standard parameters of a sound generator
80–83
General Purpose 
Controllers
Generic button-switch controllers that can be assigned to 
various On/Off parameters. They have only two positions: 
Off (values between 0 and 63) and On (values between 64 
and 127)
84
Portamento Control
Controls the amount of Portamento
85–90
Undefined
(Continued)

12
CHAPTER 1  Setting up Your Creative Environment: The Studio
Table 1.2  (Continued)
Controller no.
Function
Use
  91
Effect 1 Depth
Controls the depth of Effect 1 (mostly used to control the 
Reverb Send amount)
  92
Effect 2 Depth
Controls the depth of Effect 2 (mostly used to control the 
Tremolo amount)
  93
Effect 3 Depth
Controls the depth of Effect 3 (mostly used to control the 
Chorus amount)
  94
Effect 4 Depth
Controls the depth of Effect 4 (mostly used to control the 
Celeste or detune amount)
  95
Effect 5 Depth
Controls the depth of Effect 5 (mostly used to control the 
Phaser effect amount)
  96
Data Increment (1)
Mainly used to send an increment of data for RPN and 
NRPN messages
  97
Data Increment (1)
Mainly used to send a decrement of data for RPN and 
NRPN messages
  98
Non-Registered 
Parameter Number 
(NRPN) LSB
Selects the NRPN parameter targeted by controllers 6, 38, 
96, and 97
  99
Non-Registered 
Parameter Number 
(NRPN) MSB
Selects the NRPN parameter targeted by controllers 6, 38, 
96, and 97
100
Registered Parameter 
Number (RPN) LSB
Selects the RPN parameter targeted by controllers 6, 38, 
96, and 97
101
Registered Parameter 
Number (RPN) MSB
Selects the RPN parameter targeted by controllers 6, 38, 
96, and 97
102–119
Undefined
120
All Sound Off
Mutes all sounding notes regardless of their release time 
and regardless of whether the Sustain Pedal is pressed
121
Reset All Controllers
Resets all the controllers to their default status
122
Local On/Off
Enables you to turn the internal connection between the 
keyboard and its sound generator On or Off. If you use 
your MIDI synthesizer on a MIDI network, you will probably 
need the Local to be turned Off to avoid notes being 
played twice
123
All Notes Off
Mutes all sounding notes. The notes that are turned off by 
this message will still retain their natural release time. Notes 
that are held by a Sustain Pedal will not be turned off until 
the pedal is released
124
Omni Mode Off
Sets the device to Omni Off mode
125
Omni Mode On
Sets the device to Omni On mode
126
Mono Mode
Switches the device to monophonic operation
127
Poly Mode
Switches the device to polyphonic operation
LFO: low-frequency oscillator; LSB: least significant byte; RPN: registered parameter number; NRPN: non-registered parameter 
number; MIDI: musical instrument digital interface; MSB: most significant byte; VCF: voltage control filter; 
VCA: voltage control amplifier.

13
1.3  The Music Equipment
1.3.3  Channel Mode Messages
This category includes messages that affect mainly the MIDI setup of a receiving device. It is worth not­
ing that while the MIDI messages that fall into this category are relevant and useful when dealing with 
hardware synthesizers, in a software synthesizer environment they lose most of their relevance since the 
majority of the controls of a software synthesizer are usually addressed through a graphic interface.
All Notes Off: This message turns off all the notes that are sounding on a MIDI device. Sometimes 
it is also called the “panic” function, since it is a remedy against “stuck notes”, meaning MIDI notes 
that were turned on by a Note On message but that for some reason (data dropout, transmission error, 
etc.) were never turned off by a Note Off message. It can also be activated through CC 123.
Local On/Off: This message is targeted to MIDI synthesizers. These are devices that feature a key­
board, a MIDI interface, and an internal sound generator. The “local” is the internal connection 
between the keyboard and the sound generator. If the local parameter is On, then the sound generator 
receives the triggered notes directly from the keyboard and also from the IN port of the MIDI inter­
face (Figure 1.11). This setting is not recommended in a sequencing/studio situation since the sound 
generator would play the same notes twice, reducing its polyphony (the number of notes that the 
sound generator can play simultaneously) by half. It is, however, the right setup for a live situation in 
which the MIDI ports are not used.
If the local parameter is switched Off (Figure 1.12), then the sound generator receives the trig­
gered notes only from the MIDI IN port, which makes this setting ideal for the MIDI studio. The 
local setting usually can also be accessed from the “MIDI” or “General” menu of the device or can 
be triggered by CC 122 (0–63 is Off, 64–127 is On).
Poly/Mono: A MIDI device can be set as polyphonic or monophonic. If set up as Poly, the device 
will respond as polyphonic, meaning it will be able to play more than one note at the same time. If 
set up as Mono, the device will respond as monophonic, meaning it will be able to play only one note 
at a time per MIDI channel (the number of channels can be specified by the user). In the majority of 
situations we will want a polyphonic device, to take advantage of the full potential of the synthesizer. 
The Poly/Mono parameter is usually found in the “MIDI” or “General” menu of the device, but it can 
also be selected, through CC 126 (Mono) and CC 127 (Poly).
Omni On/Off: This parameter controls how a MIDI device responds to incoming MIDI messages. 
If a device is set to Omni On, then it will receive on all 16 MIDI channels (regardless of its channel) 
FIGURE 1.11
Local set to “ON” on a hardware MIDI synthesizer.

14
CHAPTER 1  Setting up Your Creative Environment: The Studio
but redirect all the incoming MIDI messages to only one MIDI channel (the current one) (Figure 
1.13). Omni On can also be selected through CC 125.
If a device is set to Omni Off, then it will receive on all 16 MIDI channels, with each message 
received on the original MIDI channel on which it was sent (Figure 1.14). This setup is the one most 
used in sequencing, since it allows you to take full advantage of the 16 MIDI channels on which a 
device can receive. Omni Off can also be selected through CC 124.
FIGURE 1.13
Omni “ON” on a MIDI synthesizer.
FIGURE 1.12
Local set to “OFF” on a hardware MIDI synthesizer.

15
1.3  The Music Equipment
All Sound Off: This is similar to the All Notes Off message, but it does not apply to notes being 
played from the local keyboard of the device. In addition, this message mutes the notes immediately, 
regardless of their release time and whether or not the hold pedal is pressed.
Reset All Controllers: This message resets all controllers to their default state.
1.3.4  System Real-Time Messages
The Real-Time messages (like all the other system messages) are not sent to a specific channel like the 
Channel Voice and Channel Mode messages. Instead, they are sent globally to the MIDI devices in your 
studio. These messages are mainly used to synchronize all the MIDI devices in your studio that are clock 
based, such as sequencers and drum machines.
Timing Clock: This is a message specifically designed to synchronize two or more MIDI devices 
that need to be locked in to the same tempo. The devices involved in the synchronization proc­
ess need to be set up in a master–slave configuration, where the master device (sometimes labeled 
Internal Clock) sends out the clock to the slaved devices (External Clock). It is sent 24 times per 
quarter note, and therefore its frequency changes with the tempo of the song (tempo based). It is also 
referred to as the MIDI Clock or sometimes the MIDI Beat Clock.
Start, Continue, Stop: These messages allow the master device to control the status of the slave 
devices. Start instructs the slaved devices to go to the beginning of the song and start playing at the 
tempo established by the incoming Timing Clock. Continue is similar to Start, the only difference 
being that the song will start playing from the current position instead of from the beginning of the 
song. The Stop message instructs the slaved devices to stop and wait for either a Start or a Continue 
message to restart.
FIGURE 1.14
Omni “OFF” on a MIDI synthesizer.

16
CHAPTER 1  Setting up Your Creative Environment: The Studio
Active Sensing: This is a utility message that is implemented only on some devices. It is sent every 
300 ms or less and is used by the receiving device to detect whether the sending device is still con­
nected. If the connection is interrupted for some reason (e.g., the MIDI cable was disconnected), then 
the receiving device will turn off all its notes to avoid having stuck notes that keep playing.
System Reset: This restores the receiving devices to their original power-up conditions. It is not 
commonly used.
1.3.5  System Common Messages
The System Common messages are not directed to a specific channel and they are common to all receiv­
ing devices.
MIDI Time Code (MTC): This is another syncing protocol that is time based (as opposed to MIDI 
Clock, which is tempo based), and is mainly used to synchronize non-linear devices (such as sequenc­
ers) to linear devices (such as tape-based machines). It is a digital translation of the more traditional 
SMPTE (Society of Motion Picture and Television Engineers) code used to synchronize non-linear 
machines. The format is the same as SMPTE. The position in the song is described in hours: minutes: 
seconds: frames (subdivisions of 1 second). The frame rates vary depending on the format used. If you 
are dealing with video, then the frame rate is dictated by the video frame rate of your project. If you are 
using MTC simply to synchronize music devices, then it is advised to use the highest frame rate avail­
able. The frame rates are 24, 25, 29.97 Non-Drop, 29.97 Drop, 30 Non-Drop, and 30 Drop. We discuss 
the synchronization issues related to sequencing in more detail in Chapters 3 and 7.
Song Position Pointer: This message tells the receiving devices which bar and beat to jump to. It is 
mainly used in conjunction with the MIDI Clock message in a master–slave MIDI synchronization 
situation.
Song Select: This message allows you to call up a particular sequence or song from a sequencer that 
can store more than one project at the same time. Its range goes from 0 to 127, thus allowing a total 
of 128 songs to be recalled.
Tune Request: This message is used to retune certain digitally controlled analog synthesizers that 
require their tuning to be adjusted after hours of use. This function no longer really applies to modern 
devices and it is rarely used.
End of System Exclusive: This message is used to mark the end of a System Exclusive message (see 
the next section).
1.3.6  System Exclusive Messages
System Exclusive messages (SysEx) are very powerful MIDI messages that allow you to control any 
parameter of a specific device through the MIDI standard. SysEx messages are specific for each manufac­
turer, brand, model, and device, and therefore they cannot be listed like the other generic MIDI messages 
analyzed so far. In the manual of each device is a section in which all the SysEx messages for that par­
ticular model are listed and explained. These messages are particularly useful for parameter editing pur­
poses. Programs called editors/librarians use the computer to send SysEx messages to connected MIDI 
devices in order to control and edit their parameters, making the entire patch editing procedure on hard­
ware devices much simpler and faster.
Another important application of SysEx is the MIDI data bulk dump. This feature allows a device 
to send system messages that describe the internal configuration of that machine and all the parameters 
associated with it, such as patch/channel assignments and effects setting. These messages can be recorded 
by a sequencer connected to the MIDI OUT of the device and played back at a later time to restore that 

17
1.4  MIDI Components
particular configuration, making it a flexible archiving system for the MIDI settings of the devices. I dis­
cuss the details of this technique in Chapter 4, which is dedicated to advanced sequencing techniques. As 
I mentioned earlier, SysEx messages lost most of their relevancy with the introduction of software synthe­
sizers. Since these synthesizers are closely linked and interleaved with their host running on the computer, 
all the editing can be done directly through their graphic user interfaces. In addition, all their parameters 
are automatically saved inside the sequence, making the use of SysEx MIDI Dump unnecessary.
1.4  MIDI COMPONENTS: CONTROLLERS, HARDWARE SYNTHESIZERS/
SOUND MODULES, SOFTWARE SYNTHESIZERS AND SEQUENCERS
It is very important to choose the right MIDI devices and instruments to use in your studio. Remember 
that they are the virtual musicians that will be featured in your music productions, so it is essential to 
have the right type of equipment, the right variety of instruments, and a very flexible and versatile pal­
ette of sonorities to choose from in order to be an all-round composer. MIDI devices can be divided 
into four main categories: MIDI keyboard synthesizers (or MIDI synthesizers), MIDI sound modules (or 
sound expanders), keyboard controllers, and software synthesizers (hosted inside your computer by your 
sequencer or host). The main difference between the first three is based on the presence or lack of a 
built-in sound generator and keyboard. Keep in mind that, as underlined earlier in this chapter, all the 
devices that are going to be part of your MIDI network must be equipped with a MIDI interface. 
The interface is built into all the professional synthesizers, controllers, and sound modules available on 
the market. The only exceptions are vintage machines made before 1983.
1.4.1  MIDI Synthesizer
MIDI synthesizers (Figures 1.11 and 1.12) feature a MIDI interface, an internal sound generator, and a 
keyboard to output MIDI data. If they come equipped with a built-in sequencer then the term MIDI work­
station is more appropriate, since they can be used as standalone MIDI production studios. The MIDI 
synthesizer is probably the device you are most familiar with. It is also the most complete, since it allows 
you to control an external MIDI device through the keyboard and can also produce sounds through the 
internal sound generator. Notice how the three elements are connected to one another. The keyboard 
sends signals (according to which note you pressed) to both the MIDI OUT and the internal sound gen­
erator, which also receives MIDI messages from the MIDI IN port. Whereas in the past this category of 
MIDI instrument constituted the core of a production studio, these days hardware synthesizers play a big­
ger role in live performance settings than in studio work. Their role has been replaced almost entirely by 
the combination of MIDI keyboard control with software synthesizers (more on this later in this chapter).
1.4.2  Keyboard Controller
A modification of the MIDI synthesizer is the keyboard MIDI controller (Figure 1.15). This device fea­
tures only a MIDI interface (usually only a MIDI OUT) and a keyboard. There is no internal sound mod­
ule. In fact, it is called a controller because its only use is to control other MIDI devices attached to its 
MIDI OUT port. I am going to discuss different types of MIDI controllers later.
1.4.3  Sound Module
Depending on the equipment, the features of the devices, and the number of MIDI devices involved 
in your project studios, you can set up the MIDI network in different ways. In most project studio 

18
CHAPTER 1  Setting up Your Creative Environment: The Studio
situations one MIDI controller is sufficient. Through the controller you can output MIDI messages (e.g., 
Notes, Control Changes) to other MIDI devices and/or to a sequencer. However, a professional MIDI 
studio will always need new and fresh palettes of sounds. What characterizes the successful studio and 
the modern composer is the variety and flexibility of sounds and musical textures available. While in 
the pre-MIDI era synthesizers were only available with a built-in keyboard, therefore taking up a lot of 
space and costing more money, nowadays we can expand the sound palette of our virtual orchestra by 
using sound modules (or expanders). A sound module (Figure 1.16) features only a MIDI interface and 
a sound generator but not a keyboard. The advantage of this type of device is that it delivers the same 
power as a MIDI synthesizer in terms of sounds but in a more compact design and at a lower price.
The same argument I presented earlier when discussing the MIDI synthesizer can be applied for the 
sound module too. The software synthesizer has replaced almost entirely hardware-based sound genera­
tors, simplifying and streamlining the studio setup considerably.
1.4.4  Software Synthesizers
Software synthesizers have practically become the principal source of sounds in the modern project 
studio. Whereas up to five or six years ago your primary sound generators were hardware synthesiz­
ers (MIDI synthesizers and sound modules), nowadays most of the hardware gear has been replaced by 
software-based synthesizers. The main idea behind this approach resides in the fact that the modern dig­
ital hardware synthesizer is nothing more than a basic computer, with a dedicated central processing unit 
(CPU) inside and software specifically written for that CPU that runs on it (Figure 1.17). The software 
has the functions of allowing the user to interface with the CPU’s sound generators and, at the same 
time, to determine the type of synthesis used to generate the sounds (Figure 1.18). While this approach 
has worked extremely well for many decades, with the advent of faster and more sophisticated personal 
computers the need for a dedicated CPU has vanished. Therefore, software (and hardware) music com­
panies started to take advantage of the powerful and highly versatile CPUs of the personal computers to 
generate sounds by simply writing software that would run on these CPUs. This approach has three main 
advantages: first, it is much cheaper to write software than to build a hardware synthesizer, therefore the 
prices of software synthesizers are much lower than their hardware counterparts. Even including the ini­
tial cost of a desktop computer, in the long run it is cheaper to buy software synthesizers. Second, soft­
ware synthesizers are much more flexible, versatile, and upgradable than hardware synthesizers. Finally, 
since they take advantage of a large monitor and the graphic interaction tools of the computer they are 
FIGURE 1.16
The sound module.
FIGURE 1.15
The MIDI keyboard controller.

19
1.4  MIDI Components
much easier to program. Even in live settings these days laptops and software synthesizers are rapidly 
replacing MIDI hardware synthesizers.
One of the major drawbacks is that you need a very speedy computer to keep up with multiple soft­
ware synthesizers running inside the DAW. In addition, with the CPU requirements of software synthe­
sizers increasing exponentially every year you can find yourself in need of a new computer at least every 
three or four years to keep up with the latest version of your favorite soft synth. Instead of buying a 
hardware box from your local music store, in the case of a soft synth you buy only the software that will 
be installed on your computer through either a boxed CD/DVD or through a simple download (the latter 
is becoming more and more popular among distributors). Depending on the type of product you bought 
(types of synthesis, quality of sounds, etc.), the space taken up by installing a soft synth on the hard disk 
FIGURE 1.17
The hardware synthesizer.
FIGURE 1.18
The software synthesizer.

20
CHAPTER 1  Setting up Your Creative Environment: The Studio
(HD) can vary from a few hundred megabytes (for synthesizers that are not sample based) to several tens 
of gigabytes (as in the case of professional orchestral and acoustic instrument libraries). I highly recom­
mend therefore using a dedicated HD for the installation of your software synthesizers. A software syn­
thesizer usually is made of two separate components: the actual software application that installs in your 
Application (Mac) or Program (Windows) folder and the library (sample/waveform) which usually can 
be installed in a location defined by the user. For the latter I like to use one or more dedicated HDs that 
I reserve only for the samples and instruments component of my software synthesizers. If you plan to 
share your libraries among several computers (e.g., a desktop and a laptop) you should install the librar­
ies on an external hard drive with a fast connection such as FireWire (FW), USB 2, USB 3, or eSATA 
Thunderbolt (for a detailed description of these connections read later in this chapter). Once you have 
installed your software synthesizer you usually have the option to run it (use it) in either standalone mode 
or plug-in mode. The main difference between the two modes is that the former does not require any host 
application or DAW to run and it runs independently from any other program in your computer, while the 
latter requires the DAW to be launched and running in order for the soft synth to be launched as a plug-in 
(a plug-in is a way of extending the features of a host program, in this case the DAW). Usually I use the 
standalone version for either editing or creating new patches/sounds or for live performances, and the 
plug-in version for sequencing, writing, and producing (Figure 1.19).
Owing to the relatively low cost of personal computers, it is very common these days to use more 
than one computer in the studio. While one computer will run the DAW and some software synthesizers 
(these will serve as the “master” computer), the others serve as additional sound sources (satellites) all 
FIGURE 1.19
Example of a software synthesizer used as a plug-in in Digital Performer.

21
1.4  MIDI Components
running a series of soft synths of their own. The satellite computers replace the old-fashioned and less 
powerful sound modules with an incredibly flexible and sophisticated sound source. I will discuss setup 
and connection issues for this type of system later in this chapter.
1.4.5  Sequencers: An Overview
If the MIDI devices and their sound generators symbolize the musicians available to the modern com­
poser, imagine the sequencer as the contemporary score paper that the composer uses to write the music. 
Instead of scribbling notes, markers, dynamics, and repeat signs on paper, the modern composer uses 
the sequencer to record, edit, and play back both the notes (MIDI messages sent from a MIDI control­
ler) and the audio tracks (recorded through the audio channels). The advantages of using a sequencer to 
compose are huge. Infinite editing possibilities, quick comparison of different versions, and immediate 
feedback in terms of orchestration and arrangements are only a few of the many advantages this working 
technique offers. The sequencer is the central hub of the MIDI data flow between all the MIDI devices 
connected in the studio. There are two types of sequencer: hardware and software (or computer based). 
In Figure 1.20 you can see an example of a hardware sequencer.
Hardware sequencers are much more limited in terms of editing power and track count than their 
computer-based counterparts. Their high portability and low price, though, make them an appealing option 
for the live situation or for quickly sketching ideas when on the road. They also have a built-in MIDI inter­
face, which makes them ready to be used right out of the box. Sometimes hardware sequencers are built 
into keyboard synthesizers to offer a complete MIDI production center (usually referred to as a MIDI work­
station). If you plan to program and record complex sequences with many tempo changes and odd meters, 
and eventually also to record audio tracks (more on this in Section 2.5), I highly recommend investing in a 
computer-based MIDI/audio sequencer. Nowadays software-based sequencers are the center of every pro­
fessional studio, mainly because of their versatility, editing power, and multifaceted functionality.
A software sequencer requires three elements to integrate seamlessly with the other MIDI devices: a 
computer on which the software can be installed and run, a sequencer application (meaning the software 
itself), and a MIDI interface (Figure 1.21).
FIGURE 1.20
Example of a hardware sequencer.
(Courtesy of Akai Professional).

22
CHAPTER 1  Setting up Your Creative Environment: The Studio
Whereas a hardware sequencer has a built-in MIDI interface, a computer, in general, does not have 
MIDI connectors directly installed on its motherboard. Long gone are the times of the Yamaha CX5M 
and Atari ST computers with built-in MIDI interfaces that were specifically targeted to musicians. 
Therefore, one of the key elements for the studio is to have the right computer (more on this subject in 
Section 1.6.2) and the right MIDI interface. Since these days all computer-based sequencers can also 
record audio tracks (in addition to the MIDI ones), a fourth element is introduced: the audio interface. I 
will discuss audio tracks in depth later in this book, but for now it is important to note that a computer 
that has a MIDI interface, an audio interface, and a MIDI/audio sequencer is often referred to as a digital 
audio workstation or DAW (Figure 1.22).
1.4.6  Which Controller?
Depending on how big, complex, and sophisticated your studio will be, you have to choose from among 
different solutions regarding which type of equipment and devices to use. Every MIDI-based studio 
FIGURE 1.21
The software sequencer components.
(Courtesy of Avid Technology, © 2010, Apple Inc.).
FIGURE 1.22
The digital audio workstation (DAW).
(Courtesy of Avid Technology, © 2010, Apple Inc.).

23
1.4  MIDI Components
needs some type of controller to be able to generate and send out MIDI messages to other devices and, 
mainly, to the sequencer. You have already learned that both the keyboard controller and MIDI synthe­
sizer are MIDI devices capable of transmitting MIDI messages. A controller is indispensable for the stu­
dio; no matter what, you need one if you are serious about MIDI. Keep in mind, though, that keyboards 
are not the only MIDI controllers available. If you are not a keyboard player there are other options, and 
also remember that one does not exclude the others. We will learn in Section 3.6 how to integrate MIDI 
data sequenced from different types of controllers.
When deciding on a keyboard controller you have to ask a few simple questions: How many keys 
(extension) does the controller need? How important is it to have weighted-action keys rather than 
synthesizer-action keys? How important is it to have MIDI controllers such as faders and knobs? How 
important is it to have a controller with built-in sound capabilities (MIDI synthesizer)? Is portability an 
essential factor? The answers should give you a pretty clear idea about which controller to buy. You can 
use the chart in Table 1.3 to answer these questions. Let’s analyze these factors in more detail.
Weighted key action: This feature is usually important for professional pianists or musicians who 
are classically trained. Keyboards with this feature have the same response as (or at least one very 
similar to) an acoustic piano. They are usually more expensive and much heavier than keyboards of 
synthesizers with plastic keys. If you are on a tight budget and the real piano feel is not a major con­
cern, I suggest opting for a controller with a lighter key action. If you are planning to have other key­
board players in your sessions and your budget allows it, I would go for a controller with weighted 
keys. In the long run, like every investment in this business, it will pay to have something a bit more 
sophisticated. There is also a third option, based on keyboards with a semi-weighted action, which 
feature a slightly heavier action than a regular synthesizer. They are a great solution for musicians 
who travel a lot and are concerned about the weight of their gear. This type of keyboard is a good 
compromise between portability and real piano feel (Figure 1.23).
If you absolutely cannot do without the feel of a real piano keyboard and still want to keep control 
of your MIDI studio, the ultimate solution is to use, as your main MIDI controller, an acoustic piano 
able to send MIDI data. While there are different brands and models on the market, one of the most 
successful combinations of acoustic piano and MIDI controller is the Disklavier by Yamaha.
MIDI controllers: Some models of controller have one or more faders and knobs that can be 
assigned to multiple MIDI Control Change messages (Figure 1.24).
This means, for example, that you can control the volume and pan of a certain MIDI instrument 
directly from your keyboard. By moving the controller you send a specific MIDI message to the 
OUT port. Depending on the message, you can control a specific parameter of another device. You 
Table 1.3  Keyboard MIDI Controller Decision Chart.
Crucial
Marginal
Not Important at All
Weighted key action
MIDI controllers
Built-in sounds
Portability
Aftertouch
Number of keys or octaves
Built-in USB connection

24
CHAPTER 1  Setting up Your Creative Environment: The Studio
will learn more about Control Change (CC) messages in Section 2.14. This feature is important to 
advanced MIDI composers since you can achieve higher control in terms of expression and phras­
ing of certain instruments, such as strings and pads. It can also help you to create an automated mix 
without using the mouse, which sometimes can be very useful.
Built-in sounds: As explained earlier in this chapter, a keyboard controller with a built-in sound gen­
erator is called a MIDI synthesizer. This type of device acts as both controller and sound source. 
While in the past my recommendation would have been to invest in these types of devices, mainly for 
their flexibility, I strongly believe that nowadays it is wiser to obtain a solid keyboard controller with 
76 or 88 keys and a good set of MIDI controllers.
Portability: If this is an important issue I recommend avoiding a controller with weighted key 
action. If you use numerous software synthesizers with a portable computer and you plan to do most 
of your work on location or on the road, then a small portable keyboard is what you need. Portable 
controllers come in a variety of options. They usually have from 30 to 61 keys (synthesizer action), 
most of the time with plenty of knobs and sliders assignable to different MIDI CCs, and sometimes 
with a built-in USB MIDI interface to connect the controller directly to the USB port of your compu­
ter (Figure 1.25). If portability is your top priority, these are the best option.
FIGURE 1.23
Semi-weighted key action Keyboard Controller.
(Courtesy of Avid Technology, © 2010).
FIGURE 1.24
Keyboard Controller with faders and knobs able to send Control Change (CC) messages.
(Courtesy of Avid Technology, © 2010).

25
1.4  MIDI Components
Aftertouch: I highly recommend having a keyboard with Aftertouch, since it gives you much more 
control over the expressivity of the performance. Devices that respond to polyphonic Aftertouch are 
harder to find, but the majority are monophonic Aftertouch ready. When you buy your controller, 
make sure it can send Aftertouch messages.
Number of keys or octaves: The number of keys and octaves varies from model to model. The 
range goes from a small 2 1⁄2 octave (30 keys), extra-light, and portable controller to a more grown-
up version that features 4 or 5 octaves (49 or 61 keys), to a full 88 keys (7 1⁄3 octaves). For a seri­
ous project studio I highly recommend using a weighted- or semi-weighted-action keyboard with 88 
keys, while for a portable studio configuration the best solution is to use a 49-key controller, which 
still provides good flexibility in terms of sequencing range.
Built-in USB connection: Pretty much all the modern keyboard controller these days feature a 
built-in interface that allows you to connect the controller directly to a computer through a USB con­
nection. Usually these controllers feature also a MIDI OUT connector to guarantee backward com­
patibility with the more traditional multichannel MIDI interfaces. It is important to understand that 
for these types of devices, even though you see only a USB cable as the sole connection between the 
computer and the controller, the data transmitted are still 100% MIDI in their format and nature. This 
is due to the fact that inside the controller is hosted a small MIDI interface that allows the computer 
to talk MIDI language (Figure 1.26).
1.4.7  Sound Palette
Along with your MIDI controller another very important aspect of a MIDI studio is the variety of sound 
sources used to generate the sounds triggered by the MIDI messages. The types of sound generators 
(hardware versus software, and the different synthesis techniques) found in your studio determine the 
sound palette you will be working with. Remember, the sound modules and synthesizers that populate 
your studio are your virtual musicians. The way they are chosen, organized, and arranged has a direct 
FIGURE 1.25
Built-in USB MIDI interface on portable keyboard controller.
(Courtesy of Avid Technology, © 2010).

26
CHAPTER 1  Setting up Your Creative Environment: The Studio
impact on your personal sound and style. As we learned earlier, when it comes to sound sources you are 
faced with two main categories: hardware and software. Since the two present different challenges and 
setup techniques I will discuss them separately.
When considering hardware MIDI devices with built-in sound generators there are two options: MIDI 
synthesizers and sound modules. Many manufacturers offer some of their products in both versions: key­
board synthesizer and sound module. After choosing a good and solid controller, in general it is better 
to opt for the sound module version of your favorite synthesizer, since it is cheaper and takes much less 
space in your studio without sacrificing any of the features of the sound engine. In fact, keep in mind that 
in most cases a sound module has the same sound chip as its keyboard version. The more MIDI devices 
with sound generators you equip the studio with, the better. Imagine the difference between having avail­
able for your production only a local band with eight musicians instead of a full orchestra with 80 musi­
cians. Well, the same can be said for your virtual ensemble: the more devices you have, the higher the 
sound versatility and control power over your productions you can exercise. I usually like to keep every 
MIDI device I buy, and I am very reluctant to sell old ones, mainly because I always like at least a few 
patches from each device, no matter how old it might be: I still hold on to an old Roland U-110 and an 
ancient Yamaha FB-01!
The power and flexibility of each sound source depends mainly on three factors: the number of MIDI 
channels on which the device can receive simultaneously, the polyphony (the number of notes that can 
be played at the same time), and the number (and quality) of patches available on the machine. When 
the MIDI standard was adopted, the major synthesizer manufacturers decided to allow up to 16 different 
channels to be transmitted on a single MIDI cable. Think of MIDI channels as musicians in your orches­
tra that can play almost any instrument you want. Each channel is independent of the others in terms of 
notes, velocity, dynamics, volume, pan, etc. The number of simultaneous channels on which each device 
can receive may vary, depending on the level of sophistication of its sound generator. While the major­
ity of the sound modules and synthesizers available on the market can play 16 different parts at the same 
time (one for each channel), this is not always the case. Some devices may be able to respond to just two 
MIDI channels: useful for creating split or layered sounds, such as bass in the left hand and piano in the 
right hand, or piano plus strings layered. Others may respond to eight or nine channels. If your synthesizer 
can respond to more than one MIDI channel at the same time, it is called a multitimbral device (meaning 
“many-toned”). An additional 16 parts (for a total of 32) can be added if the device allows for a second 
set of MIDI IN. Usually, in order to have the maximum flexibility in terms of sounds and instrumentation 
FIGURE 1.26
MIDI keyboard controller with built-in MIDI interface.

27
1.4  MIDI Components
when sequencing, you will want to have 16- or 32-channel multitimbral devices. Many multitimbral 
devices can also work in a “single” playback mode such that they play back only one sound on one MIDI 
channel. If this is the case, you will need to change its mode to work multitimbrally. To make sure the 
synthesizer is set as a multitimbral device, navigate through its MIDI parameters (usually found in the 
“MIDI” or “Global Settings” menu) and change the “Mode” parameter to “Multi”. Keep in mind that, 
depending on the manufacturer, the multitimbral setup is sometimes referred to as “Poly”, “Multi”, or 
“Mix”. The opposite of multitimbral is a device that can receive on only one channel at a time, effectively 
reducing the number of your MIDI “musicians” to one. This setting, called Omni, is mainly used for live 
settings or to test the MIDI connections rather than for real studio and sequencing situations.
The polyphony (or number of voices) available on a MIDI device represents the number of simultane­
ous notes that can be played on that device across all 16 channels. This is a very important feature, since it 
has an impact on the flexibility and power of the synthesizer. A low-end sound engine usually has 32-note 
polyphony, making it barely acceptable for sequencing. More professional devices feature 64-note poly-
phony, while top-level synthesizers can take advantage of sound engines with 128-note polyphony. Vintage 
synthesizers do not usually go higher than 32 voices, but recent models can easily reach 64 and 128. A syn­
thesizer can have two main ways of allotting the available voices among the MIDI channels. Older models 
usually require you to manually reserve the number of voices for each part (e.g., the Roland JV-880 sound 
module), whereas more recent devices dynamically assign the voices that are still available to the parts that 
require them at any given moment. Keep in mind that even though 32- or 64-note polyphony may seem like 
plenty of voices for a device, the real number of notes you can have a synthesizer play simultaneously also 
depends on the type of patches you are using and on their level of complexity. Advanced patches are pro­
grammed using different layers, each one using a voice. Therefore if you choose a patch that is built on, say, 
four layers, then every time you press one key on the controller you actually trigger four voices. Keep this 
in mind when you work on your sequences. If the sound engine runs out of polyphony, then it will “steal” 
voices from parts that have not been active for a while, and it will assign them to the most recently triggered 
ones. This may lead to sustained notes that suddenly stop playing because of the lack of voices available.
The number of patches available on a device varies tremendously, depending on the type of device. 
Nowadays even the most basic synthesizers are equipped with around 1000 waveforms and patches that 
can be modified to create even more sounds. As a general rule, when planning and building your studio, I 
recommend planning for as many sound sources and sound modules as you can afford and also trying to 
diversify as much as possible the types of synthesis (analog, FM, sampling, wavetable, etc.), the brands, 
and the features of your devices. As a modern and versatile composer you need to be able to have almost 
unlimited sources in order not to run into any technical limitation. Plan to have at least one synthesizer or 
sound module from each of the leading manufacturers. Usually each brand has a very distinctive sonority 
that is the result of decades of research. Some textures may suit your style better than others, but do not 
be afraid to experiment with new sonorities that may not appeal to you right away. As is often the case 
with writing and composing, you may get inspiration from a new sound, or a new patch may fire up your 
creativity. Therefore, keeping the flow of inspiration running is crucial. In the modern studio this flow is 
often nourished by active listening, programming, and experimentation with new sonorities.
I usually like to have available in my studios some wavetable synthesizers, which give me a solid palette 
of sounds to work with, some FM machines, which feature more edgy and modern sonorities, and some 
synthesizers based on hybrid synthesis techniques (such as physical modeling, granular synthesis, and sam­
ple based), which usually provide more trendy and modern sonorities. For purely acoustic sonorities (e.g., 
orchestra, pianos, drums, and ethnic instruments) I recommend having some high-quality sample-based 
engines (for this particular purpose I recommend using software synthesizers). For the real analog feel, 
you should either use software synthesizers that are based on modeling of the circuitry of vintage machines 
(such as Moog and Arp) or, if you can find it them, equip your studio with vintage hardware synthesizers.

28
CHAPTER 1  Setting up Your Creative Environment: The Studio
These are only general guidelines to help you understand the importance of a wide and diverse sound 
palette. The best approach to building such a creative and powerful environment is to experiment and lis­
ten to as many sound sources as you can and to decide which ones are suitable for your studio. Overall, 
the keyword here is diversify, which means try to cover as many areas as possible to achieve a complete 
and functional writing and production experience.
1.5  CONNECTING THE MIDI DEVICES: SOFT SYNTH-ONLY, DAISY CHAIN, 
AND STAR NETWORK SETUPS
After choosing the right devices for your MIDI studio it is time to connect them together and analyze 
the MIDI signal flow. There are three main types of MIDI configuration to connect your devices: Soft 
Synthonly (SS), Daisy Chain (DC), and Star Network (SN). Let’s analyze these three options and their 
practical applications.
1.5.1  The Software Synthesizer-Only Studio
For home studio and small production studio this is by far the most common setup and it is also the 
simplest. In this situation the main computer provides the power for both the MIDI sequencing/audio 
recording and the sound generation. For complex production the main computer must be very powerful 
in order to handle all the real-time processing required. Usually you will have a single MIDI controller 
connected directly to a USB port on the computer (Figure 1.27).
While this setup can seem simple, it is very common. Favoring the simplicity of the production envi­
ronment allows you to focus more on the music and less on the gear. Do not be deceived, though, by the 
FIGURE 1.27
Soft synth-only setup.
(Courtesy of Apple Inc.).

29
1.5  Connecting the MIDI Devices
FIGURE 1.28
Basic Daisy Chain setup.
(Courtesy of Apple Inc.).
streamlined connections, since this particular setup relies on the power of the main computer. If the com­
puter is really powerful in terms of CPU speed then it will be enough to run all your soft synths and the 
MIDI/audio tracks.
1.5.2  Daisy Chain Setup
In a Daisy Chain configuration the MIDI data generated by the controller (device A) are sent directly 
to device B through the OUT port. The same data are then sent to the sound generator of device B and 
passed to device C using the THRU port of device B, which sends out an exact copy of the MIDI data 
received from the IN port. The same happens between devices C and D (Figure 1.28).
A variation of the original Daisy Chain configuration is shown in Figure 1.29, where in addition to 
the four devices of the previous example, a computer with a software sequencer and a basic MIDI inter­
face (1 IN, 1 OUT) are added.

30
CHAPTER 1  Setting up Your Creative Environment: The Studio
In this setup the MIDI data are sent to the computer from the MIDI synthesizer (device A), where the 
sequencer records them and plays them back. The data are sent to the MIDI network through the MIDI 
OUT of the computer’s interface and through the Daisy Chain. This is a basic setup for simple sequencing, 
where the computer uses a single-port (or single-cable) MIDI interface (Figure 1.30), meaning an interface 
FIGURE 1.29
Extended Daisy Chain setup.
(Courtesy of Apple Inc.).
FIGURE 1.30
Example of a single-cable MIDI interface.
(Courtesy of Avid Technology, © 2010).

31
1.5  Connecting the MIDI Devices
with only one set of IN and OUT. There are many brands and types of computer MIDI interface, which usu­
ally are connected to the computer through a USB interface. Sometimes you can install a peripheral com­
ponent interconnect (PCI) card that has built-in MIDI connectors instead of using an external USB device.
A Daisy Chain configuration has several limitations, which make it a less than perfect solution for 
a complex MIDI studio. Its two main drawbacks are the delay/error factor introduced by long chains 
and the fact that you will have to split the 16 MIDI channels available among all the devices connected 
on the same chain. Usually it is not advised to chain more than three devices using the THRU port, since 
this may cause delays that can be measured in milliseconds to which we have to add the time it takes for 
the sound generator, filters, and other components to be triggered before the sound is actually generated. 
Unless you want to layer sounds by sending the same note to more than one MIDI channel to multiple 
devices, you will also have to split the 16 MIDI channels available among all the sound generators of 
your studio. As you can see in Figure 1.31, each device has to be assigned to a specific range of chan­
nels to avoid having two sound generators receiving on the same channel (which would create a layer of 
sounds). This will reduce the overall potential of your studio since in theory each multitimbral device is 
capable of receiving on 16 channels at once.
The vast majority of MIDI interfaces is on USB, with features that range from a basic 1    1 to a 
top-of-the-line 8    8 with SMPTE, ADAT, and Word Clock synchronization options.
FIGURE 1.31
Daisy Chain setup with a 16-channel split.

32
CHAPTER 1  Setting up Your Creative Environment: The Studio
1.5.3  Star Network Setup
A MIDI interface that has more than one set of INs and OUTs is called a multicable MIDI interface 
(Figure 1.32). For an advanced and flexible MIDI studio, a multicable interface is the best solution, since 
it allows you to take full advantage of the potential of your MIDI devices and also prevents the delay 
problems related to a Daisy Chain setup.
By using a multicable interface, all the devices connect to the computer in parallel, meaning that the 
MIDI data will not experience any delay related to the Daisy Chain setup. This configuration involving 
the use of a multicable MIDI interface is referred to as a Star Network. One of the big advantages of the 
Star Network setup is that it allows you to use all 16 MIDI channels available on each device, since the 
computer is able to redirect the MIDI messages received by the controller to each cable separately, as 
shown in Figure 1.33.
FIGURE 1.32
Example of a multicable MIDI interface.
(Courtesy of Avid Technology, © 2010).
FIGURE 1.33
Star Network setup with the addition of a satellite computer running software synthesizers.
(Courtesy of Apple Inc.).

33
1.6  The Audio Equipment
1.5.4  The Future of MIDI
Even though the MIDI standard has been (and still is) an extremely successful tool for the interaction 
of several components in a modern studio, its age starts showing when complex MIDI networks need 
to be set up. The MIDI protocol has kept evolving in order to accommodate some of the newest tech­
nologies. To address the constant evolution of complex MIDI networks and the new needs of the modern 
project studios, new protocols and improvements have been developed in the last few years. Some have 
been more successful and gained more traction than others. New technologies have emerged mainly in 
two categories: the MIDI protocol itself and communication between devices. The MIDI Manufacturers’ 
Association (MMA) has been working for the past few years on a higher resolution MIDI standard 
(MIDI HD) that will allow to have a finer resolution for all MIDI messages, therefore overcoming the 
limitation of the current 8-bit system. While this new standard is very promising, it is a developmental 
stage and at the moment there is no clear release date.
Several attempts have also been conducted to improve the way MIDI devices connect to each other. 
mLAN is an acronym for music Local Area Network. It is a digital networking system developed by 
Yamaha and based on the FW high-speed protocol. It is capable of transmitting both MIDI and audio 
information at the same time. In addition, it is capable of adaptively managing devices and connections 
without the need for network reconfiguration. Unfortunately, mLAN has not gained enough traction 
among manufacturers to become a standard, and it has had very little impact on the industry. In contrast, 
MIDI over local area network (LAN) has become more and more popular, particularly for complex MIDI 
setups that involve the use of multiple computers running software synthesizers. The idea behind this 
technology relies on the use of standard computer network protocols (primarily Ethernet and Wi-Fi) to 
exchange MIDI data. This approach has several advantages, such as eliminating the need for dedicated 
USB MIDI interfaces for each computer used in the MIDI setup, reducing the latency introduced by the 
USB protocol (this is true for Ethernet connections only) and allowing the transmission of MIDI data 
over a much larger number of MIDI cables. While this new solution sounds like a dream come true it has 
a few drawbacks associated with it. Setting up a MIDI over LAN studio can be fairly complicated and not 
very intuitive. My experience varies from absolutely flawless to incredibly frustrating. In addition, the 
lack of a true standard protocol to transmit data over LAN could represent a major roadblock, especially 
if using a combination of Mac and PC computers. At the moment a few organizations are working on 
developing a reliable and latency-free protocol capable of transferring MIDI messages over Ethernet and 
Wi-Fi. Among them the most credited organizations are MIDI OverLAN (by Music LAB), iMIDI (based 
on the TCP protocol), and Apple with its MIDI Network built into their operating system. To use MIDI 
over LAN in your studio you will need all the computers equipped with a fast Ethernet port (1000 Mbps), 
an Ethernet switch (or hub) that allows you to distribute the Ethernet data to all the devices at once, and 
the appropriate software installed on each computer that will handle and synchronize the data traffic. Take 
a look at Figure 1.34 for an example of a MIDI studio completely based on MIDI over LAN.
1.6  THE AUDIO EQUIPMENT
While the MIDI network is the spinal cord of your studio, the audio network constitutes the necessary 
link between the MIDI devices and the delivery of a final audio mix. As I mentioned earlier in this chap­
ter, on the MIDI network only a description of MIDI events is carried and there is no audio information 
whatsoever. The translation from MIDI to audio is made by the sound generator built into the MIDI 
devices that receive the notes to trigger.
To complete your studio you need a mixing board to collect all the audio outputs from the MIDI 
sound sources. From the mixing board the audio signal is sent to the amplifier and speakers or directly to 

34
CHAPTER 1  Setting up Your Creative Environment: The Studio
the active speakers for monitoring. The mixing board will also receive the input from the acoustic instru­
ments, microphones, and other sources you will use for your productions. Figure 1.6 illustrates the two 
signal paths: MIDI and audio.
1.6.1  Mixing Board and Monitors
The mixing board becomes the central hub of the audio signal flow. The type of mixing board you will 
use in your studio depends on several factors. The main aspect to consider is the number of channels 
(or inputs) you need. In general, the higher the number of sound sources (hardware MIDI synthesizers, 
sound modules, satellite computers) the higher the number of channels you will need on the board. Keep 
in mind that each device will have at least two audio outputs (left and right channels), but more sophisti­
cated devices come equipped with multiple outputs, sometimes up to eight mono or 16 for satellite com­
puters running software synthesizers.
A good starting point for a small to mid-sized MIDI studio is a mixing board with 16 channels. If you 
are planning a bigger studio, consider a 24- or 32-channel board: the output of a sound generator is a “line 
level out”, and it can be either balanced or unbalanced, depending on the model. Remember also that in 
most cases you want to have room for other live/acoustic instruments, such as acoustic/electric guitars and 
basses (which use a Hi-Z input), and microphones (which use a mic input equipped with mic preamps). 
In addition to an adequate number of inputs, the mixing board needs to be equipped with a flexible output 
system. While the main out connectors usually feed your monitor speakers and/or your mix-down machine 
(DAT, reel-to-reel, DAW, etc.), a series of extra outputs called buses allows you to send the signal of indi­
vidual channels to the multitrack recorder inputs. With a four- or eight-bus board you will be able to send, 
respectively, four or eight independent channels out from the board to the inputs of your multitrack system. 
Therefore, the higher the number of buses available, the more flexible your working environment will be.
The audio signals collected by the mixing board are sent to the monitor section and the main outputs 
and from there to the main set of speakers. To be heard through the monitors, the audio signal needs 
to be amplified first. You can opt for passive speakers combined with a separate amplifier or for active 
speakers that have built-in amplifiers. I recommend the second option since usually the amplifier in 
FIGURE 1.34
A computers/soft synthesizers MIDI studio based on MIDI over LAN connections.
(Courtesy of Apple Inc.).

35
1.6  The Audio Equipment
active speakers is calibrated to give you the best sound for those particular monitors. Depending on the 
design, use, and distance at which they provide the best performance, speakers can be divided into near-
field, mid-field, and far-field types. In a project studio you usually are going to use near-field or mid-field 
monitors, which are designed for mixing at a short or medium distance, respectively, to avoid the distur­
bances introduced by a faulty design of the room. For detailed information on audio monitors and speak­
ers I recommend the comprehensive text by John Borwick, Loudspeaker and Headphone Handbook.
One of the big questions that for the past few years has sparked lively discussion among engineers 
and producers around the world is whether digital boards are better than analog ones. It is not within the 
scope of this book to settle this dispute, but I would like nevertheless to give some advice and clarify a 
few points to help you make the best decision when planning your studio (and, yes, I have a favorite, and 
you are going find out soon which one it is!).
First of all let me explain the main differences between an analog and a digital board. In an analog 
board (Figure 1.35) the audio signal is always kept in its original analog stage (that is, continuous varia­
tions in electric voltage) all the way from the input section to the output, after passing through the equal­
izer (which can be more or less sophisticated, depending on the board), pan, and volume sections.
In a digital board, the signal is converted after the input stage by the analog-to-digital converter 
(ADC), as shown in Figure 1.36. Very simply put, the ADC translates the analog signal coming from the 
inputs into a digital stream of binary numbers (1s and 0s) by sampling the analog waveform at regular 
intervals and representing the amplitude of each sample in binary format. At this point all the internal 
connections inside the board are digital. The signal will stay digital up to the output stage, where an 
inverse process, through a device called a digital-to-analog converter (DAC), is applied to the signal, 
FIGURE 1.35
Signal path in an analog mixing board.
(Courtesy of Avid Technology, © 2010).

36
CHAPTER 1  Setting up Your Creative Environment: The Studio
converting it back to analog form. I discuss this process in a little more detail in the next section. It is 
very common for a digital board to have a built-in digital signal processor (DSP) which, among other 
functions, allows you to add a series of effects (such as reverb, delay, chorus, equalization, compression) 
to the digital signal. All digital boards also feature at least one digital input and one digital output that 
bypass the internal converter and are able to receive and send out digital data directly.
Both analog and digital boards have pros and cons. There is no doubt that a top-of-the-line (and also 
very expensive) analog board will sound better than anything else (even though less expensive digital 
mixers are catching up fairly quickly), so if what you are after is the best sound and you can afford it, 
go with an analog board; you won’t be dissatisfied. However, analog boards have a few drawbacks, one 
being that the average such board lacks both the ability to recall the settings of a mix (such as equaliza­
tion, pan, volume) and the option of automating the mix (high-end analog boards are an exception) or 
serving as a control surface for your sequencer. A digital mixer has many advantages that can make 
up for some of the acoustic disadvantages. For a project studio, where several assignments can pile up 
quickly, the ability to store and recall mixes and setups at the touch of a button is very important, and it 
can save time and money. Just imagine how painful it would be to recall every setting of your board for 
a 32-channel mix. With a digital board every detail of a mix can be stored in its internal memory and 
recalled whenever you need. Digital boards usually come equipped with built-in effects (such as reverb, 
delay, chorus, compressor) that can be very handy for last-minute touches or quick mixes. In addition to 
the audio inputs and outputs, a digital board has MIDI IN and OUT ports, allowing you to connect it to 
FIGURE 1.36
Signal path in a digital mixing board.
(Courtesy of Apple Inc., Avid Technology, © 2010).

37
1.6  The Audio Equipment
your MIDI interface and use the faders to send out CC messages to control your software sequencer or 
automate the board with MIDI messages sent from the sequencer. Especially if you are planning to use 
one or more satellite computers running software synthesizers, I highly recommend choosing a digital 
board with a good number of digital INs and OUTs (8 or 16 at least) to connect digitally the output of 
the satellite computers (Figure 1.36). The price of digital boards has dropped recently, making a digital 
mixer affordable for many composers and independent producers. Several top-of-the-line manufacturers 
have digital boards in their catalogs.
1.6.2  Computer and Audio Connections
Yes, you read it right! As I mentioned earlier, not only is the computer the central hub of the MIDI net­
work, but nowadays it is also handling digital audio. Over the years since the mid-1990s, computers have 
become powerful enough to handle the incredible amount of data required by digital audio. For several 
years now the software sequencers running in your computer have been capable of recording, editing, and 
playing back not only MIDI data but also audio tracks (meaning audio signal coming from microphones, 
guitars, bass, acoustic instruments, sound generators of synthesizers, and internal software synthesizers), 
replacing less flexible and less powerful multitrack recording media (i.e., tape and reel-to-reel machines). 
Remember that a computer and its software are capable of understanding only binary data, so everything 
that needs to be sent to a computer must be converted to digital format (binary code) for the computer 
to be able to process it. Thus, in order for a computer and its software sequencer to be able to deal with 
audio information coming from an external source, we have to translate the analog signal into a digital 
signal. We just saw something very similar when I described how a digital board works. The ADC takes 
the signal from an analog source (e.g., a microphone) and translates it into binary format by sampling the 
waveform and assigning a definite value to each sample of the waveform. If the number of samples in a 
certain time-frame (also called sampling frequency, which is measured in samples per second, or hertz, 
Hz) is high enough and the resolution of the amplitude of each sample is accurate enough (also called 
bit rate, which is measured by the number of bits allocated by the converters to describe the amplitude 
value of each sample), then the digital reconstruction will be very similar to the original analog wave­
form, making the original analog wave and the digital one almost identical (Figure 1.37).
After the original analog signal has been converted to binary code it is sent to the computer’s CPU, 
which processes the data and stores them on the HD as audio files. A similar but reverse process is applied 
in playback, when the digital signal is read from the HD, processed by the CPU, and translated back into 
an analog signal by the DAC (Figure 1.38).
Usually, a computer comes equipped with very basic converters that are bundled with the machine. 
While the internal converters are good enough for basic tasks, their quality is definitely insufficient for 
professional work. Therefore, another very important piece of equipment in your studio is the audio 
interface, which contains the ADC and DAC. An audio interface varies in price, depending on its specifi­
cations and the quality of its converters. The main factors to consider when analyzing an audio interface 
are the following: the maximum sample rate and the bit resolution at which the ADC and DAC work, the 
number of inputs and outputs available, the type of interface used to connect it to the computer, and the 
type of software that supports that particular interface.
The sample rate and the bit resolution of the audio interface have a direct impact on the quality of 
your recordings. While here I don’t pretend to cover the sampling theorem in detail, it is important for 
modern composers and producers to understand the basic concept behind how a converter works, to 
make an educated choice when buying and using an audio interface. As stated earlier, the higher the 
sample rate and the bit resolution of the converters, the higher the sound quality and the definition of the 
recording. Let’s analyze how these two parameters affect the sound.

38
CHAPTER 1  Setting up Your Creative Environment: The Studio
The sample rate determines the number of samples per second taken by the converters. At compact 
disc (CD) quality we have 44,100 samples per second, and we say we use a 44.1 kHz sampling fre­
quency, which means that the analog waveform fed to the converters is sliced 44,100 times every second. 
This may seem like a lot, but this is just the starting point nowadays for digital audio. Because of the 
FIGURE 1.38
Analog-to-digital (ADC)–digital-to-analog (DAC) process.
FIGURE 1.37
Analog-to-digital converter (ADC).

39
1.6  The Audio Equipment
Nyquist sampling theorem, the sampling frequency has a direct impact on the highest frequency we can 
sample. Thus, if we have a sampling frequency of x, then the highest frequency we can sample without 
introducing artifacts in the digital domain will be x/2. At a sampling frequency of 44,100 Hz, the highest 
frequency the converters are allowed to sample will be 22,050 Hz. To avoid allowing frequencies higher 
than the Nyquist point to reach the ADC, a low-pass filter is inserted before the ADC, with its cutoff 
frequency set to half the sampling frequency. If you want more information on this subject, refer to the 
comprehensive Introduction to Digital Audio, by John Watkinson.
After the analog signal has been sampled, the converters need to assign an amplitude value to each 
sample. This value is set according to the bit resolution of the converters. The number of bits of an audio 
converter determines the number of steps in which the amplitude axis is divided. If the converters can 
sample at only a very low bit resolution, let’s say 2 bit, then the amplitude axis can be divided into only 
four steps (Figure 1.39); if the bit resolution increases, let’s say to 4 bit, then the amplitude axis will be 
divided into 16 steps (Figure 1.40).
FIGURE 1.39
Two-bit resolution.
FIGURE 1.40
Four-bit resolution.

40
CHAPTER 1  Setting up Your Creative Environment: The Studio
Simply as a reference, the formula to calculate the number of steps of a binary system is 2n, where 
n is the number of bits of the system. For example, an audio converter that uses only 8 bits would allow 
for only 256 steps on the amplitude axis (28    256), resulting in a low definition and grainy digital con­
version (keep in mind that a low bit resolution has also a negative impact on other aspects of the digital 
conversion, such as dynamic range, but here the main concern is the most obvious and overall sound 
degradation). The main problem of having a low bit resolution is that if the amplitude of a sample does 
not fall exactly on one of the steps into which the amplitude axis is divided, then the converters will have 
to move it, or better, to quantize it, to the closest available value, introducing a misrepresentation of that 
sample in the digital domain that will result in what is called quantization noise (Figure 1.41).
The bottom line is that a higher bit resolution allows for a much more detailed representation of the 
amplitude of each sample, resulting in a much more detailed and defined sound. At CD quality we sam­
ple with a 16-bit resolution, which means 65,536 steps! But wait, while this was groundbreaking tech­
nology up to just a few years ago, nowadays the most advanced audio converters are capable of sampling 
at 24 bits, and most software sequencers are able to internally handle digital audio up to 32 bits. Do the 
math to see what an improvement this is over the old 16-bit system! Another way to understand and gain 
a visual representation of how the sampling frequency and the bit resolution affect the final sound qual­
ity of your recording is to compare the audio signal to a picture taken by a digital camera. The process 
used to digitize, or scan, a picture is very similar to the audio sampling process found in a digital audio 
interface. If we compare the picture to the audio signal we see that the sampling frequency has a direct 
impact on how wide the spectrum of the sampled signal will be. With higher sampling rates we are able 
to expand the range of the converters and therefore can capture higher frequencies (Figure 1.42).
The bit resolution has an impact on the clarity and definition of the digitized sound. As shown in 
Figure 1.43, an analog signal sampled at a low bit resolution has a low definition, and the details of the 
sound are lost, while at a higher resolution the sound will be crisper and more defined.
Listen to Examples 1.1–1.6 on the website and compare the same audio material recorded with a sam­
pling frequency of 44.1, 22.05, and 11 kHz and at a resolution of 16, 10, and 8 bits.
FIGURE 1.41
Digital quantization noise.

41
1.6  The Audio Equipment
1.6.3  Audio Interface Inputs and Outputs
Another important factor to consider when buying an audio interface is the number of inputs and outputs 
available. The higher the number, the more flexible the interface. Usually you should consider an interface 
with at least eight inputs and eight outputs; this will guarantee enough flexibility to handle a multitrack 
recording situation in your project studio. When talking about the IN and OUT of an audio interface we 
have to differentiate between two types of connection: analog and digital. Analog connections (line or mic) 
allow you to connect to the converters of the audio interface microphones, acoustic instruments, sound gen­
erators from synthesizers, etc. Digital connectors transfer digital signals without any conversion, and they 
bypass the internal converters. While analog IN and OUT use popular connectors such as XLR, ¼-inch 
jacks, and RCA, digital IN and OUT can use a variety of connectors and formats. The names, pictures, and 
specifications of the most common digital protocol connectors are presented in Table 1.4.
FIGURE 1.42
Impact of sampling frequency.
FIGURE 1.43
Impact of bit resolution.

42
CHAPTER 1  Setting up Your Creative Environment: The Studio
Usually, even the most basic audio interfaces come equipped with S/PDIF or TosLink connectors. 
If you are planning to have several digital components in your studio, it is important to use digital con­
nectors so that they can all be connected digitally without any loss of audio quality. If you have a digital 
mixing board it is particularly interesting to be able to connect it to the audio interface digitally, in which 
case you have to check the connection format of both devices to make sure they are compatible. Keep in 
mind that it is always possible to buy converters between the different formats mentioned in Table 1.4.
To guarantee a perfect transition of the samples in the digital domain from the digital output of one 
device to the digital input of another, the digital stream needs to be appropriately synchronized so that the 
bits and bytes are transferred without errors. For this reason one of the key points in connecting two or 
Table 1.4  Digital Connectors and Specifications.
Connector Type
Picture
Specifications
S/PDIF: Sony/Philips Digital 
Interface
Uses 75 Ω coaxial cable and RCA 
connectors to carry two channels of 
digital signal of up to 48 kHz and 24 bit
Cables can reach up to 10–15 m in length
AES/EBU: Audio Engineering 
Society/European Broadcasting 
Union
Carries the same digital signal as S/PDIF 
with the addition of subcode information
Uses XLR connectors on 110 Ω balanced 
cable that can run for at least 50 m
TosLink
An optical version of S/PDIF
Carries two channels of digital signal of up 
to 48 kHz and 24 bits
Carried on fiberoptic cable
ADAT Lightpipe
Carries eight channels of digital audio up 
to 48 kHz and 24 bits
Carried on fiberoptic cable
TDIF: Teac Digital Interface 
Format
Carries eight channels of digital audio up 
to 48 kHz and 24 bits
Carried on 25-pin D-subconnectors

43
1.6  The Audio Equipment
more digital devices together is to make sure they all follow a “clock” to transfer the digital data. This par­
ticular clock is called a word clock since it synchronizes the digital words transferred between the devices. 
The word clock, depending on the type of connection used, can be embedded in the digital stream of data, 
as in the S/PDIF and AES/EBU protocol, or transmitted separately on BNC connectors. Depending on 
your configuration and the features of your devices, you can have the word clock simply transmitted from 
the output of one device (the master device) to the input of the receiving device (the slave). If your studio 
has many digital devices that need to be connected digitally with one another, then the best option is to buy 
a word clock generator/distributor that will make sure all the digital devices in your studio are fed with the 
same word clock. This option will guarantee a smooth and error-free digital network.
A typical audio interface is equipped with eight analog inputs (with at least two inputs with mic 
preamps), eight analog outputs, eight digital inputs, eight digital outputs (most likely in ADAT Light 
pipe format or TDIF), and S/PDIF or AES/EBU stereo IN and OUT. This type of interface would pro­
vide a good starting point for your digitally equipped project studio. More expensive devices will pro­
vide additional inputs and outputs, a higher sample rate, and a higher bit resolution.
1.6.4  Audio Interface Connections
An audio interface can connect to your computer in three main ways: through a PCI slot, a USB interface, 
or an FW type a and b (also called IEEE 1394a/b) interface. The type of connection to your computer 
will not have a definite impact on the sound quality, but you have the option to choose among these three 
main types of connection based primarily on the features of your computer. PCI-based audio interfaces 
are attached directly to the motherboard of the computer, reducing the need for extra cables. Another 
advantage of this type of interface is that, because of its direct connection, it does not tax the CPU of the 
computer as much as FW or USB interfaces. The analog and digital connections on a PCI interface can 
be either built into the back of the card (Figure 1.44) or sent to a breakout box connected to the PCI card 
with a proprietary cable (Figure 1.45).
While the design with built-in connectors is more compact, its connections are awkward to reach and 
sometimes, owing to the vicinity of the circuitry of the computer, noisier than the breakout box version. 
The breakout box design offers instead the best and most flexible design for your project studio. Both 
models need a free PCI slot on your host computer to accommodate the card. While this is not a problem 
FIGURE 1.44
Audio interface with PCI card.
(Avid Technology, © 2010).

44
CHAPTER 1  Setting up Your Creative Environment: The Studio
if you have a desktop machine with at least one PCI slot, it is definitely a restrictive factor if you own a 
notebook computer or an all-in-one computer, such as the iMac. There are a few options available if you 
really need to use a PCI audio card with your notebook computer, but it is pretty expensive and not worth 
the investment unless portability is mandatory for your working situation. A company called MAGMA 
Mission Technology Group, Inc. manufactures a series of expansion chassis for both desktop and note­
book computers that allow you to increase the number of PCI slots available on your machine. In the case 
of a notebook computer, the expansion chassis is connected through a card-bus expansion that slides into 
the card-bus slot of your notebook computer. If you have an all-in-one machine, then you are out of luck 
in terms of PCI audio cards. But don’t despair, for the answer to your problem is in the next paragraph!
Another way of connecting an audio interface to your computer is through an FW or a USB connec­
tion. These types of interface have the advantage of not requiring a PCI slot and therefore are ideal for 
notebook computers. This is becoming true of other types of computers as well, as more and more man­
ufacturers are switching to USB and FW interfaces to address the needs of professional studios. These 
devices are connected to the CPU by a single USB or FW cable, minimizing the setup and installation 
time. The main difference between the two is the type of cable, but the performance features also differ. 
While in terms of audio quality the type of connection has no impact whatsoever, it makes a difference 
in terms of simultaneous audio channels that can be transferred to and from the audio interface. Since 
the information traveling from the interface to the computer, and vice versa, is digital, the bandwidth of 
the connection has a direct impact on the amount of data (meaning the number of channels) that can be 
transferred at the same time. Table 1.5 depicts the three types of USB and two types of FW interfaces 
currently available and lists their performance differences.
At the moment, among the options presented in Table 1.5, FW a and FW b interfaces are better for 
professional audio interfaces that require stability, a high number of audio channels, and high sample 
FIGURE 1.45
Audio interface with PCI card and breakout box.
(Avid Technology, © 2010).

Table 1.5  Universal Serial Bus (USB) and FireWire (FW) Connector Specifications.
Type of 
Interface
Connector
Burst Speed 
(MB/s)
Audio Applications
USB 1.1
1.5
Basic audio applications
Not recommended for more 
than 2/4 tracks in record 
44.1 kHz 24 bit
USB 2
60
Advanced audio 
applications
Can easily handle at least 
16 tracks in record at 
96 kHz 24 bit
USB 3
400
New evolution of USB 2
Much faster than any 
other connection currently 
available
Not common at the 
moment for audio interfaces
FW 400a
50
Advanced audio 
applications
Can easily handle at least 
16 tracks in record at 
96 kHz 24 bit
Well established
FW 800b
100
Advanced audio 
applications
Theoretically twice the 
bandwidth of FW 400a

46
CHAPTER 1  Setting up Your Creative Environment: The Studio
frequencies (Figure 1.46). USB 1.1 is a basic solution for portable interfaces that require a lower number 
of simultaneous channels in record (Figure 1.47).
USB 2 is a promising standard and it has been adopted by a series of new audio interfaces, since it 
features a slightly better bandwidth than FW a but is cheaper to build and it allows you to record high 
definition multichannel audio at 96 kHz, 24 bit. USB 3 is the newest of the aforementioned connectors; it 
holds great promise in terms of pure transfer speed but there is uncertainty over how it will perform as a 
digital audio connector.
1.6.5  Software and Audio Interface Compatibility
An important factor in deciding which audio interface to base your studio on is the software you are 
going to use. There are two main approaches to dealing with software and interface. The first is an open 
FIGURE 1.47
USB audio interface.
FIGURE 1.46
FireWire audio interface.

47
1.7  Which Digital Audio Workstation?
approach, which means that manufacturers are willing to have their software or hardware work with 
other companies’ products. The second option is a closed approach, where the manufacturer wants to 
create a protected market and therefore produces hardware specifically targeted for its software, or vice 
versa. The main advantage of the open approach is that consumers can choose from a wide variety of 
sequencers and audio interfaces to select the option that best fits their needs. The software and hardware 
interact in this type of approach through the use of software drivers that allow the sequencer to exchange 
data with the audio interface. One of the drawbacks of this choice lies in the reliability, since these driv­
ers sometimes do not guarantee total compatibility with all products.
In the closed approach, the choice is more limited because usually the software manufacturer also 
delivers the audio interface and uses proprietary drivers to interface the software and the hardware. This 
means that if you buy a particular brand of software, then only audio interfaces of that brand will be com­
patible with that software. The advantage of this approach is that the setup is usually more stable, since 
there is only one company in charge of both hardware and software. A typical example of this approach 
used to be the one used by Avid (formerly known as Digidesign), which makes the software ProTools and 
has a solid line of audio interfaces. Recently though, Avid switched to an open solution, allowing its flag­
ship DAW Pro Tools to run with any hardware.
Among the audio sequencers that follow the open approach, the most widely used in the profes­
sional production environment are Digital Performer (by Mark of the Unicorn), Cubase and Nuendo (by 
Steinberg), Pro Tools by Avid, and Logic Pro (by Apple). Before buying your audio interface, choose the 
software you will use in your studio and make sure that it supports the audio interface you are planning 
to buy. Table 1.6 summarizes the main audio sequencers and the audio drivers they support.
1.7  WHICH DIGITAL AUDIO WORKSTATION?
This is another one of the hottest topics among musicians and producers. While up to four or five years 
ago the differences among the major DAWs were substantial, nowadays all the professional software 
allows you pretty much to achieve the same results and therefore the final choice is likely to be influ­
enced by personal and circumstantial factors. Each application has strengths and weaknesses that make 
the product appealing to different categories and levels of user. When considering the software that best 
fits your needs, analyze the following issues: What are the primary goals you want to achieve with your 
Table 1.6  Audio Drivers Supported by Main Sequencers.
Software
Mac OS X
Windows XP/Vista/7
Digital Performer
Core Audio
Avid Audio Engine (DAE)
Cubase
Core Audio
Windows DirectX
ASIO
Nuendo
Core Audio
Windows DirectX
ASIO
Logic Pro
Core Audio
Avid Audio Engine (DAE)
Pro Tools
Core Audio
Avid Audio Engine
Windows 
Avid Audio Engine

48
CHAPTER 1  Setting up Your Creative Environment: The Studio
audio sequencer? What is your level of knowledge in terms of MIDI, audio, and troubleshooting? Which 
features are most important for your work? Which software are your colleagues mainly using? Do you 
plan to do a lot of freelance work, or will you be using your studio mainly for your own productions? 
Which platform (PC or Mac) will you use? By answering these simple questions you will have a much 
better and clearer idea of the perfect match for your studio. Let’s analyze each of these points in detail.
1.7.1  The Primary Goals You Want to Achieve with Your Audio Sequencer
Nowadays the differences between the major players in the DAW arena are fairly minimal. Overall, 
you can pretty much do anything you need in any of the main platforms. The perfect fit for your audio 
sequencer depends greatly on the type of work you plan to do in your studio. If you will mainly concen­
trate on tracking live instruments, then you will need a program that guarantees the highest reliability. 
If you plan to use your studio mainly for multitrack mixing, then pretty much all the main programs 
are suitable. They all offer the same features in terms of editing power, automation, sound quality, and 
performance. Keep in mind that for multitrack mixing what can really make a difference is the quality 
and selection of the plug-ins you use. The graphic interface and the working area layout will also have a 
pretty big impact on this type of task. If you plan to do more work composing and producing music and 
you consider yourself more a composer than an audio engineer, then you should opt for a program ori­
ented toward a musical/creative working approach.
1.7.2  Ease of Use and Learning Curve
Each program has different levels of interactivity and different learning curves. While a sequencer may 
appeal to certain users for its clear and straightforward interface, another program may be more appro­
priate for advanced users in search of a more customizable environment. If you are at your first trial 
sequencing and you are looking for a simple yet powerful way to start producing your music, I suggest 
Digital Performer, Pro Tools, or Cubase (Cubase and Nuendo’s sequencing features are almost identi­
cal and therefore I will refer to Cubase only from now on). Their graphic interface is simple, clean, and 
straightforward. Their logical approach to sequencing makes them easy to control and fairly accessible 
after just a few weeks of use, even to the novice. Though their learning curve is not particularly steep, 
they can be used at different levels of complexity and therefore appeal to the beginner as much as to 
the professional. Logic Pro has a slightly steeper learning curve. Where Logic Pro really shines is in its 
infinite ability to be customized in any small detail, making it a great choice for the advanced producer. 
Keep in mind that all four programs are very similar in their overall features and that they share about 
80% of them. Therefore, the choice sometimes can be based on their ease of use or graphic interface.
1.7.3  Which Features Suit You Best?
Even though these programs have very similar characteristics, each has a few distinctive features that 
may make it more appealing to you and your sequencing style. Identifying these features is crucial in 
making the right choice.
As I mentioned before, the audio features in Pro Tools (PT) have been tested and developed for dec­
ades, and they are considered the benchmark for audio recording and mixing in the industry. PT has an 
easy and intuitive interface and a very rich set of keyboard shortcuts to expedite the editing work. Its 
interface is primarily based on three main windows (transport, mix, and edit), which makes it a breeze 
to handle and navigate (Figure 1.48). PT features a comprehensive list of bundled software synthesizers 
and audio plug-ins that make it a very appealing choice for the modern producer. Its MIDI capabilities 

49
1.7  Which Digital Audio Workstation?
have improved considerably since version 8 and above. The graphic editor and the score editor are some 
of my favorite editing environments. Therefore, if you are looking for ease of use, reliability, and power­
ful audio editing, PT is perfect for you.
Digital Performer (DP) has strong and very powerful MIDI editing features, making it the perfect 
choice for the modern composer. Its graphic interface is among the best available, very clean, linear, yet 
appealing to the eye. The main window (called the track list window) shows an overview of the project 
with MIDI and audio track assignments. DP’s learning curve and setup procedures are fairly quick and 
straightforward. While only some basic editing actions can be accomplished from this window, DP offers 
comprehensive edit modes for both audio and MIDI, including score, list, graphic (or piano-roll style), 
mix, sequencer (audio and MIDI together), drums, and graphic and list tempo changes (Figure 1.49). The 
audio features of DP are as advanced as those of PT, with a graphic interface that resembles the one used 
by Digidesign’s software. If you are looking for a program that does not get in the way of your creative 
process, enhances your inspiration, and is a breeze to set up, DP may be what you are looking for.
Cubase (CU) has a very similar approach to DP, with some enhancement in terms of graphic inter­
face and score editing/printing. CU has an advanced track list window that allows you to control differ­
ent parameters of a track, such as volume, pan, effects, equalizers, graphic editing and automation, right 
from the main screen (Figure 1.50). This approach is particularly helpful for projects that need to be done 
quickly. As in DP, more advanced editing can be done using the several edit windows available, including 
list, graphic, mix, score, drums, and graphic and list tempo changes. CU also features a substantial list of 
bundled software instruments and audio plug-ins which make it a great complete production environment.
FIGURE 1.48
Combined windows in Pro Tools.

FIGURE 1.49
Combined windows in Digital Performer.
FIGURE 1.50
Combined windows in Cubase.

51
1.7  Which Digital Audio Workstation?
Logic Pro (LP) is probably the most advanced program of the bunch in terms of customizable fea­
tures and advanced editing options. It can be personalized to fit almost any need. The editing options are 
very extensive and include the classic graphic/piano roll (in LP called the Matrix edit window), the list 
editor, the score editor, the mixer, and the hyper editor (used mainly to edit MIDI CC messages) (Figure 
1.51). The main track window (called the Arrange window, as in CU) allows you to control almost every 
aspect of your production, such as automation, plug-ins, effects, and basic parts editing. LP can be a bit 
intimidating at the beginning. The fairly complex handling of multiple windows is very powerful but a 
bit confusing for the inexperienced musician. Every aspect of your sequencer (MIDI and audio) is based 
on the environment, which represents your virtual studio. LP environment’s engine is very powerful and 
allows you to achieve very complex tasks that are not possible with other programs. Included with LP is 
an extremely comprehensive suite of effects and software synthesizer plug-ins that make it almost a per­
fect self-contained production tool.
1.7.4  Other Factors to Consider
An important aspect to consider when analyzing which audio sequencer to use is based on which pro­
gram the majority of your colleagues and clients have in their studios. Even though exchanging projects 
between programs is possible and becoming easier with interchange formats such as the OMF 2, it is 
much faster to collaborate with studios and composers that work with the same platform. Do a quick 
FIGURE 1.51
Combined windows in Logic Pro.

52
CHAPTER 1  Setting up Your Creative Environment: The Studio
survey among all your colleagues and determine which is their sequencer of choice. Also take into 
account how much work and how many collaborations you expect to share with them. Do the same with 
your most important clients, and try to get an idea about which program would make it easier to col­
laborate with them. Probably you will end up with at least two sequencers at the top of your list, which 
means that if your budget is not too high, you will have to choose one for now and buy the other one (or 
ones) later. In fact, be prepared eventually to learn, use, and master more than one program, since you 
may want to take advantage of the key features of each system to bring your sequencing skills to the next 
level.
To have a clearer idea of the pros and cons of the four audio sequencers presented in this book I have 
summed up their strengths and weaknesses in Table 1.7.
Table 1.7  Pros and Cons of the Major Digital Audio Workstations (DAWs).
Program
Strengths
Weaknesses
Pro Tools (Avid)
Very reliable
Solid audio editor
Solid mixing features
Easy learning curve
Available for Mac and PC
Good MIDI editing features
Very good notation features
Good bundle of plug-ins and software 
synthesizers
Full HD system is expensive
MIDI features not quite up to par 
with other DAWs
Real Time bounce only
Logic Pro (Apple)
Most advanced MIDI editing features
Advanced score editor
Highly customizable
Supports a variety of audio interfaces including 
Digidesign HD systems
Excellent bundle of software synthesizers and 
audio plug-ins
Streamlined one-window workspace (Arrange 
plus one consolidated window)
Steep learning curve for advanced 
features
Sometimes cumbersome graphic 
interface
Non-conventional nomenclature 
and shortcuts
Available for Mac only
Cubase (Steinberg)
Extremely intuitive interface
Advanced score editor
Advanced MIDI editing features
Extremely versatile arrange/track window
Slick graphic interface
Available for Mac and PC
Extremely flexible environment and workflow 
organization
Not stellar bundle of software 
synthesizer
Sore reliability issues at time
Digital Performer 
(Mark of the Unicorn)
Very clear graphic interface
Great MIDI features
Very musical and creative approach
Fairly easy learning curve
Excellent for scoring to visual applications
Score editor needs improvement
Could be more stable
Available for Mac only

53
1.7  Which Digital Audio Workstation?
1.7.5  What About the Computer?
OK, everything is almost set. But there is one more piece of equipment in the studio that we have to ana­
lyze before getting into sequencing in the next chapter: the computer. Even though not strictly a musical 
instrument or an audio device, the computer plays a crucial role in your productions. We know by now 
that it serves as both a MIDI and audio recorder/playback device, and therefore we can think of it as the 
brain of your studio. Keep in mind that we could devote hundreds of pages to this topic, but that would 
go beyond the scope of this book. Here I want to give you some practical information that will come in 
handy when you have to decide which computer to buy for your studio.
First of all, let’s take a look at the two main platforms available: Apple computers (which run the 
Mac OS) and PCs (which run Microsoft Windows). The debate on which one is better has been going on 
a long time, basically since the mid-1980s, when both operating systems (or OS) became available to the 
general public. As you have probably learned by now, there is no perfect OS. Each one has advantages 
and disadvantages that are really up to the end user to consider. Apple machines have the reputation of 
being more expensive, while PCs are believed to be cheaper. Even though to a certain extent this is true, 
you have to consider that Apple computers are slightly more expensive for a reason, mainly because they 
usually come equipped with features you may have to add as options to a standard PC, such as FW 800 
ports, dual video outputs, and dual internal HD. Another advantage that can be attributed to Apple com­
puters is their stability. No matter how much your next-door neighbor brags about his superfast PC that 
can show you 200 frames per second (fps) playing the latest video game hit, for your studio you need a 
machine that is reliable, stable, easy to configure, and troubleshoot. Macs here have a clear advantage, 
especially since the introduction of OS X.
Remember that you are going to use your computer for music only; do not try to run games, Internet 
surfing, e-mail, bank account management, shopping list, and more on your main machine. You must 
have a clean, simple, and well-organized computer to run the audio sequencer in your studio, so leave 
the other tasks to a second machine.
The latest version of Windows 7 is a greatly improved OS, and it has become a stable alternative to 
Mac OS X. The timing of its data transfer paths has come up to standard for MIDI and audio applica­
tions, while its stability is solid. Nevertheless, one of the main advantages of Apple computers is that the 
same company makes both the machines and the OS. This guarantees a tighter and more stable inter­
play between the two. In the case of PCs we have thousands of different designs made by thousands of 
companies over which Microsoft has no direct control. This results in several incompatibility issues that 
can ruin your creative day more often than you would like. Remember that all the equipment discussed 
so far (computer included) are only tools that allow your creativity to develop and grow. As with any 
other tool, you do not want them to get in the way of your compositional process. If you were writing 
for an orchestra you wouldn’t be happy if your first violinist came up to you every time he or she broke 
a string, right? Well, since the MIDI studio is your modern orchestra, in the same way you do not want 
to be interrupted by technical problems. Therefore, my advice is always to choose the more stable, safe, 
and sound options, even though they can sometimes be a bit more expensive.
When deciding which computer and operating system to choose, pay particular attention to 32-bit 
and 64-bit versions. While the latest Apple OS is only available in 64 bit, Windows 7 (and its pred­
ecessors, XP and Vista) can be purchased and installed in their 32- or 64-bit versions. The main differ­
ence between 32 and 64 is that the former can only address (use) up to 4 GB of physical random-access 
memory (RAM) while the latter can address up to more than 128 GB. Why should you care? Being able 
to access a larger amount of RAM is crucial when using sample-based software synthesizers (such as the 
ones that feature orchestral libraries), since having more RAM will allow you not only to load a higher 
number of patches simultaneously but also to use libraries with a larger number of samples per patch 
(resulting in more realistic rendition of acoustic instruments).

54
CHAPTER 1  Setting up Your Creative Environment: The Studio
When considering a computer for your studio, think in terms of the best machine you can afford. 
While for MIDI-only sequencing you do not need a very fast CPU, for audio data handling you need 
the fastest CPU available, especially if you plan to have a high track count with many plug-in effects. 
It is hard to give some reference about CPU speed, since the rate at which computers evolve in terms 
of features and options is extremely high. Besides considering the clock speed of your main processor 
(or processors in the case of a multiprocessor machine), you have to make sure you have the fastest bus 
speed supported by your motherboard. The bus system is the connection path over which the data are 
transferred from and to the different components of the motherboard, such as the I/O interfaces, CPU, 
and memory. Having a fast bus speed allows your computer to perform better in all the tasks involved 
with digital audio recording and playback. Always try to install as much memory as your computer and 
system allow. RAM is the temporary storage space your CPU uses to store big chunks of data in tran­
sit to and from the I/O ports; if the CPU runs out of RAM, then it will use the HD (permanent storage 
space) instead, as virtual memory. Remember that the HD handles information much more slowly than 
RAM. This option therefore would work fine for applications where the timing of data fetching is not 
an issue. But with MIDI and audio you need very tight time management, and therefore more available 
RAM allows your computer to handle more data with tighter timing.
Permanent storage space devices, such as internal and external HDs, are also a very important aspect 
of your computer system. When you record audio sessions in your audio sequencer, the data converted 
from the ADCs are stored on the HD of your computer as a sequence of 1s and 0s. The higher the sam­
pling frequency and bit resolution of the digital conversion, the more data the computer needs to store 
on the HD. You will soon realize that the 250 GB of HD space you thought would last forever will not be 
enough. While I will discuss space and file management of your sessions later in the book, for now keep 
in mind that, as with the memory issue, for HD the bigger the better. Not only can a bigger HD hold more 
data, but it is also usually faster, since the computer has more space and freedom to organize the data in a 
rational way. In terms of speed, when buying an HD for audio applications, get a model that uses mecha­
nisms with at least 7200 rpm, 10 ms or better access time, and at least 8–10 MB/s transfer rate.
Always have at least three HDs in your studio. The first one is the internal one that holds the OS and 
the applications (or programs) you run, such as sequencer, notation program, and utilities. The second 
HD, which can be either internal or external, is the main session disk, where you store your projects dur­
ing the recording sessions. The third HD is the overnight backup (more on backing up and archiving in 
later chapters), where you make a copy of your project as soon as you have a minute, even during the 
session itself. For no reason should you allow even one day to go by without making a copy of your cur­
rent projects to the backup device! Catastrophic events always seem to happen right before the deadline 
of your important project, so you must have a plan B (as in backup!).
Another feature you should consider equipping your computer with is a multiple video monitor setup. 
The regular one-video configuration is good enough for word-processing and Web-surfing applications, but 
it is far too limited for MIDI and audio composing/sequencing. Imagine having to write a score for a studio 
orchestra on a two- by two-inch piece of paper. Well, if you had only one monitor, that’s pretty much the 
feeling you would get. On the other hand, if you have two (or more) monitors, you increase your desk­
top real estate without affecting considerably your budget. What you need is an additional video card for 
your computer—nothing fancy, a card that supports at least a video resolution of 1400    1050 will work 
fine—and a second monitor. The advantage of having a second monitor is that you will be able to organize 
the edit windows of your sequencer across both desktops and drag elements and windows of your current 
project across the two monitors, making recording and mixing on your computer a much more enjoyable 
experience. What I usually do is to set a template so that when I open my audio sequencer I immediately 
have the track/arrange window displayed on the main monitor and the mix window on the second. This 
setup allows me to do quick edits and to apply changes to my mix without opening and closing different 

55
1.8  Final Considerations and Budget Issues
windows. Because of the low cost of monitors and video cards, this is something you should seriously 
consider.
1.8  FINAL CONSIDERATIONS AND BUDGET ISSUES
Well, now you should have most of the pieces together to move on to the next chapter and finally start 
sequencing and using your equipment. We added different sections of your project studios together little 
by little, starting from the MIDI gear, moving to the MIDI interface and the computer, to the audio gear, 
such as the mixing board and the speakers, to the audio interface. Your studio now should look like the 
one in Figure 1.52.
While the studio shown in Figure 1.52 is a theoretical project studio, you can use it as a starting 
point to design your own. When planning a new MIDI/audio setup you should always start with a sketch 
where you carefully plan how your devices will be connected, which types of connection are required, 
and if possible also the configuration of the MIDI and audio devices. The creation of a blueprint allows 
you to plan your budget precisely. I highly recommend when building a project studio to start from the 
budget available to you and buy the equipment that fits it. These are the steps to build a balanced and 
efficient project studio. Start with a budget, then make a sketch of the equipment you need in your stu­
dio, and finally do some research with different music dealers and find out the prices for each device. 
Remember that you have to stick to the plan you designed on paper. If you did things right, you will 
FIGURE 1.52
The project studio.

56
CHAPTER 1  Setting up Your Creative Environment: The Studio
need every device you see on the blueprint, and this will allow you to focus on the pieces of equipment 
you really need. One of the most common mistakes is spending most of the budget on one device (e.g., 
the computer) and then not having enough money to buy a MIDI interface, which makes the studio unus­
able. Therefore, be careful when planning and balancing your budget and the equipment. Remember 
that your final goal is to have a fully functional, versatile and, as much as possible, trouble-free working 
environment so that you can focus on the creative process of writing music.
1.9  SUMMARY
In this first chapter you learned how to prepare and set up a working environment for music produc­
tions. The devices in the studio fall mainly into three categories of equipment: MIDI/electronic instru­
ments, sound/audio equipment, and the computer. Two main path signals connect your devices: the MIDI 
network and the audio network. The former uses five-pin DIN MIDI cables to connect all the MIDI-
equipped devices through built-in MIDI interfaces. Each device has an IN, an OUT and, on most devices, 
a THRU port. The computer needs an external or a PCI internal MIDI interface to be able to connect 
to the MIDI network. This interface can be a single cable (meaning with one set of IN and OUT only) 
or multiple cables (meaning with multiple sets of INs and OUTs). If you have the latter, then yours is a 
much more advanced studio that allows you to connect your MIDI devices in parallel (Star Network), 
reducing the transmission delay to a minimum and taking advantage of all 16 channels available on every 
device. If you use the former, then you have a basic MIDI setup, called Daisy Chain, where multiple 
devices are connected in a serial chain using the MIDI THRU ports. Remember that this configuration is 
limited to only 16 channels in total and that they need to be split among all your devices. The MIDI data 
transmitted over the MIDI network are sent to the computer which, through a software sequencer, is able 
to record, play back, store, and edit them. The data transmitted on the MIDI network contain no audio 
information. The sound generator of each MIDI device, by reacting to incoming MIDI messages, such as 
Note On, is responsible for producing the audio signal that is eventually sent to the mixing board and to 
the speakers.
The audio network has the mixing board as the central hub of its audio connections. It collects the 
audio outputs from the sound generator of the MIDI devices. The audio from the mixing board is sent 
to the speakers through the main outputs and to the audio interface connected to the computer through 
the bus outputs. These outputs, usually two, four, or eight in small to mid-sized boards, allow you to 
send to the audio interface inputs several channels of audio separately. The audio interface is connected 
to the computer usually using FW, USB, or PCI interfaces. Through the use of ADCs/DACs, the audio 
interfaces allow the computer to record analog audio in digital binary format. The computer functions 
therefore as both the MIDI and audio central hub of your studio and, along with the software sequencer, 
it constitutes the heart of your music production environment.
1.10  EXERCISES
Here are some exercises to help familiarize you with the material presented in the chapter. Use them as 
a starting point not only to review the concepts of MIDI and audio network but also to design and create 
your project studio.
Exercise 1.1
Identify the MIDI devices and their components shown in Figure 1.53.

57
1.10  Exercises
Exercise 1.2
Connect the MIDI and audio devices shown in Figure 1.54, indicating clearly the MIDI and audio cables.
FIGURE 1.53
Exercise 1.1.

58
CHAPTER 1  Setting up Your Creative Environment: The Studio
Exercise 1.3
a.	 Create the blueprint of the project studio you want to build, including all the devices that you will use 
for your production, such as MIDI controllers, MIDI synthesizers, sound modules, mixing board, computer 
(including satellite computers if needed), MIDI interface, and audio interface.
b.	 Connect the MIDI and audio devices you designed in your blueprint, indicating clearly the MIDI and audio 
cables.
Exercise 1.4
Using the resources on the Web or catalogs from different music stores, replace the blueprint from Exercise 
1.3 with real devices available on the market that appeal to you. Come up with three different studio scenarios 
that can fit the following three budgets:
a.	 Studio A: budget of $25,000
b.	 Studio B: budget of $12,000
c.	 Studio C: budget of $8,000.
FIGURE 1.54
Exercise 1.2.

59
Creative Sequencing Techniques for Music Production.
© 2011, 2005 Andrea Pejrolo. Published by Elsevier Ltd. All rights reserved.
CHAPTER
Basic Sequencing Techniques
2
2.1  INTRODUCTION
Now that you have designed and set up your studio, it is time to start sequencing. If you read carefully 
the preceding discussion on studio setup, you should be familiar already with the equipment you are 
going to use in the following pages. In this chapter you start learning the concepts of sequencing. Let me 
be very clear from the beginning: sequencing is a lot of fun! Making music while having almost no limi­
tations in terms of instruments, sound palette, editing features, and effects is an amazing experience; at 
the same time, it can be a bit overwhelming if not approached systematically and with organization. You 
are going to learn how to set up a sequencing session from different points of view in order to minimize 
the problems and maximize the fun of writing and producing your own music.
In the first part of this chapter you learn how a sequencer works. How you organize your session is 
crucial in order to have a smooth sequencing experience. We will go through all the necessary steps to 
avoid mistakes that could cause problems down the road.
In the second part of the chapter we start sequencing, using MIDI tracks first and then including 
audio tracks with some basic editing techniques. At the end of the chapter you will find a quick review, 
some tips, and some exercises you can use as a starting point to experiment on your own.
2.2  THE SEQUENCER: CONCEPTS AND REVIEW
As explained in Chapter 1, a MIDI sequencer (software or hardware) allows you to record, play back, 
edit, and store MIDI data. From now on we are going to learn sequencing techniques on the four top-of-
the-line software sequencers: Pro Tools (PT), Digital Performer (DP), Cubase (CU), and Logic Pro (LP). 
While some of the techniques can also be applied to hardware sequencers, to take full advantage of the 
MIDI and audio features available nowadays you really should have a computer-based studio running a 
software sequencer. It is also important to recognize that as a result of the evolution in technology we 
have experienced since the mid-1990s, modern computer-based sequencers have the ability to record not 
only MIDI data but also audio information. This means you will be able to create complex arrangements 
that include synthesized MIDI tracks and acoustic tracks at the same time, raising the entire sequencing 
experience to a much higher level. The ability to merge MIDI and audio tracks in a mix not only makes 
it possible for you to improve the sonority and realism of sampled and synthesized sounds, but also ena­
bles you to record or import any audio material (such as vocal tracks, acoustic instruments, lead parts) 
and include it in your sequence. Of course, the path signals that MIDI and audio data follow are very dif­
ferent, and it is very important to understand the difference between the two. As we learned in Chapter 1, 
while the MIDI data reach the computer through the MIDI interface (which allows the computer to com­
municate and exchange MIDI data with other MIDI devices), audio data are recorded and played back 

60
CHAPTER 2  Basic Sequencing Techniques
through the use of an audio interface, which mainly converts the analog audio signal into digital and vice 
versa. These two main types of data constitute the basic elements of your sequences, and that is where 
you are going to start your first project. It is time to begin sequencing!
2.3  HOW A SEQUENCER WORKS AND HOW IT IS ORGANIZED
The first time you start a new project in your sequencer you are presented with the main track list win­
dow. Keep in mind that this option can be changed depending on the sequencer and its preferences. For 
example, you can create templates that have different windows and editor layouts, or you can have the 
sequencer customized to start up with an “Open” dialog window so you can load a preexisting project. 
There’s no perfect setup, but I usually like to have the sequencer set to a “do nothing” option and choose 
each time I launch the software which action to take. There are exceptions to this setting, such as when 
I am working frenetically on a specific project for an extended period. In this case I prefer to have the 
sequencer loading automatically the latest project: it definitely saves time. The track list window is where 
all the tracks available in your project are organized and presented in a horizontal way. The layout of the 
tracks may change slightly among the four sequencers analyzed in this book, but the concept is the same 
for all of them. There are three main types of track: MIDI, audio, and software synthesizer. The MIDI 
tracks receive the data from the MIDI interface, usually through the universal serial bus (USB) port. The 
audio tracks receive the audio from the audio interface connected to the computer through a FireWire 
(FW), USB, or peripheral component interconnect (PCI) connection. The software synthesizer tracks are 
basically a hybrid combination of the other two, where MIDI data (note, controllers, etc.) are recorded 
and edited as on a MIDI track but the channel strip behaves like an audio track with an audio output and, 
more importantly, has the ability to add audio effects (through both inserts and sends). In Figure 2.1 you 
can see a comparison between the track list windows of the four sequencers.
Tracks can be added to accommodate the most complex projects. It is usually a good idea to cre­
ate one or more templates with the instrumental ensembles you use the most. To set up a song as a 
template in DP, PT, LP, and CU you can open a new project by selecting “New” from the File menu of 
your application. Create and customize your tracks according to your ensemble, and then select “Save as 
Template” from the File menu. In PT and CU you can specify where to save the templates (either on a 
specific folder or to the default system folder). Now every time you start a new project you will be able 
to select the template that better matches its instrumentation. You should have a variety of templates 
available in order to speed up your setup work. Here’s a list of suggested templates.
Jazz Ensemble:
l	 Trumpet
l	 Saxophone
l	 Drums
l	 Percussion
l	 Piano
l	 Bass
Rock Ensemble:
l	 Distorted guitar 1
l	 Guitar 2
l	 Lead guitar
l	 Drums

61
2.3  How a Sequencer Works and How it is Organized
l	 Percussion
l	 Keyboards
l	 Bass
Latin Ensemble:
l	 Trumpet
l	 Trombone
l	 Saxophone
l	 Drums
l	 Percussion 1
l	 Percussion 2
l	 Keyboards
l	 Piano
l	 Bass
Symphony Orchestra:
l	 Piccolo
l	 Flutes
FIGURE 2.1
The main track list window in PT, DP, CU, and LP.

62
CHAPTER 2  Basic Sequencing Techniques
l	 Oboes
l	 Cor anglais
l	 Clarinets
l	 Bass clarinets
l	 Bassoons
l	 Double bassoons
l	 Trumpets
l	 Trombones
l	 Bass trombone
l	 Tuba
l	 Horns
l	 1st violins
l	 2nd violins
l	 Violas
l	 Cellos
l	 Double basses
l	 Harps
l	 Percussion
l	 Timpani
Studio Orchestra:
l	 Piccolo
l	 Flutes
l	 Oboes
l	 Cor anglais
l	 Clarinets
l	 Bass clarinets
l	 Bassoons
l	 Saxophones
l	 Trumpets
l	 Trombones
l	 Bass trombone
l	 Tuba
l	 Horns
l	 1st violins
l	 2nd violins
l	 Violas
l	 Cellos
l	 Double basses
l	 Harps
l	 Percussion
l	 Timpani
l	 Drum set
l	 Electric/acoustic bass
l	 Piano
l	 Keyboards/synthesizers

63
2.4  MIDI Tracks
These are just a few ideas that you can expand and adapt according to your style of writing. It is 
advisable, though, to have a comprehensive palette of templates to be ready for any situation.
On the website you can find the foregoing templates for DP, CU, PT, and LP.
2.4  MIDI TRACKS
Each MIDI track needs to be assigned to a MIDI output. Depending on your setup you will have either 
a single-cable or multicable MIDI interface (refer to Chapter 1 for a review of these two configurations). 
If you have a single-cable interface, then you have to set up the output of each track to a MIDI channel 
between 1 and 16; this will tell the sequencer to which channel to send out the MIDI data. The receiv­
ing channel will play back the MIDI messages sent in real time from the controller or recorded on the 
sequencer track. Remember that the sequencer and the computer act as the central hub of the MIDI net­
work, so it is crucial to assign the track to the right output. If you have a multicable interface, then you 
have to specify for each track the cable (meaning the MIDI device) and the channel to which the MIDI 
data will be output. Depending on the software and the interface, each cable will be identified with a let­
ter, a number, or the name of the device connected to it (Figure 2.2).
In Figure 2.2 the different cables of the MIDI interface are named with the model number of the 
device connected to it. The computer and your sequencer know this information because it was entered 
in the MIDI setup application that is part of the operating system of your computer. In OS X this appli­
cation is called Audio-MIDI setup. To configure your MIDI setup, launch the Audio-MIDI setup, which 
is located inside the Utility folder inside the Applications folder on your HD. After launching it, make 
sure that the MIDI window is visible by navigating to Window  Show MIDI window. Make sure that 
FIGURE 2.2
Example of MIDI output channel/cable assignment in DP.

64
CHAPTER 2  Basic Sequencing Techniques
your MIDI interface is connected to your computer. Hopefully your MIDI interface will appear in the 
center of the window with all the I/Os “open” (as a quick troubleshooting step in case the MIDI interface 
is not recognized by the application, try disconnecting and reconnecting its USB cable from the com­
puter). At this point you have to add every MIDI device that is present in your studio by clicking on the 
“Add Device” icon located at the top of the window. You can specify the brand and model of the device 
by double-clicking on its icon. The last step of the MIDI configuration consists of connecting the I/Os of 
each device to the I/Os of the MIDI interface in the same way they are connected in your studio. To do 
this simply click and drag the arrows at the edge of each device to the corresponding arrows located on 
the MIDI interface icon. Once you have made the right assignments for all MIDI tracks, you are ready to 
start sequencing. Remember that the I/O assignments are also saved with your templates, so use this fea­
ture to customize the templates as much as you want. For example, if you know that your oboe part will 
always be played by the device on cable 2/channel 1, then you can assign the MIDI out of the oboe track 
to cable 2/channel 1 so that you won’t have to assign it every time you start a new orchestral project. 
This parameter can be changed at any time.
MIDI tracks are extremely flexible when it comes to editing. Since in this type of track only perform­
ance data are recorded (as learned in Chapter 1), it is very easy, with the help of the sequencer, to change 
almost any parameter of the MIDI messages recorded. First of all, we can look at the data in several 
ways, depending on the type of edits we need to make. Among the most used editors are the graphic (or 
piano roll) window, the list window, and the score window, which are available in almost every sequencer. 
Remember that no matter which edit method you use, you will always be able to change pretty much any 
parameter of your MIDI performance, so most of the time it is really up to you to choose the one that 
suits best your style and working habits. I usually recommend the graphic editor for quick fixes, common 
edits such as copy, paste, and delete, and especially for Control Change (CC) message edits. This editor is 
very intuitive and easy to grasp (Figure 2.3).
The list editor is another well-known tool that has its roots in the early hardware sequencers. It is 
usually the favorite tool of more seasoned MIDI composers and producers because it was really the only 
FIGURE 2.3
Graphic edit window in DP.

65
2.5  Audio Tracks
editing option in the early days of sequencing. It has the advantage of being very specific and selec­
tive: all the data are categorized by event, which makes changing a single parameter quick and effective 
(Figure 2.4). The list editor looks very similar in all four digital audio workstations (DAWs).
The score editor is the most popular among trained musicians, since it allows you to see the flow of 
MIDI data in a more traditional notation view (Figure 2.5). From this editor you can not only edit the MIDI 
data but also organize the score in a page layout mode to print out professional scores and parts for your 
projects. PT, CU, and LP are excellent in terms of flexibility and quality of the notation printouts.
As mentioned before, all three types of editor allow you to edit all the parameters of your MIDI data. 
Here’s a list of the most common elements that can be edited from the aforementioned windows:
l	 Note number
l	 Velocity ON and OFF
l	 CC number and value
l	 Position of each event
l	 Duration of each event
l	 Pitch Bend.
I will describe these editing techniques in more detail in the following paragraphs.
2.5  AUDIO TRACKS
Even though we are not going to use audio tracks right away in this chapter, it is important to understand 
how these tracks differ from the MIDI tracks you just learned to set up. In an audio track the input/
output (I/O) labels are related not to the MIDI interface but to the audio interface. The input and output 
setup depends on the type of audio interface you have and on the number of physical inputs and outputs 
available. Another difference between MIDI and audio tracks is that the latter can be set up as mono, 
stereo, and surround, depending on the material you are going to record and play back. In your templates 
FIGURE 2.4
List edit window in LP.

66
CHAPTER 2  Basic Sequencing Techniques
it is also a good idea to have at least four mono tracks for solo instrument recording and two stereo 
tracks for mix-downs and loop import. Figure 2.6 shows the I/O audio track option in LP.
Audio tracks can be edited in two ways: destructively and nondestructively. While the former alters 
directly the original audio file and its data that were stored on the HD after the analog-to-digital conver­
sion, the latter does not alter the original file but stores the edits as a list of events that points to areas 
(regions) of the original audio file without altering it. This technique allows the sequencer always to recall 
the original file without the risk of losing part of it by committing to permanent edits. All four sequencers 
have options to edit both ways. Most of the time you will want to make audio edits in a nondestructive 
way, but keep in mind that there are situations in which you will have to use the destructive editors.
In addition to the regular audio tracks just described, there are three other main types of tracks that 
can be related to this category: auxiliary, virtual instrument, and master tracks. These types of tracks are 
different from the audio tracks because they do not contain any audio data and they do not show any 
waveform in the edit window. Nevertheless, these are very important tools for your final productions. 
Let’s examine them one by one.
Auxiliary tracks: These have the same function as the aux channels on a mixing board. They col­
lect a signal from an input (which usually is a bus) and send it to an output (which can be another 
FIGURE 2.5
Score editor window in PT.

67
2.5  Audio Tracks
bus, another track, or the master output). Auxiliary tracks are very versatile. They can be used for 
assigning multiple audio tracks to a single fader or to share a common effect (such as reverb) among 
several audio tracks. The application of auxiliary tracks can also be extended to multiapplication sig­
nal routing. In this case, if the programs used allow it, a signal from one application can be sent to 
another running at the same time through the use of virtual auxiliary channels. Usually the interap­
plication signal exchange is controlled by software protocols that take care of the synchronization 
and data flow between applications. One of the most widespread protocols is ReWire, which allows 
programs such as DP, CU, LP, and PT to communicate in real time with other applications, such as 
Reason and Live. The ability to route signal between applications opens up infinite creative mixing 
and signal processing opportunities, such as adding effects to a signal generated by one application 
with plug-ins inserted in another. The signal coming in from another application not only can be 
routed through plug-ins and effects but in some cases also recorded to an audio track, making the 
exchange of audio material among different software fairly easy.
Virtual instrument tracks: DAWs use dedicated tracks to insert software (or virtual) synthesizer 
modules that can be triggered by a MIDI controller. Virtual instrument tracks do not contain any wave­
form (like the audio tracks) but are more similar to the aforementioned auxiliary tracks. Their peculiar­
ity resides in the fact that while their audio nature is similar to auxiliary tracks their editing features 
are identical to those of a MIDI track. Therefore, you can record and edit MIDI data in the same 
way you would do on a MIDI track. To use a virtual instrument track in PT you simply create a new 
track from Track  New…, and when prompted to select the type of track you want to create choose 
Stereo and Instrument Track. In DP a software synthesizer track is created by selecting Project  Add 
Track  Instrument Track and then choosing the desired software instrument. In CU you can add a 
FIGURE 2.6
I/O audio track option in LP.

68
CHAPTER 2  Basic Sequencing Techniques
software synthesizer through Project  Add Track  Instrument. This action will prompt you with a 
window in which you can choose a software instrument among all the ones installed and available on 
your HD. In LP the track assignment is very flexible and therefore each track can be reconfigured to 
be set as any type of track you want (you could, for example, easily redirect a MIDI track to an instru­
ment track) but if you need to create a new one from scratch you can navigate to Track  New… and 
then select “Software Instrument” from the drop-down menu of the New Tracks window. To select the 
software instrument that you want to assign to the newly created track, click on the I/O section of its 
channel strip and select the plug-in that you need. The software synthesizer tracks act as additional 
mixing board channels that receive the input signal not from the sound generator of an external MIDI 
device but from the virtual output of the inserted software synthesizer.
Master track: A master track has the same function as a master fader on a hardware mixing board. It 
receives the input from all the active tracks (audio, auxiliary, and virtual instrument) in your sequence. 
It is useful if you want to insert mastering effects on the entire mix, for example, or if you want to gen­
erate an overall fade without having to work on the individual channels. All the main sequencers allow 
you to create one or more master faders in the same way as you create new audio, MIDI, or instrument 
tracks.
2.6  ORGANIZING YOUR PROJECTS
Now that you know how to organize your tracks and have your templates set up, we should take a 
look at how to organize your projects and your session files on your hard disk (HD). Even though each 
sequencer has a slightly different approach to storing session data, the overall techniques are very simi­
lar. For a problem-free composing experience it is absolutely crucial to understand how your sequencer 
stores the data and the hierarchical structure of the files associated with your project. As we learned 
previously, MIDI and audio tracks store two very different types of data: performance data in the form of 
MIDI messages in the former, and digitized audio renderings of an analog waveform in the latter. These 
two types of information, plus other data, are stored on your HD in different files. Let’s take a look at the 
file/session organization of a project.
When you first launch your sequencer and choose to start a new project, the program asks you to 
choose in which folder of the HD to store it. Notice that this operation needs to be done before you start 
recording any audio data, since the audio information coming in from the audio interface is stored in real 
time during the recording process. Your new projects should always be saved in separate new folders. 
Keep your folders very well organized; this is crucial for a problem-free working environment.
After naming your new project you will be brought back to the main window of your sequencer. It 
is important to understand the structure of the folder of your projects and where the different files are 
stored in your project folder. The two most important items for every project are the session file and the 
audio files folder. The session file stores all the information about your MIDI tracks, MIDI data, channel 
assignments, links to the audio files, automation data, preferences, etc. Inside the audio files folder only 
the actual audio files data are stored. Remember that those two items, session file and audio files folder, 
should never be separated. If you misplace the audio files folder, then all your audio information will be 
lost, resulting in empty audio tracks when you try to open the session file. Depending on the software 
you use in your project folder, you can find several other items that also should never be separated from 
the session file. In Table 2.1 you can see a description of the file structure for DP, PT, CU, and LP.
The main purpose of having a well-organized project folder has to do with backup and archiving 
procedures. If all your files are kept constantly in the same folder, then your backup sessions will run 
smoothly and quickly. If your project files are scattered around your HD, then you increase the chances of 
deleting or forgetting some of the files of your project, resulting in a waste of time, energy, and creativity.

69
2.7  Flight-Check Before Take-off!
To obtain the best performance from your system it is highly recommended to use a dedicated HD 
for your recording sessions and projects. Since the main internal HD hosts the operating system and 
the programs, it is usually busy taking care of the tasks related to the normal computer activities. If 
we recorded the audio and session data to the same internal HD, the overall performance of the sys­
tem would be affected, resulting in lower track and plug-in counts. By using one or more dedicated 
HDs you ensure that your recording system runs smoothly and with the highest performance available. 
Partitioning your HD does not achieve the same result as having a physical second HD; buy a new HD 
instead of partitioning your existing one.
2.7  FLIGHT-CHECK BEFORE TAKE-OFF!
If you followed carefully all the preparation steps up to this point, you should be in good shape to start 
sequencing and finally having fun with your studio. In this section we are going to make sure that eve­
rything is in working condition, meaning that all the signals are routed following the right paths and that 
you won’t have any technical problems to stop your creative flow.
Table 2.1  Project File Structure in the Four Digital Audio Workstations.
Program
File Structure: Folder and Subfolder
Digital Performer
Project Folder
Session File: contains the MIDI data, automation, etc.
Audio Files Folder: contains the audio files only
Undo Folder: contains the list of edits and undos
Analysis Files Folder: contains data regarding the overview of waveforms
Pro Tools
Project Folder
Session File: contains the MIDI data, automation, etc.
Audio Files Folder: contains the audio files only
Fades Folder: contains list and record of fades applied to audio files
Region Groups Folder: contains references to linked regions
Session Files Backup: contains backups of current project
Video Files Folder: contains video files used in current project
WaveCache: a temporary file that contains information regarding the redrawing of the 
waveforms
Cubase
Project Folder
Session File (.cpr): contains the MIDI data, automation, etc.
Session File (.bak): contains backup file of current project
Audio Folder: contains the audio files only
Edits Folder: contains list and record edits
Images Folder: contains data regarding the overview of waveform
Logic Pro
Project Folder
Session File: contains the MIDI data, automation, etc.
Project Files Backup Folder (.bak): contains backups of previously saved sessions
Bounces Folder: contains audio bounces generated in LP
Audio Files Folder: contains the audio files only
Undo Data Folder: contains the undo history

70
CHAPTER 2  Basic Sequencing Techniques
Since in this chapter we are going to use MIDI tracks, first let’s focus on the MIDI signal path. To 
check whether your connections are working, try to record-enable a MIDI track, meaning make sure that 
the record button of the MIDI track you are going to test is checked or highlighted. By doing this you 
tell your sequencer to reroute the MIDI messages coming in from your controller through the computer 
MIDI interface to the cable/channel assigned to the output of the MIDI track record-enabled. Try to 
send some MIDI data by pressing the keys of your controller. You should hear sound coming from your 
speakers. If not, then it is time to learn some troubleshooting techniques that can help you to fix techni­
cal problems related to wrong signal path assignments.
There are two different areas to check when you don’t get any signal from your MIDI tracks. First, 
check that the MIDI data flow is set up correctly. Check whether you see MIDI activity on the sequencer 
by pressing a key on the controller. Depending on the software you use, you can monitor the messages 
received by the sequencer in several ways. In Cubase and Logic there is a MIDI activity meter in the 
transport window. In PT you can record-enable a MIDI track and check in the mix window whether you 
see activity in its view meter. In DP there is a useful dedicated window called MIDI Monitor (accessible 
from the Studio menu) that shows messages received for every cable and channel independently. If your 
sequencer does not have this function, you can check whether MIDI messages are received by simply hit­
ting record, pressing on some keys, and seeing whether the data were recorded on the MIDI track selected.
If no data are received by the sequencer, then check whether you record-enabled a track, check the 
MIDI cable connection, check that the MIDI interface is connected in the right way to your computer, 
and also double-check your MIDI setup in the Audio and MIDI setup control panel of the OS. The same 
techniques can be applied when troubleshooting instrument tracks. First check that the track is receiving 
MIDI data (the procedure is the same described above for the MIDI track) then check that the audio out­
put of the instrument track is routed to a valid audio output of your audio interface.
If your sequencer is receiving MIDI messages but your device is not, then check that the track you 
record-enabled has a cable/channel assignment. You can check whether the device is receiving data 
by looking at the “MIDI activity” light-emitting diode (LED) located on the front panel of most MIDI 
devices. If the LED is not reacting when you press a key on your controller, then check for the cables 
connecting the receiving device to the MIDI interface of the computer. If the LED shows activity by 
blinking, then the problem probably lies in the audio path. Make sure that the audio cables connecting 
the MIDI devices to your mixing board are well plugged in and that the faders on your board are up. 
If you see signal activity on the view-meters of the board, then the problem is probably part of the bus 
assignment or the speaker system. In general, the best way to fix signal problems (both audio and MIDI) 
is to narrow down the areas that can cause the problem. By isolating different regions of the signal path, 
you can fix your studio problems quickly and easily so that you can move on to the creative aspect 
of music production. If everything fails, try shutting down your system, computer included, waiting a 
minute, and then restarting from fresh; sometimes this is the only solution to the mysteries of electronic 
equipment! If everything is in working condition, you are ready to move on to the next section.
2.8  THE FIRST SESSION: CLICK TRACK AND TEMPO SETUP
Depending on your style of writing, the way you set up the tempo and meter of your compositions can 
vary significantly. The type of project (e.g., a groove-based song, a jazz ballad, an orchestral piece, a movie 
score) has an impact on how you set the tempo, or tempos, of your sequences. One thing to remember, 
though, is that no matter what style your project is based on, it is always recommended to sequence to a 
click track, even if you plan to have passages or entire sections as rubato. It is crucial to play to a click track 
because it is the only way the sequencer can assist you later during the edit and quantize operations. In this 

71
2.8  The First Session: Click Track and Tempo Setup
chapter you will learn how to set up the click track, the metronome, and the meter of your sequence using 
a steady tempo mark; in the following chapters I will discuss how to create tempo/meter changes and some 
advanced tempo-related sequencing techniques that allow you to achieve more fluid and realistic results.
2.8.1  Who Plays the Metronome?
The first thing to set up when starting a new project is the metronome and in particular its sound. Keep 
in mind that the settings you select for the metronome sounds are shared by all projects, so it is neces­
sary to go through this process only once, unless you decide to change them later. The metronome can 
usually be played by three different sources: the internal sound generator of the computer, any exter­
nal MIDI device connected to the interface, and from a software synthesizer track. It is usually wise 
to choose only one of the three, even though technically you could choose more than one option at 
the same time. These settings are selected from the “Click and Count Off Options” menu (sometimes 
referred to as “Metronome Settings” in LP and CU, or as “Click Options” in PT). In Figure 2.7 you can 
see the metronome settings window in CU.
I recommend in general using the internal audio metronome of your DAW since it is more accurate, 
it does not suffer from the intrinsic latency of the MIDI network, and it is much easier to setup an exter­
nal MIDI device or a software synthesizer. You can choose two different notes for the “high note” (beat 
1, or strong beat) and for the “low note” (all the other beats depending on your time signature, or weak 
beats). You can have different intensities for high and low notes. Usually you want to have the strong 
beat slightly louder than the weak beats. If you prefer to have the all the clicks with the same volume, 
then just set them up with the same intensity.
You also have control over when the click track will be played and over the count-off options. The click 
can be set up to play always (when the click is on), only when you record, or only during count-off. I rec­
ommend having the click always on and turning it off manually when you don’t need it. The main reason 
is that while the click is a valuable tool during recording, it is also essential during playback to check the 
FIGURE 2.7
Metronome settings window in CU.

72
CHAPTER 2  Basic Sequencing Techniques
rhythmic accuracy of your performances and recorded takes. Regarding the count-off, you can choose the 
number of precount bars and other options, depending on the sequencer you use. For example, in DP you 
can also choose to have the count-off playing only when recording (which is a very useful feature), or in 
CU you can have the count-off set at a different time signature than the one set for your project.
To access the Metronome setup window in CU go to Transport  Metronome Setup.
In DP go to Digital Performer  Preferences  Click.
In PT navigate to Setup  Click/Countoff Options.
Metronome settings in LP can be found in Files  Project Settings  Metronome.
No matter how you set up your metronome options, one crucial aspect to consider when sequencing 
is to select the right tempo at which to perform and compose your music. The tempo of your sequence 
can be changed from the transport bar of your sequencer. In this chapter we are going to deal with a 
steady tempo throughout the sequence. The first time you play to a click track is going to be a challenge, 
whether you are inexperienced or an accomplished musician. During many sessions I have run into pro­
fessional musicians who had a hard time getting used to playing musically against the metronome. You 
will get used to it; don’t worry, it is just matter of practice. The first thing I recommend before playing 
anything on your controller is always to keep in mind the tempo at which you want your sequence. If 
the tempo is well established in your mind, then it is going to be much easier to follow it in your per­
formance. If you have a groove, a melody, or a rhythmic phrase in your head, try to find out its tempo 
first. Do not try to match your musical idea to a specific tempo simply because it is already set to your 
sequencer. A composer does not write a piece of music at a certain tempo because the orchestra can play 
only at one tempo. Therefore, keep in mind that the technology you have in front of you is only a tool. 
You, not your computer, are in charge of the creative process. Sing the musical idea in your mind or 
out loud first and snap your fingers or tap your foot in order to have a preliminary idea about the beats 
per minute (BPM) at which you will set your metronome. Then press Play on your sequencer and start 
changing the tempo slowly until it matches the tempo you have in mind. You will see that now it will be 
very easy to play your musical idea over the metronome. When you are more experienced you will be 
able to skip this process, but if this is your first real sequencing experience, I highly recommend follow­
ing the aforementioned procedure.
2.9  RECORDING MIDI TRACKS
The moment you have been waiting for has finally arrived. You have learned about all the components 
that form your studio, and how to connect them and how to have them interact nicely and (hopefully) 
smoothly with each other. You have decided on and set up the tempo of your sequence. Now it is time to 
record and to put to use your knowledge and creativity.
As I explained earlier in this chapter, you must have a track record-enabled in order to have MIDI 
data moving from the IN to the OUT of your computer’s MIDI interface. If you plan to use a software 
synthesizer (or instrument) track simply create one with the desired instrument as explained earlier in 
this chapter and then record-enable it. If you use external hardware devices first make sure the local 
control of your controller is set to OFF (in case you have a MIDI synthesizer as controller), as explained 
in Chapter 1. You can change the sound assigned to the channel to which the track is directed either by 
selecting a bank and patch from the front panel of the receiving MIDI hardware device, or by sending 
a Patch Change message from the sequencer. If you choose the latter, your options will vary depending 
on the setup and sequencer you use. In the worst-case scenario, when clicking on the patch menu of a 
track in the sequencer you will see only a list of numbers, indicating the corresponding patch numbers 
on your device. If your sequencer was programmed with the names of the patches stored in your device, 

73
2.9  Recording MIDI Tracks
then instead of a meaningless list of numbers you will see a list with the patch names. To choose your 
patch in DP, simply click on the column named “Default Patch” in the track list window; in CU select 
the patch in the “Inspector Window” on the left side of the Project Window and select the desired Bank 
and Patch number; while in LP you can double-click on the track icon. The same result can be achieved 
in PT by clicking on the small icon on the channel strip of the track in the mix window (Figure 2.8).
Click the Record button (Record and Play in PT) in the transport window of your sequencer and start 
playing your part. While you are sequencing you will see the MIDI data being recorded by the com­
puter displayed on the record-enabled track. If you don’t like what you played, you can go back to the 
beginning of your part, delete the data by selecting them with the mouse and pressing delete, or simply 
undo your last take by choosing Undo from the Edit menu (Control-Z in Windows or Command-Z on 
a Mac).
If you like the part you just recorded but think you can do a better job, it is a good idea to keep the 
old take and try a new one. This technique is called multiple takes. In a MIDI sequencer it is possible to 
achieve the multiple-takes feature in several ways. The simplest one, and sometimes also the fastest, is to 
record-enable a different track, mute the one you just recorded, and start recording again. You can repeat 
this procedure until you are satisfied with your last take or you know you can compile the perfect take 
FIGURE 2.8
Patch list icon in PT.

74
CHAPTER 2  Basic Sequencing Techniques
by copying and pasting sections from several previous takes. This technique has some advantages: it can 
be used on any sequencer, and it allows you to see all the different takes at the same time, which makes 
editing easier. Another option is to use the “multiple-take” feature. Virtual tracks, called takes in DP or 
playlists in PT, are manually selected for each track (Figure 2.9).
After you record, if you want to try a different take all you have to do is to switch the selected track 
to another take and rerecord the part. This process can be repeated as many times as the memory of your 
computer allows.
In DP, if you want to compare two takes, you can simply switch back to any previous take by click­
ing on the Take column. You can also copy and paste between takes. This technique has the advantage 
of keeping the track count down, since each take is virtually superimposed on the others (keep in mind, 
though, that you can listen to only one take at a time for each track), but it has the disadvantage of not 
showing the data of more than one take for each track at the same time.
A similar approach is used by PT, where different takes can be stored in different playlists. To create 
a new playlist in PT simply click on the “arrow” button next to the track name of a track. From the list 
that appears select “New” for a new playlist or “Duplicate” to create a new playlist with the same con­
tent as the current one. By choosing “Delete Unused” you can purge all playlists that are not currently 
utilized in the project for that particular track. Use the same technique to switch among playlists. Other 
sequencers, such as CU and LP, use a slightly different technique, called cycle recording mode, that can 
be as effective as the one based on multiple takes.
In CU this feature is activated from the transport window and it allows you to create a loop between 
two locators (named “Left” and “Right” locators). When you hit the Record button, the sequencer will 
keep cycling between the two locators until you press Stop. You can have the sequencer set to erase the 
FIGURE 2.9
Multiple-take selection in DP.

75
2.9  Recording MIDI Tracks
old data at every new pass (“overwrite” mode), to keep the last complete pass (“keep last” mode), or to 
create a new part for each pass (“stacked” and “stacked no mute” modes). These options are available 
directly from the CU transport window. Each new part will be stacked over the previous one without 
erasing it and it is stored in a new lane.
In LP, after selecting the Cycle button from the transport window, you can choose to have the 
sequencer create a new take automatically after each pass. To set these parameters go to File  Project 
Settings  Recording and set the Overlapping Recording option to “Create Take Folders”.
Another aspect of recording MIDI tracks that is important to recognize is that you can keep over­
dubbing a MIDI track without erasing the data that were already stored on that track. This feature is 
called the overdub or merge function. In most cases you can choose from the transport window of your 
sequencer whether you want to merge the new data to the preexisting ones on the record-enabled track. 
If you prefer, you can opt for a more traditional approach (more similar to a regular tape recorder), 
where, if you record over a track, you automatically erase the preexisting data. I recommend having the 
overdub option always on to avoid deleting precious takes. If you want to delete them, it is easier simply 
to select the MIDI data in any editor window and manually erase them.
In DP and PT the overdub function is accessible directly from the transport window.
In LP the merge function is controlled from the transport window using the Replace button. If the 
button is highlighted, then the merge function is off and the preexisting data will be replaced by the new 
ones recorded.
In CU you can choose from the transport window between “Merge” or “Normal” mode (which over­
dub the MIDI data) and “Replace” mode (which erases the preexisting data) (Figure 2.10).
Now that you have recorded your first track, keep recording a few more to have enough material to 
learn the basic editing techniques of the next section.
You can open some of the examples found on the website to practice your editing skills.
FIGURE 2.10
Overdub options in DP, PT, CU, and LP.

76
CHAPTER 2  Basic Sequencing Techniques
2.10  BASIC MIDI EDITING TECHNIQUES
In this section I discuss the three most popular edit windows for MIDI data: the graphic (or piano roll) 
editor, the list editor, and the score (notation) editor. These three constitute the most used type of editors 
for MIDI data. Their features are very similar in DP, LP, CU, and PT.
2.10.1  The Graphic Editor
The graphic editor provides a very intuitive and quick way of accessing the MIDI data you want to edit. 
It is based on a “piano roll” view that shows a vertical replica of the piano keyboard on the left side of 
the window. It represents the notes as horizontal bars lined up according to their pitch (y axis) and their 
positions in the song (x axis). The advantage of this kind of editor is that it appeals to both the musician 
and the nonmusician MIDI programmer. You do not need classical music training to edit notes and data 
in this editor, since the notes and their parameters are easily read and changed in a very intuitive way.
To open the editor in DP, select the option “Graphic Editor” from the Project menu.
In LP select the option “Piano Roll” from the Window menu.
In CU choose “Open Key Editor” from the MIDI menu.
In PT navigate to the Window menu and select “MIDI Editor”.
In the graphic editor in DP and CU all the parameters of a note can be seen, and if necessary 
changed, by simply clicking on a note and looking at the top of the main window. Here all the data of the 
selected note (such as velocity, start position, end position, length, and MIDI channel) are displayed and 
can be changed by simply clicking on each field.
The same technique can be used in LP by selecting the “Event Float” option in the “View” menu 
of the Piano Roll editor. This will open a floating window that displays all the parameters of a selected 
event. The graphic editor layouts and features are very similar in all four DAWs. Another advantage of 
the graphic editor is that it allows you to edit not only note data but also any other MIDI data, such as 
CC, Pitch Bend, and velocities of the notes in a graphic way. These additional data are shown at the bot­
tom of the graphic editor window in an expandable frame. Figure 2.11 shows the graphic editor and the 
editable parameters in CU.
In addition to being able to change notes’ parameters by using the information area at the top of the 
window, you can change the notes’ position, start/end points, and length by using the mouse. To move 
a note, click in the middle of it and drag it horizontally and vertically as needed. To change its length, 
move the cursor toward the beginning or end of the note until you see the shape of the cursor change 
(into a small hand in DP, a trim tool in LP, a double arrow in CU). In PT you can change the start or 
end of a MIDI note by selecting the Trimmer tool first (Command-3 on a Mac and Control-3 on a PC) 
and then clicking and dragging the beginning or end of the note. A series of additional graphic tools 
also allows you to create, change their shape, and delete several events. The most common tools are the 
pencil, the eraser, and the reshape tool. The pencil allows you to insert new data, such as notes, CC, and 
Pitch Bend. In LP it also allows you to extend the beginning of a note. These tools are easily accessible 
from your tools window of your DAW: in DP use the shortcut Shift-O, in LP press the Esc key, in PT and 
CU they are available in the top left corner of the MIDI Editor window. With the eraser you can delete 
several data at the same time, including CC and Pitch Bend. The reshape tool in DP (Command-click in 
CU) is designed to change the parameters of existing data without inserting new ones. It is particularly 
useful in situations where you want to quickly alter a volume fade, modulation, or Pitch Bend data with­
out playing the part again or inserting new messages. In the graphic editor all the usual edit functions 
(accessible from the Edit menu) and shortcuts apply, as shown in Table 2.2.

77
2.10  Basic MIDI Editing Techniques
A quick way of copying data in the graphic editor is to press the Option key while clicking and drag­
ging the event (note, CC, or other). The sequencer will create an exact copy of the event that will be 
placed where you release the mouse.
2.10.2  Level of Undos
Every editing operation you do in your sequence can be undone by selecting the Undo function in the 
Edit menu (Command-Z on a Mac or Control-Z on a PC). It is possible not only to undo the last com­
mand, as in many low- and mid-range sequencers, but also to go back numerous operations before the 
last one and restore your sequence as it was before the edits. Most professional DAWs feature an unlim­
ited undo history, meaning that theoretically you can go back in time as much as you wish and bring 
FIGURE 2.11
The graphic editor in CU.
Table 2.2  Edit Functions and Shortcuts.
Function
Mac OS
Windows
Copy
Command-C
Control-C
Cut
Command-X
Control-X
Paste
Command-V
Control-V
Select All
Command-A
Control-A
Undo
Command-Z
Control-Z

78
CHAPTER 2  Basic Sequencing Techniques
back your project to any previous stage. This is an extremely valuable feature that allows you to experi­
ment creatively with your music and gives you the freedom to make mistakes and fix them with the 
touch of a button. The undo history is a list of all the actions and commands you executed on your 
sequencer since the beginning of your project or since the last time you purged the list. Clicking on the 
list you can jump to any point in time in your project history and revert all the action up to that point. 
The undo list in LP, CU, and DP can be opened and altered by selecting the “Undo History” option from 
the Edit menu (called simply “History” in CU). In PT the undos are controlled using the Command-Z 
keys (Undo) and the Shift-Command-Z key (Redo).
2.10.3  The List Editor
The list editor is probably one of the oldest editing tools for MIDI data. It has its roots in the first hard­
ware sequencers and it has been a valuable tool ever since. The philosophy behind the list editor is 
very simple: the MIDI data and messages are listed in chronological order and can be seen, edited, and 
deleted by clicking on each event. While this method may look a bit basic and unsophisticated compared 
to the graphic editor, nevertheless it can be very useful and speed things up, especially when used in 
association with a view filter (more on this in a little bit). It can be used to quickly delete certain data in 
your track or take a quick look at which data are quantized and which are not.
To open the list editor in DP navigate to Project  Event List.
In LP go to Window  Event List.
In PT go to Window  MIDI Event List.
In CU you can find it under MIDI  Open List Editor. The parameters displayed in the list window 
are the same as those we learned in the graphic editor (note, velocity, position, length, etc.) and they can 
be edited by clicking on the value field of each event (Figure 2.12).
A very helpful tool that can be used in conjunction with any editor but particularly so with the list 
editor is the view filter option. It allows you to decide which data are displayed in any window of your 
sequencer. This is a feature that can be handy when you are looking for a particular MIDI message or 
CC. Let’s say that you want to delete all of CC 7 from the bass track. One option would be to open the 
graphic editor, set the lower part of the window to display CC 7, and then graphically delete the data. 
But another way (probably faster) would be to set the view filter of your sequencer to show only CC 7 
and then to select all (Command-A or Control-A) and choose cut (Command-X or Control-X).
In DP the view filter can be found in the “Set View Filter” option under the “Setup” menu.
FIGURE 2.12
The list editor in DP.

79
2.11  Basic Principles of MIDI Note Quantization
In LP you can filter data by selecting the messages you want to see at the top of the list window 
(you can choose between notes, program change data, Pitch Bend, Controller, Channel Pressure, Poly 
Pressure, and System Exclusive messages).
In PT you can filter the messages by pressing the shortcut Command-F on Mac or Control-F on PC.
In CU the filter is activated by clicking the “Show Filter View” in the top left corner of the List 
Editor window and enabling the events you want to hide.
2.10.4  The Score Editor
The MIDI messages recorded in your sequencer can also be displayed and edited as regular notation 
in the score editor. This is a very useful tool, not only to check and edit in a more traditional way the 
MIDI parts you recorded but also to prepare and print out scores and parts of your compositions. In the 
score editor you can edit MIDI events such as notes, velocity, duration, position, and CCs such as CC 
64 (Sustain Pedal). While it is a good idea to use it for checking and inputting your parts, I discourage 
using it as your primary editing tool. The main reason is that all score editors use automatic quantiza­
tion correction to display the notes in a readable way. If you played your part fairly freely (and did not 
quantize it), the sequencer in the notation window would try to make some educated guesses to display 
the part in the clearest possible way. This could mislead you while you are trying to edit the data, since 
what you see is not precisely what you recorded. Therefore, I suggest using the notation editor mainly 
for scores and part preparation. LP and CU feature advanced and complete notation editors. They can be 
used for both quick parts checkup or in professional scoring situations. DP and PT have a pretty good 
notation editor that can be used for quick parts checkup, and some basic parts and score preparation, but 
lack the advanced layout features of CU and LP, such as automatic parts extraction and use of advanced 
nonstandard notation (e.g., tablature). CU has a very advanced score editor that can be effectively used 
for most professional parts and score preparation in both traditional classical and contemporary styles. 
PT has recently added a notation option that is based on the more advanced notation program called 
Sibelius. While the features of PT Score Editor are fairly basic, the ability in PT to export a project score 
in Sibelius native format makes this DAW a very effective choice for production that involves both elec­
tronic and acoustic tracks.
The notation editor has two main views: the edit view and the page layout view. The former is used 
mainly to edit the score and the parts and gives you a scroll view of the tracks. In edit view the song 
position is constantly updated on screen, making it very easy to keep track of errors and changes you 
wish to make to your parts. Page view allows you to see the score and parts as they will be printed. 
Here you can freely add layout symbols, markers, dynamic markers, annotations, bar numbers, etc. 
Every item or graphic object can be moved or placed freely around the score. Usually the edits and set­
tings applied to a part or track in the score editor do not affect the playback of MIDI messages in the 
track list window. In CU, for example, they affect only their appearance. Nevertheless, you can, if neces­
sary, apply those edits to the actual MIDI data by selecting the “Scores Notes to MIDI” function in the 
“Global Functions” menu under the Score main menu.
2.11  BASIC PRINCIPLES OF MIDI NOTE QUANTIZATION
One of the biggest advantages that MIDI and sequencing offer to the modern composer is the ability to 
quickly fix performance mistakes and the option to tie up the parts in a precise and quick way. The idea 
of quantization was introduced in the first hardware sequencers, but it has been perfected and brought 
to a higher level of sophistication with the introduction of the latest generation of software sequencers. 

80
CHAPTER 2  Basic Sequencing Techniques
To quantize literally means “to limit the possible values of a quantity to a discrete set of values”. This 
concept, applied to a MIDI sequencer, describes a function that allows you to rhythmically correct the 
position of MIDI data in general, and notes in particular, according to a grid determined by the quantiza­
tion value. In practice, when you quantize a part, a track, or a region of your sequence, you are telling 
the computer to move each piece of MIDI data to the closest grid point specified in the quantization 
value. To better understand this concept, compare Figure 2.13(a) with Figure 2.13(b).
In (a) you see a sequenced part that has not been quantized. The notes were not played accurately in 
rhythm with the metronome. While this sometimes can create a nice natural and smooth effect, most of 
the time it results in a sloppy sequence and can lead to a poorly produced project. In (b) the same part 
has been quantized with a 16th note grid. As you can see, every event has been moved to the closest 16th 
note, rhythmically cleaning the part, which now plays in perfect sync with the metronome.
Listen to audio Examples 2.1 and 2.2 on the website to hear the difference between quantized and 
nonquantized parts.
For basic quantization you must first select the part, region, or notes you want to quantize. You 
can do so from any editor window of your sequencer. If you want to quantize a large section, I suggest 
selecting the region from the track list (or arrange) window. If you want to quantize only a section or few 
notes, then it is better to open the track in the graphic or list editor and specifically select the events you 
want to quantize. Remember that you can select noncontiguous events by holding the Shift key (in some 
DAWs you have to use the Command key instead) and clicking on the individual events. At this point 
you have to select the quantization option.
In DP go to the “Region” menu and select “Quantize…”.
FIGURE 2.13
(a) Unquantized part and (b) quantized part, in 16th notes.

81
2.11  Basic Principles of MIDI Note Quantization
In LP open any MIDI editor (I recommend using the piano roll) and select the desired quantization 
value from the quantization value drop-down menu (set to “off” as a default).
In PT you have to select “Quantize” from Event  Event Operations.
In CU use the “Over Quantize” option found in the “MIDI” menu.
The next step is to choose the right quantization value for the region you have selected (Figure 2.14). 
This can be tricky, depending on the rhythmic complexity of the part. The main rule is to select a quan­
tization value that is equal to (or in certain cases smaller than) the smallest rhythmic subdivision present 
in the selected region. For example, if you recorded a percussion part where you have mixed rhythms 
such as 8th, 16th, and 32nd notes, then you have to set the quantization value to 32nd notes. If you 
choose 16th notes instead, then the smallest rhythmic figures (in this case the 32nd notes) will be moved 
to the closest 16th note and therefore lost and misplaced.
Listen to Examples 2.3, 2.4, and 2.5 on the website and compare, respectively, the nonquantized ver­
sion, the quantized part with the right value, and the quantized part with the wrong value. While this 
procedure looks fairly simple, things get a bit more complicated when your part features mixed rhythmic 
figures, such as straight 8th notes and 8th note triplets (or more generally, any kind of straight notes 
versus tuplets). In this case you have two options. The first is to selectively choose and quantize regions 
that have similar rhythmic features; for example, open the graphic editor and select and quantize all 
the events that share a common rhythmic subdivision (i.e., 8th notes). Then do the same with 8th note 
triplets, and so on. Even though this technique can be a bit time consuming, it will guarantee you the 
best and most accurate results. Another technique is to use the “smart quantize” feature present in some 
sequencers, such as DP (accessible from the “Region” menu). This function creates a floating grid that 
automatically adjusts itself depending on the notes recorded. This way you do not have to choose a 
quantization value. It is ideal for quantizing a long section of a track where mixed rhythms are most 
likely to be found.
FIGURE 2.14
Quantization options window in PT.

82
CHAPTER 2  Basic Sequencing Techniques
Quantization is almost a magic tool that can make your parts sound exactly in time with the metro­
nome. While this feature can definitely improve the general feel of the production, at the same time it 
can take away the life of your music by flattening the groove of your performance and making it sound 
very stiff. But don’t worry, the techniques that I just described are only a small part of what a sequencer 
can do in terms of quantization. In succeeding chapters you will learn more advanced techniques that 
will help to compensate for the stiffness introduced by straight quantization.
The quantization action, like any other editing action, can be undone using the undo history in the 
Edit menu. You can also apply quantization by inserting a MIDI filter on the track or tracks you want to 
quantize. This feature gives the option of turning quantization on and off at any time without making a 
real commitment to a certain quantization value. It has the drawback that it cannot be applied to single 
notes or data but instead it is used to quantize entire regions or tracks.
In DP, for example, you can insert a quantization plug-in on the mixer channel of a MIDI track by 
simply selecting it from the list of plug-ins that shows up when clicking on an insert slot in the mix win­
dow (Figure 2.15).
A similar result can be obtained in CU by inserting the MIDI effect called “Quantizer” in the mixing 
board channel of the MIDI track you want to quantize (Figure 2.16).
In LP you can quantize a region directly from the sequence/audio region parameters window 
located at the top left of the Arrange window. An entire track or each region individually can have 
separate quantization values that can be changed or even turned off at any time (Figure 2.17). To 
FIGURE 2.15
The quantization MIDI plug-in in DP.
FIGURE 2.16
The quantizer plug-in 
in CU.

83
2.12  Audio Track Basics
make the quantization final you can go to MIDI  Region Parameter  Apply Quantization Settings 
Destructively.
In PT (as in LP) real-time quantization can be applied to a region only and not to individual data. 
This feature in PT can be accessed by selecting Event    MIDI Real-Time Properties (Figure 2.18). 
Notice how you can make the quantization final (not real-time) by clicking on the “Write to region” 
button.
In succeeding chapters you will learn more advanced quantization techniques, such as groove quan­
tization, and more detailed parameters, such as swing feel, sensitivity, and strength, that you can use to 
avoid the mechanical groove caused by straight quantization.
2.12  AUDIO TRACK BASICS
So far in this chapter you have learned how to record, edit, and quantize MIDI tracks. While the MIDI 
standard is an extremely valuable music production tool, the integration of MIDI tracks with audio 
tracks has brought the sequencing experience to a much higher level. Therefore, it is time now to start 
exploring the interaction of MIDI and audio tracks.
Audio tracks can be of two types: mono (meaning tracks that contain and play back only one channel 
of audio) and stereo (two channels of audio, usually codified as left and right). Before recording and/or 
importing an audio file, you should set the track to the right type, depending on the type of audio mate­
rial you will be working on (mono or stereo). Let’s create a new stereo track.
In DP, select “Add Track” and then “Stereo Audio Track” from the Project menu. A new track with a 
double waveform symbol will be added to the track list window.
In LP go to Track  New and select the Audio checkbox. After creating the audio track, you can 
change the type (mono or stereo) by clicking on the respective symbol at the bottom of the channel strip 
for the track you just created (a mono track has a single meter while a stereo track has two).
FIGURE 2.17
The real-time quantization in LP.
FIGURE 2.18
The real-time quantization in PT.

84
CHAPTER 2  Basic Sequencing Techniques
In PT, simply select “New Track” from the File menu and change the default “Mono Track” option to 
“Stereo”.
In CU you can create a new track by choosing “Add Track” from the Project menu, selecting 
“Audio” from the menu, and then clicking on “Stereo”.
Once you have at least one stereo audio track ready, import some audio loops to play along with the 
MIDI tracks in the project. You can import almost every type of file you want in the sequencers pre­
sented in this book. They are all professional tools able to deal with pretty much any format.
On the website you can find a series of recorded loops and grooves inside the folder named “Audio 
Loops”. Feel free to use them to get familiar with importing and editing audio files. Most of the audio 
applications use the following file formats to record and store the audio information on the HD: AIFF 
(audio interchange file format), WAV (wave), SDII (Sound Designer 2), and BWF (broadcast wave for­
mat). While these are the most popular in the industry at the moment, other formats are supported, and 
they may vary depending on the sequencer you are using. Table 2.3 shows a summary of the most com­
mon file formats supported by the four DAWs.
The fastest way to import an audio loop into your sequence is to drag the audio file directly into the 
arrange/track list window of your current project. This procedure works for all four DAWs. Your imported 
file will be automatically copied to the Audio Files folder of your project. After the overview of the wave­
form has been calculated in the background, it will be placed in the arrange/track list window (Figure 2.19).
Try to import one of the files you find on the website, and make sure that it plays back.
The file you just imported can be edited and manipulated in several ways, but while some of the tech­
niques are similar to the ones you learned for the MIDI track, others differ drastically. In this chapter I 
describe the basic graphic editing procedures available for audio tracks and sound bites, while in subse­
quent chapters you will learn more advanced options and techniques.
2.12.1  Destructive and Nondestructive Audio Editing
In your DAW, the audio files can be edited in two different ways: destructively and nondestructively. To 
understand the difference between the two techniques we have to become familiar with the way audio 
Table 2.3  Common File Formats 
Supported by the Four Digital 
Audio Workstations.
Audio File Format
AIFF
WAV
SDII
BWF
Audio CD
MP3
REX2
QuickTime
AVI
AIFC

85
2.12  Audio Track Basics
data are stored by the sequencer on the HD. When you record or import a file, the data are stored in 
audio files generated or copied into the working directory of the current project. These files usually 
assume the names of the track on which they were recorded or the name of the files imported. Any non­
destructive edit that is done does not actually change the contents of the original file that was recorded or 
imported but instead changes virtual pointers that point to different parameters of that file.
For example, when you change the beginning or end of an audio region (sometimes called sound 
bite), you do not trim the original file on the hard disk but trim a virtual copy instead. This technique has 
many advantages; for example, you can always go back to the original file in case you decide that the 
edits you made were not suitable for the project, and it saves a lot of space on the HD, since when you 
copy a region you do not actually copy the audio file but just a new set of pointers that still point to the 
original file on the HD. Destructive editing is less forgiving, in the sense that the edits you do are per­
manently incorporated in the original file (or in an entirely new one). This technique is more dangerous 
because it gives you less space for error. Both editing options are nevertheless available and used in the 
four main sequencers in one way or other. In this chapter we discuss the nondestructive ones, which usu­
ally are the most common.
In most cases, all nondestructive edits can be done from the arrange/track list window, making it 
really easy to adjust audio sound bites, depending on your needs. In LP, CU, and PT it is possible, 
directly from their Arrange windows, to trim, separate, cut, copy, paste, repeat, and fade audio regions 
without having to open a separate editor.
To trim the beginning or end of an audio region in LP, with the arrow tool selected, move the cursor 
to the beginning or end of it until you see the Trim icon. Now you can simply click and drag to trim the 
region. You will see your region’s boundary move with it. When you release the mouse, the sound bite is 
trimmed.
In CU the procedure is very similar. With the arrow tool selected click on the small handles posi­
tioned in the lower left (beginning) and right (end) corner of the region you want to trim and then click 
and drag.
In PT all the audio edits are done from the main edit window. To trim a sound bite, simply select the 
trimmer tool (Figure 2.20) from the main edit tools in the top part of the edit window, and click and drag 
from the beginning or end of the audio region.
To trim a sound bite in DP you first have to select the sound bite you want to edit, then open the 
Sequencer Editor (Shift-S or double-click on it from the track list window) and then with the arrow tool 
selected move your cursor to the beginning or end of it until you see the Trim icon. Now you can simply 
FIGURE 2.19
Waveform overview in the Arrange window of CU.

86
CHAPTER 2  Basic Sequencing Techniques
click and drag to trim the region. Remember, though, that, as I mentioned before, this a nondestructive 
edit, so you can also revert the region to its original status by clicking, dragging, and trimming back its 
boundaries.
The basic operations of cut, copy, and paste also apply to audio regions and follow the regular proce­
dures of editing. You can split a region by selecting the scissor tool from the tool palette in DP, CU, and 
LP and click wherever you want to insert the split. This action will cut the region in two, creating two 
completely independent bites (but not a new audio file, since it is based on nondestructive editing).
In PT you would have to choose the selector tool (Figure 2.20), click on the point where you want 
to cut the region, and select Separate Region  At Selection from the Edit menu (or use the shortcut 
Command-E). There are several techniques to join back together two or more regions that were previ­
ously split. In PT simply select the regions with the grabber tool by Shift-clicking on all of them, and 
then select “Heal Separation” from the Edit menu (Command-H).
In CU and LP simply select the glue tool and click (Shift-click in LP) on the parts you want to join 
together.
In DP you have to select all the sound bites you want to join by Shift-clicking on all of them and 
choose “Merge Sound bites” from the Audio menu (keep in mind that this action will create a new audio 
file and therefore will increase the size of the project).
2.12.2  Playing it Nice with the Other Tracks
Now that you have learned the basics of handling audio tracks, let’s deal with some practical issues 
that very often puzzle beginners as well as advanced MIDI producers. When you import a loop from an 
audio file, most likely its tempo won’t match perfectly the tempo of your sequence, which can be a real 
disaster. Fortunately, this problem is easily fixable in several ways. The first thing that probably comes to 
your mind is to adapt the tempo of the sequence to the loop. This works fine if you are flexible with the 
tempo of your composition. But in most cases composers don’t like to make adjustments (read “compro­
mises”) when dealing with their compositions. Also keep in mind that if you import more than one loop 
and they all have different tempos, then you are in trouble. Therefore, the goal is first to find the tempo 
of the imported loops and then to time-stretch them (meaning change their tempos without affecting 
their pitch) in order to have them match the tempo of your sequence.
The first thing you have to do is find out the exact tempo of the loop (or loops) you imported in your 
sequence. While in some cases the tempo is indicated on the booklet of the loop library you bought, 
there are other situations on the file name of each loop where we don’t know for sure the tempo of an 
audio loop. There are several ways to determine it.
FIGURE 2.20
PT tool palette description.

87
2.12  Audio Track Basics
The most generic technique involves a simple equation that can be applied to any digital audio file. 
By knowing the sample rate, the number of beats, and the number of samples of a loop, you can find out 
the tempo, in beats per minute, by applying the following formula:

(
)
NB
SF
6
/NS

0
where NB is the number of beats of the loop, SF is the sampling frequency in hertz, and NS is the 
number of samples of the loop. For example, if you have a two-bar loop in 4/4 meter (a total of 8 beats), 
recorded at 44,100 Hz, that contains 200,540 samples, then the tempo will be

(
,
)
,
.
8
44 1
6
/2
54
1 5 5 BPM



00
0
00
0
0
To find out the number of beats in your loop, simply listen to it. Information on the sampling fre­
quency and number of samples can be collected in different windows, depending on the sequencer you 
use. Table 2.4 sums up where you can find them on each sequencer.
Table 2.4  Sampling Frequency and Number of Samples.
Sampling Frequency
Number of Samples
DP
The sampling frequency is the one of 
the current project and it can be seen 
in the Transport window
Open the Time Formats window from the Setup menu and 
make sure that “Samples” is selected (this will allow you to 
see the information of a sound bite as samples). Now open 
the Sequencer editor and click on the loop. At the top of 
Sequencer window you will see all the information about the 
selected soundbite, including the length of samples
LP
Press the “B” key to open the Bin 
window. In the Bin window at the 
right of each audio file you will see 
information about both the sample 
rate and if the file is stereo or mono
Open the destructive audio editor (called Sample Editor) by 
double-clicking on the audio region of your loop from the main 
Arrange window. Use the view menu of the window and select 
“Samples” as a view option (this will guarantee that the length 
of the audio files is shown in samples and not in other formats). 
Select the entire loop (Command-A) and read the number of 
samples in the upper left corner of the destructive audio editor 
window
PT
The sample rate of any audio files 
in a session matches the one of the 
session, since the sample rate of 
every audio file that was imported is 
converted into the one at which the 
session was created
Use the selector tool to highlight the region of which you want 
to find the tempo. Make sure you have selected the main 
counter to show the number of samples by choosing the 
“Samples” option in the Display menu. Read the number of 
samples of the area you selected in the Event Edit Area next to 
the Location Indicators
CU
Open the Audio Pool window located 
in Project  Pool. You can see all the 
information regarding any particular 
audio file, including the sample rate
First make sure that you enable Sample view from the Project 
Setup (Project  Project Setup), then open the destructive 
audio editor (called Sample Editor) by double-clicking on the 
audio region of your loop from the main Arrange window. 
Select the entire loop (Command-A) and read the number of 
samples in the field named “Selection Range” located in the 
upper right corner of the window
DP: Digital Performer; LP: Logic Pro; PT: Pro Tools; CU: Cubase.

88
CHAPTER 2  Basic Sequencing Techniques
The technique explained in the preceding paragraph can be used from any audio sequencer. Each 
application has specific procedures and tools for finding out the tempo of a loop that are worth consid­
ering because in most cases they can help to speed up the process. You will find a brief explanation of 
these techniques in the following paragraphs.
In DP a quick way to find out the tempo of a loop is to trim the beginning and end of the sound bite to 
the length of the loop so that you are able to select the exact number of measures that form the loop. Select 
the trimmed sound bite and choose “Set Soundbite Tempo” from the Audio menu. Punch in the time 
length of the loop in beats. The tempo of the sound bite will be calculated automatically and listed next 
to the “Tempo” field in the lower right corner of the active window. By clicking the OK button, you will 
stamp the selected sound bite with the tempo that was calculated by the computer. Now, to simply change 
the tempo of the loop to match the tempo of the song, select the function “Adjust Soundbites to Sequence 
Tempo” from the Audio menu. If you want to change the tempo of the sequence in order to match the 
tempo of the selected loop, choose “Adjust Sequence to Soundbites Tempo” from the same menu instead.
A similar technique is available in PT. Use the selector tool to highlight an exact number of beats of 
your loop. Zoom in if necessary to be as precise as possible; you can also use the Tab to Transient function 
(located below the Trimmer tool) which allows you to use the Tab key to jump to the next transient (usu­
ally where the downbeats of a loop are). Select the function “Identify Beat” from the Event menu. Make 
the necessary changes to the start and end time signature fields, depending on the time signature of the 
loop. Insert the end location value, which will be equal to the start location plus the number of beats of the 
loop you selected. After you click the OK button, you will see that a new tempo change marker has been 
inserted in the tempo ruler at the top of the edit window. That tempo is the tempo of your loop. Now that 
you know its exact tempo, you can adapt it to the tempo of the sequence by selecting the loop/region in 
the Edit window and choosing AudioSuite  Other   Time Compression Expansion. Look at the Tempo 
fields: in the “Source” field insert the original tempo of the loop, and in the “Destination” field insert the 
tempo to which you want to convert the loop (probably the tempo of your sequence). Press the Return 
key and then click on “Process”. Your loop now will match the tempo of the sequence. You can also use 
the Time Compression/Expander Trimmer to achieve the same result. This tool allows you time-stretch the 
audio region instead of trimming its end. Select the tool by clicking-and-holding on the Trimmer tool and 
select the TCE tool. Then switch to Grid mode in the edit mode area (top left corner of the Edit window) 
and simply drag the end of the region to line up with the closest downbeat of your sequence.
In LP, if you want the tempo of the loop to match the one of the sequence, trim the loop to an exact 
number of bars (2, 4, 8, or similar) and then set the locators to match the same number of bars of the 
trimmed loop. From the Audio menu of the Arrange window select “Time Stretch Region to Locators”. 
The loop will be time-stretched according to the tempo of the sequence. To adapt the tempo of the 
sequence to the tempo of the loop follow the same procedure for selecting the region, but then navigate 
to Options  Tempo  Adjust Tempo using Length and Locators. The tempo of your sequence will 
automatically adapt to the tempo of the selected region.
In CU, open the sample editor by double-clicking on the audio loop from the main arrange window. In 
the sample editor make sure that at the top of the window the Bars and Beats field reflects the actual dura­
tion of the loop you are using (CU usually fills this part automatically). Now turn on the “Music Mode” by 
clicking on the Note icon to the left of the Bars field (it becomes red). If the loop was already pretrimmed 
and it was made of an exact number of bars this is all you need to do. From now on the loop will automati­
cally adapt to the tempo of your sequence. If you feel that you have to manually adjust how the loop fits 
the bars and beats of your sequence, click on the “Definition” tab on the left side of the Sample Editor 
window. Click on the “Manual Adjust” button and manually move each reference point to line up with the 
transient of the loop. In CU, if you have to quickly find the tempo of a loop (or a section of an audio track) 
I recommend using the Beat Calculator function. To use it, in the Arrange window select an exact number 

89
2.13  Basic Automation
of bars from the audio material you are working with. Then navigate to Project  Beat Calculator and enter 
the exact number of bars you selected previously (Beats field). If you want you can quickly insert the new 
tempo in the tempo track by clicking on “Insert Tempo into Tempo Tracks”.
While the aforementioned techniques are pretty basic when it comes to audio manipulation they are 
extremely useful on a daily basis. In the next chapters you will learn more advanced tempo-based audio 
tricks such as beat mapping and flexible tempo tools. Keep also in mind that so far we have used regular 
types of audio files (such as AIFF or WAV) that do not automatically adapt to the tempo of the sequence 
unless you use the aforementioned techniques, and that is why I call them “dumb” loops. Later in the 
book we are going to learn how to use more advanced types of audio files such as REX2 and Apple 
loops that can automatically adapt to the tempo of your sequence (and that is why I call them “smart” 
loops). Listen to Examples 2.6–2.9 on the website to compare the same loop time-stretched to accommo­
date three different, new tempos.
2.13  BASIC AUTOMATION
Another big advantage of using an audio sequencer to record, mix, and produce your projects is that 
almost every parameter of a track can be automated either graphically or in real time. In this section we 
focus on the automation of basic parameters, such as volume, pan, and mute. The automation of volume 
and pan can contribute greatly to the enrichment and improvement of your production. Through automa­
tion you can create crescendos, decrescendos, and fades independently for each audio track and MIDI 
channel, or you can create stereo effects by having the signal moving from left to right, and vice versa.
Listen to Examples 2.20–2.22 on the website. The automation can be applied in similar ways to both 
MIDI and audio tracks. In the case of MIDI track automation it is achieved through the use of CC mes­
sages (CC 7 controls the volume of a MIDI channel, while CC 10 controls its pan). The automation of 
audio tracks is controlled by sequencer internal protocols. Depending on your background, you may find 
yourself more comfortable inserting automation data in real time through the use of a hardware control 
surface or a digital mixing board, by recording a fader’s movement controlled by the movement of your 
mouse, or by inserting automation data graphically through the graphic editor of the sequencer. No mat­
ter which option you choose, the type of data inserted will always be the same, and the automation infor­
mation can be edited in all the aforementioned ways at any time. I usually recommend doing a rough 
automation mix with the mouse or with an external control surface and then going back to polish your 
final mix through graphic editing of the data previously inserted.
There are two main types of automation: static and dynamic. Static automation is achieved through 
the use of snapshots taken at different locations in your project. The parameters of your virtual mixing 
board will change according to the data inserted on each snapshot. This is a pretty basic way of automat­
ing your mixes. Its advantages are that it can be a quick way to insert basic ideas that can be developed 
later and that it uses very little processing power and MIDI bandwidth. A dynamic mix involves continu­
ous changes over time of different mix parameters at the same time. In a dynamic mix scenario you can 
have the volume of track 1 slowly fading in while the pan of track 2 moves continuously from left to 
right. This type of mix gives you much more flexibility, versatility, and control over your mixes.
2.13.1  Static Automation
Static automation can be recorded in two different ways. In DP you can use the snapshot function avail­
able from the mix window, while in PT, CU, and LP you can manually insert data either in the graphic 
editor or in the list editor.

90
CHAPTER 2  Basic Sequencing Techniques
To insert a snapshot in DP simply open the mix window, set the locator at the point in time where 
you want to insert the snapshot, make the necessary changes to your mix (set the volume, pan, and mute 
assignments for your tracks), and click on the small camera icon in the lower left corner of the mix win­
dow. A menu will appear with a few parameters regarding the snapshot you are going to take. In Table 2.5 
you can see a brief explanation of these parameters.
By clicking on the OK button you take a snapshot, and all the automation data for all the tracks 
specified in the previous settings window will be inserted. In order for the sequencer to follow and play 
back automation data, you need to make sure that each track on which you want to have the automation 
executed has the automation playback option enabled. In DP you do so by clicking on the automation 
playback button of each track from either the track list window or the mix window (Figure 2.21a).
In CU click on the “R” (Read) button next to the faders on the mix window or in the inspector win­
dow (Figure 2.21b).
Table 2.5  Snapshot Mix Parameters in Digital Performer.
Parameter
Description
Time Range
Allows you to specify an exact time range in which the snapshot settings will be inserted
Tracks
Allows you to specify which tracks will be included in the snapshot
Data Types
Allows you to specify which automation data (volume, pan, etc.) will be included in the 
snapshot
FIGURE 2.21
Real-time playback automation enabler in (a) DP, (b) CU, (c) LP, and (d) PT.

91
2.13  Basic Automation
Table 2.6  Record Automation Modes.
Mode
Description
Overwrite
If you select overwrite the automation data will be recorded as soon as you press the 
playback button on your sequencer even if you don’t move any faders or knobs. The 
existing automation data present on the track will be replaced by the new settings. The 
recording of automation data will stop only by stopping playback. This mode can be a 
bit dangerous if you are not careful because you may end up deleting some prerecorded 
automation data if you don’t stop the sequencer in time. It is useful though when you insert 
automation for the first time on a track since it allows you to insert data without moving any 
object in the mix window
Touch
The touch mode allows you to record automation only when you click on an object of the 
mixing board and change a parameter. When you press the play button no automation 
will be recorded until you click and move a fader or a knob. If you release the controller no 
automation data will be recorded.This is the safest option because you will be recording 
data only when you move a controller on the mix window. It is the best option to replace 
only specific passages of automation
Latch
The latch mode is a combination of the two previous modes. Like touch mode, after you 
press the play button, the sequencer starts recording automation data but only when you 
click and move a fader or change a parameter. Like overwrite, though, the recording of 
automation data will stop only by stopping playback, meaning that even after releasing the 
mouse the current settings will be recorded until you press the stop button
In LP and PT make sure you select “Read” from the menu located above the fader of each channel in 
the mix window (Figure 2.21c, d).
While LP, CU, and PT do not have a snapshot function built in, you can still take advantage of static 
automation by manually inserting automation data for each track and parameter you want to automate. 
You can do so in the graphic editor, the list editor or, in the case of LP, in the “Hyper Edit” window. I am 
going to talk specifically about these techniques in the next few paragraphs, where I describe how to edit 
automation data.
2.13.2  Dynamic Mix: Real-Time Automation
The first step in recording automation data for your project in real time is to record-enable the automa­
tion for the tracks on which you want to record the data. This can be set from the mix windows.
In DP, click on the record button in the “Automation” section of the track’s channel strip in the mix 
window. Be careful not to click on the actual record-enable button of the track (Figure 2.21a).
In CU, click on the “W” (Write) automation button, placed to the left of each channel in the mix win­
dow (Figure 2.21b) and select the desired auto record mode at the top of the Arrange window.
In LP and PT select one of the automation recording modes (Touch, Late, Write) by clicking on the 
automation field right above the view meters of the track in the mix window (Figure 2.21c, d).
The next step is to choose the record-automation mode you are going to use. There are three main 
modes with which automation can be recorded: overwrite (called write in PT and LP), touch, and latch. 
The main difference between these modes is the way the automation data are written on the track. Take a 
look at Table 2.6 to get a better understanding of the differences between the modes.
In addition to the settings mentioned in Table 2.6, each sequencer has its own variations of automa­
tion modes that simply expand the basic options explained earlier. For example, in CU the latch func­
tion is called Auto-latch and in addition you have the Cross-Over mode that works in a similar way 

92
CHAPTER 2  Basic Sequencing Techniques
to Auto-Latch, with the only difference that the recording of automation stops (punch-out) when you 
crossover an already existing automation curve after touching the parameter for a second time.
In DP and CU the trim mode (this option is available in CU only through Project  Automation 
Panel) allows you to reset the offset of preexisting automation data and therefore to modify their values 
without changing the original shape of the automation.
2.13.3  Editing Automation Data
Automation data inserted manually, through snapshots or through real-time recording techniques, can 
be edited in two ways: graphically and in the list editor. Even though the latter option could work for a 
small amount of data, it would quickly become overwhelming if used to edit a large amount of data for 
several tracks. A much easier way of editing and inserting automation data is based on the use of the 
graphic editors available in every sequencer. Automation data on MIDI tracks can be edited using the 
techniques you learned in the first part of this chapter. Volume data are represented by CC 7 and pan 
data by CC 10. The automation data for audio tracks and the editing techniques associated with them 
are represented using proprietary methods of each sequencer, and, even though they are fairly similar, 
the operational differences can be substantial. Therefore, I am going to list the different ways of editing 
automation on a per sequencer basis.
In DP you have plenty of ways to edit the parameters and data related to automation. For MIDI 
(since you are dealing with MIDI data) you can edit automation in the graphic editor and filter the data 
you want to work on by simply selecting the right CC number from the “Continuous Data Filter” avail­
able in the lower left corner of the graphic window. If you choose CC 7, then on the lower part of the 
window you will see only volume MIDI data. At this point, by using the mouse, you can select, delete, 
copy, cut, and paste the automation events previously recorded.
Another way to edit the automation data is through the Sequence Editor window (Shift-S) shown in 
Figure 2.22. This is probably the best option since from this window you can edit MIDI and audio tracks 
at the same time, including their automation data. To edit the automation, simply open the Sequencer 
Editor and select the track you want to edit by clicking on the track names to the left of the window (if the 
list is hidden, click on the “Expand” button in the lower bar of the window). Select the data type you want 
to edit (i.e., volume, pan, mute, etc.) by choosing the parameter from the first drop-down menu below 
the track name. Use the pencil tool (Shift-O opens up the tool bar) to insert new data, the reshape tool to 
alter existing data, and the marquee tool to select, insert, and move individual data. You can also use the 
“reshape flavor” button to choose between different shapes when you use the pencil or reshape tool.
In CU the automation data can be inserted and edited directly from the main Arrange window, mak­
ing it a very easy, intuitive, and quick process. For each track you can look at the automation data in the 
form of additional track layers. You can make changes at any time and in real time using the tools in the 
tool palette. To open the different automation layers for a track, move the mouse to the lower left corner 
of a track and click on the “” sign that will appear (Figure 2.23).
A new track layer will show a grayed-out version of the MIDI data or waveform plus a line with the 
automation data for a particular parameter (such as volume, pan, mute). Using the draw tool (pencil) you 
can insert new automation data. With the line tool (line) you can change or insert data with different pre­
programmed shapes, such as line, parabola, sine, square, and triangle. Use the object selection (arrow) 
tool to change or insert single points on the data curve.
A similar approach is used in LP, where automation can be edited and inserted from the main 
Arrange window. To reveal the automation parameter layers, choose the “Track Automation” option 
in the View menu. The default parameter set is going to be Volume for both MIDI and audio tracks. 
You can see several parameters at the same time on several automation layer-tracks (each displaying a 

93
2.13  Basic Automation
FIGURE 2.22
Graphic editing of automation data in DP.
FIGURE 2.23
Graphic editing of automation data in CU.

94
CHAPTER 2  Basic Sequencing Techniques
different controller) by clicking on the small arrow pointing right in the last automation track available 
(Figure 2.24).
Use the pencil to insert or modify data, the eraser to delete data, the arrow to insert or change sin­
gle data, and the automation tool to select or change the curvature of existing automation (four types of 
curve are available: concave, convex, and two types of S-curve).
In PT all the automation edits are done in the edit window (Figure 2.25). Each track has different 
layers, which can be activated by clicking on the drop-down menus below the track names (Automation 
Layer Selection). As a default you can access volume, pan, and mute parameters. Eventually, as with the 
other sequencers, you can automate any parameters of any plug-in and effect.
To edit the automation parameters of a track, you can use the main editing tools. Use the pencil 
tool to insert or change data. As with the other sequencers, you have several preset curves, such as line, 
square, triangle, and random shape, to facilitate repetitive edits. With the grabber tool you can insert sin­
gle data or change existing ones, while with the selector tool you can select and delete multiple data at 
the same time.
2.14  PRACTICAL APPLICATIONS OF AUTOMATION
The actual uses and practical applications of automation can have a big impact on the final result of your 
projects. While you will discover new frontiers in automation in every project and sequence you will 
work on, there are some very useful applications I would like to share with you. Use them as a starting 
point for your sequencing activities.
FIGURE 2.24
Graphic editing of automation data in LP.

95
2.14  Practical Applications of Automation
2.14.1  Volume Automation
The automation of the volume parameter has some common applications, such as fade in and fade out of 
single track, multiple tracks, or entire sequences if applied to a master fader track. These can be applied 
to both MIDI and audio tracks. Keep in mind, though, that for MIDI tracks you are allowed only one 
volume per MIDI channel, not one per track. If, for example, you have a drum part spread among several 
MIDI tracks (let’s say bass drum, snare drum, hi-hat, and toms), all sent to the same device and MIDI 
channel, then you will have only one common volume available for all of them, making it fairly imprac­
tical to fade one drum track at a time.
There are several workarounds, though. The first solution would be to assign different tracks to dif­
ferent channels or devices if your setup allows it. Otherwise you can record the MIDI tracks as individ­
ual audio tracks by routing the output of the sound generators of the MIDI device to the inputs of your 
audio interface (this process is referred to as rendering MIDI as audio). Once the tracks are recorded as 
audio, you will have independent control over any parameters, including volume, pan, mute, and effects. 
Do not delete the original MIDI tracks, in case you want to change some of the parts at a later time. 
While this is a good workaround, it will increase considerably the size of the project in terms of HD 
space. A third solution would be to use the velocity parameters of each part to achieve different volumes, 
fade-ins, and fade-outs. Use the graphic editor to open the track on which you want to create the volume 
change, select the notes for which you want to edit the velocity, and, by using either the pencil or the 
reshape tool, change the velocity according to your needs. If you use this procedure, make sure to cre­
ate backup copies of the original MIDI tracks in case you want to restore the original velocities of your 
performances.
Volume automation can be a solid tool to improve MIDI performances of acoustic samples and syn­
thesized sounds. The main problem with sequencing acoustic instruments such as string instruments, 
woodwinds, and brass consists in reproducing a realistic attack and release for different passages and 
FIGURE 2.25
Graphic editing of automation data in PT.

96
CHAPTER 2  Basic Sequencing Techniques
dynamics. In slow passages and long sustained sections I recommend using volume automation to 
achieve a much more realistic reproduction of the string instruments’ dynamics.
Listen to Example 2.10 on the website. It features a string section played without the use of automa­
tion. As you can hear, the parts sound unrealistic and choppy. If you use a bit of volume automation at 
the beginning and end of each sustained note to fade in and out each bow movement slightly, you will 
achieve a much more realistic effect. This is because string instruments, when played with the bow, have 
the natural tendency to slowly fade in at the beginning of the bow movement before reaching full volume 
when the string is completely set in motion and reaches full amplitude.
Listen to Example 2.11 on the website for a more realistic string effect. You can add the automation 
fade-in and fade-out for each sustained note either while playing or graphically after you record the part. 
If available, I recommend using a fader from your keyboard controller (digital mixing board or control 
surface), assigned to CC 7, to record the volume automation while performing the part. It will give you a 
better idea of when and how to use the volume to increase the realism of the sampled string section.
The same idea can be applied to wind instruments. This is particularly true for sections in which you 
alternate fast passages with slow and sustained notes. The use of the regular attack and release param­
eters on your synthesizer would not be able to accommodate both playing styles (fast attack and release 
for staccato parts, and slow attack and release for slow passages). The best option is to keep a short 
attack and release and to use automation for smoothing the beginning and end of sustained notes.
Listen to and compare Examples 2.12 and 2.13 on the website. Notice how the oboe part sounds 
much more realistic in the second example, where the beginning and end of the sustained notes are 
edited with volume automation.
The automation of volume can also be used to simulate dynamic compression on both MIDI and audio 
tracks. If you feel that the performance you just recorded has abrupt changes in dynamics that seldom occur in 
the track, you could adjust the volume to correct and smooth the highest peaks using a compressor. Listen to 
Examples 2.14 and 2.15 on the website. The first one is a guitar track without compression that has big jumps 
in volume. The problem was fixed with volume automation in order to have a smoother overall performance.
2.14.2  Pan Automation
The automation of the pan parameter can be used to create interesting stereo effects to help widen the 
stereo image of synthesized pads, leads, or acoustic instruments for both MIDI and audio tracks. For 
example, by choosing a predefined shape, such as triangle, sine, or square, and using the pencil tool to 
draw pan automation, you can create the effect of a constantly moving instrument on the stereo image. 
This will help to add stereo image depth, clarity in the final mix, and interest for the listener.
Use a small grid (16th note or 32nd note) for fast and rhythmic parts such as synth arpeggios 
(Examples 2.16 and 2.17 on the website), or use slower resolution (whole note or half note) to open the 
stereo image of sustained pads and sustained parts in general (Examples 2.18 and 2.19 on the website).
Another creative use of the pan is to apply it to cymbal swells, mark tree passages, or harp glissandos 
and guitar arpeggios, where the pan of the track is automated from one side to the other following the 
glissando. This technique adds a dramatic effect to the part, shifting the attention of the listener from the 
orchestral foreground (melody and harmony) to the background (percussion and sound effects).
Listen to Examples 2.20, 2.21, and 2.22 on the website to compare pan automation techniques 
applied to percussion and harp parts.
If used on acoustic harmonic instruments such as guitars, harps, and keyboards, pan automation can 
help to increase the stereo image and the depth of their sounds. In this case you want to use a very mild 
and subtle automation in conjunction with a slow rhythmic setting in order to simulate the movement of 
the performer in front of a stereo microphone.

97
2.15  Exporting and Importing your Work: Standard MIDI Files
Listen to Examples 2.23 and 2.24 on the website and compare a guitar track without and with pan 
automation.
2.14.3  Mute Automation
Even though most of the effects that can be achieved with the automation of the mute parameter can be 
accomplished with regular editing techniques, I usually like to experiment with this type of automation, 
especially on synthesized pads and sustained parts, to create rhythmic effects that would be too compli­
cated and tedious to achieve through regular editing. You can, for example, model interesting rhythmic 
patterns on a sustained pad by simply selecting a random pattern curve and by using the pencil tool to 
set mutes on the track. It will bring the sound in and out at randomly selected intervals (Example 2.25 
on the website). Or you could use a more regular pattern, such as the square curve, and a medium-to-fast 
grid and insert mutes every other subdivision to create an arpeggio effect (Example 2.26 on the website). 
By applying all three types of automation together (volume, pan, and mute) you can achieve some very 
interesting and original effects, such as the one in Example 2.27 on the website.
2.15  EXPORTING AND IMPORTING YOUR WORK: STANDARD MIDI FILES
Being able to exchange MIDI data with other DAWs is an important feature for any contemporary com­
poser and producer. Often you will face situations were you will have to bring your sequence to a differ­
ent studio that uses a different DAW, or you will have to import a basic score done in a scoring software 
(like Finale or Sibelius) into your DAW for further sequencing. Since the MIDI protocol is based on 
a standard communication system it is relatively easy to exchange MIDI files, data, and information 
between different platforms. MIDI data are universal, meaning that any MIDI message is the same no 
matter in which DAW it is recorded or played back. For this reason the only necessary step you will have 
to take when importing/exporting a MIDI sequence is to make sure that you save it in a file format that is 
recognized by any DAW (and not in a proprietary file format). The format in question is called Standard 
MIDI File (SMF) and it is a universal format that is recognized by most MIDI devices. There are two 
types of SMF: type 0 and type 1. Type 0 combines all the MIDI data from all your MIDI tracks in one 
single track, maintained through the MIDI channel of each event. Type 1 SMF preserves the single MIDI 
tracks and therefore is more advanced than type 0. Usually, when available, you want to use type 1 since 
it is more flexible when moving complex MIDI sequences. All four DAWs can easily import and export 
SMF. Let’s take a look at each procedure.
In DP to export your sequence in SMF select File    Save As and under the Format tab choose 
“Standard MIDI File”. Click Save and then select between SMF type 0 and 1 (you can select also to save 
only the tempo map if you prefer, by selecting “Type 0 tempo/meter map only”). To import an SMF in 
DP simply drag the MIDI file from its location into the track list window of DP.
To export your sequence as SMF in LP first select the tracks/object you would like to save as SMF, 
then select File  Export  Selection As MIDI File. After naming the new MIDI file and pressing the 
Save button your sequence will be exported as SMF. To import an SMF in LP you can either drag the 
file into the Arrange window or select File  Import and select the SMF you want to add to your current 
sequence. Using these two techniques it will not import the tempo and meter tracks but only the MIDI 
data present on MIDI tracks. If you want to load also the tempo/meter information I advise selecting 
File  Open, which will create a brand new Arrange window with both MIDI data and tempo/meter 
information.

98
CHAPTER 2  Basic Sequencing Techniques
Exporting an SMF from CU requires you to select File  Export  MIDI File. Give a name to the new 
file and click the Save button. In the Export Options window you can select several parameters such as the 
tick resolution and the file type (1 is the default). To import an SMF into CU simply drag the file into the 
main window; both tempo and meter maps will be automatically imported along with all the MIDI data.
To export a sequence from PT in SMF select File  Export  MIDI. In the Export MIDI Settings 
window you can choose the type of file to export (1 or 0) and whether you want to apply the real-time 
effects applied to the MIDI regions. Importing an SMF into PT is as straightforward as dragging the 
MIDI file into the Edit window. PT gives options regarding importing tempo and meter maps in the 
MIDI Import Options window that is presented to you after dragging the SMF to the Edit window.
2.16  SUMMARY
The sequencer is the central hub of your MIDI and audio network. It allows you to record, play back, 
edit, and store MIDI and audio data. To achieve the best results in a problem-free and smooth working 
environment, you have to organize your sessions and projects in the right way. The use of templates can 
improve considerably the organization and the speed of your sessions. I recommend having a series of 
premade templates for different ensembles and project situations that you can recall when necessary. 
After organizing your MIDI and audio tracks, make sure all the connections in your studio work prop­
erly. Always do a “preflight” check before every session, especially if you expect to work with a client 
or a colleague. Check that the MIDI tracks are working and that every device in your studio is receiving 
and sending MIDI data. Do the same for your audio tracks. Try to record and play back a few seconds 
of audio to test the audio connections. Make sure before starting your project that you have a clear idea 
where all the session files are located on your HD, that you have enough space to record, and that you 
have a quick backup plan to use during session downtimes. Before starting recording, also make sure 
that the metronome is set up right, that it is played by the internal audio engine of your DAW, and that 
it is set at the tempo you feel most comfortable. Remember that the click track is your best friend in a 
sequencing environment, not your worst enemy, so play with it and do not fight it.
When recording MIDI or audio tracks, make use of multiple takes so you have as much material as 
possible to edit later. Record different passages, choose the best takes, and compile the final track from 
several passages.
When editing a part, use the edit window that best suits what you are trying to achieve. Each editor 
has pros and cons. Use the one you feel most comfortable with and that you think is going to help you 
to complete the editing task quickly and easily. Quantization can vastly improve the final result of your 
projects if it is used in the right way. Remember that quantization is not a magic tool; it can only fix 
what was already good enough to be interpreted in the right way by the sequencer. Choosing the right 
quantization value is crucial: it should be equal to or smaller than the smallest rhythmic subdivision 
present in the region you are quantizing.
The addition of audio loops and audio parts in general can greatly improve the overall feel of your 
project. When dealing with rhythmic loops, make sure they match the tempo of your sequence. After 
importing the loop, if you do not know its tempo you can use the following formula to calculate the tempo, 
in beats per minute: (NB  SF  60)/NS, where NB is the number of beats of the loop, SF is the sampling 
frequency in hertz, and NS is the number of samples of the loop. You can also use specific shortcuts avail­
able on each sequencer to make the tempo of the loop match the tempo of the sequence, or vice versa.
The automation of parameters such as volume, pan, and mute is a fantastic tool for improving and 
expanding the boundaries of your projects. An automated mix can be static (with the insertion of snap­
shots of single data) or dynamic (with the insertion and recording of continuous movements and data). 

99
2.17  Exercises
The latter can be achieved in three different ways: by recording automation data through a control surface, 
though the movement of the mouse, or through the insertion of data in one of the editors. Once the data 
are inserted, they can be edited either graphically or through the list editor. The same principles for the 
editing of MIDI data are applicable to automation data. Practical applications of automation include fades 
(in and out), crescendos and decrescendos, simulation of attack and release of acoustic instruments such 
as strings, woodwind and brass, and more creative applications, such as creative panning and mute.
In this chapter you learned the basic methods that constitute the building blocks for more advanced 
sequencing techniques. If you think what you learned was exciting, wait until you read the next few 
chapters, where we will experience more detailed and advanced sequencing tools.
2.17  EXERCISES
Exercise 2.1
Set up a sequence with the following features:
a.	 Fourteen MIDI tracks
b.	 Five audio tracks
c.	 Instrumentation for MIDI tracks: violins, violas, cellos, basses, drums (with one track each for bass drum, 
snare drum, hi-hat, toms, and cymbals), electric bass, woodwind, synthesizer pad, rhythmic guitar, and 
tenor saxophone
d.	 Tempo: 110 BPM.
Exercise 2.2
a.	 Using the template created in Exercise 2.1, sequence 16 bars using at least 10 MIDI tracks (at least three 
must be drums). Make sure to use the multiple takes feature of your sequencer.
b.	 Quantize each track using the right quantization value.
c.	 Copy and paste material from the first 16 bars in a creative way in order to have a total of 64 bars.
Exercise 2.3
Find the tempo, in beats per minute, of the loops listed in Table 2.7 according to their sampling frequency 
(SF), number of samples (NS), and number of beats (NB).
Table 2.7  List of Loops for Exercise 2.3.
SF
NS
NB
Tempo
Loop 1
44.1 kHz
100,340
  8
Loop 2
96 kHz
400,567
16
Loop 3
48 kHz
220,100
  8
Loop 4
44.1 kHz
150,320
  8
Note: 1 kHz  1000 Hz.

100
CHAPTER 2  Basic Sequencing Techniques
Exercise 2.4
Using the sequence created in Exercise 2.2, import at least two audio loops found on the companion website 
or from your own library. Find the tempo of each loop, and adapt their tempos to the tempo of your sequence 
using the time-stretch feature of your sequencer. Insert the loops in your project as you wish.
Exercise 2.5
Insert automation in order to have the following data:
a.	 Fade the strings in with a slow fade 4 bars long
b.	 Automate the pan for the synth pad track moving constantly from left to right, and vice versa, at a pace of 
16th notes throughout the entire 64 bars
c.	 Mute the bass from bars 17 to 32
d.	 Program a crescendo of one track of the drum kit using one of the techniques explained in this chapter.

101
Creative Sequencing Techniques for Music Production.
© 
, 2005 Andrea Pejrolo. Published by Elsevier Ltd. All rights reserved.
CHAPTER
2011
Intermediate Sequencing 
Techniques
3
3.1  INTRODUCTION
What you learned in the first two chapters has given you a solid background and a basic knowledge of 
sequencing. By now you should have a good idea about the potential offered by the tools you are using 
and how they can inspire your creativity and improve your productions. If what you learned in Chapter 2 
provided you with enough information and skills to start sequencing, in this chapter you will gain a deeper 
knowledge of more advanced sequencing techniques and tools, such as groove quantization options, MIDI 
channel layering, audio and MIDI track layering, editing techniques, and how to use and program tempo 
changes. In the second part of this chapter you will learn about the practical aspects of sequencer-based 
production and project management, such as backup and archiving techniques.
Before continuing with this chapter, make sure you are familiar with all the concepts presented in 
Chapter 2. Master the basic techniques introduced in that chapter, and use the exercises listed at the 
end of it to familiarize yourself with how a sequencer works and how to start a project with both MIDI 
and audio tracks. Since each chapter of this book builds on the previous ones, it is very important to be 
familiar with the material presented earlier. If you did your homework, then you are ready to continue 
discovering more advanced and exciting sequencing techniques. Let’s go!
3.2  GROOVE QUANTIZATION AND THE “HUMANIZE” FUNCTION
In Chapter 2, when I discussed the basic quantization options available in your sequencer, we learned 
that by quantizing your MIDI parts, the computer can check and correct the timing (rhythmic position) 
of the MIDI events you recorded by moving and aligning them according to a grid. The quantization 
value you select determines the grid setting. As you probably experienced by quantizing the parts in the 
sequences you recorded so far, this basic quantization technique seems to flatten out the original groove 
of your performance. Since every event is moved to a position determined by the quantization value, the 
original rhythmic freedom that was part of your performance is lost. This can lead to very stiff, cold, and 
impersonal parts and sequences. Until very recently, the terms sequencer and quantization were synony­
mous with mechanical, stiff, and drum-machine-like music. This preconception has for years kept many 
composers away from the MIDI environment. Unfortunately, it is really a misconception that sequencers 
can be used only for mechanical and loop-based composition. With the latest innovations introduced by 
most digital audio workstations (DAWs), the modern composer is able to achieve levels of smoothness, 
precision, and overall rhythmic feel that were impossible before. Gone are the days when sequencing 
meant boring and rigid rhythms. These improvements are possible mainly through the increased sophis­
tication of the quantization algorithms implemented in the sequencers. The options available to compos­
ers are countless and include quantization parameters such as swing percentage, sensitivity, strength, 

102
CHAPTER 3  Intermediate Sequencing Techniques
randomization, iterative quantization, groove quantization, and MIDI-to-audio quantization. Let’s ana­
lyze all the different quantization options provided by the four sequencers we are learning about [Pro 
Tools (PT), Digital Performer (DP), Cubase (CU), and Logic Pro (LP)], and examine their practical 
applications.
3.2.1  Quantization Filters
In Chapter 2, when applying quantization, it was assumed that all the notes and events in the selected 
region needed the same quantization settings in terms of particular notes/events and how they were 
going to be affected. This approach implies a very basic type of quantization. It assumes that all events 
need the same amount of quantization and that all notes need to be quantized. The idea of quantization 
filters can be used to gain more control over which parameters and which notes will be quantized, and 
how much the events will be quantized. The main idea is that instead of having only the option to fully 
quantize an event (also referred to as 100% quantization) or not quantize at all (0% quantization), we 
can create several variations between the two extreme settings (100% to 0%) that allow us to be more 
precise and to decide how much an event will be quantized. Here, 0% corresponds to no quantization 
and 100% corresponds to full, or straight, quantization. The same idea can be applied to which events 
will be quantized or to the amount of, say, swing that will be applied to the quantization (more on this 
later). Let’s learn these techniques.
One of the most basic quantization filters you can set up allows you to determine which parameters 
and events will be affected by the quantization action. You can choose to quantize MIDI or audio events, 
MIDI Control Changes (CCs), or any other events. By choosing one or more parameters you instruct the 
sequencer to disregard all the others that were not included in the list. The quantization therefore will 
be applied only to those events selected. You can also choose to quantize the attack point of the MIDI 
note (which is usually the case) or the release point (meaning the end of the note). In the latter case the 
computer, instead of aligning the beginning of the note to the grid, will align the end of it (this option is 
rarely used in a practical sequencing situation). The quantization filter options vary from sequencer to 
sequencer.
In DP you access these settings by opening the quantization menu located in the Region menu 
(Figure 3.1) and clicking on the events you want to be affected by the quantization. To see more options 
click on the “What to quantize” drop-down menu. Keep in mind that a similar function is also available for 
MIDI tracks when using quantization as an insert in the channel strip of a MIDI track. In this case you will 
have the options to select any type of MIDI data to be quantized (CCs, Pitch Bend, etc.).
To access the quantization window in PT navigate to Event  Event Operation  Quantize. In PT 
you can quantize also audio transients through the Beat Detective window (more on this later in the 
book).
Although a generic quantization filter is not directly accessible from LP, you have two other options: 
you can select the events you want to quantize manually from any editing window, or you can use the 
very sophisticated “Transform editor”, which allows you to choose any filter events based on several 
criteria assignable to different parameters. Keep in mind that these filters can be used for many other 
situations. This editor can be opened by selecting “Transform” from the Window menu. The window 
is split in two sections. The top part sets the conditions, meaning which events will be affected by the 
operations, which are set in the lower part of the window. For example, you could set the conditions and 
operations as “apply 16th note quantization only to notes between bar 1 and bar 5 on MIDI channel 1” 
(Figure 3.2). This allows you to be very specific in terms of which events will be quantized and how they 
will be affected by the quantization. I particularly like this option since it allows you to quantize any 
type of MIDI data (controllers, Pitch Bend, etc.).

103
3.2  Groove Quantization and the “Humanize” Function
In CU you can set the quantization filters in several ways. If you select the option “Advanced 
Quantize” from the MIDI menu, then you can choose to quantize either the end of the notes (“Quantize 
Ends”) or their lengths (“Quantize Lengths”), which means that lengths of all the notes selected will be 
readjusted according to the quantization value selected. Remember that the quantization value in CU 
is set at the top right of the Arrange window using the “Quantize-Type” drop-down menu. The “Undo 
Quantize” function allows you to restore the original timing of the performance you recorded no mat­
ter how many quantizations you applied to the part. Use “Freeze Quantize” to make the quantization 
permanent for the selected part. The “Part to Groove” option allows you to create custom quantization 
FIGURE 3.1
Quantization parameters and filters in DP.
FIGURE 3.2
Transform window in LP.

104
CHAPTER 3  Intermediate Sequencing Techniques
templates (more on this in Chapter 4). Other, more advanced quantization parameters and filters are 
available from the “Quantize Setup” submenu accessible from the MIDI menu, which is discussed later 
in the chapter.
One of the most important features of modern quantization algorithms is the ability to choose the 
events you will quantize based not only on their type (e.g., notes, CCs, audio sound bites), but also on 
their position relative to the grid. This allows you to preserve the original groove of your performance 
without compromising its tightness to the click track. In other words, by controlling the sensitivity of the 
quantization algorithm, you can choose which events will be quantized and which will be left unquan­
tized, based on their position and not on their type.
In Figure 3.1, the sensitivity parameter in DP ranges from 0% to 100%. With a setting of 0% no 
events will be quantized, while with a value of 100% all the events will be affected. Any value in 
between will allow you to extend or reduce the area around each grid point influenced by the quantiza­
tion action. With a setting of 100% or with the regular quantization options, each grid point has an area 
of influence (a sort of “magnetized” area) that extends 50% before and 50% after (a total of 100%) the 
point itself. Events that fall in these two areas will be quantized and moved to the closest grid point. By 
reducing the sensitivity, you reduce the influence area controlled by the grid points. Therefore, if you 
choose a sensitivity of 50%, each point will attract only notes that are 25% ahead of or 25% behind (a 
total of 50%) the grid points. This setting is used mainly to clean up the events around the grid point and 
leave the natural rhythmic feel of the other notes. Use positive values of sensitivity very carefully since 
usually the final result will be a part that will sound sloppier than the original. This is due to the fact that 
while the notes that were closer to the grid (and therefore the ones that were giving a nice human feel 
to the part) are quantized, the other notes, the ones that were farther away from the grid and therefore 
sounded sloppier, are left unquantized. If you choose a negative value for the sensitivity parameter, you 
will achieve the opposite effect: only the events that were played farther from the grid points will be 
quantized, leaving the ones that were fairly close to the click in their original position (Figure 3.3). This 
setting is perfect to fix the most obvious mistakes but leave the overall natural feel of your performance 
intact. On a practical level I recommend using a sensitivity value between 50% and 80% to fix major 
rhythmic mistakes but keep the overall feel and groove of your performance.
Listen to Examples 3.1, 3.2, and 3.3 on the website to compare different sensitivity settings.
Another parameter you can use to improve the quantization of your parts is the strength option. 
While the sensitivity parameter has an impact on which events will be affected by the quantization, the 
strength allows you to control how much the events will be quantized (Figure 3.4).
By choosing a value of 100%, the events will be moved all the way to the closest grid point. If you 
choose a value of 0%, their original position will not be changed. If you choose a 50% value, the events 
will be moved halfway between their original position and the closest grid point. This option gives you 
great control over the stiffness of your quantization. While with a 100% strength (usually the default), 
your parts will sound very rigid and mechanical, choosing a value between 50% and 80% will help to 
maintain the original smoothness of the parts and at the same time correct the major mistakes.
Listen to Examples 3.4–3.7 on the website to hear the difference between different amounts of 
strength quantization. A third parameter, called randomize, allows you to place the events randomly 
inside the grid area of each point. If you keep its percentage under 50%, it can generate enough random 
data to simulate a more loose performance. Sometimes this function is also referred to as humanize. I 
find this option pretty useless on a practical level since the random nature of the algorithm doesn’t match 
any real performance situation. You can experiment with it, though, and see if something useful ran­
domly comes up.
In DP and PT these settings are laid out exactly as explained in the preceding paragraph (Figure 3.1). In 
PT the sensitivity parameter (accessible from Event  Event Operations  Quantize), instead of having a 

105
3.2  Groove Quantization and the “Humanize” Function
positive and a negative value, is set using the “include within” option (same as positive sensitivity values) 
and the “exclude within” option (same as negative sensitivity values).
In CU you can control the sensitivity of the quantization by selecting the “Quantize setup” in the 
MIDI menu (Figure 3.5). The parameter called “Magnetic Area” controls the positive values of sensitiv­
ity, while the parameter labeled “Non Quantize” controls the negative sensitivity.
The strength of the quantization is set through the parameter called “Iterative Strength”. The iterative 
quantization (accessible from the MIDI menu) can be repeated several times in a row to move the events 
closer and closer to the grid points each time by the amount specified in the “Iterative Strength” field. 
This technique allows you to quantize your part slightly more each time you apply it, giving you very 
precise control over the rhythmic feel of your performance.
In LP the additional quantization features are accessible from the “Advanced Quantization” tab of the 
“Inspector” window (found on the “left” of the Arrange window). The Q-Range has the same function as 
sensitivity. The value, instead of being expressed in positive or negative percentages, is inserted in ticks 
FIGURE 3.3
Quantization parameters: sensitivity.

106
CHAPTER 3  Intermediate Sequencing Techniques
FIGURE 3.4
Quantization parameters: strength.
FIGURE 3.5
Quantize setup in CU.

107
3.2  Groove Quantization and the “Humanize” Function
(subdivisions of a beat). If the Q-Range is set to positive values, the quantization will affect only events 
for which the distance in ticks is smaller than the value inserted (same as having a positive sensitivity 
percentage). If the Q-Range is set to negative values, the quantization will affect only events for which 
distance in ticks is greater than the value inserted (same as having a negative sensitivity percentage). The 
Q-Strength parameter gives you the ability to control the strength of the quantization with a range of 
0–100%.
3.2.2  Swing Quantization
One of the most controversial uses of a sequencer in the past has been the production of genres that 
don’t require a straight-note feel, such as jazz, hip-hop, and R&B. The ability to reproduce a convincing 
and realistic swing feel has been (and in a certain way still is) one of the most challenging tasks when 
sequencing. This is due mainly to the intrinsic feature of the swing feel, which is based on a constant 
and subtle variation of the rhythmic relationship between the notes in every beat. In fact, while straight 
quantization can be applied to different styles and genres without affecting the overall groove too much, 
when it comes to a swing feel there are almost infinite possibilities regarding how the notes can occupy 
the space inside a single beat.
Let’s take 8th notes as an example. Straight 8th notes are represented in notation as in Figure 3.6(a). 
When you quantize without swing, this is the rhythmic feel you get. The 8th note swing feel has a much 
rounder and smoother groove; this is represented in the notation of Figure 3.6(b).
Listen to Example 3.8 on the website and compare the difference between the two types of rhythmic 
feel. While these two rhythmic subdivisions can easily be recreated by using, respectively, straight 8th 
note quantization (Figure 3.6a) and 8th note triplet quantization (Figure 3.6b), through the use of the 
swing parameter available in the quantization window of your sequencer you can create any variation 
between these two extremes.
In CU, DP, and PT with the swing parameter set to 0% you will get the same result as using a straight 
quantization; with the swing parameter set to 100% you will get the same rhythmic feel as an 8th note 
triplet (in DP values between 100% and 300% move the notes closer to the next beat). What is interest­
ing, though, is that you can now choose any value in between to control how much your computer can 
swing! With values below 50% you will get a slightly looser feel than with straight quantization, provid­
ing a more relaxed and slightly rounder rhythmic feel to the quantized part.
If you use values between 70% and 120% you can achieve a more natural swing feel, avoiding the 
unnatural and sometimes mechanical 8th note triplet feel (website Examples 3.9–3.11).
In LP you can apply different swing settings by using the swing presets available in the Parameters 
Box located on the left of the Arrange window. Here, in addition to the straight quantization options, you 
can find values with the addition of swing. Their amount varies from light (i.e., “Swing A”) to extreme 
(i.e., “Swing F”). An additional swing percentage parameter can be selected from the Object Inspector 
to the right of the Arrange window. Using the Q-Swing parameter you can fine-tune the amount of swing 
applied to the selected sequence: with a 50% setting the part is quantized with no swing, with values 
FIGURE 3.6
(a) Straight 8th notes; (b) swung 8th notes.

108
CHAPTER 3  Intermediate Sequencing Techniques
above 50% every second note will be pushed closer to the next grid point, while with values below 50% 
every second note will be pushed closer to the previous grid point. In Figure 3.7 you can see a graphic 
representation of the impact that the swing settings have on an 8th note pattern.
To improve the swing groove of your parts, you can use different percentages in different sections 
of your projects. This helps to recreate a more human and variable rhythmic feel. Try to vary the swing 
parameter values by 10% among different sections of your project. This creates a variable swing feel 
that will improve the overall groove of your parts. The swing parameter can also be used to improve the 
rhythmic interaction between tracks. By quantizing different tracks with slightly different swing values, 
you can create a more coherent and realistic virtual band. The final result will be just as if every musi­
cian in your studio had his or her own rhythmic personality and groove.
The interaction between all the different grooves and swing settings will generate a more convincing 
interplay effect among the tracks (website Examples 3.12 and 3.13).
3.2.3  Groove Quantization
So far we have learned how to improve the basic quantization of your parts through the use of filters 
such as sensitivity, strength, and swing. But what if you want to recreate a much more realistic and 
cohesive groove based on what a real musician would be capable of? The answer is a function called 
groove quantize, which can be found in some of the top sequencers. The principle behind groove quan­
tize is simple: starting from templates provided with your sequencer or that you can buy as expansion 
packs, you can apply several styles, such as laid back, push forward, or shuffles, to your parts. These 
grooves were generated by extracting timing and accents information from real musicians’ live per­
formances. The grooves control primarily the timing, velocity, and lengths of the notes in the MIDI 
tracks or parts you are quantizing. According to the template you choose, the recorded MIDI events 
will be shaped to give you a more realistic rhythmic feel. Keep in mind that this type of quantization 
FIGURE 3.7
Different swing-feel quantization settings in the CU graphic editor.

109
3.2  Groove Quantization and the “Humanize” Function
should be used not to correct rhythmic mistakes but to create more realistic and natural-sounding per­
formances. Therefore, I recommend quantizing your part first with the regular techniques explained 
earlier and then applying groove quantization in a second pass to loosen up the stiffness of straight 
quantization. It is important to underline how these grooves do not contain any MIDI events per se; 
they are not loops, they simply contain morphological data that can be applied to preexisting MIDI data 
you have sequenced.
All four main sequencers allow you to apply predefined groove templates to MIDI parts. Some of 
them go even further by giving you the ability to create your grooves from audio or MIDI parts.
DP is bundled with some DNA Groove Templates (created by Canadian company Numerical 
Sounds). The templates can be accessed from the “Region” menu and the “Groove Quantize” submenu. 
In the Groove Quantize window you can navigate through all the grooves available by going into the 
DNA Grooves folder and choosing one of the general groove categories (e.g., “Pushing”, “Laid Back”, 
“Shuffle”) by double-clicking on it. Once you have specified the overall groove category you will be pre­
sented with a list of the variations available for that specific group (e.g., Medium Shuffle, Soft Shuffle, 
Hard Shuffle). You can alter the parameters of each groove by using the slider on the right that controls 
the percentage of groove application for timing, velocity, and duration (Figure 3.8).
If you increase the percentage of each parameter, you will amplify the impact that the parameter will 
have on the selected groove. For example, choosing 100% timing means that the timing of your part will 
be highly affected by the predefined groove you have chosen. If you select a small percentage, then your 
part will be only marginally affected by the groove quantization. Usually I recommend using only a mild 
setting (50% and lower). High values can alter the timing and the velocity to an extreme, making the 
quantization process useless. Use the “Beat Division” drop-down window to choose the basic rhythmic 
groove of the part. The same rules listed for the quantization value can be applied in this case.
Listen to audio Examples 3.14–3.17 on the website to hear the difference between straight quantiza­
tion and groove quantization.
FIGURE 3.8
Groove Quantize window in DP.

110
CHAPTER 3  Intermediate Sequencing Techniques
In PT the parameters are very similar to those found in DP. To access the Groove Quantization func­
tion in PT navigate to Event  Event Operations  Quantize. Under the Quantization Grid drop-down 
menu select the type of groove you want to apply to your region. As a default you have four main cat­
egories to choose from: Cubase Style Grooves, Feel Injector Templates, Logic Style Grooves, and MPC 
Style Grooves. Each category has different subgrooves available. I find the Feel Injector Templates to 
be the most effective in PT and I highly recommend them. After electing the desired grove you will be 
presented with the familiar slider that allows you to fine-tune how the groove is applied to the region 
(Timing, Duration, and Velocity), as we saw in DP.
LP can import and use predefined DNA groove templates created by companies such as Numerical 
Sounds. To apply a groove template, you will first have to import the grooves into LP (or create them). 
To import a DNA groove go to Files    Import and navigate to the folder where the DNA grooves 
are located. The imported groove will be added as a MIDI region in the Arrange window. To add this 
new groove to the Quantize pop-up menu, select a groove template and then choose “Make Groove 
Template” from the Quantize pop-up menu of the Region Parameter box. Your new groove templates 
will be listed in the regular quantization list and can be used for any regions or sequence. You can also 
create your own templates based on preexisting audio parts (more on this in Chapter 4).
Since CU does not come bundled with any DNA grooves you will have to create your own first. In 
CU you can apply grooves generated from either MIDI or audio tracks. First, you have to have MIDI or 
audio material suitable for this task. You can have MIDI files generated by drummers through the use of 
a drum pad or drum controller, or simply create your own or import a MIDI file from preexisting tem­
plates. To generate a groove from a MIDI part, first make sure to rename the part with a suitable name, 
since the groove will automatically take the same name (to rename a part select it and then click on its 
name field in the info line at the top of the Arrange window), then select the part with the groove you 
want to extract, and choose MIDI  Advanced Quantize  Part to Groove. At this point the groove has 
been extracted and it is available for you to use. If you want to apply a groove to a MIDI track, select 
MIDI  Quantize Setup, choose the groove you want to use from the drop-down menu called “Presets”, 
and click on the “Apply” button.
In addition to the predefined templates that either are bundled with your sequencer or can be bought 
separately, you can create your own templates from audio files. This can yield an incredible number of 
options. Imagine, for example, importing a website track from your favorite album featuring “Philly” Joe 
Jones or Dave Wackel, capturing their groove, and applying it to your own parts. In some DAWs this is 
possible, and we will analyze this in the next chapter, where you will discover more advanced sequenc­
ing techniques.
3.3  LAYERING OF MIDI TRACKS
Now that you have mastered the quantization techniques and feel comfortable organizing and moving 
around your session, we are going to focus on some new procedures and techniques to improve the qual­
ity of your productions in terms of sound palette. In Chapter 2 we learned how to assign a MIDI track to 
a certain device and MIDI channel. While sometimes you are going to use a single patch for each track, 
very often you will find yourself looking for a new sonority or for a richer texture that is not directly 
available to you from a single device or channel. In this case you can use a technique called track lay­
ering (sometimes referred to as patch layering). Through this technique you can output a single MIDI 
track to several devices, soft synths, and/or MIDI channel at the same time. Its advantage is that you can 
create complex patches and sounds without altering and reprogramming the actual MIDI synthesizer or 
sound module. Instead you use several MIDI channels that receive the MIDI data from a single track of 

111
3.3  Layering of MIDI Tracks
your sequencer. For example, if you want to create a supersynth sound that is a combination of a total 
of four patches, one from your Roland XV-3080 module, one from your trustworthy Motif MIDI syn­
thesizer, and two from a software synthesizer, you can assign the output of one of your sequencer tracks 
to two available MIDI channels on each device. Although you could achieve the same result by copying 
the MIDI data from one track to other tracks and assigning each of them to a different device and sound 
module, that would result in two major problems: first, if your sequencer doesn’t allow you to multi­
record (meaning to record two or more tracks simultaneously) when you record your part, you will be 
able to hear only one sound of your layer at a time; and second, when you edit the tracks, you will have 
to repeat the edits on each track. With track layering, in contrast, you can simply assign the output of 
one track to multiple devices and MIDI channel, and therefore eliminate these problems.
Almost every patch of your synth palette will sound richer and fresher if layered with other patches 
from other devices. Here are some suggestions and recommendations for a creative use of the patch lay­
ering technique. When dealing with acoustic sounds such as stringed instruments (strings, guitars, acous­
tic and electric basses), try to layer two or more patches from different sources. Try to mix and match 
between different textures. For example, if you have a nice and edgy electric bass that is missing some 
depth in the lower range, try to layer it with a deep acoustic bass. The result will be a completely new 
patch that has the edginess of the electric bass patch but the low end of the acoustic bass patch. Another 
practical application of patch layering is the combination of sampled and synthesized sounds. A good 
way to make your string patches more robust and natural sounding is to layer a sampled string patch 
with a synth string patch. The former will give you the edginess and realistic feel, while the latter will 
provide adequate lower-end support. Of course, you can apply the same technique to constructing and 
building your own synthesized patches by combining as many layers as you want. This works particu­
larly well in the case of synth pads, where the nature and texture of several individual patches will enrich 
the final quality of the sound. Let’s now take a look at how your DAW allows you to create layers.
Constructing layers in DP is very simple. In the Output column in the track list window, instead of 
selecting a single device and MIDI channel, select “New Device Group”. From the new window that 
appears (Figure 3.9) you can select any device and MIDI channel available in your studio.
From now on, any MIDI data played or recorded on this track will be output to all the devices and 
the MIDI channel assigned to its device group. To toggle the checkbox view shown in Figure 3.9, use 
the expand/compress icon located below the small keyboard sign next to the group name. To rename a 
group, simply Option-click on the group name from the Device Groups window. If you need to edit a 
group from the track list window, Option-click on the group name of the track.
In LP you can use two different techniques to create MIDI track layers by using the “Environment 
Editor”. The fastest and simplest one consists of record-enabling several tracks at the same time by 
clicking on the record button of each track. When you start recording the MIDI data will be recorded 
on one track while on the other tracks you will end up with alias objects. These objects do not contain 
actual MIDI data but are simply aliases that point to the original recorded ones. The advantage of this 
approach is that you can edit only the data of the “master” track and have the other tracks that are part 
of the layer follow accordingly. The alias objects are easily identifiable because their names are in italic. 
The other option is a bit more complicated but it is also more flexible: Open the “Environment” window 
(located inside the Windows menu) and create a new instrument, either single or multi, by selecting the 
“Instrument” option from the New menu. Assign the output of the instrument to different devices and 
a different channel by Option-clicking on the connection cable of the instrument (the small arrow that 
points toward the outside from the newly created instrument) or by clicking and dragging the output 
cable to one of the software synthesizers available in your setup, as shown in Figure 3.10.
From the hierarchical multi-menu, select the device and channel using the list of available devices 
in your studio. Repeat the same procedure for all the instruments you want to layer. The newly created 

112
CHAPTER 3  Intermediate Sequencing Techniques
“instrument” will be added to the list of available MIDI outputs. From the Arrange window, select a 
MIDI track (or create a new one) and assign its output to the new instrument. All the data recorded 
on this track will be output to the cables and MIDI channel you specified in the Environment window. 
These outputs can be changed at any time to experiment with different layers.
FIGURE 3.9
Device Group in DP.
FIGURE 3.10
Patch layering in LP.

113
3.3  Layering of MIDI Tracks
In PT you can assign the MIDI output of a track to multiple devices and MIDI channels by simply 
Control-clicking on the device and channel assignment drop-down menu of an empty MIDI track. You 
will be presented with all the available software and hardware synthesizers present in your studio. Select 
all the ones that you want to receive MIDI data from the selected MIDI track (Figure 3.11).
To achieve a higher control over the blend of your layers it is essential to be able to control the vol­
ume of every single MIDI channel that is part of the combined sound. Let’s say you just created a layer 
where you combined a synth pad from a Yamaha Motif set on MIDI channel 1 and a synth bass from 
a Korg Triton set on MIDI channel 3. If you control the volume from the main track that is output to 
both devices and channels, you will send the same CC 7 to both tracks and therefore will not be able 
to control the individual volume of each patch. Instead, you will have to create two more MIDI tracks, 
each assigned to the individual device and channel (in our example, one will be assigned to channel 1 
of the Motif and the other to channel 3 of the Triton). These two new tracks will not be used to record 
actual MIDI notes, but they will serve as individual volume for each patch. In this way you will be able 
to control the blend of the two patches to create the ultimate layered sound. Make sure to label the tracks 
with the right names. For example, you could call the layered track “Fat Synth Pad” and the two vol­
ume tracks “Volume Fat Pad Motif” and “Volume Fat Pad Triton”. If your layer involves more than two 
tracks, you will need volume tracks for each additional layer.
FIGURE 3.11
Patch layering in PT.

114
CHAPTER 3  Intermediate Sequencing Techniques
This issue is irrelevant, of course, if you are using only soft synth to create your new layered patch, 
since each soft synth has already a dedicated channels strip with independent volumes, pans, and effects.
3.4  LAYERING OF MIDI AND AUDIO TRACKS
The techniques involving the use of layers to produce richer, newer, and fresher sonorities can be not 
only applied to MIDI tracks, but also used with MIDI and audio tracks together. The layering techniques 
involving the two types of tracks can be used in a variety of ways and situations to achieve very different 
results. One of the major applications of these techniques is the combination of live acoustic instruments 
(such as strings, woodwind, or brass) and synthesized or sampled sounds. One of my favorite layers 
consists of using a MIDI track to sequence a large string section and to overdub, using one or more 
audio tracks, real string solo instruments (such as a violin or a viola) doubling the same parts sequenced 
on the MIDI track (for an in-depth discussion of this and other topics related to MIDI orchestration I 
recommend reading my book Acoustic and MIDI Orchestration for the Contemporary Composer ISBN: 
9780240520216). While the sampled sound of the MIDI track provides the main body of the orchestra, 
the acoustic recording of the violin adds a realistic touch and a genuine edge to the performance. You 
can obtain even better results by overdubbing several acoustic instruments (e.g., three violins, three vio­
las, two cellos, and two basses) on different audio tracks. The same technique can be applied not only to 
strings but also to any acoustic instruments, especially large ensemble instruments.
Listen to website Examples 3.18 and 3.19 to hear the difference between a MIDI track sequenced 
using sampled sound only and one sequenced by layering MIDI sampled sounds and live acoustic 
strings. You can also use the same technique to create totally new sonorities by combining synth pads 
and acoustic instruments. In this case feel free to experiment as much as you can. Try to combine sharp 
and edgy live instruments with deep and round synth pads, or vice versa, or use bright synth leads with 
low and powerful basses. Any combination will bring new ideas and fresh sonorities to your productions.
In general, the use of some acoustic instruments in your projects will increase their overall quality. 
Sometimes, just adding one or two acoustic tracks will bring your MIDI production to life. No matter 
how simple or complicated your sequence is or how well programmed your MIDI tracks are, without 
an acoustic touch they will always have something missing. Most of the time, just adding an audio track 
with an acoustic guitar, an electric bass, or some live percussion will improve your project dramatically. 
Try to avoid loops for anything but percussion or drum parts (unless the style you are writing in calls for 
it); they tend to be too repetitive and most of the time will kill the spontaneity of the composition. Keep 
this as a mantra: always use at least one live instrument in your production!
3.5  ALTERNATIVE MIDI TRACK EDITING TECHNIQUES: THE DRUM EDITOR
While the editing techniques you learned in Chapter 2 (graphic, list, and score) are the most common 
ones, they are not the only ones available. Each sequencer features some peculiar ways of editing and 
inserting MIDI data that are worth mentioning and learning. One of my favorite unconventional edi­
tors is the drum editor available in DP, LP, and CU. The philosophy behind this editor is to recreate the 
working environment of a vintage drum machine, such as the Roland TR-808, and expand it to a higher 
functionality and flexibility. The drum editor can be used in several ways, depending on your needs and 
skills. It can be used to input rhythmic MIDI parts (such as, but not only, drum parts) or to clean up and 
edit MIDI parts that were sequenced from a controller but need some minor tweaking, or it can also be 
used to create arpeggios and repetitive melodic parts. The drum editor can be seen as a subsequencing 

115
3.5  Alternative MIDI Track Editing Techniques: The Drum Editor
environment for a selected track or for a series of tracks. This means that when you open the drum editor 
for a track, you will be presented with a series of subtracks, each corresponding to a single note or key 
of your keyboard. That is why I call it a subenvironment, because each MIDI track is split into several 
subtracks, each assigned to a different note. This configuration is particularly appealing to drum tracks 
because of their intrinsic nature of multiple sound-based sets. A drum patch is made up of several sounds 
and sonorities corresponding to several drums and percussive instruments. By choosing the drum edi­
tor to record or edit drum tracks, you have greater control over each individual piece of the drums and 
percussion set. You have not only individual mute buttons but also independent grid-quantization settings 
for each note/sound.
In the case of CU you can access the drum editor by selecting the “Open Drum Editor” option from 
the MIDI menu. After selecting a Drum Map (lower left corner) you can start assigning each subtrack 
to a different Note (both as Input and Output, MIDI channel and track) (Figure 3.12). Each subtrack 
can also be quantized individually. These settings can be stored in different “Drum Maps” that can be 
recalled at any time. The Drum Map feature is one of my favorites, since it allows you to create custom­
ized drum sets that share different sounds and instruments among different devices and MIDI channels. 
In CU you can insert MIDI events using the drum stick tool (the resolution and quantization grid is set 
by the general Quantize parameter selected at the top of the window) and delete them using the eraser 
tool (both accessible from the tool palette located in the top right corner of the Drum Editor window).
In DP’s drum editor (located in the Project menu), the layout and concepts are very similar to the ones 
found in CU (Figure 3.13). Here you cannot assign different subtracks to different devices or MIDI chan­
nels but only to different notes. In the top left part of the editor you can view several MIDI tracks at once, 
and each track can be assigned to different channels and devices in your studio. Keep in mind that each 
FIGURE 3.12
Drum Editor in CU.

116
CHAPTER 3  Intermediate Sequencing Techniques
subtrack needs to be assigned to a note, otherwise you will not be able to add/edit events for that subtrack. 
To assign a subtrack to a note, click on the “Pitch” field next to the playback button of that track.
As in CU, you can set separate quantization grids for each subtrack by selecting a subtrack with the 
mouse and changing the quantization settings for that particular subtrack in the lower left area of the edi­
tor. This option makes it very easy to create complex rhythmic parts with intricate rhythmic subdivisions. 
Use the pencil tool to insert or edit events in any subtrack (the View Resolution parameter located in the 
upper left corner sets the grid used to insert events). For each subtrack, you can also switch (through the 
display mode menu) between different views that allow you to see notes only, notes and velocity, velocity 
and duration, and a free setting that displays the notes not according to the grid but according to their real 
location (a sort of a miniature piano roll window). A great feature of the drum editor in DP is the use of 
the brush tool, accessible from the tool palette (Shift-O). With this tool selected you can paint a predefined 
pattern on any subtrack. You can choose the pattern from a list of several preinstalled templates, or you 
can define your own patterns. This feature is very convenient for quick drum part layouts. Even though 
the predefined patterns are so-so, they provide a good starting point for sequencing quick drum grooves.
In LP there is a similar editor that is much more powerful than a regular drum editor. The “Hyper 
Editor” (accessible from the Window menu) is an extremely flexible and multifaceted environment that 
you can customize to edit basically any parameters or MIDI event you wish. The default set allows you 
to edit common parameters such as volume, pan, modulation, and pressure. Another predefined set, 
called “GM Drum Kit”, transforms the Hyper Editor in a regular drum editor with all the features we 
saw earlier in the case of DP and CU. Switch to this mode using the drop-down menu located in the left 
side of the Hyper Editor, inside the Inspector window (Figure 3.14). You will see separate subtracks for 
each note of the keyboard, and for each note you can change the name, MIDI channel and device assign­
ment, quantization settings, etc. Use the pencil tool to insert an event and input its velocity according to 
the grid value chosen for that particular subtrack (the grid value can be changed using the “Grid” param­
eters located in the Parameters Box in the left part of the window).
FIGURE 3.13
Drum Editor in DP.

117
3.5  Alternative MIDI Track Editing Techniques: The Drum Editor
You can group instruments together by Control-clicking on two or more subtracks. This will set the 
group tracks not to play events at the same time, which is very useful when programming hi-hat parts, for 
example, where you do not want the closed and open hi-hats to be played at the same time. But where the 
Hyper Editor really shines is when you create your own sets. This feature can be used not only to create 
custom drum sets but also specifically designed control sets that allow you to program a series of CCs, 
events, and any other MIDI events you may need. You can choose the type of message assigned to a cer­
tain subtrack by selecting the “status” option in the Inspectors window to the left of the Hyper Editor.
Here are some practical suggestions and applications to consider when using the drum editor. If you 
are not familiar with playing the parts on a controller (keyboard or others), the drum editor provides a 
good starting point to sequence drums or monophonic parts such as simple bass lines or melodic lines. 
Not only is it more advanced than an old-fashioned step editor, but it provides more flexibility in terms 
of quantization and velocity programming. Start with simple and short patterns, get familiar with it, and 
then keep exploring more complicated rhythms. Try to avoid using a steady velocity parameter for each 
event you create.
If you do so, then your parts will sound very mechanical, no matter how hard you work on their 
quantization (website Example 3.20). Try to create variation in the velocity pattern instead, as a real 
drummer would do when playing a drum set or a percussion set (website Example 3.21). With this editor 
it is very easy to quickly change the velocity of several events by clicking and dragging across the screen 
with the pencil tool (or with the drum stick tool in CU).
I often like to use the drum editor to create or edit small variations in a drum pattern. After recording, 
quantizing, and groove quantizing the part, I open the drum editor and insert small variations here and 
there that would be too fast to play from a keyboard controller. For example, you can create some inter­
esting hi-hat parts by choosing a small resolution for the hi-hat subtrack (such as 32nd notes) and insert­
ing quick passages every so often to create contrast and variation.
FIGURE 3.14
Drum Editor in LP.

118
CHAPTER 3  Intermediate Sequencing Techniques
Listen to Example 3.22 on the website to get an idea about this concept.
As I mentioned before, you can also use the drum editor in creative ways such as arpeggio generator 
or groove creator. If instead of opening the drum editor for a track assigned to a drum patch you use it to 
edit a track assigned to a synth bass patch, you can create as complicated patterns as you would using a 
“matrix sequencer”.
Simply create a subtrack for each note that will be part of your arpeggio and then insert notes using 
the pencil tool to generate the new pattern (website Example 3.23). As you can see, the applications of 
this editor are several and it is really up to you to experiment with new ways to create music.
3.6  ALTERNATIVE MIDI CONTROLLERS
Keyboard controllers and MIDI keyboard synthesizers are the most common choice for inputting MIDI 
data into a sequencer. We are so used to sequencing with such devices that often the modern musician/
producer associates the composition process with a keyboard instrument. This can sometimes be limit­
ing, and it can narrow the overall spectrum of your projects. This is mainly due to the fact that many 
musicians are not keyboard players, and their keyboard performing skills can present a big obstacle for 
their sequencing projects. Fortunately, keyboard controllers are not the only way to input MIDI data into 
a sequencer. Here’s a description, analysis, and a few practical applications of some of the other MIDI 
controllers available on the market.
3.6.1  Guitar/Bass-to-MIDI Converters
One of the most widely used and widespread alternative MIDI controllers is the guitar-to-MIDI converter. 
This technology allows a regular acoustic or electric guitar to be connected to a MIDI system and to output 
MIDI messages and notes to any MIDI device, including a sequencer. This technology has been around for 
many years and constantly perfected by companies like Roland, Yamaha, and Axon/Terratec. Even though 
the models vary from manufacturer to manufacturer, all the major manufacturers use the same connector 
for their pickups (13 pin), making it very easy to buy one pickup and then choose the breakout box that 
best fits your needs. The principle on which this type of controller is based is simple: a pickup (divided into 
six segments, one for each string) is mounted next to the bridge of the guitar (Figure 3.15).
FIGURE 3.15
Guitar-to-MIDI converter pickup: Yamaha G1D.

119
3.6  Alternative MIDI Controllers
The pickup detects the frequency of the notes played on each single string by analyzing their 
cycles. This information is passed to a breakout unit that converts the frequencies in MIDI Note 
On and Note Off messages. From the unit, a regular MIDI OUT port sends the messages to the 
MIDI network. The pickup can also detect bending of the strings, which is translated in Pitch Bend 
messages.
Even though it takes a little practice to adjust your playing style to this type of converter, you 
will find yourself entering a whole new world of sonorities and possibilities. Imagine sequencing 
exotic synth lead parts using the natural legato and bending features of your guitar. Once again, the 
possibilities are really endless. After getting used to it, you start wondering how you could have 
sequenced without it. If you are an electric bass player, you are not out of luck. There are pickups 
for all sorts of stringed instruments: four, five, and six strings. Keep in mind, though, that because 
of the pitch tracking system of the device, the delay between the acoustic note and the MIDI note is 
inversely proportional to the frequency of the note. Therefore, for very low notes the delay is notice­
able, which might discourage some bass players. This is due to the fact that lower frequencies have a 
longer cycle and so the pickup has to wait longer before detecting a full cycle. A way around this is 
to use a piccolo bass, meaning a bass with strings that sound one octave higher. This is an alternative 
controller that is worth having in your studio, either as your main controller or as an occasional sub­
stitute for your keyboard.
Listen to Examples 3.24 and 3.25 on the website to compare between a part sequenced using a regu­
lar keyboard controller and one sequenced via a MIDI guitar controller.
As I mentioned earlier, practical applications of the guitar/bass-to-MIDI technology vary from sim­
ple sequencing of melodic or harmonic parts using the voice leading of six-stringed instruments to more 
creative applications, such as sequencing of drum parts using the feel offered by the guitar or bass. 
Other applications are more targeted to live performance, where, by assigning different strings to dif­
ferent MIDI channels and patches, you can create a one-man band and generate interesting layers and 
ensemble combinations. The same concept can also be used in a studio/sequencing situation. You can 
record-enable multiple tracks (maximum of six, one per string), all receiving on different MIDI chan­
nels, and assign their outputs to separate devices and MIDI channels. By pressing the record button you 
will record separate parts on separate tracks. You may assign a low sustained pad to the low E and A 
strings, use the D and G strings for sound effects or tenor voice counterpoint, and use the B and high E 
for the melody. The advantage of this approach is that you can capture a much better groove and live feel 
than from sequencing the tracks separately. Experiment with these techniques, and let your imagination 
run free. You will be surprised by the results.
3.6.2  MIDI Drums and Pads
MIDI drums and MIDI percussion pads were the precursors of a series of alternative controllers. 
They have been more successful and widespread than others, mainly because they started as mere 
triggers of Note On/Off messages without having to convert a definite pitch. This factor makes the 
MIDI drums the fastest alternative controller. The percussive MIDI controllers can vary, depending 
on their size, style, and features. Nowadays you can find four main types of percussive controllers: 
drums, pads, trigger-pads, and live triggers. Drum controllers recreate the entire drum kit, including 
bass drum and cymbals (Figure 3.16a). Each pad of the kit has several velocity-sensitive zones that 
can be assigned to different notes and MIDI channels. Roland has mastered this technique with the 
V-Drum series. There are several configurations available, ranging from a full studio kit to a portable 
live gig setup. MIDI pads are smaller controllers meant to complement and not substitute for an entire 
drum kit (Figure 3.16b).

120
CHAPTER 3  Intermediate Sequencing Techniques
MIDI pads usually feature four, six, or eight pads that can be played with either sticks or hands (to 
simulate percussive instruments such as bongos and congas). The MIDI channel and note assignment 
can be set for each pad individually. This type of controller is ideal for a small studio situation and for 
infrequent use on certain projects. Another creative way to program a percussion/drum controller is to 
use it as a MIDI pitched instrument by assigning different pads to different notes of a pitched instrument 
or synth patch. You can also assign certain pads to trigger loops or samples and use them as background 
layers for a one-man-band type of performance. Trigger pads (Figure 3.16c) are similar to MIDI pads 
but they are smaller and usually they are part of a MIDI keyboard controller (or a drum machine). They 
are played with fingers rather than sticks and provide a good alternative to regular keys when trying to 
program drum parts. Even though they are not nearly as effective as MIDI pads they are nevertheless 
more affordable.
A fourth type of device that takes advantage of audio-to-MIDI conversion is an audio-to-MIDI trig­
ger. This is a simpler device that does not include the actual pads to trigger MIDI messages but instead 
allows you to connect, to a regular audio input, any audio source (it works better with percussive sounds 
with fast transients). The audio signal is then used to trigger MIDI notes. The unit (usually a one-rack-
space unit) has several audio inputs on the back panel. You can connect a live instrument (such as an 
acoustic drum kit with individual microphones for each piece of the kit) or use prerecorded material 
output from a multitrack recorder. The audio signal on each channel will trigger different MIDI notes, 
depending on the settings you programmed. These devices (such as the old Alesis D4 and the newer 
DM5) are pretty inexpensive and fairly accurate.
The use of drum, percussion, and MIDI triggers in your studio will increase dramatically the way 
you sequence percussive parts.
Listen to website Examples 3.26 and 3.27 to hear the difference between a drum part sequenced 
using a keyboard controller and a similar part sequenced using a drum MIDI converter.
FIGURE 3.16
(a) MIDI drums, (b) MIDI percussion pads, and (c) trigger pads.
(Courtesy of Roland Corporation US and Avid Technology, © 2010).

121
3.7  Complex Tempo Tracks: Tempo and Meter Changes
3.6.3  MIDI Wind Controllers
MIDI devices can be controlled not only by keyboard, guitar/bass, and percussive instruments, but also 
by special electronic devices called wind controllers that emulate the shape, playing style, and features 
of saxophones and flutes. A wind controller is an incredible tool for improving your MIDI projects and, 
as in the case of the other alternative controllers, for firing up your imagination and creativity. Two of 
the most used wind controllers are the one made by Yamaha (the WX5) and the one made by AKAI (the 
EWI series). They both look very similar in recreating the look and feel of a soprano saxophone or clari­
net. These types of controller are appealing to both the seasoned saxophonist or professional flutist and 
the beginner. You can choose between several fingering positions and mouthpiece settings that simulate 
the ones for standard saxophones and flutes. The main unit can be connected directly to any MIDI sound 
module through a regular MIDI cable, or it can be connected to an expander specifically designed to 
enhance the controls and the tone quality of the instrument. Among the most obvious applications is the 
sequencing of wind instrument parts that usually are particularly difficult to render from a keyboard con­
troller. The problem of recording wind instrument parts from a keyboard is that most likely you will find 
yourself forgetting that wind instrument performers need to breathe between phrases. This will result in 
very long lines that sound unrealistic, mainly because they would not be playable in a live situation. By 
using a wind controller, the volume and the Note On/Off messages are directly linked to the amount of 
air you blow into its mouthpiece. This has two main effects. First, you will be able to accurately control 
the dynamics of your performance (on a keyboard controller you would have to use a fader to send CC 7 
volume to achieve a similar effect). Second, you will not be able to sequence phrases that are longer 
than your breath, which will result in well-balanced and convincing lines. In addition to the regular keys 
and mouthpiece, these wind controllers have a series of extra controllers, such as a Pitch Bend wheel, 
program-change buttons, and quick-transpose keys.
3.7  COMPLEX TEMPO TRACKS: TEMPO AND METER CHANGES
In Chapter 2 we learned how to create a basic click track, where the tempo would stay steady all the 
way through the project. While this could work for some basic sequences it is definitely restrictive for 
more advanced and complicated compositions. Modern sequencers are extremely flexible when it comes 
to tempo and meter changes. In the following paragraphs we learn how to program complex tempo 
changes, rallentandos, accelerandos, and meter changes.
The tempo and meter changes in a sequencer can be inserted and edited in three different ways: 
through an edit list window, graphically through a graphic editor, and through tap-tempo, meaning a live 
input of MIDI messages that can control the tempo of the project in real time. No matter which method 
or methods you use, they all have the same result on the programmed tempo and meter changes. Keep in 
mind that not only is a tempo map used to follow and accommodate the changes according to the score 
of your music, but it can also be used in several creative ways to improve the quality of your production, 
as we are going to learn later.
Tempo changes are usually recorded in a separate track that acts as the “conductor” (sometimes 
called tempo track) of your virtual orchestra. In fact, in DP this track is called the conductor track. 
These changes affect the tempo and the meter of your composition. You can program and insert any 
tempo/meter changes you want. The way your sequencer follows these changes can vary according to 
your needs. If you are using a steady tempo all the way through your composition, then you will dis­
engage the tempo track and therefore your sequencer will follow and react to the specified tempo (usu­
ally located in the transport window of your DAW). This is the simplest use of the tempo controls (and 
also the most limited in terms of features). With this setup you basically can set only one tempo for 

122
CHAPTER 3  Intermediate Sequencing Techniques
your entire project, and you can change it only by altering the tempo parameter in the transport window 
of your sequencer. This action can be done at any time, even during playback. This method, though, 
does not give you any control over programmed tempo changes. By switching to “conductor track” 
mode (in DP and PT it is called “Conductor Track”, while in CU it is called “Master Track”, and in LP 
“Tempo Track”) the sequencer will follow the tempo and meter changes you programmed or recorded in 
advance. Let’s take a look at how to program and edit tempo/meter changes in all four sequencers.
In DP the tempo and meter changes are stored and edited from the “Conductor Track”, which by 
default is the first track listed in the track list window. If you select it and then open the list editor win­
dow (Shift-E) you will be prompted with the familiar list editor environment we used for editing MIDI 
data. In the case of tempo and meter changes, the techniques are similar to the ones used to edit regular 
MIDI messages. You can insert tempo and meter data by selecting them from the drop-down menu of 
the Event List window (select either “Tempo Change” or “Meter Change”) then clicking on the “” sign 
(Insert event)—see Figure 3.17. Select the location where you want to insert the change (in the case of 
tempo changes select the basic pulse: quarter note, 8th note, etc.) and press Return. The new inserted 
value will appear in both the list editor and track list windows. You can repeat the same procedure to 
insert other tempos and other meter changes at any measure of your project. To have the sequencer fol­
low these changes, you must enable “Conductor Track” mode from the control window. Click on the 
Tempo Slider drop-down (Figure 3.17) menu located next to the Tempo value in the main transport win­
dow and select “Conductor Track”. After doing so you will no longer be able to use the tempo slider to 
change tempos, since the sequencer will be locked into the conductor track. This technique allows you 
to create drastic tempo changes but is not very convenient if your goal is to create smooth acceleran­
dos and rallentandos, since you would have to insert many tempo change events one after the other. 
FIGURE 3.17
Change Tempo window in DP.

123
3.7  Complex Tempo Tracks: Tempo and Meter Changes
To effectively create a smooth transition between tempos, you have two techniques. The first involves 
the insertion of a continuous series of tempo change events using the “Modify Conductor Track” func­
tion found in the Project menu (Figure 3.17).
After selecting this option, you will have to punch in the start and end positions where you want the 
tempo change to take place, using the Start Tempo and End Tempo fields. Choose the type of curve you 
wish the change to have (linear, exponential, or polynomial) using the Curve parameters, and select the 
beginning and end tempos (if you select the steady curve, you will see only one tempo field marked 
Start; if you choose any other curve, a tempo field marked End will appear automatically). You can 
also randomize the change using either a percentage parameter or a tempo variable by clicking on the 
Randomize Tempo(s) button and by specifying the randomization range in either percentage or BPM. 
After clicking on the “OK” button you will see in the track list window that a series of data has been 
inserted in the conductor track. Make sure that “Conductor Track” mode is enabled in the transport win­
dow, and press play; you will see the tempo of your project smoothly change according to the data and 
the type of curve you have chosen. The tempo change data you just inserted can be edited in the list edi­
tor window but can also be edited (or inserted) using the graphic window. To do so, select the conductor 
track and click on the graphic editor icon in the transport area (or press Shift-G). Instead of the usual 
piano roll layout, you will see a graphic representation of the changes in tempo over time (x axis) and 
the tempo value in BPM (y axis). By using the usual tools available from the tool palette, you can insert 
data (pencil tool) or reshape the tempo change data already inserted (reshape tool). You can also select 
different shapes to create smooth accelerandos or gradual rallentandos.
Similar results can be achieved in CU using techniques comparable to the one used in DP. You can 
access the tempo and meter change data through the “Tempo Track” information panel located in the 
Project menu. This graphic editor is very similar to the one used in DP. By using the usual graphic/draw­
ing tools, you can insert, change, or delete tempo and meter data. To be able to edit the tempo track data 
you have to be sure to have activated the master track button either by clicking on the “Tempo” button 
in the transport window or by clicking on the metronome button in the upper left corner of the tempo 
window. You are free to insert single tempo changes or a series of data to create smooth changes. Any 
inserted data can be moved and altered by simply selecting and moving their insertion points. To insert 
different time signatures, simply input the type of signature you want to insert in the relative field in 
the top area of the tempo track window and by using the pencil tool insert the meter changes in the line 
between the ruler and the tempo window. If you want to delete a time signature, simply select the eraser 
tool and click on the items you want to delete.
In PT the tempo and meter changes are inserted and edited using the Tempo and Meter rulers. If your 
session is not set up to show the tempo and meter rulers, choose View  Rulers  Tempo. To insert a 
tempo change go to Event  Tempo Operations  Tempo Operations Window (Figure 3.18). This win­
dow looks very similar to the one found in DP. For a static tempo change choose the “Constant” option 
from the drop-down menu.
If you plan to create smooth accelerando and decrescendo choose one of the other curves (linear, 
parabolic, or S-curve) and specify the start and end of the tempo change. After clicking the “Apply” but­
ton the new tempo variations will be automatically inserted in the tempo track. To insert a meter change 
select Event  Time Operations  Change Meter. If you prefer to insert tempo changes graphically you 
can do so directly from the edit window by using the pencil tool and drawing the variations in the tempo 
track (Figure 3.18).
In order for PT to be able to follow the tempo and meter changes, you have to activate the conductor 
mode by clicking on the conductor button (the icon looks vaguely like a conductor) located in the lower 
right area (next to the meter indicator) of the transport window. If the conductor mode is not activated, 
then PT will follow the tempo programmed in the tempo slider in the transport window.

124
CHAPTER 3  Intermediate Sequencing Techniques
In LP the tempo changes can be inserted in three different ways: from the tempo list editor, through 
the Tempo Operations window, or graphically directly in the tempo track from the Arrange win­
dow. The tempo list editor can be accessed by selecting Options  Tempo  Tempo List. To add a 
new tempo, use the pencil tool, click on an empty line, and specify the location and the new tempo. 
If you want to add rallentandos or accellerandos, open the Tempo Operations window by selecting 
Options  Tempo  Tempo Operations (Option-Shift-T). Select the beginning/end where the change 
will occur, and select the curve (and variation) and the beginning and end tempos (Figure 3.19).
You can also insert tempo changes graphically directly in the tempo track (as we learned in PT). To 
do so, open the Tempo Track from the Global Tracks area located at the top of the Arrange window. Get 
the pencil tool and click to insert single tempo changes. If you want to add rallentandos or accellerandos 
insert two single tempo changes and then with the arrow tool selected drag the connecting point.
One of the advantages of LP is that you can have up to nine “Tempo alternatives”. This function 
allows you to select nine different tempo tracks and therefore to experiment with different tempo options 
before committing to the final choice. It is the same as having alternate takes for your tempo track. To 
change the take, use the drop-down menu located directly in the Tempo Track. This is a very useful fea­
ture. Changes to the meter of the project can be made directly from the Signature Track located at the 
top of the Arrange window as part of the Global Tracks. Here you can change not only the meter but also 
the key signature (Figure 3.20).
3.7.1  Creative Use of Tempo Changes
Insertion and editing of tempo changes provide a valuable tool to make your MIDI project conform to 
the score of your composition, and also provide a great flexibility that can transform your sequences into 
something less mechanical and stiff. Even if your project is based on a steady tempo from beginning to 
FIGURE 3.18
Tempo Operations window and the Tempo Track in PT.

125
3.7  Complex Tempo Tracks: Tempo and Meter Changes
FIGURE 3.19
Tempo Operations window in LP.
FIGURE 3.20
Tempo and Signature tracks in LP.

126
CHAPTER 3  Intermediate Sequencing Techniques
end, your sequence can greatly improve if you take advantage of the tempo change feature. One of the 
best examples is to vary slightly the tempo of your sequence in key points of your composition. If you 
are sequencing a pop tune with a steady tempo, try to insert few tempo changes along the way to give 
a more natural flow to the song and to mask the mechanical nature of the sequencer. One of the biggest 
problems related to MIDI sequencing and the use of loops is that they both sound fine for a short time 
but after a while become repetitive and monotonous. This is due mainly to the lack of variation in tempo 
and groove, two aspects that make a real drummer and real musicians irreplaceable. By inserting tiny 
tempo changes in your conductor track, though, you can simulate the fluctuations that a live ensemble 
would necessarily create. You do not have to insert continuous variation, just a few events in the right 
points of your production. In general, do not slow down the tempo unless this is required by the arrange­
ment. Most of the rhythm section tends to speed up and not to slow down. By increasing the tempo 
slightly and little by little, you also help to create more excitement and simulate the animation that a 
live rhythm section would engender. Another thing to keep in mind is that the tempo usually increases 
slightly in crucial passages, such as during the transitions from the verse to the chorus of a song or 
during an exciting solo of a lead instrument. These tempo increments should be minimal and barely 
noticeable; in fact, they shouldn’t be noticeable at all, they should be felt but not heard. Therefore, try to 
increase the tempo from a minimum of 0.5 BPM to a maximum of 1 BPM. This will create the effect of a 
live ensemble that translates the excitement of execution into tiny tempo increments.
Listen to Examples 3.28 and 3.29 on the website to compare the same excerpts played without and 
with tempo changes, respectively.
One of the best and most effective practical applications of creative tempo changes is the function 
called “Tap Tempo”. This option allows you to literally conduct your sequencer by inputting MIDI 
messages from a controller that acts like a conductor’s baton. In this way you have very natural tempo 
changes that the virtual orchestra will follow during the performance of your composition.
To activate this feature in DP, select “Receive Sync” from the Setup menu, choose the “Tap Tempo” 
option, and select the source of your sync (one of the MIDI controllers present in your studio). This will 
instruct DP to wait for incoming MIDI messages (such as notes and controllers) to receive the sync and 
tempo. Next select the type of message and device that will be used to “conduct” your virtual orchestra. 
Usually select a note and the main MIDI controller in your studio. After making these settings, click “OK” 
and set DP for slave to external sync by pressing Command-7 or selecting “Slave To External Sync” from 
the “Setup” menu. As you will notice, the Tempo Control parameter in the transport window has changed; 
it now indicates “Tap Tempo”. If you press Play, DP will wait for you to tap on the controller and conduct 
the sequence. You will be basically inputting in real time the quarter notes that DP will use as a reference 
for its conductor track. You can also record the tempo changes to insert this way by record-enabling the 
Conductor track and pressing the Record button. You can then edit the tempo change data inserted with 
the Tap Tempo control with either the graphic or list tempo editor, as we saw earlier in this chapter. This 
technique performs magic for compositions that feature rubato passages, such as classical orchestral scores 
or solo sections. Remember that I strongly advise to always record your parts with a click first and then to 
use Tap Tempo or regular tempo changes to recreate rubato, accelerandos, and rallentandos. This will give 
you much greater control over the editing and quantization options, since even during rubato passages your 
parts will still be following the bars and tempo laid out by the conductor track.
Listen to Example 3.30 on the website to experience the flexibility and power of the Tap Tempo 
technique.
A similar method, called Tempo Interpreter, can be used in LP. Via this technique, LP can receive 
sync from either an external MIDI controller or the keyboard of the computer. To activate this function 
select Options  Tempo  Tempo Interpreter and adjust the Tempo Interpreter parameters according to 
your needs. In Table 3.1 you can find a short description of each parameter.

127
3.7  Complex Tempo Tracks: Tempo and Meter Changes
After setting up all the parameters, you can start “conducting” your sequence by tapping on the key you 
selected as your virtual baton. After four beats (or whatever number of beats you chose in the Tap Count-In 
option), the sequence will start playing according to the tempo you tap, and it will follow any variation.
CU uses a slightly different approach to achieve similar results to those of DP and LP. In CU you 
record the conducting pattern on a regular MIDI track by tapping on any key of your MIDI controller 
(the sustain pedal will not work). After recording the conducting pattern of tempo changes, you can 
transfer the MIDI events recorded to the Tempo Track by selecting the part you just recorded and choos­
ing the option “Merge Tempo from Tapping” from the Functions submenu in the MIDI menu. You have 
to indicate the type of rhythmic subdivision you tapped (1⁄4, 1⁄8, etc.). The “Begins at Bar Start” option 
allows you to choose whether the first MIDI message will automatically start at the beginning of a bar 
after the computer calculates the tempo changes. Make sure the Tempo parameter is set to “Track” in the 
transport window. If you press the play button, the sequence will now follow the tempo you “conducted” 
earlier from your MIDI controller. The tempo data can be edited in the tempo graphic editor, as seen 
previously. You can also record tempo changes in real time by using the “Tempo Recording” slider avail­
able in the tempo graphic window. When in playback mode, you can record tempo changes by moving 
the slider horizontally. This technique is particularly useful to quickly insert accelerandos and rallentan­
dos on the fly, without using Tap Tempo.
In PT you can create “live” tempo changes using a technique similar to the one we learned for CU 
using the Beat Detective (BD) function. As we saw in CU, first you have to record on a new MIDI 
Table 3.1  Tempo Interpreter Parameters in LP.
Parameter
Description
Notes
Tap Step
Sets the basic rhythmic value the 
sequencer will assign to the manual 
taps
Usually ¼ is a good starting point
Window
Determines the size of the region 
that will determine the tempo 
changes
Larger values create more drastic 
tempo changes, smaller values 
create smoother and more precise 
changes
Tempo Response
Controls the sensitivity related to 
tempo changes
Larger values imply greater 
sensibility
Max Tempo Change
Controls the maximum tempo 
change allowed
Tap Count-In
Sets the count off you have to input 
before LP detects the tempo
Smoothing
Controls how responsive LP is to 
your “conducting”
Tempo Recording
If selected, allows you to record the 
tempo changes in the tempo track
Pre and Post
Visual indicators that give you 
feedback about your “conducting”. 
Pre displays the taps you input; 
Post displays the taps that have 
been accepted by LP
A yellow flash means that the tap 
was within the allowed range; a 
red flash indicates that the tap was 
outside the allowed range
LP: Logic Pro.

128
CHAPTER 3  Intermediate Sequencing Techniques
track the beats that will form your new tempo track (I recommend recording quarter notes, even though 
you could record smaller subdivisions). If you plan to have a constantly changing tempo track you 
will have to record it for the entire length of the song. Once done, select all the MIDI events recorded 
(from the first to the last) and open the BD window (Event  Beat Detective). In the BD window set the 
“Operation” field to “MIDI” (Figure 3.21) and to “Bar | Beat Marker Generation”. Next, you have to 
let PT know the selection you made in the Edit window; to do so in the Selection area of BD, click the 
“Capture Selection”, set the basic rhythmic value to quarter note, and set the correct Time Signature. In 
the Detection area of BD set the resolution to Beats and click the “Analyze” button. If you also check the 
“Show Trigger Time” box you will see the actual new beats identified inside the selected region in the 
Edit window (Figure 3.21). To translate the identified beats in tempo change click the “Generate” button 
in the BD window.
As you can see, the tempo change feature can be an incredible tool for improving your sequencing 
techniques and skills. Used in the right way it can bring your virtual orchestra to life and help you to 
overcome the intrinsic mechanical nature of the sequencer.
3.8  TEMPO CHANGES AND AUDIO TRACKS
When inserting, editing, and programming tempo changes, you can really appreciate the flexibility of 
the MIDI system. Since all the MIDI data recorded in a sequencer are only descriptions of performance 
actions, it is fairly easy to have them follow tempo and meter changes. The same cannot be said, though, 
for audio tracks. This type of track is much less flexible and forgiving than its MIDI counterpart. As you 
FIGURE 3.21
Beat Detective window in PT.

129
3.8  Tempo Changes and Audio Tracks
have probably already noticed if you had some audio loops in the sequence you were using to experiment 
with tempo changes, audio tracks usually do not adapt automatically to the tempo changes programmed 
in your sequence (there are exceptions, such as audio files in Recycle and Apple Loop format, but I will 
discuss these in the next chapter). While this used to be a major problem in early MIDI/audio sequencers, 
nowadays there are several techniques that can make audio tracks easily adjustable to tempo changes. In 
the following paragraphs we are going to learn how to overcome the static nature of audio files.
3.8.1  Time-Stretching Audio Files
One way to have audio files adjust to tempo changes is to use the time-stretching technique we learned 
in Chapter 2. This approach involves changing the tempo of a loop or sound bite without changing its 
pitch. It is based on complex algorithms (such as phase vocoder and time domain) that try to “guess”, 
through a frequency analysis, which samples need to be eliminated (when you increase the tempo of an 
audio file) or added (when you decrease the tempo of an audio file) and interpolated. Once you know at 
which tempo the original audio file was recorded it is fairly easy to adjust its tempo to the new tempo 
track. Keep in mind that as a general rule it is always better to increase the tempo of an audio file than to 
decrease it, since in the first case the computer will have to eliminate some of the samples, while in the 
second it will have to generate new samples through a process of interpolation. Let’s take a look at how 
the four sequencers allow you to adjust the tempo of audio files to a different tempo than the original one 
at which they were recorded.
In DP you can use the function called “Adjust Soundbites to Sequence Tempo”. After selecting the 
sound bite you want to modify (remember that you must have already established the original tempo of 
the sound bite as explained in the previous chapter), simply select this function from the Audio menu. 
The sequencer will change the tempo of the audio file selected according to the actual tempo marker 
of where the bite is located. Another quick way to time-stretch an audio file is to open the graphic edi­
tor for the sound bite by double-clicking on the part in the track list window. From the graphic editor, 
simply move the cursor toward the top corners of the edges of the sound bite you want to time-stretch. 
The pointer will change into a grabbing hand tool. By clicking and moving the mouse horizontally, you 
will be able to adjust the length of the sound bite without changing its pitch, almost in real time. Keep 
in mind that with any time-stretching technique, results may vary depending on the material you are 
working on. Usually more harmonic complex materials, such as full mixes or audio files with a high 
harmonics content, return much higher signal degradation than rhythmic or percussive parts. A set of 
independent tools called “Spectral Effects” (Figure 3.22) is also available to apply time-stretch and 
pitch-shift changes to audio sound bites.
You can access it from the Audio menu. Choose the percentage (ratio) of the time-stretch that will 
be applied to the selected sound bites by either inserting the value in the field named “Tempo” (posi­
tive values will speed the audio up, while negative values will slow it down) or moving the ball in 
three-dimensional space with the mouse. Using the “Spectral Effects” tools you can also pitch-shift the 
selected audio material (change the pitch without changing the tempo). You can do so by specifying the 
transposition in the “Pitch” field.
The same techniques can be applied in PT, where the time-stretching function can be accessed by 
navigating to Audio Suite    Other    Time Compression Expansion. Having selected the region of 
audio you want to modify, you can alter its tempo using several parameters. In Chapter 2 we analyzed 
how to time-stretch a region by inserting the original tempo (source) and the new tempo (destination). 
You can alter a region’s tempo by using other parameters, such as length in samples, length of the region 
in real time, or length of the region in time code. These options are particularly useful when applying 
time-stretching to sound design or postproduction projects. In addition, you can use a series of sliders 

130
CHAPTER 3  Intermediate Sequencing Techniques
that allow you to control parameters such as ratio (which allows you to set the length of the destination 
region in relation to the source length; by moving the slider to the right you increase the length of the 
destination region, and vice versa) and crossfade (which allows you to control the amount of crossfade 
applied between samples that are either eliminated or added during the time-stretching process; a shorter 
setting is better for percussive parts with fast transients, while longer settings are more appropriate for 
long and sustained parts). The parameter Min Pitch has the function of limiting the frequency range of 
the time-stretching operation, resulting in a more accurate rendition of the original material. Use the 
accuracy slider to target the time-stretching algorithm to the material you are processing. Another very 
useful way of quickly applying time-stretching to a region is to use the TCE (time compression expan­
sion) Trimmer tool. This is a variation of the regular trimmer tool used to shorten or lengthen a region. 
To select the TCE, click and hold on the regular Trimmer Tool and select the TCE option (it looks 
like the trimmer tool but with the addition of a watch icon). With the TCE tool, instead of reducing or 
increasing the size of a region, you can stretch it to fit the tempo of your sequence. If used in conjunction 
with the grid mode (which allows you to drag elements in the edit window only by the grid specified in 
the grid value fields), you can easily reshape your audio material to fit any tempos.
To use the time-stretching function in CU, select the part you want to alter and then choose the option 
“Time Stretch” from the Process submenu located in the Audio menu. In the window that appears on 
your screen you will see some of the familiar parameters we encountered in PT. At the top of the win­
dow you choose the algorithm used by the CU engine to apply time-stretch-compression. If you opt for 
the “Realtime” algorithm you will get a choice of seven presets (Drums, Plucked, Pads, Vocals, Mix, 
Advanced, and Solo) to choose from based on the audio content of the region selected. If you opt for 
MPEX 4 instead, you will rely on an algorithm developed by Prosoniq, and it usually guarantees best 
results for complex materials such as full stereo mixes. After selecting the algorithm, insert the length 
in bars and beats, and the time signature of the audio material selected in the Define Bars area (this will 
automatically have CU calculate the tempo of the loop that will be shown in the Original Length area). 
FIGURE 3.22
Spectral Effects window in DP.

131
3.8  Tempo Changes and Audio Tracks
To select the new tempo after the time-stretching-compression, insert it in the Resulting Length area or 
use the Seconds Range Selection to specify an exact end point to which the region will be stretched-
compressed. You can also specify the amount of the effect applied by using the Time Stretch Ratio located 
at the bottom of the window. Use the preview option to listen to the result of the process before actually 
creating a new sound file with the processed material. If you are satisfied with the quality, click “Process”; 
otherwise, fine-tune the parameters to obtain the best result and then apply the changes to the audio file.
The time-stretching features of LP are comparable to those of the other DAWs we have analyzed 
so far. For quick and precise adjustments of loop and in fact any other audio materials, you can use the 
two functions named “Time Stretch Region to Locators” and “Time Stretch Region to Nearest Bar”. 
The former will speed up or slow down your selection according to the area selected between the two 
locators; the latter will automatically adjust the speed of the audio material to fit into the closest bar. 
Both functions can be accessed from the “Audio” small menu of the Arrange window. For more complex 
time-stretching edits, you will need to use the powerful Time and Pitch Machine (Figure 3.23). This 
engine is similar to the one found in PT and DP. It can be accessed from the Sampler Editor window 
under the “Factory” menu (you can also use the shortcut Control-T from the Sampler Editor window).
The Time and Pitch Machine allows you to control several time-stretching parameters, and you can 
decide to change the tempo of an audio region using criteria such as tempo change percentage, length in 
samples, length in time code (SMPTE), and length in bars. As we saw in CU, you can choose among dif­
ferent algorithms targeted at different types of audio material to achieve the best quality. The algorithms 
are pretty much self-explanatory, targeting audio materials such as pads, monophonic, rhythmic, beats 
only, etc. Use free transposition if you want to have time and pitch independent of each other (this is the 
most likely situation in which you will work). Choose classic if instead you want to have both the tempo 
and the pitch affected simultaneously by the Time and Pitch Machine. To operate the changes set the 
FIGURE 3.23
Time and Pitch Machine in LP.

132
CHAPTER 3  Intermediate Sequencing Techniques
destination tempo you want the material selected to be changed to, press “Process and Paste”, and use 
the “Pre-listen” button to preview the result before committing to it.
You can see how powerful the time-stretching engines are and how useful they can be not only for 
matching loops recorded at different tempos but also for creating sound effects and new sonorities. For 
example, you could overly slow down or speed up a vocal track or a solo instrument to come up with 
a completely new sound effect. Or you could time-stretch an entire track should you decide to slightly 
speed up your composition when it is too late to recall the musician for an additional recording session. 
Keep in mind, as I mentioned earlier, that it is better (if possible) to speed up the audio material than 
to slow it down. Also, try to keep the tempo changes within a small range, usually up to 10 BPM; for 
higher values, results may vary depending on the complexity of the audio materials.
3.9  INTERAPPLICATION PROTOCOLS: REWIRE
While a few years ago thinking of running more than one audio application simultaneously in your compu­
ter was highly discouraged and, in most cases, practically impossible owing to the limitation of the central 
processing unit (CPU), nowadays it is not only possible, but has become the norm. The advantage of running 
several audio applications at the same time is that you can take advantage of the strengths of each program 
without constantly having to import/export your work. The key factor to understand is that for two or more 
applications to run, exchange data, and collaborate in real time, they must share a path on which to exchange 
information. This has to happen inside your computer. This exchange path is a virtual one taking the shape 
of a software protocol that, if compatible with all the applications running, can effectively allow for fast data 
transfer in any direction. While in the past there have been a few protocols that have attempted to reliably 
allow interapplication connectivity, the de facto standard nowadays is a protocol called “ReWire” that was 
originally developed by Steinberg and Propeller heads. Think of ReWire as virtual patch chords that can be 
run between two or more applications running simultaneously on your computer. As long as the applica­
tions are ReWire compatible (meaning they can interface to other ReWire applications) they can exchange 
both audio and MIDI information. In theory, it is possible to run more than one DAW at the same time but I 
highly discourage it since there could be problems with sharing a single audio interface. However, ReWire 
works really well when using production suites like Ableton Live or Propellerheads’ Reason. These are 
applications that were designed with ReWire in mind and therefore constitute the perfect companion for our 
DAWs. The ReWire protocol allows you to exchange not only audio signal but also MIDI data (including 
synchronization) among applications. This means that, if set up correctly, you can have your DAW control­
ling (“master”) the ReWire applications (“slave”) in terms of MIDI notes, CC, tempo, and meter changes. 
Using ReWire applications along with your DAW allows you to open new horizons in terms of production, 
since you can take advantage of different tools that are specific to each application. For example, using LP 
along with Live gives access to the advanced MIDI and audio recording/editing features of LP along with 
the incredible loop and effect selection of Live. I also often use PT ReWired with Reason to combine the 
infinite sound capabilities of the software synthesizers in Reason with the exceptional audio features of PT. 
As you can see, the options are endless and you will quickly become addicted to using ReWire applications 
with your DAW. Setting up ReWire is usually pretty straightforward but it requires you to follow precisely 
the required steps to have a smooth experience. Let’s learn how to set it up for our four DAWs.
3.9.1  Setting Up ReWire
In general, you must launch your DAW first; never launch your ReWired applications (the slave ones) first! 
While some DAWs are more permissive than others, to avoid problems, always start your DAW first.

133
3.9  Interapplication Protocols: ReWire
In DP, in order to establish the ReWire connection, you have to add a new stereo Aux track 
(Project  Add Track  Aux Track). Make sure to set the input of the Aux track to one of the outputs 
of the ReWire application you plan to use. To do so click on the input of the Aux track and under “New 
Stereo Bundle” select the main output of your ReWire application (let’s say Ableton Live) as shown in 
Figure 3.24. This action tells the computer that a new connection patch is open and it is waiting to send/
receive data from Live to DP. Now, and only now, launch the ReWire application. If everything was set 
up correctly you are all set. The two applications are now ReWired and ready to work together. DP is the 
master and the ReWire application is the slave. The audio signal will flow from the main output of Live 
to the Aux track of DP. If you want to record the audio signal coming from the ReWire application into 
DP, simply create an audio track with its input set to the same input of the Aux track (in this case Live 
Main 1-2). As soon as the ReWire connection is established you will notice that DP MIDI tracks will 
automatically show new MIDI OUT options. These new outputs correspond to the MIDI devices present 
in the ReWire application (Figure 3.24). You can now quickly control MIDI instruments that are hosted 
by the ReWire application from DP.
Setting up ReWire in LP is a bit more convoluted. First launch LP and then the ReWire application. 
Now go to the Environment window in LP (Command-8) and select New  Internal  ReWire (Figure 
3.25a). This will create a new ReWire object that allows LP to open a communication channel with the 
ReWire application. Make sure to select the correct ReWire application in the Inspector window (in this 
example I choose to use Live as my ReWire device, as shown in Figure 3.25a). To receive Audio in LP 
through the ReWire channel create a new Aux track from the Environment (Figure 3.25b) and assign 
its input to the ReWire application main outputs L-R (you can choose additional ReWire outputs if you 
need to, so that you can have multiple outputs coming in at the same time on different Aux tracks).
If you also want to send MIDI from LP to any device hosted by the ReWire application you have 
to first create a new External MIDI track and then from the Library panel located to the right of the 
Arrange window select the ReWire MIDI device you want to send MIDI data to (Figure 3.26).
FIGURE 3.24
ReWire settings in DP.

134
CHAPTER 3  Intermediate Sequencing Techniques
FIGURE 3.25
ReWire setup in LP: (a) creating the ReWire object, (b) creating the Aux object.
FIGURE 3.26
ReWire setup in LP: assigning a MIDI track to a ReWire device.

135
3.9  Interapplication Protocols: ReWire
Setting up ReWire in CU is extremely straightforward. First, launch CU and then the ReWire appli­
cation. Next, go to the Devices menu in CU and select the ReWire application you want to setup (all 
your ReWire applications that are installed on your computer will be listed, even the ones that are not 
currently open). A new window with all the available outputs from the specified ReWire application will 
open (Figure 3.27).
Now activate the outputs that you would like to use (Figure 3.27) and automatically a new folder 
with a new Aux track receiving from the activated outputs will be created (Figure 3.27). To send MIDI 
data to the ReWire application simply create a new MIDI track and assign its MIDI output to the avail­
able MIDI devices hosted by the ReWire application.
The sequence of actions in PT for setting up ReWire is similar to the one learned for DP. After 
launching PT, but before launching the ReWire application, create a new Instrument track (Figure 3.28) 
and insert on it a new software instrument from the list of software instruments that will have the same 
name of the ReWire application you are setting up (in this case Ableton Live, as shown in Figure 3.28). 
The new Instrument track will have the inputs set as default from the main out L-R of the ReWire appli­
cation as shown in Figure 3.28; this can be changed by using the floating ReWire window in PT. Now, 
launch the ReWire application that will start in ReWire slave mode automatically (note that if you do 
not follow this exact sequence the ReWire application will start in master mode and will conflict with 
your DAW). Your audio connection now is set and working. To send MIDI data from PT to the ReWire 
application simply create a new MIDI track and assign its output to the MIDI devices that are active in 
the ReWire application.
FIGURE 3.27
ReWire Device window in CU.

136
CHAPTER 3  Intermediate Sequencing Techniques
3.10  SYNCHRONIZATION
Up to this point we built our studio around a computer sequencer that acted as the main central hub of 
the MIDI and audio network. While this is probably one of the most common situations, there are other 
scenarios in which other sequencers (hardware or software) and other multitrack recorders or video 
machines need to be included in your setup. If this is the case, then some new issues need to be ana­
lyzed in order to have a smooth integration of your main sequencer and the other devices. One of the 
main aspects we have to consider is the synchronization of time-based devices (such as drum machines, 
satellite computers running other DAWs, video tapes, multitrack tape recorders, HD recorders, to men­
tion just a few) to your main computer sequencer. All these devices store data either on tape or on 
random-access media (RAM and HD, for example). To be able to use them in the same studio situa­
tion, they all need to move at the same pace and to be always in sync with the others. When we set 
up our studio in Chapter 1 we didn’t run into this issue because we only had one main sequencer that 
received and sent data to devices that did not need any synchronization, such as sound modules and 
MIDI synthesizers.
There are two different categories of devices that can be synchronized to our sequencer: nonlinear 
and linear machines. The main difference between the two is based on the way they record and play 
back information. Nonlinear machines store data in a random-access way, meaning that data can be 
accessed immediately without waiting for the machine to actually reach a certain area of the storage 
media. Classic examples of nonlinear machines are MIDI sequencers or DAWs, HD recording systems, 
and drum machines. With a sequencer you are free to jump to any location almost instantly without hav­
ing to wait to fast-forward or rewind to that location. This type of machine is the most flexible and easy 
to synchronize. Linear machines, in contrast, store the pieces of information recorded one after another, 
mostly on tape (in fact they are also referred to as tape-based machines), which means that in order to 
reach a certain spot of your project you have to physically fast-forward or rewind the tape. This type of 
FIGURE 3.28
ReWire setup in PT.

137
3.10  Synchronization
machine is definitely less flexible than the nonlinear type and presents a more complex synchronization 
setup. Next you are going to learn how to synchronize nonlinear machines to your sequencer. Later in 
the book we will approach the synchronization of linear machines.
3.10.1  Synchronization of Nonlinear Machines
Let’s analyze the synchronization process and techniques used for nonlinear devices. One of the advan­
tages that nonlinear machines have over linear devices is that, in most cases, they feature a built-in MIDI 
interface, making it fairly easy to set them up for synchronization. The synchronization process for non­
linear machines involves the use of data that are received and transmitted using standard MIDI mes­
sages. There are two main types of sync protocol that can be used for nonlinear machines: MIDI Clock 
(MC) and MIDI Time Code (MTC). While they both allow you to synchronize multiple devices together, 
their nature and their features are very different.
MIDI Clock (sometimes also called Beat Clock) is a tempo-based sync protocol. This means that it 
is able to carry information about the tempo at which the sequence is programmed and therefore is ideal 
for MIDI studios that need to synchronize several nonlinear machines. To better understand the proc­
ess involved, let’s examine a practical example. Let’s say we need to synchronize a drum machine to 
our main sequencer. Keep in mind that all the other devices will continue to function and are connected 
exactly as explained in Chapter 1. The only difference now is that we have added a drum machine that 
not only needs to receive regular MIDI messages, but also needs to keep in sync with the main sequencer 
in order to play the programmed patterns in time with the other MIDI tracks. When trying to synchro­
nize two or more devices, you have to set up one as the master and the others as the slave. The master 
will distribute the sync to the slaves, while the slaves will wait to receive the sync from the master in 
order to play back. The master device is sometimes also referred to as Internal Sync and the slave as 
External Sync, depending on the software, model, or brand. Once you have set up the master and slave 
devices (I recommend usually making the main sequencer the master), you should be ready to go. The 
slave devices will wait for incoming MIDI Clock messages; as soon as they detect the MIDI messages 
coming from the IN port, they will start playing in sync with the master. The MC data are System Real 
Time MIDI messages that are sent 24 times every quarter note. As long as no MC messages are lost in 
the connection between master and slaves, the devices will move at the same tempo and pace. Three 
other types of MIDI messages, called start, stop, and continue, complement MC by allowing the master 
device to instruct the slave machines to start the sequence, stop playback, or continue from the current 
position.
One of the problems with MC is that when a connected device receives a “start” message, it will 
start from the beginning of the sequence, making it fairly problematic to work on long sequences. 
Fortunately, an extra MIDI message, called Song Position Pointer (SPP), allows the master to control the 
current position of slave devices by describing its current position in 16th notes. When the slave devices 
receive an SPP message, they will set their counters to the position indicated and then wait for the 
incoming MC. As soon as the MC messages are sent and received from the master, the slave machines 
will start playing in sync. It is not uncommon for MIDI programmers and composers of loop-/groove-
based sequences to favor groove boxes or hardware sequencers because of their particularly tight timing 
(e.g., the Akai MPC 2500). It is therefore fairly common to run into sessions that need MC in order to 
have all the devices in sync. The fairly basic setup is based mainly on the right assignment of the master 
and slave devices. In the next section we will analyze how the four main sequencers handle the master/
slave assignment and how to set them up in the right way in order to have a problem-free session.
MIDI Time Code is a real-time-based synchronization protocol that contains no information on 
the tempo of the sequence but instead defines the passing of time. Its syncing format is divided into 

138
CHAPTER 3  Intermediate Sequencing Techniques
hours:minutes:seconds:frames (hh:mm:ss:ff). This format is derivative of the widely used protocol 
SMPTE (Society of Motion Picture and Television Engineers) and is based on the same format. While 
SMPTE is an analog-based signal used to synchronize linear to either linear or nonlinear machines, 
MTC is its digital translation, meaning a digital binary code sent over the MIDI network as MIDI mes­
sages. MTC falls into the System Common messages category. Whereas MC is directly related to the 
tempo of the sequence (since it is sent 24 times per quarter note), MTC is an absolute rendition of time 
and does not include any information about tempo or meter changes. If you plan to use this syncing pro­
tocol to sync two sequencers, you will have to have an identically programmed tempo/meter map in both 
devices, otherwise the bar numbers and tempos of the two sequences will not correspond. As we saw 
in the case of MC, you will have to set up one device as master and one or more devices as slave. The 
smallest time subdivision on which MTC is based is the quarter-frame message. It takes eight quarter-
frame messages to describe an entire MTC position (two messages for the frame, two for the seconds, 
two for the minutes, and two for the hours). As in the case of SMPTE, there are four main types of frame 
rate (in frames per second, fps): 30, 29.97, 25, and 24. In Table 3.2 you see a description of the different 
frame rates and their use in the industry.
As a standard the frame rates are assumed to be nondrop formats, meaning they express exactly the 
frame rate indicated. Another form of MTC (and SMPTE) is called drop-frame. This particular for­
mat drops frame numbers 00 and 01 at the beginning of each minute, except for the minutes falling 
on the 10th minute and its multiple. The drop-frame format is used to compensate for the discrepancy 
between the 30 fps rate and the 29.97 fps rate, which is the one used in color television in the USA and 
Japan. While for MC all the machines work with a standard resolution of 24 clicks per quarter note, 
with MTC the frame rate needs to be set up and needs to be the same for each synchronized machine. 
If you are scoring to a picture and synchronizing your sequencer to a video linear machine (more on 
this later), you will have to set the frame rate of your devices to match the one used in the video sync 
track. Usually the video production company will tell you at which frame rate the tape was recorded. 
If you are not synchronizing your MIDI devices to any video source but instead are using MTC to syn­
chronize audio devices and sequencers only, you can select the frame rate you want (usually 30 fps is a 
good choice since it gives the best resolution) as long as all the devices are set to the same frame rate. 
After choosing the frame rate for all the devices that need to be synchronized, you have to set the slave 
devices to “external sync” and put them in play. The slave devices will wait for incoming MTC mes­
sages, and they will start playback as soon as the first full message describing the position of the mas­
ter, in hh:mm:ss:ff format, is received. While it is not the main intent of this book to go deeply into a 
discussion of the MIDI message format (for a full analysis I recommend the book MIDI for the 
Professional, by Paul Lehrman and Tim Tully), I would like to point out that MTC consists of four types 
Table 3.2  SMPTE Frame Rates.
Frame Rate (fps)
Use
24
Film, ATSC
25
PAL (Europe, Argentina), SECAM, DVB. Color and black and white TV in 
Europe, Australia, and all countries using a frequency of 50 Hz
29.97 (drop and nondrop)
NTSC American System, PAL-M. Color TV in USA, Japan, and all countries 
using a frequency of 60 Hz
30
ATSC. Black and white TV in USA and Japan
SMPTE: Society of Motion Picture and Television Engineers; fps: frames per second.

139
3.10  Synchronization
of messages: full, quarter-frame, cueing, and user bits. The first two are for synchronization, while cue­
ing messages are for automation and user bits messages, at the moment, are not used.
MC and MTC have advantages and disadvantages, and they have different practical applications. One 
of the advantages of MC is that it carries tempo information, minimizing the amount of programming 
required to insert tempo maps in all the devices that need to be synchronized. On the other hand, MTC 
has a much higher resolution than MC (at least for BPM rates lower than 300), meaning in general a 
tighter synchronization among devices. MTC also has the advantage of not requiring a “play” message, 
since slave devices will start playback as soon as MTC messages are received. MTC has the disadvan­
tage of taking a fairly big chunk of the bandwidth reserved on the MIDI network (up to 8%) and there­
fore should be used only if synchronization is required and should be sent only to the devices that need 
it. Keep in mind that MC and MTC do not necessarily exclude each other in a studio situation. In fact it 
is quite common to have several devices synchronized where some require MTC and others require MC. 
If this is the case it is recommended to have the same sequencer set as master for both MC and MTC. 
In Figure 3.29 you can see a variation of the studio setup learned in Chapter 1, with the addition of two 
devices, an HD recorder and a drum machine, slaved to the main computer sequencer through MTC and 
MC, respectively.
FIGURE 3.29
Studio setup with master–slave synchronization through MTC and MC.
(Courtesy of Apple Inc., Roland Corporation US, Avid Technology, © 2010).

140
CHAPTER 3  Intermediate Sequencing Techniques
3.10.2  Sequencer Setup for MC and MTC Synchronization
As we just learned, one of the crucial steps to effectively synchronize nonlinear devices is to make sure 
the master–slave relationship is correctly established between the devices and that, in the case of MTC, 
the same frame rate is selected for all the devices. In Table 3.3 I sum up how to set up the four sequenc­
ers to be either master or slave and how to set up the synchronization parameters.
Table 3.3  Internal and External Synchronization Settings.
Sequencer
“Master” Settings
“Slave” Settings
DP
From the Setup menu select “Transmit 
Sync”, choose on which port you will 
send either (or both) MTC or Beat 
Clock
For MTC select the frame rate from the 
“Frame Rate” submenu in the Setup 
menu
From the Setup menu select “Receive Sync” and choose 
the MIDI cable on which DP will receive the sync
Select the type of sync you want the sequencer to 
receive
Select the frame rate
From the Setup menu select “Slave to External 
Sync” and press play. DP will wait for incoming 
synchronization code to start playback
PT
Set the MTC port to which PT 
will send the sync by selecting 
Setup  Peripherals under the 
“Synchronization” tab
Select “Session” from the Setup menu 
and set the correct frame rate
The settings are similar to those explained in the 
case of PT as master. Set the MTC port from 
which PT will receive the sync by selecting 
Setup  Peripherals under the “Synchronization” tab
In the same window you can also select the error 
correction options: none  no error correction; 
freewheel  PT will keep playing for the number 
of frames specified in the field even if MTC is 
interrupted (it allows to overcome short dropouts 
of the sync code); Jam Sync  allows PT to keep 
playing even when time code is dropped
Set PT to be slaved. To do so click on the “Slave 
to External Sync” icon located in the transport 
window (the small watch icon)
CU
Select the “Project Synchronization 
Setup” option from the Transport menu
Set the “MIDI Time Code Out” option to 
the desired device
For MC set the MIDI ports on which you 
want to send MC
Make sure that CU is in “internal sync” 
mode by selecting “Sync Online” in the 
Transport menu
Make sure that CU is in “internal sync” 
mode by declicking on the “Sync” 
button in the Transport menu
Select the “Project Synchronization Setup” from the 
Transport menu
Set the “MIDI Time Code Source” option to “MIDI 
Time Code”
Set the MIDI ports from which you want to receive 
MTC
You can set up parameters such as Drop Out Time, 
which has the same effect as freewheel in PT, 
and Lock Frames, which allows you to control the 
number of frames that CU needs to sync up to MTC
Make sure that CU is in “external sync” mode by 
clicking on the “Sync” button in the Transport menu
LP
Navigate to Files  Project 
Settings  Synchronization
Under the “General” tab set the Sync 
Mode parameter to “Internal” and 
specify the desired frame rate
Under the “MIDI” tab check the “Transmit 
MIDI Clock” or “MIDI Time Code” and 
select the respective destination ports
Navigate to Files  Project 
Settings  Synchronization
Under the “General” tab set the Sync Mode 
parameter to “MTC” and specify the desired frame 
rate
Make sure that LP is in slave mode by clicking on the 
Synchronization icon in the transport window (it has 
a clock icon)
DP: Digital Performer; PT: Pro Tools; CU: Cubase; LP: Logic Pro.

141
3.11  Safeguard Your Work: Back it Up!
Even though the number of situations in which synchronization is required has been dropping in 
recent years because of the increasing computer power available, you will still find yourself, sooner or 
later, in a situation where you need to be able to synchronize some devices. It could be a situation where 
a client is bringing some old sequences recorded on a hardware sequencer or where you really like to 
record your drum patterns on your MPC 2500. The bottom line is that synchronization protocols such 
as MTC and even the older MC are going to be around for a while and therefore it is good to know how 
to deal with them. I suggest that you practice and make some tests with the equipment available in your 
studio and get comfortable setting up the devices and your sequencer in both master and slave modes.
3.11  SAFEGUARD YOUR WORK: BACK IT UP!
All the information you have acquired so far about sequencing and the creative use of your DAW is 
important material with which you have practiced your production proficiency and with which you will 
improve your skills in the next chapter. We have been focusing mainly on learning setup procedures, stu­
dio arrangement, and software techniques from which your sequencing ability will benefit tremendously. 
One aspect, though, that is never stressed enough in the mainstream production literature is the impor­
tance of safeguarding your precious work during and after the composition and sequencing process. 
Usually, and unfortunately, you realize how important it is to regularly back up your data only after you 
have lost them. There is always a crucial moment in composers’ careers when, the night before deliver­
ing the most important project of their life, the computer or HD decides to give up, crash, and put them 
through hell for a few hours (or days). As you probably know by now, my working studio philosophy is 
based on the concept of a smooth and well-oiled technical environment that does not interfere with the 
creative process. It is for this reason that I want to devote the last part of this chapter to project backup 
issues and how to organize sessions, files, and documents on your HD.
As I just mentioned, disasters always strike when least needed, so it is essential to prevent them 
rather than try to fix them later. To have quick, easy, and efficient backup sessions it is crucial to have 
your files organized in a methodical way on your HD. This will speed up the process. Here are few 
tips about your HD’s organization. In the best-case scenario you will have at least two separate HDs 
(let’s call them 1 and 2). HD1 (usually your internal HD) will contain all the applications along with 
the operating system. HD2 (it could be either internal or external) is dedicated to projects and sessions 
and is used as the current working medium. As mentioned in Chapter 1, the two-HD setup is a maximal 
performance environment, since one HD’s head will deal with all the operations related to the OS and to 
the running applications, while the second HD’s head (in our example HD2) will store and retrieve data 
related to the project/session you are currently working on. Keep also in mind that a bigger HD will give 
you more freedom later and therefore I recommend using at least 750 GB or 1 TB disks. Besides being 
more efficient, this setup is recommended to avoid catastrophic crashes, which are more likely to occur 
on the OS and applications HD.
Now that your working data environment is set up, it is time to think about how to organize your 
documents and sessions. Keep your projects very well organized on your HD. Do not allow two or more 
projects to share the same audio files folder. This could be dangerous during a backup session, since you 
might be intending to back up a certain project without realizing that you forgot to back up its precious 
and irreplaceable audio files. Keep your documents in general and your sessions in particular divided by 
client, sessions, and projects. If you use different software to work on different projects, you might have 
separate folders dedicated to separate applications.
In Figure 3.30 you see the basic organization of a data storage system for a generic project stu­
dio. The main idea behind this organization is to have the documents separated from any other files or 

142
CHAPTER 3  Intermediate Sequencing Techniques
applications. Inside the “Documents” folder you will have other folders containing files categorized by 
generic types of applications (e.g., music, graphic, Internet, utilities). Inside each generic type you will 
organize your project first by clients and then for each client by session. This way you will always be 
sure of what you are backing up and where to find your sessions. Once you have your HD organized, 
you are ready to start backing up your data.
3.11.1  Backup and Archive
Even though the terms backup and archive are sometimes used to mean the same thing, their applica­
tions are fairly different. Backup refers to the copying of files related to a particular project and executed 
every so often during its realization. An archive session, on the other hand, is usually done at the end of 
the project, after all work is completed, and involves the copying of the files and the deletion of the orig­
inal work project. In other words you back up your files at the end of every day of composition, sequenc­
ing, and recording, for the length of the entire production. At the end of the production, when the project 
is over and you have delivered the final material to your client, you can archive it, meaning copy the ses­
sion to a different medium and delete it from the working HD to have free space to start a new project. 
Both actions can be done using different types of media. Most likely you will use a quick medium, such 
FIGURE 3.30
Hard disk, folders, and file data organization.

143
3.11  Safeguard Your Work: Back it Up!
as a separate HD, for your fast, overnight backups, and CD-R/RW, DVD  / R/RW, or tape for safer 
backups and archiving.
While the backup and archive session can be done manually by simply copying files from your work­
ing HD (HD2 in our example) to the backup media, there are software applications that can considerably 
facilitate this task. There are two main types of software used for backup: the synchronization utilities 
and the so-called backup utilities. Let’s look at how the two systems work and what their differences are.
A synchronization utility is a simple application that compares the content of two folders, volumes, 
or directories and always keeps it updated to the latest version of each file by copying only the files that 
were modified since the last synchronization. It basically mirrors the content of the two selected loca­
tions so that they are constantly updated. This approach is great for quick, overnight backup sessions 
because it is fast and accurate, and always leaves two versions of your projects, each of which can be 
accessed at any time. What I recommend is a third, external HD (let’s call it HD3) as your overnight 
backup (Figure 3.30). Every day, after working on a project, spend 30 minutes synchronizing your work­
ing HD (HD2) with your backup HD (HD3). This way you know at any moment that you can count on 
a set of files you can access without problems. You can also use the mirrored copy (HD3) to bring the 
project to an external studio to work on it and then come back to your studio and resynchronize it with 
your working HD. You can save the settings related to a particular synchronization setup (such as the 
two locations that need to be synchronized, and eventual filters) in a file called “Set”. You should create 
a separate set for each project so that when you want to synchronize the latest changes made to Project 
X, for example, the only thing you have to do is run the “Set Project X”. The computer will compare 
the two folders and update each location with the latest file versions. This system is very flexible and it 
works really well.
A backup utility works in a slightly different way. When you start a backup application, you have to 
create a backup set in which you specify the directory, folder, or volume you want to back up, the type 
of media where the files will be copied, whether you want the files to be compressed, and where you 
want to store the set itself. One of the main differences between this approach and the aforementioned 
synchronization system is that the backed-up files will not be directly accessible unless you restore them 
through the same application you used to back them up. While this system is efficient, you will some­
times be wondering where in fact your files are, and in case of a system failure you have to rely on the 
auto-recovery capabilities of the backup software. The backup approach has some advantages, such as 
the capability of compressing the data, sometimes up to 50% (depending on the nature of the files), its 
reliability, and the ability to use several types of media, including tape, optical, and even remote servers. 
This type of application can also serve as an archiving system.
After your project is finished, mastered, and handed to the client it is time to archive it, meaning 
make a copy and delete it from the working HD and all the other places where you backed it up. Usually 
you archive projects that you are sure won’t need any rework for a certain amount of time (this depends 
on your preferences and work cycle). I usually recommend archiving projects you are not going to work 
on for at least three months. To archive your project you can select either backup or synchronization 
software. I usually use backup software with the compression option to save some media space. Delete 
all the files from the working HDs after making your archive copy. When you next need to work on the 
project you archived, you will have to restore it to an available working HD.
Another important aspect of the backup/archive system is what I call the “3 rule”. This involves 
always backing up and/or archiving to at least three different media and having one copy in a different 
physical location from the other two. For each major project, do quick, overnight backups on at least two 
separate HDs, and if possible use a third option, such as an optical medium (e.g., CD, DVD, or Blu-Ray 
DVD). The same technique needs to be applied in the case of an archive session. For example, set your 
software to archive to tape and two DVDs simultaneously. Keep one of the two DVD copies in a location 

144
CHAPTER 3  Intermediate Sequencing Techniques
other than your studio. An option that has become more and more common in recent years is the storage 
area network solution, which can be applied to both Intranet and Internet servers. It is based on the idea 
of storing backup copies of your files to storage units connected on the same computer network. For a 
small session you can use fairly inexpensive hardware, such as an older computer with multiple HDs and 
an Ethernet connector. For a large session you will have to set up a more expensive, dedicated network 
based on a fast server. In any case, backup software and synchronization software are able to use the net­
worked devices as viable storage units to set up your backup sessions.
3.11.2  How to Calculate the Size of a Session
No matter which system and software you will be using, the bottom line is not to let time pass without 
protecting your data from fatal crashes. Take any steps and any necessary action to prevent problems 
that could stop your creative flow. When you are ready to back up a project, it is important to know how 
much HD space you will need and whether the session will fit on a single medium or require multiple 
media. While the MIDI data and the sequence file do not take up much space (usually less than 700 KB), 
the project audio files take up the majority of HD space. Therefore, being able to estimate the size of a 
project is crucial.
In Table 3.4 you see a quick summary of the space taken up by one minute of mono audio at different 
sampling frequencies and bit resolutions.
Knowing the number of tracks, the number of takes, the sampling frequency, the bit resolution, and 
the length of the project, you will be able to estimate the overall size of a session. For example, to cal­
culate the size of the audio files related to a project that features 16 tracks recorded at 96 kHz and 24-bit 
resolution for a song that is six minutes long, you would use the following formula:
Number of tracks
Length of project, in minutes
Size, i
(
)
(
)


n MB, for  minute of mono audio
1
(
)
In our example we would have
16
6
17 28
1659



.
MB
Regarding the media you can use for your backup and archiving sessions, there are two issues that 
need to be considered: the reliability and duration of the medium, and the maximum size of the medium. 
The reliability and duration depend largely on the type of storage technology used. Magnetic media, 
such as HDs, removable disks, and tapes, are generally pretty reliable (especially tape backup systems), 
and they offer large capacity. Optical media, such as CD and DVD, usually are good for smaller projects 
(especially CDs), but they are best suited for short-term storage and data exchange. In Table 3.5 is a 
summary of the most used media and their specifications.
Table 3.4  Size for 1 Minute of Mono Audio at Different Sample Rates and Bit Resolutions.
Sample Rate/Bit Resolution
Size for 1 minute of Mono Audio
192 kHz/24 bit
34.56 MB
96 kHz/24 bit
17.28 MB
44.1 kHz/24 bit
  7.93 MB
44.1 kHz/16 bit
  5.3 MB

145
3.12  Summary
3.12  SUMMARY
In this chapter we covered some of the intermediate sequencing techniques that can improve your 
sequencing skills. While a basic quantization of your MIDI tracks can very often take the life and groove 
out of your parts, a more advanced quantization can fix the rhythmic errors and at the same time leave 
the original groove intact. Make use of quantization parameters such as sensitivity, strength, and swing 
to reduce the mechanical effect introduced by the quantization action. The “groove quantize” option ena­
bles you to apply different rhythmic styles and techniques to your parts. For example, you can have a 
straight 8th notes drum part converted into an 8th notes shuffle groove either by applying a preset groove 
or by creating your own styles.
Track layering can be applied to MIDI or audio tracks. This technique can be used creatively in sev­
eral ways. By combining two or more MIDI tracks you can create new sonorities without having to play 
the same part several times or without having to copy the same part over and over. Simply assign the 
output of a track to several devices and MIDI channels at the same time. This technique is particularly 
effective in creating new patches and in complementing existing sounds. A similar layering technique 
can be used with MIDI and audio tracks to improve sampled acoustic sounds by adding one or more live 
instruments to the MIDI tracks.
Alternative MIDI track editors and MIDI controllers can improve your sequencing skills and, 
if used in the right way, inspire your creative process. The drum editor, for example, not only can be 
used to accurately edit rhythmic parts but can also become part of your sequence as a matrix editor and 
arpeggiator. Alternative controllers can be used to sequence parts in a more natural way. While it can be 
hard to render a woodwind solo from a keyboard controller, you can achieve a much more realistic result 
with a wind controller. The same principle applies in the case of MIDI drums/pads and guitar/bass-to-
MIDI converters.
Complex tempo and meter changes can be automated and programmed through the use of a con­
ductor track (or tempo track). Using either the graphic or list editors you can insert single events, such 
as tempo changes and meter changes, or continuous variations to create smooth accelerandos and ral­
lentandos. You can also use the tempo map in creative ways. For example, you can insert subtle tempo 
changes to recreate the real feel of a live ensemble or slightly increase the tempo in certain sections of 
the composition to increase the overall rhythmic groove. With the “Tap-Tempo” function (also called 
“Tempo Interpreter”), you can conduct your sequencer using your controller as a virtual conducting 
baton. Tempo changes can be a great tool for improving the groove of your productions, but they can 
also create problems when used in conjunction with audio tracks. While MIDI data are very flexible and 
can automatically adjust to the variation in tempo programmed in the conductor track, audio data need 
Table 3.5  Backup and Archive Media Options and Main Features.
Medium
Technology
Size
Comments
CD-R/RW
Optical
700 MB
Good for small and temporary backups
DVD-R/RW
Optical
4.7–9.4 GB
Good for mid-sized backups
Blue-Ray DVD
Optical
25–50 GB
Good for large backups
Tape
Magnetic
Up to 1000 GB
Good for big backups; reliable; not user-friendly
HD
Magnetic
Up to 4000 GB
Fast; good for big backups; not very reliable for long-term 
storage

146
CHAPTER 3  Intermediate Sequencing Techniques
to be time-stretched to follow the tempo changes. Time-stretching requires the computer to calculate and 
adjust the samples of the audio files according to preprogrammed algorithms present in your sequencer. 
The sequencer will construct the new audio file according to the original tempo of the file and the desti­
nation tempo. In most sequencers this action can also be done in real time by simply clicking and drag­
ging one side of the audio region (e.g., DP and PT).
By using different synchronization protocols we can have different devices and machines running 
together in perfect sync. While linear machines (meaning tape-based devices, such as multitrack tape 
recorders and video machines) can use a protocol called SMPTE to achieve synchronization, non­
linear machines (sequencers, drum machines, HD recorders, etc.) can be synchronized using either 
MIDI Clock or MIDI Time Code. These two protocols, which both use the MIDI network to send 
synchronization messages, require one device to be set up as master and the others as slaves. In the 
case of MC (which is a tempo-based protocol) the same message is sent 24 times for every quarter 
note. Through the use of an additional message, called Song Position Pointer (SPP), the master is also 
able to describe its position in the sequence by counting the number of 16th notes that passed since 
the beginning of the project. MTC is a real-time-based protocol that derives from the original SMPTE 
set. They both describe the passing of time in hours:minutes:seconds:frames. MIDI messages are sent 
each quarter-frame to describe the exact position of the master device. As soon as the slave devices 
receive the starting position, they will move to it and start playing, continuing as long as they receive 
MTC messages. Frame rates for both SMPTE and MTC include 24, 25, 29.97, and 30 fps. Two extra 
rates, called 29.97 drop and 30 drop, exist. While the latter is rarely used, the former is utilized to 
make up for the discrepancy between the video NTSC frame rate of 29.97 and the 30 fps rate used by 
broadcasters.
Backup and archiving sessions are crucial to the safeguarding of your precious creative work. 
Backup sessions are conducted during the realization of a project as quick, overnight copies after 
each working session. When a project is finished and you don’t plan to work on it again for a long 
time, you will archive it, meaning make a copy on a separate medium (CD, DVD, etc.) and delete it 
from the original working HD. Software that can help in the process of backing up and archiving can 
be divided into synchronization applications and backup/archiving applications. The former keep a 
perfect copy of the current project on the main working HD and another medium (usually another 
HD), and they keep track of the latest modified files in both locations. The latter create a copy in a 
proprietary format (usually compressed up to 50%) that can be accessed only by restoring it onto 
an HD via the same software used to back it up. Media that can be used to back up and archive are 
HD, CD-R/RW, DVD  / R/RW, tapes, and Blu-Ray DVD. No matter which system or media you 
use, remember to back up often and to keep several backup/archive copies of your most important 
projects.
3.13  EXERCISES
Exercise 3.1
Set up and record a sequence with the following features:
a.	 Fourteen MIDI tracks
b.	 Instrumentation: drums, bass, guitar, keyboards, percussion, five horns, strings
c.	 Tempo of your choice.

147
3.13  Exercises
Exercise 3.2
Using the sequence created in Exercise 3.1, quantize each track with a different groove and quantization set­
ting. Take note of the parameters you chose for each track and compare them by listening first to each track 
individually and later to multiple tracks together.
Answer the following questions:
a.	 How does the “swing” parameter affect the quantized material?
b.	 How does the “sensitivity” parameter affect the quantized material?
c.	 How does the “strength” parameter affect the quantized material?
d.	 With which settings do you think your tracks improved the most?
Exercise 3.3
Using the sequence produced in Exercise 3.1, create MIDI layers for at least four tracks so that their outputs 
are assigned to at least two different MIDI channels and/or cables. Try to create some interesting combinations 
in terms of sound texture.
Exercise 3.4
Use the drum editor to create several patterns that will be added to your sequence.
Exercise 3.5
Add the following tempo changes to your sequence:
a.	 Bar 1, 120 BPM
b.	 Bar 9, 100 BPM
c.	 From bar 17 to bar 19, accelerando from 100 to 130 BPM
d.	 Bar 24, 133 BPM
e.	 From bar 28 to bar 32, rallentando from 133 to 90 BPM.
Exercise 3.6
Create four audio tracks and using some of the loops. From the website take four loops and insert them in 
your sequence at the following locations. Make sure to time-stretch the audio material so that it fits with the 
tempo changes programmed in Exercise 3.5. Insert the loops according to the following directions:
a.	 First loop at bar 1
b.	 Second loop at bar 9
c.	 Third loop at bar 19
d.	 Fourth loop at bar 24.

148
CHAPTER 3  Intermediate Sequencing Techniques
Exercise 3.7
List and explain the main differences between the two synchronization protocols: MTC and MC.
Exercise 3.8
Calculate the space, in megabytes, required to store the audio files of the following two projects:
a.	 A 7-minute project recorded at 196 kHz and 24-bit resolution with 24 mono tracks
b.	 A 3-minute project recorded at 44.1 kHz and 24-bit resolution with 16 mono tracks.
Exercise 3.9
Make a backup of the sequence you created in Exercise 3.1 and its audio files.

149
Creative Sequencing Techniques for Music Production.
© 2011, 2005 Andrea Pejrolo. Published by Elsevier Ltd. All rights reserved.
CHAPTER
Advanced Sequencing 
Techniques
4
4.1  INTRODUCTION
In the first half of this book we learned information that ranged from how to set up a MIDI project stu­
dio to fairly complex synchronization settings, from basic session creation and MIDI track recording to 
sophisticated tempo changes and groove quantization techniques. While all this information is meant to 
improve sequencing skills and enhance your final productions, it focused mainly on predefined settings 
and options, leaving little space for personal customization. The goal of this chapter is to explore some 
of the advanced techniques offered by the four main digital audio workstations (DAWs) analyzed [Pro 
Tools (PT), Digital Performer (DP), Cubase (CU), and Logic Pro (LP)] in order to customize as much 
as we can their quantization, editing, effects, and automation parameters. As you know, what transforms 
a skilled composer into an original and creative one is a unique sound and unique melodies, harmonies, 
and orchestration style. The same can be said for the modern MIDI composer. The more we can bend the 
virtual orchestra to follow our own style (and not what the computer and the machine dictate), the more 
our production will sound fresh, original, and innovative. This is where the customization of quantiza­
tion parameters and the use of advanced tools play an important role in the creative process. In this chap­
ter we will learn how to turn a good-sounding sequence into our great-sounding sequence! Let’s move 
on to the advanced sequencing techniques.
4.2  ADVANCED QUANTIZATION TECHNIQUES
So far we have been using predefined templates to quantize the MIDI tracks we recorded. Although 
parameters such as swing, sensitivity, and strength are a major improvement over the straight quantiza­
tion we learned in Chapter 2, they are still fairly limited in terms of complete quantization freedom. 
What if we want certain MIDI parts to be able to adapt to the groove that we played on a different MIDI 
track, or what if we want to have a MIDI track follow the exact groove recorded on a separate audio 
track? Even better, wouldn’t it be wonderful if we could control the quantization of audio tracks as we 
do on MIDI? All these options are available in several sequencers, and they represent one of the most 
exciting aspects of sequencing.
4.2.1  Custom Groove Creation
The grooves provided with some of the sequencers we have analyzed so far are basic starting points on 
which we can build a groove library that best fits our compositional style. We can create custom grooves 
from three different sources. First, we can edit the existing ones, using them as templates to make small 
or more drastic variations of the original. This allows us to tailor the quantization settings to a particular 

150
CHAPTER 4  Advanced Sequencing Techniques
style, genre, playing style, or project. In addition, we can create completely new grooves from scratch, 
starting from prerecorded MIDI or audio tracks. This feature opens up an incredible number of possi­
bilities. Imagine importing a drum track from your favorite drummer, extrapolating the intrinsic groove, 
and morphing it into any of your MIDI drum tracks. The same can be done from any MIDI track used 
as a source. The principle on which this technique is based is simple: by analyzing the MIDI notes on a 
MIDI track or the transients present on an audio file, the computer is able to extrapolate key points that 
the new groove will use as a reference for the new quantization algorithm. Let’s look at how this tech­
nique works and how it can be used in specific applications.
4.2.2  Editing a Groove
We have already learned how to use the groove quantization feature in DP. Now let’s analyze how to 
edit preexisting grooves; this will make it easier to understand the process involved in creating custom 
grooves from scratch later.
In order to edit one of the preexisting DNA grooves, simply select the “Groove Quantize” option 
found in the Region menu. Choose the category and groove you want to customize and click on the 
“Edit” button. This will open the Groove Editor window, where you can change the three main param­
eters of a groove: timing of the events, velocity, and length, for each rhythmic subdivision (Figure 4.1). 
You can change each parameter for each grid value (e.g., 8th note, 16th note) specified in the original 
settings of the groove you are editing, independent of the others. Notice that this is different from the 
basic parameters available from the Groove Quantize window, which allow you to change timing, veloc­
ity, and duration only for the overall groove and not for each individual grid value.
The Groove Editor gives incredible flexibility in terms of timing, velocity, and duration of each 
event. For each grid point, you can change the timing by clicking and dragging left (meaning a rushing, 
or ahead-of-the-beat, feel) and right (a laidback, or behind-the-beat, feel). The velocity of each grid point 
FIGURE 4.1
Groove Editor in DP.

151
4.2  Advanced Quantization Techniques
can be set either via the mouse (click and drag on the velocity arrow on top of each point) or by insert­
ing a numeric value in the respective field. The same techniques can be used for the duration parameter 
(click and drag left to reduce the duration value or right to increase it). Once you are done with the 
changes, click the “OK” button to save the changes or “Cancel” to discard them.
While using an already available groove to create a new one is a good starting point, you can go even 
farther by creating custom grooves based on preexisting MIDI parts in your sequence. This is a very 
flexible tool not only to increase the groove libraries and quantization options available for your produc­
tions but also, for example, to have several tracks to match perfectly the rhythmic feel that is embedded 
in the target MIDI track.
In DP, in order to create a groove from a preexisting part, first select the region you want to use as a 
template for the new groove from the track list window (usually two or four bars are a good starting point). 
Select the “Create Groove” option from the Region menu. You can create a new groove folder to keep your 
templates organized or select a preexisting one. Next, select the basic grid values of the groove (8th notes, 
16th notes, etc.) and the meter on which the grid is based, and insert a name for the new groove. When you 
click the “OK” button, a new groove will be inserted based on the rhythmic subdivision and MIDI events 
that were present in the region you selected. From now on that groove will be available from the regular 
Groove Quantize menu, and it can be used and applied to any region you want to quantize.
The same principle can be applied to your parts in CU. Cubase (and, as we will see later, LP and PT) 
can generate quantization grooves not only from MIDI parts but also from audio files, making it extremely 
interesting to create custom quantization settings based on your favorite audio material. Let’s start with 
the procedure used to create a quantization groove from a MIDI part. First, select one of the parts in a 
MIDI track on which you want to base your quantization setting. This could be a simple part on which 
you recorded a single-note rhythmic groove, or it could be a more complex part that you imported from 
another sequence or from a MIDI file created with another sequencer. Once you have selected the part, 
simply navigate to MIDI  Advanced Quantize  Part to Groove and a new groove will be automati­
cally added to your quantization groove list (accessible from MIDI  Quantize Setup). The newly created 
groove will take the name of the selected part, so make sure to give it a representative name before creat­
ing the groove. To rename a part, you can use the “Event Infoline” at the top of the Arrange window. From 
now on when you select a part to quantize, you will have the option to apply the groove you just created.
In LP, to create a new groove from a specific part, click and select the source part first, then click 
on the quantization value area in the Inspector area to the left of the Arrange window and select “Make 
Groove Template”. As in the case of CU, a new groove with the name of the part it was created from 
will automatically be added to your quantization value list and it is available right away for you to use on 
any MIDI part you want to quantize.
In PT you can create MIDI-based groove templates using Beat Detective (BD). With BD you can 
achieve several tasks, all related to a selected audio region. You can, for example, find out the tempo of 
a loop, or extrapolate the groove of an audio loop, and save it as a groove template to use on any MIDI 
or audio region. In order to extrapolate the groove from MIDI parts, first you have to select the MIDI 
region you want the computer to analyze. Make sure the region selected is a complete set of bars, such as 
two or four complete measures. Next, you have to define the selection in the BD window. To do so, open 
the BD window by selecting “Beat Detective” from the Event menu (Figure 4.2). Change the Start/End 
and the Time signature parameters according to the material you selected, and then click on the “Capture 
Selection” option. Once BD has all the information about the MIDI region, in the BD window, choose the 
“Groove Template Extraction” mode, click the “Analyze” button, and adjust the “Sensitivity” slider to have 
all the MIDI data that make up the groove identified. PT will insert so-called beat triggers (same as CU hit­
points) according to the MIDI events of the selected region. You can change the sensitivity of BD by using 
the Sensitivity slider. The detected beat triggers can easily be customized (moved, deleted, and inserted) 

152
CHAPTER 4  Advanced Sequencing Techniques
using the standard tool palette. The grabber tool allows you to move them (click and drag), to delete a spe­
cific trigger (press the option key and click on the trigger), or add a new trigger (double-click). To extrapo­
late the groove based on the beat triggers generated by BD, simply click on the “Extract” button and insert 
a comment about the groove if you need one. The newly created groove template can be used directly in 
the current project (“Save to Groove Clipboard” option) or saved to disk (“Save to Disk” option). From 
now on you can use the extracted groove template to quantize any other MIDI or audio region.
4.2.3  Audio to MIDI Groove Creation
While the option I just described is interesting and definitely has practical applications, it seems a little 
bit limiting to be restricted to grooves based on MIDI parts, especially since the main point here is to 
“humanize” your MIDI parts as much as possible. We can clone a groove from an audio file and transfer 
it as a quantization groove set that will be available for any quantization operation you need. While the 
procedure is a bit more elaborate than the one we just saw, it is definitely worth learning.
In CU the first step is to import the audio loop or audio file from which you want the groove trans­
planted. Once you have the audio file in your sequence, double-click onto open the sample editor. In this 
window we are going to use Hitpoints to extrapolate the groove of the audio region selected. To do so, click 
on the “Hitpoints” tab located to the left of the waveform editor (Figure 4.3) and use the “Sensitivity” slider 
to start inserting hitpoints.
If you move it toward the left, the number of hitpoints will decrease; if you move it to the right, the 
number will increase. To effectively create a groove that captures the rhythmic essence of the loop, you 
should have hitpoints only on the major transients. Too many points will clutter the groove, making it too 
FIGURE 4.2
Beat Detective in PT.

153
4.2  Advanced Quantization Techniques
complicated; not enough points will leave you with a groove that will not be effective when used in the 
quantization process. Through the “Use” drop-down menu you can specify to have hitpoints dropped at 
regular intervals (such as 16th or 8th notes). The hitpoints inserted can be used for several operations, 
including one that allows us to extrapolate the groove from the audio file. After you have made the final 
decision on the number of hitpoints to keep, it is time to transplant the groove to a quantization set. In 
the Hitpoints tab in the Sample Editor select “Create Groove”. The computer will build a new custom­
ized quantization set based on the audio loop and hitpoints you marked. The quantization grooves (includ­
ing the one you just created) are going to be available from the “Quantize Setup” submenu in the MIDI 
menu. The groove will take the name of the audio part it was built on, so remember to rename the audio 
part in order to have a clear description of the groove, using the “Event Infoline” at the top of the Arrange 
window. This opens up an incredible number of options and creative opportunities for improving your 
productions.
To create a groove from an audio file in DP you need to have DP analyze the “Beats” (a beat usually 
corresponds to a specific transient of the waveform that is being analyzed) inside the audio file. The first 
step we need to take in identifying the beats inside a soundbite is to find the soundbite’s tempo. This is 
a procedure that I described earlier, and although the operation is not strictly necessary to identify the 
beats inside a soundbite, it will definitely increase the accuracy of your final identification, so I recom­
mend including it in your to-do list. Here’s a review of how to find the tempo of a soundbite:
l	 Select an exact number of bars (trim the soundbite so that it is exactly two, four, or eight bars in 
length).
FIGURE 4.3
Hitpoint mode in CU.

154
CHAPTER 4  Advanced Sequencing Techniques
l	 Select the soundbite, then choose Audio  Soundbite Tempo  Set Soundbite Tempo.
l	 In the Set Soundbite Tempo window, input the Length according to how many beats and ticks you 
selected (for example, if you selected a two-bar loop in 4/4, input 8/000).
l	 DP will automatically show the tempo of the soundbite in the lower right corner of the Set Soundbite 
Tempo window, and it will stamp it into the header of the audio file so that this particular soundbite 
will carry the tempo information across multiple DP projects.
Once DP is aware of the soundbite’s tempo, you can move on and start identifying the beats. First, 
select the soundbite. Open the Waveform Editor by selecting Audio  Edit in Waveform Editor. To ana­
lyze the transients of the soundbite and identify its beats, select the Beats tab at the top of the Waveform 
Editor window.
You should now see a series of vertical blue bars running across the selected soundbite (Figure 4.4). 
These lines identify the transients/beats that DP automatically detected. The green handle linked to each 
beat represents the relative velocity/volume of each beat.
To change that, simply click and drag up or down with the mouse on the corresponding handle. 
(Keep in mind that this will not alter the actual volume of the transient in the waveform.) You can 
quickly adjust the threshold used by DP to identify the beats by selecting Adjust Beat Sensitivity from 
the Beats drop-down menu in the top area of the window. The Adjust Beat Sensitivity slider (Figure 4.4) 
can be used to fine-tune DP’s beat-detection engine for the selected range of the soundbite. Lower values 
will instruct DP to mark as beats only transients with higher amplitudes, while higher values will allow 
DP to select even transients with lower amplitudes. While the slider gives you control over the general 
transient/beats ratio in most situations, you will need to manually remove or insert beats to achieve a 
higher level of accuracy, but DP makes it easy to manually change the beats detected. For example, you 
FIGURE 4.4
Beats editing in a soundbite in DP.

155
4.2  Advanced Quantization Techniques
can suspend or reactivate a beat by simply Option-clicking on its handle at the top of the window. To 
completely delete a beat or a series of beats inside a selected range, use the Delete Beat and Delete Beats 
in Selection functions, respectively, found in the Beats drop-down menu. To move a beat and adjust its 
position relative to a transient, simply click and drag it left or right.
Once you’re happy with your beats assignment, you can close the Waveform Editor window. At this 
point, DP knows everything about your soundbite, its tempo and its beats, and you can use this information 
to accomplish a series of tasks that take full advantage of the new audio engine. To create a new groove 
template from a soundbite that has been analyzed, simply select the soundbite and choose Region  Create 
Groove. At this point, the procedure is similar to that used for groove creation from a MIDI part. You can 
create a new groove folder, category and subcategory by navigating though the Create Groove window. 
The new template created will be accessible through the usual Region  Groove Quantize menu.
In LP you can apply to a MIDI part a groove extrapolated from an audio file. While the procedure 
is similar to the one learned for CU, there are a few differences worth pointing out. To create a groove 
from an audio region, first import the loop or audio file on which you want your new groove to be based 
(see Chapter 2 to review this procedure). Trim the loop as necessary, and make sure the beginning of 
the sequence/object (part) corresponds to the beginning of the audio region. If you want to trim the 
audio region, select an even number of bars: usually two measures work best for this kind of opera­
tion. After your audio is ready, open the waveform editor (double-click on the audio region) and navi­
gate to Factory  Audio to MIDI Groove Template. LP will show the audio editor along with a special 
“Quantization” window for audio tracks (Figure 4.5).
FIGURE 4.5
Audio to MIDI Groove Template window in LP.

156
CHAPTER 4  Advanced Sequencing Techniques
To help the computer analyze and extrapolate the groove embedded in the audio loop, we have to 
provide a series of parameters regarding its rhythmic features, basic quantization, etc. A brief description 
of the parameters available in the Audio Quantization window is shown in Table 4.1.
In the drop-down menu (in the Quantize floating window) you can specify the type of audio mate­
rial you are processing by choosing among a series of presets, such as drums (Slow, Mid, Fast), gui­
tar (Chords, Picked, Single, Distorted), pop, and classical. Even though these categories are pretty 
generic, try to choose the one that best describes the type of audio material you are working on. In the 
main audio editor window you can visually check how the changes applied to the parameters in the 
Quantization window affect the final quantization groove. The three separate lines named “Audio”, 
“Quantize”, and “Result” describe the hitpoints identified by the Logic engine, the hitpoints specified by 
you in the Basis Quantization field, and the final result, meaning the actual points that will be used by 
LP in the new quantization set, respectively. You can preview the result by clicking on the “Try” button. 
This will temporarily apply the groove to all the selected parts in the sequence. If you are satisfied, click 
on the “Use” button; otherwise, you can keep changing the parameters or cancel the entire operation. 
From now on the new quantization set will be available in every window through the quantization list.
In PT audio to groove creation is available through an audio engine called Beat Detective (BD). To 
extrapolate the groove from an audio loop, first you have to select the region of the loop you want the 
Table 4.1  Audio to Groove Parameters in Logic Pro.
Parameter
Description
Comments
Preset
Allows you to pick the best algorithm 
for the audio material selected
Granulation
Indicates the time span of louder 
events of the audio material
Used by LP to determine velocity points
Values between 20 and 200 ms are usually a 
good starting point
Attack Range
Determines the attack time of the 
phrasing of the audio material
For drum part choose a short attack time (i.e., 
20 ms)
For instruments with long attack times such as 
pads or strings use higher values (i.e., 40 ms)
Smooth Release
Used to process audio materials with 
long releases such as audio files 
with long reverbs
Usually use values between 0% and 6% (0% 
for very dry material and 6% for moderately 
reverberated audio parts)
For audio parts with a high amount of 
reverberation use values higher than 6%
Velocity Threshold
Determines the threshold below which 
the audio material is ignored
Usually leave it at 1 unless you are processing a 
very busy rhythmic part
Increase the value if the audio part has a 
noticeable background noise
Basis Quantization
Allows you to insert “fake” hitpoints 
that are not detected by the 
computer based on loud transients
Used mainly to have more quantization points 
than the actual audio material features
Time Correction
Allows you to compensate for delays 
embedded in your MIDI setup (the 
time between a MIDI message sent 
and the actual sound produced by 
the sound generator of a MIDI device)
Use values between 0 and 215 ms

157
4.2  Advanced Quantization Techniques
computer to analyze. Make sure the region selected is a complete set of bars, such as two or four com­
plete measures. Next you have to define the selection in the BD window. To do so, open the BD window 
by selecting “Beat Detective” from the Event menu (Figure 4.2).
Change the Start/End and the Time signature parameters according to the audio material you selected, 
and then click on the “Capture Selection” option. Once BD has all the information about the loop, you 
have to let the computer analyze and detect the transient of the audio material. In the BD window, choose 
the “Groove Template Extraction” mode, and select the right type of algorithm used to detect the tran­
sient: High Emphasis works best with high-frequency material, such as hi-hat and cymbals, while Low 
Emphasis works best with low-frequency material, such as toms and bass drums. When you click on the 
“Analyze” button, PT will insert beat triggers according to the transients of the waveform analyzed. You 
can change the sensitivity of BD using the Sensitivity slider. The detected beat triggers can easily be cus­
tomized (moved, deleted, and inserted) using the standard tool palette. The grabber tool allows you to 
move them (click and drag), to delete a specific trigger (press the Option key and click on the trigger), or 
add a new trigger (double-click). To extrapolate the groove based on the beat triggers generated by BD, 
simply click on the “Extract” button and insert a comment about the groove if you need one. The newly 
created groove template can be used directly in the current project (“Save to Groove Clipboard” option) or 
saved to disk (“Save to Disk” option). From now on you can use the extracted groove template to quantize 
any other MIDI or audio region.
As you can see, with the advanced quantization techniques we just learned, there are almost no limi­
tations to what can be achieved in terms of rhythmic flexibility and precision. It is very important that 
you experiment with creating grooves of your own and use them as quantization templates for your 
projects. Start with generating short MIDI parts using a one-note rhythmic pattern, such as a swing feel 
you particularly like (MIDI drums kits and MIDI pads for this option are the ideal solution). Then use 
the MIDI part just created to generate a quantization groove and try to apply it to other MIDI parts/
tracks. If your sequencer allows it, do the same starting from an audio loop. Start with very simple rhyth­
mic grooves and audio material to get a better grasp of all the parameters involved in the process. When 
you feel comfortable, move to more complex grooves and patterns. The goal is to create a personal 
library of quantization grooves that you will use in your projects and that will contribute to give a unique 
signature and sonority to your productions.
4.2.4  Audio Quantization
If you have read carefully and practiced the exercises suggested at the end of each chapter, I am sure you 
will start thinking about how nice it would be if you could quantize not only MIDI parts but also audio 
regions. By now you may be so addicted to quantization that you would use it for every single aspect of 
your production: so why not for audio parts? Well, you will be happy to learn that there are possibilities 
for quantizing audio regions of your sequences, and that is exactly what we are going to learn in the fol­
lowing section.
Before learning the exact techniques related to each sequencer, we have to understand what the quan­
tization of an audio region involves and how it differs from the procedure we learned in the case of MIDI 
parts. While the quantization of MIDI events is fairly simple for the sequencer since MIDI tracks hold 
only a digital description of the performance you executed on a controller, digital audio tracks contain a 
continuous flow of data that constitute the waveform. From a regular waveform it is impossible for the 
computer to distinguish between discrete events, such as a between a single snare and a single bass drum 
hit. While in MIDI those hits would be represented as two separate events, in an audio file they would 
be part of the same region, and therefore the computer would not be able to treat them as separate enti­
ties. Things get even more complicated when the audio material we are trying to quantize is a complex 

158
CHAPTER 4  Advanced Sequencing Techniques
mix of several instruments. Fortunately, there are several ways to help the sequencer to itemize an audio 
region. The main goal in quantizing an audio part is to divide it into slices that can be treated, and there­
fore later quantized, as if they were separate and independent events. This technique works best with 
drum parts or rhythmic loops in general. Since the procedure is based on the detection of fast transients 
(more typical of percussive parts) to determine where the slices need to be placed, it doesn’t give the 
best results with slow-attack parts, such as strings or pads.
There are specific dedicated applications that allow you to edit an audio file and automatically detect 
and create slices based on transients present in a waveform. One of the most used applications for this 
purpose is “Recycle”, by Propeller heads. This application can slice a loop and save it in a proprietary 
format (.rex2) that can be imported by the majority of audio sequencers. What makes this format incred­
ibly versatile is that each created slice can be treated as an independent event inside the original loop, 
making it easy not only to quantize the audio region but also to anchor the slices to bars and beats (as in 
the case of a MIDI track). This will allow you to have the audio region follow the tempo of the sequence 
in real time, without having to use the time-stretching feature of your sequencer. The Recycle software 
will not be covered here because it is outside the scope of this book. However, there are built-in tools in 
each DAW that resemble the Recycle engine and allow us to achieve almost identical results.
In DP we can quantize audio in a couple of different ways. There is a manual technique that is as 
effective as the procedure found in Recycle which is based on slicing an audio region exactly as described 
in the previous paragraph. To divide an audio region into slices, you have to import the audio loop first 
and find its tempo, as explained in Chapter 2. Also make sure the tempo of the sequence matches the 
tempo of the loop, which is crucial in order to divide the loop into rhythmically accurate slices. Open the 
graphic audio editor and select the grid value (called “Unit” in DP) according to the rhythmic resolution 
you want the slice to have. You will find the Unit parameter in the upper right corner of the graphic editor 
window. Make sure the grid mode is enabled by clicking the Unit checkbox. Usually you want to choose 
the smallest rhythmic subdivision the loop is based on, but the way you handle this parameter depends on 
a number of factors, such as which rhythmic elements and positions are more important inside the loop. 
To slice the loop, select the scissor tool from the tool palette (if the tool palette is not open, press Shift-O 
to open it). With this tool you can click on any area of the loop and insert a cut that will be inserted only 
according to the grid value you selected earlier. You can even click and drag across the loop to insert 
slices for every grid value. For example, if you selected a 16th note grid, by clicking and dragging you 
will insert slices every 16th note (Figure 4.6a, b). Keep in mind that this slicing technique is grid based 
and not transient based.
The most interesting aspect of generating several events from audio material is that the created slices 
(or events) can now be treated as individual entities and therefore quantized separately. You may remem­
ber that when we discussed the quantization filters for DP in Chapter 3, we learned that in the options 
available in the “Quantization” window we can decide which elements of a selected region we want to 
quantize. So far, we have always chosen to quantize the attack of MIDI data. In fact, as we just learned, 
now we can expand the quantization feature to audio data as well and to each single slice of an audio 
region. The technique of slicing an audio file has two major applications. The first is to quantize audio 
files that need some rhythmic makeover. For example, let’s say you recorded a rhythmic guitar part on 
an audio track and you are not satisfied with the rhythmic accuracy of your performance. You can slice 
the audio file recorded (this time you will have to do it manually with the grid set to “Off”, otherwise 
your slices will be inserted in the wrong place) and select the slices you want to quantize. Then simply 
select the “Quantize” option from the Region menu, and make sure that from the quantization window 
you select to quantize “Soundbites”. This will tell the sequencer to move the selected audio slices to the 
closest grid point specified by the quantization value parameter.

159
4.2  Advanced Quantization Techniques
Another extremely useful application of this technique involves being able to anchor the created 
slices to the measures and beats, exactly as a Recycle file would do. After you split an audio file in 
slices, the slices will be linked to their position, not in real time (as any regular audio file would be) but 
in bars and beats (as MIDI data are). Therefore, when you change the tempo of a sequence, the computer 
will adjust the tempo of the audio files, not through the time-stretching algorithm but instead by simply 
moving the slices closer to each other (faster tempo) or farther from each other (slower tempo), allow­
ing you to execute tempo changes in real time. This second application has some limitations, though. 
First, the final result and quality of the tempo change greatly depend on the accuracy with which the 
slices were created. For simple rhythmic parts this technique works well, but for more complicated and 
rhythmically busy parts it might not work as well. Second, as we saw in the case of the time-stretching 
technique, usually you get better results with tempo changes that occur in a limited range. The same 
technique with some slight variations is also available in the other sequencers.
A faster technique for quantizing audio is to take advantage of the “beats” feature of DP. I discussed the 
concept of beats when we learned how to extrapolate a groove from audio material. Since beats identify fast 
transients and key rhythmic elements inside a soundbite in DP we can use them to quantize the event inside 
an audio part. Here is how: first, select the audio part you want to quantize and open the waveform editor 
(Audio  Edit in Waveform Editor). Now click on the “Beats” tab and use the “Adjust Beat Sensitivity” 
option as explained earlier in the chapter. After the computer has inserted the desired amount of beats inside 
your soundbite, close the Waveform editor. Now go to the quantization window (Region  Quantize) and 
make sure that under “What to quantize” you choose “Beats within soundbites”. Select the desired quantiza­
tion value as you would do for any normal quantization of MIDI material and click Apply. That’s it! This is 
truly one of the greatest ways to quantize audio I have found so far, it is quick, intuitive, and simple to apply. 
Let’s take a look at how the other applications handle the slicing technique.
To prepare an audio file to be quantized in CU you use a similar procedure to the one we learned for 
creating customized grooves from audio loops. CU can use the same slices we created when analyzing 
an audio file to capture its groove. The first step is to import or record an audio file. Once the audio part 
is inserted on an audio track, you double-click on it to open the waveform editor. Use the “Hitpoints” tab 
FIGURE 4.6
(a) Nonsliced and (b) sliced soundbite in DP.

160
CHAPTER 4  Advanced Sequencing Techniques
(the same one we used in Section 4.2.3) and the sensitivity slider to insert the hitpoints. Once the compu­
ter has set the slices according to the transients of the waveform and the information has been provided 
about meter, tempo, and bars, you can freely move or delete slices that you believe were not placed in 
the right position. To delete a hitpoint, simply click on the “Edit Hitpoints” button under the Hitpoints 
tab and click on the hitpoint you want to delete. To add a hitpoint, press the option key and click where 
you want the new hitpoint to be inserted. To move a hitpoint, just click and drag it left or right. After you 
have made the right edits to the hitpoints so that all the important transients are marked with a hitpoint, 
select the “Create Slices” option from the Hitpoints tab. The newly created audio part will look very 
similar to the original, except for the slices marked by a thin line (Figure 4.7). If you change the tempo 
now, the slices will remain anchored to their bar/beat location, meaning the loop will change its tempo 
according to the tempo tracks or to the current tempo setting of the sequence. Notice that the name of the 
new audio part has changed into the original name of the file plus the extension “Sliced”. This is because 
the new audio part is a special type of audio event that does not follow all the same rules as regular audio 
files.
CU gives incredible control over the parameters of each slice. To edit a sliced audio part, double-
click on it. The new audio editor (which is different from the waveform editor we use for nonsliced 
audio parts) allows you to control the start/end positions, volume, fades, mute, and name of each indi­
vidual slice of a loop. To change these parameters, simply click on a slice, select the parameter you need 
to alter in the ruler above the waveform, and change its value.
By once again using the Beat Detective (BD) function available in PT, we can create independent 
regions from a single audio loop based on transients. These regions will then be conformed to the cur­
rent tempo map to adapt the tempo of the original loop to the changes inserted in the tempo map. The 
first step is to create the slices according to the transients of the audio loop. This procedure is identical to 
the one involved in the groove extraction method (Section 4.2.3). Here’s a brief summary.
Select the region of the loop you want to slice (make sure you select an exact number of bars and 
beats), open the BD window (found in the Event menu), and select the “Region Separation” button to 
the right of the window. Now set the start/end positions and meter, select the algorithm you want to 
use (High or Low Emphasis), and click on “Analyze” to generate the slices. When the beat triggers are 
set (you can edit them using the Sensitivity slider), click on the “Separate” button to create individual 
audio regions based on the triggers. The “Trigger Pad” option allows you to insert a very short time 
delay between the beginning of each region and its “Sync Point” (meaning the actual marker that will be 
anchored to a bar/beat position) to guarantee that the attack of the transient is preserved. At this point, 
if you want to quantize the regions that you just created, simply choose the new separated audio regions 
in the Edit window, recapture the selection by inserting the start/end locations and the time signature of 
FIGURE 4.7
(a) Nonsliced and (b) sliced audio region in CU.

161
4.2  Advanced Quantization Techniques
the regions selected, and click the “Region Conform” button (Figure 4.8). You can also choose among 
several parameters that allow you to decide which regions are going to be affected (“Exclude Within” 
parameter) and how much they are going to be affected (“Strength” parameter) by the “Conform” com­
mand. The “Swing” option lets you decide how much swing feel you want to apply to the selected 
region before conforming it. When you are ready, click the “Conform” button to apply the changes.
In LP you can generate audio regions manually by using the scissor tool, just as we learned to do in 
DP. You have to import or record an audio track first, and then simply select the scissor tool from the 
tool palette and create the new regions by clicking on the audio file. The new regions will automatically 
stay linked to their position in bars and beats, so when you change the tempo they will automatically 
update their position to adapt to the new tempo. You can select the grid value at which the regions will 
be created from the “Snap” drop-down menu in the top-right corner of the Arrange window. I recom­
mend selecting “Division”, which allows you to use the grid value specified in the “Division” parameter 
(the value visible under the “Time Signature” field in the Transport window). To quickly create multiple 
regions, hold the Option key while clicking; this will create multiple regions for the entire length of the 
sequence you are working on based on the first rhythmic subdivision you choose.
Whereas the procedure I just explained is grid based, there is a more flexible and intuitive way in LP to 
quantize audio, called “Flex Time”. This technique is based on the same concept we learned in DP where 
fast transients contained in an audio region are automatically analyzed and marked to be either moved or 
quantized. To turn on the Flex mode for a region in LP, select the desired region and select the appropriate 
Flex Mode in either the drop-down menu of the track or the Track Parameter box (Figure 4.9).
You should set the Flex Mode according to the type of audio material you are working with. I like to 
use Rhythmic for a single-track percussion/drums part (i.e., a snare drum track or a bass drum track) and 
use Polyphonic for entire drum kits tracks (e.g., a stereo mix of a drum set). Once you have picked the 
FIGURE 4.8
Beat Detective in Conform mode in PT.

162
CHAPTER 4  Advanced Sequencing Techniques
appropriate Flex Mode, LP will analyze the waveform and assign a handle to each transient in the audio 
region. You can freely reposition the handles by Option-clicking-and-dragging on the handle you want to 
move. To insert a new handle simply click on the waveform where you want the handle to be added, while 
to delete it Control-click the handle and select Flex Marker. Once all the handles are set, you can quan­
tize your audio in several ways. The fastest one is to use the familiar quantization value located in the 
Channel Strip Inspector for the region selected. Choose the quantization value needed from the drop-down 
menu and the audio transients will be automatically snapped to the selected grid. Another way is to freely 
drag each handle by simply clicking-and-dragging it, which will give a lot of freedom in terms of creat­
ing your own groove. You can also quickly quantize the region’s audio to another region’s material. To 
do so, first you have to create a new quantization groove based on the audio material of a region. Select 
the region, activate the appropriate Flex Mode, and select “Make Groove Template” from the quantization 
menu located in the Channel Strip Inspector. Now this groove will be available to quantize any other region 
(audio or MIDI).
4.3  ADVANCED EDITING TECHNIQUES
In Chapter 2 we learned basic editing techniques for both MIDI and audio tracks, such as straight quan­
tization, basic graphic data editing, loop import, and basic track automation. In Chapter 3 we moved to 
more complex editing techniques, such as the drum editor, complex tempo changes, track layering, and 
groove quantization. In this section of this chapter we will learn advanced editing techniques that can 
greatly speed up your work and also improve the quality of your productions. Such techniques vary from 
application to application, since each sequencer usually covers certain areas better than others. In this 
section I present each tool or technique with a general description and then go into details regarding how 
to use it and its practical application for each sequencer.
4.3.1  Advanced MIDI Editors
Advanced MIDI editors can be used in several ways to speed up your work or make it more flexible. I like 
to distinguish between two types of advanced MIDI editors, based on how they are applied and used. The 
first category includes editors that are not applied in real time but that need an “offline” processing time. 
The second category includes editing tools that can be applied in real time as MIDI inserts on a MIDI 
FIGURE 4.9
Flex Mode in LP.

163
4.3  Advanced Editing Techniques
track or as a filter applied to the MIDI data before they are sent out to the devices. Let’s take a look at the 
tools available in each category and how to take advantage of their features.
4.3.2  “Offline” Global MIDI Data Transformers
While so far we have learned how to edit MIDI data with a micro approach (meaning targeting a 
small number of MIDI messages or restricted regions), there are often situations (especially when you 
are under a tight deadline) where you need to process large sections of MIDI data or large regions at 
the same time. For this type of situation we need to be able to process the data with a macro editing 
approach; that is, we need to be able to batch-process a series of data based on filters and conditions 
set in advance. Depending on the application, these types of global editors are sometimes called logic 
editors or transformers, or they simply assume the name of the specific functions associated with them, 
such as “Change Velocity” or “Change Duration”. Behind these editors is the assumption that you want 
to quickly affect with a single command all the data or an entire category of messages contained in a 
selected region. Depending on the application and the type of editor you use, you can set filters and 
conditions to establish which data will be affected by the action and then choose the type of action you 
want to apply to the screened data. For example, let’s say that we want to limit the velocity of all Note 
On velocity data recorded on a MIDI track in a region that includes measures 1–25 and that the bounda­
ries of the velocities have to be between 10 and 100. With a “logical” editor we can translate our goal 
into logical conditions that, if met, will achieve the wanted result. In our example we would have to set 
the first condition to filter only the Note On velocity data, the second condition would set the boundaries 
between bars 1 and 25, while the third condition would move all velocities lower than 10 to exactly 10, 
and all velocities higher than 100 to exactly 100. The practical applications of these types of editors are 
endless. Their real-life sequencing functions range from a simple velocity change filter to a sophisticated 
data transformer, or from a basic quantization tool to an advanced “reverse pitch and position” function. 
Let’s take a look at how each sequencer implements and integrates the logic editor capabilities.
In LP the logic editor, called the “Transform window”, can be accessed from the “Window” menu 
by choosing the “Open Transform” option. To program it, after selecting a track or tracks, open the 
“Transform” editor (Figure 4.10).
The window on which this editor is based is divided into two horizontal sections. The upper section 
(named “Select Events by Conditions”) is where we set the conditions under which the computer selects 
the data to edit. The lower section (called “Operations on Selected Events”) establishes the action that 
will be applied to the data that met the previous conditions. As you can see, it is very logical, and it 
seems more a mathematical tool than one for a musician’s environment. Nevertheless, this editor repre­
sents an extremely valuable resource for your sequencing projects.
LP’s “Transform” editor is programmed with useful presets to speed up some of the most common 
tasks. In the drop-down menu in the top left corner of the window you will see the list of such functions 
(e.g., quantize note length, crescendo, half speed, double speed). If you want to program your own filters 
and actions choose the “Create Initialized User Set” option instead of selecting a programmed function. 
This will clear all the fields in both sections of the window. The next step consists of setting the condi­
tions under which the computer will select the events to be edited. Change each parameter (position, 
status, channel, etc.) according to the type of edit you want to achieve. In Table 4.2 you can find a quick 
explanation of each parameter.
The conditions of each parameter can be changed according to logical expressions such as equal, 
greater, smaller, and inside. After you have selected the type of data you want to be affected by the oper­
ation, move to the lower part of the window and choose the action you want to apply to the filtered data. 
Here you can choose among several operations, such as min, max, randomize, and fix. Refer to Table 4.3 
for a more detailed explanation of some of the most common operations.

164
CHAPTER 4  Advanced Sequencing Techniques
Table 4.2  Transform Window Parameters in Logic Pro.
Parameter
Function
Comments
Position
Main position of the event
Status
Type of MIDI message
Note
P-Press
Control
Program
C-Press
Pitch Bend
Meta
Fader
Channel
MIDI channel
1
First data byte value
In the case of a Note message it indicates the note 
number
In the case of a CC it indicates the controller number
2
Second data byte value
In the case of a Note message it indicates the velocity
In the case of a CC it indicates the controller value
Length
Length of the event
Subposition
Positioning of the event within a 
measure
FIGURE 4.10
Transform Editor in LP.

165
4.3  Advanced Editing Techniques
Keep in mind that each operation affects the type of data located directly above. If you want to leave 
a certain type of data unaffected, simply set its operation to “Thru”.
Once you have selected both the conditions and the operations, you are ready to apply the setting to 
the data. If you click on “Select and Operate”, both actions will be taken. If, instead, you prefer to use 
the “Transformer” as a simple filter tool to choose certain types of data, you can click on the “Select 
Only” button. If you want the operation to be carried out on data and events selected manually, choose 
“Operate Only”. A final option allows you to choose what to do with the edited data after the selection/
operation process is executed. Use the drop-down menu to apply the operation to the selected events, 
apply and delete the unselected events, delete the selected events, or apply and copy the selected events 
to the clipboard. Keep in mind that after you create a new set in the “Transformer” it will be available 
from any editor window through the “Functions” menu.
In LP the logic editor is extremely powerful, and I strongly recommend using it regularly to speed up 
your work. Here is a practical application of the logic editor. Let’s say we want to smoothly program the 
velocity data of all the close hi-hat notes in our drum track from a value of 1 to a value of 127 from bars 
33 to 48. If we had to do it manually it would take a long time to make sure that every close hi-hat note 
(F#1) is processed and to calculate perfectly the ramp for each note. Using the logic editor instead, we 
can do it in a matter of minutes. The settings for this particular example are reproduced in Figure 4.11.
A similar editor to the one described in LP can be found in CU. The “Logical Editor” shares many of 
the concepts we learned about the “Transformer” in LP. To use this editor you first have to select a MIDI 
Table 4.3  Operators in the Transform Editor in Logic Pro.
Operation
Function
Comments
Thru
Leaves the event unaltered
Fix
Sets the event to the value indicated
If applied to the “Status” event you can change 
the type of event and choose among a list of 
MIDI messages
Add/Sub
The indicated value is added or subtracted 
to the original value of the event
Min/Max
Events that are smaller/bigger than the set 
value will be replaced by it
Mul/Div
Events will be multiplied/divided by the 
indicated value
Range
Same as a combination of Min/Max
Random
A random value is created between the 
limits indicated
/Rand
Adds a positive or negative value between 
zero and the indicated value
Quantize
Quantizes the events to a multiple of the 
indicated value
Qua&Min is similar but with combination of the 
Min operation
Exponent
Events are changed according to an 
exponential curve. The shape of the curve 
depends on the indicated value
Positive values  exponential curve
Negative values  logarithmic curve

166
CHAPTER 4  Advanced Sequencing Techniques
part in the main Arrange window and then choose “Logical Editor” from the MIDI menu. The window 
that appears is very similar to the one we used in LP’s “Transformer”. The drop-down menu in the top 
left corner allows you to select presets that cover most of the basic editing situations (such as “Set notes 
to fixed pitch”). The “Logical Editor” window has two different sections. The upper section allows you to 
create the filters and conditions that select which data will be passed to the operation; the lower part of the 
window hosts the operation section, where you can decide the action to be applied to the data that met the 
conditions. For each of the two sections you can create multiple lines (using the  and  buttons located 
FIGURE 4.11
Practical example of the Transform Editor in LP.
FIGURE 4.12
Logical Editor in CU.

167
4.3  Advanced Editing Techniques
under each section), all containing different parameters. This allows you to generate extremely complex 
filters. In Figure 4.12 you can see the “Logical Filter” set up to add a value of 20 (operation) to the veloc­
ity of all the notes between bars 1 and 5 that have a velocity lower than 100 (condition). The idea again is 
to speed up bulk editing operations by avoiding individual selection of data and instead globally choosing 
to apply a certain action only to the data that need to be altered.
For each line in the “condition” field you can set several parameters to define which data will be 
affected by the action. A short description of these parameters is given in Table 4.4.
As you can see in Table 4.4, the MIDI messages in general can be filtered according to their data 
bytes, meaning two of the major building blocks of a MIDI message (along with the initial status byte). 
Most of the messages (especially the Channel Voice messages) are built on three bytes: the status byte 
(which contains information about the type of message and the MIDI channel on which the message is 
sent), and one or two (or sometimes more, depending on the type of message) data bytes, where only 
parameters strictly related to the particular type of message sent are stored. Therefore, depending on 
which message is sent, the data bytes assume different functions. Table 4.5 contains a quick summary of 
the functions of bytes 1 and 2 for each Channel Voice MIDI message.
Use the condition field to set the logical expression that will filter the data. The fields named 
Parameter 1 and Parameter 2 change according to the “Filter Target” choice, while the “Bar Range” 
option lets you choose the position range in which the filter and condition will work. If you use multiple 
lines to filter your data, you can choose between the two Boolean conditions “And” and “Or” to imple­
ment more complex conditions. Once the filters and conditions are set, you can move on and program the 
actions that will be applied to the filtered messages. To do so, simply select the “Action Target” (same 
Table 4.4  Operations Available in the Logical Editor in Cubase.
Operation
Function
Comments
Position
Defines the range of the data in terms of 
bars and beats
Length
Selects the events based on their length
Value 1
Represents the first data byte of a 
message
The meaning of the first data byte changes 
depending on the type of message you are dealing 
with. Refer to Table 4.5 for a list of channel voice 
messages and their data byte structure
Value 2
Represents the second data byte of a 
message
The meaning of the second data byte changes 
depending on the type of message you are dealing 
with. Refer to Table 4.5 for a list of Channel Voice 
messages and their data byte structure
Channel
Filters the data according to the MIDI 
channel on which they were recorded
Type
Data will be filtered according to their 
type
For example, Note, CC, Aftertouch
Property
Filters the data based on their properties
For example, whether the data are muted or not
Value 3
Represents the third data byte of a 
message
This byte is seldom used
Last Event
Indicates the state of an event that has 
just been sent

168
CHAPTER 4  Advanced Sequencing Techniques
options as the “Filter Target”), the mathematical operation in the “Operations” field, and the values that 
will be used by the operation (Parameters 1 and 2). At the bottom of the Logical Editor window you can 
choose the type of function that will be performed when clicking the “Apply” button. The selected data 
can be deleted, transformed, inserted, inserted exclusively (the selected data will be transformed and the 
data not selected will be deleted), copied, extracted, or selected. Once you have set all the conditions and 
actions, you can apply them by simply clicking on the “Apply” button, or you can store them for later 
use by selecting the store button located next to the preset list drop-down menu. As we learned in LP, the 
Logical Editor is extremely powerful and can be a lifesaver for quick and extensive editing tasks.
Some people may find the approach adopted by LP’s Transformer and CU’s Logical Editor a bit 
intimidating, especially if they are new to the MIDI and sequencing environment. Producers and com­
posers sometimes try to avoid using such tools because they fear the unfamiliar interface will interfere 
with the creative process. If this is the case, there are other applications that give you some of the power­
ful features available in logical editors without their intimidating interface.
In DP and PT, for example, we find not a real logical editor but instead a series of premade filters and 
tools that can achieve fairly complex results without any programming time. These tools are based on 
the principle of selecting a region of data (from any editor available) and then selecting one of the func­
tions you want to apply. Among the most useful features available in DP that take advantage of this tech­
nology are “Change Velocity”, “Change Duration”, “Split Notes”, and “Transpose”. All four options can 
be found in DP’s “Region” menu. Let’s take a look at how to use these DP features.
The Change Velocity option allows you to alter the On and Off velocity of MIDI notes according to 
several criteria, which can be all edited and customized through a series of parameters. The velocities 
of the selected MIDI events can be changed using one of the following options: Set, Add, Scale, Limit, 
Compress/Expand (Figure 4.13), and Smooth. As you can see, most of these options are similar to the 
ones we analyzed in the logical editors of LP and CU. The difference is that here the parameters are 
more easily accessible, since they are clearly laid out and have a specific function already assigned. In 
Table 4.6 (page 170) you will see a detailed description of each function.
The Change Velocity function can be used in several ways to improve your projects. You can set 
it to quickly add or subtract absolute values (“Add”) or relative values (“Scale”) in order to lower a 
part without using volume automation; remember, though, that some patches are programmed to change 
their sonic features depending on the velocities of the notes, and therefore creating crescendos or decre­
scendos through velocity changes may affect the way your part sounds. I find the “Limit” function 
Table 4.5  Data Byte/Channel Voice MIDI Message Assignments.
Type of Message
Data Byte 1
Data Byte 2
Comments
Note On
Note number
Velocity
Note Off
Note number
Velocity
Program Change
Program number
Pitch Bend
LSB
MSB
CC
ID Number
Value
The ID number represents the 
CC number
Mono Aftertouch
Pressure value
Poly Aftertouch
Note number
Pressure value

169
4.3  Advanced Editing Techniques
particularly useful. You will often find that the parts recorded will have a few notes with a higher veloc­
ity than the rest. While you could use the graphic editor to manually change their velocities, I recom­
mend applying the Limit function to reduce the dynamic range of the MIDI part. This will create a much 
more cohesive and solid MIDI performance. Use the “Compress” option to gently remove uneven veloc­
ities from the parts. The parameters will vary according to the specifics of the parts you are working 
on, but I recommend starting with mild settings (low ratio and high threshold) to gently limit the high­
est velocities and then slowly to increase the ratio and lower the threshold until you obtain the perfect 
balance.
By using the “Change Duration” function in DP you can automatically alter the “Duration” param­
eter of the MIDI events included in a selected region. The parameters used in this function are similar to 
those listed in Table 4.6. I recommend using the Change Duration option to quickly create legato or stac­
cato passages. I find, for example, that when sequencing string parts it works well to apply a bit of legato 
articulation by selecting a region and changing the duration of the notes by using the “Extend Releases” 
option first (this will prolong the release of each note to the next attack) and then by using the “Add” 
option (use values between 10 and 20 ticks); this will make each note overlap the following by exactly 
the amount of ticks you specified in the Add parameter. To create staccato parts, you can apply the same 
technique, but this time choose the “Subtract” option instead of “Add”.
When I discussed the logical editors, you learned about functions that allow you to cut defined notes 
or events inside a selected region. A similar function, called “Split Notes”, is available in DP. This edi­
tor, available from the Region menu, allows you to copy or cut selected notes from one track to another 
track (an already existing one or one created anew, ad hoc). The targeted notes are selected according to 
several criteria that can be configured from the Split Notes window (Figure 4.14, page 171).
I recommend using the Split Notes function when, for example, you want to extract a single-note 
percussion instrument from a multi-instrument drum track. Simply select the note you want to cut, paste 
FIGURE 4.13
Compression/Expansion option in the Change Velocity function in DP.

170
CHAPTER 4  Advanced Sequencing Techniques
it to another track from the keyboard in the Split Notes window, select the operation you want to per­
form (cut), and select the target track from the “Send the note to” option. It also works well for cutting 
or copying outer voices in a polyphonic part: use the “Top notes of each chord” or the “Bottom notes 
of each chord” option to quickly select, respectively, the highest or lowest part in polyphonic tracks. 
Another very useful feature of the predefined logical functions in DP is the “Transpose” option (acces­
sible from the Region menu). By selecting this function you can not only transpose the MIDI notes up 
or down in the selected region, but also harmonize them, meaning transpose the selected notes and keep 
the original pitches at the same time. The transposition can be made following a specific interval, dia­
tonically according to a specific scale, from a specific scale to a target scale (“Key/Scale” option), and 
according to a custom map. I recommend this tool to quickly transpose extended regions or to try har­
monized parts inside or outside a key without having to replay the harmonized voices.
PT uses a similar approach to DP. From the Event  Event Operations menu you can access func­
tions such as “Change Velocity”, “Change Duration”, “Transpose”, and “Split Notes” (Figure 4.15). While 
these options give basic access to advanced editing tools, their parameters are much more simplified and 
streamlined than in LP or CU.
Table 4.6  Parameters of the Change Velocity Function in Digital Performer.
Function
Description
Comments
Set
Sets the velocities of all the MIDI 
notes selected to the value specified
Use this option if you want to flatten out a performance 
to increase its mechanical and drum-machine-like effect
Add
Adds the specified value to all 
the velocities of the MIDI events 
selected
Use this function to raise (positive values) or lower 
(negative values) the volume of selected notes
Scale
Similar to the previous function, 
except here the velocities can 
be reduced or increased by a 
percentage value
The velocities will be increased for values above 100% 
and decreased for values below 100%
Limit
Sets a low and high threshold; all 
velocities higher or lower than the 
thresholds will be moved to the 
values specified
This option is particularly useful if you have a region 
with some extremely high or low velocities that stick out 
compared to the majority of the other velocities
Compress/
Expand 
(Figure 4.13)
Allows you to limit the dynamic 
range of your region (compress) or 
to increase it (expand)
Works in a similar way to an audio signal compressor. 
If the velocities go over the threshold their values will 
be reduced according to the Ratio parameter (1:1 no 
compression, 8:1 maximum compression). If the Ratio 
is set to values of 1:2 to 1:8 the effect works as an 
expander, meaning that if the velocities go over the set 
threshold their values will increase according to the 
Ratio. The Gain allows you to raise the velocity values 
of all the MIDI events selected to reestablish the original 
volume of the track
Smooth
Smoothly sets and changes the 
velocities of the selected events 
between two values; you can choose 
to use absolute values (0–127) or 
relative percentage (1–999%)
This function can be effectively used to quickly create 
crescendos and decrescendos of velocities. The 
curvature parameter allows you to control the shape of 
the curve applied to the velocities. 0  linear; positive 
values  exponential; negative values  logarithmic

171
4.3  Advanced Editing Techniques
FIGURE 4.14
Split Notes function in DP.
FIGURE 4.15
Event Operations window in PT.

172
CHAPTER 4  Advanced Sequencing Techniques
4.3.3  “Real-Time” MIDI Effects
All the advanced MIDI tools I presented in the previous section are applied offline, meaning that the 
computer calculates the function first and then applies the results by changing and inserting the new data 
in the track. This is a valuable way to alter the MIDI data, but it can be limiting if you are not 100% sure 
you want to commit to the result.
There are several ways to overcome the limitation of such a procedure. You can use the “unlimited 
undos” function featured in all the DAWs presented here until you find the settings that satisfy you com­
pletely. Another way is to create an alternate take that is an exact copy of the original track and to keep 
it as a safety copy to which you can always revert in case you are not completely satisfied with the edits. 
There is another solution, though, that I often find the most convenient when I want to quickly experi­
ment with edits such as transposition, quantization, and velocity change. This option is based on the 
strategy that instead of applying the edits offline, you insert a MIDI filter directly into the channel of 
the MIDI track you want to process. The filter is inserted right before the MIDI data are sent out to the 
Table 4.7  Advantages and Disadvantages of “Real-Time” MIDI Effects.
Advantages
Disadvantages
Comments
Settings are easily 
changeable
The parameters can be changed at any time, 
even during playback
Quick revert to original
You can bypass or delete the edit/effect at any 
time
Fairly good control 
over effects such as 
delay and echo even 
on MIDI tracks
A decent selection of MIDI effects (echo, delay, 
harmonizer and arpeggios) is available
The edits/effects can 
be “printed” to track 
after the settings are 
considered satisfying
The “print MIDI effects” options lets you 
achieve the same result as the “Off-line” 
method after having experimented with several 
settings and parameters
Settings are applied to the entire 
track. To apply different settings to 
different sections or regions of the 
same track you have to create a new 
MIDI track and move the data on the 
newly created track, then you can 
apply a new MIDI effect with different 
parameters
The insertion of several MIDI effects 
at once could slow down the MIDI 
network
The bandwidth required by the MIDI network 
is fairly small compared to the one required by 
the audio network and therefore the insertion 
of a reasonable number of MIDI effects should 
not create noticeable problems
The use of effects such as echoes and 
multiple delays could cause “out of 
polyphony” problems
MIDI effects such as echo and delays are 
generated by retriggering the same MIDI notes 
over and over but with decreasing velocities; 
therefore, a device will use more notes 
than expected since each repetition will be 
calculated as a new triggered note

173
4.3  Advanced Editing Techniques
respective cable and MIDI channel. The computer therefore alters the data in real time right before they 
reach the MIDI device. This technique has a few advantages and disadvantages, as shown in Table 4.7.
While the types of effects you use can vary from application to application, the procedure is very 
similar for all sequencers. The system is very similar to the one used in the regular signal flow of an 
analog mixing board. In some cases (DP and CU), the MIDI effects are inserted on the channel strip of 
the MIDI tracks from the Mixing Board window, while in others (LP and PT) the effects are inserted 
on each object separately. Since the effects are applied in real time and inserted right before the MIDI 
OUT of each track, you can change their parameters, bypass them, or delete them at any time. The actual 
effects available (and their parameters) are usually the same ones you can access from the offline list. To 
insert these MIDI effects, simply open the Mixing Windows of your sequencer and use one of the empty 
insert slots available on the channel of the MIDI track on which you want to apply the effect/filter.
In DP, for example, you can insert MIDI effects such as arpeggiator, change duration/velocity, and 
echo (Figure 4.16).
In the same way, in CU you can insert a variety of very interesting real-time MIDI effects and 
also some more basic ones through the Insert slots available in the Mix window for each MIDI track 
(Figure 4.17). Among my favorite CU real-time MIDI effects is the “Step Designer”. This very interesting 
filter transforms the current MIDI track in a small step sequencer. You can program a total of 200 patterns 
FIGURE 4.16
Online effects list in DP.
FIGURE 4.17
Online effects list in CU.

174
CHAPTER 4  Advanced Sequencing Techniques
that can be triggered in real time from a controller while the sequencer is playing. The notes generated by 
the step sequencer are sent out to the channel and cable to which the current MIDI track is assigned. This is 
a particularly useful tool for live performance or for a creative recording of live patterns.
Another useful MIDI filter in CU is the “Chorder”, which is a single-note-to-chord trigger. You can pro­
gram the Chorder to trigger a chord by playing a single note from your MIDI controller. CU also offers the 
possibility of using the logical editor as an insert on a MIDI track. The parameters available are the same 
ones we learned for the offline version. More basic effects can be accessed by inserting the MIDI Modifiers 
plug-in. From here you can transpose the notes, alter their velocity (Vel. Shift), compress/expand the veloc­
ity of the notes (Vel. Comp.), and alter by compression/expansion their lengths (Len. Comp.). The last two 
parameters mentioned (Vel. Comp. and Len. Comp.) work as compressors if the nominator is smaller than 
the denominator (e.g., 1/2) and as expanders if the nominator is bigger than the denominator (e.g., 2/1).
The capabilities of LP in terms of MIDI effect insertion are as sophisticated as (if not more sophisti­
cated than) those of DP and CU. The best way to quickly filter a MIDI track (or a single region) through 
MIDI effects is to select the desired track (or object) and then use the Parameter window (located in the 
upper left corner of the Arrange window) (Figure 4.18) to insert filters such as transposition, delay, and 
velocity change.
Use the “Dynamics” filter either to expand the difference between loud and soft notes (values higher 
than 100%) or to decrease the difference between loud and soft notes (values lower than 100%). Via the 
“Gate Time” option we can automatically create staccato effects (values lower than 100%) and legato 
effects (values higher than 100%). If you choose the Fix setting you will obtain a complete staccato, 
while opting for Leg will generate a complete legato effect.
While these filters cover the most basic functions, in LP we can use the Environment editor to cre­
ate incredibly complicated filters comparable to those found in CU. To take advantage of these effects, 
FIGURE 4.18
Real-time effects in LP.
FIGURE 4.19
Example of real-time effects using the Environment in 
LP: the arpeggiator.

175
4.3  Advanced Editing Techniques
first open the Environment window, found in Window  Environment (or use the shortcut Command-8). 
Then create a new element in the environment assigned to your current project by selecting “New” from 
the Environment window and choosing the object you want to create (e.g., an arpeggiator as shown in 
Figure 4.19). Once the object is added to the environment, you have to make the right connections to 
have it connected to one of the MIDI devices or virtual instruments.
To do so, simply Option-click on the output arrow on the right side of its icon and select a destination 
device from the list of available outputs (Figure 4.20).
To access the new effect, assign a track to this new virtual device from the Arrange window in the 
same way you would assign any MIDI track to an output. Some of my favorite functions accessible from 
the Environment editor are the “Arpeggiator” (Figure 4.21a) and the “Chord Memorizer” (Figure 4.21b). 
FIGURE 4.20
Example of real-time effects using the Environment in LP: assigning the target devices for a real-time MIDI effect.
FIGURE 4.21
(a) Arpeggiator and (b) Chord Memorizer in LP.

176
CHAPTER 4  Advanced Sequencing Techniques
The Arpeggiator allows you to create sophisticated MIDI arpeggios by controlling parameters such as 
the direction of the arpeggio, its velocity, its range, the resolution/length of the notes, and its octave 
span. The Chord Memorizer is designed to build complex chords that can be assigned and triggered by a 
single key or note. To edit the key/chord assignment, double-click on the “Chord Memorizer” icon and 
select the note that will trigger the chord (top keyboard) and the chord that will be triggered (bottom 
keyboard). Beside having some practical applications in a live environment, the Chord Memorizer can 
be very useful to sequence complex parts with complex chords, especially if the piano is not your main 
instrument. All the features available in the Transform editor can also be used to insert the same param­
eters we learned earlier.
4.4  OVERVIEW OF AUDIO TRACK EFFECTS
Even though this book focuses on MIDI and its practical applications, I would like in this section to 
explain some important concepts about the use of effects applied to audio tracks. Since we just learned 
how we can use specifically designed effects with MIDI tracks, I believe it is also crucial to under­
stand how effects such as reverb, delay, chorus, and compressor can be applied to audio tracks. While 
this section will cover the most important aspects of practical applications of audio plug-in effects in 
an audio sequencer, I encourage you to consult the excellent plug-ins guide by Mike Collins, entitled 
A Professional Guide to Audio Plug-ins and Virtual Instruments.
The way effects are used and applied to audio tracks in a sequencer is very similar to how we 
use them in a more traditional analog-mixing environment. In effect, the signal flow is the same, but 
instead of using external effect units, hardware patch bays, and cables to connect the devices and 
route the signal, we use only virtual connections created inside the application. In general, there are 
two separate ways to add an effect to an audio source. The first is called insert; the second is called 
send. While they both have the result of adding some sort of effect to a dry (meaning without effect) 
signal, they differ substantially in their nature and applications. Let’s learn more about these two 
techniques.
4.4.1  “Insert” Effects
The simplest and often quickest way of adding an effect to an audio track is by directly using an insert 
on the audio track. This means literally inserting an effect in the virtual signal flow of the mixing board 
of your sequencer between the source (the actual audio material that is played back from the audio file 
stored on your hard disk) and the virtual output of the mixing board channel of the sequencer. By insert­
ing the effect, you allow the entire signal of the track (meaning 100% of the signal) to be affected by the 
inserted effect (Figure 4.22).
In a normal analog situation it would be the same as taking your guitar output and plugging it into 
the input of your effect unit, and then taking the output of the effect unit and sending it to the mixing 
board or the guitar amplifier. In this case the entire signal will be sent to the effect. Sometimes, depend­
ing on the effect you insert, you can still adjust the balance between the original dry signal and the 
affected wet signal.
While this technique is quick and fairly easy to implement, it is used mostly for effects that require 
the entire audio signal to be processed. Such effects are mainly dynamic (such as compressors, limiters, 
gates, and expanders) and equalization effects. By using effects as inserts, you can speed up the process 
of adding effects without interrupting the creative flow. If you have to apply effects such as reverb, delay, 
echo, or other ambience effects, then the insert technique is not the most appropriate one, for a variety 

177
4.4  Overview of Audio Track Effects
of reasons. The main one is exemplified by the high inefficiency of the insert technique. Imagine hav­
ing a 32-track audio session where you need to apply reverb to all 32 tracks. By adding 32 reverb units 
as inserts you will choke the central processing unit (CPU) of your computer, since reverbs are among 
the more CPU-hungry audio effects. In addition, if you want to change the overall ambience of the mix, 
you will have to modify the parameters of all 32 reverb effects, which is an incredible waste of creative 
time. Therefore, for ambience effects (reverbs, delays, and echo), you should use a different technique to 
apply effects, based on the “send” principle.
4.4.2  “Send” Effects
The technique explained in the previous section is a valuable tool for effects that require the entire audio 
source to be affected, but there is a much more functional and efficient way to apply effects to audio 
tracks, based on the auxiliary “send” procedure. Keep in mind, though, that this technique is particularly 
indicated for effects you want to share among several tracks, such as reverbs, delays, and echo, and more 
generally for “time-based” effects. The main idea here is that you will have a separate track(s) (called the 
auxiliary track or effect track, depending on the software you use) onto which you insert the effect (let’s 
say a reverb). The auxiliary track receives the input from an internal channel path called a bus. On each 
audio track on which you want to apply the effect, you have to create a “send” output and assign it to the 
FIGURE 4.22
Signal flow of an effect applied as “insert”.

178
CHAPTER 4  Advanced Sequencing Techniques
same bus on which you set the auxiliary track to receive. The main difference from the “insert” technique 
we learned earlier is that here we can decide how much of the original dry signal is sent to the effect (and 
not only 100% like in the insert example); this gives us greater control over the balance between dry and 
wet signal (Figure 4.23). In addition, we can share a single effect among several tracks by assigning each 
track’s sends to the same bus. Each send will serve as an individual effect control for each track, while the 
fader of the auxiliary effect track (called the effect return) will serve as the general effect volume.
As I briefly mentioned before, the technique you use to apply effects to your project depends mainly 
on the type of effects you need to use. In Table 4.8 I list a series of common effects and the most fre­
quent techniques for applying them to audio tracks.
On a practical level, the ways the four sequencers handle the two techniques (insert and send) are 
very similar. If you want to insert an effect on an audio track, all you have to do is open the mixing 
board window and click on the insert field of the channel strip of the audio track. After clicking on an 
empty insert slot (Figure 4.24), a list of all the plug-in effects available for your sequencer will appear.
If you want to use an effect as a send in DP and PT, you first have to create an aux track. DP 
and PT use the Project menu (Project  Add Track  Aux Track) and the Track menu (navigate to 
Track  New and then select “Mono” or “Stereo Aux Input”), respectively, to create a new aux chan­
nel. Next, assign the input of the aux channel to a bus (usually you want to choose the first available 
one) by clicking on the Input of the aux track and selecting a bus as Input. Insert on one of the available 
inserts of the aux channel the effect you want to use and share among the tracks on the aux channel (e.g., 
Reverb). In DP and PT the inserts are located at the top of the channel strip of each channel in the mix 
window, as shown in Figure 4.24. As a final step, assign the output of an available send of the tracks on 
FIGURE 4.23
Signal flow of an effect applied as “aux send”.

179
4.4  Overview of Audio Track Effects
Table 4.8  Most Common Application Techniques for Effects.
Type of Effect
Insert
Send
Comments
Compressor
X
Any dynamic effect needs to be inserted to affect 100% of the 
original signal
Limiter
X
Gate
X
Expander
X
Equalizer
X
The Equalizer needs also to be inserted on the channel strip to 
filter the entire original signal
Reverb
X
Ambience effects such as reverb, delay, and echo are usually used 
as “send” since they are often shared among several channels 
at the same time. You can use them as “insert” (which I don’t 
recommend) and control the dry/wet ratio by using the “Mix” 
control directly from the control menu of the effect
Delay
X
X
Echo
X
X
Chorus
X
X
Modulation effects such as chorus and flange can be used equally 
well as “insert” or “send”
Flange
X
X
X identifies the correct use.
FIGURE 4.24
“Inserts” slots in (a) DP, (b) LP, (c) CU, and (d) PT.

180
CHAPTER 4  Advanced Sequencing Techniques
which you want to apply the selected effect to the same bus number on which the aux channel is receiv­
ing (in DP and PT the sends are located below the inserts in the channel strip of each track from the mix 
window); now by raising the send level you can control the amount of dry signal that is sent through the 
bus to the aux channel, meaning to the effect. Use the fader of the aux channel to control the overall vol­
ume (return) of the effect.
In LP the auxiliary channels are added automatically whenever you assign an empty Aux send of an 
audio track to an empty bus. This makes using effects as aux send a very intuitive and quick process. 
Simply click on one of the free Aux sends of a track, select a free bus and automatically a new Aux track 
will be created that receives from the same bus you selected in the previous step. Now insert on the Aux 
track the effect you want to use and trim the send knobs on the audio tracks to decide how much signal 
you want to be affected.
In CU you have to create an Effect track (CU terminology for an Aux track) by selecting 
Project    Add Track    FX Channel. In the AddFX Channel Track window that appears, select the 
effects that you want to insert on the newly created track (you can change the effect at any time later by 
inserting a different effect on its channel strip in the mixing board). Once the channel is created, it will 
serve as the aux channel. CU automatically assigns the buses to the effect channels, so the only thing 
you have to do is assign the send of each audio track to the effect channel just created (as I mentioned, 
CU handles the bus assignments automatically; therefore from each send slot you will see a list of all 
the effect channels available, with the name of the effect inserted for each aux). In CU you can see the 
inserts and the sends of the channels in the mixing board by selecting them from the information win­
dow located at the extreme left of the mix window.
A good and efficient management of the effects is crucial for a balanced and professional-sounding 
project, so I recommend you spend time getting familiar with the two techniques I have described (insert 
and send). Use your DAW of choice as a starting point, and if possible try to experiment with other 
sequencers to master the art of effects handling. Also remember that effects in general are never the 
solution to badly orchestrated or poorly conceived music. Always try to see if your mix can be improved 
first by changing some aspects of the instrumentation. Effects are nice additions (and mostly final 
touches) that can help your projects to sparkle and shine.
4.5  MIDI SYSTEM EXCLUSIVE MESSAGES
As part of the advanced sequencing techniques, I would like to discuss one aspect of the MIDI specifica­
tion that is often overlooked or ignored by most books, especially regarding its practical implications. 
While we learned in the first chapter that System Exclusive (SysEx) messages are MIDI messages that 
are not standard and that they vary depending on the type of device, model, and manufacturer, we also 
learned that they are very powerful messages that allow us to get deep into the brain of a MIDI device. 
As a contemporary composer and producer I believe it its important to know how to deal with these type 
of messages in order to take full advantage of the tools available to you. The most practical applications 
of SysEx messages are the creation of custom virtual consoles in your DAW (faders, knobs, buttons, 
etc.) that can control parameters of external MIDI devices and the SysEx MIDI Dump function. Let’s 
take a look at these two applications.
4.5.1  SysEx Custom Consoles
The creation of custom objects to which you can assign SysEx messages can be extremely useful when 
you want to control specific functions of an external MIDI device. For example, I use a custom console 

181
4.5  MIDI System Exclusive Messages
in DP to program my Breath Controller to send specific MIDI Control Changes (CCs). Some machines 
can only be programmed using SysEx messages and the easiest way to do it is to use virtual objects that 
can be created and programmed to send SysEx from your DAW. Creating custom objects in your DAW 
is very simple.
In LP go to the Environment window and from the Environment window menu select New and the 
object you would like to create (let’s say a new fader as shown in Figure 4.25).
The newly created object will be placed in the Environment window and its parameters will be 
shown in the inspector window on the left side of the window. To assign a SysEx message to this object 
select SysEx from the Output drop-down menu of the object in the inspector area (Figure 4.25). A 
new list editor window will appear, allowing you to insert the SysEx message. Now you will be asking 
“Yes, but how do I know which message I should write?” Well, this is a good question but impossible 
to answer since the SysEx message varies from device to device. You should consult the MIDI section 
of the manual of your device(s) to locate the correct SysEx message that targets the function you need 
to control. Finally, set the target device by Option-clicking on the small output arrow at the top of the 
object and select the target from the list of the devices preset in your studio.
A very similar process can be used in DP. To create a custom console and object, navigate to 
Project  Consoles  New Console. A blank new console will appear along with a dedicated object tool 
palette (Figure 4.26).
From the Console tool palette choose the object you would like to create (i.e., a fader, a knob, or 
a button) and drag it inside the console window. The Control Assignment window of the new object 
will be automatically opened, allowing you to choose the type of message you would like to send when 
FIGURE 4.25
Example of custom SysEx object in LP.

182
CHAPTER 4  Advanced Sequencing Techniques
activating the object. In the “Target” area select the desired device that will receive the SysEx messages. 
Under the “Send” drop-down menu select “System exclusive” and click on the “Set SysEx” button. In 
the Assign SysEx window insert the desired SysEx message and its parameters and click OK on both 
open windows. From now on, every time you activate the object you will be able to send the assigned 
SysEx message to the target device.
4.5.2  SysEx “MIDI Dump” Function
Another practical application of the SysEx messages is the “MIDI dump” function, which allows you 
to take a virtual snapshot of all or some (depending on the settings) of the internal parameters of a 
hardware MIDI device connected to your MIDI network and send the collected data to another device 
through its MIDI OUT port. If the connected device is a sequencer, then we can record these messages 
and eventually play them back to restore the parameters and settings we recorded earlier. The param­
eters that can be sent out as SysEx include the general system settings of the device (such as intona­
tion, overall volume, and effects assignments), the multitimbral settings, also called the performance or 
mix settings (such as channel assignments, patch assignments, volume, and pan of each part), and the 
parameters of all or individual patches. As you can see, this function can be used only for hardware 
synthesizers and therefore its relevance has been diminished by the increase of software synthesizers. 
Nevertheless, I still find this feature useful for any hardware MIDI device in my studio. While it is true 
that almost all devices can store all this information in their own internal memory locations, it is also 
true that eventually you will run out of memory locations to store new patches or new performances. 
I usually recommend avoiding storing the multitimbral settings on only the devices’ internal memory, 
FIGURE 4.26
Example of custom SysEx object in DP.

183
4.5  MIDI System Exclusive Messages
for a variety of reasons. First, unless you use expansion cards, it is not possible to have a backup copy 
of these data, and if you read Chapter 3 carefully you know how I feel about not having any backup of 
a project’s data. Second, I like to keep all the data related to a certain project together in the same file 
for functionality, transportability, and organization reasons. The answer to these issues lies in the SysEx 
MIDI Dump function. As I mentioned earlier, a SysEx MIDI dump allows you to make a copy, through 
MIDI messages, of the internal parameters of a device. Depending on the device, you usually access 
this function from the “MIDI”, “Utility”, or “Global” menu (a specific device might have a different 
menu layout, but the procedure would be the same). Under these menus is a function called “MIDI Bulk 
Dump” that allows you to duplicate the entire internal memory (or part of it) of the device and send it 
through the MIDI OUT to a connected sequencer. When you prepare to make a MIDI dump transfer, you 
have to select which information and parameters you want to output. Usually you have several choices. I 
have summarized the most common ones in Table 4.9.
Let’s learn in more practical terms how to organize SysEx dump sessions and how to integrate them 
with your projects and sequences. After finishing your work on a project or after a temporary sequencing 
session, it is a good idea to store all the settings of your synthesizers, sound modules, and MIDI devices 
in general as a SysEx MIDI dump. You can choose among several ways of organizing your MIDI dump 
data. You should create a new MIDI track for each device and record each dump for each track/device 
separately. If you choose to do so, remember these few hints. Mute the other tracks before recording the 
SysEx, because these data take a big chunk of the bandwidth of the MIDI network, and the last thing you 
want to have is a MIDI device receiving performance data while at the same time sending SysEx. This 
may result in corrupted SysEx messages or MIDI system crashes. Also remember to mute the SysEx 
tracks when playing back the regular performance data, for the same reason. Another option you have 
(which I prefer) is to create a separate sequence, as part of your current project, where you record only 
the MIDI dump. This means that, depending on your sequencer of choice, you will have to either cre­
ate a separate sequence just for the SysEx data (DP) or create a new arrangement that will be dedicated 
to holding only the SysEx data (CU and LP). Another option is to create a new single track and to take 
Table 4.9  Most Used System Exclusive Messages (SysEx) Dump Functions.
Type of 
Parameter
Function
Comments
All
Sends a MIDI dump of 
the entire memory of 
the machine, saving 
everything
It is a good idea to make a global dump once in a while to 
have a fresh copy of all the patches, performance, and system 
settings. I recommend also performing such a MIDI dump 
right after buying the device so that you will always be able to 
restore the device to its original conditions if necessary
Performances
Sends a MIDI dump of the 
performances (settings 
about the multitimbral 
parameters)
This function, depending on the manufacturer and model of 
the keyboard, can sometimes also be referred to as “Mix” or 
“Multi”. You can also select a range of performances (or a 
single performance) to send as a MIDI dump
Patches
Sends a MIDI dump of the 
patches (settings about 
the individual patches)
You can also select a range of performances (or a single 
performance) to send as a MIDI dump
Temporary 
Memory
Allows you to send as 
MIDI dump only the 
current performance or 
patch that is being edited
This is particularly helpful if you adopt the MIDI dump technique 
as a regular way of storing your devices’ settings

184
CHAPTER 4  Advanced Sequencing Techniques
advantage of multiple takes/playlists to accommodate several MIDI dumps at the same time (DP and 
PT). Make sure when you record the SysEx for a certain device that the other tracks are muted to avoid 
unnecessary MIDI data flow. Let’s take a closer look at how each sequencer can be optimized to handle 
a SysEx MIDI dump session.
In DP I suggest creating a separate sequence to store the MIDI dump data. You can create a new 
sequence by using the drop-down menu called “Sequence Menu” in the top left corner of the Track List 
window. Once you have selected the new sequence, rename it “MIDI Dumps” and make sure to leave 
only the MIDI tracks you need for the SysEx dump of each device. Record-enable the first track (name 
the track with the device’s model), and make sure you’re ready to send the dump from the device. How a 
MIDI device sends MIDI SysEx dumps depends on the manufacturer and the model. In most situations 
you have to browse to the MIDI Dump menu of your device, select the data you want to include in the 
SysEx messages, start recording on your sequencer, and then trigger the transmission of the SysEx data 
by pressing the “Enter” key on the device’s front panel. Once the data are recorded, you can see them as 
regular MIDI data, except they will appear in hexadecimal code and you cannot really edit them without 
damaging the dump irreparably, unless you are a seasoned MIDI programmer.
Since SysEx messages take a big chunk of bandwidth of the MIDI network and they are not so com­
mon in terms of basic sequencing techniques, sometimes sequencers are set by default to ignore such 
messages. It is good practice to double-check that there are no MIDI filters programmed to ignore SysEx 
messages before recording a MIDI dump. In DP you can check the status of the MIDI filters by navigat­
ing to the “Set Input Filters” in the Setup menu (Figure 4.27). Make sure the “System Exclusive” item is 
checked.
Remember that you have to rerecord the MIDI dump after every session if you made some changes 
to the current setup of a device. To “recall” the settings when you reopen the project, simply select the 
sequence with the MIDI dump data, and play back the tracks containing the SysEx messages. Make sure 
FIGURE 4.27
Input Filter in DP.

185
4.6  Summary
that each track is output to the correct device. As soon as the playback of the data begins, the devices 
will display a message that acknowledges the receipt of a MIDI dump. After a few seconds (or a few 
minutes, depending on the amount of data you originally transferred as SysEx), the device will go back 
to regular performance or patch mode, with all the settings you originally programmed.
As in DP, in LP, CU, and PT create tracks for each device, and make sure they are output to each 
device for which you record a MIDI SysEx dump. In LP, to check whether the SysEx messages are 
filtered, open File  Project Settings  MIDI and select the “Input Filter” tab. Make sure the SysEx 
checkbox is not checked and that in the General tab the “System Exclusive MIDI Thru” function is 
not active (this option is used only when hardware controller units are employed to program an exter­
nal sound module or synthesizer). In CU you can check the MIDI filter options by opening the main 
Preferences menu and selecting the “MIDI Filter” submenu. Make sure the SysEx parameter is not 
checked in the “record” column. In PT you can either create a separate session or reserve some of the 
MIDI tracks for the SysEx MIDI dump. Once again, make sure the SysEx messages are not filtered, by 
selecting Setup  MIDI  Input Filter.
No matter which DAW you use or which technique you prefer, the use of SysEx messages could save 
you precious time that can be put into the creative flow, and it is also a good way of avoiding major dis­
asters in terms of data loss, since the SysEx dumps will be saved and (hopefully) backed up with all the 
other project information.
4.6  SUMMARY
In this chapter we focused on advanced sequencing techniques. Having learned the basics of MIDI and 
some intermediate features, it was time to make the big jump and master complex techniques, such as 
custom groove-quantize creation, quantization of audio material, and the use of MIDI and audio plug-ins.
The creation of customized grooves to use during the quantization session is one of the most intrigu­
ing features of modern sequencing. Custom grooves can be created from scratch or morphed from MIDI 
or audio parts. In DP we can customize the quantization grooves by manually creating and altering the 
parameters of the groove through the “Create Groove” option found in the Region menu. The groove 
can be based on a preexisting MIDI part already present in your sequence. All four DAWs allow you to 
create new quantization grooves not only from an existing MIDI part but also from audio material. This 
option greatly expands the boundaries of sequencing and helps reduce the stiffness often associated with 
traditional quantization techniques. While the technique used to create custom grooves in CU, LP, and 
PT is similar to the one learned for DP, the procedure applied to create grooves based on audio is slightly 
more complex. In CU you have to create, from the waveform editor, hitpoints that correspond to the most 
prominent and rhythmically important waveform transients. The markers created will be used by CU as 
a reference to calculate the parameters for the new groove. In LP a similar result is achieved using the 
“Audio to MIDI Groove Template” from the Functions menu in the waveform editor. A series of param­
eters, such as granulation, attack range, smooth release, and velocity threshold, then allows you to choose 
the best settings for the specific audio material you are working on. In PT you can access similar features 
regarding groove quantization and audio files using the engine called “Beat Detective” (BD). With BD 
you can achieve several tasks all related to a selected audio region. You can, for example, find out the 
tempo of a loop or extrapolate the groove of an audio loop and save it as a groove template which is 
usable on any MIDI or audio region.
All the grooves created through the aforementioned procedures can be used and accessed from any 
quantization window in your sequencer and utilized for both MIDI and audio parts. In order to be able 
to quantize an audio part, the part itself needs to be prepared. It has to be sliced according to a fixed grid 

186
CHAPTER 4  Advanced Sequencing Techniques
(8th, 16th notes, etc.) or by using its waveform’s transients. Once the audio part has been cut into several 
smaller subsections, each region can be moved and quantized as if it were an independent element, greatly 
increasing the quantization flexibility of audio tracks. If your sequencer is able to import and directly han­
dle audio files in Recycle’s rex2 format (which means audio files or loops that are already processed and 
sliced), then your “recycled” audio files will follow automatically the tempo changes programmed in the 
sequence. If not, you will have to slice them manually from the waveform editor of your sequencer.
Even though the editing techniques we learned in Chapter 3 are extremely useful and constitute the 
most common procedures for editing MIDI data, there are more advanced options when it comes to crea­
tive use of MIDI. The “offline” editors allow you to apply some predetermined editing macros according 
to parameters and conditions set in the editor. While certain sequencers, such as CU and LP, provide dedi­
cated offline editors that allow you to filter and transform MIDI data according to the nature, position, and 
type of the data, DP and PT have more structured and predefined filters that can achieve similar results 
with less programming. Such filters include “Change Velocity”, “Change Duration”, “Transpose”, and 
“Split Notes”. In LP and CU you can use the “Transformer” and the “Logical Editor” to create your own 
MIDI filters tailored to specific tasks and editing goals, respectively. By setting the conditions (meaning 
which events will be affected) and the functions (meaning which actions will be applied to the filtered 
data), you can edit large sections of your project with a single command. In DP, PT, and CU, most of the 
offline editing techniques can also be applied in real time through the use of MIDI plug-ins that act as 
filters inserted in the mixing board channel of the track right before the MIDI data are sent out to a MIDI 
device. Such filters can include MIDI delays, quantization, arpeggiators, velocity change, and duration 
change. In LP and CU you can access basic MIDI filters from the Parameters Box and the Inspector win­
dow, respectively. Real-time MIDI plug-ins are handled by LP through the Environment editor.
Audio tracks allow for greater manipulation through effects and plug-ins than MIDI tracks. Since in 
the case of external MIDI devices the generation of the actual sound resides outside the computer and 
the sequencer (software synthesizers are an exception), we cannot use the internal sequencer plug-ins to 
affect MIDI tracks directly. The audio material recorded on audio tracks, in contrast, is generated inside 
the computer. We can therefore take advantage of the plug-in system to treat the audio material. The way 
effects are used and applied to audio tracks in a sequencer is very similar to the more traditional analog 
mixing environment. By using an effect as an “insert”, we let 100% of the original dry audio signal 
be affected. This technique is used mainly for dynamic effects (such as compressor, limiter, gate, and 
expander) or equalizers. While the insert technique works well in situations where the effect does not 
need to be shared and we want to affect only the track on which it was inserted, it is not the most flexible 
and efficient way of using effects that need to be shared among several tracks. For this particular purpose 
we can use the “send” technique, which involves the use of an auxiliary channel on which we insert an 
effect (usually an ambience effect, such as reverb, delay, or echo). The aux channel is set to receive the 
input from a bus (an internal signal path that connects different internal components of the virtual mix­
ing board in the sequencer). Each channel we want to be affected by the effect inserted on the aux chan­
nel will be sent to the aux through the use of a send, which is output to the same bus on which the aux 
is receiving. By controlling the amount of signal sent to the aux through the send fader, we can control 
how much effect will be applied to the tracks.
As part of advanced editing techniques you learned how to use the SysEx MIDI messages to program 
custom SysEx objects, and also how to store and recall entire configurations and settings from and to 
the MIDI devices in your studio through the MIDI Dump function. This is a very useful technique not 
only to quickly reset your studio with all the configurations related to a certain project, but also to have a 
backup copy of the settings, multiple channel assignments, and patches you created for a project. In order 
to do a MIDI dump, make sure your sequencer is not set to filter and therefore ignore System Exclusive 

187
4.7  Exercises
messages. Create a separate sequence/arrangement for the SysEx data (this will guarantee that you won’t 
try to play back the regular sequence data and the SysEx data at the same time). Create or reserve a single 
track for each device you wish to send a MIDI dump from. Next, record-enable one track at a time, press 
Record, and, after waiting a few seconds, start the MIDI dump from the selected device. You can choose 
to dump all the settings stored inside the device (“All”) or specific parameters, such as patches, multiple 
channel assignments, or even single entries, such as a single patch. To recall the setting, simply assign the 
output of each track containing the SysEx dump to its respective device and press Play; the data will be 
received by the devices, and every setting saved in the dump will be restored.
If you have mastered the techniques explained in this chapter, you should congratulate yourself! 
From Chapter 1 up to this point you have explored, learned, and practiced several concepts and tech­
niques that allowed you to improve your sequencing skills in order to bring your productions to the next 
level, and to be able to focus on the creative process without having the technical aspect of sequencing 
interfering too much with your compositional attitude. In the next chapter we will learn how to specifi­
cally deal with different sections of an ensemble in terms of sequencing techniques, sound layering, and 
MIDI editing.
4.7  EXERCISES
Exercise 4.1
Using any of the standard MIDI file sequences on the website, import two MIDI parts and create two groove-
quantize presets you consider useful for your particular style of writing.
Exercise 4.2
Using any of the audio loops on the website, import two audio loops and create two groove-quantize presets 
you consider useful for your particular style of writing.
Exercise 4.3
Using any of the audio loops on the website, import two audio loops (different from the ones used in Exercise 
4.2), and with the slicing technique alter the original groove via different quantization options (try, for example, 
to apply an 8th note swing feel to an 8th note straight feel, and vice versa).
Exercise 4.4
Set up and record a sequence with the following features:
a.	 Ten MIDI tracks
b.	 Two audio tracks with loops of live instruments
c.	 Free instrumentation and tempo.

188
CHAPTER 4  Advanced Sequencing Techniques
Exercise 4.5
With the sequence created in Exercise 4.4, apply the following edits using a combination of offline and real-
time MIDI filters:
a.	 Transpose at least two MIDI tracks one octave higher
b.	 Set an arpeggiator with a 16th note pattern
c.	 Apply a note velocity reduction of 10 to the first four bars of a MIDI track of your choice
d.	 Apply a note duration reduction of 5 to the first 16 bars of a MIDI track of your choice
e.	 Select all the notes of a percussion instrument (such as a hi-hat) from a composite drums track; cut them 
and paste them onto an empty track.
Exercise 4.6
On the two audio tracks created for the sequence of Exercise 4.4, apply a reverb effect as a send and apply 
one equalizer for each track as an insert.

189
Creative Sequencing Techniques for Music Production.
© 2011, 2005 Andrea Pejrolo. Published by Elsevier Ltd. All rights reserved.
CHAPTER
Elements of MIDI Orchestration
5
5.1  INTRODUCTION
In the first four chapters we learned several concepts and techniques related to the practical aspects of 
sequencing, such as MIDI messages, editing techniques, quantization options, audio and MIDI track use, and 
synchronization procedures. While these subjects represent a very important part of the knowledge required 
to achieve a higher standard in modern and contemporary production using MIDI/audio sequencers, the musi­
cal aspect of such productions also plays an extremely important role. The orchestration techniques involved 
in the realization and production of a project based on MIDI and audio are crucial to obtaining professional 
final results. I have referred to the equipment involved in the contemporary composition and production proc­
ess as “the orchestra of the twenty-first century”. This concept involves a learning process based not only on 
technical skills and procedures but also (and sometimes mainly) on orchestration techniques and rules that are 
part of a more traditional approach to writing, arranging, and orchestrating. This often means that no matter 
how well you know your sequencer, synthesizer, and sound modules, if you orchestrate a part for the wrong 
instrument or out of its range the results will be disappointing and the final production unprofessional.
As I mentioned earlier, the role of the modern composer has changed drastically in the past few 
years. Nowadays the composer is often also the producer, the orchestrator, the arranger, the MIDI and 
audio engineer, and the mixing engineer. Such multifaceted involvement in the production process 
requires a wide range of skills. This is why I firmly believe that to have an edge over the competition and 
to be a successful contemporary producer and composer you have to learn some traditional orchestration 
and arranging notions, and at the same time learn modern sequencing techniques.
In this chapter you are going to learn some of the most important elements of orchestration for 
acoustic and electronic instruments and how to merge the technical concepts you mastered in the previ­
ous chapters with the more traditional aspects of orchestration. Each of this chapter’s main parts ana­
lyzes a different section of the modern orchestra, starting with the rhythm section and moving to the 
string section, the wind section, and finally synthesizers. For each I will provide key features, sequenc­
ing techniques, the instruments’ range, and sound layering techniques to achieve the best results in terms 
of MIDI orchestration. This chapter is useful both to seasoned orchestrators who need information on 
how to create convincing mock-up versions of their scores and to seasoned MIDI programmers who 
need to brush up on their orchestration skills. For more in-depth analysis of MIDI orchestration I recom­
mend reading my book Acoustic and MIDI Orchestration for the Contemporary Composer.
5.2  RHYTHM SECTION: OVERVIEW
The rhythm section constitutes the core of most commercial and contemporary compositions and pro­
ductions. Since most of the current writing (especially in the pop and commercial style) is groove 

190
CHAPTER 5  Elements of MIDI Orchestration
driven, the rhythm section has achieved a predominant role in most sequences and projects. The charac­
teristics of the instruments that form the rhythm section can greatly influence the main style of the com­
positions and, depending on their qualities and timbres, the sequencing techniques involved can change 
drastically.
The modern rhythm section usually comprises the following instruments: piano, guitar(s), bass, 
drums, and percussion. I intentionally did not include synthesizers in this category because they are able 
to produce such a large, versatile, and heterogeneous group of textures and sonorities that they deserve a 
separate section (Section 5.6). As I mentioned earlier in the book, the best way to achieve more convinc­
ing and professional productions is to use a combination of real acoustic instruments and MIDI instru­
ments. Therefore, where it is possible, I highly recommend taking advantage of the audio capabilities of 
your sequencer and recording as many live instruments as the budget allows. Nevertheless, in most situ­
ations you will find yourself having to use MIDI samplers and synthesizers to sequence your projects. 
Let’s analyze in detail how to get the most out of your MIDI gear to achieve a realistic feel and texture 
for the rhythm section instrumentation.
5.2.1  The Piano
By the term piano we usually mean the piano forte, acoustic piano, or grand piano. This instrument has 
the widest range of any acoustic instrument (Figure 5.1), covering more than eight octaves, from A0 
(27.5 Hz) to C8 (4186 Hz).
The piano can play several roles within the rhythm section. It provides the necessary harmonic sup­
port by outlining the chord progression of a composition, and it provides the rhythmic support and defi­
nition required by the rest of the rhythm section. Because of its versatility in terms of dynamic range it 
can be used to reinforce passages in which other sections can take advantage of its fast attack, and it can 
be used as a solo instrument during soft passages. The acoustic piano is one of the acoustic instruments 
that is easy to reproduce in a MIDI environment, mainly because its acoustic features can be fairly easy 
to synthesize and reproduce through sampling techniques. In addition, the controller used to sequence 
piano parts is usually a keyboard.
Even though this may seem obvious, one of the first aspects to pay attention to when sequencing 
a piano is making sure you use a sustain pedal attached to your keyboard controller to simulate the 
realistic feel of an acoustic piano. In my career I have seen many MIDI producers trying to sequence 
piano parts without a sustain pedal, something you should definitely avoid. What really makes a dif­
ference when sequencing piano parts is the sound or patch you use. Most of the midrange synthesizers 
you can buy nowadays have pretty decent acoustic piano patches that can be applied in pop, rock, or 
FIGURE 5.1
Range of the acoustic piano.

191
5.2  Rhythm Section: Overview
contemporary commercial music. In most cases they cannot emulate a realistic piano texture in more tra­
ditional and acoustic music styles, such as jazz and classical. Most of the piano voices available in syn­
thesizers are thin and bright, and they do not capture the real feel and warmth of an acoustic instrument.
In general, synthesizers have a limited amount of read-only memory (ROM) in which to store the 
raw samples used to construct the built-in patches. This means that the original samples need to be either 
compressed in size or limited in length, contributing to a poor sonic fidelity of the final sound. If you are 
looking for a realistic and versatile acoustic piano sonority, you can look for a ROM expansion board for 
your synthesizer. Most of the professional synthesizers allow you to expand their sound palette with ded­
icated and proprietary expansion boards that usually contain high-quality samples of a specific instru­
ment category (e.g., bass, piano, vintage keyboards, strings, brass) or musical style (Latin, Asian, drum 
and bass, techno, hip-hop, etc.). A better option is to use a sample-based software synthesizer, meaning 
a software sampler that doesn’t reproduce sounds through synthesis but instead plays back waveforms 
stored in the computer’s random-access memory (RAM). The waveforms can be sampled from real 
acoustic instruments or from any other available source. The advantage of this technique is that (depend­
ing on the memory installed) the sampler can hold multiple samples, each longer than the ones stored in 
a synthesizer, resulting in a more realistic reproduction of the original acoustic sonority. I highly recom­
mend the use of a sample-based software synthesizer to reproduce any acoustic instrument, especially 
complex timbres such as strings, brass, and percussion.
As I mentioned earlier in this book, in recent years, owing to the increased power of computers, 
several software samplers have emerged and become the standard source for the sequencing of acoustic 
instruments (though not only acoustic). The advantage of software samplers (software acoustic libraries 
in general) is mainly that computers usually can take advantage of a larger amount of memory and that, 
if the particular software allows, they are able to stream the various samples directly from the hard disk 
(HD), virtually eliminating any sample-size restriction associated with traditional synthesizers or hard­
ware samplers. Among the most used software samplers are Kontakt by Native Instruments, MachFive 
by MOTU (Figure 5.2), EXS 24 by Apple, Independence by Yellowtools, and SampleTank 2 by IK 
Multimedia (Figure 5.3), just to mention a few.
FIGURE 5.2
Software samplers: (a) Kontakt and (b) MachFive.

192
CHAPTER 5  Elements of MIDI Orchestration
Another advantage that software samplers have is that you can assign multiple samples (instead of a 
single sample) to each key. Multisamples allow you to reach a higher level of realism in terms of sound 
and performance by assigning several waveforms of the same note to each key. Each multisample is 
triggered by a different Velocity On value. This means that when playing soft passages, the sampler will 
play back waveforms that were sampled by striking the keys of the acoustic piano with a softer pressure, 
while when playing louder passages the sampler will trigger waveforms sampled by striking the keys of 
the acoustic piano with a harder pressure. This technique is applied to any other sampled acoustic sound, 
such as string and brass. The number of multisamples allowed by the sampler depends on the sophistica­
tion of the software engine used.
Another option that allows you to have the best of both worlds (MIDI and acoustic) when it comes 
to piano sequencing is based on the integration of an acoustic piano with a MIDI interface. I mentioned 
this type of device earlier in the book when discussing different types of keyboard controllers. Such 
devices involve the use of an acoustic piano to which hardware mechanisms are applied in order to trans­
late the notes played on its keyboard in MIDI data, and vice versa, to translate incoming MIDI data into 
hammers and pedal actions. This allows you to sequence piano parts using a real piano keyboard action 
and, even more important, to play back the sequenced MIDI part through the acoustic mechanism of 
the piano. Among such devices the Yamaha Disklavier has been recognized as the most reliable and the 
most used in the industry. These types of devices are very flexible. You can use them to input the MIDI 
parts in a sequencer, edit the MIDI data as you wish, and then, when the final part is ready, simply play 
back the MIDI track and record the acoustic piano with microphones on a stereo audio track. The sound 
is that of a real acoustic instrument, while the data editing applied has the flexibility of a regular MIDI 
track.
In terms of sequencing techniques when laying down piano parts, I suggest avoiding as much as pos­
sible copying and pasting to repeat recurring passages. Instead, play the part live as much as you can. 
Small changes in velocity, duration, and position of the notes can add a much more realistic feel to the 
part you sequence. If your controller gives you the option, use a soft pedal (Controller 68) to attenuate 
the volume during soft passages. You can also achieve similar results via a generic expression pedal con­
nected to your keyboard controller. This will allow you to control the volume of the MIDI devices con­
nected to your MIDI network.
FIGURE 5.3
Software samplers: (a) EXS 24 and (b) SampleTank 2.

193
5.2  Rhythm Section: Overview
In terms of the patch and panning option, I recommend using a program that has a mild panning of 
the stereo image based on the range of the piano. This means the middle of the key range will be panned 
in the center, with the higher and lower ranges panned slightly left and right, respectively. Try to avoid 
extreme wide or narrow panning settings unless required by a particular style of music, for example solo 
piano pieces.
The category of pianos includes not only the pianoforte but other keyboard instruments, such as elec­
tric piano, Rhodes, Wurlitzer, and MIDI piano. The range of electric pianos is usually more limited than 
that of the grand piano, as shown in Figure 5.4(a, b).
For this type of sonority you can use either a sampler or a synthesizer, with very good results. The 
sampler will give you a sonority closer to the original “real” instrument, while a synthesizer will give a 
more creative approach in terms of texture and sonority. One thing you may consider experimenting with 
is patch layering with several piano sounds (pianoforte, electric piano, etc.) to create new sonorities or to 
correct timbre unbalances that might limit your production. I often find that layering two electric pianos 
with opposite acoustic features helps in achieving a much richer and appealing timbre. I always try to 
start with the best sound I can come up with before using effects such as equalization and dynamics to 
tweak it. Often a lack of bass frequencies in a patch can be fixed by simply layering a slightly differ­
ent patch that features more of the low end of the spectrum. In the same way a lack of definition in the 
middle–high register can be resolved with the addition of an edgier patch.
5.2.2  The Guitar
The six-string guitar is one of the most versatile instruments in the rhythm section. Its range is shown in 
Figure 5.5. Keep in mind that the guitar sounds an octave lower than written. The open strings (E, A, D, 
G, B, E) are common to all different types of guitars normally used in contemporary music production. 
Parts written for guitar usually are in keys in which the open strings are featured the most, such as the 
keys of C, G, D, A, and, to a certain extent, F. Special tuning techniques can be used though to facilitate 
the performance of parts not directly related to these keys.
When sequencing guitar parts there are a few points that you have to keep in mind. As we will see 
with other acoustic instruments, with guitar sequencing it is very important to start with a convincing 
and realistic part. If you try to sequence a guitar part that would be impossible to play on a real guitar, 
then your battle is lost right from the beginning. Always try to sequence parts that a guitarist would be 
able to play on a real instrument. This may seem obvious, but it is crucial always to keep this concept in 
FIGURE 5.4
Range of (a) the Rhodes Mark I and (b) the Wurlitzer 200 series electric pianos.

194
CHAPTER 5  Elements of MIDI Orchestration
mind for any acoustic part you are sequencing. In the case of the guitar it is important to use a convinc­
ing voicing of the notes you play. Definitely avoid using close “piano voicing” that would sound too 
clattered. Also avoid intervals smaller than a major 2nd unless you use one of the open strings. If you 
are sequencing a melody, use the Pitch Bend and Modulation wheels to create small sliding effects and 
light vibrato, respectively. This will add a nice realism and will cover up for some of the stiffness of the 
synthesized patch.
No matter how hard you try to sequence convincing guitar parts from a keyboard controller, the 
results will most likely be disappointing. For complex parts I definitely suggest using a guitar-to-MIDI 
controller like the one described in Chapter 3. The first advantage of using such a device is the correct 
voicing. A second, more subtle but no less important advantage is the timing with which each note of 
a chord would be sequenced. By using a keyboard most of the time you have a tendency to play all 
the notes forming a chord at the same time, as you would with a piano. When strumming the strings 
of a guitar, though, the notes forming a chord are separated by a few milliseconds, contributing to the 
peculiar guitar sonority. Even though through meticulous editing you could reach the same effect after 
sequencing the part with a keyboard controller, by using a guitar controller directly, not only can you 
save a lot of time, but you will achieve more convincing results. The same can be said for bending and 
vibrato effects. While the use of the Pitch Bend and Modulation wheels can be helpful in mimicking the 
expression techniques of a guitar, you will never get the same results as you would through a guitar-to-
MIDI controller. Keep in mind that, depending on your playing skills and guitar controller, you may end 
up having to edit your parts even after sequencing them with such a device. Therefore, I always recom­
mend a little bit of cleaning after recording such a part.
In terms of sounds and sonority, most of the concepts I pointed out when discussing the piano can be 
applied to the guitar. Sample-based software synthesizers usually give you the best patches and the most 
realistic timbres when it comes to acoustic guitar. In particular, samples of nylon string guitar can be 
fairly accurate in reproducing the acoustic instrument. Steel string guitars can also be reproduced accu­
rately, especially in arpeggio passages, while in strumming passages they are harder to replicate (almost 
impossible from a keyboard and a little bit easier from a guitar controller).
One technique I usually use in sequencing and recording MIDI guitar parts is to run the output of 
the synthesizer or sampler I am using to generate the guitar sound through a guitar amplifier in order to 
capture a warmer and more realistic sound (this technique is called re-amping). After sequencing the part 
and after making sure that all the notes are edited, quantized, and trimmed in the way you want, send 
the audio signal of the software synthesizer playing the guitar part through a guitar amplifier. Record 
the signal of the amp through a good microphone onto an audio track in the sequencer. The warmth and 
realism of the ambience captured by the microphone will add a great deal of smoothness and realistic 
texture to the synthesized guitar patch. This technique can be used with any type of guitar patch, from 
classical to folk, jazz, or rock. This approach works particularly well with guitars that need medium to 
FIGURE 5.5
Range and open strings of the guitar.

195
5.2  Rhythm Section: Overview
heavy distortion or saturation applied to their sound. Even though it is possible to apply such effects 
through a plug-in inside the sequencer, I prefer using a real amp and the microphone technique to get as 
close as possible to a live situation.
Listen to Examples 5.1 and 5.2 on the website to compare a stiff guitar part, sequenced using 
a keyboard controller played directly from the MIDI module, and a more convincing part, sequenced 
using a guitar-to-MIDI controller and the sound run through an amplifier to which I applied some mild 
saturation.
5.2.3  The Bass
The bass is also a transposed instrument, like the guitar. It is written on staff paper one octave higher than 
it actually sounds. The two main types of basses are the electric (also called bass guitar) and the acoustic 
(also called double bass). The electric bass can have four, five, or six strings. The four-string bass fea­
tures E, A, D, and G as open strings; the five-string bass usually has a lower B to add a deeper texture to 
the original setting; while the six-string version adds a top C that can be also tuned as a B, depending on 
the preferences of the player. Acoustic basses are somewhat more limited in range; they usually feature 
four strings (E, A, D, G), although some models have five. Classical double basses can have an extension 
installed on the low E string that allows the player to go down to a low C. In Figure 5.6 you can see the 
range of the bass.
What you learned when I discussed the guitar can be applied almost verbatim to the bass and electric 
bass. In sequencing acoustic and electric bass parts, try to avoid double-stop intervals (two notes played 
at the same time) lower than a perfect 4th unless you want to create a particular dissonant effect. If you 
have a bass part that requires a double-stop with a major or minor 3rd interval, it is better and more real­
istic to use a major or minor 10th instead.
Most of the synthesizers available nowadays can reproduce convincing electric bass sounds, espe­
cially for certain musical styles, such as rock and pop. As in the case of guitar sounds, a sample-based 
engine will give you better option when trying to reproduce the acoustic bass sonority. Use the amplifier 
technique illustrated earlier to capture a warmer and more realistic sound. If you do not have an ampli­
fier at hand or your microphones are not of professional quality, you can also use a tube preamplifier to 
filter the output of your MIDI module. While this technique is not as effective, it is nevertheless more 
practical and less time consuming. While the sequencing of bass parts is somewhat easier than guitar 
parts using a keyboard controller (because bass features mainly monophonic lines), I highly recommend 
sequencing bass parts with a bass-to-MIDI controller if possible. The feel and natural flow will increase 
dramatically, and this factor will contribute not only to the bass part itself but also to the overall groove 
of the project.
FIGURE 5.6
Range and open strings of the six-string bass.

196
CHAPTER 5  Elements of MIDI Orchestration
5.2.4  Drums and Percussion
The results obtained when sequencing drums and percussion parts can vary substantially, depending on 
several factors, such as the style of music, the groove you are trying to reproduce, and the repetitive­
ness of the patterns. There are two key elements that play a very important role in drums and percussion 
sequencing: sounds and rhythmic feel. In terms of sounds and patches, both synthesized and sample-
based patches offer a good palette of sonorities. While the latter are indicated more for recreating live 
drum sets or percussion ensembles (e.g., jazz ensembles, classical orchestra), the former are usually used 
more in artificial orchestration projects (e.g., techno, dance, pop), where the drums and percussion are 
targeted to a more creative use of timbres.
I recommend always having at hand a software drum library entirely dedicated to drum sounds, in 
order to be able to quickly find the most appropriate sonorities in a short space of time. By using a sam­
ple-based engine you can take advantage of the multisample technology, which in the case of drums and 
percussive sounds can greatly improve the accuracy and fidelity of your parts. This technology is par­
ticularly effective for drum sounds since they change depending on the strength with which a drummer 
hits a certain instrument. A snare, for example, features a totally different attack, release, and color if it 
is struck softly versus hard. The multisample technique allows you to experience a similar response by 
assigning several different samples to different MIDI velocities. When you play a drum part, the sound 
will change depending on the velocity at which you play the notes on your controller. Particularly inter­
esting are advanced software libraries, such as BFD 2 by FXpansion (which I used for the drum parts 
of the examples on the website), which allows you not only to control multisampled sounds but also to 
change the room and virtual microphone placement of the kit you are using in the sequence. The quality 
of the final result depends greatly on the style of music you are sequencing. Usually repetitive rhythms 
that involve little or no variation (such as dance and, to a certain extent, pop rhythms) can be sequenced, 
with very good results. Other styles, where variations and changing groove play a predominant role, are 
more problematic. A typical example is jazz. This is one of the hardest drum feels to reproduce with a 
MIDI sequencer. One of the main aspects that defines jazz is the swing feel that is outlined by the drum­
mer, using a combination of patterns and accents that is almost impossible to recreate in a MIDI setting.
While the quantization techniques we learned are definitely a good starting point, there are a few 
other approaches I would like to suggest for getting the best possible rhythm tracks for your projects. 
First of all you should invest in a MIDI pad device in order to avoid playing drum parts on a keyboard 
controller. Using a regular keyboard can flatten out the groove considerably, thereby making your drum 
parts sound stiff and mechanical. Sequencing a swing feel pattern on a MIDI drum set or MIDI pads is 
going to improve the overall rhythmic feel of the production. Even if you are not familiar with drums 
and percussion techniques, it is a good idea to use such MIDI controllers to sequence your parts. Some 
of the most advanced MIDI pads allow you to use either sticks or hands to trigger the pads. This gives 
you a lot of flexibility when it comes to inputting percussion parts such as bongos and congas.
Another technique involves using a mix of MIDI and audio tracks to achieve a more realistic result. 
I always like to include at least one live percussive instrument, such as cymbals, shaker, or hi-hat (HH) 
along with the other MIDI drum tracks. As we learned at a macro level, the addition of one or more 
acoustic instruments can bring to life the other MIDI tracks, and the same can be said at a micro level 
when sequencing drums and percussion. You will be surprised at how much your groove will improve 
by simply adding a live shaker or tambourine to your MIDI parts. These instruments are cheap and fairly 
easy to play but their impact on a groove can be crucial.
Listen to Examples 5.3 and 5.4 on the website and compare the same groove sequenced using only 
MIDI tracks and after replacing some of the MIDI tracks with live instruments. In the case of a swing 
feel, I recommend, if possible, using a live ride cymbal or hi-hat, since these two instruments are the 
most characteristic of this particular style.

197
5.2  Rhythm Section: Overview
As with any MIDI track, try to avoid as much as possible the use of the “copy and paste” command 
to extend your drum patterns. This definitely works for styles based on repetitive (drum-machine-like) 
grooves, but for music styles where a live drummer would be required it is better to play all parts at least 
for an entire section without using the paste command. This will help to take some of the stiffness of 
the MIDI sequencer out. Sequence each drum and percussion instrument separately and on a dedicated 
MIDI track. This makes the editing easier and quicker. The only exceptions to this are the bass drum 
(BD) and the snare drum (SD). I strongly recommend playing these two voices together to get a tighter 
feel, similar to the one that a live drummer would get.
Another very important rule to keep in mind is that (as for any other instruments), to sound realis­
tic, the parts you sequence need to be playable by a live musician. Just as you shouldn’t sequence parts 
for instruments that are out of their playable range, in the same way you have to imagine a drummer 
performing the parts you are sequencing. One of the most common mistakes is to have more than four 
pieces of the drum set playing at the same time. Unless the drummer you are writing for has more than 
two arms and two legs, the chances are very slim that he or she can play more than four percussive 
instruments simultaneously (one exception would be the use of the special BD or HH pedals sometimes 
found in custom drum kits). Always remember to keep it real; this is one of the most important rules. 
You can always double-check the number of notes triggered simultaneously for the drum parts in either 
the graphic editor or the drum editor.
When you combine MIDI and audio drums tracks in the same mix, the effect you achieve can be 
very satisfying in rhythmic feel. Keep in mind, though, that mixing together such different audio sources 
can sometimes be difficult, owing to their substantially different sonic characteristics. The acoustic 
tracks (such as cymbals and shaker) can stand out too much compared to the more mellow and preproc­
essed MIDI tracks. In order to avoid problems during the mix and to give to the MIDI tracks a more nat­
ural ambience sonority, I usually rerecord through a couple of good condenser microphones, on separate 
audio tracks, the MIDI material played through the main speakers of the studio. This technique is similar 
to the one explained for the guitar and the bass. This will add a more natural reverberation to the MIDI 
parts and will allow you to avoid problems during the mix between MIDI and audio percussion tracks. 
If you want to achieve an even more “live” sound (perfect, for example, for jazz, funk, and orchestral 
percussion situations), you can record the entire drum mix played through the main speakers of your 
studio on a stereo track using a pair of good microphones. The farther you place the microphones from 
the speakers, the more you will be capturing the ambience of the room and therefore the more you will 
add “live” ambience. Conversely, the closer to the speakers you get, the more punchy quality and direct 
sound you will get. Take your time finding the perfect spot for the material you are dealing with; make 
a few attempts and choose the one that best fits the drum part you are sequencing. I usually find that a 
distance of four to six feet between speakers and microphones gives the best results, but this can change 
with the room, the microphones, and the speakers used.
For drums and percussion parts, it is becoming more common to bypass completely the MIDI sequenc­
ing techniques and use a series of loops to replace any attempt at live recording (MIDI or acoustic). I am 
not a big fan of this approach, for several reasons. While the use of loops can be interesting for embellish­
ing your production and gives a nice touch to a series of MIDI tracks, loops (by definition) are very repeti­
tive and can really flatten your sequences and make it extremely boring and cold, especially if used as the 
primary source for the rhythmic tracks of your productions. While loops can be a good source of inspira­
tion and can definitely be used for quick mock-up versions, they are uncreative and impersonal. I would 
rather listen to a simple MIDI drum track that is groovy and original than to a short sophisticated rhythm 
that keeps repeating over and over without variation. If a loop becomes an essential part of your project, 
then I strongly suggest spicing it up with some variations using either MIDI parts, acoustic live recording 
of a few percussive elements (e.g., shaker, hi-hat, cymbals) or, even better, a combination of the two.

198
CHAPTER 5  Elements of MIDI Orchestration
A good alternative to premixed stereo loops (which, unfortunately, constitute the majority of the 
loop-based rhythmic material available) are the multitrack versions of the prerecorded drum and per­
cussion parts offered by companies like Discrete Drums. This type of drum track has the advantage of 
providing the original multitrack version of the loops, allowing a much higher degree of flexibility at 
both the editing and mixing stages. Having separate tracks for each groove allows you to replace single 
instruments of the drum set to create variety (every so often you can change the HH or the BD pat­
tern, for example), plus during mixing you can apply different effects to each single track to achieve 
more realistic results. You can also use the multitrack sessions in a creative way by mixing and matching 
parts from different grooves (e.g., the HH part from a hip-hop groove with the BD and SD from a funk­
rhythm). No matter which technique or combination of techniques you use, always try to be creative, 
and do not let the sequencer or the loops dictate your music.
A fairly new hybrid approach, based on the use of loops and a more traditional drum library, has 
become more popular in drum libraries such as Digidesign’s Strike and Toontrack’s EZdrummer. The 
idea behind this approach consists of combining the flexibility of the event-based sequencing of MIDI 
and the quality and fidelity of loop-based grooves. In Strike, for example, you can choose a groove and 
its variations from a comprehensive bundled library of styles as you would do with a loop library, but 
then the software engine allows you to adjust the timing, intensity, dynamic of every piece of the kit sep­
arately and control these parameters overtime through MIDI Control Changes (CCs). The final results 
are absolutely stunning and usually it takes a fraction of the time it would take with a traditional MIDI 
drum library. I recommend having one of these tools in your digital orchestra!
5.3  STRING SECTION: OVERVIEW
The string section is one of the most discussed and controversial aspects of MIDI, especially with regard 
to the sonority and the sound library used to reproduce the acoustic instruments. The modern string sec­
tion includes the violin, viola, cello, and double bass. Each of these instruments presents different sonic 
characteristics, playing techniques, and applications. It is very limited to refer to such different sonori­
ties with the generic term strings, as often found in many synthesizers’ patch names. If you used only 
the synthesizer patch called strings to sequence string section parts, you greatly limit the real potential 
of your virtual orchestra, since these generic patches often do not take into consideration the very dis­
tinctive sonic character of each string instrument. This is why it is extremely important to understand 
the functions and characteristics of each instrument forming the string section in order to achieve better 
results when sequencing string parts. In Figure 5.7 you can see the ranges and the open-string subdivi­
sion of the violin, viola, cello, and acoustic bass.
Each of the four main instruments forming the string section has specific characteristics, so let’s ana­
lyze each of them separately to outline their main features. Table 5.1 sums up the most important sonic 
elements and orchestration principles that apply to each of them.
5.3.1  Sequencing Strings
When sequencing the string section, the first thing to keep in mind is that you must voice them (meaning 
choose the notes to apply to each section) in the same way you would voice the parts for the live acous­
tic instruments. Often by playing the parts on the keyboard you can be tempted to use piano voicing that 
would be perfect for a keyboard instrument but does not work as well on string instruments.
When voicing for strings there are a few rules to follow. Try to avoid intervals equal to or smaller 
than a major 2nd in the higher range of the strings, as shown in Figure 5.8 (unless for some particular 
effects). This will guarantee a more powerful and stronger sonority.

199
5.3  String Section: Overview
Try to avoid close voicing between sections. It is better to use open voicing techniques to reach max­
imum clarity and full sonority. By using mainly 3rds you can achieve a very mellow and smooth texture, 
while the use of 4ths helps to create the impression of a larger ensemble, even with a small string sec­
tion. The 5th is used mainly in the lower register and in the bass and cello section to create a solid base 
on which the higher extensions of the chords can be built.
Experiment with different types of voicing before committing to the final one. Try always to think 
horizontally and in terms of lines that each section would play, instead of vertically. By doing so you 
will achieve a much more coherent and realistic-sounding string section, as if your virtual orchestra was 
really composed of several independent players. When sequencing the different sections, play each line 
FIGURE 5.7
Range and open strings of the violin, viola, cello, and bass.

200
CHAPTER 5  Elements of MIDI Orchestration
separately on a different track. For divisi create a separate new track and record the split section on inde­
pendent tracks. If possible, assign each section of the divisi to separate MIDI channels and slightly dif­
ferent patches to have more control over the individual volumes and pan of each part.
5.3.2  Sonorities and Sound Libraries
As with the other instruments analyzed so far, a sampler (instead of a synthesizer) gives you the 
best options in terms of sounds. This is particularly true for the string section, where the complexity, 
Table 5.1  Characteristics of the String Family Instruments.
Instrument
Sonority
Range
Comments
Violin
Each string has a particular sonority: the 
E and G project the sound better, while 
the two inner strings (D and A) are more 
mellow. Violins usually take the lead part 
such as the melody in a section. If the 
section is big enough the lines can be 
split in “divisi” style. If you use the divisi 
option, try to keep the balance between 
sections by using more violins playing 
the top line (melody)
The higher range of the 
violin has a thinner sound 
and therefore it is often a 
good idea to reinforce it 
with other violins or with 
violas
Violins usually work better 
in larger groups; the 
bigger the section the 
sweeter and mellower 
the sonority. Try to avoid 
sequencing solo violin 
lines for fewer than three 
violins in a section
Viola
Violas have a darker and warmer 
sound than violins, mainly because 
of the lower range
The violas usually play 
the inner notes of a chord 
when voiced with violins 
and cellos
Try to avoid sequencing 
solo viola lines for fewer 
than two violas in a 
section
Cello
In terms of sonority the cello has the 
strongest one of the entire string family.
It can be used as a solo instrument 
even against a larger number of violins 
or violas
It has the same open 
strings position of the 
viola but pitched 1 
octave lower
The high register of 
the cello is loud and 
pleasantly edgy, while the 
lower register is warm 
and rich
Double bass
The bass has a deep, dark sonority. In 
the higher register it can be melodic 
but not as agile as the cello
It features the lower 
register of the string family 
instruments
Pizzicato style can be 
effective in both classical 
and jazz styles
FIGURE 5.8
Full sonority range for strings.

201
5.3  String Section: Overview
richness, and selection of tones are greater than for any other acoustic instrument. While synthesized 
strings can be effectively used for more creative productions in musical styles that do not necessarily 
need real acoustic sonorities, more acoustically-oriented projects that demand a realistic sonority must 
employ sample-based libraries. In terms of sounds, here are some suggestions for improving your vir­
tual orchestra. What makes a MIDI string part “alive” is variation and the correct use of all the different 
colors that are peculiar to the string family. The use of different bowing techniques and color effects, 
such as sul tasto, sul ponticello, col legno, tremolo, pizzicato, and glissando, will improve the realism 
of the sequenced string parts. Table 5.2 gives you a quick description of these techniques and sonorities.
You can achieve different timbers and colors by using one of these three techniques: applying pro­
gram changes when needed, assigning different patches for each bowing technique to different MIDI 
channels, or “preferably, using key switch” patches if available. When you need a different color, you 
will sequence that particular part on the most appropriate track. Let’s take a look at each technique.
For smaller studios, the use of program changes to switch the patch/sonority is a valid alternative. 
Unfortunately, if the switch needs to be quick, sometimes this approach can be problematic owing to the 
delay introduced by the sound generator during the patch change. In addition, usually, program changes 
work fine with external MIDI hardware devices but can be hard to implement for software synthesizers.
Using different MIDI tracks for different articulations can be very effective since it allows you to 
quickly jump from one color to the other. Keep in mind that this procedure will take several MIDI chan­
nels and therefore can be used only if your studio has enough sound modules or software samplers 
available. In addition, track management can become problematic since you could easily end up with 
hundreds of tracks, and in the case of the software synthesizer it could really put your computer to the 
test in terms of power.
The key switches technique is probably the most advanced and definitely the most musical of the 
three. Key switches allow you to quickly instruct your virtual section or orchestra to switch to a dif­
ferent set of samples while remaining on the same MIDI channel and devices. When you load a key-
switch-enabled patch in your sampler, you load not only one sonority (e.g., pizzicato) but several at 
the same time (e.g., sustained, tremolo, sforzando). In contrast to the multilayers technique, with key 
switches you do not change samples with velocity or CC, but instead you use preprogrammed MIDI 
Table 5.2  Bowing Techniques.
Color/Technique
Description
Comment
Sul tasto
Indicates that the player needs to use the bow 
closer to the fingerboard
Produces a mellow and velvety 
sound
Sul ponticello
Indicates that the player needs to use the bow 
closer to the bridge
The opposite of sul tasto. It 
produces a harsh and edgy sound
Col legno
Indicates that the player needs to use the 
reverse side of the bow (the wooden part) to 
tap or strike the strings
Mainly used for staccato or 
rhythmic effects
Tremolo
Achieved by bowing the string by rapidly 
alternating between up- and down-bow
Creates a dramatic and suspended 
feel
Pizzicato
Achieved by plucking the strings with the fingers
Creates a gentle percussive effect
Glissando
Achieved by sliding the left hand on a string 
between two notes on the same string (also 
referred to as portamento)
Can be used for particular effect

202
CHAPTER 5  Elements of MIDI Orchestration
notes, conveniently placed outside the playable range of the instrument you are sequencing to quickly 
switch between all the available articulations of a particular patch. There are two main techniques to use 
key switches patches. You can record the switches while you are actually recording the part itself. This 
approach has the advantage of saving you time and allowing you to actually perform the part as it was 
written. This translates into a more flowing and realistic performance. The main disadvantage of this 
technique is that it requires you to have good skills on your MIDI controller since you have to include 
the key switches changes as part of your performance. If the part is particularly fast and articulated it 
could be challenging. The other approach, which I usually prefer, is to sequence the entire part with a 
fairly generic sonority (e.g., “sustained”) and on a second passage overdub the key switches changes. 
This approach has the advantage of releasing the pressure of performing both tasks at the same time. 
Most of the top orchestra libraries on the market feature an extensive use of this technique. In addition, 
the Modulation wheel can be assigned to alter some parameters of the instruments, such as attack and 
release.
In terms of layered sound, there are two main approaches I find particularly effective when 
sequencing string parts. The first technique involves the use of both sampled and synthesized patches. 
Sometimes sampled patches can sound a bit thin and edgy, lacking some of the natural lower overtones 
that make an acoustic string sound so rich. A way to partially overcome this limitation is to layer a syn­
thesized patch underneath a sampled one. The balance between the two can vary, depending on the qual­
ity of both programs, but in general I have found that a ratio of 70–80% of sampled string to 30–20% of 
synthesized strings works well. If you layer a few patches together, then in addition to the main track that 
is output to all the layered devices and MIDI channels, remember to create different volume tracks for 
each single MIDI channel/device. This will give you independent control over the volume of each patch 
and will make it easier to find the right balance between the layered patches (see Chapter 3 to review the 
technical aspects of this procedure).
While the combination of sampled and synthesized can be useful for improving your string parts, 
a second approach to layering involves the use of one or more acoustic instruments recorded on audio 
tracks that double one or more lines of the string sections. This is an essential tool to bring your 
sequenced string parts to the highest production level. The addition of live acoustic instruments to the 
sampled patches adds freshness, edginess, and harmonic content to the relatively flat sampled waveform. 
If possible, try to double the top voice of each string section (violins, violas, cellos, and basses) and the 
divisi sections.
The right balance between the sampled and acoustic strings depends on several factors. What I rec­
ommend is to record the live instruments as dry as possible so that at the mix stage you can match the 
sampled and live instruments with the addition of some artificial ambience.
Listen to Examples 5.5, 5.6, and 5.7 on the website to compare a string part sequenced with syn­
thesized patches only, a version with layered sampled and synthesized patches, and a final version with 
acoustic and sampled strings, respectively. You can also combine the two layering techniques I explained 
to achieve a fuller sonority, but avoid layering too many patches together, especially with string sounds, 
since this can create awkward phasing effects that can spoil the final result.
One of the most challenging aspects of sequencing string parts lies in recreating the natural attack of 
the vibrating strings set in motion by the movement of the bow. The attack of string instruments changes 
depending on the pressure and the intensity with which the bow is moved. A light pressure causes the 
strings to have a longer attack, while a more abrupt and decisive movement generates a sound with a 
faster attack. This property is one of the major factors that give string instruments such a peculiar and 
flexible sonority. Recreating this aspect through samplers and a sequencer can be challenging and time 
consuming. While the aforementioned technique involving the layering of sampled and acoustic instru­
ments can definitely improve the final result, in certain situations where the alternation of soft and loud 

203
5.3  String Section: Overview
passages is featured, a precise control over the attack of the MIDI parts is necessary. Some software 
samplers and libraries allow you to control the attack of the samples by using velocity-switching tech­
niques to trigger different samples according to the velocity with which you hit the keys.
One of the most effective techniques for controlling directly the attack of the string patch you are 
using is to take advantage of CC 11 (Expression). If you have a controller that allows you to assign a 
slider to a CC message (Expression in this case), you can change the volume and therefore the attack 
of each note while you are sequencing your parts. This approach works particularly well for slow sec­
tions with long sustained notes. By moving the fader, you can accurately control the volume of each 
note, simulating the real behavior of the vibrating strings. While it is possible to insert and fine-tune the 
volume changes after you have played the parts, I recommend using the controller while you play. This 
will give you a much more fluent and natural feel than adding the control messages at a later stage. One 
of the hardest effects to recreate when sequencing the string section is legato passages, where each note 
needs to be seamlessly linked to the next one. This is a particular distinctive sonority of string instru­
ments, where down-bows and up-bows can be combined in creating a continuous sound. In order to 
achieve the same effect with your virtual orchestra you have to carefully edit each note so that the end of 
the previous note overlaps slightly with the beginning of the next note (Figure 5.9a, b).
By editing the notes this way and carefully controlling the release parameter of your patches, you 
will construct a very realistic legato passage.
The amount of overlap each note needs to have can vary, depending on the patch you use, but as a 
starting point I suggest a value between 10% and 20% of the overall tick resolution of your sequencer. 
For example, if your sequencer is based on 480 ticks per beat (ticks are subdivisions of a beat), then you 
should use overlapping values of around 50–100 ticks.
5.3.3  Panning and Reverb Settings
Having voiced, arranged, sequenced, and appropriately edited your parts, the next step is to mix the 
string section. Two main aspects can have a pretty drastic influence on the final string sound: panning 
and reverb. The way you pan the different sections of the virtual orchestra can definitely contribute 
toward enhancing the realism of the MIDI parts. While the details of panning can vary from project to 
project and from arrangement to arrangement, a good starting point is to refer to the classical disposition 
of the orchestra on stage during a live performance (Figure 5.10).
FIGURE 5.9
(a) Staccato and (b) legato effect for strings.

204
CHAPTER 5  Elements of MIDI Orchestration
While the classical disposition shown in Figure 5.10 can work, it presents a few problems related 
mainly to the placement of the basses, which would need to be panned hard right. This approach can 
work effectively if the hard panning of the basses is balanced by another instrument (such as the piano) 
or another low-range section (such as trombones and tubas) on the left. In a more practical setting you 
can set the basses in the middle (or slightly panned right) and spread the violins hard left, cellos hard 
right, and the violas either centered or slightly right to balance the violins divisi panned slightly left 
(Figure 5.11).
If the panning allows you to control the position of the instruments in a horizontal way, then the addi­
tion of ambience effects such as reverb and delay gives you the ability to place the instruments in a bidi­
mensional position, allowing you to have precise control over your virtual orchestra. The string section 
usually needs to have a little bit of reverb to recreate the natural environment in which a live orchestra 
would perform. I found that reverb lengths between 1.9 and 2.8 seconds usually work well to recreate a 
variety of halls. Do not overuse the reverb since this can cause a loss of definition and impact. For fast 
dramatic passages use shorter reverbs or reduce the volume of the return.
For long sustained passages use lower diffusion settings, while for fast and rhythmically busy pas­
sages use a higher diffusion. The diffusion parameters control the distance between bounces during the 
early reflection of a reverb. With low diffusion, the bounces are more separated from each other, giving 
a more detailed and clear reverb. A higher diffusion moves the bounces closer to each other, resulting 
in a more even (but muddier) sonority. In general, I recommend using less reverb on the low-frequency 
instruments, such as basses and to a certain extent cellos. Too much reverb applied to low-frequency 
instruments can cause your ensemble to sound muddy and without definition.
The amount of reverb you use on the string section also depends on the type of library and samples 
you use. Some sampled libraries are recorded with closed microphone techniques and in very dry envi­
ronments in order to provide the user with a dry ensemble and leave the ambience options open during 
the final mix. Other libraries are intentionally recorded in more acoustically live halls in order to provide 
a more realistic ensemble right out of the box. Which one you prefer is a matter of personal taste and 
a matter of musical style. For traditional classical rendition I like to use drier libraries that give me an 
edgier and “woodier” sound, while for contemporary and pop production I prefer something smoother 
FIGURE 5.10
Disposition of the classical orchestra.
FIGURE 5.11
Alternative strings panning setup.

205
5.4  Wind Instruments: Overview
with more natural ambience. You should have at least a dry and a wet orchestral library in your palette to 
give you enough choice and flexibility.
New sampled string libraries are being produced constantly, so it is impossible to provide a compre­
hensive and updated list. Among the ones I recommend for their flexibility, sonic quality, and compre­
hensive sonic variety are the Vienna Symphonic Library, the IK-Multimedia Philarmonik, and the East 
West Quantum Leap, just to mention a few.
5.4  WIND INSTRUMENTS: OVERVIEW
Wind instruments refers to a generic category of instruments in which the sound is generated by a col­
umn of air enclosed in a pipe. This category can be divided into several subcategories, as indicated in 
Figure 5.12. As you can see, the wind instruments family covers a wide area of sonorities and ranges, so 
it is better to analyze them according to the categories presented in Figure 5.12. Nevertheless, there are 
some general rules when sequencing for wind instruments that apply equally to every category.
As with the string section, one aspect you must remember is to consider your virtual orchestra as 
if it was a live one. Do not sequence passages that would make no sense to write for live players. This 
will guarantee much more credible results when dealing with sampled sounds and MIDI parts. Follow 
precisely the range of each instrument as indicated in the following paragraphs and the voicing that best 
applies to the nature of the section you are sequencing. One of the advantages the string section has 
over wind instruments is the freedom of phrasing. Strings can sustain very long phrasing without inter­
ruptions or rests, since the sound is generated when the strings are set in motion by the bow. For wind 
instruments, unfortunately, it is not that simple, since the sound is generated when the player blows into 
the pipe. This factor limits considerably the ability of the player to sustain long phrases.
One of the most common mistakes when sequencing wind instrument parts is to keep them going 
for long sections, forgetting that players need to breathe once in a while. This is mainly because in most 
cases you will sequence these parts on a keyboard. There are a couple of ways to efficiently avoid this 
mistake. The best way is to use a wind controller and to sequence each part individually, as you would 
play each wind instrument in the orchestra. The phrases will sound much more natural, and any phrase 
FIGURE 5.12
Wind instruments family.

206
CHAPTER 5  Elements of MIDI Orchestration
that would be too long for a live player to perform would stand out and could be easily fixed. These 
types of controllers also allow you more realistic dynamics, pitch bend effects, and phrasing. If you lack 
access to a wind controller, a quick solution to the “long phrases” problem can be achieved by recreating 
the inhale–exhale experience while sequencing the parts on the keyboard controller. Take a deep breath 
at the beginning of the phrase and slowly exhale while playing. If you run out of breath in the middle 
of a phrase it means that the phrase was probably too long and therefore would not sound natural when 
played by the virtual orchestra.
5.4.1  The Brass Section: The Trumpet and the Flugelhorn
The brass section normally includes the trumpet, the trombone, the horn (also called French horn), and 
the tuba. While these instruments produce sound in a similar fashion, their tone quality, timbre, and tex­
ture vary considerably. The trumpet has a strong but poignant sonority. If used with mutes it can drasti­
cally change its sound into a more mellow texture or into a more edgy and sometimes “buzzy” sonority. 
The range of the trumpet is shown in Figure 5.13.
Variations of the trumpet are the cornet, which has a more mellow and “shy” sound, and the flugel­
horn, which has a smaller range (Figure 5.14) and a much warmer sonority.
The trumpet section offers a fairly broad range of dynamics and sonorities. It is usually best 
voiced in octave, unison, or close position voicing to create a stronger sound. Usually sections can be 
formed using four or five trumpets, with the lead trumpet more often brighter than the others in order 
to cut through the other instruments better. The trumpet is a transposing instrument; that is, it is writ­
ten a whole step higher than it sounds. As with all the other acoustic instruments, sampled libraries are 
the ones that can best reproduce the live sonority of the trumpet. I recommend using trumpet sounds 
from different libraries to recreate the live acoustic variation in sound that is peculiar to each player. 
Remember that, as with strings, you should assign a different MIDI track and MIDI channel for each 
part. This allows you greater control over the pan, volume, and intonation of each line.
To recreate a more realistic effect, you can also slightly detune each trumpet playing a different 
line of the voicing. Remember to detune the parts so that the overall tuning is centered on a perfectly 
“in-tune” part. For example, with a five-trumpet section, leave trumpet 1 in tune and detune trumpet 2 by 
112 cents, trumpet 3 by 122 cents, trumpet 4 by 211 cents, and trumpet 5 by 221 cents. This will avoid 
having one part of the lines more out of tune than the others. Keep in mind that the detuning settings 
can vary with the quality and accuracy with which the live samples were recorded. To avoid unpleasant 
results, start with more moderate settings and make sure not to overdo it.
As for panning, I recommend treating each section of the brass as a unit by itself instead of micro­
panning each instrument of each section. As a general rule, try to avoid panning each trumpet indi­
vidually hard left or hard right, but instead think in terms of where the section would stand in a live 
FIGURE 5.13
Range of the trumpet.
FIGURE 5.14
Range of the flugelhorn.

207
5.4  Wind Instruments: Overview
performance situation. The same principles of detuning and panning can be applied to the other sections 
of the entire woodwind family, as we will learn later in the chapter.
5.4.2  The Trombone
The trombone covers the mid–low range of the brass section, usually reinforcing the trumpet section. 
Notice the so-called pedal tones, which can be used to extend the lower range of the trombone (while in 
theory there are seven fundamental pedal tones, ranging from B down to E, only the first three are con­
sidered reliable enough to be played in a live situation). The bass trombone can actually reach the fully 
extended low range through the use of additional tubing and triggers. The normal ranges of the trombone 
and bass trombone are shown in Figure 5.15.
A normal section usually comprises four or five trombones. A common way to combine trumpets 
and trombone sections is to have the trombones doubling the trumpets an octave lower. This represents 
a starting point for basic orchestration that can be used to experiment with new sonorities and brought 
to more adventurous levels. Trombones are operated through a slide that can be used to create portamen 
effects that are typical of this instrument. Pitch Bend can generally be used to recreate this effect with 
mixed results, depending on the parts. The best way to reproduce such sliding effects is to use multi­
sample libraries that allow you to create different sonorities by triggering different samples. Particularly 
effective are samples that reproduce the short drops (also called “fall-off ”) and long drops, which fea­
ture a “falling” pitch at the end of a phrase. The same effect can be used for the other brass sections and 
in particular for the trumpet section. Trombones, as with other brass instruments, can be voiced in close 
position, creating a cluster of notes that is dramatic and tense, or in open voicing, which is indicated 
more for powerful and triumphant passages. You can apply the same rules of panning as you learned for 
the trumpet section, keeping in mind, though, that since the trombone section covers a lower frequency 
range, it is usually better not to pan it hard left or hard right, as with any other bass-frequency-based 
instrument.
5.4.3  The French Horn
The French horn has a beautiful sonority that can enrich the brass section by adding a nice round 
color. The horn can play delicate melodic passages as well as powerful section parts, depending on the 
dynamic used. In comparison with the trumpet and trombones, it is less powerful in terms of sonority 
and projection, and because of its construction it is not capable of achieving the same agility on fast pas­
sages. The horn is a transposing instrument written a perfect 5th higher than its actual sound. Its range is 
shown in Figure 5.16.
FIGURE 5.15
Range of (a) the trombone and (b) the bass trombone.

208
CHAPTER 5  Elements of MIDI Orchestration
The French horn can be reproduced very well by sampled-sound libraries. The quality of its sound 
at both low and high dynamics is very effective when sequenced with a multisample patch. Its timbre 
changes drastically from mezzopiano to fortissimo, going from mellow and gentle to powerful and dra­
matic, and for this reason you will almost inevitably need several samples to cover such a wide range of 
sonorities. An average section can be formed by three to five horns, making it a very interesting addi­
tional color to be added to the trombones and the trumpets. Because of the nature of the instrument, 
the French horn is often used to underline the harmonic progression through the use of long sustained 
notes. In this case, remember to insert in your MIDI part spaces and pauses where the musicians would 
breathe, in order to add a realistic effect to the sequence.
5.4.4  The Tuba
The tuba covers the extreme low register of the brass section. There are several different types of tubas, 
such as the Bb tuba (also called the euphonium), the F tuba (or bass tuba), and the BBb tuba (or contrabass 
tuba), among others. The most commonly used is the contrabass tuba. Its range is shown in Figure 5.17.
Sampled libraries can effectively reproduce the sonority of the tuba. Usually it blends extremely 
well with a four-horn section, providing the perfect complement to their mid–low range sonority. When 
sequencing tuba parts, pay particular attention to the phrasing you use, owing to its somewhat limited 
ability to play rhythmically complicated passages at fast tempos.
5.4.5  Sequencing Brass: Libraries, Pan, and Reverb
I have a few recommendations on how to sequence for brass. Always strive for clarity and balance. As I 
mentioned earlier, think in terms of real instruments and not in terms of electronic samples. It is crucial 
that you follow the range of the acoustic instruments and not write passages that would be unrealistic on 
a live instrument. In terms of panning and positioning, if scoring for a larger ensemble such as a studio 
or symphonic orchestra, use the diagrams in Figures 5.10 and 5.11 as a starting point.
If scoring for brass only, you can use alternative settings such as the one shown in Figure 5.18.
This should be only a starting point to be altered and modified depending on the type of project and 
the type of instruments that are sequenced at the same time. If you are sequencing a brass-only ensem­
ble, for example, then you have more freedom in terms of panning, since the real essence and main focus 
of the sequence are the brass sections. If, however, you are sequencing the brass sections as background 
pads for a more complicated ensemble, then I recommend keeping a more conservative approach to pan­
ning. As with the string section, try as much as possible to double (or substitute) the top line of each sec­
tion with a live acoustic audio track. This will help to add dynamics and a much more realistic color tone 
to the MIDI parts.
FIGURE 5.16
Range of the French horn.
FIGURE 5.17
Range of the BBb tuba.

209
5.5  Woodwind: Overview
5.5  WOODWIND: OVERVIEW
The woodwind instruments include saxophones, flutes, oboe, English horn, clarinets, bass clarinets, bas­
soon, and contrabassoon. The color palette provided by the woodwind is wide and extremely versatile. 
The saxophones, for example, can be used equally effectively in sequencing fast, swinging passages and 
velvety, slow, pad-like parts. The flutes, clarinets, and oboes can be used in addition to the saxophone 
section to add a sparkling and edgy timbre. The combinations are literally countless, but in the modern 
studio orchestra a few settings have emerged and have been standardized as part of the current live and 
recording setup. Keeping these formulas in mind will help you in sequencing more realistic and credible 
woodwind parts.
5.5.1  The Saxophones
The saxophone family includes soprano, alto, tenor, baritone, and bass. For modern orchestration you 
will most likely find the alto, tenor, and baritone to be the main voices. The soprano is often used as a 
solo instrument or as an addition to a regular section. A common saxophone section includes the use of 
two altos and two tenors, even though there are many variations of this setting, such as one alto and three 
tenors or a five-saxophone section that features the addition of the baritone. It is important to understand 
which ensemble you are sequencing for in order to reserve enough MIDI tracks and channels for each 
line. The range of the saxophones is shown in Figure 5.19.
The saxophones are transposing instruments. Table 5.3 lists their transposition keys.
The saxophones are very versatile in voicing. They sound equally well and balanced in unison, 
octaves, or closed or open voicing. When sequencing for saxophones, try to use lines that feature real­
istic and smooth voice leadings, just as a real player would have to perform them. This will guaran­
tee a convincing sonority for the saxophone section. Unfortunately, the saxophone in general is one 
of the hardest wind instruments to reproduce with either a synthesizer or a sampler. While synthesized-
saxophone patches sound very stiff and fake, sampler patches do a much better job in reproducing the 
instrument’s original tones and nuances. The main problem, though, is that the saxophones feature sonori­
ties and harmonic content that are very hard to reproduce in their full range through a sampler. Again, as 
seen earlier, a multisample patch can help us in recreating the complex sonorities of a saxophone.
FIGURE 5.18
Suggested panning disposition of a brass ensemble.

210
CHAPTER 5  Elements of MIDI Orchestration
FIGURE 5.19
Range of the saxophone family.
Table 5.3  Transposition for the Saxophone Family.
Instrument
Transposition
Soprano
The soprano saxophone is written a whole step higher than it sounds
Alto
The alto saxophone is written a major 6th higher than it sounds
Tenor
The tenor saxophone is written a major 2nd plus an octave (major 9th) higher than it 
sounds
Baritone
The baritone saxophone is written a major 6th plus an octave higher than it sounds
Bass
The bass saxophone is written a major 2nd plus 2 octaves higher than it sounds

211
5.5  Woodwind: Overview
When sequencing saxophones, more than with any other wind instrument section, it is advisable to 
include one or more audio tracks of live recorded saxophones along with the MIDI parts. The phrasing 
(especially for jazz and swing projects) that a live saxophone can achieve cannot be reproduced through 
MIDI without the introduction of artificial and mechanical effects. If you use a two-alto, two-tenor setup, 
you should either layer or substitute the lead alto and the lead tenor. If you sequence for two altos, two ten­
ors, and one baritone, then you can opt for layering the lead alto, the lead tenor and the baritone (if possible).
The remaining instruments of the woodwind family (flutes, clarinets, oboes, and bassoons) can very 
effectively be sequenced using sampled libraries. The simpler nature of the waveform of these instru­
ments (the clarinet and flute in particular) makes their reproduction through synthesis and sampling tech­
niques fairly easy. Their tone quality is pretty stable in terms of sonorities and dynamics. Particularly 
effective is the use of oboe and bassoon. These two instruments can easily be placed in a sequence with­
out the need to replace them with live performances.
5.5.2  The Flutes
The flute is the most agile of the woodwinds and is therefore a fairly easy instrument to sequence. The 
flute in C is probably the most common instrument in the flute family, although it is not rare to sequence 
for piccolo flute and more seldom for alto flute. Their ranges are shown in Figure 5.20.
The flute can be used in sections (open voicing usually works better than clusters), as a solo instru­
ment, or as reinforcement to the string or brass section. It blends particularly well with the other wood­
wind, especially if voiced in unison or in octaves. The flute can be employed in an extremely wide range 
of styles and situations, from pop to jazz, from classical to ethnic.
5.5.3  The Clarinets
The clarinet is a very agile instrument (second only to the flute) and can be used in both classical and 
jazz ensembles. It has a dark and melancholy sonority that can also be used in sparkling and up-tempo 
FIGURE 5.20
Range of the flute family.

212
CHAPTER 5  Elements of MIDI Orchestration
passages. Since its waveform resembles a square wave, even basic synthesizers can reproduce a convinc­
ing clarinet sound. The tone quality depends mainly on the range used. The lower range has a dark qual­
ity, the middle register has a colder sonority, while the higher register features a bright and exciting sound. 
The clarinet is a transposed instrument in Bb (it is written a whole step higher than it sounds). The bass 
clarinet covers the lower range of the clarinet, providing a deep and pointed sound. It is a transposing 
instrument, written a major 9th above where it sounds. The range of the clarinet and bass clarinet is shown 
in Figure 5.21.
As with the flutes, the clarinets are very versatile instruments in section or solo. In section it is better 
to use an open voicing setting, while as a solo instrument it can be sequenced as a double of the lead part 
(unison or octave) of pretty much any combination of brass and woodwind. To achieve a more realistic 
sound in a MIDI setting I would recommend using two patches of the same sonority and detuning one 
of them by a range of between 1 and 3 cents. By alternating the use of the two patches you can recre­
ate a realistic detuning effect like one that would occur in a live situation. This technique can be also 
applied to the other woodwinds, especially the oboe, which has a rather difficult intonation. Be cautious, 
however, in applying any of the detuning techniques explained so far. Misuse could make your virtual 
orchestra sound like a bunch of amateur musicians!
5.5.4  The Oboe and the English Horn
The oboe and the English horn are, among the woodwind, the instruments that can be particularly well 
reproduced by a sampler. Their sound is melodic and expressive, especially in the middle register. The 
oboe is a non transposing instrument (it sounds where it is written), whereas the English horn is written 
a perfect 5th higher than it sounds. Their range is shown in Figure 5.22.
One of the hardest elements to reproduce when sequencing wind instruments in general, and the 
oboe and the English horn in particular, is the natural and acoustic release of the instrument. While this 
aspect of the sound in an acoustic performance is controlled by the change in air that is blown into the 
pipe, in a MIDI setting one way to control the natural release of such instruments is to alter or reprogram 
the patch being used by lengthening the release parameter. While this will offer you the perfect solution 
for long and sustained passages, it will create problems in fast passages, where a short and responsive 
patch is needed.
FIGURE 5.21
Range of the clarinet and bass clarinet.
FIGURE 5.22
Range of the oboe and English horn.

213
5.6  Synthesizers: Overview
A better solution is to take advantage of the flexibility of the CC messages and in particular of 
CC 11, which controls the expression. In passages where a staccato sonority alternates with longer sus­
tained phrases, you can use CC 11 to gently fade the sustained notes as an acoustic instrument would 
do. You can achieve this effect either by controlling the volume through a slider programmed to send 
CC 11 while you are sequencing the part, or by inserting the volume changes from the graphic editor 
after sequencing the part. I suggest using a combination of the two techniques by recording rough volume 
changes while playing the part and then using the graphic editor to smooth out the details of the fades.
In Examples 5.8 and 5.9 on the website you can listen to the difference between a woodwind part 
sequenced without volume changes and one that was sequenced using the volume change technique just 
described.
5.5.5  The Bassoon and the Contrabassoon
The bassoon and contrabassoon are part of the same family of double reeds as the oboe and the English 
horn. The bassoon is a non transposing instrument written in bass clef for the middle to lower range and 
in tenor clef for middle to high register. In the lower register it has a robust sound that gets more vibrant 
and edgy in the middle and high range. It can be effectively combined with the other wood wind to give 
more support to the lower register of the section. It can also be replaced by the bass clarinet, which has a 
more pleasant overall sonority. The contrabassoon is pitched an octave lower than the bassoon. It is used 
mainly to reinforce the bassoon in lower passages or to give more strength to the double bass or to the 
woodwind. Its low range is strong and powerful, while its high range is preferably avoided or substituted 
with the bassoon. The range of the bassoon and contrabassoon is shown in Figure 5.23.
5.6  SYNTHESIZERS: OVERVIEW
Even though the acoustic instruments just analyzed play an important role in the modern MIDI orchestra 
setup, most of the contemporary productions feature synthesized sounds that have little or no connection 
FIGURE 5.23
Range of the bassoon and contrabassoon.

214
CHAPTER 5  Elements of MIDI Orchestration
with acoustic instruments. However, one of the most interesting parts of having synthesized instruments 
available in your palette as a composer and arranger is their variety and endless resources in terms of 
color and sonorities. In fact, as we saw for the acoustic instruments, synthesizers can also be grouped 
into families or categories, depending on the type of sound generator and technique used to produce the 
waveforms. The boundaries between these categories, called synthesis techniques, in reality are much 
less clear than those in the acoustic realm of instruments.
Nowadays the main synthesizers include hybrid versions of several techniques in order to produce 
machines that are more versatile and well rounded than ever. Nevertheless, each synthesis technique has 
a distinctive sonority and is mainly used to produce a certain category of sounds and patches. As we 
learned earlier in this chapter, different categories of wind instruments can be arranged and combined to 
produce effective ensembles. In the same way, synthesizers and synthesis techniques can be effectively 
mixed and combined to achieve a versatile and powerful contemporary orchestra.
We already learned how to layer different sounds to create innovative sonorities and how to com­
bine acoustic and synthesized sounds to program realistic acoustic/MIDI parts. In this section of the 
chapter we are going to learn more about some of the most popular synthesis techniques and their sonic 
features to be able to make the right choice when selecting a synthesizer or sound module for our stu­
dio or projects. The type of synthesis used by a particular machine can have a big impact on the color, 
timbre, and final sonority of the patches produced. Among the many types of synthesis, I will briefly dis­
cuss those that are most commonly found in contemporary synthesizers: subtractive, additive, frequency 
modulation, wavetable, sampling, physical modeling, and granular. The goal of this section is not to 
provide a complete guide to synthesis and its many features, a subject that would take a manual of its 
own. Instead, it is to render an overall picture of the aforementioned types of synthesis available on the 
market for the modern composer and producer to enable them to make the right decisions when selecting 
sounds and devices for their projects. For more specific and detailed information on synthesis I recom­
mend reading Sound Synthesis and Sampling by Martin Russ and Computer Sound Design: Synthesis 
Techniques and Programming by Eduardo Miranda.
5.6.1  Hardware and Software Synthesizers
Before getting to the specifics of different types of synthesis it is important to understand the difference 
between two ways of conceiving the role of the synthesizer. Up to the late 1990s the word synthesizer 
meant, to most composers and producers, some sort of hardware component (sound module, keyboard 
synthesizer, drum machine, etc.) able to reproduce both acoustic sonorities and generate completely 
new waveforms, such as pads and leads. As hardware synthesizers became more and more sophisticated 
and demanding of hardware power and complicated to program, a new breed of synthesizer started to 
become popular: the software synthesizer. As I mentioned earlier, the main difference between hardware 
and software synthesizers is in how the sound is generated. While the former utilize a dedicated central 
processing unit (CPU) to produce the sounds, the latter take advantage of the CPU of a computer and 
specially written software to create the waveforms. The advantages of software synthesizers are many. 
After the initial investment in the computer, the price of a software synthesizer is much lower than its 
hardware counterpart. In addition, the continuous evolution of algorithms, raw waveforms, and patches 
can easily be integrated with a new version of the software, while the upgrade of hardware synthesizers 
would be more problematic.
Another important aspect that makes software synthesizers extremely popular is their seamless inte­
gration with sequencers and other music software. Through the use of plug-ins and standalone versions 
it is now possible to create, sequence, record, mix, and master an entire project completely inside a 

215
5.6  Synthesizers: Overview
computer, without the audio signal having to leave the machine. The availability of portable computers 
that are almost as powerful as their desktop counterparts contributed greatly to the spread of software 
synthesizers.
The types of synthesis I will describe in the following pages are not platform dependent, meaning 
that the actual way the waveform is produced does not change from hardware to software. The idea 
behind each synthesis technique, at its most basic level, is not affected by the type of synthesizer (hard­
ware or software). In the same way, the sonorities typical of each synthesis technique are not substantially 
changed by the fact that the waveforms are produced by a software or hardware synthesizer. In general, 
though, the software approach opens up possibilities that are usually not conceivable at affordable prices 
on hardware platforms.
A software synthesizer is usually available in two formats: as a plug-in or as standalone. The former 
requires a host application to run. Without this application, the synthesizer is unable to launch. All four 
sequencers analyzed in this book function as host applications for the main software synthesizers availa­
ble on the market. Plug-in software synthesizers come in different formats, depending on the application 
used to host them. Table 5.4 lists the plug-in formats supported by Digital Performer, Logic Pro, Cubase, 
and Pro Tools. The same plug-in formats are used for regular effects such as reverb, delay, and chorus.
As noted in the “Comments” column of Table 5.4, through the use of a “wrapper”, certain formats 
unavailable natively for a certain application can be used. A wrapper is a small application (a sort of 
software adapter) that is able to translate one plug-in format into another. In recent years a few host 
applications, not sequencer based, have emerged. These programs are used only as plug-in hosts for soft­
ware synthesizers. The advantage of such hosts is that they make multiple plug-ins available without 
the need to run an entire MIDI/audio sequencer. They are particularly well suited for live performances, 
where stability and streamlined settings are extremely important, or for studios with one or more satellite 
computer setups.
An alternative to the plug-in format is the standalone application. A standalone software synthesizer 
is a separate independent application that does not need a host to run. This option has the advantage of 
being slightly less demanding in terms of CPU power, since it does not require an entire sequencer to 
run. One of its drawbacks, though, is a lower integration in terms or routing and interaction with other 
components of your virtual studio. Most software synthesizers are available as both plug-ins and stan­
dalone versions. The choice between one or another format really depends on how you will use the soft­
ware synthesizer. For live settings I recommend the standalone version, since it is usually more stable. 
Table 5.4  Plug-in Formats Supported by DP, LP, CU, PT, and PT TDM.
Application
Plug-in Format
Comments
DP
MAS, AU
VST format plug-ins can be used through a VST 
to MAS wrapper
LP
AU, Proprietary Logic Format
VST format plug-ins can be used through a VST 
to AU wrapper
CU
VST, DirectX (Windows Only)
PT
RTAS
VST format plug-ins can be used through a VST 
to RTAS wrapper
DP: Digital Performer; LP: Logic Pro; CU: Cubase; PT: Pro Tools.

216
CHAPTER 5  Elements of MIDI Orchestration
For a MIDI/audio studio situation, where the sequencer becomes the central hub of both MIDI and audio 
signals, I recommend a plug-in system, which is more flexible in terms of signal routing.
5.6.2  Synthesis Techniques
The majority of the synthesizers (both hardware and software) available on the market use one or more 
synthesis techniques that have been developed since the 1960s. Each approach to synthesis has a pecu­
liar way of creating (or, better, synthesizing) the waveforms. This factor contributes to linking a specific 
synthesis technique to a unique sonority. This is particularly important to keep in mind when selecting 
a synthesizer and integrating it in your studio. The right choice of machine or module to buy or to use 
in a particular project depends mainly on the type of sonority and patches you need. For example, you 
would hardly choose an analog synthesizer to program a realistic string ensemble, just as you probably 
wouldn’t choose a sampler to program a multistage complex synthesized pad. Let’s take a look at the 
most common and popular types of synthesis to gain a better understanding of their features and typical 
sonorities.
5.6.3  Analog Subtractive Synthesis
A synthesizer is a device capable of generating electrical sound waves through one or more oscillators 
[voltage-controlled oscillator (VCO)], which are electronic sound sources (analog or digital) and can be 
simple or complex depending on the level of sophistication of the oscillators used. Analog subtractive 
synthesis constitutes one of the oldest types of synthesis, made commercially available and marketed in 
the 1960s. In the case of analog subtractive synthesis, one or more oscillators generate a basic and repeti­
tive waveform: sine waves, triangular waves, square waves, saw tooth waves, pulse waves, and noise. 
This section constitutes the “sound source” or “generator” section of a synthesizer. The other two main 
areas of a synthesizer based on the subtractive approach are the “control” section and the “modifiers” 
section. The former comprises the keyboards, Pitch Bend, Modulation wheel, foot pedals, etc.; the lat­
ter includes the filter section, which is used to alter the basic and repetitive waveform generated by the 
oscillators.
The name subtractive comes from the process involved in creating more complex waveforms from 
a basic and repetitive sound wave. After being generated, the simple waveform is sent to the modifiers 
section, where a series of high-pass and low-pass filters (whose number and complexity vary with the 
sophistication of the synthesizer) “subtracts” (or, better, remove) harmonics to the waveform, producing 
more interesting and complex wave shapes. The altered sound is then sent to the amplifier section, called 
a voltage-controlled amplifier (VCA), which is still part of the modifiers, where the amplitude of the 
waveform is amplified. The VCA can be altered through the use of an envelope generator (EG), which is 
a multistage controller that allows the synthesizer to control the amplitude of a waveform overtime.
The most basic EG has four stages: attack, decay, sustain, and release. In more modern synthesizers 
the EG can be divided into more stages to allow higher flexibility. In addition to controlling the VCA, 
the EG can be assigned to control how the filters change overtime, enabling the synthesizer to produce 
even more complex waveforms. To be able to introduce some variations into the repetitive cycle of the 
subtractive synthesizer, one or more auxiliary oscillators are introduced. These oscillators, called low-
frequency oscillators (LFOs), have a rate much lower than the one used to generate the waveforms. They 
can be assigned to control several parameters of the synthesizers, such as the filters section or the pitch 
of the main oscillators. Usually the LFO can be altered by changing its rate (the speed at which the LFO 
changes over time) and its attack time. The signal flow and the interaction among the sections and com­
ponents of a subtractive synthesizer can be seen in Figure 5.24.

217
5.6  Synthesizers: Overview
Some of the most important synthesizers that used subtractive synthesis as their main sound genera­
tor techniques are the famous Minimoog, the Prophet 5 by Sequential Circuits, and the Juno series by 
Roland. However, the most important aspect to consider when discussing this type of synthesizer is the 
sound quality and the overall sonority they are capable of producing. Subtractive synthesis is particularly 
suited for low and deep analog basses, and edgy and punchy analog leads. These are among the sounds 
and patches that made subtractive synthesis famous and that are still largely used in contemporary dance 
productions. Another area in which subtractive synthesis is capable of producing original and interesting 
sonorities is synth pads, which are rich and thick.
Subtractive synthesis, because of its limited basic waveforms, is usually not suited to recreating acous­
tic instruments such as strings and woodwind, even though it can be somewhat effective in producing 
FIGURE 5.24
Components and sections of a subtractive synthesizer.

218
CHAPTER 5  Elements of MIDI Orchestration
synthesized brass. Even though in the 1970s and 1980s this type of synthesizer was used to sequence 
some orchestral parts, they cannot compete with more advanced sample-based synthesis options.
For some audio examples of analog synthesis generated by a subtractive synthesizer listen to 
Examples 5.10–5.13 on the website. I always recommend having one or two subtractive synthesizers as 
part of your setup in order to be able to use some vintage analog sounds. It is important to have as many 
sounds and patches available in your studio as possible to have a rich palette to choose from, and the 
vintage sonority offered by subtractive synthesis is a must-have.
5.6.4  Additive Synthesis
Additive synthesis is based on the basic concept that even the most complex acoustic waveforms can 
be reproduced by summation of multiple sine waves. Additive synthesis, by using multiple oscillators 
generating sine waves, tries to reproduce complex and sophisticated sonorities by adding each waveform 
according to frequencies and amplitudes determined by the programmer.
A graphic example of this approach can be seen in Figure 5.25. In this example a simple sine wave 
(the fundamental) of amplitude 1 and frequency 1 is added to a sine wave (3rd harmonic) of amplitude 1/3 
and frequency 3 (the numbers used are simply illustrative and have no reference to real frequencies and 
amplitudes). The result is a complex waveform compiled from the addition of sine wave components. By 
adding other harmonics at different amplitudes, the original sine wave will be changed even further, pro­
ducing a completely new sonority. The example illustrates the power of this type of synthesis. One of the 
main drawbacks of the additive approach is that many oscillators are needed to produce complex wave­
forms, and therefore synthesizers based on the additive approach require a very powerful sound engine.
One of the first commercially available additive synthesizers was the legendary Synclavier, pro­
duced by New England Digital in the mid-1970s. Another fairly successful example of commercially 
FIGURE 5.25
Example of basic additive synthesis.

219
5.6  Synthesizers: Overview
available additive synthesizers was the Kawai K5, which used a 126-harmonic system to produce com­
plex waveforms. In the case of additive synthesis there are three main sections, just as in the case of 
subtractive synthesizers: controller, generator, and modifier. The same structure, in fact, can be applied 
to almost any type of synthesis. The main difference among several synthesis techniques resides in the 
way complex waveforms are generated. The controller section is very similar to any synthesis approach. 
The modifier part can change and get more and more sophisticated as more modern filtering techniques 
become available.
Additive synthesizers are usually able to produce convincing thin sounds, mainly related to vibrating 
strings, such as guitars and harps. They are not particularly effective in reproducing percussive sonori­
ties with fast transients, such as drum sounds. A new series of software synthesizers based on an evolu­
tion of additive synthesizers has recently emerged. These applications use the concept of resampling to 
resynthesize waveforms that are fed to the software as audio files. In other words, a resampler analyzes a 
complex waveform provided as an audio file and then tries to synthesize it by reconstructing its complex 
wave through the summation of sine wave components. The results are particularly effective not only 
in creating fairly accurate acoustic sounds but mainly in generating synthesized pads and leads that are 
based on acoustic “genetic” audio material.
5.6.5  Frequency Modulation Synthesis
As we have learned up to this point, subtractive synthesis offers limited possibilities in terms of creating 
complex sonorities starting from elementary waveforms, and additive synthesis is capable of generating 
more advanced patches but with stringent requirements in terms of processing power. Another answer to 
the need for synthesizing more complex and sophisticated waveforms came from yet another innovative 
approach to synthesis: frequency modulation (FM). This technique involves a multiple-oscillator system 
such as in the case of additive synthesis. But instead of using the oscillators in a passive way, where the 
resulting waveform of each oscillator is passively added to the others, in FM each oscillator influences 
and changes the output of the others.
In its most simple setup, FM synthesis uses two oscillators, one called the modulator and the other 
the carrier (advanced FM synthesizers use three or more oscillators to generate more complex wave­
forms). The modulator changes and alters the frequency of the carrier by constantly modulating the basic 
frequency at which the carrier operates. In a multiple-oscillator setup there are usually several matrixes 
available that control the flow of the signal path among modulators and carriers according to predeter­
mined algorithms. When you program an FM synthesizer, you can usually select several options as to 
which matrix to use to produce a certain complex waveform.
One of the most commercially successful FM synthesizers ever made was the Yamaha DX7, which 
was released in the early 1980s and saw many successful revisions and improvements over the years. 
FM synthesis can usually produce glassy and bell-like tones with good, fast transients and a string-
like quality. FM synthesizers are considered best at generating patches such as electric pianos, harps, 
synth electric basses, bells, and guitars. One of the drawbacks of FM synthesis is that it is fairly hard to 
program.
As with the other synthesis techniques analyzed so far, I recommend having at least one FM synthe­
sizer available in your studio. There are a few software synthesizers based on FM synthesis and on the 
engine of the original DX7 but with new controls and matrixes available to create more sophisticated 
sonorities more quickly and more easily.
Listen to Examples 5.14–5.17 on the website to get an idea about the quality of the timbres generated 
by FM synthesis.

220
CHAPTER 5  Elements of MIDI Orchestration
5.6.6  Wavetable Synthesis
While all the synthesis techniques we have analyzed so far use basic waveforms with complex filters 
and algorithms to produce more sophisticated and complex sonorities, wavetable synthesis starts with 
complex waves that are sampled from both acoustic and synthesized instruments. The sampled waves 
are stored in tables in the ROM of the synthesizer to be recalled and used to build complex patches. This 
approach became more and more popular among synthesizer manufacturers in the mid- to late 1980s, 
when the price of memory started to decrease and the size of chips to increase. Wavetable synthesis has 
several advantages, the main one being that it can reproduce acoustic instruments with surprising fidelity 
since it actually utilizes sampled versions of the acoustic waveforms. A generic wavetable synthesizer 
stores the attack and sustain parts of the waves. When the sample is triggered from the keyboard, the syn­
thesizer will play the first part of the waveform once (the attack) and then will keep looping the sustained 
portion until the key is released. Depending on the amount of memory available, the length of the sus­
tained part can vary considerably. Earlier wavetable synthesizers used to hold only a few seconds of sam­
ples, and therefore the loop would be fairly short, affecting the overall acoustic accuracy of the waves.
Modern wavetable synthesizers can hold several minutes of samples, thereby reducing the loop effect 
and rendering a much more accurate timbre. Complex sounds can also be achieved by layering several 
waves to generate richer pads and sustained patches. Each layer is assigned to a partial that has separate 
control in terms of envelope, filters, effects, etc. The number of partials available changes from machine 
to machine, but generally in commercially available synthesizers you can find structures that use four 
or eight partials. The initial set of waves available with the machine can be increased through the use of 
expansion slots or cards that can be installed inside the device or inserted in the front panel. The expan­
sion cards vary greatly in size and content.
One of the main problems that early wavetable synthesizers had to face was the limitation of ROM 
initially installed onboard. This had a clear impact on the overall quality of the sounds, since the loop 
section had to be very small. Later models could afford a much larger memory and therefore be more 
accurate. The samples generated are usually processed through a series of filters and envelope genera­
tors, similar to the ones we saw with subtractive synthesis, to further shape the waveform. Wavetable 
synthesizers are particularly appreciated for their overall flexibility, comprehensive lists of patches, all-
round use, and good to excellent sound quality. These types of synthesizers are the most valuable in a 
studio since they can reproduce with good results pretty much any type of sonority. I highly recommend 
having at least one machine based on this type of synthesizer as a MIDI workhorse. Some of the most 
successful synthesizers that use this approach are the Roland JV series (1080, 2080) and their evolution, 
the XV series (3080 and 5080, and the Fantom series).
5.6.7  Sampling
A sampler is a particular type of synthesizer. It takes a similar approach to that seen in the wavetable 
technique, but instead of being limited to a table of small samples stored by the manufacturer in the 
machine’s ROM, it stores the sample in RAM which can be erased and refilled with new samples at the 
user’s will. When you turn on a hardware sampler, it contains no samples in RAM; you have to either 
load a bank of samples from an HD or CD-ROM, or sample your own waveforms. Before turning the 
unit off, you have to remember to save to HD the samples and the changes made to the banks to be 
able, at a later time, to reload the settings for future sessions. Samplers are the best options to reproduce 
acoustic instruments, as I explained in the first part of this chapter. The amount of RAM available on a 
device reflects the number and length of samples you can load and use at the same time. The samples 
recorded and stored in the memory are mapped to the keyboard in order to have the full extension and 
range of the instrument we sampled. The higher the number of samples that form a particular patch, the 

221
5.6  Synthesizers: Overview
more accurate the patch will be, since for every key that doesn’t have an original sample assigned the 
machine will have to create one by interpolating the digital information of the two closest samples avail­
able. As in the case of wavetable synthesizers, the sampled waves can be altered through a filter section 
similar to the one found in subtractive synthesizers.
Software samplers are the de facto standard in contemporary production these days. Their success 
comes mainly from their ability to take advantage of large memory sizes and their unique feature of 
being able to stream the samples directly from the HD [direct from disk (DFD)], basically putting an end 
to the limitations created by the RAM-based architecture of hardware samplers. This type of approach 
to sound creation constitutes the core of the modern MIDI and audio studio. The majority of acoustic 
instruments that are sequenced nowadays are recorded using sample-based devices. Among the most 
used sampler applications are the MachFive 2 by Mark of the Unicorn, Kontakt by Native Instruments, 
ESX24 by Apple/Emagic, and Independence by Yellowtools.
5.6.8  Physical Modeling Synthesis
Physical modeling (PM) synthesis is newer than the other types of synthesis analyzed so far. With PM 
synthesis, instead of trying to recreate and produce a waveform starting from an analysis of its original 
acoustic counterpart and then trying to reshape a synthesized version of it through the use of filters and 
modifiers, the synthesizing process involves the analytical study of how a waveform is produced and 
of all the elements and physical parameters that come into play when a certain sound is produced. The 
sound-producing source is the key element here and not the resulting waveform.
PM synthesis is based on a series of complex mathematical equations and algorithms that describe 
the different stages of the sound-producing instrument. These formulas are derived from physics models 
designed through the analysis of acoustic instruments. Behind PM lies the principle of the interaction 
of a vibrating object (strings, reeds, lips, etc.), the medium (air), and an amplifier (e.g., the bell of a 
trumpet, the body of a piano, the cone of a speaker in an amplifier) in producing a particular sonority. 
Mathematical models that describe how the several elements of any sound-producing instrument interact 
are stored in the synthesizer that generates the waveform, calculating in real time the conditions, rela­
tions, and connections between each part involved in creating a particular sound. The number of algo­
rithms present in a PM synthesizer determines its versatility and sound-generating power. An example of 
such models is the famous Karplus–Strong algorithm, which describes in mathematical terms a plucked 
string. A diagram representing the various stages in which PM would dissect the process involved in the 
production of a violin waveform is given in Figure 5.26.
One of the drawbacks of PM is that it requires an incredible amount of CPU power to process in real 
time the calculations necessary to generate the waveform. Because of this, PM has only fairly recently 
become one of the most advanced and interesting types of synthesis implemented in both hardware and 
software synthesizers. The strengths of PM are many, and they all impact the final users (meaning the 
programmers, composers, and producers) in one way or another. The programmer is presented with 
parameters that are usually fairly easy to understand and deal with, since they mostly reflect real acoustic 
parameters. No longer do you have to guess which parameter (or parameters) will have an impact on the 
way a sound will change when sending a higher velocity value or how another sound will change when 
using a certain vibrato technique. With PM, sounds are programmed using real parameters that apply to 
real acoustic instruments and that are therefore much easier to relate to a predetermined sonic result.
PM synthesizers require much less RAM than wavetable devices and samplers. The former can 
calculate the waveform for the entire range of the instrument without requiring a multisample setup, 
while the latter need several samples not only to cover the full range but also to provide all the vari­
ations necessary to reproduce several tonal colors. In order to slim down the amount of calculation a 

222
CHAPTER 5  Elements of MIDI Orchestration
PM synthesizer needs to do in real time, the original algorithms are often simplified and translated in 
a sequence of filters already programmed. Other tricks, such as cycle loops, are often used to simplify 
even further the computational process.
One of the most active manufacturers in developing accessible and marketable solutions for musi­
cians based on PM synthesis has been Yamaha, which in the mid-1990s introduced one of the first com­
mercially available and fairly affordable PM synthesizers, the VL1, which had a two-note polyphony 
limit.
As with other synthesis techniques, PM is progressively taking advantage of the software synthesizer 
revolution. The extreme flexibility and adaptability of software synthesizers make them perfect compan­
ions for a type of synthesis such as PM. As we saw earlier, the number of algorithms preprogrammed in 
a PM synthesizer determines the capabilities of the machine. While most hardware devices are based on 
a pretty closed architecture that makes software upgrades rather complicated and limits the possibilities 
of expendability in terms of new models, a software synthesizer allows for a much higher degree of flex­
ibility, owing to its intrinsic open architecture.
One intriguing aspect of PM is not related to the reproduction of real acoustic instruments but instead 
to the unlimited possibilities in generating morphed instruments that are constructed by mixing and 
FIGURE 5.26
Example of a physical modeling model for a violin sound.

223
5.7  Summary
matching models and stages from several sources. Imagine having an “energy-generation” module of a 
violin fed into the “resonance and damping” section of a drum and then sent to an “amplifier” stage of a 
trumpet. The resulting sonority will be something totally new yet extremely musical and inspiring!
5.6.9  Granular Synthesis
This type of synthesis is newer than some of the more established and widely used techniques analyzed 
so far. Even though the concept on which granular synthesis (GS) is based can be traced back to the end 
of the nineteenth century, actual practical use in synthesis was developed only around the late 1940s and 
early 1950s. The main idea behind GS is that a sound can be synthesized starting from extremely small 
acoustic events called grains, which are usually between 10 and 100 ms in length. Granular synthesiz­
ers start from a waveform (it can be any type of waveform, such as FM, wavetable, or sampled) that is 
“sliced” to create a “cloud”, which is made up of hundreds or even thousands of grains. A completely 
new and complex waveform is then assembled by reordering the grains according to several algorithms 
that can be randomly based or functional.
One advantage of GS is that the sounds created from such small units can constantly change and 
evolve, depending on the grain-selection algorithm. Since the grains forming the sound cloud are so small 
and so numerous, their repetition is perceived not as a loop (as in the case of wavetable synthesis or sam­
pling) but instead as “quantum” material that constitutes the basic substance of which sounds are made.
Grains and the resulting waveforms can be processed using more standard modifiers, such as enve­
lope generators and filters. Usually GS is particularly suited for the creation of sophisticated and fresh-
sounding synthesized pads and leads: check out Examples 5.18 and 5.19 on the website. One of the 
drawbacks of GS used to be the CPU power required to calculate the grains and the clouds in real time, 
but with the current speed of modern CPUs these limitations are no longer an issue.
This overview of some of the most popular and tested synthesis techniques was intended to give you 
a sense of how different approaches to synthesis can affect, in practical terms, the work of the modern 
composer and producer. Each technique has strengths and weaknesses that make it the ideal choice for 
a certain style, texture, and sonority. Understanding their fundamentals is key to building versatile and 
efficient studios as well as to producing coherent and balanced projects. When buying or considering 
using a particular synthesizer (software or hardware), think about which texture and sonority you are 
looking for, keeping in mind the strengths of the synthesis techniques just analyzed, and make a decision 
based on facts and goals. While the synthesis techniques we learned can be specifically and individu­
ally found in devices on the market, it is more likely that you will have to choose between products that 
will incorporate several approaches to synthesis at the same time. Most of the devices (and applications) 
available today feature three or more techniques integrated with one another, expanding their flexibility 
and power.
5.7  SUMMARY
In the previous chapters I covered the sequencing techniques related to the MIDI and audio aspects of 
music production. While these techniques are extremely important in the overall environment of the 
modern producer and composer, the actual orchestration techniques related to the interaction of acoustic, 
MIDI, and synthesized instruments represent an even more valuable area of contemporary music produc­
tion. The study of the different components that come into play when sequencing is crucial to achieving 
advanced and professional results. We analyzed the main sections of a modern virtual orchestra: rhythm 
section, string section, wind instruments, woodwind, and synthesizers.

224
CHAPTER 5  Elements of MIDI Orchestration
The rhythm section, usually drums and percussion, guitar, bass, and piano, constitutes the core of 
many productions. The piano is among the instruments that are usually easier to sequence and reproduce 
in a MIDI setting, because most controllers used to input the MIDI data are keyboard based. Sampled 
libraries are usually the best option to choose from in terms of sounds and patches, mainly because of 
their multisample capabilities, which allow you to render with extreme fidelity all the colors and nuances 
of such a dynamic instrument. If possible, one of the best options when sequencing piano parts is to take 
advantage of acoustic pianos that are integrated with MIDI interfaces, such as the Yamaha Disklavier. 
This way, you will be able to combine the realistic acoustic sound of the piano with the editing capabili­
ties of the MIDI sequencer.
The guitar and bass can be effectively reproduced and sequenced through the use of guitar- and bass-
to-MIDI controllers. The realism achieved with such devices is far superior to anything else programmed 
using a keyboard controller. This approach will allow you to use convincing and realistic voicing that 
would be hard to reproduce on any other type of controller. This concept applies particularly to parts that 
involve chord-strumming passages. While arpeggios and single-line parts can be effectively sequenced 
from a keyboard controller, strummed passages are very hard to reproduce. If you lack access to a guitar-
to-MIDI converter, you can edit the notes belonging to a certain chord in order to have each note of the 
chord slightly delayed by a few ticks, as it would be for an acoustic guitar. To achieve more realistic 
sonorities for both bass and guitar sequenced parts, I recommend rerecording with a microphone the 
MIDI tracks as audio after sending the output of your synthesizer (or sampler) through a guitar or bass 
amp. This will add a realistic ambience and tone to the sometimes sterile sonority of the synthesized 
patch. As with most acoustic instruments, samplers represent the best option to effectively reproduce 
the sophisticated sonorities of the guitar. The electric bass sound is a little bit more forgiving and can be 
well reproduced by the most modern and advanced synthesizers.
The realism of sequenced drums and percussion parts greatly depends on the type of music and style 
you are programming. Usually, contemporary rhythms and loop-oriented grooves are easier to program, 
while more acoustic and feel-based parts (such as jazz and orchestral ensembles) represent a challenge 
when it comes to MIDI sequencing. Sound and groove (or rhythmic feel) are the two main aspects 
you have to focus on when it comes to drum sequencing. To achieve a higher degree of realism I rec­
ommend using MIDI drums or MIDI pads to record the parts. In the case of a drum kit, avoid having 
passages where more than four drum pieces are playing at the same time, since this probably would 
not be possible in a live situation (unless you have multiple drummers). To improve the feel of a 
drum MIDI part, try to include one or two percussive instruments recorded live, such as shakers or 
tambourines. To improve the acoustic ambience and interaction with other tracks of drums and percus­
sion, try to rerecord through a couple of good condenser microphones, on separate audio tracks, the 
MIDI material played through the main speakers of the studio (this technique is basically similar to the 
one explained for guitar and bass parts). Loops constitute a valuable alternative to MIDI parts but can 
sound too repetitive if used without variation. Use loops recorded in multitrack settings to provide more 
variety.
The string section is among the hardest and most controversial sets of instruments to reproduce with 
a MIDI system. The modern string section includes violin, viola, cello, and bass. Because of the sonic 
differences among these four instruments it is usually better to avoid using a generic “strings” patch. 
Instead, use specific patches for each section and also for divisi passages. Sequencing strings in a MIDI 
setting is no different than writing the part for their acoustic counterpart. Therefore, use the same voic­
ing and intervals as for a real set of players. Avoid close voicing, especially in the higher register of the 
violins, unless you are looking for a particularly harsh sonority (3rds, 4ths, and 5ths usually work better 
and can provide a more robust sound). Sequence each part and line individually and on separate MIDI 
tracks and MIDI channels. This will give you more flexibility when editing the data.

225
5.7  Summary
Sample-based libraries are the first choice when it comes to string sounds. If your score makes use of 
alternate bowing techniques such as sul tasto, sul ponticello, col legno, tremolo, pizzicato, and glissando, 
use different MIDI channels for each sonority. To improve the realism of string patches, you can layer 
sampled sounds with synthesized ones in a ratio of roughly 70% to 30%, respectively. Another technique 
involves the layering of sampled instruments with real acoustic instruments. This approach is also valid 
for brass and wind instruments in general. If you choose to use this technique in your sequences, double 
the top voice of each section with an acoustic instrument recorded on audio tracks. To recreate the natu­
ral attack produced by the bow on the strings of violin, viola, cello, and bass, use volume changes while 
playing the parts. If your controller allows it, assign one of its sliders to CC 11 and while playing create 
short crescendos for each note. This technique works particularly well for long and sustained passages. 
For legato passages, extend the end of each note to the beginning of the following note so that they 
overlap by a few ticks (I suggest a value between 10% and 20% of the overall tick resolution of your 
sequencer). When mixing the string section, you should follow the natural configuration of a live orches­
tra, as shown in Figures 5.10 and 5.11. Adding a medium reverb to the strings can improve the overall 
texture and mix among the instruments. A reverb length between 1.7 and 2.3 seconds usually works well 
for a wide range of styles; remember to use a low diffusion setting for sustained passages and a higher 
setting for fast staccato parts.
The wind instruments category includes the brass section and the woodwind section. The latter is 
further divided into single-reed, double-reed, and mouth-hole instruments. Even though the wind instru­
ment category covers a wide range of sonorities and sonic textures there are some common sequencing 
techniques that can be applied to all these instruments. As we saw in the case of string instruments, you 
have to sequence as you would write for acoustic instruments, that is, respecting range, voicing, and 
performance techniques that could be applied to live performances. Keep in mind that wind instruments 
have limited phrase length capabilities because the sound is produced by the air blown into the pipes. 
Therefore, always check and avoid phrases that would not be realistic and performable in a live set­
ting. When possible, use a wind MIDI controller to sequence wind instrument parts. Brass instruments 
include trumpet, flugelhorn, trombone, bass trombone, French horn, and tuba. Each of these instruments 
has a wide range of sonorities and textures that can even be increased by the use of mutes. For classical 
brass ensembles, you can use effectively sampled libraries to reproduce their entire range of dynamics 
and sonic nuances. For more modern and rhythmic brass parts, synthesized brass patches can provide a 
useful alternative. In terms of voicing, try to avoid clusters and intervals smaller than or equal to a major 
2nd in the low register of the brass section (5ths and octaves work better). In the higher register, clusters 
are effective and can provide a nice, punchy sonority.
The woodwind category includes the saxophones (soprano, alto, tenor, baritone, and bass), the flutes 
(C, piccolo, and alto flute), oboe, English horn, clarinets, bass clarinets, bassoon, and contrabassoon. 
The woodwind instruments are extremely flexible in terms of sonorities and combinations. The saxo­
phones, for example, can be used equally effectively to sequence fast, swinging passages as well as 
velvety, slow, pad-like parts. The flutes, clarinets, and oboes can be used in addition to the saxophone 
section to add a sparkling and edgy timbre.
The saxophones sound equally well balanced in unison, octaves, or closed or open voicing. When 
sequencing for saxophones, try to use lines that feature realistic and smooth voice leadings, just as a real 
player would have to perform them. Saxophones are usually extremely hard to reproduce in a MIDI set­
ting. Sampled libraries are definitely the way to go to get convincing and realistic results. In the case of 
the saxophone section, I highly recommend layering acoustic and sampled tracks to increase the realism 
and phrasing accuracy of the sequenced parts.
The remaining woodwind instruments can be very effectively sequenced using sampled libraries. The 
simpler nature of the waveform of these instruments makes their reproduction through synthesis and 

226
CHAPTER 5  Elements of MIDI Orchestration
Table 5.5  Types of Synthesis and Their Main Features.
Type of 
Synthesis
Description
Pros
Cons
Best Used For
Subtractive
A three-section synthesis: 
controller, generator (using 
basic repetitive waveforms), 
modifier (filters, envelopes, 
amplifier)
Does not require a 
lot of CPU power
Somehow limited 
in terms of creating 
complex and 
changing waveforms
Synth basses, synth 
leads, synth pads
Additive
Based on summation of 
sine waves at different 
frequencies and amplitudes 
to create more complex 
sonorities
In theory, capable 
of reconstructing 
with precision any 
acoustic waveform
Requires a lot of 
CPU power to 
run the necessary 
calculation in 
real time; hard to 
program
Acoustic and 
synthesized 
instruments
FM
Based on the interaction 
of two or more waveforms 
(modulator and carrier). 
The modulator interacts 
and modifies the carrier to 
create complex waveforms
Allows the creation 
of complex 
waveforms
Complex to program
Bells, synth basses, 
electric pianos, 
plucked instruments
Wavetable
Based on a series of 
complex waveforms stored 
in ROM and modified 
through filters similar to 
those found in subtractive 
synthesis
Well suited for 
a wide range 
of instruments; 
expandable through 
expansion cards
Because of the 
limitation of ROM 
size the complex 
waveforms are 
limited in length and 
are looped
Very versatile; 
can effectively 
produce acoustic 
and synthesized 
sonorities
Sampling
Based on samples of 
complex waveforms loaded 
and stored in RAM
Users can 
sample their own 
waveforms; patches 
can be updated 
through sampler 
library CDs and 
DVDs
Not ideal for 
complex and 
constantly changing 
synth pads; 
hardware sampler 
limited by RAM size
Acoustic 
instruments, e.g., 
strings, winds, 
guitars, piano, and 
percussion
Physical 
modeling
Based on mathematical 
descriptions (algorithms) 
of the different sound-
production stages of 
instruments
Extremely powerful 
and innovative
Requires a very 
powerful CPU
Wind instruments, 
voice-based 
patches, guitars, 
and string 
instruments
Granular
Based on a simple or 
complex waveform 
subdivided into small 
grains forming a cloud of 
audio material. The grains 
are combined in patterns 
following mathematical 
or random algorithms to 
generate new complex 
waveforms
Constantly varying 
and innovative 
sonorities
Requires a 
very powerful 
CPU; somehow 
unpredictable results
Synth pads and 
leads

227
5.8  Exercises
sampling techniques more acceptable and easier to accomplish. Several combinations of different wood­
wind can ignite a spark in your sequences. The flute, for example, blends particularly well with the other 
woodwind instruments, especially if voiced in unison or in octaves. The clarinet is a very agile instru­
ment (second only to the flute) and can be used in both classical and jazz ensembles, mainly because of 
its multifaceted sonority (dark in the low register, bright and exciting in the high register). To recreate 
the natural detuning effect that often characterizes woodwind solo instruments and sections, you can 
apply a fine detuning of around 1–3 cents. The use of volume changes allows you to effectively recreate 
the natural release of wind instruments. This technique is particularly telling when slow, sustained pas­
sages alternate with fast, staccato parts.
The synthesizer is given the same consideration, respect, and use in the modern orchestra or ensem­
ble as any other instrument. Most productions nowadays feature synthesized sounds that have little or 
no connection with acoustic instruments. The use of synthesizers can be seen by the modern composer 
and producer as an additional color palette to play with when arranging and orchestrating a new project. 
The tone, color, and texture possibilities of a synthesizer can vary considerably, depending on the type 
of synthesis adopted to generate the waveforms and patches. Among the many types of synthesis dis­
cussed, those most commonly found in today’s synthesizers are subtractive, additive, frequency modula­
tion, wavetable, sampling, physical modeling, and granular. Table 5.5 gives the main features and the pros 
and cons of these synthesis techniques.
5.8  EXERCISES
Exercise 5.1
Using as reference the categories of instruments analyzed in this chapter and the ones provided here, make 
a list of the best patches available in your studio for each category. Make sure to note the device and patch 
number, and write some comments about each chosen patch.
Category
Device
Patch/Bank No.
Comments
Piano
Electric piano
Electric guitar
Acoustic guitar
Electric bass
Acoustic bass
Drums
Generic strings
Violins
Violas
Cellos
Basses
 
(Continued)

228
CHAPTER 5  Elements of MIDI Orchestration
Category
Device
Patch/Bank No.
Comments
Generic brass
Trumpets
Flugelhorns
Trombones
French horns
Tubas
Clarinets
Bass clarinets
Soprano sax
Alto sax
Tenor sax
Baritone sax
Oboe
English horn
Bassoon
Contrabassoon
C flute
Piccolo
Alto flute
Synth pads
Synth leads
(Continued)

229
Creative Sequencing Techniques for Music Production.
© 
, 2005 Andrea Pejrolo. Published by Elsevier Ltd. All rights reserved.
CHAPTER
2011
The Final Mix
6
6.1  INTRODUCTION
Practical sequencing techniques and orchestration principles constitute the core of modern music pro­
duction. The way you sequence and arrange the various parts of your project is crucial to achieving pro­
fessional-sounding results. The previous chapters provided information and hints on how to improve the 
craft of sequencing, MIDI editing, and creative MIDI orchestration. Maintaining the highest standards in 
every respect and at every stage of the music production process is extremely important. As a composer 
and producer you probably have always focused your efforts mainly on the creative aspects of music 
production, sometimes forgetting the technicalities involved in this process. By now I hope you have 
learned how to balance the two by always keeping in mind how creativity and technology can go hand 
in hand. The latter is a tool that serves the former. Do not forget, though, that technology can also be an 
important source of ideas and inspiration to spark the creative process.
The fact that you have learned the techniques presented so far is meritorious and is challenging. 
Knowing more about what your studio can do for you can be useful and intimidating at the same time. 
What is often forgotten during the production of a project is that each stage, each moment is crucial and 
that the way we approach it has an impact on the final result. This is particularly true for the mix and 
the mastering (or premastering) stages. Often the composer and producer are so focused on the creative 
process that when it comes to mixing and delivering the final product they lose interest or they are not 
as focused as much as they should be (this is true particularly for composers). The days of the composer 
as the person in charge of only the writing and arranging stages of music production are gone. The com­
poser today most often is also the producer, programmer, performer, and audio engineer. This is why I 
want to dedicate the last part of this book to the final two stages of the music production process: mixing 
and premastering.
These two aspects are too often disregarded as “technicalities” by the composer/producer. A good 
mix can make a project shine and can improve its overall quality. A bad mix can damage irreparably a 
beautiful piece of music in the same way that bad sequencing can completely undermine a well-written 
composition. This is why I want to devote this chapter to presenting some fundamental principles pertain­
ing to the mixing and premastering stages of music production. While this chapter does not pretend to 
cover extensively such a vast subject, it is true that most composers are unfamiliar with some of the most 
important principles of mixing. Therefore, I consider it important for them to be exposed to some of the 
fundamental procedures and approaches involved at the mixing stage of music production. With just a 
few concepts and hints, your productions will sound better and will improve considerably in balance and 
clarity.

230
CHAPTER 6  The Final Mix
6.2  THE MIXING STAGE: OVERVIEW
All the effort you put into composing, sequencing, editing, and orchestrating your project deserves the best 
final results. The mixing stage is where everything comes together. All your tracks, equalizers, effects, and 
so on, are channeled down to a stereo track, or to a multichannel mix in the case of a surround project. 
Let’s look at how to approach the final mix and how to solve a few of the most common issues that often 
puzzle the modern composer/producer.
The first step in approaching the final mix for a generic hybrid project composed of MIDI and audio 
tracks is to choose how to handle the MIDI tracks. If your session contains a combination of both MIDI 
and audio tracks, then first decide whether the MIDI tracks need some extra “tweaking” from an audio 
(equalization, reverb, effects in general) point of view. If the sounds associated with the MIDI tracks are 
generated externally (by hardware MIDI devices or satellite computers), then to add effects to them you 
most likely will use the external processing power of your mixing board and outboard gear, or the built-
in effects of the MIDI devices. This technique is definitely faster and can save you time, but it is also 
limited by the number of channels and features of your mixing board and by the number of audio out­
puts present on your MIDI devices. If you opt for this solution, then the procedure is fairly simple. The 
audio and software synthesizer tracks of your sequencer will be mixed by taking advantage of the inter­
nal plug-in effects installed within the applications, while the MIDI tracks assigned to external MIDI 
devices will be processed and mixed using the equalizers and effects connected to your mixing board or, 
in the case of a digital board, through its internal effects.
Having applied all the effects and made the needed changes, you will mix down the main output of 
the mixing board to a stereo audio track of your sequencer. As I mentioned before, while this technique 
is probably the fastest, it has some limitations and also presents a few problems. For example, it is usu­
ally pretty hard to match the reverb, equalization, and dynamic effects of the plug-ins hosted by the 
sequencer and those of the outboard gear. By using two different sets of effects (plug-ins and external 
gear), most of the time you will accentuate the already noticeable difference between audio and MIDI 
tracks. In addition, if your MIDI devices do not feature enough separate outputs, it is hard to effectively 
apply equalization and effects to single MIDI tracks.
A better solution consists of “rendering” (which means converting ) all the MIDI tracks into audio and 
then mixing the entire project inside your digital audio workstation (DAW). This approach will guaran­
tee a more coherent and balanced mix. Some of the drawbacks of this technique, though, are related to 
the fact that the MIDI-to-audio conversion for hardware devices can be time consuming, since it has to 
be done in real time and for every single track independently. If your project is 5 minutes long and you 
have to convert 20 MIDI tracks, it could take up to 100 minutes to dump all the MIDI tracks as individual 
audio tracks. Keep in mind that this figure can vary, depending on how many inputs and buses are avail­
able on your audio interface and mixing board, respectively. The higher the number of inputs on the inter­
face and the higher the number of buses available on the mixing board, the faster the process will be.
Nevertheless, this technique is recommended, especially if you have a powerful enough computer to han­
dle a high number of audio tracks and plug-ins. By recording each MIDI track individually as audio mate­
rial, you will have greater control over equalization and effects such as reverb and dynamics. In addition, for 
each track you will be able to take advantage of the automation techniques we learned in Chapters 2 and 3.
I highly recommend rendering all the MIDI tracks as audio, for both the software and hardware syn­
thesizer. Because of the different nature of these two types of MIDI track, let’s analyze the rendering 
process for each type separately.
6.2.1  Rendering MIDI Tracks for Hardware Synthesizers
To record each MIDI track as a separate audio track coming from an external hardware synthesizer, you 
have to solo the MIDI track you want to record, record-enable an empty stereo track in the sequencer, 

231
6.2  The Mixing Stage: Overview
route the channels of your mixing board receiving the output from the selected MIDI device to a bus, 
and connect the output of the bus to the input of the audio interface attached to your computer. If you 
have multiple buses available on your mixing board, you can actually simultaneously transfer several 
tracks (one for each bus). If your mixing board lacks a bus system, you can use its main output as a tem­
porary bus and connect the main output of the board to the input of the audio interface (make sure not to 
send the output of the audio interface back to the board, to avoid a feedback loop). While this description 
can sound a bit intimidating, take a look at Figures 6.1 and 6.2 to compare the two options.
In Figure 6.1, the mixing board features a main set of outputs (L-R) and four buses (1–4). The inputs 
from MIDI device 1 are bused to buses 1 and 2 and then sent (through the buses) to inputs 1 and 2 of 
the audio interface. In the same way, the inputs from MIDI device 2 are bused to buses 3 and 4 and then 
sent to inputs 3 and 4 of the audio interface. MIDI device 3 will have to be bounced in a second pass and 
bused to any of the four buses that become available. Notice how all the inputs of the board are bused to 
the main output so that we can listen to them while bouncing. If the board does not have a bus system, 
you will temporarily have to use the main stereo output to send the inputs from the MIDI devices to the 
inputs of the audio interface. In this case you have to remember to send to the main L-R output only the 
channels of the board you want to be recorded. This process takes longer since, if you want individual 
audio tracks for each MIDI track, you will have to record them one by one.
FIGURE 6.1
Signal routing to record MIDI tracks as audio with a multibus mixing board.

232
CHAPTER 6  The Final Mix
In any case, remember to record the signal on the audio tracks as loud as possible but without distor­
tion. In the digital domain distortion can be a disaster. While in the analog world a little bit of distortion or 
saturation can create a nice warm and compressed effect, in the digital domain as soon as the signal goes 
over the 0 dB level you will get a nasty digital distortion noise that you will not be able to remove from 
the audio track. Usually try to keep your recording meters between 28 and 0 dB. When it comes to record­
ing volume settings, before transferring the MIDI tracks as audio there are two approaches you can fol­
low. If you have a fairly simple automation setup, where basically you only used Control Change (CC) 7 
and CC10 to set up initial volume and pan settings for the MIDI devices, I recommend suspending the 
automation for the MIDI tracks and resetting all the MIDI volumes to 127 (maximum) to be able to get 
the most output from each MIDI device. You will have to recreate the automation for volume and pan 
later, at the final mixing stage. If, however, you have created a complex automation for your sequence, to 
recreate the natural attack and release of live instruments, as explained earlier in this book, I recommend 
leaving the automation “On” and dealing with overall volume issues later, at the mixing stage.
In the case of a system where one ore more satellite computers are used you can follow the same pro­
cedure I described earlier, using the outputs of the audio interface of your satellite computers. If the sat­
ellite computers have interfaces with digital outputs I highly recommend using them to render the tracks 
since the quality will be higher; also, in the case of multi-out formats (like the Light-pipe), you will be 
able to transfer eight channels of audio in a single pass.
FIGURE 6.2
Signal routing to record MIDI tracks as audio with a mixing board without buses.

233
6.2  The Mixing Stage: Overview
6.2.2  Rendering MIDI Tracks for Software Synthesizers
The flexibility of using software synthesizers for your productions comes in very handy, particularly when 
it is time to mix your project. As I described earlier in the book, the beauty of using software synthesizers 
relies on the fact that their channel strips are virtually identical to those found in audio tracks. This means 
that you could mix them without rendering them first. In general, though, I recommend also rendering 
the software synthesizer tracks, for a couple of reasons. First, by having all the tracks rendered as audio, 
at the mix stage you will be more focused on the mix and you will treat the mix session as though it was 
composed of all audio tracks. Therefore, you won’t have to deal with any central processing unit (CPU) 
issues or MIDI hiccups, and your computer’s resources will be dedicated exclusively to the mix plug-ins. 
The second reason for rendering the software synthesizer tracks is for archival purposes. At any point 
in time, after the session is finished, you will be able to go back and remix the project without worry­
ing about compatibility issues that your “old” software synthesizers might have, basically future-proofing 
your project (not to mention the ability to mix your project in any DAW by simply importing all the audio 
files). Let’s take a look at how to render software synthesizer tracks in our four DAWs: Digital Performer 
(DP), Logic Pro (LP), Cubase (CU), and Pro Tools (PT).
In DP, rendering a MIDI soft synth track is extremely easy. DP refers to this process as “Freezing”. 
Having recorded your MIDI part and assigned the output of the MIDI track to one of your software syn­
thesizer tracks, select both the MIDI track and the virtual instrument track for the length of the part you 
want to render (usually it will be the entire length of the track plus one extra bar to accommodate the 
release and reverberation of the patch). It is really important that you choose both tracks, otherwise the 
rendering will not work. Now go to the Audio menu and select “Freeze Selected Tracks”. DP will auto­
matically create a new stereo track, it will switch its input to an open bus, and it will temporarily switch 
the output of the virtual track to the same bus. Recording will start automatically and a new audio version 
of your MIDI part will be available on the newly created audio track. When the rendering is complete you 
can mute the MIDI track and suspend the software synthesizer track to save CPU power.
In LP the process is called “Bounce in Place” and it is equally straightforward. To render a MIDI 
track right-click on the object(s) you want to render and select Bounce and Merge  Bounce in Place. 
A window will display some options you can choose from for the bounce (Figure 6.3).
FIGURE 6.3
Bounce in Place options in LP.

234
CHAPTER 6  The Final Mix
I recommend using a new track for the destination and switch to “Mute” for the source. You can also 
choose the Normalization parameters and to include or not include the channel effects and Volume/Pan 
automation.
In CU you can very easily render the outputs of your software synthesizers automatically by select­
ing File  Export  Audio Mix-down. The Export Audio Mix-down window gives the option to select 
different parameters and options as shown in Figure 6.4.
On the left side of the window you can choose the software synthesizer output that will be rendered. 
On the right side you will specify the file location, the file format, the audio engine outputs, and whether 
you want to import the newly generate file back into the project automatically (you definitely want 
to select this last option). Once done, click “Export” and remember to mute the MIDI track and the 
software synthesizer to save CPU power. With this technique, instead of rendering individual tracks or 
regions you will export all the output of a single software synthesizer (or multiple synthesizers) at once.
In PT you can follow the same process as for DP, with the only difference that instead of being auto­
matic you need to do it manually. After recording your MIDI tracks, to render them assign the output of 
your software synthesizer tracks to a free bus (3-4 in the example in Figure 6.5), then create a new audio 
track and assign its input to the same bus (3-4). Now record-enable the audio track, rewind to the posi­
tion where you want the rendering to take place, and hit record. The rendering will be done in real time.
6.2.3  Track Organization and Submixes
Once you have transferred all the MIDI tracks to audio, but before the start of mixing, take a few min­
utes to reorder the audio tracks, which will definitely speed up the process of mixing later. I recommend 
ordering the tracks by section: woodwind, brass, strings, synthesizers, piano, guitar, bass, and drums/per­
cussion. Keeping a standardized order for the tracks is particularly useful over time, since you will start 
getting used to it and it will get easier and easier to orient yourself in complicated mixing sessions. The 
order I suggest works well because it follows the standard order with which the sections are arranged in 
FIGURE 6.4
Export Audio Mix-down options in CU.

235
6.2  The Mixing Stage: Overview
a score layout. With this template everybody can easily look at your sequence and find any instrument in 
a matter of seconds.
After transferring the MIDI tracks as audio tracks, make sure to keep the MIDI data; do not delete 
them. They will come handy if you ever want to go back to the original MIDI tracks to make changes. 
Make sure you mute the MIDI tracks after each transfer, and move them to the bottom of the track list. 
Also keep in mind that, depending on the sequencer, you may be limited in the number of audio tracks or 
voices you can use. If the track count is limited, you may have to create submixes of certain instruments 
to be able to handle a high audio track count. Although I strongly recommend keeping as many separate 
audio tracks as possible, Table 6.1 lists possible instruments that can be grouped without creating too 
many limitations in mixing capabilities.
From Table 6.1 you can probably discern the four main factors that allow us to group certain instru­
ments together without having to sacrifice too much flexibility at the mixing stage: frequency range, 
reverb (and more generally overall effects), equalization, and pan.
FIGURE 6.5
Bus assignment for software instrument tracks rendering in PT.

236
CHAPTER 6  The Final Mix
The frequency range covered by each instrument and section plays an important role in grouping 
tracks. Usually instruments that cover a similar frequency range share similar settings of pan and reverb 
and therefore can be submixed on the same stereo track. The frequency range thus has a direct impact on 
the reverb settings and panning of the parts. Usually instruments that cover the lower frequencies of the 
audible spectrum need less reverb (or none at all) because high reverb would cause the mix to be muddy 
and unclear. For the same reason, high frequencies usually can handle and need more reverb; therefore, 
instruments that cover this area of the audible spectrum can be submixed on a common stereo track.
Table 6.1  Suggested Submixes in Case You Run Out of Tracks During a Mixing Session.
Instruments/Groups
Comments
High woodwind
Can be grouped together because of their similar sonorities and acoustic response 
to reverberation; includes clarinet, flutes, oboe, and English horn
Low woodwind
In general require less reverberation to maintain a clear and intelligible mix; includes 
bass clarinet, bassoon, and contrabassoon
Saxophones
Can be set as a separate category mainly because of their distinctive phrasing and 
sonority, even if they cover a wide frequency range
Trumpets
Constitute the higher range of the brass section, so they usually require different 
settings in terms of reverb and equalization
Trombones, French horns
Cover the middle and low range of the brass section; ideally you want to keep the 
two separated to have better control over the equalization of each sonority
Violins, violas
Even though there are substantial differences in terms of overall sonorities, they 
cover the high and mid–high range of the string family and for this reason they can 
be treated together in a mix where track count is an issue
String basses, cellos
Cover the low and mid-range of the string family; usually require substantially different 
settings in terms of equalization, panning and reverb than the violins and violas
Synthesized leads
Can be grouped together unless their sonic features and purposes are completely 
different
Synthesized pads
Usually can be easily and effectively mixed together since they can share most of 
the panning and reverb settings
Piano, electric piano, 
keyboard synthesizers
Acoustic piano would preferably be on a separate stereo track
Acoustic guitars
Make sure to separate the acoustic guitars from the electric guitar since they have 
different needs in terms of reverb and equalization
Electric guitars
See Acoustic guitars
Electric/acoustic bass
See Acoustic guitars
Bass drum
Needs to be separated from the rest of the drum kit mainly because of its need to 
be fairly dry (no reverb) and its peculiar equalization
Snare drum, toms
Ideally, the snare would be separated from the toms, but you can sometimes have 
them on the same track since they usually both require a similar amount of reverb
Hi-hat, cymbals, shaker, 
tambourine
Can be assigned to the same groups because of their frequency range and reverb 
settings

237
6.3  Panning
It is a bit more complicated to group several tracks based on their equalization needs, though. 
Usually each instrument has peculiar frequency characteristics that are specific to its sonority. If you 
have to make a choice, try to keep instruments inside the same section separated according to the fre­
quency range to which they belong (low, middle, or high).
6.2.4  The “Rough” Mix
Once you have the audio tracks in order and (if necessary) submixed according to the criteria just listed, 
it is time to start getting an overall balance among the tracks. This is an important step. Before starting 
to add effects and working on equalization, create an overall good balance between tracks. Even though 
there is no specific approach or technique for creating a draft mix, I recommend starting from the ground 
up, meaning from the foundation of the orchestration. Begin with the bass drums and then move to bass, 
then to the rest of the drum kit, followed by the piano and keys, guitars, strings, brass, woodwind, and 
then leads (which could include synthesizers, acoustic instruments, or vocals). Every time you add an 
instrument, go back and adjust the volumes of the other tracks. If you are doing things right, you will 
probably have to apply small changes to the tracks that are playing back already.
To achieve a perfect balance among instruments and tracks, keep in mind a few important aspects. 
Each style and composition has a certain feel and overall sonority that need to be achieved. Since you 
are the composer and producer of the project, you have the advantage of knowing extremely well the 
material being mixed. Focus on the elements important to that particular project.
Mixing these days is very much a companion to orchestration. By raising the volume of a certain 
section or instrument, you basically rewrite the dynamics you had in mind while writing and sequencing 
that part. I like to think of mixing as “conducting” the orchestra by indicating crescendos and decrescen­
dos, sforzando and pianissimo, and so on.
Once you’ve set in place a decent rough mix, I recommend saving it as a snapshot or storing each 
level as automation data; this will speed things up later if you have to program automation data for some 
of the tracks. When trying to get a good balance, look for two or three elements of the mix that represent 
the most characteristic and important features of that particular project, and work the other instruments 
and parts around it. This is a good approach to avoid a massive global wall of sound where nothing is 
clearly distinguishable and to reach overall clarity. Usually, lead instruments and especially vocal tracks 
are a good starting point for featured tracks. Always have two or three parts that you consider more pre­
dominant and exposed than the other background parts. It is also extremely important to always keep in 
mind that at any given moment in a piece there should be only one element in the foreground (e.g., the 
solo vocalist, or the soloist during a solo section), while the others will be spread between the middle 
ground (e.g., the rhythm section) and the background (e.g., pads and background parts). Using this struc­
tural approach will guarantee you a clear mix.
6.3  PANNING
The way you position your instruments in the horizontal left and right axes has an impact not only on the 
stereo imaging of the virtual orchestra but also on the balance between sections and instruments. Even 
though panning can change drastically from project to project, there are a few tips I find particularly 
useful. You will find that by changing the panning you will have to adjust the volume of some tracks. 
Usually instruments that are panned hard left or hard right tend to be more exposed and separated than 
instruments left more in a central position.

238
CHAPTER 6  The Final Mix
6.3.1  Balance
There are two main criteria to follow when panning instruments on the stereo image: balance and fre­
quency placement. Even though balance may seem an obvious consideration, it is often forgotten; if it is 
overlooked during the mix process it will have to be fixed later, at the mastering stage.
Balance is achieved by panning sections and instruments so you reach steady equality between the 
left and right channels. You definitely want to avoid having one side continually more active or predomi­
nant than the other. Check the balance between the two channels by listening carefully to the mix and by 
controlling the meters of the stereo master track of your sequencer.
A good starting point for maintaining balance is to have a clear idea of the instrumentation featured 
in the project and the importance of each instrument to the arrangement. Avoid extreme pan settings for 
lead vocal and instruments that are featured continually in the piece unless they can be counterbalanced 
by similar parts, such as a vocal counter voice or another lead instrument. For short and temporarily fea­
tured solos or passages, it is acceptable to have instruments hard panned, especially if you can alternate 
sides between several featured solos (e.g., a short electric guitar solo panned left followed by a short sax­
ophone solo panned right). Before deciding on the panning, I recommend sketching out graphically the 
instruments featured in the project and their placement, to come up with a sort of blueprint of the mix. 
However, avoid placing too many instruments in the center without taking advantage of the clarity and 
spatial openness offered by the stereo field. Pads, keyboards, and strings offer a good starting point to 
open the stereo image by either recording the patches in stereo and panning the stereo track hard left and 
right, or carefully panning each instrument using more extreme settings. For strings, brass, and wood­
wind sections, refer to Figures 5.10, 5.11, and 5.18 as a starting point for your mixes. As I mentioned 
earlier, each project is very different, so I suggest taking the settings shown as a general starting point to 
be adapted to your needs.
6.3.2  Frequency Placement
The second principle to keep in mind when working on panning and instrument placement is based 
on the frequencies featured in each part. The frequency placement approach is based on the fact that 
low frequencies are harder to place in space than high frequencies. This is because the brain perceives 
sounds in space according to the difference in phase of the waveforms received by the two ears. Since 
low frequencies have longer periods, a small difference in phase is harder to perceive and, therefore, low 
frequencies are more difficult for the brain to place precisely in space. Therefore, usually it is more natu­
ral for the ears to listen to audio material that features low-frequency instruments placed in the middle 
of the stereo image rather than panned hard left or right. You should also avoid extreme panning settings 
for low-frequency instruments, such as bass, bass drum, and string basses.
Remember, rules can be broken if there is a good reason. For example, if your project features a virtual 
ensemble mimicking a live ensemble such as a jazz quartet, a classical orchestra, or a string quartet, then 
it is better to use the placement that the live ensemble would follow in a live performance setting. A typi­
cal example would be a jazz quartet comprising piano, bass, drums, and saxophone. Here, you can place 
the instruments with more freedom by panning the piano slightly left, the drums in the center, and the bass 
slightly right. The saxophone could be placed either in the middle or slightly to the right to counterbalance 
the piano. The same can be said for a string quartet, where the cello (which covers the lower end of the 
spectrum in such an ensemble) can be panned right, just as it would be in a live performance setting.
As you can see, panning, like all the other elements, from sequencing to orchestration, can have a 
clear impact on how real your virtual ensemble sounds. Try to keep realistic settings in panning as well, 
and always try to reproduce a real live performance situation as much as possible, no matter which style 

239
6.4  Reverberation and Ambience Effects
or ensemble you are sequencing for. Keep in mind that you can always find creative and original solu­
tions when it comes to panning and that rules can be broken for creative reasons.
Whereas low frequencies usually sound more natural if panned in the center, high frequencies can be 
panned at any degree, ranging from center to hard left or right. An example is the hi-hat and cymbals of the 
drum set. These instruments can help to open the stereo image if panned with more extreme settings. Crash 
and ride cymbals can be panned, respectively, hard left and hard right (or the other way round), just as in an 
acoustic drum kit. The same can be said for shakers, tambourines, and high-pitched percussion in general.
The final goal of successful panning is to open up the stereo image of your mixes without creating 
unbalanced positioning of the instruments. By taking advantage of the entire stereo image and accu­
rately planned and precise panning settings, you can greatly improve the intelligibility and clarity of the 
production.
6.4  REVERBERATION AND AMBIENCE EFFECTS
Another important aspect of mixing is the type and amount of reverb you apply to single instruments and 
sections to place them correctly in a natural ambience. Natural reverberation is produced by a buildup 
and complex blend of multiple reflections of the waveforms generated by a vibrating object (e.g., the 
string of a violin or the head of a snare drum). The reflections are generated when the waveforms bounce 
against the walls of an enclosure, such as a room, a stadium, or a theater. Depending on the size of the 
enclosure and the type of material that covers the walls, reverberation can change in length, clarity, and 
sonic characteristics. In a DAW, reverberation is usually added at the mix stage in order to place instru­
ments accurately and realistically in a predetermined acoustic environment. The style of the project and 
the type of instrument are crucial in determining and properly choosing the best reverb type and in setting 
its parameters.
While pan allows you to control the positioning of the instruments along the horizontal axis, reverb, 
along with volume, allows you to control the positioning of the instruments bidimensionally, while the 
frequency content (and therefore the equalization of a specific instrument/track) gives control over the 
vertical placement, adding a third dimension to the virtual stage (Figure 6.6).
As you can see in Figure 6.6, by controlling the balance between reverb, volume, and pan you can 
precisely position any instrument in any place on the bidimensional canvas of your mix. In general, by 
adding more reverb (wetter signal) and by lowering the volume of the dry signal you can position an 
FIGURE 6.6
Multidimensional placement of the instrumentation through the use of pan, reverb, volume, and frequency control.

240
CHAPTER 6  The Final Mix
instrument in the background of a mix. If you lower the reverb level and slightly raise the dry volume, 
you can bring the same instrument closer to the listener and therefore inside the middle-ground area. A 
louder and dryer signal will sound very much in the front of the mix (foreground).
Listen to Examples 6.1–6.4 on the website to hear how volume, reverb, and pan can be used to place 
an instrument in space. In addition, you have all the panning settings I described earlier to move instru­
ments and sections on the horizontal axis. The way you program the parameters of reverb is crucial to 
getting the best out of it. Table 6.2 lists the most important parameters present on a reverb unit or plug-in 
and a brief explanation of the impact they have on the overall sound.
To appreciate how these parameters affect reverberation, listen to Examples 6.5–6.12 on the website.
The parameters listed in Table 6.2 are generic indications of some of the main settings found on 
the majority of reverb units (hardware and software). Some units are simpler, providing more basic 
Table 6.2  Generic Reverb Parameters.
Parameter
Description
Comments
Type
Describes the type and overall sonority 
of a reverb
Typical types include hall, chamber, plate, 
gate, and reverse
Length (Decay)
Represents the decay or length of the 
actual reverb
Usually expressed in seconds or milliseconds
More sophisticated reverbs allow for control 
of the decay parameter for two or more 
frequency ranges
Room Size
Controls the size of the room in which 
the reverb is created. Bigger rooms 
are usually associated with longer 
reverberation times
Sometimes, depending on the type of plug-in, 
you can also find a control that allows you to 
control the room shape
Early Reflections (ER)
Controls the first bounces that follow 
the dry signal before the actual reverb
Can have several subparameters, such as ER 
size, gain, and Pre-Delay
Pre-Delay
Controls the time between the dry 
signal and the early reflections
Delay
Controls the time between the dry 
signal and the actual reverb
Diffusion
Controls the distance between the 
reverb’s reflections
A low diffusion features more scattered and 
distant reflections; a high diffusion closer and 
more frequent reflections
Low diffusion is usually more indicated for 
sustained pads and passages (i.e., string 
sections); high diffusion for percussive 
instruments and fast transients audio material
High Shelf Filter
Cuts some of the high frequencies of 
the reverberated signal
Not found in all the reverbs
Useful to avoid excess of high frequencies 
in the reverberated signal to recreate a more 
natural sound
Mix
Controls the balance between dry and 
wet signals
If the reverb plug-in is used as insert, set the 
Mix parameter to 50% or lower. If you use the 
reverb as a send, set the Mix parameter to 
around 100%

241
6.4  Reverberation and Ambience Effects
parameters (for example, you might not find a control for the diffusion in low-end reverbs); other units 
may give you more control parameters in filtering and room size capabilities. Remember that reverbs, in 
terms of CPU power, require a lot of calculation and therefore are among the plug-ins that necessitate 
more resources. When possible, use reverbs through an auxiliary send and not as inserts (see Chapter 4 
for a review on how to use effects such as insert and send). This will save CPU power because you will 
need fewer reverbs and can share them among several tracks.
When considering which reverb to use and how to choose the right settings, let your ears be your 
guide. Each scenario is different and can call for very different configurations. Try not to use the stan­
dard presets that come with your plug-in. They can be a useful starting point, but in my many years 
of composing, sequencing, and production I have never found a preset that was 100% useful without 
customization.
When setting the parameters of a reverb always think in terms of bidimensional placement of the 
audio material you are working on. Longer reverbs have the effect of pushing the instruments farther back 
from the listener; shorter reverbs move the instruments closer to the listener. As a general rule (remember, 
rules are to be broken if necessary!), try to keep the reverb length between 1.5 and 2.5 seconds, consid­
ered a good starting point for generic reverberation for vocals (2.0–2.5 s), drums (1.5–2.0 s), and piano, 
strings, and pads (1.7–2.2 s).
As we saw with panning, reverberation too is affected by frequency-related issues. Instruments that 
cover mainly the low end of the frequency range, such as bass and bass drums, usually require much 
less reverberation (or none), while on instruments that cover the middle- and high-frequency range, such 
as guitars, hi-hat, and even snare drum, you can apply more reverb. This is mainly because reverb in 
general adds muddiness to the mix through the tail created by its reflections. Adding too much reverb 
to instruments that cover the low-frequency range reduces the definition of the overall mix, especially 
for the electric bass and the bass drums, which can quickly lose sharpness and clarity. For sections that 
cover a wider range of the audible spectrum, such as the string section and the brass section, I recom­
mend using slightly different reverb settings, depending on the specific instruments forming the section. 
With strings, use slightly less reverb for the basses, a bit more for the cellos and violas, and a wetter mix 
for the violins.
The best way to accurately control the amount of reverb assigned to each track is to use the aux send 
available on each track, as explained in Chapter 4. The same is true for the brass section, where the tuba 
can use very little reverb, while the trombones can handle medium reverberation settings. Trumpets and 
French horns are the brass instruments that can use a good amount of reverb to blend nicely with the 
other instruments.
6.4.1  Specific Reverb Settings for DP, CU, LP, and PT
All four sequencers come bundled with one or more plug-ins to simulate reverberation effects. All 
of the included plug-ins offer a comprehensive list of parameters and controls, such as eVerb in DP, 
RoomWorks in CU, AIR Reverb in PT, and PlatinumVerb in LP (Figures 6.7–6.10). Some of the DAWs 
also offer excellent convolution reverbs based on sample technology, which are able to reproduce the 
most realistic environments. Convolution reverbs are based on an innovative technique and algorithm 
that, instead of “synthesizing” a reverb effect by calculating the reflections through standard algorithms, 
uses a more realistic approach called the convolution process. This plug-in is able to generate the reflec­
tions of the reverberated signal by merging the input dry signal of a track with a sampled reverbera­
tion signal that was prerecorded in a real acoustic space, such as a church, a recording studio, a theater, 
or a generic room. The quality of reverbs built on the convolution process is much higher than those 
generated via conventional algorithms. In addition, users technically could “sample” their own acoustic 

242
CHAPTER 6  The Final Mix
­environments and build their own reverb libraries. I will discuss the Space Designer reverb in detail later 
in this chapter (see Section 6.4.3).
6.4.2  Synthesized Reverbs
Though convolution reverbs are definitely the most popular effects these days they have a high CPU 
demand, so synthesized reverbs are still a good option for slower computers. In addition, these reverbs give 
you a much higher level of control over the parameters of the reverberation and therefore are the perfect 
choice for creating interesting effects that do not necessarily try to achieve realistic results but instead try 
to create original sonorities closer to a sound design approach. Most of the parameters explained in Table 
6.2 are present in each of the reverbs I just mentioned, and therefore you should be fairly familiar with 
them already. Other, more specific parameters change according to the plug-in used. In Tables 6.3–6.6 you 
FIGURE 6.7
eVerb in DP.
FIGURE 6.8
RoomWorks in CU.

243
6.4  Reverberation and Ambience Effects
will find brief descriptions of the parameters of each reverb, so that you can completely customize the set­
tings and adapt each parameter to your projects and needs.
One extra parameter can be revealed and adjusted by clicking on the small arrow located in the 
­bottom left corner of PlatinumVerb (Figure 6.10). The Early Reflections (ER) Scale controls the scaling 
factor for the Early Reflections Predelay.
FIGURE 6.9
AIR Reverb in PT.
FIGURE 6.10
PlatinumVerb in LP.

244
CHAPTER 6  The Final Mix
Table 6.3  Digital Performer’s eVerb Parameters.
Parameter
Description
Comments
Mix
Same as the generic description (Table 6.2)
Initial Reflection: Size, 
Level, and Pre-Delay
Size, Level (dB) and Pre-Delay of the early 
reflection environment
Pre-Delay controls the time delay 
between dry signal and early reflections
Reverb: Delay, Level
Delay and Level of the actual reverb tail
Delay controls the time delay between 
early reflections and the beginning of 
the reverb tail
Reverb Time: Low 
end, High End and 
Crossover
Low End and High End allow you to set two 
separate reverb lengths for two separate 
ranges of the frequency spectrum. Crossover 
controls the split point between the two 
frequency areas
Diffusion
Same as the generic description (Table 6.2)
Shelf Filter: High Cut, 
High Damp
A shelf filter that allows you to cut some of 
the high frequencies of the reverb tail. Cut 
(0.5–18 kHz) selects the frequency, while 
Damp select the value (dB) (from 0 to 240) 
of the damping
Color
Allows you to send some of the early 
reflections into the reverb tail, adding tonal 
coloration to the overall sound
A very subtle parameter that, when 
activated, morphs the parameters of the 
Early Reflections into the reverb tail
Hi-Q Link
Connects the High Frequency Reverb time 
with the Shelf Filter parameters so that when 
you increase the former the latter will cut 
more of the higher frequencies
Creates as natural a reverberation as 
possible. If turned Off, the reverb will 
sound brighter; if turned On, it will sound 
darker and in most cases more realistic
Table 6.4  Cubase’s RoomWorks Parameters.
Parameter
Description
Comments
Input Filters Freq. (lo–hi)
Control the frequencies of the filters at 
the input of the reverb
Useful to shape the sound before it reaches 
the reverb
Input Filters Gain (lo–hi)
Control the gain of the filters at the 
input of the reverb
Useful to shape the sound before it reaches 
the reverb
Pre-Delay
Same as the generic description 
(Table 6.2)
Reverb Time
Same as reverb Length (Table 6.2)
Size
Same as the generic description for 
Room Size (Table 6.2)
Diffusion
Same as the generic description 
(Table 6.2)
Width
Controls the width of the stereo image 
of the reverb (100%  full stereo; 
0%  mono)
(Continued)

245
6.4  Reverberation and Ambience Effects
Table 6.4  (Continued)
Parameter
Description
Comments
Damping Filters Freq. 
(lo–hi)
Control the frequencies of the filters of 
the signal post reverberation
Useful to shape the sound after it reaches 
the reverb
Damping Filters Gain 
(lo–hi)
Control the frequencies of the filters of 
the signal post reverberation
Useful to shape the sound after it reaches 
the reverb
Envelope Amount
Controls how much the Attack and 
Release parameters affect the reverb
Envelope Attack
Controls the Attack time of the 
envelope stage of the reverb
Controls how long (milliseconds) it takes for 
the reverb to reach full volume
Envelope Release
Controls the Release time of the 
envelope stage of the reverb
Controls how long (milliseconds) it takes for 
the reverb to fade out
Mix
Same as the generic description 
(Table 6.2)
The “Wet Only” button bypasses the Mix 
knob, giving a 100% wet signal (click this 
button when using the reverb as Aux/Send)
Efficiency
Controls the amount of CPU reserved 
to run the reverb
Lower settings will result in a higher usage of 
CPU and a higher quality reverberation
Table 6.5  Pro Tool’s AIR Reverb Parameters.
Parameter
Description
Comments
Pre-Delay
Same as the generic description (Table 6.2)
Room Size
Same as reverb Type (Table 6.2)
Early Reflection 
Type
Determines the type and characteristics of the 
early reflection
Early Reflection 
Spread
Determines the stereo spread of the early 
reflection (100%  full stereo image; 0%  
mono)
Reverb In Width
Determines the stereo spread of the signal 
before the reverberation
Reverb Out Width
Determines the stereo spread of the signal 
after the reverberation
Delay
Controls the size of the delay lines of the 
reverb
Higher values translate into longer 
reverberation
Room Ambience
Controls the attack of the reverberation
Lower values translate into a faster attack
Room Density
Controls the rate at which the reverb density 
increases overtime
Higher densities translate into a smoother 
reverberation; lower values translate into a 
choppier reverberation
Diffusion
Same as the generic description (Table 6.2)
Decay
Same as reverb Length (Table 6.2)
(Continued)

246
CHAPTER 6  The Final Mix
Table 6.5  (Continued)
Parameter
Description
Comments
HF Time
Controls the decay time for the HF of the 
reverb
Using this parameter (along with the HF 
Freq. and the Cut) you can control the 
color of the reverberation
HF Freq.
Control the frequency used by the HF Time 
parameter
Cut
Controls the frequencies for the High Cut filter
LF Time
Controls the decay time for the LF of the 
reverb
Using this parameter (along with the LF 
Freq.) you can control the color of the 
reverberation
LF Freq.
Control the frequency used by the LF Time 
parameter
Reverb Time
Same as reverb Length (Table 6.2)
Balance
Controls the mix between early reflections and 
tail of the reverb
Very effective in making the reverberation 
more generic (more tail reverb) or more 
colored (more early reflections)
Mix
Same as the generic description (Table 6.2)
HF: High Frequencies; LF: Low Frequencies.
Table 6.6  Logic Pro’s PlatinumVerb Parameters.
Parameter
Description
Comments
Pre-Delay
Same as the generic description (Table 6.2)
Room Shape
Determines the shape of the room in 
which the reverberation is created. The 
value goes from 3 to 7, representing the 
number of corners of the room
Stereo Base
Controls the positioning of the virtual microphones 
used to audition the reverb generated
Room Size
Same as the generic description (Table 6.2)
The value represents the length of the walls
Initial Delay
Same as Delay (Table 6.2)
Spread
Controls the stereo image of the reverb
With a value of 0% you get a mono reverb 
(same reverberation on both channels); with a 
value of 100% you get a full stereo effect
Crossover
Allows you to set the split point at which 
two separate settings for the reverb can 
be made, transforming the effect in a real dual-
band engine
Low Ratio
Controls the deviation from the main 
Reverb Time section for the lower 
frequencies below the crossover point
With a value of 100% the settings reflect 
exactly those of the main Reverb Time section; 
with values below 100% the reverberation time 
for the lower frequency section gets shorter; 
for values above 100% it gets longer
(Continued)

247
6.4  Reverberation and Ambience Effects
6.4.3  Convolution Reverbs
As I mentioned earlier, convolution reverbs utilize sampled responses of specific environments (such 
as theaters, churches, and studios, as well as outdoor environments) to produce (or, better, reproduce) 
artificial reverberation. This approach has the indisputable advantage of creating an extremely realistic 
reverberation. Some of the drawbacks include the high CPU power needed to calculate the reflections 
in real time and also, to some extent, the limited changes you can apply to the original settings with­
out obtaining unwanted sonic artifacts. Two of our four DAWs have bundled convolution reverbs that 
­provide excellent reverberation. Let’s take a look at these reverbs.
The convolution reverb in LP is called Space Designer (Figure 6.11). While some of the parame­
ters used in Space Designer should look familiar by now (Low Shelving EQ, Stereo Spread, Crossover), 
Table 6.6  (Continued)
Parameter
Description
Comments
Low Freq. 
Level
Indicates the overall level for the reverb for the 
frequencies below the crossover point
High Cut
An LP filter that cuts the high frequencies 
above the frequency set
Density
Controls the density of the reflections forming 
the reverb tail
A denser reverb usually sounds better but 
requires more processing power
Diffusion
Same as the generic description (Table 6.2)
Reverb Time
Same as reverb Length (Table 6.2)
Mix
Same as the generic description (Table 6.2)
Balance ER/
Reverb
Controls the balance between the early 
reflections and the reverb tail
FIGURE 6.11
Space Designer, the convolution reverb in LP.

248
CHAPTER 6  The Final Mix
others need further explanation. For example, the familiar Mix parameter in Space Designer is controlled 
using the two sliders on the right. One controls the dry signal (“direct”) and the other one controls the 
signal of the reverb (“reverb”). The input section on the left of the control panel is used to set how Space 
Designer routes the input from the channel on which the effect is inserted. Fairly self-explanatory are 
stereo and mono, receiving the input in stereo or mono from the channel on which the effect is inserted. 
Xstereo receives a stereo input but inverts the left and right channels. For a more detailed description of 
the other Space Designer parameters, consult Table 6.7.
If you are in Impulse Response (IR) Sample mode you have control over the envelope parameters of 
Volume and Filter, much like in the modifier section of a synthesizer. You can switch between the two 
different edit modes from the top of the graphic display of the sampled reverberation. To alter the parame­
ters, either use the graphic display and grab/drag the insert points of the envelopes, or change their values 
in the parameter list section at the bottom of the window. The same procedure can be used in Synthesized 
IR mode, with additional control over the Density envelope, which allows you to precisely control the 
change in density of the reflections over time. The Pre-Delay of the reverb is controlled from the window 
Table 6.7  Logic Pro’s Space Designer Parameters.
Parameter
Description
Comments
IR Sample
Allows you to load new Impulse Response 
files or to switch back to a Sampled 
response from a Synthesized one
Click on the drop-down menu and select 
either “Load” or “Load and Initialize the 
parameters” to open a new response
Sample Rate
Determines the sample rate at which the 
Impulse Response works
The starting sampling rate is the one at 
which LP operates. By choosing /2, /4, or /8 
the effect operates at the respective fraction 
of the original frequency. By lowering the 
frequency the CPU is freed up, requiring 
less computational power. Unless “Preserve 
Length” is selected, the length of the reverb 
will get longer or shorter depending on the 
ratio chosen (e.g., /2 will set the reverb 
length to double its original time)
Preserve Length
If checked, changing the sample rate ratio 
will not affect the length of the reverb but 
only its quality
Synthesized IR
Generates a synthesized Impulse Response 
(instead of a sampled IR) that is based on 
the current settings
Filters
Allow you to control the color of the 
reverberation:
HP: high-pass filter to reduce low 
frequencies
BP: band-pass filter to reduce mid 
frequencies
6 dB LP: a gentle low-pass filter
12 dB LP: a more drastic low-pass filter for 
warmer reverberation
Resonance: boosts some of the frequencies 
around the cutoff frequency

249
6.5  Equalization
underneath the graphic display of the sampled waveform. The “Reverb Volume Compensation” but­
ton allows you to automatically match the volume of different Impulse Response files. The “Latency 
Compensation” option allows you to automatically delay the dry signal of the necessary amount of sam­
ples it takes the reverb to calculate the reflections. If you want to create your own Impulse Response 
files, you can use the “Deconvolution” button in the top right corner of the reverb window. For a detailed 
description of how to prepare the files used by Space Designer to create successful Impulse Response 
files, I recommend reading the Space Designer manual’s comprehensive section on this topic.
DP also features a convolution reverb called ProVerb (Figure 6.12). The principle behind ProVerb is 
similar to those discussed for Space Designer. You load an IR by clicking on the small down-pointing 
arrow in the middle of the plug-in window (Figure 6.12). You have a comprehensive list of different sam­
pled spaces. The reverb features a nice equalization section at the bottom of the window, plus pre-delay, 
damping, and mix knobs at the top. ProVerb also features an interesting dynamic mix processor (Figure 
6.12) that allows the compression of the signal after the reverb (but before the return), where the dry sig­
nal drives the side-chain input. In other words, this compressor allows you to lower the level of the reverb 
in the mix (through the Sensitivity knob) according to the level of the dry signal (Threshold knob).
6.5  EQUALIZATION
An equalizer, in its broad sense, allows you to boost or cut the volume of specified frequencies. During 
the mix, equalization can be effectively used in different ways either to correct problems that were cre­
ated during the recording session or problems that arise because of incompatibility among instruments, 
or simply in a creative way to produce original sonic effects.
No matter what you are going to use an equalizer for, there are a few things you should know when 
embarking on an equalization session. Equalizers generally need to be used as inserts on the channel 
FIGURE 6.12
ProVerb, the convolution reverb in DP.

250
CHAPTER 6  The Final Mix
and not as auxiliary sends. You have to be familiar with the most common types of equalizers in a DAW 
setting. Among the several types of equalizer available nowadays there are six main categories that have 
proven to be the most useful in a mixing situation: peak, notch, high shelf, low shelf, high pass, and low 
pass. Table 6.8 describes their parameters and some of their most common uses.
Table 6.8  Characteristics and Parameters of the Most Common Types of Equalizer.
Type of 
Equalizer
Description and Parameters
Typical Uses
Peak
Allows you to cut or boost frequencies around the 
center frequency
Center frequency: determines the frequency to cut or 
boost
Gain: positive gain boosts, negative gain cuts
Q point: determines the “shape of the bell” or how 
wide the area around the cutoff point is going to be: 
the lower the value the larger the bell, and the higher 
the value the smaller the bell. The Q parameter can 
usually (but not always) vary from a value of 0.7 (equal 
to a two-octave frequency range) to 2.8 (½ octave)
Extremely versatile. It can be used to 
pinpoint and cut/boost a very precise 
frequency or it can be used in a broader 
way to correct wider acoustic problems. 
It is usually used in the middle of the 
frequency range
Notch
Leaves all the frequencies unaltered except the one(s) 
specified by the center frequency that will be cut
Center frequency: determines the frequency to cut
Gain: positive gain boosts, negative gain cuts
Q point: determines the “shape of the bell” or how 
wide the area around the cutoff point is going to be: 
the lower the value the larger the bell; the higher the 
value the smaller the bell. The Q parameter can usually 
(but not always) vary from a value of 0.7 (equal to a 
two-octave frequency range) to 2.8 (½ octave)
Usually used in live concerts to reduce 
feedback since it allows you to choose 
a very narrow set of frequencies to cut 
drastically
High Shelf
Cuts or boosts the frequency at the cutoff and all the 
frequencies higher than the set cutoff point. Has only 
two parameters: the cutoff frequency and the gain
Usually used in the mid–high and high 
end of the spectrum. Can be effectively 
used to brighten up a track by using 
a positive gain of 3 or 4 dB and a 
cutoff frequency of 10 kHz and higher 
(be careful because this setting can 
increase the overall noisiness of the 
track). Can also be used to reduce the 
noise of a track by reducing by 3 or 4 dB 
frequencies around 15 kHz and higher
Low Shelf
Cuts or boosts the frequency at the cutoff and all the 
frequencies lower than the set cutoff point. Has only 
two parameters: the cutoff frequency and the gain
Usually used in the low–mid and low range 
of the audible spectrum to reduce some of 
the rumble noise caused by microphone 
stands and other low-end sources
High Pass
Cuts all the frequencies below the cutoff point. Has 
only one parameter: the cutoff frequency
A very drastic filter, often used to cut very 
low rumble noises below 60 Hz
Low Pass
Cuts all the frequencies above the cutoff point. Has 
only one parameter: the cutoff frequency
A very drastic filter, often used to cut very 
high hiss noises above 18 kHz. Use with 
caution to avoid cutting too much high 
end of the track

251
6.5  Equalization
Figure 6.13 displays the symbols with which these types of equalizer are usually indicated.
Remember that equalizing is mainly a problem-fixing procedure. This means that there is no point in 
starting to play around with the settings if you’re not clear about what you want to achieve and how the 
final result should sound. A good approach to equalizing is to listen carefully to the soloed track and to 
come up with a list of things you might want to improve or correct. The next step is to bring the gain up 
and sweep across the frequency range until you find the exact cutoff or center point you want to cut or 
boost. Finally, set the gain as desired.
Keep in mind when equalizing that you will have to make small adjustments every time you add 
tracks to the mix, since the way an instrument sounds is affected by the frequencies and ranges of the 
other instruments. The most important concept here is to be able to emphasize the characteristic frequen­
cies of the track you are working on and to eliminate frequencies that do not enhance its sonic features 
in any particular way. You should be able to carve a small niche within the audible range for each instru­
ment and section so that it is clearly intelligible and not masked by other instruments. If the mix sounds 
muddy and cluttered, start by trying to focus on which instruments contribute to the lack of clarity. Then 
try to use the equalizer to add clarity by gently shifting the center of each instrument involved so that 
there is no overlap. As a general rule it is always better to cut than to boost, mainly because the human 
ear is more used to a reduction than to an augmentation in the intensity of frequencies. While it is hard 
to generalize, as I mentioned earlier with reverb settings, there are a few common settings that make a 
useful starting point during an equalization session, as summarized in Table 6.9.
When equalizing, you must pay attention to some of the most common mistakes that even the seasoned 
engineer sometimes makes. First, always try to keep your equalization gain parameter to a reasonable level. 
FIGURE 6.13
Conventional symbols for the most common types of equalizer.

252
CHAPTER 6  The Final Mix
Table 6.9  Generic Frequency Ranges and Their Features Used in an Equalization Session.
Frequency
Application
Comments
20–60 Hz
Cut to reduce rumble and noises related to 
electric interference
It is a good idea always to reduce 
this area by 4–6 dB to lower the low-
frequency noise
60–80 Hz
Boost to add fullness to low-frequency 
instruments such as bass and bass drums
100–200 Hz
Boost to add fullness to guitars, French horns, 
trombones, piano, and snares
Cut to reduce “boomy” effect on mid-range 
instruments
This frequency range effectively controls 
the powerful low end of a mix
200–300 Hz
Cut to reduce low and unwanted resonances 
on cymbals
Boost to add fullness to vocal tracks
Be careful not to boost too much of this 
frequency range so as to avoid adding 
muddiness to the mix
400–600 Hz
Cut to reduce unnatural “boxy” sound 
on drums
Boost to add presence and clarity to bass
This frequency range can also be effective 
to boost the low range of the guitar
1.4–1.5 kHz
Boost for intelligibility of bass and piano
2.8–3 kHz
Boost to add clarity to bass
Boost to add attack and punch to guitars
This range can also be used effectively to 
add clarity on vocal parts
5–6 kHz
Boost for vocal presence
Boost for attack on piano, guitars, and drums
A general mid-range frequency area to 
add presence and attack
7.5–9 kHz
Cut to avoid sibilance on vocal and voice 
tracks
Boost to add attack on percussion
Boost to add clarity, breath, and sharpness to 
synthesizers, piano, and guitars
A mid–high-range area that controls the 
clarity and the attack of the mid–high-
range instruments
10–11 kHz
Boost to increase sharpness on cymbals
Boost to add sharpness on piano and guitars
Cut to darken piano, guitars, drums, and 
percussion
A high-range section that affects clarity 
and sharpness
14–15 kHz
Cut to reduce sharpness on cymbals, piano, 
and guitars
Boost to add brightness on vocals
Boost to add real ambience to synthesized and 
sampled patches
18 kHz
Cut to reduce hiss noise
Boost to add clarity to overall mix
A delicate high-range section that should 
require drastic positive or negative gain 
settings only in extreme situations

253
6.5  Equalization
As a general rule, avoid cutting or boosting by more than 6 dB unless absolutely necessary. If for some rea­
son you see that some of your equalizer settings go over this limit, try to question why and see if there is a 
better solution to the problem. The same can be said for situations where you end up boosting (or cutting) 
several frequencies at the same time whose only effect is a raising (or lowering) of the overall volume of 
the track with no real change to its sonic content. In this case, try to bypass the equalizer and experiment 
with volume changes instead. All four sequencers provide comprehensive equalization tools that can be 
inserted on any audio track. In the next section I provide a brief description of some of the equalization 
tools that each sequencer features, to give you a better idea of the possibilities they offer.
6.5.1  Equalizers in DP, CU, PT, and LP
DP is bundled with a comprehensive seven-band equalizer (Figure 6.14) called MasterWorks EQ.
Two bands are locked as pass filters (where you can control only the cutoff frequency); the other five 
can all work as peak/notch but two of these five (the two located in the middle on the lower portion of the 
window) can also be set as shelving by clicking on the familiar Shelving icon (Figure 6.14). The param­
eters of each band can be changed either graphically by clicking and dragging the respective band handle 
in the graphic window or by altering the values through the knobs of the equalizer window. Each frequency 
can be bypassed independently. Another great feature of this equalizer is the built-in fast Fourier transform 
(FFT) frequency analyzer that allows you to see the frequency spectrum of the signal being equalized. To 
turn it on or off, click on the small button marked FFT, located at the bottom right of the graphic window.
In CU the equalization section (a four-band equalizer) is automatically inserted on each audio chan­
nel of the virtual mixing board. The only action you have to take to use the equalizer is to activate the 
“On” button on each band (Figure 6.15).
FIGURE 6.14
MasterWorks EQ in DP.

254
CHAPTER 6  The Final Mix
If you open the mixing window in CU you can choose to look at the equalizer section in two differ­
ent ways: a combined linear and curve view, and a traditional linear view, with the parameters shown 
as horizontal bar lines (Figure 6.15). The type of view can be changed by selecting it from the sidebar 
menu of the mix window (Figure 6.15).
The equalization plug-in bundled with PT is very comprehensive and flexible. You have the option of 
two, four, or seven bands. The latter is the most flexible (Figure 6.16), providing two pass filters (HPF 
and LPF, which can also be converted into notch filters), two shelving (HF and LF, which can also be 
converted into peak filters), and three peak filters.
You also have control over the overall Input and Output gain of the equalizer. As we saw for DP, you 
can control the parameter either graphically or through the control knobs.
LP is among the four sequencers that offer the most comprehensive equalization bundle options. 
The list of plug-ins dedicated to equalization includes the most flexible and complete: “Channel EQ”, 
“Linear Phase EQ”, “Fat EQ”, legacy filters such as “DJ EQ” and “Silver EQ”, and a series of single-
band “high and low” equalizers. The Channel EQ and the Linear Phase EQ are the most advanced. 
While they are identical in terms of features, the former is more CPU efficient while the latter provides 
a phase that is perfectly linear, which translates into a much clearer signal at the output stage but with 
higher CPU usage. Since the two are identical in terms of parameters, I will describe them as Channel 
EQ from now on. This multiband equalizer provides an extensive number of settings and eight bands, 
which include High Pass, Low Pass, High Shelf, Low Shelf, and four peak equalizers (Figure 6.17).
FIGURE 6.15
A four-band equalizer in CU showing a combined linear and curve view, and the traditional linear view.

255
6.5  Equalization
FIGURE 6.16
Multiband equalizer in PT.
FIGURE 6.17
Channel equalizer in LP.

256
CHAPTER 6  The Final Mix
The parameters for each band can be set by either moving the mouse over the band you want to 
edit in the graphic window and simply clicking and dragging to make changes, or using the numeric 
fields located in the lower part of the window. The parameters in the field can be changed by moving the 
mouse over the desired field and clicking and dragging (an upward motion increases the value; a down­
ward motion decreases it). To reset a band to its original default setting, Option-click on its field.
As was the case for the MasterWorks EQ in DP, the Channel EQ features a built-in real-time spec­
trum analyzer that shows the frequency content of the track on which the effect is inserted. You can 
activate the analyzer by clicking the button to the left, located underneath the “Master Gain” slider. To 
change the resolution of the analyzer, click on the field marked “low”, “medium”, or “high”, depending 
on the default settings, and change it to the desired value. The resolution controls how detailed the ana­
lyzer is. A low setting uses very little CPU power but gives you a more approximate reading; a high set­
ting gives a more detailed representation but uses more CPU power. You can monitor the analyzer either 
pre- or post-equalizer by selecting the respective setting in the field underneath the “Analyzer” button.
6.6  DYNAMIC EFFECTS: COMPRESSOR, LIMITER, EXPANDER, AND GATE
Through reverb and panning you can control the positioning of an audio signal and through equaliza­
tion you can control its shape in terms of frequency response. The last type of effects I want to discuss 
in this chapter is the dynamic effects. Their name comes from the fact that they are designed to alter, in 
one way or another, the dynamic range of an audio signal. They are important because they allow you to 
control over time the ratios between high and low peaks in the dynamics of an audio track. These effects 
are more often used on live acoustic instruments than on synthesizers; therefore, their application in a 
sequencer/MIDI environment is somehow limited. Nevertheless, I would like to introduce you to their 
practical uses and parameters.
There are four main dynamic effects: compressor, limiter, expander, and gate. Table 6.10 lists their 
main features and applications.
Among the dynamic effects shown in the table, the compressor is the one most commonly used on 
acoustic live tracks. Usually its effectiveness is somewhat reduced when inserted on audio tracks that 
contain material recorded from a synthesizer, since their audio material is for the most part well bal­
anced in terms of amplitude variations. I recommend using a compressor sparsely on synthesized tracks 
Table 6.10  Dynamic Effects.
Effect
Description
Comments
Compressor
Allows you to reduce the dynamic range 
(difference in amplitude between high and 
low peaks) of an audio signal
Useful in situations where the audio signal has 
a very high dynamic range
Limiter
Represents a drastic version of a 
compressor, where the “Ratio” parameter 
is set extremely high
Used mainly during tracking to avoid distortion 
and during mastering to maximize the overall 
volume of the mix
Expander
The exact opposite of a compressor; allows 
you to increase the dynamic range of an 
audio signal
Sometimes useful to “regenerate” a track that 
was over compressed
Gate
An extreme application of an expander, 
where the ratio is set extremely high
Very useful to reduce unwanted noises during 
“silent” passages of an audio track

257
6.6  Dynamic Effects: Compressor, Limiter, Expander, and Gate
and only if really needed. Synthesized tracks in general can benefit from a higher dynamic range and not 
a reduced one. Reducing their dynamic range even farther can sometimes flatten their sonority too much, 
compromising their realism. However, a compressor can help an acoustic track to “seat” better on a com­
plex mix with the other MIDI tracks.
As mentioned in Table 6.10, a limiter is very similar to a compressor. The only difference is that a 
limiter yields a more drastic reduction of the highest peak in dynamics, practically setting a ceiling over 
which the signal cannot go. It can be effectively used to increase the overall volume of a signal without 
getting distortion. The parameters of compressor and limiter are listed in Table 6.11.
Table 6.11  Parameters of Compressor and Limiter.
Parameter
Description
Comments
Threshold
Sets the level (dB) above which the effect 
starts reducing the gain of the signal
If the signal is below the threshold the effect 
doesn’t affect the signal. As soon as the level goes 
over the threshold the effect starts reducing the 
gain of the signal
Ratio
Sets how much the gain of the signal 
is reduced after the level goes over the 
threshold
It is usually set as an x:y value. With a setting of 
1:1 the level is not altered; at 30:1 the level is highly 
compressed; at 100:1 the effect is considered a 
soft limiter. In certain limiters the Ratio parameter is 
omitted, implying a ratio set to infinity. In this case 
the effect can be referred to as a hard limiter
Attack
Sets how quickly the effect reacts after the 
signal goes over the threshold
Release
Sets how quickly the effect reacts after the 
signal returns below the threshold
Knee
Controls the curvature during the 
transitional moment below and above the 
threshold point. A “soft-knee” allows for a 
more gentle transition, while a “hard-knee” 
generates a more drastic transition
Not found on every plug-in
Gain
Allows you to control the overall gain of 
the signal after compression/limiting
Since the clear effect that compression and 
limiting have on the signal is a reduction of 
amplitude, the Gain parameter allows you to 
boost back the compressed signal by the amount 
specified
Some dynamic plug-ins feature an “Auto Gain” 
option that automatically sets the level of 
amplification so that the output after compression 
matches the input before compression
Side-Chain 
Input
Allows you to trigger the dynamic effect of 
the track where it is inserted (e.g., track 
A) through the signal coming from another 
track (e.g., track B). To set it up, send part 
of the signal from track B to a bus (via an 
aux send), then set the input of the side-
chain dynamic on track A to the same bus
Not all the dynamics plug-ins feature a side-chain 
input. Practical applications include compressing 
a bass track with the side-chain input set to the 
output of the bass drum, to add clarity and punch 
to the combination of the two signals

258
CHAPTER 6  The Final Mix
Figure 6.18 shows the effect that a compressor has on a generic waveform. Notice how the gain 
parameter is set to boost the overall level of the signal, in order to bring back the highest peak to its 
original value.
Expanders and noise gates have effects opposite to those of compressors and limiters. An expander 
increases the dynamic range of a signal by lowering the level of audio material that is below a set thresh­
old. The amount of reduction is set by the ratio. If an extreme ratio setting is chosen (such as negative 
infinity), then the effect is a noise gate. This means that when the signal goes below the threshold, its 
amplitude is turned to negative infinity, meaning that the signal is basically muted. While expanders are 
usually not very common in a MIDI studio, noise gates can be effectively used to reduce the noise in 
between “silent” passages. Look at Figure 6.19 to see how a noise gate would alter a generic waveform.
6.6.1  Dynamic Effects in DP, PT, CU, and LP
All four sequencers come bundled with dynamic plug-ins. In DP and PT the dynamic plug-ins include 
compressor, limiter, expander, and gate. Their parameters feature the same options described in Table 6.11.
FIGURE 6.18
Example of a compressor.

259
6.6  Dynamic Effects: Compressor, Limiter, Expander, and Gate
In DP the dynamic effects are grouped under a single plug-in (Figure 6.20). Their parameters and 
settings are standard and are easy to use if you are familiar with dynamic effects in general. Use Table 
6.11 as a guide on how to set the parameters in DP.
In PT you will find two different plug-ins, one for compression/limiting and one for expansion/gating 
(Figure 6.21a, b). As you can see in Figure 6.21, the parameters of the dynamic effects in PT are pretty 
much standard. Both effects feature a side-chain input with high-pass and low-pass filters. These filters 
are very useful for equalizing the signal of the side-chain input before it reaches the dynamic effect.
The dynamic plug-ins the CU include compressor, gate, expander, and limiter. In the compressor 
(Figure 6.22a) bundled with CU, all the traditional parameters, threshold, ratio, make-up (gain), attack, 
hold, and release, should look familiar.
In CU’s compressor you can also control the analysis of the input to be set between peak or RMS. This 
option (also present in the compressor plug-in of LP) allows you to choose the dynamic-peak detection 
algorithm utilized by the plug-in. RMS uses the average power in amplitude of the signal as a basis for 
determining whether the threshold is reached. With the “Peak” option selected, the plug-in will determine 
the level based on a peak-by-peak analysis. In general, use RMS for sources, such as vocals and strings, 
that do not feature a high number of fast transients, and use peak for percussion and drums parts with fast 
transients. The “Auto” buttons allow you to let the compressor decide the best make-up gain and release 
depending on the program preset selected. The “Live” button on the other end disengages the look-ahead 
option, which will translate into a less accurate processing but will reduce the latency of the effect (excel­
lent for live performances). The limiter in CU is very simple in terms of parameters (Figure 6.22b). You 
can control the input and output signals along with the release time (an auto option is also provided for 
the release).
FIGURE 6.19
Example of a noise gate.
FIGURE 6.20
Dynamic effect in DP.

260
CHAPTER 6  The Final Mix
CU’s expander (Figure 6.23b) features all the same parameters of the compressor, while the gate 
(Figure 6.23a) has some the interesting options of a fully selectable low-pass, high-pass, or band-pass 
filter for either the input or the side-chain input.
LP offers an extensive and flexible suite of dynamic plug-ins, including compressors, limiters, 
de-essers, expanders, and gates. While the basic dynamic effects (compressor, limiter, expander, and gate) 
feature the same standard parameters listed in Table 6.11, the most comprehensive and peculiar among all 
the different dynamic plug-ins available in LP is the “Multipressor” (Figure 6.24).
This dynamic effect allows you to control compressor, expander, and noise reduction settings for 
up to four different bands (you can also choose to use it for only two or three bands, saving some CPU 
power). In practical terms the Multipressor is a multiband compressor/expander that allows you inde­
pendent control over the compression/expansion parameters of each band individually. This type of 
dynamic effect is particularly useful in mastering situations where detailed and precise control over the 
entire spectrum is a crucial feature.
FIGURE 6.21
Dynamic effects in PT: (a) compression/limiting, and (b) expansion/gating.
FIGURE 6.22
(a) Compressor and (b) Limiter in CU.

261
6.6  Dynamic Effects: Compressor, Limiter, Expander, and Gate
In the Multipressor the parameters for each band are the same. The range of each band can be 
adjusted by clicking and dragging horizontally the separation lines between the bands or by entering the 
crossover band value in hertz (Hz) (Figure 6.24). Each band has a separate volume that can be adjusted 
by clicking and dragging up and down on the horizontal lines of each band or through the gain make-up 
fields. You can turn on and off a band by clicking on its identifying number.
The parameters of each band are typical of those found in a compressor/limiter, including the com­
pressor’s threshold, ratio, RMS/peak, attack, release, and expander’s threshold, ratio, and reduction, 
which allows you to specify the floor level used for noise reduction. If you move the slider all the way 
to the left, maximum reduction will be applied; if you move it all the way to the right, the reduction fil­
ter is bypassed. The “Lookahead” parameter allows you to specify how far in advance the Multipressor 
looks for incoming peaks. Usually the longer the value of the lookahead parameter, the better it is, since 
it allows the computer more time to react to incoming peaks. The Multipressor gives you also the option 
FIGURE 6.23
(a) Gate and (b) Expander in CU.
FIGURE 6.24
Multipressor in LP.

262
CHAPTER 6  The Final Mix
of autogain. I find this effect a Swiss-Army knife for dynamics, useful for any situation that requires 
detailed dynamic intervention.
As I mentioned with the other effects, it is almost impossible to come up with presets that would 
work in every situation. This is particularly true for dynamic effects. Each piece of audio material is dif­
ferent and therefore calls for specifically targeted parameters and settings. Compressors and expanders 
seem the hardest to set since their effect is very often subtle and difficult to identify. In general, when 
working with such effects, I recommend starting with very mild settings and then working your way 
toward more drastic and aggressive changes. In the case of a compressor I suggest starting with a low 
ratio and a fairly high threshold. This allows the compressor to kick in sporadically and to reduce the 
volume very gently. Then start lowering the threshold and raising the ratio until you start hearing unde­
sired audio artifacts, such as pumping effects. At that point, back up a little by raising the threshold and 
lowering the ratio again just a notch. This should give you a good starting point in basic settings. I also 
recommend not overusing dynamic effects. They can be very useful if applied with parsimony, but they 
can be disastrous if unnecessarily placed everywhere.
6.7  BOUNCE TO DISK
When your mix is ready it is time to mix down the multitrack session into a stereo file (or multiple mono 
files in the case of a surround mix). This process is often referred to as bounce to disk. This means that 
the sequencer does a digital mix-down of all the audio, aux, and software instruments tracks present in 
a sequence and creates a mix on a stereo file (for regular stereo projects) or multiple mono files (for sur­
round projects). The entire operation is done in the digital domain and in some cases at a speed that is 
faster than real time. The way the four sequencers handle this operation is very similar. The main thing 
to keep in mind is that basically the mix-down that will be created reflects exactly the mix you are hear­
ing from your sequencer, so make sure to check carefully that the mix is exactly how you want it before 
creating the bounce. Let’s take a look at how each sequencer handles the bounce-to-disk operation.
In DP make sure that all the tracks you want to be bounced are playback-enabled. Select the bounda­
ries of the bounce by clicking and dragging over the tracks and the bars in the track list window. Include 
all the tracks and bars you want in the bounce. Select the “Bounce to disk” option from the Audio menu. 
In the window that appears you can specify several parameters that will affect the bounce (Figure 6.25).
Starting from the top, the first drop-down menu allows you to select the file format of the bounce, if 
your bounce will be a stereo file (split stereo), a regular mono file (mono no attenuation), or mono with 
attenuation (mono with 3.5 dB attenuation). This last option prevents your mono mix from clipping and 
distorting when the two channels (left and right) are combined.
If you took advantage of the multichannel capabilities of DP, you can also select a multichannel 
option for the mix-down. Select for resolution 16 bit if you eventually want to burn an audio CD, use 24 
or 32 if you want to work further on the stereo file (for premastering and mastering applications). With 
the “Import” option you can select whether you want the file to be imported directly into the sequence 
(Add to sequence) or added only to the Soundbites window (Add to Soundbites window), or not imported 
at all. The “Source” menu specifies the virtual output of DP that will be used to collect the audio mate­
rial to be bounced, usually the main output to which each audio channel is assigned. You can choose the 
destination of the files on your hard disk by clicking on the “Choose Folder” button at the bottom of the 
window.
Once all options are set, give a name to the bounce and simply click the “OK” button. The time 
at which DP creates the bounce depends on the number of plug-ins used and on how powerful your 
computer is.

263
6.7  Bounce to Disk
A similar procedure is followed by CU. To bounce to disk in this sequencer, make sure the left and 
right locators are set to the boundaries you wish to bounce (you can set them either by punching in the 
start and end points in the transport window or by clicking and dragging their icons in the ruler at the 
top of the Arrange window). Make sure all the tracks you want to include in the mix are not muted, and 
select the File  Export  Audio Mix-down (Figure 6.26).
FIGURE 6.25
Bounce to Disk options in DP.
FIGURE 6.26
Audio Mix-down in CU.

264
CHAPTER 6  The Final Mix
The window that appears allows you to specify several settings that will affect the bounced file. Table 
6.12 sums up all the different parameters available in CU for the “Audio Mix-down” operation.
In LP the bounce operation is triggered by pressing the “Bounce” button located on every audio 
output track in the mix window (Figure 6.27). Clicking on the Bounce button will open the “Bounce 
Dialog” window. This window offers you parameters similar to those we analyzed with CU. Table 6.13 
(page 266) has a summary of the parameters used by LP during the bounce operation.
If you want to add the bounced file to the current audio file list of the Audio window, click on 
the “Add to Audio Bin”. After setting the parameters for the mix-down, click on the Bounce button. 
Dithering is often used when downgrading the bit resolution during the bounce. Using a lower bit 
Table 6.12  Audio Mix-down Options in Cubase.
Parameter
Description
Comments
Name
Specifies the name of the file that will be created 
by the bounce
Path
The location where the bounce will be saved
File Format
You can choose to have the bounce 
automatically converted to Wave, AIFC, AIFF, 
MP3 (MPEG-1 Layer 3), Ogg Vorbis File, Wave 
64, or Windows Media Audio File
For a more detailed description of the 
most common audio file formats refer to 
Table 6.15
Channels
Mono creates a mono mix
You can also choose to bounce a mono 
version of the mix (Mono Mix-down),
the L/R channels only (L/R Channels), the 
stereo channels as separate files 
non interleaved (Split Channels), and opt 
for a real-time export instead of an offline 
one (Real-Time Export)
Stereo Split creates a stereo mix that is formed 
by two separate files, one for each channel
Stereo Interleaved creates a stereo mix where 
both channels are stored in a single file
N. Chan Split is used if you want to export a 
surround mix as independent split files or create 
mixes from buses
N. Chan Interleaved is the same as the above 
option but it generates interleaved stereo files
Audio Engine 
Output
Sample Rate: determines the SR of the bounce 
(8–96 kHz)
Bit Depth: determines the BD of the bounce (8, 
16, 24, 32)
Import into 
Project
Pool: the bounce will be automatically imported 
in the Audio Pool
Audio Track: the bounce will be automatically 
imported in an audio track
Create new Project: a new project will be created 
with the bounce automatically imported
Channel Batch 
Export
You can choose to batch export each individual 
audio and soft synth channels on single files

265
6.7  Bounce to Disk
resolution introduces audio artifacts that can greatly deteriorate the sound. This is because a low bit rate 
is responsible for the introduction of higher quantization noise during the conversion, especially for sig­
nals at extremely low amplitudes. When the amplitude of a digital signal is very low, the ratio between 
amplitude and quantization noise is very low and therefore even a small quantization artifact can con­
siderably damage the quality of the sound. Dither can reduce this problem by adding to the signal a very 
low white noise that is able to both mask and reduce the distortion created by the quantization process. 
Since the introduction of the noise can be annoying, another filter, called Noiseshaping, can be intro­
duced. This filter moves the noise generated by the dithering process toward the extreme ranges of the 
audible spectrum, reducing the overall perception of the white noise.
In PT, to mix down your project, select File  Bounce To  Disk. The range, in terms of time/bars 
included in the bounce, depends on the region selected in the Edit window. If nothing is selected, then 
the entire sequence will be included in the bounce. The options included in the Bounce window are 
summed up in Table 6.14.
After selecting “Bounce…” you will be prompted with the typical “Save” menu where you can select 
where the bounced file will be saved.
6.7.1  Audio File Formats
As we saw in the previous section, each sequencer can bounce in different file formats, which can be 
used for different applications and in different situations. Table 6.15 lists these file formats, their main 
features, and their uses.
FIGURE 6.27
Bounce button in LP.

266
CHAPTER 6  The Final Mix
Table 6.13  Options Available in the Bounce Window in Logic Pro.
Parameter
Description
Comments
Destination
LP allows you to simultaneously bounce to 
up to four formats: PCM, MP3, AAC, and 
burn an audio CD
You can check any or all of the four 
types of bounce
Start/End Position
Selects the range of bars that will be 
included in the bounce
File Format
Varies depending on the “Destination” 
option selected
For PCM you can choose among four 
formats: AIFF, WAV, SD2, and MP3
For MP3 you can select the bit rate, 
quality, and stereo options
For AAC you can select the bit rate
For the burn option you can select 
to burn either an audio CD or an 
audio DVD
Resolution
For the PCM bounce option it sets the bit 
resolution of the bounce (8, 16, or 24)
If you selected MP3 as the file format 
the bounce options will change, 
allowing you to change all the 
parameters related to MP3 encoding, 
including Bit Rate, VBR (Variable Bit 
Rate), Quality, High Pass filter at 10 Hz, 
and Stereo Mode (Joint or Normal)
Sample Rate
For the PCM bounce option it allows you to 
select the sample rate of the bounce: from 
11.25 kHz to 192 kHz
Stereo File type
Split: creates a stereo mix that is formed by 
two separate files, one for each channel
Interleaved: creates a stereo mix where both 
channels are stored in a single file
Dithering
Allows you to apply dithering to the 
bounced file to reduce quantization noise 
when going from a higher bit resolution to a 
lower one (e.g., files recorded at 24 bit that 
are bounced at 16 bit)
Dithering should be used only when 
going from a higher bit resolution 
to a lower one during the bounce 
process. There are three different 
types of dither in LP, based on POW-r 
(Psychoacoustically Optimized Word- 
length Reduction) technology: POW-r1, 
dithering; POW-r2, with wide-frequency 
range Noiseshaping; POW-r3, with 
2–4 kHz Noiseshaping
Bounce Mode
Real-time: the bounce is executed in real-
time
Offline: it speeds up the bounce
Normalize
If set to “On” the final mix will be 
automatically normalized

267
6.7  Bounce to Disk
Table 6.14  “Bounce to Disk” Options in Pro Tools.
Parameter
Description
Comments
Bounce Source
Selects the output that will be used to collect 
the audio material
Can be set as an audio output or a bus
File Type
Selects the type of file format that will be 
used for the bounce
The options are: SD2, WAV, AIFF, MP3, 
QuickTime, and MXF
Format
Selects the type of channel format (Stereo/
Mono) of the bounce:
Mono (Summed): it is a mono bounce
Multiple Mono: a multichannel mix-
down with each file stored separately 
(noninterleaved)
Stereo Interleaved: a stereo mix where both 
channels are stored in a single file
Resolution
Sets the bit resolution of the bounce (8, 16, 
or 24)
Sample Rate
Sets the sample rate of the mix-down, from 
44.1 kHz to 192 kHz
You can also choose Pull up/down rates 
which are slight variations of the main 
sample frequencies
Conversion Quality
You can choose from Low, Good, Better, 
Best, and Tweak Head. The last one gives 
the best results but is the slowest
This option is available only if you choose 
to bounce at a sampling frequency and bit 
resolution that are different from those at 
which the session was recorded
Convert During 
Bounce
If selected, PT converts the files while they 
are bounced
This option allows for a faster bounce but 
affects the accuracy of the timing of plug-in 
automation
Convert After 
Bounce
If selected, PT converts the files after they 
are bounced
This option allows for a slower bounce but 
provides the highest accuracy in terms of 
timing of plug-in automation
Import After 
Bounce
Allows you to automatically reimport the 
bounced files in the current session for 
further editing and processing
Table 6.15  Major Audio File Formats Supported by DP, LP, PT, and CU During the Bounce Operation.
Audio File 
Format
Features
Uses and Comments
AIFF
Audio Interchange File Format is an 
uncompressed format used mainly on 
Macintosh machines but compatible with any 
major operative system. It is an interleaved 
format
Mainly used to prepare audio files for audio CD 
burning on Macintosh platforms
AIFC
A compressed variation of AIFF
The compression can vary depending on the 
algorithm used. Some examples of algorithms 
used are IMA 4:1 and A-Law 2:1
(Continued)

268
CHAPTER 6  The Final Mix
Table 6.15  (Continued)
Audio File 
Format
Features
Uses and Comments
WAV (Wave)
Mostly used on Windows machines, it is also 
compatible with Macintosh machines. It can 
be either compressed or uncompressed. The 
uncompressed version is also referred to as 
PCM/uncompressed, while the compression 
formats depend on the CODEC installed with 
the operative system. It is an interleaved format
In its uncompressed format, it is mainly used to 
prepare audio files for audio CD burning on PC 
platforms
Sound 
Designer II 
(SD2)
A file format developed by Digidesign, it is 
among the most popular audio file formats 
used by audio professionals. It can be either 
interleaved or noninterleaved. It can be time-
stamped, meaning that an audio file can be 
stamped with the original SMPTE position at 
which it was recorded. In addition to its original 
time-stamp location a “User” location can be 
added for easier handling during editing
The perfect choice for exchanging audio files 
among platforms and applications, since the 
time-stamping options allow you to set each 
imported file to its original location
Broadcast 
Wave File 
(BWF)
Has the same features of Wave file but it lacks 
the compression option. In addition, it can be 
time-stamped.
Wave 64
A format developed by Sonic Foundry that is 
identical to the regular Wave format but it is 
capable of handling much larger files (over 2 GB)
Because of its capability of storing large files, 
can be effectively used to record live concerts 
or very long sessions
MPEG-1 Layer 
3 (MP3)
A compressed format based on lossy-
compression algorithms. The data reduction 
of the algorithm depends on the bit rate 
chosen. As a general indication a bit rate of 
128 kbit/s reduces the original size of the 
uncompressed file to 1/10
Should be used only for quick references mixes 
or for mixes that need to be posted on the 
Internet or sent by email. A bit rate of 128 kbit/s 
gives a fairly good quality and a fairly small sized 
file. For more accurate compression (and slightly 
larger files) choose either 160 or 192 kbit/s; for 
smaller files (but lower quality) choose 96 or 
80 kbit/s. Depending on the software installed 
on your computer there are other parameters 
that you can control when bouncing/encoding in 
MP3 format. The most common options are:
l	 Variable Bit Rate (VBR): sets the encoder 
to constantly change bit rate according to 
the complexity of the audio material being 
processed
l	 Stereo Mode: if set to “Normal” the encoded 
file will hold information for each channel in 
two separate tracks, while if set to “Joint 
Stereo” the file will have one track holding 
information common to both channels and 
the other track holding information unique 
for each channel
l	 Filter: usually allows you to cut all the 
frequencies below 10 Hz, reducing the size 
of the file without compromising its quality
(Continued)

269
6.8  Premastering: Introduction
6.8  PREMASTERING: INTRODUCTION
Once your mix is ready, all your tracks are balanced, well placed in space, and equalized, and the final 
mix has been bounced to disk, your project is almost ready to be delivered. I say almost, because one 
of the steps involved in the production process that leads to the final product is premastering. This step 
is sometimes overlooked by modern composers because their focus is mainly on the composition and 
orchestration stages.
Before discussing any further what a premastering session involves, I would like to clarify the differ­
ence between premastering and mastering. The former is done by the composer/producer right after the mix 
stage to prepare the final mixes for professional mastering or small-scale distribution. The latter is usually 
done by a professional mastering engineer just before going to the mass production stage, and it involves 
a series of steps, tools, and techniques specifically targeted at certain audio material. In other words, a pre­
mastering session usually involves tools that are directly accessible in a regular project studio, while a mas­
tering session requires more sophisticated tools, studio equipment, and a specially trained engineer.
Nevertheless, I always advise producers and composer to premaster their projects, mainly because 
the sound and quality of their productions will improve greatly, even with a few small premastering 
touches. You will be amazed by what a multiband compressor, a limiter, and a little bit of equalization 
can do to improve the overall sound of a mix. Premastering does not pretend to be a substitute for a more 
detailed and in-depth mastering session. Rather, it is a way to get your material as ready as possible 
for one. For a detailed analysis of the mastering process I recommend the excellent book by Bob Katz 
Table 6.15  (Continued)
Audio File 
Format
Features
Uses and Comments
AAC
Advanced Audio Coding
This format is based on the same principles of 
MP3. Achieves better audio quality than MP3 
at the same bit rate
QuickTime
The multimedia proprietary format created 
by Apple for its multimedia applications. Can 
handle both audio and digital movies. In PT, 
for example, you can use it to “Bounce to 
Movie” to create a new movie file with the 
edited audio material from the current session
Ogg Vorbis
A patent-free compressed audio format 
similar to MP3 that provides good audio 
quality with relatively small file sizes
Windows 
Media Audio 
format
Developed by Microsoft, this format is 
the equivalent of QuickTime for Windows 
machines. Included as a default encoding 
option in PT and CU for PC, but not in the 
Macintosh version
A compressed format based on a lossy-
compression system similar to MP3
Windows 
Media Audio 
Pro format
A more recent evolution of the Windows 
Media Audio format that allows you to 
compress the audio files and bounces with a 
lossless algorithm. Also supports multichannel 
formats
At the moment, available only as default in CU 
for Windows
DP: Digital Performer; LP: Logic Pro; PT: Pro Tools; CU: Cubase.

270
CHAPTER 6  The Final Mix
entitled Mastering Audio: The Art and the Science. Let’s take a look at the steps in a premastering ses­
sion and the tools involved.
6.8.1  The Premastering Process: To Normalize or Not?
To premaster the audio material you have bounced to disk, you have to reimport it to your sequencer of 
choice. You can choose the reimport option just before the bounce operation, or you can create a com­
pletely new project and manually reimport the audio files you want to premaster. If you choose the first 
option, I recommend creating a new arrangement/sequence for the premastering session to keep it sepa­
rated from the original multitrack session. I usually choose this option if I am working with a single song 
or cue, since I prefer to keep the entire project in a single session file. However, if I am working with mul­
tiple cues, I prefer the second option, which allows me to create a separate project file for the premaster­
ing session of the entire multicue project.
Once your files have been imported to your premastering project, there are a few steps to follow to get 
your material at the best level of audio quality possible. The first step involves making sure the beginning and 
end of the stereo mix contain no digital clicks or pops. These artifacts are usually generated if a waveform 
doesn’t start with an amplitude of negative infinity (centerline). Figure 6.28(a) shows a waveform that would 
click when played back. To fix this problem you have to fade in the beginning of the file and fade out the end 
of it to smooth out such glitches. Figure 6.28(b) shows the same waveform after a fade-in was applied.
To avoid unwanted digital artifacts, always apply a very short fade-in and fade-out at the beginning 
and end, respectively, of the bounced audio file. Some engineers like to apply “Normalization” to the 
audio files before the fade-in/fade-out process. This is highly controversial among engineers. Personally 
I do not particularly like normalizing audio files before the final mastering process; nor, as a matter of 
fact, do I favor normalization at all. Normalizing a file means bringing the highest peak in amplitude of 
the file to a certain level (usually set to the maximum of 0 dB) without changing its overall dynamic. In 
other words, when you normalize a file you increase its amplitude just up to the level of distortion with­
out altering the dynamic of the file. While this process may seem good since it makes the mix louder by 
increasing the overall volume of a file, it also increases the noise in the file, thereby limiting its real ben­
efits. In addition, when we alter the original audio file even further, the signal degradation introduced by 
the normalization process may be counterproductive because of the extra quantization distortion intro­
duced during the gain-changing process. Even though controversial, the normalization function can be 
useful in raising the volume for material recorded at a particularly low volume. In Figure 6.29 you can 
compare the same waveform before and after normalization.
Listen to Examples 6.13 and 6.14 on the website to compare the same material before and after nor­
malization. Table 6.16 sums up the procedures for applying normalization for all four sequencers.
FIGURE 6.28
(a) A waveform with a digital click at the beginning, and (b) the same waveform with a fade-in applied.

271
6.8  Premastering: Introduction
6.8.2  Premastering Equalization
The next step in premastering is equalization. The type and amount of equalization applied for premas­
tering can vary, depending on the audio material you are processing. Keep in mind that equalization is 
not always needed and so it is really up to you whether to apply it or not. In most cases you will find 
that a small amount of equalization at this stage can bring to life the mix, and it can also be used to 
fix smaller mistakes overlooked at the mixing stage. At this point you do not want to apply anything 
extremely drastic; you should not boost or cut more than 2 or 3 dB maximum per band. If you find your­
self having to apply more correction, then consider going back and remixing the project.
The main idea here is to apply several small corrections to fine-tune the production. You can use a four- 
or eight-band equalizer to correct small imperfections here and there. Usually you end up boosting the high 
frequencies (around 12–5 kHz) a little bit to add sparkle, and correcting the bass frequencies to avoid mud­
diness and increase the low end of the mix (you can start by cutting around 250 Hz and boosting a little bit 
around 100 Hz). I usually like to work in the middle frequencies to avoid the boxed and nasal sonority that 
is typical of some MIDI productions. For this reason I like to cut frequencies around 1 kHz a bit and boost 
frequencies between 6 and 7 kHz, paying particular attention to not affecting too much the vocal range of 
the singer, if present. As I mentioned before, each boost or cut should be no greater than 2 or 3 dB.
Listen to Examples 6.15 and 6.16 on the website to compare a stereo mix before and after a premas­
tering equalization session. All four DAWs provide excellent multiband equalizers to tackle this task 
very well.
6.8.3  Multiband Compression
When you are satisfied with the equalization, it is time to insert a multiband compressor on the audio 
channel you are premastering. By applying band-specific compression to the stereo material, you can 
effectively and precisely rebalance the dynamic relation between the main sections of your mix. As with 
equalization, I recommend not over compressing your material, especially since at the final mastering 
stage the engineer will probably apply more compression.
The same rules learned for single-band compression apply at this stage. Start with mild settings (low 
ratio and high threshold) and slowly increase the ratio and lower the threshold until you can start hear­
ing the compressor creating unwanted audio artifacts. At that point back up a little by gently raising the 
threshold slightly. Repeat the same process for each band until you reach a satisfying balance among the 
bands. The range of each band can vary extremely, depending on the material you are compressing and 
on the number of bands available on the compressor plug-in you are using. For a three-band compressor 
(a)
(b)
FIGURE 6.29
The same waveform (a) before and (b) after normalization.

272
CHAPTER 6  The Final Mix
I recommend starting with the low range from 20 to 500 Hz, the middle range from 500 to 5000 Hz, and 
the high range from 5000 to 20,000 Hz. In a four-band compressor (such as the one bundled with LP 
that we analyzed in the previous section) you can start with the bands divided in the following manner: 
20–200 Hz, 200–1000 Hz, 1000–8000 Hz, and 8000–20,000 Hz. Once again, these are starting point set­
tings to be adapted according to the audio material you are compressing. In general, try to isolate the key 
elements of the music you are compressing, and target your band separation so that you have individual 
control over these elements. For example, if you are compressing a dance piece where the bass drum is 
prominent, try to focus on its signal and frequencies to achieve a tighter and punchier sound. While PT 
is not bundled with a multiband compressor (although there are plenty of third-party plug-ins that can be 
used), LP comes bundled with the Multipressor, which was discussed earlier in this chapter.
DP features a three-band compressor (MasterWorks compressor) that allows you to control the usual 
parameters of a compressor for each of the three bands independently (Figure 6.30).
In addition, you can solo and/or bypass each of the three bands to monitor the compression on each band 
separately. The solo/bypass buttons for each band are located in the upper left corner of the MasterWorks 
compressor window. The band ranges can be adjusted by moving the arrows located below the band ranges 
area in the top left corner of the window, or by punching in the values in the parameters fields.
Table 6.16  The Normalization Process in DP, CU, LP, and PT.
Sequencer
Procedure
Parameters
DP
Click on the Soundbite you want to 
normalize and open the Waveform Editor 
(Control-Option-Command-W). This brings 
up the Destructive audio editor. Select the 
area of the audio file you want to normalize 
(if you want to normalize the entire file use 
Command-A to select all), then choose the 
“Normalize” option from the Audio menu
There are no options in DP regarding the 
normalization process. Normalization is applied 
always at 100%
CU
Click on the audio part you 
want to normalize. Select 
Audio  Process  Normalize
The Maximum value represents the ceiling at 
which the highest amplitude of the audio part 
will be raised. By clicking on the “More” button 
you can also set two other parameters: pre- 
and post-crossfade. These two options allow 
you to control how the normalization process 
is applied to the file by mixing over time 
unprocessed and processed audio (and vice 
versa)
LP
Open the Sample Audio editor by double-
clicking on the audio object you want to 
normalize. Select the area you want to 
process (Command-A to select all) and 
select the “Normalize” option from the 
Functions menu
You can change the percentage of 
normalization under the “Settings” option 
found in the Functions menu (100%  0 dB 
ceiling)
PT
From the Edit window select the 
region you want to normalize and then 
select the Normalize option from the 
AudioSuite  Other menu
You can specify the percentage of 
normalization (100%  0 dB ceiling)
DP: Digital Performer; LP: Logic Pro; PT: Pro Tools; CU: Cubase.

273
6.8  Premastering: Introduction
CU features an impressive five-band compressor called “Multiband Compressor” (Figure 6.31). The 
band ranges are set by moving, with the mouse, the handles located to the left and right of each band 
range at the top of the plug-in window. You can change the gain of each band by clicking and dragging 
up and down handles at the top of each frequency range.
For each band you can control the traditional compressor parameters: threshold, ratio, attack, and 
release. An auto option is also available. As was the case for the multiband compressor in DP, you can 
also solo or mute each band independently using the “B” and “S” buttons.
FIGURE 6.30
MasterWorks multiband compressor in DP.
FIGURE 6.31
Multiband Compressor in CU.

274
CHAPTER 6  The Final Mix
Listen to Examples 6.17 and 6.18 on the website to compare the same audio file with and without the 
applied multiband compressor.
6.8.4  The Limiter
The last step of a premastering session involves the limiter. By inserting this effect at the end of the 
chain we can accomplish two results. First, we can make sure there will be no distortion generated in the 
final bounce, and second, we will be able, if necessary, to increase the overall volume of the track with­
out passing the distortion level of 0 dB. As explained earlier, a limiter is basically a compressor with an 
extremely high ratio. What a limiter accomplishes is to set a ceiling (the threshold) over which the signal 
cannot go. Every time the signal might go over it, thereby creating distortion, it is “smashed” right to the 
threshold level. This can be very effective during tracking to avoid distortion for instruments that have a 
very wide dynamic range (e.g., brass).
An interesting application (especially in a premastering and mastering situation) involves the use 
of the limiter to increase the overall volume of the material processed and at the same time reduce its 
dynamic range. Some of the bundled limiter plug-ins that come with your sequencer can be effectively 
used to increase the volume of your tracks for premastering purposes.
Among them, the most efficient are the MasterWorks Limiter bundled with DP and the Limiter and 
Adaptive Limiter (Ad-Limiter) bundled with LP. The MasterWorks Limiter (Figure 6.32) is among my 
favorites for a premastering session since it easily adapts to any type of audio material and is very easy to 
set up. The parameters of the MasterWorks Limiter can appear a bit overwhelming at the beginning, but in 
fact they are fairly simple to set. Table 6.17 gives a brief explanation of the parameters and their functions.
To increase the overall volume of the audio material you are processing, try to set a short release, set 
the ceiling to 20.5 dB, set the lookahead to around 10 ms, and lower the threshold until you start hearing 
distortion artifacts. This will give you an idea of how far you can push the limiter. At that point, raise the 
threshold again until you are satisfied with the result.
FIGURE 6.32
MasterWorks Limiter in DP.

275
6.8  Premastering: Introduction
In LP there are two plug-ins that can be effectively used to increase the overall volume of a mix: the 
Limiter and the Adaptive Limiter (Figure 6.33). While they are both based on the same principle, the 
former is basically a compressor with infinite ratio, while the latter, in addition to limiting the audio sig­
nal, provides a warmer color tone that tends to resemble the one generated by analog clipping.
The parameters of the Limiter are very simple to set up. The gain determines the amount of signal 
increase you want the audio material to acquire. The lookahead sets the amount of time the plug-in fore­
sees before the material is limited. This allows for an increased level of accuracy when high peaks reach 
the limiter. The release works, as in a regular compressor, by setting the amount of time it takes to turn 
the effect off. The output level sets the ceiling of the limiter. Usually you should set it to 0 dB or slightly 
lower (0.5 dB). The softknee allows you to obtain a smooth transaction between unlimited and limited 
signal. If it is disabled, then the transition at the threshold point will be fairly abrupt. The rules stated 
Table 6.17  Parameters of the MasterWorks Limiter in Digital Performer.
Parameter
Function
Comments
Gain (Left and 
Right)
Controls the input gain for each 
channel independently
The button located between the Left and Right 
rotary knobs is the “Link” button. If selected 
(green), the gains of the two channels will change 
together, preserving their relative values
Threshold
Sets the point at which the limiter will 
start operating
In practical terms, if you lower the threshold the 
overall volume of the track will rise, since the 
average level of the audio signal raises and the 
limiter stops the highest peaks from distorting
Lookahead
Sets the amount of time in milliseconds 
that the computer uses to look ahead 
for incoming peaks that need to be 
limited
Release
Has the same function as the release 
parameter on a compressor. Sets how 
long the limiter will keep working after 
the signal goes below the threshold
Ceiling
Sets the maximum value (dB) (output) 
at which the limiter will limit the signal
Bits
Sets the output bit resolution of the 
material processed
If you downsize the word length of the material 
(e.g., from 24 to 16 bit) using the Dither option it is 
recommended
Dither and 
Noiseshaping
Depending on the bit resolution you 
selected, guarantees better downsizing 
of the word length of the audio material 
(Dither introduces a low-level white 
noise to reduce the quantization noise 
that occurs at low amplitude with a low 
bit resolution)
Noiseshaping can be introduced to make the noise 
artificially inserted by the Dithering process less 
noticeable
Indicators
The Peak value indicates the highest 
peak in dynamic detected by the 
limiter. The RMS indicates the average 
level of the signal

276
CHAPTER 6  The Final Mix
in the case of the MasterWorks Limiter also apply to the limiter in LP. Set the ceiling, lookahead, and 
release parameters and then start raising the gain until you are satisfied with both the volume and the 
overall sonority.
As I mentioned before, the Adaptive Limiter is a very good alternative to LP’s regular limiter. It has 
the ability to color the sound slightly with a more “analog” texture. Its parameters are very simple. The 
input scale works in a similar way to a “recording” volume on a digital board. Set it so that the audio 
material never goes over the 0 dB ceiling, then adjust the gain to increase the overall volume of the audio 
material according to your taste and to the type of audio material you are working on. The out ceiling 
sets the ceiling of the limiter. This particular effect can add personality and warmth to a digital record­
ing, and it can really improve the overall texture of your projects.
The compressor bundled with PT can be easily transformed into a limiter (Figure 6.34) by setting its 
ratio higher than 20:1 (with a value of 100:1 it becomes a “brick-wall” limiter). I find this limiter par­
ticularly helpful for light uses on tracks that do not require a hard limiting process.
The limiter bundled with CU provides fairly basic controls. You have control over input, output and 
release, with an option of setting the release to auto (Figure 6.35). If you want to increase the overall 
level of a track without distortion simply raise the output and the track will get louder by the amount 
specified.
Once you have applied the limiter and double-checked that all the previous effects inserted on the 
channel are perfectly balanced with one another, it is time to bounce the premastered file to disk to make 
the changes permanent. The process is exactly the same as described for a multitrack session. Make sure 
to select the entire part or section you want to bounce and that every parameter of every inserted premas­
tering effect is set as you wish.
In general, do not overuse any of these premastering tools. You want to be able to increase the overall 
quality of the track without limiting the final mastering engineer in doing his or her job. If you apply too 
much compression or too much limiting it is going to be impossible for the mastering engineer to take 
it out. So always remember that less is more in this case. I also recommend bringing to the mastering 
FIGURE 6.33
Limiter and Adaptive Limiter in LP.

277
6.9  Summary
session two versions of the final mix, one premastered and one without any postmix effects. This will 
allow full control over the final product in the mastering session.
Listen to Examples 6.19 and 6.20 on the website to compare an audio file before and after a full 
­premastering session.
6.9  SUMMARY
Sequencing techniques and MIDI orchestration principles constitute an important part of contemporary 
music production. As a modern composer/producer, though, it is crucial to strive for high-level produc­
tion during the entire production process. This is why it is extremely important also to be familiar with 
the two steps that follow the composition and the sequencing session: mixing and premastering. The 
mixing stage involves not only the correct balancing of the levels among tracks but also panning, equal­
izing, and effects handling. Before moving to the mixing stage it is important to decide how to incor­
porate MIDI and audio tracks together to reach a cohesive sonority. To mix your project you have two 
options. First, you can mix the MIDI tracks externally from the sequencer using the effects and equali­
zation available through the mixing board, along with the audio tracks recorded in your computer, and 
second, opt for transferring all the MIDI tracks as audio tracks inside your sequencer and mixing eve­
rything taking advantage of your computer’s effect plug-ins. The second option offers several benefits, 
such as a higher integration in terms of balance between tracks (since all the tracks will be processed 
with a similar effect processor and equalizer) and a higher flexibility in terms of automation and mixing 
possibilities. Depending on the bus setup of your mixing board, you can use the main output or the buses 
to record the MIDI tracks as audio. When recording the MIDI tracks as audio you may end up having a 
fairly large number of audio tracks to mix. If your sequencer limits you to a maximum number of audio 
tracks, you may need to create submixes to reduce the overall number of tracks. Refer to Table 6.1 for a 
list of suggested submixes. Once all the audio tracks are ready for the mixing session, I suggest ordering 
FIGURE 6.34
Limiter in PT.
FIGURE 6.35
Limiter in CU.

278
CHAPTER 6  The Final Mix
them according to a standard template that you would apply to every project to speed up the mixing 
process and be able to locate each track as quickly as possible.
The real mixing process begins with a rough mix that has the main goal of giving you an overall idea 
of how the sequence should sound. Start from the ground up with the orchestra. Begin with bass drums, 
then bass, the rest of the drums, followed by the piano and keys, guitars, strings, brass, woodwind, and 
then leads (which could include synthesizers, acoustic instruments, or vocals). Keep checking the mix 
after every addition, and go back to make corrections to the volume of the tracks already mixed every 
time you feel it is necessary. Panning allows you to accurately place instruments in the stereo image (or 
in surround if you are doing a surround mix). Try to achieve two main goals when working on the pan­
ning: balance between channels and respect for frequency placement. Balance is achieved by correctly 
placing instruments across the stereo image without favoring the left or right channel. At the end of the 
mix the two should be equally featured. Frequency placement also has an impact on the way you deal 
with panning. Usually, instruments that feature low frequencies are more naturally placed in the center 
of the stereo image, while instruments that feature high frequencies can be panned with more extreme 
settings.
While panning allows you to place instruments left and right, reverberation, along with volume, allows 
you to place them in a bidimensional setting. By adding more reverb (wetter signal) and by lowering 
the volume of the dry signal, you can position an instrument on the background level of a mix. If you 
lower the reverb level and slightly raise the dry volume, you can bring the same instrument closer to the 
listener and therefore inside the middle-ground area. A louder and drier signal will have the yield sound 
very much in the front of the mix (foreground). Typical parameters of a reverb include length, room size, 
early reflections, pre-delay, delay, high filter, and mix. As a general rule, a reverb time between 1.5 and 
2.5 ­seconds is a good starting point for a wide range of instruments. Instruments that cover mainly the 
low end of the frequency range, such as bass and bass drums, usually require much less reverberation 
(or none), while on instruments that cover the mid- and high-frequency range, such as guitars, hi-hat, 
and even snare drum, you can apply more reverb. All four sequencers come bundled with good sounding 
reverb plug-ins.
An equalizer allows you to boost or cut the volume of specified frequencies. In the mixing session 
an equalizer can be used either to correct problems that were created during the recording session or that 
arise due to incompatibility among instruments, or simply in a creative way to produce original effects. 
Use equalizers as inserts and not as aux sends. The several types of equalizer available these days fall 
into six main categories that have proven to be the most useful in a mixing situation: peak, notch, high 
shelf, low shelf, high pass, and low pass. One of the most important concepts to keep in mind when 
equalizing is being able to emphasize the characteristic frequencies of the track you are working on and 
eliminate frequencies that do not enhance its sonic features in any way. Try to listen to your mix, under­
stand the problems, and find solutions using the tools available. For generic equalization tips, consult 
Table 6.9. In general, remember that it is better to cut than to boost when using an equalizer.
Dynamic effects are designed to alter the dynamic range of an audio signal by controlling over time 
the ratios between high- and low-amplitude peaks of an audio track. While used mainly on acoustic 
tracks, they can also be effective, with parsimony, on MIDI tracks. The dynamic effects include com­
pressor, limiter, expander, and gate. A compressor allows you to reduce the dynamic range of an audio 
signal. A limiter allows you to set a ceiling that the signal cannot pass, to avoid distortion. An expander 
is the exact opposite of a compressor: it allows you to increase the dynamic range of an audio signal. 
A gate is an extreme application of an expander, where the signal is muted after it goes below the set 
threshold. In general, I recommend using a compressor sparsely and only if really needed. In fact, syn­
thesized tracks in general can benefit from a higher dynamic range and not a reduced one. A limiter can 
be effectively used to increase the overall volume of a signal without getting distortion. A noise gate can 

279
6.10  Exercises
be effectively used to reduce the noise between quiet passages that if note-muted would contribute to the 
overall noise level of the mix.
Once the mix is ready, it is time to bounce it to disk, meaning to mix down digitally the multitrack 
session to a stereo (or surround) file. The mix-down that will be created reflects exactly the mix you are 
hearing from your sequencer, so make sure to check carefully that the mix is exactly how you want it 
before creating the bounce. Each sequencer handles the bounce-to-disk operation in very similar ways. 
Common parameters that can be set during the bounce procedure include the file format of the bounce, 
the output channels used, resolution, sample rate, and the option to automatically reimport the bounce 
in the current project. The audio file formats in widest use for bounced mixes include AIFF, WAV, SD2, 
and BWF. Other formats that, though less popular, are part of the options offered by most sequencers are 
Wave 64, MP3, QuickTime, Ogg Vorbis, Windows Media Audio format, and Windows Media Audio Pro 
format.
The premastering session is designed to increase the overall quality of the bounced file before sending 
it to the final mastering session. In general, a premastering session is conducted by the composer/producer 
right after the mix stage to prepare the final mixes for professional mastering or for small-scale distribution. 
A mastering session is usually done by a professional mastering engineer just before going to the mass pro­
duction stage, and it involves a series of steps, tools, and techniques that are specifically targeted to a cer­
tain audio material. I always advise producers and composers to premaster their projects, mainly because 
the sound and quality of their projects will improve greatly even with a few small premastering touches.
A premastering session can be conducted inside the main sequence. (In this case it is preferable to 
create a separate arrangement/sequence in order to manage the session in an easier way.) Another option 
is to create a separate project dedicated to the premastering session. Once you have your files imported 
to your premastering project, make sure the beginning and end of the stereo mix contain no digital clicks 
or pops. To fix this problem you have to fade in and out the beginning and end of the file. Normalization 
is sometimes necessary to increase the overall volume of the file without changing its dynamics. A 
multiband equalizer is then inserted to help bring the mix to life and to fix smaller mistakes overlooked 
at the mixing stage. Use the equalizer sparsely without boosting or cutting more than 2 or 3 dB per band. 
The main idea here is to apply several small corrections to fine-tune the overall sonority of the mix.
After equalization, insert a multiband compressor. By applying a band-specific compression to the 
stereo material you can effectively and precisely rebalance the dynamic relation between the main sec­
tions of your mix. Do not overcompress your material, especially since at the final mastering stage the 
engineer will probably apply more compression. At the end of the premastering process, use a limiter to 
avoid distortion and to increase the overall volume of the stereo mix. Try not to overlimit the material 
before sending it to the final mastering facility. Always bring to the mastering session two versions of 
the final mix, one premastered and one without any postmix effects. This will allow the mastering engi­
neer maximum flexibility. Once the premastering effects are set, bounce your mix to disk.
6.10  EXERCISES
Exercise 6.1
Set up and record a sequence with the following features:
a.	 Twelve MIDI tracks
b.	 Two audio tracks with loops of live instruments
c.	 Free instrumentation and tempo.

280
CHAPTER 6  The Final Mix
Exercise 6.2
Using the sequence created in Exercise 6.1 record each MIDI track as a separate audio track. Remember that 
you can use either your mixing board for external hardware or satellite computer synthesizers, or your DAW’s 
internal bus system for software synthesizers.
Exercise 6.3
Mix the previous project using at least two reverbs as aux sends, equalization on each channel, and four 
dynamic effects. When done, bounce the final mix to disk using at least two different file formats.
Exercise 6.4
Start a new project and reimport the bounce created in the previous exercise. Following the steps and tech­
niques indicated in this chapter, create a premastered file.

281
Creative Sequencing Techniques for Music Production.
© 
, 2005 Andrea Pejrolo. Published by Elsevier Ltd. All rights reserved.
CHAPTER
2011
Working with Video
7
7.1  WORKING WITH VIDEO: IMPORTING MOVIES IN DIGITAL FORMAT
One of the many advantages of composing and orchestrating with an advanced audio/MIDI sequencer 
is that you can easily work with video material without resorting to complicated synchronization tech­
niques or mysterious synchronization protocols such as SMPTE (a technique I will describe later in this 
chapter). In order to understand how to work and synchronize video to your digital audio workstation 
(DAW) it is important to be familiar with the concept of linear and nonlinear devices. A linear device 
stores the information (digital or analog) in a linear way. A typical example of such a device is a tape 
recorder, where the information is stored sequentially on tape. In order to reach a specific location you 
will have to physically fast-forward and advance to that location. A nonlinear device stores the informa­
tion in a random-access way. An example would be a compact disc (CD) or a hard disk (HD). With such 
a device you can instantly reach any data on the media by simply jumping to the desired location. Linear 
devices are less flexible since they always require additional time to locate a specific point on the media, 
while nonlinear devices can easily adapt to any situation and they are much easier to synchronize. While 
video devices used to be tape based and therefore linear, nowadays nonlinear video editing suites (com­
puter based) are the standard in the professional (and even in the consumer) environment. This transition 
brought great flexibility not only to video editors, but also to composers and orchestrators who regularly 
work on projects involving scoring to video. Nevertheless, it is important to understand and learn how 
to synchronize linear video devices to gain a full knowledge of the techniques involved in the scoring to 
picture process. So, let’s start!
7.2  SMPTE: SYNCHRONIZATION OF LINEAR TO NONLINEAR MACHINES
In Chapter 3 we learned how to synchronize nonlinear machines to nonlinear machines. This type of 
synchronization is fairly simple to set up since it relies on MIDI messages, such as MIDI Time Code 
(MTC) and MIDI Clock, to establish and maintain the synchronization among two or more machines. 
This is possible since the majority of nonlinear machines are equipped with a MIDI interface. Therefore, 
using the MIDI network we can exchange synchronization information between the devices in our stu­
dio. When dealing with linear machines, however, things get a little more complicated. The absence of a 
MIDI interface and the fact that we cannot record or play back MIDI information directly onto and from 
a tape make the synchronization of such devices less straightforward.
There are several practical applications that derive from the synchronization of linear machines to 
nonlinear devices. For example, if you really like the sound of analog tape and do not want to record 
your audio material directly to HD, you can synchronize a reel-to-reel analog tape machine with your 
digital sequencer. This setup allows you to run MIDI tracks (from the sequencer) along with analog 
audio tracks (from the reel-to-reel machine). Another application involves the synchronization of video 

282
CHAPTER 7  Working with Video
machines such as video cassette recorders (VCRs) with an audio/MIDI sequencer. When we score to 
picture, in ideal conditions we will be provided with a digitized version of the video. If this is not the 
case, however, and the only video format we have is a videotape, we will have to use old-fashioned 
synchronization tools, such as SMPTE (Society of Motion Picture and Television Engineers) code, to 
synchronize the VCR to the audio/MIDI sequencer. Let’s take a look at this technique and at the steps 
involved in this procedure.
In the case of nonlinear machines, we learned that MTC allows us to synchronize several devices by 
describing the passing of time in the format hh:mm:ss:ff. Unfortunately, it is not possible to record the MIDI 
data directly onto tape; therefore, an adapted version of this format needs to be found to be able to record it to 
tape and subsequently synchronize the machines. SMPTE code does exactly this: it is an “analog” translation 
of the MTC, which can easily be recorded to tape. This code was created in 1967 to make the editing of vide­
otape easier, so to be precise MTC is in fact a digital translation of SMPTE in MIDI messages. The bottom 
line, though, is that SMPTE is able to encode the digital format to tape by modulating (using a biphase-mark­
ing system) the 1 s and 0 s of the digital signal into a square wave (Figure 7.1). The frequencies of the wave 
depend on the frame rate, and they range from approximately 2400 to 4800 Hz.
The format and frame rates of SMPTE are the same as those we analyzed when we discussed the 
MTC protocol (Table 3.2, page 138). To synchronize devices using SMPTE, we first have to record the 
SMPTE code onto an empty audio track of the linear machine. In the case of a multitrack tape recorder, 
choose one of the tracks placed at the edge of the tape, to reduce the bleeding of the SMPTE code onto 
adjacent tracks. SMPTE code has a very piercing sound and therefore can easily spill onto other tracks 
on tape (listen to Example 4.1 on the website to hear what SMPTE code sounds like). The operation of 
recording an SMPTE signal onto an empty audio track is referred to as striping. To stripe SMPTE code 
into an empty audio track you need an SMPTE generator, which is usually part of the most sophisticated 
multicable MIDI interfaces. On this type of interface, in addition to the regular MIDI IN and OUT ports, 
there are two extra audio connectors (normally labeled SMPTE IN and SMPTE OUT) used, respectively, 
to read and write SMPTE code. Before starting to record any audio track related to your project, you 
should stripe SMPTE for the entire length of the tape.
For example, let’s say you have an analog eight-track reel-to-reel recorder you want to synchronize 
to your sequencer and MIDI studio (Figure 7.2).
First, you record the code onto track 8 of the analog machine for the entire length of the tape. 
Depending on which SMPTE generator/MIDI interface you have, you usually can decide the start­
ing time of the code either through software or through parameters accessible from the front panel 
of the interface. It is a good idea to stripe starting not from 00:00:00:00 but from 00:59:00:00. This 
assumes you leave 1 minute open before the starting of the project, which will occur at approximately 
01:00:00:00. This is recommended for two main reasons: first, it is better to have a little bit of lead-in 
time (1 minute is enough), since it will take a few seconds for the machines to synchronize together; and 
FIGURE 7.1
Linear graphic representation of SMPTE code.

283
7.2  SMPTE: Synchronization of Linear to Nonlinear Machines
second, synchronizers (especially older ones) sometimes have a hard time recognizing a starting time of 
00:00:00:00. After the SMPTE code has been striped, you have basically time-stamped the tape so that 
you can now assign specific location times for every position of the tape.
One of the disadvantages of operating with linear machines is their relatively limited flexibility. 
In other words, whereas nonlinear devices can locate and move to any position in time inside a spe­
cific project/sequence without literally fast-forwarding (or rewinding) to the specified location, linear 
machines usually cannot. This means that when synchronizing linear to nonlinear machines, the former 
should be set as master while the latter (more flexible) need to be set as slave. Make sure that on your 
sequencer (as in the case of any master/slave connection) the frame rate matches that with which the 
tape was striped. Once the tape is striped with the SMPTE code, every time you press the “Play” but­
ton on the multitrack tape recorder, the head will read the SMPTE code recorded on one of its tracks 
(in this example track 8). The code will be sent to the MIDI interface through the output of track 8 to 
the SMPTE input of the MIDI interface and here translated in MTC. The sequencer will start following 
the incoming MTC. As soon as you press the “Stop” button on the multitrack tape recorder, the flow of 
timing data will stop, and therefore the sequencer will stop until the SMPTE/MTC data start flowing in 
again. In this way the computer (and any other MIDI device connected) will follow the SMPTE code 
that was striped originally. Remember to set the MIDI devices, sequencer included, as a slave (for a 
review of this operation, refer to Chapter 3, Table 3.3).
A similar technique is used when you want to synchronize a VCR to your sequencer for scoring to 
picture. In this case the tape provided by the video production company usually already has SMPTE 
FIGURE 7.2
Studio setup with master/slave synchronization through SMPTE, MTC, and MC.
(Courtesy of Apple Computer).

284
CHAPTER 7  Working with Video
code that was striped by the video editor. For this type of application, a stereo VCR is indispensable, 
since on one channel you will have the voiceover or dialog of the video and on the other you will have 
the SMPTE code (Figure 7.3).
In the example in Figure 7.3, the left channel of the VCR outputs the prerecorded SMPTE code to 
the MIDI interface, which translates it into MTC, synchronizing the sequencer and all the other MIDI 
devices that need synchronization. The right channel of the VCR is sent to the mixing board with the 
voiceover signal. The video output of the VCR is sent to a video monitor with the video signal. Keep in 
mind that the video signal usually has a copy of the SMPTE code burnt into the picture so that the com­
poser can have a visual reference of the code. In this way you can easily spot a movie and set the most 
important markers by looking at the picture and at the burnt-in SMPTE code.
To set up the sequencer to act as a slave, the directions are the same as in Chapter 3, when learn­
ing how to synchronize nonlinear to nonlinear devices. Whenever you press the “Play” button on the 
VCR, the SMPTE code is sent to the MIDI interface, translated in MTC, and sent to the sequencer and 
to all the MIDI devices that need synchronization. When you press the “Stop” button, the SMPTE and 
MTC will be interrupted, and consequentially all the synchronized MIDI devices will stop waiting for 
new incoming code. While for several years SMPTE has played a very important role in synchronizing 
linear devices to nonlinear devices, with the evolution of computers and nonlinear devices in both the 
audio and video fields, more and more of the synchronization aspect of a studio will be left to nonlinear 
machines, mainly because of their flexibility and efficiency in dealing with such issues.
FIGURE 7.3
Studio setup with VCR synchronization through SMPTE.
(Courtesy of Apple Computer).

285
7.3  Synchronizing and Working with Nonlinear Video Media
7.3  SYNCHRONIZING AND WORKING WITH NONLINEAR VIDEO MEDIA
These days it is possible to import a scene, episode, or entire digitized movie to your sequencer and 
score directly to the picture without the need for external video devices, synchronizers, or complex syn­
chronization protocols. Once you have a digitized movie (meaning a digital version of the original video 
source) and therefore the movie itself is in a nonlinear format, you can easily import it to your sequencer 
and anchor the video tracks to the conductor track of your sequencer. The sequence and the video mate­
rial will always stay in sync, since they are both played by the computer (which acts as virtual master for 
both sources) and streamed from the computer’s HD. Another big advantage of using a digitized video 
source to score to picture is that you will save a huge amount of time. Since the digitized video will 
behave as a nonlinear device, you will not experience the delays related to fast-forwarding and rewind­
ing of the tape that usually occur with linear video devices. One of the key issues is being able to obtain 
a digitized version of the video (movie, documentary, commercial, etc.) you have to score. These days 
you can obtain a digital copy of the video directly from the video production company involved with 
the project. If this is not the case you will end up with your video on a VHS tape without SMPTE sync 
(more on this later), and you will have to digitize it yourself. But let’s take this one step at the time and 
see what options you should consider if the video production company agrees to provide a digital copy 
of the video material.
Depending on the length of the project, you have a few options in terms of formats and compression 
settings of the video source. The best video format in which to have the source delivered to you is Apple’s 
QuickTime (QT) movie. This format has really matured during the past two or three years; its video digital 
format features very good quality even at high compression rates. QT is preinstalled in every Macintosh 
computer, and, even better, all major sequencers support it, making it the de facto standard for composers 
and producers as the nonlinear temp format for scoring to picture. If the video production company agrees 
to deliver the video in QT format, then most of your problems are solved. The only thing you have to do is 
copy the video files onto your session HD (I recommend keeping them in the same directory as the project 
into which they will be imported) and select the import movie option in your sequencer (more details on 
this in a little while).
Keep in mind that the size and resolution of the QT movie have an impact on the performance of the 
central processing unit (CPU) of your computer. A full-screen movie with a low compression ratio could 
slow down your computer (especially if the CPU is not one of the latest generation), and consequently the 
performance of your sequencer will suffer too. I therefore recommend reducing the size and increasing the 
compression of the movie you import to avoid having to sacrifice tracks or plug-ins. Usually, I like to have 
a small version of the movie with a resolution such as 320  240 (or 640  480) and H.264 compression. 
This allows a quick reference for the scoring without taxing the computer’s CPU too much.
Sometimes the video production company can give you the movie in digital video (DV) format. This 
option has advantages and disadvantages. The good news is that if you use the DV format to import the 
video to your sequencer you will be able to output the video to a separate video monitor through the 
FireWire port of your computer via a regular FireWire-to-RGB converter. The bad news is that DV uses a 
larger bandwidth than compressed QT; in addition, you will be using bandwidth on the FireWire port that 
should be left for the session HD and eventually for the audio interface. If you have a computer with a dual 
FireWire bus, make sure that the DV output and the HD/audio interface output are connected to two sepa­
rate buses. If the video production company is not going to provide a digital version of the video, you will 
have to digitize it yourself. This operation used to require expensive video cards and very powerful comput­
ers. Nowadays, fortunately, you can easily convert an analog video source into digital stream using a simple 
DV converter that takes the analog input from your VCR and converts it into digital data by outputting the 
video in DV format to your computer through a FireWire port. You can use basic video software to run 

286
CHAPTER 7  Working with Video
the conversion, such as MPEG Streamclip or FinalCut Express. Don’t forget, though, to always store the 
converted movie in the working HD/directory of the project you will be importing the movie into. Once the 
video material is ready, you can import it to your sequencer of choice.
Before going into the specifics of how each DAW imports and handles synchronization of QT mov­
ies, it is important to discuss briefly the necessary steps you will have to take to work efficiently and 
productively with movies. The sequence of events that needs to be followed is always the same and the 
main steps are listed in Table 7.1. I cannot stress enough how important it is to take care of all these 
steps in the right order to have a successful and relatively problem-free scoring to video experience.
Now let’s take a look at how each sequencer [Digital Performer (DP), Logic Pro (LP), Cubase (CU), 
and Pro Tools] handles QT movies and their synchronization.
7.3.1  Video Syncing in DP, LP, CU, and PT
All four sequencers are able to import QT movies and automatically synchronize them to the sequence, 
and they all offer a series of parameters for customizing the start point and the synchronization features 
available.
To import a QT movie in DP simply go to the Project menu, select the “Movie” option, and browse 
to the QT movie you want to import. Once the movie is loaded it is automatically locked to the current 
sequence. From the mini-menu of the movie window you can control parameters such as the size of the 
window (half, normal, and double size), the starting point of the movie (in relation to SMPTE time), 
Table 7.1  Synchronization Steps Required for a Smooth Synchronization of Nonlinear Video Media.
Step
Description
Comments
1
Import QT movie
Import the QT nonlinear movie into your DAW to be able 
to synchronize it to your sequence
2
Set correct frame rate
Set the correct frame rate so that the rate of the 
sequence matches the rate at which the video was 
recorded
3
Synchronize beginning of the 
movie with bar 1 of the sequence
Line up the beginning of the video (first frame of the 
picture and SMPTE code) with the beginning of the 
sequence (bar 1)
4
Place markers
Spot the video and insert markers for every relevant 
video or voiceover event that will be eventually 
underscored by the music
5
Set tempo (or tempos for the 
sequence)
Decide on the tempo of your music. This is a crucial step 
since it affects how the markers (and therefore the video 
cues/events) will line up with the music. This is the only 
variable that you have to plan accurately in advance, 
since a tempo change at a later stage will affect every 
other element of the music
6
Add two bars of count-off
Add a count-off in order to have at least two bars for 
cueing in live instruments or to add a two-beep sound 
(see Section 7.3.2)
7
Transfer movie’s audio track to 
sequence
It is always advisable to transfer the audio track of the 
movie from the QT file directly to the sequence. This will 
allow you a better mix of voiceover and music

287
7.3  Synchronizing and Working with Nonlinear Video Media
whether you also want the movie locked to the sequence in edit mode (both graphical and numerical), 
and the video output option. Keep in mind that this last option can be changed only if you imported a 
movie in DV format, in which case you will be able to output the video to an external video monitor 
through the FireWire port and a FireWire-to-RGB converter. To improve the performance of your com­
puter I recommend keeping the size of the movie to normal or half. Also, try to maintain the original 
aspect ratio of the movie instead of altering it. Once the movie is imported to the sequence, it is very 
easy to spot it, set markers and tempo changes, and start scoring to picture. If the “Lock to Transport” 
option from the mini-menu of the video window is selected, the sequence and the movie are locked in, 
meaning that whenever you press Play (or Fast-forward, Rewind, or Scroll) on the sequencer or on the 
movie, the other will follow. If you want the two items to be independent, simply deselect the “Lock to 
Transport” option. A thumbnail view is available through the “Sequence Editor” window. After import­
ing the movie, select the correct frame rate from the Setup  Frame Rate menu (for a review of the 
different frame rates see Chapter 3). At this point a key step needs to be accomplished. In order to suc­
cessfully score your video you need to line up bar 1 of your sequence with the beginning of the picture 
(and not with the beginning of the video file!). If this sounds a bit confusing (and it can be if you are 
doing it for the first time), look at Figure 7.4 for a graphic explanation of this terminology.
To make sure that bar 1 is synchronized with the beginning of the picture, first you have to let DP 
know where the movie starts (including the black frames if present). To do so, first go to the mini-menu 
of the movie window and select “Set Movie Start Time”. In the “Frames” field input the SMPTE time of 
the beginning of the movie including the black frames (Figure 7.5).
Once DP knows exactly where the movie starts with reference to its SMPTE time you have to specify 
where the first frame of the picture is and then link it to bar 1 of the sequence. To do so, navigate to 
Project  Chunks, select the sequence with the imported movie you are working on and in the mini-
menu of the Chunks window select “Set Chunk Start”. Here, in the Frames field insert the SMPTE time 
of the first frame of the picture and click the “OK” button (Figure 7.6).
Now you will notice that your bar 1 is linked to the beginning of the picture as planned. It is usu­
ally a good idea to insert a two-bar count-off at the beginning of the sequence to leave space for any 
pickup measure, click count-off, and a two-beep count (more on this in Section 7.3.2). To do so in DP, 
go to Project    Modify Conductor Track    Insert Measures and insert two measures before bar 1. 
Make sure that the “Maintain all times following the insertion point” option is checked. This is crucial 
FIGURE 7.4
Example of a movie with black frames before the beginning of the picture.

288
CHAPTER 7  Working with Video
otherwise your movie will be out of sync in relation to the sequence. It is essential to understand that any 
tempo change occurring before bar 1 will affect the syncing of the first frame of the picture and bar 1. 
Therefore, before inserting the two bars I strongly recommend spotting the movie and picking the 
desired tempo for your sequence. If you decide later to change the tempo you will have to repeat the 
previous steps to resync bar 1 to the movie. The final step is to transfer the audio track of the QT 
movie to a new stereo track of the sequence to have more flexibility in terms of mixing and balance 
between voiceover and music. To do so, click on the mini-menu of the video window in DP and select 
FIGURE 7.5
Set Movie Start Time in DP.
FIGURE 7.6
Set Chunk Start in DP.

289
7.3  Synchronizing and Working with Nonlinear Video Media
“Copy Movie Audio to Sequence”. A new stereo audio track will be created with a copy of the audio 
track of the movie. Remember to lock the track to time-code so that it will always be in sync with the 
movie.
In LP you can import a movie by choosing File  Open Movie. The movie will appear containing 
the selected video in a floating window that will always stay on top of the other windows. By right-click­
ing on the video you can choose the size of the movie window (to freely change its size you can also 
click on its lower right corner and drag the window). At this point you have to set the movie start time in 
LP. To do so, select File  Project Settings  Video (Figure 7.7a) and in the field named “Movie Start” 
insert the SMPTE code value corresponding to the beginning of the actual QT movie file.
The setting regarding Frame Rate and SMPTE start time can be changed from the Synchronization 
window located in File  Project Settings  Synchronization. Make sure the frame rate of the project 
matches that of the movie by using the Synchronization window, accessible from the Transport window 
(click and hold on the “Synchronization” button). In the same Synchronization window you have to line 
up bar 1 of your sequence with the first frame of the picture by inserting bar 1 in the “Bar position” field 
and the time-code position of the first frame of the picture in the “Plays at SMPTE” field (Figure 7.7b). 
To add a two-bar count-off, simply drag the handle in the timeline where bar 1 is to the left to bar 1. 
LP is very good at keeping bar 1 synchronized to the beginning of the picture and therefore even if you 
change the tempo before bar 1 the synchronization link will be always kept. To transfer the audio track 
of the movie to a stereo audio track, in the Arrange window select File  Import Audio from Movie. A 
new stereo audio track will be created with the movie’s audio material inserted at the correct location. 
The new object will be automatically locked to time-code.
As we saw in DP, you can choose to output the movie to an external monitor through the FireWire 
port from the video preference window (File    Project Settings    Synchronization). The thumbnail 
FIGURE 7.7
(a) Video preference window and (b) synchronization preference window in LP.

290
CHAPTER 7  Working with Video
function is very useful when you score to picture since it allows you to see a rendition of the video on 
a special video track with poster-frames located below the Global Tracks in the Arrange window. This 
allows you always to have a constant visual representation of the relation between the sequence and the 
movie.
The same parameters and options are available in CU, where, to import a video clip, you choose the 
option “Video File” found in the “Import” submenu in the File menu. Make sure to check the “Extract 
Audio From Video” option to automatically transfer the audio track of the QT movie to a new stereo 
track in the main track list window. Once the video has been imported, you can bring a floating window 
to the front by selecting “Video Player” under the Devices menu or by simply double-clicking on the 
video track in the Arrange window. To set more advanced parameters, such as video size, video ratio 
and quality, right-click on the floating video window. To access more advanced parameters, select the 
“Device Setup” option first (found in the Devices menu) and then select “Video Player”.
The synchronization of the movie in CU follows a similar process to the one we learned for DP and 
LP. First, open the Project Setup window (Project  Project Setup). In this window (Figure 7.8) set 
the Start field to the first frame of the movie (including the black frames), then select the correct frame 
rate. In order to have a two-bar count-off, set the Bar Offset field to 2. After clicking “OK” you will be 
asked whether you want to keep the project content at its time-code position. Click “No”, since you want 
to have your video left at its relative position to the project for now. The next step is to line up bar 1 
with the beginning of the picture. To do so, move your song position exactly to bar 1 (you can do so by 
double-clicking on the counter in the transport bar, type “1” and hit the return key). Now go to 
Project  Set timecode at cursor, and in the start field type in the time code of the beginning of the 
picture (first frame of the picture). Click “OK” and when asked the question “Do you want to keep the 
project content at its timecode positions?” click “Yes”. Now your first frame of the picture is lined up 
with bar 1 and you have a two bar count-off. As was the case for DP, any tempo change that occurs 
before bar 1 will affect the synchronization of the movie with the sequence. Therefore, I strongly recom­
mend spotting the movie and picking the tempo for your sequence before synchronizing the movie.
I really like the way that CU automatically creates a thumbnail track and an audio track with the 
audio material of the video. This speeds up the workflow and is less distracting during the compositional 
FIGURE 7.8
Video synchronization in CU.

291
7.3  Synchronizing and Working with Nonlinear Video Media
process. As with the other sequencers, make sure the frame rate of the current sequence matches that of 
the imported movie. To do so, select the “Project Setup” option from the Project menu and change the 
frame rate parameter according to the movie you imported.
Whereas in PT 8 you could not use time code unless you had the Tool Kit expansion, since PT 9 you 
can easily work with video even without using PT HD. To import the movie you will be scoring, select 
File  Import  Video. Remember to check the option “Import Audio from File” to have the QT audio 
track directly imported and copied into a new stereo track in the edit window.
After selecting the QT movie you plan to work on, a thumbnail track will be inserted automatically 
in the edit window. Make sure the frame rate of the project matches that of the movie you imported 
by navigating to Setup  Session and selecting the correct frame rate in the “Time Code Rate” menu 
(Figure 7.9). To line up bar 1 with the beginning of the picture in PT you have to first specify in PT the 
exact beginning of the QT movie you are working on. To do so, insert the movie start time code in the 
field named “Session Start” in the Session Setup window (Figure 7.9). This will tell PT the exact time 
code start for the current session.
Next, you have to line up bar 1 with the beginning of the picture. This can easily be achieved by 
selecting Events    Time Operations    Time Operations Window (Figure 7.9) and, under the Move 
Song Start tab, setting the “Move start to” field to the time-code location of the first picture frame of the 
movie. From now on your bar 1 will correspond with the beginning of the picture.
7.3.2  Adding a “Two-Beep”
The importance of having two bars before bar 1 (and therefore before the beginning of the first picture 
frame) lies not only in being able to have a count-off for live musicians but also (and primarily, I would 
say) in having the necessary space to add a “two-beep”. A two-beep is a standard sound (similar to a 
beep) that is used in postproduction to mark exactly the location of two seconds before the first frame of 
the picture. I am sure that you are familiar with the standard countdown of 5, 4, 3, 2 seen in rough cuts 
FIGURE 7.9
Movie synchronization in PT.

292
CHAPTER 7  Working with Video
of movies and television shows. This countdown is used in postproduction to line up video and audio 
stems precisely. While the seconds before “2” are not marked with any audio cue, the marker placed 
at two seconds before the first picture frame is highlighted with a beep (the so-called two-beep). As a 
composer you will have, in most cases, to deliver your cues and stems with a two-beep inserted exactly 
at two seconds before the first picture frame. Keep in mind that you do not have to use the standard beep 
sound used in postproduction; any sound with a strong and fast transient (such as a click) will do the job.
If you followed the previous steps you should have your movie synchronized and with at least a 
two-bar count off. In this situation it is very easy to place your two-beep exactly at two seconds before 
bar 1. The basic idea that works for all four DAWs is first to import the sound onto a new audio track 
and then to move and place the region containing the two-beep to the spot exactly two seconds before 
bar 1. For example, in a movie that has its first picture frame at 01:02:00:00, the two-beep will be 
inserted at 01:01:58:00 (exactly two seconds before the first picture frame).
To do so in DP open the Sequence editor, select the audio region containing the two-beep sound and 
open the Event Information window (Studio  Event Information). Here, insert in the “Time” field the 
exact location you want to move the soundbite to (in this case 01:01:58:00).
In LP select the region containing the two-beep sound from the Arrange window and open the Event 
List floating window by pressing Option-E. Make sure that the position is shown in time code (look at 
the right end of the Event List window: if you see a small clock icon you are all set; if not, click on the 
small note and switch to time code). In the start field (the one located on the left of the window) insert 
the location you want to move the region to (in this case 01:01:58:00).
In CU, from the main Arrange window click on the audio region containing the two-beep. All the infor­
mation regarding the region will be displayed automatically in the Info Line at the top of the Arrange win­
dow. In the Start field insert the desired location to move the region to (in this case 01:01:58:00). If the Info 
Line shows Bars and Beats instead of Time Code, simply switch the unit by right-clicking on it and select­
ing Time Code.
In PT, to move a region to a specific location make sure that you are in Spot Edit mode. Next, using 
the grabber tool, click on the region and in the Spot Dialog window select the Start field. Here, insert the 
desired location to move the region to (in this case 01:01:58:00).
7.4  USING MARKERS
Markers are an essential component of scoring to picture in any DAW. They are also extremely useful in 
any sequencing situation. The main concept of markers is very simple. They are like bookmarks that can 
be easily inserted (or “dropped”) at any given position in your sequence and they can be used to jump 
quickly to a specific location (e.g., Verse 1 or Chorus 2) or used to easily select regions of a track inside 
two specific points in the sequence.
It is very important to understand how markers can be locked in relation to the sequence (bars and 
beats) or timeline (time code or real time). While the concept of both types of locking system is similar, 
the two serve different functions. Let’s learn how they differ.
7.4.1  Beat-Based Markers
Beat-based markers (BBMs) are created and locked to a specific bar, beat, and tick. These are the most 
common markers when sequencing music-based material only. For example, if you are writing and 
sequencing a song (not to picture) you will be using this type of marker. The main concept to understand 
here is that since BBMs are locked to a specific bar, beat, and tick, when you change the tempo of your 

293
7.4  Using Markers
sequence their relative position to bars, beats, and ticks will not change. If marker number 2 (Chorus 1) 
is set to location 4|1|000 with a tempo of 100 BPM, changing the tempo to 120 BPM will not move the 
marker since Chorus 1 will still fall at position 4|1|000 (Figure 7.10a, b).
BBMs, therefore, are well suited for marking specific locations that are song based. In some DAWs 
these markers are referred to as “unlocked markers” since they are not locked to time code but instead 
they are linked to bars, beats, and ticks.
7.4.2  Time-Code-Based Markers
When you score to picture, BBMs are not particularly useful for any location that is specifically linked 
to the picture. This is why another category of markers is used when scoring to video. These markers 
are basically the same as those discussed in the previous paragraphs, meaning that they look the same 
and are inserted in the same way, but they are locked to time-code instead. Time-code-based markers 
(TBMs) are used to mark important moments of the video that you will use later as scoring cues. Typical 
examples of TBMs are a specific cut to a different scene, or a visual event (e.g., an explosion or a kiss), 
a fade in/out, or a specific dialog cue. These markers are usually also referred to as “Locked” since they 
are locked to time-code and not to bars, beats, and ticks. If you lock your markers they will always fol­
low and stay with the video independently of the tempo of the sequence. This is a very important aspect 
to understand because if you change the tempo of your sequence, the BBMs (unlocked markers) will not 
change in reference to the sequence, while the TBMs will change (Figure 7.11).
FIGURE 7.10
Beat-based markers (a) before and (b) after applying a tempo change.
FIGURE 7.11
Time-code-based markers (a) before and (b) after applying a tempo change.

294
CHAPTER 7  Working with Video
This will impact the relation between your musical cues and the video cues, often resulting in rescor­
ing of the scene. Now that we have a clearer idea about what markers do, let’s take a look at how the 
four DAWs handle them.
In DP you can quickly insert BBMs from the Track List window by clicking and dragging to the 
desired location the Marker icon located in the top right corner of the Track List window (Figure 7.12). 
If you use this method, the markers that you insert will be automatically snapped to the view grid that is 
currently in use.
To insert markers in a location that is not snapped to the current grid you can set the locator to the 
desired position, then open the Markers window (Project  Markers) and from the mini-menu select 
“New Marker” (Figure 7.12). To lock a marker, first select it in the Markers window and then choose 
“Lock Marker” from the mini-menu.
In LP you can insert a marker directly from the Arrange window using the pencil tool. Make sure 
that the Marker Global Track is shown at the top of the Arrange window, select the pencil tool, and sim­
ply click on the Marker area to insert BBMs. To rename the markers double-click on their name with 
the regular arrow tool. To change their positions click and drag the left divider of a marker. To insert 
a marker in an exact position open the Markers window (Options  Marker  Open Marker List) and 
with the pencil tool click anywhere in the “Position” field on the left (Figure 7.13).
From this window you can also lock markers and therefore transform them into a TBM. To do so, 
select the marker (or markers) you want to lock and from the Options menu of the Marker List Window 
select “Lock SMPTE Position”.
As was the case for DP and LP, in PT you can also insert markers directly from the main Edit win­
dow by dropping the locator to the desired position and then clicking on the small plus sign positioned in 
the top left corner of the Edit window, next to the Marker track (Figure 7.14).
After adding the marker you are prompted with the “New Memory Location” window, where 
you can name the marker, number it, and decide whether it is a BBM (reference set to “Bar|Beat”) or 
a TBM (reference set to “Absolute”). Markers are managed from the Memory Locations window 
(Window  Memory Locations). Here, you can see a list of all markers and their features, including 
whether they are BBM or TBM (Figure 7.15).
FIGURE 7.12
Marker window in DP.

295
7.4  Using Markers
CU adopts a very similar system to the other DAWs. If a Marker Track is not present, create one by 
right-clicking on any track in the left side of the main Arrange window. Keep in mind that you can create 
only one marker track per project. To insert a new marker select the marker track and then using the pen­
cil tool click where you want to add the new marker. Alternatively, you can drop the locator and click on 
the small marker icon with the plus located in the header of the master track (Figure 7.16).
In CU the entire marker track can be set to either BBM or TBM. To do so, simply select either the 
small note icon (BBM) or the small clock icon (TBM) in the track header of the marker track (Figure 
7.16). To manage the markers you can open the Marker window from Project  Markers, where you can 
insert, remove, and move markers.
FIGURE 7.13
Marker window in LP.
FIGURE 7.14
Markers in the Edit window of PT.
FIGURE 7.15
Memory Locations window in PT.

296
CHAPTER 7  Working with Video
7.5  BOUNCING TO MOVIE
When you are working to picture one of the key elements is to be able, having finished writing and 
producing your music, to export the current cue as a movie directly from the DAW. This process is 
known as “bouncing to movie”. The idea here is basically the same as discussed in Chapter 6 when you 
learned how to bounce to disk. The main difference lies in the fact that when we bounce to movie we are 
including not only the audio mix-down of the audio tracks but also the QT movie. This is the standard 
for delivering mixes to producers for approval and revisions. All four DAWs are capable of delivering 
bounces to movie quickly and easily. The procedures for bouncing to movie are very similar to those 
described in the previous chapter.
In DP make sure that all the tracks you want to be bounced are playback-enabled. Select the bounda­
ries of the bounce by clicking and dragging over the tracks and the bars in the track list window. Include 
all the tracks and bars you want in the bounce. Select the “Bounce to disk” option from the Audio menu. 
In the window that appears you can specify several parameters that will affect the bounce (Figure 7.17).
To bounce to movie, select the option “QuickTime Export: Movie” from the File Format drop-down 
menu (Figure 7.17). Clicking “OK” will bring you to the QuickTime Export Options window (Figure 
7.18). Here, make sure to select the “Duplicate original video data” option to generate a new QT 
movie that is self-contained. Also make sure that the option “Extend video tracks with black frames” 
is checked, so that any extra portion of the sequence that has audio material (count-off, reverb tail, etc.) 
before or after the movie is extended with additional black frames. I always recommend having the new 
video bounced inside the files folder of the current project or inside a new folder called “Bounces”.
To bounce to movie in LP simply right-click on the Movie Global track at the top of the Arrange 
window and select “Export Audio to Movie” from the drop-down menu. In the Sound Settings window 
(Figure 7.19) adapt the bounce settings to your needs and then click “OK”.
In PT, to mix down your project to movie, select File  Bounce To  QuickTime Movie (Figure 
7.20). The range, in terms of time/bars included in the bounce, depends on the region selected in the 
FIGURE 7.16
Markers management in CU.

297
7.5  Bouncing to Movie
Edit window. If nothing is selected, then the entire sequence will be included in the bounce. The options 
included in the “Bounce” window are summarized in Table 6.14 (page 267).
The bounce to movie operation in CU follows a different procedure than the one used for the other 
DAWs. Before starting to combine video and final mix I highly recommend making a copy of the video 
you are working on inside a folder called “Video Bounces” that you will place inside your project folder. 
Because of the specific nature of how CU handles the video bounce this will guarantee that you will not 
FIGURE 7.17
Bounce to Movie options in DP.
FIGURE 7.18
QuickTime Export Options window in DP.
FIGURE 7.19
Sound Settings window in LP.
FIGURE 7.20
Bounce to QuickTime movie options in PT.

298
CHAPTER 7  Working with Video
replace the original video with your final bounce. To combine the final mix with the video track, first 
follow the procedures we learned in Chapter 6 for bouncing the audio tracks. Let’s assume you call the 
audio bounce “Audio Mix”. Now go to File  Replace Audio in Video File and in the open window 
(Figure 7.21) that will appear select the copy of the movie you are working on (the one that we put 
inside the “Video Bounces” folder).
Upon clicking the “Open” button you will be prompted with another standard “Open” window 
(Figure 7.22) where you can choose the stereo bounce (in our example “Audio Mix”) that will be used to 
substitute the original audio track of the movie. After clicking the “Open” button CU will substitute the 
original audio track of the movie with the selected file.
FIGURE 7.21
Replace Audio in Video File window in CU.
FIGURE 7.22
Select Audio File window in CU.

299
7.6  Stem Creation
7.6  STEM CREATION
The creation of “stems” (sometimes referred to as submixes) is a very useful technique not only for 
projects involving video but also for any audio-based project. A stem is a stereo track that combines 
(or submixes) several other tracks. These tracks usually (but not always) have similar sonic characteris­
tics. For example, if your piece features a string section (violins 1, violins 2, violas, and cellos), drums 
and percussion, piano, and bass, you could submix the strings in one stem, drums and percussion in 
another, and leave piano and bass as separate tracks. The principle here is that tracks that share similar 
sonic, rhythmic, positioning characteristics or roles can be submixed since they eventually can be placed, 
equalized, or compressed in similar ways. Look at Table 7.2 for some examples of stems.
The main purpose of creating stems is to simplify the delivery, management, and handling of large 
productions with a large number of tracks. While this technique applies mainly to postproduction for 
film and television, it is also used in music production. I find it particularly useful when working long 
distance with other artists or producers. If I had to share a complete multitrack session with a producer 
thousands of miles away it would become problematic in terms of storage space and file transfer. By 
creating stems I can reduce drastically the number of tracks that I have to share, making long-distance 
collaboration more manageable and efficient.
The process of creating stems is based on the concept of collecting the output of all the tracks that 
will be included in the stem and directing them to a single stereo track. This process can be done either 
in real time or offline depending on the DAW you use. From a conceptual point of view the routing can 
be viewed in Figure 7.23.
As you can see in Figure 7.23, to create a stem in real time you have to route the output of the tracks 
that you want to be included in the stem to a bus (let’s say 1-2), then create a new stereo track, assign its 
input to the same bus (1-2), record-enable it, and hit Record. This technique can be applied to all DAWs 
that offer advanced routing through a bus system. The advantage of real-time bounce is that you can cre­
ate several stems at the same time by using different buses and stereo tracks simultaneously.
Offline stem creation involves muting all of the tracks that you don’t want to include in the stem and 
then using the bounce-to-disk function to create the audio file with the submix. Since our four DAWs use 
the aforementioned techniques in slightly different ways, let’s take a look in detail at each procedure.
In DP real-time stem creation can be achieved in two different ways that use the same principle 
explained in Figure 7.23. Using the bus system, route the output of all the tracks that you want to include 
in the stem to an unused bus (for example 1-2) as shown in Figure 7.24. Then, create a new stereo track 
and assign its input to the same bus (1-2). Record-enable the newly created stereo track and press Record 
at the point where you want to start creating the submix. When you press Stop all the selected tracks will 
be combined into the stem track. Keep in mind that the same procedure can be effectively used to create 
several stems at the same time. By creating more than one stereo track receiving from different buses 
you can quickly create multiple stems in one recording passage. To create an offline stem you can use 
the same procedure as for bouncing to disk, with the only difference that you must mute all the tracks 
that you do not want to include in the stem (Figure 7.25, page 302). In addition, I recommend selecting 
the “Add to Sequence” option from the Bounce to Disk window (Figure 7.25) to have the stem automati­
cally reimported in the sequence for further mixing or editing.
In LP you can easily create stems in real time by assigning the output of the tracks that you want to 
include in the submix to a stereo bus (bus 1 in the example shown in Figure 7.26, page 302). Then create 
a new stereo track that has the input assigned to the same bus (bus 1 in this case), record-enable it, and 
start recording. The new stem will be created on the newly created stereo track. By creating more than 
one stereo track receiving from different buses you can quickly create multiple stems in one recording 
passage.

300
CHAPTER 7  Working with Video
Offline stems can be created using the bounce-to-disk feature. Before bouncing, make sure that you 
mute all the tracks that do not belong to the stems you want to create.
The procedure for creating a real-time stem in PT is based on the same bussing system explained in 
Figure 7.23. To create a stem in PT, assign the output of the tracks that you want to include in the sub­
mix to a stereo bus (bus 1-2 in the example shown in Figure 7.27, page 302). Then create a new stereo 
track that has the input assigned to the same bus (bus 1-2 in this case), record-enable it, and start record­
ing. The new stem will be created on the newly created stereo track. As we learned for the other DAWs, 
creating more than one stereo track receiving from different buses allows you to create multiple stems in 
one recording passage.
Table 7.2  Examples of Stems.
Stem
Tracks Submixed
Comments
Strings
Violin1
Smaller stems could be created for violins 
(violin 1 and 2) and bass/cello
Violin 2
Viola
Cello
Bass
Drums
Bass drum
One of the most common stems
Snare
Toms
Hi-hat
Cymbals
Rhythm section
Piano
A very comprehensive stem, usually used 
when mixing down stems for video production
Bass
Drums
Percussion
Guitar
Brass
Trumpets
Used in both classical and pop ensembles
Trombones
French horns
Tuba
Woodwinds
Flute
A classical orchestra stem
Oboe
Clarinet
Bassoon
Contrabassoon
Voiceover
Dialogs
Voiceovers
Sound effects
Sound effects
Foley

301
7.6  Stem Creation
In the case of PT, offline stems can be created using the bounce-to-disk feature. Remember again to 
make sure that you mute all the tracks that do not belong to the stems you want to create.
In CU, I recommend using the very flexible offline stem creation described in the next paragraph. If 
you want to do real-time stems you can use basically the same option described for the previous DAWs, 
FIGURE 7.23
Virtual routing for stem creation.
FIGURE 7.24
Real-time stem creation routing in DP.

302
CHAPTER 7  Working with Video
FIGURE 7.25
Offline settings for stem creation in DP.
FIGURE 7.26
Real-time stem creation routing in LP.
FIGURE 7.27
Real-time stem creation routing in PT.

303
7.6  Stem Creation
with the main difference of having to create the buses (called groups in CU) before routing the output of 
your tracks that will be included in the stem. To create a new group go to Devices  VST Connection 
and select the Group/FX tab (Figure 7.28).
To add a new group click the “Add Group” button and name the new group with the name of the 
stem you are creating. The new group will appear at the bottom of the track window inside a newly 
created folder called Group Tracks (Figure 7.29). The group functions basically as a bus. Now simply 
FIGURE 7.28
VST Connections window in CU.
FIGURE 7.29
Real-time stem creation routing in CU.

304
CHAPTER 7  Working with Video
assign the output of all the tracks you want to include in the stem to the group (in this case the group is 
called “Strings Group”) (Figure 7.29), create a new audio stereo track, assign its input to the same group, 
record-enable the track, and hit Record.
CU is particularly efficient in creating stems offline. Whereas for the other DAWs we created offline 
stems muting the unwanted tracks first and using the traditional bounce-to-disk function, in CU you 
can quickly create multiple stems offline in one single pass. To do so, create as many groups as you 
need, one for each stem (for example Strings, Drums, and Brass). Assign each track to the desired group 
from the main track window, as we did for the real-time bounces. Then select File  Export  Audio 
Mixdown (Figure 7.30).
In the “Export Audio Mixdown” window make sure to select the “Channel Batch Export” option 
in the top left corner (Figure 7.30). This option allows you to bounce multiple stems at the same time. 
In the Channel Selection column select all the groups that you want to bounce (in this case I have 
selected the Strings, Drums, and Brass groups). I also recommend having the stems reimported automat­
ically in the session audio pool and newly created audio tracks. After clicking the Export button all your 
stems will be automatically created.
7.7  SUMMARY
Working with professional DAWs allows you to write and produce for video without resorting to com­
plicated synchronization techniques. With the introduction of more powerful computers, modern 
sequencers can handle MIDI and audio as well as import video. By importing a video source inside the 
sequencer, we avoid having to deal with synchronization protocols used to synchronize the computer and 
FIGURE 7.30
Multiple offline stem bounce in CU.

305
7.7  Summary
an external video machine. If the video production company you are working for can provide you with a 
digitized version of the movie, then the only thing you have to do is to copy the video file to your work­
ing HD and import it to your sequence. All four sequencers handle this function in similar ways. They 
all can import video material in QuickTime format. If you don’t have a QT version of the movie, you 
can fairly quickly and inexpensively digitize it using a simple analog-to-FireWire video converter. When 
working with video you can use either linear or nonlinear devices. A linear device stores the information 
(digital or analog) in a linear way (e.g., a video tape recorder). A nonlinear device stores the information 
in a random-access way (e.g., a video DVD player/recorder). Linear devices are less flexible since they 
always require additional time to locate a specific point on the media, while nonlinear devices can easily 
adapt to any situation and they are much easier to synchronize.
In Chapter 3 we learned how to synchronize nonlinear to nonlinear machines through the use of spe­
cific MIDI messages such as MTC and MIDI Clock. If we want to synchronize linear machines to non­
linear machines instead, we need to use a variation of MTC that can be recorded on tape, since it is not 
possible, owing to a limitation in bandwidth, to store the MTC directly on tape. SMPTE is a synchroni­
zation code used in this type of synchronization. It has the same format as MTC (hh:mm:ss:ff), and it is 
able to encode the digital format to tape by modulating (using a biphase-marking system) the 1 s and 0 s 
of the digital signal into a square wave. The first step in synchronizing a linear machine to a nonlinear 
machine is to stripe SMPTE code onto an empty track of the linear device using an SMPTE generator 
(which can usually be found in the most sophisticated MIDI interfaces). By striping the code we basi­
cally time-stamp the tape. After the tape has been striped, the output of the track with SMPTE code on 
it is sent to the SMPTE input of the MIDI interface. Here, it is translated to MTC by the MIDI interface, 
and it is sent to the sequencer and to the connected MIDI devices which, if set in slave mode, will fol­
low the SMPTE/MTC code frame by frame. This technique can be use to synchronize reel-to-reel tape 
machines, VCR, or digital tape recorders that lack other synchronization protocols.
Despite the established legacy of linear devices in postproduction nowadays, most media are 
recorded, edited, produced, and scored in nonlinear formats. Once you have a digitized movie and there­
fore the movie itself is in a nonlinear format, you can easily import it to your sequencer and anchor the 
video tracks to the conductor track of your sequencer. The best video format in which receive the source 
is Apple’s QuickTime movie. The only thing you have to do is copy the video files onto your session 
HD (I recommend keeping them in the same directory as the project into which they will be imported) 
and select the import movie option in your sequencer. If the video production company is not going to 
provide a digital version of the video, you will have to digitize it yourself using a simple DV converter 
that takes the analog input from your VCR and converts it into digital data by outputting the video in DV 
format to your computer through a FireWire port.
When dealing with nonlinear video I recommend always following these steps to ensure that your 
project is accurately synchronized with the QT video: (1) import the QT movie; (2) set the correct frame 
rate; (3) synchronize the beginning of the movie with bar 1 of the sequence; (4) place markers; (5) set the 
tempo of the sequence; (6) add a two-bar count-off; and (7) transfer the movie’s audio track to the sequence.
When working with video, markers play a crucial role in scoring to picture. Markers are like book­
marks that can be easily inserted (or “dropped”) at any given position in your sequence and can be used 
to quickly jump to a specific location. There are two different types of marker: beat-based markers 
(BBMs) and time-code-based markers (TBMs). The former are locked to a specific bar, beat, and tick, 
and when you change the tempo of your sequence their relative position to bars, beats, and ticks will 
not change, whereas the latter are locked to time-code and not to bars, beats, and ticks. If you lock your 
markers they will always follow and stay with the video independently of the tempo of the sequence.
In a typical scoring to picture project you will deliver your music along with the video. To do so, you 
need to mix down your music along with the video using the bounce-to-movie function. The idea here is 

306
CHAPTER 7  Working with Video
basically the same one discussed in Chapter 6, when you learned how to bounce to disk. The main dif­
ference lies in the fact that when we bounce to movie we are including not only the audio mix down of 
our audio tracks but also the QT movie.
Another technique that is used mainly (but not only) when scoring to picture is the creation of stems. 
A stem is a stereo track that combines, or submixes, several other tracks together. These tracks usually 
(but not always) share similar sonic characteristics (Table 7.2). The main purpose of creating stems is 
to simplify the delivery, management, and handling of large productions with large numbers of tracks. 
By creating stems you can reduce drastically the number of tracks that you have to share, making long-
distance collaboration more manageable and efficient.
7.8  EXERCISES
Exercise 7.1
Start a new project in your DAW and import a QT movie. Using the techniques explained in Chapter 7 do the 
following:
a.	 Set the correct frame rate
b.	 Synchronize the beginning of the movie with bar 1 of the sequence
c.	 Place markers
d.	 Set the tempo (or tempos for the sequence)
e.	 Add two bars of count-off
 f.	 Transfer the movie’s audio track to the sequence.
Exercise 7.2
Starting from Exercise 7.1, score the movie using any combination of instruments. Create a new movie that 
includes the movie’s original audio track and voiceover plus the sound track using the bounce to movie 
function.
Exercise 7.3
Starting from Exercise 7.2, create three different stems and reimport them inside the current sequence on 
three new stereo tracks.

307
Further Reading
Borwick, J. (ed.) (2001). Loudspeaker and Headphone Handbook (3rd edn.). Focal Press.
Collins, M. (2003). A Professional Guide to Audio Plug-ins and Virtual Instruments. Focal Press.
Katz, B. (2007). Mastering Audio: The Art and the Science (2nd edn.). Focal Press.
Lehrman, P.D. & Tully, T. (1993). MIDI for the Professional. Amsco Publications.
Miranda, E.R. (2002). Computer Sound Design: Synthesis Techniques and Programming (2nd edn.). 
Focal Press.
Newell, P. (2007). Recording Studio Design (2nd edn.). Focal Press.
Pejrolo, A. & DeRosa, R. (2007). Acoustic and MIDI Orchestration for the Contemporary Composer: 
A Practical Guide to Writing and Sequencing for the Studio Orchestra. Focal Press.
Russ, M. (2008). Sound Synthesis and Sampling (3rd edn.). Focal Press.
Watkinson, J. (2002). Introduction to Digital Audio (2nd edn.). Focal Press.


Index
“3 Rule”, 143–4
16-channel split Daisy Chain setups, 30, 31
32-bit operating systems, 53
64-bit operating systems, 53
A
Absorption panels, 1, 2
Accelerandos, 122–3
Acoustics, 1, 3, 4
Active Sensing MIDI messages, 16
Adaptive Limiter (Ad-Limiter), 274, 275, 276
ADC see Analog-to-digital converters
Additive synthesis, 218–19
Adjust Soundbites to Sequence Tempo functions, 129
Advanced sequencing techniques, 149–88
audio track effects, 176–80
editing, 162–76
MIDI system exclusive messages, 180–5
quantization, 149–62
Aftertouch, 8, 25
AIR reverb, 242–3, 245–6
Alias objects, 111
All Notes Off MIDI messages, 13
All Sound Off MIDI messages, 15
Ambience effects, 239–49
Analog mixing boards, 35
Analog subtractive synthesis, 216–18
Analog-to-digital converters (ADC), 35, 37–8
Apple computers, 52–3
Archives, 142–5
Arpeggiator, 175, 176
Arrow tools, 85–6, 94
Attack points, 102
“Attack” of string sections, 202–3
Attenuation, 262
Audio equipment:
connections, 37–41
inputs/outputs, 41–3
interface connections, 43–6
mixing boards/monitors, 34–7
software compatibility, 46–7
studio set-up, 5, 34–47
Audio files:
folders, 68
mixing, 265, 267–9
time-stretching, 129–32
Audio-MIDI setup, 63–4
Audio networks, 4, 5
Audio quantization, 157–62
Audio tracks, 65–8, 83–9
destructive editing, 84–6
effects, 176–80
“insert” effects, 176–7
layering, 114
nondestructive editing, 84–6
“send” effects, 177–80
tempo changes, 128–32
Auto-latch automation modes, 91–2
Automation, 89–94, 95–7
data editing, 92–4, 95
mute, 97
pan, 96–7
practical applications, 94–7
real-time, 91–2
static, 89–91
volume, 95–6
Auxiliary (Aux) tracks, 66–7, 133, 134, 178, 180
B
Back-ups, 141–5
Backup utilities, 143
Balance in panning, 238
Banks, 9
Basic sequencing techniques, 59–100
audio tracks, 65–8, 83–9
automation, 89–97
click tracks, 70–2
concepts/review, 59–60
editing, 76–9
file exporting/importing, 97–8
function/organization, 59–63
MIDI tracks, 63–5, 72–5
note quantization, 79–83
pre-operative checks, 69–70
project organization, 68–9
recording, 72–5
tempo setups, 70–2
Bass, 118–19, 195, 198, 199
Bass clarinets, 209–225
Bassoon, 213
Bass-traps, 2
Bass tuba, 208
Bb/BBb tuba, 208
BBMs see Beat-based markers
BD see Beat Detective
Beat-based markers (BBMs), 292–3, 295, 296
Beat Clock see MIDI Clock
Beat Detective (BD):
audio quantization, 160–1
groove quantization, 152, 156–7
tempo changes, 127–8
Beats editing, 154–5
309

310
Index
Beats per minute (BPM), 72
Beats quantization, 159
Beat triggers, 157
Bit rates, 37–40, 41, 265
Black video frames, 287
Bounce in Place, 233
Bounce to Disk, 262–9, 299, 300, 302
Bouncing video to movie, 296–8
Bowing techniques, 201
Brass sections, 204, 206–8, 209
Brush tools, 116
Built-in sounds, 24
Built-in USB connections, 24, 25
Buses, 34, 230–1, 234, 235
see also USB connections
C
Carrier oscillators, 219
CCs see Control Change messages
Cello, 198, 199
Central processing units (CPUs):
digital workstation selection, 53–4
granular synthesis, 223
“insert” effects, 177
orchestration, 214
physical modeling synthesis, 221
software synthesizers, 18, 19–20
video, 285
Change Duration options, 168, 169, 170
Change Velocity options, 168–9, 170
Channel equalizers, 254, 255
Channel MIDI messages:
Mode messages, 13–15
Voice messages, 8–12
Chorder MIDI filters, 174
Chord Memorizer, 175, 176
Clarinets, 211–12
Click tracks, 70–2
Closed audio/software interface, 47
Compression:
dynamic effects, 256–62
premastering, 271–4
Conductor tracks, 121–2
Consoles, 180–2
Continue MIDI messages, 15
Contrabassoon, 213
Contrabass tuba, 208
Control Change (CC) messages:
mixing, 232
studio set-up, 9–12, 23–4
woodwind instruments, 213
Controllers, 17, 18, 22–5, 118–20, 121
Convolution processes, 241–2, 247–9
Cornet, 206
Cross-Over automation modes, 91–2
Cubase (CU), 59–63
audio quantization, 159–60
audio tracks, 84, 85, 86–9
automation, 89–93
bounce to disk, 263, 264
bouncing to movie, 297–8
cycle recording modes, 74–5
digital workstation selection, 49, 50
drum editor, 115
dynamic effects, 258–62
equalizers, 253–6
global MIDI data transformers, 165–8
graphic editors, 76
groove quantization, 110, 151, 152–3
The Limiter, 276, 277
list editors, 78, 79
major audio formats, 267–9
markers, 295, 296
merge functions, 75
meter/tempo changes, 123
metronomes, 71
MIDI file exporting/importing, 98
multiband compression, 273
normalization, 270, 272
orchestration, 215
patch lists, 73
project organization, 68, 69
quantization, 81–2, 103–6, 110, 151–3, 159–60
real-time automation, 91–2
real-time MIDI effects, 173–4
reverberation, 241–3, 244–5
ReWire, 135
score editors, 79
“send” effects, 178, 179, 180
sensitivity parameters, 105, 106
software synthesizers, 234
static automation, 89–91
stem creation, 301, 303–4
swing quantization, 107–8
System Exclusive messages, 184, 185
tempo tracks, 130–1
Undo functions, 78
video synchronization, 286–91, 292
virtual instrument tracks, 67–8
waveforms, 84, 85
Custom virtual consoles, 180–2
Cycle recording modes, 74–5
D
DAC see Digital-to-audio converters
Daisy Chain (DC) setups, 7, 28–31
Data bytes, 167, 168
Data editing, 6, 7, 92–4, 95
Data storage, 141–2
DAWs see Digital Audio Workstations
DC see Daisy Chain setups

311
Index
“Deconvolution” buttons, 249
Destructive editing, 66, 84–6
Diffusers, 1, 2
Diffusion settings, 204
Digital Audio Workstations (DAWs):
audio tracks, 84, 85
mixing, 230, 233, 239
studio set-up, 22, 47–54
System Exclusive messages, 180, 181
Undo functions, 77–8
see also Cubase; Digital Performer; Logic Pro; Pro Tools
Digital mixing boards, 36
Digital Performer (DP), 59–64
audio quantization, 158, 159
audio tracks, 83, 85–8
automation, 89–90, 92, 93
bounce to disk, 262, 263
bouncing to movie, 296, 297
convolution reverberation, 249
digital workstation selection, 47–9, 50
drum editor, 115–16
dynamic effects, 258–62
equalizers, 253–6
global MIDI data transformers, 168–70, 171
graphic editors, 76
groove quantization, 109, 150–1, 152, 153–5
list editors, 78
major audio formats, 267–9
markers, 294
meter changes, 126
metronomes, 72
MIDI file exporting/importing, 97
multiband compression, 272, 273
multiple takes techniques, 74
normalization, 270, 272
orchestration, 215
overdub functions, 75
patch lists, 73
pre-operative checks, 70
project organization, 68, 69
quantization, 80, 82, 102–5, 107, 109, 150–5, 158–9
real-time MIDI effects, 173
reverberation, 241–3, 244, 249
ReWire, 133
score editors, 79
“send” effects, 178, 179, 180
sensitivity parameters, 104–5
stem creation, 299, 302
swing quantization, 107
System Exclusive messages, 181–2, 184–5
tempo changes, 126
tempo tracks, 121–3, 129–30
The Limiter, 274, 275
track layering, 111, 112
Undo functions, 78
video synchronization, 286–91, 292
virtual instrument tracks, 67
Digital signal processors (DSP), 36
Digital-to-audio converters (DAC), 35, 38
Digital video (DV), 281, 285
Disklavier, 192
Distortion, 232
Dithering, 265
Double waveform symbols, 83
DP see Digital Performer
Drop-frame formats, 138
Drum editor, 114–18
Drums, 119–20, 196–8
Drum stick tools, 115, 117
Dry effects, 176
DSP see Digital signal processors
“Dump” functions, 182–5
DV see Digital video
Dynamic effects, 256–62
Dynamics MIDI filters, 174
E
Early Reflections Scale controls, 243
Editing, 114–18, 162–76
automation, 92–4, 95
destructive editing, 66, 84–6
editors/librarians programs, 16
Environment editors, 174–6
global MIDI data transformers, 163–71
graphic editors, 64, 76–7, 92, 93, 94
groove quantization, 150–2
list editors, 64–5, 78–9, 92
logic editors, 163–7
MIDI editors, 62–3, 76–9, 114–18, 162–3
nondestructive editing, 66, 84–6
real-time MIDI effects, 172–6
studio set-up, 6, 7
Effect return, 178
Effect tracks, 178
EG see Envelope generators
End of System Exclusive MIDI messages, 16
End time signature fields, 88
English horn, 212–13
Envelope generators (EG), 216
Environment editors, 174–6
Equalization, 235, 236, 249–56, 271
Eraser tools, 76, 94, 115
Euphonium tuba, 208
EVerb, 242–3, 244
Exclusive MIDI systems messages, 180–5
Expander effects, 256–62
see also Sound modules
Expansion boards, 191
Export Audio Mixdown, 304
Extended Daisy Chain (DC) setups, 29, 30
External Clock, 15

312
Index
External hard disks (HD3), 142, 143
External Synchronization settings, 137, 140
F
Far-field type speakers, 35
Fast Fourier transform (FFT), 253
File data organization, 141–2
Filters, 78, 102–7, 174
Final mix see Mixing
Firewire (FW) audio interface, 43–6
Flex Mode, 161–2
Flugelhorn, 206–7
Flutes, 211
FM see Frequency modulation synthesis
Folders, 141–2
Four-band equalizers, 253, 254
Four-bit resolution, 39
French horn, 206, 207–8
Frequency modulation (FM) synthesis, 219
Frequency placement, 238–9
Frequency ranges, 235, 236–7, 251, 252
Frequency-related issues: mixing, 241
F tuba, 208
FW see Firewire audio interface
G
Gate dynamic effects, 256–62
Global MIDI data transformers, 163–71
Glue tools, 86
Grabber tools, 86
Grand piano, 193
Granular synthesis (GS), 223
Graphic editors, 64, 76–7, 92, 93, 94
Groove quantization, 101, 108–10, 149–57
GS see Granular synthesis
Guitar, 118–19, 193–5
H
Hard disks (HD), 68–9
back-ups, 141–5
digital workstation selection, 54
sampling synthesis, 220–1
software synthesizers, 19–20
Hardware sequencers, 13, 14, 21
Hardware synthesizers:
mixing, 230–2
orchestration, 214–16
studio set-up, 13, 14, 17, 18, 19
HD3 (External hard disks), 142, 143
HD see Hard disks
High Emphasis algorithms, 157
Hitpoints modes, 152, 153
Home studios, 2–3
Horn, 206–7, 212–13
Host applications, 215
Humanize function, 101, 104
I
IEEE 1394a/b interface see Firewire audio interface
Importing movies, 281
Impulse Response (IR), 248–9
IN MIDI ports, 7
Input/output (I/O) labels, 65–6, 67
“Insert” effects, 176–7
“Inserts” slots, 178–80
Instruments, 111–12, 189–228
rhythm sections, 189–98
string sections, 198–205
synthesizers, 213–23
wind instruments, 205–8, 209
woodwind instruments, 209–13
see also Mixing
Interfacing, 5–6
Intermediate sequencing techniques, 101–48
back-ups, 141–5
complex tempo tracks, 121–8
controllers, 118–21
editing, 114–18
interapplication protocols, 132–5, 136
layering, 110–14
meter changes, 121–8
quantization, 101–10
ReWire, 132–5, 136
synchronization, 136–41
tempo tracks, 121–32
Internal Clock, 15
Internal Synchronization settings, 137, 140
IR see Impulse Response
K
Keyboard controllers, 17, 18
see also Controllers
Key switches techniques, 201–2
L
“Latency Compensation”, 249
Layering:
audio tracks, 114
MIDI tracks, 110–14
LEDs see Light-emitting diodes
Legato effects, 203
LFOs see Low-frequency oscillators
Libraries:
brass sections, 204, 208
drums/percussion, 196, 198
string sections, 200–3, 204–5
studio set-up, 6
Light-emitting diodes (LEDs), 70
The Limiter: premastering, 274–7

313
Index
Limiter dynamic effects, 256–62
Linear synchronization machines, 136–7
Linear video machines, 281–4
Line tools, 92
List editors, 64–5, 78–9, 92
Local area network (LAN), 33
Local On/Off MIDI messages, 13, 14
Logic editors, 163–7
Logic Pro (LP), 59–63
audio quantization, 161–2
audio tracks, 83, 86–8
automation, 89–91, 92, 94
bounce to disk, 264, 265, 266
bouncing to movie, 296, 297
convolution reverberation, 247–8
cycle recording modes, 74, 75
digital workstation selection, 49, 51
dynamic effects, 258–62
equalizers, 253–6
global MIDI data transformers, 163–5, 168
graphic editors, 76
groove quantization, 110, 155–6
input/output labels, 66, 67
layering, 111
The Limiter, 274, 275–6
list editors, 78, 79
major audio formats, 267–9
markers, 294, 295
merge functions, 75
meter/tempo changes, 124, 125, 126–7
metronomes, 72
MIDI file exporting/importing, 97
MIDI track editing, 116–17
multiband compression, 272
normalization, 270, 272
orchestration, 215
patch lists, 73
quantization, 81–3, 102–3, 105, 107–8, 110, 155–6, 161–2
real-time MIDI effects, 174–6
reverberation, 241–3, 246–7
ReWire, 133, 134
score editors, 79
“send” effects, 178, 179, 180
sensitivity parameters, 105, 107
sequencer project organization, 68, 69
software synthesizers, 233–4
stem creation, 299, 302
swing quantization, 107–8
System Exclusive messages, 181, 184, 185
tempo tracks, 131–2
Undo functions, 78
video synchronization, 286–91, 292
Long drops, 207
“Lookahead” parameters, 261–2
Low Emphasis algorithms, 157
Low-frequency oscillators (LFOs), 216, 217
LP see Logic Pro
M
Main out connectors, 34
Markers, 292–5, 296
Marquee tools, 92
Master tracks, 68
MasterWorks Compressor, 272, 273
MasterWorks Limiter, 274, 275–6
Matrix sequencers, 118
MC see MIDI Clock
Merge functions, 75
Meter changes, 121–8
Metronomes, 71–2
Mid-field type speakers, 35
MIDI Beat Clock, 15
MIDI Clock (MC), 15, 137–41
“MIDI Dump” functions, 182–5
MIDI Time Code (MTC), 16, 137–41, 281–4
Min Pitch parameters, 130
Mixing, 229–80
ambience effects, 239–49
bounce to disk, 262–9
dynamic effects, 256–62
equalization, 249–56
hardware synthesizers, 230–2
MIDI tracks, 230–2, 233–4
overview, 230–7
panning, 235, 236, 237–9
premastering, 269–77
reverberation, 239–49
software synthesizers, 233–4
Mixing boards, 34–7
Mixing down, 230, 234, 263, 264, 304
Mix settings see Multitimbral settings
mLAN see Music local area network
Modulation Wheels, 9–10
Modulator oscillators, 219
Monitors, 34–7, 54
Mono with 3.5 dB attenuation, 262
Mono audio tracks, 83
Mono no attenuation, 262
Monophonic After touch MIDI messages, 8
Movies see Video
MTC see MIDI Time Code
Multiband compression, 271–4
Multiband equalizers, 254, 255
Multicable MIDI interface, 32
Multi-out formats, 232
Multiple samples, 192
Multiple takes techniques, 73–4
Multiple video monitor setup, 54
Multipressor dynamic effects, 260, 261–2
Multitimbral devices, 26

314
Index
Multitimbral settings, 182–3
Multitrack premixed stereo loops, 198
Musical instrument digital interface (MIDI), 63–5, 72–5
components, 17–27
connection, 27–33
controllers, 22–5, 118–20, 121
data transformers, 163–71
drums, 119–20
editing, 62–3, 76–9, 114–18
exclusive messages, 180–5
exporting files, 97–8
future prospects, 33
graphic editors, 76–7
groove quantization, 152–7
guitar/bass-to-MIDI converters, 118–19
importing files, 97–8
layering, 110–14
list editors, 78–9
MIDI signal paths, 70
mixing, 230–2, 233–4
note quantization, 79–83
orchestration, 189–228
pad devices, 119–20, 196
quantization, 79–83, 152–7
real-time effects, 172–6
recording, 72–5
score editors, 79
sound palettes, 25–7
studio set-up, 4, 5–8
Undo functions, 77–8
wind controllers, 121
workstation selection, 47–54
Music equipment, 3–17
Music local area network (mLAN), 33
Mute, 234
Mute automation, 97
N
Near-field type speakers, 35
Noise see Gate dynamic effects
Noiseshaping, 265
Nondestructive editing, 66, 84–6
Nondrop framerate formats, 138
Nonlinear synchronization machines, 136, 137–9
Nonlinear video machines, 281–4
Nonlinear video media, 285–92
Nonsliced soundbites, 158, 159
Normalization, 270, 271
Notation formats, 6
Note Off MIDI messages, 8
Note On MIDI messages, 8
Number of keys (controllers), 25
O
Oboe, 212–13
“Offline” global MIDI data transformers, 163–71
Offline stems, 300, 302, 304
Omni On/Off MIDI messages, 13–14, 15
Online effects lists, 173–4
Open audio/software interface, 46–7
Operating Systems (OS), 53
Orchestration, 189–228
rhythm sections, 189–98
string sections, 198–205
synthesizers, 213–23
wind instruments, 205–8, 209
woodwind instruments, 209–13
OS see Operating Systems
Oscillators, 216, 217, 219
OUT MIDI ports, 7
Overdub functions, 75
P
Pads, 119–20, 196
see also Drums; Percussion
Panning:
automation, 96–7
balance, 238
brass sections, 204, 208, 209
frequency placement, 238–9
mixing, 235, 236, 237–9
string sections, 203–5
Partials, 220
Patches, 27, 202
Patch layering see Layering
Patch lists, 72–3
PCI see Peripheral component interconnect cards
PCs (Personal Computers), 52–3
Peak input, 259
Pedal tones, 207
Pencil tools:
automation, 92, 94
graphic editors, 76
meter/tempo changes, 123
MIDI track editing, 116, 117, 118
Percussion, 196–8
see also Drums; Pads
Performance settings see Multitimbral settings
Peripheral component interconnect (PCI) cards, 31, 
43–4
Personal Computers (PCs), 52–3
Physical modeling (PM) synthesis, 221–3
Piano, 190–3
Pitch Bend MIDI messages, 9
PlatinumVerb, 242–3, 246–7
Plug-ins, 215–16
see also Dynamic effects
PM see Physical modeling synthesis
Poly/Mono MIDI messages, 13
Polyphonic After touch MIDI messages, 8
Polyphony, 8, 26–7
Portability of controllers, 24
Pre-Delay, 248–9

315
Index
Premastering:
equalization, 271
The Limiter, 274–7
mixing, 269–77
multiband compression, 271–4
normalization, 270, 271
Premixed stereo loops, 197–8
Program Change MIDI messages, 9
Project studios, 2–3, 4
see also Studio set-up
Pro Tools (PT), 59–63
audio quantization, 160–1
audio tracks, 84, 86–8
automation, 89–91, 94, 95
bounce to disk, 265, 267
bouncing to movie, 296–7
digital workstation selection, 48, 49
dynamic effects, 258–62
equalizers, 253–6
global MIDI data transformers, 168, 170, 171
graphic editors, 76
groove quantization, 110, 151–2, 156–7
The Limiter, 276, 277
list editors, 78, 79
major audio formats, 267–9
markers, 294, 295
meter/tempo changes, 123, 124, 127–8
metronomes, 72
MIDI file exporting/importing, 98
multiband compression, 272
multiple takes techniques, 74
normalization, 270, 272
orchestration, 215
overdub functions, 75
patch lists, 73
quantization, 81, 83, 102–7, 110, 151–2, 156–7, 160–1
reverberation, 241–3, 245–6
ReWire, 135, 136
score editors, 79
“send” effects, 178, 179, 180
sensitivity parameters, 104–5
sequencer pre-operative checks, 70
sequencer project organization, 68, 69
software synthesizers, 234, 235
stem creation, 300–2
swing quantization, 107
System Exclusive messages, 185
tempo tracks, 129
track layering, 113
Undo functions, 78
video synchronization, 286–91, 292
virtual instrument tracks, 67
ProVerb, 249
PT see Pro Tools
Q
Q-Ranges, 105, 107
QT see QuickTime movies
Quantization, 101–10, 149–62
audio equipment, 39–40
filters, 102–7
groove, 101, 108–10, 149–57
MIDI notes, 79–83
swing, 107–8
Quarter-frame messages, 138
QuickTime (QT) movies, 285–91, 296–8
R
Rallentandos, 122–3
Random-access memory (RAM):
digital workstation selection, 53–4
physical modeling synthesis, 221–2
rhythm sections, 191
sampling synthesis, 220–1
Randomize parameters, 104
Read-only memory (ROM), 191
Real-time automation, 91–2
Real-time-based synchronization see MIDI Time Code
Real-time MIDI effects, 172–6
Re-amping, 194–5
Recycle, 158
Release points, 102
Rendering, 230–4
Reset All Controllers MIDI messages, 15
Reshape tools, 76, 92, 123
Resolution see Bit rates
Reverb:
brass sections, 204, 208
convolution reverberation, 247–9
mixing, 235, 236, 239–49
string sections, 203–5
synthesized, 242–7
“Reverb Volume Compensation”, 249
ReWire, 67, 132–5, 136
Rhodes Mark land electric piano, 193
Rhythm sections, 189–98
bass, 195
drums/percussion, 196–8
guitars, 193–5
piano, 190–3
RMS input, 259
RoomWorks, 242–3, 244–5
“Rough” mixes, 237
S
Sample-based software synthesizers, 191
Sampled libraries, 204–5
Sampled patches, 202
Sampling frequencies, 37–8, 40, 41, 87
Sampling (synthesizers), 220–1
Satellite computers:
mixing, 232
software synthesizers, 20–1
Star Networks, 32

316
Index
Saxophones, 209–11
Scissor tools, 86, 158, 161
Score editors, 65, 66, 79
Selector tools, 86, 88, 94
“Send” effects, 176, 177–80
Sensitivity parameters, 101, 104–5, 107
Sequencer/audio region parameters, 82–3
Sequencing, 149–88
advanced techniques, 149–88
audio tracks, 65–8, 83–9, 176–80
automation, 89–97
back-ups, 141–5
basic techniques, 59–100
brass sections, 204, 208, 209
click tracks, 70–2
concepts/review, 59–60
editing, 76–9, 114–18, 162–76
file exporting/importing, 97–8
function/organization, 59–63
intermediate techniques, 101–48
MIDI tracks, 63–5, 72–5
note quantization, 79–83
pre-operative checks, 69–70
project organization, 68–9
quantization, 79–83, 149–62
recording, 72–5
string sections, 198–200
studio set-up, 6, 7–8, 17, 21–7
system exclusive messages, 180–5
tempo setups, 70–2
see also Cubase; Digital Performer; Logic Pro; Pro Tools
Session files, 69
Set Chunk Start, 287, 288
Set Movie Start Time, 287, 288
Short drops, 207
Simple sequencing techniques see Basic sequencing 
techniques
Single-port Daisy Chain (DC) setups, 28, 29, 30
Sliced soundbites, 159, 160
SMF, 97–8
SMPTE see Society of Motion Picture and Television 
Engineers
SN see Star Network MIDI device connections
Snapshots, 89–90
Society of Motion Picture and Television Engineers (SMPTE), 
137–8, 281–4, 289
Soft Synth-Only (SS) MIDI device connections, 28
Software compatibility, 46–7
Software libraries, 196
Software samplers, 191–2
Software sequencers, 21, 22
Software synthesizers:
mixing, 233–4
orchestration, 191, 214–16
rhythm sections, 191
studio set-up, 17, 18–21
Song Position Pointer (SPP), 16, 137
Song Select MIDI messages, 16
Sonority, 200–3
Sound bites, 85
Sound libraries, 200–3
Sound modules, 17–18, 26
Sound palettes, 25–7
Space Designer, 247–8
Spectral Effects, 129, 130
Split Notes, 168, 169–70, 171
Split stereo, 262
SPP see Song Position Pointer
SS see Soft Synth-Only MIDI device connections
Staccato effects, 203
Standalone formats, 215
Star Network (SN) MIDI device connections, 28, 31, 32
Start MIDI messages, 15
Start time signature fields, 88
Static automation, 89–91
Status bytes, 167, 168
Stem creation, 299–304
Stereo audio tracks, 83
Stop MIDI messages, 15
Storage area network solutions, 143–4
Storage devices, 54
see also Buses; USB connections
Strength options, 104, 106
String sections:
panning, 203–5
reverb, 203–5
sequencing, 198–200
sonority, 200–3
sound libraries, 200–3
Studio set-up, 1–58
audio equipment, 34–47
basic information, 1–2
digital workstation selection, 47–54
MIDI components, 17–27
MIDI device connections, 27–33
MIDI equipment, 5–8
MIDI messages, 5–8
music equipment, 3–17
project studios, 2–3, 4
Submixes, 234–7
Subtractive synthesizers, 216–18
Swing quantization, 107–8
Synchronization, 136–41
linear machines, 136–7
nonlinear machines, 136, 137–9
utilities, 143
video, 281–4, 285–92
Synthesized Impulse Response (IR), 248–9
Synthesized patches, 202
Synthesized reverberation, 242–7
Synthesizers:
additive, 218–19

317
Index
analog subtractive, 216–18
frequency modulation, 219
granular, 223
mixing, 230–2
orchestration, 213–23
physical modeling, 221–3
rhythm sections, 191
sampling, 220–1
software, 233–4
studio-set-up, 13, 14, 17, 18–21, 26–7
wavetable, 220
SysEx see System Exclusive messages
System Exclusive (SysEx) messages, 16–17, 180–5
System MIDI messages, 8, 15–17
System Reset MIDI messages, 16
T
Tab to Transient functions, 88
Tape-based synchronization machines, 136–7
Tap Tempo, 126
TBMs see Time-code-based markers
TCE see Time compression expansion
Tempo-based sync protocols see MIDI Clock
Tempo Interpreter, 126–7
Tempo setups, 70–2, 121–32
The Limiter, 274–7
Three-band compression, 271–2, 273
THRU MIDI ports, 7
Ticks, 105, 107
Time-based effects, 177–8
Time-code-based markers (TBMs), 293–5, 296
Time Compression/Expander Trimmer, 88
Time compression expansion (TCE), 80, 130
Time and Pitch Machine, 131–2
Time-stretching, 129–32
Timing Clock MIDI messages, 15
Track list windows, 60, 61
Track organization, 234–7
Transform editors see Logic editors
Transpose options, 168, 170
Trigger pads, 120
Trimmer tools, 85–6, 88, 130
Trim modes, 92
Trombone, 206, 207
Trumpets, 206–7
Tuba, 208
Tune Request MIDI messages, 16
“Two-beep”, 291–2
Two-bit resolution, 39
U
Undo functions, 77–8
USB connections, 25, 43–6
V
VCA see Voltage-controlled amplifiers
VCO see Voltage-controlled oscillators
VCRs see Video Cassette Recorders
Video, 281–306
bouncing to movie, 296–8
importing movies, 281
markers, 292–5, 296
nonlinear video media, 285–92
SMPTE, 281–4, 289
stem creation, 299–304
synchronization, 281–4, 285–92
Video Cassette Recorders (VCRs), 283–4
View filter options, 78
Violas, 198, 199
Violins, 198, 199, 221, 222
Virtual instrument tracks, 67–8
VL1 PM synthesizer, 222
Voltage-controlled amplifiers (VCA), 216
Voltage-controlled oscillators (VCO), 216
Volume automation, 95–6
W
Wavetable synthesis, 220
Weighted key action controller features, 23, 24
Wind controllers, 121
Wind instruments, 205–8, 209
brass sections, 206–8, 209
flugelhorn, 206–7
French horn, 206, 207–8
trombone, 207
trumpets, 206–7
tuba, 208
Woodwind instruments, 209–13
bassoon, 213
clarinets, 211–12
contrabassoon, 213
English horn, 212–13
flutes, 211
oboe, 212–13
saxophones, 209–11
Word clocks, 43
Wurlitzer 200 series electric piano, 193
Y
Yamaha Disklavier, 192
Yamaha VL1 PM synthesizer, 222

