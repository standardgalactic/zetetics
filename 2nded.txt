PREVIEW
Practical Foundations for Programming Languages
SECOND EDITION
Robert Harper
Carnegie Mellon University

PREVIEW
Copyright © 2016 by Robert Harper.
All Rights Reserved.
This is an abbreviated version of a book published by Cambridge University Press
(http://www.cambridge.org). This draft is made available for the personal use of a
single individual. The reader may make one copy for personal use. No unauthorized
distribution of any kind is allowed. No alterations are permitted.

PREVIEW
Preface to the Second Edition
Writing the second edition to a text book incurs the same risk as building the second version of a
software system. It is difficult to make substantive improvements, while avoiding the temptation
to overburden and undermine the foundation on which one is building. With the hope of avoiding
the second system effect, I have sought to make corrections, revisions, expansions, and deletions
that improve the coherence of the development, remove some topics that distract from the main
themes, add new topics that were omitted from the first edition, and include exercises for almost
every chapter.
The revision removes a number of typographical errors, corrects a few material errors (espe-
cially the formulation of the parallel abstract machine and of concurrency in Algol), and improves
the writing throughout. Some chapters have been deleted (general pattern matching and polar-
ization, restricted forms of polymorphism), some have been completely rewritten (the chapter
on higher kinds), some have been substantially revised (general and parametric inductive defi-
nitions, concurrent and distributed Algol), several have been reorganized (to better distinguish
partial from total type theories), and a new chapter has been added (on type refinements). Titular
attributions on several chapters have been removed, not to diminish credit, but to avoid confusion
between the present and the original formulations of several topics. A new system of (pronounce-
able!) language names has been introduced throughout. The exercises generally seek to expand
on the ideas in the main text, and their solutions often involve significant technical ideas that merit
study. Routine exercises of the kind one might include in a homework assignment are deliberately
few.
My purpose in writing this book is to establish a comprehensive framework for formulating
and analyzing a broad range of ideas in programming languages. If language design and pro-
gramming methodology are to advance from a trade-craft to a rigorous discipline, it is essential
that we first get the definitions right. Then, and only then, can there be meaningful analysis and
consolidation of ideas. My hope is that I have helped to build such a foundation.
I am grateful to Stephen Brookes, Evan Cavallo, Karl Crary, Jon Sterling, James R. Wilcox, and
Todd Wilson for their help in critiquing drafts of this edition and for their suggestions for revision.
I thank my department head, Frank Pfenning, for his support of my work on the completion of
this edition. Thanks also to my editors, Ada Brunstein and Lauren Cowles, for their guidance and
assistance. And thanks to Evan Cavallo and Andrew Shulaev for corrections to the draft.
Neither the author nor the publisher make any warranty, express or implied, that the defi-
nitions, theorems, and proofs contained in this volume are free of error, or are consistent with

PREVIEW
any particular standard of merchantability, or that they will meet requirements for any particular
application. They should not be relied on for solving a problem whose incorrect solution could
result in injury to a person or loss of property. If you do use this material in such a manner, it is at
your own risk. The author and publisher disclaim all liability for direct or consequential damage
resulting from its use.
Pittsburgh
July, 2015

PREVIEW
Preface to the First Edition
Types are the central organizing principle of the theory of programming languages. Language fea-
tures are manifestations of type structure. The syntax of a language is governed by the constructs
that define its types, and its semantics is determined by the interactions among those constructs.
The soundness of a language design—the absence of ill-defined programs—follows naturally.
The purpose of this book is to explain this remark. A variety of programming language features
are analyzed in the unifying framework of type theory. A language feature is defined by its statics,
the rules governing the use of the feature in a program, and its dynamics, the rules defining how
programs using this feature are to be executed. The concept of safety emerges as the coherence of
the statics and the dynamics of a language.
In this way we establish a foundation for the study of programming languages. But why these
particular methods? The main justification is provided by the book itself. The methods we use are
both precise and intuitive, providing a uniform framework for explaining programming language
concepts. Importantly, these methods scale to a wide range of programming language concepts,
supporting rigorous analysis of their properties. Although it would require another book in itself
to justify this assertion, these methods are also practical in that they are directly applicable to imple-
mentation and uniquely effective as a basis for mechanized reasoning. No other framework offers
as much.
Being a consolidation and distillation of decades of research, this book does not provide an
exhaustive account of the history of the ideas that inform it. Suffice it to say that much of the de-
velopment is not original, but rather is largely a reformulation of what has gone before. The notes
at the end of each chapter signpost the major developments, but are not intended as a complete
guide to the literature. For further information and alternative perspectives, the reader is referred
to such excellent sources as Constable (1986), Constable (1998), Girard (1989), Martin-L¨of (1984),
Mitchell (1996), Pierce (2002, 2004), and Reynolds (1998).
The book is divided into parts that are, in the main, independent of one another. Parts I and II,
however, provide the foundation for the rest of the book, and must therefore be considered prior
to all other parts. On first reading it may be best to skim Part I, and begin in earnest with Part II,
returning to Part I for clarification of the logical framework in which the rest of the book is cast.
Numerous people have read and commented on earlier editions of this book, and have sug-
gested corrections and improvements to it. I am particularly grateful to Umut Acar, Jesper Louis
Andersen, Carlo Angiuli, Andrew Appel, Stephanie Balzer, Eric Bergstrom, Guy E. Blelloch, Il-
iano Cervesato, Lin Chase, Karl Crary, Rowan Davies, Derek Dreyer, Dan Licata, Zhong Shao,

PREVIEW
Rob Simmons, and Todd Wilson for their extensive efforts in reading and criticizing the book. I
also thank the following people for their suggestions: Joseph Abrahamson, Arbob Ahmad, Zena
Ariola, Eric Bergstrome, William Byrd, Alejandro Cabrera, Luis Caires, Luca Cardelli, Manuel
Chakravarty, Richard C. Cobbe, James Cooper, Yi Dai, Daniel Dantas, Anupam Datta, Jake Don-
ham, Bill Duff, Matthias Felleisen, Kathleen Fisher, Dan Friedman, Peter Gammie, Maia Gins-
burg, Byron Hawkins, Kevin Hely, Kuen-Bang Hou (Favonia), Justin Hsu, Wojciech Jedynak, Cao
Jing, Salil Joshi, Gabriele Keller, Scott Kilpatrick, Danielle Kramer, Dan Kreysa, Akiva Leffert,
Ruy Ley-Wild, Karen Liu, Dave MacQueen, Chris Martens, Greg Morrisett, Stefan Muller, Tom
Murphy, Aleksandar Nanevski, Georg Neis, David Neville, Adrian Trejo Nu˜nez, Cyrus Omar,
Doug Perkins, Frank Pfenning, Jean Pichon, Benjamin Pierce, Andrew M. Pitts, Gordon Plotkin,
David Renshaw, John Reynolds, Andreas Rossberg, Carter Schonwald, Dale Schumacher, Dana
Scott, Shayak Sen, Pawel Sobocinski, Kristina Sojakova, Daniel Spoonhower, Paulo Tanimoto, Joe
Tassarotti, Peter Thiemann, Bernardo Toninho, Michael Tschantz, Kami Vaniea, Carsten Varming,
David Walker, Dan Wang, Jack Wileden, Sergei Winitzki, Roger Wolff, Omer Zach, Luke Zarko,
and Yu Zhang. I am very grateful to the students of 15–312 and 15–814 at Carnegie Mellon who
have provided the impetus for the preparation of this book and who have endured the many
revisions to it over the last ten years.
I thank the Max Planck Institute for Software Systems for its hospitality and support. I also
thank Espresso a Mano in Pittsburgh, CB2 Cafe in Cambridge, and Thonet Cafe in Saarbr¨ucken
for providing a steady supply of coffee and a conducive atmosphere for writing.
This material is, in part, based on work supported by the National Science Foundation under
Grant Nos. 0702381 and 0716469. Any opinions, findings, and conclusions or recommendations
expressed in this material are those of the author(s) and do not necessarily reflect the views of the
National Science Foundation.
Robert Harper
Pittsburgh
March, 2012

PREVIEW
Contents
Preface to the Second Edition
iii
Preface to the First Edition
v
I
Judgments and Rules
1
1
Abstract Syntax
3
1.1
Abstract Syntax Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.2
Abstract Binding Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.3
Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
2
Inductive Definitions
13
2.1
Judgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
2.2
Inference Rules
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
2.3
Derivations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2.4
Rule Induction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
2.5
Iterated and Simultaneous Inductive Definitions . . . . . . . . . . . . . . . . . . . . .
18
2.6
Defining Functions by Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.7
Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
3
Hypothetical and General Judgments
23
3.1
Hypothetical Judgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
3.1.1
Derivability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
3.1.2
Admissibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
3.2
Hypothetical Inductive Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
3.3
General Judgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
3.4
Generic Inductive Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
3.5
Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30

PREVIEW
viii
CONTENTS
II
Statics and Dynamics
33
4
Statics
35
4.1
Syntax
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
4.2
Type System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
4.3
Structural Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
4.4
Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
5
Dynamics
41
5.1
Transition Systems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
5.2
Structural Dynamics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
5.3
Contextual Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
5.4
Equational Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
5.5
Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
6
Type Safety
51
6.1
Preservation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
6.2
Progress
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
6.3
Run-Time Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
6.4
Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
7
Evaluation Dynamics
57
7.1
Evaluation Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
7.2
Relating Structural and Evaluation Dynamics . . . . . . . . . . . . . . . . . . . . . . .
58
7.3
Type Safety, Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
7.4
Cost Dynamics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
7.5
Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
III
Total Functions
63
8
Function Definitions and Values
65
8.1
First-Order Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
8.2
Higher-Order Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
8.3
Evaluation Dynamics and Definitional Equality
. . . . . . . . . . . . . . . . . . . . .
69
8.4
Dynamic Scope
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
8.5
Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
9
System T of Higher-Order Recursion
73
9.1
Statics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
9.2
Dynamics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
9.3
Definability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
9.4
Undefinability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
9.5
Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79

PREVIEW
CONTENTS
ix
IV
Finite Data Types
81
10 Product Types
83
10.1 Nullary and Binary Products
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
10.2 Finite Products
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
10.3 Primitive Mutual Recursion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
10.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
11 Sum Types
89
11.1 Nullary and Binary Sums
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
11.2 Finite Sums
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
11.3 Applications of Sum Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
11.3.1 Void and Unit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
11.3.2 Booleans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
11.3.3 Enumerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
11.3.4 Options
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
11.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
V
Types and Propositions
97
12 Constructive Logic
99
12.1 Constructive Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
12.2 Constructive Logic
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
12.2.1 Provability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
12.2.2 Proof Terms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
12.3 Proof Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
12.4 Propositions as Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
12.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
13 Classical Logic
109
13.1 Classical Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
13.1.1 Provability and Refutability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
13.1.2 Proofs and Refutations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
13.2 Deriving Elimination Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
13.3 Proof Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
13.4 Law of the Excluded Middle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
13.5 The Double-Negation Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
13.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
VI
Infinite Data Types
121
14 Generic Programming
123
14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123

PREVIEW
x
CONTENTS
14.2 Polynomial Type Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
14.3 Positive Type Operators
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
14.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
15 Inductive and Coinductive Types
129
15.1 Motivating Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
15.2 Statics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
15.2.1 Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
15.2.2 Expressions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
15.3 Dynamics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
15.4 Solving Type Equations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
15.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
VII
Variable Types
139
16 System F of Polymorphic Types
141
16.1 Polymorphic Abstraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
16.2 Polymorphic Definability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
16.2.1 Products and Sums . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
16.2.2 Natural Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
16.3 Parametricity Overview
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
16.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
17 Abstract Types
151
17.1 Existential Types
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
17.1.1 Statics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
17.1.2 Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
17.1.3 Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
17.2 Data Abstraction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
17.3 Definability of Existential Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
17.4 Representation Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
17.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
18 Higher Kinds
159
18.1 Constructors and Kinds
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
18.2 Constructor Equality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
18.3 Expressions and Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
18.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
VIII
Partiality and Recursive Types
165
19 System PCF of Recursive Functions
167
19.1 Statics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169

PREVIEW
CONTENTS
xi
19.2 Dynamics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
19.3 Definability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
19.4 Finite and Infinite Data Structures
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
19.5 Totality and Partiality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
19.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
20 System FPC of Recursive Types
177
20.1 Solving Type Equations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
20.2 Inductive and Coinductive Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
20.3 Self-Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
20.4 The Origin of State
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
20.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
IX
Dynamic Types
185
21 The Untyped λ-Calculus
187
21.1 The λ-Calculus
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
21.2 Definability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
21.3 Scott’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
21.4 Untyped Means Uni-Typed
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
21.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
22 Dynamic Typing
195
22.1 Dynamically Typed PCF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
22.2 Variations and Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
22.3 Critique of Dynamic Typing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
22.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
23 Hybrid Typing
205
23.1 A Hybrid Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
23.2 Dynamic as Static Typing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
23.3 Optimization of Dynamic Typing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
23.4 Static Versus Dynamic Typing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210
23.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
X
Subtyping
213
24 Structural Subtyping
215
24.1 Subsumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
24.2 Varieties of Subtyping
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
24.3 Variance
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
24.4 Dynamics and Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
24.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224

PREVIEW
xii
CONTENTS
25 Behavioral Typing
227
25.1 Statics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
25.2 Boolean Blindness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
25.3 Refinement Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
25.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
XI
Dynamic Dispatch
241
26 Classes and Methods
243
26.1 The Dispatch Matrix
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244
26.2 Class-Based Organization
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
26.3 Method-Based Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
26.4 Self-Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
26.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250
27 Inheritance
253
27.1 Class and Method Extension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
27.2 Class-Based Inheritance
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
27.3 Method-Based Inheritance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
27.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
XII
Control Flow
259
28 Control Stacks
261
28.1 Machine Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
28.2 Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
28.3 Correctness of the Stack Machine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
28.3.1 Completeness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
28.3.2 Soundness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
28.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
29 Exceptions
269
29.1 Failures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
29.2 Exceptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
29.3 Exception Values
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272
29.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
30 Continuations
275
30.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
30.2 Continuation Dynamics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
30.3 Coroutines from Continuations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
30.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281

PREVIEW
CONTENTS
xiii
XIII
Symbolic Data
283
31 Symbols
285
31.1 Symbol Declaration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
31.1.1 Scoped Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
31.1.2 Scope-Free Dynamics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
31.2 Symbol References
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
31.2.1 Statics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
31.2.2 Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
31.2.3 Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
31.3 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
32 Fluid Binding
293
32.1 Statics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
32.2 Dynamics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294
32.3 Type Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
32.4 Some Subtleties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
32.5 Fluid References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
32.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
33 Dynamic Classification
301
33.1 Dynamic Classes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
33.1.1 Statics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
33.1.2 Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302
33.1.3 Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
33.2 Class References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
33.3 Definability of Dynamic Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
33.4 Applications of Dynamic Classification
. . . . . . . . . . . . . . . . . . . . . . . . . . 305
33.4.1 Classifying Secrets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
33.4.2 Exception Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
33.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
XIV
Mutable State
309
34 Modernized Algol
311
34.1 Basic Commands . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
34.1.1 Statics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
34.1.2 Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313
34.1.3 Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315
34.2 Some Programming Idioms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316
34.3 Typed Commands and Typed Assignables . . . . . . . . . . . . . . . . . . . . . . . . . 317
34.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319

PREVIEW
xiv
CONTENTS
35 Assignable References
323
35.1 Capabilities
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
35.2 Scoped Assignables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324
35.3 Free Assignables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
35.4 Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328
35.5 Benign Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
35.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332
36 Lazy Evaluation
335
36.1 PCF By-Need . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
36.2 Safety of PCF By-Need . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338
36.3 FPC By-Need . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340
36.4 Suspension Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
36.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343
XV
Parallelism
345
37 Nested Parallelism
347
37.1 Binary Fork-Join . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347
37.2 Cost Dynamics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
37.3 Multiple Fork-Join
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
37.4 Bounded Implementations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355
37.5 Scheduling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
37.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360
38 Futures and Speculations
363
38.1 Futures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364
38.1.1 Statics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364
38.1.2 Sequential Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364
38.2 Speculations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
38.2.1 Statics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
38.2.2 Sequential Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
38.3 Parallel Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366
38.4 Pipelining With Futures
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368
38.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369
XVI
Concurrency and Distribution
371
39 Process Calculus
373
39.1 Actions and Events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
39.2 Interaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
39.3 Replication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377

PREVIEW
CONTENTS
xv
39.4 Allocating Channels
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378
39.5 Communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380
39.6 Channel Passing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383
39.7 Universality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385
39.8 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386
40 Concurrent Algol
389
40.1 Concurrent Algol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390
40.2 Broadcast Communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392
40.3 Selective Communication
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394
40.4 Free Assignables as Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396
40.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 398
41 Distributed Algol
399
41.1 Statics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399
41.2 Dynamics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402
41.3 Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404
41.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404
XVII
Modularity
407
42 Modularity and Linking
409
42.1 Simple Units and Linking
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409
42.2 Initialization and Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410
42.3 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 412
43 Singleton Kinds and Subkinding
413
43.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 414
43.2 Singletons
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 414
43.3 Dependent Kinds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416
43.4 Higher Singletons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419
43.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421
44 Type Abstractions and Type Classes
423
44.1 Type Abstraction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424
44.2 Type Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425
44.3 A Module Language
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 428
44.4 First- and Second-Class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 432
44.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433

PREVIEW
xvi
CONTENTS
45 Hierarchy and Parameterization
435
45.1 Hierarchy
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435
45.2 Abstraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438
45.3 Hierarchy and Abstraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440
45.4 Applicative Functors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 442
45.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 443
XVIII
Equational Reasoning
445
46 Equality for System T
447
46.1 Observational Equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447
46.2 Logical Equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 450
46.3 Logical and Observational Equivalence Coincide . . . . . . . . . . . . . . . . . . . . . 452
46.4 Some Laws of Equality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
46.4.1 General Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
46.4.2 Equality Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455
46.4.3 Induction Law
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455
46.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 456
47 Equality for System PCF
457
47.1 Observational Equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457
47.2 Logical Equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458
47.3 Logical and Observational Equivalence Coincide . . . . . . . . . . . . . . . . . . . . . 458
47.4 Compactness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461
47.5 Lazy Natural Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464
47.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465
48 Parametricity
467
48.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 467
48.2 Observational Equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468
48.3 Logical Equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469
48.4 Parametricity Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 474
48.5 Representation Independence, Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . 477
48.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478
49 Process Equivalence
479
49.1 Process Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 479
49.2 Strong Equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481
49.3 Weak Equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 484
49.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485

PREVIEW
CONTENTS
xvii
XIX
Appendices
487
A Answers to the Exercises
489
B
Background on Finite Sets
541

PREVIEW

PREVIEW
Part I
Judgments and Rules

PREVIEW

PREVIEW
Chapter 1
Abstract Syntax
Programming languages express computations in a form comprehensible to both people and ma-
chines. The syntax of a language specifies how various sorts of phrases (expressions, commands,
declarations, and so forth) may be combined to form programs. But what are these phrases? What
is a program made of?
The informal concept of syntax involves several distinct concepts. The surface, or concrete, syn-
tax is concerned with how phrases are entered and displayed on a computer. The surface syntax
is usually thought of as given by strings of characters from some alphabet (say, ASCII or Uni-
code). The structural, or abstract, syntax is concerned with the structure of phrases, specifically
how they are composed from other phrases. At this level a phrase is a tree, called an abstract
syntax tree, whose nodes are operators that combine several phrases to form another phrase. The
binding structure of syntax is concerned with the introduction and use of identifiers: how they are
declared, and how declared identifiers can be used. At this level phrases are abstract binding trees,
which enrich abstract syntax trees with the concepts of binding and scope.
We will not concern ourselves in this book with concrete syntax, but will instead consider
pieces of syntax to be finite trees augmented with a means of expressing the binding and scope
of identifiers within a syntax tree. To prepare the ground for the rest of the book, we define in
this chapter what is a “piece of syntax” in two stages. First, we define abstract syntax trees, or
asts, which capture the hierarchical structure of a piece of syntax, while avoiding commitment
to their concrete representation as a string. Second, we augment abstract syntax trees with the
means of specifying the binding (declaration) and scope (range of significance) of an identifier.
Such enriched forms of abstract syntax are called abstract binding trees, or abts for short.
Several functions and relations on abts are defined that give precise meaning to the informal
ideas of binding and scope of identifiers. The concepts are infamously difficult to define properly,
and are the mother lode of bugs for language implementors. Consequently, precise definitions are
essential, but they are also fairly technical and take some getting used to. It is probably best to skim
this chapter on first reading to get the main ideas, and return to it for clarification as necessary.

PREVIEW
4
1.1 Abstract Syntax Trees
1.1
Abstract Syntax Trees
An abstract syntax tree, or ast for short, is an ordered tree whose leaves are variables, and whose in-
terior nodes are operators whose arguments are its children. Asts are classified into a variety of sorts
corresponding to different forms of syntax. A variable stands for an unspecified, or generic, piece
of syntax of a specified sort. Asts can be combined by an operator, which has an arity specifying
the sort of the operator and the number and sorts of its arguments. An operator of sort s and arity
s1, . . . , sn combines n ≥0 asts of sort s1, . . . , sn, respectively, into a compound ast of sort s.
The concept of a variable is central, and therefore deserves special emphasis. A variable is
an unknown object drawn from some domain. The unknown can become known by substitution
of a particular object for all occurrences of a variable in a formula, thereby specializing a general
formula to a particular instance. For example, in school algebra variables range over real numbers,
and we may form polynomials, such as x2 + 2 x + 1, that can be specialized by substitution of, say,
7 for x to obtain 72 + (2 × 7) + 1, which can be simplified according to the laws of arithmetic to
obtain 64, which is (7 + 1)2.
Abstract syntax trees are classified by sorts that divide asts into syntactic categories. For exam-
ple, familiar programming languages often have a syntactic distinction between expressions and
commands; these are two sorts of abstract syntax trees. Variables in abstract syntax trees range
over sorts in the sense that only asts of the specified sort of the variable can be plugged in for
that variable. Thus it would make no sense to replace an expression variable by a command, nor
a command variable by an expression, the two being different sorts of things. But the core idea
carries over from school mathematics, namely that a variable is an unknown, or a place-holder, whose
meaning is given by substitution.
As an example, consider a language of arithmetic expressions built from numbers, addition,
and multiplication. The abstract syntax of such a language consists of a single sort Exp generated
by these operators:
1. An operator num[ n ] of sort Exp for each n ∈N;
2. Two operators, plus and times, of sort Exp, each with two arguments of sort Exp.
The expression 2 + (3 × x), which involves a variable, x, would be represented by the ast
plus( num[ 2 ] ; times( num[ 3 ] ; x ) )
of sort Exp, under the assumption that x is also of this sort. Because, say, num[ 4 ], is an ast of sort
Exp, we may plug it in for x in the above ast to obtain the ast
plus( num[ 2 ] ; times( num[ 3 ] ; num[ 4 ] ) ),
which is written informally as 2 + (3 × 4). We may, of course, plug in more complex asts of sort
Exp for x to obtain other asts as result.
The tree structure of asts provides a very useful principle of reasoning, called structural induc-
tion. Suppose that we wish to prove that some property P(a) holds of all asts a of a given sort.
To show this it is enough to consider all the ways in which a can be generated, and show that the
property holds in each case under the assumption that it holds for its constituent asts (if any). So,
in the case of the sort Exp just described, we must show

PREVIEW
1.1 Abstract Syntax Trees
5
1. The property holds for any variable x of sort Exp: prove that P(x).
2. The property holds for any number, num[ n ]: for every n ∈N, prove that P(num[ n ]).
3. Assuming that the property holds for a1 and a2, prove that it holds for plus( a1 ; a2 ) and
times( a1 ; a2 ): if P(a1) and P(a2), then P(plus( a1 ; a2 )) and P(times( a1 ; a2 )).
Because these cases exhaust all possibilities for the formation of a, we are assured that P(a) holds
for any ast a of sort Exp.
It is common to apply the principle of structural induction in a form that takes account of the
interpretation of variables as place-holders for asts of the appropriate sort. Informally, it is often
useful to prove a property of an ast involving variables in a form that is conditional on the property
holding for the variables. Doing so anticipates that the variables will be replaced with asts that
ought to have the property assumed for them, so that the result of the replacement will have the
property as well. This amounts to applying the principle of structural induction to properties P(a)
of the form “if a involves variables x1, . . . , xk, and Q holds of each xi, then Q holds of a”, so that
a proof of P(a) for all asts a by structural induction is just a proof that Q(a) holds for all asts a
under the assumption that Q holds for its variables. When there are no variables, there are no
assumptions, and the proof of P is a proof that Q holds for all closed asts. On the other hand if x is
a variable in a, and we replace it by an ast b for which Q holds, then Q will hold for the result of
replacing x by b in a.
For the sake of precision, we now give precise definitions of these concepts. Let S be a finite set
of sorts. For a given set S of sorts, an arity has the form (s1, . . . , sn)s, which specifies the sort s ∈S
of an operator taking n ≥0 arguments, each of sort si ∈S. Let O = { Oα } be an arity-indexed
family of disjoint sets of operators Oα of arity α. If o is an operator of arity (s1, . . . , sn)s, we say that
o has sort s and has n arguments of sorts s1, . . . , sn.
Fix a set S of sorts and an arity-indexed family O of sets of operators of each arity. Let X =
{ Xs }s∈S be a sort-indexed family of disjoint finite sets Xs of variables x of sort s. When X is clear
from context, we say that a variable x is of sort s if x ∈Xs, and we say that x is fresh for X , or just
fresh when X is understood, if x /∈Xs for any sort s. If x is fresh for X and s is a sort, then X , x is
the family of sets of variables obtained by adding x to Xs. The notation is ambiguous in that the
sort s is not explicitly stated, but determined from context.
The family A[X ] = { A[X ]s }s∈S of abstract syntax trees, or asts, of sort s is the smallest family
satisfying the following conditions:
1. A variable of sort s is an ast of sort s: if x ∈Xs, then x ∈A[X ]s.
2. Operators combine asts: if o is an operator of arity (s1, . . . , sn)s, and if a1 ∈A[X ]s1, ...,
an ∈A[X ]sn, then o( a1 ; . . . ; an ) ∈A[X ]s.
It follows from this definition that the principle of structural induction can be used to prove that
some property P holds of every ast. To show P(a) holds for every a ∈A[X ], it is enough to show:
1. If x ∈Xs, then Ps(x).
2. If o has arity (s1, . . . , sn)s and Ps1(a1) and ... and Psn(an), then Ps(o( a1 ; . . . ; an )).

PREVIEW
6
1.2 Abstract Binding Trees
For example, it is easy to prove by structural induction that A[X ] ⊆A[Y] whenever X ⊆Y.
Variables are given meaning by substitution. If a ∈A[X , x]s′, and b ∈A[X ]s, then [b/x]a ∈
A[X ]s′ is the result of substituting b for every occurrence of x in a. The ast a is called the target,
and x is called the subject, of the substitution. Substitution is defined by the following equations:
1. [b/x]x = b and [b/x]y = y if x ̸= y.
2. [b/x]o( a1 ; . . . ; an ) = o( [b/x]a1 ; . . . ; [b/x]an ).
For example, we may check that
[num[ 2 ]/x]plus( x ; num[ 3 ] ) = plus( num[ 2 ] ; num[ 3 ] ).
We may prove by structural induction that substitution on asts is well-defined.
Theorem 1.1. If a ∈A[X , x], then for every b ∈A[X ] there exists a unique c ∈A[X ] such that
[b/x]a = c
Proof. By structural induction on a. If a = x, then c = b by definition, otherwise if a = y ̸= x,
then c = y, also by definition. Otherwise, a = o( a1 ; . . . ; an ), and we have by induction unique
c1, . . . , cn such that [b/x]a1 = c1 and ... [b/x]an = cn, and so c is c = o( c1 ; . . . ; cn ), by definition
of substitution.
1.2
Abstract Binding Trees
Abstract binding trees, or abts, enrich asts with the means to introduce new variables and symbols,
called a binding, with a specified range of significance, called its scope. The scope of a binding is an
abt within which the bound identifier can be used, either as a place-holder (in the case of a variable
declaration) or as the index of some operator (in the case of a symbol declaration). Thus the set
of active identifiers can be larger within a subtree of an abt than it is within the surrounding tree.
Moreover, different subtrees may introduce identifiers with disjoint scopes. The crucial principle is
that any use of an identifier should be understood as a reference, or abstract pointer, to its binding.
One consequence is that the choice of identifiers is immaterial, so long as we can always associate
a unique binding with each use of an identifier.
As a motivating example, consider the expression let x be a1 in a2, which introduces a variable
x for use within the expression a2 to stand for the expression a1. The variable x is bound by the
let expression for use within a2; any use of x within a1 refers to a different variable that happens
to have the same name. For example, in the expression let x be 7 in x + x occurrences of x in
the addition refer to the variable introduced by the let. On the other hand in the expression
let x be x ∗x in x + x, occurrences of x within the multiplication refer to a different variable than
those occurring within the addition. The latter occurrences refer to the binding introduced by the
let, whereas the former refer to some outer binding not displayed here.
The names of bound variables are immaterial insofar as they determine the same binding.
So, for example, let x be x ∗x in x + x could just as well have been written let y be x ∗x in y + y,
without changing its meaning. In the former case the variable x is bound within the addition, and

PREVIEW
1.2 Abstract Binding Trees
7
in the latter it is the variable y, but the “pointer structure” remains the same. On the other hand the
expression let x be y ∗y in x + x has a different meaning to these two expressions, because now
the variable y within the multiplication refers to a different surrounding variable. Renaming of
bound variables is constrained to the extent that it must not alter the reference structure of the
expression. For example, the expression
let x be 2 in let y be 3 in x + x
has a different meaning than the expression
let y be 2 in let y be 3 in y + y,
because the y in the expression y + y in the second case refers to the inner declaration, not the
outer one as before.
The concept of an ast can be enriched to account for binding and scope of a variable. These
enriched asts are called abstract binding trees, or abts for short. Abts generalize asts by allowing an
operator to bind any finite number (possibly zero) of variables in each argument. An argument
to an operator is called an abstractor, and has the form x1, . . . , xk . a. The sequence of variables
x1, . . . , xk are bound within the abt a. (When k is zero, we elide the distinction between . a and
a itself.) Written in the form of an abt, the expression let x be a1 in a2 has the form let( a1 ; x .
a2 ), which more clearly specifies that the variable x is bound within a2, and not within a1. We
often write ⃗x to stand for a finite sequence x1, . . . , xn of distinct variables, and write ⃗x . a to mean
x1, . . . , xn . a.
To account for binding, operators are assigned generalized arities of the form (υ1, . . . , υn)s, which
specifies operators of sort s with n arguments of valence υ1, . . . , υn. In general a valence υ has the
form s1, . . . , sk.s, which specifies the sort of an argument as well as the number and sorts of the
variables bound within it. We say that a sequence ⃗x of variables is of sort⃗s to mean that the two
sequences have the same length k and that the variable xi is of sort si for each 1 ≤i ≤k.
Thus, to specify that the operator let has arity (Exp, Exp.Exp)Exp indicates that it is of sort
Exp whose first argument is of sort Exp and binds no variables, and whose second argument
is also of sort Exp, within which is bound one variable of sort Exp.
The informal expression
let x be 2 + 2 in x × x may then be written as the abt
let( plus( num[ 2 ] ; num[ 2 ] ) ; x . times( x ; x ) )
in which the operator let has two arguments, the first of which is an expression, and the second
of which is an abstractor that binds one expression variable.
Fix a set S of sorts, and a family O of disjoint sets of operators indexed by their generalized
arities. For a given family of disjoint sets of variables X , the family of abstract binding trees, or abts
B[X ] is defined similarly to A[X ], except that X is not fixed throughout the definition, but rather
changes as we enter the scopes of abstractors.
This simple idea is surprisingly hard to make precise. A first attempt at the definition is as the
least family of sets closed under the following conditions:
1. If x ∈Xs, then x ∈B[X ]s.

PREVIEW
8
1.2 Abstract Binding Trees
2. For each operator o of arity (⃗s1.s1, . . . ,⃗sn.sn)s, if a1 ∈B[X ,⃗x1]s1, ..., and an ∈B[X ,⃗xn]sn,
then o(⃗x1 . a1 ; . . . ;⃗xn . an ) ∈B[X ]s.
The bound variables are adjoined to the set of active variables within each argument, with the sort
of each variable determined by the valence of the operator.
This definition is almost correct, but fails to properly account for renaming of bound variables.
An abt of the form let( a1 ; x . let( a2 ; x . a3 ) ) is ill-formed according to this definition, because
the first binding adds x to X , which implies that the second cannot also add x to X , x, because it is
not fresh for X , x. The solution is to ensure that each of the arguments is well-formed regardless of
the choice of bound variable names, which is achieved using fresh renamings, which are bijections
between sequences of variables. Specifically, a fresh renaming (relative to X ) of a finite sequence
of variables ⃗x is a bijection ρ : ⃗x ↔⃗x′ between ⃗x and ⃗x′, where ⃗x′ is fresh for X . We write bρ(a) for
the result of replacing each occurrence of xi in a by ρ(xi), its fresh counterpart.
This is achieved by altering the second clause of the definition of abts using fresh renamings as
follows:
For each operator o of arity (⃗s1.s1, . . . ,⃗sn.sn)s, if for each 1 ≤i ≤n and each fresh
renaming ρi : ⃗xi ↔⃗x′
i, we have bρi(ai) ∈B[X ,⃗x′
i], then o(⃗x1 . a1 ; . . . ;⃗xn . an ) ∈B[X ]s.
The renaming, bρi(ai), of each ai ensures that collisions cannot occur, and that the abt is valid for
almost all renamings of any bound variables that occur within it.
The principle of structural induction extends to abts, and is called structural induction modulo
fresh renaming. It states that to show that P[X ](a) holds for every a ∈B[X ], it is enough to show
the following:
1. if x ∈Xs, then P[X ]s(x).
2. For every o of arity (⃗s1.s1, . . . ,⃗sn.sn)s, if for each 1 ≤i ≤n, P[X ,⃗x′
i]si(bρi(ai)) holds for every
ρi : ⃗xi ↔⃗x′
i with ⃗x′
i /∈X , then P[X ]s(o(⃗x1 . a1 ; . . . ;⃗xn . an )).
The second condition ensures that the inductive hypothesis holds for all fresh choices of bound
variable names, and not just the ones actually given in the abt.
As an example let us define the judgment x ∈a, where a ∈B[X , x], to mean that x occurs free
in a. Informally, this means that x is bound somewhere outside of a, rather than within a itself.
If x is bound within a, then those occurrences of x are different from those occurring outside the
binding. The following definition ensures that this is the case:
1. x ∈x.
2. x ∈o(⃗x1 . a1 ; . . . ;⃗xn . an ) if there exists 1 ≤i ≤n such that for every fresh renaming ρ : ⃗xi ↔
⃗zi we have x ∈bρ(ai).
The first condition states that x is free in x, but not free in y for any variable y other than x. The
second condition states that if x is free in some argument, independently of the choice of bound
variable names in that argument, then it is free in the overall abt.
The relation a =α b of α-equivalence (so-called for historical reasons), means that a and b are
identical up to the choice of bound variable names. The α-equivalence relation is the strongest
congruence containing the following two conditions:

PREVIEW
1.2 Abstract Binding Trees
9
1. x =α x.
2. o(⃗x1 . a1 ; . . . ;⃗xn . an ) =α o(⃗x′
1 . a′
1 ; . . . ;⃗x′
n . a′
n ) if for every 1 ≤i ≤n, bρi(ai) =α bρ′
i(a′
i) for all
fresh renamings ρi : ⃗xi ↔⃗zi and ρ′
i : ⃗x′
i ↔⃗zi.
The idea is that we rename ⃗xi and ⃗x′
i consistently, avoiding confusion, and check that ai and a′
i are
α-equivalent. If a =α b, then a and b are α-variants of each other.
Some care is required in the definition of substitution of an abt b of sort s for free occurrences of
a variable x of sort s in some abt a of some sort, written [b/x]a. Substitution is partially defined by
the following conditions:
1. [b/x]x = b, and [b/x]y = y if x ̸= y.
2. [b/x]o(⃗x1 . a1 ; . . . ;⃗xn . an ) = o(⃗x1 . a′
1 ; . . . ;⃗xn . a′
n ), where, for each 1 ≤i ≤n, we require
that ⃗xi /∈b, and we set a′
i = [b/x]ai if x /∈⃗xi, and a′
i = ai otherwise.
The definition of [b/x]a is quite delicate, and merits careful consideration.
One trouble spot for substitution is to notice that if x is bound by an abstractor within a, then
x does not occur free within the abstractor, and hence is unchanged by substitution. For example,
[b/x]let( a1 ; x . a2 ) = let( [b/x]a1 ; x . a2 ), there being no free occurrences of x in x . a2. Another
trouble spot is the capture of a free variable of b during substitution. For example, if y ∈b, and
x ̸= y, then [b/x]let( a1 ; y . a2 ) is undefined, rather than being let( [b/x]a1 ; y . [b/x]a2 ), as one
might at first suspect. For example, provided that x ̸= y, [y/x]let( num[ 0 ] ; y . plus( x ; y ) ) is
undefined, not let( num[ 0 ] ; y . plus( y ; y ) ), which confuses two different variables named y.
Although capture avoidance is an essential characteristic of substitution, it is, in a sense, merely
a technical nuisance. If the names of bound variables have no significance, then capture can always
be avoided by first renaming the bound variables in a to avoid any free variables in b. In the forego-
ing example if we rename the bound variable y to y′ to obtain a′ ≜let( num[ 0 ] ; y′ . plus( x ; y′ ) ),
then [b/x]a′ is defined, and is equal to let( num[ 0 ] ; y′ . plus( b ; y′ ) ). The price for avoiding cap-
ture in this way is that substitution is only determined up to α-equivalence, and so we may no
longer think of substitution as a function, but only as a proper relation.
To restore the functional character of substitution, it is sufficient to adopt the identification con-
vention, which is stated as follows:
Abstract binding trees are always identified up to α-equivalence.
That is, α-equivalent abts are regarded as identical. Substitution can be extended to α-equivalence
classes of abts to avoid capture by choosing representatives of the equivalence classes of b and a
in such a way that substitution is defined, then forming the equivalence class of the result. Any
two choices of representatives for which substitution is defined gives α-equivalent results, so that
substitution becomes a well-defined total function. We will adopt the identification convention for abts
throughout this book.
It will often be necessary to consider languages whose abstract syntax cannot be specified by
a fixed set of operators, but rather requires that the available operators be sensitive to the context
in which they occur. For our purposes it will suffice to consider a set of symbolic parameters, or
symbols, that index families of operators so that as the set of symbols varies, so does the set of

PREVIEW
10
1.3 Notes
operators. An indexed operator o is a family of operators indexed by symbols u, so that o[ u ] is
an operator when u is an available symbol. If U is a finite set of symbols, then B[U ; X ] is the
sort-indexed family of abts that are generated by operators and variables as before, admitting all
indexed operator instances by symbols u ∈U. Whereas a variable is a place-holder that stands for
an unknown abt of its sort, a symbol does not stand for anything, and is not, itself, an abt. The only
significance of symbol is whether it is the same as or differs from another symbol; the operator
instances o[ u ] and o[ u′ ] are the same exactly when u is u′, and are the same symbol.
The set of symbols is extended by introducing a new, or fresh, symbol within a scope using
the abstractor u . a, which binds the symbol u within the abt a. An abstracted symbol is “new”
in the same sense as for an abstracted variable: the name of the bound symbol can be varied at
will provided that no conflicts arise. This renaming property ensures that an abstracted symbol
is distinct from all others in scope. The only difference between symbols and variables is that the
only operation on symbols is renaming; there is no notion of substitution for a symbol.
Finally, a word about notation: to help improve the readability we often “group” and “stage”
the arguments to an operator, using round brackets and braces to show grouping, and generally
regarding stages to progress from right to left. All arguments in a group are considered to occur at
the same stage, though their order is significant, and successive groups are considered to occur in
sequential stages. Staging and grouping is often a helpful mnemonic device, but has no fundamen-
tal significance. For example, the abt o{a1 ; a2}( a3 ; x . a4 ) is the same as the abt o( a1 ; a2 ; a3 ; x . a4 ),
as would be any other order-preserving grouping or staging of its arguments.
1.3
Notes
The concept of abstract syntax has its origins in the pioneering work of Church, Turing, and G¨odel,
who first considered writing programs that act on representations of programs. Originally pro-
grams were represented by natural numbers, using encodings, now called G¨odel-numberings, based
on the prime factorization theorem. Any standard text on mathematical logic, such as Kleene
(1952), has a thorough account of such representations. The Lisp language (McCarthy, 1965; Allen,
1978) introduced a much more practical and direct representation of syntax as symbolic expressions.
These ideas were developed further in the language ML (Gordon et al., 1979), which featured
a type system capable of expressing abstract syntax trees. The AUTOMATH project (Nederpelt
et al., 1994) introduced the idea of using Church’s λ notation (Church, 1941) to account for the
binding and scope of variables. These ideas were developed further in LF (Harper et al., 1993).
The concept of abstract binding trees presented here was inspired by the system of notation
developed in the NuPRL Project, which is described in Constable (1986) and from Martin-L¨of’s
system of arities, which is described in Nordstrom et al. (1990). Their enrichment with symbol
binders is influenced by Pitts and Stark (1993).
Exercises
1.1. Prove by structural induction on abstract syntax trees that if X ⊆Y, then A[X ] ⊆A[Y].

PREVIEW
1.3 Notes
11
1.2. Prove by structural induction modulo renaming on abstract binding trees that if X ⊆Y,
then B[X ] ⊆B[Y].
1.3. Show that if a =α a′ and b =α b′ and both [b/x]a and [b′/x]a′ are defined, then [b/x]a =α
[b′/x]a′.
1.4. Bound variables can be seen as the formal analogs of pronouns in natural languages. The
binding occurrence of a variable at an abstractor fixes a “fresh” pronoun for use within its
body that refers unambiguously to that variable (in contrast to English, in which the referent
of a pronoun can often be ambiguous). This observation suggests an alternative representa-
tion of abts, called abstract binding graphs, or abg’s for short, as directed graphs constructed as
follows:
(a) Free variables are atomic nodes with no outgoing edges.
(b) Operators with n arguments are n-ary nodes, with one outgoing edge directed at each
of their children.
(c) Abstractors are nodes with one edge directed to the scope of the abstracted variable.
(d) Bound variables are back edges directed at the abstractor that introduced it.
Notice that asts, thought of as abts with no abstractors, are acyclic directed graphs (more
precisely, variadic trees), whereas general abts can be cyclic. Draw a few examples of abg’s
corresponding to the example abts given in this chapter. Give a precise definition of the
sort-indexed family G[X ] of abstract binding graphs. What representation would you use
for bound variables (back edges)?

PREVIEW
12
1.3 Notes

PREVIEW
Chapter 2
Inductive Definitions
Inductive definitions are an indispensable tool in the study of programming languages. In this
chapter we will develop the basic framework of inductive definitions, and give some examples of
their use. An inductive definition consists of a set of rules for deriving judgments, or assertions, of a
variety of forms. Judgments are statements about one or more abstract binding trees of some sort.
The rules specify necessary and sufficient conditions for the validity of a judgment, and hence
fully determine its meaning.
2.1
Judgments
We start with the notion of a judgment, or assertion, about an abstract binding tree. We shall make
use of many forms of judgment, including examples such as these:
n nat
n is a natural number
n1 + n2 = n
n is the sum of n1 and n2
τ type
τ is a type
e : τ
expression e has type τ
e ⇓v
expression e has value v
A judgment states that one or more abstract binding trees have a property or stand in some
relation to one another. The property or relation itself is called a judgment form, and the judgment
that an object or objects have that property or stand in that relation is said to be an instance of
that judgment form. A judgment form is also called a predicate, and the objects constituting an
instance are its subjects. We write a J or J a, for the judgment asserting that J holds of the abt
a. Correspondingly, we sometimes notate the judgment form J by −J, or J −, using a dash to
indicate the absence of an argument to J. When it is not important to stress the subject of the
judgment, we write J to stand for an unspecified judgment, that is, an instance of some judgment
form. For particular judgment forms, we freely use prefix, infix, or mix-fix notation, as illustrated
by the above examples, in order to enhance readability.

PREVIEW
14
2.2 Inference Rules
2.2
Inference Rules
An inductive definition of a judgment form consists of a collection of rules of the form
J1
. . .
Jk
J
(2.1)
in which J and J1, . . . , Jk are all judgments of the form being defined. The judgments above the
horizontal line are called the premises of the rule, and the judgment below the line is called its
conclusion. If a rule has no premises (that is, when k is zero), the rule is called an axiom; otherwise
it is called a proper rule.
An inference rule can be read as stating that the premises are sufficient for the conclusion: to
show J, it is enough to show J1, . . . , Jk. When k is zero, a rule states that its conclusion holds
unconditionally. Bear in mind that there may be, in general, many rules with the same conclusion,
each specifying sufficient conditions for the conclusion. Consequently, if the conclusion of a rule
holds, then it is not necessary that the premises hold, for it might have been derived by another
rule.
For example, the following rules form an inductive definition of the judgment form −nat:
zero nat
(2.2a)
a nat
succ( a ) nat
(2.2b)
These rules specify that a nat holds whenever either a is zero, or a is succ( b ) where b nat for some
b. Taking these rules to be exhaustive, it follows that a nat iff a is a natural number.
Similarly, the following rules constitute an inductive definition of the judgment form −tree:
empty tree
(2.3a)
a1 tree
a2 tree
node( a1 ; a2 ) tree
(2.3b)
These rules specify that a tree holds if either a is empty, or a is node( a1 ; a2 ), where a1 tree and
a2 tree. Taking these to be exhaustive, these rules state that a is a binary tree, which is to say it is
either empty, or a node consisting of two children, each of which is also a binary tree.
The judgment form a is b expressing the equality of two abts a and b such that a nat and b nat
is inductively defined by the following rules:
zero is zero
(2.4a)
a is b
succ( a ) is succ( b )
(2.4b)

PREVIEW
2.3 Derivations
15
In each of the preceding examples we have made use of a notational convention for specifying
an infinite family of rules by a finite number of patterns, or rule schemes. For example, rule (2.2b)
is a rule scheme that determines one rule, called an instance of the rule scheme, for each choice of
object a in the rule. We will rely on context to determine whether a rule is stated for a specific object
a or is instead intended as a rule scheme specifying a rule for each choice of objects in the rule.
A collection of rules is considered to define the strongest judgment form that is closed under, or
respects, those rules. To be closed under the rules simply means that the rules are sufficient to show
the validity of a judgment: J holds if there is a way to obtain it using the given rules. To be the
strongest judgment form closed under the rules means that the rules are also necessary: J holds only
if there is a way to obtain it by applying the rules. The sufficiency of the rules means that we may
show that J holds by deriving it by composing rules. Their necessity means that we may reason
about it using rule induction.
2.3
Derivations
To show that an inductively defined judgment holds, it is enough to exhibit a derivation of it. A
derivation of a judgment is a finite composition of rules, starting with axioms and ending with
that judgment. It can be thought of as a tree in which each node is a rule whose children are
derivations of its premises. We sometimes say that a derivation of J is evidence for the validity of
an inductively defined judgment J.
We usually depict derivations as trees with the conclusion at the bottom, and with the children
of a node corresponding to a rule appearing above it as evidence for the premises of that rule.
Thus, if
J1
. . .
Jk
J
is an inference rule and `
1, . . . , `
k are derivations of its premises, then
`
1
. . .
`
k
J
is a derivation of its conclusion. In particular, if k = 0, then the node has no children.
For example, this is a derivation of succ( succ( succ( zero ) ) ) nat:
zero nat
succ( zero ) nat
succ( succ( zero ) ) nat
succ( succ( succ( zero ) ) ) nat
.
(2.5)

PREVIEW
16
2.4 Rule Induction
Similarly, here is a derivation of node( node( empty ; empty ) ; empty ) tree:
empty tree
empty tree
node( empty ; empty ) tree
empty tree
node( node( empty ; empty ) ; empty ) tree
.
(2.6)
To show that an inductively defined judgment is derivable we need only find a derivation
for it. There are two main methods for finding derivations, called forward chaining, or bottom-
up construction, and backward chaining, or top-down construction. Forward chaining starts with the
axioms and works forward towards the desired conclusion, whereas backward chaining starts
with the desired conclusion and works backwards towards the axioms.
More precisely, forward chaining search maintains a set of derivable judgments, and continu-
ally extends this set by adding to it the conclusion of any rule all of whose premises are in that
set. Initially, the set is empty; the process terminates when the desired judgment occurs in the
set. Assuming that all rules are considered at every stage, forward chaining will eventually find
a derivation of any derivable judgment, but it is impossible (in general) to decide algorithmically
when to stop extending the set and conclude that the desired judgment is not derivable. We may
go on and on adding more judgments to the derivable set without ever achieving the intended
goal. It is a matter of understanding the global properties of the rules to determine that a given
judgment is not derivable.
Forward chaining is undirected in the sense that it does not take account of the end goal when
deciding how to proceed at each step. In contrast, backward chaining is goal-directed. Back-
ward chaining search maintains a queue of current goals, judgments whose derivations are to be
sought. Initially, this set consists solely of the judgment we wish to derive. At each stage, we
remove a judgment from the queue, and consider all rules whose conclusion is that judgment.
For each such rule, we add the premises of that rule to the back of the queue, and continue. If
there is more than one such rule, this process must be repeated, with the same starting queue, for
each candidate rule. The process terminates whenever the queue is empty, all goals having been
achieved; any pending consideration of candidate rules along the way can be discarded. As with
forward chaining, backward chaining will eventually find a derivation of any derivable judgment,
but there is, in general, no algorithmic method for determining in general whether the current
goal is derivable. If it is not, we may futilely add more and more judgments to the goal set, never
reaching a point at which all goals have been satisfied.
2.4
Rule Induction
Because an inductive definition specifies the strongest judgment form closed under a collection of
rules, we may reason about them by rule induction. The principle of rule induction states that to
show that a property a P holds whenever a J is derivable, it is enough to show that P is closed
under, or respects, the rules defining the judgment form J. More precisely, the property P respects
the rule
a1 J
. . .
ak J
a J

PREVIEW
2.4 Rule Induction
17
if P(a) holds whenever P(a1), . . . , P(ak) do. The assumptions P(a1), . . . , P(ak) are called the
inductive hypotheses, and P(a) is called the inductive conclusion of the inference.
The principle of rule induction is simply the expression of the definition of an inductively
defined judgment form as the strongest judgment form closed under the rules comprising the def-
inition. Thus, the judgment form defined by a set of rules is both (a) closed under those rules,
and (b) sufficient for any other property also closed under those rules. The former means that a
derivation is evidence for the validity of a judgment; the latter means that we may reason about
an inductively defined judgment form by rule induction.
When specialized to rules (2.2), the principle of rule induction states that to show P(a) when-
ever a nat, it is enough to show:
1. P(zero).
2. for every a, if P(a), then P(succ( a )).
The sufficiency of these conditions is the familiar principle of mathematical induction.
Similarly, rule induction for rules (2.3) states that to show P(a) whenever a tree, it is enough to
show
1. P(empty).
2. for every a1 and a2, if P(a1), and if P(a2), then P(node( a1 ; a2 )).
The sufficiency of these conditions is called the principle of tree induction.
We may also show by rule induction that the predecessor of a natural number is also a natural
number. Although this may seem self-evident, the point of the example is to show how to derive
this from first principles.
Lemma 2.1. If succ( a ) nat, then a nat.
Proof. Define P(a) to mean that if a = succ( b ), then b nat. It suffices to show that P is closed
under rules (2.2).
Rule (2.2a) Vacuous, because zero is not of the form succ( −).
Rule (2.2b) The premise of the rule ensures that b nat when a = succ( b ).
Using rule induction we may show that equality, as defined by rules (2.4) is reflexive.
Lemma 2.2. If a nat, then a is a.
Proof. By rule induction on rules (2.2):
Rule (2.2a) Applying rule (2.4a) we obtain zero is zero.
Rule (2.2b) Assume that a is a. It follows that succ( a ) is succ( a ) by an application of rule (2.4b).

PREVIEW
18
2.5 Iterated and Simultaneous Inductive Definitions
Similarly, we may show that the successor operation is injective.
Lemma 2.3. If succ( a1 ) is succ( a2 ), then a1 is a2.
Proof. Similar to the proof of Lemma 2.1.
2.5
Iterated and Simultaneous Inductive Definitions
Inductive definitions are often iterated, meaning that one inductive definition builds on top of
another. In an iterated inductive definition the premises of a rule
J1
. . .
Jk
J
may be instances of either a previously defined judgment form, or the judgment form being de-
fined. For example, the following rules define the judgment form −list, which states that a is a list
of natural numbers:
nil list
(2.7a)
a nat
b list
cons( a ; b ) list
(2.7b)
The first premise of rule (2.7b) is an instance of the judgment form a nat, which was defined
previously, whereas the premise b list is an instance of the judgment form being defined by these
rules.
Frequently two or more judgments are defined at once by a simultaneous inductive definition.
A simultaneous inductive definition consists of a set of rules for deriving instances of several
different judgment forms, any of which may appear as the premise of any rule. Because the rules
defining each judgment form may involve any of the others, none of the judgment forms can be
taken to be defined prior to the others. Instead we must understand that all of the judgment forms
are being defined at once by the entire collection of rules. The judgment forms defined by these
rules are, as before, the strongest judgment forms that are closed under the rules. Therefore the
principle of proof by rule induction continues to apply, albeit in a form that requires us to prove a
property of each of the defined judgment forms simultaneously.
For example, consider the following rules, which constitute a simultaneous inductive defini-
tion of the judgments a even, stating that a is an even natural number, and a odd, stating that a is
an odd natural number:
zero even
(2.8a)
b odd
succ( b ) even
(2.8b)
a even
succ( a ) odd
(2.8c)

PREVIEW
2.6 Defining Functions by Rules
19
The principle of rule induction for these rules states that to show simultaneously that P(a)
whenever a even and Q(b) whenever b odd, it is enough to show the following:
1. P(zero);
2. if Q(b), then P(succ( b ));
3. if P(a), then Q(succ( a )).
As an example, we may use simultaneous rule induction to prove that (1) if a even, then either
a is zero or a is succ( b ) with b odd, and (2) if a odd, then a is succ( b ) with b even. We define P(a)
to hold iff a is zero or a is succ( b ) for some b with b odd, and define Q(b) to hold iff b is succ( a )
for some a with a even. The desired result follows by rule induction, because we can prove the
following facts:
1. P(zero), which holds because zero is zero.
2. If Q(b), then succ( b ) is succ( b′ ) for some b′ with Q(b′). Take b′ to be b and apply the
inductive assumption.
3. If P(a), then succ( a ) is succ( a′ ) for some a′ with P(a′). Take a′ to be a and apply the
inductive assumption.
2.6
Defining Functions by Rules
A common use of inductive definitions is to define a function by giving an inductive definition of
its graph relating inputs to outputs, and then showing that the relation uniquely determines the
outputs for given inputs. For example, we may define the addition function on natural numbers
as the relation sum(a ; b ; c), with the intended meaning that c is the sum of a and b, as follows:
b nat
sum(zero ; b ; b)
(2.9a)
sum(a ; b ; c)
sum(succ( a ) ; b ; succ( c ))
(2.9b)
The rules define a ternary (three-place) relation sum(a ; b ; c) among natural numbers a, b, and c.
We may show that c is determined by a and b in this relation.
Theorem 2.4. For every a nat and b nat, there exists a unique c nat such that sum(a ; b ; c).
Proof. The proof decomposes into two parts:
1. (Existence) If a nat and b nat, then there exists c nat such that sum(a ; b ; c).
2. (Uniqueness) If sum(a ; b ; c), and sum(a ; b ; c′), then c is c′.
For existence, let P(a) be the proposition if b nat then there exists c nat such that sum(a ; b ; c). We
prove that if a nat then P(a) by rule induction on rules (2.2). We have two cases to consider:

PREVIEW
20
2.7 Notes
Rule (2.2a) We are to show P(zero). Assuming b nat and taking c to be b, we obtain sum(zero ;
b ; c) by rule (2.9a).
Rule (2.2b) Assuming P(a), we are to show P(succ( a )). That is, we assume that if b nat then
there exists c such that sum(a ; b ; c), and are to show that if b′ nat, then there exists c′ such that
sum(succ( a ) ; b′ ; c′). To this end, suppose that b′ nat. Then by induction there exists c such
that sum(a ; b′ ; c). Taking c′ to be succ( c ), and applying rule (2.9b), we obtain sum(succ( a ) ;
b′ ; c′), as required.
For uniqueness, we prove that if sum(a ; b ; c1), then if sum(a ; b ; c2), then c1 is c2 by rule induction
based on rules (2.9).
Rule (2.9a) We have a is zero and c1 is b. By an inner induction on the same rules, we may show
that if sum(zero ; b ; c2), then c2 is b. By Lemma 2.2 we obtain b is b.
Rule (2.9b) We have that a is succ( a′ ) and c1 is succ( c′
1 ), where sum(a′ ; b ; c′
1). By an inner
induction on the same rules, we may show that if sum(a ; b ; c2), then c2 is succ( c′
2 ) where
sum(a′ ; b ; c′
2). By the outer inductive hypothesis c′
1 is c′
2 and so c1 is c2.
2.7
Notes
Aczel (1977) provides a thorough account of the theory of inductive definitions on which the
present account is based. A significant difference is that we consider inductive definitions of
judgments over abts as defined in Chapter 1, rather than with natural numbers. The emphasis
on judgments is inspired by Martin-L¨of’s logic of judgments (Martin-L¨of, 1983, 1987).
Exercises
2.1. Give an inductive definition of the judgment max(m ; n ; p), where m nat, n nat, and p nat,
with the meaning that p is the larger of m and n. Prove that every m and n are related to a
unique p by this judgment.
2.2. Consider the following rules, which define the judgment hgt(t ; n) stating that the binary tree
t has height n.
hgt(empty ; zero)
(2.10a)
hgt(t1 ; n1)
hgt(t2 ; n2)
max(n1 ; n2 ; n)
hgt(node( t1 ; t2 ) ; succ( n ))
(2.10b)
Prove that the judgment hgt defines a function from trees to natural numbers.

PREVIEW
2.7 Notes
21
2.3. Given an inductive definition of ordered variadic trees whose nodes have a finite, but variable,
number of children with a specified left-to-right ordering among them. Your solution should
consist of a simultaneous definition of two judgments, t tree, stating that t is a variadic tree,
and f forest, stating that f is a “forest” (finite sequence) of variadic trees.
2.4. Give an inductive definition of the height of a variadic tree of the kind defined in Exercise 2.3.
Your definition should make use of an auxiliary judgment defining the height of a forest of
variadic trees, and will be defined simultaneously with the height of a variadic tree. Show
that the two judgments so defined each define a function.
2.5. Give an inductive definition of the binary natural numbers, which are either zero, twice a
binary number, or one more than twice a binary number. The size of such a representation is
logarithmic, rather than linear, in the natural number it represents.
2.6. Give an inductive definition of addition of binary natural numbers as defined in Exercise 2.5.
Hint: Proceed by analyzing both arguments to the addition, and make use of an auxiliary
function to compute the successor of a binary number. Hint: Alternatively, define both the
sum and the sum-plus-one of two binary numbers mutually recursively.

PREVIEW
22
2.7 Notes

PREVIEW
Chapter 3
Hypothetical and General Judgments
A hypothetical judgment expresses an entailment between one or more hypotheses and a conclusion.
We will consider two notions of entailment, called derivability and admissibility.
Both express a
form of entailment, but they differ in that derivability is stable under extension with new rules,
admissibility is not. A general judgment expresses the universality, or genericity, of a judgment.
There are two forms of general judgment, the generic and the parametric. The generic judgment
expresses generality with respect to all substitution instances for variables in a judgment. The
parametric judgment expresses generality with respect to renamings of symbols.
3.1
Hypothetical Judgments
The hypothetical judgment codifies the rules for expressing the validity of a conclusion conditional
on the validity of one or more hypotheses. There are two forms of hypothetical judgment that
differ according to the sense in which the conclusion is conditional on the hypotheses. One is
stable under extension with more rules, and the other is not.
3.1.1
Derivability
For a given set R of rules, we define the derivability judgment, written J1, . . . , Jk ⊢R K, where each
Ji and K are basic judgments, to mean that we may derive K from the expansion R ∪{ J1, . . . , Jk } of
the rules R with the axioms
J1
. . .
Jk
.
We treat the hypotheses, or antecedents, of the judgment, J1, . . . , Jk as “temporary axioms”, and de-
rive the conclusion, or consequent, by composing rules in R. Thus, evidence for a hypothetical
judgment consists of a derivation of the conclusion from the hypotheses using the rules in R.
We use capital Greek letters, usually Γ or ∆, to stand for a finite set of basic judgments, and
write R ∪Γ for the expansion of R with an axiom corresponding to each judgment in Γ. The

PREVIEW
24
3.1 Hypothetical Judgments
judgment Γ ⊢R K means that K is derivable from rules R ∪Γ, and the judgment ⊢R Γ means that
⊢R J for each J in Γ. An equivalent way of defining J1, . . . , Jn ⊢R J is to say that the rule
J1
. . .
Jn
J
(3.1)
is derivable from R, which means that there is a derivation of J composed of the rules in R aug-
mented by treating J1, . . . , Jn as axioms.
For example, consider the derivability judgment
a nat ⊢(2.2) succ( succ( a ) ) nat
(3.2)
relative to rules (2.2). This judgment is valid for any choice of object a, as shown by the derivation
a nat
succ( a ) nat
succ( succ( a ) ) nat
(3.3)
which composes rules (2.2), starting with a nat as an axiom, and ending with succ( succ( a ) ) nat.
Equivalently, the validity of (3.2) may also be expressed by stating that the rule
a nat
succ( succ( a ) ) nat
(3.4)
is derivable from rules (2.2).
It follows directly from the definition of derivability that it is stable under extension with new
rules.
Theorem 3.1 (Stability). If Γ ⊢R J, then Γ ⊢R∪R′ J.
Proof. Any derivation of J from R ∪Γ is also a derivation from (R ∪R′) ∪Γ, because any rule in
R is also a rule in R ∪R′.
Derivability enjoys a number of structural properties that follow from its definition, indepen-
dently of the rules R in question.
Reflexivity Every judgment is a consequence of itself: Γ, J ⊢R J. Each hypothesis justifies itself as
conclusion.
Weakening If Γ ⊢R J, then Γ, K ⊢R J. Entailment is not influenced by un-exercised options.
Transitivity If Γ, K ⊢R J and Γ ⊢R K, then Γ ⊢R J. If we replace an axiom by a derivation of it,
the result is a derivation of its consequent without that hypothesis.
Reflexivity follows directly from the meaning of derivability. Weakening follows directly from the
definition of derivability. Transitivity is proved by rule induction on the first premise.

PREVIEW
3.1 Hypothetical Judgments
25
3.1.2
Admissibility
Admissibility, written Γ |=R J, is a weaker form of hypothetical judgment stating that ⊢R Γ implies
⊢R J. That is, the conclusion J is derivable from rules R when the assumptions Γ are all derivable
from rules R. In particular if any of the hypotheses are not derivable relative to R, then the
judgment is vacuously true. An equivalent way to define the judgment J1, . . . , Jn |=R J is to state
that the rule
J1
. . .
Jn
J
(3.5)
is admissible relative to the rules in R. Given any derivations of J1, . . . , Jn using the rules in R, we
may build a derivation of J using the rules in R.
For example, the admissibility judgment
succ( a ) even |=(2.8) a odd
(3.6)
is valid, because any derivation of succ( a ) even from rules (2.8) must contain a sub-derivation of
a odd from the same rules, which justifies the conclusion. This fact can be proved by induction on
rules (2.8). That judgment (3.6) is valid may also be expressed by saying that the rule
succ( a ) even
a odd
(3.7)
is admissible relative to rules (2.8).
In contrast to derivability the admissibility judgment is not stable under extension to the rules.
For example, if we enrich rules (2.8) with the axiom
succ( zero ) even
,
(3.8)
then rule (3.6) is inadmissible, because there is no composition of rules deriving zero odd. Admis-
sibility is as sensitive to which rules are absent from an inductive definition as it is to which rules
are present in it.
The structural properties of derivability ensure that derivability is stronger than admissibility.
Theorem 3.2. If Γ ⊢R J, then Γ |=R J.
Proof. Repeated application of the transitivity of derivability shows that if Γ ⊢R J and ⊢R Γ, then
⊢R J.
To see that the converse fails, note that
succ( zero ) even ̸⊢(2.8) zero odd,
because there is no derivation of the right-hand side when the left-hand side is added as an axiom
to rules (2.8). Yet the corresponding admissibility judgment
succ( zero ) even |=(2.8) zero odd

PREVIEW
26
3.2 Hypothetical Inductive Definitions
is valid, because the hypothesis is false: there is no derivation of succ( zero ) even from rules (2.8).
Even so, the derivability
succ( zero ) even ⊢(2.8) succ( succ( zero ) ) odd
is valid, because we may derive the right-hand side from the left-hand side by composing rules (2.8).
Evidence for admissibility can be thought of as a mathematical function transforming deriva-
tions ▽1, . . . , ▽n of the hypotheses into a derivation ▽of the consequent. Therefore, the admissi-
bility judgment enjoys the same structural properties as derivability, and hence is a form of hypo-
thetical judgment:
Reflexivity If J is derivable from the original rules, then J is derivable from the original rules:
J |=R J.
Weakening If J is derivable from the original rules assuming that each of the judgments in Γ are
derivable from these rules, then J must also be derivable assuming that Γ and K are derivable
from the original rules: if Γ |=R J, then Γ, K |=R J.
Transitivity If Γ, K |=R J and Γ |=R K, then Γ |=R J. If the judgments in Γ are derivable, so is K,
by assumption, and hence so are the judgments in Γ, K, and hence so is J.
Theorem 3.3. The admissibility judgment Γ |=R J enjoys the structural properties of entailment.
Proof. Follows immediately from the definition of admissibility as stating that if the hypotheses
are derivable relative to R, then so is the conclusion.
If a rule r is admissible with respect to a rule set R, then ⊢R,r J is equivalent to ⊢R J. For if
⊢R J, then obviously ⊢R,r J, by simply disregarding r. Conversely, if ⊢R,r J, then we may replace
any use of r by its expansion in terms of the rules in R. It follows by rule induction on R, r that
every derivation from the expanded set of rules R, r can be transformed into a derivation from R
alone. Consequently, if we wish to prove a property of the judgments derivable from R, r, when
r is admissible with respect to R, it suffices show that the property is closed under rules R alone,
because its admissibility states that the consequences of rule r are implicit in those of rules R.
3.2
Hypothetical Inductive Definitions
It is useful to enrich the concept of an inductive definition to allow rules with derivability judg-
ments as premises and conclusions. Doing so lets us introduce local hypotheses that apply only
in the derivation of a particular premise, and also allows us to constrain inferences based on the
global hypotheses in effect at the point where the rule is applied.
A hypothetical inductive definition consists of a set of hypothetical rules of the following form:
Γ Γ1 ⊢J1
. . .
Γ Γn ⊢Jn
Γ ⊢J
.
(3.9)

PREVIEW
3.2 Hypothetical Inductive Definitions
27
The hypotheses Γ are the global hypotheses of the rule, and the hypotheses Γi are the local hypotheses
of the ith premise of the rule. Informally, this rule states that J is a derivable consequence of Γ when
each Ji is a derivable consequence of Γ, augmented with the hypotheses Γi. Thus, one way to show
that J is derivable from Γ is to show, in turn, that each Ji is derivable from Γ Γi. The derivation
of each premise involves a “context switch” in which we extend the global hypotheses with the
local hypotheses of that premise, establishing a new set of global hypotheses for use within that
derivation.
We require that all rules in a hypothetical inductive definition be uniform in the sense that they
are applicable in all global contexts. Uniformity ensures that a rule can be presented in implicit, or
local form,
Γ1 ⊢J1
. . .
Γn ⊢Jn
J
,
(3.10)
in which the global context has been suppressed with the understanding that the rule applies for
any choice of global hypotheses.
A hypothetical inductive definition is to be regarded as an ordinary inductive definition of a
formal derivability judgment Γ ⊢J consisting of a finite set of basic judgments Γ and a basic judgment
J. A set of hypothetical rules R defines the strongest formal derivability judgment that is structural
and closed under uniform rules R. Structurality means that the formal derivability judgment must
be closed under the following rules:
Γ, J ⊢J
(3.11a)
Γ ⊢J
Γ, K ⊢J
(3.11b)
Γ ⊢K
Γ, K ⊢J
Γ ⊢J
(3.11c)
These rules ensure that formal derivability behaves like a hypothetical judgment. We write Γ ⊢R J
to mean that Γ ⊢J is derivable from rules R.
The principle of hypothetical rule induction is just the principle of rule induction applied to the
formal hypothetical judgment. So to show that P(Γ ⊢J) when Γ ⊢R J, it is enough to show that P
is closed under the rules of R and under the structural rules.1 Thus, for each rule of the form (3.9),
whether structural or in R, we must show that
if P(Γ Γ1 ⊢J1) and . . . and P(Γ Γn ⊢Jn), then P(Γ ⊢J).
But this is just a restatement of the principle of rule induction given in Chapter 2, specialized to
the formal derivability judgment Γ ⊢J.
In practice we usually dispense with the structural rules by the method described in Sec-
tion 3.1.2. By proving that the structural rules are admissible any proof by rule induction may
restrict attention to the rules in R alone. If all rules of a hypothetical inductive definition are uni-
form, the structural rules (3.11b) and (3.11c) are clearly admissible. Usually, rule (3.11a) must be
postulated explicitly as a rule, rather than shown to be admissible on the basis of the other rules.
1Writing P(Γ ⊢J) is a mild abuse of notation in which the turnstile is used to separate the two arguments to P for the
sake of readability.

PREVIEW
28
3.3 General Judgments
3.3
General Judgments
General judgments codify the rules for handling variables in a judgment. As in mathematics in
general, a variable is treated as an unknown ranging over a specified set of objects. A generic judg-
ment states that a judgment holds for any choice of objects replacing designated variables in the
judgment. Another form of general judgment codifies the handling of symbolic parameters. A
parametric judgment expresses generality over any choice of fresh renamings of designated sym-
bols of a judgment. To keep track of the active variables and symbols in a derivation, we write
Γ ⊢U;X
R
J to say that J is derivable from Γ according to rules R, with objects consisting of abts over
symbols U and variables X .
The concept of uniformity of a rule must be extended to require that rules be closed under re-
naming and substitution for variables and closed under renaming for parameters. More precisely, if R
is a set of rules containing a free variable x of sort s then it must also contain all possible substitu-
tion instances of abts a of sort s for x, including those that contain other free variables. Similarly,
if R contains rules with a parameter u, then it must contain all instances of that rule obtained by
renaming u of a sort to any u′ of the same sort. Uniformity rules out stating a rule for a variable,
without also stating it all instances of that variable. It also rules out stating a rule for a parameter
without stating it for all possible renamings of that parameter.
Generic derivability judgment is defined by
Y | Γ ⊢X
R J
iff
Γ ⊢X Y
R
J,
where Y ∩X = ∅. Evidence for generic derivability consists of a generic derivation ▽involving the
variables X Y. So long as the rules are uniform, the choice of Y does not matter, in a sense to be
explained shortly.
For example, the generic derivation ▽,
x nat
succ( x ) nat
succ( succ( x ) ) nat
,
is evidence for the judgment
x | x nat ⊢X
(2.2) succ( succ( x ) ) nat
provided x /∈X . Any other choice of x would work just as well, as long as all rules are uniform.
The generic derivability judgment enjoys the following structural properties governing the be-
havior of variables, provided that R is uniform.
Proliferation If Y | Γ ⊢X
R J, then Y, y | Γ ⊢X
R J.
Renaming If Y, y | Γ ⊢X
R J, then Y, y′ | [y ↔y′]Γ ⊢X
R [y ↔y′]J for any y′ /∈X Y.
Substitution If Y, y | Γ ⊢X
R J and a ∈B[X Y], then Y | [a/y]Γ ⊢X
R [a/y]J.

PREVIEW
3.4 Generic Inductive Definitions
29
Proliferation is guaranteed by the interpretation of rule schemes as ranging over all expansions of
the universe. Renaming is built into the meaning of the generic judgment. It is left implicit in the
principle of substitution that the substituting abt is of the same sort as the substituted variable.
Parametric derivability is defined analogously to generic derivability, albeit by generalizing
over symbols, rather than variables. Parametric derivability is defined by
V ∥Y | Γ ⊢U;X
R
J
iff
Y | Γ ⊢U V;X
R
J,
where V ∩U = ∅. Evidence for parametric derivability consists of a derivation ▽involving the
symbols V. Uniformity of R ensures that any choice of parameter names is as good as any other;
derivability is stable under renaming.
3.4
Generic Inductive Definitions
A generic inductive definition admits generic hypothetical judgments in the premises of rules, with
the effect of augmenting the variables, as well as the rules, within those premises. A generic rule
has the form
Y Y1 | Γ Γ1 ⊢J1
. . .
Y Yn | Γ Γn ⊢Jn
Y | Γ ⊢J
.
(3.12)
The variables Y are the global variables of the inference, and, for each 1 ≤i ≤n, the variables Yi are
the local variables of the ith premise. In most cases a rule is stated for all choices of global variables
and global hypotheses. Such rules can be given in implicit form,
Y1 | Γ1 ⊢J1
. . .
Yn | Γn ⊢Jn
J
.
(3.13)
A generic inductive definition is just an ordinary inductive definition of a family of formal
generic judgments of the form Y | Γ ⊢J. Formal generic judgments are identified up to renaming
of variables, so that the latter judgment is treated as identical to the judgment Y′ | bρ(Γ) ⊢bρ(J) for
any renaming ρ : Y ↔Y′. If R is a collection of generic rules, we write Y | Γ ⊢R J to mean that
the formal generic judgment Y | Γ ⊢J is derivable from rules R.
When specialized to a set of generic rules, the principle of rule induction states that to show
P(Y | Γ ⊢J) when Y | Γ ⊢R J, it is enough to show that P is closed under the rules R. Specifically,
for each rule in R of the form (3.12), we must show that
if P(Y Y1 | Γ Γ1 ⊢J1) . . . P(Y Yn | Γ Γn ⊢Jn) then P(Y | Γ ⊢J).
By the identification convention (stated in Chapter 1) the property P must respect renamings of
the variables in a formal generic judgment.
To ensure that the formal generic judgment behaves like a generic judgment, we must always
ensure that the following structural rules are admissible:
Y | Γ, J ⊢J
(3.14a)

PREVIEW
30
3.5 Notes
Y | Γ ⊢J
Y | Γ, J′ ⊢J
(3.14b)
Y | Γ ⊢J
Y, x | Γ ⊢J
(3.14c)
Y, x′ | [x ↔x′]Γ ⊢[x ↔x′J]
Y, x | Γ ⊢J
(3.14d)
Y | Γ ⊢J
Y | Γ, J ⊢J′
Y | Γ ⊢J′
(3.14e)
Y, x | Γ ⊢J
a ∈B[Y]
Y | [a/x]Γ ⊢[a/x]J
(3.14f)
The admissibility of rule (3.14a) is, in practice, ensured by explicitly including it. The admissibility
of rules (3.14b) and (3.14c) is assured if each of the generic rules is uniform, because we may
assimilate the added variable x to the global variables, and the added hypothesis J, to the global
hypotheses. The admissibility of rule (3.14d) is ensured by the identification convention for the
formal generic judgment. Rule (3.14f) must be verified explicitly for each inductive definition.
The concept of a generic inductive definition extends to parametric judgments as well. Briefly,
rules are defined on formal parametric judgments of the form V ∥Y | Γ ⊢J, with symbols V, as
well as variables, Y. Such formal judgments are identified up to renaming of its variables and its
symbols to ensure that the meaning is independent of the choice of variable and symbol names.
3.5
Notes
The concepts of entailment and generality are fundamental to logic and programming languages.
The formulation given here builds on Martin-L¨of (1983, 1987) and Avron (1991). Hypothetical and
general reasoning are consolidated into a single concept in the AUTOMATH languages (Nederpelt
et al., 1994) and in the LF Logical Framework (Harper et al., 1993). These systems allow arbitrarily
nested combinations of hypothetical and general judgments, whereas the present account con-
siders only general hypothetical judgments over basic judgment forms. On the other hand we
consider here symbols, as well as variables, which are not present in these previous accounts.
Parametric judgments are required for specifying languages that admit the dynamic creation of
“new” objects (see Chapter 34).
Exercises
3.1. Combinators are inductively defined by the rule set C given as follows:
s comb
(3.15a)

PREVIEW
3.5 Notes
31
k comb
(3.15b)
a1 comb
a2 comb
ap( a1 ; a2 ) comb
(3.15c)
Give an inductive definition of the length of a combinator defined as the number of occur-
rences of S and K within it.
3.2. The general judgment
x1, . . . , xn | x1 comb, . . . , xn comb ⊢C A comb
states that A is a combinator that may involve the variables x1, . . . , xn. Prove that if x |
x comb ⊢C a2 comb and a1 comb, then [a1/x]a2 comb by induction on the derivation of the
first hypothesis of the implication.
3.3. Conversion, or equivalence, of combinators is expressed by the judgment A ≡B defined by
the rule set E extending C as follows:2
a comb
a ≡a
(3.16a)
a2 ≡a1
a1 ≡a2
(3.16b)
a1 ≡a2
a2 ≡a3
a1 ≡a3
(3.16c)
a1 ≡a′
1
a2 ≡a′
2
a1 a2 ≡a′
1 a′
2
(3.16d)
a1 comb
a2 comb
k a1 a2 ≡a1
(3.16e)
a1 comb
a2 comb
a3 comb
s a1 a2 a3 ≡( a1 a3 ) ( a2 a3 )
(3.16f)
The no-doubt mysterious motivation for the last two equations will become clearer in a mo-
ment. For now, show that
x | x comb ⊢C∪E s k k x ≡x.
3.4. Show that if x | x comb ⊢C a comb, then there is a combinator a′, written [ x ] a and called
bracket abstraction, such that
x | x comb ⊢C∪E a′ x ≡a.
Consequently, by Exercise 3.2, if a′′ comb, then
( [ x ] a ) a′′ ≡[a′′/x]a.
2The combinator ap( a1 ; a2 ) is written a1 a2 for short, left-associatively when used in succession.

PREVIEW
32
3.5 Notes
Hint: Inductively define the judgment
x | x comb ⊢absx a is a′,
where x | x comb ⊢a comb. Then argue that it defines a′ as a binary function of x and
a. The motivation for the conversion axioms governing k and s should become clear while
developing the proof of the desired equivalence.
3.5. Prove that bracket abstraction, as defined in Exercise 3.4, is non-compositional by exhibiting a
and b such that a comb and
x y | x comb y comb ⊢C b comb
such that [a/y]([ x ] b) ̸= [ x ] ([a/y]b). Hint: Consider the case that b is y.
Suggest a modification to the definition of bracket abstraction that is compositional by show-
ing under the same conditions given above that
[a/y]([ x ] b) = [ x ] ([a/y]b).
3.6. Consider the set B[X ] of abts generated by the operators ap, with arity (Exp, Exp)Exp, and
λ, with arity (Exp.Exp)Exp, and possibly involving variables in X , all of which are of sort
Exp. Give an inductive definition of the judgment b closed, which specifies that b has no free
occurrences of the variables in X . Hint: it is essential to give an inductive definition of the
hypothetical, general judgment
x1, . . . , xn | x1 closed, . . . , xn closed ⊢b closed
in order to account for the binding of a variable by the λ operator. The hypothesis that a
variable is closed seems self-contradictory in that a variable obviously occurs free in itself.
Explain why this is not the case by examining carefully the meaning of the hypothetical and
general judgments.

PREVIEW
Part II
Statics and Dynamics

PREVIEW

PREVIEW
Chapter 4
Statics
Most programming languages exhibit a phase distinction between the static and dynamic phases of
processing. The static phase consists of parsing and type checking to ensure that the program is
well-formed; the dynamic phase consists of execution of well-formed programs. A language is
said to be safe exactly when well-formed programs are well-behaved when executed.
The static phase is specified by a statics comprising a set of rules for deriving typing judgments
stating that an expression is well-formed of a certain type. Types mediate the interaction between
the constituent parts of a program by “predicting” some aspects of the execution behavior of the
parts so that we may ensure they fit together properly at run-time. Type safety tells us that these
predictions are correct; if not, the statics is considered to be improperly defined, and the language
is deemed unsafe for execution.
In this chapter we present the statics of a simple expression language, E, as an illustration of
the method that we will employ throughout this book.
4.1
Syntax
When defining a language we shall be primarily concerned with its abstract syntax, specified by a
collection of operators and their arities. The abstract syntax provides a systematic, unambiguous
account of the hierarchical and binding structure of the language, and is considered the official
presentation of the language. However, for the sake of clarity, it is also useful to specify minimal
concrete syntax conventions, without going through the trouble to set up a fully precise grammar
for it.
We will accomplish both of these purposes with a syntax chart, whose meaning is best illus-

PREVIEW
36
4.2 Type System
trated by example. The following chart summarizes the abstract and concrete syntax of E.
Typ
τ
::=
num
num
numbers
str
str
strings
Exp
e
::=
x
x
variable
num[ n ]
n
numeral
str[ s ]
”s”
literal
plus( e1 ; e2 )
e1 + e2
addition
times( e1 ; e2 )
e1 ∗e2
multiplication
cat( e1 ; e2 )
e1 ^ e2
concatenation
len( e )
|e|
length
let( e1 ; x . e2 )
let x be e1 in e2
definition
This chart defines two sorts, Typ, ranged over by τ, and Exp, ranged over by e. The chart de-
fines a set of operators and their arities. For example, it specifies that the operator let has arity
(Exp, Exp.Exp)Exp, which specifies that it has two arguments of sort Exp, and binds a variable of
sort Exp in the second argument.
4.2
Type System
The role of a type system is to impose constraints on the formations of phrases that are sensitive to
the context in which they occur. For example, whether the expression plus( x ; num[ n ] ) is sensible
depends on whether the variable x is restricted to have type num in the surrounding context of
the expression. This example is, in fact, illustrative of the general case, in that the only informa-
tion required about the context of an expression is the type of the variables within whose scope
the expression lies. Consequently, the statics of E consists of an inductive definition of generic
hypothetical judgments of the form
X | Γ ⊢e : τ,
where X is a finite set of variables, and Γ is a typing context consisting of hypotheses of the form
x : τ, one for each x ∈X . We rely on typographical conventions to determine the set of variables,
using the letters x and y to stand for them. We write x /∈dom(Γ) to say that there is no assumption
in Γ of the form x : τ for any type τ, in which case we say that the variable x is fresh for Γ.
The rules defining the statics of E are as follows:
Γ, x : τ ⊢x : τ
(4.1a)
Γ ⊢str[ s ] : str
(4.1b)
Γ ⊢num[ n ] : num
(4.1c)
Γ ⊢e1 : num
Γ ⊢e2 : num
Γ ⊢plus( e1 ; e2 ) : num
(4.1d)
Γ ⊢e1 : num
Γ ⊢e2 : num
Γ ⊢times( e1 ; e2 ) : num
(4.1e)

PREVIEW
4.3 Structural Properties
37
Γ ⊢e1 : str
Γ ⊢e2 : str
Γ ⊢cat( e1 ; e2 ) : str
(4.1f)
Γ ⊢e : str
Γ ⊢len( e ) : num
(4.1g)
Γ ⊢e1 : τ1
Γ, x : τ1 ⊢e2 : τ2
Γ ⊢let( e1 ; x . e2 ) : τ2
(4.1h)
In rule (4.1h) we tacitly assume that the variable x is not already declared in Γ. This condition
may always be met by choosing a suitable representative of the α-equivalence class of the let
expression.
It is easy to check that every expression has at most one type by induction on typing, which is
rule induction applied to rules (4.1).
Lemma 4.1 (Unicity of Typing). For every typing context Γ and expression e, there exists at most one τ
such that Γ ⊢e : τ.
Proof. By rule induction on rules (4.1), making use of the fact that variables have at most one type
in any typing context.
The typing rules are syntax-directed in the sense that there is exactly one rule for each form
of expression. Consequently it is easy to give necessary conditions for typing an expression that
invert the sufficient conditions expressed by the corresponding typing rule.
Lemma 4.2 (Inversion for Typing). Suppose that Γ ⊢e : τ. If e = plus( e1 ; e2 ), then τ = num,
Γ ⊢e1 : num, and Γ ⊢e2 : num, and similarly for the other constructs of the language.
Proof. These may all be proved by induction on the derivation of the typing judgment Γ ⊢e : τ.
In richer languages such inversion principles are more difficult to state and to prove.
4.3
Structural Properties
The statics enjoys the structural properties of the generic hypothetical judgment.
Lemma 4.3 (Weakening). If Γ ⊢e′ : τ′, then Γ, x : τ ⊢e′ : τ′ for any x /∈dom(Γ) and any type τ.
Proof. By induction on the derivation of Γ ⊢e′ : τ′. We will give one case here, for rule (4.1h). We
have that e′ = let( e1 ; z . e2 ), where by the conventions on variables we may assume z is chosen
such that z /∈dom(Γ) and z ̸= x. By induction we have
1. Γ, x : τ ⊢e1 : τ1,
2. Γ, x : τ, z : τ1 ⊢e2 : τ′,
from which the result follows by rule (4.1h).

PREVIEW
38
4.3 Structural Properties
Lemma 4.4 (Substitution). If Γ, x : τ ⊢e′ : τ′ and Γ ⊢e : τ, then Γ ⊢[e/x]e′ : τ′.
Proof. By induction on the derivation of Γ, x : τ ⊢e′ : τ′. We again consider only rule (4.1h). As in
the preceding case, e′ = let( e1 ; z . e2 ), where z is chosen so that z ̸= x and z /∈dom(Γ). We have
by induction and Lemma 4.3 that
1. Γ ⊢[e/x]e1 : τ1,
2. Γ, z : τ1 ⊢[e/x]e2 : τ′.
By the choice of z we have
[e/x]let( e1 ; z . e2 ) = let( [e/x]e1 ; z . [e/x]e2 ).
It follows by rule (4.1h) that Γ ⊢[e/x]let( e1 ; z . e2 ) : τ′, as desired.
From a programming point of view, Lemma 4.3 allows us to use an expression in any context
that binds its free variables: if e is well-typed in a context Γ, then we may “import” it into any
context that includes the assumptions Γ. In other words introducing new variables beyond those
required by an expression e does not invalidate e itself; it remains well-formed, with the same
type.1 More importantly, Lemma 4.4 expresses the important concepts of modularity and linking.
We may think of the expressions e and e′ as two components of a larger system in which e′ is a client
of the implementation e. The client declares a variable specifying the type of the implementation,
and is type checked knowing only this information. The implementation must be of the specified
type to satisfy the assumptions of the client. If so, then we may link them to form the composite
system [e/x]e′. This implementation may itself be the client of another component, represented by
a variable y that is replaced by that component during linking. When all such variables have been
implemented, the result is a closed expression that is ready for execution (evaluation).
The converse of Lemma 4.4 is called decomposition. It states that any (large) expression can be
decomposed into a client and implementor by introducing a variable to mediate their interaction.
Lemma 4.5 (Decomposition). If Γ ⊢[e/x]e′ : τ′, then for every type τ such that Γ ⊢e : τ, we have
Γ, x : τ ⊢e′ : τ′.
Proof. The typing of [e/x]e′ depends only on the type of e wherever it occurs, if at all.
Lemma 4.5 tells us that any sub-expression can be isolated as a separate module of a larger
system. This property is especially useful when the variable x occurs more than once in e′, because
then one copy of e suffices for all occurrences of x in e′.
The statics of E given by rules (4.1) exemplifies a recurrent pattern. The constructs of a language
are classified into one of two forms, the introduction and the elimination. The introduction forms
for a type determine the values, or canonical forms, of that type. The elimination forms determine
how to manipulate the values of a type to form a computation of another (possibly the same) type.
1This point may seem so obvious that it is not worthy of mention, but, surprisingly, there are useful type systems
that lack this property. Because they do not necessarily validate the structural principle of weakening, they are called
substructural type systems.

PREVIEW
4.4 Notes
39
In the language E the introduction forms for the type num are the numerals, and those for the type
str are the literals. The elimination forms for the type num are addition and multiplication, and
those for the type str are concatenation and length.
The importance of this classification will become clear once we have defined the dynamics
of the language in Chapter 5. Then we will see that the elimination forms are inverse to the in-
troduction forms in that they “take apart” what the introduction forms have “put together.” The
coherence of the statics and dynamics of a language expresses the concept of type safety, the subject
of Chapter 6.
4.4
Notes
The concept of the statics of a programming language was historically slow to develop, perhaps be-
cause the earliest languages had relatively few features and only very weak type systems. Statics in
the sense considered here was introduced in the definition of the Standard ML programming lan-
guage (Milner et al., 1997), building on earlier work by Church and others on the typed λ-calculus
(Barendregt, 1992). The concept of introduction and elimination, and the associated inversion prin-
ciple, was introduced by Gentzen in his pioneering work on natural deduction (Gentzen, 1969).
These principles were applied to the structure of programming languages by Martin-L¨of (1984,
1980).
Exercises
4.1. It is sometimes useful to give the typing judgment Γ ⊢e : τ an “operational” reading that
specifies more precisely the flow of information required to derive a typing judgment (or
determine that it is not derivable). The analytic mode corresponds to the context, expression,
and type being given, with the goal to determine whether the typing judgment is derivable.
The synthetic mode corresponds to the context and expression being given, with the goal
to find the unique type τ, if any, possessed by the expression in that context. These two
readings can be made explicit as judgments of the form e ↓τ, corresponding to the analytic
mode, and e ↑τ, corresponding to the synthetic mode.
Give a simultaneous inductive definition of these two judgments according to the following
guidelines:
(a) Variables are introduced in synthetic form.
(b) If we can synthesize a unique type for an expression, then we can analyze it with respect
to a given type by checking type equality.
(c) Definitions need care, because the type of the defined expression is not given, even
when the type of the result is given.
There is room for variation; the point of the exercise is to explore the possibilities.

PREVIEW
40
4.4 Notes
4.2. One way to limit the range of possibilities in the solution to Exercise 4.1 is to restrict and
extend the syntax of the language so that every expression is either synthetic or analytic
according to the following suggestions:
(a) Variables are analytic.
(b) Introduction forms are analytic, elimination forms are synthetic.
(c) An analytic expression can be made synthetic by introducing a type cast of the form
cast{τ}( e ) specifying that e must check against the specified type τ, which is synthe-
sized for the whole expression.
(d) The defining expression of a definition must be synthetic, but the scope of the definition
can be either synthetic or analytic.
Reformulate your solution to Exercise 4.1 to take account of these guidelines.

PREVIEW
Chapter 5
Dynamics
The dynamics of a language describes how programs are executed. The most important way to de-
fine the dynamics of a language is by the method of structural dynamics, which defines a transition
system that inductively specifies the step-by-step process of executing a program. Another method
for presenting dynamics, called contextual dynamics, is a variation of structural dynamics in which
the transition rules are specified in a slightly different way. An equational dynamics presents the dy-
namics of a language by a collection of rules defining when one program is definitionally equivalent
to another.
5.1
Transition Systems
A transition system is specified by the following four forms of judgment:
1. s state, asserting that s is a state of the transition system.
2. s final, where s state, asserting that s is a final state.
3. s initial, where s state, asserting that s is an initial state.
4. s 7−→s′, where s state and s′ state, asserting that state s may transition to state s′.
In practice we always arrange things so that no transition is possible from a final state: if s final,
then there is no s′ state such that s 7−→s′. A state from which no transition is possible is stuck.
Whereas all final states are, by convention, stuck, there may be stuck states in a transition system
that are not final. A transition system is deterministic iff for every state s there exists at most one
state s′ such that s 7−→s′, otherwise it is non-deterministic.
A transition sequence is a sequence of states s0, . . . , sn such that s0 initial, and si 7−→si+1 for every
0 ≤i < n. A transition sequence is maximal iff there is no s such that sn 7−→s, and it is complete
iff it is maximal and sn final. Thus every complete transition sequence is maximal, but maximal
sequences are not necessarily complete.

PREVIEW
42
5.2 Structural Dynamics
The iteration of transition judgment s 7−→∗s′ is inductively defined by the following rules:
s 7−→∗s
(5.1a)
s 7−→s′
s′ 7−→∗s′′
s 7−→∗s′′
(5.1b)
When applied to the definition of iterated transition, the principle of rule induction states that
to show that P(s, s′) holds when s 7−→∗s′, it is enough to show these two properties of P:
1. P(s, s).
2. if s 7−→s′ and P(s′, s′′), then P(s, s′′).
The first requirement is to show that P is reflexive. The second is to show that P is closed under
head expansion, or closed under inverse evaluation. Using this principle, it is easy to prove that 7−→∗is
reflexive and transitive.
The n-times iterated transition judgment s 7−→n s′, where n ≥0, is inductively defined by the
following rules.
s 7−→0 s
(5.2a)
s 7−→s′
s′ 7−→n s′′
s 7−→n+1 s′′
(5.2b)
Theorem 5.1. For all states s and s′, s 7−→∗s′ iff s 7−→k s′ for some k ≥0.
Proof. From left to right, by induction on the definition of multi-step transition. From right to left,
by mathematical induction on k ≥0.
5.2
Structural Dynamics
A structural dynamics for the language E is given by a transition system whose states are closed
expressions. All states are initial. The final states are the (closed) values, which represent the com-
pleted computations. The judgment e val, which states that e is a value, is inductively defined by
the following rules:
num[ n ] val
(5.3a)
str[ s ] val
(5.3b)
The transition judgment e 7−→e′ between states is inductively defined by the following rules:
n1 + n2 = n
plus( num[ n1 ] ; num[ n2 ] ) 7−→num[ n ]
(5.4a)

PREVIEW
5.2 Structural Dynamics
43
e1 7−→e′
1
plus( e1 ; e2 ) 7−→plus( e′
1 ; e2 )
(5.4b)
e1 val
e2 7−→e′
2
plus( e1 ; e2 ) 7−→plus( e1 ; e′
2 )
(5.4c)
s1 ˆ s2 = s
cat( str[ s1 ] ; str[ s2 ] ) 7−→str[ s ]
(5.4d)
e1 7−→e′
1
cat( e1 ; e2 ) 7−→cat( e′
1 ; e2 )
(5.4e)
e1 val
e2 7−→e′
2
cat( e1 ; e2 ) 7−→cat( e1 ; e′
2 )
(5.4f)


e1 7−→e′
1
let( e1 ; x . e2 ) 7−→let( e′
1 ; x . e2 )


(5.4g)
[e1 val]
let( e1 ; x . e2 ) 7−→[e1/x]e2
(5.4h)
We have omitted rules for multiplication and computing the length of a string, which follow a
similar pattern. Rules (5.4a), (5.4d), and (5.4h) are instruction transitions, because they correspond
to the primitive steps of evaluation. The remaining rules are search transitions that determine the
order of execution of instructions.
The bracketed rule, rule (5.4g), and bracketed premise on rule (5.4h), are included for a by-
value interpretation of let, and omitted for a by-name interpretation. The by-value interpretation
evaluates an expression before binding it to the defined variable, whereas the by-name interpreta-
tion binds it in unevaluated form. The by-value interpretation saves work if the defined variable
is used more than once, but wastes work if it is not used at all. Conversely, the by-name inter-
pretation saves work if the defined variable is not used, and wastes work if it is used more than
once.
A derivation sequence in a structural dynamics has a two-dimensional structure, with the
number of steps in the sequence being its “width” and the derivation tree for each step being
its “height.” For example, consider the following evaluation sequence.
let( plus( num[ 1 ] ; num[ 2 ] ) ; x . plus( plus( x ; num[ 3 ] ) ; num[ 4 ] ) )
7−→
let( num[ 3 ] ; x . plus( plus( x ; num[ 3 ] ) ; num[ 4 ] ) )
7−→
plus( plus( num[ 3 ] ; num[ 3 ] ) ; num[ 4 ] )
7−→
plus( num[ 6 ] ; num[ 4 ] )
7−→
num[ 10 ]

PREVIEW
44
5.3 Contextual Dynamics
Each step in this sequence of transitions is justified by a derivation according to rules (5.4). For
example, the third transition in the preceding example is justified by the following derivation:
plus( num[ 3 ] ; num[ 3 ] ) 7−→num[ 6 ] (5.4a)
plus( plus( num[ 3 ] ; num[ 3 ] ) ; num[ 4 ] ) 7−→plus( num[ 6 ] ; num[ 4 ] ) (5.4b)
The other steps are similarly justified by composing rules.
The principle of rule induction for the structural dynamics of E states that to show P(e 7−→e′)
when e 7−→e′, it is enough to show that P is closed under rules (5.4). For example, we may show
by rule induction that the structural dynamics of E is determinate, which means that an expres-
sion may transition to at most one other expression. The proof requires a simple lemma relating
transition to values.
Lemma 5.2 (Finality of Values). For no expression e do we have both e val and e 7−→e′ for some e′.
Proof. By rule induction on rules (5.3) and (5.4).
Lemma 5.3 (Determinacy). If e 7−→e′ and e 7−→e′′, then e′ and e′′ are α-equivalent.
Proof. By rule induction on the premises e 7−→e′ and e 7−→e′′, carried out either simultaneously
or in either order. The primitive operators, such as addition, are assumed to have a unique value
when applied to values.
Rules (5.4) exemplify the inversion principle of language design, which states that the elimina-
tion forms are inverse to the introduction forms of a language. The search rules determine the
principal arguments of each elimination form, and the instruction rules specify how to evaluate
an elimination form when all of its principal arguments are in introduction form. For example,
rules (5.4) specify that both arguments of addition are principal, and specify how to evaluate an
addition once its principal arguments are evaluated to numerals. The inversion principle is cen-
tral to ensuring that a programming language is properly defined, the exact statement of which is
given in Chapter 6.
5.3
Contextual Dynamics
A variant of structural dynamics, called contextual dynamics, is sometimes useful. There is no
fundamental difference between contextual and structural dynamics, rather one of style. The main
idea is to isolate instruction steps as a special form of judgment, called instruction transition, and
to formalize the process of locating the next instruction using a device called an evaluation context.
The judgment e val, defining whether an expression is a value, remains unchanged.

PREVIEW
5.3 Contextual Dynamics
45
The instruction transition judgment e1 −→e2 for E is defined by the following rules, together
with similar rules for multiplication of numbers and the length of a string.
m + n = p
plus( num[ m ] ; num[ n ] ) −→num[ p ]
(5.5a)
s ˆ t = u
cat( str[ s ] ; str[ t ] ) −→str[ u ]
(5.5b)
let( e1 ; x . e2 ) −→[e1/x]e2
(5.5c)
The judgment E ectx determines the location of the next instruction to execute in a larger ex-
pression. The position of the next instruction step is specified by a “hole”, written ◦, into which
the next instruction is placed, as we shall detail shortly. (The rules for multiplication and length
are omitted for concision, as they are handled similarly.)
◦ectx
(5.6a)
E1 ectx
plus( E1 ; e2 ) ectx
(5.6b)
e1 val
E2 ectx
plus( e1 ; E2 ) ectx
(5.6c)
The first rule for evaluation contexts specifies that the next instruction may occur “here”, at the
occurrence of the hole. The remaining rules correspond one-for-one to the search rules of the
structural dynamics. For example, rule (5.6c) states that in an expression plus( e1 ; e2 ), if the first
argument, e1, is a value, then the next instruction step, if any, lies at or within the second argument,
e2.
An evaluation context is a template that is instantiated by replacing the hole with an instruction
to be executed. The judgment e′ = E{e} states that the expression e′ is the result of filling the hole
in the evaluation context E with the expression e. It is inductively defined by the following rules:
e = ◦{e}
(5.7a)
e1 = E1{e}
plus( e1 ; e2 ) = plus( E1 ; e2 ){e}
(5.7b)
e1 val
e2 = E2{e}
plus( e1 ; e2 ) = plus( e1 ; E2 ){e}
(5.7c)
There is one rule for each form of evaluation context. Filling the hole with e results in e; otherwise
we proceed inductively over the structure of the evaluation context.
Finally, the contextual dynamics for E is defined by a single rule:
e = E{e0}
e0 −→e′
0
e′ = E{e′
0}
e 7−→e′
(5.8)

PREVIEW
46
5.4 Equational Dynamics
Thus, a transition from e to e′ consists of (1) decomposing e into an evaluation context and an
instruction, (2) execution of that instruction, and (3) replacing the instruction by the result of its
execution in the same spot within e to obtain e′.
The structural and contextual dynamics define the same transition relation. For the sake of the
proof, let us write e 7−→
str e′ for the transition relation defined by the structural dynamics (rules (5.4)),
and e 7−→
ctx e′ for the transition relation defined by the contextual dynamics (rules (5.8)).
Theorem 5.4. e 7−→
str e′ if, and only if, e 7−→
ctx e′.
Proof. From left to right, proceed by rule induction on rules (5.4). It is enough in each case to
exhibit an evaluation context E such that e = E{e0}, e′ = E{e′
0}, and e0 −→e′
0. For example, for
rule (5.4a), take E = ◦, and note that e −→e′. For rule (5.4b), we have by induction that there exists
an evaluation context E1 such that e1 = E1{e0}, e′
1 = E1{e′
0}, and e0 −→e′
0. Take E = plus( E1 ; e2 ),
and note that e = plus( E1 ; e2 ){e0} and e′ = plus( E1 ; e2 ){e′
0} with e0 −→e′
0.
From right to left, note that if e 7−→
ctx
e′, then there exists an evaluation context E such that
e = E{e0}, e′ = E{e′
0}, and e0 −→e′
0. We prove by induction on rules (5.7) that e 7−→
str
e′. For
example, for rule (5.7a), e0 is e, e′
0 is e′, and e −→e′. Hence e 7−→
str
e′. For rule (5.7b), we have
that E = plus( E1 ; e2 ), e1 = E1{e0}, e′
1 = E1{e′
0}, and e1 7−→
str e′
1. Therefore e is plus( e1 ; e2 ), e′ is
plus( e′
1 ; e2 ), and therefore by rule (5.4b), e 7−→
str e′.
Because the two transition judgments coincide, contextual dynamics can be considered an al-
ternative presentation of a structural dynamics. It has two advantages over structural dynam-
ics, one relatively superficial, one rather less so. The superficial advantage stems from writing
rule (5.8) in the simpler form
e0 −→e′
0
E{e0} 7−→E{e′
0}
.
(5.9)
This formulation is superficially simpler in that it does not make explicit how an expression is
decomposed into an evaluation context and a reducible expression. The deeper advantage of con-
textual dynamics is that all transitions are between complete programs. One need never consider a
transition between expressions of any type other than the observable type, which simplifies certain
arguments, such as the proof of Lemma 47.16.
5.4
Equational Dynamics
Another formulation of the dynamics of a language regards computation as a form of equational
deduction, much in the style of elementary algebra. For example, in algebra we may show that
the polynomials x2 + 2 x + 1 and (x + 1)2 are equivalent by a simple process of calculation and
re-organization using the familiar laws of addition and multiplication. The same laws are enough
to determine the value of any polynomial, given the values of its variables. So, for example, we

PREVIEW
5.4 Equational Dynamics
47
may plug in 2 for x in the polynomial x2 + 2 x + 1 and calculate that 22 + 2 × 2 + 1 = 9, which is
indeed (2 + 1)2. We thus obtain a model of computation in which the value of a polynomial for a
given value of its variable is determined by substitution and simplification.
Very similar ideas give rise to the concept of definitional, or computational, equivalence of expres-
sions in E, which we write as X | Γ ⊢e ≡e′ : τ, where Γ consists of one assumption of the form
x : τ for each x ∈X . We only consider definitional equality of well-typed expressions, so that
when considering the judgment Γ ⊢e ≡e′ : τ, we tacitly assume that Γ ⊢e : τ and Γ ⊢e′ : τ.
Here, as usual, we omit explicit mention of the variables X when they can be determined from the
forms of the assumptions Γ.
Definitional equality of expressions in E under the by-name interpretation of let is inductively
defined by the following rules:
Γ ⊢e : τ
Γ ⊢e ≡e : τ
(5.10a)
Γ ⊢e′ ≡e : τ
Γ ⊢e ≡e′ : τ
(5.10b)
Γ ⊢e ≡e′ : τ
Γ ⊢e′ ≡e′′ : τ
Γ ⊢e ≡e′′ : τ
(5.10c)
Γ ⊢e1 ≡e′
1 : num
Γ ⊢e2 ≡e′
2 : num
Γ ⊢plus( e1 ; e2 ) ≡plus( e′
1 ; e′
2 ) : num
(5.10d)
Γ ⊢e1 ≡e′
1 : str
Γ ⊢e2 ≡e′
2 : str
Γ ⊢cat( e1 ; e2 ) ≡cat( e′
1 ; e′
2 ) : str
(5.10e)
Γ ⊢e1 ≡e′
1 : τ1
Γ, x : τ1 ⊢e2 ≡e′
2 : τ2
Γ ⊢let( e1 ; x . e2 ) ≡let( e′
1 ; x . e′
2 ) : τ2
(5.10f)
n1 + n2 = n
Γ ⊢plus( num[ n1 ] ; num[ n2 ] ) ≡num[ n ] : num
(5.10g)
s1 ˆ s2 = s
Γ ⊢cat( str[ s1 ] ; str[ s2 ] ) ≡str[ s ] : str
(5.10h)
Γ ⊢e1 : τ1
Γ, x : τ1 ⊢e : τ2
Γ ⊢let( e1 ; x . e2 ) ≡[e1/x]e2 : τ2
(5.10i)
Rules (5.10a) through (5.10c) state that definitional equality is an equivalence relation. Rules (5.10d)
through (5.10f) state that it is a congruence relation, which means that it is compatible with all
expression-forming constructs in the language. Rules (5.10g) through (5.10i) specify the meanings
of the primitive constructs of E. We say that rules (5.10) define the strongest congruence closed
under rules (5.10g), (5.10h), and (5.10i).
Rules (5.10) suffice to calculate the value of an expression by a deduction similar to that used
in high school algebra. For example, we may derive the equation
let x be 1 + 2 in x + 3 + 4 ≡10 : num

PREVIEW
48
5.5 Notes
by applying rules (5.10). Here, as in general, there may be many different ways to derive the same
equation, but we need find only one derivation in order to carry out an evaluation.
Definitional equality is rather weak in that many equivalences that we might intuitively think
are true are not derivable from rules (5.10). A prototypical example is the putative equivalence
x1 : num, x2 : num ⊢x1 + x2 ≡x2 + x1 : num,
(5.11)
which, intuitively, expresses the commutativity of addition. Although we shall not prove this here,
this equivalence is not derivable from rules (5.10). And yet we may derive all of its closed instances,
n1 + n2 ≡n2 + n1 : num,
(5.12)
where n1 nat and n2 nat are particular numbers.
The “gap” between a general law, such as Equation (5.11), and all of its instances, given by
Equation (5.12), may be filled by enriching the notion of equivalence to include a principle of proof
by mathematical induction. Such a notion of equivalence is sometimes called semantic equivalence,
because it expresses relationships that hold by virtue of the dynamics of the expressions involved.
(Semantic equivalence is developed rigorously for a related language in Chapter 46.)
Theorem 5.5. For the expression language E, the relation e ≡e′ : τ holds iff there exists e0 val such that
e 7−→∗e0 and e′ 7−→∗e0.
Proof. The proof from right to left is direct, because every transition step is a valid equation. The
converse follows from the following, more general, proposition, which is proved by induction on
rules (5.10): if x1 : τ1, . . . , xn : τn ⊢e ≡e′ : τ, then when e1 : τ1, e′
1 : τ1, . . . , en : τn, e′
n : τn, if for each
1 ≤i ≤n the expressions ei and e′
i evaluate to a common value vi, then there exists e0 val such that
[e1, . . . , en/x1, . . . , xn]e 7−→∗e0
and
[e′
1, . . . , e′
n/x1, . . . , xn]e′ 7−→∗e0.
5.5
Notes
The use of transition systems to specify the behavior of programs goes back to the early work of
Church and Turing on computability. Turing’s approach emphasized the concept of an abstract
machine consisting of a finite program together with unbounded memory. Computation proceeds
by changing the memory in accordance with the instructions in the program. Much early work on
the operational semantics of programming languages, such as the SECD machine (Landin, 1965),
emphasized machine models. Church’s approach emphasized the language for expressing com-
putations, and defined execution in terms of the programs themselves, rather than in terms of aux-
iliary concepts such as memories or tapes. Plotkin’s elegant formulation of structural operational
semantics (Plotkin, 1981), which we use heavily throughout this book, was inspired by Church’s

PREVIEW
5.5 Notes
49
and Landin’s ideas (Plotkin, 2004). Contextual semantics, which was introduced by Felleisen and
Hieb (1992), may be seen as an alternative formulation of structural semantics in which “search
rules” are replaced by “context matching”. Computation viewed as equational deduction goes
back to the early work of Herbrand, G¨odel, and Church.
Exercises
5.1. Prove that if s 7−→∗s′ and s′ 7−→∗s′′, then s 7−→∗s′′.
5.2. Complete the proof of Theorem 5.1 along the lines suggested there.
5.3. Complete the proof of Theorem 5.5 along the lines suggested there.
5.4. Prove that if Γ ⊢e ≡e′ : τ according to Rules (5.10), then Γ ⊢e : τ and Γ ⊢e′ : τ according
to Rules (4.1).

PREVIEW
50
5.5 Notes

PREVIEW
Chapter 6
Type Safety
Most programming languages are safe (or, type safe, or strongly typed). Informally, this means that
certain kinds of mismatches cannot arise during execution. For example, type safety for E states
that it will never arise that a number is added to a string, or that two numbers are concatenated,
neither of which is meaningful.
In general type safety expresses the coherence between the statics and the dynamics. The statics
may be seen as predicting that the value of an expression will have a certain form so that the
dynamics of that expression is well-defined. Consequently, evaluation cannot “get stuck” in a
state for which no transition is possible, corresponding in implementation terms to the absence
of “illegal instruction” errors at execution time. Safety is proved by showing that each step of
transition preserves typability and by showing that typable states are well-defined. Consequently,
evaluation can never “go off into the weeds,” and hence can never encounter an illegal instruction.
Type safety for the language E is stated precisely as follows:
Theorem 6.1 (Type Safety).
1. If e : τ and e 7−→e′, then e′ : τ.
2. If e : τ, then either e val, or there exists e′ such that e 7−→e′.
The first part, called preservation, says that the steps of evaluation preserve typing; the second,
called progress, ensures that well-typed expressions are either values or can be further evaluated.
Safety is the conjunction of preservation and progress.
We say that an expression e is stuck iff it is not a value, yet there is no e′ such that e 7−→e′. It
follows from the safety theorem that a stuck state is necessarily ill-typed. Or, putting it the other
way around, that well-typed states do not get stuck.

PREVIEW
52
6.1 Preservation
6.1
Preservation
The preservation theorem for E defined in Chapters 4 and 5 is proved by rule induction on the
transition system (rules (5.4)).
Theorem 6.2 (Preservation). If e : τ and e 7−→e′, then e′ : τ.
Proof. We will give the proof in two cases, leaving the rest to the reader. Consider rule (5.4b),
e1 7−→e′
1
plus( e1 ; e2 ) 7−→plus( e′
1 ; e2 )
.
Assume that plus( e1 ; e2 ) : τ. By inversion for typing, we have that τ = num, e1 : num, and e2 : num.
By induction we have that e′
1 : num, and hence plus( e′
1 ; e2 ) : num. The case for concatenation is
handled similarly.
Now consider rule (5.4h),
let( e1 ; x . e2 ) 7−→[e1/x]e2
.
Assume that let( e1 ; x . e2 ) : τ2. By the inversion lemma 4.2, e1 : τ1 for some τ1 such that x : τ1 ⊢
e2 : τ2. By the substitution lemma 4.4 [e1/x]e2 : τ2, as desired.
It is easy to check that the primitive operations are all type-preserving; for example, if a nat
and b nat and a + b = c, then c nat.
The proof of preservation is naturally structured as an induction on the transition judgment,
because the argument hinges on examining all possible transitions from a given expression. In
some cases we may manage to carry out a proof by structural induction on e, or by an induction on
typing, but experience shows that this often leads to awkward arguments, or, sometimes, cannot
be made to work at all.
6.2
Progress
The progress theorem captures the idea that well-typed programs cannot “get stuck”. The proof
depends crucially on the following lemma, which characterizes the values of each type.
Lemma 6.3 (Canonical Forms). If e val and e : τ, then
1. If τ = num, then e = num[ n ] for some number n.
2. If τ = str, then e = str[ s ] for some string s.
Proof. By induction on rules (4.1) and (5.3).
Progress is proved by rule induction on rules (4.1) defining the statics of the language.

PREVIEW
6.3 Run-Time Errors
53
Theorem 6.4 (Progress). If e : τ, then either e val, or there exists e′ such that e 7−→e′.
Proof. The proof proceeds by induction on the typing derivation. We will consider only one case,
for rule (4.1d),
e1 : num
e2 : num
plus( e1 ; e2 ) : num
,
where the context is empty because we are considering only closed terms.
By induction we have that either e1 val, or there exists e′
1 such that e1 7−→e′
1. In the latter
case it follows that plus( e1 ; e2 ) 7−→plus( e′
1 ; e2 ), as required. In the former we also have by
induction that either e2 val, or there exists e′
2 such that e2 7−→e′
2. In the latter case we have that
plus( e1 ; e2 ) 7−→plus( e1 ; e′
2 ), as required. In the former, we have, by the Canonical Forms
Lemma 6.3, e1 = num[ n1 ] and e2 = num[ n2 ], and hence
plus( num[ n1 ] ; num[ n2 ] ) 7−→num[ n1 + n2 ].
Because the typing rules for expressions are syntax-directed, the progress theorem could equally
well be proved by induction on the structure of e, appealing to the inversion theorem at each step
to characterize the types of the parts of e. But this approach breaks down when the typing rules
are not syntax-directed, that is, when there is more than one rule for a given expression form. Such
rules present no difficulites, so long as the proof proceeds by induction on the typing rules, and
not on the structure of the expression.
Summing up, the combination of preservation and progress together constitute the proof of
safety. The progress theorem ensures that well-typed expressions do not “get stuck” in an ill-
defined state, and the preservation theorem ensures that if a step is taken, the result remains
well-typed (with the same type). Thus the two parts work together to ensure that the statics and
dynamics are coherent, and that no ill-defined states can ever be encountered while evaluating a
well-typed expression.
6.3
Run-Time Errors
Suppose that we wish to extend E with, say, a quotient operation that is undefined for a zero
divisor. The natural typing rule for quotients is given by the following rule:
e1 : num
e2 : num
div( e1 ; e2 ) : num
.
But the expression div( num[ 3 ] ; num[ 0 ] ) is well-typed, yet stuck! We have two options to correct
this situation:
1. Enhance the type system, so that no well-typed program may divide by zero.

PREVIEW
54
6.3 Run-Time Errors
2. Add dynamic checks, so that division by zero signals an error as the outcome of evaluation.
Either option is, in principle, practical, but the most common approach is the second. The first
requires that the type checker prove that an expression be non-zero before permitting it to be used
in the denominator of a quotient. It is difficult to do this without ruling out too many programs as
ill-formed. We cannot predict statically whether an expression will be non-zero when evaluated,
so the second approach is most often used in practice.
The overall idea is to distinguish checked from unchecked errors. An unchecked error is one
that is ruled out by the type system. No run-time checking is performed to ensure that such an
error does not occur, because the type system rules out the possibility of it arising. For example,
the dynamics need not check, when performing an addition, that its two arguments are, in fact,
numbers, as opposed to strings, because the type system ensures that this is the case. On the other
hand the dynamics for quotient must check for a zero divisor, because the type system does not
rule out the possibility.
One approach to modeling checked errors is to give an inductive definition of the judgment
e err stating that the expression e incurs a checked run-time error, such as division by zero. Here
are some representative rules that would be present in a full inductive definition of this judgment:
e1 val
div( e1 ; num[ 0 ] ) err
(6.1a)
e1 err
div( e1 ; e2 ) err
(6.1b)
e1 val
e2 err
div( e1 ; e2 ) err
(6.1c)
Rule (6.1a) signals an error condition for division by zero. The other rules propagate this error
upwards: if an evaluated sub-expression is a checked error, then so is the overall expression.
Once the error judgment is available, we may also consider an expression, error, which forcibly
induces an error, with the following statics and dynamics:
Γ ⊢error : τ
(6.2a)
error err
(6.2b)
The preservation theorem is not affected by checked errors. However, the statement (and proof)
of progress is modified to account for checked errors.
Theorem 6.5 (Progress With Error). If e : τ, then either e err, or e val, or there exists e′ such that
e 7−→e′.
Proof. The proof is by induction on typing, and proceeds similarly to the proof given earlier, except
that there are now three cases to consider at each point in the proof.

PREVIEW
6.4 Notes
55
6.4
Notes
The concept of type safety was first formulated by Milner (1978), who invented the slogan “well-
typed programs do not go wrong.” Whereas Milner relied on an explicit notion of “going wrong”
to express the concept of a type error, Wright and Felleisen (1994) observed that we can instead
show that ill-defined states cannot arise in a well-typed program, giving rise to the slogan “well-
typed programs do not get stuck.” However, their formulation relied on an analysis showing
that no stuck state is well-typed. The progress theorem, which relies on the characterization of
canonical forms in the style of Martin-L¨of (1980), eliminates this analysis.
Exercises
6.1. Complete the proof of Theorem 6.2 in full detail.
6.2. Complete the proof of Theorem 6.4 in full detail.
6.3. Give several cases of the proof of Theorem 6.5 to illustrate how checked errors are handled
in type safety proofs.

PREVIEW
56
6.4 Notes

PREVIEW
Chapter 7
Evaluation Dynamics
In Chapter 5 we defined evaluation of expressions in E using a structural dynamics. Structural
dynamics is very useful for proving safety, but for some purposes, such as writing a user manual,
another formulation, called evaluation dynamics is preferable. An evaluation dynamics is a relation
between a phrase and its value that is defined without detailing the step-by-step process of evalu-
ation. A cost dynamics enriches an evaluation dynamics with a cost measure specifying the resource
usage of evaluation. A prime example is time, measured as the number of transition steps required
to evaluate an expression according to its structural dynamics.
7.1
Evaluation Dynamics
An evaluation dynamics, consists of an inductive definition of the evaluation judgment e ⇓v stating
that the closed expression e evaluates to the value v. The evaluation dynamics of E is defined by
the following rules:
num[ n ] ⇓num[ n ]
(7.1a)
str[ s ] ⇓str[ s ]
(7.1b)
e1 ⇓num[ n1 ]
e2 ⇓num[ n2 ]
n1 + n2 = n
plus( e1 ; e2 ) ⇓num[ n ]
(7.1c)
e1 ⇓str[ s1 ]
e2 ⇓str[ s2 ]
s1 ˆ s2 = s
cat( e1 ; e2 ) ⇓str[ s ]
(7.1d)
e ⇓str[ s ]
|s| = n
len( e ) ⇓num[ n ]
(7.1e)
[e1/x]e2 ⇓v2
let( e1 ; x . e2 ) ⇓v2
(7.1f)

PREVIEW
58
7.2 Relating Structural and Evaluation Dynamics
The value of a let expression is determined by substitution of the binding into the body. The rules
are not syntax-directed, because the premise of rule (7.1f) is not a sub-expression of the expression
in the conclusion of that rule.
Rule (7.1f) specifies a by-name interpretation of definitions. For a by-value interpretation the
following rule should be used instead:
e1 ⇓v1
[v1/x]e2 ⇓v2
let( e1 ; x . e2 ) ⇓v2
(7.2)
Because the evaluation judgment is inductively defined, we prove properties of it by rule in-
duction. Specifically, to show that the property P(e ⇓v) holds, it is enough to show that P is
closed under rules (7.1):
1. Show that P(num[ n ] ⇓num[ n ]).
2. Show that P(str[ s ] ⇓str[ s ]).
3. Show that P(plus( e1 ; e2 ) ⇓num[ n ]), if P(e1 ⇓num[ n1 ]), P(e2 ⇓num[ n2 ]), and n1 + n2 = n.
4. Show that P(cat( e1 ; e2 ) ⇓str[ s ]), if P(e1 ⇓str[ s1 ]), P(e2 ⇓str[ s2 ]), and s1 ˆ s2 = s.
5. Show that P(let( e1 ; x . e2 ) ⇓v2), if P([e1/x]e2 ⇓v2).
This induction principle is not the same as structural induction on e itself, because the evaluation
rules are not syntax-directed.
Lemma 7.1. If e ⇓v, then v val.
Proof. By induction on rules (7.1). All cases except rule (7.1f) are immediate. For the latter case, the
result follows directly by an appeal to the inductive hypothesis for the premise of the evaluation
rule.
7.2
Relating Structural and Evaluation Dynamics
We have given two different forms of dynamics for E. It is natural to ask whether they are equiv-
alent, but to do so first requires that we consider carefully what we mean by equivalence. The
structural dynamics describes a step-by-step process of execution, whereas the evaluation dynam-
ics suppresses the intermediate states, focusing attention on the initial and final states alone. This
remark suggests that the right correspondence is between complete execution sequences in the
structural dynamics and the evaluation judgment in the evaluation dynamics.
Theorem 7.2. For all closed expressions e and values v, e 7−→∗v iff e ⇓v.
How might we prove such a theorem? We will consider each direction separately. We consider
the easier case first.
Lemma 7.3. If e ⇓v, then e 7−→∗v.

PREVIEW
7.3 Type Safety, Revisited
59
Proof. By induction on the definition of the evaluation judgment.
For example, suppose that
plus( e1 ; e2 ) ⇓num[ n ] by the rule for evaluating additions. By induction we know that e1 7−→∗
num[ n1 ] and e2 7−→∗num[ n2 ]. We reason as follows:
plus( e1 ; e2 )
7−→∗
plus( num[ n1 ] ; e2 )
7−→∗
plus( num[ n1 ] ; num[ n2 ] )
7−→
num[ n1 + n2 ]
Therefore plus( e1 ; e2 ) 7−→∗num[ n1 + n2 ], as required. The other cases are handled similarly.
For the converse, recall from Chapter 5 the definitions of multi-step evaluation and complete
evaluation. Because v ⇓v when v val, it suffices to show that evaluation is closed under converse
evaluation:1
Lemma 7.4. If e 7−→e′ and e′ ⇓v, then e ⇓v.
Proof. By induction on the definition of the transition judgment. For example, suppose that plus( e1 ;
e2 ) 7−→plus( e′
1 ; e2 ), where e1 7−→e′
1. Suppose further that plus( e′
1 ; e2 ) ⇓v, so that e′
1 ⇓num[ n1 ],
and e2 ⇓num[ n2 ], and n1 + n2 = n, and v is num[ n ]. By induction e1 ⇓num[ n1 ], and hence
plus( e1 ; e2 ) ⇓num[ n ], as required.
7.3
Type Safety, Revisited
Type safety is defined in Chapter 6 as preservation and progress (Theorem 6.1). These concepts are
meaningful when applied to a dynamics given by a transition system, as we shall do throughout
this book. But what if we had instead given the dynamics as an evaluation relation? How is type
safety proved in that case?
The answer, unfortunately, is that we cannot. Although there is an analog of the preservation
property for an evaluation dynamics, there is no clear analog of the progress property. Preser-
vation may be stated as saying that if e ⇓v and e : τ, then v : τ. It can be readily proved by
induction on the evaluation rules. But what is the analog of progress? We might be tempted to
phrase progress as saying that if e : τ, then e ⇓v for some v. Although this property is true for E,
it demands much more than just progress — it requires that every expression evaluate to a value!
If E were extended to admit operations that may result in an error (as discussed in Section 6.3), or
to admit non-terminating expressions, then this property would fail, even though progress would
remain valid.
One possible attitude towards this situation is to conclude that type safety cannot be properly
discussed in the context of an evaluation dynamics, but only by reference to a structural dynamics.
Another point of view is to instrument the dynamics with explicit checks for dynamic type errors,
and to show that any expression with a dynamic type fault must be statically ill-typed. Re-stated
1Converse evaluation is also known as head expansion.

PREVIEW
60
7.4 Cost Dynamics
in the contrapositive, this means that a statically well-typed program cannot incur a dynamic type
error. A difficulty with this point of view is that we must explicitly account for a form of error
solely to prove that it cannot arise! Nevertheless, a semblance of type safety can be established
using evaluation dynamics.
We define a judgment e ?? stating that the expression e goes wrong when executed. The exact
definition of “going wrong” is given by a set of rules, but the intention is that it should cover all
situations that correspond to type errors. The following rules are representative of the general
case:
plus( str[ s ] ; e2 ) ??
(7.3a)
e1 val
plus( e1 ; str[ s ] ) ??
(7.3b)
These rules explicitly check for the misapplication of addition to a string; similar rules govern each
of the primitive constructs of the language.
Theorem 7.5. If e ??, then there is no τ such that e : τ.
Proof. By rule induction on rules (7.3). For example, for rule (7.3a), we note that str[ s ] : str, and
hence plus( str[ s ] ; e2 ) is ill-typed.
Corollary 7.6. If e : τ, then ¬(e ??).
Apart from the inconvenience of having to define the judgment e ?? only to show that it is irrel-
evant for well-typed programs, this approach suffers a very significant methodological weakness.
If we should omit one or more rules defining the judgment e ??, the proof of Theorem 7.5 remains
valid; there is nothing to ensure that we have included sufficiently many checks for run-time type
errors. We can prove that the ones we define cannot arise in a well-typed program, but we cannot
prove that we have covered all possible cases. By contrast the structural dynamics does not spec-
ify any behavior for ill-typed expressions. Consequently, any ill-typed expression will “get stuck”
without our explicit intervention, and the progress theorem rules out all such cases. Moreover,
the transition system corresponds more closely to implementation—a compiler need not make
any provisions for checking for run-time type errors. Instead, it relies on the statics to ensure that
these cannot arise, and assigns no meaning to any ill-typed program. Therefore, execution is more
efficient, and the language definition is simpler.
7.4
Cost Dynamics
A structural dynamics provides a natural notion of time complexity for programs, namely the num-
ber of steps required to reach a final state. An evaluation dynamics, however, does not provide
such a direct notion of time. Because the individual steps required to complete an evaluation
are suppressed, we cannot directly read off the number of steps required to evaluate to a value.
Instead we must augment the evaluation relation with a cost measure, resulting in a cost dynamics.

PREVIEW
7.5 Notes
61
Evaluation judgments have the form e ⇓k v, with the meaning that e evaluates to v in k steps.
num[ n ] ⇓0 num[ n ]
(7.4a)
e1 ⇓k1 num[ n1 ]
e2 ⇓k1 num[ n2 ]
plus( e1 ; e2 ) ⇓k1+k2+1 num[ n1 + n2 ]
(7.4b)
str[ s ] ⇓0 str[ s ]
(7.4c)
e1 ⇓k1 s1
e2 ⇓k2 s2
cat( e1 ; e2 ) ⇓k1+k2+1 str[ s1 ˆ s2 ]
(7.4d)
[e1/x]e2 ⇓k2 v2
let( e1 ; x . e2 ) ⇓k2+1 v2
(7.4e)
For a by-value interpretation of let, rule (7.4e) is replaced by the following rule:
e1 ⇓k1 v1
[v1/x]e2 ⇓k2 v2
let( e1 ; x . e2 ) ⇓k1+k2+1 v2
(7.5)
Theorem 7.7. For any closed expression e and closed value v of the same type, e ⇓k v iff e 7−→k v.
Proof. From left to right proceed by rule induction on the definition of the cost dynamics. From
right to left proceed by induction on k, with an inner rule induction on the definition of the struc-
tural dynamics.
7.5
Notes
The structural similarity between evaluation dynamics and typing rules was first developed in
The Definition of Standard ML (Milner et al., 1997). The advantage of evaluation dynamics is its
directness; its disadvantage is that it is not well-suited to proving properties such as type safety.
Robin Milner introduced the apt phrase “going wrong” as a description of a type error. Cost
dynamics was introduced by Blelloch and Greiner (1996) in a study of parallel computation (see
Chapter 37).
Exercises
7.1. Show that evaluation is deterministic: if e ⇓v1 and e ⇓v2, then v1 = v2.

PREVIEW
62
7.5 Notes
7.2. Complete the proof of Lemma 7.3.
7.3. Complete the proof of Lemma 7.4. Then show that if e 7−→∗e′ with e′ val, then e ⇓e′.
7.4. Augment the evaluation dynamics with checked errors, along the lines sketched in Chap-
ter 5, using e ?? to say that e incurs a checked (or an unchecked) error. What remains unsat-
isfactory about the type safety proof? Can you think of a better alternative?
7.5. Consider generic hypothetical judgments of the form
x1 ⇓v1, . . . , xn ⇓vn ⊢e ⇓v
where v1 val, . . . , vn val, and v val. The hypotheses, written ∆, are called the environment of
the evaluation; they provide the values of the free variables in e. The hypothetical judgment
∆⊢e ⇓v is called an environmental evaluation dynamics.
Give a hypothetical inductive definition of the environmental evaluation dynamics without
making any use of substitution. In particular, you should include the rule
∆, x ⇓v ⊢x ⇓v
defining the evaluation of a free variable.
Show that x1 ⇓v1, . . . , xn ⇓vn ⊢e ⇓v iff [v1, . . . , vn/x1, . . . , xn]e ⇓v (using the by-value
form of evaluation).

PREVIEW
Part III
Total Functions

PREVIEW

PREVIEW
Chapter 8
Function Definitions and Values
In the language E we may perform calculations such as the doubling of a given expression, but we
cannot express doubling as a concept in itself. To capture the pattern of doubling a number, we
abstract away from the particular number being doubled using a variable to stand for a fixed, but
unspecified, number, to express the doubling of an arbitrary number. Any particular instance of
doubling may then be obtained by substituting a numeric expression for that variable. In general
an expression may involve many distinct variables, necessitating that we specify which of several
possible variables is varying in a particular context, giving rise to a function of that variable.
In this chapter we will consider two extensions of E with functions. The first, and perhaps
most obvious, extension is by adding function definitions to the language. A function is defined by
binding a name to an abt with a bound variable that serves as the argument of that function. A
function is applied by substituting a particular expression (of suitable type) for the bound variable,
obtaining an expression.
The domain and range of defined functions are limited to the types nat and str, because these
are the only types of expression. Such functions are called first-order functions, in contrast to higher-
order functions, which permit functions as arguments and results of other functions. Because the
domain and range of a function are types, this requires that we introduce function types whose
elements are functions. Consequently, we may form functions of higher type, those whose domain
and range may themselves be function types.
8.1
First-Order Functions
The language ED extends E with function definitions and function applications as described by
the following grammar:
Exp
e
::=
apply{ f }( e )
f ( e )
application
fun{τ1 ; τ2}( x1 . e2 ; f . e )
fun f ( x1 : τ1 ) : τ2 = e2 in e
definition
The expression fun{τ1 ; τ2}( x1 . e2 ; f . e ) binds the function name f within e to the pattern x1 . e2,
which has argument x1 and definition e2. The domain and range of the function are, respectively,

PREVIEW
66
8.1 First-Order Functions
the types τ1 and τ2. The expression apply{ f }( e ) instantiates the binding of f with the argument
e.
The statics of ED defines two forms of judgment:
1. Expression typing, e : τ, stating that e has type τ;
2. Function typing, f ( τ1 ) : τ2, stating that f is a function with argument type τ1 and result
type τ2.
The judgment f ( τ1 ) : τ2 is called the function header of f; it specifies the domain type and the
range type of a function.
The statics of ED is defined by the following rules:
Γ, x1 : τ1 ⊢e2 : τ2
Γ, f ( τ1 ) : τ2 ⊢e : τ
Γ ⊢fun{τ1 ; τ2}( x1 . e2 ; f . e ) : τ
(8.1a)
Γ ⊢f ( τ1 ) : τ2
Γ ⊢e1 : τ1
Γ ⊢apply{ f }( e1 ) : τ2
(8.1b)
Function substitution, written [[x1 . e2/ f ]]e, is defined by induction on the structure of e much
like ordinary substitution. However, a function name f can only occur in an application of the
form apply{ f }( e1 ), so we arrange that the function body is expanded with the argument during
substitution. Thus, function substitution is defined by the following rule:
[[x1 . e2/ f ]]apply{ f }( e1 ) = let( [[x1 . e2/ f ]]e1 ; x1 . e2 )
(8.2)
At application sites to f with argument e1, function substitution yields a let expression that binds
x1 to the result of expanding any further applications to f within e1.
Lemma 8.1. If Γ, f ( τ1 ) : τ2 ⊢e : τ and Γ, x1 : τ1 ⊢e2 : τ2, then Γ ⊢[[x1 . e2/ f ]]e : τ.
Proof. By rule induction on the first premise, similarly to the proof of Lemma 4.4.
The dynamics of ED is defined using function substitution:
fun{τ1 ; τ2}( x1 . e2 ; f . e ) 7−→[[x1 . e2/ f ]]e
(8.3)
Because function substitution replaces all applications of f by appropriate let expressions, there
is no need to give a rule for application expressions (essentially, they behave like variables that are
replaced during evaluation, and not like a primitive operation of the language.)
The safety of ED may, with some effort, be derived from the safety theorem for higher-order
functions, which we discuss next.

PREVIEW
8.2 Higher-Order Functions
67
8.2
Higher-Order Functions
The similarity between variable definitions and function definitions in ED is striking. Is it possible
to combine them? The gap that must be bridged is the segregation of functions from expressions.
A function name f is bound to an abstractor x . e specifying a pattern that is instantiated when f is
applied. To reduce function definitions to ordinary definitions, we reify the abstractor into a form
of expression, called a λ-abstraction, written λ{τ1}( x . e ). Applications generalize to ap( e1 ; e2 ),
where e1 is an expression denoting a function, and not just a function name. λ-abstraction and
application are the introduction and elimination forms for the function type arr( τ1 ; τ2 ), which
classifies functions with domain τ1 and range τ2.
The language EF enriches E with function types, as specified by the following grammar:
Typ
τ
::=
arr( τ1 ; τ2 )
τ1 →τ2
function
Exp
e
::=
λ{τ}( x . e )
λ ( x : τ ) e
abstraction
ap( e1 ; e2 )
e1( e2 )
application
In EF functions are first-class in that they are a form of expression that can be used like any other. In
particular functions may be passed as arguments to, and returned as results from, other functions.
For this reason first-class functions are said to be higher-order, rather than first-order.
The statics of EF is given by extending rules (4.1) with the following rules:
Γ, x : τ1 ⊢e : τ2
Γ ⊢λ{τ1}( x . e ) : τ1 →τ2
(8.4a)
Γ ⊢e1 : τ2 →τ
Γ ⊢e2 : τ2
Γ ⊢ap( e1 ; e2 ) : τ
(8.4b)
Lemma 8.2 (Inversion). Suppose that Γ ⊢e : τ.
1. If e = λ{τ1}( x . e2 ), then τ = τ1 →τ2 and Γ, x : τ1 ⊢e2 : τ2.
2. If e = ap( e1 ; e2 ), then there exists τ2 such that Γ ⊢e1 : τ2 →τ and Γ ⊢e2 : τ2.
Proof. The proof proceeds by rule induction on the typing rules. Observe that for each rule, exactly
one case applies, and that the premises of the rule provide the required result.
Lemma 8.3 (Substitution). If Γ, x : τ ⊢e′ : τ′, and Γ ⊢e : τ, then Γ ⊢[e/x]e′ : τ′.
Proof. By rule induction on the derivation of the first judgment.
The dynamics of EF extends that of E with the following rules:
λ{τ}( x . e ) val
(8.5a)
e1 7−→e′
1
ap( e1 ; e2 ) 7−→ap( e′
1 ; e2 )
(8.5b)

PREVIEW
68
8.2 Higher-Order Functions


e1 val
e2 7−→e′
2
ap( e1 ; e2 ) 7−→ap( e1 ; e′
2 )


(8.5c)
[e2 val]
ap( λ{τ2}( x . e1 ) ; e2 ) 7−→[e2/x]e1
(8.5d)
The bracketed rule and premise are included for a call-by-value interpretation of function applica-
tion, and excluded for a call-by-name interpretation.1
When functions are first-class, there is no need for function declarations: simply replace the
function declaration fun f ( x1 : τ1 ) : τ2 = e2 in e by the definition let f be λ ( x : τ1 ) e2 in e, and re-
place second-class function application f ( e ) by the first-class function application f ( e ). Because
λ-abstractions are values, it makes no difference whether the definition is evaluated by-value or
by-name for this replacement to make sense. However, using ordinary definitions we may, for
example, give a name to a partially applied function, as in the following example:
let k be λ ( x1 : num ) λ ( x2 : num ) x1
in let kz be k( 0 ) in kz( 3 ) + kz( 5 ).
Without first-class functions, we cannot even form the function k, which returns a function as
result when applied to its first argument.
Theorem 8.4 (Preservation). If e : τ and e 7−→e′, then e′ : τ.
Proof. The proof is by induction on rules (8.5), which define the dynamics of the language.
Consider rule (8.5d),
ap( λ{τ2}( x . e1 ) ; e2 ) 7−→[e2/x]e1
.
Suppose that ap( λ{τ2}( x . e1 ) ; e2 ) : τ1. By Lemma 8.2 we have e2 : τ2 and x : τ2 ⊢e1 : τ1, so by
Lemma 8.3 [e2/x]e1 : τ1.
The other rules governing application are handled similarly.
Lemma 8.5 (Canonical Forms). If e : τ1 →τ2 and e val, then e = λ ( x : τ1 ) e2 for some variable x and
expression e2 such that x : τ1 ⊢e2 : τ2.
Proof. By induction on the typing rules, using the assumption e val.
Theorem 8.6 (Progress). If e : τ, then either e val, or there exists e′ such that e 7−→e′.
Proof. The proof is by induction on rules (8.4). Note that because we consider only closed terms,
there are no hypotheses on typing derivations.
Consider rule (8.4b) (under the by-name interpretation). By induction either e1 val or e1 7−→e′
1.
In the latter case we have ap( e1 ; e2 ) 7−→ap( e′
1 ; e2 ). In the former case, we have by Lemma 8.5
that e1 = λ{τ2}( x . e ) for some x and e. But then ap( e1 ; e2 ) 7−→[e2/x]e.
1Although the term “call-by-value” is accurately descriptive, the origin of the term “call-by-name” remains shrouded
in mystery.

PREVIEW
8.3 Evaluation Dynamics and Definitional Equality
69
8.3
Evaluation Dynamics and Definitional Equality
An inductive definition of the evaluation judgment e ⇓v for EF is given by the following rules:
λ{τ}( x . e ) ⇓λ{τ}( x . e )
(8.6a)
e1 ⇓λ{τ}( x . e )
[e2/x]e ⇓v
ap( e1 ; e2 ) ⇓v
(8.6b)
It is easy to check that if e ⇓v, then v val, and that if e val, then e ⇓e.
Theorem 8.7. e ⇓v iff e 7−→∗v and v val.
Proof. In the forward direction we proceed by rule induction on rules (8.6), following along similar
lines as the proof of Theorem 7.2.
In the reverse direction we proceed by rule induction on rules (5.1). The proof relies on an
analog of Lemma 7.4, which states that evaluation is closed under converse execution, which is
proved by induction on rules (8.5).
Definitional equality for the call-by-name dynamics of EF is defined by extension of rules (5.10).
Γ ⊢ap( λ{τ}( x . e2 ) ; e1 ) ≡[e1/x]e2 : τ2
(8.7a)
Γ ⊢e1 ≡e′
1 : τ2 →τ
Γ ⊢e2 ≡e′
2 : τ2
Γ ⊢ap( e1 ; e2 ) ≡ap( e′
1 ; e′
2 ) : τ
(8.7b)
Γ, x : τ1 ⊢e2 ≡e′
2 : τ2
Γ ⊢λ{τ1}( x . e2 ) ≡λ{τ1}( x . e′
2 ) : τ1 →τ2
(8.7c)
Definitional equality for call-by-value requires a bit more machinery. The main idea is to re-
strict rule (8.7a) to require that the argument be a value. In addition values must be expanded
to include variables, because in call-by-value, the argument variable of a function stands for the
value of its argument. The call-by-value definitional equality judgment takes the form
Γ ⊢e1 ≡e2 : τ,
where Γ consists of paired hypotheses x : τ, x val stating, for each variable x in scope, its type
and that it is a value. We write Γ ⊢e val to show that e is a value under these hypotheses, so that
x : τ, x val ⊢x val.

PREVIEW
70
8.4 Dynamic Scope
8.4
Dynamic Scope
The dynamics of function application given by rules (8.5) is defined only for expressions without
free variables. When a function is applied, the argument is substituted for the argument variable,
ensuring that the result remains closed. Moreover, because substitution of closed expressions can
never incur capture, the scopes of variables are not disturbed by the dynamics, ensuring that the
principles of binding and scope described in Chapter 1 are respected. This treatment of variables
is called static scoping, or static binding, to contrast it with an alternative approach that we now
describe.
Another approach, called dynamic scoping, or dynamic binding, is sometimes advocated as an
alternative to static binding. The crucial difference is that with dynamic scoping the principle of
identification of abts up to renaming of bound variables is denied. Consequently, capture-avoiding
substitution is not available. Instead evaluation is defined for open terms, with the bindings of
free variables provided by an environment mapping variable names to (possibly open) values. The
binding of a variable is determined as late as possible, at the point where the variable is evalu-
ated, rather than where it is bound. If the environment does not provide a binding for a variable,
evaluation is aborted with a run-time error.
For first-order functions dynamic and static scoping coincide, but in the higher-order case the
two approaches diverge. For example, there is no difference between static and dynamic scope
when it comes to evaluation of an expression such as ( λ ( x : num ) x + 7 )( 42 ). Whether 42 is
substituted for x in the body of the function before evaluation, or the body is evaluated in the
presence of the binding of x to 42, the outcome is the same.
In the higher-order case the equivalence of static and dynamic scope breaks down. For exam-
ple, consider the expression
e ≜( λ ( x : num ) λ ( y : num ) x + y )( 42 ).
With static scoping e evaluates to the closed value v ≜λ ( y : num ) 42 + y, which, if applied, would
add 42 to its argument. It makes no difference how the bound variable x is chosen, the outcome
will always be the same. With dynamic scoping e evaluates to the open value v′ ≜λ ( y : num ) x + y
in which the variable x occurs free. When this expression is evaluated, the variable x is bound to
42, but this is irrelevant because the binding is not needed to evaluate the λ-abstraction. The
binding of x is not retrieved until such time as v′ is applied to an argument, at which point the
binding for x in force at that time is retrieved, and not the one in force at the point where e is
evaluated.
Therein lies the difference. For example, consider the expression
e′ ≜( λ ( f : num →num ) ( λ ( x : num ) f ( 0 ) )( 7 ) )( e ).
When evaluated using dynamic scope, the value of e′ is 7, whereas its value is 42 under static
scope. The discrepancy can be traced to the re-binding of x to 7 before the value of e, namely v′, is
applied to 0, altering the outcome.
Dynamic scope violates the basic principle that variables are given meaning by capture-avoiding
substitution as defined in Chapter 1. Violating this principle has at least two undesirable conse-
quences. One is that the names of bound variables matter, in contrast to static scope which obeys

PREVIEW
8.5 Notes
71
the identification principle. For example, had the innermost λ-abstraction of e′ bound the vari-
able y, rather than x, then its value would have been 42, rather than 7. Thus one component of a
program may be sensitive to the names of bound variables chosen in another component, a clear
violation of modular decomposition.
Another problem is that dynamic scope is not, in general, type-safe. For example, consider the
expression
e′ ≜( λ ( f : num →num ) ( λ ( x : str ) f ( ”zero” ) )( 7 ) )( e ).
Under dynamic scoping this expression gets stuck attempting to evaluate x + y with x bound to
the string ”zero”, and no further progress can be made. For this reason dynamic scope is only ever
advocated for so-called dynamically typed languages, which replace static consistency checks by
dynamic consistency checks to ensure a weak form of progress. Compile-time errors are thereby
transformed into run-time errors.
(For more on dynamic typing, see Chapter 22, and for more on dynamic scope, see Chapter 32.)
8.5
Notes
Nearly all programming languages provide some form of function definition mechanism of the
kind illustrated here. The main point of the present account is to demonstrate that a more natural,
and more powerful, approach is to separate the generic concept of a definition from the specific
concept of a function. Function types codify the general notion in a systematic way that encom-
passes function definitions as a special case, and moreover, admits passing functions as arguments
and returning them as results without special provision. The essential contribution of Church’s λ-
calculus (Church, 1941) was to take functions as primary, and to show that nothing more is needed
to get a fully expressive programming language.
Exercises
8.1. Formulate an environmental evaluation dynamics (see Exercise 7.5) for ED. Hint: Introduce
a new form of judgment for evaluation of function identifiers.
8.2. Consider an environmental dynamics for EF, which includes higher-order functions. What
difficulties arise? Can you think of a way to evade these difficulties? Hint: One approach is
to “substitute away” all free variables in a λ-abstraction at the point at which it is evaluated.
The second is to “freeze” the values of each of the free variables in a λ-abstraction, and to
“thaw” them when such a function is applied. What problems arise in each case?

PREVIEW
72
8.5 Notes

PREVIEW
Chapter 9
System T of Higher-Order Recursion
System T, well-known as G¨odel’s T, is the combination of function types with the type of natural
numbers. In contrast to E, which equips the naturals with some arbitrarily chosen arithmetic
operations, the language T provides a general mechanism, called primitive recursion, from which
these primitives may be defined. Primitive recursion captures the essential inductive character of
the natural numbers, and hence may be seen as an intrinsic termination proof for each program in
the language. Consequently, we may only define total functions in the language, those that always
return a value for each argument. In essence every program in T “comes equipped” with a proof
of its termination. Although this may seem like a shield against infinite loops, it is also a weapon
that can be used to show that some programs cannot be written in T. To do so would demand
a master termination proof for every possible program in the language, something that we shall
prove does not exist.
9.1
Statics
The syntax of T is given by the following grammar:
Typ
τ
::=
nat
nat
naturals
arr( τ1 ; τ2 )
τ1 →τ2
function
Exp
e
::=
x
x
variable
z
z
zero
s( e )
s( e )
successor
rec{τ}( e ; e0 ; x . y . e1 )
rec e {z ,→e0 | s( x ) with y ,→e1}
recursion
λ{τ}( x . e )
λ ( x : τ ) e
abstraction
ap( e1 ; e2 )
e1( e2 )
application
We write n for the expression s( . . . s( z ) ), in which the successor is applied n ≥0 times to zero.
The expression rec{τ}( e ; e0 ; x . y . e1 ) is called the recursor. It represents the e-fold iteration of the

PREVIEW
74
9.2 Dynamics
transformation x . y . e1 starting from e0. The bound variable x represents the predecessor and the
bound variable y represents the result of the x-fold iteration. The “with” clause in the concrete
syntax for the recursor binds the variable y to the result of the recursive call, as will become clear
shortly.
Sometimes the iterator, iter{τ}( e ; e0 ; y . e1 ), is considered as an alternative to the recursor. It
has essentially the same meaning as the recursor, except that only the result of the recursive call is
bound to y in e1, and no binding is made for the predecessor. Clearly the iterator is a special case
of the recursor, because we can always ignore the predecessor binding. Conversely, the recursor is
definable from the iterator, provided that we have product types (Chapter 10) at our disposal. To
define the recursor from the iterator, we simultaneously compute the predecessor while iterating
the specified computation.
The statics of T is given by the following typing rules:
Γ, x : τ ⊢x : τ
(9.1a)
Γ ⊢z : nat
(9.1b)
Γ ⊢e : nat
Γ ⊢s( e ) : nat
(9.1c)
Γ ⊢e : nat
Γ ⊢e0 : τ
Γ, x : nat, y : τ ⊢e1 : τ
Γ ⊢rec{τ}( e ; e0 ; x . y . e1 ) : τ
(9.1d)
Γ, x : τ1 ⊢e : τ2
Γ ⊢λ{τ1}( x . e ) : arr( τ1 ; τ2 )
(9.1e)
Γ ⊢e1 : arr( τ2 ; τ )
Γ ⊢e2 : τ2
Γ ⊢ap( e1 ; e2 ) : τ
(9.1f)
As usual, admissibility of the structural rule of substitution is crucially important.
Lemma 9.1. If Γ ⊢e : τ and Γ, x : τ ⊢e′ : τ′, then Γ ⊢[e/x]e′ : τ′.
9.2
Dynamics
The closed values of T are defined by the following rules:
z val
(9.2a)
[e val]
s( e ) val
(9.2b)
λ{τ}( x . e ) val
(9.2c)

PREVIEW
9.2 Dynamics
75
The premise of rule (9.2b) is included for an eager interpretation of successor, and excluded for a
lazy interpretation.
The transition rules for the dynamics of T are as follows:


e 7−→e′
s( e ) 7−→s( e′ )


(9.3a)
e1 7−→e′
1
ap( e1 ; e2 ) 7−→ap( e′
1 ; e2 )
(9.3b)


e1 val
e2 7−→e′
2
ap( e1 ; e2 ) 7−→ap( e1 ; e′
2 )


(9.3c)
[e2 val]
ap( λ{τ}( x . e ) ; e2 ) 7−→[e2/x]e
(9.3d)
e 7−→e′
rec{τ}( e ; e0 ; x . y . e1 ) 7−→rec{τ}( e′ ; e0 ; x . y . e1 )
(9.3e)
rec{τ}( z ; e0 ; x . y . e1 ) 7−→e0
(9.3f)
s( e ) val
rec{τ}( s( e ) ; e0 ; x . y . e1 ) 7−→[e, rec{τ}( e ; e0 ; x . y . e1 )/x, y]e1
(9.3g)
The bracketed rules and premises are included for an eager successor and call-by-value applica-
tion, and omitted for a lazy successor and call-by-name application. Rules (9.3f) and (9.3g) specify
the behavior of the recursor on z and s( e ). In the former case the recursor reduces to e0, and in
the latter case the variable x is bound to the predecessor e and y is bound to the (unevaluated)
recursion on e. If the value of y is not required in the rest of the computation, the recursive call is
not evaluated.
Lemma 9.2 (Canonical Forms). If e : τ and e val, then
1. If τ = nat, then either e = z or e = s( e′ ) for some e′.
2. If τ = τ1 →τ2, then e = λ ( x : τ1 ) e2 for some e2.
Theorem 9.3 (Safety).
1. If e : τ and e 7−→e′, then e′ : τ.
2. If e : τ, then either e val or e 7−→e′ for some e′.

PREVIEW
76
9.3 Definability
9.3
Definability
A mathematical function f : N →N on the natural numbers is definable in T iff there exists an
expression e f of type nat →nat such that for every n ∈N,
e f ( n ) ≡f (n) : nat.
(9.4)
That is, the numeric function f : N →N is definable iff there is an expression e f of type nat →nat
such that, when applied to the numeral representing the argument n ∈N, the application is
definitionally equal to the numeral corresponding to f (n) ∈N.
Definitional equality for T, written Γ ⊢e ≡e′ : τ, is the strongest congruence containing these
axioms:
Γ, x : τ1 ⊢e2 : τ2
Γ ⊢e1 : τ1
Γ ⊢ap( λ{τ1}( x . e2 ) ; e1 ) ≡[e1/x]e2 : τ2
(9.5a)
Γ ⊢e0 : τ
Γ, x : nat, y : τ ⊢e1 : τ
Γ ⊢rec{τ}( z ; e0 ; x . y . e1 ) ≡e0 : τ
(9.5b)
Γ ⊢e0 : τ
Γ, x : nat, y : τ ⊢e1 : τ
Γ ⊢rec{τ}( s( e ) ; e0 ; x . y . e1 ) ≡[e, rec{τ}( e ; e0 ; x . y . e1 )/x, y]e1 : τ
(9.5c)
For example, the doubling function, d(n) = 2 × n, is definable in T by the expression ed :
nat →nat given by
λ ( x : nat ) rec x {z ,→z | s( u ) with v ,→s( s( v ) )}.
To check that this defines the doubling function, we proceed by induction on n ∈N. For the basis,
it is easy to check that
ed( 0 ) ≡0 : nat.
For the induction, assume that
ed( n ) ≡d(n) : nat.
Then calculate using the rules of definitional equality:
ed( n + 1 ) ≡s( s( ed( n ) ) )
≡s( s( 2 × n ) )
= 2 × (n + 1)
= d(n + 1).
As another example, consider the following function, called Ackermann’s function, defined by
the following equations:
A(0, n) = n + 1
A(m + 1, 0) = A(m, 1)
A(m + 1, n + 1) = A(m, A(m + 1, n)).

PREVIEW
9.4 Undefinability
77
The Ackermann function grows very quickly. For example, A(4, 2) ≈265,536, which is often cited
as being larger than the number of atoms in the universe! Yet we can show that the Ackermann
function is total by a lexicographic induction on the pair of arguments (m, n). On each recursive
call, either m decreases, or else m remains the same, and n decreases, so inductively the recursive
calls are well-defined, and hence so is A(m, n).
A first-order primitive recursive function is a function of type nat →nat that is defined using the
recursor, but without using any higher order functions. Ackermann’s function is defined so that it
is not first-order primitive recursive, but is higher-order primitive recursive. The key to showing
that it is definable in T is to note that A(m + 1, n) iterates n times the function A(m, −), starting
with A(m, 1). As an auxiliary, let us define the higher-order function
it : ( nat →nat ) →nat →nat →nat
to be the λ-abstraction
λ ( f : nat →nat ) λ ( n : nat ) rec n {z ,→id | s( ) with g ,→f ◦g},
where id = λ ( x : nat ) x is the identity, and f ◦g = λ ( x : nat ) f ( g( x ) ) is the composition of f
and g. It is easy to check that
it( f )( n )( m ) ≡f (n)(m) : nat,
where the latter expression is the n-fold composition of f starting with m. We may then define the
Ackermann function
ea : nat →nat →nat
to be the expression
λ ( m : nat ) rec m {z ,→s | s( ) with f ,→λ ( n : nat ) it( f )( n )( f ( 1 ) )}.
It is instructive to check that the following equivalences are valid:
ea( 0 )( n ) ≡s( n )
(9.6)
ea( m + 1 )( 0 ) ≡ea( m )( 1 )
(9.7)
ea( m + 1 )( n + 1 ) ≡ea( m )( ea( s( m ) )( n ) ).
(9.8)
That is, the Ackermann function is definable in T.
9.4
Undefinability
It is impossible to define an infinite loop in T.
Theorem 9.4. If e : τ, then there exists v val such that e ≡v : τ.
Proof. See Corollary 46.15.

PREVIEW
78
9.4 Undefinability
Consequently, values of function type in T behave like mathematical functions: if e : τ1 →τ2
and e1 : τ1, then e( e1 ) evaluates to a value of type τ2. Moreover, if e : nat, then there exists a
natural number n such that e ≡n : nat.
Using this, we can show, using a technique called diagonalization, that there are functions on the
natural numbers that are not definable in T. We make use of a technique, called G¨odel-numbering,
that assigns a unique natural number to each closed expression of T. By assigning a unique num-
ber to each expression, we may manipulate expressions as data values in T so that T is able to
compute with its own programs.1
The essence of G¨odel-numbering is captured by the following simple construction on abstract
syntax trees. (The generalization to abstract binding trees is slightly more difficult, the main com-
plication being to ensure that all α-equivalent expressions are assigned the same G¨odel number.)
Recall that a general ast a has the form o( a1, . . . , ak ), where o is an operator of arity k. Enumer-
ate the operators so that every operator has an index i ∈N, and let m be the index of o in this
enumeration. Define the G¨odel number ⌜a⌝of a to be the number
2m 3n1 5n2 . . . pnk
k ,
where pk is the kth prime number (so that p0 = 2, p1 = 3, and so on), and n1, . . . , nk are the
G¨odel numbers of a1, . . . , ak, respectively. This procedure assigns a natural number to each ast.
Conversely, given a natural number, n, we may apply the prime factorization theorem to “parse”
n as a unique abstract syntax tree. (If the factorization is not of the right form, which can only be
because the arity of the operator does not match the number of factors, then n does not code any
ast.)
Now, using this representation, we may define a (mathematical) function funiv : N →N →N
such that, for any e : nat →nat, funiv(⌜e⌝)(m) = n iff e( m ) ≡n : nat.2 The determinacy of the
dynamics, together with Theorem 9.4, ensure that funiv is a well-defined function. It is called the
universal function for T because it specifies the behavior of any expression e of type nat →nat.
Using the universal function, let us define an auxiliary mathematical function, called the diagonal
function δ : N →N, by the equation δ(m) = funiv(m)(m). The δ function is chosen so that
δ(⌜e⌝) = n iff e( ⌜e⌝) ≡n : nat. (The motivation for its definition will become clear in a moment.)
The function funiv is not definable in T. Suppose that it were definable by the expression euniv,
then the diagonal function δ would be definable by the expression
eδ = λ ( m : nat ) euniv( m )( m ).
But in that case we would have the equations
eδ( ⌜e⌝) ≡euniv( ⌜e⌝)( ⌜e⌝)
≡e( ⌜e⌝).
Now let e∆be the function expression
λ ( x : nat ) s( eδ( x ) ),
1The same technique lies at the heart of the proof of G¨odel’s celebrated incompleteness theorem. The undefinability of
certain functions on the natural numbers within T may be seen as a form of incompleteness like that considered by G¨odel.
2The value of funiv(k)(m) may be chosen arbitrarily to be zero when k is not the code of any expression e.

PREVIEW
9.5 Notes
79
so that we may deduce
e∆( ⌜e∆⌝) ≡s( eδ( ⌜e∆⌝) )
≡s( e∆( ⌜e∆⌝) ).
But the termination theorem implies that there exists n such that e∆( ⌜e∆⌝) ≡n, and hence we
have n ≡s( n ), which is impossible.
We say that a language L is universal if it is possible to write an interpreter for L in L itself.
It is intuitively clear that funiv is computable in the sense that we can define it in some sufficiently
powerful programming language. But the preceding argument shows that T is not up to the
task; it is not a universal programming language. Examination of the foregoing proof reveals an
inescapable tradeoff: by insisting that all expressions terminate, we necessarily lose universality—
there are computable functions that are not definable in the language.
9.5
Notes
System T was introduced by G¨odel in his study of the consistency of arithmetic (G¨odel, 1980). He
showed how to “compile” proofs in arithmetic into well-typed terms of system T, and to reduce
the consistency problem for arithmetic to the termination of programs in T. It was perhaps the first
programming language whose design was directly influenced by the verification (of termination)
of its programs.
Exercises
9.1. Prove Lemma 9.2.
9.2. Prove Theorem 9.3.
9.3. Attempt to prove that if e : nat is closed, then there exists n such that e 7−→∗n under the
eager dynamics. Where does the proof break down?
9.4. Attempt to prove termination for all well-typed closed terms: if e : τ, then there exists e′ val
such that e 7−→∗e′. You are free to appeal to Lemma 9.2 and Theorem 9.3 as necessary. Where
does the attempt break down? Can you think of a stronger inductive hypothesis that might
evade the difficulty?
9.5. Define a closed term e of type τ in T to be hereditarily terminating at type τ by induction on
the structure of τ as follows:
(a) If τ = nat, then e is hereditarily terminating at type τ iff e is terminating (that is, iff
e 7−→∗n for some n.)
(b) If τ = τ1 →τ2, then e is hereditarily terminating iff when e1 is hereditarily terminating
at type τ1, then e( e1 ) is hereditarily terminating at type τ2.

PREVIEW
80
9.5 Notes
Attempt to prove hereditary termination for well-typed terms: if e : τ, then e is hereditarily
terminating at type τ. The stronger inductive hypothesis bypasses the difficulty that arose
in Exercise 9.4, but introduces another obstacle. What is the complication? Can you think of
an even stronger inductive hypothesis that would suffice for the proof?
9.6. Show that if e is hereditarily terminating at type τ, e′ : τ, and e′ 7−→e, then e′ is also hered-
itarily terminating at type τ. (The need for this result will become clear in the solution to
Exercise 9.5.)
9.7. Define an open, well-typed term
x1 : τ1, . . . , xn : τn ⊢e : τ
to be open hereditarily terminating iff every substitution instance
[e1, . . . , en/x1, . . . , xn]e
is closed hereditarily terminating at type τ when each ei is closed hereditarily terminating at
type τi for each 1 ≤i ≤n. Derive Exercise 9.3 from this result.

PREVIEW
Part IV
Finite Data Types

PREVIEW

PREVIEW
Chapter 10
Product Types
The binary product of two types consists of ordered pairs of values, one from each type in the or-
der specified. The associated elimination forms are projections, which select the first and second
component of a pair. The nullary product, or unit, type consists solely of the unique “null tuple”
of no values, and has no associated elimination form. The product type admits both a lazy and an
eager dynamics. According to the lazy dynamics, a pair is a value without regard to whether its
components are values; they are not evaluated until (if ever) they are accessed and used in another
computation. According to the eager dynamics, a pair is a value only if its components are values;
they are evaluated when the pair is created.
More generally, we may consider the finite product, ⟨τi⟩i∈I, indexed by a finite set of indices I.
The elements of the finite product type are I-indexed tuples whose ith component is an element
of the type τi, for each i ∈I. The components are accessed by I-indexed projection operations,
generalizing the binary case. Special cases of the finite product include n-tuples, indexed by sets
of the form I = { 0, . . . , n −1 }, and labeled tuples, or records, indexed by finite sets of symbols.
Similarly to binary products, finite products admit both an eager and a lazy interpretation.
10.1
Nullary and Binary Products
The abstract syntax of products is given by the following grammar:
Typ
τ
::=
unit
unit
nullary product
prod( τ1 ; τ2 )
τ1 × τ2
binary product
Exp
e
::=
triv
⟨⟩
null tuple
pair( e1 ; e2 )
⟨e1, e2⟩
ordered pair
pr[ l ]( e )
e · l
left projection
pr[ r ]( e )
e · r
right projection
There is no elimination form for the unit type, there being nothing to extract from the null tuple.

PREVIEW
84
10.1 Nullary and Binary Products
The statics of product types is given by the following rules.
Γ ⊢⟨⟩: unit
(10.1a)
Γ ⊢e1 : τ1
Γ ⊢e2 : τ2
Γ ⊢⟨e1, e2⟩: τ1 × τ2
(10.1b)
Γ ⊢e : τ1 × τ2
Γ ⊢e · l : τ1
(10.1c)
Γ ⊢e : τ1 × τ2
Γ ⊢e · r : τ2
(10.1d)
The dynamics of product types is defined by the following rules:
⟨⟩val
(10.2a)
[e1 val]
[e2 val]
⟨e1, e2⟩val
(10.2b)


e1 7−→e′
1
⟨e1, e2⟩7−→⟨e′
1, e2⟩


(10.2c)


e1 val
e2 7−→e′
2
⟨e1, e2⟩7−→⟨e1, e′
2⟩


(10.2d)
e 7−→e′
e · l 7−→e′ · l
(10.2e)
e 7−→e′
e · r 7−→e′ · r
(10.2f)
[e1 val]
[e2 val]
⟨e1, e2⟩· l 7−→e1
(10.2g)
[e1 val]
[e2 val]
⟨e1, e2⟩· r 7−→e2
(10.2h)
The bracketed rules and premises are omitted for a lazy dynamics, and included for an eager
dynamics of pairing.
The safety theorem applies to both the eager and the lazy dynamics, with the proof proceeding
along similar lines in each case.

PREVIEW
10.2 Finite Products
85
Theorem 10.1 (Safety).
1. If e : τ and e 7−→e′, then e′ : τ.
2. If e : τ then either e val or there exists e′ such that e 7−→e′.
Proof. Preservation is proved by induction on transition defined by rules (10.2). Progress is proved
by induction on typing defined by rules (10.1).
10.2
Finite Products
The syntax of finite product types is given by the following grammar:
Typ
τ
::=
prod( i ,→τi | i ∈I )
⟨τi⟩i∈I
product
Exp
e
::=
tpl( i ,→ei | i ∈I )
⟨i ,→ei | i ∈I⟩
tuple
pr[ i ]( e )
e · i
projection
The variable I stands for a finite index set over which products are formed. The type prod( i ,→τi |
i ∈I ), or ⟨τi⟩i∈I for short, is the type of I-tuples of expressions ei of type τi, one for each i ∈I. An
I-tuple has the form tpl( i ,→ei | i ∈I ), or ⟨i ,→ei | i ∈I⟩for short, and for each i ∈I the ith
projection from an I-tuple e is written pr[ i ]( e ), or e · i for short.
When I = { i1, . . . , in }, the I-tuple type may be written in the form
⟨i1 ,→τ1 , . . . , in ,→τn⟩
where we make explicit the association of a type to each index i ∈I. Similarly, we may write
⟨i1 ,→e1 , . . . , in ,→en⟩
for the I-tuple whose ith component is ei.
Finite products generalize empty and binary products by choosing I to be empty or the two-
element set { l, r }, respectively. In practice I is often chosen to be a finite set of symbols that serve
as labels for the components of the tuple to enhance readability.
The statics of finite products is given by the following rules:
Γ ⊢e1 : τ1
. . .
Γ ⊢en : τn
Γ ⊢⟨i1 ,→e1 , . . . , in ,→en⟩: ⟨i1 ,→τ1 , . . . , in ,→τn⟩
(10.3a)
Γ ⊢e : ⟨i1 ,→τ1 , . . . , in ,→τn⟩
(1 ≤k ≤n)
Γ ⊢e · ik : τk
(10.3b)
In rule (10.3b) the index ik ∈I is a particular element of the index set I, whereas in rule (10.3a), the
indices i1, . . . , in range over the entire index set I.
The dynamics of finite products is given by the following rules:
[e1 val
. . .
en val]
⟨i1 ,→e1 , . . . , in ,→en⟩val
(10.4a)

PREVIEW
86
10.3 Primitive Mutual Recursion


(e1 val
. . .
ej−1 val
e′
1 = e1
. . .
e′
j−1 = ej−1
ej 7−→e′
j
e′
j+1 = ej+1
. . .
e′
n = en
)
⟨i1 ,→e1 , . . . , in ,→en⟩7−→⟨i1 ,→e′
1 , . . . , in ,→e′
n⟩


(10.4b)
e 7−→e′
e · i 7−→e′ · i
(10.4c)
[⟨i1 ,→e1 , . . . , in ,→en⟩val]
⟨i1 ,→e1 , . . . , in ,→en⟩· ik 7−→ek
(10.4d)
As formulated, rule (10.4b) specifies that the components of a tuple are evaluated in some sequen-
tial order, without specifying the order in which the components are considered. It is not hard, but
a bit technically complicated, to impose an evaluation order by imposing a total ordering on the
index set and evaluating components according to this ordering.
Theorem 10.2 (Safety). If e : τ, then either e val or there exists e′ such that e′ : τ and e 7−→e′.
Proof. The safety theorem is decomposed into progress and preservation lemmas, which are proved
as in Section 10.1.
10.3
Primitive Mutual Recursion
Using products we may simplify the primitive recursion construct of T so that only the recursive
result on the predecessor, and not the predecessor itself, is passed to the successor branch. Writing
this as iter{τ}( e ; e0 ; x . e1 ), we may define rec{τ}( e ; e0 ; x . y . e1 ) to be e′ · r, where e’ is the
expression
iter e {z ,→⟨z, e0⟩| s( x′ ) ,→⟨s( x′ · l ), [x′ · l, x′ · r/x, y]e1⟩}.
The idea is to compute inductively both the number n and the result of the recursive call on n,
from which we can compute both n + 1 and the result of another recursion using e1. The base case
is computed directly as the pair of zero and e0. It is easy to check that the statics and dynamics of
the recursor are preserved by this definition.
We may also use product types to implement mutual primitive recursion, in which we define two
functions simultaneously by primitive recursion. For example, consider the following recursion
equations defining two mathematical functions on the natural numbers:
e(0) = 1
o(0) = 0
e(n + 1) = o(n)
o(n + 1) = e(n)
Intuitively, e(n) is non-zero if and only if n is even, and o(n) is non-zero if and only if n is odd.

PREVIEW
10.4 Notes
87
To define these functions in T enriched with products, we first define an auxiliary function eeo
of type
nat →( nat × nat )
that computes both results simultaneously by swapping back and forth on recursive calls:
λ ( n : nat ) iter n {z ,→⟨1, 0⟩| s( b ) ,→⟨b · r, b · l⟩}.
We may then define eev and eod as follows:
eev ≜λ ( n : nat ) eeo( n ) · l
eod ≜λ ( n : nat ) eeo( n ) · r.
10.4
Notes
Product types are the most basic form of structured data. All languages have some form of product
type, but often in a form that is combined with other, separable, concepts. Common manifestations
of products include: (1) functions with “multiple arguments” or “multiple results”; (2) “objects”
represented as tuples of mutually recursive functions; (3) “structures,” which are tuples with mu-
table components. There are many papers on finite product types, which include record types
as a special case. Pierce (2002) provides a thorough account of record types, and their subtyping
properties (for which, see Chapter 24). Allen et al. (2006) analyzes many of the key ideas in the
framework of dependent type theory.
Exercises
10.1. A database schema may be thought of as a finite product type ⟨τ⟩i∈I, in which the columns, or
attributes. are labeled by the indices I whose values are restricted to atomic types, such as nat
and str. A value of a schema type is called a tuple, or instance, of that schema. A database
may be thought of as a finite sequence of such tuples, called the rows of the database. Give a
representation of a database using function, product, and natural numbers types, and define
the project operation that sends a database with columns I to a database with columns I′ ⊆I
by restricting each row to the specified columns.
10.2. Rather than choose between a lazy and an eager dynamics for products, we can instead
distinguish two forms of product type, called the positive and the negative. The statics of the
negative product is given by rules (10.1), and the dynamics is lazy. The statics of the positive
product, written τ1 ⊗τ2, is given by the following rules:
Γ ⊢e1 : τ1
Γ ⊢e2 : τ2
Γ ⊢⊗( e1 ; e2 ) : τ1 ⊗τ2
(10.5a)
Γ ⊢e0 : τ1 ⊗τ2
Γ x1 : τ1 x2 : τ2 ⊢e : τ
Γ ⊢split( e0 ; x1, x2 . e ) : τ
(10.5b)

PREVIEW
88
10.4 Notes
The dynamics of fuse, the introduction form for the positive pair, is eager, essentially be-
cause the elimination form, split, extracts both components simultaneously.
Show that the negative product is definable in terms of the positive product using the unit
and function types to express the lazy dynamics of negative pairing. Show that the positive
product is definable in terms of the negative product, provided that we have at our disposal a
let expression with a by-value dynamics so that we may enforce eager evaluation of positive
pairs.
10.3. Specializing Exercise 10.2 to nullary products, we obtain a positive and a negative unit type.
The negative unit type is given by rules (10.1), with no elimination forms and one introduc-
tion form. Give the statics and dynamics for a positive unit type, and show that the positive
and negative unit types are inter-definable without any further assumptions.

PREVIEW
Chapter 11
Sum Types
Most data structures involve alternatives such as the distinction between a leaf and an interior
node in a tree, or a choice in the outermost form of a piece of abstract syntax. Importantly, the
choice determines the structure of the value. For example, nodes have children, but leaves do not,
and so forth. These concepts are expressed by sum types, specifically the binary sum, which offers a
choice of two things, and the nullary sum, which offers a choice of no things. Finite sums generalize
nullary and binary sums to allow an arbitrary number of cases indexed by a finite index set. As
with products, sums come in both eager and lazy variants, differing in how values of sum type are
defined.
11.1
Nullary and Binary Sums
The abstract syntax of sums is given by the following grammar:
Typ
τ
::=
void
void
nullary sum
sum( τ1 ; τ2 )
τ1 + τ2
binary sum
Exp
e
::=
case{τ}( e )
case e { }
null case
in[ l ]{τ1 ; τ2}( e )
l · e
left injection
in[ r ]{τ1 ; τ2}( e )
r · e
right injection
case( e ; x1 . e1 ; x2 . e2 )
case e {l · x1 ,→e1 | r · x2 ,→e2}
case analysis
The nullary sum represents a choice of zero alternatives, and hence admits no introduction form.
The elimination form, case e { }, expresses the contradiction that e is a value of type void. The
elements of the binary sum type are labeled to show whether they are drawn from the left or the
right summand, either l · e or r · e. A value of the sum type is eliminated by case analysis.
The statics of sum types is given by the following rules.
Γ ⊢e : void
Γ ⊢case e { } : τ
(11.1a)

PREVIEW
90
11.1 Nullary and Binary Sums
Γ ⊢e : τ1
Γ ⊢l · e : τ1 + τ2
(11.1b)
Γ ⊢e : τ2
Γ ⊢r · e : τ1 + τ2
(11.1c)
Γ ⊢e : τ1 + τ2
Γ, x1 : τ1 ⊢e1 : τ
Γ, x2 : τ2 ⊢e2 : τ
Γ ⊢case e {l · x1 ,→e1 | r · x2 ,→e2} : τ
(11.1d)
For the sake of readability, in rules (11.1b) and (11.1c) we have written l · e and r · e in place of
the abstract syntax in[ l ]{τ1 ; τ2}( e ) and in[ r ]{τ1 ; τ2}( e ), which includes the types τ1 and τ2
explicitly. In rule (11.1d) both branches of the case analysis must have the same type. Because
a type expresses a static “prediction” on the form of the value of an expression, and because an
expression of sum type could evaluate to either form at run-time, we must insist that both branches
yield the same type.
The dynamics of sums is given by the following rules:
e 7−→e′
case e { } 7−→case e′ { }
(11.2a)
[e val]
l · e val
(11.2b)
[e val]
r · e val
(11.2c)


e 7−→e′
l · e 7−→l · e′


(11.2d)


e 7−→e′
r · e 7−→r · e′


(11.2e)
e 7−→e′
case e {l · x1 ,→e1 | r · x2 ,→e2} 7−→case e′ {l · x1 ,→e1 | r · x2 ,→e2}
(11.2f)
[e val]
case l · e {l · x1 ,→e1 | r · x2 ,→e2} 7−→[e/x1]e1
(11.2g)
[e val]
case r · e {l · x1 ,→e1 | r · x2 ,→e2} 7−→[e/x2]e2
(11.2h)
The bracketed premises and rules are included for an eager dynamics, and excluded for a lazy
dynamics.
The coherence of the statics and dynamics is stated and proved as usual.

PREVIEW
11.2 Finite Sums
91
Theorem 11.1 (Safety).
1. If e : τ and e 7−→e′, then e′ : τ.
2. If e : τ, then either e val or e 7−→e′ for some e′.
Proof. The proof proceeds by induction on rules (11.2) for preservation, and by induction on
rules (11.1) for progress.
11.2
Finite Sums
Just as we may generalize nullary and binary products to finite products, so may we also gener-
alize nullary and binary sums to finite sums. The syntax for finite sums is given by the following
grammar:
Typ
τ
::=
sum( i ,→τi | i ∈I )
[τi]i∈I
sum
Exp
e
::=
in[ i ]{⃗τ}( e )
i · e
injection
case( e ; i ,→xi . ei | i ∈I )
case e {i · xi ,→ei | i ∈I}
case analysis
The variable I stands for a finite index set over which sums are formed. The notation ⃗τ stands for
a finite function (i ,→τi)i∈I for some index set I. The type sum( i ,→τi | i ∈I ), or [τi]i∈I for short,
is the type of I-classified values of the form in[ i ]{I}( ei ), or i · ei for short, where i ∈I and ei is
an expression of type τi. An I-classified value is analyzed by an I-way case analysis of the form
case( e ; i ,→xi . ei | i ∈I ).
When I = { i1, . . . , in }, the type of I-classified values may be written
[i1 ,→τ1 , . . . , in ,→τn]
specifying the type associated with each class li ∈I. Correspondingly, the I-way case analysis has
the form
case e {i1 · x1 ,→e1 | . . . | in · xn ,→en}.
Finite sums generalize empty and binary sums by choosing I to be empty or the two-element set
{ l, r }, respectively. In practice I is often chosen to be a finite set of symbols that serve as names
for the classes so as to enhance readability.
The statics of finite sums is defined by the following rules:
Γ ⊢e : τk
(1 ≤k ≤n)
Γ ⊢ik · e : [i1 ,→τ1 , . . . , in ,→τn]
(11.3a)
Γ ⊢e : [i1 ,→τ1 , . . . , in ,→τn]
Γ, x1 : τ1 ⊢e1 : τ
. . .
Γ, xn : τn ⊢en : τ
Γ ⊢case e {i1 · x1 ,→e1 | . . . | in · xn ,→en} : τ
(11.3b)
These rules generalize the statics for nullary and binary sums given in Section 11.1.
The dynamics of finite sums is defined by the following rules:
[e val]
i · e val
(11.4a)

PREVIEW
92
11.3 Applications of Sum Types


e 7−→e′
i · e 7−→i · e′


(11.4b)
e 7−→e′
case e {i · xi ,→ei | i ∈I} 7−→case e′ {i · xi ,→ei | i ∈I}
(11.4c)
i · e val
case i · e {i · xi ,→ei | i ∈I} 7−→[e/xi]ei
(11.4d)
These again generalize the dynamics of binary sums given in Section 11.1.
Theorem 11.2 (Safety). If e : τ, then either e val or there exists e′ : τ such that e 7−→e′.
Proof. The proof is like that for the binary case, as described in Section 11.1.
11.3
Applications of Sum Types
Sum types have many uses, several of which we outline here. More interesting examples arise
once we also have inductive and recursive types, which are introduced in Parts VI and Part VIII.
11.3.1
Void and Unit
It is instructive to compare the types unit and void, which are often confused with one another.
The type unit has exactly one element, ⟨⟩, whereas the type void has no elements at all. Con-
sequently, if e : unit, then if e evaluates to a value, that value is ⟨⟩— in other words, e has no
interesting value. On the other hand, if e : void, then e must not yield a value; if it were to have a
value, it would have to be a value of type void, of which there are none. Thus what is called the
void type in many languages is really the type unit because it indicates that an expression has no
interesting value, not that it has no value at all!
11.3.2
Booleans
Perhaps the simplest example of a sum type is the familiar type of Booleans, whose syntax is given
by the following grammar:
Typ
τ
::=
bool
bool
booleans
Exp
e
::=
true
true
truth
false
false
falsity
if( e ; e1 ; e2 )
if e then e1 else e2
conditional
The expression if( e ; e1 ; e2 ) branches on the value of e : bool.
The statics of Booleans is given by the following typing rules:
Γ ⊢true : bool
(11.5a)

PREVIEW
11.3 Applications of Sum Types
93
Γ ⊢false : bool
(11.5b)
Γ ⊢e : bool
Γ ⊢e1 : τ
Γ ⊢e2 : τ
Γ ⊢if e then e1 else e2 : τ
(11.5c)
The dynamics is given by the following value and transition rules:
true val
(11.6a)
false val
(11.6b)
if true then e1 else e2 7−→e1
(11.6c)
if false then e1 else e2 7−→e2
(11.6d)
e 7−→e′
if e then e1 else e2 7−→if e′ then e1 else e2
(11.6e)
The type bool is definable in terms of binary sums and nullary products:
bool = unit + unit
(11.7a)
true = l · ⟨⟩
(11.7b)
false = r · ⟨⟩
(11.7c)
if e then e1 else e2 = case e {l · x1 ,→e1 | r · x2 ,→e2}
(11.7d)
In the last equation above the variables x1 and x2 are chosen arbitrarily such that x1 /∈e1 and
x2 /∈e2. It is a simple matter to check that the readily-defined statics and dynamics of the type
bool are engendered by these definitions.
11.3.3
Enumerations
More generally, sum types can be used to define finite enumeration types, those whose values are
one of an explicitly given finite set, and whose elimination form is a case analysis on the elements
of that set. For example, the type suit, whose elements are ♣, ♢, ♡, and ♠, has as elimination
form the case analysis
case e {♣,→e0 | ♢,→e1 | ♡,→e2 | ♠,→e3},
which distinguishes among the four suits. Such finite enumerations are easily representable as
sums. For example, we may define suit = [unit] ∈I, where I = { ♣, ♢, ♡, ♠} and the type family
is constant over this set. The case analysis form for a labeled sum is almost literally the desired

PREVIEW
94
11.3 Applications of Sum Types
case analysis for the given enumeration, the only difference being the binding for the uninteresting
value associated with each summand, which we may ignore.
Other examples of enumeration types abound. For example, most languages have a type char
of characters, which is a large enumeration type containing all possible Unicode (or other such
standard classification) characters. Each character is assigned a code (such as UTF-8) used for in-
terchange among programs. The type char is equipped with operations such as chcode( n ) that
yield the char associated to the code n, and codech( c ) that yield the code of character c. Using the
linear ordering on codes we may define a total ordering of characters, called the collating sequence
determined by that code.
11.3.4
Options
Another use of sums is to define the option types, which have the following syntax:
Typ
τ
::=
opt( τ )
τ opt
option
Exp
e
::=
null
null
nothing
just( e )
just( e )
something
ifnull{τ}( e ; e1 ; x . e2 )
ifnull e {null ,→e1 | just( x ) ,→e2}
null test
The type opt( τ ) represents the type of “optional” values of type τ. The introduction forms are
null, corresponding to “no value”, and just( e ), corresponding to a specified value of type τ. The
elimination form discriminates between the two possibilities.
The option type is definable from sums and nullary products according to the following equa-
tions:1
τ opt = unit + τ
(11.8a)
null = l · ⟨⟩
(11.8b)
just( e ) = r · e
(11.8c)
ifnull e {null ,→e1 | just( x2 ) ,→e2} = case e {l · ,→e1 | r · x2 ,→e2}
(11.8d)
We leave it to the reader to check the statics and dynamics implied by these definitions.
The option type is the key to understanding a common misconception, the null pointer fallacy.
This fallacy arises from two related errors. The first error is to deem values of certain types to
be mysterious entities called pointers. This terminology arises from suppositions about how these
values might be represented at run-time, rather than on their semantic role in the language. The
second error compounds the first. A particular value of a pointer type is distinguished as the null
pointer, which, unlike the other elements of that type, does not stand for a value of that type at all,
but rather rejects all attempts to use it.
To help avoid such failures, such languages usually include a function, say null : τ →bool,
that yields true if its argument is null, and false otherwise. Such a test allows the programmer to
1We often write an underscore in place of a bound variable that is not used within its scope.

PREVIEW
11.4 Notes
95
take steps to avoid using null as a value of the type it purports to inhabit. Consequently, programs
are riddled with conditionals of the form
if null( e ) then ...error ... else ...proceed ... .
(11.9)
Despite this, “null pointer” exceptions at run-time are rampant, in part because it is quite easy
to overlook the need for such a test, and in part because detection of a null pointer leaves little
recourse other than abortion of the program.
The underlying problem is the failure to distinguish the type τ from the type τ opt. Rather than
think of the elements of type τ as pointers, and thereby have to worry about the null pointer, we
instead distinguish between a genuine value of type τ and an optional value of type τ. An optional
value of type τ may or may not be present, but, if it is, the underlying value is truly a value of type
τ (and cannot be null). The elimination form for the option type,
ifnull e {null ,→eerror | just( x ) ,→eok},
(11.10)
propagates the information that e is present into the non-null branch by binding a genuine value
of type τ to the variable x. The case analysis effects a change of type from “optional value of type
τ” to “genuine value of type τ”, so that within the non-null branch no further null checks, explicit
or implicit, are necessary. Note that such a change of type is not achieved by the simple Boolean-
valued test exemplified by expression (11.9); the advantage of option types is precisely that they
do so.
11.4
Notes
Heterogeneous data structures are ubiquitous. Sums codify heterogeneity, yet few languages sup-
port them in the form given here. The best approximation in commercial languages is the concept
of a class in object-oriented programming. A class is an injection into a sum type, and dispatch is
case analysis on the class of the data object. (See Chapter 26 for more on this correspondence.) The
absence of sums is the origin of C.A.R. Hoare’s self-described “billion dollar mistake,” the null
pointer (Hoare, 2009). Bad language designs put the burden of managing “null” values entirely at
run-time, instead of making the possibility or the impossibility of “null” apparent at compile time.
Exercises
11.1. Complete the definition of a finite enumeration type sketched in Section 11.3.3. Derive enu-
meration types from finite sum types.
11.2. The essence of Hoare’s mistake is the misidentification of the type τ opt with the type bool ×
τ. Values of the latter type are pairs consisting of a boolean “flag” and a value of type τ. The
idea is that the flag indicates whether the associated value is “present”. When the flag is
true, the second component is present, and, when the flag is false, the second component
is absent.

PREVIEW
96
11.4 Notes
Analyze Hoare’s mistake by attempting to define τ opt to be the type bool × τ by filling in
the following chart:
null ≜?
just( e ) ≜?
ifnull e {null ,→e1 | just( x ) ,→e2} ≜?
Argue that even if we adopt Hoare’s convention of admitting a “null” value of every type,
the chart cannot be properly filled.
11.3. Databases have a version of the “null pointer” problem that arises when not every tuple
provides a value for every attribute (such as a person’s middle name). More generally, many
commercial databases are limited to a single atomic type for each attribute, presenting prob-
lems when the value of that attribute may have several types (for example, one may have
different sorts of postal codes depending on the country). Consider how to address these
problems using the methods discussed in Exercise 10.1. Suggest how to handle null val-
ues and heterogeneous values that avoids some of the complications that arise in traditional
formulations of databases.
11.4. A combinational circuit is an open expression of type
x1 : bool, . . . , xn : bool ⊢e : bool,
which computes a boolean value from n boolean inputs. Define a NOR and a NAND gate as
boolean circuits with two inputs and one output. There is no reason to restrict to a single
output. For example, define an HALF-ADDER that takes two boolean inputs, but produces
two boolean outputs, the sum and the carry outputs of the HALF-ADDER. Then define a
FULL-ADDER that takes three inputs, the addends and an incoming carry, and produces two
outputs, the sum and the outgoing carry. Define the type NYBBLE to be the product bool ×
bool × bool × bool. Define the combinational circuit NYBBLE-ADDER that takes two nybbles
as input and produces a nybble and a carry-out bit as output.
11.5. A signal is a time-varying sequence of booleans, representing the status of the signal at each
time instant. An RS latch is a fundamental digital circuit with two input signals and two
output signals. Define the type signal of signals to be the function type nat →bool of
infinite sequences of booleans. Define an RS latch as a function of type
( signal × signal ) →( signal × signal ).

PREVIEW
Part V
Types and Propositions

PREVIEW

PREVIEW
Chapter 12
Constructive Logic
Constructive logic codifies the principles of mathematical reasoning as it is actually practiced. In
mathematics a proposition is judged true exactly when it has a proof, and is judged false exactly
when it has a refutation. Because there are, and always will be, unsolved problems, we cannot
expect in general that a proposition is either true or false, for in most cases we have neither a proof
nor a refutation of it. Constructive logic can be described as logic as if people matter, as distinct from
classical logic, which can be described as the logic of the mind of god.
From a constructive viewpoint a proposition is true when it has a proof. What is a proof is a
social construct, an agreement among people as to what is a valid argument. The rules of logic
codify a set of principles of reasoning that may be used in a valid proof. The valid forms of
proof are determined by the outermost structure of the proposition whose truth is asserted. For
example, a proof of a conjunction consists of a proof of each conjunct, and a proof of an implication
transforms a proof of its antecedent to a proof of its consequent. When spelled out in full, the forms
of proof are seen to correspond exactly to the forms of expression of a programming language. To
each proposition is associated the type of its proofs; a proof is then an expression of the associated
type. This association between programs and proofs induces a dynamics on proofs. In this way
proofs in constructive logic have computational content, which is to say that they are interpreted
as executable programs of the associated type. Conversely, programs have mathematical content as
proofs of the proposition associated to their type.
The unification of logic and programming is called the propositions as types principle. It is a
central organizing principle of the theory of programming languages. Propositions are identified
with types, and proofs are identified with programs. A programming technique corresponds to
a method of proof; a proof technique corresponds to a method of programming. Viewing types
as behavioral specifications of programs, propositions are problem statements whose proofs are
solutions that implement the specification.

PREVIEW
100
12.1 Constructive Semantics
12.1
Constructive Semantics
Constructive logic is concerned with two judgments, namely ϕ prop, stating that ϕ expresses a
proposition, and ϕ true, stating that ϕ is a true proposition. What distinguishes constructive from
non-constructive logic is that a proposition is not conceived of as merely a truth value, but instead
as a problem statement whose solution, if it has one, is given by a proof. A proposition is true exactly
when it has a proof, in keeping with ordinary mathematical practice. In practice there is no other
criterion of truth than the existence of a proof.
Identifying truth with proof has important, and possibly surprising, consequences. The most
important consequence is that we cannot say, in general, that a proposition is either true or false.
If for a proposition to be true means to have a proof of it, what does it mean for a proposition
to be false? It means that we have a refutation of it, showing that it cannot be proved. That is,
a proposition is false if we can show that the assumption that it is true (has a proof) contradicts
known facts. In this sense constructive logic is a logic of positive, or affirmative, information — we
must have explicit evidence in the form of a proof to affirm the truth or falsity of a proposition.
In light of this it is clear that not every proposition is either true or false. For if ϕ expresses
an unsolved problem, such as the famous P
?= NP problem, then we have neither a proof nor a
refutation of it (the mere absence of a proof not being a refutation). Such a problem is undecided,
precisely because it has not been solved. Because there will always be unsolved problems (there
being infinitely many propositions, but only finitely many proofs at a given point in time), we
cannot say that every proposition is decidable, that is, either true or false.
Of course, some propositions are decidable, and hence are either true or false. For example, if
ϕ expresses an inequality between natural numbers, then ϕ is decidable, because we can always
work out, for given natural numbers m and n, whether m ≤n or m ̸≤n — we can either prove or
refute the given inequality. This argument does not extend to the real numbers. To get an idea of
why not, consider the representation of a real number by its decimal expansion. At any finite time
we will have explored only a finite initial segment of the expansion, which is not enough to decide
if it is, say, less than 1. For if we have calculated the expansion to be 0.99 . . . 9, we cannot decide at
any time, short of infinity, whether or not the number is 1.
The constructive attitude is simply to accept the situation as inevitable, and make our peace
with that. When faced with a problem we have no choice but to roll up our sleeves and try to prove
it or refute it. There is no guarantee of success! Life is hard, but we muddle through somehow.
12.2
Constructive Logic
The judgments ϕ prop and ϕ true of constructive logic are rarely of interest by themselves, but
rather in the context of a hypothetical judgment of the form
ϕ1 true, . . . , ϕn true ⊢ϕ true.
This judgment says that the proposition ϕ is true (has a proof), under the assumptions that each of
ϕ1, . . . , ϕn are also true (have proofs). Of course, when n = 0 this is just the same as the judgment
ϕ true.

PREVIEW
12.2 Constructive Logic
101
The structural properties of the hypothetical judgment, when specialized to constructive logic,
define what we mean by reasoning under hypotheses:
Γ, ϕ true ⊢ϕ true
(12.1a)
Γ ⊢ϕ1 true
Γ, ϕ1 true ⊢ϕ2 true
Γ ⊢ϕ2 true
(12.1b)
Γ ⊢ϕ2 true
Γ, ϕ1 true ⊢ϕ2 true
(12.1c)
Γ, ϕ1 true, ϕ1 true ⊢ϕ2 true
Γ, ϕ1 true ⊢ϕ2 true
(12.1d)
Γ1, ϕ2 true, ϕ1 true, Γ2 ⊢ϕ true
Γ1, ϕ1 true, ϕ2 true, Γ2 ⊢ϕ true
(12.1e)
The last two rules are implicit in that we regard Γ as a set of hypotheses, so that two “copies” are
as good as one, and the order of hypotheses does not matter.
12.2.1
Provability
The syntax of propositional logic is given by the following grammar:
Prop
ϕ
::=
⊤
⊤
truth
⊥
⊥
falsity
∧( ϕ1 ; ϕ2 )
ϕ1 ∧ϕ2
conjunction
∨( ϕ1 ; ϕ2 )
ϕ1 ∨ϕ2
disjunction
⊃( ϕ1 ; ϕ2 )
ϕ1 ⊃ϕ2
implication
The connectives of propositional logic are given meaning by rules that define (a) what constitutes
a “direct” proof of a proposition formed from that connective, and (b) how to exploit the existence
of such a proof in an “indirect” proof of another proposition. These are called the introduction and
elimination rules for the connective. The principle of conservation of proof states that these rules are
inverse to one another — the elimination rule cannot extract more information (in the form of a
proof) than was put into it by the introduction rule, and the introduction rules can reconstruct a
proof from the information extracted by the elimination rules.
Truth
Our first proposition is trivially true. No information goes into proving it, and so no
information can be obtained from it.
Γ ⊢⊤true
(12.2a)
(no elimination rule)
(12.2b)

PREVIEW
102
12.2 Constructive Logic
Conjunction
Conjunction expresses the truth of both of its conjuncts.
Γ ⊢ϕ1 true
Γ ⊢ϕ2 true
Γ ⊢ϕ1 ∧ϕ2 true
(12.3a)
Γ ⊢ϕ1 ∧ϕ2 true
Γ ⊢ϕ1 true
(12.3b)
Γ ⊢ϕ1 ∧ϕ2 true
Γ ⊢ϕ2 true
(12.3c)
Implication
Implication expresses the truth of a proposition under an assumption.
Γ, ϕ1 true ⊢ϕ2 true
Γ ⊢ϕ1 ⊃ϕ2 true
(12.4a)
Γ ⊢ϕ1 ⊃ϕ2 true
Γ ⊢ϕ1 true
Γ ⊢ϕ2 true
(12.4b)
Falsehood
Falsehood expresses the trivially false (refutable) proposition.
(no introduction rule)
(12.5a)
Γ ⊢⊥true
Γ ⊢ϕ true
(12.5b)
Disjunction
Disjunction expresses the truth of either (or both) of two propositions.
Γ ⊢ϕ1 true
Γ ⊢ϕ1 ∨ϕ2 true
(12.6a)
Γ ⊢ϕ2 true
Γ ⊢ϕ1 ∨ϕ2 true
(12.6b)
Γ ⊢ϕ1 ∨ϕ2 true
Γ, ϕ1 true ⊢ϕ true
Γ, ϕ2 true ⊢ϕ true
Γ ⊢ϕ true
(12.6c)
Negation
The negation, ¬ϕ, of a proposition ϕ is defined as the implication ϕ ⊃⊥. As a result
¬ϕ true if ϕ true ⊢⊥true, which is to say that the truth of ϕ is refutable in that we may derive a
proof of falsehood from any purported proof of ϕ. Because constructive truth is defined to be the
existence of a proof, the implied semantics of negation is rather strong. In particular, a problem ϕ
is open exactly when we can neither affirm nor refute it. In contrast the classical conception of truth
assigns a fixed truth value to each proposition so that every proposition is either true or false.

PREVIEW
12.2 Constructive Logic
103
12.2.2
Proof Terms
The key to the propositions-as-types principle is to make explicit the forms of proof. The basic
judgment ϕ true, which states that ϕ has a proof, is replaced by the judgment p : ϕ, stating that p
is a proof of ϕ. (Sometimes p is called a “proof term”, but we will simply call p a “proof.”) The
hypothetical judgment is modified correspondingly, with variables standing for the presumed, but
unknown, proofs:
x1 : ϕ1, . . . , xn : ϕn ⊢p : ϕ.
We again let Γ range over such hypothesis lists, subject to the restriction that no variable occurs
more than once.
The syntax of proof terms is given by the following grammar:
Prf
p
::=
true-I
⟨⟩
truth intro
and-I( p1 ; p2 )
⟨p1, p2⟩
conj. intro
and-E[ l ]( p )
p · l
conj. elim
and-E[ r ]( p )
p · r
conj. elim
imp-I( x . p )
λ ( x ) p
impl. intro
imp-E( p1 ; p2 )
p1( p2 )
impl. elim
false-E( p )
case p { }
false elim
or-I[ l ]( p )
l · p
disj. intro
or-I[ r ]( p )
r · p
disj. intro
or-E( p ; x1 . p1 ; x2 . p2 )
case p {l · x1 ,→p1 | r · x2 ,→p2}
disj. elim
The concrete syntax of proof terms is chosen to stress the correspondence between propositions
and types discussed in Section 12.4 below.
The rules of constructive propositional logic can be restated using proof terms as follows.
Γ ⊢⟨⟩: ⊤
(12.7a)
Γ ⊢p1 : ϕ1
Γ ⊢p2 : ϕ2
Γ ⊢⟨p1, p2⟩: ϕ1 ∧ϕ2
(12.7b)
Γ ⊢p : ϕ1 ∧ϕ2
Γ ⊢p · l : ϕ1
(12.7c)
Γ ⊢p : ϕ1 ∧ϕ2
Γ ⊢p · r : ϕ2
(12.7d)
Γ, x : ϕ1 ⊢p2 : ϕ2
Γ ⊢λ ( x ) p2 : ϕ1 ⊃ϕ2
(12.7e)
Γ ⊢p : ϕ1 ⊃ϕ2
Γ ⊢p1 : ϕ1
Γ ⊢p( p1 ) : ϕ2
(12.7f)

PREVIEW
104
12.3 Proof Dynamics
Γ ⊢p : ⊥
Γ ⊢case p { } : ϕ
(12.7g)
Γ ⊢p1 : ϕ1
Γ ⊢l · p1 : ϕ1 ∨ϕ2
(12.7h)
Γ ⊢p2 : ϕ2
Γ ⊢r · p2 : ϕ1 ∨ϕ2
(12.7i)
Γ ⊢p : ϕ1 ∨ϕ2
Γ, x1 : ϕ1 ⊢p1 : ϕ
Γ, x2 : ϕ2 ⊢p2 : ϕ
Γ ⊢case p {l · x1 ,→p1 | r · x2 ,→p2} : ϕ
(12.7j)
12.3
Proof Dynamics
Proof terms in constructive logic are given a dynamics by Gentzen’s Principle. It states that the
elimination forms are inverse to the introduction forms. One aspect of Gentzen’s Principle is
the principle of conservation of proof, which states that the information introduced into a proof
of a proposition can be extracted without loss by elimination. For example, we may state that
conjunction elimination is post-inverse to conjunction introduction by the definitional equations:
Γ ⊢p1 : ϕ1
Γ ⊢p2 : ϕ2
Γ ⊢⟨p1, p2⟩· l ≡p1 : ϕ1
(12.8a)
Γ ⊢p1 : ϕ1
Γ ⊢p2 : ϕ2
Γ ⊢⟨p1, p2⟩· r ≡p2 : ϕ2
(12.8b)
Another aspect of Gentzen’s Principle is that principle of reversibility of proof, which states that
every proof can be reconstructed from the information that can be extracted from it by elimination.
In the case of conjunction this can be stated by the definitional equation
Γ ⊢p : ϕ1 ∧ϕ2
Γ ⊢⟨p · l, p · r⟩≡p : ϕ1 ∧ϕ2
(12.9)
Similar equivalences can be stated for the other connectives. For example, the conservation
and reversibility principles for implication are given by these rules:
Γ, x : ϕ1 ⊢p2 : ϕ2
Γ ⊢p1 : ϕ1
Γ ⊢( λ ( x ) p2 )( p1 ) ≡[p1/x]p2 : ϕ2
(12.10a)
Γ ⊢p : ϕ1 ⊃ϕ2
Γ ⊢λ ( x ) ( p( x ) ) ≡p : ϕ1 ⊃ϕ2
(12.10b)
The corresponding rules for disjunction and falsehood are given as follows:
Γ ⊢p : ϕ1
Γ, x1 : ϕ1 ⊢p1 : ψ
Γ, x2 : ϕ2 ⊢p2 : ψ
Γ ⊢case l · p {l · x1 ,→p1 | r · x2 ,→p2} ≡[p/x1]p1 : ψ
(12.11a)

PREVIEW
12.4 Propositions as Types
105
Γ ⊢p : ϕ2
Γ, x1 : ϕ1 ⊢p1 : ψ
Γ, x2 : ϕ2 ⊢p2 : ψ
Γ ⊢case r · p {l · x1 ,→p1 | r · x2 ,→p2} ≡[p/x2]p2 : ψ
(12.11b)
Γ ⊢p : ϕ1 ∨ϕ2
Γ, x : ϕ1 ∨ϕ2 ⊢q : ψ
Γ ⊢[p/x]q ≡case p {l · x1 ,→[l · x1/x]q | r · x2 ,→[r · x2/x]q} : ψ
(12.11c)
Γ ⊢p : ⊥
Γ, x : ⊥⊢q : ψ
Γ ⊢[p/x]q ≡case p { } : ψ
(12.11d)
12.4
Propositions as Types
Reviewing the statics and dynamics of proofs in constructive logic reveals a striking similarity to
the statics and dynamics of expressions of various types. For example, the introduction rule for
conjunction specifies that a proof of a conjunction consists of a pair of proofs, one for each con-
junct, and the elimination rule inverts this, allowing us to extract a proof of each conjunct from
any proof of a conjunction. There is an obvious analogy with the statics of product types, whose
introduction form is a pair and whose elimination forms are projections. Gentzen’s Principle ex-
tends the analogy to the dynamics as well, so that the elimination forms for conjunction amount
to projections that extract the appropriate components from an ordered pair.
The following chart summarizes the correspondence between propositions and types and be-
tween proofs and programs:
Prop
Type
⊤
unit
⊥
void
ϕ1 ∧ϕ2
τ1 × τ2
ϕ1 ⊃ϕ2
τ1 →τ2
ϕ1 ∨ϕ2
τ1 + τ2
The correspondence between propositions and types is a cornerstone of the theory of program-
ming languages. It exposes a deep connection between computation and deduction, and serves as
a framework for the analysis of language constructs and reasoning principles by relating them to
one another.
12.5
Notes
The propositions as types principle has its origins in the semantics of intuitionistic logic developed
by Brouwer, according to which the truth of a proposition is witnessed by a construction providing
computable evidence for it. The forms of evidence are determined by the form of the proposition,
so that evidence for an implication is a computable function transforming evidence for the hypoth-
esis into evidence for the conclusion. An explicit formulation of this semantics was introduced by
Heyting, and further developed by several people, including de Bruijn, Curry, Gentzen, Girard,
Howard, Kolmogorov, Martin-L¨of, and Tait. The propositions-as-types correspondence is some-
times called the Curry-Howard Isomorphism, but this terminology neglects the crucial contributions
of the others just mentioned. Moreover, the correspondence is not, in general, an isomorphism;

PREVIEW
106
12.5 Notes
rather, it expresses Brouwer’s Dictum that the concept of proof is best explained by the more gen-
eral concept of construction (program).
Exercises
12.1. The law of the excluded middle (LEM) is the statement that every proposition ϕ is decidable in
the sense that ϕ ∨¬ϕ true. Constructively the law of the excluded middle states that, for
every proposition ϕ, we either have a proof of ϕ or a refutation of ϕ (proof of its negation).
Because this is manifestly not the case in general, one may suspect that the law of the ex-
cluded middle is not constructively valid. This is so, but not in the sense that the law is
refuted, but rather in the sense that it is not affirmed. First, any proposition ϕ for which we
have a proof or a refutation is already decided, and so is decidable. Second, there are broad
classes of propositions for which we can, on demand, produce a proof or a refutation. For
example, it is decidable whether or not two integers are equal. Third, and most importantly,
there are, and always will be, propositions ϕ whose status is unresolved: it may turn out that
ϕ is true, or it may turn out that ϕ is false. For all these reasons constructive logic does not
refute the decidability propositions: ¬¬( ϕ ∨¬ϕ ) true for any proposition ϕ. Prove it using
the rules given in this chapter.
12.2. The proposition ¬¬ϕ is no stronger than ϕ: prove ϕ ⊃¬¬ϕ true. The law of double-negation
elimination (DNE) states that ( ¬¬ϕ ) ⊃ϕ true for every proposition ϕ. It follows immediately
from Exercise 12.1 that DNE entails LEM; prove the converse.
12.3. Define the relation ϕ ≤ψ to mean that ϕ true ⊢ψ true according to the rules of constructive
logic given above. With respect to this relation, show the following facts:
(a) It is a pre-order, which is say that it is reflexive and transitive.
(b) ϕ ∧ψ is the meet, or greatest lower bound, of ϕ and ψ, and ⊤is the top, or greatest, element.
(c) Show that ϕ ∨ψ is the join, or least upper bound, of ϕ and ψ, and that ⊥is the bottom, or
least, element.
(d) Show that ϕ ⊃ψ is an exponential, or pseudo-complement, in the sense that it is the largest
ρ such that ϕ ∧ρ ≤ψ. (The exponential ϕ ⊃ψ is sometimes written ψϕ.)
Altogether these facts state that entailment in constructive propositional logic forms a Heyt-
ing algebra. Show that a general Heyting algebra (that is, an ordering with the above struc-
ture) is distributive in the sense that
ϕ ∧( ψ1 ∨ψ2 ) ≡( ϕ ∧ψ1 ) ∨( ϕ ∧ψ2 )
ϕ ∨( ψ1 ∧ψ2 ) ≡( ϕ ∨ψ1 ) ∧( ϕ ∨ψ2 ),
where ϕ ≡ψ means ϕ ≤ψ and ψ ≤ϕ.

PREVIEW
12.5 Notes
107
12.4. In any Heyting algebra we have ϕ ∧¬ϕ ≤⊥, which is to say that the negation is inconsistent
with the negated. But ¬ϕ is not necessarily the complement of ϕ in the sense that ϕ ∨¬ϕ ≤⊤.
A Boolean algebra is a Heyting algebra in which negation is the complement of the negated:
⊤≤ϕ ∨¬ϕ for every ϕ. Check that the logical connectives for truth tables define a Bol-
lean algebra. Conclude that it is consistent to adjoin LEM to constructive logic, which is to
say that classical logic is a special case of constructive logic in which we assume that every
proposition is decidable. Being a Heyting algebra, every Boolean algebra is clearly distribu-
tive. Show that every Boolean algebra also satisfies the de Morgan duality laws:
¬( ϕ ∨ψ ) ≡¬ϕ ∧¬ψ
¬( ϕ ∧ψ ) ≡¬ϕ ∨¬ψ.
The first of these is valid in any Heyting algebra; the second only in a Boolean algebra.

PREVIEW
108
12.5 Notes

PREVIEW
Chapter 13
Classical Logic
In constructive logic a proposition is true exactly when it has a proof, a derivation of it from axioms
and assumptions, and is false exactly when it has a refutation, a derivation of a contradiction from
the assumption that it is true. Constructive logic is a logic of positive evidence. To affirm or deny
a proposition requires a proof, either of the proposition itself, or of a contradiction, under the
assumption that it has a proof. We are not always able to affirm or deny a proposition. An open
problem is one for which we have neither a proof nor a refutation— constructively speaking, it is
neither true nor false.
In contrast classical logic (the one we learned in school) is a logic of perfect information where
every proposition is either true or false. We may say that classical logic corresponds to “god’s
view” of the world—there are no open problems, rather all propositions are either true or false. Put
another way, to assert that every proposition is either true or false is to weaken the notion of truth
to encompass all that is not false, dually to the constructively (and classically) valid interpretation
of falsity as all that is not true. The symmetry between truth and falsity is appealing, but there is a
price to pay for this: the meanings of the logical connectives are weaker in the classical case than
in the constructive.
The law of the excluded middle provides a prime example. Constructively, this principle is not
universally valid, as we have seen in Exercise 12.1. Classically, however, it is valid, because every
proposition is either false or not false, and being not false is the same as being true. Nevertheless,
classical logic is consistent with constructive logic in that constructive logic does not refute classical
logic. As we have seen, constructive logic proves that the law of the excluded middle is positively
not refuted (its double negation is constructively true). Consequently, constructive logic is stronger
(more expressive) than classical logic, because it can express more distinctions (namely, between
affirmation and irrefutability), and because it is consistent with classical logic.
Proofs in constructive logic have computational content: they can be executed as programs,
and their behavior is described by their type. Proofs in classical logic also have computational con-
tent, but in a weaker sense than in constructive logic. Rather than positively affirm a proposition,
a proof in classical logic is a computation that cannot be refuted. Computationally, a refutation
consists of a continuation, or control stack, that takes a proof of a proposition and derives a con-
tradiction from it. So a proof of a proposition in classical logic is a computation that, when given

PREVIEW
110
13.1 Classical Logic
a refutation of that proposition derives a contradiction, witnessing the impossibility of refuting it.
In this sense the law of the excluded middle has a proof, precisely because it is irrefutable.
13.1
Classical Logic
In constructive logic a connective is defined by giving its introduction and elimination rules. In
classical logic a connective is defined by giving its truth and falsity conditions. Its truth rules
correspond to introduction, and its falsity rules to elimination. The symmetry between truth and
falsity is expressed by the principle of indirect proof. To show that ϕ true it is enough to show
that ϕ false entails a contradiction, and, conversely, to show that ϕ false it is enough to show that
ϕ true leads to a contradiction. Although the second of these is constructively valid, the first is
fundamentally classical, expressing the principle of indirect proof.
13.1.1
Provability and Refutability
There are three basic judgment forms in classical logic:
1. ϕ true, stating that the proposition ϕ is provable;
2. ϕ false, stating that the proposition ϕ is refutable;
3. #, stating that a contradiction has been derived.
These are extended to hypothetical judgments in which we admit both provability and refutability
assumptions:
ϕ1 false, . . . , ϕm false ψ1 true, . . . , ψn true ⊢J.
The hypotheses are divided into two zones, one for falsity assumptions, ∆, and one for truth
assumptions, Γ.
The rules of classical logic are organized around the symmetry between truth and falsity, which
is mediated by the contradiction judgment.
The hypothetical judgment is reflexive:
∆, ϕ false Γ ⊢ϕ false
(13.1a)
∆Γ, ϕ true ⊢ϕ true
(13.1b)
The remaining rules are stated so that the structural properties of weakening, contraction, and
transitivity are admissible.
A contradiction arises when a proposition is judged both true and false. A proposition is true
if its falsity is absurd, and is false if its truth is absurd.
∆Γ ⊢ϕ false
∆Γ ⊢ϕ true
∆Γ ⊢#
(13.1c)
∆, ϕ false Γ ⊢#
∆Γ ⊢ϕ true
(13.1d)

PREVIEW
13.1 Classical Logic
111
∆Γ, ϕ true ⊢#
∆Γ ⊢ϕ false
(13.1e)
Truth is trivially true, and cannot be refuted.
∆Γ ⊢⊤true
(13.1f)
A conjunction is true if both conjuncts are true, and is false if either conjunct is false.
∆Γ ⊢ϕ1 true
∆Γ ⊢ϕ2 true
∆Γ ⊢ϕ1 ∧ϕ2 true
(13.1g)
∆Γ ⊢ϕ1 false
∆Γ ⊢ϕ1 ∧ϕ2 false
(13.1h)
∆Γ ⊢ϕ2 false
∆Γ ⊢ϕ1 ∧ϕ2 false
(13.1i)
Falsity is trivially false, and cannot be proved.
∆Γ ⊢⊥false
(13.1j)
A disjunction is true if either disjunct is true, and is false if both disjuncts are false.
∆Γ ⊢ϕ1 true
∆Γ ⊢ϕ1 ∨ϕ2 true
(13.1k)
∆Γ ⊢ϕ2 true
∆Γ ⊢ϕ1 ∨ϕ2 true
(13.1l)
∆Γ ⊢ϕ1 false
∆Γ ⊢ϕ2 false
∆Γ ⊢ϕ1 ∨ϕ2 false
(13.1m)
Negation inverts the sense of each judgment:
∆Γ ⊢ϕ false
∆Γ ⊢¬ϕ true
(13.1n)
∆Γ ⊢ϕ true
∆Γ ⊢¬ϕ false
(13.1o)
An implication is true if its conclusion is true when the assumption is true, and is false if its
conclusion is false yet its assumption is true.
∆Γ, ϕ1 true ⊢ϕ2 true
∆Γ ⊢ϕ1 ⊃ϕ2 true
(13.1p)
∆Γ ⊢ϕ1 true
∆Γ ⊢ϕ2 false
∆Γ ⊢ϕ1 ⊃ϕ2 false
(13.1q)

PREVIEW
112
13.1 Classical Logic
13.1.2
Proofs and Refutations
To explain the dynamics of classical proofs we first introduce an explicit syntax for proofs and
refutations. We will define three hypothetical judgments for classical logic with explicit deriva-
tions:
1. ∆Γ ⊢p : ϕ, stating that p is a proof of ϕ;
2. ∆Γ ⊢k ÷ ϕ, stating that k is a refutation of ϕ;
3. ∆Γ ⊢k # p, stating that k and p are contradictory.
The falsity assumptions ∆are given by a context of the form
u1 ÷ ϕ1, . . . , um ÷ ϕm,
where m ≥0, in which the variables u1, . . . , um stand for refutations. The truth assumptions Γ are
given by a context of the form
x1 : ψ1, . . . , xn : ψn,
where n ≥0, in which the variables x1, . . . , xn stand for proofs.
The syntax of proofs and refutations is given by the following grammar:
Prf
p
::=
true-T
⟨⟩
truth
and-T( p1 ; p2 )
⟨p1, p2⟩
conjunction
or-T[ l ]( p )
l · p
disjunction left
or-T[ r ]( p )
r · p
disjunction right
not-T( k )
not( k )
negation
imp-T( x . p )
λ ( x ) p
implication
ccr( u . ( k # p ) )
ccr( u . ( k # p ) )
contradiction
Ref
k
::=
false-F
exfalso
falsehood
and-F[ l ]( k )
fst ; k
conjunction left
and-F[ r ]( k )
snd ; k
conjunction right
or-F( k1 ; k2 )
case( k1 ; k2 )
disjunction
not-F( p )
not( p )
negation
imp-F( p ; k )
ap( p ) ; k
implication
ccp( x . ( k # p ) )
ccp( x . ( k # p ) )
contradiction
Proofs serve as evidence for truth judgments, and refutations serve as evidence for false judg-
ments. Contradictions are witnessed by the juxtaposition of a proof and a refutation.
A contradiction arises when a proposition is both true and false:
∆Γ ⊢k ÷ ϕ
∆Γ ⊢p : ϕ
∆Γ ⊢k # p
(13.2a)
Truth and falsity are defined symmetrically in terms of contradiction:
∆, u ÷ ϕ Γ ⊢k # p
∆Γ ⊢ccr( u . ( k # p ) ) : ϕ
(13.2b)

PREVIEW
13.1 Classical Logic
113
∆Γ, x : ϕ ⊢k # p
∆Γ ⊢ccp( x . ( k # p ) ) ÷ ϕ
(13.2c)
Reflexivity corresponds to the use of a variable hypothesis:
∆, u ÷ ϕ Γ ⊢u ÷ ϕ
(13.2d)
∆Γ, x : ϕ ⊢x : ϕ
(13.2e)
The other structure properties are admissible.
Truth is trivially true, and cannot be refuted.
∆Γ ⊢⟨⟩: ⊤
(13.2f)
A conjunction is true if both conjuncts are true, and is false if either conjunct is false.
∆Γ ⊢p1 : ϕ1
∆Γ ⊢p2 : ϕ2
∆Γ ⊢⟨p1, p2⟩: ϕ1 ∧ϕ2
(13.2g)
∆Γ ⊢k1 ÷ ϕ1
∆Γ ⊢fst ; k1 ÷ ϕ1 ∧ϕ2
(13.2h)
∆Γ ⊢k2 ÷ ϕ2
∆Γ ⊢snd ; k2 ÷ ϕ1 ∧ϕ2
(13.2i)
Falsity is trivially false, and cannot be proved.
∆Γ ⊢exfalso ÷ ⊥
(13.2j)
A disjunction is true if either disjunct is true, and is false if both disjuncts are false.
∆Γ ⊢p1 : ϕ1
∆Γ ⊢l · p1 : ϕ1 ∨ϕ2
(13.2k)
∆Γ ⊢p2 : ϕ2
∆Γ ⊢r · p2 : ϕ1 ∨ϕ2
(13.2l)
∆Γ ⊢k1 ÷ ϕ1
∆Γ ⊢k2 ÷ ϕ2
∆Γ ⊢case( k1 ; k2 ) ÷ ϕ1 ∨ϕ2
(13.2m)
Negation inverts the sense of each judgment:
∆Γ ⊢k ÷ ϕ
∆Γ ⊢not( k ) : ¬ϕ
(13.2n)
∆Γ ⊢p : ϕ
∆Γ ⊢not( p ) ÷ ¬ϕ
(13.2o)

PREVIEW
114
13.2 Deriving Elimination Forms
An implication is true if its conclusion is true when the assumption is true, and is false if its
conclusion is false, yet its assumption is true.
∆Γ, x : ϕ1 ⊢p2 : ϕ2
∆Γ ⊢λ ( x ) p2 : ϕ1 ⊃ϕ2
(13.2p)
∆Γ ⊢p1 : ϕ1
∆Γ ⊢k2 ÷ ϕ2
∆Γ ⊢ap( p1 ) ; k2 ÷ ϕ1 ⊃ϕ2
(13.2q)
13.2
Deriving Elimination Forms
The price of achieving a symmetry between truth and falsity in classical logic is that we must very
often rely on the principle of indirect proof: to show that a proposition is true, we often must
derive a contradiction from the assumption of its falsity. For example, a proof of
(ϕ ∧(ψ ∧θ)) ⊃(θ ∧ϕ)
in classical logic has the form
λ ( w ) ccr( u . ( k # w ) ),
where k is the refutation
fst ; ccp( x . ( snd ; ccp( y . ( snd ; ccp( z . ( u # ⟨z, x⟩) ) # y ) ) # w ) ).
And yet in constructive logic this proposition has a direct proof that avoids the circumlocutions of
proof by contradiction:
λ ( w ) ⟨w · r · r, w · l⟩.
But this proof cannot be expressed (as is) in classical logic, because classical logic lacks the elimi-
nation forms of constructive logic.
However, we may package the use of indirect proof into a slightly more palatable form by
deriving the elimination rules of constructive logic. For example, the rule
∆Γ ⊢ϕ ∧ψ true
∆Γ ⊢ϕ true
is derivable in classical logic:
∆, ϕ false Γ ⊢ϕ false
∆, ϕ false Γ ⊢ϕ ∧ψ false
∆Γ ⊢ϕ ∧ψ true
∆, ϕ false Γ ⊢ϕ ∧ψ true
∆, ϕ false Γ ⊢#
∆Γ ⊢ϕ true
The other elimination forms are derivable similarly, in each case relying on indirect proof to con-
struct a proof of the truth of a proposition from a derivation of a contradiction from the assumption
of its falsity.

PREVIEW
13.3 Proof Dynamics
115
The derivations of the elimination forms of constructive logic are most easily exhibited using
proof and refutation expressions, as follows:
case p { } = ccr( u . ( exfalso # p ) )
p · l = ccr( u . ( fst ; u # p ) )
p · r = ccr( u . ( snd ; u # p ) )
p1( p2 ) = ccr( u . ( ap( p2 ) ; u # p1 ) )
case p1 {l · x ,→p2 | r · y ,→p} = ccr( u . ( case( ccp( x . ( u # p2 ) ) ; ccp( y . ( u # p ) ) ) # p1 ) )
The expected elimination rules are valid for these definitions. For example, the rule
∆Γ ⊢p1 : ϕ ⊃ψ
∆Γ ⊢p2 : ϕ
∆Γ ⊢p1( p2 ) : ψ
(13.3)
is derivable using the definition of p1( p2 ) given above. By suppressing proof terms, we may
derive the corresponding provability rule
∆Γ ⊢ϕ ⊃ψ true
∆Γ ⊢ϕ true
∆Γ ⊢ψ true
.
(13.4)
13.3
Proof Dynamics
The dynamics of classical logic arises from the simplification of the contradiction between a proof
and a refutation of a proposition. To make this explicit we will define a transition system whose
states are contradictions k # p consisting of a proof p and a refutation k of the same proposition.
The steps of the computation consist of simplifications of the contradictory state based on the form
of p and k.
The truth and falsity rules for the connectives play off one another in a pleasing way:
fst ; k # ⟨p1, p2⟩7−→k # p1
(13.5a)
snd ; k # ⟨p1, p2⟩7−→k # p2
(13.5b)
case( k1 ; k2 ) # l · p1 7−→k1 # p1
(13.5c)
case( k1 ; k2 ) # r · p2 7−→k2 # p2
(13.5d)
not( p ) # not( k ) 7−→k # p
(13.5e)
ap( p1 ) ; k # λ ( x ) p2 7−→k # [p1/x]p2
(13.5f)
The rules of indirect proof give rise to the following transitions:
ccp( x . ( k1 # p1 ) ) # p2 7−→[p2/x]k1 # [p2/x]p1
(13.5g)
k1 # ccr( u . ( k2 # p2 ) ) 7−→[k1/u]k2 # [k1/u]p2
(13.5h)

PREVIEW
116
13.3 Proof Dynamics
The first of these defines the behavior of the refutation of ϕ that proceeds by contradicting the
assumption that ϕ is true. Such a refutation is activated by presenting it with a proof of ϕ, which
is then substituted for the assumption in the new state. Thus, “ccp” stands for “call with current
proof.” The second transition defines the behavior of the proof of ϕ that proceeds by contradicting
the assumption that ϕ is false. Such a proof is activated by presenting it with a refutation of ϕ,
which is then substituted for the assumption in the new state. Thus, “ccr” stands for “call with
current refutation.”
Rules (13.5g) to (13.5h) overlap in that there are two transitions for a state of the form
ccp( x . ( k1 # p1 ) ) # ccr( u . ( k2 # p2 ) ),
one to the state [p/x]k1 # [p/x]p1, where p is ccr( u . ( k2 # p2 ) ), and one to the state [k/u]k2 #
[k/u]p2, where k is ccp( x . ( k1 # p1 ) ). The dynamics of classical logic is non-deterministic. To
avoid this one may impose a priority ordering among the two cases, preferring one transition
over the other when there is a choice. Preferring the first corresponds to a “lazy” dynamics for
proofs, because we pass the unevaluated proof p to the refutation on the left, which is thereby
activated. Preferring the second corresponds to an “eager” dynamics for proofs, in which we pass
the unevaluated refutation k to the proof, which is thereby activated.
All proofs in classical logic proceed by contradiction. In terms of the classical logic machine
the initial and final states of a computation are as follows:
haltϕ # p initial
(13.6a)
p canonical
haltϕ # p final
(13.6b)
where p is a proof of ϕ, and haltϕ is the assumed refutation of ϕ. The judgment p canonical
states that p is a canonical proof, which holds of any proof other than an indirect proof. Execution
consists of driving a general proof to a canonical proof, under the assumption that the theorem is
false.
Theorem 13.1 (Preservation). If k ÷ ϕ, p : ϕ, and k # p 7−→k′ # p′, then there exists ϕ′ such that
k′ ÷ ϕ′ and p′ : ϕ′.
Proof. By rule induction on the dynamics of classical logic.
Theorem 13.2 (Progress). If k ÷ ϕ and p : ϕ, then either k # p final or k # p 7−→k′ # p′.
Proof. By rule induction on the statics of classical logic.

PREVIEW
13.4 Law of the Excluded Middle
117
13.4
Law of the Excluded Middle
The law of the excluded middle is derivable in classical logic:
ϕ ∨¬ϕ false, ϕ true ⊢ϕ true
ϕ ∨¬ϕ false, ϕ true ⊢ϕ ∨¬ϕ true
ϕ ∨¬ϕ false, ϕ true ⊢ϕ ∨¬ϕ false
ϕ ∨¬ϕ false, ϕ true ⊢#
ϕ ∨¬ϕ false ⊢ϕ false
ϕ ∨¬ϕ false ⊢¬ϕ true
ϕ ∨¬ϕ false ⊢ϕ ∨¬ϕ true
ϕ ∨¬ϕ false ⊢ϕ ∨¬ϕ false
ϕ ∨¬ϕ false ⊢#
ϕ ∨¬ϕ true
When written out using explicit proofs and refutations, we obtain the proof term p0 : ϕ ∨¬ϕ:
ccr( u . ( u # r · not( ccp( x . ( u # l · x ) ) ) ) ).
To understand the computational meaning of this proof, let us juxtapose it with a refutation k ÷
ϕ ∨¬ϕ and simplify it using the dynamics given in Section 13.3. The first step is the transition
k # ccr( u . ( u # r · not( ccp( x . ( u # l · x ) ) ) ) )
7−→
k # r · not( ccp( x . ( k # l · x ) ) ),
wherein we have replicated k so that it occurs in two places in the result state. By virtue of its type
the refutation k must have the form case( k1 ; k2 ), where k1 ÷ ϕ and k2 ÷ ¬ϕ. Continuing the
reduction, we obtain:
case( k1 ; k2 ) # r · not( ccp( x . ( case( k1 ; k2 ) # l · x ) ) )
7−→
k2 # not( ccp( x . ( case( k1 ; k2 ) # l · x ) ) ).
By virtue of its type k2 must have the form not( p2 ), where p2 : ϕ, and hence the transition pro-
ceeds as follows:
not( p2 ) # not( ccp( x . ( case( k1 ; k2 ) # l · x ) ) )
7−→
ccp( x . ( case( k1 ; k2 ) # l · x ) ) # p2.

PREVIEW
118
13.5 The Double-Negation Translation
Observe that p2 is a valid proof of ϕ. Proceeding, we obtain
ccp( x . ( case( k1 ; k2 ) # l · x ) ) # p2
7−→
case( k1 ; k2 ) # l · p2
7−→
k1 # p2
The first of these two steps is the crux of the matter: the refutation, k = case( k1 ; k2 ), which was
replicated at the outset of the derivation, is re-used, but with a different argument. At the first
use, the refutation k which is provided by the context of use of the law of the excluded middle, is
presented with a proof r · p1 of ϕ ∨¬ϕ. That is, the proof behaves as though the right disjunct of
the law is true, which is to say that ϕ is false. If the context is such that it inspects this proof, it
can only be by providing the proof p2 of ϕ that refutes the claim that ϕ is false. Should this occur,
the proof of the law of the excluded middle “backtracks” the context, providing instead the proof
l · p2 to k, which then passes p2 to k1 without further incident. The proof of the law of the excluded
middle boldly asserts ¬ϕ true, regardless of the form of ϕ. Then, if caught in its lie by the context
providing a proof of ϕ, it “changes its mind” and asserts ϕ to the original context k after all. No
further reversion is possible, because the context has itself provided a proof p2 of ϕ.
The law of the excluded middle illustrates that classical proofs are interactions between proofs
and refutations, which is to say interactions between a proof and the context in which it is used.
In programming terms this corresponds to an abstract machine with an explicit control stack, or
continuation, representing the context of evaluation of an expression. That expression may access
the context (stack, continuation) to backtrack so as to maintain the perfect symmetry between truth
and falsity. The penalty is that a closed proof of a disjunction no longer need show which disjunct
it proves, for as we have just seen, it may, on further inspection, “change its mind.”
13.5
The Double-Negation Translation
One consequence of the greater expressiveness of constructive logic is that classical proofs may be
translated systematically into constructive proofs of a classically equivalent proposition. There-
fore, by systematically reorganizing the classical proof, we may, without changing its meaning
from a classical perspective, turn it into a constructive proof of a constructively weaker propo-
sition. Consequently, there is no loss in adhering to constructive proofs, because every classical
proof is a constructive proof of a constructively weaker, but classically equivalent, proposition.
Moreover, it proves that classical logic is weaker (less expressive) than constructive logic, contrary
to a na¨ıve interpretation which would say that the added reasoning principles, such as the law
of the excluded middle, afforded by classical logic makes it stronger. In programming language
terms adding a “feature” does not necessarily strengthen (improve the expressive power) of your
language; on the contrary, it may weaken it.
We will define a translation ϕ∗of propositions that interprets classical into constructive logic

PREVIEW
13.6 Notes
119
according to the following correspondences:
Classical
Constructive
∆Γ ⊢ϕ true
¬∆∗Γ∗⊢¬¬ϕ∗true
truth
∆Γ ⊢ϕ false
¬∆∗Γ∗⊢¬ϕ∗true
falsity
∆Γ ⊢#
¬∆∗Γ∗⊢⊥true
contradiction
Classical truth is weakened to constructive irrefutability, but classical falsehood is constructive
refutability, and classical contradiction is constructive falsehood. Falsity assumptions are negated
after translation to express their falsehood; truth assumptions are merely translated as is. Because
the double negations are classically cancelable, the translation will be easily seen to yield a classi-
cally equivalent proposition. But because ¬¬ϕ is constructively weaker than ϕ, we also see that a
proof in classical logic is translated to a constructive proof of a weaker statement.
There are many choices for the translation; here is one that makes the proof of the correspon-
dence between classical and constructive logic especially simple:
⊤∗= ⊤
( ϕ1 ∧ϕ2 )∗= ϕ∗
1 ∧ϕ∗
2
⊥∗=⊥
( ϕ1 ∨ϕ2 )∗= ϕ∗
1 ∨ϕ∗
2
( ϕ1 ⊃ϕ2 )∗= ϕ∗
1 ⊃¬¬ϕ∗
2
( ¬ϕ )∗= ¬ϕ∗
One may show by induction on the rules of classical logic that the correspondences summarized
above hold, using constructively valid entailments such as
¬¬ϕ true ¬¬ψ true ⊢¬¬( ϕ ∧ψ ) true.
13.6
Notes
The computational interpretation of classical logic was first explored by Griffin (1990) and Murthy
(1991). The present account is influenced by Wadler (2003), transposed by Nanevski from sequent
calculus to natural deduction using multiple forms of judgment. The terminology is inspired by
Lakatos (1976), an insightful and inspiring analysis of the discovery of proofs and refutations of
conjectures in mathematics. Versions of the double-negation translation were originally given by
G¨odel and Gentzen. The computational content of the double negation translation was first eluci-
dated by Murthy (1991), who established the important relationship with continuation passing.
Exercises

PREVIEW
120
13.6 Notes
13.1. If the continuation type expresses negation, the types shown to be inhabited in Exercise 30.2,
when interpreted under the proposition-as-types interpretation, look suspiciously like the
following propositions:
(a) ϕ ∨¬ϕ.
(b) ( ¬¬ϕ ) ⊃ϕ.
(c) ( ¬ϕ2 ⊃¬ϕ1 ) ⊃( ϕ1 ⊃ϕ2 ).
(d) ¬( ϕ1 ∨ϕ2 ) ⊃( ¬ϕ1 ∧¬ϕ2 ).
None of these propositions is true, in general, in constructive logic. Show that each of these
propositions is true in classical logic by exhibiting a proof term for each. (The first one is
done for you in Section 13.4; you need only do the other three.) Compare the proof term you
get for each with the inhabitant of the corresponding type that you gave in your solution to
Exercise 30.2.
13.2. Complete the proof of the double-negation interpretation sketched in Section 13.5, providing
explicit proof terms for clarity. Because ( ϕ ∨¬ϕ )∗= ϕ∗∨¬ϕ∗, the double-negation trans-
lation applied to the proof of LEM (for ϕ) given in Section 13.4 yields a proof of the double
negation of LEM (for ϕ∗) in constructive logic. How does the translated proof compare to
the one you derived by hand in Exercise 12.1?

PREVIEW
Part VI
Infinite Data Types

PREVIEW

PREVIEW
Chapter 14
Generic Programming
14.1
Introduction
Many programs are instances of a pattern in a particular situation. Sometimes types determine
the pattern by a technique called (type) generic programming. For example, in Chapter 9 recursion
over the natural numbers is introduced in an ad hoc way. As we shall see, the pattern of recursion
on values of an inductive type is expressed as a generic program.
To get a flavor of the concept, consider a function f of type ρ →ρ′, which transforms values of
type ρ into values of type ρ′. For example, f might be the doubling function on natural numbers.
We wish to extend f to a transformation from type [ρ/t]τ to type [ρ′/t]τ by applying f to various
spots in the input where a value of type ρ occurs to obtain a value of type ρ′, leaving the rest of
the data structure alone. For example, τ might be bool × t, in which case f could be extended to a
function of type bool × ρ →bool × ρ′ that sends the pairs ⟨a, b⟩to the pair ⟨a, f ( b )⟩.
The foregoing example glosses over an ambiguity arising from the many-one nature of substi-
tution. A type can have the form [ρ/t]τ in many different ways, according to how many occur-
rences of t there are within τ. Given f as above, it is not clear how to extend it to a function from
[ρ/t]τ to [ρ′/t]τ. To resolve the ambiguity we must be given a template that marks the occurrences
of t in τ at which f is applied. Such a template is known as a type operator, t . τ, which is an abstrac-
tor binding a type variable t within a type τ. Given such an abstractor, we may unambiguously
extend f to instances of τ given by substitution for t in τ.
The power of generic programming depends on the type operators that are allowed. The sim-
plest case is that of a polynomial type operator, one constructed from sum and product of types,
including their nullary forms. These are extended to positive type operators, which also allow
certain forms of function types.
14.2
Polynomial Type Operators
A type operator is a type equipped with a designated variable whose occurrences mark the spots
in the type where a transformation is applied. A type operator is an abstractor t . τ such that

PREVIEW
124
14.2 Polynomial Type Operators
t type ⊢τ type. An example of a type operator is the abstractor
t . unit + ( bool × t )
in which occurrences of t mark the spots in which a transformation is applied. An instance of the
type operator t . τ is obtained by substituting a type ρ for the variable t within the type τ.
The polynomial type operators are those constructed from the type variable t the types void and
unit, and the product and sum types τ1 × τ2 and τ1 + τ2. More precisely, the judgment t . τ poly is
inductively defined by the following rules:
t . t poly
(14.1a)
t . unit poly
(14.1b)
t . τ1 poly
t . τ2 poly
t . τ1 × τ2 poly
(14.1c)
t . void poly
(14.1d)
t . τ1 poly
t . τ2 poly
t . τ1 + τ2 poly
(14.1e)
Exercise 14.1 asks for a proof that polynomial type operators are closed under substitution.
Polynomial type operators are templates describing the structure of a data structure with slots
for values of a particular type. For example, the type operator t . t × ( nat + t ) specifies all types
ρ × ( nat + ρ ) for any choice of type ρ. Thus a polynomial type operator designates points of
interest in a data structure that have a common type. As we shall see shortly, this allows us
to specify a program that applies a given function to all values lying at points of interest in a
compound data structure to obtain a new one with the results of the applications at those points.
Because substitution is not injective, one cannot recover the type operator from its instances. For
example, if ρ were nat, then the instance would be nat × ( nat + nat ); it is impossible to know
which occurrences of nat are in designated spots unless we are given the pattern by the type
operator.
The generic extension of a polynomial type operator is a form of expression with the following
syntax
Exp
e
::=
map{t . τ}( x . e′ )( e )
map{t . τ}( x . e′ )( e )
generic extension.
Its statics is given as follows:
t . τ poly
Γ, x : ρ ⊢e′ : ρ′
Γ ⊢e : [ρ/t]τ
Γ ⊢map{t . τ}( x . e′ )( e ) : [ρ′/t]τ
(14.2)
The abstractor x . e′ specifies a mapping that sends x : ρ to e′ : ρ′. The generic extension of t . τ
along x . e′ specifies a mapping from [ρ/t]τ to [ρ′/t]τ. The latter mapping replaces values v of

PREVIEW
14.2 Polynomial Type Operators
125
type ρ occurring at spots corresponding to occurrences of t in τ by the transformed value [v/x]e′
of type ρ′ at the same spot. The type operator t . τ is a template in which certain spots, marked by
occurrences of t, show where to apply the transformation x . e′ to a value of type [ρ/t]τ to obtain a
value of type [ρ′/t]τ.
The following dynamics makes precise the concept of the generic extension of a polynomial
type operator.
map{t . t}( x . e′ )( e ) 7−→[e/x]e′
(14.3a)
map{t . unit}( x . e′ )( e ) 7−→e
(14.3b)
map{t . τ1 × τ2}( x . e′ )( e )
7−→
⟨map{t . τ1}( x . e′ )( e · l ), map{t . τ2}( x . e′ )( e · r )⟩
(14.3c)
map{t . void}( x . e′ )( e ) 7−→case e { }
(14.3d)
map{t . τ1 + τ2}( x . e′ )( e )
7−→
case e {l · x1 ,→l · map{t . τ1}( x . e′ )( x1 ) | r · x2 ,→r · map{t . τ2}( x . e′ )( x2 )}
(14.3e)
Rule (14.3a) applies the transformation x . e′ to e itself, because the operator t . t specifies that
the transformation is performed directly. Rule (14.3b) states that the empty tuple is transformed to
itself. Rule (14.3c) states that to transform e according to the operator t . τ1 × τ2, the first component
of e is transformed according to t . τ1 and the second component of e is transformed according to
t . τ2. Rule (14.3d) states that the transformation of a value of type void cannot arise, because there
are no such values. Rule (14.3e) states that to transform e according to t . τ1 + τ2, case analyze e
and reconstruct it after transforming the injected value according to t . τ1 or t . τ2.
Consider the type operator t . τ given by t . unit + (bool × t). Let x . e be the abstractor x . s( x ),
which increments a natural number. Using rules (14.3) we may derive that
map{t . τ}( x . e )( r · ⟨true, n⟩) 7−→∗r · ⟨true, n + 1⟩.
The natural number in the second component of the pair is incremented, because the type variable
t occurs in that spot in the type operator t . τ.
Theorem 14.1 (Preservation). If map{t . τ}( x . e′ )( e ) : τ′ and map{t . τ}( x . e′ )( e ) 7−→e′′, then
e′′ : τ′.

PREVIEW
126
14.3 Positive Type Operators
Proof. By inversion of rule (14.2) we have
1. t type ⊢τ type;
2. x : ρ ⊢e′ : ρ′ for some ρ and ρ′;
3. e : [ρ/t]τ;
4. τ′ is [ρ′/t]τ.
The proof proceeds by cases on rules (14.3). For example, consider rule (14.3c). It follows from in-
version that map{t . τ1}( x . e′ )( e · l ) : [ρ′/t]τ1, and similarly that map{t . τ2}( x . e′ )( e · r ) : [ρ′/t]τ2.
It is easy to check that
⟨map{t . τ1}( x . e′ )( e · l ), map{t . τ2}( x . e′ )( e · r )⟩
has type [ρ′/t]( τ1 × τ2 ), as required.
14.3
Positive Type Operators
The positive type operators extend the polynomial type operators to admit restricted forms of func-
tion type. Specifically, t . τ1 →τ2 is a positive type operator, if (1) t does not occur in τ1, and (2) t . τ2
is a positive type operator. In general, any occurrences of a type variable t in the domain of a
function type are negative occurrences, whereas any occurrences of t within the range of a function
type, or within a product or sum type, are positive occurrences.1 A positive type operator is one for
which only positive occurrences of the type variable t are allowed. Positive type operators, like
polynomial type operators, are closed under substitution.
We define the judgment t . τ pos, which states that the abstractor t . τ is a positive type operator
by the following rules:
t . t pos
(14.4a)
t . unit pos
(14.4b)
t . τ1 pos
t . τ2 pos
t . τ1 × τ2 pos
(14.4c)
t . void pos
(14.4d)
t . τ1 pos
t . τ2 pos
t . τ1 + τ2 pos
(14.4e)
τ1 type
t . τ2 pos
t . τ1 →τ2 pos
(14.4f)
1The origin of this terminology is that a function type τ1 →τ2 is analogous to the implication ϕ1 ⊃ϕ2, which is
classically equivalent to ¬ϕ1 ∨ϕ2, so that occurrences in the domain are under the negation.

PREVIEW
14.4 Notes
127
In rule (14.4f), the type variable t is excluded from the domain of the function type by demanding
that it be well-formed without regard to t.
The generic extension of a positive type operator is defined similarly to that of a polynomial
type operator, with the following dynamics on function types:
map+{t . τ1 →τ2}( x . e′ )( e ) 7−→λ ( x1 : τ1 ) map+{t . τ2}( x . e′ )( e( x1 ) )
(14.5)
Because t is not allowed to occur within the domain type, the type of the result is τ1 →[ρ′/t]τ2,
assuming that e is of type τ1 →[ρ/t]τ2. It is easy to verify preservation for the generic extension
of a positive type operator.
It is instructive to consider what goes wrong if we try to extend the generic extension to an
arbitrary type operator, without any positivity restriction. Consider the type operator t . τ1 →τ2,
without restriction on t, and suppose that x : ρ ⊢e′ : ρ′. The generic extension map{t . τ1 →τ2}( x .
e′ )( e ) should have type [ρ′/t]τ1 →[ρ′/t]τ2, given that e has type [ρ/t]τ1 →[ρ/t]τ2. The extension
should yield a function of the form
λ ( x1 : [ρ′/t]τ1 ) . . .( e( . . .( x1 ) ) )
in which we apply e to a transformation of x1 and then transform the result. The trouble is that we
are given, inductively, that map{t . τ1}( x . e′ )( −) transforms values of type [ρ/t]τ1 into values of
type [ρ′/t]τ1, but we need to go the other way around to make x1 suitable as an argument for e.
14.4
Notes
The generic extension of a type operator is an example of the concept of a functor in category
theory (MacLane, 1998). Generic programming is essentially functorial programming, exploiting
the functorial action of polynomial type operators (Hinze and Jeuring, 2003).
Exercises
14.1. Prove that if t . τ poly and t′ . τ′ poly, then t . [τ/t′]τ′ poly.
14.2. Show that the generic extension of a constant type operator is essentially the identity in that
it sends each closed value to itself. More precisely, show that, for each value e of type τ, the
expression
map{ . τ}( x . e′ )( e )
evaluates to e, regardless of the choice of e′. For simplicity, assume an eager dynamics for
products and sums, and consider only polynomial type operators. What complications arise
when extending this observation to positive type operators?

PREVIEW
128
14.4 Notes
14.3. Consider Exercises 10.1 and 11.3 in which a database schema is represented by a finite prod-
uct type indexed by the attributes of the schema, and a database with that schema is a finite
sequence of instances of tuples of that type. Show that any database transformation that ap-
plies a function to one or more of the columns of each row of a database can be programmed
in two steps using generic programming according to the following plan:
(a) Specify a type operator whose type variable shows which columns are transformed.
All specified columns must be of the same type in order for the transformation to make
sense.
(b) Specify the transformation on the type of column that will be applied to each tuple of
the database to obtain an updated database.
(c) Form the generic extension of the type operator with the given transformation, and
apply it to the given database.
For specificity, consider a schema whose attributes I include the attributes first and last,
both of type str. Let c : str →str be a function that capitalizes strings according to some
convention. Use generic programming to capitalize the first and last attributes of each
row of a given database with the specified schema.
14.4. Whereas t occurs negatively in the type t →bool, and does not occur positively in the type
( t →bool ) →bool, we may say that t occurs non-negatively in the latter type. This example
illustrates that occurrences of t in the domain of a function are negative, and that occurrences
in the domain of the domain of a left-iterated function type are non-negative. Every positive
occurrence counts as non-negative, but not every non-negative occurrence is positive.2 Give
a simultaneous induction definition of the negative and non-negative type operators. Check
that the type operator t . ( t →bool ) →bool is non-negative according to your definition.
14.5. Using the definitions of negative and non-negative type operators requested in Exercise 14.4,
give the definition of the generic extension of a non-negative type operator. Specifically,
define simultaneously map−−{t . τ}( x . e′ )( e ) and map−{t . τ}( x . e )( e′ ) by induction on
the structure of τ with the statics give by these rules:
t . τ non-neg
Γ, x : ρ ⊢e′ : ρ′
Γ ⊢e : [ρ/t]τ
Γ ⊢map−−{t . τ}( x . e′ )( e ) : [ρ′/t]τ
(14.6a)
t . τ neg
Γ, x : ρ ⊢e′ : ρ′
Γ ⊢e : [ρ′/t]τ
Γ ⊢map−{t . τ}( x . e′ )( e ) : [ρ/t]τ
(14.6b)
Note well the reversal of the types of e and the overall type in these two rules. Calculate the
generic extension of the type operator t . ( t →bool ) →bool.
2Often in the literature what we have called “positive” is called “strictly positive”, and what we have called “non-
negative” is called “positive”.

PREVIEW
Chapter 15
Inductive and Coinductive Types
The inductive and the coinductive types are two important forms of recursive type. Inductive types
correspond to least, or initial, solutions of certain type equations, and coinductive types correspond
to their greatest, or final, solutions. Intuitively, the elements of an inductive type are those that are
given by a finite composition of its introduction forms. Consequently, if we specify the behavior
of a function on each of the introduction forms of an inductive type, then its behavior is defined
for all values of that type. Such a function is an iterator, or catamorphism. Dually, the elements of a
coinductive type are those that behave properly in response to a finite composition of its elimina-
tion forms. Consequently, if we specify the behavior of an element on each elimination form, then
we have fully specified a value of that type. Such an element is a generator, or anamorphism.
15.1
Motivating Examples
The most important example of an inductive type is the type of natural numbers as formalized in
Chapter 9. The type nat is the least type containing z and closed under s( −). The minimality
condition is expressed by the existence of the iterator, iter e {z ,→e0 | s( x ) ,→e1}, which trans-
forms a natural number into a value of type ρ, given its value for zero, and a transformation from
its value on a number to its value on the successor of that number. This operation is well-defined
precisely because there are no other natural numbers.
With a view towards deriving the type nat as a special case of an inductive type, it is useful to
combine zero and successor into a single introduction form, and to correspondingly combine the
basis and inductive step of the iterator. The following rules specify the statics of this reformulation:
Γ ⊢e : unit + nat
Γ ⊢foldnat( e ) : nat
(15.1a)
Γ, x : unit + ρ ⊢e1 : ρ
Γ ⊢e2 : nat
Γ ⊢recnat( x . e1 ; e2 ) : ρ
(15.1b)

PREVIEW
130
15.1 Motivating Examples
The expression foldnat( e ) is the unique introduction form of the type nat. Using this, the ex-
pression z is foldnat( l · ⟨⟩), and s( e ) is foldnat( r · e ). The iterator, recnat( x . e1 ; e2 ), takes as
argument the abstractor x . e1 that combines the basis and inductive step into a single computation
that, given a value of type unit + ρ, yields a value of type ρ. Intuitively, if x is replaced by the
value l · ⟨⟩, then e1 computes the base case of the recursion, and if x is replaced by the value r · e,
then e1 computes the inductive step from the result e of the recursive call.
The dynamics of the combined representation of natural numbers is given by the following
rules:
[e val]
foldnat( e ) val
(15.2a)


e 7−→e′
foldnat( e ) 7−→foldnat( e′ )


(15.2b)
e2 7−→e′
2
recnat( x . e1 ; e2 ) 7−→recnat( x . e1 ; e′
2 )
(15.2c)
foldnat( e2 ) val
recnat( x . e1 ; foldnat( e2 ) )
7−→
[map{t . unit + t}( y . recnat( x . e1 ; y ) )( e2 )/x]e1
(15.2d)
Rule (15.2d) uses (polynomial) generic extension (see Chapter 14) to apply the iterator to the pre-
decessor, if any, of a natural number. If we expand the definition of the generic extension in place,
we obtain this rule:
foldnat( e2 ) val
recnat( x . e1 ; foldnat( e2 ) )
7−→
[case e2 {l · ,→l · ⟨⟩| r · y ,→r · recnat( x . e1 ; y )}/x]e1
An illustrative example of a coinductive type is the type of streams of natural numbers. A
stream is an infinite sequence of natural numbers such that an element of the stream can be com-
puted only after computing all preceding elements in that stream. That is, the computations of
successive elements of the stream are sequentially dependent in that the computation of one el-
ement influences the computation of the next. In this sense the introduction form for streams is
dual to the elimination form for natural numbers.
A stream is given by its behavior under the elimination forms for the stream type: hd( e ) re-
turns the next, or head, element of the stream, and tl( e ) returns the tail of the stream, the stream
resulting when the head element is removed. A stream is introduced by a generator, the dual of an
iterator, that defines the head and the tail of the stream in terms of the current state of the stream,

PREVIEW
15.1 Motivating Examples
131
which is represented by a value of some type. The statics of streams is given by the following
rules:
Γ ⊢e : stream
Γ ⊢hd( e ) : nat
(15.3a)
Γ ⊢e : stream
Γ ⊢tl( e ) : stream
(15.3b)
Γ ⊢e : σ
Γ, x : σ ⊢e1 : nat
Γ, x : σ ⊢e2 : σ
Γ ⊢genstream x is e in ⟨hd ,→e1,tl ,→e2 ⟩: stream
(15.3c)
In rule (15.3c) the current state of the stream is given by the expression e of some type σ, and the
head and tail of the stream are determined by the expressions e1 and e2, respectively, as a function
of the current state. (The notation for the generator is chosen to emphasize that every stream has
both a head and a tail.)
The dynamics of streams is given by the following rules:
[e val]
genstream x is e in ⟨hd ,→e1,tl ,→e2 ⟩val
(15.4a)


e 7−→e′
genstream x is e in ⟨hd ,→e1,tl ,→e2 ⟩7−→genstream x is e′ in ⟨hd ,→e1,tl ,→e2 ⟩


(15.4b)
e 7−→e′
hd( e ) 7−→hd( e′ )
(15.4c)
genstream x is e in ⟨hd ,→e1,tl ,→e2 ⟩val
hd( genstream x is e in ⟨hd ,→e1,tl ,→e2 ⟩) 7−→[e/x]e1
(15.4d)
e 7−→e′
tl( e ) 7−→tl( e′ )
(15.4e)
genstream x is e in ⟨hd ,→e1,tl ,→e2 ⟩val
tl( genstream x is e in ⟨hd ,→e1,tl ,→e2 ⟩)
7−→
genstream x is [e/x]e2 in ⟨hd ,→e1,tl ,→e2 ⟩
(15.4f)
Rules (15.4d) and (15.4f) express the dependency of the head and tail of the stream on its current
state. Observe that the tail is obtained by applying the generator to the new state determined by
e2 from the current state.
To derive streams as a special case of a coinductive type, we combine the head and the tail into
a single elimination form, and reorganize the generator correspondingly. Thus we consider the
following statics:
Γ ⊢e : stream
Γ ⊢unfoldstream( e ) : nat × stream
(15.5a)

PREVIEW
132
15.2 Statics
Γ, x : σ ⊢e1 : nat × σ
Γ ⊢e2 : σ
Γ ⊢genstream( x . e1 ; e2 ) : stream
(15.5b)
Rule (15.5a) states that a stream may be unfolded into a pair consisting of its head, a natural num-
ber, and its tail, another stream. The head hd( e ) and tail tl( e ) of a stream e are the projections
unfoldstream( e ) · l and unfoldstream( e ) · r, respectively. Rule (15.5b) states that a stream is gen-
erated from the state element e2 by an expression e1 that yields the head element and the next state
as a function of the current state.
The dynamics of streams is given by the following rules:
[e2 val]
genstream( x . e1 ; e2 ) val
(15.6a)


e2 7−→e′
2
genstream( x . e1 ; e2 ) 7−→genstream( x . e1 ; e′
2 )


(15.6b)
e 7−→e′
unfoldstream( e ) 7−→unfoldstream( e′ )
(15.6c)
genstream( x . e1 ; e2 ) val
unfoldstream( genstream( x . e1 ; e2 ) )
7−→
map{t . nat × t}( y . genstream( x . e1 ; y ) )( [e2/x]e1 )
(15.6d)
Rule (15.6d) uses generic extension to generate a new stream whose state is the second component
of [e2/x]e1. Expanding the generic extension we obtain the following reformulation of this rule:
genstream( x . e1 ; e2 ) val
unfoldstream( genstream( x . e1 ; e2 ) )
7−→
⟨( [e2/x]e1 ) · l, genstream( x . e1 ; ( [e2/x]e1 ) · r )⟩
Exercise 15.3 asks for a derivation of genstream x is e in ⟨hd ,→e1,tl ,→e2 ⟩from the coinductive
generation form.
15.2
Statics
We may now give a general account of inductive and coinductive types, which are defined in
terms of positive type operators. We will consider a variant of T, which we will call M, with
nullary and binary products and sums, and natural numbers replaced by a rich class of inductive
and coinductive types. The natural numbers can be reinstated as a particular inductive type.

PREVIEW
15.2 Statics
133
15.2.1
Types
The syntax of inductive and coinductive types involves type variables, which are, of course, vari-
ables ranging over types. The abstract syntax of inductive and coinductive types is given by the
following grammar:
Typ
τ
::=
t
t
self-reference
mu( t . τ )
µ( t . τ )
inductive
nu( t . τ )
ν( t . τ )
coinductive
Type formation judgments have the form
t1 type, . . . , tn type ⊢τ type,
where t1, . . . , tn are type names. We let ∆range over finite sets of hypotheses of the form t type,
where t is a type name. The type formation judgment is inductively defined by the following rules:
∆, t type ⊢t type
(15.7a)
∆⊢unit type
(15.7b)
∆⊢τ1 type
∆⊢τ2 type
∆⊢prod( τ1 ; τ2 ) type
(15.7c)
∆⊢void type
(15.7d)
∆⊢τ1 type
∆⊢τ2 type
∆⊢sum( τ1 ; τ2 ) type
(15.7e)
∆⊢τ1 type
∆⊢τ2 type
∆⊢arr( τ1 ; τ2 ) type
(15.7f)
∆, t type ⊢τ type
∆⊢t . τ pos
∆⊢mu( t . τ ) type
(15.7g)
∆, t type ⊢τ type
∆⊢t . τ pos
∆⊢nu( t . τ ) type
(15.7h)
15.2.2
Expressions
The abstract syntax of M is given by the following grammar:
Exp
e
::=
fold{t . τ}( e )
fold{t . τ}( e )
constructor
rec{t . τ}( x . e1 ; e2 )
rec{t . τ}( x . e1 ; e2 )
iterator
unfold{t . τ}( e )
unfold{t . τ}( e )
destructor
gen{t . τ}( x . e1 ; e2 )
gen{t . τ}( x . e1 ; e2 )
generator

PREVIEW
134
15.3 Dynamics
The statics for M is given by the following typing rules:
Γ ⊢e : [mu( t . τ )/t]τ
Γ ⊢fold{t . τ}( e ) : mu( t . τ )
(15.8a)
Γ, x : [ρ/t]τ ⊢e1 : ρ
Γ ⊢e2 : mu( t . τ )
Γ ⊢rec{t . τ}( x . e1 ; e2 ) : ρ
(15.8b)
Γ ⊢e : nu( t . τ )
Γ ⊢unfold{t . τ}( e ) : [nu( t . τ )/t]τ
(15.8c)
Γ ⊢e2 : σ
Γ, x : σ ⊢e1 : [σ/t]τ
Γ ⊢gen{t . τ}( x . e1 ; e2 ) : nu( t . τ )
(15.8d)
15.3
Dynamics
The dynamics of M is given in terms of the positive generic extension operation described in
Chapter 14. The following rules specify the dynamics for M.
[e val]
fold{t . τ}( e ) val
(15.9a)


e 7−→e′
fold{t . τ}( e ) 7−→fold{t . τ}( e′ )


(15.9b)
e2 7−→e′
2
rec{t . τ}( x . e1 ; e2 ) 7−→rec{t . τ}( x . e1 ; e′
2 )
(15.9c)
fold{t . τ}( e2 ) val
rec{t . τ}( x . e1 ; fold{t . τ}( e2 ) )
7−→
[map+{t . τ}( y . rec{t . τ}( x . e1 ; y ) )( e2 )/x]e1
(15.9d)
[e2 val]
gen{t . τ}( x . e1 ; e2 ) val
(15.9e)


e2 7−→e′
2
gen{t . τ}( x . e1 ; e2 ) 7−→gen{t . τ}( x . e1 ; e′
2 )


(15.9f)
e 7−→e′
unfold{t . τ}( e ) 7−→unfold{t . τ}( e′ )
(15.9g)

PREVIEW
15.4 Solving Type Equations
135
gen{t . τ}( x . e1 ; e2 ) val
unfold{t . τ}( gen{t . τ}( x . e1 ; e2 ) )
7−→
map+{t . τ}( y . gen{t . τ}( x . e1 ; y ) )( [e2/x]e1 )
(15.9h)
Rule (15.9d) states that to evaluate the iterator on a value of recursive type, we inductively apply
the iterator as guided by the type operator to the value, and then apply the inductive step to the
result. Rule (15.9h) is simply the dual of this rule for coinductive types.
Lemma 15.1. If e : τ and e 7−→e′, then e′ : τ.
Proof. By rule induction on rules (15.9).
Lemma 15.2. If e : τ, then either e val or there exists e′ such that e 7−→e′.
Proof. By rule induction on rules (15.8).
Although a proof of this fact lies beyond our current reach, all programs in M terminate.
Theorem 15.3 (Termination for M). If e : τ, then there exists e′ val such that e 7−→∗e′.
It may, at first, seem surprising that a language with infinite data structures, such as streams,
can enjoy such a termination property. But bear in mind that infinite data structures, such as
streams, are represented as in a continuing state of creation, and not as a completed whole.
15.4
Solving Type Equations
For a positive type operator t . τ, we may say that the inductive type µ( t . τ ) and the coinductive
type ν( t . τ ) are both solutions (up to isomorphism) of the type equation t ∼= τ:
µ( t . τ ) ∼= [µ( t . τ )/t]τ
ν( t . τ ) ∼= [ν( t . τ )/t]τ.
According to Rule (15.8a), every value of an inductive type is the folding of a value of the unfold-
ing of the inductive type. Similarly, according to rule (15.8c), every value of the unfolding of a
coinductive type is the unfolding of a value of the coinductive type itself. It is a good exercise to
define functions back and forth between the isomorphic types, and to convince yourself informally
that they are mutually inverse to one another.
Whereas both are solutions to the same type equation, they are not isomorphic to each other.
To see why, consider the inductive type nat ≜µ( t . unit + t ) and the coinductive type conat ≜
ν( t . unit + t ). Informally, nat is the smallest (most restrictive) type containing zero, given by
fold( l · ⟨⟩), and closed under formation of the successor of another e of type nat, given by
fold( r · e ). Dually, conat is the largest (most permissive) type of expressions e for which the

PREVIEW
136
15.5 Notes
unfolding, unfold( e ), is either zero, given by l · ⟨⟩, or the successor of some other e′ of type
conat, given by r · e′.
Because nat is defined by the composition of its introduction forms and sum injections, it is
clear that only finite natural numbers can be constructed in finite time. Because conat is defined
by the composition of its elimination forms (unfoldings plus case analyses), it is clear that a co-
natural number can only be explored to finite depth in finite time—essentially we can only exam-
ine some finite number of predecessors of a given co-natural number in a terminating program.
Consequently,
1. there is a function i : nat →conat embedding every finite natural number into the type of
possibly infinite natural numbers; and
2. there is an “actually infinite” co-natural number ∞that is essentially an infinite composition
of successors.
Defining the embedding of nat into conat is the subject of Exercise 15.1. The infinite co-natural
number ∞is defined as follows:
∞≜gen{t . unit + t}( x . r · x ; ⟨⟩).
One may check that unfold( ∞) 7−→∗r · ∞, which means that ∞is its own predecessor. The
co-natural number ∞is larger than any finite natural number in that any finite number of prede-
cessors of ∞is non-zero.
Summing up, the mere fact of being a solution to a type equation does not uniquely character-
ize a type: there can be many different solutions to the same type equation, the natural and the
co-natural numbers being good examples of the discrepancy. However, we will show in Part VIII
that type equations have unique solutions (up to isomorphism), and that the restriction to polyno-
mial type operators is no longer required. The price we pay for the additional expressive power is
that programs are no longer guaranteed to terminate.
15.5
Notes
The language M is named after Mendler, on whose work the present treatment is based (Mendler,
1987). Mendler’s work is grounded in category theory, specifically the concept of an algebra for
a functor (MacLane, 1998; Taylor, 1999). The functorial action of a type constructor (described in
Chapter 14) plays a central role. Inductive types are initial algebras and coinductive types are final
coalgebras for the functor given by a (polynomial or positive) type operator.
Exercises
15.1. Define a function i : nat →conat that sends every natural number to “itself” in the sense
that every finite natural number is sent to its correlate as a co-natural number.

PREVIEW
15.5 Notes
137
(a) unfold( i( z ) ) 7−→∗l · ⟨⟩.
(b) unfold( i( s( n ) ) ) 7−→∗r · i( n ).
15.2. Derive the iterator, iter e {z ,→e0 | s( x ) ,→e1}, described in Chapter 9 from the iterator for
the inductive type of natural numbers given in Section 15.1.
15.3. Derive the stream generator, genstream x is e in ⟨hd ,→e1,tl ,→e2 ⟩from the generator for the
coinductive stream type given in Section 15.1.
15.4. Consider the type seq ≜nat →nat of infinite sequences of natural numbers. Every stream
can be turned into a sequence by the following function:
λ ( stream : s ) λ ( n : nat ) hd( iter n {z ,→s | s( x ) ,→tl( x )} ).
Show that every sequence can be turned into a stream whose nth element is the nth element
of the given sequence.
15.5. The type of lists of natural numbers is defined by the following introduction and elimination
forms:
Γ ⊢nil : natlist
(15.10a)
Γ ⊢e1 : nat
Γ ⊢e2 : natlist
Γ ⊢cons( e1 ; e2 ) : natlist
(15.10b)
Γ ⊢e : natlist
Γ ⊢e0 : ρ
Γ x : nat y : ρ ⊢e1 : ρ
Γ ⊢reclist e {nil ,→e0 | cons( x ; y ) ,→e1} : ρ
(15.10c)
The associated dynamics, whether eager or lazy, can be derived from that of the recursor for
the type nat given in Chapter 9. Give a definition of natlist as an inductive type, including
the definitions of its associated introduction and elimination forms. Check that they validate
the expected dynamics.
15.6. Consider the type itree of possibly infinite binary trees with the following introduction and
elimination forms:
Γ ⊢e : itree
Γ ⊢view( e ) : ( itree × itree ) opt
(15.11a)
Γ ⊢e : σ
Γ x : σ ⊢e′ : ( σ × σ ) opt
Γ ⊢genitree x is e in e′ : itree
(15.11b)
Because a possibly infinite tree must be in a state of continual generation, viewing a tree
exposes only its top-level structure, an optional pair of possibly infinite trees.1 If the view
is null, the tree is empty, and if it is just( e1 )e2, then it is non-empty, with children given
by e1 and e2. To generate an infinite tree, choose a type σ of its state of generation, and
provide its current state e and a state transformation e′ that, when applied to the current
state, announces whether or not generation is complete, and, if not, provides the state for
each of the children.
1See Chapter 11 for the definition of option types.

PREVIEW
138
15.5 Notes
(a) Give a precise dynamics for the itree operations as just described informally. Hint: use
generic programming!
(b) Reformulate the type itree as a coinductive type, and derive the statics and dynamics
of its introduction and elimination forms.
15.7. Exercise 11.5 asked you to define an RS latch as a signal transducer, in which signals are
expressed explicitly as functions of time. Here you are asked again to define an RS latch
as a signal transducer, but this time with signals expressed as streams of booleans. Under
such a representation time is implicitly represented by the successive elements of the stream.
Define an RS latch as a transducer of signals consisting of pairs of booleans.

PREVIEW
Part VII
Variable Types

PREVIEW

PREVIEW
Chapter 16
System F of Polymorphic Types
The languages we have considered so far are all monomorphic in that every expression has a unique
type, given the types of its free variables, if it has a type at all. Yet it is often the case that essentially
the same behavior is required, albeit at several different types. For example, in T there is a distinct
identity function for each type τ, namely λ ( x : τ ) x, even though the behavior is the same for each
choice of τ. Similarly, there is a distinct composition operator for each triple of types, namely
◦τ1,τ2,τ3 = λ ( f : τ2 →τ3 ) λ ( g : τ1 →τ2 ) λ ( x : τ1 ) f ( g( x ) ).
Each choice of the three types requires a different program, even though they all have the same
behavior when executed.
Obviously it would be useful to capture the pattern once and for all, and to instantiate this
pattern each time we need it. The expression patterns codify generic (type-independent) behaviors
that are shared by all instances of the pattern. Such generic expressions are polymorphic. In this
chapter we will study the language F, which was introduced by Girard under the name System F
and by Reynolds under the name polymorphic typed λ-calculus. Although motivated by a simple
practical problem (how to avoid writing redundant code), the concept of polymorphism is central
to an impressive variety of seemingly disparate concepts, including the concept of data abstraction
(the subject of Chapter 17), and the definability of product, sum, inductive, and coinductive types
considered in the preceding chapters. (Only general recursive types extend the expressive power
of the language.)

PREVIEW
142
16.1 Polymorphic Abstraction
16.1
Polymorphic Abstraction
The language F is a variant of T in which we eliminate the type of natural numbers, but add, in
compensation, polymorphic types:1
Typ
τ
::=
t
t
variable
arr( τ1 ; τ2 )
τ1 →τ2
function
all( t . τ )
∀( t . τ )
polymorphic
Exp
e
::=
x
x
λ{τ}( x . e )
λ ( x : τ ) e
abstraction
ap( e1 ; e2 )
e1( e2 )
application
Λ( t . e )
Λ( t ) e
type abstraction
App{τ}( e )
e[ τ ]
type application
A type abstraction Λ( t . e ) defines a generic, or polymorphic, function with type variable t standing for
an unspecified type within e. A type application, or instantiation App{τ}( e ) applies a polymorphic
function to a specified type, which is plugged in for the type variable to obtain the result. The
universal type, all( t . τ ), classifies polymorphic functions.
The statics of F consists of two judgment forms, the type formation judgment,
∆⊢τ type,
and the typing judgment,
∆Γ ⊢e : τ.
The hypotheses ∆have the form t type, where t is a variable of sort Typ, and the hypotheses Γ have
the form x : τ, where x is a variable of sort Exp.
The rules defining the type formation judgment are as follows:
∆, t type ⊢t type
(16.1a)
∆⊢τ1 type
∆⊢τ2 type
∆⊢arr( τ1 ; τ2 ) type
(16.1b)
∆, t type ⊢τ type
∆⊢all( t . τ ) type
(16.1c)
The rules defining the typing judgment are as follows:
∆Γ, x : τ ⊢x : τ
(16.2a)
∆⊢τ1 type
∆Γ, x : τ1 ⊢e : τ2
∆Γ ⊢λ{τ1}( x . e ) : arr( τ1 ; τ2 )
(16.2b)
∆Γ ⊢e1 : arr( τ2 ; τ )
∆Γ ⊢e2 : τ2
∆Γ ⊢ap( e1 ; e2 ) : τ
(16.2c)
1Girard’s original version of System F included the natural numbers as a basic type.

PREVIEW
16.1 Polymorphic Abstraction
143
∆, t type Γ ⊢e : τ
∆Γ ⊢Λ( t . e ) : all( t . τ )
(16.2d)
∆Γ ⊢e : all( t . τ′ )
∆⊢τ type
∆Γ ⊢App{τ}( e ) : [τ/t]τ′
(16.2e)
Lemma 16.1 (Regularity). If ∆Γ ⊢e : τ, and if ∆⊢τi type for each assumption xi : τi in Γ, then
∆⊢τ type.
Proof. By induction on rules (16.2).
The statics admits the structural rules for a general hypothetical judgment. In particular, we
have the following critical substitution property for type formation and expression typing.
Lemma 16.2 (Substitution).
1. If ∆, t type ⊢τ′ type and ∆⊢τ type, then ∆⊢[τ/t]τ′ type.
2. If ∆, t type Γ ⊢e′ : τ′ and ∆⊢τ type, then ∆[τ/t]Γ ⊢[τ/t]e′ : [τ/t]τ′.
3. If ∆Γ, x : τ ⊢e′ : τ′ and ∆Γ ⊢e : τ, then ∆Γ ⊢[e/x]e′ : τ′.
The second part of the lemma requires substitution into the context Γ as well as into the term
and its type, because the type variable t may occur freely in any of these positions.
Returning to the motivating examples from the introduction, the polymorphic identity func-
tion, I, is written
Λ( t ) λ ( x : t ) x;
it has the polymorphic type
∀( t . t →t ).
Instances of the polymorphic identity are written I[ τ ], where τ is some type, and have the type
τ →τ.
Similarly, the polymorphic composition function, C, is written
Λ( t1 ) Λ( t2 ) Λ( t3 ) λ ( f : t2 →t3 ) λ ( g : t1 →t2 ) λ ( x : t1 ) f ( g( x ) ).
The function C has the polymorphic type
∀( t1 . ∀( t2 . ∀( t3 . ( t2 →t3 ) →( t1 →t2 ) →( t1 →t3 ) ) ) ).
Instances of C are obtained by applying it to a triple of types, written C[ τ1 ][ τ2 ][ τ3 ]. Each such
instance has the type
( τ2 →τ3 ) →( τ1 →τ2 ) →( τ1 →τ3 ).

PREVIEW
144
16.1 Polymorphic Abstraction
Dynamics
The dynamics of F is given as follows:
λ{τ}( x . e ) val
(16.3a)
Λ( t . e ) val
(16.3b)
[e2 val]
ap( λ{τ1}( x . e ) ; e2 ) 7−→[e2/x]e
(16.3c)
e1 7−→e′
1
ap( e1 ; e2 ) 7−→ap( e′
1 ; e2 )
(16.3d)


e1 val
e2 7−→e′
2
ap( e1 ; e2 ) 7−→ap( e1 ; e′
2 )


(16.3e)
App{τ}( Λ( t . e ) ) 7−→[τ/t]e
(16.3f)
e 7−→e′
App{τ}( e ) 7−→App{τ}( e′ )
(16.3g)
The bracketed premises and rule are included for a call-by-value interpretation, and omitted for a
call-by-name interpretation of F.
It is a simple matter to prove safety for F, using familiar methods.
Lemma 16.3 (Canonical Forms). Suppose that e : τ and e val, then
1. If τ = arr( τ1 ; τ2 ), then e = λ{τ1}( x . e2 ) with x : τ1 ⊢e2 : τ2.
2. If τ = all( t . τ′ ), then e = Λ( t . e′ ) with t type ⊢e′ : τ′.
Proof. By rule induction on the statics.
Theorem 16.4 (Preservation). If e : τ and e 7−→e′, then e′ : τ.
Proof. By rule induction on the dynamics.
Theorem 16.5 (Progress). If e : τ, then either e val or there exists e′ such that e 7−→e′.
Proof. By rule induction on the statics.

PREVIEW
16.2 Polymorphic Definability
145
16.2
Polymorphic Definability
The language F is astonishingly expressive. Not only are all (lazy) finite products and sums de-
finable in the language, but so are all (lazy) inductive and coinductive types. Their definability is
most naturally expressed using definitional equality, which is the least congruence containing the
following two axioms:
∆Γ, x : τ1 ⊢e2 : τ2
∆Γ ⊢e1 : τ1
∆Γ ⊢( λ ( x : τ1 ) e2 )( e1 ) ≡[e1/x]e2 : τ2
(16.4a)
∆, t type Γ ⊢e : τ
∆⊢ρ type
∆Γ ⊢( Λ( t ) e )[ ρ ] ≡[ρ/t]e : [ρ/t]τ
(16.4b)
In addition there are rules omitted here specifying that definitional equality is a congruence rela-
tion (that is, an equivalence relation respected by all expression-forming operations).
16.2.1
Products and Sums
The nullary product, or unit, type is definable in F as follows:
unit ≜∀( r . r →r )
⟨⟩≜Λ( r ) λ ( x : r ) x
The identity function plays the role of the null tuple, because it is the only closed value of this
type.
Binary products are definable in F by using encoding tricks similar to those described in Chap-
ter 21 for the untyped λ-calculus:
τ1 × τ2 ≜∀( r . (τ1 →τ2 →r) →r )
⟨e1, e2⟩≜Λ( r ) λ ( x : τ1 →τ2 →r ) x( e1 )( e2 )
e · l ≜e[ τ1 ]( λ ( x : τ1 ) λ ( y : τ2 ) x )
e · r ≜e[ τ2 ]( λ ( x : τ1 ) λ ( y : τ2 ) y )
The statics given in Chapter 10 is derivable according to these definitions. Moreover, the following
definitional equalities are derivable in F from these definitions:
⟨e1, e2⟩· l ≡e1 : τ1
and
⟨e1, e2⟩· r ≡e2 : τ2.
The nullary sum, or void, type is definable in F:
void ≜∀( r . r )
case e { } ≜e[ ρ ]

PREVIEW
146
16.2 Polymorphic Definability
Binary sums are also definable in F:
τ1 + τ2 ≜∀( r . ( τ1 →r ) →( τ2 →r ) →r )
l · e ≜Λ( r ) λ ( x : τ1 →r ) λ ( y : τ2 →r ) x( e )
r · e ≜Λ( r ) λ ( x : τ1 →r ) λ ( y : τ2 →r ) y( e )
case e {l · x1 ,→e1 | r · x2 ,→e2} ≜
e[ ρ ]( λ ( x1 : τ1 ) e1 )( λ ( x2 : τ2 ) e2 )
provided that the types make sense. It is easy to check that the following equivalences are deriv-
able in F:
case l · d1 {l · x1 ,→e1 | r · x2 ,→e2} ≡[d1/x1]e1 : ρ
and
case r · d2 {l · x1 ,→e1 | r · x2 ,→e2} ≡[d2/x2]e2 : ρ.
Thus the dynamic behavior specified in Chapter 11 is correctly implemented by these definitions.
16.2.2
Natural Numbers
As we remarked above, the natural numbers (under a lazy interpretation) are also definable in F.
The key is the iterator, whose typing rule we recall here for reference:
e0 : nat
e1 : τ
x : τ ⊢e2 : τ
iter{τ}( e0 ; e1 ; x . e2 ) : τ
.
Because the result type τ is arbitrary, this means that if we have an iterator, then we can use it to
define a function of type
nat →∀( t . t →( t →t ) →t ).
This function, when applied to an argument n, yields a polymorphic function that, for any result
type, t, given the initial result for z and a transformation from the result for x into the result for
s( x ), yields the result of iterating the transformation n times, starting with the initial result.
Because the only operation we can perform on a natural number is to iterate up to it, we may
simply identify a natural number, n, with the polymorphic iterate-up-to-n function just described.
Thus we may define the type of natural numbers in F by the following equations:
nat ≜∀( t . t →( t →t ) →t )
z ≜Λ( t ) λ ( z : t ) λ ( s : t →t ) z
s( e ) ≜Λ( t ) λ ( z : t ) λ ( s : t →t ) s( e[ t ]( z )( s ) )
iter{τ}( e0 ; e1 ; x . e2 ) ≜e0[ τ ]( e1 )( λ ( x : τ ) e2 )
It is easy to check that the statics and dynamics of the natural numbers type given in Chapter 9
are derivable in F under these definitions. The representations of the numerals in F are called the
polymorphic Church numerals.

PREVIEW
16.3 Parametricity Overview
147
The encodability of the natural numbers shows that F is at least as expressive as T. But is it more
expressive? Yes! It is possible to show that the evaluation function for T is definable in F, even
though it is not definable in T itself. However, the same diagonal argument given in Chapter 9
applies here, showing that the evaluation function for F is not definable in F. We may enrich F a bit
more to define the evaluator for F, but as long as all programs in the enriched language terminate,
we will once again have an undefinable function, the evaluation function for that extension.
16.3
Parametricity Overview
A remarkable property of F is that polymorphic types severely constrain the behavior of their
elements. We may prove useful theorems about an expression knowing only its type—that is,
without ever looking at the code. For example, if i is any expression of type ∀( t . t →t ), then it
is the identity function. Informally, when i is applied to a type, τ, and an argument of type τ, it
returns a value of type τ. But because τ is not specified until i is called, the function has no choice
but to return its argument, which is to say that it is essentially the identity function. Similarly, if b
is any expression of type ∀( t . t →t →t ), then b is equivalent to either Λ( t ) λ ( x : t ) λ ( y : t ) x or
Λ( t ) λ ( x : t ) λ ( y : t ) y. Intuitively, when b is applied to two arguments of a given type, the only
value it can return is one of the givens.
Properties of a program in F that can be proved knowing only its type are called parametricity
properties. The facts about the functions i and b stated above are examples of parametricity prop-
erties. Such properties are sometimes called “free theorems,” because they come from typing “for
free”, without any knowledge of the code itself. It bears repeating that in F we prove non-trivial
behavioral properties of programs without ever examining the program text. The key to this in-
credible fact is that we are able to prove a deep property, called parametricity, about the language F,
that then applies to every program written in F. One may say that the type system “pre-verifies”
programs with respect to a broad range of useful properties, eliminating the need to prove those
properties about every program separately. The parametricity theorem for F explains the remark-
able experience that if a piece of code type checks, then it “just works.” Parametricity narrows the
space of well-typed programs sufficiently that the opportunities for programmer error are reduced
to almost nothing.
So how does the parametricity theorem work? Without getting into too many technical details
(but see Chapter 48 for a full treatment), we can give a brief summary of the main idea. Any
function i : ∀( t . t →t ) in F enjoys the following property:
For any type τ and any property P of the type τ, then if P holds of x : τ, then P holds of
i[ τ ]( x ).
To show that for any type τ, and any x of type τ, the expression i[ τ ]( x ) is equivalent to x, it
suffices to fix x0 : τ, and consider the property Px0 that holds of y : τ iff y is equivalent to x0.
Obviously P holds of x0 itself, and hence by the above-displayed property of i, it sends any argu-
ment satisfying Px0 to a result satisfying Px0, which is to say that it sends x0 to x0. Because x0 is
an arbitrary element of τ, it follows that i[ τ ] is the identity function, λ ( x : τ ) x, on the type τ, and
because τ is itself arbitrary, i is the polymorphic identity function, Λ( t ) λ ( x : t ) x.

PREVIEW
148
16.4 Notes
A similar argument suffices to show that the function b, defined above, is either Λ( t ) λ ( x : t ) λ ( y : t ) x
or Λ( t ) λ ( x : t ) λ ( y : t ) y. By virtue of its type the function b enjoys the parametricity property
For any type τ and any property P of τ, if P holds of x : τ and of y : τ, then P holds of
b[ τ ]( x )( y ).
Choose an arbitrary type τ and two arbitrary elements x0 and y0 of type τ. Define Qx0,y0 to hold
of z : τ iff either z is equivalent to x0 or z is equivalent to y0. Clearly Qx0,y0 holds of both x0
and y0 themselves, so by the quoted parametricity property of b, it follows that Qx0,y0 holds of
b[ τ ]( x0 )( y0 ), which is to say that it is equivalent to either x0 or y0. Since τ, x0, and y0 are arbi-
trary, it follows that b is equivalent to either Λ( t ) λ ( x : t ) λ ( y : t ) x or Λ( t ) λ ( x : t ) λ ( y : t ) y.
The parametricity theorem for F implies even stronger properties of functions such as i and
b considered above. For example, the function i of type ∀( t . t →t ) also satisfies the following
condition:
If τ and τ′ are any two types, and R is a binary relation between τ and τ′, then for any x : τ
and x′ : τ′, if R relates x to x′, then R relates i[ τ ]( x ) to i[ τ′ ]( x′ ).
Using this property we may again prove that i is equivalent to the polymorphic identity function.
Specifically, if τ is any type and g : τ →τ is any function on that type, then it follows from the
type of i alone that i[ τ ]( g( x ) ) is equivalent to g( i[ τ ]( x ) ) for any x : τ. To prove this, simply
choose R to the be graph of the function g, the relation Rg that holds of x and x′ iff x′ is equivalent
to g( x ). The parametricity property of i, when specialized to Rg, states that if x′ is equivalent to
g( x ), then i[ τ ]( x′ ) is equivalent to g( i[ τ ]( x ) ), which is to say that i[ τ ]( g( x ) ) is equivalent
to g( i[ τ ]( x ) ). To show that i is equivalent to the identity function, choose x0 : τ arbitrarily, and
consider the constant function g0 on τ that always returns x0. Because x0 is equivalent to g0( x0 ),
it follows that i[ τ ]( x0 ) is equivalent to x0, which is to say that i behaves like the polymorphic
identity function.
16.4
Notes
System F was introduced by Girard (1972) in the context of proof theory and by Reynolds (1974)
in the context of programming languages. The concept of parametricity was originally isolated
by Strachey, but was not fully developed until the work of Reynolds (1983). The phrase “free
theorems” for parametricity theorems was introduced by Wadler (1989).
Exercises
16.1. Give polymorphic definitions and types to the s and k combinators defined in Exercise 3.1.
16.2. Define in F the type bool of Church booleans. Define the type bool, and define true and false
of this type, and the conditional if e then e0 else e1, where e is of this type.

PREVIEW
16.4 Notes
149
16.3. Define in F the inductive type of lists of natural numbers as defined in Chapter 15. Hint:
Define the representation in terms of the recursor (elimination form) for lists, following the
pattern for defining the type of natural numbers.
16.4. Define in F an arbitrary inductive type, µ( t . τ ). Hint: generalize your answer to Exer-
cise 16.3.
16.5. Define the type t list as in Exercise 16.3, with the element type, t, unspecified. Define the
finite set of elements of a list l to be those x given by the head of some number of tails of l.
Now suppose that f : ∀( t . t list →t list ) is an arbitrary function of the stated type. Show
that the elements of f [ τ ]( l ) are a subset of those of l. Thus f may only permute, replicate,
or drop elements from its input list to obtain its output list.

PREVIEW
150
16.4 Notes

PREVIEW
Chapter 17
Abstract Types
Data abstraction is perhaps the most important technique for structuring programs. The main
idea is to introduce an interface that serves as a contract between the client and the implementor
of an abstract type. The interface specifies what the client may rely on for its own work, and,
simultaneously, what the implementor must provide to satisfy the contract. The interface serves to
isolate the client from the implementor so that each may be developed in isolation from the other.
In particular one implementation can be replaced by another without affecting the behavior of the
client, provided that the two implementations meet the same interface and that each simulates
the other with respect to the operations of the interface. This property is called representation
independence for an abstract type.
Data abstraction is formalized by extending the language F with existential types. Interfaces
are existential types that provide a collection of operations acting on an unspecified, or abstract,
type. Implementations are packages, the introduction form for existential types, and clients are
uses of the corresponding elimination form. It is remarkable that the programming concept of
data abstraction is captured so naturally and directly by the logical concept of existential type
quantification. Existential types are closely connected with universal types, and hence are often
treated together. The superficial reason is that both are forms of type quantification, and hence
both require the machinery of type variables. The deeper reason is that existential types are de-
finable from universals — surprisingly, data abstraction is actually just a form of polymorphism!
Consequently, representation independence is an application of the parametricity properties of
polymorphic functions discussed in Chapter 16.
17.1
Existential Types
The syntax of FE extends F with the following constructs:
Typ
τ
::=
some( t . τ )
∃( t.τ )
interface
Exp
e
::=
pack{t . τ}( ρ ; e )
pack ρ with e as ∃( t.τ )
implementation
open{t . τ ; ρ}( e1 ; t, x . e2 )
open e1 as t with x:τ in e2
client

PREVIEW
152
17.1 Existential Types
The introduction form ∃( t.τ ) is a package of the form pack ρ with e as ∃( t.τ ), where ρ is a type
and e is an expression of type [ρ/t]τ. The type ρ is the representation type of the package, and
the expression e is the implementation of the package.
The elimination form is the expression
open e1 as t with x:τ in e2, which opens the package e1 for use within the client e2 by binding its
representation type to t and its implementation to x for use within e2. Crucially, the typing rules
ensure that the client is type-correct independently of the actual representation type used by the
implementor, so that it can be varied without affecting the type correctness of the client.
The abstract syntax of the open construct specifies that the type variable t and the expression
variable x are bound within the client. They may be renamed at will by α-equivalence without
affecting the meaning of the construct, provided, of course, that the names do not conflict with
any others in scope. In other words the type t is a “new” type, one that is distinct from all other
types, when it is introduced. This principle is sometimes called generativity of abstract types: the
use of an abstract type by a client “generates” a “new” type within that client. This behavior relies
on the identification convention stated in Chapter 1.
17.1.1
Statics
The statics of FE is given by these rules:
∆, t type ⊢τ type
∆⊢some( t . τ ) type
(17.1a)
∆⊢ρ type
∆, t type ⊢τ type
∆Γ ⊢e : [ρ/t]τ
∆Γ ⊢pack{t . τ}( ρ ; e ) : some( t . τ )
(17.1b)
∆Γ ⊢e1 : some( t . τ )
∆, t type Γ, x : τ ⊢e2 : τ2
∆⊢τ2 type
∆Γ ⊢open{t . τ ; τ2}( e1 ; t, x . e2 ) : τ2
(17.1c)
Rule (17.1c) is complex, so study it carefully! There are two important things to notice:
1. The type of the client, τ2, must not involve the abstract type t. This restriction prevents
the client from attempting to export a value of the abstract type outside of the scope of its
definition.
2. The body of the client, e2, is type checked without knowledge of the representation type, t.
The client is, in effect, polymorphic in the type variable t.
Lemma 17.1 (Regularity). Suppose that ∆Γ ⊢e : τ. If ∆⊢τi type for each xi : τi in Γ, then ∆⊢τ type.
Proof. By induction on rules (17.1), using substitution for expressions and types.
17.1.2
Dynamics
The dynamics of FE is defined by the following rules (including the bracketed material for an
eager interpretation, and omitting it for a lazy interpretation):
[e val]
pack{t . τ}( ρ ; e ) val
(17.2a)

PREVIEW
17.2 Data Abstraction
153


e 7−→e′
pack{t . τ}( ρ ; e ) 7−→pack{t . τ}( ρ ; e′ )


(17.2b)
e1 7−→e′
1
open{t . τ ; τ2}( e1 ; t, x . e2 ) 7−→open{t . τ ; τ2}( e′
1 ; t, x . e2 )
(17.2c)
[e val]
open{t . τ ; τ2}( pack{t . τ}( ρ ; e ) ; t, x . e2 ) 7−→[ρ, e/t, x]e2
(17.2d)
It is important to see that, according to these rules, there are no abstract types at run time! The rep-
resentation type is propagated to the client by substitution when the package is opened, thereby
eliminating the abstraction boundary between the client and the implementor. Thus, data abstrac-
tion is a compile-time discipline that leaves no traces of its presence at execution time.
17.1.3
Safety
Safety of FE is stated and proved by decomposing it into progress and preservation.
Theorem 17.2 (Preservation). If e : τ and e 7−→e′, then e′ : τ.
Proof. By rule induction on e 7−→e′, using substitution for both expression- and type variables.
Lemma 17.3 (Canonical Forms). If e : some( t . τ ) and e val, then e = pack{t . τ}( ρ ; e′ ) for some type
ρ and some e′ such that e′ : [ρ/t]τ.
Proof. By rule induction on the statics, using the definition of closed values.
Theorem 17.4 (Progress). If e : τ then either e val or there exists e′ such that e 7−→e′.
Proof. By rule induction on e : τ, using the canonical forms lemma.
17.2
Data Abstraction
To illustrate the use of FE, we consider an abstract type of queues of natural numbers supporting
three operations:
1. Forming the empty queue.
2. Inserting an element at the tail of the queue.
3. Removing the head of the queue, which is assumed non-empty.

PREVIEW
154
17.2 Data Abstraction
This is clearly a bare-bones interface, but suffices to illustrate the main ideas of data abstraction.
Queue elements are natural numbers, but nothing depends on this choice.
The crucial property of this description is that nowhere do we specify what queues actually
are, only what we can do with them. The behavior of a queue is expressed by the existential type
∃( t.τ ) which serves as the interface of the queue abstraction:
∃( t.⟨emp ,→t , ins ,→nat × t →t , rem ,→t →( nat × t ) opt⟩).
The representation type t of queues is abstract — all that is known about it is that it supports the
operations emp, ins, and rem, with the given types.
An implementation of queues consists of a package specifying the representation type, together
with the implementation of the associated operations in terms of that representation. Internally
to the implementation, the representation of queues is known and relied upon by the operations.
Here is a very simple implementation el in which queues are represented as lists:
pack natlist with ⟨emp ,→nil, ins ,→ei, rem ,→er⟩as ∃( t.τ ),
where
ei : nat × natlist →natlist = λ ( x : nat × natlist ) . . .,
and
er : natlist →( nat × natlist ) opt = λ ( x : natlist ) . . ..
The elided body of ei conses the first component of x, the element, onto the second component of
x, the queue, and the elided body of er reverses its argument, and returns the head element paired
with the reversal of the tail. Both of these operations “know” that queues are represented as values
of type natlist, and are programmed accordingly.
It is also possible to give another implementation ep of the same interface ∃( t.τ ), but in which
queues are represented as pairs of lists, consisting of the “back half” of the queue paired with the
reversal of the “front half”. This two-part representation avoids the need for reversals on each call,
and, as a result, achieves amortized constant-time behavior:
pack natlist × natlist with ⟨emp ,→⟨nil, nil⟩, ins ,→ei, rem ,→er⟩as ∃( t.τ ).
In this case ei has type
nat × ( natlist × natlist ) →( natlist × natlist ),
and er has type
( natlist × natlist ) →( nat × ( natlist × natlist ) ) opt.
These operations “know” that queues are represented as values of type natlist × natlist, and
are implemented accordingly.
The important point is that the same client type checks regardless of which implementation
of queues we choose, because the representation type is hidden, or held abstract, from the client
during type checking. Consequently, it cannot rely on whether it is natlist or natlist × natlist
or some other type. That is, the client is independent of the representation of the abstract type.

PREVIEW
17.3 Definability of Existential Types
155
17.3
Definability of Existential Types
The language FE is not a proper extension of F, because existential types (under a lazy dynamics)
are definable in terms of universal types. Why should this be possible? Note that the client of an
abstract type is polymorphic in the representation type. The typing rule for
open e1 as t with x:τ in e2 : τ2,
where e1 : ∃( t.τ ), specifies that e2 : τ2 under the assumptions t type and x : τ. In essence, the
client is a polymorphic function of type
∀( t . τ →τ2 ),
where t may occur in τ (the type of the operations), but not in τ2 (the type of the result).
This suggests the following encoding of existential types:
∃( t.τ ) ≜∀( u . ∀( t . τ →u ) →u )
pack ρ with e as ∃( t.τ ) ≜Λ( u ) λ ( x : ∀( t . τ →u ) ) x[ ρ ]( e )
open e1 as t with x:τ in e2 ≜e1[ τ2 ]( Λ( t ) λ ( x : τ ) e2 )
An existential is encoded as a polymorphic function taking the overall result type u as argument,
followed by a polymorphic function representing the client with result type u, and yielding a
value of type u as overall result. Consequently, the open construct simply packages the client as
such a polymorphic function, instantiates the existential at the result type, τ2, and applies it to the
polymorphic client. (The translation therefore depends on knowing the overall result type τ2 of
the open construct.) Finally, a package consisting of a representation type ρ and an implementation
e is a polymorphic function that, when given the result type u and the client x instantiates x with
ρ and passes to it the implementation e.
17.4
Representation Independence
An important consequence of parametricity is that it ensures that clients are insensitive to the
representations of abstract types. More precisely, there is a criterion, bisimilarity, for relating two
implementations of an abstract type such that the behavior of a client is unaffected by swapping
one implementation by another that is bisimilar to it. This principle leads to a simple method for
proving the correctness of a candidate implementation of an abstract type, which is to show that it
is bisimilar to an obviously correct reference implementation of it. Because the candidate and the
reference implementations are bisimilar, no client may distinguish them from one another, and
hence if the client behaves properly with the reference implementation, then it must also behave
properly with the candidate.
To derive the definition of bisimilarity of implementations, it is helpful to examine the defini-
tion of existential types in terms of universals given in Section 17.3. It is immediately clear that
the client of an abstract type is polymorphic in the representation of the abstract type. A client c
of an abstract type ∃( t.τ ) has type ∀( t . τ →τ2 ), where t does not occur free in τ2 (but may, of

PREVIEW
156
17.4 Representation Independence
course, occur in τ). Applying the parametricity property described informally in Chapter 16 (and
developed rigorously in Chapter 48), this says that if R is a bisimulation relation between any two
implementations of the abstract type, then the client behaves identically on them. The fact that t
does not occur in the result type ensures that the behavior of the client is independent of the choice
of relation between the implementations, provided that this relation is preserved by the operations
that implement it.
Explaining what is a bisimulation is best done by example.
Consider the existential type
∃( t.τ ), where τ is the labeled tuple type
⟨emp ,→t , ins ,→nat × t →t , rem ,→t →( nat × t ) opt⟩.
This specifies an abstract type of queues. The operations emp, ins, and rem specify, respectively, the
empty queue, an insert operation, and a remove operation. For the sake of simplicity the element
type is the type of natural numbers. The result of removal is an optional pair, according to whether
the queue is empty or not.
Theorem 48.12 ensures that if ρ and ρ′ are any two closed types, and if R is a relation between
expressions of these two types, then if the implementations e : [ρ/x]τ and e′ : [ρ′/x]τ respect R,
then c[ ρ ]e behaves the same as c[ ρ′ ]e′. It remains to define when two implementations respect
the relation R. Let
e ≜⟨emp ,→em, ins ,→ei, rem ,→er⟩
and
e′ ≜⟨emp ,→e′
m, ins ,→e′
i, rem ,→e′
r⟩.
For these implementations to respect R means that the following three conditions hold:
1. The empty queues are related: R(em, e′m).
2. Inserting the same element on each of two related queues yields related queues: if d : τ and
R(q, q′), then R(ei( d )( q ), e′
i( d )( q′ )).
3. If two queues are related, then either they are both empty, or their front elements are the
same and their back elements are related: if R(q, q′), then either
(a) er( q ) 7−→∗null and e′r( q′ ) 7−→∗null, or
(b) er( q ) 7−→∗just( ⟨d, r⟩) and e′r( q′ ) 7−→∗just( ⟨d, r′⟩) and R(r, r′).
If such a relation R exists, then the implementations e and e′ are bisimilar. The terminology stems
from the requirement that the operations of the abstract type preserve the relation: if it holds
before an operation is performed, then it must also hold afterwards, and the relation must hold for
the initial state of the queue. Thus each implementation simulates the other up to the relationship
specified by R.
To see how this works in practice, let us consider informally two implementations of the ab-
stract type of queues defined earlier. For the reference implementation we choose ρ to be the type
natlist, and define the empty queue to be the empty list, define insert to add the given element

PREVIEW
17.5 Notes
157
to the head of the list, and define remove to remove the last element of the list. The code is as
follows:
t ≜natlist
emp ≜nil
ins ≜λ ( x : nat ) λ ( q : t ) cons( x ; q )
rem ≜λ ( q : t ) case rev( q ) {nil ,→null | cons( f ; qr ) ,→just( ⟨f, rev( qr )⟩)}.
Removing an element takes time linear in the length of the list, because of the reversal.
For the candidate implementation we choose ρ′ to be the type natlist × natlist of pairs of
lists ⟨b, f ⟩in which b is the “back half” of the queue, and f is the reversal of the “front half” of the
queue. For this representation we define the empty queue to be a pair of empty lists, define insert
to extend the back with that element at the head, and define remove based on whether the front
is empty or not. If it is non-empty, the head element is removed from it, and returned along with
the pair consisting of the back and the tail of the front. If it is empty, and the back is not, then we
reverse the back, remove the head element, and return the pair consisting of the empty list and the
tail of the now-reversed back. The code is as follows:
t ≜natlist × natlist
emp ≜⟨nil, nil⟩
ins ≜λ ( x : nat ) λ ( ⟨bs, f s⟩: t ) ⟨cons( x ; bs ), f s⟩
rem ≜λ ( just( ⟨bs, f s⟩) : t ) case f s {nil ,→e | cons( f ; f s′ ) ,→just( ⟨f, ⟨bs, f s′⟩⟩)}, where
e ≜case rev( bs ) {nil ,→null | cons( b ; bs′ ) ,→just( ⟨b, ⟨nil, bs′⟩⟩)}.
The cost of the occasional reversal is amortized across the sequence of inserts and removes to show
that each operation in a sequence costs unit time overall.
To show that the candidate implementation is correct, we show that it is bisimilar to the
reference implementation.
To do so, we specify a relation R between the types natlist and
natlist × natlist such that the two implementations satisfy the three simulation conditions
given earlier. The required relation states that R(l, ⟨b, f ⟩) iff the list l is the list app( b )( rev( f ) ),
where app is the evident append function on lists. That is, thinking of l as the reference represen-
tation of the queue, the candidate must ensure that the elements of b followed by the elements of f
in reverse order form precisely the list l. It is easy to check that the implementations just described
preserve this relation. Having done so, we are assured that the client c behaves the same regard-
less of whether we use the reference or the candidate. Because the reference implementation is
obviously correct (albeit inefficient), the candidate must also be correct in that the behavior of any
client is not affected by using it instead of the reference.
17.5
Notes
The connection between abstract types in programming languages and existential types in logic
was made by Mitchell and Plotkin (1988). Closely related ideas were already present in Reynolds

PREVIEW
158
17.5 Notes
(1974), but the connection with existential types was not explicitly drawn there. The present for-
mulation of representation independence follows closely Mitchell (1986).
Exercises
17.1. Show that the statics and dynamics of existential types are correctly simulated using the
interpretation given in Section 17.3.
17.2. Define in FE of the coinductive type of streams of natural numbers as defined in Chapter 15.
Hint: Define the representation in terms of the generator (introduction form) for streams.
17.3. Define in FE an arbitrary coinductive type ν( t . τ ). Hint: generalize your answer to Exer-
cise 17.2.
17.4. Representation independence for abstract types is a corollary of the parametricity theorem
for polymorphic types, using the interpretation of FE in F given in Section 17.3. Recast
the proof of equivalence of the two implementations of queues given in Section 17.4 as an
instance of parametricity as defined informally in Chapter 16.

PREVIEW
Chapter 18
Higher Kinds
The concept of type quantification naturally leads to the consideration of quantification over type
constructors, such as list, which are functions mapping types to types. For example, the abstract
type of queues of natural numbers considered in Section 17.4 could be generalized to an abstract
type constructor of queues that does not fix the element type. In the notation that we shall develop
in this chapter such an abstraction is expressed by the existential type ∃q :: Ty →Ty . σ, where σ is
the labeled tuple type
⟨emp ,→∀t :: Ty . q[ t ] , ins ,→∀t :: Ty . t × q[ t ] →q[ t ] , rem ,→∀t :: Ty . q[ t ] →( t × q[ t ] ) opt⟩.
The existential type quantifies over the kind Ty →Ty of type constructors, which map types to
types. The operations are polymorphic, or generic, in the type of the elements of the queue. Their
types involve instances of the abstract queue constructor q[ t ] representing the abstract type of
queues whose elements are of type t. The client instantiates the polymorphic quantifier to specify
the element type; the implementations are parametric in this choice (in that their behavior is the
same in any case). A package of the existential type given above consists of a representation type
constructor and an implementation of the operations in terms of this choice. Possible represen-
tations include the constructor λ ( u :: Ty ) u list and the constructor λ ( u :: Ty ) u list × u list,
both of which have kind Ty →Ty. It is easy to check that the implementations of the queue oper-
ations given in Section 17.4 carry over to the more general case, almost without change, because
they do not rely on the type of the elements of the queue.
The language Fω enriches the language F with universal and existential quantification over
kinds, such as Ty →Ty, used in the queues example. For example, an implementation of the exis-
tential given in the preceding paragraph has to give implementations for the operations in terms of
the choice of representation for q. If, say, q is the constructor λ ( u :: Ty ) u list, then the ins opera-
tion takes a type argument specifying the element type t and a queue of type ( λ ( u :: Ty ) u list )[ t ],
which should simplify to t list by substitution of t for u in the body of the λ-abstraction. Defini-
tional equality of constructors defines the permissible rules of simplification, and thereby defines
when two types are equal. Equal types should be interchangeable as classifiers, meaning that if e is
of type τ and τ is definitionally equal to τ′, then e should also have type τ′. In the queues example

PREVIEW
160
18.1 Constructors and Kinds
any expression of type t list should also be of the unsimplified type to which it is definitionally
equal.
18.1
Constructors and Kinds
The syntax of kinds of Fω is given by the following grammar:
Kind
κ
::=
Ty
Ty
types
Unit
1
nullary product
×( κ1 ; κ2 )
κ1 × κ2
binary product
→( κ1 ; κ2 )
κ1 →κ2
function
The kinds consist of the kind of types Ty and the unit kind Unit and are closed under formation
of product and function kinds.
The syntax of constructors of Fω is defined by this grammar:
Con
c
::=
u
u
variable
arr
→
function constructor
∀{κ}
∀κ
universal quantifier
∃{κ}
∃κ
existential quantifier
proj[ l ]( c )
c · l
first projection
proj[ r ]( c )
c · r
second projection
app( c1 ; c2 )
c1[ c2 ]
application
triv
⟨⟩
null tuple
pair( c1 ; c2 )
⟨c1,c2⟩
pair
λ( u . c )
λ ( u ) c
abstraction
The syntax of constructors follows the syntax of kinds in that there are introduction and elimi-
nation forms for all kinds. The constants →, ∀κ, and ∃κ are the introduction forms for the kind
Ty; there are no elimination forms, because types are only used to classify expressions. We use
the meta-variable τ for constructors of kind Ty, and write τ1 →τ2 for the application →[ τ1 ][ τ2 ],
∀u :: κ . τ for ∀κ[ λ ( u :: κ ) τ ], and similarly for the existential quantifier.
The statics of constructors and kinds of Fω is specified by the judgment
∆⊢c :: κ
which states that the constructor c is well-formed with kind κ. The hypotheses ∆consist of a finite
set of assumptions
u1 :: κ1, . . . , un :: κn,
where n ≥0, specifying the kinds of the active constructor variables.
The statics of constructors is defined by the following rules:
∆, u :: κ ⊢u :: κ
(18.1a)

PREVIEW
18.2 Constructor Equality
161
∆⊢→:: Ty →Ty →Ty
(18.1b)
∆⊢∀κ :: ( κ →Ty ) →Ty
(18.1c)
∆⊢∃κ :: ( κ →Ty ) →Ty
(18.1d)
∆⊢c :: κ1 × κ2
∆⊢c · l :: κ1
(18.1e)
∆⊢c :: κ1 × κ2
∆⊢c · r :: κ2
(18.1f)
∆⊢c1 :: κ2 →κ
∆⊢c2 :: κ2
∆⊢c1[ c2 ] :: κ
(18.1g)
∆⊢⟨⟩:: 1
(18.1h)
∆⊢c1 :: κ1
∆⊢c2 :: κ2
∆⊢⟨c1,c2⟩:: κ1 × κ2
(18.1i)
∆, u :: κ1 ⊢c2 :: κ2
∆⊢λ ( u ) c2 :: κ1 →κ2
(18.1j)
The kinds of the type-forming constants specify that they may be used to build constructors of
kind Ty, the kind of types, which, as usual, classify expressions.
18.2
Constructor Equality
The rules of definitional equality for Fω define when two constructors, in particular two types, are
interchangeable by differing only by simplifications that can be performed to obtain one from the
other. The judgment
∆⊢c1 ≡c2 :: κ
states that c1 and c2 are definitionally equal constructors of kind κ. When κ is the kind Ty, the
constructors c1 and c2 are definitionally equal types.
Definitional equality of constructors is defined by these rules:
∆⊢c :: κ
∆⊢c ≡c :: κ
(18.2a)
∆⊢c ≡c′ :: κ
∆⊢c′ ≡c :: κ
(18.2b)
∆⊢c ≡c′ :: κ
∆⊢c′ ≡c′′ :: κ
∆⊢c ≡c′′ :: κ
(18.2c)
∆⊢c ≡c′ :: κ1 × κ2
∆⊢c · l ≡c′ · l :: κ1
(18.2d)

PREVIEW
162
18.3 Expressions and Types
∆⊢c ≡c′ :: κ1 × κ2
∆⊢c · r ≡c′ · r :: κ2
(18.2e)
∆⊢c1 ≡c′
1 :: κ1
∆⊢c2 ≡c′
2 :: κ2
∆⊢⟨c1,c2⟩≡⟨c′
1,c′
2⟩:: κ1 × κ2
(18.2f)
∆⊢c1 :: κ1
∆⊢c2 :: κ2
∆⊢⟨c1,c2⟩· l ≡c1 :: κ1
(18.2g)
∆⊢c1 :: κ1
∆⊢c2 :: κ2
∆⊢⟨c1,c2⟩· r ≡c2 :: κ2
(18.2h)
∆⊢c1 ≡c′
1 :: κ2 →κ
∆⊢c2 ≡c′
2 :: κ2
∆⊢c1[ c2 ] ≡c′
1[ c′
2 ] :: κ
(18.2i)
∆, u :: κ ⊢c2 ≡c′
2 :: κ2
∆⊢λ ( u :: κ ) c2 ≡λ ( u :: κ ) c′
2 :: κ →κ2
(18.2j)
∆, u :: κ1 ⊢c2 :: κ2
∆⊢c1 :: κ1
∆⊢( λ ( u :: κ ) c2 )[ c1 ] ≡[c1/u]c2 :: κ2
(18.2k)
In short definitional equality of constructors is the strongest congruence containing the rules (18.2g), (18.2h),
and (18.2k).
18.3
Expressions and Types
The statics of expressions of Fω is defined using two judgment forms:
∆⊢τ type
type formation
∆Γ ⊢e : τ
expression formation
Here, as before, Γ is a finite set of hypotheses of the form
x1 : τ1, . . . , xk : τk
such that ∆⊢τi type for each 1 ≤i ≤k.
The types of Fω are the constructors of kind Ty:
∆⊢τ :: Ty
∆⊢τ type
.
(18.3)
This being the only rule for introducing types, the only types are the constructors of kind Ty.
Definitionally equal types classify the same expressions:
∆Γ ⊢e : τ1
∆⊢τ1 ≡τ2 :: Ty
Γ ⊢e : τ2
.
(18.4)

PREVIEW
18.4 Notes
163
This rule ensures that in situations such as that described in the introduction to this chapter, typing
is influenced by simplification of types.
The language Fω extends F to permit universal quantification over arbitrary kinds; the lan-
guage FEω extends Fω with existential quantification over arbitrary kinds. The statics of the quan-
tifiers in FEω is defined by the following rules:
∆, u :: κ Γ ⊢e : τ
∆Γ ⊢Λ( u :: κ ) e : ∀u :: κ . τ
(18.5a)
∆Γ ⊢e : ∀u :: κ . τ
∆⊢c :: κ
∆Γ ⊢e[ c ] : [c/u]τ
(18.5b)
∆⊢c :: κ
∆, u :: κ ⊢τ type
∆Γ ⊢e : [c/u]τ
∆Γ ⊢pack c with e as ∃u :: κ . τ : ∃u :: κ . τ
(18.5c)
∆Γ ⊢e1 : ∃u :: κ . τ
∆, u :: κ Γ, x : τ ⊢e2 : τ2
∆⊢τ2 type
∆Γ ⊢open e1 as u :: κ with x : τ in e2 : τ2
(18.5d)
The dynamics of FEω is the subject of Exercise 18.2.
18.4
Notes
The language Fω given here is standard, apart from details of notation. The rule of invariance of
typing under definitional equality of types demands that a type checking algorithm must include
as a subroutine an algorithm for checking definitional equality. Numerous methods for checking
such equivalences are given in the literature, all of which proceed by various means to simplify
both sides of an equation, and check whether the results are the same. Another approach, pio-
neered by Watkins et al. (2008) in another setting, is to avoid definitional equality by maintaining
constructors in simplified form. The discussion in the introduction shows that substitution of a
simplified constructor into a simplified constructor is not necessarily simplified. The burden is
then shifted to defining a form of simplifying substitution whose result is always in simplified
form.
Exercises
18.1. Adapt the two implementations of queues given in Chapter 17 to match the signature of
queue constructors given in the introduction,
∃q :: Ty →Ty.⟨emp ,→∀t :: Ty . q[ t ] , ins ,→∀t :: Ty . t × q[ t ] →q[ t ] , rem ,→∀t :: Ty . q[ t ] →( t × q[ t ] ) opt⟩.
Consider the role played by definitional equality in ensuring that both implementations have
this type.
18.2. Give an equational dynamics for FEω. What role does definitional equality of constructors
play in it? Formulate a transition dynamics for FEω extended with a type of observable
results, say nat. What role does definitional equality play in the transition dynamics?

PREVIEW
164
18.4 Notes

PREVIEW
Part VIII
Partiality and Recursive Types

PREVIEW

PREVIEW
Chapter 19
System PCF of Recursive Functions
We introduced the language T as a basis for discussing total computations, those for which the
type system guarantees termination. The language M generalizes T to admit inductive and coin-
ductive types, while preserving totality. In this chapter we introduce PCF as a basis for discussing
partial computations, those that may not terminate when evaluated, even when they are well-
typed. At first blush this may seem like a disadvantage, but as we shall see in Chapter 20 it admits
greater expressive power than is possible in T.
The source of partiality in PCF is the concept of general recursion, which permits the solution
of equations between expressions. The price for admitting solutions to all such equations is that
computations may not terminate—the solution to some equations might be undefined (divergent).
In PCF the programmer must make sure that a computation terminates; the type system does not
guarantee it. The advantage is that the termination proof need not be embedded into the code
itself, resulting in shorter programs.
For example, consider the equations
f (0) ≜1
f (n + 1) ≜(n + 1) × f (n).
Intuitively, these equations define the factorial function. They form a system of simultaneous
equations in the unknown f which ranges over functions on the natural numbers. The function
we seek is a solution to these equations—a specific function f : N →N such that the above
conditions are satisfied.
A solution to such a system of equations is a fixed point of an associated functional (higher-
order function). To see this, let us re-write these equations in another form:
f (n) ≜
(
1
if n = 0
n × f (n′)
if n = n′ + 1.

PREVIEW
168
Re-writing yet again, we seek f given by
n 7→
(
1
if n = 0
n × f (n′)
if n = n′ + 1.
Now define the functional F by the equation F( f ) = f ′, where f ′ is given by
n 7→
(
1
if n = 0
n × f (n′)
if n = n′ + 1.
Note well that the condition on f ′ is expressed in terms of f, the argument to the functional F, and
not in terms of f ′ itself! The function f we seek is a fixed point of F, a function f : N →N such
that f = F( f ). In other words f is defined to be fix(F), where fix is a higher-order operator on
functionals F that computes a fixed point for it.
Why should an operator such as F have a fixed point? The key is that functions in PCF are
partial, which means that they may diverge on some (or even all) inputs. Consequently, a fixed
point of a functional F is the limit of a series of approximations of the desired solution obtained
by iterating F. Let us say that a partial function ϕ on the natural numbers, is an approximation to
a total function f if ϕ(m) = n implies that f (m) = n. Let ⊥: N ⇀N be the totally undefined
partial function—⊥(n) is undefined for every n ∈N. This is the “worst” approximation to the
desired solution f of the recursion equations given above. Given any approximation ϕ of f, we
may “improve” it to ϕ′ = F(ϕ). The partial function ϕ′ is defined on 0 and on m + 1 for every
m ≥0 on which ϕ is defined. Continuing, ϕ′′ = F(ϕ′) = F(F(ϕ)) is an improvement on ϕ′, and
hence a further improvement on ϕ. If we start with ⊥as the initial approximation to f, then pass
to the limit
lim
i≥0 F(i)(⊥),
we will obtain the least approximation to f that is defined for every m ∈N, and hence is the
function f itself. Turning this around, if the limit exists, it is the solution we seek.
Because this construction works for any functional F, we conclude that all such operators have
fixed points, and hence that all systems of equations such as the one given above have solutions.
The solution is given by general recursion, but there is no guarantee that it is a total function
(defined on all elements of its domain). For the above example it happens to be true, because we
can prove by induction that this is so, but in general the solution is a partial function that may
diverge on some inputs. It is our task as programmers to ensure that the functions defined by
general recursion are total, or at least that we have a grasp of those inputs for which it is well-
defined.

PREVIEW
19.1 Statics
169
19.1
Statics
The syntax of PCF is given by the following grammar:
Typ
τ
::=
nat
nat
naturals
parr( τ1 ; τ2 )
τ1 ⇀τ2
partial function
Exp
e
::=
x
x
variable
z
z
zero
s( e )
s( e )
successor
ifz{e0 ; x . e1}( e )
ifz e {z ,→e0 | s( x ) ,→e1}
zero test
λ{τ}( x . e )
λ ( x : τ ) e
abstraction
ap( e1 ; e2 )
e1( e2 )
application
fix{τ}( x . e )
fix x : τ is e
recursion
The expression fix{τ}( x . e ) is general recursion; it is discussed in more detail below. The ex-
pression ifz{e0 ; x . e1}( e ) branches according to whether e evaluates to z or not, binding the
predecessor to x in the case that it is not.
The statics of PCF is inductively defined by the following rules:
Γ, x : τ ⊢x : τ
(19.1a)
Γ ⊢z : nat
(19.1b)
Γ ⊢e : nat
Γ ⊢s( e ) : nat
(19.1c)
Γ ⊢e : nat
Γ ⊢e0 : τ
Γ, x : nat ⊢e1 : τ
Γ ⊢ifz{e0 ; x . e1}( e ) : τ
(19.1d)
Γ, x : τ1 ⊢e : τ2
Γ ⊢λ{τ1}( x . e ) : parr( τ1 ; τ2 )
(19.1e)
Γ ⊢e1 : parr( τ2 ; τ )
Γ ⊢e2 : τ2
Γ ⊢ap( e1 ; e2 ) : τ
(19.1f)
Γ, x : τ ⊢e : τ
Γ ⊢fix{τ}( x . e ) : τ
(19.1g)
Rule (19.1g) reflects the self-referential nature of general recursion. To show that fix{τ}( x . e ) has
type τ, we assume that it is the case by assigning that type to the variable x, which stands for the
recursive expression itself, and checking that the body, e, has type τ under this very assumption.
The structural rules, including in particular substitution, are admissible for the statics.
Lemma 19.1. If Γ, x : τ ⊢e′ : τ′, Γ ⊢e : τ, then Γ ⊢[e/x]e′ : τ′.

PREVIEW
170
19.2 Dynamics
19.2
Dynamics
The dynamics of PCF is defined by the judgments e val, specifying the closed values, and e 7−→e′,
specifying the steps of evaluation.
The judgment e val is defined by the following rules:
z val
(19.2a)
[e val]
s( e ) val
(19.2b)
λ{τ}( x . e ) val
(19.2c)
The bracketed premise on rule (19.2b) is included for the eager interpretation of the successor oper-
ation, and omitted for the lazy interpretation. (See Chapter 36 for a further discussion of laziness.)
The transition judgment e 7−→e′ is defined by the following rules:


e 7−→e′
s( e ) 7−→s( e′ )


(19.3a)
e 7−→e′
ifz{e0 ; x . e1}( e ) 7−→ifz{e0 ; x . e1}( e′ )
(19.3b)
ifz{e0 ; x . e1}( z ) 7−→e0
(19.3c)
s( e ) val
ifz{e0 ; x . e1}( s( e ) ) 7−→[e/x]e1
(19.3d)
e1 7−→e′
1
ap( e1 ; e2 ) 7−→ap( e′
1 ; e2 )
(19.3e)


e1 val
e2 7−→e′
2
ap( e1 ; e2 ) 7−→ap( e1 ; e′
2 )


(19.3f)
[e2 val]
ap( λ{τ}( x . e ) ; e2 ) 7−→[e2/x]e
(19.3g)
fix{τ}( x . e ) 7−→[fix{τ}( x . e )/x]e
(19.3h)

PREVIEW
19.3 Definability
171
The bracketed rule (19.3a) is included for an eager interpretation of the successor, and omitted
otherwise. Bracketed rule (19.3f) and the bracketed premise on rule (19.3g) are included for a
call-by-value interpretation, and omitted for a call-by-name interpretation, of function applica-
tion. Rule (19.3h) implements self-reference by substituting the recursive expression itself for the
variable x in its body; this is called unwinding the recursion.
Theorem 19.2 (Safety).
1. If e : τ and e 7−→e′, then e′ : τ.
2. If e : τ, then either e val or there exists e′ such that e 7−→e′.
Proof. The proof of preservation is by induction on the derivation of the transition judgment.
Consider rule (19.3h). Suppose that fix{τ}( x . e ) : τ. By inversion and substitution we have
[fix{τ}( x . e )/x]e : τ, as required. The proof of progress proceeds by induction on the derivation
of the typing judgment. For example, for rule (19.1g) the result follows because we may make
progress by unwinding the recursion.
It is easy to check that if e val, then e is irreducible in that there is no e′ such that e 7−→e′. The
safety theorem implies the converse, that an irreducible expression is a value, provided that it is
closed and well-typed.
Definitional equality for the call-by-name variant of PCF, written Γ ⊢e1 ≡e2 : τ, is the
strongest congruence containing the following axioms:
Γ ⊢ifz{e0 ; x . e1}( z ) ≡e0 : τ
(19.4a)
Γ ⊢ifz{e0 ; x . e1}( s( e ) ) ≡[e/x]e1 : τ
(19.4b)
Γ ⊢fix{τ}( x . e ) ≡[fix{τ}( x . e )/x]e : τ
(19.4c)
Γ ⊢ap( λ{τ1}( x . e2 ) ; e1 ) ≡[e1/x]e2 : τ
(19.4d)
These rules suffice to calculate the value of any closed expression of type nat: if e : nat, then
e ≡n : nat iff e 7−→∗n.
19.3
Definability
Let us write fun x( y:τ1 ):τ2 is e for a recursive function within whose body, e : τ2, are bound two
variables, y : τ1 standing for the argument and x : τ1 ⇀τ2 standing for the function itself. The

PREVIEW
172
19.3 Definability
dynamics of this construct is given by the axiom
( fun x( y:τ1 ):τ2 is e )( e1 ) 7−→[fun x( y:τ1 ):τ2 is e, e1/x, y]e
.
That is, to apply a recursive function, we substitute the recursive function itself for x and the
argument for y in its body.
Recursive functions are defined in PCF using fixed points, writing
fix x : τ1 ⇀τ2 is λ ( y : τ1 ) e
for fun x( y:τ1 ):τ2 is e. We may easily check that the static and dynamics of recursive functions
are derivable from this definition.
The primitive recursion construct of T is defined in PCF using recursive functions by taking
the expression
rec e {z ,→e0 | s( x ) with y ,→e1}
to stand for the application e′( e ), where e′ is the general recursive function
fun f ( u:nat ):τ is ifz u {z ,→e0 | s( x ) ,→[ f ( x )/y]e1}.
The static and dynamics of primitive recursion are derivable in PCF using this expansion.
In general, functions definable in PCF are partial in that they may be undefined for some
arguments. A partial (mathematical) function, ϕ : N ⇀N, is definable in PCF iff there is an
expression eϕ : nat ⇀nat such that ϕ(m) = n iff eϕ( m ) ≡n : nat. So, for example, if ϕ is the
totally undefined function, then eϕ is any function that loops without returning when it is applied.
It is informative to classify those partial functions ϕ that are definable in PCF. The partial re-
cursive functions are defined to be the primitive recursive functions extended with the minimization
operation: given ϕ(m, n), define ψ(n) to be the least m ≥0 such that (1) for m′ < m, ϕ(m′, n) is
defined and non-zero, and (2) ϕ(m, n) = 0. If no such m exists, then ψ(n) is undefined.
Theorem 19.3. A partial function ϕ on the natural numbers is definable in PCF iff it is partial recursive.
Proof sketch. Minimization is definable in PCF, so it is at least as powerful as the set of partial
recursive functions. Conversely, we may, with some tedium, define an evaluator for expressions of
PCF as a partial recursive function, using G¨odel-numbering to represent expressions as numbers.
Therefore PCF does not exceed the power of the set of partial recursive functions.
Church’s Law states that the partial recursive functions coincide with the set of effectively
computable functions on the natural numbers—those that can be carried out by a program written
in any programming language that is or will ever be defined.1 Therefore PCF is as powerful as
any other programming language with respect to the set of definable functions on the natural
numbers.
1See Chapter 21 for further discussion of Church’s Law.

PREVIEW
19.4 Finite and Infinite Data Structures
173
The evaluator, or universal, function ϕuniv for PCF is the partial function on the natural numbers
defined by
ϕuniv(⌜e⌝)(m) = n iff e( m ) ≡n : nat.
In contrast to T, the universal function ϕuniv for PCF is partial (might be undefined for some
inputs). It is, in essence, an interpreter that, given the code ⌜e⌝of a closed expression of type
nat ⇀nat, simulates the dynamics to calculate the result, if any, of applying it to the m, obtaining
n. Because this process may fail to terminate, the universal function is not defined for all inputs.
By Church’s Law the universal function is definable in PCF. In contrast, we proved in Chap-
ter 9 that the analogous function is not definable in T using the technique of diagonalization. It is
instructive to examine why that argument does not apply in the present setting. As in Section 9.4,
we may derive the equivalence
e∆( ⌜e∆⌝) ≡s( e∆( ⌜e∆⌝) )
for PCF. But now, instead of concluding that the universal function, euniv, does not exist as we did
for T, we instead conclude for PCF that euniv diverges on the code for e∆applied to its own code.
19.4
Finite and Infinite Data Structures
Finite data types (products and sums), including their use in pattern matching and generic pro-
gramming, carry over verbatim to PCF. However, the distinction between the eager and lazy
dynamics for these constructs becomes more important. Rather than being a matter of preference,
the decision to use an eager or lazy dynamics affects the meaning of a program: the “same” types
mean different things in a lazy dynamics than in an eager dynamics. For example, the elements of
a product type in an eager language are pairs of values of the component types. In a lazy language
they are instead pairs of unevaluated, possibly divergent, computations of the component types,
a very different thing indeed. And similarly for sums.
The situation grows more acute for infinite types such as the type nat of “natural numbers.”
The scare quotes are warranted, because the “same” type has a very different meaning under
an eager dynamics than under a lazy dynamics. In the former case the type nat is, indeed, the
authentic type of natural numbers—the least type containing zero and closed under successor.
The principle of mathematical induction is valid for reasoning about the type nat in an eager
dynamics. It corresponds to the inductive type µ( t . unit + t ) in the sense of Chapter 15.
On the other hand, under a lazy dynamics the type nat is no longer the type of natural numbers
at all. For example, it includes the value
ω ≜fix x : nat is s( x ),
which has itself as predecessor! It is, intuitively, an “infinite stack of successors”, growing without
end. It is clearly not a natural number (it is larger than all of them), so the principle of mathemat-
ical induction does not apply. In a lazy setting nat could be renamed lnat to remind us of the
distinction. It corresponds to the coinductive type ν( t . unit + t ) in the sense of Chapter 15.

PREVIEW
174
19.5 Totality and Partiality
19.5
Totality and Partiality
The advantage of a total programming language such as T is that it ensures, by type checking,
that every program terminates, and that every function is total. There is no way to have a well-
typed program that goes into an infinite loop. This prohibition may seem appealing, until one
considers that the upper bound on the time to termination may be large, so large that it might
as well diverge for all practical purposes. But let us grant for the moment that it is a virtue of T
that it precludes divergence. Why, then, bother with a language such as PCF that does not rule
out divergence? After all, infinite loops are invariably bugs, so why not rule them out by type
checking? The notion seems appealing until one tries to write a program in a language such as T.
Consider the computation of the greatest common divisor (gcd) of two natural numbers. It can
be programmed in PCF by solving the following equations using general recursion:
gcd(m, 0) = m
gcd(0, n) = n
gcd(m, n) = gcd(m −n, n)
if m > n
gcd(m, n) = gcd(m, n −m)
if m < n
The type of gcd defined this way is ( nat × nat ) ⇀nat, which suggests that it may not terminate
for some inputs. But we may prove by induction on the sum of the pair of arguments that it is, in
fact, a total function.
Now consider programming this function in T. It is, in fact, programmable using only primi-
tive recursion, but the code to do it is rather painful (try it!). One way to see the problem is that
in T the only form of looping is one that reduces a natural number by one on each recursive call;
it is not (directly) possible to make a recursive call on a smaller number other than the immedi-
ate predecessor. In fact one may code up more general patterns of terminating recursion using
only primitive recursion as a primitive, but if you check the details, you will see that doing so
comes at a price in performance and program complexity. Program complexity can be mitigated
by building libraries that codify standard patterns of reasoning whose cost of development should
be amortized over all programs, not just one in particular. But there is still the problem of perfor-
mance. Indeed, the encoding of more general forms of recursion into primitive recursion means
that, deep within the encoding, there must be a “timer” that goes down by ones to ensure that the
program terminates. The result will be that programs written with such libraries will be slower
than necessary.
But, one may argue, T is simply not a serious language. A more serious total programming
language would admit sophisticated patterns of control without performance penalty. Indeed, one
could easily envision representing the natural numbers in binary, rather than unary, and allowing
recursive calls by halving to get logarithmic complexity. Such a formulation is possible, as would
be quite a number of analogous ideas that avoid the awkwardness of programming in T. Could
we not then have a practical language that rules out divergence?
We can, but at a cost. We have already seen one limitation of total programming languages:
they are not universal. You cannot write an interpreter for T within T, and this limitation extends
to any total language whatever. If this does not seem important, then consider the Blum Size
Theorem (BST), which places another limitation on total languages. Fix any total language L that

PREVIEW
19.6 Notes
175
permits writing functions on the natural numbers. Pick any blowup factor, say 22n. The BST states
that there is a total function on the natural numbers that is programmable in L, but whose shortest
program in L is larger by the given blowup factor than its shortest program in PCF!
The underlying idea of the proof is that in a total language the proof of termination of a program must
be baked into the code itself, whereas in a partial language the termination proof is an external verification
condition left to the programmer. There are, and always will be, programs whose termination proof
is rather complicated to express, if you fix in advance the means of proving it total. (In T it was
primitive recursion, but one can be more ambitious, yet still get caught by the BST.) But if you
leave room for ingenuity, then programs can be short, because they do not have to embed the
proof of their termination in their own running code.
19.6
Notes
The solution to recursion equations described here is based on Kleene’s fixed point theorem for
complete partial orders, specialized to the approximation ordering of partial functions. The lan-
guage PCF is derived from Plotkin (1977) as a laboratory for the study of semantics of program-
ming languages. Many authors have used PCF as the subject of study of many problems in se-
mantics. It has thereby become the E. coli of programming languages.
Exercises
19.1. Consider the problem considered in Section 10.3 of how to define the mutually recursive
“even” and “odd” functions. There we gave a solution in terms of primitive recursion. You
are, instead, to give a solution in terms of general recursion. Hint: consider that a pair of
mutually recursive functions is a recursive pair of functions.
19.2. Show that minimization, as explained before the statement of Theorem 19.3, is definable in
PCF.
19.3. Consider the partial function ϕhalts such that if e : nat ⇀nat, then ϕhalts(⌜e⌝) evaluates to
zero iff e( ⌜e⌝) converges, and evaluates to one otherwise. Prove that ϕhalts is not definable
in PCF.
19.4. Suppose that we changed the specification of minimization given prior to Theorem 19.3 so
that ψ(n) is the least m such that ϕ(m, n) = 0, and is undefined if no such m exists. Is this
“simplified” form of minimization definable in PCF?
19.5. Suppose that we wished to define, in the lazy variant of PCF, a version of the parallel or
function specified as a function of two arguments that returns z if either of its arguments
is z, and s( z ) if both are non-zero. That is, we wish to find an expression e satisfying the

PREVIEW
176
19.6 Notes
following properties:
e( e1 )( e2 ) 7−→∗z if e1 7−→∗z
e( e1 )( e2 ) 7−→∗z if e2 7−→∗z
e( e1 )( e2 ) 7−→∗s( z ) if e1 7−→∗s( ) and e2 7−→∗s( ).
Thus, e defines a total function of its two arguments, even if one of the arguments diverges, as
long as the other is z. Clearly such a function cannot be defined in the call-by-value variant
of PCF, but can it be defined in the call-by-name variant? If so, show how; if not, prove that
it cannot be, and suggest an extension of PCF that would allow it to be defined.
19.6. We appealed to Church’s Law to argue that the universal function for PCF is definable in
PCF. See what is behind this claim by considering two aspects of the problem: (1) G¨odel-
numbering, the representation of abstract syntax by a number; (2) evaluation, the process of
interpreting a function on its inputs. Part (1) is a technical issue arising from the limited data
structures available in PCF. Part (2) is the heart of the matter; explore its implementation in
terms of a solution to Part (1).

PREVIEW
Chapter 20
System FPC of Recursive Types
In this chapter we study FPC, a language with products, sums, partial functions, and recursive
types. Recursive types are solutions to type equations t ∼= τ where there is no restriction on where
t may occur in τ. Equivalently, a recursive type is a fixed point up to isomorphism of the associated
unrestricted type operator t . τ. By removing the restrictions on the type operator we may consider
the solution of a type equation such as t ∼= t ⇀t, which describes a type that is isomorphic to the
type of partial functions defined on itself. If types were sets, such an equation could not be solved,
because there are more partial functions on a set than there are elements of that set. But types are
not sets: they classify computable functions, not arbitrary functions. With types we may solve such
“dubious” type equations, even though we cannot expect to do so with sets. The penalty is that
we must admit non-termination. For one thing, type equations involving functions have solutions
only if the functions involved are partial.
A benefit of working in the setting of partial functions is that type equations have unique so-
lutions (up to isomorphism). Therefore it makes sense, as we shall do in this chapter, to speak of
the solution to a type equation. But what about the distinct solutions to a type equation given in
Chapter 15? These turn out to coincide for any fixed dynamics, but give rise to different solutions
according to whether the dynamics is eager or lazy (as illustrated in Section 19.4 for the special
case of the natural numbers). Under a lazy dynamics (where all constructs are evaluated lazily),
recursive types have a coinductive flavor, and the inductive analogs are inaccessible. Under an
eager dynamics (where all constructs are evaluated eagerly), recursive types have an inductive
flavor. But the coinductive analogs are accessible as well, using function types to selectively im-
pose laziness. It follows that the eager dynamics is more expressive than the lazy dynamics, because
it is impossible to go the other way around (one cannot define inductive types in a lazy language).

PREVIEW
178
20.1 Solving Type Equations
20.1
Solving Type Equations
The language FPC has products, sums, and partial functions, extended with the new concept of
recursive types. The syntax of recursive types is defined as follows:
Typ
τ
::=
t
t
self-reference
rec( t . τ )
rec t is τ
recursive type
Exp
e
::=
fold{t . τ}( e )
fold( e )
fold
unfold( e )
unfold( e )
unfold
Recursive types have the same general form as the inductive and coinductive types discussed
in Chapter 15, but without restriction on the type operator involved. Recursive type are formed
according to the rule:
∆, t type ⊢τ type
∆⊢rec( t . τ ) type
(20.1)
The statics of folding and unfolding is given by the following rules:
Γ ⊢e : [rec( t . τ )/t]τ
Γ ⊢fold{t . τ}( e ) : rec( t . τ )
(20.2a)
Γ ⊢e : rec( t . τ )
Γ ⊢unfold( e ) : [rec( t . τ )/t]τ
(20.2b)
The dynamics of folding and unfolding is given by these rules:
[e val]
fold{t . τ}( e ) val
(20.3a)


e 7−→e′
fold{t . τ}( e ) 7−→fold{t . τ}( e′ )


(20.3b)
e 7−→e′
unfold( e ) 7−→unfold( e′ )
(20.3c)
fold{t . τ}( e ) val
unfold( fold{t . τ}( e ) ) 7−→e
(20.3d)
The bracketed premise and rule are included for an eager interpretation of the introduction form,
and omitted for a lazy interpretation. As mentioned in the introduction, the choice of eager or lazy
dynamics affects the meaning of recursive types.
Theorem 20.1 (Safety).
1. If e : τ and e 7−→e′, then e′ : τ.
2. If e : τ, then either e val, or there exists e′ such that e 7−→e′.

PREVIEW
20.2 Inductive and Coinductive Types
179
20.2
Inductive and Coinductive Types
Recursive types may be used to represent inductive types such as the natural numbers. Using an
eager dynamics for FPC, the recursive type
ρ = rec t is [z ,→unit , s ,→t]
satisfies the type equation
ρ ∼= [z ,→unit , s ,→ρ],
and is isomorphic to the type of eager natural numbers. The introduction and elimination forms
are defined on ρ by the following equations:1
z ≜fold( z · ⟨⟩)
s( e ) ≜fold( s · e )
ifz e {z ,→e0 | s( x ) ,→e1} ≜case unfold( e ) {z · ,→e0 | s · x ,→e1}.
It is a good exercise to check that the eager dynamics of natural numbers in PCF is correctly
simulated by these definitions.
On the other hand, under a lazy dynamics for FPC, the same recursive type ρ′,
rec t is [z ,→unit , s ,→t],
satisfies the same type equation,
ρ′ ∼= [z ,→unit , s ,→ρ′],
but is not the type of natural numbers! Rather, it is the type lnat of lazy natural numbers intro-
duced in Section 19.4. As discussed there, the type ρ′ contains the “infinite number” ω, which is
of course not a natural number.
Similarly, using an eager dynamics for FPC, the type natlist of lists of natural numbers is
defined by the recursive type
rec t is [n ,→unit , c ,→nat × t],
which satisfies the type equation
natlist ∼= [n , unit ,→c ,→nat × natlist].
The list introduction operations are given by the following equations:
nil ≜fold( n · ⟨⟩)
cons( e1 ; e2 ) ≜fold( c · ⟨e1, e2⟩).
1The “underscore” stands for a variable that does not occur free in e0.

PREVIEW
180
20.3 Self-Reference
A conditional list elimination form is given by the following equation:
case e {nil ,→e0 | cons( x ; y ) ,→e1} ≜case unfold( e ) {n · ,→e0 | c · ⟨x, y⟩,→e1},
where we have used pattern-matching syntax to bind the components of a pair for the sake of
clarity.
Now consider the same recursive type, but in the context of a lazy dynamics for FPC. What
type is it? If all constructs are lazy, then a value of the recursive type
rec t is [n ,→unit , c ,→nat × t],
has the form fold( e ), where e is an unevaluated computation of the sum type, whose values are
injections of unevaluated computations of either the unit type or of the product type nat × t. And
the latter consists of pairs of an unevaluated computation of a (lazy!) natural number, and an
unevaluated computation of another value of this type. In particular, this type contains infinite
lists whose tails go on without end, as well as finite lists that eventually reach an end. The type is,
in fact, a version of the type of infinite streams defined in Chapter 15, rather than a type of finite
lists as is the case under an eager dynamics.
It is common in textbooks to depict data structures using “box-and-pointer” diagrams. These
work well in the eager setting, provided that no functions are involved. For example, an eager list
of eager natural numbers may be depicted using this notation. We may think of fold as an abstract
pointer to a tagged cell consisting of either (a) the tag n with no associated data, or (b) the tag c
attached to a pair consisting of an authentic natural number and another list, which is an abstract
pointer of the same type. But this notation does not scale well to types involving functions, or to
languages with a lazy dynamics. For example, the recursive type of “lists” in lazy FPC cannot be
depicted using boxes and pointers, because of the unevaluated computations occurring in values
of this type. It is a mistake to limit one’s conception of data structures to those that can be drawn
on the blackboard using boxes and pointers or similar informal notations. There is no substitute
for a programming language to express data structures fully and accurately.
It is deceiving that the “same” recursive type can have two different meanings according to
whether the underlying dynamics is eager or lazy. For example, it is common for lazy languages
to use the name “list” for the recursive type of streams, or the name “nat” for the type of lazy
natural numbers. This terminology is misleading, considering that such languages do not (and
can not) have a proper type of finite lists or a type of natural numbers. Caveat emptor!
20.3
Self-Reference
In the general recursive expression fix{τ}( x . e ) the variable x stands for the expression itself.
Self-reference is effected by the unrolling transition
fix{τ}( x . e ) 7−→[fix{τ}( x . e )/x]e,
which substitutes the expression itself for x in its body during execution. It is useful to think of x
as an implicit argument to e that is instantiated to itself when the expression is used. In many well-
known languages this implicit argument has a special name, such as this or self, to emphasize
its self-referential interpretation.

PREVIEW
20.3 Self-Reference
181
Using this intuition as a guide, we may derive general recursion from recursive types. This
derivation shows that general recursion may, like other language features, be seen as a manifes-
tation of type structure, instead of as an ad hoc language feature. The derivation isolates a type of
self-referential expressions given by the following grammar:
Typ
τ
::=
self( τ )
τ self
self-referential type
Exp
e
::=
self{τ}( x . e )
self x is e
self-referential expression
unroll( e )
unroll( e )
unroll self-reference
The statics of these constructs is given by the following rules:
Γ, x : self( τ ) ⊢e : τ
Γ ⊢self{τ}( x . e ) : self( τ )
(20.4a)
Γ ⊢e : self( τ )
Γ ⊢unroll( e ) : τ
(20.4b)
The dynamics is given by the following rule for unrolling the self-reference:
self{τ}( x . e ) val
(20.5a)
e 7−→e′
unroll( e ) 7−→unroll( e′ )
(20.5b)
unroll( self{τ}( x . e ) ) 7−→[self{τ}( x . e )/x]e
(20.5c)
The main difference, compared to general recursion, is that we distinguish a type of self-referential
expressions, instead of having self-reference at every type. However, as we shall see, the self-
referential type suffices to implement general recursion, so the difference is a matter of taste.
The type self( τ ) is definable from recursive types. As suggested earlier, the key is to consider
a self-referential expression of type τ to depend on the expression itself. That is, we seek to define
the type self( τ ) so that it satisfies the isomorphism
self( τ ) ∼= self( τ ) ⇀τ.
We seek a fixed point of the type operator t . t ⇀τ, where t /∈τ is a type variable standing for the
type in question. The required fixed point is just the recursive type
rec( t . t ⇀τ ),
which we take as the definition of self( τ ).
The self-referential expression self{τ}( x . e ) is the expression
fold( λ ( x : self( τ ) ) e ).

PREVIEW
182
20.4 The Origin of State
We may check that rule (20.4a) is derivable according to this definition. The expression unroll( e )
is correspondingly the expression
unfold( e )( e ).
It is easy to check that rule (20.4b) is derivable from this definition. Moreover, we may check that
unroll( self{τ}( y . e ) ) 7−→∗[self{τ}( y . e )/y]e.
This completes the derivation of the type self( τ ) of self-referential expressions of type τ.
The self-referential type self( τ ) can be used to define general recursion for any type. We may
define fix{τ}( x . e ) to stand for the expression
unroll( self{τ}( y . [unroll( y )/x]e ) )
where the recursion at each occurrence of x is unrolled within e. It is easy to check that this verifies
the statics of general recursion given in Chapter 19. Moreover, it also validates the dynamics, as
shown by the following derivation:
fix{τ}( x . e ) = unroll( self{τ}( y . [unroll( y )/x]e ) )
7−→∗[unroll( self{τ}( y . [unroll( y )/x]e ) )/x]e
= [fix{τ}( x . e )/x]e.
It follows that recursive types can be used to define a non-terminating expression of every type,
fix{τ}( x . x ).
20.4
The Origin of State
The concept of state in a computation—which will be discussed in Part XIV—has its origins in
the concept of recursion, or self-reference, which, as we have just seen, arises from the concept of
recursive types. For example, the concept of a flip-flop or a latch is a circuit built from combinational
logic elements (typically, nor or nand gates) that have the characteristic that they maintain an
alterable state over time. An RS latch, for example, maintains its output at the logical level of zero
or one in response to a signal on the R or S inputs, respectively, after a brief settling delay. This
behavior is achieved using feedback, which is just a form of self-reference, or recursion: the output
of the gate feeds back into its input so as to convey the current state of the gate to the logic that
determines its next state.
We can implement an RS latch using recursive types. The idea is to use self-reference to model
the passage of time, with the current output being computed from its input and its previous out-
puts. Specifically, an RS latch is a value of type τrsl given by
rec t is ⟨X ,→bool , Q ,→bool, N ,→t⟩.
The X and Q components of the latch represent its current outputs (of which Q represents the current
state of the latch), and the N component represents the next state of the latch.
If e is of type τrsl,

PREVIEW
20.5 Notes
183
then we define e @ X to mean unfold( e ) · X, and define e @ Q and e @ N similarly. The expressions
e @ X and e @ Q evaluate to the boolean outputs of the latch e, and e @ N evaluates to another latch
representing its evolution over time based on these inputs.
For given values r and s, a new latch is computed from an old latch by the recursive function
rsl defined as follows:2
fix rsl is λ ( l : τrsl ) ersl,
where ersl is the expression
fix this is fold( ⟨X ,→enor( ⟨s, l @ Q⟩), Q ,→enor( ⟨r, l @ X⟩), N ,→rsl( this )⟩),
where enor is the obvious binary function on booleans. The outputs of the latch are computed in
terms of the r and s inputs and the outputs of the previous state of the latch. To get the construction
started, we define an initial state of the latch in which the outputs are arbitrarily set to false, and
whose next state is determined by applying the recursive function rsl to that state:
fix this is fold( ⟨X ,→false, Q ,→false, N ,→rsl( this )⟩).
Selection of the N component causes the outputs to be recalculated based on their current values.
Notice the role of self-reference in maintaining the state of the latch.
20.5
Notes
The systematic study of recursive types in programming was initiated by Scott (1976, 1982) to give
a mathematical model of the untyped λ-calculus. The derivation of recursion from recursive types
is an application of Scott’s theory. The category-theoretic view of recursive types was developed
by Wand (1979) and Smyth and Plotkin (1982). Implementing state using self-reference is funda-
mental to digital logic (Ward and Halstead, 1990). The example given in Section 20.4 is inspired
by Cook (2009) and Abadi and Cardelli (1996). The account of signals as streams (explored in the
exercises) is inspired by the pioneering work of Kahn (MacQueen, 2009). The language name FPC
is taken from Gunter (1992).
Exercises
20.1. Show that the recursive type D ≜rec t is t ⇀t is non-trivial by interpreting the sk-combinators
defined in Exercise 3.1 into it. Specifically, define elements k : D and s : D and a (left-
associative) “application” function
x : D y : D ⊢x · y : D
such that
2For convenience we assume that fold is evaluated lazily.

PREVIEW
184
20.5 Notes
(a) k · x · y 7−→∗x;
(b) s · x · y · z 7−→∗( x · z ) · ( y · z ).
20.2. Recursive types admit the structure of both inductive and coinductive types. Consider the
recursive type τ ≜rec t is τ′ and the associated inductive and coinductive types µ( t . τ′ )
and ν( t . τ′ ). Complete the following chart consistently with the statics of inductive and
coinductive types on the left-hand side and with the statics of recursive types on the right:
fold{t . t opt}( e ) ≜fold( e )
rec{t . t opt}( x . e′ ; e ) ≜?
unfold{t . t opt}( e ) ≜unfold( e )
gen{t . t opt}( x . e′ ; e ) ≜?
Check that the statics is derivable under these definitions. Hint: you will need to use general
recursion on the right to fill in the missing cases. You may also find it useful to use generic
programming.
Now consider the dynamics of these definitions, under both an eager and a lazy interpreta-
tion. What happens in each case?
20.3. Define the type signal of signals to be the coinductive type of infinite streams of booleans
(bits). Define a signal transducer to be a function of type signal ⇀signal. Combinational
logic gates, such as the NOR gate, can be defined as signal transducers. Give a coinductive
definition of the type signal, and define NOR as a signal transducer. Be sure to take account
of the underlying dynamics of PCF.
The passage from combinational to digital logic (circuit elements that maintain state) relies
on self-reference. For example, an RS latch can be built from NOR two nor gates in this way.
Define an RS latch using general recursion and two of the NOR gates just defined.
20.4. The type τrsl given in Section 20.4 above is the type of streams of pairs of booleans. Give
another formulation of an RS latch as a value of type τrsl, but this time using the coinductive
interpretation of the recursive type proposed in Exercise 20.2 (using the lazy dynamics for
FPC). Expand and simplify this definition using your solution to Exercise 20.2, and compare
it with the formulation given in Section 20.4. Hint: the internal state of the stream is a pair of
booleans corresponding to the X and Q outputs of the latch.

PREVIEW
Part IX
Dynamic Types

PREVIEW

PREVIEW
Chapter 21
The Untyped λ-Calculus
In this chapter we study the premier example of a uni-typed programming language, the (untyped)
λ-calculus. This formalism was introduced by Church in the 1930’s as a universal language of com-
putable functions. It is distinctive for its austere elegance. The λ-calculus has but one “feature”,
the higher-order function. Everything is a function, hence every expression may be applied to an
argument, which must itself be a function, with the result also being a function. To borrow a turn
of phrase, in the λ-calculus it’s functions all the way down.
21.1
The λ-Calculus
The abstract syntax of the untyped λ-calculus, called Λ, is given by the following grammar:
Exp
u
::=
x
x
variable
λ( x . u )
λ ( x ) u
λ-abstraction
ap( u1 ; u2 )
u1( u2 )
application
The statics of Λ is defined by general hypothetical judgments of the form x1 ok, . . . , xn ok ⊢
u ok, stating that u is a well-formed expression involving the variables x1, . . . , xn. (As usual, we
omit explicit mention of the variables when they can be determined from the form of the hypothe-
ses.) This relation is inductively defined by the following rules:
Γ, x ok ⊢x ok
(21.1a)
Γ ⊢u1 ok
Γ ⊢u2 ok
Γ ⊢u1( u2 ) ok
(21.1b)
Γ, x ok ⊢u ok
Γ ⊢λ ( x ) u ok
(21.1c)

PREVIEW
188
21.2 Definability
The dynamics of Λ is given equationally, rather than via a transition system. Definitional equal-
ity for Λ is a judgment of the form Γ ⊢u ≡u′, where Γ ⊢u ok and Γ ⊢u′ ok. It is inductively
defined by the following rules:
Γ, u ok ⊢u ≡u
(21.2a)
Γ ⊢u ≡u′
Γ ⊢u′ ≡u
(21.2b)
Γ ⊢u ≡u′
Γ ⊢u′ ≡u′′
Γ ⊢u ≡u′′
(21.2c)
Γ ⊢u1 ≡u′
1
Γ ⊢u2 ≡u′
2
Γ ⊢u1( u2 ) ≡u′
1( u′
2 )
(21.2d)
Γ, x ok ⊢u ≡u′
Γ ⊢λ ( x ) u ≡λ ( x ) u′
(21.2e)
Γ, x ok ⊢u2 ok
Γ ⊢u1 ok
Γ ⊢( λ ( x ) u2 )( u1 ) ≡[u1/x]u2
(21.2f)
We often write just u ≡u′ when the variables involved need not be emphasized or are clear from
context.
21.2
Definability
Interest in the untyped λ-calculus stems from its surprising expressiveness. It is a Turing-complete
language in the sense that it has the same capability to express computations on the natural num-
bers as does any other known programming language. Church’s Law states that any conceivable
notion of computable function on the natural numbers is equivalent to the λ-calculus. This asser-
tion is true for all known means of defining computable functions on the natural numbers. The
force of Church’s Law is that it postulates that all future notions of computation will be equiv-
alent in expressive power (measured by definability of functions on the natural numbers) to the
λ-calculus. Church’s Law is therefore a scientific law in the same sense as, say, Newton’s Law of
Universal Gravitation, which predicts the outcome of all future measurements of the acceleration
in a gravitational field.1
We will sketch a proof that the untyped λ-calculus is as powerful as the language PCF de-
scribed in Chapter 19. The main idea is to show that the PCF primitives for manipulating the
natural numbers are definable in the untyped λ-calculus. In particular, we must show that the
natural numbers are definable as λ-terms in such a way that case analysis, which discriminates
between zero and non-zero numbers, is definable. The principal difficulty is with computing the
predecessor of a number, which requires a bit of cleverness. Finally, we show how to represent
general recursion, completing the proof.
1It is debatable whether there are any scientific laws in Computer Science. In the opinion of the author, Church’s Law,
which is usually called Church’s Thesis, is a strong candidate for being a scientific law.

PREVIEW
21.2 Definability
189
The first task is to represent the natural numbers as certain λ-terms, called the Church numerals.
0 ≜λ ( b ) λ ( s ) b
(21.3a)
n + 1 ≜λ ( b ) λ ( s ) s( n( b )( s ) )
(21.3b)
It follows that
n( u1 )( u2 ) ≡u2( . . . ( u2( u1 ) ) ),
the n-fold application of u2 to u1. That is, n iterates its second argument (the induction step) n
times, starting with its first argument (the basis).
Using this definition it is not difficult to define the basic functions of arithmetic. For example,
successor, addition, and multiplication are defined by the following untyped λ-terms:
succ ≜λ ( x ) λ ( b ) λ ( s ) s( x( b )( s ) )
(21.4)
plus ≜λ ( x ) λ ( y ) y( x )( succ )
(21.5)
times ≜λ ( x ) λ ( y ) y( 0 )( plus( x ) )
(21.6)
It is easy to check that succ( n ) ≡n + 1, and that similar correctness conditions hold for the
representations of addition and multiplication.
To define ifz{u0 ; x . u1}( u ) requires a bit of ingenuity. The key is to define the “cut-off prede-
cessor”, pred, such that
pred( 0 ) ≡0
(21.7)
pred( n + 1 ) ≡n.
(21.8)
To compute the predecessor using Church numerals, we must show how to compute the result
for n + 1 in terms of its value for n. At first glance this seems simple—just take the successor—
until we consider the base case, in which we define the predecessor of 0 to be 0. This formulation
invalidates the obvious strategy of taking successors at inductive steps, and necessitates some
other approach.
What to do? A useful intuition is to think of the computation in terms of a pair of “shift
registers” satisfying the invariant that on the nth iteration the registers contain the predecessor of
n and n itself, respectively. Given the result for n, namely the pair (n −1, n), we pass to the result
for n + 1 by shifting left and incrementing to obtain (n, n + 1). For the base case, we initialize the
registers with (0, 0), reflecting the stipulation that the predecessor of zero be zero. To compute the
predecessor of n we compute the pair (n −1, n) by this method, and return the first component.
To make this precise, we must first define a Church-style representation of ordered pairs.
⟨u1, u2⟩≜λ ( f ) f ( u1 )( u2 )
(21.9)
u · l ≜u( λ ( x ) λ ( y ) x )
(21.10)
u · r ≜u( λ ( x ) λ ( y ) y )
(21.11)

PREVIEW
190
21.3 Scott’s Theorem
It is easy to check that under this encoding ⟨u1, u2⟩· l ≡u1, and that a similar equivalence holds
for the second projection. We may now define the required representation, up, of the predecessor
function:
u′
p ≜λ ( x ) x( ⟨0, 0⟩)( λ ( y ) ⟨y · r, succ ( y · r )⟩)
(21.12)
up ≜λ ( x ) u′
p( x ) · l
(21.13)
It is easy to check that this gives us the required behavior. Finally, define ifz{u0 ; x . u1}( u ) to be
the untyped term
u( u0 )( λ ( ) [up( u )/x]u1 ).
This definition gives us all the apparatus of PCF, apart from general recursion. But general
recursion is also definable in Λ using a fixed point combinator. There are many choices of fixed
point combinator, of which the best known is the Y combinator:
Y ≜λ ( F ) ( λ ( f ) F( f ( f ) ) )( λ ( f ) F( f ( f ) ) ).
It is easy to check that
Y( F ) ≡F( Y( F ) ).
Using the Y combinator, we may define general recursion by writing Y( λ ( x ) u ), where x stands
for the recursive expression itself.
Although it is clear that Y as just defined computes a fixed point of its argument, it is probably
less clear why it works or how we might have invented it in the first place. The main idea is quite
simple. If a function is recursive, it is given an extra first argument, which is arranged at call sites
to be the function itself. Whenever we wish to call a self-referential function with an argument,
we apply the function first to itself and then to its argument; this protocol is imposed on both the
“external” calls to the function and on the “internal” calls that the function may make to itself.
For this reason the first argument is often called this or self, to remind you that it will be, by
convention, bound to the function itself.
With this in mind, it is easy to see how to derive the definition of Y. If F is the function whose
fixed point we seek, then the function F′ = λ ( f ) F( f ( f ) ) is a variant of F in which the self-
application convention has been imposed internally by substituting for each occurrence of f in
F( f ) the self-application f ( f ). Now check that F′( F′ ) ≡F( F′( F′ ) ), so that F′( F′ ) is the desired
fixed point of F. Expanding the definition of F′, we have derived that the desired fixed point of F
is
λ ( f ) F( f ( f ) )( λ ( f ) F( f ( f ) ) ).
To finish the derivation, we need only note that nothing depends on the particular choice of F,
which means that we can compute a fixed point for F uniformly in F. That is, we may define a
single function, the term Y as defined above, that computes the fixed point of any F.
21.3
Scott’s Theorem
Scott’s Theorem states that definitional equality for the untyped λ-calculus is undecidable: there is
no algorithm to determine whether or not two untyped terms are definitionally equal. The proof

PREVIEW
21.3 Scott’s Theorem
191
uses the concept of inseparability. Any two properties, A0 and A1, of λ-terms are inseparable if there
is no decidable property, B, such that A0 u implies that B u holds, and A1 u implies that B u does
not hold. We say that a property, A, of untyped terms is behavioral iff whenever u ≡u′, then A u
iff A u′.
The proof of Scott’s Theorem decomposes into two parts:
1. For any untyped λ-term u, we may find an untyped term v such that u( ⌜v⌝) ≡v, where ⌜v⌝
is the G¨odel number of v, and ⌜v⌝is its representation as a Church numeral. (See Chapter 9
for a discussion of G¨odel-numbering.)
2. Any two non-trivial2 behavioral properties A0 and A1 of untyped terms are inseparable.
Lemma 21.1. For any u there exists v such that u( ⌜v⌝) ≡v.
Proof Sketch. The proof relies on the definability of the following two operations in the untyped
λ-calculus:
1. ap( ⌜u1⌝)( ⌜u2⌝) ≡⌜u1( u2 )⌝.
2. nm( n ) ≡⌜n⌝.
Intuitively, the first takes the representations of two untyped terms, and builds the representation
of the application of one to the other. The second takes a numeral for n, and yields the repre-
sentation of the Church numeral n. Given these, we may find the required term v by defining
v ≜w( ⌜w⌝), where w ≜λ ( x ) u( ap( x )( nm( x ) ) ). We have
v = w( ⌜w⌝)
≡u( ap( ⌜w⌝)( nm( ⌜w⌝) ) )
≡u( ⌜w( ⌜w⌝)⌝)
≡u( ⌜v⌝).
The definition is very similar to that of Y( u ), except that u takes as input the representation of
a term, and we find a v such that, when applied to the representation of v, the term u yields v
itself.
Lemma 21.2. Suppose that A0 and A1 are two non-trivial behavioral properties of untyped terms. Then
there is no untyped term w such that
1. For every u either w( ⌜u⌝) ≡0 or w( ⌜u⌝) ≡1.
2. If A0 u, then w( ⌜u⌝) ≡0.
3. If A1 u, then w( ⌜u⌝) ≡1.
2A property of untyped terms is trivial if it either holds for all untyped terms or never holds for any untyped term.

PREVIEW
192
21.4 Untyped Means Uni-Typed
Proof. Suppose there is such an untyped term w. Let v be the untyped term
λ ( x ) ifz{u1 ; . u0}( w( x ) ),
where u0 and u1 are chosen such that A0 u0 and A1 u1. (Such a choice must exist by non-triviality
of the properties.) By Lemma 21.1 there is an untyped term t such that v( ⌜t⌝) ≡t. If w( ⌜t⌝) ≡0,
then t ≡v( ⌜t⌝) ≡u1, and so A1 t, because A1 is behavioral and A1 u1. But then w( ⌜t⌝) ≡1
by the defining properties of w, which is a contradiction. Similarly, if w( ⌜t⌝) ≡1, then A0 t, and
hence w( ⌜t⌝) ≡0, again a contradiction.
Corollary 21.3. There is no algorithm to decide whether u ≡u′.
Proof. For fixed u, the property Eu u′ defined by u′ ≡u is a non-trivial behavioral property of
untyped terms. So it is inseparable from its negation, and hence is undecidable.
21.4
Untyped Means Uni-Typed
The untyped λ-calculus can be faithfully embedded in a typed language with recursive types.
Thus every untyped λ-term has a representation as a typed expression in such a way that execution
of the representation of a λ-term corresponds to execution of the term itself. This embedding is
not a matter of writing an interpreter for the λ-calculus in FPC, but rather a direct representation
of untyped λ-terms as typed expressions in a language with recursive types.
The key observation is that the untyped λ-calculus is really the uni-typed λ-calculus. It is not the
absence of types that gives it its power, but rather that it has only one type, the recursive type
D ≜rec t is t ⇀t.
A value of type D is of the form fold( e ) where e is a value of type D ⇀D — a function whose
domain and range are both D. Any such function can be regarded as a value of type D by “fold-
ing”, and any value of type D can be turned into a function by “unfolding”. As usual, a recursive
type is a solution to a type equation, which in the present case is the equation
D ∼= D ⇀D.
This isomorphism specifies that D is a type that is isomorphic to the space of partial functions on
D itself, which is impossible if types are just sets.
This isomorphism leads to the following translation, of Λ into FPC:
x† ≜x
(21.14a)
λ ( x ) u† ≜fold( λ ( x : D ) u† )
(21.14b)
u1( u2 )† ≜unfold( u†
1 )( u†
2 )
(21.14c)

PREVIEW
21.5 Notes
193
Note that the embedding of a λ-abstraction is a value, and that the embedding of an application
exposes the function being applied by unfolding the recursive type. And so we have
( ( λ ( x ) u1 )( u2 ) )† = unfold( fold( λ ( x : D ) u†
1 ) )( u†
2 )
≡( λ ( x : D ) u†
1 )( u†
2 )
≡[u†
2/x]u†
1
= ([u2/x]u1)†.
The last step, stating that the embedding commutes with substitution, is proved by induction on
the structure of u1. Thus β-reduction is implemented by evaluation of the embedded terms.
Thus we see that the canonical untyped language, Λ, which by dint of terminology stands in
opposition to typed languages, turns out to be but a typed language after all. Rather than eliminat-
ing types, an untyped language consolidates an infinite collection of types into a single recursive
type. Doing so renders static type checking trivial, at the cost of incurring dynamic overhead to
coerce values to and from the recursive type. In Chapter 22 we will take this a step further by
admitting many different types of data values (not just functions), each of which is a component
of a “master” recursive type. This generalization shows that so-called dynamically typed languages
are, in fact, statically typed. Thus this traditional distinction cannot be considered an opposition,
because dynamic languages are but particular forms of static languages in which undue emphasis
is placed on a single recursive type.
21.5
Notes
The untyped λ-calculus was introduced by Church (1941) as a formalization of the informal con-
cept of a computable function. Unlike the well-known machine models, such as the Turing ma-
chine or the random access machine, the λ-calculus codifies mathematical and programming prac-
tice. Barendregt (1984) is the definitive reference for all aspects of the untyped λ-calculus; the proof
of Scott’s theorem is adapted from Barendregt’s account. Scott (1980a) gave the first model of the
untyped λ-calculus in terms of an elegant theory of recursive types. This construction underlies
Scott’s apt description of the λ-calculus as “uni-typed”, rather than “untyped.” The idea to char-
acterize Church’s Law as such was communicated to the author, independently of each other, by
Robert L. Constable and Mark Lillibridge.
Exercises
21.1. Define an encoding of finite products as defined in Chapter 10 in Λ.
21.2. Define the factorial function in Λ two ways, one without using Y, and one using Y. In both
cases show that your solution, u, has the property that u( n ) ≡n!.
21.3. Define the “Church booleans” in Λ by defining terms true and false such that

PREVIEW
194
21.5 Notes
(a) true( u1 )( u2 ) ≡u1.
(b) false( u1 )( u2 ) ≡u2.
What is the encoding of if u then u1 else u2?
21.4. Define an encoding of finite sums as defined in Chapter 11 in Λ.
21.5. Define an encoding of finite lists of natural numbers as defined in Chapter 15 in Λ.
21.6. Define an encoding of the infinite streams of natural numbers as defined in Chapter 15 in Λ.
21.7. Show that Λ can be “compiled” to sk-combinators using bracket abstraction (see Exercises 3.4
and 3.5. Define a translation u∗from Λ into sk combinators such that
if u1 ≡u2, then u∗
1 ≡u∗
2.
Hint: Define u∗by induction on the structure of u, using the compositional form of bracket
abstraction considered in Exercise 3.5. Show that the translation is itself compositional in
that it commutes with substitution:
([u2/x]u1)∗= [u∗
2/x]u∗.
Then proceed by rule induction on rules (21.2) to show the required correctness condition.

PREVIEW
Chapter 22
Dynamic Typing
We saw in Chapter 21 that an untyped language is a uni-typed language in which “untyped”
terms are just terms of single recursive type. Because all expressions of Λ are well-typed, type
safety ensures that no misinterpretation of a value is possible. When spelled out for Λ, type safety
follows from there being exactly one class of values, that of functions on values. No application
can get stuck, because every value is a function that may be applied to an argument.
This safety property breaks down once more than one class of value is admitted. For example,
if the natural numbers are added as a primitive to Λ, then it is possible to incur a run-time error
by attempting to apply a number to an argument. One way to manage this is to embrace the
possibility, treating class mismatches as checked errors, and weakening the progress theorem as
outlined in Chapter 6. Such languages are called dynamic languages because an error such as the
one described is postponed to run time, rather than precluded at compile time by type checking.
Languages of the latter sort are called static languages.
Dynamic languages are often considered in opposition to static languages, but the opposition
is illusory. Just as the untyped λ-calculus is uni-typed, so dynamic languages are but special cases
of static languages in which there is only one recursive type (albeit with multiple classes of value).

PREVIEW
196
22.1 Dynamically Typed PCF
22.1
Dynamically Typed PCF
To illustrate dynamic typing we formulate a dynamically typed version of PCF, called DPCF. The
abstract syntax of DPCF is given by the following grammar:
Exp
d
::=
x
x
variable
num[ n ]
n
numeral
zero
zero
zero
succ( d )
succ( d )
successor
ifz{d0 ; x . d1}( d )
ifz d {zero ,→d0 | succ( x ) ,→d1}
zero test
fun( x . d )
λ ( x ) d
abstraction
ap( d1 ; d2 )
d1( d2 )
application
fix( x . d )
fix x is d
recursion
There are two classes of values in DPCF, the numbers, which have the form num[ n ], and the func-
tions, which have the form fun( x . d ). The expressions zero and succ( d ) are not themselves
values, but rather are constructors that evaluate to values. General recursion is definable using a
fixed point combinator, but is taken as primitive here to simplify the analysis of the dynamics in
Section 22.3.
As usual, the abstract syntax of DPCF is what matters, but we use the concrete syntax to im-
prove readability. However, notational conveniences can obscure important details, such as the
tagging of values with their class and the checking of these tags at run-time. For example, the
concrete syntax for a number, n, suggests a “bare” representation, the abstract syntax reveals that
the number is labeled with the class num to distinguish it from a function. Correspondingly, the
concrete syntax for a function is λ ( x ) d, but its abstract syntax, fun( x . d ), shows that it also sports
a class label. The class labels are required to ensure safety by run-time checking, and must not be
overlooked when comparing static with dynamic languages.
The statics of DPCF is like that of Λ; it merely checks that there are no free variables in the
expression. The judgment
x1 ok, . . . xn ok ⊢d ok
states that d is a well-formed expression with free variables among those in the hypotheses. If the
assumptions are empty, then we write just d ok to mean that d is a closed expression of DPCF.
The dynamics of DPCF must check for errors that would never arise in a language such as
PCF. For example, evaluation of a function application must ensure that the value being applied
is indeed a function, signaling an error if it is not. Similarly the conditional branch must ensure that
its principal argument is a number, signaling an error if it is not. To account for these possibilities,

PREVIEW
22.1 Dynamically Typed PCF
197
the dynamics is given by several judgment forms, as summarized in the following chart:
d val
d is a (closed) value
d 7−→d′
d evaluates in one step to d′
d err
d incurs a run-time error
d is num n
d is of class num with value n
d isnt num
d is not of class num
d is fun x . d
d is of class fun with body x . d
d isnt fun
d is not of class fun
The last four judgment forms implement dynamic class checking. They are only relevant when d is
already a value. The affirmative class-checking judgments have a second argument that represents
the underlying structure of a value; this argument is not itself an expression of DPCF.
The value judgment d val states that d is a evaluated (closed) expression:
num[ n ] val
(22.1a)
fun( x . d ) val
(22.1b)
The affirmative class-checking judgments are defined by the following rules:
num[ n ] is num n
(22.2a)
fun( x . d ) is fun x . d
(22.2b)
The negative class-checking judgments are correspondingly defined by these rules:
num[ n ] isnt fun
(22.3a)
fun( x . d ) isnt num
(22.3b)
The transition judgment d 7−→d′ and the error judgment d err are defined simultaneously by
the following rules:
zero 7−→num[ z ]
(22.4a)
d 7−→d′
succ( d ) 7−→succ( d′ )
(22.4b)
d err
succ( d ) err
(22.4c)
d is num n
succ( d ) 7−→num[ s( n ) ]
(22.4d)
d isnt num
succ( d ) err
(22.4e)

PREVIEW
198
22.1 Dynamically Typed PCF
d 7−→d′
ifz{d0 ; x . d1}( d ) 7−→ifz{d0 ; x . d1}( d′ )
(22.4f)
d err
ifz{d0 ; x . d1}( d ) err
(22.4g)
d is num z
ifz{d0 ; x . d1}( d ) 7−→d0
(22.4h)
d is num s( n )
ifz{d0 ; x . d1}( d ) 7−→[num[ n ]/x]d1
(22.4i)
d isnt num
ifz{d0 ; x . d1}( d ) err
(22.4j)
d1 7−→d′
1
ap( d1 ; d2 ) 7−→ap( d′
1 ; d2 )
(22.4k)


d1 val
d2 7−→d′
2
ap( d1 ; d2 ) 7−→ap( d1 ; d′
2 )


(22.4l)
d1 err
ap( d1 ; d2 ) err
(22.4m)
d1 is fun x . d
[d2 val]
ap( d1 ; d2 ) 7−→[d2/x]d
(22.4n)
d1 isnt fun
ap( d1 ; d2 ) err
(22.4o)
fix( x . d ) 7−→[fix( x . d )/x]d
(22.4p)
Rule (22.4i) labels the predecessor with the class num to maintain the invariant that variables are
bound to expressions of DPCF.
Lemma 22.1 (Class Checking). If d val, then
1. either d is num n for some n, or d isnt num;
2. either d is fun x . d′ for some x and d′, or d isnt fun.
Proof. By inspection of the rules defining the class-checking judgments.
Theorem 22.2 (Progress). If d ok, then either d val, or d err, or there exists d′ such that d 7−→d′.

PREVIEW
22.2 Variations and Extensions
199
Proof. By induction on the structure of d. For example, if d = succ( d′ ), then we have by induction
either d′ val, or d′ err, or d′ 7−→d′′ for some d′′. In the last case we have by rule (22.4b) that
succ( d′ ) 7−→succ( d′′ ), and in the second-to-last case we have by rule (22.4c) that succ( d′ ) err.
If d′ val, then by Lemma 22.1, either d′ is num n or d′ isnt num. In the former case succ( d′ ) 7−→
num[ s( n ) ], and in the latter succ( d′ ) err. The other cases are handled similarly.
Lemma 22.3 (Exclusivity). For any d in DPCF, exactly one of the following holds: d val, or d err, or
d 7−→d′ for some d′.
Proof. By induction on the structure of d, making reference to rules (22.4).
22.2
Variations and Extensions
The dynamic language DPCF defined in Section 22.1 parallels the static language PCF defined
in Chapter 19. One discrepancy, however, is in the treatment of natural numbers. Whereas in
PCF the zero and successor operations are introduction forms for the type nat, in DPCF they are
elimination forms that act on specially defined numerals. The present formulation uses only a
single class of numbers.
One could instead treat zero and succ( d ) as values of separate classes, and introduce the obvi-
ous class checking judgments for them. When written in this style, the dynamics of the conditional
branch is given as follows:
d 7−→d′
ifz{d0 ; x . d1}( d ) 7−→ifz{d0 ; x . d1}( d′ )
(22.5a)
d is zero
ifz{d0 ; x . d1}( d ) 7−→d0
(22.5b)
d is succ d′
ifz{d0 ; x . d1}( d ) 7−→[d′/x]d1
(22.5c)
d isnt zero
d isnt succ
ifz{d0 ; x . d1}( d ) err
(22.5d)
Notice that the predecessor of a value of the successor class need not be a number, whereas in the
previous formulation this possibility does not arise.
DPCF can be extended with structured data similarly. A classic example is to consider a class
nil, consisting of a “null” value, and a class cons, consisting of pairs of values.
Exp
d
::=
nil
nil
null
cons( d1 ; d2 )
cons( d1 ; d2 )
pair
ifnil( d ; d0 ; x, y . d1 )
ifnil d {nil ,→d0 | cons( x ; y ) ,→d1}
conditional

PREVIEW
200
22.2 Variations and Extensions
The expression ifnil( d ; d0 ; x, y . d1 ) distinguishes the null value from a pair, and signals an error
on any other class of value.
Lists (finite sequences) can be encoded using null and pairing. For example, the list consisting
of three zeros can be represented by the value
cons( zero ; cons( zero ; cons( zero ; nil ) ) ).
But what to make of the following value?
cons( zero ; cons( zero ; cons( zero ; λ ( x ) x ) ) )
It is not a list, because it does not end with nil, but it is a permissible value in the enriched
language.
A difficulty with encoding lists using null and pair emerges when defining functions on them.
For example, here is a definition of the function append that concatenates two lists:
fix a is λ ( x ) λ ( y ) ifnil( x ; y ; x1, x2 . cons( x1 ; a( x2 )( y ) ) )
Nothing prevents us from applying this function to any two values, regardless of whether they
are lists. If the first argument is not a list, then execution aborts with an error. But because the
function does not traverse its second argument, it can be any value at all. For example, we may
apply append with a list and a function to obtain the “list” that ends with a λ given above.
It might be argued that the conditional branch that distinguishes null from a pair is inappro-
priate in DPCF, because there are more than just these two classes in the language. One approach
that avoids this criticism is to abandon pattern matching on the class of data, replacing it by a gen-
eral conditional branch that distinguishes null from all other values, and adding to the language
predicates1 that test the class of a value and destructors that invert the constructors of each class.
We could instead reformulate null and and pairing as follows:
Exp
d
::=
cond( d ; d0 ; d1 )
cond( d ; d0 ; d1 )
conditional
nil?( d )
nil?( d )
nil test
cons?( d )
cons?( d )
pair test
car( d )
car( d )
first projection
cdr( d )
cdr( d )
second projection
The conditional cond( d ; d0 ; d1 ) distinguishes d between nil and all other values. If d is not nil,
the conditional evaluates to d0, and otherwise evaluates to d1. In other words the value nil repre-
sents boolean falsehood, and all other values represent boolean truth. The predicates nil?( d ) and
cons?( d ) test the class of their argument, yielding nil if the argument is not of the specified class,
and yielding some non-nil if so. The destructors car( d ) and cdr( d ) decompose cons( d1 ; d2 )
into d1 and d2, respectively.2
1Predicates evaluate to the null value to mean that a condition is false, and some non-null value to mean that it is true.
2The terminology for the projections is archaic, but well-established. It is said that car originally stood for “contents of
the address register” and that cdr stood for “contents of the data register”, referring to the details of the original imple-
mentation of Lisp.

PREVIEW
22.3 Critique of Dynamic Typing
201
Written in this form, the function append is given by the expression
fix a is λ ( x ) λ ( y ) cond( x ; cons( car( x ) ; a( cdr( x ) )( y ) ) ; y ).
The behavior of this formulation of append is no different from the earlier one; the only differ-
ence is that instead of dispatching on whether a value is either null or a pair, we instead allow
discrimination on any predicate of the value, which includes such checks as special cases.
An alternative, which is not widely used, is to enhance, and not restrict, the conditional branch
so that it includes cases for each possible class of value in the language. So in a language with num-
bers, functions, null, and pairing, the conditional would have four branches. The fourth branch,
for pairing, would deconstruct the pair into its constituent parts. The difficulty with this approach
is that in realistic languages there are many classes of data, and such a conditional would be rather
unwieldy. Moreover, even once we have dispatched on the class of a value, it is nevertheless neces-
sary for the primitive operations associated with that class to admit run-time checks. For example,
we may determine that a value d is of the numeric class, but there is no way to propagate this in-
formation into the branch of the conditional that then adds d to some other number. The addition
operation must still check the class of d, recover the underlying number, and create a new value
of numeric class. It is an inherent limitation of dynamic languages that they do not allow values
other than classified values.
22.3
Critique of Dynamic Typing
The safety theorem for DPCF is an advantage of dynamic over static typing. Unlike static lan-
guages, which rule out some candidate programs as ill-typed, every piece of abstract syntax in
DPCF is well-formed, and hence, by Theorem 22.2, has a well-defined dynamics (albeit one with
checked errors). But this convenience is also a disadvantage, because errors that could be ruled
out at compile time by type checking are not signaled until run time.
Consider, for example, the addition function in DPCF, whose specification is that, when passed
two values of class num, returns their sum, which is also of class num:3
fun( x . fix( p . fun( y . ifz{x ; y′ . succ( p( y′ ) )}( y ) ) ) ).
The addition function may, deceptively, be written in concrete syntax as follows:
λ ( x ) fix p is λ ( y ) ifz y {zero ,→x | succ( y′ ) ,→succ( p( y′ ) )}.
It is deceptive, because it obscures the class tags on values, and the operations that check the
validity of those tags. Let us now examine the costs of these operations in a bit more detail.
First, note that the body of the fixed point expression is labeled with class fun. The dynamics of
the fixed point construct binds p to this function. Consequently, the dynamic class check incurred
by the application of p in the recursive call is guaranteed to succeed. But DPCF offers no means of
suppressing the redundant check, because it cannot express the invariant that p is always bound
to a value of class fun.
3This specification imposes no restrictions on the behavior of addition on arguments that are not classified as numbers,
but we could make the further demand that the function abort when applied to arguments that are not classified by num.

PREVIEW
202
22.4 Notes
Second, note that the result of applying the inner λ-abstraction is either x, the argument of
the outer λ-abstraction, or the successor of a recursive call to the function itself. The successor
operation checks that its argument is of class num, even though this condition is guaranteed to
hold for all but the base case, which returns the given x, which can be of any class at all. In
principle we can check that x is of class num once, and note that it is otherwise a loop invariant
that the result of applying the inner function is of this class. However, DPCF gives us no way
to express this invariant; the repeated, redundant tag checks imposed by the successor operation
cannot be avoided.
Third, the argument y to the inner function is either the original argument to the addition
function, or is the predecessor of some earlier recursive call. But as long as the original call is to
a value of class num, then the dynamics of the conditional will ensure that all recursive calls have
this class. And again there is no way to express this invariant in DPCF, and hence there is no way
to avoid the class check imposed by the conditional branch.
Classification is not free—storage is required for the class label, and it takes time to detach the
class from a value each time it is used and to attach a class to a value when it is created. Although
the overhead of classification is not asymptotically significant (it slows down the program only by
a constant factor), it is nevertheless non-negligible, and should be eliminated when possible. But
this is impossible within DPCF, because it cannot enforce the restrictions required to express the
required invariants. For that we need a static type system.
22.4
Notes
The earliest dynamically typed language is Lisp (McCarthy, 1965), which continues to influence
language design a half century after its invention. Dynamic PCF is the core of Lisp, but with a
proper treatment of variable binding, correcting what McCarthy himself has described as an error
in the original design. Informal discussions of dynamic languages are often complicated by the
elision of the dynamic checks that are made explicit here. Although the surface syntax of dynamic
PCF is almost the same as that for PCF, minus the type annotations, the underlying dynamics is
different. It is for this reason that static PCF cannot be seen as a restriction of dynamic PCF by the
imposition of a type system.
Exercises
22.1. Surface syntax can be deceiving; even simple arithmetic expressions do not have the same
meaning in DPCF that they do in PCF. To see why, define the addition function, plus,
in DPCF, and examine the dynamics of evaluating expressions such as plus( 5 )( 7 ). Even
though this expression might be written as “5 + 7” in both static and dynamic languages,
they have different meanings.
22.2. Give a precise dynamics to the data structuring primitives described informally in Sec-
tion 22.2. What class restrictions should cons impose on its arguments? Check the dynamics
of the append function when called with two lists as arguments.

PREVIEW
22.4 Notes
203
22.3. To avoid the difficulties with the representation of lists using cons and nil, introduce a class
of lists that are constructed using revised versions of nil and cons that operate on the class
of lists. Revisit the dynamics of the append function when redefined using the class of lists.
22.4. Allowing multiple arguments to, and multiple results from, functions is a notorious source
of trouble in dynamic languages. The restriction to a single type makes it impossible even
to distinguish n things from m things, let alone express more subtle properties of a program.
Numerous workarounds have been proposed. Explore the problem yourself by enriching
DPCF with multi-argument and multi-result functions. Be sure to consider these questions:
(a) If a function is defined with n parameters, what should happen if it is called with more
or fewer than n arguments?
(b) What happens if one were to admit functions with a varying number of arguments?
How would you refer to these arguments within the body of such a function? How
does this relate to pattern matching?
(c) What if one wished to admit keyword parameter passing by giving names to the ar-
guments, and allowing them to be passed in any order by associating them with their
names?
(d) What notation would you suggest for functions returning multiple results? For exam-
ple, a division function might return the quotient and the remainder. How might one
notate this in the function body? How would a caller access the results individually or
collectively?
(e) How would one define the composition of two functions when either or both can take
multiple arguments or return multiple results?

PREVIEW
204
22.4 Notes

PREVIEW
Chapter 23
Hybrid Typing
A hybrid language is one that combines static and dynamic typing by enriching a statically typed
language with a distinguished type dyn of dynamic values. The dynamically typed language con-
sidered in Chapter 22 can be embedded into the hybrid language by viewing a dynamically typed
program as a statically typed program of type dyn. Static and dynamic types are not opposed to
one another, but may coexist harmoniously. The ad hoc device of adding the type dyn to a static
language is unnecessary in a language with recursive types, wherein it is definable as a particular
recursive type. Thus, one may say that dynamic typing is a mode of use of static typing, reconciling an
apparent opposition between them.
23.1
A Hybrid Language
Consider the language HPCF, which extends PCF with the following constructs:
Typ
τ
::=
dyn
dyn
dynamic
Exp
e
::=
new[ l ]( e )
l ! e
construct
cast[ l ]( e )
e @ l
destruct
inst[ l ]( e )
l ? e
discriminate
Cls
l
::=
num
num
number
fun
fun
function
The type dyn is the type of dynamically classified values. The constructor attaches a classifier to
a value of a type associated to that classifer, the destructor recovers the value classified with the
given classifier, and the discriminator tests the class of a classified value.
The statics of HPCF extends that of PCF with the following rules:
Γ ⊢e : nat
Γ ⊢new[ num ]( e ) : dyn
(23.1a)
Γ ⊢e : dyn ⇀dyn
Γ ⊢new[ fun ]( e ) : dyn
(23.1b)

PREVIEW
206
23.1 A Hybrid Language
Γ ⊢e : dyn
Γ ⊢cast[ num ]( e ) : nat
(23.1c)
Γ ⊢e : dyn
Γ ⊢cast[ fun ]( e ) : dyn ⇀dyn
(23.1d)
Γ ⊢e : dyn
Γ ⊢inst[ num ]( e ) : bool
(23.1e)
Γ ⊢e : dyn
Γ ⊢inst[ fun ]( e ) : bool
(23.1f)
The statics ensures that classifiers are attached to values of the right type, namely natural numbers
for num, and functions on classified values for fun.
The dynamics of HPCF extends that of PCF with the following rules:
e val
new[ l ]( e ) val
(23.2a)
e 7−→e′
new[ l ]( e ) 7−→new[ l ]( e′ )
(23.2b)
e 7−→e′
cast[ l ]( e ) 7−→cast[ l ]( e′ )
(23.2c)
new[ l ]( e ) val
cast[ l ]( new[ l ]( e ) ) 7−→e
(23.2d)
new[ l′ ]( e ) val
l ̸= l′
cast[ l ]( new[ l′ ]( e ) ) err
(23.2e)
e 7−→e′
inst[ l ]( e ) 7−→inst[ l ]( e′ )
(23.2f)
new[ l ]( e ) val
inst[ l ]( new[ l ]( e ) ) 7−→true
(23.2g)
new[ l ]( e ) val
l ̸= l′
inst[ l′ ]( new[ l ]( e ) ) 7−→false
(23.2h)
Casting compares the class of the object to the required class, returning the underlying object if
these coincide, and signaling an error otherwise.1
1The judgment e err signals a checked error that is to be treated as described in Section 6.3.

PREVIEW
23.2 Dynamic as Static Typing
207
Lemma 23.1 (Canonical Forms). If e : dyn and e val, then e = new[ l ]( e′ ) for some class l and some
e′ val. If l = num, then e′ : nat, and if l = fun, then e′ : dyn ⇀dyn.
Proof. By rule induction on the statics of HPCF.
Theorem 23.2 (Safety). The language HPCF is safe:
1. If e : τ and e 7−→e′, then e′ : τ.
2. If e : τ, then either e val, or e err, or e 7−→e′ for some e′.
Proof. Preservation is proved by rule induction on the dynamics, and progress is proved by rule
induction on the statics, making use of the canonical forms lemma. The opportunities for run-time
errors are the same as those for DPCF—a well-typed cast might fail at run-time if the class of the
cast does not match the class of the value.
In a language such as FPC (Chapter 20) with recursive types, there is no need to add dyn as a
primitive type. Instead, it is defined to be type
rec t is [num ,→nat , fun ,→t ⇀t].
(23.3)
The introduction and elimination forms for this definition of dyn are definable as follows:2
new[ num ]( e ) ≜fold( num · e )
(23.4)
new[ fun ]( e ) ≜fold( fun · e )
(23.5)
cast[ num ]( e ) ≜case unfold( e ) {num · x ,→x | fun · x ,→error}
(23.6)
cast[ fun ]( e ) ≜case unfold( e ) {num · x ,→error | fun · x ,→x}.
(23.7)
These definition simply decompose the class operations for dyn into recursive unfoldings and case
analyses on values of a sum type.
23.2
Dynamic as Static Typing
The language DPCF of Chapter 22 can be embedded into HPCF by a simple translation that makes
explicit the class checking in the dynamics of DPCF. Specifically, we may define a translation d†
of expressions of DPCF into expressions of HPCF according to the following static correctness
criterion:
Theorem 23.3. If x1 ok, . . . , xn ok ⊢d ok according to the statics of DPCF, then x1 : dyn, . . . , xn : dyn ⊢
d† : dyn in HPCF.
2The expression error aborts the computation with an error; this can be accomplished using exceptions, which are the
subject of Chapter 29.

PREVIEW
208
23.3 Optimization of Dynamic Typing
The proof of Theorem 23.3 is given by induction on the structure of d based on the following
translation:
x† ≜x
num[ n ]† ≜new[ num ]( n )
zero† ≜new[ num ]( z )
succ( d )† ≜new[ num ]( s( cast[ num ]( d† ) ) )
ifz{d0 ; x . d1}( d ) ≜ifz{d†
0 ; x . [new[ num ]( x )/x]d†
1}( cast[ num ]( d† ) )
(fun( x . d ))† ≜new[ fun ]( λ ( x : dyn ) d† )
(ap( d1 ; d2 ))† ≜cast[ fun ]( d†
1 )( d†
2 )
fix( x . d ) ≜fix{dyn}( x . d† )
A rigorous proof of correctness of this translation requires methods like those in Chapter 47.
23.3
Optimization of Dynamic Typing
The language HPCF combines static and dynamic typing by enriching PCF with the type dyn of
classified values. It is, for this reason, called a hybrid language. Unlike a purely dynamic type sys-
tem, a hybrid type system can express invariants that are crucial to the optimization of programs
in HPCF.
Consider the addition function in DPCF given in Section 22.3, which we transcribe here for
convenience:
λ ( x ) fix p is λ ( y ) ifz y {zero ,→x | succ( y′ ) ,→succ( p( y′ ) )}.
It is a value of type dyn in HPCF given as follows:
fun ! λ ( x : dyn ) fix p : dyn is fun ! λ ( y : dyn ) ex,p,y,
(23.8)
within which the fragment
x : dyn, p : dyn, y : dyn ⊢ex,p,y : dyn
stands for the expression
ifz ( y @ num ) {zero ,→x | succ( y′ ) ,→num ! ( s( ( p @ fun )( num ! y′ ) @ num ) )}.
The embedding into HPCF makes explicit the run-time checks that are implicit in the dynamics of
DPCF.
Careful examination of the embedded formulation of addition reveals a great deal of redun-
dancy and overhead that can be eliminated in the statically typed version. Eliminating this redun-
dancy requires a static type discipline, because the intermediate computations involve values of a

PREVIEW
23.3 Optimization of Dynamic Typing
209
type other than dyn. This transformation shows that the freedoms offered by dynamic languages
accruing from the absence of types are, instead, limitations on their expressive power arising from
the restriction to a single type.
The first redundancy arises from the use of recursion in a dynamic language. In the above ex-
ample we use recursion to define the inner loop p of the computation. The value p is, by definition,
a λ-abstraction, which is explicitly tagged as a function. Yet the call to p within the loop checks
at run-time whether p is in fact a function before applying it. Because p is an internally defined
function, all of its call sites are under the control of the addition function, which means that there
is no need for such pessimism at calls to p, provided that we change its type to dyn ⇀dyn, which
directly expresses the invariant that p is a function acting on dynamic values.
Performing this transformation, we obtain the following reformulation of the addition function
that eliminates this redundancy:
fun ! λ ( x : dyn ) fun ! fix p : dyn ⇀dyn is λ ( y : dyn ) e′
x,p,y,
where e′
x,p,y is the expression
ifz ( y @ num ) {zero ,→x | succ( y′ ) ,→num ! ( s( p( num ! y′ ) @ num ) )}.
We have “hoisted” the function class label out of the loop, and suppressed the cast inside the loop.
Correspondingly, the type of p has changed to dyn ⇀dyn.
Next, note that the variable y of type dyn is cast to a number on each iteration of the loop before
it is tested for zero. Because this function is recursive, the bindings of y arise in one of two ways:
at the initial call to the addition function, and on each recursive call. But the recursive call is made
on the predecessor of y, which is a true natural number that is labeled with num at the call site,
only to be removed by the class check at the conditional on the next iteration. This observation
suggests that we hoist the check on y outside of the loop, and avoid labeling the argument to the
recursive call. Doing so changes the type of the function, however, from dyn ⇀dyn to nat ⇀dyn.
Consequently, further changes are required to ensure that the entire function remains well-typed.
Before doing so, let us make another observation. The result of the recursive call is checked to
ensure that it has class num, and, if so, the underlying value is incremented and labeled with class
num. If the result of the recursive call came from an earlier use of this branch of the conditional,
then obviously the class check is redundant, because we know that it must have class num. But
what if the result came from the other branch of the conditional? In that case the function returns
x, which need not be of class num because it is provided by the caller of the function. However,
we may reasonably insist that it is an error to call addition with a non-numeric argument. This
restriction can be enforced by replacing x in the zero branch of the conditional by x @ num.
Combining these optimizations we obtain the inner loop e′′
x defined as follows:
fix p : nat ⇀nat is λ ( y : nat ) ifz y {zero ,→x @ num | succ( y′ ) ,→s( p( y′ ) )}.
It has the type nat ⇀nat, and runs without class checks when applied to a natural number.
Finally, recall that the goal is to define a version of addition that works on values of type dyn.
Thus we need a value of type dyn ⇀dyn, but what we have at hand is a function of type nat ⇀nat.

PREVIEW
210
23.4 Static Versus Dynamic Typing
It can be converted to the needed form by pre-composing with a cast to num and post-composing
with a coercion to num:
fun ! λ ( x : dyn ) fun ! λ ( y : dyn ) num ! ( e′′
x( y @ num ) ).
The innermost λ-abstraction converts the function e′′
x from type nat ⇀nat to type dyn ⇀dyn by
composing it with a class check that ensures that y is a natural number at the initial call site, and
applies a label to the result to restore it to type dyn.
The outcome of these transformations is that the inner loop of the computation runs at “full
speed”, without any manipulation of tags on functions or numbers. But the outermost form of ad-
dition remains; it is a value of type dyn encapsulating a curried function that takes two arguments
of type dyn. Doing so preserves the correctness of all calls to addition, which pass and return
values of type dyn, while optimizing its execution during the computation. Of course, we could
strip the class tags from the addition function, changing its type from dyn to the more descriptive
dyn ⇀dyn ⇀dyn, but this imposes the obligation on the caller to treat addition not as a value of
type dyn, but rather as a function that must be applied to two successive values of type dyn whose
class is num. As long as the call sites to addition are under programmer control, there is no obstacle
to effecting this transformation. It is only when there are external call sites, not directly under
programmer control, that there is any need to package addition as a value of type dyn. Applying
this principle generally, we see that dynamic typing is only of marginal utility—it is used only at
the margins of a system where uncontrolled calls arise. Internally to a system there is no benefit,
and considerable drawback, to restricting attention to the type dyn.
23.4
Static Versus Dynamic Typing
There have been many attempts by advocates of dynamic typing to distinguish dynamic from
static languages. It is useful to consider the supposed distinctions from the present viewpoint.
1. Dynamic languages associate types with values, whereas static languages associate types to variables.
Dynamic languages associate classes, not types, to values by tagging them with identifiers
such as num and fun. This form of classification amounts to a use of recursive sum types
within a statically typed language, and hence cannot be seen as a distinguishing feature of
dynamic languages. Moreover, static languages assign types to expressions, not just vari-
ables. Because dynamic languages are just particular static languages (with a single type),
the same can be said of dynamic languages.
2. Dynamic languages check types at run-time, whereas static language check types at compile time.
Dynamic languages are just as surely statically typed as static languages, albeit for a degen-
erate type system with only one type. As we have seen, dynamic languages do perform
class checks at run-time, but so too do static languages that admit sum types. The difference
is only the extent to which we must use classification: always in a dynamic language, only
as necessary in a static language.
3. Dynamic languages support heterogeneous collections, whereas static languages support homoge-
neous collections. The purpose of sum types is to support heterogeneity, so that any static

PREVIEW
23.5 Notes
211
language with sums admits heterogeneous data structures. A typical example is a list such
as
cons( num[ 1 ] ; cons( fun( x . x ) ; nil ) )
(written in abstract syntax for emphasis). It is sometimes said that such a list is not repre-
sentable in a static language, because of the disparate nature of its components. Whether in
a static or a dynamic language, lists are type homogeneous, but can be class heterogeneous.
All elements of the above list are of type dyn; the first is of class num, and the second is of
class fun.
Thus the seeming opposition between static and dynamic typing is an illusion. The question is
not whether to have static typing, but rather how best to embrace it. Confining one’s attention to
a single recursive type seems pointlessly restrictive. Indeed, many so-called untyped languages
have implicit concessions to there being more than one type. The classic example is the ubiquitous
concept of “multi-argument functions”, which are a concession to the existence of products of
the type of values (with pattern matching). It is then a short path to considering “multi-result
functions”, and other ad hoc language features that amount to admitting a richer and richer static
type discipline.
23.5
Notes
Viewing dynamic languages as static languages with recursive types was first proposed by Dana
Scott (Scott, 1980b), who also suggested glossing “untyped” as “uni-typed”. Most modern stati-
cally typed languages, such as Java or Haskell or OCaml or SML, include a type similar to dyn, or
admit recursive types with which to define it. For this reason one might expect that the opposition
between dynamic and static typing would fade away, but industrial and academic trends suggest
otherwise.
Exercises
23.1. Consider the extensions to DPCF described in Section 22.2 to admit null and pairing and
their associated operations. Extend the statics and dynamics of HPCF to account for these
extensions, and give a translation of the null and pairing operations described informally in
Chapter 22 in terms of this extension to HPCF.
23.2. Continue the interpretation of the null and pairing operations in HPCF given in Exercise 23.1
to provide an interpretation in FPC. Specifically, define the expanded dyn as a recursive type,
and give a direct implementation of the null and pairing primitives in terms of this recursive
type.
23.3. Consider the append function defined in Chapter 22 using nil and cons to represent lists:
fix a is λ ( x ) λ ( y ) cond( x ; cons( car( x ) ; a( cdr( x ) )( y ) ) ; y ).

PREVIEW
212
23.5 Notes
Rewrite append in HPCF using the definitions given in Exercise 23.1. Then optimize the
implementation to eliminate unnecessary overhead while ensuring that append still has type
dyn.

PREVIEW
Part X
Subtyping

PREVIEW

PREVIEW
Chapter 24
Structural Subtyping
A subtype relation is a pre-order (reflexive and transitive relation) on types that validates the sub-
sumption principle:
if τ′ is a subtype of τ, then a value of type τ′ may be provided when a value of type τ is required.
The subsumption principle relaxes the strictures of a type system to allow values of one type to be
treated as values of another.
Experience shows that the subsumption principle, although useful as a general guide, can be
tricky to apply correctly in practice. The key to getting it right is the principle of introduction and
elimination. To see whether a candidate subtyping relationship is sensible, it suffices to consider
whether every introduction form of the subtype can be safely manipulated by every elimination
form of the supertype. A subtyping principle makes sense only if it passes this test; the proof of
the type safety theorem for a given subtyping relation ensures that this is the case.
A good way to get a subtyping principle wrong is to think of a type merely as a set of values
(generated by introduction forms), and to consider whether every value of the subtype can also
be considered to be a value of the supertype. The intuition behind this approach is to think of
subtyping as akin to the subset relation in ordinary mathematics. But, as we shall see, this can
lead to serious errors, because it fails to take account of the elimination forms that are applicable
to the supertype. It is not enough to think only of the introduction forms; subtyping is a matter of
behavior, and not containment.
24.1
Subsumption
A subtyping judgment has the form τ′ <: τ, and states that τ′ is a subtype of τ. At the least we
demand that the following structural rules of subtyping be admissible:
τ <: τ
(24.1a)
τ′′ <: τ′
τ′ <: τ
τ′′ <: τ
(24.1b)

PREVIEW
216
24.2 Varieties of Subtyping
In practice we either tacitly include these rules as primitive, or prove that they are admissible for
a given set of subtyping rules.
The point of a subtyping relation is to enlarge the set of well-typed programs, which is accom-
plished by the subsumption rule:
Γ ⊢e : τ′
τ′ <: τ
Γ ⊢e : τ
(24.2)
In contrast to most other typing rules, the rule of subsumption is not syntax-directed, because
it does not constrain the form of e. That is, the subsumption rule can be applied to any form of
expression. In particular, to show that e : τ, we have two choices: either apply the rule appropriate
to the particular form of e, or apply the subsumption rule, checking that e : τ′ and τ′ <: τ.
24.2
Varieties of Subtyping
In this section we will informally explore several different forms of subtyping in the context of
extensions of the language FPC introduced in Chapter 20.
Numeric Types
We begin with an informal discussion of numeric types such as are common in many programming
languages. Our mathematical experience suggests subtyping relationships among numeric types.
For example, in a language with types int, rat, and real, representing the integers, the rationals,
and the reals, it is tempting to postulate the subtyping relationships
int <: rat <: real
by analogy with the set containments
Z ⊆Q ⊆R.
But are these subtyping relationships sensible? The answer depends on the representations
and interpretations of these types. Even in mathematics, the containments just mentioned are
usually not true—or are true only in a rough sense. For example, the set of rational numbers can
be considered to consist of ordered pairs (m, n), with n ̸= 0 and gcd(m, n) = 1, representing the
ratio m/n. The set Z of integers can be isomorphically embedded within Q by identifying n ∈Z
with the ratio n/1. Similarly, the real numbers are often represented as convergent sequences of
rationals, so that strictly speaking the rationals are not a subset of the reals, but rather can be
embedded in them by choosing a canonical representative (a particular convergent sequence) of
each rational.
For mathematical purposes it is entirely reasonable to overlook fine distinctions such as that
between Z and its embedding within Q. Ignoring the difference is justified because the operations
on rationals restrict to the embedding in the expected way: if we add two integers thought of
as rationals in the canonical way, then the result is the rational associated with their sum. And
similarly for the other operations, provided that we take some care in defining them to ensure that
it all works out properly. For the purposes of computing, however, we must also take account of
algorithmic efficiency and the finiteness of machine representations. For example, what are often

PREVIEW
24.2 Varieties of Subtyping
217
called “real numbers” in a programming language are, of course, floating point numbers, a finite
subset of the rational numbers. Not every rational can be exactly represented as a floating point
number, nor does floating point arithmetic restrict to rational arithmetic, even when its arguments
are exactly represented as floating point numbers.
Product Types
Product types give rise to a form of subtyping based on the subsumption principle. The only
elimination form applicable to a value of product type is a projection. Under mild assumptions
about the dynamics of projections, we may consider one product type to be a subtype of another by
considering whether the projections applicable to the supertype can be validly applied to values
of the subtype.
Consider a context in which a value of type τ = ⟨τj⟩j∈J is required. The statics of finite products
(rules (10.3)) ensures that the only elimination we may perform on a value of type τ is to take the
jth projection from it for some j ∈J to obtain a value of type τj. Now suppose that e is of type
τ′. For the projection e · j to be well-formed, then τ′ is a finite product type ⟨τ′
i ⟩i∈I such that j ∈I.
Moreover, for the projection to be of type τj, it is enough to require that τ′
j = τj. Because j ∈J is
arbitrary, we arrive at the following subtyping rule for finite product types:
J ⊆I
⟨τi⟩i∈I <: ⟨τj⟩j∈J
.
(24.3)
This rule sufices for the required subtyping, but not necessary; we will consider a more liberal form
of this rule in Section 24.3. The justification for rule (24.3) is that we may evaluate e · i regardless
of the actual form of e, provided only that it has a field indexed by i ∈I.
Sum Types
By an argument dual to the one given for finite product types we may derive a related subtyping
rule for finite sum types. If a value of type [τj]j∈J is required, the statics of sums (rules (11.3))
ensures that the only non-trivial operation that we may perform on that value is a J-indexed case
analysis. If we provide a value of type [τ′
i ]i∈I instead, no difficulty will arise so long as I ⊆J and
each τ′
i is equal to τi. If the containment is strict, some cases cannot arise, but this does not disrupt
safety.
I ⊆J
[τi]i∈I <: [τj]j∈J
.
(24.4)
Note well the reversal of the containment as compared to rule (24.3).
Dynamic Types
A popular form of subtyping is associated with the type dyn introduced in Chapter 23. The type
dyn provides no information about the class of a value of this type. One might argue that it is
whole the point of dynamic typing to suppress this information statically, making it available only

PREVIEW
218
24.3 Variance
dynamically. On the other hand, it is not much trouble to introduce subtypes of dyn that specify
the class of a value, relying on subsumption to “forget” the class when it cannot be determined
statically.
Working in the context of Chapter 23 this amounts to introduce two new types, dyn[ num ] and
dyn[ fun ], governed by the following two subtyping axioms:
dyn[ num ] <: dyn
(24.5a)
dyn[ fun ] <: dyn
(24.5b)
Of course, in a richer language with more classes of dynamic values one would correspondingly
introduce more such subtypes of dyn, one for each additional class. As a matter of notation, the
type dyn is frequently spelled object, and its class-specific subtypes dyn[ num ] and dyn[ fun ], are
often written as num and fun, respectively. But doing so invites confusion between the separate
concepts of class and type, as discussed in detail in Chapters 22 and 23.
The class-specific subtypes of dyn come into play by reformulating the typing rules for intro-
ducing values of type dyn to note the class of the created value:
Γ ⊢e : nat
Γ ⊢new[ num ]( e ) : dyn[ num ]
(24.6a)
Γ ⊢e : dyn ⇀dyn
Γ ⊢new[ fun ]( e ) : dyn[ fun ]
(24.6b)
Thus, in this formulation, classified values “start life” with class-specific types, because in those
cases it is statically apparent what is the class of the introduced value. Subsumption is used to
weaken the type to dyn in those cases where no static prediction can be made—for example, when
the branches of a conditional evaluate to dynamic values of different classes it is necessary to
weaken the type of the branches to dyn.
The advantage of such a subtyping mechanism is that we can express more precise types, such
as the type dyn[ num ] ⇀dyn[ num ] of functions mapping a value of type dyn with class num to
another such value. This typing is more precise than, say, dyn ⇀dyn, which merely classifies func-
tions that act on dynamically typed values. In this way weak invariants can be expressed and
enforced, but only insofar as it is possible to track the classes of the values involved in a computa-
tion. Subtyping is not nearly a powerful enough mechanism for practical situations, rendering the
additional specificity not worth the effort of including it. (A more powerful approach is developed
in Chapter 25.)
24.3
Variance
In addition to basic subtyping principles such as those considered in Section 24.2, it is also impor-
tant to consider the effect of subtyping on type constructors. A type constructor is covariant in an

PREVIEW
24.3 Variance
219
argument if subtyping in that argument is preserved by the constructor. It is contravariant if sub-
typing in that argument is reversed by the constructor. It is invariant in an argument if subtyping
for the constructed type is not affected by subtyping in that argument.
Product and Sum Types
Finite product types are covariant in each field. For if e is of type ⟨τ′
i ⟩i∈I, and the projection e · j is
to be of type τj, then it suffices to require that j ∈I and τ′
j <: τj.
(∀i ∈I) τ′
i <: τi
⟨τ′
i ⟩i∈I <: ⟨τi⟩i∈I
(24.7)
It is implicit in this rule that the dynamics of projection cannot be sensitive to the precise type of
any of the fields of a value of finite product type.
Finite sum types are also covariant, because each branch of a case analysis on a value of the
supertype expects a value of the corresponding summand, for which it suffices to provide a value
of the corresponding subtype summand:
(∀i ∈I) τ′
i <: τi
[τ′
i ]i∈I <: [τi]i∈I
(24.8)
Partial Function Types
The variance of the function type constructors is a bit more subtle. Let us consider first the variance
of the function type in its range. Suppose that e : τ1 ⇀τ′
2. Then if e1 : τ1, then e( e1 ) : τ′
2, and if
τ′
2 <: τ2, then e( e1 ) : τ2 as well.
τ′
2 <: τ2
τ1 ⇀τ′
2 <: τ1 ⇀τ2
(24.9)
Every function that delivers a value of type τ′
2 also delivers a value of type τ2, provided that
τ′
2 <: τ2. Thus the function type constructor is covariant in its range.
Now let us consider the variance of the function type in its domain. Suppose again that e :
τ1 ⇀τ2. Then e can be applied to any value of type τ1 to obtain a value of type τ2. Hence, by the
subsumption principle, it can be applied to any value of a subtype τ′
1 of τ1, and it will still deliver
a value of type τ2. Consequently, we may just as well think of e as having type τ′
1 ⇀τ2.
τ′
1 <: τ1
τ1 ⇀τ2 <: τ′
1 ⇀τ2
(24.10)
The function type is contravariant in its domain position. Note well the reversal of the subtyping
relation in the premise as compared to the conclusion of the rule!
Combining these rules we obtain the following general principle of contra- and covariance for
function types:
τ′
1 <: τ1
τ′
2 <: τ2
τ1 ⇀τ′
2 <: τ′
1 ⇀τ2
(24.11)

PREVIEW
220
24.3 Variance
Beware of the reversal of the ordering in the domain!
Recursive Types
The language FPC has a partial function types, which behave the same under subtyping as total
function types, sums and products, which behave as described above, and recursive types, which
introduce some subtleties that have been the source of error in language design. To gain some
intuition, consider the type of labeled binary trees with natural numbers at each node,
rec t is [empty ,→unit , binode ,→⟨data ,→nat , lft ,→t , rht ,→t⟩],
and the type of “bare” binary trees, without data attached to the nodes,
rec t is [empty ,→unit , binode ,→⟨lft ,→t , rht ,→t⟩].
Is either a subtype of the other? Intuitively, we might expect the type of labeled binary trees to be
a subtype of the type of bare binary trees, because any use of a bare binary tree can simply ignore
the presence of the label.
Now consider the type of bare “two-three” trees with two sorts of nodes, those with two chil-
dren, and those with three:
rec t is [empty ,→unit , binode ,→τ2 , trinode ,→τ3],
where
τ2 ≜⟨lft ,→t , rht ,→t⟩, and
τ3 ≜⟨lft ,→t , mid ,→t , rht ,→t⟩.
What subtype relationships should hold between this type and the preceding two tree types? In-
tuitively the type of bare two-three trees should be a supertype of the type of bare binary trees,
because any use of a two-three tree proceeds by three-way case analysis, which covers both forms
of binary tree.
To capture the pattern illustrated by these examples, we need a subtyping rule for recursive
types. It is tempting to consider the following rule:
t type ⊢τ′ <: τ
rec t is τ′ <: rec t is τ ??
(24.12)
That is, to check whether one recursive type is a subtype of the other, we simply compare their
bodies, with the bound variable treated as an argument. Notice that by reflexivity of subtyping,
we have t <: t, and hence we may use this fact in the derivation of τ′ <: τ.
Rule (24.12) validates the intuitively plausible subtyping between labeled binary tree and bare
binary trees just described. Deriving this requires checking that the subtyping relationship
⟨data ,→nat , lft ,→t , rht ,→t⟩<: ⟨lft ,→t , rht ,→t⟩,

PREVIEW
24.3 Variance
221
holds generically in t, which is evidently the case.
Unfortunately, Rule (24.12) also underwrites incorrect subtyping relationships, as well as some
correct ones. As an example of what goes wrong, consider the recursive types
τ′ = rec t is ⟨a ,→t ⇀nat , b ,→t ⇀int⟩
and
τ = rec t is ⟨a ,→t ⇀int , b ,→t ⇀int⟩.
We assume for the sake of the example that nat <: int, so that by using rule (24.12) we may derive
τ′ <: τ, which is incorrect. Let e : τ′ be the expression
fold( ⟨a ,→λ ( x : τ′ ) 4, b ,→λ ( x : τ′ ) q( ( unfold( x ) · a )( x ) )⟩),
where q : nat ⇀nat is the discrete square root function. Because τ′ <: τ, it follows that e : τ as
well, and hence
unfold( e ) : ⟨a ,→τ ⇀int , b ,→τ ⇀int⟩.
Now let e′ : τ be the expression
fold( ⟨a ,→λ ( x : τ ) -4, b ,→λ ( x : τ ) 0⟩).
(The important point about e′ is that the a method returns a negative number; the b method is of
no significance.) To finish the proof, observe that
( unfold( e ) · b )( e′ ) 7−→∗q( -4 ),
which is a stuck state. We have derived a well-typed program that “gets stuck”, refuting type
safety!
Rule (24.12) is therefore incorrect. But what has gone wrong? The error lies in the choice of a
single variable to stand for both recursive types, which does not correctly model self-reference. In
effect we are treating two distinct recursive types as if they were equal while checking their bodies
for a subtyping relationship. But this is clearly wrong! It fails to take account of the self-referential
nature of recursive types. On the left side the bound variable stands for the subtype, whereas on
the right the bound variable stands for the super-type. Confusing them leads to the unsoundness
just illustrated.
As is often the case with self-reference, the solution is to assume what we are trying to prove,
and check that this assumption can be maintained by examining the bodies of the recursive types.
To do so we use hypothetical judgments of the form ∆⊢τ′ <: τ, where ∆consists of hypotheses
t type and t <: τ that declares a fresh type variable t that is not otherwise declared in ∆. Using such
hypothetical judgments we may state the correct rule for subtyping recursive types as follows:
∆, t type, t′ type, t′ <: t ⊢τ′ <: τ
∆, t′ type ⊢τ′ type
∆, t type ⊢τ type
∆⊢rec t′ is τ′ <: rec t is τ
.
(24.13)
That is, to check whether rec t′ is τ′ <: rec t is τ, we assume that t′ <: t, because t′ and t stand for
the corresponding recursive types, and check that τ′ <: τ under this assumption. It is instructive
to check that the unsound subtyping example given above is not derivable using this rule: the
subtyping assumption is at odds with the contravariance of the function type in its domain.

PREVIEW
222
24.3 Variance
Quantified Types
Consider extending FPC with the universal and existential quantified types discussed in Chap-
ters 16 and 17. The variance principles for the quantifiers state that they are uniformly covariant
in the quantified types:
∆, t type ⊢τ′ <: τ
∆⊢∀( t . τ′ ) <: ∀( t . τ )
(24.14a)
∆, t type ⊢τ′ <: τ
∆⊢∃( t.τ′ ) <: ∃( t.τ )
(24.14b)
Consequently, we may derive the principle of substitution:
Lemma 24.1. If ∆, t type ⊢τ1 <: τ2, and ∆⊢τ type, then ∆⊢[τ/t]τ1 <: [τ/t]τ2.
Proof. By induction on the subtyping derivation.
It is easy to check that the above variance principles for the quantifiers are consistent with the
principle of subsumption. For example, a package of the subtype ∃( t.τ′ ) consists of a representa-
tion type ρ and an implementation e of type [ρ/t]τ′. But if t type ⊢τ′ <: τ, we have by substitution
that [ρ/t]τ′ <: [ρ/t]τ, and hence e is also an implementation of type [ρ/t]τ. So the package is also
of the supertype.
It is natural to extend subtyping to the quantifiers by allowing quantification over all subtypes
of a specified type; this is called bounded quantification.
∆, t type, t <: τ ⊢t <: τ
(24.15a)
∆⊢τ type
∆⊢τ <: τ
(24.15b)
∆⊢τ′′ <: τ′
∆⊢τ′ <: τ
∆⊢τ′′ <: τ
(24.15c)
∆⊢τ′
1 <: τ1
∆, t type, t <: τ′
1 ⊢τ2 <: τ′
2
∆⊢∀t <: τ1.τ2 <: ∀t <: τ′
1.τ′
2
(24.15d)
∆⊢τ1 <: τ′
1
∆, t type, t <: τ1 ⊢τ2 <: τ′
2
∆⊢∃t <: τ1.τ2 <: ∃t <: τ′
1.τ′
2
(24.15e)
Rule (24.15d) states that the universal quantifier is contravariant in its bound, whereas rule (24.15e)
states that the existential quantifier is covariant in its bound.

PREVIEW
24.4 Dynamics and Safety
223
24.4
Dynamics and Safety
There is a subtle assumption in the definition of product subtyping in Section 24.2, namely that the
same projection operation from an I-tuple applies also to a J-tuple, provided J ⊇I. But this need
not be the case. One could represent I-tuples differently from J-tuples at will, so that the meaning
of the projection at position i ∈I ⊆J is different in the two cases. Nothing rules out this possibility,
yet product subtyping relies on it not being the case. From this point of view product subtyping
is not well-justified, but one may instead consider that subtyping limits possible implementations
to ensure that it makes sense.
Similar considerations apply to sum types. An J-way case analysis need not be applicable to
an I-way value of sum type, even when I ⊆J and all the types in common agree. For example,
one might represent values of a sum type with a “small” index set in a way that is not applicable
for a “large” index set. In that case the “large” case analysis would not make sense on a value
of “small” sum type. Here again we may consider either that subtyping is not justified, or that it
imposes limitations on the implementation that are not otherwise forced.
These considerations merit careful consideration of the safety of languages with subtyping. As
an illustrative case we consider the safety of FPC enriched with product subtyping. The main con-
cern is that the subsumption rule obscures the “true” type of a value, complicating the canonical
forms lemma. Moreover, we assume that the same projection makes sense for a wider tuple than
a narrower one, provided that it is within range.
Lemma 24.2 (Structurality).
1. The tuple subtyping relation is reflexive and transitive.
2. The typing judgment Γ ⊢e : τ is closed under weakening and substitution.
Proof.
1. Reflexivity is proved by induction on the structure of types. Transitivity is proved by in-
duction on the derivations of the judgments τ′′ <: τ′ and τ′ <: τ to obtain a derivation of
τ′′ <: τ.
2. By induction on rules (10.3), augmented by rule (24.2).
Lemma 24.3 (Inversion).
1. If e · j : τ, then e : ⟨τi⟩i∈I, j ∈I, and τj <: τ.
2. If ⟨i ,→ei | i ∈I⟩: τ, then ⟨τ′
i ⟩i∈I <: τ where ei : τ′
i for each i ∈I.
3. If τ′ <: ⟨τj⟩j∈J, then τ′ = ⟨τ′
i ⟩i∈I for some I and some types τ′
i for i ∈I.
4. If ⟨τ′
i ⟩i∈I <: ⟨τj⟩j∈J, then J ⊆I and τ′
j <: τj for each j ∈J.
Proof. By induction on the subtyping and typing rules, paying special attention to rule (24.2).

PREVIEW
224
24.5 Notes
Theorem 24.4 (Preservation). If e : τ and e 7−→e′, then e′ : τ.
Proof. By induction on rules (10.4). For example, consider rule (10.4d), so that e = ⟨i ,→ei | i ∈I⟩· k
and e′ = ek. By Lemma 24.3 we have ⟨i ,→ei | i ∈I⟩: ⟨τj⟩j∈J, with k ∈J and τk <: τ. By another
application of Lemma 24.3 for each i ∈I there exists τ′
i such that ei : τ′
i and ⟨τ′
i ⟩i∈I <: ⟨τj⟩j∈J. By
Lemma 24.3 again, we have J ⊆I and τ′
j <: τj for each j ∈J. But then ek : τk, as desired. The
remaining cases are similar.
Lemma 24.5 (Canonical Forms). If e val and e : ⟨τj⟩j∈J, then e is of the form ⟨i ,→ei | i ∈I⟩, where
J ⊆I, and ej : τj for each j ∈J.
Proof. By induction on rules (10.3) augmented by rule (24.2).
Theorem 24.6 (Progress). If e : τ, then either e val or there exists e′ such that e 7−→e′.
Proof. By induction on rules (10.3) augmented by rule (24.2). The rule of subsumption is han-
dled by appeal to the inductive hypothesis on the premise of the rule. rule (10.4d) follows from
Lemma 24.5.
24.5
Notes
Subtyping is perhaps the most widely misunderstood concept in programming languages. Sub-
typing is principally a convenience, akin to type inference, that makes some programs simpler to
write. But the subsumption rule cuts both ways. Inasmuch as it allows the implicit passage from
τ′ to τ when τ′ is a subtype of τ, it also weakens the meaning of a type assertion e : τ to mean
that e has some type contained in the type τ. Subsumption precludes expressing the requirement
that e has exactly the type τ, or that two expressions jointly have the same type. And it is just this
weakness that creates so many of the difficulties with subtyping.
Much has been written about subtyping, often in relation to object-oriented programming.
Standard ML (Milner et al., 1997) is one of the first languages to make use of subtyping, in two
forms, called enrichment and realization. The former corresponds to product subtyping, and the
latter to the “forgetful” subtyping associated with type definitions (see Chapter 43). The first sys-
tematic studies of subtyping include those by Mitchell (1984); Reynolds (1980), and Cardelli (1988).
Pierce (2002) give a thorough account of subtyping, especially of recursive and polymorphic types,
and proves that subtyping for bounded impredicative universal quantification is undecidable.
Exercises
24.1. Check the variance of the type
( unit ⇀τ ) × ( τ ⇀unit ).
When viewed as a constructor with argument τ, is it covariant or contravariant? Give a
precise proof or counterexample in each case.

PREVIEW
24.5 Notes
225
24.2. Consider the two recursive types,
ρ1 ≜rec t is ⟨eq ,→( t ⇀bool )⟩,
and
ρ2 ≜rec t is ⟨eq ,→( t ⇀bool ) , f ,→bool⟩.
It is clear that ρ1 could not be a subtype of ρ2, because, viewed as a product after unrolling,
a value of the former type lacks a component that a value of the latter has. But is ρ2 a
subtype of ρ1? If so, prove it by exhibiting a derivation of this fact using the rules given
in Section 24.3. If not, give a counterexample showing that the suggested subtyping would
violate type safety.
24.3. Another approach to the dynamics of subtyping that ensures safety, but gives subsumption
dynamic significance, associates a witness, called a coercion, to each subtyping relation, and
inserts a coercion wherever subsumption is used. More precisely,
(a) Assign to each valid subtyping τ <: τ′ a coercion function χ : τ ⇀τ′ that transforms a
value of type τ into a value of type τ′.
(b) Interpret the subsumption rule as implicit coercion. Specifically, when τ <: τ′ is wit-
nessed by χ : τ ⇀τ′, applying subsumption to e : τ inserts an application of χ to obtain
χ( e ) : τ′.
Formulate this idea precisely for the case of a subtype relation generated by “width” sub-
typing for products, and the variance principles for product, sum and function types. Your
solution should make clear that it evades the tacit projection assumption mentioned above.
But there may be more than one coercion χ : τ ⇀τ′ corresponding to the subtyping τ <: τ′.
The meaning of a program would then depend on which coercion is chosen when subsump-
tion is used. If there is exactly one coercion for each subtyping relation, it is said to be
coherent. Is your coercion interpretation of product subtyping coherent? (A proper treatment
of coherence requires expression equivalence, which is discussed in Chapter 47.)

PREVIEW
226
24.5 Notes

PREVIEW
Chapter 25
Behavioral Typing
In Chapter 23 we demonstrated that dynamic typing is but a mode of use of static typing, one in
which dynamically typed values are of type dyn, a particular recursive sum type. A value of type
dyn is always of the form new[ c ]( e ), where c is its class and e is its underlying value. Importantly,
the class c determines the type of the underlying value of a dynamic value. The type system of the
hybrid language is rather weak in that every dynamically classified value has the same type, and
there is no mention of the class in its type. To correct this shortcoming it is common to enrich the
type system of the hybrid language so as to capture such information, for example as described in
Section 24.2.
In such a situation subtyping is used to resolve a fundamental tension between structure and
behavior in the design of type systems. On the one hand types determine the structure of a pro-
gramming language, and on the other serve as behavioral specifications of expressions written
in that language. Subtyping attempts to resolve this tension, unsuccessfully, by allowing certain
forms of retyping. Although subtyping works reasonably well for small examples, things get far
more complicated when we wish to specify the deep structure of a value, say that it is of a class c
and its underlying value is of another class d whose underlying value is a natural number. There is
no limit to the degree of specificity one may wish in such descriptions, which gives rise to endless
variations on type systems to accommodate various special situations.
Another resolution of the tension between structure and behavior in typing is to separate these
aspects by distinguishing types from type refinements. Type refinements specify the execution be-
havior of an expression of a particular type using specifications that capture whatever properties
are of interest, limited only by the difficulty of proving that a program satisfies the specification
given by a refinement.
Certain limited forms of behavioral specifications can express many useful properties of pro-
grams while remaining mechanically checkable. These include the fundamental behavioral prop-
erties determined by the type itself, but can be extended to include sharper conditions than just
these structural properties. In this chapter we will consider a particular notion of refinement tai-
lored to the hybrid language of Chapter 23. It is based on two basic principles:
1. Type constructors, such as product, sum, and function space, act on refinements of their com-

PREVIEW
228
25.1 Statics
ponent types to induce a refinement on the compound type formed by those constructors.
2. It is useful to track the class of a value of type dyn, and to assign multiple refinements to
specify distinct behaviors of expressions involving values of dynamic type.
We will formulate a system of refinements based on these principles that ensures that a well-
refined program cannot incur a run-time error arising from the attempt to cast a value to a class
other than its own.
25.1
Statics
We will develop a system of refinements for the extension with sums and products of the language
HPCF defined in Chapter 23 in which there are but two classes of values of type dyn, namely num
and fun.1 The syntax of refinements ϕ is given by the following grammar:
Ref
ϕ
::=
⊤{τ}
⊤τ
truth
∧{τ}( ϕ1 ; ϕ2 )
ϕ1 ∧τ ϕ2
conjunction
new[ num ]( ϕ )
num ! ϕ
dynamic number
new[ fun ]( ϕ )
fun ! ϕ
dynamic function
prod( ϕ1 ; ϕ2 )
ϕ1 × ϕ2
product
sum( ϕ1 ; ϕ2 )
ϕ1 + ϕ2
sum
parr( ϕ1 ; ϕ2 )
ϕ1 ⇀ϕ2
function
Informally, a refinement is a predicate specifying a property of the values of some type. Equiv-
alently, one may think of a refinement as a subset of the values of a type, those that satisfy the
specified property. To expose the dependence of refinements on types, the syntax of truth and
conjunction is parameterized by the type whose values they govern. In most cases the underlying
type is clear from context, in which case it is omitted. Note that the syntax of the product, sum,
and function refinements is exactly the same as the syntax of the types they govern, but they are
refinements, not types.
The judgment ϕ ⊑τ means that ϕ is a refinement of the type τ. It is defined by the following
rules:
⊤⊑τ
(25.1a)
ϕ1 ⊑τ
ϕ2 ⊑τ
ϕ1 ∧ϕ2 ⊑τ
(25.1b)
ϕ ⊑nat
num ! ϕ ⊑dyn
(25.1c)
ϕ ⊑dyn ⇀dyn
fun ! ϕ ⊑dyn
(25.1d)
1Of course, in a richer language there would be more classes than just these two, each with an associated type of the
underlying data that it classifies.

PREVIEW
25.1 Statics
229
ϕ1 ⊑τ1
ϕ2 ⊑τ2
ϕ1 × ϕ2 ⊑τ1 × τ2
(25.1e)
ϕ1 ⊑τ1
ϕ2 ⊑τ2
ϕ1 + ϕ2 ⊑τ1 + τ2
(25.1f)
ϕ1 ⊑τ1
ϕ2 ⊑τ2
ϕ1 ⇀ϕ2 ⊑τ1 ⇀τ2
(25.1g)
It is easy to see that each refinement refines a unique type, the underlying type of that refinement.
The concrete syntax num ! ϕ and fun ! ϕ is both concise and commonplace, but, beware, it tends to
obscure the critical distinctions between types, classes, and refinements.
The refinement satisfaction judgment, e ∈τ ϕ, where e : τ and ϕ ⊑τ, states that the well-typed
expression e exhibits the behavior specified by ϕ. The hypothetical form,
x1 ∈τ1 ϕ1, . . . , xn ∈τn ϕn ⊢e ∈τ ϕ,
constrains the expressions that may be substituted for a variable to satisfy its associated refinement
type (which could be the trivial refinement ⊤, that imposes no constraints). We write Φ for such
a finite sequence of refinement assumptions on variables, called a refinement context. Each such
Φ determines a typing context Γ given by x1 : τ1, . . . , xn : τn, specifying only the types of the
variables involved. We often write ΦΓ to state that Γ is the unique typing context determined by
Φ in this way.
The definition of the refinement satisfaction judgment makes use of an auxiliary judgment,
ϕ1 ≤τ ϕ2, where ϕ1 ⊑τ and ϕ2 ⊑τ, which we shall often just write as ϕ1 ≤ϕ2 when τ is clear
from context. This judgment is called refinement entailment. It states that the refinement ϕ1 is at
least as strong as, or no weaker than, the refinement ϕ2. Informally, this means that if e : τ satisfies ϕ1,
then it must also satisfy ϕ2. According to this interpretation, refinement entailment is reflexive and
transitive. The refinement ⊤, which holds of any well-typed expression, is greater than (entailed
by) any other refinement, and the conjunction of two refinements, ϕ1 ∧ϕ2 is the meet (greatest
lower bound) of ϕ1 and ϕ2. Because no value can be of two different classes, the conjunction of
num ! ϕ1 and fun ! ϕ2 entails any refinement at all. Finally, refinement entailment satisfies the same
variance principles given for subtyping in Section 24.3.
ϕ ⊑τ
ϕ ≤τ ϕ
(25.2a)
ϕ1 ≤τ ϕ2
ϕ2 ≤τ ϕ3
ϕ1 ≤τ ϕ3
(25.2b)
ϕ ⊑τ
ϕ ≤τ ⊤
(25.2c)
ϕ1 ⊑τ
ϕ2 ⊑τ
ϕ1 ∧ϕ2 ≤τ ϕ1
(25.2d)
ϕ1 ⊑τ
ϕ2 ⊑τ
ϕ1 ∧ϕ2 ≤τ ϕ2
(25.2e)

PREVIEW
230
25.1 Statics
ϕ ≤τ ϕ1
ϕ ≤τ ϕ2
ϕ ≤τ ϕ1 ∧ϕ2
(25.2f)
num ! ϕ1 ∧fun ! ϕ2 ≤dyn ϕ
(25.2g)
ϕ ≤nat ϕ′
num ! ϕ ≤dyn num ! ϕ′
(25.2h)
ϕ ≤dyn⇀dyn ϕ′
fun ! ϕ ≤dyn fun ! ϕ′
(25.2i)
ϕ1 ≤τ1 ϕ′
1
ϕ2 ≤τ2 ϕ′
2
ϕ1 × ϕ2 ≤τ1×τ2 ϕ′
1 × ϕ′
2
(25.2j)
ϕ1 ≤τ1 ϕ′
1
ϕ2 ≤τ2 ϕ′
2
ϕ1 + ϕ2 ≤τ1+τ2 ϕ′
1 + ϕ′
2
(25.2k)
ϕ′
1 ≤τ1 ϕ1
ϕ2 ≤τ2 ϕ′
2
ϕ1 ⇀ϕ2 ≤τ1⇀τ2 ϕ′
1 ⇀ϕ′
2
(25.2l)
For the sake of brevity we usually omit the type subscripts from refinements and the refinement
entailment relation.
We are now in a position to define the refinement satisfaction judgment, ΦΓ ⊢e ∈τ ϕ, in which
we assume that Γ ⊢e : τ. When such a satisfaction judgment holds, we say that e is well-refined,
a property that can be stated only for expressions that are well-typed. The goal is to ensure that
well-refined expressions do not incur (checked) run-time errors. In the present setting refinements
rule out casting a value of class c to a class c′ ̸= c. The formulation of satisfaction, though simple-
minded, will involve many important ideas.
To simplify the exposition, it is best to present the rules in groups, rather than all at once. The
first group consists of the rules that pertain to expressions independently of their types.
ΦΓ, x ∈τ ϕ ⊢x ∈τ ϕ
(25.3a)
Φ ⊢e ∈τ ϕ′
ϕ′ ≤τ ϕ
Φ ⊢e ∈τ ϕ
(25.3b)
Φ ⊢e ∈τ ϕ1
Φ ⊢e ∈τ ϕ2
Φ ⊢e ∈τ ϕ1 ∧ϕ2
(25.3c)
Φ, x ∈τ ϕ ⊢e ∈τ ϕ
Φ ⊢fix x : τ is e ∈τ ϕ
(25.3d)
Rule (25.3a) expresses the obvious principle that if a variable is assumed to satisfy a refinement ϕ,
then of course it does satisfy that refinement. As usual, the principle of substitution is admissible.
It states that if a variable is assumed to satisfy ϕ, then we may substitute for it any expression that

PREVIEW
25.1 Statics
231
does satisfy that refinement, and the resulting instance will continue to satisfy the same refinement
it had before the substitution.
Rule (25.3b) is analogous to the subsumption principle given in Chapter 24, although here
it has a subtly different meaning. Specifically, if an expression e satisfies a refinement ϕ′, and
ϕ′ is stronger than some refinement ϕ (as determined by rules (25.2)), then e must also satisfy the
refinement ϕ. This inference is simply a matter of logic: the judgment ϕ′ ≤ϕ states that ϕ′ logically
entails ϕ.
Rule (25.3c) expresses the logical meaning of conjunction. If an expression e satisfies both ϕ1
and ϕ2, then it also satisfies ϕ1 ∧ϕ2. Rule (25.3b) ensures that the converse holds as well, noting
that by rules (25.2d) and (25.2e) a conjunction is stronger than either of its conjuncts. Similarly, the
same rule ensures that if e satisfies ϕ, then it also satisfies ⊤, but we do not postulate that every
well-typed expression satisfies ⊤, for that would defeat the goal of ensuring that well-refined
expressions do not incur a run-time fault.
Rule (25.3d) states that refinements are closed under general recursion (formation of fixed
points). To show that fix x : τ is e satisfies a refinement ϕ, it suffices to show that e satisfies ϕ,
under the assumption that x, which stands for the recursive expression itself, satisfies ϕ. It is thus
obvious that non-terminating expressions, such as fix x : τ is x, satisfy any refinement at all. In
particular, such a divergent expression does not incur a run-time error, the guarantee we are after
with the present system of refinements.
The second group concerns the type dyn of classified values.
Φ ⊢e ∈nat ϕ
Φ ⊢num ! e ∈dyn num ! ϕ
(25.4a)
Φ ⊢e ∈dyn⇀dyn ϕ
Φ ⊢fun ! e ∈dyn fun ! ϕ
(25.4b)
Φ ⊢e ∈dyn num ! ϕ
Φ ⊢e @ num ∈nat ϕ
(25.4c)
Φ ⊢e ∈dyn fun ! ϕ
Φ ⊢e @ fun ∈dyn⇀dyn ϕ
(25.4d)
Φ ⊢e ∈dyn ⊤
Φ ⊢num ? e ∈bool ⊤
(25.4e)
Φ ⊢e ∈dyn ⊤
Φ ⊢fun ? e ∈bool ⊤
(25.4f)
Rules (25.4a) and (25.4b) state that a newly-created value of class c satisfies the refinement of the
type dyn stating this fact, provided that the underlying value satisfies the given refinement ϕ of
the type associated to that class (nat for num and dyn ⇀dyn for fun).
Rules (25.4c) and (25.4d) state that a value of type dyn may only be safely cast to a class c if the
value is known statically to be of this class. This condition is stated in the premises of these rules,
which require the class of the cast value to be known and suitable for the cast. The result of a well-
refined cast satisfies the refinement given to its underlying value. It is important to realize that in

PREVIEW
232
25.1 Statics
the quest to avoid run-time faults, it is not possible to cast a value whose only known refinement is
⊤, which imposes no restrictions on it. This limitation is burdensome, because in many situations
it is not possible to determine statically what is the class of a value. We will return to this critical
point shortly.
Rules (25.4e) and (25.4f) compute a boolean based on the class of its argument. We shall have
more to say about this in Section 25.2.
The third group of rules govern nullary and binary product types.
Φ ⊢⟨⟩∈unit ⊤
(25.5a)
Φ ⊢e1 ∈τ1 ϕ1
Φ ⊢e2 ∈τ2 ϕ2
Φ ⊢⟨e1, e2⟩∈τ1×τ2 ϕ1 × ϕ2
(25.5b)
Φ ⊢e ∈τ1×τ2 ϕ1 × ϕ2
Φ ⊢e · l ∈τ1 ϕ1
(25.5c)
Φ ⊢e ∈τ1×τ2 ϕ1 × ϕ2
Φ ⊢e · r ∈τ2 ϕ2
(25.5d)
Rule (25.5a) states the obvious: the null-tuple is well-refined by the trivial refinement. Because
unit contains only one element, little else can be said about it. Rule (25.5b) states that a pair satis-
fies a product of refinements if each component satisfies the corresponding refinement. Rules (25.5c)
and (25.5d) state the converse.
The fourth group of rules govern nullary and binary sum types.
Φ ⊢e ∈void ϕ′
Φ ⊢e ∈void ϕ
(25.6a)
Φ ⊢e1 ∈τ1 ϕ1
Φ ⊢l · e1 ∈τ1+τ2 ϕ1 + ϕ2
(25.6b)
Φ ⊢e2 ∈τ2 ϕ2
Φ ⊢r · e2 ∈τ1+τ2 ϕ1 + ϕ2
(25.6c)
Φ ⊢e ∈τ1+τ2 ϕ1 + ϕ2
Φ, x1 ∈τ1 ϕ1 ⊢e1 ∈τ ϕ
Φ, x2 ∈τ2 ϕ2 ⊢e2 ∈τ ϕ
Φ ⊢case e {l · x1 ,→e1 | r · x2 ,→e2} ∈τ ϕ
(25.6d)
Rule (25.6a) states that if an expression of type void satisfies some refinement (and hence is error-
free), then it satisfies every refinement (of type void), because there are no values of this type, and
hence, being error-free, must diverge.
Rules (25.6b) and (25.6c) are similarly motivated. If e1 satisfies ϕ1, then l · e1 satisfies ϕ1 + ϕ2 for
any refinement ϕ2, precisely because the latter refinement is irrelevant to the injection. Similarly,
the right injection is independent of the refinement of the left summand.
Rule (25.6d) is in some respects the most interesting rule of all, one that we shall have occa-
sion to revise shortly. The salient feature of this rule is that it propagates refinement information

PREVIEW
25.1 Statics
233
about the injected value into the corresponding branch by stating an assumption about the bound
variable of each branch. But it does not propagate any information into the branches about what is
known about e in each branch, namely that in the first branch e must be of the form l · e1 and in
the second branch e must be of the form r · e2.
The failure to propagate this information may seem harmless, but it is, in fact, quite restrictive.
To see why, consider the special case of the type bool, which is defined in Section 11.3.2 to be
unit + unit. The conditional expression if e then e1 else e2 is defined to be a case analysis in
which there is no associated data to pass into the branches of the conditional. So, within the then
branch e1 it is not known statically that e is in fact true, nor is it known within the else branch e2
that e is in fact false.
The fifth group of rules governs the function type.
Φ, x ∈τ1 ϕ1 ⊢e2 ∈τ2 ϕ2
Φ ⊢λ ( x : τ1 ) e2 ∈τ1⇀τ2 ϕ1 ⇀ϕ2
(25.7a)
Φ ⊢e1 ∈τ2⇀τ ϕ2 ⇀ϕ
e2 ∈τ2 ϕ2
Φ ⊢e1( e2 ) ∈τ ϕ
(25.7b)
Rule (25.7a) states that a λ-abstraction satisfies a function refinement if its body satisfies the range
refinement, under the assumption that its argument satisfies the domain refinement. This is only
to be expected.
Rule (25.7b) states the converse. If an expression of function type satisfies a function refine-
ment, and it is applied to an argument satisfying the domain refinement, then the application
must satisfy the range refinement.
The last group of rules govern the type nat:
Φ ⊢z ∈nat ⊤
(25.8a)
Φ ⊢e ∈nat ⊤
Φ ⊢s( e ) ∈nat ⊤
(25.8b)
Φ ⊢e ∈nat ⊤
Φ ⊢e0 ∈τ ϕ
Φ, x ∈nat ⊤⊢e1 ∈τ ϕ
Φ ⊢ifz e {z ,→e0 | s( x ) ,→e1} ∈τ ϕ
(25.8c)
These rules are completely unambitious: they merely restate the typing rules as refinement rules
that impose no requirements and make no guarantees of any properties of the natural numbers.
One could envision adding refinements of the natural numbers, for instance stating that a natural
number is known to be zero or non-zero, for example.
To get a feel for the foregoing rules, it is useful to consider some simple examples. First, we
may combine rules (25.3b) and (25.4a) to derive the judgment
num ! n ∈dyn ⊤
for any natural number n. That is, we may “forget” the class of a value by applying subsumption,
and appealing to rule (25.2c). Second, such reasoning is essential in stating refinement satisfac-
tion for a boolean conditional (or, more generally, any case analysis). For example, the following

PREVIEW
234
25.2 Boolean Blindness
judgment is directly derivable without subsumption:
Φ, x ∈bool ⊤⊢if x then ( num ! z ) else ( num ! s( z ) ) ∈dyn num ! ⊤.
But the following judgment is only derivable because we may weaken knowledge of the class of a
value in each branch to a common refinement, in this case the weakest refinement of all:
Φ, x ∈bool ⊤⊢if x then ( num ! z ) else ( fun ! ( λ ( y : dyn ) y ) ) ∈dyn ⊤.
In general conditionals attenuate the information we have about a value, except in those cases
where the same information is known about both branches. Conditionals are the main source of
lossiness in checking refinement satisfaction.
Conjunction refinements are used to express multiple properties of a single expression. For
example, the identity function on the type dyn satisfies the conjunctive refinement
( num ! ⊤⇀num ! ⊤) ∧( fun ! ⊤⇀fun ! ⊤).
The occurrences of ⊤in the first conjunct refine the type nat, whereas the occurrences in the
second conjunct refine the type dyn ⇀dyn. It is a good exercise to check that λ ( x : dyn ) x satisfies
the above refinement of the type dyn ⇀dyn.
25.2
Boolean Blindness
Let us consider a very simple example that exposes a serious disease suffered by many program-
ming languages, a condition called boolean blindness. Suppose that x is a variable of type dyn with
refinement ⊤, and consider the expression
if ( num ? x ) then x @ num else z.
Although it is clear that this expression has type nat, it is nevertheless ill-refined (satisfies no re-
finement), even though it does not incur a run-time error. Specifically, within the then branch,
we as programmers know that x is a value of class num, but this fact is not propagated into the
then branch by rules (25.6) (of which the boolean conditional is a special case). Consequently,
rule (25.4c) does not apply (formally we do not know enough about x to cast it safely), so the
expression is ill-refined. The branches of the conditional are “blind” both to the outcome and the
meaning of the boolean value computed by the test whether x is of class num.
Boolean blindness is endemic among programming languages. The difficulty is that a boolean
carries exactly one bit of information, which is not sufficient to capture the meaning of that bit.
A boolean, which is (literally) a bit of data, ought to be distinguished from a proposition, which
expresses a fact. Taken by itself, a boolean conveys no information other than its value, whereas
a proposition expresses the reasoning that goes into ensuring that a piece of code is correct. In
terms of the above example knowing that num ? x evaluates dynamically to the boolean true is not
connected statically with the fact that the class of x is num or not. That information lives elsewhere,
in the specification of the class test primitive. The question is how to connect the boolean value
returned by the class test with relevant facts about whether the class of x is num or not.

PREVIEW
25.2 Boolean Blindness
235
Because the purpose of a type refinement system is precisely to capture such facts in a form that
can be stated and verified, one may suspect that the difficulty with the foregoing example is that
the system of refinements under consideration is too weak to capture the property that we need
to ensure that run-time faults do not occur. The example shows that something more is needed
if type refinement is to be useful, so we first consider what might be involved in enriching the
definition of refinement satisfaction to ensure that the proper connections are made. The matter
boils down to two issues:
1. Propagating that num ? x returned true into the then branch, and that it returned false into
the else branch.
2. Connecting facts about the return value of a test to facts about the value being tested.
These are, in the present context, distinct aspects of the problem. Let us consider them in turn. The
upshot of the discussion will be to uncover a design flaw in casting, and to suggest an alternative
formulation that does not suffer from boolean blindness.
To address the problem as stated, we first need to enrich the language of refinements to include
true ⊑bool and false ⊑bool, stating that a boolean is either true or false, respectively. That
way we can hope to express facts about boolean-valued expressions as refinement judgments.
But what is the refinement rule for the boolean conditional? As a first guess one might consider
something like the following:
Φ, e ∈bool true ⊢e1 ∈τ ϕ
Φ, e ∈bool false ⊢e2 ∈τ ϕ
Φ ⊢if e then e1 else e2 ∈τ ϕ
(25.9)
Such a rule is a bit unusual in that it introduces a hypothesis about an expression, rather than a
variable, but let us ignore that for the time being and press on.
Having re-formulated the refinement rule for the conditional, we immediately run into another
problem: how to deduce that x ∈dyn num ! ⊤, which is required for casting, from the assumption
that num ? x ∈bool true? One way to achieve this is by modifying the refinement rule for the
conditional to account for the special case in which the boolean expression is literally num ? e for
some e. If it is, then we propagate into the then branch the additional assumption e ∈dyn num ! ⊤,
but if it is not, then no fact about the class of e is propagated into the else branch.2
This change is enough to ensure that the example under consideration is well-refined. But
what if the boolean on which we are conditioning is not literally num ? e, but merely implies the
test would return true? How then are we to make the connection between such expressions and
relevant facts about the class of e? Clearly there is no end to the special cases we could consider to
restore vision to refinements, but there is no unique best solution.
All is not lost, however! The foregoing analysis suggests that the fault lies not in our refine-
ments, but in our language design. The sole source of run-time errors is the “naked cast” that
attempts to extract the underlying natural number from a value of type dyn—and signals a run-
time error if it cannot do so because the class of the value is not num. The boolean-valued test for
the class of a value seems to provide a way to avoid these errors. But, as we have just seen, the true
2For the special case of there being exactly two classes, we could propagate that the class of e is fun into the else branch,
but this approach does not generalize.

PREVIEW
236
25.3 Refinement Safety
cause of the problem is the attempt to separate the test from the cast. We may, instead, combine
these into a single form, say
ifofcl[ num ]( e ; x0 . e0 ; e1 ),
that tests whether the class of e is num, and, if so, passes the underlying number to e0 by substituting
it for x0, and otherwise evaluates e1. No run-time error is possible, and hence no refinements are
needed to ensure that it cannot occur.
But does this not mean that type refinements are pointless? Not at all. It simply means that
in some cases verification methods, such as type refinements, are needed solely to remedy a lan-
guage design flaw, and not provide a useful tool for programmers to help express and verify the
correctness of their programs. We may still wish to express invariants such as the property that a
particular function maps values of class c into values of class c′, simply for the purpose of stating
a programmer’s intention. Or we may enrich the system of refinements to track properties such
as whether a number is even or odd, and to state conditions such as that a given function maps
evens to odds and odds to evens. There is no limit, in principle, to the variations and extensions
to help ensure that programs behave as expected.
25.3
Refinement Safety
The judgment ΦΓ ⊢e ∈τ ϕ presupposes that Γ ⊢e : τ, so by adapting the proofs given earlier, we
may prove type preservation and progress for well-typed terms.
Theorem 25.1 (Type Safety). Suppose that e : τ for closed e.
1. If e 7−→e′, then e′ : τ.
2. Either e err, or e val, or there exists e′ such that e 7−→e′.
The proof of progress requires a canonical forms lemma, which for the type dyn is stated as
follows:
Lemma 25.2 (Canonical Forms). Suppose that e : dyn and e val. Then either e = num ! e′ or e = fun ! e′
for some e′.
The expression e′ would also be a value under an eager dynamics, but need not be under a lazy
dynamics. The proof of Lemma 25.2 proceeds as usual, by analyzing the typing rules for values.
The goal of the refinement system introduced in Section 25.1 is to ensure that errors cannot arise
in a well-refined program. To show this, we first show that the dynamics preserves refinements.
Lemma 25.3. Suppose that e val and e ∈dyn ϕ. If ϕ ≤num ! ϕ′, then e = num ! e′, where e′ ∈nat ϕ′, and if
ϕ ≤fun ! ϕ′, then e = fun ! e′, where e′ ∈dyn⇀dyn ϕ′.
Proof. The proof requires Lemma 25.2 to characterize the possible values of dyn, and an analysis
of the refinement satisfaction rules. The lemma accommodates rule (25.3b), which appeals to the
transitivity of refinement entailment.

PREVIEW
25.4 Notes
237
Theorem 25.4 (Refinement Preservation). If e ∈τ ϕ and e 7−→e′, then e′ ∈τ ϕ.
Proof. We know by the preceding theorem that e′ : τ. To show that e′ ∈τ ϕ we proceed by induction
on the definition of refinement satisfaction given in Section 25.1. The type-independent group,
rules (25.3), are all easily handled, apart from the rule for fixed points, which requires an appeal
to a substitution lemma, just as in Theorem 19.2. The remaining groups are all handled easily,
bearing in mind that an incorrect expression cannot make a transition.
Theorem 25.5 (Refinement Error-Freedom). If e ∈τ ϕ, then ¬(e err).
Proof. By induction on the definition of refinement satisfaction. The only interesting cases are
rules (25.4c) and (25.4d), which are handled by appeal to Lemma 25.3 in the case that the cast
expression is a value.
Corollary 25.6 (Refinement Safety). If e ∈τ ϕ, then either e val or there exists e′ such that e′ ∈τ ϕ and
e 7−→e′. In particular, ¬(e err).
Proof. By Theorems 25.1, 25.4, and 25.5.
25.4
Notes
The distinction between types and refinements is fundamental, yet the two are often conflated.
Types determine the structure of a programming language, including its statics and dynamics;
refinements specify the behavior of well-typed programs. In full generality the satisfaction judg-
ment e ∈τ ϕ need not be decidable, whereas it is sensible to insist that the typing judgment e : τ
be decidable. The refinement system presented in this chapter is decidable, but one may consider
many notions of refinement that are not. For example, one may postulate that if e ∈τ ϕ and that e′
is indistinguishable from e by any program in the language,3 then e′ ∈τ ϕ. In contrast such a move
is not sensible in a type system, because the dynamics is derived from the statics by the inversion
principle. Therefore, refinement is necessarily posterior to typing.
The syntactic formulation of type refinements considered in this chapter was originally given
by Freeman and Pfenning (1991), and extended by Davies and Pfenning (2000); Davies (2005),
Xi and Pfenning (1998), Dunfield and Pfenning (2003), and Mandelbaum et al. (2003). A more
general semantic formulation of type refinement was given explicitly by Denney (1998) in the style
of the realizability interpretation of type theory on which NuPRL (Constable, 1986) is based. (See
the survey by van Oosten (2002) for the history of the realizability interpretations of constructive
logic.)
3See Chapter 47 for a precise definition and development of this concept.

PREVIEW
238
25.4 Notes
Exercises
25.1. Show that if ϕ1 ≤ϕ′
1 and ϕ2 ≤ϕ′
2, then ϕ1 ∧ϕ2 ≤ϕ′
1 ∧ϕ′
2.
25.2. Show that ϕ ≤ϕ′ iff for every ϕ′′, if ϕ′′ ≤ϕ, then ϕ′′ ≤ϕ′. (This property of entailment is an
instance of the more general Yoneda Lemma in category theory.)
25.3. Extend the system of refinements to recursive types by introducing a refinement fold( ϕ )
that classifies values of recursive type rec t is τ in terms of a refinement of the unfolding of
that recursive type.
25.4. Consider the following two forms of refinement for sum types, summand refinements:
ϕ1 ⊑τ1
l · ϕ1 ⊑τ1 + τ2
(25.10a)
ϕ2 ⊑τ2
r · ϕ2 ⊑τ1 + τ2
(25.10b)
Informally, l · ϕ1 classifies expressions of type τ1 + τ2 that lie within the left summand and
whose underlying value satisfies ϕ1, and similarly for r · ϕ2.
(a) State entailment rules governing summand refinements.
(b) State refinement rules assigning summand refinements to the introduction forms of a
sum type.
(c) Give rules for the case analysis construct using summand refinements to allow unreach-
able branches to be disregarded during refinement checking.
(d) Modify rule (25.6d) so that the information “learned” by examining the value of e at
execution time is propagated into the appropriate branch of the case analysis.
Check the importance of this extension to the prospects for a cure for Boolean blindness.
25.5. Using the preceding exercise, derive the refinements num ! ϕ and fun ! ϕ from the other refine-
ment rules, including the refinement fold( ϕ ) considered in Exercise 25.3.
25.6. Show that the addition function (23.8), a value of type dyn, satisfies the refinement
fun ! ( num ! ⊤⇀fun ! ( num ! ⊤⇀num ! ⊤) ),
stating that
(a) It is itself a value of class fun.
(b) The so-classified function maps a value of class num to a result, if any, of class fun.
(c) The so-classified function maps a value of class num to a result, if any, of class num.
This description exposes the hidden complexity in the superficial simplicity of a uni-typed
language.

PREVIEW
25.4 Notes
239
25.7. Revisit the optimization process of the addition function carried out in Section 23.3 in view
of your answer to Exercise 25.6. Show that the validity of the optimizations is guaranteed by
the satisfaction of the stated type refinement for addition.

PREVIEW
240
25.4 Notes

PREVIEW
Part XI
Dynamic Dispatch

PREVIEW

PREVIEW
Chapter 26
Classes and Methods
It often arises that the values of a type are partitioned into a variety of classes, each classifying data
with distinct internal structure. A simple example is provided by the type of points in the plane,
which are classified according to whether they are represented in cartesian or polar form. Both are
represented by a pair of real numbers, but in the cartesian case these are the x and y coordinates
of the point, whereas in the polar case these are its distance r from the origin and its angle θ with
the polar axis. A classified value is an object, or instance, of its class. The class determines the type
of the classified data, the instance type of the class; the classified data itself is the instance data of the
object.
Methods are functions that act on classified values. The behavior of a method is determined
by the class of its argument. The method dispatches on the class of the argument.1 Because the
selection is made at run-time, it is called dynamic dispatch. For example, the squared distance of
a point from the origin is calculated differently according to whether the point is represented in
cartesian or polar form. In the former case the required distance is x2 + y2, whereas in the latter
it is simply r2. Similarly, the quadrant of a cartesian point can be determined by examining the
sign of its x and y coordinates, and the quadrant of a polar point can be calculated by taking the
integral part of the angle θ divided by π/2.
Dynamic dispatch is often described in terms of a particular implementation strategy, which
we will call the class-based organization. In this organization each object is represented by a vec-
tor of methods specialized to the class of that object. We may equivalently use a method-based
organization in which each method branches on the class of an object to determine its behavior.
Regardless of the organization used, the fundamental idea is that (a) objects are classified, and (b)
methods dispatch on the class of an object. The class-based and method-based organizations are
interchangeable, and, in fact, related by a natural duality between sum and product types. We
explain this symmetry by focusing first on the behavior of each method on each object, which is
given by a dispatch matrix. From this we derive both a class-based and a method-based organiza-
tion in such a way that their equivalence is obvious.
1More generally, we may dispatch on the class of multiple arguments simultaneously. We concentrate on single dispatch
for the sake of simplicity.

PREVIEW
244
26.1 The Dispatch Matrix
26.1
The Dispatch Matrix
Because each method acts by dispatch on the class of its argument, we may envision the entire
system of classes and methods as a dispatch matrix edm whose rows are classes, whose columns are
methods, and whose (c, d)-entry defines the behavior of method d acting on an argument of class
c, expressed as a function of the instance data of the object. Thus, the dispatch matrix has a type
of the form
⟨⟨τc ⇀ρd⟩d∈D⟩c∈C,
where C is the set of class names, D is the set of method names, τc is the instance type associated
with class c and ρd is the result type of method d. The instance type is the same for all methods
acting on a given class, and the result type is the same for all classes acted on by a given method.
As an illustrative example, let us consider the type of points in the plane classified into two
classes, cart and pol, corresponding to the cartesian and polar representations. The instance data
for a cartesian point has the type
τcart = ⟨x ,→float , y ,→float⟩,
and the instance data for a polar point has the type
τpol = ⟨r ,→float , th ,→float⟩.
Consider two methods acting on points, dist and quad, which compute, respectively, the
squared distance of a point from the origin and the quadrant of a point. The squared distance
method is given by the tuple edist = ⟨cart ,→ecart
dist, pol ,→epol
dist⟩of type
⟨cart ,→τcart ⇀ρdist , pol ,→τpol ⇀ρdist⟩,
where ρdist = float is the result type,
ecart
dist = λ ( u : τcart ) ( u · x )2 + ( u · y )2
is the squared distance computation for a cartesian point, and
epol
dist = λ ( v : τpol ) ( v · r )2
is the squared distance computation for a polar point. Similarly, the quadrant method is given by
the tuple equad = ⟨cart ,→ecart
quad, pol ,→epol
quad⟩of type
⟨cart ,→τcart ⇀ρquad , pol ,→τpol ⇀ρquad⟩,
where ρquad = [I , II , III , IV] is the type of quadrants, and ecart
quad and epol
quad are expressions that
compute the quadrant of a point in rectangular and polar forms, respectively.
Now let C = { cart, pol } and let D = { dist, quad }, and define the dispatch matrix edm to
be the value of type
⟨⟨τc ⇀ρd⟩d∈D⟩c∈C

PREVIEW
26.1 The Dispatch Matrix
245
such that, for each class c and method d,
edm · c · d 7−→∗ec
d.
That is, the entry in the dispatch matrix edm for class c and method d defines the behavior of that
method acting on an object of that class.
Dynamic dispatch is an abstraction given by the following components:
• An abstract type tobj of objects, which are classified by the classes on which the methods act.
• An operation new[ c ]( e ) of type tobj that creates an object of the class c with instance data
given by the expression e of type τc.
• An operation e ⇐d of type ρd that invokes method d on the object given by the expression e
of type tobj.
These operations must satisfy the defining characteristic of dynamic dispatch,
( new[ c ]( e ) ) ⇐d 7−→∗ec
d( e ),
which states that invoking method d on an object of class c with instance data e amounts to apply-
ing ec
d, the code in the dispatch matrix for class c and method d to the instance data e.
In other words dynamic dispatch is an abstract type with interface given by the existential type
∃( tobj.⟨new ,→⟨τc ⇀tobj⟩c∈C , snd ,→⟨tobj ⇀ρd⟩d∈D⟩).
(26.1)
There are two main ways to implement this abstract type. The class-based organization, defines
objects as tuples of methods, and creates objects by specializing the methods to the given instance
data. The method-based organization creates objects by tagging the instance data with the class,
and defines methods by examining the class of the object. These two organizations are isomorphic
to one another, and hence can be interchanged at will. Nevertheless, many languages favor one
representation over the other, asymmetrizing an inherently symmetric situation.
The abstract type (26.1) calls attention to shortcoming of dynamic dispatch, namely that a mes-
sage can be sent to exactly one object at a time. This viewpoint seems natural in certain cases,
such as discrete event simulation in the language Simula-67. But often it is essential to act on sev-
eral classes of object at once. For example, the multiplication of a vector by a scalar combines the
elements of a field and a commutative monoid; there is no natural way to associate scalar multipli-
cation with either the field or the monoid, nor any way to anticipate that particular combination.
Moreover, the multiplication is not performed by checking at run-time that one has a scalar and a
vector in hand, for there is nothing inherent in a scalar or a vector that marks them as such. The
right tool for handling such situations is a module system (Chapters 44 and 45), and not dynamic
dispatch. The two mechanisms serve different purposes, and complement each other.
The same example serves to refute a widely held fallacy, namely that the values of an abstract
type cannot be heterogeneous. It is sometimes said that an abstract type of complex numbers must
commit to a single representation, say rectangular, and cannot accommodate multiple representa-
tions. This is a fallacy. Although it is true that an abstract type defines a single type, it is wrong
to say that only one representation of objects is possible. The abstract type can be implemented as
a sum, and the operations may correspondingly dispatch on the summand to compute the result.
Dynamic dispatch is a mode of use of data abstraction, and therefore cannot be opposed to it.

PREVIEW
246
26.2 Class-Based Organization
26.2
Class-Based Organization
The class-based organization starts with the observation that the dispatch matrix can be reorga-
nized to “factor out” the instance data for each method acting on that class to obtain the class vector
ecv of type
τcv ≜⟨τc ⇀(⟨ρd⟩d∈D)⟩c∈C.
Each entry of the class vector consists of a constructor that determines the result of each of the
methods when acting on given instance data.
An object has the type ρ = ⟨ρd⟩d∈D consisting of the product of the result types of the methods.
For example, in the case of points in the plane, the type ρ is the product type
⟨dist ,→ρdist , quad ,→ρquad⟩.
Each component specifies the result of the methods acting on that object.
The message send operation e ⇐d is just the projection e · d. So, in the case of points in the
plane, e ⇐dist is the projection e · dist, and similarly e ⇐quad is the projection e · quad.
The class-based organization combines the implementation of each class into a class vector ecv a
tuple of type τcv consisting of the constructor of type τc ⇀ρ for each class c ∈C. The class vector
is defined by ecv = ⟨c ,→c ,→ec | c ∈C⟩, where for each c ∈C the expression ec is
λ ( u : τc ) ⟨d ,→d ,→edm · c · d( u ) | d ∈D⟩.
For example, the constructor for the class cart is the function ecart given by the expression
λ ( u : τcart ) ⟨dist ,→edm · cart · dist( u ), quad ,→edm · cart · quad( u )⟩.
Similarly, the constructor for the class pol is the function epol given by the expression
λ ( u : τpol ) ⟨dist ,→edm · pol · dist( u ), quad ,→edm · pol · quad( u )⟩.
The class vector ecv in this case is the tuple ⟨cart ,→ecart, pol ,→epol⟩of type ⟨cart ,→τcart ⇀ρ ,
pol ,→τpol ⇀ρ⟩.
An object of a class is obtained by applying the constructor for that class to the instance data:
new[ c ]( e ) ≜ecv · c( e ).
For example, a cartesian point is obtained by writing new[ cart ]( ⟨x ,→x0, y ,→y0⟩), which is de-
fined by the expression
ecv · cart( ⟨x ,→x0, y ,→y0⟩).
Similarly, a polar point is obtained by writing new[ pol ]( r ,→r0, th ,→θ0 ), which is defined by the
expression
ecv · pol( ⟨r ,→r0, th ,→θ0⟩).
It is easy to check for this organization of points that for each class c and method d, we may derive
( new[ c ]( e ) ) ⇐d 7−→∗( ecv · c( e ) ) · d
7−→∗edm · c · d( e ).
That is, the message send evokes the behavior of the given method on the instance data of the
given object.

PREVIEW
26.3 Method-Based Organization
247
26.3
Method-Based Organization
The method-based organization starts with the transpose of the dispatch matrix, which has the type
⟨⟨τc ⇀ρd⟩c∈C⟩d∈D.
By observing that each row of the transposed dispatch matrix determines a method, we obtain the
method vector emv of type
τmv ≜⟨([τc]c∈C) ⇀ρd⟩d∈D.
Each entry of the method vector consists of a dispatcher that determines the result as a function of
the instance data associated with a given object.
An object is a value of type τ = [τc]c∈C, the sum over the classes of the instance types. For
example, the type of points in the plane is the sum type
[cart ,→τcart , pol ,→τpol].
Each point is labeled with its class, specifying its representation as having either cartesian or polar
form.
An object of a class c is just the instance data labeled with its class to form an element of the
object type:
new[ c ]( e ) ≜c · e.
For example, a cartesian point with coordinates x0 and y0 is given by the expression
new[ cart ]( ⟨x ,→x0, y ,→y0⟩) ≜cart · ⟨x ,→x0, y ,→y0⟩.
Similarly, a polar point with distance r0 and angle θ0 is given by the expression
new[ pol ]( ⟨r ,→r0, th ,→θ0⟩) ≜pol · ⟨r ,→r0, th ,→θ0⟩.
The method-based organization consolidates the implementation of each method into the method
vector emv of type τmv defined by ⟨d ,→d ,→ed | d ∈D⟩, where for each d ∈D the expression
ed : τ ⇀ρd is
λ ( this : τ ) case this {c · u ,→edm · c · d( u ) | c ∈C}.
Each entry in the method vector is a dispatch function that defines the action of that method on each
class of object.
In the case of points in the plane, the method vector has the product type
⟨dist ,→τ ⇀ρdist , quad ,→τ ⇀ρquad⟩.
The dispatch function for the dist method has the form
λ ( this : τ ) case this {cart · u ,→edm · cart · dist( u ) | pol · v ,→edm · pol · dist( v )},
and the dispatch function for the quad method has the similar form
λ ( this : τ ) case this {cart · u ,→edm · cart · quad( u ) | pol · v ,→edm · pol · quad( v )}.

PREVIEW
248
26.4 Self-Reference
The message send operation e ⇐d applies the dispatch function for method d to the object e:
e ⇐d ≜emv · d( e ).
Thus we have, for each class c and method d
( new[ c ]( e ) ) ⇐d 7−→∗emv · d( c · e )
7−→∗edm · c · d( e )
The result is, of course, the same as for the class-based organization.
26.4
Self-Reference
It is often useful to allow methods to create new objects or to send messages to objects. It is not
possible to do so using the simple dispatch matrix described in Section 26.1, for the simple reason
that there is no provision for self-reference within its entries. This deficiency may be remedied
by changing the type of the entries of the dispatch matrix to account for sending messages and
creating objects, as follows:
⟨⟨∀( tobj . τcv ⇀τmv ⇀τc ⇀ρd )⟩d∈D⟩c∈C.
The type variable tobj represents the abstract object type.2 The types τcv and τmv, are, respectively,
the type of the class and method vectors, defined in terms of the abstract type of objects tobj. They
are defined by the equations
τcv ≜⟨(τc ⇀tobj)⟩c∈C
and
τmv ≜⟨(tobj ⇀ρd)⟩d∈D.
The component of the class vector corresponding to a class c is a constructor that builds a value
of the abstract object type tobj from the instance data for c. The component of the method vector
corresponding to a method d is a dispatcher that yields a result of type ρd when applied to a value
of the abstract object type tobj.
In accordance with the revised type of the dispatch matrix the behavior associated to class c
and method d has the form
Λ( tobj ) λ ( cv : τcv ) λ ( mv : τmv ) λ ( u : τc ) ec
d.
The arguments cv and mv are used to create new objects and to send messages to objects. Within
the expression ec
d an object of class c′ with instance data e′ is created by writing cv · c′( e′ ), which
selects the appropriate constructor from the class vector cv and applies it to the given instance data.
The class c′ may well be the class c itself; this is one form of self-reference within ec
d. Similarly,
2The variable tobj is chosen not to occur in any τc or ρd. This restriction can be relaxed; see Exercise 26.4.

PREVIEW
26.4 Self-Reference
249
within ec
d a method d′ is invoked on e′ by writing mv · d′( e′ ). The method d′ may well be the
method d itself; this is another aspect of self-reference within ec
d.
To account for self-reference in the method-based organization, the method vector emv will be
defined to have the self-referential type self( [τ/tobj]τmv ) in which the object type τ is, as before,
the sum of the instance types of the classes [τc]c∈C. The method vector is defined by the following
equation:
emv ≜self mv is ⟨d ,→d ,→λ ( this : τ ) case this {c · u ,→edm · c · d[ τ ]( e′
cv )( e′
mv )( u ) | c ∈C} | d ∈D⟩,
where
e′
cv ≜⟨c ,→c ,→λ ( u : τc ) c · u | c ∈C⟩: [τ/tobj]τcv.
and
e′
mv ≜unroll( mv ) : [τ/tobj]τmv.
Object creation is defined by the equation
new[ c ]( e ) ≜c · e : τ
and message send is defined by the equation
e ⇐d ≜unroll( emv ) · d( e ) : ρd.
To account for self-reference in the class-based organization, the class vector ecv will be defined
to have the type self( [ρ/tobj]τcv ) in which the object type ρ is, as before, the product of the result
types of the methods ⟨ρd⟩d∈D. The class vector is defined by the following equation:
ecv ≜self cv is ⟨c ,→c ,→λ ( u : τc ) ⟨d ,→d ,→edm · c · d[ ρ ]( e′′
cv )( e′′
mv )( u ) | d ∈D⟩| c ∈C⟩,
where
e′′
cv ≜unroll( cv ) : [ρ/tobj]τcv
and
e′′
mv ≜⟨d ,→d ,→λ ( this : ρ ) this · d | d ∈D⟩: [ρ/tobj]τmv.
Object creation is defined by the equation
new[ c ]( e ) ≜unroll( ecv ) · c( e ) : ρ,
and message send is defined by the equation
e ⇐d ≜e · d : ρd.
The symmetries between the two organizations are striking. They are a reflection of the funda-
mental symmetries between sum and product types.

PREVIEW
250
26.5 Notes
26.5
Notes
The term “object-oriented” means many things to many people, but certainly dynamic dispatch,
the action of methods on instances of classes, is one of its central concepts. These characteristic
features emerge from the more general concepts of sum-, product-, and function types, which are
useful, alone and in combination, in a wider variety of circumstances. A bias towards either a
class- or method-based organization seems misplaced in view of the inherent symmetries of the
situation. The dynamic dispatch abstraction given by the type (26.1) admits either form of imple-
mentation, as demonstrated in Sections 26.2 and 26.3. The literature on object-oriented program-
ming, of which dynamic dispatch is a significant aspect, is extensive. Abadi and Cardelli (1996)
and Pierce (2002) give a thorough account of much of this work.
Exercises
26.1. Consider the possibility that some methods may only be defined on instances of some classes,
so that a message send operation may result in a “not understood” error at run-time. Use
the type τ opt defined in Section 11.3.4 to rework the dispatch matrix to account for “not
understood” errors. Reformulate the class- and method-based implementations of dynamic
dispatch using the revised dispatch matrix representation. Proceed in two stages. In the first
stage ignore the possibility of self-reference so that the behavior associated to a method on
an instance of a particular class cannot incur a “not understood” error. In the second stage
use your solution to the first stage to further rework the dispatch matrix and the implemen-
tations of dynamic dispatch to account for the behavior of a method to include incurring a
“not understood” error.
26.2. Type refinements can be used to ensure the absence of specified “not understood” errors that
may otherwise arise in the context of Exercise 26.1. To do so begin by specifying, for each
c ∈C, a subset Dc ⊆D of methods that must be well-defined on instances of class c. This
definition determines, for each d ∈D, the set Cd ≜{ c ∈C | d ∈Dc } of classes on which
method d must be well-defined. Using summand refinements for the type τ opt, define the
type refinement
ϕdm ≜⟨( ⟨just( ⊤τc ⇀⊤ρd )⟩d∈Dc ) × ( ⟨⊤( τc⇀ρd ) opt⟩d∈D\Dc )⟩c∈C,
which refines the type of dispatch matrix to within a permutation of its columns.3 It specifies
that if d ∈Dc, then the dispatch matrix entry for class c and method d must be present, and
imposes no restriction on any other entry. Assume that edm ∈τdm ϕdm, as expected. Assume
a method-based organization in which the object type tobj is the sum over all classes of their
instance types.
3Working up to such a permutation is a notational convenience, and can be avoided at the expense of some clarity in
the presentation.

PREVIEW
26.5 Notes
251
(a) Define the refinements inst[ c ] and admits[ d ] of tobj stating that e ∈tobj is an instance
of class c ∈C and that e admits method d ∈D, respectively. Show that if d ∈Dc, then
inst[ c ] ≤admits[ d ], which is to say that any instance of class c admits any method d
for that class.
(b) Define ϕcv ⊑τcv and ϕmv ⊑τmv in terms of inst[ c ] and admits[ d ] so that ecv ∈τcv
ϕcv and emv ∈τmv ϕmv. Remember to use the class- and method vectors derived in
Exercise 26.1.
(c) Referring to the definitions of object creation and message send and of the class- and
method vectors derived in Exercise 26.1, conclude that if message d ∈Dc is sent to an
instance of c ∈C, then no “not understood” error can arise at run-time in a well-refined
program.
26.3. Using self-reference set up a dispatch matrix in which two methods may call one another
mutually recursively when invoked on an instance of a class. Specifically, let num be a class
of numbers with instance type τnum = nat, and let ev and od be two methods with result type
ρev = ρod = bool. Define the dispatch entries for methods ev and od for the class num so that
they determine, by laborious mutual recursion, whether the instance datum is an even or an
odd natural number.
26.4. Generalize the account of self-reference to admit constructors whose arguments may involve
objects and methods whose results may involve objects. Specifically, allow the abstract object
type tobj to occur in the instance type τc of a class c or in the result type ρd of a method
d. Rework the development in Section 26.4 to account for this generalization. Hint: Use
recursive types as described in Chapter 20.

PREVIEW
252
26.5 Notes

PREVIEW
Chapter 27
Inheritance
In this chapter we build on Chapter 26 and consider the process of defining the dispatch matrix
that determines the behavior of each method on each class. A common strategy is to build the
dispatch matrix incrementally by adding new classes or methods to an existing dispatch matrix.
To add a class requires that we define the behavior of each method on objects of that class, and to
define a method requires that we define the behavior of that method on objects of the classes. The
definition of these behaviors can be given by any means available in the language. However, it is
often suggested that a useful means of defining a new class is to inherit the behavior of another
class on some methods, and to override its behavior on others, resulting in an amalgam of the old
and new behaviors. The new class is often called a subclass of the old class, which is then called the
superclass. Similarly, a new method can be defined by inheriting the behavior of another method
on some classes, and overriding the behavior on others. By analogy we may call the new method
a sub-method of a given super-method. For the sake of clarity we restrict attention to the non-self-
referential case in the following development.
27.1
Class and Method Extension
We begin by extending a given dispatch matrix, edm, of type
⟨⟨τc →ρd⟩d∈D⟩c∈C
with a new class c∗/∈C and a new method d∗/∈D to obtain a new dispatch matrix e∗
dm of type
⟨⟨τc →ρd⟩d∈D∗⟩c∈C∗,
where C∗= C ∪{ c∗} and D∗= D ∪{ d∗}.
To add a new class c∗to the dispatch matrix, we must specify the following information:1
1. The instance type τc∗of the new class c∗.
1The extension with a new method will be considered separately for the sake of clarity.

PREVIEW
254
27.2 Class-Based Inheritance
2. The behavior ec∗
d of each method d ∈D on an object of the new class c∗, a function of type
τc∗→ρd.
This data determines a new dispatch matrix e∗
dm such that the following conditions are satisfied:
1. For each c ∈C and d ∈D, the behavior e∗
dm · c · d is the same as the behavior edm · c · d.
2. For each d ∈D, the behavior e∗
dm · c∗· d is given by ec∗
d .
To define c∗as a subclass of some class c ∈C means to define the behavior ec∗
d to be ec
d for some
(perhaps many) d ∈D. It is sensible to inherit a method d in this way only if the subtype relation-
ship
τc →ρd <: τc∗→ρd
is valid, which will be the case if τc∗<: τc. This subtyping condition ensures that the inherited
behavior can be invoked on the instance data of the new class.
Similarly, to add a new method d∗to the dispatch matrix, we must specify the following infor-
mation:
1. The result type ρd∗of the new method d∗.
2. The behavior ec
d∗of the new method d∗on an object of each class c ∈C, a function of type
τc →ρd∗.
This data determines a new dispatch matrix e∗
dm such that the following conditions are satisfied:
1. For each c ∈C and d ∈D, the behavior e∗
dm · c · d is the same as edm · c · d.
2. The behavior e∗
dm · c · d∗is given by ec
d∗.
To define d∗as a sub-method of some d ∈D means to define the behavior ec
d∗to be ec
d for some
(perhaps many) classes c ∈C. This definition is only sensible if the subtype relationship
τc →ρd <: τc →ρd∗
holds, which is the case if ρd <: ρd∗. This subtyping relationship ensures that the result of the old
behavior suffices for the new behavior.
We will now consider how inheritance relates to the method- and class-based organizations of
dynamic dispatch considered in Chapter 26.
27.2
Class-Based Inheritance
Recall that the class-based organization given in Chapter 26 consists of a class vector ecv of type
τcv ≜⟨τc →ρ⟩c∈C,
where the object type ρ is the finite product type ⟨ρd⟩d∈D. The class vector consists of a tuple of
constructors that specialize the methods to a given object of each class.

PREVIEW
27.3 Method-Based Inheritance
255
Let us consider the effect of adding a new class c∗as described in Section 27.1. The new class
vector e∗cv has type
τ∗
cv ≜⟨τc →ρ⟩c∈C∗.
There is an isomorphism, written ( )†, between τ∗cv and the type
τcv × τc∗→ρ,
which can be used to define the new class vector e∗cv as follows:
⟨ecv, λ ( u : τc∗) ⟨d ,→d ,→ec∗
d ( u ) | d ∈D⟩⟩
†.
This definition makes clear that the old class vector ecv is reused intact in the new class vector,
which extends the old class vector with a new constructor.
Although the object type ρ is the same both before and after the extension with the new class,
the behavior of an object of class c∗may differ arbitrarily from that of any other object, even that
of the superclass from which it inherits its behavior. So, knowing that c∗inherits from c tells us
nothing about the behavior of its objects, but only about the means by which the class is defined.
Inheritance carries no semantic significance, but is only a record of the history of how a class is
defined.
Now let us consider the effect of adding a new method d∗as described in Section 27.1. The
new class vector e∗cv has type
τ∗
cv ≜⟨τc →ρ∗⟩c∈C,
where ρ∗is the product type ⟨ρd⟩d∈D∗. There is an isomorphism, written ( )‡, between ρ∗and the
type ρ × ρd∗, where ρ is the old object type. Using this the new class vector e∗cv is defined by
⟨c ,→c ,→λ ( u : τc ) ⟨⟨d ,→d ,→( ( ecv · c )( u ) ) · d | d ∈D⟩, ec
d∗( u )⟩‡ | c ∈C⟩.
Observe that each constructor must be re-defined to account for the new method, but the definition
makes use of the old class vector for the definitions of the old methods.
By this construction the new object type ρ∗is a subtype of the old object type ρ. Consequently,
any objects with the new method can be used in situations expecting an object without the new
method, as might be expected. To avoid redefining old classes when a new method is introduced,
we may restrict inheritance so that new methods are only added to new subclasses. Subclasses
may then have more methods than super-classes, and objects of the subclass can be provided
when an object of the superclass is required.
27.3
Method-Based Inheritance
The method-based organization is dual to that of the class-based organization. Recall that the
method-based organization given in Chapter 26 consists of a method vector emv of type
τmv ≜⟨τ →ρd⟩d∈D,

PREVIEW
256
27.4 Notes
where the instance type τ is the sum type [τc]c∈C. The method vector consists of a tuple of func-
tions that dispatch on the class of the object to determine their behavior.
Let us consider the effect of adding a new method d∗as described in Section 27.1. The new
method vector e∗mv has type
τ∗
mv ≜⟨τ →ρd⟩d∈D∗.
There is an isomorphism, written ( )‡, between τ∗mv and the type
τmv × (τ →ρd∗).
Using this isomorphism, the new method vector e∗mv is defined as
⟨emv, λ ( this : τ ) case this {c · u ,→ec
d∗( u ) | c ∈C}⟩‡.
The old method vector is re-used intact, extended with a dispatch function for the new method.
The object type does not change under the extension with a new method, but because ρ∗<: ρ,
there is no difficulty using a new object in a context expecting an old object—the added method is
ignored.
Finally, let us consider the effect of adding a new class c∗as described in Section 27.1. The new
method vector, e∗mv, has the type
τ∗
mv ≜⟨τ∗→ρd⟩d∈D,
where τ∗is the new object type [τc]c∈C∗, which is a super-type of the old object type τ. There is an
isomorphism, written ( )†, between τ∗and the sum type τ + τc∗, which we may use to define the
new method vector e∗mv as follows:
⟨d ,→d ,→λ ( this : τ∗) case this† {l · u ,→( emv · d )( u ) | r · u ,→ec∗
d ( u )} | d ∈D⟩.
Every method must be redefined to account for the new class, but the old method vector is reused.
27.4
Notes
Abadi and Cardelli (1996) and Pierce (2002) provide thorough accounts of the interaction of in-
heritance and subtyping. Liskov and Wing (1994) discuss it from a behavioral perspective. They
propose to require that subclasses respect the behavior of the superclass when inheritance is used.
Exercises
27.1. Consider the case of extending a dispatch matrix with self-reference by a new class c∗in
which a method d is inherited from an existing class c. What requirements ensure that such
an inheritance is properly defined? What happens if we extend a self-referential dispatch
matrix with a new method, d∗that inherits its behavior on class c from another method d?

PREVIEW
27.4 Notes
257
27.2. Consider the example of two mutually recursive methods given in Exercise 26.3. Suppose
that num∗is a new class with instance type τnum∗<: τnum that inherits the ev method from
num, but defines its own version of the od method. What happens when message ev is sent
to an instance of num∗? Will the revised od method ever be invoked?
27.3. Method specialization consists of defining a new class by inheriting methods from another
class or classes, while redefining some of the methods that the inherited methods might
invoke. The behavior of the inherited methods on instances of the new class is altered to the
extent that they invoke a method that is specialized to the new class. Reconsider Exercise 26.3
in light of Exercise 27.2, seeking to ensure that the specialization of od is invoked when the
inherited method ev is invoked on instances of the new class.
(a) Redefine the class num along the following lines. The instance data of num is an object
admitting methods ev and od. The class num admits these methods, and simply hands
them off to the instance object.
(b) The classes zero or of succ admit both the ev and od methods, and are defined using
message send to effect mutual recursion as necessary.
(c) Define a subclass succ∗of succ that overrides the od method. Show that ev on an
instance of succ∗correctly invokes the overridden od method.

PREVIEW
258
27.4 Notes

PREVIEW
Part XII
Control Flow

PREVIEW

PREVIEW
Chapter 28
Control Stacks
Structural dynamics is convenient for proving properties of languages, such as a type safety the-
orem, but is less convenient as a guide for implementation. A structural dynamics defines a tran-
sition relation using rules that determine where to apply the next instruction without spelling out
how to find where the instruction lies within an expression. To make this process explicit we in-
troduce a mechanism, called a control stack, that records the work that remains to be done after an
instruction is executed. Using a stack eliminates the need for premises on the transition rules so
that the transition system defines an abstract machine whose steps are determined by information
explicit in its state, much as a concrete computer does.
In this chapter we develop an abstract machine K for evaluating expressions in PCF. The
machine makes explicit the context in which primitive instruction steps are executed, and the
process by which the results are propagated to determine the next step of execution. We prove
that K and PCF are equivalent in the sense that both achieve the same outcomes for the same
expressions.
28.1
Machine Definition
A state s of the stack machine K for PCF consists of a control stack k and a closed expression e.
States take one of two forms:
1. An evaluation state of the form k ▷e corresponds to the evaluation of a closed expression e on
a control stack k.
2. A return state of the form k ◁e, where e val, corresponds to the evaluation of a stack k on a
closed value e.
As an aid to memory, note that the separator “points to” the focal entity of the state, the expression
in an evaluation state and the stack in a return state.
The control stack represents the context of evaluation. It records the “current location” of eval-
uation, the context into which the value of the current expression is returned. Formally, a control

PREVIEW
262
28.1 Machine Definition
stack is a list of frames:
ϵ stack
(28.1a)
f frame
k stack
k ; f stack
(28.1b)
The frames of the K machine are inductively defined by the following rules:
s( −) frame
(28.2a)
ifz{e0 ; x . e1}( −) frame
(28.2b)
ap( −; e2 ) frame
(28.2c)
The frames correspond to search rules in the dynamics of PCF. Thus, instead of relying on the
structure of the transition derivation to keep a record of pending computations, we make an ex-
plicit record of them in the form of a frame on the control stack.
The transition judgment between states of the PCF machine is inductively defined by a set of
inference rules. We begin with the rules for natural numbers, using an eager dynamics for the
successor.
k ▷z 7−→k ◁z
(28.3a)
k ▷s( e ) 7−→k ; s( −) ▷e
(28.3b)
k ; s( −) ◁e 7−→k ◁s( e )
(28.3c)
To evaluate z we simply return it. To evaluate s( e ), we push a frame on the stack to record the
pending successor, and evaluate e; when that returns with e′, we return s( e′ ) to the stack.
Next, we consider the rules for case analysis.
k ▷ifz{e0 ; x . e1}( e ) 7−→k ; ifz{e0 ; x . e1}( −) ▷e
(28.4a)
k ; ifz{e0 ; x . e1}( −) ◁z 7−→k ▷e0
(28.4b)
k ; ifz{e0 ; x . e1}( −) ◁s( e ) 7−→k ▷[e/x]e1
(28.4c)

PREVIEW
28.2 Safety
263
The test expression is evaluated, recording the pending case analysis on the stack. Once the value
of the test expression is determined, the zero or non-zero branch of the condition is evaluated,
substituting the predecessor in the latter case.
Finally, we give the rules for functions, which are evaluated by-name, and the rule for general
recursion.
k ▷λ{τ}( x . e ) 7−→k ◁λ{τ}( x . e )
(28.5a)
k ▷ap( e1 ; e2 ) 7−→k ; ap( −; e2 ) ▷e1
(28.5b)
k ; ap( −; e2 ) ◁λ{τ}( x . e ) 7−→k ▷[e2/x]e
(28.5c)
k ▷fix{τ}( x . e ) 7−→k ▷[fix{τ}( x . e )/x]e
(28.5d)
It is important that evaluation of a general recursion requires no stack space.
The initial and final states of the K machine are defined by the following rules:
ϵ ▷e initial
(28.6a)
e val
ϵ ◁e final
(28.6b)
28.2
Safety
To define and prove safety for the PCF machine requires that we introduce a new typing judgment,
k ÷ τ, which states that the stack k expects a value of type τ. This judgment is inductively defined
by the following rules:
ϵ ÷ τ
(28.7a)
k ÷ τ′
f : τ ⇝τ′
k ; f ÷ τ
(28.7b)
This definition makes use of an auxiliary judgment, f : τ ⇝τ′, stating that a frame f transforms a
value of type τ to a value of type τ′.
s( −) : nat ⇝nat
(28.8a)
e0 : τ
x : nat ⊢e1 : τ
ifz{e0 ; x . e1}( −) : nat ⇝τ
(28.8b)

PREVIEW
264
28.3 Correctness of the Stack Machine
e2 : τ2
ap( −; e2 ) : parr( τ2 ; τ ) ⇝τ
(28.8c)
The states of the PCF machine are well-formed if their stack and expression components match:
k ÷ τ
e : τ
k ▷e ok
(28.9a)
k ÷ τ
e : τ
e val
k ◁e ok
(28.9b)
We leave the proof of safety of the PCF machine as an exercise.
Theorem 28.1 (Safety).
1. If s ok and s 7−→s′, then s′ ok.
2. If s ok, then either s final or there exists s′ such that s 7−→s′.
28.3
Correctness of the Stack Machine
Does evaluation of an expression e using the K machine yield the same result as does the structural
dynamics of PCF? The answer to this question can be derived from the following facts.
Completeness If e 7−→∗e′, where e′ val, then ϵ ▷e 7−→∗ϵ ◁e′.
Soundness If ϵ ▷e 7−→∗ϵ ◁e′, then e 7−→∗e′ with e′ val.
To prove completeness a plausible first step is to consider a proof by induction on the definition
of multi-step transition, which reduces the theorem to the following two lemmas:
1. If e val, then ϵ ▷e 7−→∗ϵ ◁e.
2. If e 7−→e′, then, for every v val, if ϵ ▷e′ 7−→∗ϵ ◁v, then ϵ ▷e 7−→∗ϵ ◁v.
The first can be proved easily by induction on the structure of e. The second requires an inductive
analysis of the derivation of e 7−→e′ that gives rise to two complications. The first complication is
that we cannot restrict attention to the empty stack, for if e is, say, ap( e1 ; e2 ), then the first step of
the K machine is
ϵ ▷ap( e1 ; e2 ) 7−→ϵ ; ap( −; e2 ) ▷e1.
To handle such situations we consider the evaluation of e1 on any stack, not just the empty stack.
Specifically, we prove that if e 7−→e′ and k ▷e′ 7−→∗k ◁v, then k ▷e 7−→∗k ◁v. Reconsider the
case e = ap( e1 ; e2 ), e′ = ap( e′
1 ; e2 ), with e1 7−→e′
1. We are given that k ▷ap( e′
1 ; e2 ) 7−→∗k ◁v,
and we are to show that k ▷ap( e1 ; e2 ) 7−→∗k ◁v. It is easy to show that the first step of the former
derivation is
k ▷ap( e′
1 ; e2 ) 7−→k ; ap( −; e2 ) ▷e′
1.

PREVIEW
28.3 Correctness of the Stack Machine
265
We would like to apply induction to the derivation of e1 7−→e′
1, but to do so we need a value v1
such that e′
1 7−→∗v1, which is not at hand.
We therefore consider the value of each sub-expression of an expression. This information is
given by the evaluation dynamics described in Chapter 7, which has the property that e ⇓e′ iff
e 7−→∗e′ and e′ val.
Lemma 28.2. If e ⇓v, then for every k stack, k ▷e 7−→∗k ◁v.
The desired result follows by the analog of Theorem 7.2 for PCF, which states that e ⇓v iff
e 7−→∗v.
To prove soundness, we note that it is awkward to reason inductively about a multi-step tran-
sition from ϵ ▷e 7−→∗ϵ ◁v. The intermediate steps could involve alternations of evaluation and
return states. Instead we consider a K machine state to encode an expression, and show that the
machine transitions are simulated by the transitions of the structural dynamics.
To do so we define a judgment, s ↬e, stating that state s “unravels to” expression e. It will
turn out that for initial states, s = ϵ ▷e, and final states, s = ϵ ◁e, we have s ↬e. Then we show
that if s 7−→∗s′, where s′ final, s ↬e, and s′ ↬e′, then e′ val and e 7−→∗e′. For this it is enough to
show the following two facts:
1. If s ↬e and s final, then e val.
2. If s 7−→s′, s ↬e, s′ ↬e′, and e′ 7−→∗v, where v val, then e 7−→∗v.
The first is quite simple, we need only note that the unraveling of a final state is a value. For the
second, it is enough to prove the following lemma.
Lemma 28.3. If s 7−→s′, s ↬e, and s′ ↬e′, then e 7−→∗e′.
Corollary 28.4. e 7−→∗n iff ϵ ▷e 7−→∗ϵ ◁n.
28.3.1
Completeness
Proof of Lemma 28.2. The proof is by induction on an evaluation dynamics for PCF.
Consider the evaluation rule
e1 ⇓λ{τ2}( x . e )
[e2/x]e ⇓v
ap( e1 ; e2 ) ⇓v
(28.10)
For an arbitrary control stack k we are to show that k ▷ap( e1 ; e2 ) 7−→∗k ◁v. Applying both of

PREVIEW
266
28.3 Correctness of the Stack Machine
the inductive hypotheses in succession, interleaved with steps of the K machine, we obtain
k ▷ap( e1 ; e2 ) 7−→k ; ap( −; e2 ) ▷e1
7−→∗k ; ap( −; e2 ) ◁λ{τ2}( x . e )
7−→k ▷[e2/x]e
7−→∗k ◁v.
The other cases of the proof are handled similarly.
28.3.2
Soundness
The judgment s ↬e′, where s is either k ▷e or k ◁e, is defined in terms of the auxiliary judgment
k ▷◁e = e′ by the following rules:
k ▷◁e = e′
k ▷e ↬e′
(28.11a)
k ▷◁e = e′
k ◁e ↬e′
(28.11b)
In words, to unravel a state we wrap the stack around the expression to form a complete program.
The unraveling relation is inductively defined by the following rules:
ϵ ▷◁e = e
(28.12a)
k ▷◁s( e ) = e′
k ; s( −) ▷◁e = e′
(28.12b)
k ▷◁ifz{e0 ; x . e1}( e ) = e′
k ; ifz{e0 ; x . e1}( −) ▷◁e = e′
(28.12c)
k ▷◁ap( e1 ; e2 ) = e
k ; ap( −; e2 ) ▷◁e1 = e
(28.12d)
These judgments both define total functions.
Lemma 28.5. The judgment s ↬e relates every state s to a unique expression e, and the judgment k ▷◁e =
e′ relates every stack k and expression e to a unique expression e′.
We are therefore justified in writing k ▷◁e for the unique e′ such that k ▷◁e = e′.
The following lemma is crucial. It states that unraveling preserves the transition relation.
Lemma 28.6. If e 7−→e′, k ▷◁e = d, k ▷◁e′ = d′, then d 7−→d′.

PREVIEW
28.4 Notes
267
Proof. The proof is by rule induction on the transition e 7−→e′. The inductive cases, where the
transition rule has a premise, follow easily by induction. The base cases, where the transition is an
axiom, are proved by an inductive analysis of the stack k.
For an example of an inductive case, suppose that e = ap( e1 ; e2 ), e′ = ap( e′
1 ; e2 ), and e1 7−→e′
1.
We have k ▷◁e = d and k ▷◁e′ = d′. It follows from rules (28.12) that k ; ap( −; e2 ) ▷◁e1 = d and
k ; ap( −; e2 ) ▷◁e′
1 = d′. So by induction d 7−→d′, as desired.
For an example of a base case, suppose that e = ap( λ{τ2}( x . e ) ; e2 ) and e′ = [e2/x]e with
e 7−→e′ directly. Assume that k ▷◁e = d and k ▷◁e′ = d′; we are to show that d 7−→d′. We
proceed by an inner induction on the structure of k. If k = ϵ, the result follows immediately.
Consider, say, the stack k = k′ ; ap( −; c2 ). It follows from rules (28.12) that k′ ▷◁ap( e ; c2 ) = d
and k′ ▷◁ap( e′ ; c2 ) = d′. But by the structural dynamics ap( e ; c2 ) 7−→ap( e′ ; c2 ), so by the inner
inductive hypothesis we have d 7−→d′, as desired.
We may now complete the proof of Lemma 28.3.
Proof of Lemma 28.3. The proof is by case analysis on the transitions of the K machine. In each case,
after unraveling, the transition will correspond to zero or one transitions of the PCF structural
dynamics.
Suppose that s = k ▷s( e ) and s′ = k ; s( −) ▷e. Note that k ▷◁s( e ) = e′ iff k ; s( −) ▷◁e = e′,
from which the result follows immediately.
Suppose that s = k ; ap( −; e2 ) ◁λ{τ}( x . e1 ), and s′ = k ▷[e2/x]e1. Let e′ be such that
k ; ap( −; e2 ) ▷◁λ{τ}( x . e1 ) = e′ and let e′′ be such that k ▷◁[e2/x]e1 = e′′. Noting that k ▷◁
ap( λ{τ}( x . e1 ) ; e2 ) = e′, the result follows from Lemma 28.6.
28.4
Notes
The abstract machine considered here is typical of a wide class of machines that make control flow
explicit in the state. The prototype is the SECD machine (Landin, 1965), which is a linearization of
a structural operational semantics (Plotkin, 1981). The advantage of a machine model is that the
explicit treatment of control is needed for languages that allow the control state to be manipulated
(see Chapter 30 for a prime example). The disadvantage is that the control state of the computation
must be made explicit, necessitating rules for manipulating it that are left implicit in a structural
dynamics.
Exercises
28.1. Give the proof of Theorem 28.1 for conditional expressions.
28.2. Formulate a call-by-value variant of the PCF machine.

PREVIEW
268
28.4 Notes
28.3. Analyze the worst-case asymptotic complexity of executing each instruction of the K ma-
chine.
28.4. Refine the proof of Lemma 28.2 by bounding the number of machine steps taken for each
step of the PCF dynamics.

PREVIEW
Chapter 29
Exceptions
Exceptions effect a non-local transfer of control from the point at which the exception is raised
to an enclosing handler for that exception. This transfer interrupts the normal flow of control in
a program in response to unusual conditions. For example, exceptions can be used to signal an
error condition, or to signal the need for special handling in unusual circumstances. We could
use conditionals to check for and process errors or unusual conditions, but using exceptions is
often more convenient, particularly because the transfer to the handler is conceptually direct and
immediate, rather than indirect via explicit checks.
In this chapter we will consider two extensions of PCF with exceptions. The first, FPCF, en-
riches PCF with the simplest form of exception, called a failure, with no associated data. A failure
can be intercepted, and turned into a success (or another failure!) by transferring control to an-
other expression. The second, XPCF, enriches PCF with exceptions, with associated data that is
passed to an exception handler that intercepts it. The handler may analyze the associated data to
determine how to recover from the exceptional condition. A key choice is to decide on the type of
the data associated to an exception.
29.1
Failures
The syntax of FPCF is defined by the following extension of the grammar of PCF:
Exp
e
::=
fail
fail
signal a failure
catch( e1 ; e2 )
catch e1 ow e2
catch a failure
The expression fail aborts the current evaluation, and the expression catch( e1 ; e2 ) catches any
failure in e1 by evaluating e2 instead. Either e1 or e2 may themselves abort, or they may diverge or
return a value as usual in PCF.
The statics of FPCF is given by these rules:
Γ ⊢fail : τ
(29.1a)

PREVIEW
270
29.1 Failures
Γ ⊢e1 : τ
Γ ⊢e2 : τ
Γ ⊢catch( e1 ; e2 ) : τ
(29.1b)
A failure can have any type, because it never returns. The two expressions in a catch expression
must have the same type, because either might determine the value of that expression.
The dynamics of FPCF is given using a technique called stack unwinding. Evaluation of a catch
pushes a frame of the form catch( −; e ) onto the control stack that awaits the arrival of a failure.
Evaluation of a fail expression pops frames from the control stack until it reaches a frame of the
form catch( −; e ), at which point the frame is removed from the stack and the expression e is
evaluated. Failure propagation is expressed by a state of the form k ◀, which extends the two
forms of state considered in Chapter 28 to express failure propagation.
The FPCF machine extends the PCF machine with the following additional rules:
k ▷fail 7−→k ◀
(29.2a)
k ▷catch( e1 ; e2 ) 7−→k ; catch( −; e2 ) ▷e1
(29.2b)
k ; catch( −; e2 ) ◁v 7−→k ◁v
(29.2c)
k ; catch( −; e2 ) ◀7−→k ▷e2
(29.2d)
( f ̸= catch( −; e ))
k ; f ◀7−→k ◀
(29.2e)
Evaluating fail propagates a failure up the stack. The act of failing itself, fail, will, of course,
give rise to a failure. Evaluating catch( e1 ; e2 ) consists of pushing the handler on the control stack
and evaluating e1. If a value reaches to the handler, the handler is removed and the value is passed
to the surrounding frame. If a failure reaches the handler, the stored expression is evaluated with
the handler removed from the control stack. Failures propagate through all frames other than the
catch frame.
The initial and final states of the FPCF machine are defined by the following rules:
ϵ ▷e initial
(29.3a)
e val
ϵ ◁e final
(29.3b)
ϵ ◀final
(29.3c)

PREVIEW
29.2 Exceptions
271
The definition of stack typing given in Chapter 28 can be extended to account for the new forms
of frame so that safety can be proved in the same way as before. The only difference is that the
statement of progress must be weakened to take account of failure: a well-typed expression is
either a value, or may take a step, or may signal failure.
Theorem 29.1 (Safety for FPCF).
1. If s ok and s 7−→s′, then s′ ok.
2. If s ok, then either s final or there exists s′ such that s 7−→s′.
29.2
Exceptions
The language XPCF enriches FPCF with exceptions, failures to which a value is attached. The
syntax of XPCF extends that of PCF with the following forms of expression:
Exp
e
::=
raise( e )
raise( e )
raise an exception
try( e1 ; x . e2 )
try e1 ow x ,→e2
handle an exception
The argument to raise is evaluated to determine the value passed to the handler. The expression
try( e1 ; x . e2 ) binds a variable x in the handler e2. The associated value of the exception is bound
to that variable within e2, should an exception be raised when e1 is evaluated.
The statics of exceptions extends the statics of failures to account for the type of the value
carried with the exception:
Γ ⊢e : exn
Γ ⊢raise( e ) : τ
(29.4a)
Γ ⊢e1 : τ
Γ, x : exn ⊢e2 : τ
Γ ⊢try( e1 ; x . e2 ) : τ
(29.4b)
The type exn is some fixed, but as yet unspecified, type of exception values. (The choice of exn is
discussed in Section 29.3.)
The dynamics of XPCF is similar to that of FPCF, except that the failure state k ◀is replaced
by the exception state k ◀e which passes an exception value e to the stack k. There is only one
notion of exception, but the associated value can be used to identify the source of the exception.
The stack frames of the PCF machine are extended to include raise( −) and try( −; x . e2 ).
These are used in the following rules:
k ▷raise( e ) 7−→k ; raise( −) ▷e
(29.5a)
k ; raise( −) ◁e 7−→k ◀e
(29.5b)
k ▷try( e1 ; x . e2 ) 7−→k ; try( −; x . e2 ) ▷e1
(29.5c)

PREVIEW
272
29.3 Exception Values
k ; try( −; x . e2 ) ◁e 7−→k ◁e
(29.5d)
k ; try( −; x . e2 ) ◀e 7−→k ▷[e/x]e2
(29.5e)
( f ̸= try( −; x . e2 ))
k ; f ◀e 7−→k ◀e
(29.5f)
The main difference compared to Rules (29.2) is that an exception passes a values to the stack,
whereas a failure does not.
The initial and final states of the XPCF machine are defined by the following rules:
ϵ ▷e initial
(29.6a)
e val
ϵ ◁e final
(29.6b)
e val
ϵ ◀e final
(29.6c)
Theorem 29.2 (Safety for XPCF).
1. If s ok and s 7−→s′, then s′ ok.
2. If s ok, then either s final or there exists s′ such that s 7−→s′.
29.3
Exception Values
The statics of XPCF is parameterized by the type exn of values associated to exceptions. The
choice of exn is important because it determines how the source of an exception is identified in a
program. If exn is the one-element type unit, then exceptions degenerate to failures, which are
unable to identify their source. Thus exn must have more than one value to be useful.
This fact suggests that exn should be a finite sum. The classes of the sum identify the sources of
exceptions, and the classified value carries information about the particular instance. For example,
exn might be a sum type of the form
[div ,→unit , fnf ,→string , . . . ].
Here the class div might represent an arithmetic fault, with no associated data, and the class fnf
might represent a “file not found” error, with associated data being the name of the file that was
not found.
Using a sum means that an exception handler can dispatch on the class of the exception value
to identify its source and cause. For example, we might write

PREVIEW
29.4 Notes
273
handle e1 ow x ,→
match x {
div ⟨⟩,→ediv
| fnf s ,→efnf }
to handle the exceptions specified by the above sum type. Because the exception and its associated
data are coupled in a sum type, there is no possibility of misinterpreting the data associated to one
exception as being that of another.
The disadvantage of choosing a finite sum for exn is that it specifies a closed world of possible
exception sources. All sources must be identified for the entire program, which impedes modular
development and evolution. A more modular approach admits an open world of exception sources
that can be introduced as the program evolves and even as it executes. A generalization of finite
sums, called dynamic classification, defined in Chapter 33, is required for an open world. (See that
Chapter for further discussion.)
When exn is a type of classified values, its classes are often called exceptions, so that one may
speak of “the fnf exception” in the above example. This terminology is harmless, and all but
unavoidable, but it invites confusion between two separate ideas:
1. Exceptions as a control mechanism that allows the course of evaluation to be altered by raising
and handling exceptions.
2. Exceptions as a data value associated with such a deviation of control that allows the source
of the deviation to be identified.
As a control mechanism exceptions can be eliminated using explicit exception passing. A computa-
tion of type τ that may raise an exception is interpreted as an exception-free computation of type
τ + exn.
29.4
Notes
Various forms of exceptions were considered in Lisp (Steele, 1990). The original formulation of
ML (Gordon et al., 1979) as a metalanguage for mechanized logic used failures to implement back-
tracking proof search. Most modern languages now have exceptions, but differ in the forms of
data that may be associated with them.
Exercises
29.1. Prove Theorem 29.2. Are any properties of exn required for the proof?
29.2. Give an evaluation dynamics for XPCF using the following judgment forms:
• Normal evaluation: e ⇓v, where e : τ, v : τ, and v val.
• Exceptional evaluation: e ⇑v, where e : τ, and v : exn, and v val.

PREVIEW
274
29.4 Notes
The first states that e evaluates normally to value v, the second that e raises an exception with
value v.
29.3. Give a structural operational dynamics to XPCF by inductively defining the following judg-
ment forms:
• e 7−→e′, stating that expression e transitions to expression e′;
• e val, stating that expression e is a value.
Ensure that e ⇓v iff e 7−→∗v, and e ⇑v iff e 7−→∗raise( v ), where v val in both cases.

PREVIEW
Chapter 30
Continuations
The semantics of many control constructs (such as exceptions and coroutines) can be expressed in
terms of reified control stacks, a representation of a control stack as a value that can be reactivated at
any time, even if control has long since returned past the point of reification. Reified control stacks
of this kind are called continuations; they are values that can be passed and returned at will in a
computation. Continuations never “expire”, and it is always sensible to reinstate a continuation
without compromising safety. Thus continuations support unlimited “time travel” — we can go
back to a previous step of the computation, then return to some point in its future.
Why are continuations useful? Fundamentally, they are representations of the control state of
a computation at a given time. Using continuations we can “checkpoint” the control state of a pro-
gram, save it in a data structure, and return to it later. In fact this is precisely what is necessary to
implement threads (concurrently executing programs) — the thread scheduler suspends a program
for later execution from where it left off.
30.1
Overview
We will consider the extension KPCF of PCF with the type cont( τ ) of continuations accepting
values of type τ. The introduction form for cont( τ ) is letcc{τ}( x . e ), which binds the current
continuation (that is, the current control stack) to the variable x, and evaluates the expression e. The
corresponding elimination form is throw{τ}( e1 ; e2 ), which restores the value given by e1 to the
control stack given by e2.
To illustrate the use of these primitives, consider the problem of multiplying the first n elements
of an infinite sequence q of natural numbers, where q is represented by a function of type nat ⇀
nat. If zero occurs among the first n elements, we would like to effect an “early return” with the
value zero, without further multiplication. This problem can be solved using exceptions, but we
will solve it with continuations to show how they are used.
Here is the solution in PCF, without short-cutting:

PREVIEW
276
30.1 Overview
fix ms is
λ q : nat ⇀nat.
λ n : nat.
case n {
z ,→s(z)
| s(n’) ,→(q z) × (ms (q ◦succ) n’)
}
The recursive call composes q with the successor function to shift the sequence by one step.
Here is the solution in KPCF, with short-cutting:
λ q : nat ⇀nat.
λ n : nat.
letcc ret : nat cont in
let ms be
fix ms is
λ q : nat ⇀nat.
λ n : nat.
case n {
z ,→s(z)
| s(n’) ,→
case q z {
z ,→throw z to ret
| s(n’’) ,→(q z) × (ms (q ◦succ) n’)
}
}
in
ms q n
The letcc binds the return point of the function to the variable ret for use within the main loop
of the computation. If an element is zero, control is thrown to ret, effecting an early return with
the value zero.
To take another example, given that k has type τ cont and f has type τ′ ⇀τ, return a continu-
ation k′ of type τ′ cont such that throwing a value v′ of type τ′ to k′ throws the value of f (v′) to k.
Thus we seek to define a function compose of type
( τ′ ⇀τ ) ⇀τ cont ⇀τ′ cont.
The continuation we seek is the one in effect at the point of the ellipsis in the expression throw
f(...) to k. It is the continuation that, when given a value v′, applies f to it, and throws the
result to k. We can seize this continuation using letcc by writing
throw f(letcc x:τ′ cont in ...) to k
The desired continuation is bound to x, but how can we return it as the result of compose? We use
the same idea as for short-circuit multiplication, writing

PREVIEW
30.2 Continuation Dynamics
277
letcc ret:τ′ cont cont in
throw (f (letcc r in throw r to ret)) to k
as the body of compose. Note that the type of ret is τ′ cont cont, that of a continuation that expects
to be thrown a continuation!
30.2
Continuation Dynamics
The syntax of KPCF is as follows:
Type
τ
::=
cont( τ )
τ cont
continuation
Expr
e
::=
letcc{τ}( x . e )
letcc x in e
mark
throw{τ}( e1 ; e2 )
throw e1 to e2
goto
cont( k )
cont( k )
continuation
The expression cont( k ) is a reified control stack, which arises during evaluation.
The statics of KPCF is defined by the following rules:
Γ, x : cont( τ ) ⊢e : τ
Γ ⊢letcc{τ}( x . e ) : τ
(30.1a)
Γ ⊢e1 : τ1
Γ ⊢e2 : cont( τ1 )
Γ ⊢throw{τ}( e1 ; e2 ) : τ
(30.1b)
The result type of a throw expression is arbitrary because it does not return to the point of the call.
The statics of continuation values is given by the following rule:
k : τ
Γ ⊢cont( k ) ÷ cont( τ )
(30.2)
A continuation value cont( k ) has type cont( τ ) exactly if it is a stack accepting values of type τ.
To define the dynamics of KPCF we extend the PCF machine with two forms of stack frame:
throw{τ}( −; e2 ) frame
(30.3a)
e1 val
throw{τ}( e1 ; −) frame
(30.3b)
Every reified control stack is a value:
k stack
cont( k ) val
(30.4)
The transition rules of the PCF machine governing continuations are as follows:
k ▷cont( k′ ) 7−→k ◁cont( k′ )
(30.5a)

PREVIEW
278
30.3 Coroutines from Continuations
k ▷letcc{τ}( x . e ) 7−→k ▷[cont( k )/x]e
(30.5b)
k ▷throw{τ}( e1 ; e2 ) 7−→k ; throw{τ}( −; e2 ) ▷e1
(30.5c)
e1 val
k ; throw{τ}( −; e2 ) ◁e1 7−→k ; throw{τ}( e1 ; −) ▷e2
(30.5d)
e val
k ; throw{τ}( e ; −) ◁cont( k′ ) 7−→k′ ◁e
(30.5e)
Evaluation of a letcc expression duplicates the control stack; evaluation of a throw expression
destroys the current control stack.
The safety of KPCF is proved by extending the safety proof for the K machine given in Chap-
ter 28.
We need only add typing rules for the two new forms of frame, which are as follows:
e2 : cont( τ )
throw{τ′}( −; e2 ) : τ ⇝τ′
(30.6a)
e1 : τ
e1 val
throw{τ′}( e1 ; −) : cont( τ ) ⇝τ′
(30.6b)
The rest of the definitions remain as in Chapter 28.
Lemma 30.1 (Canonical Forms). If e : cont( τ ) and e val, then e = cont( k ) for some k such that k : τ.
Theorem 30.2 (Safety).
1. If s ok and s 7−→s′, then s′ ok.
2. If s ok, then either s final or there exists s′ such that s 7−→s′.
30.3
Coroutines from Continuations
The distinction between a routine and a subroutine is the distinction between a manager and a
worker. The routine calls the subroutine to do some work, and the subroutine returns to the routine
when its work is done. The relationship is asymmetric in that there is a distinction between the
caller, the main routine, and the callee, the subroutine. It is useful to consider a symmetric situation
in which two routines each call the other to do some work. Such a pair of routines are called
coroutines; their relationship to one another is symmetric, not hierarchical.
A subroutine is implemented by having the caller pass to the callee a continuation representing
the work to be done once the subroutine finishes. When it does, it throws the return value to that
continuation, without the possibility of return. A coroutine is implemented by having two routines
each call each other as subroutines by providing a continuation when control is ceded from one to
the other. The only tricky part is how the entire process gets started.
Consider the type of each routine of the pair. A routine is a continuation accepting two argu-
ments, a data to be passed to the routine when it is resumed, and a continuation to be resumed

PREVIEW
30.3 Coroutines from Continuations
279
when the routine has finished its task. The datum represents the state of the computation, and the
continuation is a coroutine that accepts arguments of the same form. Thus, the type of a coroutine
must satisfy the type isomorphism
τ coro ∼= ( τ × τ coro ) cont.
Therefore we define τ coro to be the recursive type
τ coro ≜rec t is ( τ × t ) cont.
Up to isomorphism, the type τ coro is the type of continuations that accept a value of type τ,
representing the state of the coroutine, and the partner coroutine, a value of the same type.
A coroutine r passes control to another coroutine r′ by evaluating the expression resume( ⟨s, r′⟩),
where s is the current state of the computation. Doing so creates a new coroutine whose entry point
is the return point (calling site) of resume. Therefore the type of resume is
τ × τ coro ⇀τ × τ coro.
The definition of resume is as follows:
λ ( ⟨s, r′⟩: τ × τ coro ) letcc k in throw ⟨s, fold( k )⟩to unfold( r′ )
When applied, resume seizes the current continuation, and passes the state, s, and the seized con-
tinuation (packaged as a coroutine) to the called coroutine.
Because the state is explicitly passed from one routine to the other, a coroutine is a state trans-
formation function that, when activated with the current state, determines the next state of the
computation. A system of coroutines is created by establishing a joint exit point to which the re-
sult of the system is thrown, and creating a pair of coroutines that transform the state and pass
control to the partner routine. If either routine wishes to terminate the computation, it does so by
throwing a result value to their common exit point. Thus, a coroutine is a function of type
( τ′,τ ) rout ≜τ′ cont ⇀τ ⇀τ,
where τ′ is the result type and τ is the state type of the system of coroutines.
To set up a system of coroutines we define a function run that, given two routines, creates
a function of type τ ⇀τ′ that, when applied to the initial state, computes a result of type τ′.
The computation consists of a cooperating pair of routines that share a common exit point. The
definition of run begins as follows:
λ ( ⟨r1, r2⟩) λ ( s0 ) letcc x0 in let r′
1 be r1( x0 ) in let r′
2 be r2( x0 ) in . . .
Given two routines, run establishes their common exit point, and passes this continuation to both
routines. By throwing to this continuation either routine may terminate the computation with a
result of type τ′. The body of the run function continues as follows:
rep( r′
2 )( letcc k in rep( r′
1 )( ⟨s0, fold( k )⟩) )

PREVIEW
280
30.3 Coroutines from Continuations
The auxiliary function rep creates an infinite loop that transforms the state and passes control to
the other routine:
λ ( t ) fix l is λ ( ⟨s, r⟩) l( resume( ⟨t( s ), r⟩) ).
The system is initialized by starting routine r1 with the initial state, and arranging that, when it
cedes control to its partner, it starts routine r2 with the resulting state. At that point the system is
bootstrapped: each routine will resume the other on each iteration of the loop.
A good example of coroutining is the interleaving of input and output in a computation. This
is done by coroutining between a producer routine and a consumer routine. The producer emits the
next element of the input, if any, and passes control to the consumer, removing that element from
the input. The consumer processes the next data item, and returns control to the producer, with the
result of processing attached to the output. For simplicity input and output are modeled as lists
of type τi list and τo list, respectively, which are passed back and forth between the routines.
The routines exchange messages according to the following protocol. The message OK( ⟨i, o⟩) is
sent from the consumer to producer to acknowledge receipt of the previous message, and to pass
back the current state of the input and output channels. The message EMIT( ⟨e, ⟨i, o⟩⟩), where e is a
value of type τi opt, is sent from the producer to the consumer to emit the next value (if any) from
the input, and to pass the current state of the input and output channels to the consumer.
Here is an implementation of the producer/consumer coroutines. The type τ of the state main-
tained by the routines is the labeled sum type
[OK ,→τi list × τo list , EMIT ,→τi opt × ( τi list × τo list )].
The above type specifies the message protocol between the producer and the consumer described
in the preceding paragraph.
The producer P is defined by the expression
λ ( x0 ) λ ( msg ) case msg {b1 | b2 | b3},
where the first branch b1 is
OK · ⟨nil, os⟩,→EMIT · ⟨null, ⟨nil, os⟩⟩
and the second branch b2 is
OK · ⟨cons( i ; is ), os⟩,→EMIT · ⟨just( i ), ⟨is, os⟩⟩,
and the third branch b3 is
EMIT · ,→error.
In words, if the input is exhausted, the producer emits the value null, along with the current
channel state. Otherwise, it emits just( i ), where i is the first remaining input, and removes that
element from the passed channel state. The producer cannot see an EMIT message, and signals an
error if it should occur.
The consumer C is defined by the expression
λ ( x0 ) λ ( msg ) case msg {b′
1 | b′
2 | b′
3},

PREVIEW
30.4 Notes
281
where the first branch b′
1 is
EMIT · ⟨null, ⟨, os⟩⟩,→throw os to x0,
the second branch b′
2 is
EMIT · ⟨just( i ), ⟨is, os⟩⟩,→OK · ⟨is, cons( f ( i ) ; os )⟩,
and the third branch b′
3 is
OK · ,→error.
The consumer dispatches on the emitted datum. If it is absent, the output channel state is passed
to x0 as the overall value of the computation. If it is present, the function f (unspecified here)
of type τi ⇀τo is applied to transform the input to the output, and the result is added to the
output channel. If the message OK is received, the consumer signals an error, as the producer never
produces such a message.
The initial state s0 has the form OK · ⟨is, os⟩, where is and os are the initial input and output
channel state, respectively. The computation is created by the expression
run( ⟨P, C⟩)( s0 ),
which sets up the coroutines as described earlier.
Although it is relatively easy to visualize and implement coroutines involving only two part-
ners, it is more complex, and less useful, to consider a similar pattern of control among n ≥2 par-
ticipants. In such cases it is more common to structure the interaction as a collection of n routines,
each of which is a coroutine of a central scheduler. When a routine resumes its partner, it passes
control to the scheduler, which determines which routine to execute next, again as a coroutine of
itself. When structured as coroutines of a scheduler, the individual routines are called threads. A
thread yields control by resuming its partner, the scheduler, which then determines which thread
to execute next as a coroutine of itself. This pattern of control is called cooperative multi-threading,
because it is based on voluntary yields, rather than forced suspensions by a scheduler.
30.4
Notes
Continuations are a ubiquitous notion in programming languages. Reynolds (1993) provides an
excellent account of the multiple discoveries of continuations. The formulation given here is in-
spired by Felleisen and Hieb (1992), who pioneered the development of linguistic theories of con-
trol and state.
Exercises
30.1. Type safety for KPCF follows almost directly from Theorem 28.1. Isolate the key observa-
tions required to extend the proof to include continuation types.

PREVIEW
282
30.4 Notes
30.2. Exhibit a closed KPCF expression of each of the following types:
(a) τ + ( τ cont ).
(b) τ cont cont →τ.
(c) ( τ2 cont →τ1 cont ) →( τ1 →τ2 ).
(d) ( τ1 + τ2 ) cont →( τ1 cont × τ2 cont ).
(Compare exercise 13.1.)
30.3. The type stream of infinite streams of natural numbers defined in Chapter 15 can be imple-
mented using continuations. Define stream to be the recursive type satisfying the isomor-
phism
stream ∼= ( nat × stream ) cont cont.
To examine the front of the stream, throw to it a continuation expecting a natural number and
another stream. When passed such a continuation, the stream throws to it the next number
in the stream, paired with another stream (that is, another continuation) representing the
stream of numbers following that number. Define the introduction and elimination forms
for streams defined in Chapter 15 using this representation.

PREVIEW
Part XIII
Symbolic Data

PREVIEW

PREVIEW
Chapter 31
Symbols
A symbol is an atomic datum with no internal structure. Whereas a variable is given meaning
by substitution, a symbol is given meaning by a family of operations indexed by symbols. A
symbol is just a name, or index, for a family of operations. Many different interpretations may
be given to symbols according to the operations we choose to consider, giving rise to concepts
such as fluid binding, dynamic classification, mutable storage, and communication channels. A
type is associated to each symbol whose interpretation depends on the particular application. For
example, in the case of mutable storage, the type of a symbol constrains the contents of the cell
named by that symbol to values of that type.
In this chapter we consider two constructs for computing with symbols. The first is a means of
declaring new symbols for use within a specified scope. The expression newsym a ~ ρ in e introduces
a “new” symbol a with associated type ρ for use within e. The declared symbol a is “new” in
the sense that it is bound by the declaration within e, and so may be renamed at will to ensure
that it differs from any finite set of active symbols. Whereas the statics determines the scope of a
declared symbol, its range of significance, or extent, is determined by the dynamics. There are two
different dynamic interpretations of symbols, the scoped and the free (short for scope-free) dynamics.
The scoped dynamics limits the extent of the symbol to its scope; the lifetime of the symbol is
restricted to the evaluation of its scope. Alternatively, under the free dynamics the extent of a
symbol exceeds its scope, extending to the entire computation of which it is a part. We may say
that in the free dynamics a symbol “escapes its scope,” but it is more accurate to say that its scope
widens to encompass the rest of the computation.
The second construct associated with symbols is the concept of a symbol reference, an expression
whose purpose is to refer to a particular symbol. Symbol references are values of a type ρ sym and
are written ′a for some symbol a with associated type ρ. The elimination form for the type ρ sym
is a conditional branch that determines whether a symbol reference refers to a statically specified
symbol. The statics of the elimination form ensures that, in the positive case, the type associated
to the referenced symbol is manifested, whereas in the negative case, no type information can be
gleaned from the test.

PREVIEW
286
31.1 Symbol Declaration
31.1
Symbol Declaration
We will consider here an extension SPCF of PCF with the means to allocate new symbols. This
capability will be used in later chapters that use symbols for other purposes. Here we will only be
concerned with symbol allocation, and the introduction and elimination of symbols as values of a
type of plain symbols.
The syntax for symbol declaration in SPCF is given by the following grammar:
Exp
e
::=
newsym{τ}( a . e )
newsym a ~ τ in e
generation
The statics of symbol declaration makes use of a signature, or symbol context, that associates a type
to each of a finite set of symbols. We use the letter Σ to range over signatures, which are finite
sets of pairs a ~ τ, where a is a symbol and τ is a type. The typing judgment Γ ⊢Σ e : τ is
parameterized by a signature Σ associating types to symbols. In effect there is an infinite family
of typing judgments, one for each choice of Σ. The expression newsym a ~ τ in e shifts from one
instance of the family to another by adding a new symbol to Σ.
The statics of symbol declaration makes use of a judgment, τ mobile, whose definition depends
on whether the dynamics is scoped. In a scoped dynamics mobility is defined so that the computed
value of a mobile type cannot depend on any symbol. By constraining the scope of a declaration to
have mobile type, we can, under this interpretation, ensure that the extent of a symbol is confined
to its scope. In a free dynamics every type is deemed mobile, because the dynamics ensures that
the scope of a symbol is widened to accommodate the possibility that the value returned from the
scope of a declaration may depend on the declared symbol. The term “mobile” reflects the infor-
mal idea that symbols may or may not be “moved” from the scope of their declaration according
to the dynamics given to them. A free dynamics allows symbols to be moved freely, whereas a
scoped dynamics limits their range of motion.
The statics of symbol declaration itself is given by the following rule:
Γ ⊢Σ,a~ρ e : τ
τ mobile
Γ ⊢Σ newsym{ρ}( a . e ) : τ
(31.1)
As mentioned, the condition on τ ensures that the returned value does not escape its scope, if any.
31.1.1
Scoped Dynamics
The scoped dynamics of symbol declaration is given by a transition judgment of the form e 7−→
Σ
e′
indexed by a signature Σ specifying the active symbols of the transition. Either e or e′ may involve
the symbols declared in Σ, but no others.
e 7−−−→
Σ,a~ρ e′
newsym{ρ}( a . e ) 7−→
Σ
newsym{ρ}( a . e′ )
(31.2a)
e valΣ
newsym{ρ}( a . e ) 7−→
Σ
e
(31.2b)

PREVIEW
31.1 Symbol Declaration
287
Rule (31.2a) specifies that evaluation takes place within the scope of the declaration of a symbol.
Rule (31.2b) specifies that the declared symbol is “forgotten” once its scope has been evaluated.
The definition of the judgment τ mobile must be chosen to ensure that the following mobility
condition is satisfied:
If τ mobile, ⊢Σ,a~ρ e : τ, and e valΣ,a~ρ, then ⊢Σ e : τ and e valΣ.
For example, in the presence of symbol references (see Section 31.2 below), a function type cannot
be deemed mobile, because a function may contain a reference to a local symbol. The type nat
may only be deemed mobile if the successor is evaluated eagerly, for otherwise a symbol reference
may occur within a value of this type, invalidating the condition.
Theorem 31.1 (Preservation). If ⊢Σ e : τ and e 7−→
Σ
e′, then ⊢Σ e′ : τ.
Proof. By induction on the dynamics of symbol declaration. Rule (31.2a) follows by induction,
applying rule (31.1). Rule (31.2b) follows from the condition on mobility.
Theorem 31.2 (Progress). If ⊢Σ e : τ, then either e 7−→
Σ
e′, or e valΣ.
Proof. There is only one rule to consider, Rule (31.1). By induction we have either e 7−−−→
Σ,a~ρ
e′,
in which case rule (31.2a) applies, or e valΣ,a~ρ, in which case by the mobility condition we have
e valΣ, and hence rule (31.2b) applies.
31.1.2
Scope-Free Dynamics
The scope-free dynamics of symbols is defined by a transition system between states of the form
ν Σ { e }, where Σ is a signature and e is an expression over this signature. The judgment ν Σ { e } 7−→
ν Σ′ { e′ } states that evaluation of e relative to symbols Σ results in the expression e′ in the exten-
sion Σ′ of Σ.
ν Σ { newsym{ρ}( a . e ) } 7−→ν Σ, a ~ ρ { e }
(31.3)
Rule (31.3) specifies that symbol generation enriches the signature with the newly introduced sym-
bol by extending the signature for all future transitions.
All other rules of the dynamics are changed to account for the allocated symbols. For example,
the dynamics of function application cannot be inherited from Chapter 19, but is reformulated as
follows:
ν Σ { e1 } 7−→ν Σ′ { e′
1 }
ν Σ { e1( e2 ) } 7−→ν Σ′ { e′
1( e2 ) }
(31.4a)
ν Σ { λ ( x : τ ) e( e2 ) } 7−→ν Σ { [e2/x]e }
(31.4b)
These rules shuffle around the signature to account for symbol declarations within the constituent
expressions of the application. Similar rules are required for all other constructs of SPCF.

PREVIEW
288
31.2 Symbol References
Theorem 31.3 (Preservation). If ν Σ { e } 7−→ν Σ′ { e′ } and ⊢Σ e : τ, then Σ′ ⊇Σ and ⊢Σ′ e′ : τ.
Proof. There is only one rule to consider, rule (31.3), which is handled by inversion of rule (31.1).
Theorem 31.4 (Progress). If ⊢Σ e : τ, then either e valΣ or ν Σ { e } 7−→ν Σ′ { e′ } for some Σ′ and e′.
Proof. Immediate, by rule (31.3).
31.2
Symbol References
Symbols are not themselves values, but they may be used to form values. One useful example
is provided by the type τ sym of symbol references. A value of this type has the form ′a, where a
is a symbol in the signature. To compute with a reference we may branch according to whether
it is a reference to a specified symbol. The syntax of symbol references is given by the following
grammar:
Typ
τ
::=
sym( τ )
τ sym
symbols
Exp
e
quote[ a ]
′a
reference
is[ a ]{t . τ}( e ; e1 ; e2 )
if e is a then e1 ow e2
comparison
The expression quote[ a ] is a reference to the symbol a, a value of type sym( τ ). The expression
is[ a ]{t . τ}( e ; e1 ; e2 ) compares the value of e, which is a reference to some symbol b, with the
given symbol a. If b is a, the expression evaluates to e1, and otherwise to e2.
31.2.1
Statics
The typing rules for symbol references are as follows:
Γ ⊢Σ,a~ρ quote[ a ] : sym( ρ )
(31.5a)
Γ ⊢Σ,a~ρ e : sym( ρ′ )
Γ ⊢Σ,a~ρ e1 : [ρ/t]τ
Γ ⊢Σ,a~ρ e2 : [ρ′/t]τ
Γ ⊢Σ,a~ρ is[ a ]{t . τ}( e ; e1 ; e2 ) : [ρ′/t]τ
(31.5b)
Rule (31.5a) is the introduction rule for the type sym( ρ ). It states that if a is a symbol with associ-
ated type ρ, then quote[ a ] is an expression of type sym( ρ ). Rule (31.5b) is the elimination rule for
the type sym( ρ ). The type associated to the given symbol a need not be the same as the type of
the symbol referred to by the expression e. If e evaluates to a reference to a, then these types will
coincide, but if it refers to another symbol, a′ ̸= a, then these types may well differ.
With this in mind, consider rule (31.5b). A priori there is a discrepancy between the type ρ
of a and the type ρ′ of the symbol referred to by e. This discrepancy is mediated by the type
operator t . τ.1 Regardless of the outcome of the comparison, the overall type of the expression is
1See Chapter 14 for a discussion of type operators.

PREVIEW
31.2 Symbol References
289
[ρ′/t]τ. If e evaluates to the symbol a, then we “learn” that the types ρ′ and ρ coincide, because the
specified and referenced symbol coincide. This coincidence is reflected by the type [ρ/t]τ for e1. If
e evaluates to some other symbol, a′ ̸= a, then the comparison evaluates to e2, which is required to
have type [ρ′/t]τ; no further information about the type of the symbol is acquired in this branch.
31.2.2
Dynamics
The (scoped) dynamics of symbol references is given by the following rules:
quote[ a ] valΣ,a~ρ
(31.6a)
is[ a ]{t . τ}( quote[ a ] ; e1 ; e2 ) 7−−−→
Σ,a~ρ e1
(31.6b)
is[ a ]{t . τ}( quote[ a′ ] ; e1 ; e2 ) 7−−−−−−−→
Σ,a~ρ,a′~ρ′
e2
(31.6c)
e 7−−−→
Σ,a~ρ e′
is[ a ]{t . τ}( e ; e1 ; e2 ) 7−−−→
Σ,a~ρ is[ a ]{t . τ}( e′ ; e1 ; e2 )
(31.6d)
Rules (31.6b) and (31.6c) specify that is[ a ]{t . τ}( e ; e1 ; e2 ) branches according to whether the
value of e is a reference to the symbol a.
31.2.3
Safety
To ensure that the mobility condition is satisfied, it is important that symbol reference types not be
deemed mobile.
Theorem 31.5 (Preservation). If ⊢Σ e : τ and e 7−→
Σ
e′, then ⊢Σ e′ : τ.
Proof. By rule induction on rules (31.6). The most interesting case is rule (31.6b). When the com-
parison is positive, the types ρ and ρ′ must be the same, because each symbol has at most one
associated type. Therefore, e1, which has type [ρ′/t]τ, also has type [ρ/t]τ, as required.
Lemma 31.6 (Canonical Forms). If ⊢Σ e : sym( ρ ) and e valΣ, then e = quote[ a ] for some a such that
Σ = Σ′, a ~ ρ.
Proof. By rule induction on rules (31.5), taking account of the definition of values.
Theorem 31.7 (Progress). Suppose that ⊢Σ e : τ. Then either e valΣ, or there exists e′ such that e 7−→
Σ
e′.

PREVIEW
290
31.3 Notes
Proof. By rule induction on rules (31.5). For example, consider rule (31.5b), in which we have
that is[ a ]{t . τ}( e ; e1 ; e2 ) has some type τ and that e : sym( ρ ) for some ρ. By induction either
rule (31.6d) applies, or else we have that e valΣ, in which case we are assured by Lemma 31.6 that e
is quote[ a ] for some symbol b of type ρ declared in Σ. But then progress is assured by rules (31.6b)
and (31.6c), because equality of symbols is decidable (either a is b or it is not).
31.3
Notes
The concept of a symbol in a programming language was considered by McCarthy in the original
formulation of Lisp (McCarthy, 1965). Unfortunately, symbols were not clearly distinguished from
variables, leading to unexpected behaviors (see Chapter 32). The present account of symbols was
influenced by Pitts and Stark (1993) on the declaration of names in the π-calculus (Milner, 1999).
The associated type of a symbol may be used for applications that associate information with the
symbol, such as its fluid binding (see Chapter 32) or its string representation (its “print name” in
Lisp jargon).
Exercises
31.1. The elimination form for symbol references given in Section 31.2 is “one-sided” in the sense
that one may compare a reference to an unknown symbol to a known symbol with a known
type. An alternative elimination form provides an equality test on symbol references. Formu-
late such a variation.
31.2. A list of type ( τ sym × τ ) list is called an association list. Using your solution to Exercise 31.1
define a function find that sends an association list to a mapping of type τ sym ⇀τ opt.
31.3. It would be more efficient to represent an association list by a balanced tree associating values
to symbols, but to do so would require a total ordering on symbols (at least among the
symbols with the same associated type). What obstacles arise when introducing a linear
ordering on symbols?
31.4. In Lisp a symbolic expression, or s-expression, or sexpr, may be thought of as a value of the
recursive type
sexpr ≜rec s is [sym ,→sym( unit ) , nil ,→unit , cons ,→s × s].
It is customary to write cons( e0 ; e1 ) for fold( cons · ⟨e0, e1⟩), where e0 : sexpr and e1 :
sexpr, and to write nil for fold( nil · ⟨⟩). The list notation ( e0, . . . , en−1 ) is then used as
shorthand for the s-expression
cons( e0 ; . . . cons( en−1 ; nil ) . . . ).
Because lists involving symbols arise often, it is customary to extend the quotation notation
from symbols to general s-expressions so that one need not quote each symbol contained

PREVIEW
31.3 Notes
291
within it. Give the definition of this extension, and work out its meaning for the special case
of lists described above.

PREVIEW
292
31.3 Notes

PREVIEW
Chapter 32
Fluid Binding
In this chapter we return to the concept of dynamic scoping of variables that was criticized in
Chapter 8. There it was observed that dynamic scoping is problematic for at least two reasons.
One is that renaming of bound variables is not respected; another is that dynamic scope is not
type safe. These violations of the expected behavior of variables is intolerable, because they are at
variance with mathematical practice and because they compromise modularity.
It is possible, however, to recover a type-safe analog of dynamic scoping by divorcing it from
the concept of a variable, and instead introducing a new mechanism, called fluid binding. Fluid
binding associates to a symbol (and not a variable) a value of a specified type within a specified
scope. The identification principle for bound variables is retained, type safety is not compromised,
yet some of the benefits of dynamic scoping are preserved.
32.1
Statics
To account for fluid binding we enrich SPCF defined in Chapter 31 with these constructs to obtain
FSPCF:
Exp
e
::=
put[ a ]( e1 ; e2 )
put e1 for a in e2
binding
get[ a ]
get a
retrieval
The expression get[ a ] evaluates to the value of the current binding of a, if it has one, and is stuck
otherwise. The expression put[ a ]( e1 ; e2 ) binds the symbol a to the value e1 for the duration of
the evaluation of e2, at which point the binding of a reverts to what it was prior to the execution.
The symbol a is not bound by the put expression, but is instead a parameter of it.
The statics of FSPCF is defined by judgments of the form
Γ ⊢Σ e : τ,
much as in Chapter 31. Thus Σ is here defined to be a finite set of declarations of the form a ~ τ
such that no symbol is declared more than once in the same signature. Note that the association
of a type to a symbol is not a typing assumption. In particular the signature Σ enjoys no structural
properties, and cannot be considered as a form of hypothesis as defined in Chapter 3.

PREVIEW
294
32.2 Dynamics
The following rules govern the new expression forms:
Γ ⊢Σ,a~τ get[ a ] : τ
(32.1a)
Γ ⊢Σ,a~τ1 e1 : τ1
Γ ⊢Σ,a~τ1 e2 : τ2
Γ ⊢Σ,a~τ1 put[ a ]( e1 ; e2 ) : τ2
(32.1b)
Rule (32.1b) specifies that the symbol a is a parameter of the expression that must be declared in
Σ.
32.2
Dynamics
The dynamics of FSPCF relies on a stack-like allocation of symbols in SPCF, and maintains an
association of values to symbols that tracks this stack-like allocation discipline. To do so we define
a family of transition judgments of the form e
µ7−→
Σ
e′, where Σ is as in the statics, and µ is a finite
function mapping some subset of the symbols declared in Σ to values of the right type. If µ is
defined for some symbol a, then it has the form µ′ ⊗a ,→e for some µ′ and value e. If µ is undefined
for some symbol a, we may regard it as having the form µ′ ⊗a ,→•. We will write a ,→
to stand
for either a ,→• or a ,→e for some expression e.
The dynamics of FSPCF is defined by the following rules:
get[ a ]
µ⊗a,→e
7−−−−→
Σ,a~τ
e
(32.2a)
e1
µ
7−−−→
Σ,a~τ
e′
1
put[ a ]( e1 ; e2 )
µ
7−−−→
Σ,a~τ put[ a ]( e′
1 ; e2 )
(32.2b)
e1 valΣ,a~τ
e2
µ⊗a,→e1
7−−−−−→
Σ,a~τ
e′
2
put[ a ]( e1 ; e2 )
µ⊗a,→
7−−−−→
Σ,a~τ
put[ a ]( e1 ; e′
2 )
(32.2c)
e1 valΣ,a~τ
e2 valΣ,a~τ
put[ a ]( e1 ; e2 )
µ
7−−−→
Σ,a~τ
e2
(32.2d)
Rule (32.2a) specifies that get[ a ] evaluates to the current binding of a, if any. Rule (32.2b) specifies
that the binding for the symbol a is evaluated before the binding is created. Rule (32.2c) evaluates
e2 in an environment where the symbol a is bound to the value e1, regardless of whether or not a is
already bound in the environment. Rule (32.2d) eliminates the fluid binding for a once evaluation
of the extent of the binding has completed.

PREVIEW
32.3 Type Safety
295
According to the dynamics of FSPCF given by rules (32.2), there is no transition of the form
get[ a ]
µ7−→
Σ
e if µ(a) = •. The judgment e unboundΣ states that execution of e will lead to such a
“stuck” state, and is inductively defined by the following rules:
µ(a) = •
get[ a ] unboundµ
(32.3a)
e1 unboundµ
put[ a ]( e1 ; e2 ) unboundµ
(32.3b)
e1 valΣ
e2 unboundµ
put[ a ]( e1 ; e2 ) unboundµ
(32.3c)
In a larger language it would also be necessary to include error propagation rules of the sort
discussed in Chapter 6.
32.3
Type Safety
We first define the auxiliary judgment µ : Σ by the following rules:
∅: ∅
(32.4a)
⊢Σ e : τ
µ : Σ
µ ⊗a ,→e : Σ, a ~ τ
(32.4b)
µ : Σ
µ ⊗a ,→• : Σ, a ~ τ
(32.4c)
These rules specify that if a symbol is bound to a value, then that value must be of the type associ-
ated to the symbol by Σ. No demand is made in the case that the symbol is unbound (equivalently,
bound to a “black hole”).
Theorem 32.1 (Preservation). If e
µ7−→
Σ e′, where µ : Σ and ⊢Σ e : τ, then ⊢Σ e′ : τ.
Proof. By rule induction on rules (32.2). Rule (32.2a) is handled by the definition of µ : Σ. Rule (32.2b)
follows by induction. Rule (32.2d) is handled by inversion of rules (32.1). Finally, rule (32.2c) is
handled by inversion of rules (32.1) and induction.
Theorem 32.2 (Progress). If ⊢Σ e : τ and µ : Σ, then either e valΣ, or e unboundµ, or there exists e′ such
that e
µ7−→
Σ e′.
Proof. By induction on rules (32.1). For rule (32.1a), we have Σ ⊢a ~ τ from the premise of the rule,
and hence, because µ : Σ, we have either µ(a) = • or µ(a) = e for some e such that ⊢Σ e : τ. In the
former case we have e unboundµ, and in the latter we have get[ a ]
µ7−→
Σ
e. For rule (32.1b), we have

PREVIEW
296
32.4 Some Subtleties
by induction that either e1 valΣ or e1 unboundµ, or e1
µ7−→
Σ
e′
1. In the latter two cases we may apply
rule (32.2b) or rule (32.3b), respectively. If e1 valΣ, we apply induction to obtain that either e2 valΣ,
in which case rule (32.2d) applies; e2 unboundµ, in which case rule (32.3c) applies; or e2
µ7−→
Σ
e′
2, in
which case rule (32.2c) applies.
32.4
Some Subtleties
The value of put e1 for a in e2 is the value of e2, calculated in a context where a is bound to the
value of e1. If e2 is of a basic type, such as nat, then the reversion of the binding of a cannot
influence the meaning of the result.1
But what if the type of put e1 for a in e2 is a function type, so that the returned value is a λ-
abstraction? The body of the returned λ may refer to the binding of a, which is reverted upon
return from the put. For example, consider the expression
put 17 for a in λ ( x : nat ) x + get a,
(32.5)
which has type nat ⇀nat, given that a is a symbol of type nat. Let us assume, for the sake of
discussion, that a is unbound at the point at which this expression is evaluated. Evaluating the put
binds a to the number 17, and returns the function λ ( x : nat ) x + get a. But because a is reverted
to its unbound state upon exiting the put, applying this function to an argument will result in an
error, unless a binding for a is given. Thus, if f is bound to the result of evaluating (32.5), then the
expression
put 21 for a in f ( 7 )
(32.6)
will evaluate to 28, whereas evaluation of f ( 7 ) in the absence of a surrounding binding for a will
incur an error.
Contrast this with the similar expression
let y be 17 in λ ( x : nat ) x + y,
(32.7)
where we have replaced the fluid-bound symbol a by a statically bound variable y. This expression
evaluates to λ ( x : nat ) x + 17, which adds 17 to its argument when applied. There is no possibility
of an unbound symbol arising at execution time, because variables are interpreted by substitution.
One way to think about this situation is to consider that fluid-bound symbols serve as an al-
ternative to passing extra arguments to a function to specialize its value when it is called. To see
this, let e stand for the value of expression (32.5), a λ-abstraction whose body is dependent on the
binding of the symbol a. To use this function safely, it is necessary that the programmer provide a
binding for a prior to calling it. For example, the expression
put 7 for a in ( e( 9 ) )
1As long as the successor is evaluated eagerly; if not, the following examples are adaptable to situations where the value
of e2 is a lazily evaluated number.

PREVIEW
32.5 Fluid References
297
evaluates to 16, and the expression
put 8 for a in ( e( 9 ) )
evaluates to 17. Writing just e( 9 ), without a surrounding binding for a, results in a run-time error
attempting to retrieve the binding of the unbound symbol a.
This behavior can be simulated by adding an argument to the function value that will be bound
to the current binding of the symbol a at the point where the function is called. Instead of using
fluid binding, we would provide an extra argument at each call site, writing
e′( 7 )( 9 )
and
e′( 8 )( 9 ),
respectively, where e′ is the λ-abstraction
λ ( y : nat ) λ ( x : nat ) x + y.
Adding arguments can be cumbersome, though, especially when several call sites provide the
same binding for a. Using fluid binding we may write
put 7 for a in ⟨e( 8 ), e( 9 )⟩,
whereas using an extra argument we must write
⟨e′( 7 )( 8 ), e′( 7 )( 9 )⟩.
However, such redundancy can be reduced by factoring out the common part, writing
let f be e′( 7 ) in ⟨f ( 8 ), f ( 9 )⟩.
The awkwardness of this simulation is usually taken as an argument in favor of including fluid
binding in a language. The drawback, which is often perceived as an advantage, is that nothing in
the type of a function reveals its dependency on the binding of a symbol. It is therefore quite easy
to forget that such a binding is required, leading to run-time failures that might better be caught
at compile time.
32.5
Fluid References
The get and put operations for fluid binding are indexed by a symbol that must be given as part of
the syntax of the operator. It is sometimes useful to defer until runtime the choice of fluid on which
a get or put acts. References to fluids allow the name of the fluid to be a value. References come
equipped with analogs of the get and put primitives, but for a dynamically determined symbol.

PREVIEW
298
32.5 Fluid References
We may extend FSPCF with fluid references by adding the following syntax:
Typ
τ
::=
fluid( τ )
τ fluid
fluid
Exp
e
::=
fl[ a ]
& a
reference
getfl( e )
getfl e
retrieval
putfl( e ; e1 ; e2 )
putfl e is e1 in e2
binding
The expression fl[ a ] is the symbol a considered as a value of type fluid( τ ). The expressions
getfl( e ) and putfl( e ; e1 ; e2 ) are analogs of the get and put operations for fluid-bound symbols.
The statics of these constructs is given by the following rules:
Γ ⊢Σ,a~τ fl[ a ] : fluid( τ )
(32.8a)
Γ ⊢Σ e : fluid( τ )
Γ ⊢Σ getfl( e ) : τ
(32.8b)
Γ ⊢Σ e : fluid( τ )
Γ ⊢Σ e1 : τ
Γ ⊢Σ e2 : τ2
Γ ⊢Σ putfl( e ; e1 ; e2 ) : τ2
(32.8c)
Because we are using a scoped dynamics, references to fluids cannot be deemed mobile.
The dynamics of references consists of resolving the referent and deferring to the underlying
primitives acting on symbols.
fl[ a ] valΣ,a~τ
(32.9a)
e
µ7−→
Σ e′
getfl( e )
µ7−→
Σ getfl( e′ )
(32.9b)
getfl( fl[ a ] )
µ7−→
Σ get[ a ]
(32.9c)
e
µ7−→
Σ e′
putfl( e ; e1 ; e2 )
µ7−→
Σ putfl( e′ ; e1 ; e2 )
(32.9d)
putfl( fl[ a ] ; e1 ; e2 )
µ7−→
Σ put[ a ]( e1 ; e2 )
(32.9e)

PREVIEW
32.6 Notes
299
32.6
Notes
Dynamic binding arose in early dialects of Lisp from not distinguishing variables from symbols.
When separated, variables retain their substitutive meaning, and symbols give rise to a separate
concept of fluid binding. Allen (1978) discusses the implementation of fluid binding. The present
formulation here draws on Nanevski (2003).
Exercises
32.1. Deep binding is an implementation of fluid binding where the value associated to a symbol
is stored on the control stack as part of a put frame, and is retrieved by finding the most
recent such association. Define a stack machine for FSPCF that implements deep binding by
extending the FPCF machine. Be sure to consider newfl as well as put and get. Attempting
to get the binding for an unbound symbol signals a failure; otherwise, its most recent binding
is returned. Where do the issues discussed in Section 32.4 arise? Hint: you will need to
introduce an auxiliary judgment k ≥k′ ? a, which searches for the binding of the symbol a
on the stack k′, returning its value (or failure) to the stack k.
32.2. Shallow binding is an implementation of fluid binding that maintains a mapping sending
each active symbol to a stack of values, the topmost being the active binding for that symbol.
Define a stack machine for FSPCF that maintains such a mapping to facilitate access to the
binding of a symbol. Hint: use evaluation states of the form µ ∥k ▷e, where µ is a mapping
each symbol a allocated on k to a stack of values, the topmost element of which, if any, is the
current binding of a. Use similar forms of return and fail states, and ensure that the mapping
invariant is maintained.
32.3. Exception handlers can be implemented by combining fluid binding with continuations
(Chapter 30). Reserve a single fluid-bound symbol hdlr that is always bound to the ac-
tive exception handler, which is represented by a continuation accepting a value of type exn.
Raising an exception consists of throwing an exception value to this continuation. When en-
tering the scope of a handler, a continuation representing the “otherwise” clause is put as the
binding of hdlr. Give a precise formulation of exception handling based on this summary.
Hint: it is important to ensure that the current handler is maintained for both normal and
exceptional returns.

PREVIEW
300
32.6 Notes

PREVIEW
Chapter 33
Dynamic Classification
In Chapters 11 and 26 we investigated the use of sums for classifying values of disparate type.
Every value of a classified type is labeled with a symbol that determines the type of the instance
data. A classified value is decomposed by pattern matching against a known class, which reveals
the type of the instance data. Under this representation the possible classes of an object are deter-
mined statically by its type. However, it is sometimes useful to allow the possible classes of data
value to be determined dynamically.
Dynamic generation of classes has many applications, most of which derive from the guaran-
tee that a newly allocated class is distinct from all others that have been or ever will be generated.
In this regard a dynamic class is a “secret” whose disclosure can be used to limit the flow of infor-
mation in a program. In particular a dynamically classified value is opaque unless its identity has
been disclosed by its creator. Thus dynamic classification can be used to ensure that an exception
reaches only its intended handler, or that a message on a communication channel reaches only the
intended recipient.
33.1
Dynamic Classes
A dynamic class is a symbol that is generated at run-time. A classified value consists of a symbol
of type τ together with a value of that type. To compute with a classified value, it is compared with
a known class. If the value is of this class, the underlying instance data is passed to the positive
branch, otherwise the negative branch is taken, where it is matched against other known classes.
33.1.1
Statics
The syntax of dynamic classification is given by the following grammar:
Typ
τ
::=
clsfd
clsfd
classified
Exp
e
::=
in[ a ]( e )
a · e
instance
isin[ a ]( e ; x . e1 ; e2 )
match e as a · x ,→e1 ow ,→e2
comparison

PREVIEW
302
33.1 Dynamic Classes
The expression in[ a ]( e ) is a classified value with class a and underlying value e. The expression
isin[ a ]( e ; x . e1 ; e2 ) checks whether the class of the value given by e is a. If so, the classified value
is passed to e1; if not, the expression e2 is evaluated instead.
The statics of dynamic classification is defined by these rules:
Γ ⊢Σ,a~τ e : τ
Γ ⊢Σ,a~τ in[ a ]( e ) : clsfd
(33.1a)
Γ ⊢Σ,a~τ e : clsfd
Γ, x : τ ⊢Σ,a~τ e1 : τ′
Γ ⊢Σ,a~τ e2 : τ′
Γ ⊢Σ,a~τ isin[ a ]( e ; x . e1 ; e2 ) : τ′
(33.1b)
The typing judgment is indexed by a signature associating a type to each symbol. Here the type
governs the instance data associated to each symbol.
33.1.2
Dynamics
To maximize the flexibility in using dynamic classification, we will consider a free dynamics for
symbol generation. Within this framework the dynamics of classification is given by the following
rules:
e valΣ,a~τ
in[ a ]( e ) valΣ,a~τ
(33.2a)
ν Σ, a ~ τ { e } 7−→ν Σ′, a ~ τ { e′ }
ν Σ, a ~ τ { in[ a ]( e ) } 7−→ν Σ′, a ~ τ { in[ a ]( e′ ) }
(33.2b)
e valΣ,a~τ
ν Σ, a ~ τ { isin[ a ]( in[ a ]( e ) ; x . e1 ; e2 ) } 7−→ν Σ, a ~ τ { [e/x]e1 }
(33.2c)
e′ valΣ,a~τ,a′~τ′
ν Σ, a ~ τ, a′ ~ τ′ { isin[ a ]( in[ a′ ]( e′ ) ; x . e1 ; e2 ) } 7−→ν Σ, a ~ τ, a′ ~ τ′ { e2 }
(33.2d)
ν Σ, a ~ τ { e } 7−→ν Σ′, a ~ τ { e′ }
ν Σ, a ~ τ { isin[ a ]( e ; x . e1 ; e2 ) } 7−→ν Σ′, a ~ τ { isin[ a ]( e′ ; x . e1 ; e2 ) }
(33.2e)
Each rule makes explicit the symbols involved in the transition. Importantly, Rule (33.2d) specifies
two symbols, a and a′, both of which occur in the case analysis. These two symbols are necessarily
distinct, there being no two declarations for the same symbol in any signature, and the transition
is to the negative branch.
This example illustrates the necessity to prohibit substitution of one symbol for another, for
doing so can cause two distinct symbols to become the same, invalidating the transition. To see
what goes wrong, consider the expression
match b · ⟨⟩as a ·
,→true ow ,→match a′ · ⟨⟩as a′ ·
,→false ow ,→true,

PREVIEW
33.2 Class References
303
which evaluates to false, because the outer conditional is on the class a, which differs from the
class a′. If it were permissible to substitute a′ for a, though, the result would be the expression
match b · ⟨⟩as a′ ·
,→true ow ,→match a′ · ⟨⟩as a′ ·
,→false ow ,→true,
which evaluates to true! Because transition is not stable under such substitutions (the answer
would change), it is not permitted to perform substitutions of symbols for symbols.
33.1.3
Safety
Theorem 33.1 (Safety).
1. If ⊢Σ e : τ and ν Σ { e } 7−→ν Σ′ { e′ }, then Σ′ ⊇Σ and ⊢Σ′ e′ : τ.
2. If ⊢Σ e : τ, then either e valΣ or ν Σ { e } 7−→ν Σ′ { e′ } for some e′ and Σ′.
Proof. Similar to the safety proofs given in Chapters 11 and 31.
33.2
Class References
The type cls( τ ) has as values references to classes.
Typ
τ
::=
cls( τ )
τ cls
class reference
Exp
e
::=
cls[ a ]
& a
reference
inref( e1 ; e2 )
inref( e1 ; e2 )
instance
isinref( e0 ; e1 ; x . e2 ; e3 )
isinref( e0 ; e1 ; x . e2 ; e3 )
dispatch
The statics of these constructs is given by the following rules:
Γ ⊢Σ,a~τ cls[ a ] : cls( τ )
(33.3a)
Γ ⊢Σ e1 : cls( τ )
Γ ⊢Σ e2 : τ
Γ ⊢Σ inref( e1 ; e2 ) : clsfd
(33.3b)
Γ ⊢Σ e0 : cls( τ )
Γ ⊢Σ e1 : clsfd
Γ, x : τ ⊢Σ e2 : τ′
Γ ⊢Σ e3 : τ′
Γ ⊢Σ isinref( e0 ; e1 ; x . e2 ; e3 ) : τ′
(33.3c)
The corresponding dynamics is given by these rules:
ν Σ { e1 } 7−→ν Σ′ { e′
1 }
ν Σ { inref( e1 ; e2 ) } 7−→ν Σ′ { inref( e′
1 ; e2 ) }
(33.4a)
e1 valΣ
ν Σ { e2 } 7−→ν Σ′ { e′
2 }
ν Σ { inref( e1 ; e2 ) } 7−→ν Σ′ { inref( e1 ; e′
2 ) }
(33.4b)

PREVIEW
304
33.3 Definability of Dynamic Classes
e valΣ
ν Σ { inref( cls[ a ] ; e ) } 7−→ν Σ { in[ a ]( e ) }
(33.4c)
ν Σ { e0 } 7−→ν Σ′ { e′
0 }
ν Σ { isinref( e0 ; e1 ; x . e2 ; e3 ) } 7−→ν Σ′ { isinref( e′
0 ; e1 ; x . e2 ; e3 ) }
(33.4d)
ν Σ { isinref( cls[ a ] ; e1 ; x . e2 ; e3 ) } 7−→ν Σ { isin[ a ]( e1 ; x . e2 ; e3 ) }
(33.4e)
Rules (33.4d) and (33.4e) specify that the first argument is evaluated to determine the target class,
which is then used to check whether the second argument, a classified data value, is of the target
class. This formulation is a two-stage process in which e0 determines the pattern against which to
match the classified value of e1.
33.3
Definability of Dynamic Classes
The type clsfd can be defined in terms of symbolic references, product types, and existential types
by the type expression
clsfd ≜∃( t.t sym × t ).
The introduction form in[ a ]( e ), in which a is a symbol with associated type is τ and e is an
expression of type τ, is defined to be the package
pack τ with ⟨′a, e⟩as ∃( t.t sym × t ).
(33.5)
The elimination form isin[ a ]( e ; x . e1 ; e2 ), of some type τ′, and where the type associated to a is τ,
is defined in terms of symbol comparison (see Chapter 31), together with existential and product
elimination, and function types. By rule (33.1b) the type of e is clsfd, which is now the existential
type (33.5). Similarly, the branches both have the overall type τ′, and within e1 the variable x has
type τ. The elimination form for the type clsfd is defined to be
open e as t with ⟨x, y⟩:t sym × t in ( ebody( y ) ),
where ebody is an expression to be defined shortly. It opens the package e which is an element of
the type (33.5), decomposing it into a type t a symbol reference x of type t sym, and an associated
value y of type t. The expression ebody will turn out to have the type t ⇀τ′ so that the application
to y will be type correct.
The expression ebody compares the symbolic reference x to the symbol a of type τ, yielding a
value of type t ⇀τ′. The expression ebody is
is[ a ]{u . u ⇀τ′}( x ; e′
1 ; e′
2 ),
where, as specified by rule (31.5b), e′
1 has type [τ/u]( u ⇀τ′ ) = τ ⇀τ′, and e′
2 has type [t/u]( u ⇀τ′ ) =
t ⇀τ′. The expression e′
1 “knows” that the abstract type t is τ, the type associated to the symbol

PREVIEW
33.4 Applications of Dynamic Classification
305
a, because the comparison is positive. On the other hand e′
2 does not “learn” anything about the
type t.
It remains to choose the expressions e′
1 and e′
2. In the case of a positive comparison, we wish to
pass the classified value to the expression e1 by substitution for the variable x. We therefore define
e′
1 to be the expression
λ ( x : τ ) e1 : τ ⇀τ′.
In the case of a negative comparison no value is propagated to e2. We therefore define e′
2 to be the
expression
λ ( : t ) e2 : t ⇀τ′.
We may then check that the statics and dynamics given in Section 33.1 are derivable under these
definitions.
33.4
Applications of Dynamic Classification
Dynamic classification has a number of interesting applications in programming. The most ob-
vious is to generalize dynamic dispatch (Chapter 26) to support computation over a dynamically
extensible type of heterogeneous values. Introducing a new class requires introducing a new row
in the dispatch matrix defining the behavior of the methods on the newly defined class. To allow
for this the rows of the matrix must be indexed by class references, rather than classes, so that it is
accessible without knowing statically the class.
Another application is to use dynamic classification as a form of “perfect encryption” that
ensures that classified values can neither be constructed nor deconstructed without knowing the
class in question. Abstract encryption of this form can be used to ensure privacy of communication
among the parties in a computation. One example of such a scenario is in channel-based commu-
nication, as will be considered in Chapter 40. Another, less obvious, application is to ensure that
an exception value may only be received by the intended handler, and no other.
33.4.1
Classifying Secrets
Dynamic classification can be used to enforce confidentiality and integrity of data values in a pro-
gram. A value of type clsfd may only be constructed by sealing it with some class a and may only
be deconstructed by a case analysis that includes a branch for a. By controlling which parties in
a multi-party interaction have access to the classifier a we may control how classified values are
created (ensuring their integrity) and how they are inspected (ensuring their confidentiality). Any
party that lacks access to a cannot decipher a value classified by a, nor may it create a classified
value with this class. Because classes are dynamically generated symbols, they offer an absolute
confidentiality guarantee among parties in a computation.1
Consider the following simple protocol for controlling the integrity and confidentiality of data
in a program. A fresh symbol a is introduced, and we return a pair of functions of type
( τ ⇀clsfd ) × ( clsfd ⇀τ opt ),
1Of course, this guarantee is for programs written in conformance with the statics given here. If the abstraction imposed
by the type system is violated, no guarantees of confidentiality can be made.

PREVIEW
306
33.5 Notes
called the constructor and destructor functions for that class, which is accomplished by writing
newcls a ∼τ in
⟨λ ( x : τ ) a · x,
λ ( x : clsfd ) match x as a · y ,→just( y ) ow ,→null ⟩.
The first function creates a value classified by a, and the second function recovers the instance data
of a value classified by a. Outside of the scope of the declaration the symbol a is an unguessable
secret.
To enforce the integrity of a value of type τ it suffices to ensure that only trusted parties have
access to the constructor. To enforce the confidentiality of a value of type τ it suffices to ensure that
only trusted parties have access to the destructor. Ensuring the integrity of a value amounts to
associating an invariant to it that is maintained by the trusted parties that may create an instance
of that class. Ensuring the confidentiality of a value amounts to propagating the invariant to
parties that may decipher it.
33.4.2
Exception Values
Exception handling is a communication between two agents, one that may raise an exception, and
one that may handle it. It is useful to ensure that an exception can be caught only by a designated
handler, without fear that any intervening handler may intercept it. This secrecy property can
be ensured by using dynamic class allocation. A new class is declared, with the ability to create
an instance given only to the raising agent and the ability to match an instance given only to the
handler. The exception value cannot be intercepted by any other handler, because no other handler
is capable of matching it. This property is crucial to “black box” composition of programs from
components. Without dynamic classification one can never be sure that alien code cannot intercept
an exception intended for a handler within one’s own code, or vice versa.
With this in mind, let us now reconsider the choice of the type exn of exception values specified
in Chapter 29. There we distinguished the closed-world assumption, which amounts to defining
exn to be a finite sum type known to the whole program, from the open-world assumption, which is
realized by defining exn to be the type clsfd of dynamically classified values. This choice supports
modularity and evolution by allowing fresh exceptions (classes) to be allocated at will, avoiding
the need for an up-front agreement on the forms of exception. Another perspective is that dynamic
classification treats an exception as a shared secret between the handler and the raiser of that
exception. When an exception value is raised, it can only be intercepted and analyzed by a handler
that can match the value against the specified class. It is only by using dynamic classification that
one can gain control over the flow of information in a program that uses exceptions. Without it, an
unintended handler can intercept an exception that was not intended for it, disrupting the logic of
the program.
33.5
Notes
Dynamic classification appears in Standard ML (Milner et al., 1997) as the type exn. The usefulness
of the type exn is obscured by its too-close association with the exception mechanism. The π-

PREVIEW
33.5 Notes
307
calculus (Milner, 1999) promoted using “name generation” and “channel passing” to control the
connectivity and information flow in a process network. In Chapter 40 we shall make explicit that
this aspect of the π-calculus is an application of dynamic classification.
Exercises
33.1. Consider the following open-world named exception mechanism, which is typical of the excep-
tion mechanism found in many languages.
dclexc a of τ in e
declare an exception
raiseexc a · e
raise exception a with value e
tryexc e ow a1 · x1 ,→e1 | . . . | an · xn ,→en | x ,→e′
handle exceptions a1, . . . , an
Exceptions are declared by name, specifying the type of their associated values. Each ex-
ecution of an exception declaration generates a fresh exception. An exception is raised by
specifying the exception name and a value to associate with it. The handler intercepts any fi-
nite number of named exceptions, passing their associated values to handlers, and otherwise
propagates the exception to the default handler.
The following rules define the statics of these constructs:
Γ ⊢Σ,a~τ e : τ′
Γ ⊢Σ dclexc τ of a in e : τ′
(33.6a)
Σ ⊢a ~ τ
Γ ⊢Σ e : τ
Γ ⊢Σ raiseexc a · e : τ′
(33.6b)
Σ ⊢a1 ~ τ1
. . .
Σ ⊢an ~ τn
Γ ⊢Σ e : τ′
Γ, x1 : τ1 ⊢Σ e1 : τ′
. . .
Γ, xn : τn ⊢en : τ′
Γ, x : exn ⊢e′ : τ′
Γ ⊢Σ tryexc e ow a1 · x1 ,→e1 | . . . | an · xn ,→en | x ,→e′ : τ′
(33.6c)
Give an implementation of named exceptions in terms of dynamic classification and general
value-passing exceptions (Chapter 29).
33.2. Show that dynamic classification with dynamic classes can be implemented in the combi-
nation of FPC and FEω enriched with references to free assignables, and with no modal
separation (so as to permit benign effects). Specifically, provide a package of the following
higher-kind existential type:
τ ≜∃clsfd :: Ty.∃class :: Ty →Ty . ⟨newcls ,→τnewcls , inref ,→τinref , isinref ,→τisinref⟩.
where
τnewcls ≜∀( t . cls[ t ] )
τinref ≜∀( t . ( cls[ t ] × t ) →clsfd )
τisinref ≜∀( t . ∀( u . ( cls[ t ] × clsfd × ( t →u ) × u ) →u ) ).

PREVIEW
308
33.5 Notes
These operations correspond to the mechanisms of dynamic classification described earlier
in this chapter. Hint: Define cls[ t ] to be t opt ref, and define clsfd so that a classified value
is represented by an encapsulated assignment to its class. Creating a new class allocates a
reference, creating a classified value creates an encapsulated assignment, and testing for a
class is implemented by assigning to the target class, then running the classified value, and
seeing whether the contents of the target class has changed.

PREVIEW
Part XIV
Mutable State

PREVIEW

PREVIEW
Chapter 34
Modernized Algol
Modernized Algol, or MA, is an imperative, block-structured programming language based on
the classic language Algol. MA extends PCF with a new syntactic sort of commands that act on
assignables by retrieving and altering their contents. Assignables are introduced by declaring them
for use within a specified scope; this is the essence of block structure. Commands are combined
by sequencing, and are iterated using recursion.
MA maintains a careful separation between pure expressions, whose meaning does not de-
pend on any assignables, and impure commands, whose meaning is given in terms of assignables.
The segregation of pure from impure ensures that the evaluation order for expressions is not con-
strained by the presence of assignables in the language, so that they can be manipulated just as in
PCF. Commands, on the other hand, have a constrained execution order, because the execution of
one may affect the meaning of another.
A distinctive feature of MA is that it adheres to the stack discipline, which means that assignables
are allocated on entry to the scope of their declaration, and deallocated on exit, using a conven-
tional stack discipline. Stack allocation avoids the need for more complex forms of storage man-
agement, at the cost of reducing the expressive power of the language.
34.1
Basic Commands
The syntax of the language MA of modernized Algol distinguishes pure expressions from impure
commands. The expressions include those of PCF (as described in Chapter 19), augmented with
one construct, and the commands are those of a simple imperative programming language based
on assignment. The language maintains a sharp distinction between variables and assignables. Vari-
ables are introduced by λ-abstraction and are given meaning by substitution. Assignables are
introduced by a declaration and are given meaning by assignment and retrieval of their contents,
which is, for the time being, restricted to natural numbers. Expressions evaluate to values, and
have no effect on assignables. Commands are executed for their effect on assignables, and return
a value. Composition of commands not only sequences their execution order, but also passes the
value returned by the first to the second before it is executed. The returned value of a command

PREVIEW
312
34.1 Basic Commands
is, for the time being, restricted to the natural numbers. (But see Section 34.3 for the general case.)
The syntax of MA is given by the following grammar, from which we have omitted repetition
of the expression syntax of PCF for the sake of brevity.
Typ
τ
::=
cmd
cmd
command
Exp
e
::=
cmd( m )
cmd m
encapsulation
Cmd
m
::=
ret( e )
ret e
return
bnd( e ; x . m )
bnd x ←e ; m
sequence
dcl( e ; a . m )
dcl a := e in m
new assignable
get[ a ]
@ a
fetch
set[ a ]( e )
a := e
assign
The expression cmd( m ) consists of the unevaluated command m thought of as a value of type
cmd. The command ret( e ) returns the value of the expression e without having any effect on
the assignables. The command bnd( e ; x . m ) evaluates e to an encapsulated command, then this
command is executed for its effects on assignables, with its value substituted for x in m. The
command dcl( e ; a . m ) introduces a new assignable, a, for use within the command m whose
initial contents is given by the expression e. The command get[ a ] returns the current contents of
the assignable a and the command set[ a ]( e ) changes the contents of the assignable a to the value
of e, and returns that value.
34.1.1
Statics
The statics of MA consists of two forms of judgment:
1. Expression typing: Γ ⊢Σ e : τ.
2. Command formation: Γ ⊢Σ m ok.
The context Γ specifies the types of variables, as usual, and the signature Σ consists of a finite set
of assignables. As with other uses of symbols, the signature cannot be interpreted as a form of
typing hypothesis (it enjoys no structural properties of entailment), but must be considered as an
index of a family of judgments, one for each choice of Σ.
The statics of MA is inductively defined by the following rules:
Γ ⊢Σ m ok
Γ ⊢Σ cmd( m ) : cmd
(34.1a)
Γ ⊢Σ e : nat
Γ ⊢Σ ret( e ) ok
(34.1b)
Γ ⊢Σ e : cmd
Γ, x : nat ⊢Σ m ok
Γ ⊢Σ bnd( e ; x . m ) ok
(34.1c)
Γ ⊢Σ e : nat
Γ ⊢Σ,a m ok
Γ ⊢Σ dcl( e ; a . m ) ok
(34.1d)

PREVIEW
34.1 Basic Commands
313
Γ ⊢Σ,a get[ a ] ok
(34.1e)
Γ ⊢Σ,a e : nat
Γ ⊢Σ,a set[ a ]( e ) ok
(34.1f)
Rule (34.1a) is the introduction rule for the type cmd, and rule (34.1c) is the corresponding elimi-
nation form. Rule (34.1d) introduces a new assignable for use within a specified command. The
name a of the assignable is bound by the declaration, and so may be renamed to satisfy the implicit
constraint that it not already occur in Σ. Rule (34.1e) leaves implicit that the command to retrieve
the contents of an assignable a returns a natural number, as all do. Rule (34.1f) states that we may
assign a natural number to an assignable.
34.1.2
Dynamics
The dynamics of MA is defined in terms of a memory µ a finite function assigning a numeral to
each of a finite set of assignables.
The dynamics of expressions consists of these two judgment forms:
1. e valΣ, stating that e is a value relative to Σ.
2. e 7−→
Σ
e′, stating that the expression e steps to the expression e′.
These judgments are inductively defined by the following rules, together with the rules defining
the dynamics of PCF (see Chapter 19). It is important, however, that the successor operation be
given an eager, instead of lazy, dynamics so that a closed value of type nat is a numeral (for reasons
that will be explained in Section 34.3).
cmd( m ) valΣ
(34.2a)
Rule (34.2a) states that an encapsulated command is a value.
The dynamics of commands is defined in terms of states µ ∥m, where µ is a memory mapping
assignables to values, and m is a command. There are two judgments governing such states:
1. µ ∥m finalΣ. The state µ ∥m is complete.
2. µ ∥m 7−→
Σ
µ′ ∥m′. The state µ ∥m steps to the state µ′ ∥m′; the set of active assignables is
given by the signature Σ.
These judgments are inductively defined by the following rules:
e valΣ
µ ∥ret( e ) finalΣ
(34.3a)
e 7−→
Σ
e′
µ ∥ret( e ) 7−→
Σ
µ ∥ret( e′ )
(34.3b)

PREVIEW
314
34.1 Basic Commands
e 7−→
Σ
e′
µ ∥bnd( e ; x . m ) 7−→
Σ
µ ∥bnd( e′ ; x . m )
(34.3c)
e valΣ
µ ∥bnd( cmd( ret( e ) ) ; x . m ) 7−→
Σ
µ ∥[e/x]m
(34.3d)
µ ∥m1 7−→
Σ
µ′ ∥m′
1
µ ∥bnd( cmd( m1 ) ; x . m2 ) 7−→
Σ
µ′ ∥bnd( cmd( m′
1 ) ; x . m2 )
(34.3e)
µ ⊗a ,→e ∥get[ a ] 7−−→
Σ,a µ ⊗a ,→e ∥ret( e )
(34.3f)
e 7−−→
Σ,a e′
µ ∥set[ a ]( e ) 7−−→
Σ,a µ ∥set[ a ]( e′ )
(34.3g)
e valΣ,a
µ ⊗a ,→
∥set[ a ]( e ) 7−−→
Σ,a µ ⊗a ,→e ∥ret( e )
(34.3h)
e 7−→
Σ
e′
µ ∥dcl( e ; a . m ) 7−→
Σ
µ ∥dcl( e′ ; a . m )
(34.3i)
e valΣ
µ ⊗a ,→e ∥m 7−−→
Σ,a µ′ ⊗a ,→e′ ∥m′
µ ∥dcl( e ; a . m ) 7−→
Σ
µ′ ∥dcl( e′ ; a . m′ )
(34.3j)
e valΣ
e′ valΣ,a
µ ∥dcl( e ; a . ret( e′ ) ) 7−→
Σ
µ ∥ret( e′ )
(34.3k)
Rule (34.3a) specifies that a ret command is final if its argument is a value. Rules (34.3c) to (34.3e)
specify the dynamics of sequential composition. The expression e must, by virtue of the type
system, evaluate to an encapsulated command, which is executed to find its return value, which
is then substituted into the command m before executing it.
Rules (34.3i) to (34.3k) define the concept of block structure in a programming language. Dec-
larations adhere to the stack discipline in that an assignable is allocated during evaluation of the
body of the declaration, and deallocated after evaluation of the body is complete. Therefore the
lifetime of an assignable can be identified with its scope, and hence we may visualize the dynamic
lifetimes of assignables as being nested inside one another, in the same way as their static scopes
are nested inside one another. The stack-like behavior of assignables is a characteristic feature of
what are known as Algol-like languages.

PREVIEW
34.1 Basic Commands
315
34.1.3
Safety
The judgment µ ∥m okΣ is defined by the rule
⊢Σ m ok
µ : Σ
µ ∥m okΣ
(34.4)
where the auxiliary judgment µ : Σ is defined by the rule
∀a ∈Σ
∃e
µ(a) = e and e val∅and ⊢∅e : nat
µ : Σ
(34.5)
That is, the memory must bind a number to each assignable in Σ.
Theorem 34.1 (Preservation).
1. If e 7−→
Σ
e′ and ⊢Σ e : τ, then ⊢Σ e′ : τ.
2. If µ ∥m 7−→
Σ
µ′ ∥m′, with ⊢Σ m ok and µ : Σ, then ⊢Σ m′ ok and µ′ : Σ.
Proof. Simultaneously, by induction on rules (34.2) and (34.3).
Consider rule (34.3j). Assume that ⊢Σ dcl( e ; a . m ) ok and µ : Σ. By inversion of typing we
have ⊢Σ e : nat and ⊢Σ,a m ok. Because e valΣ and µ : Σ, we have µ ⊗a ,→e : Σ, a. By induction we
have ⊢Σ,a m′ ok and µ′ ⊗a ,→e′ : Σ, a, from which the result follows immediately.
Consider rule (34.3k). Assume that ⊢Σ dcl( e ; a . ret( e′ ) ) ok and µ : Σ. By inversion we have
⊢Σ e : nat, and ⊢Σ,a ret( e′ ) ok, and so ⊢Σ,a e′ : nat. But because e′ valΣ,a, and e′ is a numeral, and
we also have ⊢Σ e′ : nat, as required.
Theorem 34.2 (Progress).
1. If ⊢Σ e : τ, then either e valΣ, or there exists e′ such that e 7−→
Σ
e′.
2. If ⊢Σ m ok and µ : Σ, then either µ ∥m finalΣ or µ ∥m 7−→
Σ
µ′ ∥m′ for some µ′ and m′.
Proof. Simultaneously, by induction on rules (34.1). Consider rule (34.1d). By the first inductive
hypothesis we have either e 7−→
Σ
e′ or e valΣ. In the former case rule (34.3i) applies. In the latter, we
have by the second inductive hypothesis,
µ ⊗a ,→e ∥m finalΣ,a
or
µ ⊗a ,→e ∥m 7−−→
Σ,a µ′ ⊗a ,→e′ ∥m′.
In the former case we apply rule (34.3k), and in the latter, rule (34.3j).

PREVIEW
316
34.2 Some Programming Idioms
34.2
Some Programming Idioms
The language MA is designed to expose the elegant interplay between the execution of an expres-
sion for its value and the execution of a command for its effect on assignables. In this section we
show how to derive several standard idioms of imperative programming in MA.
We define the sequential composition of commands, written {x ←m1 ; m2}, to stand for the com-
mand bnd x ←cmd ( m1 ) ; m2. Binary composition readily generalizes to an n-ary form by defining
{x1 ←m1 ; . . . xn−1 ←mn−1 ; mn},
to stand for the iterated composition
{x1 ←m1 ; . . . {xn−1 ←mn−1 ; mn}}.
We sometimes write just {m1 ; m2} for the composition { ←m1 ; m2} where the returned value
from m1 is ignored; this generalizes in the obvious way to an n-ary form.
A related idiom, the command do e, executes an encapsulated command and returns its result.
By definition do e stands for the command bnd x ←e ; ret x.
The conditional command ifz ( m ) m1 else m2 executes either m1 or m2 according to whether
the result of executing m is zero or not:
{x ←m ; do ( ifz x {z ,→cmd m1 | s( ) ,→cmd m2} )}.
The returned value of the conditional is the value returned by the selected command.
The while loop command while ( m1 ) m2 repeatedly executes the command m2 while the com-
mand m1 yields a non-zero number. It is defined as follows:
do ( fix loop : cmd is cmd ( ifz ( m1 ) {ret z} else {m2 ; do loop} ) ).
This commands runs the self-referential encapsulated command that, when executed, first exe-
cutes m1, branching on the result. If the result is zero, the loop returns zero (arbitrarily). If the
result is non-zero, the command m2 is executed and the loop is repeated.
A procedure is a function of type τ ⇀cmd that takes an argument of some type τ and yields
an unexecuted command as result. Many procedures have the form λ ( x : τ ) cmd m, which we
abbreviate to proc ( x : τ ) m. A procedure call is the composition of a function application with the
activation of the resulting command. If e1 is a procedure and e2 is its argument, then the procedure
call call e1( e2 ) is defined to be the command do ( e1( e2 ) ), which immediately runs the result of
applying e1 to e2.
As an example, here is a procedure of type nat ⇀cmd that returns the factorial of its argument:

PREVIEW
34.3 Typed Commands and Typed Assignables
317
proc (x:nat) {
dcl r := 1 in
dcl a := x in
{ while ( @ a ) {
y ←@ r
; z ←@ a
; r := (x-z+1)× y
; a := z-1
}
; x ←@ r
; ret x
}
}
The loop maintains the invariant that the contents of r is the factorial of the quantity x minus the
contents of a. Initialization makes this invariant true, and it is preserved by each iteration of the
loop, so that upon completion of the loop the assignable a contains 0 and r contains the factorial
of x, as required.
34.3
Typed Commands and Typed Assignables
So far we have restricted the type of the returned value of a command, and the contents of an
assignable, to be nat. Can this restriction be relaxed, while adhering to the stack discipline?
The key to admitting returned and assignable values of other types may be uncovered by a
close examination of the proof of Theorem 34.1. For the proof to go through it is crucial that
values of type nat, the type of assignables and return values, cannot contain an assignable, for
otherwise the embedded assignable would escape the scope of its declaration. This property is
self-evidently true for eagerly evaluated natural numbers, but fails when they are evaluated lazily.
Thus the safety of MA hinges on the evaluation order for the successor operation, in contrast to
most other situations where either interpretation is also safe.
When extending MA to admit assignables and returned values of other types, it is necessary
to pay close attention to whether assignables can be embedded in a value of a candidate type.
For example, if return values of procedure type are allowed, then the following command violates
safety:
dcl a := z in {ret ( proc ( x : nat ) {a := x} )}.
This command, when executed, allocates a new assignable a and returns a procedure that, when
called, assigns its argument to a. But this makes no sense, because the assignable a is deallocated
when the body of the declaration returns, but the returned value still refers to it. If the returned
procedure is called, execution will get stuck in the attempt to assign to a.
A similar example shows that admitting assignables of procedure type is also unsound. For
example, suppose that b is an assignable whose contents are of type nat ⇀cmd, and consider the
command
dcl a := z in {b := proc ( x : nat ) {a := x} ; ret z}.

PREVIEW
318
34.3 Typed Commands and Typed Assignables
We assign to b a procedure that uses a locally declared assignable a and then leaves the scope of
the declaration. If we then call the procedure stored in b, execution will get stuck attempting to
assign to the non-existent assignable a.
To admit declarations that return values other than nat and to admit assignables with contents
of types other than nat, we must rework the statics of MA to record the returned type of a com-
mand and to record the type of the contents of each assignable. First, we generalize the finite set
Σ of active assignables to assign a mobile type to each active assignable so that Σ has the form
of a finite set of assumptions of the form a ~ τ, where a is an assignable. Second, we replace the
judgment Γ ⊢Σ m ok by the more general form Γ ⊢Σ m ∼·· τ, stating that m is a well-formed com-
mand returning a value of type τ. Third, the type cmd is generalized to cmd( τ ), which is written
in examples as τ cmd, to specify the return type of the encapsulated command.
The statics given in Section 34.1.1 is generalized to admit typed commands and typed assignables
as follows:
Γ ⊢Σ m ∼·· τ
Γ ⊢Σ cmd( m ) : cmd( τ )
(34.6a)
Γ ⊢Σ e : τ
Γ ⊢Σ ret( e ) ∼·· τ
(34.6b)
Γ ⊢Σ e : cmd( τ )
Γ, x : τ ⊢Σ m ∼·· τ′
Γ ⊢Σ bnd( e ; x . m ) ∼·· τ′
(34.6c)
Γ ⊢Σ e : τ
τ mobile
Γ ⊢Σ,a~τ m ∼·· τ′
τ′ mobile
Γ ⊢Σ dcl( e ; a . m ) ∼·· τ′
(34.6d)
Γ ⊢Σ,a~τ get[ a ] ∼·· τ
(34.6e)
Γ ⊢Σ,a~τ e : τ
Γ ⊢Σ,a~τ set[ a ]( e ) ∼·· τ
(34.6f)
Apart from the generalization to track returned types and content types, the most important
change is that in Rule (34.6d) both the type of a declared assignable and the return type of the
declaration is required to be mobile. The definition of the judgment τ mobile is guided by the
following mobility condition:
if τ mobile, ⊢Σ e : τ and e valΣ, then ⊢∅e : τ and e val∅.
(34.7)
That is, a value of mobile type may not depend on any active assignables.
As long as the successor operation is evaluated eagerly, the type nat is mobile:
nat mobile
(34.8)
Similarly, a product of mobile types may safely be deemed mobile, if pairs are evaluated eagerly:
τ1 mobile
τ2 mobile
τ1 × τ2 mobile
(34.9)

PREVIEW
34.4 Notes
319
And the same goes for sums, if the injections are evaluated eagerly:
τ1 mobile
τ2 mobile
τ1 + τ2 mobile
(34.10)
In each of these cases laziness defeats mobility, because values may contain suspended computa-
tions that depend on an assignable. For example, if the successor operation for the natural num-
bers were evaluated lazily, then s( e ) would be a value for any expression e including one that
refers to an assignable a.
Because the body of a procedure may involve an assignable, no procedure type is mobile, nor
is any command type. What about function types other than procedure types? We may think
they are mobile, because a pure expression cannot depend on an assignable. Although this is the
case, the mobility condition need not hold. For example, consider the following value of type
nat ⇀nat:
λ ( x : nat ) ( λ ( : τ cmd ) z )( cmd {@ a} ).
Although the assignable a is not actually needed to compute the result, it nevertheless occurs in
the value, violating the mobility condition.
The mobility restriction on the statics of declarations ensures that the type associated to an
assignable is always mobile. We may therefore assume, without loss of generality, that the types
associated to the assignables in the signature Σ are mobile.
Theorem 34.3 (Preservation for Typed Commands).
1. If e 7−→
Σ
e′ and ⊢Σ e : τ, then ⊢Σ e′ : τ.
2. If µ ∥m 7−→
Σ
µ′ ∥m′, with ⊢Σ m ∼·· τ and µ : Σ, then ⊢Σ m′ ∼·· τ and µ′ : Σ.
Theorem 34.4 (Progress for Typed Commands).
1. If ⊢Σ e : τ, then either e valΣ, or there exists e′ such that e 7−→
Σ
e′.
2. If ⊢Σ m ∼·· τ and µ : Σ, then either µ ∥m finalΣ or µ ∥m 7−→
Σ
µ′ ∥m′ for some µ′ and m′.
The proofs of Theorems 34.3 and 34.4 follows very closely the proof of Theorems 34.1 and 34.2.
The main difference is that we appeal to the mobility condition to ensure that returned values and
stored values are independent of the active assignables.
34.4
Notes
Modernized Algol is a derivative of Reynolds’s Idealized Algol (Reynolds, 1981). In contrast to
Reynolds’s formulation, Modernized Algol maintains a separation between computations that
depend on the memory and those that do not, and does not rely on call-by-name for function
application, but rather has a type of encapsulated commands that can be used where call-by-name

PREVIEW
320
34.4 Notes
would otherwise be required. The modal distinction between expressions and commands was
present in the original formulation of Algol 60, but is developed here in using the concept of
monadic effects introduced by Moggi (1989). Its role in functional programming was emphasized
by Wadler (1992). The modal separation in MA is adapted directly from Pfenning and Davies
(2001), which stresses the connection to lax modal logic.
What are called assignables here are invariably called variables elsewhere. The distinction be-
tween variables and assignables is blurred in languages that allow assignables as forms of expres-
sion. (Indeed, Reynolds himself (personal communication, 2012) regards this as a defining feature
of Algol, in opposition to the formulation given here.) In MA we choose to make the distinction
between variables, which are given meaning by substitution, and assignables, which are given
meaning by mutation. Drawing this distinction requires new terminology; the term assignable
seems apt for the imperative programming concept.
The concept of mobility of a type was introduced in the ML5 language for distributed comput-
ing (Murphy et al., 2004), with the similar meaning that a value of a mobile type cannot depend
on local resources. Here the mobility restriction is used to ensure that the language adheres to the
stack discipline.
Exercises
34.1. Originally Algol had both scalar assignables, whose contents are atomic values, and array
assignables, which is a finite sequence of scalar assignables. Like scalar assignables, array
assignables are stack-allocated. Extend MA with array assignables, ensuring that the lan-
guage remains type safe, but allowing that computation may abort if a non-existent array
element is accessed.
34.2. Consider carefully the behavior of assignable declarations within recursive procedures, as in
the following expression
fix p is λ ( x : τ ) dcl a := e in cmd( m )
of type τ ⇀ρ cmd for some ρ. Because p is recursive, the body m of the procedure may
call itself during its execution, causing the same declaration to be executed more than once.
Explain the dynamics of getting and setting a in such a situation.
34.3. Originally Algol considered assignables as expressions that stand for their contents in mem-
ory. Thus, if a is an assignable containing a number, one could write expressions such as
a + a that would evaluate to twice the contents of a. Moreover, one could write commands
such as a := a + a to double the contents of a. These conventions encouraged programmers
to think of assignables as variables, quite the opposite of their separation in MA. This con-
vention, combined with an over-emphasis on concrete syntax, led to a conundrum about the
different roles of a in the above assignment command: its meaning on the left of the assign-
ment is different from its meaning on the right. These came to be called the left-, or l-value,
and the right-, or r-value of the assignable a, corresponding to its position in the assignment
statement. When viewed as abstract syntax, though, there is no ambiguity to be explained:

PREVIEW
34.4 Notes
321
the assignment operator is indexed by its target assignable, instead of taking as argument
an expression that happens to be an assignable, so that the command is set[ a ]( a + a ), not
set( a ; a + a ).
This still leaves the puzzle of how to regard assignables as forms of expression. As a first
cut, reformulate the dynamics of MA to account for this. Reformulate the dynamics of ex-
pressions in terms of the judgments µ ∥e 7−→
Σ
µ′ ∥e′ and µ ∥e final that allow evaluation of e
to depend on the contents of the memory. Each use of an assignable as an expression should
require one access to the memory. Then prove memory invariance:: if µ ∥e 7−→
Σ
µ′ ∥e′, then
µ′ = µ.
A natural generalization is to allow any sequence of commands to be considered as an ex-
pression, if they are all passive in the sense that no assignments are allowed. Write do {m},
where m is a passive command, for a passive block whose evaluation consists of executing the
command m on the current memory, using its return value as the value of the expression.
Observe that memory invariance holds for passive blocks.
The use of an assignable a as an expression may now be rendered as the passive block
do {@ a}. More complex uses of assignables as expressions admit several different inter-
pretations using passive blocks. For example, an expression such as a + a might be rendered
in one of two ways:
(a) do {@ a} + do {@ a}, or
(b) let x be do {@ a} in x + x.
The latter formulation accesses a only once, but uses its value twice. Comment on there
being two different interpretations of a + a.
34.4. Recursive procedures in Algol are declared using a command of the form proc p( x : τ ) :
ρ is m in m′, which is governed by the typing rule
Γ, p : τ ⇀ρ cmd, x : τ ⊢Σ m ∼·· ρ
Γ, p : τ ⇀ρ cmd ⊢Σ m′ ∼·· τ′
Γ ⊢Σ proc p( x : τ ) : ρ is m in m′ ∼·· τ′
.
(34.11)
From the present viewpoint it is peculiar to insist on declaring procedures at all, because they
are simply values of procedure type, and even more peculiar to insist that they be confined
for use within a command. One justification for this limitation, though, is that Algol included
a peculiar feature, called an own variable1 that was declared for use within the procedure, but
whose state persisted across calls to the procedure. One application would be to a procedure
that generated pseudo-random numbers based on a stored seed that influenced the behavior
of successive calls to it. Give a formulation in MA of the extended declaration
proc p( x : τ ) : ρ is {own a := e in m} in m′
1That is to say, an own assignable.

PREVIEW
322
34.4 Notes
where a is declared as an “own” of the procedure p. Contrast the meaning of the foregoing
declaration with the following one:
proc p( x : τ ) : ρ is {dcl a := e in m} in m′.
34.5. A natural generalization of own assignables is to allow the creation of many such scenarios
for a single procedure (or mutually recursive collection of procedures), with each instance
creating its own persistent state. This ability motivated the concept of a class in Simula-67 as
a collection of procedures, possibly mutually recursive, that shared common persistent state.
Each instance of a class is called an object of that class; calls to its constituent procedures mu-
tate the private persistent state. Formulate this 1967 precursor of imperative object-oriented
programming in the context of MA.
34.6. There are several ways to formulate an abstract machine for MA that accounts for both the
control stack, which sequences execution (as described in Chapter 28 for PCF), and the data
stack, which records the contents of the assignables. A consolidated stack combines these two
separate concepts into one, whereas separated stacks keeps the memory separate from the
control stack, much as we have done in the structural dynamics given by Rules (34.3). In
either case the storage required for an assignable is deallocated when exiting the scope of
that assignable, a key benefit of the stack discipline for assignables in MA.
With a modal separation between expressions and commands it is natural to use a structural
dynamics for expressions, and a stack machine dynamics for commands.
(a) Formulate a consolidated stack machine where both assignables and stack frames are
recorded on the same stack. Consider states k ▷Σ m, where ⊢Σ k ÷ τ and ⊢Σ m ∼·· τ,
and k ◁Σ e, where ⊢Σ k ÷ τ and ⊢Σ e : τ. Comment on the implementation methods
required for a consolidated stack.
(b) Formulate a separated stack machine where the memory is maintained separately from
the control stack. Consider states of the form k ∥µ ▷Σ m, where µ : Σ, ⊢Σ k ÷ τ, and
⊢Σ m ∼·· τ, and of the form k ∥µ ◁Σ e, where ⊢Σ k ÷ τ, ⊢Σ e : τ, and e valΣ.

PREVIEW
Chapter 35
Assignable References
A reference to an assignable a is a value, written &a, of reference type that refers to the assignable a. A
reference to an assignable provides the capability to get or set the contents of that assignable, even if
the assignable itself is not in scope when it is used. Two references can be compared for equality to
test whether they govern the same underlying assignable. If two references are equal, then setting
one will affect the result of getting the other; if they are not equal, then setting one cannot influence
the result of getting from the other. Two references that govern the same underlying assignable
are aliases. Aliasing complicates reasoning about programs that use references, because any two
references may refer to the same assignable.
Reference types are compatible with both a scoped and a scope-free allocation of assignables.
When assignables are scoped, the range of significance of a reference type is limited to the scope
of the assignable to which it refers. Reference types are therefore immobile, so that they cannot be
returned from the body of a declaration, nor stored in an assignable. Although ensuring adherence
to the stack discipline, this restriction precludes using references to create mutable data structures,
those whose structure can be altered during execution. Mutable data structures have a number of
applications in programming, including improving efficiency (often at the expense of expressive-
ness) and allowing cyclic (self-referential) structures to be created. Supporting mutability requires
that assignables be given a scope-free dynamics, so that their lifetime persists beyond the scope of
their declaration. Consequently, all types are mobile, so that a value of any type may be stored in
an assignable or returned from a command.
35.1
Capabilities
The commands get[ a ] and set[ a ]( e ) in MA operate on statically specified assignable a. Even to
write these commands requires that the assignable a be in scope where the command occurs. But
suppose that we wish to define a procedure that, say, updates an assignable to double its previous
value, and returns the previous value. We can write such a procedure for a specific assignable, a,
but what if we wish to write a generic procedure that works uniformly for all assignables?

PREVIEW
324
35.2 Scoped Assignables
One way to do this is give the procedure the capability to get and set the contents of some caller-
specified assignable. Such a capability is a pair consisting of a getter and a setter for that assignable.
The getter for an assignable a is a command that, when executed, returns the contents of a. The
setter for an assignable a is a procedure that, when applied to a value of suitable type, assigns that
value to a. Thus, a capability for an assignable a containing a value of type τ is a value of type
τ cap ≜τ cmd × ( τ ⇀τ cmd ).
A capability for getting and setting an assignable a containing a value of type τ is given by the
pair
⟨cmd ( @ a ), proc ( x : τ ) a := x⟩
of type τ cap. Because a capability type is a product of a command type and a procedure type,
no capability type is mobile. Thus, a capability cannot be returned from a command, nor stored
into an assignable. This is as it should be, for otherwise we would violate the stack discipline for
allocating assignables.
The proposed generic doubling procedure is programmed using capabilities as follows:
proc ( ⟨get, set⟩: nat cmd × ( nat ⇀nat cmd ) ) {x ←do get ; y ←do ( set( x + x ) ) ; ret x}.
The procedure is called with the capability to access an assignable a. When executed, it invokes the
getter to obtain the contents of a, and then invokes the setter to assign to a, returning the previous
value. Observe that the assignable a need not be accessible by this procedure; the capability given
by the caller comprises the commands required to get and set a.
35.2
Scoped Assignables
A weakness of using a capability to give indirect access to an assignable is that there is no guaran-
tee that a given getter/setter pair are in fact the capability for a particular assignable. For example,
we might pair the getter for a with the setter for b, leading to unexpected behavior. There is noth-
ing in the type system that prevents creating such mismatched pairs.
To avoid this we introduce the concept of a reference to an assignable. A reference is a value
from which we may obtain the capability to get and set a particular assignable. Moreover, two
references can be tested for equality to see whether they act on the same assignable.1 The reference
type ref( τ ) has as values references to assignables of type τ. The introduction and elimination
forms for this type are given by the following syntax chart:
Typ
τ
::=
ref( τ )
τ ref
assignable
Exp
e
::=
ref[ a ]
&a
reference
Cmd
m
::=
getref( e )
∗e
contents
setref( e1 ; e2 )
e1 ∗= e2
update
1The getter and setter do not suffice to define equality, because not all types admit a test for equality. When they do,
and when there are at least two distinct values of their type, we can determine whether they are aliases by assigning to one
and checking whether the contents of the other is changed.

PREVIEW
35.2 Scoped Assignables
325
The statics of reference types is defined by the following rules:
Γ ⊢Σ,a~τ ref[ a ] : ref( τ )
(35.1a)
Γ ⊢Σ e : ref( τ )
Γ ⊢Σ getref( e ) ∼·· τ
(35.1b)
Γ ⊢Σ e1 : ref( τ )
Γ ⊢Σ e2 : τ
Γ ⊢Σ setref( e1 ; e2 ) ∼·· τ
(35.1c)
Rule (35.1a) specifies that a reference to any active assignable is an expression of type ref( τ ).
The dynamics of reference types defers to the corresponding operations on assignables, and
does not alter the underlying dynamics of assignables:
ref[ a ] valΣ,a~τ
(35.2a)
e 7−→
Σ
e′
µ ∥getref( e ) 7−→
Σ
µ ∥getref( e′ )
(35.2b)
µ ∥getref( ref[ a ] ) 7−−−→
Σ,a~τ
µ ∥get[ a ]
(35.2c)
e1 7−→
Σ
e′
1
µ ∥setref( e1 ; e2 ) 7−→
Σ
µ ∥setref( e′
1 ; e2 )
(35.2d)
µ ∥setref( ref[ a ] ; e ) 7−−−→
Σ,a~τ
µ ∥set[ a ]( e )
(35.2e)
A reference to an assignable is a value. The getref and setref operations on references defer to
the corresponding operations on assignables once the referent has been resolved.
Because references give rise to capabilities, the reference type is immobile. As a result refer-
ences cannot be stored in assignables or returned from commands. The immobility of references
ensures safety, as can be seen by extending the safety proof given in Chapter 34.
As an example of using references, the generic doubling procedure discussed in the preceding
section is programmed using references as follows:
proc ( r : nat ref ) {x ←∗r ; r ∗= x + x ; ret x}.
Because the argument is a reference, rather than a capability, there is no possibility that the getter
and setter refer to different assignables.

PREVIEW
326
35.3 Free Assignables
The ability to pass references to procedures comes at a price, because any two references might
refer to the same assignable (if they have the same type). Consider a procedure that, when given
two references x and y, adds twice the contents of y to the contents of x. One way to write this
code creates no complications:
λ ( x : nat ref ) λ ( y : nat ref ) cmd {x′ ←∗x ; y′ ←∗y ; x ∗= x′ + y′ + y′}.
Even if x and y refer to the same assignable, the effect will be to set the contents of the assignable
referenced by x to the sum of its original contents and twice the contents of the assignable refer-
enced by y.
But now consider the following seemingly equivalent implementation of this procedure:
λ ( x : nat ref ) λ ( y : nat ref ) cmd {x + = y ; x + = y},
where x + = y is the command
{x′ ←∗x ; y′ ←∗y ; x ∗= x′ + y′}
that adds the contents of y to the contents of x. The second implementation works right, as long
as x and y do not refer to the same assignable. If they do refer to a common assignable a, with
contents n, the result is that a is to set 4 × n, instead of the intended 3 × n. The second get of y is
affected by the first set of x.
In this case it is clear how to avoid the problem: use the first implementation, rather than
the second. But the difficulty is not in fixing the problem once it has been discovered, but in
noticing the problem in the first place. Wherever references (or capabilities) are used, the problems
of interference lurk. Avoiding them requires very careful consideration of all possible aliasing
relationships among all of the references in play. The problem is that the number of possible
aliasing relationships among n references grows combinatorially in n.
35.3
Free Assignables
Although it is interesting to note that references and capabilities are compatible with the stack
discipline, for references to be useful requires that this restriction be relaxed. With immobile ref-
erences it is impossible to build data structures containing references, or to return references from
procedures. To allow this we must arrange that the lifetime of an assignable extend beyond its
scope. In other words we must give up stack allocation for heap allocation. Assignables that
persist beyond their scope of declaration are called scope-free, or just free, assignables. When all
assignables are free, every type is mobile and so any value, including a reference, may be used in
a data structure.
Supporting free assignables amounts to changing the dynamics so that allocation of assignables
persists across transitions. We use transition judgments of the form
ν Σ { µ ∥m } 7−→ν Σ′ { µ′ ∥m′ }.

PREVIEW
35.3 Free Assignables
327
Execution of a command may allocate new assignables, may alter the contents of existing assignables,
and may give rise to a new command to be executed at the next step. The rules defining the dy-
namics of free assignables are as follows:
e valΣ
ν Σ { µ ∥ret( e ) } final
(35.3a)
e 7−→
Σ
e′
ν Σ { µ ∥ret( e ) } 7−→ν Σ { µ ∥ret( e′ ) }
(35.3b)
e 7−→
Σ
e′
ν Σ { µ ∥bnd( e ; x . m ) } 7−→ν Σ { µ ∥bnd( e′ ; x . m ) }
(35.3c)
e valΣ
ν Σ { µ ∥bnd( cmd( ret( e ) ) ; x . m ) } 7−→ν Σ { µ ∥[e/x]m }
(35.3d)
ν Σ { µ ∥m1 } 7−→ν Σ′ { µ′ ∥m′
1 }
ν Σ { µ ∥bnd( cmd( m1 ) ; x . m2 ) } 7−→ν Σ′ { µ′ ∥bnd( cmd( m′
1 ) ; x . m2 ) }
(35.3e)
ν Σ, a ~ τ { µ ⊗a ,→e ∥get[ a ] } 7−→ν Σ, a ~ τ { µ ⊗a ,→e ∥ret( e ) }
(35.3f)
e 7−→
Σ
e′
ν Σ { µ ∥set[ a ]( e ) } 7−→ν Σ { µ ∥set[ a ]( e′ ) }
(35.3g)
e valΣ,a~τ
ν Σ, a ~ τ { µ ⊗a ,→
∥set[ a ]( e ) } 7−→ν Σ, a ~ τ { µ ⊗a ,→e ∥ret( e ) }
(35.3h)
e 7−→
Σ
e′
ν Σ { µ ∥dcl( e ; a . m ) } 7−→ν Σ { µ ∥dcl( e′ ; a . m ) }
(35.3i)
e valΣ
ν Σ { µ ∥dcl( e ; a . m ) } 7−→ν Σ, a ~ τ { µ ⊗a ,→e ∥m }
(35.3j)
The language RMA extends MA with references to free assignables. Its dynamics is similar to
that of references to scoped assignables given earlier.
e 7−→
Σ
e′
ν Σ { µ ∥getref( e ) } 7−→ν Σ { µ ∥getref( e′ ) }
(35.4a)

PREVIEW
328
35.4 Safety
ν Σ { µ ∥getref( ref[ a ] ) } 7−→ν Σ { µ ∥get[ a ] }
(35.4b)
e1 7−→
Σ
e′
1
ν Σ { µ ∥setref( e1 ; e2 ) } 7−→ν Σ { µ ∥setref( e′
1 ; e2 ) }
(35.4c)
ν Σ { µ ∥setref( ref[ a ] ; e2 ) } 7−→ν Σ { µ ∥set[ a ]( e2 ) }
(35.4d)
The expressions cannot alter or extend the memory, only commands may do so.
As an example of using RMA, consider the command newref[ τ ]( e ) defined by
dcl a := e in ret ( &a ).
(35.5)
This command allocates a fresh assignable, and returns a reference to it. Its static and dynamics
are derived from the foregoing rules as follows:
Γ ⊢Σ e : τ
Γ ⊢Σ newref[ τ ]( e ) ∼·· ref( τ )
(35.6)
e 7−→
Σ
e′
ν Σ { µ ∥newref[ τ ]( e ) } 7−→ν Σ { µ ∥newref[ τ ]( e′ ) }
(35.7a)
e valΣ
ν Σ { µ ∥newref[ τ ]( e ) } 7−→ν Σ, a ~ τ { µ ⊗a ,→e ∥ret( ref[ a ] ) }
(35.7b)
Oftentimes the command newref[ τ ]( e ) is taken as primitive, and the declaration command is
omitted. In that case all assignables are accessed by reference, and no direct access to assignables
is provided.
35.4
Safety
Although the proof of safety for references to scoped assignables presents few difficulties, the
safety for free assignables is tricky. The main difficulty is to account for cyclic dependencies within
data structures (such as will arise in Section 35.5.) The contents of one assignable may contain a
reference to itself, or a reference to another assignable that contains a reference to it, and so forth.
For example, consider the following procedure e of type nat ⇀nat cmd:
proc ( x : nat ) {ifz ( x ) ret ( 1 ) else { f ←@ a ; y ←f ( x −1 ) ; ret ( x ∗y )}}.
Let µ be a memory of the form µ′ ⊗a ,→e in which the contents of a contains, via the body of the
procedure, a reference to a itself. Indeed, if the procedure e is called with a non-zero argument, it
will “call itself” by indirect reference through a.

PREVIEW
35.4 Safety
329
Cyclic dependencies complicate the definition of the judgment ν Σ { µ ∥m } ok. It is defined by
the following rule:
⊢Σ m ∼·· τ
⊢Σ µ : Σ
ν Σ { µ ∥m } ok
(35.8)
The first premise of the rule states that the command m is well-formed relative to Σ. The second
premise states that the memory µ conforms to Σ, relative to all of Σ so that cyclic dependencies are
permitted. The judgment ⊢Σ′ µ : Σ is defined as follows:
∀a ~ τ ∈Σ
∃e
µ(a) = e and ⊢Σ′ e : τ
⊢Σ′ µ : Σ
(35.9)
In the safety proof to follow Σ′ is chosen to be Σ to allow for cyclicity.
Theorem 35.1 (Preservation).
1. If ⊢Σ e : τ and e 7−→
Σ
e′, then ⊢Σ e′ : τ.
2. If ν Σ { µ ∥m } ok and ν Σ { µ ∥m } 7−→ν Σ′ { µ′ ∥m′ }, then ν Σ′ { µ′ ∥m′ } ok.
Proof. Simultaneously, by induction on transition. We prove the following stronger form of the
second statement:
If ν Σ { µ ∥m } 7−→ν Σ′ { µ′ ∥m′ }, where ⊢Σ m ∼·· τ, ⊢Σ µ : Σ, then Σ′ extends Σ, and
⊢Σ′ m′ ∼·· τ, and ⊢Σ′ µ′ : Σ′.
Consider the transition
ν Σ { µ ∥dcl( e ; a . m ) } 7−→ν Σ, a ~ ρ { µ ⊗a ,→e ∥m }
where e valΣ. By assumption and inversion of rule (34.6d) we have ⊢Σ e : ρ, ⊢Σ,a~ρ m ∼·· τ, and
⊢Σ µ : Σ. But because extension of Σ with a fresh assignable does not affect typing, we also have
⊢Σ,a~ρ µ : Σ and ⊢Σ,a~ρ e : ρ, from which it follows by rule (35.9) that ⊢Σ,a~ρ µ ⊗a ,→e : Σ, a ~ ρ.
The other cases follow a similar pattern, and are left as an exercise for the reader.
Theorem 35.2 (Progress).
1. If ⊢Σ e : τ, then either e valΣ or there exists e′ such that e 7−→
Σ
e′.
2. If ν Σ { µ ∥m } ok then either ν Σ { µ ∥m } final or ν Σ { µ ∥m } 7−→ν Σ′ { µ′ ∥m′ } for some Σ′,
µ′, and m′.
Proof. Simultaneously, by induction on typing. For the second statement we prove
If ⊢Σ m ∼·· τ and ⊢Σ µ : Σ, then either ν Σ { µ ∥m } final, or ν Σ { µ ∥m } 7−→ν Σ′ { µ′ ∥
m′ } for some Σ′, µ′, and m′.

PREVIEW
330
35.5 Benign Effects
Consider the typing rule
Γ ⊢Σ e : ρ
Γ ⊢Σ,a~ρ m ∼·· τ
Γ ⊢Σ dcl( e ; a . m ) ∼·· τ
We have by the first inductive hypothesis that either e valΣ or e 7−→
Σ
e′ for some e′. In the latter case
we have by rule (35.3i)
ν Σ { µ ∥dcl( e ; a . m ) } 7−→ν Σ { µ ∥dcl( e′ ; a . m ) }.
In the former case we have by rule (35.3j) that
ν Σ { µ ∥dcl( e ; a . m ) } 7−→ν Σ, a ~ ρ { µ ⊗a ,→e ∥m }.
Now consider the typing rule
Γ ⊢Σ,a~τ get[ a ] ∼·· τ
By assumption ⊢Σ,a~τ µ : Σ, a ~ τ, and hence there exists e valΣ,a~τ such that µ = µ′ ⊗a ,→e and
⊢Σ,a~τ e : τ. By rule (35.3f)
ν Σ, a ~ τ { µ′ ⊗a ,→e ∥get[ a ] } 7−→ν Σ, a ~ τ { µ′ ⊗a ,→e ∥ret( e ) },
as required. The other cases are handled similarly.
35.5
Benign Effects
The modal separation between commands and expressions ensures that the meaning of an expres-
sion does not depend on the (ever-changing) contents of assignables. Although this is helpful in
many, perhaps most, situations, it also precludes programming techniques that use storage effects
to implement purely functional behavior. A prime example is memoization. Externally, a sus-
pended computation behaves exactly like the underlying computation; internally, an assignable
is associated with the computation that stores the result of any evaluation of the computation for
future use. Other examples are self-adjusting data structures, which use state to improve their effi-
ciency without changing their functional behavior. For example, a splay tree is a binary search tree
that uses mutation internally to re-balance the tree as elements are inserted, deleted, and retrieved,
so that lookup takes time proportional to the logarithm of the number of elements.
These are examples of benign storage effects, uses of mutation in a data structure to improve
efficiency without disrupting its functional behavior. One class of examples are self-adjusting data
structures that reorganize themselves during one use to improve efficiency of later uses. Another
class of examples are memoized, or lazy, data structures, which are discussed in Chapter 36. Be-
nign effects such as these are impossible to implement if a strict separation between expressions
and commands is maintained. For example, a self-adjusting tree involves mutation, but is a value

PREVIEW
35.5 Benign Effects
331
just like any other, and this cannot be achieved in MA. Although several special-case techniques
are known, the most general solution is to do away with the modal distinction, coalescing expres-
sions and commands into a single syntactic category. The penalty is that the type system no longer
ensures that an expression of type τ denotes a value of that type; it might also have storage effects
during its evaluation. The benefit is that one may freely use benign effects, but it is up to the
programmer to ensure that they truly are benign.
The language RPCF extends PCF with references to free assignables. The following rules de-
fine the statics of the distinctive features of RPCF:
Γ ⊢Σ e1 : τ1
Γ ⊢Σ,a~τ1 e2 : τ2
Γ ⊢Σ dcl( e1 ; a . e2 ) : τ2
(35.10a)
Γ ⊢Σ,a~τ get[ a ] : τ
(35.10b)
Γ ⊢Σ,a~τ e : τ
Γ ⊢Σ,a~τ set[ a ]( e ) : τ
(35.10c)
Correspondingly, the dynamics of RPCF is given by transitions of the form
ν Σ { µ ∥e } 7−→ν Σ′ { µ′ ∥e′ },
where e is an expression, and not a command. The rules defining the dynamics are very similar to
those for RMA, but with commands and expressions integrated into a single category.
To illustrate the concept of a benign effect, consider the technique of back-patching to implement
recursion. Here is an implementation of the factorial function that uses an assignable to implement
recursive calls:
dcl a := λn:nat.0 in
{ f ←a := λn:nat.ifz(n, 1, n′.n×(@a)(n′))
; ret(f)
}
This declaration returns a function of type nat ⇀nat that is obtained by (a) allocating a free
assignable initialized arbitrarily with a function of this type, (b) defining a λ-abstraction in which
each “recursive call” consists of retrieving and applying the function stored in that assignable, (c)
assigning this function to the assignable, and (d) returning that function. The result is a function
on the natural numbers, even though it uses state in its implementation.
Backpatching is not expressible in RMA, because it relies on assignment. Let us attempt to
recode the previous example in RMA:
dcl a := proc(n:nat){ret 0} in
{ f ←a := . . .
; ret(f)
},
where the elided procedure assigned to a is given by

PREVIEW
332
35.6 Notes
proc(n:nat) {if (ret(n)) {ret(1)} else {f←@a; x←f(n-1); ret(n×x)}}.
The difficulty is that what we have is a command, not an expression. Moreover, the result of
the command is of the procedure type nat ⇀( nat cmd ), and not of the function type nat ⇀nat.
Consequently, we cannot use the factorial procedure in an expression, but have to execute it as a
command using code such as this:
{ f ←fact ; x ←f(n); ret(x) }.
35.6
Notes
Reynolds (1981) uses capabilities to provide indirect access to assignables; references are just an
abstract form of capability. References are often permitted only for free assignables, but with
mobility restrictions one may also have references to scoped assignables. The proof of safety of
free references outlined here follows those given by Wright and Felleisen (1994) and Harper (1994).
Benign effects are central to the distinction between Haskell, which provides an Algol-like sep-
aration between commands and expressions, and ML, which integrates evaluation with execution.
The choice between them is classic trade-off, with neither superior to the other in all respects.
Exercises
35.1. Consider scoped array assignables as described in Exercise 34.1. Extend the treatment of
array assignables in Exercise 34.1, to account for array assignable references.
35.2. References to scope-free assignables are often used to implement recursive data structures
such as mutable lists and trees. Examine such data structures in the context of RMA enriched
with sum, product, and recursive types.
Give six different types that could be considered a type of linked lists, according to the fol-
lowing characteristics:
(a) A mutable list may only be updated in toto by replacing it with another (immutable)
list.
(b) A mutable list can be altered in one of two ways, to make it empty, or to change both its
head and tail element simultaneously. The tail element is any other such mutable list,
so circularities may arise.
(c) A mutable list is, permanently, either empty or non-empty. If not, both its head and tail
can be modified simultaneously.
(d) A mutable list is, permanently, either empty or non-empty. If not, its tail, but not its
head, can be set to another such list.
(e) A mutable list is, permanently, either empty or non-empty. If not, either its head or its
tail elements can be modified independently.

PREVIEW
35.6 Notes
333
(f) A mutable list can be altered to become either empty or non-empty. If it is non-empty,
either it head, or its tail, can be modified independently of one another.
Discuss the merits and deficiencies of each representation.

PREVIEW
334
35.6 Notes

PREVIEW
Chapter 36
Lazy Evaluation
Lazy evaluation comprises a variety of methods to defer evaluation of an expression until it is re-
quired, and to share the results of any such evaluation among all uses of a deferred computation.
Laziness is not merely an implementation device, but it also affects the meaning of a program.
One form of laziness is the by-need evaluation strategy for function application. Recall from
Chapter 8 that the by-name evaluation order passes the argument to a function in unevaluated
form so that it is only evaluated if it is actually used. But because the argument is replicated by
substitution, it might be evaluated more than once. By-need evaluation ensures that the argument
to a function is evaluated at most once, by ensuring that all copies of an argument share the result
of evaluating any one copy.
Another form of laziness is the concept of a lazy data structure. As we have seen in Chap-
ters 10, 11, and 20, we may choose to defer evaluation of the components of a data structure until
they are actually required, and not when the data structure is created. But if a component is re-
quired more than once, then the same computation will, without further provision, be repeated
on each use. To avoid this, the deferred portions of a data structure are shared so an access to one
will propagate its result to all occurrences of the same computation.
Yet another form of laziness arises from the concept of general recursion considered in Chap-
ter 19. Recall that the dynamics of general recursion is given by unrolling, which replicates the
recursive computation on each use. It would be preferable to share the results of such compu-
tation across unrollings. A lazy implementation of recursion avoids such replications by sharing
those results.
Traditionally, languages are biased towards either eager or lazy evaluation. Eager languages
use a by-value dynamics for function applications, and evaluate the components of data structures
when they are created. Lazy languages adopt the opposite strategy, preferring a by-name dynam-
ics for functions, and a lazy dynamics for data structures. The overhead of laziness is reduced by
managing sharing to avoid redundancy. Experience has shown, however, that the distinction is
better drawn at the level of types. It is important to have both lazy and eager types, so that the
programmer controls the use of laziness, rather than having it enforced by the language dynamics.

PREVIEW
336
36.1 PCF By-Need
36.1
PCF By-Need
We begin by considering a lazy variant of PCF, called LPCF, in which functions are called by
name, and the successor operator is evaluated lazily. Under a lazy interpretation variables are
bound to unevaluated expressions, and the argument to the successor left unevaluated: any suc-
cessor is a value, regardless of whether the predecessor is or not. By-name function applica-
tion replicates the unevaluated argument by substitution, which means that there can arise many
copies of the same expression, each evaluated separately, if at all. By-need evaluation uses a device
called memoization to share all such copies of an argument and to ensure that if it is evaluated at all,
its value is stored so that all other uses of it will avoid re-computation. Computations are named
during evaluation, and are accessed by a level of indirection using this name to index the memo
table, which records the expression and, if it is every evaluated, its value.
The dynamics of LPCF is based on a transition system with states of the form ν Σ { e ∥µ },
where Σ is a finite set of hypotheses a1 ~ τ1, . . . , an ~ τn associating types to symbols, e is an expres-
sion that can involve the symbols in Σ, and µ is a memo table that maps each symbol declared in
Σ to either an expression or a special symbol, •, called the black hole. (The role of the black hole is
explained below.) A new form of expression, @ a, is used to represent an indirect reference through
the memo table.
The dynamics of LPCF is given by he following two forms of judgment:
1. e valΣ, stating that e is a value that can involve the symbols in Σ.
2. ν Σ { e ∥µ } 7−→ν Σ′ { e′ ∥µ′ }, stating that one step of evaluation of the expression e relative
to memo table µ with the symbols declared in Σ results in the expression e′ relative to the
memo table µ′ with symbols declared in Σ′.
The dynamics is defined so that the active symbols grow during evaluation. The memo table may
be altered destructively during execution to show progress in the evaluation of the expression
associated with a symbol.
The judgment e valΣ expressing that e is a closed value is defined by the following rules:
z valΣ
(36.1a)
s( @ a ) valΣ,a~nat
(36.1b)
λ ( x : τ ) e valΣ
(36.1c)
Rules (36.1a) through (36.1c) specify that z is a value, any expression of the form s( @ a ), where
a is a symbol, is a value, and that any λ-abstraction, possibly containing symbols, is a value. It is
important that symbols themselves are not values, rather they stand for (possibly unevaluated)
expressions as specified by the memo table. The expression @ a, which is short for via( a ), is not a
value. Rather, it is accessed to obtain, and possibly update, the binding of the symbol a in memory.

PREVIEW
36.1 PCF By-Need
337
The initial and final states of evaluation are defined as follows:
ν ∅{ e ∥∅} initial
(36.2a)
e valΣ
ν Σ { e ∥µ } final
(36.2b)
Rule (36.2a) specifies that an initial state consists of an expression evaluated relative to an
empty memo table. Rule (36.2b) specifies that a final state has the form ν Σ { e ∥µ }, where e is a
value relative to Σ.
The transition judgment for the dynamics of LPCF is defined by the following rules:
e valΣ,a~τ
ν Σ, a ~ τ { @ a ∥µ ⊗a ,→e } 7−→ν Σ, a ~ τ { e ∥µ ⊗a ,→e }
(36.3a)
ν Σ, a ~ τ { e ∥µ ⊗a ,→• } 7−→ν Σ′, a ~ τ { e′ ∥µ′ ⊗a ,→• }
ν Σ, a ~ τ { @ a ∥µ ⊗a ,→e } 7−→ν Σ′, a ~ τ { @ a ∥µ′ ⊗a ,→e′ }
(36.3b)
(e ̸= @ b)
ν Σ { s( e ) ∥µ } 7−→ν Σ, a ~ nat { s( @ a ) ∥µ ⊗a ,→e }
(36.3c)
ν Σ { e ∥µ } 7−→ν Σ′ { e′ ∥µ′ }
ν Σ { ifz e {z ,→e0 | s( x ) ,→e1} ∥µ } 7−→ν Σ′ { ifz e′ {z ,→e0 | s( x ) ,→e1} ∥µ′ }
(36.3d)
ν Σ { ifz z {z ,→e0 | s( x ) ,→e1} ∥µ } 7−→ν Σ { e0 ∥µ }
(36.3e)





ν Σ, a ~ nat { ifz s( @ a ) {z ,→e0 | s( x ) ,→e1} ∥µ ⊗a ,→e }
7−→
ν Σ, a ~ nat { [@ a/x]e1 ∥µ ⊗a ,→e }





(36.3f)
ν Σ { e1 ∥µ } 7−→ν Σ′ { e′
1 ∥µ′ }
ν Σ { e1( e2 ) ∥µ } 7−→ν Σ′ { e′
1( e2 ) ∥µ′ }
(36.3g)





ν Σ { ( λ ( x : τ ) e )( e2 ) ∥µ }
7−→
ν Σ, a ~ τ { [@ a/x]e ∥µ ⊗a ,→e2 }





(36.3h)

PREVIEW
338
36.2 Safety of PCF By-Need
ν Σ { fix x : τ is e ∥µ } 7−→ν Σ, a ~ τ { @ a ∥µ ⊗a ,→[@ a/x]e }
(36.3i)
Rule (36.3a) governs a symbol whose associated expression is a value; the value of the symbol is
the value associated to that symbol in the memo table. Rule (36.3b) specifies that if the expression
associated to a symbol is not a value, then it is evaluated “in place” until such time as rule (36.3a)
applies. This is achieved by switching the focus of evaluation to the associated expression, while
at the same time associating the black hole to that symbol. The black hole represents the absence of
a value for that symbol, so that any attempt to use it during evaluation of its associated expression
cannot make progress. The black hole signals a circular dependency that, if not caught using a
black hole, would initiate an infinite regress.
Rule (36.3c) specifies that evaluation of s( e ) allocates a fresh symbol a for the expression e,
and yields the value s( @ a ). The value of e is not determined until such time as the predecessor
is required in a later computation, implementing a lazy dynamics for the successor. Rule (36.3f),
which governs a conditional branch on a successor, substitutes @ a for the variable x when com-
puting the predecessor of a non-zero number, ensuring that all occurrences of x share the same
predecessor computation.
Rule (36.3g) specifies that the value of the function position of an application must be deter-
mined before the application can be executed. Rule (36.3h) specifies that to evaluate an application
of a λ-abstraction we allocate a fresh symbol a for the argument, and substitute @ a for the argu-
ment variable of the function. The argument is evaluated only if it is needed in the later computa-
tion, and then that value is shared among all occurrences of the argument variable in the body of
the function.
General recursion is implemented by rule (36.3i). Recall from Chapter 19 that the expression
fix x : τ is e stands for the solution of the recursion equation x = e. Rule (36.3i) computes this
solution by associating a fresh symbol a with the body e substituting @ a for x within e to effect the
self-reference. It is this substitution that permits a named expression to depend on its own name.
For example, the expression fix x : τ is x associates the expression @ a to a in the memo table, and
returns @ a. The next step of evaluation is stuck, because it seeks to evaluate @ a with a bound to
the black hole. In contrast an expression such as fix f : τ′ ⇀τ is λ ( x : τ′ ) e does not get stuck,
because the self-reference is “hidden” within the λ-abstraction, and hence need not be evaluated
to determine the value of the binding.
36.2
Safety of PCF By-Need
We write Γ ⊢Σ e : τ to mean that e has type τ under the assumptions Γ, treating symbols declared
in Σ as expressions of their associated type. The rules are as in Chapter 19, extended with the
following rule for symbols:
Γ ⊢Σ,a~τ @ a : τ
(36.4)
This rule states that the demand for the binding of a symbol, @ a, is a form of expression. It is a
“delayed substitution” that lazily replaces a demand for a by its binding.

PREVIEW
36.2 Safety of PCF By-Need
339
The judgment ν Σ { e ∥µ } ok is defined by the following rules:
⊢Σ e : τ
⊢Σ µ : Σ
ν Σ { e ∥µ } ok
(36.5a)
∀a ~ τ ∈Σ
µ(a) = e ̸= • =⇒⊢Σ′ e : τ
⊢Σ′ µ : Σ
(36.5b)
Rule (36.5b) permits self-reference through the memo table by allowing the expression associated
to a symbol a to contain occurrences of @ a. A symbol that is bound to the “black hole” is consid-
ered to be of any type.
Theorem 36.1 (Preservation). If ν Σ { e ∥µ } 7−→ν Σ′ { e′ ∥µ′ } and ν Σ { e ∥µ } ok, then ν Σ′ { e′ ∥µ′ } ok.
Proof. We prove by induction on rules (36.3) that if ν Σ { e ∥µ } 7−→ν Σ′ { e′ ∥µ′ } and ⊢Σ µ : Σ
and ⊢Σ e : τ, then Σ′ ⊇Σ and ⊢Σ′ µ′ : Σ′ and ⊢Σ′ e′ : τ.
Consider rule (36.3b), for which we have e = e′ = @ a, µ = µ0 ⊗a ,→e0, µ′ = µ′
0 ⊗a ,→e′
0, and
ν Σ, a ~ τ { e0 ∥µ0 ⊗a ,→• } 7−→ν Σ′, a ~ τ { e′
0 ∥µ′
0 ⊗a ,→• }.
Assume that ⊢Σ,a~τ µ : Σ, a ~ τ. It follows that ⊢Σ,a~τ e0 : τ and ⊢Σ,a~τ µ0 : Σ, and hence that
⊢Σ,a~τ µ0 ⊗a ,→• : Σ, a ~ τ.
We have by induction that Σ′ ⊇Σ and ⊢Σ′,a~τ e′
0 : τ′ and
⊢Σ′,a~τ µ0 ⊗a ,→• : Σ, a ~ τ.
But then
⊢Σ′,a~τ µ′ : Σ′, a ~ τ,
which suffices for the result.
Consider rule (36.3g), so that e is the application e1( e2 ) and
ν Σ { e1 ∥µ } 7−→ν Σ′ { e′
1 ∥µ′ }.
Suppose that ⊢Σ µ : Σ and ⊢Σ e : τ. By inversion of typing ⊢Σ e1 : τ2 ⇀τ for some type τ2 such
that ⊢Σ e2 : τ2. By induction Σ′ ⊇Σ and ⊢Σ′ µ′ : Σ′ and ⊢Σ′ e′
1 : τ2 ⇀τ. By weakening we have
⊢Σ′ e2 : τ2, so that ⊢Σ′ e′
1( e2 ) : τ, which is enough for the result.
The statement of the progress theorem allows for the occurrence of a black hole, representing
a checkable form of non-termination. The judgment ν Σ { e ∥µ } loops, stating that e diverges by
virtue of encountering the black hole, is defined by the following rules:
ν Σ, a ~ τ { @ a ∥µ ⊗a ,→• } loops
(36.6a)

PREVIEW
340
36.3 FPC By-Need
ν Σ, a ~ τ { e ∥µ ⊗a ,→• } loops
ν Σ, a ~ τ { @ a ∥µ ⊗a ,→e } loops
(36.6b)
ν Σ { e ∥µ } loops
ν Σ { ifz e {z ,→e0 | s( x ) ,→e1} ∥µ } loops
(36.6c)
ν Σ { e1 ∥µ } loops
ν Σ { e1( e2 ) ∥µ } loops
(36.6d)
There are other ways of forming an infinite loop. The looping judgment simply codifies those
cases in which the looping behavior is a self-dependency, which is mediated by a black hole.
Theorem 36.2 (Progress). If ν Σ { e ∥µ } ok, then either ν Σ { e ∥µ } final, or ν Σ { e ∥µ } loops, or
there exists µ′ and e′ such that ν Σ { e ∥µ } 7−→ν Σ′ { e′ ∥µ′ }.
Proof. We proceed by induction on the derivations of ⊢Σ e : τ and ⊢Σ µ : Σ implicit in the deriva-
tion of ν Σ { e ∥µ } ok.
Consider rule (36.4), where the symbol a is declared in Σ. Thus Σ = Σ0, a ~ τ and ⊢Σ µ : Σ. It
follows that µ = µ0 ⊗a ,→e0 with ⊢Σ µ0 : Σ0 and ⊢Σ e0 : τ. Note that ⊢Σ µ0 ⊗a ,→• : Σ. Applying
induction to the derivation of ⊢Σ e0 : τ, we consider three cases:
1. ν Σ { e0 ∥µ ⊗a ,→• } final.
By inversion of rule (36.2b) we have e0 valΣ, and hence by
rule (36.3a) we obtain ν Σ { @ a ∥µ } 7−→ν Σ { e0 ∥µ }.
2. ν Σ { e0 ∥µ0 ⊗a ,→• } loops. By applying rule (36.6b) we obtain ν Σ { @ a ∥µ } loops.
3. ν Σ { e0 ∥µ0 ⊗a ,→• } 7−→ν Σ′ { e′
0 ∥µ′
0 ⊗a ,→• }. By applying rule (36.3b) we obtain
ν Σ { @ a ∥µ ⊗a ,→e0 } 7−→ν Σ′ { @ a ∥µ′ ⊗a ,→e′
0 }.
36.3
FPC By-Need
The language LFPC is FPC but with a by-need dynamics. For example, the dynamics of product
types in LFPC is given by the following rules:
⟨@ a1, @ a2⟩valΣ,a1~τ1,a2~τ2
(36.7a)
(e1 and e2 not references)





ν Σ { ⟨e1, e2⟩∥µ }
7−→
ν Σ, a1 ~ τ1, a2 ~ τ2 { ⟨@ a1, @ a2⟩∥µ ⊗a1 ,→e1 ⊗a2 ,→e2 }





(36.7b)

PREVIEW
36.4 Suspension Types
341
ν Σ { e ∥µ } 7−→ν Σ′ { e′ ∥µ′ }
ν Σ { e · l ∥µ } 7−→ν Σ′ { e′ · l ∥µ′ }
(36.7c)
ν Σ { e ∥µ } loops
ν Σ { e · l ∥µ } loops
(36.7d)





ν Σ, a1 ~ τ1, a2 ~ τ2 { ⟨@ a1, @ a2⟩· l ∥µ }
7−→
ν Σ, a1 ~ τ1, a2 ~ τ2 { @ a1 ∥µ }





(36.7e)
ν Σ { e ∥µ } 7−→ν Σ′ { e′ ∥µ′ }
ν Σ { e · r ∥µ } 7−→ν Σ′ { e′ · r ∥µ′ }
(36.7f)
ν Σ { e ∥µ } loops
ν Σ { e · r ∥µ } loops
(36.7g)





ν Σ, a1 ~ τ1, a2 ~ τ2 { ⟨@ a1, @ a2⟩· r ∥µ }
7−→
ν Σ, a1 ~ τ1, a2 ~ τ2 { @ a2 ∥µ }





(36.7h)
A pair is considered a value only if its arguments are symbols (rule (36.7a)), which are introduced
when the pair is created (rule (36.7b)). The first and second projections evaluate to one or the other
symbol in the pair, inducing a demand for the value of that component (rules (36.7e) and (36.7h)).
Similar ideas can be used to give a by-need dynamics to sums and recursive types.
36.4
Suspension Types
The dynamics of LFPC outlined in the previous section imposes a by-need interpretation on every
type. A more flexible approach is to isolate the machinery of by-need evaluation by introducing a
type τ susp of memoized computations, called suspensions, of a value of type τ to an eager variant
of FPC. Doing so allows the programmer to choose the extent to which a by-need dynamics is
imposed.
Informally, the type τ susp has as introduction form susp x is e representing a suspended, self-
referential, computation, e, of type τ. It has as elimination form the operation force( e ) that
evaluates the suspended computation presented by e, records the value in a memo table, and
returns that value as result. Using suspension types we can construct lazy types at will. For
example, the type of lazy pairs with components of type τ1 and τ2 is expressible as the type
τ1 susp × τ2 susp

PREVIEW
342
36.4 Suspension Types
and the type of by-need functions with domain τ1 and range τ2 is expressible as the type
τ1 susp ⇀τ2.
We may also express more complex combinations of eagerness and laziness, such as the type of
“lazy lists” consisting of computations that, when forced, evaluate either to the empty list, or a
non-empty list consisting of a natural number and another lazy list:
rec t is ( unit + ( nat × t ) ) susp.
Contrast this preceding type with this one:
rec t is ( unit + ( nat × t susp ) ).
Values of the latter type are the empty list and a pair consisting of a natural number and a compu-
tation of another such value.
The language SFPC extends FPC with a type of suspensions:
Typ
τ
::=
susp( τ )
τ susp
suspension
Exp
e
::=
susp{τ}( x . e )
susp x is e
delay
force( e )
force( e )
force
cell[ a ]
&a
indirection
Suspended computations are potentially self-referential; the bound variable x refers to the suspen-
sion itself. The expression cell[ a ] is a reference to the suspension named a, which is introduced
when a susp expression is evaluated.
The statics of SFPC is given using a judgment of the form Γ ⊢Σ e : τ, where Σ assigns types to
the names of suspensions. It is defined by the following rules:
Γ, x : susp( τ ) ⊢Σ e : τ
Γ ⊢Σ susp{τ}( x . e ) : susp( τ )
(36.8a)
Γ ⊢Σ e : susp( τ )
Γ ⊢Σ force( e ) : τ
(36.8b)
Γ ⊢Σ,a~τ cell[ a ] : susp( τ )
(36.8c)
Rule (36.8a) checks that the expression, e, has type τ under the assumption that x, which stands
for the suspension itself, has type susp( τ ).
The dynamics of SFPC is eager, with memoization confined to the suspension type as de-
scribed by the following rules:
cell[ a ] valΣ,a~τ
(36.9a)

PREVIEW
36.5 Notes
343





ν Σ { susp{τ}( x . e ) ∥µ }
7−→
ν Σ, a ~ τ { cell[ a ] ∥µ ⊗a ,→[cell[ a ]/x]e }





(36.9b)
ν Σ { e ∥µ } 7−→ν Σ′ { e′ ∥µ′ }
ν Σ { force( e ) ∥µ } 7−→ν Σ′ { force( e′ ) ∥µ′ }
(36.9c)
e valΣ,a~τ





ν Σ, a ~ τ { force( cell[ a ] ) ∥µ ⊗a ,→e }
7−→
ν Σ, a ~ τ { e ∥µ ⊗a ,→e }





(36.9d)
ν Σ, a ~ τ { e ∥µ ⊗a ,→• }
7−→
ν Σ′, a ~ τ { e′ ∥µ′ ⊗a ,→• }





ν Σ, a ~ τ { force( cell[ a ] ) ∥µ ⊗a ,→e }
7−→
ν Σ′, a ~ τ { force( cell[ a ] ) ∥µ′ ⊗a ,→e′ }





(36.9e)
Rule (36.9a) specifies that a reference to a suspension is a value. Rule (36.9b) specifies that evalu-
ation of a delayed computation consists of allocating a fresh symbol for it in the memo table, and
returning a reference to that suspension. Rules (36.9c) to (36.9e) specify that demanding the value
of a suspension forces evaluation of the suspended computation, which is then stored in the memo
table and returned as the result.
36.5
Notes
The by-need dynamics given here is inspired by Ariola and Felleisen (1997), but with the differ-
ence that by-need cells are regarded as assignables, rather than variables. Doing so maintains the
principle that variables are given meaning by substitution. In contrast by-need cells are a form of
assignable to which at most one assignment is ever done.
Exercises
36.1. Recall from Chapter 20 that, under a lazy interpretation, the recursive type
rec t is [z ,→unit , s ,→t]

PREVIEW
344
36.5 Notes
contains the “infinite number” ω ≜fix x : nat is s( x ). Contrast the behavior of ω under
the by-need interpretation given in this chapter with that by-name interpretation given in
Chapters 19 and 20.
36.2. In LFPC the putative recursive type of “lists” of natural numbers,
rec t is [nil ,→unit , cons ,→nat × t],
is, rather, the type of finite or infinite streams of natural numbers. To prove this, exhibit the
stream of all natural numbers as an element of this type.
36.3. Complete the definition of LFPC by giving the by-need dynamics for unit, void, sum, and
recursive types.
36.4. LFPC can be interpreted into SFPC. Complete the following chart defining the interpreta-
tion, bτ, of the type τ:
[
unit ≜. . .
\
τ1 × τ2 ≜. . .
[
void ≜. . .
\
τ1 + τ2 ≜. . .
\
rec t is τ ≜. . . .
Hint: Characterize the values of the lazy types in the left column, and express those values as
eager types in the right column, using suspensions where necessary.

PREVIEW
Part XV
Parallelism

PREVIEW

PREVIEW
Chapter 37
Nested Parallelism
Parallel computation seeks to reduce the running times of programs by allowing many computa-
tions to be carried out simultaneously. For example, if we wish to add two numbers, each given by
a complex computation, we may consider evaluating the addends simultaneously, then comput-
ing their sum. The ability to exploit parallelism is limited by the dependencies among parts of a
program. Obviously, if one computation depends on the result of another, then we have no choice
but to execute them sequentially so that we may propagate the result of the first to the second.
Consequently, the fewer dependencies among sub-computations, the greater the opportunities for
parallelism.
In this chapter we discuss the language PPCF, which is the extension of PCF with nested par-
allelism. Nested parallelism has a hierarchical structure arising from forking two (or more) parallel
computations, then joining these computations to combine their results before proceeding. Nested
parallelism is also known as fork-join parallelism. We will consider two forms of dynamics for
nested parallelism. The first is a structural dynamics in which a single transition on a compound
expression may involve multiple transitions on its constituent expressions. The second is a cost dy-
namics (introduced in Chapter 7) that focuses attention on the sequential and parallel complexity
(also known as the work and the depth, or span) of a parallel program by associating a series-parallel
graph with each computation.
37.1
Binary Fork-Join
The syntax of PPCF extends that of PCF with the following construct:
Exp
e
::=
par( e1 ; e2 ; x1 . x2 . e )
par x1 = e1 and x2 = e2 in e
parallel let
The variables x1 and x2 are bound only within e, and not within e1 or e2, which ensures that they
are not mutually dependent and hence can be evaluated simultaneously. The variable bindings
represent a fork of two parallel computations e1 and e2, and the body e represents their join.

PREVIEW
348
37.1 Binary Fork-Join
The statics of PPCF enriches that of PCF with the following rule for parallel let:
Γ ⊢e1 : τ1
Γ ⊢e2 : τ2
Γ, x1 : τ1, x2 : τ2 ⊢e : τ
Γ ⊢par( e1 ; e2 ; x1 . x2 . e ) : τ
(37.1)
The sequential structural dynamics of PPCF is defined by a transition judgment of the form
e 7−−→
seq e′ defined by these rules:
e1 7−−→
seq e′
1
par( e1 ; e2 ; x1 . x2 . e ) 7−−→
seq par( e′
1 ; e2 ; x1 . x2 . e )
(37.2a)
e1 val
e2 7−−→
seq e′
2
par( e1 ; e2 ; x1 . x2 . e ) 7−−→
seq par( e1 ; e′
2 ; x1 . x2 . e )
(37.2b)
e1 val
e2 val
par( e1 ; e2 ; x1 . x2 . e ) 7−−→
seq [e1, e2/x1, x2]e
(37.2c)
The parallel structural dynamics of PPCF is given by a transition judgment of the form e 7−−→
par e′,
defined as follows:
e1 7−−→
par e′
1
e2 7−−→
par e′
2
par( e1 ; e2 ; x1 . x2 . e ) 7−−→
par par( e′
1 ; e′
2 ; x1 . x2 . e )
(37.3a)
e1 7−−→
par e′
1
e2 val
par( e1 ; e2 ; x1 . x2 . e ) 7−−→
par par( e′
1 ; e2 ; x1 . x2 . e )
(37.3b)
e1 val
e2 7−−→
par e′
2
par( e1 ; e2 ; x1 . x2 . e ) 7−−→
par par( e1 ; e′
2 ; x1 . x2 . e )
(37.3c)
e1 val
e2 val
par( e1 ; e2 ; x1 . x2 . e ) 7−−→
par [e1, e2/x1, x2]e
(37.3d)
The parallel dynamics abstracts away from any limitations on processing capacity; such limitations
are considered in Section 37.4.
The implicit parallelism theorem states that the sequential and the parallel dynamics coincide.
Consequently, we need never be concerned with the meaning of a parallel program (its meaning
is given by the sequential dynamics), but only with its efficiency. As a practical matter, this means
that a program can be developed on a sequential platform, even if it is meant to run on a parallel
platform, because the behavior is not affected by whether we execute it using a sequential or a
parallel dynamics. Because the sequential dynamics is deterministic (every expression has at most

PREVIEW
37.1 Binary Fork-Join
349
one value), the implicit parallelism theorem implies that the parallel dynamics is also determin-
istic. For this reason the implicit parallelism theorem is also known as the deterministic parallelism
theorem. This terminology emphasizes the distinction between deterministic parallelism, the subject
of this chapter, from non-deterministic concurrency, the subject of Chapters 39 and 40.
A proof of the implicit parallelism theorem can be given by giving an evaluation dynamics
e ⇓v in the style of Chapter 7, and showing that
e 7−−→
par
∗v
iff
e ⇓v
iff
e 7−−→
seq
∗v
(where v is a closed expression such that v val). The most important rule of the evaluation dynam-
ics is for the evaluation of a parallel binding:
e1 ⇓v1
e2 ⇓v2
[v1, v2/x1, x2]e ⇓v
par( e1 ; e2 ; x1 . x2 . e ) ⇓v
(37.4)
The other rules are easily derived from the structural dynamics of PCF as in Chapter 7.
It is possible to show that the sequential dynamics of PPCF agrees with its evaluation dynam-
ics by extending the proof of Theorem 7.2.
Lemma 37.1. For all v val, e 7−−→
seq
∗v if, and only if, e ⇓v.
Proof. It suffices to show that if e 7−−→
seq
e′ and e′ ⇓v, then e ⇓v, and that if e1 7−−→
seq
∗v1 and
e2 7−−→
seq
∗v2 and [v1, v2/x1, x2]e 7−−→
seq
∗v, then
par x1 = e1 and x2 = e2 in e 7−−→
seq
∗v.
By a similar argument we may show that the parallel dynamics also agrees with the evaluation
dynamics, and hence with the sequential dynamics.
Lemma 37.2. For all v val, e 7−−→
par
∗v if, and only if, e ⇓v.
Proof. It suffices to show that if e 7−−→
par
e′ and e′ ⇓v, then e ⇓v, and that if e1 7−−→
par
∗v1 and
e2 7−−→
par
∗v2 and [v1, v2/x1, x2]e 7−−→
par
∗v, then
par x1 = e1 and x2 = e2 in e 7−−→
par
∗v.
The proof of the first is by induction on the parallel dynamics. The proof of the second proceeds
by simultaneous induction on the derivations of e1 7−−→
par
∗v1 and e2 7−−→
par
∗v2. If e1 = v1 with v1 val
and e2 = v2 with v2 val, then the result follows immediately from the third premise. If e2 = v2
but e1 7−−→
par
e′
1 7−−→
par
∗v1, then by induction we have that par x1 = e′
1 and x2 = v2 in e 7−−→
par
∗v, and
hence the result follows by an application of rule (37.3b). The symmetric case follows similarly by
an application of rule (37.3c), and in the case that both e1 and e2 transition, the result follows by
induction and rule (37.3a).

PREVIEW
350
37.2 Cost Dynamics
Theorem 37.3 (Implicit Parallelism). The sequential and parallel dynamics coincide: for all v val, e 7−−→
seq
∗
v iff e 7−−→
par
∗v.
Proof. By Lemmas 37.1 and 37.2.
The implicit parallelism theorem states that parallelism does not affect the meaning of a pro-
gram, only the efficiency of its execution. Correctness is not affected by parallelism, only efficiency.
37.2
Cost Dynamics
In this section we define a parallel cost dynamics that assigns a cost graph to the evaluation of a PPCF
expression. Cost graphs are defined by the following grammar:
Cost
c
::=
0
zero cost
1
unit cost
c1 ⊗c2
parallel combination
c1 ⊕c2
sequential combination
A cost graph is a series-parallel ordered directed acyclic graph, with a designated source node and
sink node. For 0 the graph consists of one node and no edges, with the source and sink both being
the node itself. For 1 the graph consists of two nodes and one edge directed from the source to
the sink. For c1 ⊗c2, if g1 and g2 are the graphs of c1 and c2, respectively, then the graph has two
extra nodes, a source node with two edges to the source nodes of g1 and g2, and a sink node, with
edges from the sink nodes of g1 and g2 to it. The children of the source are ordered according to
the sequential evaluation order. Finally, for c1 ⊕c2, where g1 and g2 are the graphs of c1 and c2,
the graph has as source node the source of g1, as sink node the sink of g2, and an edge from the
sink of g1 to the source of g2.
The intuition behind a cost graph is that nodes represent subcomputations of an overall com-
putation, and edges represent sequentiality constraints stating that one computation depends on
the result of another, and hence cannot be started before the one on which it depends completes.
The product of two graphs represents parallelism opportunities in which there are no sequentiality
constraints between the two computations. The assignment of source and sink nodes reflects the
overhead of forking two parallel computations and joining them after they have both completed.
At the structural level, we note that only the root has no ancestors, and only the final node of
the cost graph has no descendents. Interior nodes may have one or two descendents, the former
representing a sequential dependency, and the latter representing a fork point. Such nodes may
have one or two ancestors, the former corresponding to a sequential dependency and the latter
representing a join point.
We associate with each cost graph two numeric measures, the work, wk(c), and the depth, dp(c).

PREVIEW
37.2 Cost Dynamics
351
The work is defined by the following equations:
wk(c) =









0
if c = 0
1
if c = 1
wk(c1) + wk(c2)
if c = c1 ⊗c2
wk(c1) + wk(c2)
if c = c1 ⊕c2
(37.5)
The depth is defined by the following equations:
dp(c) =









0
if c = 0
1
if c = 1
max(dp(c1), dp(c2))
if c = c1 ⊗c2
dp(c1) + dp(c2)
if c = c1 ⊕c2
(37.6)
Informally, the work of a cost graph determines the total number of computation steps repre-
sented by the cost graph, and thus corresponds to the sequential complexity of the computation.
The depth of the cost graph determines the critical path length, the length of the longest depen-
dency chain within the computation, which imposes a lower bound on the parallel complexity of a
computation. The critical path length is a lower bound on the number of steps required to com-
plete the computation.
In Chapter 7 we introduced cost dynamics to assign time complexity to computations. The proof
of Theorem 7.7 shows that e ⇓k v iff e 7−→k v. That is, the step complexity of an evaluation of e to
a value v is just the number of transitions required to derive e 7−→∗v. Here we use cost graphs
as the measure of complexity, then relate these cost graphs to the structural dynamics given in
Section 37.1.
The judgment e ⇓c v, where e is a closed expression, v is a closed value, and c is a cost graph
specifies the cost dynamics. By definition we arrange that e ⇓0 e when e val. The cost assignment
for let is given by the following rule:
e1 ⇓c1 v1
e2 ⇓c2 v2
[v1, v2/x1, x2]e ⇓c v
par( e1 ; e2 ; x1 . x2 . e ) ⇓(c1⊗c2)⊕1⊕c v
(37.7)
The cost assignment specifies that, under ideal conditions, e1 and e2 are evaluated in parallel, and
that their results are passed to e. The cost of fork and join is implicit in the parallel combination
of costs, and assign unit cost to the substitution because we expect it to be implemented by a
constant-time mechanism for updating an environment. The cost dynamics of other language
constructs is specified in a similar way, using only sequential combination to isolate the source of
parallelism to the par construct.
Two simple facts about the cost dynamics are important to keep in mind. First, the cost assign-
ment does not influence the outcome.
Lemma 37.4. e ⇓v iff e ⇓c v for some c.

PREVIEW
352
37.2 Cost Dynamics
Proof. From right to left, erase the cost assignments to construct an evaluation derivation. From
left to right, decorate the evaluation derivations with costs as determined by the rules defining the
cost dynamics.
Second, the cost of evaluating an expression is uniquely determined.
Lemma 37.5. If e ⇓c v and e ⇓c′ v, then c is c′.
Proof. By induction on the derivation of e ⇓c v.
The link between the cost dynamics and the structural dynamics is given by the following
theorem, which states that the work cost is the sequential complexity, and the depth cost is the
parallel complexity, of the computation.
Theorem 37.6. If e ⇓c v, then e 7−−→
seq
w v and e 7−−→
par
d v, where w = wk(c) and d = dp(c). Conversely, if
e 7−−→
seq
w v, then there exists c such that e ⇓c v with wk(c) = w, and if e 7−−→
par
d v′, then there exists c′ such
that e ⇓c′ v′ with dp(c′) = d.
Proof. The first part is proved by induction on the derivation of e ⇓c v, the interesting case being
rule (37.7). By induction we have e1 7−−→
seq
w1 v1, e2 7−−→
seq
w2 v2, and [v1, v2/x1, x2]e 7−−→
seq
w v, where
w1 = wk(c1), w2 = wk(c2), and w = wk(c). By pasting together derivations we get a derivation
par( e1 ; e2 ; x1 . x2 . e ) 7−−→
seq
w1 par( v1 ; e2 ; x1 . x2 . e )
7−−→
seq
w2 par( v1 ; v2 ; x1 . x2 . e )
7−−→
seq [v1, v2/x1, x2]e
7−−→
seq
w v.
Noting that wk((c1 ⊗c2) ⊕1 ⊕c) = w1 + w2 + 1 + w completes the proof. Similarly, we have
by induction that e1 7−−→
par
d1 v1, e2 7−−→
par
d2 v2, and [v1, v2/x1, x2]e 7−−→
par
d v, where d1 = dp(c1),
d2 = dp(c2), and d = dp(c). Assume, without loss of generality, that d1 ≤d2 (otherwise simply
swap the roles of d1 and d2 in what follows). We may paste together derivations as follows:
par( e1 ; e2 ; x1 . x2 . e ) 7−−→
par
d1 par( v1 ; e′
2 ; x1 . x2 . e )
7−−→
par
d2−d1 par( v1 ; v2 ; x1 . x2 . e )
7−−→
par [v1, v2/x1, x2]e
7−−→
par
d v.
Calculating dp((c1 ⊗c2) ⊕1 ⊕c) = max(d1, d2) + 1 + d completes the proof.

PREVIEW
37.3 Multiple Fork-Join
353
Turning to the second part, it suffices to show that if e 7−−→
seq
e′ with e′ ⇓c′ v, then e ⇓c v with
wk(c) = wk(c′) + 1, and if e 7−−→
par e′ with e′ ⇓c′ v, then e ⇓c v with dp(c) = dp(c′) + 1.
Suppose that e = par( e1 ; e2 ; x1 . x2 . e0 ) with e1 val and e2 val. Then e 7−−→
seq
e′, where e =
[e1, e2/x1, x2]e0 and there exists c′ such that e′ ⇓c′ v. But then e ⇓c v, where c = (0 ⊗0) ⊕1 ⊕c′,
and a simple calculation shows that wk(c) = wk(c′) + 1, as required. Similarly, e 7−−→
par
e′ for e′ as
above, and hence e ⇓c v for some c such that dp(c) = dp(c′) + 1, as required.
Suppose that e = par( e1 ; e2 ; x1 . x2 . e0 ) and e 7−−→
seq
e′, where e′ = par( e′
1 ; e2 ; x1 . x2 . e0 ) and
e1 7−−→
seq
e′
1. From the assumption that e′ ⇓c′ v, we have by inversion that e′
1 ⇓c′
1 v1, e2 ⇓c′
2 v2,
and [v1, v2/x1, x2]e0 ⇓c′
0 v, with c′ = (c′
1 ⊗c′
2) ⊕1 ⊕c′
0. By induction there exists c1 such that
wk(c1) = 1 + wk(c′
1) and e1 ⇓c1 v1. But then e ⇓c v, with c = (c1 ⊗c′
2) ⊕1 ⊕c′
0.
By a similar argument, suppose that e = par( e1 ; e2 ; x1 . x2 . e0 ) and e 7−−→
par
e′, where e′ =
par( e′
1 ; e′
2 ; x1 . x2 . e0 ) and e1 7−−→
par
e′
1, e2 7−−→
par
e′
2, and e′ ⇓c′ v. Then by inversion e′
1 ⇓c′
1 v1,
e′
2 ⇓c′
2 v2, [v1, v2/x1, x2]e0 ⇓c0 v. But then e ⇓c v, where c = (c1 ⊗c2) ⊕1 ⊕c0, e1 ⇓c1 v1 with
dp(c1) = 1 + dp(c′
1), e2 ⇓c2 v2 with dp(c2) = 1 + dp(c′
2), and [v1, v2/x1, x2]e0 ⇓c0 v. Calculating, we
get
dp(c) = max(dp(c′
1) + 1, dp(c′
2) + 1) + 1 + dp(c0)
= max(dp(c′
1), dp(c′
2)) + 1 + 1 + dp(c0)
= dp((c′
1 ⊗c′
2) ⊕1 ⊕c0) + 1
= dp(c′) + 1,
which completes the proof.
Corollary 37.7. If e 7−−→
seq
w v and e 7−−→
par
d v′, then v is v′ and e ⇓c v for some c such that wk(c) = w and
dp(c) = d.
37.3
Multiple Fork-Join
So far we have confined attention to binary fork/join parallelism induced by the parallel par con-
struct. A generalizaton, called data parallelism, allows the simultaneous creation of any number of
tasks that compute on the components of a data structure. The main example is a sequence of val-
ues of a specified type. The primitive operations on sequences are a natural source of unbounded
parallelism. For example, we may consider a parallel map construct that applies a given function
to every element of a sequence simultaneously, forming a sequence of the results.

PREVIEW
354
37.3 Multiple Fork-Join
We will consider here a simple language of sequence operations to illustrate the main ideas.
Typ
τ
::=
seq( τ )
τ seq
sequence
Exp
e
::=
seq{n}( e0, . . . ,en−1 )
⟨e0, . . . ,en−1 ⟩n
sequence
len( e )
|e|
size
sub( e1 ; e2 )
e1[ e2 ]
element
tab( x . e1 ; e2 )
tab( x . e1 ; e2 )
tabulate
map( x . e1 ; e2 )
[ e1 | x ∈e2 ]
map
cat( e1 ; e2 )
cat( e1 ; e2 )
concatenate
The expression seq{n}( e0, . . . ,en−1 ) evaluates to a sequence whose length is n and whose ele-
ments are given by the expressions e0, . . . , en−1. The operation len( e ) returns the number of ele-
ments in the sequence given by e. The operation sub( e1 ; e2 ) retrieves the element of the sequence
given by e1 at the index given by e2. The tabulate operation, tab( x . e1 ; e2 ), yields the sequence of
length given by e2 whose ith element is given by [i/x]e1. The operation map( x . e1 ; e2 ) computes
the sequence whose ith element is given by [e/x]e1, where e is the ith element of the sequence
given by e2. The operation cat( e1 ; e2 ) concatenates two sequences of the same type.
The statics of these operations is given by the following typing rules:
Γ ⊢e0 : τ
. . .
Γ ⊢en−1 : τ
Γ ⊢seq{n}( e0, . . . ,en−1 ) : seq( τ )
(37.8a)
Γ ⊢e : seq( τ )
Γ ⊢len( e ) : nat
(37.8b)
Γ ⊢e1 : seq( τ )
Γ ⊢e2 : nat
Γ ⊢sub( e1 ; e2 ) : τ
(37.8c)
Γ, x : nat ⊢e1 : τ
Γ ⊢e2 : nat
Γ ⊢tab( x . e1 ; e2 ) : seq( τ )
(37.8d)
Γ ⊢e2 : seq( τ )
Γ, x : τ ⊢e1 : τ′
Γ ⊢map( x . e1 ; e2 ) : seq( τ′ )
(37.8e)
Γ ⊢e1 : seq( τ )
Γ ⊢e2 : seq( τ )
Γ ⊢cat( e1 ; e2 ) : seq( τ )
(37.8f)
The cost dynamics of these constructs is defined by the following rules:
e0 ⇓c0 v0
. . .
en−1 ⇓cn−1 vn−1
seq{n}( e0, . . . ,en−1 ) ⇓
Nn−1
i=0 ci seq{n}( v0, . . . ,vn−1 )
(37.9a)
e ⇓c seq{n}( v0, . . . ,vn−1 )
len( e ) ⇓c⊕1 num[ n ]
(37.9b)
e1 ⇓c1 seq{n}( v0, . . . ,vn−1 )
e2 ⇓c2 num[ i ]
(0 ≤i < n)
sub( e1 ; e2 ) ⇓c1⊕c2⊕1 vi
(37.9c)

PREVIEW
37.4 Bounded Implementations
355
e2 ⇓c num[ n ]
[num[ 0 ]/x]e1 ⇓c0 v0
. . .
[num[ n −1 ]/x]e1 ⇓cn−1 vn−1
tab( x . e1 ; e2 ) ⇓c⊕Nn−1
i=0 ci seq{n}( v0, . . . ,vn−1 )
(37.9d)
e2 ⇓c seq{n}( v0, . . . ,vn−1 )
[v0/x]e1 ⇓c0 v′
0
. . .
[vn−1/x]e1 ⇓cn−1 v′
n−1
map( x . e1 ; e2 ) ⇓c⊕Nn−1
i=0 ci seq{n}( v′
0, . . . ,v′
n−1 )
(37.9e)
e1 ⇓c1 seq{m}( v0, . . . , vm−1 )
e2 ⇓c2 seq{n}( v′
0, . . . , v′
n−1 )
p = m + n
cat( e1 ; e2 ) ⇓c1⊕c2⊕Nm+n
i=0 1 seq{p}( v0, . . . , vm−1, v′
0, . . . , v′
n−1 )
(37.9f)
The cost dynamics for sequence operations is validated by introducing a sequential and parallel
cost dynamics and extending the proof of Theorem 37.6 to cover this extension.
37.4
Bounded Implementations
Theorem 37.6 states that the cost dynamics accurately models the dynamics of the parallel let
construct, whether executed sequentially or in parallel. The theorem validates the cost dynamics
from the point of view of the dynamics of the language, and permits us to draw conclusions
about the asymptotic complexity of a parallel program that abstracts away from the limitations
imposed by a concrete implementation. Chief among these is the restriction to a fixed number,
p > 0, of processors on which to schedule the workload. Besides limiting the available parallelism
this also imposes some synchronization overhead that must be taken into account. A bounded
implementation is one for which we may establish an asymptotic bound on the execution time once
these overheads are taken into account.
A bounded implementation must take account of the limitations and capabilities of the hard-
ware on which the program is run. Because we are only interested in asymptotic upper bounds,
it is convenient to formulate an abstract machine model, and to show that the primitives of the
language can be implemented on this model with guaranteed time (and space) bounds. One ex-
ample of such a model is the shared-memory multiprocessor, or SMP, model. The basic assumption
of the SMP model is that there are some fixed p > 0 processors coordinated by an interconnect
that permits constant-time access to any object in memory shared by all p processors.1 An SMP
is assumed to provide a constant-time synchronization primitive with which to control simulta-
neous access to a memory cell. There are a variety of such primitives, any of which are enough
to provide a parallel fetch-and-add instruction that allows each processor to get the current con-
tents of a memory cell and update it by adding a fixed constant in a single atomic operation—the
interconnect serializes any simultaneous accesses by more than one processor.
Building a bounded implementation of parallelism involves two major tasks. First, we must
show that the primitives of the language can be implemented efficiently on the abstract machine
model. Second, we must show how to schedule the workload across the processors to minimize
execution time by maximizing parallelism. When working with a low-level machine model such
1A slightly weaker assumption is that each access may require up to lg p time to account for the overhead of synchro-
nization, but we shall neglect this refinement in the present, simplified account.

PREVIEW
356
37.4 Bounded Implementations
as an SMP, both tasks involve a fair bit of technical detail to show how to use low-level machine
instructions, including a synchronization primitive, to implement the language primitives and to
schedule the workload. Collecting these together, we may then give an asymptotic bound on the
time complexity of the implementation that relates the abstract cost of the computation to cost
of implementing the workload on a p-way multiprocessor. The prototypical result of this kind is
Brent’s Theorem.
Theorem 37.8. If e ⇓c v with wk(c) = w and dp(c) = d, then e can be evaluated on a p-processor SMP
in time O(max(w/p, d)).
The theorem tells us that we can never execute a program in fewer steps than its depth d and
that, at best, we can divide the work up evenly into w/p rounds of execution by the p processors.
Note that if p = 1 then the theorem establishes an upper bound of O(w) steps, the sequential
complexity of the computation. Moreover, if the work is proportional to the depth, then we are
unable to exploit parallelism, and the overall time is proportional to the work alone.
Theorem 37.8 motivates consideration of a useful figure of merit, the parallelizability ratio, which
is the ratio w/d of work to depth. If w/d ≫p, then the program is parallelizable, because then
w/p ≫d, and we may therefore reduce running time by using p processors at each step. If the
parallelizability ratio is a constant, then d will dominate w/p, and we will have little opportunity
to exploit parallelism to reduce running time. It is not known, in general, whether a problem
admits a parallelizable solution. The best we can say, on present knowledge, is that there are
algorithms for some problems that have a high degree of parallelizability, and there are problems
for which no such algorithm is known. It is a difficult problem in complexity theory to analyze
which problems are parallelizable, and which are not.
Proving Brent’s Theorem for an SMP would take us much too far afield for the present pur-
poses. Instead we shall prove a Brent-type Theorem for an abstract machine, the P machine. The
machine is unrealistic in that it is defined at a very high level of abstraction. But it is designed to
match well the cost dynamics given earlier in this chapter. In particular, there are mechanisms that
account for both sequential and parallel dependencies in a computation.
At the highest level, the state of the P machine consists of a global task graph whose struc-
ture corresponds to a “diagonal cut” through the cost graph of the overall computation. Nodes
immediately above the cut are eligible to be executed, higher ancestors having already been com-
pleted, and whose immediate descendents are waiting for their ancestors to complete. Further
descendents in the full task graph are tasks yet to be created, once the immediate descendents
are finished. The P machine discards completed tasks, and future tasks beyond the immediate
dependents are only created as execution proceeds. Thus it is only those nodes next to the cut line
through the cost graph that are represented in the P machine state.
The global state of the P machine is a configuration of the form ν Σ { µ }, where Σ is degenerated
to just a finite set of (pairwise distinct) task names and µ is a finite mapping of the task names in Σ
to local states, representing the state of an individual task. A local state is either a closed PCF ex-
pression, or one of two special join points that implement the sequential and parallel dependencies
of a task on one or two ancestors, respectively.2 Thus, when expanded out, a global state has the
2The use of join points for each sequential dependency is profligate, but aligns the machine with the cost dynamics.
Realistically, individual tasks manage sequential dependencies without synchronization, by using local control stacks as in

PREVIEW
37.4 Bounded Implementations
357
form
ν a1, . . . , an { a1 ,→s1 ⊗. . . ⊗an ,→sn },
where n ≥1, and each si is a local state. The ordering of the tasks in a state, like the order of
declarations in the signature, is not significant.
A P machine state transition has the form ν Σ { µ } 7−→ν Σ′ { µ′ }. There are two forms of
such transitions, the global and the local. A global step selects as many tasks as are available, up
to a pre-specified parameter p > 0, which represents the number of processors available at each
round. (Such a scheduler is greedy in the sense that it never fails to execute an available task, up
to the specified limit for each round.) A task is finished if it consists of a closed PCF value, or
is a join point whose dependents are not yet finished; otherwise a task is available, or ready. A
ready task is always capable of taking a local step consisting of either a step of PCF, expressed in
the setting of the P machine, or a synchronization step that manages the join-point logic. Because
the P machine employs a greedy scheduler, it must complete execution in time proportional to
max(w/p, d) steps by doing up to p steps of work at a time, insofar as it is possible within the
limits of the depth of the computation. We thus get a Brent-type Theorem for the abstract machine
that illustrates more sophisticated Brent-type Theorems for other models, such as the PRAM, that
are used in the analysis of parallel algorithms.
The local transitions of the P machine corresponding to the steps of PCF itself are illustrated
by the following example rules for application; the others follow a similar pattern.3
¬(e1 val)
ν a { a ,→e1( e2 ) } 7−→
loc ν a a1 { a ,→join[ a1 ]( x1 . x1( e2 ) ) ⊗a1 ,→e1 }
(37.10a)
e1 val
¬(e2 val)
ν a { a ,→e1( e2 ) } 7−→
loc ν a a2 { a ,→join[ a2 ]( x2 . e1( x2 ) ) ⊗a2 ,→e2 }
(37.10b)
e1 val
ν a a1 { a ,→join[ a1 ]( x1 . x1( e2 ) ) ⊗a1 ,→e1 } 7−→
loc ν a { a ,→e1( e2 ) }
(37.10c)
e1 val
e2 val
ν a a2 { a1 ,→join[ a2 ]( x2 . e1( x2 ) ) ⊗a2 ,→e2 } 7−→
loc ν a { a ,→e1( e2 ) }
(37.10d)
e2 val
ν a { a ,→( λ ( x : τ2 ) e )( e2 ) } 7−→
loc ν a { a ,→[e2/x]e }
(37.10e)
Rules (37.10a) and (37.10b) create tasks for the evaluation of the function and argument of an ex-
pression. Rules (37.10c) and (37.10d) propagate the result of evaluation of the function or argument
of an application to the appropriate application expression. This rule mediates between the first
two rules and Rule (37.10e), which effects a β-reduction in-place.
Chapter 28.
3Types are omitted from Σ for brevity.

PREVIEW
358
37.4 Bounded Implementations
The local transitions of the P machine corresponding to binary fork and join are as follows:







ν a { a ,→par( e1 ; e2 ; x1 . x2 . e ) }
7−→
loc
ν a1, a2, a { a1 ,→e1 ⊗a2 ,→e2 ⊗a ,→join[ a1 ; a2 ]( x1 ; x2 . e ) }







(37.11a)
e1 val
e2 val







ν a1, a2, a { a1 ,→e1 ⊗a2 ,→e2 ⊗a ,→join[ a1 ; a2 ]( x1 ; x2 . e ) }
7−→
loc
ν a { a ,→[e1, e2/x1, x2]e }







(37.11b)
Rule (37.11a) creates two parallel tasks on which the executing task depends. The expression
join[ a1 ; a2 ]( x1 ; x2 . e ) is blocked on tasks a1 and a2, so that no local step applies to it. Rule (37.11b)
synchronizes a task with the tasks on which it depends once their execution has completed; those
tasks are no longer required, and are eliminated from the state.
Each global transition is the simultaneous execution of one step of computation on as many as
p ≥1 processors.
ν Σ1 a1 { µ1 ⊗a1 ,→s1 } 7−→
loc ν Σ′
1 a1 { µ′
1 ⊗a1 ,→s′
1 }
. . .
ν Σn an { µn ⊗an ,→sn } 7−→
loc ν Σ′
n an { µ′
n ⊗an ,→s′
n }







ν Σ0 Σ1 a1 . . . Σn an { µ0 ⊗µ1 ⊗a1 ,→s1 ⊗. . . ⊗µn ⊗an ,→sn }
7−→
glo
ν Σ0 Σ′
1 a1 . . . Σ′
n an { µ0 ⊗µ′
1 ⊗a1 ,→s′
1 ⊗. . . ⊗µ′
n ⊗an ,→s′
n }







(37.12)
At each global step some number 1 ≤n ≤p of ready tasks are scheduled for execution, where n is
maximal among the number of ready tasks. Because no two distinct tasks may depend on the same
task, we may partition the n tasks so that each scheduled task is grouped with the tasks on which
it depends as necessary for any local join step. Any local fork step adds two fresh tasks to the state
resulting from the global transition; any local join step eliminates two tasks whose execution has
completed. A subtle point is that it is implicit in our name binding conventions that the names
of any created tasks are globally unique, even though they are locally created. In implementation
terms this requires a synchronization step among the processors to ensure that task names are not
accidentally reused among the parallel tasks.
The proof of a Brent-type Theorem for the P machine is now obvious. We need only ensure
that the parameter n of Rule (37.12) is chosen as large as possible at each step, limited only by
the parameter p and the number of ready tasks. A scheduler with this property is greedy; it never
allows a processor to go idle if work remains to be done. Consequently, if there are always p
available tasks at each global step, then the evaluation will complete in w/p steps, where w is

PREVIEW
37.5 Scheduling
359
the work complexity of the program. If, at some stage, fewer than p tasks are available, then
performance degrades according to the sequential dependencies among the sub-computations. In
the limiting case the P machine must take at least d steps, where d is the depth of the computation.
37.5
Scheduling
The global transition relation of the P machine defined in Section 37.4 affords wide latitude in the
choice of tasks that are advanced by taking a local transition. Doing so abstracts from implemen-
tation details that are irrelevant to the proof of the Brent-type Theorem given later in that section,
the only requirement being that the number of tasks chosen be as large as possible up to the spec-
ified bound p, representing the number of available processors. When taking into account factors
not considered here, it is necessary to specify the scheduling policy more precisely—for example,
different scheduling policies may have asymptotically different space requirements. The overall
idea is to consider scheduling a computation on p processors as a p-way parallel traversal of its cost
graph, visiting up to p nodes at a time in an order consistent with the dependency ordering. In
this section we will consider one such traversal, p-way parallel depth-first-search, or p-DFS, which
specializes to the familiar depth-first traversal in the case that p = 1.
Recall that the depth first-search of a directed graph maintain a stack of unvisited nodes, which
is initialized with the start node. At each round, a node is popped from the stack and visited, and
then its unvisited children are pushed on the stack (in reverse order in the case of ordered graphs),
completing that round. The traversal terminates when the stack is empty. When viewed as a
scheduling strategy, visiting a node of a cost graph consists of scheduling the work associated
with that node on a processor. The job of such as scheduler is to do the work of the computation in
depth-first order, visiting the children of a node from left to right, consistently with the sequential
dynamics (which would, in particular, treat a parallel binding as two sequential bindings). Notice
that because a cost graph is directed acyclic, there are no “back edges” arising from the traversal,
and because it is series-parallel in structure, there are no “cross edges”. Thus, all children of a node
are unvisited, and no task is considered more than once.
Although evocative, viewing scheduling as graph traversal invites one to imagine that the
cost graph is given explicitly as a data structure, which is not at all the case. Instead the graph
is created dynamically as the sub-computations are executed. At each round the computation
associated with a node may complete (when it has achieved its value), continue (when more work
is yet to be done), or fork (when it generates parallel sub-computations with a specified join point).
Once a computation has completed and its value has been passed to the associated join point, its
node in the cost graph is discarded. Furthermore, the children of a node only come into existence
as a result of its execution, according to whether it completes (no children), continues (one child),
or forks (two children). Thus one may envision that the cost graph “exists” as a cut through the
abstract cost graph representing pending tasks that have not yet been activated by the traversal.
A parallel depth-first search works much the same way, except that as many as p nodes are
visited at each round, constrained only by the presence of unvisited (yet-to-be-scheduled) nodes.
One might naively think that this simply means popping up to p nodes from the stack on each
round, visiting them all simultaneously, and pushing their dependents on the stack in reverse or-
der, just as for conventional depth-first search. But a moment’s thought reveals that this is not

PREVIEW
360
37.6 Notes
correct. Because the cost graphs are ordered, the visited nodes form a sequence determined by
the left-to-right ordering of the children of a node. If a node completes, it has no children and is
removed from its position in the sequence in the next round. If a node continues, it has one child
that occupies the same relative position as its parent in the next round. And if a node forks two
children, they are inserted into the sequence after the predecessor, and immediately prior to that
node, related to each other by the left-to-right ordering of the children. The task associated to the
visited node itself becomes the join point of the immediately preceding pair of tasks, with which
it will synchronize when they complete. Thus the visited sequence of k ≤p nodes becomes, on
the next round, anywhere from 0 (if all nodes completes) to 3 × k nodes (if each node forks). These
are placed into consideration, in the specified order, for the next round to ensure that they are pro-
cessed in depth-first order. Importantly, the data structure maintaining the unvisited nodes of the
graph is not a simple pushdown stack, because of the “in-place” replacement of each visited node
by zero, one, or two nodes in between its predecessor and successor in the sequential ordering of
the visited nodes.
Consider a variant of the P machine in which the order of the tasks is significant. A task is
finished if it is a value, blocked if it is a join, and ready otherwise. Local transitions remain the same
as in Section 37.4, bearing in mind that the ordering is significant. A global transition, however,
consists of making a local transition on each of the first k ≤p ready tasks.4 After this selection the
global state is depicted as follows:
ν Σ0 a1 Σ1 . . . ak Σk Σ { µ0 ⊗a1 ,→e1 ⊗µ1 ⊗. . . ak ,→ek ⊗µ }
where each µi consists of finished or blocked tasks, and each ei is ready. A schedule is greedy If
k < p only when no task in µ is ready.
After a local transition is made on each of the k selected tasks, the resulting global state has the
form
ν Σ0Σ′
1 a1 Σ1 . . . Σ′
k ak Σk Σ { µ0 ⊗µ′
1 ⊗a1 ,→e′
1 ⊗µ1 ⊗. . . µ′
k ⊗ak ,→e′
k ⊗µ }
where each µ′
i represents the newly created task(s) of the local transition on task ai ,→ei, and each
e′
i is the expression resulting from the transition on that task. Next, all possible synchronizations
are made by replacing each occurrence of an adjacent triple of the form
ai,1 ,→e1 ⊗ai,2 ,→e2 ⊗ai ,→join[ ai,1 ; ai,2 ]( x1 ; x2 . e )
(with e1 and e2 finished) by the task ai ,→[e1, e2/x1, x2]e. Doing so propagates the values of tasks
ai,1 and ai,2 to the join point, enabling the computation to continue. The two finished tasks are
removed from the state, and the join point is no longer blocked.
37.6
Notes
Parallelism is a high-level programming concept that increases efficiency by carrying out multiple
computations simultaneously when they are mutually independent. Parallelism does not change
4Thus the local transition given by Rule (37.11b) is never applicable; the dynamics of joins will be described shortly.

PREVIEW
37.6 Notes
361
the meaning of a program, but only how fast it is executed. The cost dynamics specifies the num-
ber of steps required to execute a program sequentially and with maximal parallelism. A bounded
implementation provides a bound on the number of steps when the number of processors is lim-
ited, limiting the degree of parallelism that can be realized. This formulation of parallelism was
introduced by Blelloch (1990). The concept of a cost dynamics and the idea of a bounded imple-
mentation studied here are derived from Blelloch and Greiner (1995, 1996).
Exercises
37.1. Consider extending PPCF with exceptions, as described in Chapter 29, under the assump-
tion that exn has at least two exception values. Give a sequential and a parallel structural
dynamics to parallel let in such a way that determinacy continues to hold.
37.2. Give a matching cost dynamics to PPCF extended with exceptions (descibed in Exercise 37.1)
by inductively defining the following two judgments:
(a) e ⇓c v, stating that e evaluates to value v with cost c;
(b) e ⇑c v, stating that e raises the value v with cost c.
The analog of Theorem 37.6 remains valid for the dynamics. In particular, if e ⇑c v, then
both e 7−−→
seq
w raise( v ), where w = wk(c), and e 7−−→
par
d raise( v ), where d = dp(c), and
conversely.
37.3. Extend the P machine to admit exceptions to match your solution to Exercise 37.2. Argue
that the revised machine supports a Brent-type validation of the cost dynamics.
37.4. Another way to express the dynamics of PPCF enriched with exceptions is by rewriting
par( e1 ; e2 ; x1 . x2 . e ) into another such parallel binding, par( e′
1 ; e′
2 ; x′
1 . x′
2 . e′ ), which im-
plements the correct dynamics to ensure determinacy. Hint: Extend XPCF with sums (Chap-
ter 11), using them to record the outcome of each parallel sub-computation (e′
1 derived from
e1, and e′
2 derived from e2), then check the outcomes (e′ derived from e) in such a way to
ensure determinacy.

PREVIEW
362
37.6 Notes

PREVIEW
Chapter 38
Futures and Speculations
A future is a computation that is performed before it is value is needed. Like a suspension (dis-
cussed in Chapter 36), a future represents a value that is to be determined later. Unlike a sus-
pension, a future is always evaluated, regardless of whether its value is required. In a sequential
setting futures are of little interest; a future of type τ is just an expression of type τ. In a paral-
lel setting, however, futures are of interest because they provide a means of initiating a parallel
computation whose result is not needed until later, by which time it will have been completed.
The prototypical example of the use of futures is to implementing pipelining, a method for over-
lapping the stages of a multistage computation to the fullest extent possible. Pipelining minimizes
the latency caused by one stage waiting for a previous stage to complete by allowing the two
stages to execute in parallel until an explicit dependency arises. Ideally, the computation of the
result of an earlier stage is finished by the time a later stage needs it. At worst the later stage is
delayed until the earlier stage completes, incurring what is known as a pipeline stall.
A speculation is a delayed computation whose result might be needed for the overall compu-
tation to finish. The dynamics for speculations executes suspended computations in parallel with
the main thread of computation, without regard to whether the value of the speculation is needed
by the main thread. If the value of the speculation is needed, then such a dynamics pays off, but if
not, the effort to compute it is wasted.
Futures are work efficient in that the overall work done by a computation involving futures is no
more than the work done by a sequential execution. Speculations, in contrast, are work inefficient
in that speculative execution might be in vain—the overall computation may involve more steps
than the work needed to compute the result. For this reason speculation is a risky strategy for
exploiting parallelism. It can make use available resources, but perhaps only at the expense of
doing more work than necessary!

PREVIEW
364
38.1 Futures
38.1
Futures
The syntax of futures is given by the following grammar:
Typ
τ
::=
fut( τ )
τ fut
future
Exp
e
::=
fut( e )
fut( e )
future
fsyn( e )
fsyn( e )
synchronize
fcell[ a ]
fcell[ a ]
indirection
The type τ fut is the type of futures of type τ. Futures are introduced by the expression fut( e ),
which schedules e for evaluation and returns a reference to it. Futures are eliminated by the ex-
pression fsyn( e ), which synchronizes with the future referred to by e, returning its value. Indirect
references to future values are represented by fcell[ a ], indicating a future value to be stored at a.
38.1.1
Statics
The statics of futures is given by the following rules:
Γ ⊢e : τ
Γ ⊢fut( e ) : fut( τ )
(38.1a)
Γ ⊢e : fut( τ )
Γ ⊢fsyn( e ) : τ
(38.1b)
These rules are unsurprising, because futures add no new capabilities to the language beyond
providing an opportunity for parallel evaluation.
38.1.2
Sequential Dynamics
The sequential dynamics of futures is easily defined. Futures are evaluated eagerly; synchroniza-
tion returns the value of the future.
e val
fut( e ) val
(38.2a)
e 7−→e′
fut( e ) 7−→fut( e′ )
(38.2b)
e 7−→e′
fsyn( e ) 7−→fsyn( e′ )
(38.2c)
e val
fsyn( fut( e ) ) 7−→e
(38.2d)
Under a sequential dynamics futures have little purpose: they introduce a pointless level of
indirection.

PREVIEW
38.2 Speculations
365
38.2
Speculations
The syntax of (non-recursive) speculations is given by the following grammar:1
Typ
τ
::=
spec( τ )
τ spec
speculation
Exp
e
::=
spec( e )
spec( e )
speculate
ssyn( e )
ssyn( e )
synchronize
scell[ a ]
scell[ a ]
indirection
The type τ spec is the type of speculations of type τ. The introduction form spec( e ) creates a
computation that can be speculatively evaluated, and the elimination form ssyn( e ) synchronizes
with a speculation. A reference to the result of a speculative computation stored at a is written
scell[ a ].
38.2.1
Statics
The statics of speculations is given by the following rules:
Γ ⊢e : τ
Γ ⊢spec( e ) : spec( τ )
(38.3a)
Γ ⊢e : spec( τ )
Γ ⊢ssyn( e ) : τ
(38.3b)
Thus, the statics for speculations as given by rules (38.3) is equivalent to the statics for futures
given by rules (38.1).
38.2.2
Sequential Dynamics
The definition of the sequential dynamics of speculations is like that of futures, except that specu-
lations are values.
spec( e ) val
(38.4a)
e 7−→e′
ssyn( e ) 7−→ssyn( e′ )
(38.4b)
ssyn( spec( e ) ) 7−→e
(38.4c)
Under a sequential dynamics speculations are simply a re-formulation of suspensions.
1We confine ourselves to the non-recursive case to ease the comparison with futures.

PREVIEW
366
38.3 Parallel Dynamics
38.3
Parallel Dynamics
Futures and speculations are only interesting insofar as they admit a parallel dynamics that allows
the computation of the future to go ahead concurrently with some other computation. In this
section we give a parallel dynamics of futures and speculation in which the creation, execution,
and synchronization of tasks is made explicit. The parallel dynamics of futures and speculations is
identical, except for the termination condition. Whereas futures require that all tasks are completed
before termination, speculations may be abandoned before they are completed. For the sake of
concision we will give the parallel dynamics of futures, remarking only where alterations are made
for the parallel dynamics of speculations.
The parallel dynamics of futures relies on a modest extension to the language given in Sec-
tion 38.1 to introduce names for tasks. Let Σ be a finite mapping assigning types to names. As
mentioned earlier, the expression fcell[ a ] is a value referring to the outcome of task a. The stat-
ics of this expression is given by the following rule:2
Γ ⊢Σ,a~τ fcell[ a ] : fut( τ )
(38.5)
Rules (38.1) carry over in the obvious way with Σ recording the types of the task names.
States of the parallel dynamics have the form ν Σ { e ∥µ }, where e is the focus of evaluation,
and µ records the active parallel futures (or speculations). Formally, µ is a finite mapping assigning
expressions to the task names declared in Σ. A state is well-formed according to the following rule:
⊢Σ e : τ
(∀a ∈dom(Σ)) ⊢Σ µ(a) : Σ(a)
ν Σ { e ∥µ } ok
(38.6)
As discussed in Chapter 35 this rule admits self-referential and mutually referential futures. A
more refined condition could as well be given that avoids circularities; we leave this as an exercise
for the reader.
The parallel dynamics is divided into two phases, the local phase, which defines the basic steps
of evaluation of an expression, and the global phase, which executes all possible local steps in
parallel. The local dynamics of futures is defined by the following rules:3
fcell[ a ] valΣ,a~τ
(38.7a)
ν Σ { fut( e ) ∥µ } 7−→
loc ν Σ, a ~ τ { fcell[ a ] ∥µ ⊗a ,→e }
(38.7b)
ν Σ { e ∥µ } 7−→
loc ν Σ′ { e′ ∥µ′ }
ν Σ { fsyn( e ) ∥µ } 7−→
loc ν Σ′ { fsyn( e′ ) ∥µ′ }
(38.7c)
2A similar rule applies to scell[ a ] in the case of speculations.
3These rules are augmented by a reformulation of the dynamics of the other constructs of the language phrased in terms
of the present notion of state.

PREVIEW
38.3 Parallel Dynamics
367
e′ valΣ,a~τ







ν Σ, a ~ τ { fsyn( fcell[ a ] ) ∥µ ⊗a ,→e′ }
7−→
loc
ν Σ, a ~ τ { e′ ∥µ ⊗a ,→e′ }







(38.7d)
Rule (38.7b) activates a future named a executing the expression e and returns a reference to it.
Rule (38.7d) synchronizes with a future whose value has been determined. Note that a local tran-
sition always has the form
ν Σ { e ∥µ } 7−→
loc ν Σ Σ′ { e′ ∥µ ⊗µ′ }
where Σ′ is either empty or declares the type of a single symbol, and µ′ is either empty or of the
form a ,→e′ for some expression e′.
A global step of the parallel dynamics consists of at most one local step for the focal expression
and one local step for each of up to p futures, where p > 0 is a fixed parameter representing the
number of processors.
µ = µ0 ⊗a1 ,→e1 ⊗. . . ⊗an ,→en
µ′′ = µ0 ⊗a1 ,→e′
1 ⊗. . . ⊗an ,→e′
n
ν Σ { e ∥µ } 7−→
loc
? ν Σ Σ′ { e′ ∥µ ⊗µ′ }
(∀1 ≤i ≤n ≤p)
ν Σ { ei ∥µ } 7−→
loc ν Σ Σ′
i { e′
i ∥µ ⊗µ′
i }







ν Σ { e ∥µ }
7−→
glo
ν Σ Σ′ Σ′
1 . . . Σ′
n { e′ ∥µ′′ ⊗µ′ ⊗µ′
1 ⊗. . . ⊗µ′
n }







(38.8a)
Rule (38.8a) allows the focus expression to take either zero or one steps because it might be blocked
awaiting the completion of evaluation of a parallel future (or synchronizing with a speculation).
The futures allocated by the local steps of execution are consolidated in the result of the global
step. We assume without loss of generality that the names of the new futures in each local step
are pairwise disjoint so that the combination makes sense. In implementation terms satisfying this
disjointness assumption means that the processors must synchronize their access to memory.
The initial state of a computation, for futures or speculations, is defined by the rule
ν ∅{ e ∥∅} initial
(38.9)
For futures a state is final only if the focus and all parallel futures have completed evaluation:
e valΣ
µ valΣ
ν Σ { e ∥µ } final
(38.10a)

PREVIEW
368
38.4 Pipelining With Futures
(∀a ∈dom(Σ)) µ(a) valΣ
µ valΣ
(38.10b)
For speculations a state is final only if the focus is a value, regardless of whether any other specu-
lations have completed:
e valΣ
ν Σ { e ∥µ } final
(38.11)
All futures must terminate to ensure that the work performed in parallel matches that performed
sequentially; no future is created whose value is not needed according to the sequential dynamics.
In contrast, speculations can be abandoned when their values are not needed.
38.4
Pipelining With Futures
Pipelining is an interesting example of the use of parallel futures. Consider a situation in which a
producer builds a list whose elements represent units of work, and a consumer traverses the work
list and acts on each element of that list. The elements of the work list can be thought of as “in-
structions” to the consumer, which maps a function over that list to carry out those instructions.
An obvious sequential implementation first builds the work list, then traverses it to perform the
work indicated by the list. This strategy works well provided that the elements of the list can be
produced quickly, but if each element needs a lot of computation, it would be preferable to over-
lap production of the next list element with execution of the previous unit of work, which can be
programmed using futures.
Let flist be the recursive type rec t is unit + ( nat × t fut ), whose elements are nil, defined
to be fold( l · ⟨⟩), and cons( e1,e2 ), defined to be fold( r · ⟨e1, fut( e2 )⟩). The producer is a recur-
sive function that generates a value of type flist:
fix produce : (nat →nat opt) →nat →flist is
λ f. λ i.
case f(i) {
null ,→nil
| just x ,→cons(x, fut (produce f (i+1)))
}
On each iteration the producer generates a parallel future to produce the tail. The future continues
to execute after the producer returns so that its evaluation overlaps with subsequent computation.
The consumer folds an operation over the work list as follows:
fix consume : ((nat×nat)→nat) →nat →flist →nat is
λ g. λ a. λ xs.
case xs {
nil ,→a
| cons (x, xs) ,→consume g (g (x, a)) (fsyn xs)
}

PREVIEW
38.5 Notes
369
The consumer synchronizes with the tail of the work list just at the point where it makes a recursive
call and hence needs the head element of the tail to continue processing. At this point the consumer
will block, if necessary, to await computation of the tail before continuing the recursion.
Speculations arise naturally in lazy languages. But although they provide opportunities for
parallelism, they are not, in general, work efficient: a speculation might be evaluated even though
its value is never needed. An alternative is to combine suspensions (see Chapter 36) with futures
so that the programmer may specify which suspensions ought to be evaluated in parallel. The
notion of a spark is designed to achieve this. A spark evaluates a computation in parallel only for
its effect on suspensions that are likely to be needed later. Specifically, we may define
spark( e1 ; e2 ) ≜letfut be force( e1 ) in e2,
where e1 : τ1 susp and e2 : τ2.4 The expression force( e1 ) is evaluated in parallel, forcing the
evaluation of e1, in hopes that it will have completed evaluation before its value is needed by e2.
As an example, consider the type strm of streams of numbers defined by the recursive type
rec t is ( unit + ( nat × t ) ) susp. An element of this type is a suspended computation that, when
forced, either signals the end of stream, or produces a number and another such stream. Suppose
that s is such a stream, and assume that we know, for reasons of its construction, that it is finite.
We wish to compute map( f )( s ) for some function f, and to overlap this computation with the
production of the stream elements. We will make use of a function mapforce that forces successive
elements of the input stream, but yields no useful output. Specifically, the computation
mapforce( f )( s ) ≜letfut be map( force )( s ) in map( f )( s )
forces the elements of the stream in parallel with the computation of map( f )( s ), with the intention
that all suspensions in s are forced before their values are needed by the main computation.
38.5
Notes
Futures were introduced by Friedman and Wise (1976), and featured in the MultiLisp language
(Halstead, 1985) for parallel programming. A similar concept is proposed by Arvind et al. (1986)
under the name “I-structures.” The formulation given here is derived from Greiner and Blelloch
(1999). Sparks were introduced by Trinder et al. (1998).
Exercises
38.1. Use futures to define letfut x be e1 in e2, a parallel let in which e2 is evaluated in parallel
with e1 up to the point that e2 needs the value of x.
38.2. Use futures to encode binary nested parallelism by giving a definition of par( e1 ; e2 ; x1 .
x2 . e ). Hint: Only one future is needed if you are careful.
4The expression evaluates e1 simultaneously with e2, up to the point that the value of x is needed. Its definition in terms
of futures is the subject of Exercise 38.1.

PREVIEW
370
38.5 Notes

PREVIEW
Part XVI
Concurrency and Distribution

PREVIEW

PREVIEW
Chapter 39
Process Calculus
So far we have studied the statics and dynamics of programs in isolation, without regard to their
interaction with each other or the world. But to extend this analysis to even the most rudimentary
forms of input and output requires that we consider external agents that interact with the program.
After all, the purpose of a computer is, ultimately, to interact with a person!
To extend our investigations to interactive systems, we develop a small language, called PiC,
which is derived from a variety of similar formalisms, called process calculi, that give an abstract
formulation of interaction among independent agents. The development will be carried out in
stages, starting with simple action models, then extending to interacting concurrent processes,
and finally to synchronous and asynchronous communication. The calculus consists of two main
syntactic categories, processes and events. The basic form of process is one that awaits the arrival
of an event. Processes are formed by concurrent composition, replication, and declaration of a
channel. The basic forms of event are signaling on a channel and querying a channel; these are
later generalized to sending and receiving data on a channel. Events are formed from send and
receive events by finite non-deterministic choice.
39.1
Actions and Events
Concurrent interaction is based on events, which specify the actions that a process can take. Two
processes interact by taking two complementary actions, a signal and a query on a channel. The
processes synchronize when one signals on a channel that the other is querying, after which they
continue to interact with other processes.
To begin with we will focus on sequential processes, which simply await the arrival of one of
several possible actions, known as an event.
Proc
P
::=
await( E )
$ E
synchronize
Evt
E
::=
null
0
null
or( E1 ; E2 )
E1 + E2
choice
que[ a ]( P )
?a ; P
query
sig[ a ]( P )
!a ; P
signal

PREVIEW
374
39.1 Actions and Events
The variable a ranges over symbols serving as channels that mediate communication among the
processes.
We will not distinguish between events that differ only up to structural congruence, which is
defined to be the strongest equivalence relation closed under these rules:
E ≡E′
$ E ≡$ E′
(39.1a)
E1 ≡E′
1
E2 ≡E′
2
E1 + E2 ≡E′
1 + E′
2
(39.1b)
P ≡P′
?a ; P ≡?a ; P′
(39.1c)
P ≡P′
!a ; P ≡!a ; P′
(39.1d)
E + 0 ≡E
(39.1e)
E1 + E2 ≡E2 + E1
(39.1f)
E1 + ( E2 + E3 ) ≡( E1 + E2 ) + E3
(39.1g)
Imposing structural congruence on sequential processes enables us to think of an event as having
the form
!a ; P1 + . . . + ?a ; Q1 + . . .
consisting of a sum of signal and query events, with the sum of no events being the null event 0.
An illustrative example of Milner’s is a simple vending machine that may take in a 2p coin,
then optionally either allow a request for a cup of tea, or take another 2p coin, then allow a request
for a cup of coffee.
V = $ ( ?2p ; $ ( !tea ; V + ?2p ; $ ( !cof ; V ) ) )
(39.2)
As the example indicates, we allow recursive definitions of processes, with the understanding that
a defined identifier may always be replaced with its definition wherever it occurs. (Later we will
show how to avoid reliance on recursive definitions.)
Because the computation occurring within a process is suppressed, sequential processes have
no dynamics on their own, but only through their interaction with other processes. For the vend-
ing machine to work there must be another process (you) who initiates the events expected by
the machine, causing both your state (the coins in your pocket) and its state (as just described) to
change as a result.

PREVIEW
39.2 Interaction
375
39.2
Interaction
Processes become interesting when they are allowed to interact with one another to achieve a
common goal. To account for interaction we enrich the language of processes with concurrent
composition:
Proc
P
::=
await( E )
$ E
synchronize
stop
1
inert
conc( P1 ; P2 )
P1 ⊗P2
composition
The process 1 represents the inert process, and the process P1 ⊗P2 represents the concurrent com-
position of P1 and P2. We may identify 1 with $ 0, the process that awaits the event that will never
occur, but we prefer to treat the inert process as a primitive concept.
We will identify processes up to structural congruence, the strongest equivalence relation closed
under these rules:
P ⊗1 ≡P
(39.3a)
P1 ⊗P2 ≡P2 ⊗P1
(39.3b)
P1 ⊗( P2 ⊗P3 ) ≡( P1 ⊗P2 ) ⊗P3
(39.3c)
P1 ≡P′
1
P2 ≡P′
2
P1 ⊗P2 ≡P′
1 ⊗P′
2
(39.3d)
Up to structural congruence every process has the form
$ E1 ⊗. . . ⊗$ En
for some n ≥0, it being understood that when n = 0 this stands for the null process 1.
Interaction between processes consists of synchronization of two complementary actions. The
dynamics of interaction is defined by two forms of judgment. The transition judgment P 7−→P′
states that the process P evolves to the process P′ as a result of a single step of computation. The
family of transition judgments, P
α7−→P′, where α is an action, states that the process P may evolve
to the process P′ as long as the action α is permissible in the context in which the transition occurs.
As a notational convenience, we often regard the unlabeled transition to be the labeled transition
corresponding to the special silent action.
The possible actions are given by the following grammar:
Act
α
::=
que[ a ]
a ?
query
sig[ a ]
a !
signal
ε
ε
silent
The query action a ? and the signal action a ! are complementary, and the silent action ε, is self-
complementary. We define the complementary action to α to be the action α given by the equations
a ? = a !, a ! = a ?, and ε = ε.

PREVIEW
376
39.2 Interaction
$ (!a ; P + E)
a !
7−→P
(39.4a)
$ (?a ; P + E)
a ?
7−→P
(39.4b)
P1
α7−→P′
1
P1 ⊗P2
α7−→P′
1 ⊗P2
(39.4c)
P1
α7−→P′
1
P2
α7−→P′
2
P1 ⊗P2 7−→P′
1 ⊗P′
2
(39.4d)
Rules (39.4a) and (39.4b) specify that any of the events on which a process is synchronizing
may occur. Rule (39.4d) synchronizes two processes that take complementary actions.
As an example, let us consider the vending machine V, given by equation (39.2), interacting
with the user process U defined as follows:
U = $ !2p ; $ !2p ; $ ?cof ; 1.
Here is a trace of the interaction between V and U:
V ⊗U 7−→$ ( !tea ; V + ?2p ; $ !cof ; V ) ⊗$ !2p ; $ ?cof ; 1
7−→$ !cof ; V ⊗$ ?cof ; 1
7−→V
These steps are justified by the following pairs of labeled transitions:
U
2p !
7−−→U′ = $ !2p ; $ ?cof ; 1
V
2p ?
7−−→V′ = $ ( !tea ; V + ?2p ; $ !cof ; V )
U′
2p !
7−−→U′′ = $ ?cof ; 1
V′
2p ?
7−−→V′′ = $ !cof ; V
U′′
cof ?
7−−−→1
V′′
cof !
7−−−→V
We have suppressed uses of structural congruence in the foregoing derivation to avoid clutter, but
it is important to see its role in managing the non-deterministic choice of events by a process.

PREVIEW
39.3 Replication
377
39.3
Replication
Some presentations of process calculi forego reliance on defining equations for processes in favor
of a replication construct, which we write as ∗P. This process stands for as many concurrently
executing copies of P as needed. Implicit replication can be expressed by the structural congruence
∗P ≡P ⊗∗P.
(39.5)
Understood as a principle of structural congruence, this rule hides the steps of process creation,
and gives no hint as to how often it should be applied. We could alternatively build replication
into the dynamics to model the details of replication more closely:
∗P 7−→P ⊗∗P.
(39.6)
Because there is no constraint on the use of this rule, it can at any time create a new copy of the
replicated process P. It is also possible to tie its use to send and receive events so that replication
is causal, rather than spontaneous.
So far we have used recursive process definitions to define processes that interact repeatedly
according to some protocol. Rather than take recursive definition as a primitive notion, we may
instead use replication to model repetition. We do so by introducing an “activator” process that
is used to cause the replication. Consider the recursive definition X = P(X), where P is a process
expression that may refer to itself as X. Such a self-referential process can be simulated by defining
the activator process
A = ∗$ ( ?a ; P($ ( !a ; 1 )) ),
in which we have replaced occurrences of X within P by an initiator process that signals the event
a to the activator. Note that the activator A is structurally congruent to the process A′ ⊗A, where
A′ is the process
$ ( ?a ; P($ ( !a ; 1 )) ).
To start process P we concurrently compose the activator A with an initiator process, $ ( !a ; 1 ).
Note that
A ⊗$ ( !a ; 1 ) 7−→A ⊗P($ !a ; 1),
which starts the process P while maintaining a running copy of the activator, A.
As an example, let us consider Milner’s vending machine written using replication, instead of
recursive process definition:
V0 = $ ( !v ; 1 )
(39.7)
V1 = ∗$ ( ?v ; V2 )
(39.8)
V2 = $ ( ?2p ; $ ( !tea ; V0 + ?2p ; $ ( !cof ; V0 ) ) )
(39.9)
The process V1 is a replicated server that awaits a signal on channel v to create another instance of
the vending machine. The recursive calls are replaced by signals along v to re-start the machine.
The original machine V is simulated by the concurrent composition V0 ⊗V1.

PREVIEW
378
39.4 Allocating Channels
This example motivates replacing spontaneous replication by replicated synchronization, which
is defined by the following rules:
∗$ (!a ; P + E)
a !
7−→P ⊗∗$ (!a ; P + E)
(39.10a)
∗$ (?a ; P + E)
a ?
7−→P ⊗∗$ (?a ; P + E)
(39.10b)
The process ∗$ (E) is to be regarded not as a composition of replication and synchronization, but as
the inseparable combination of these two constructs. The advantage is that the replication occurs
only as needed, precisely when a synchronization with another process is possible, avoiding the
need “guess” when replication is needed.
39.4
Allocating Channels
It is often useful (particularly once we have introduced inter-process communication) to introduce
new channels within a process, and not assume that all channels of interaction are given a priori.
To allow for this, we enrich the syntax of processes with channel declaration:
Proc
P
::=
newch( a . P )
ν a . P
new channel
The channel a is bound within the process P. To simplify notation we sometimes write ν a1, . . . , ak . P
for the iterated declaration ν a1 . . . . ν ak . P.
We then extend structural congruence with the following rules:
P =α P′
P ≡P′
(39.11a)
P ≡P′
ν a . P ≡ν a . P′
(39.11b)
a /∈P2
( ν a . P1 ) ⊗P2 ≡ν a . ( P1 ⊗P2 )
(39.11c)
ν a . ν b . P ≡ν b . ν a . P
(39.11d)
(a /∈P)
ν a . P ≡P
(39.11e)
Rule (39.11c), called scope extrusion, will be especially important in Section 39.6. Rule (39.11e) states
that channels are de-allocated once they are no longer in use.

PREVIEW
39.4 Allocating Channels
379
To account for the scopes of channels we extend the statics of PiC with a signature Σ comprising
a finite set of active channels. The judgment ⊢Σ P proc states that a process P is well-formed
relative to the channels declared in the signature Σ.
⊢Σ 1 proc
(39.12a)
⊢Σ P1 proc
⊢Σ P2 proc
⊢Σ P1 ⊗P2 proc
(39.12b)
⊢Σ E event
⊢Σ $ E proc
(39.12c)
⊢Σ,a P proc
⊢Σ ν a . P proc
(39.12d)
The foregoing rules make use of an auxiliary judgment, ⊢Σ E event, stating that E is a well-formed
event relative to Σ.
⊢Σ 0 event
(39.13a)
⊢Σ,a P proc
⊢Σ,a ?a ; P event
(39.13b)
⊢Σ,a P proc
⊢Σ,a !a ; P event
(39.13c)
⊢Σ E1 event
⊢Σ E2 event
⊢Σ E1 + E2 event
(39.13d)
The judgment ⊢Σ α action states that α is a well-formed action relative to Σ:
⊢Σ,a a ? action
(39.14a)
⊢Σ,a a ! action
(39.14b)
⊢Σ ε action
(39.14c)
The dynamics of the current fragment of PiC is correspondingly generalized to keep track of
the set of active channels. The judgment P
α7−→
Σ
P′ states that P transitions to P′ with action α
relative to channels Σ. The dynamics of this extension is obtained by indexing the transitions by
the signature, and adding a rule for channel declaration.
$ (!a ; P + E)
a !
7−−→
Σ,a
P
(39.15a)

PREVIEW
380
39.5 Communication
$ (?a ; P + E)
a ?
7−−→
Σ,a
P
(39.15b)
P1
α7−→
Σ
P′
1
P1 ⊗P2
α7−→
Σ
P′
1 ⊗P2
(39.15c)
P1
α7−→
Σ
P′
1
P2
α7−→
Σ
P′
2
P1 ⊗P2 7−→
Σ
P′
1 ⊗P′
2
(39.15d)
P
α
7−−→
Σ,a
P′
⊢Σ α action
ν a . P
α7−→
Σ ν a . P′
(39.15e)
Rule (39.15e) ensures that no process may interact with ν a . P along the channel a by using the
identification convention to choose a /∈Σ.
Consider again the definition of the vending machine using replication instead of recursion.
The channel v used to initialize the machine is private to the machine itself. The process V =
ν v . ( V0 ⊗V1 ) declares a new channel v for use by V0 and V1, which are defined essentially as
before. The interaction of the user process U with V begins as follows:
( ν v . ( V0 ⊗V1 ) ) ⊗U 7−→
Σ
( ν v . V1 ⊗V2 ) ⊗U ≡ν v . ( V1 ⊗V2 ⊗U ).
The interaction continues within the scope of the declaration, which ensures that v does not occur
within U.
39.5
Communication
Synchronization coordinates the execution of two processes that take the complementary actions of
signaling and querying a common channel. Synchronous communication generalizes synchroniza-
tion to pass a data value betwen two synchronizing processes, one of which is the sender of the
value and the other its receiver. The type of the data is immaterial to the communication.
To account for interprocess communication we enrich the language of processes to include
variables, as well as channels, in the formalism. Variables range over types, and are given meaning
by substitution. Channels, on the other hand, are assigned types that classify the data carried on
that channel, and are given meaning by send and receive events that generalize the signal and
query events considered in Section 39.2. The abstract syntax of communication events is given by
the following grammar:
Evt
E
::=
snd[ a ]( e ; P )
! a( e ; P )
send
rcv[ a ]( x . P )
? a( x . P )
receive

PREVIEW
39.5 Communication
381
The event rcv[ a ]( x . P ) represents the receipt of a value x on the channel a, passing x to the
process P. The variable x is bound within P. The event snd[ a ]( e ; P ) represents the transmission
of e on a and continuing with P.
We modify the syntax of declarations to account for the type of value sent on a channel.
Proc
P
::=
newch{τ}( a . P )
ν a ~ τ . P
typed channel
The process newch{τ}( a . P ) introduces a new channel a with associated type τ for use within the
process P. The channel a is bound within P.
The statics is extended to account for the type of a channel. The judgment Γ ⊢Σ P proc states
that P is a well-formed process involving the channels declared in Σ and the variables declared in
Γ. It is inductively defined by the following rules, wherein we assume that the typing judgment
Γ ⊢Σ e : τ is given separately.
Γ ⊢Σ 1 proc
(39.16a)
Γ ⊢Σ P1 proc
Γ ⊢Σ P2 proc
Γ ⊢Σ P1 ⊗P2 proc
(39.16b)
Γ ⊢Σ,a~τ P proc
Γ ⊢Σ ν a ~ τ . P proc
(39.16c)
Γ ⊢Σ E event
Γ ⊢Σ $ E proc
(39.16d)
Rules (39.16) make use of the auxiliary judgment Γ ⊢Σ E event, stating that E is a well-formed
event relative to Γ and Σ, which is defined as follows:
Γ ⊢Σ 0 event
(39.17a)
Γ ⊢Σ E1 event
Γ ⊢Σ E2 event
Γ ⊢Σ E1 + E2 event
(39.17b)
Γ, x : τ ⊢Σ,a~τ P proc
Γ ⊢Σ,a~τ ? a( x . P ) event
(39.17c)
Γ ⊢Σ,a~τ e : τ
Γ ⊢Σ,a~τ P proc
Γ ⊢Σ,a~τ ! a( e ; P ) event
(39.17d)
Rule (39.17d) makes use of a typing judgment for expressions that ensures that the type of a chan-
nel is respected by communication.
The dynamics of communication extends that of synchronization by enriching send and receive
actions with the value sent or received.
Act
α
::=
rcv[ a ]( e )
a ? e
receive
snd[ a ]( e )
a ! e
send
ε
ε
silent

PREVIEW
382
39.5 Communication
Complementarity is defined as before, by switching the orientation of an action: a ? e = a ! e,
a ! e = a ? e, and ε = ε.
The statics ensures that the expression associated with these actions is a value of a type suitable
for the channel:
⊢Σ,a~τ e : τ
e valΣ,a~τ
⊢Σ,a~τ a ! e action
(39.18a)
⊢Σ,a~τ e : τ
e valΣ,a~τ
⊢Σ,a~τ a ? e action
(39.18b)
⊢Σ ε action
(39.18c)
The dynamics is defined by replacing the synchronization rules (39.15a) and (39.15b) with the
following communication rules:
e 7−−−→
Σ,a~τ
e′
$ ( ! a( e ; P ) + E ) 7−−−→
Σ,a~τ
$ ( ! a( e′ ; P ) + E )
(39.19a)
e valΣ,a~τ
$ ( ! a( e ; P ) + E )
a!e
7−−−→
Σ,a~τ
P
(39.19b)
e valΣ,a~τ
$ ( ? a( x . P ) + E )
a?e
7−−−→
Σ,a~τ [e/x]P
(39.19c)
Rule (39.19c) is non-deterministic in that it “guesses” the value e to be received along channel a.
Rules (39.19) make reference to the dynamics of expressions, which is left unspecified because
nothing depends on it.
Using synchronous communication, both the sender and the receiver of a message are blocked
until the interaction is completed. Therefore the sender must be notified whenever a message is
received, which means that there must be an implicit reply channel from receiver to sender that
carries the notification. This means that synchronous communication can be decomposed into a
simpler asynchronous send operation, which sends a message on a channel without waiting for its
receipt, together with channel passing to send an acknowledgment channel along with the message
data.
Asynchronous communication is defined by removing the synchronous send event from the pro-
cess calculus, and adding a new form of process that simply sends a message on a channel. The
syntax of asynchronous send is as follows:
Proc
P
::=
asnd[ a ]( e )
! a( e )
send
The process asnd[ a ]( e ) sends the message e on channel a, and then terminates immediately. With-
out the synchronous send event, every event is, up to structural congruence, a choice of zero or

PREVIEW
39.6 Channel Passing
383
more read events. The statics of asynchronous send is given by the following rule:
Γ ⊢Σ,a~τ e : τ
Γ ⊢Σ,a~τ ! a( e ) proc
(39.20)
The dynamics is given similarly:
e valΣ,a~τ
! a( e )
a!e
7−−−→
Σ,a~τ 1
(39.21)
The rule for communication remains unchanged. A pending asynchronous send is essentially a
buffer holding the value to be sent once a receiver is available.
39.6
Channel Passing
An interesting case of interprocess communication arises when one process passes a channel refer-
ence, a form of value, to another along a common channel. The receiving process need not have
any direct access to the channel referred to by the reference. It merely operates on it using send
and receive operations that act on channel references instead of fixed channels. Doing so allows for
new patterns of communication to be established among processes. For example, two processes,
P and Q, may share a channel a along which they may send and receive messages. If the scope of
a is confined to these processes, then no other process R may communicate on that channel; it is,
in effect, a private channel between P and Q.
The following process expression illustrates such a situation:
( ν a ~ τ . ( P ⊗Q ) ) ⊗R.
The process R is excluded from the scope of the channel a, which however includes both P and
Q. The processes P and Q may communicate with each other on channel a, but R has no access
to this channel. If P and Q wish to allow R to communicate along a, they may do so by sending a
reference to a to R along some channel b known to all three processes. Thus we have the following
situation:
ν b ~ τ chan . ( ( ν a ~ τ . ( P ⊗Q ) ) ⊗R ).
Assuming that P initiates the inclusion of R into its communication with Q along a, it has the form
$ ( ! b( & a ; P′ ) ). The process R correspondingly takes the form $ ( ? b( x . R′ ) ). The system of
processes therefore has the form
ν b ~ τ chan . ( ν a ~ τ . ( $ ( ! b( & a ; P′ ) ) ⊗Q ) ⊗$ ( ? b( x . R′ ) ) ).
Sending a reference to a to R would seem to violate the scope of a. The communication of the
reference would seem to escape the scope of the referenced channel, which would be nonsensical.
It is here that the concept of scope extrusion, introduced in Section 39.4 comes into play:
ν b ~ τ chan . ν a ~ τ . ( $ ( ! b( & a ; P′ ) ) ⊗Q ⊗$ ( ? b( x . R′ ) ) ).

PREVIEW
384
39.6 Channel Passing
The scope of a expands to encompass R, preparing the ground for communication between P and
R, resulting in
ν b ~ τ chan . ν a ~ τ . ( P′ ⊗Q ⊗[& a/x]R′ ).
The reference to the channel a is substituted for the variable x within R′.
The process R may now communicate with P and Q by sending and receiving messages along
the channel reference substituted for the variable x. For this we use dynamic forms of send and
receive in which the channel on which to communicate is determined by evaluation of an expres-
sion. For example, to send a message e of type τ along the channel referred to by x, the process R′
would have the form
$ ( !! ( x ; e ; R′′ ) ).
Similarly, to receive along the referenced channel, the process R′ would have the form
$ ( ?? ( x ; y . R′′ ) ).
In both cases the dynamic communication forms evolve to the static communication forms once
the referenced channel has been determined.
The syntax of channel reference types is given by the following grammar:
Typ
τ
::=
chan( τ )
τ chan
channel type
Exp
e
::=
chref[ a ]
& a
reference
Evt
E
::=
sndref( e1 ; e2 ; P )
!! ( e1 ; e2 ; P )
send
rcvref( e ; x . P )
?? ( e ; x . P )
receive
The events sndref( e1 ; e2 ; P ) and rcvref( e ; x . P ) are dynamic versions of the events snd[ a ]( e ; P )
and rcv[ a ]( x . P ) in which the channel reference is determined dynamically by evaluation of an
expression.
The statics of channel references is given by the following rules:
Γ ⊢Σ,a~τ & a : τ chan
(39.22a)
Γ ⊢Σ e1 : τ chan
Γ ⊢Σ e2 : τ
Γ ⊢Σ P proc
Γ ⊢Σ !! ( e1 ; e2 ; P ) event
(39.22b)
Γ ⊢Σ e : τ chan
Γ, x : τ ⊢Σ P proc
Γ ⊢Σ ?? ( e ; x . P ) event
(39.22c)
Because channel references are forms of expression, events must be evaluated to determine the
channel to which they refer.
E 7−−−→
Σ,a~τ
E′
$ ( E ) 7−−−→
Σ,a~τ
$ ( E′ )
(39.23a)
$ ( !! ( & a ; e ; P ) + E ) 7−−−→
Σ,a~τ
$ ( ! a( e ; P ) + E )
(39.23b)

PREVIEW
39.7 Universality
385
$ ( ?? ( & a ; x . P ) + E ) 7−−−→
Σ,a~τ
$ ( ? a( x . P ) + E )
(39.23c)
Events must similarly be evaluated; see Chapter 40 for guidance on how to formulate such a
dynamics.
39.7
Universality
The process calculus PiC developed in this chapter is universal in the sense that the untyped λ-
calculus can be encoded within it. Consequently, via this encoding, the same functions on the
natural numbers are definable in PiC as are definable in Λ and hence, by Church’s Law, any
known programming language. This claim is remarkable because PiC has so few capabilities that
one might suspect that it is too weak to be a useful programming language. The key to seeing that
PiC is universal is to note that communication allows processes to send and receive values of an
arbitrary type. So long as recursive and channel reference types are available, then it is a purely
technical matter to show that Λ is encodable within it. After all, what makes Λ universal is that
its one type is a recursive type (see Chapter 21), so it is natural to guess that with messages of
recursive type available then PiC would be universal. And indeed it is.
To prove universality it suffices to give an encoding of the untyped λ-calculus under a call-
by-name dynamics into PiC. To motivate the translation, consider a call-by-name stack machine
for evaluating λ-terms. A stack is a composition of frames, each of which have the form −( e2 )
corresponding to the evaluation of the function part of an application. A stack is represented in PiC
by a reference to a channel that expects an expression (the function to apply) and another channel
reference (the stack on which to evaluate the result of the application). A λ-term is represented by
a reference to a channel that expects a stack on which the expression is evaluated.
Let κ be the type of continuations. It should be isomorphic to the type of references to channels
that carry a pair of values, an argument, whose type is a reference to a channel carrying a contin-
uation, and another continuation to which to deliver the result of the application. Thus we seek to
have the following type isomorphism:
κ ∼= ( κ chan × κ ) chan.
The solution is a recursive type, as described in Chapter 20. Thus, just as for Λ itself, the key to
the universality of PiC is the use of the recursive type κ.
We now give the translation of Λ into PiC. For the sake of the induction the translation of a
Λ expression u is given relative to a variable of type κ, representing the continuation to which the
result will be sent. The representation is given by the following equations:
x @ k ≜!! ( x ; k )
λ ( x ) u @ k ≜$ ?? ( unfold( k ) ; ⟨x, k′⟩. u @ k′ )
u1( u2 ) @ k ≜
ν a1 ~ κ chan × κ . ( u1 @ fold( & a1 ) ) ⊗ν a ~ κ . ∗$ ? a( k2 . u2 @ k2 ) ⊗! a1( ⟨& a, k⟩)

PREVIEW
386
39.8 Notes
We use pattern matching on pairs for the sake of readability. Only asynchronous sends are needed.
The use of static and dynamic communication operations in the translation merits close con-
sideration. The call site of a λ-term is determined dynamically; we cannot predict at translation
time the continuation of the term. In particular, the binding of a variable can be used at several
call sites, corresponding to uses of that variable. On the other hand, the channel associated to
an argument is determined statically. The server associated to the variable listens on a statically
determined channel for a continuation, which is determined dynamically.
As a check on the correctness of the representation, consider the following derivation:
( λ ( x ) x )( y ) @ k 7−→∗
ν a1 ~ τ . ( $ ? a1( ⟨x, k′⟩. !! ( x ; k′ ) ) ) ⊗ν a ~ κ . ∗$ ? a( k2 . !! ( y ; k2 ) ) ⊗! a1( ⟨& a, k⟩)
7−→∗ν a ~ κ . ∗$ ? a( k2 . !! ( y ; k2 ) ) ⊗! a( k )
7−→∗ν a ~ κ . ∗$ ? a( k2 . !! ( y ; k2 ) ) ⊗!! ( y ; k )
Apart from the idle server process listening on channel a, this is just the translation y @ k.
39.8
Notes
Process calculi as models of concurrency and interaction were introduced and extensively devel-
oped by Hoare (1978) and Milner (1999). Milner’s original formulation, CCS, was introduced to
model pure synchronization, whereas Hoare’s, CSP, included value-passing. CCS was extended
to become the π-calculus (Milner, 1999), which includes channel-passing. Dozens upon dozens of
variations and extensions of CSP, CCS, and the π-calculus have been considered in the literature,
and continue to be a subject of intensive study. (See Engberg and Nielsen (2000) for an account of
some of the critical developments in the area.)
The process calculus considered here is derived from the π-calculus as presented in Milner
(1999). The overall line of development, and the vending machine example and the λ-calculus en-
coding, are adapted from Milner (1999). The distinction drawn here between static and dynamic
events (that is, those that are given syntactically versus those that arise by evaluation) flows nat-
urally from the distinction between variables and channels. It is possible to formulate PiC using
only channel references, suppressing any mention of channels themselves. The present formula-
tion coheres with the formulation of assignables and assignable references in Chapters 34 and 35.
The concept of dynamic events is taken one step further in Concurrent ML (Reppy, 1999), wherein
events are values of an event type (see also Chapter 40).
Exercises
39.1. Booleans can be represented in the process calculus similarly to the way in they are repre-
sented in Λ (Chapter 21), called the Milner booleans. Specifically, a boolean can be represented
by a channel carrying a pair of channel references that are signaled (sent a trivial value) to

PREVIEW
39.8 Notes
387
indicate whether the boolean is true or false. Give a definition of processes corresponding
to truth, falsehood, and conditional branch between two processes, each parameterized by a
channel a representing the boolean.
39.2. Define the sequential composition P ; Q of processes P and Q in PiC. Hint: Define an auxil-
iary translation P ▷p, where p is a channel value, such that P ▷p behaves like P, but sends
the unit value on p just before termination.
39.3. Consider again an RS latch, which was the subject of Exercises 11.4, 15.7, and 20.3. Im-
plement an RS latch as a process L(i, o) that takes a pair of booleans as input on channel i,
representing the R and S inputs to the latch, and outputs a pair of booleans on channel o, rep-
resenting the outputs Q and Z, with Q being the output of interest. Consider the following
input processes:
I(i) ≜∗! i( ⟨false, false⟩)
Ireset(i) ≜! i( ⟨true, false⟩) ; Ireset
Iset(i) ≜! i( ⟨true, false⟩) ; Iset
The first quiesces the inputs by holding both R and S at false forever. The second asserts
the R input (only), and then quiesces; the third asserts the S input (only), and then quiesces.
Show that the process L(i, o) ⊗Ireset(i) evolves to a process capable of taking the action
o ! ⟨false, false⟩, and is then forever capable of evolving to a process taking the same ac-
tion. Similarly, show that L(i, o) ⊗Iset(i) evolves to a process capable of taking the action
o ! ⟨true, false⟩, and is then forever capable of evolving to a process taking the same ac-
tion.
39.4. Some versions of process calculus to note have a distinction between events and processes.
They instead consider the non-deterministic choice P1 + P2 of two processes, which is defined
by the following silent transition rules:
P1 + P2 7−→
Σ
P1
(39.24a)
P1 + P2 7−→
Σ
P2
(39.24b)
Thus P1 + P2 may spontaneously evolve into either P1 or P2, without interacting with any
other process. Show that non-deterministic choice is definable in the asynchronous process
calculus in such a way that the given transitions are possible.
39.5. In the asynchronous process calculus events are finite sums of inputs, called an input choice,
of the form
? a1( x1 . P1 ) + . . . + ? an( xn . Pn ).

PREVIEW
388
39.8 Notes
The behavior of the process
P ≜$ ( ? a1( x1 . P1 ) + . . . + ? an( xn . Pn ) )
is tantalizingly close to that of the concurrent composition of processes receiving on a single
channel,
Q ≜$ ? a1( x1 . P1 ) ⊗. . . ⊗$ ? an( xn . Pn ).
The processes P and Q are similar in that both may synchronize with a concurrently execut-
ing sender on any of the specified receive channels. They are different in that P abandons the
other receives once one has synchronized with a sender, whereas Q leaves the other choices
available for further synchronization. So a receive-only choice can be defined in terms of
the concurrent composition of single-choice receives by arranging that the other choices are
deactivated once one has been chosen. Show that this is the case. Hint: Associate a Milner
boolean (Exercise 39.1) with each choice group that limits synchronization to at most one
sender.
39.6. The polyadic π-calculus is a process calculus in which all channels are constrained to carry
values of the recursive type π satisfying the isomorphism
π ∼= [π chan × . . . × π chan
|
{z
}
n
]n∈N.
Thus, a message value has the form
n · ⟨& a1, . . . , & an
|
{z
}
n
⟩
in which the tag n indicates the size of the tuple of channel references associated with it.
Show that the encoding of Λ given in Section 39.7 can be given using only channels of type
π, proving the universality of the polyadic π-calculus.

PREVIEW
Chapter 40
Concurrent Algol
In this chapter we integrate concurrency into the framework of Modernized Algol described in
Chapter 34. The resulting language, called Concurrent Algol, or CA, illustrates the integration of
the mechanisms of the process calculus described in Chapter 39 into a practical programming lan-
guage. To avoid distracting complications, we drop assignables from Modernized Algol entirely.
(There is no loss of generality, however, because free assignables are definable in Concurrent Algol
using processes as cells.)
The process calculus described in Chapter 39 is intended as a self-standing model of concurrent
computation. When viewed in the context of a programming language, however, it is possible to
streamline the machinery to take full advantage of types that are in any case required for other
purposes. In particular the concept of a channel, which features prominently in Chapter 39, is
identified with the concept of a dynamic class as described in Chapter 33. More precisely, we take
broadcast communication of dynamically classified values as the basic synchronization mechanism
of the language. Being dynamically classified, messages consist of a payload tagged with a class,
or channel. The type of the channel determines the type of the payload. Importantly, only those
processes that have access to the channel may decode the message; all others must treat it as
inscrutable data that can be passed around but not examined. In this way we can model not only
the mechanisms described in Chapter 39, but also formulate an abstract account of encryption and
decryption in a network using the methods described in Chapter 39.
Concurrent Algol features a modal separation between commands and expressions like in
Modernized Algol. It is also possible to combine these two levels (so as to allow benign con-
currency effects), but we do not develop this approach in detail here.

PREVIEW
390
40.1 Concurrent Algol
40.1
Concurrent Algol
The syntax of CA is obtained by removing assignables from MA, and adding a syntactic level of
processes to represent the global state of a program:
Typ
τ
::=
cmd( τ )
τ cmd
commands
Exp
e
::=
cmd( m )
cmd m
command
Cmd
m
::=
ret e
ret e
return
bnd( e ; x . m )
bnd x ←e ; m
sequence
Proc
p
::=
stop
1
idle
run( m )
run( m )
atomic
conc( p1 ; p2 )
p1 ⊗p2
concurrent
newch{τ}( a . p )
ν a ~ τ . p
new channel
The process run( m ) is an atomic process executing the command m. The other forms of process
are adapted from Chapter 39. If Σ has the form a1 ~ τ1, . . . , an ~ τn, then we sometimes write ν Σ{p}
for the iterated form ν a1 ~ τ1 . . . . ν an ~ τn . p.
The statics of CA is given by these judgments:
Γ ⊢Σ e : τ
expression typing
Γ ⊢Σ m ∼·· τ
command typing
Γ ⊢Σ p proc
process formation
Γ ⊢Σ α action
action formation
The expression and command typing judgments are essentially those of MA, augmented with the
constructs described below.
Process formation is defined by the following rules:
⊢Σ 1 proc
(40.1a)
⊢Σ m ∼·· τ
⊢Σ run( m ) proc
(40.1b)
⊢Σ p1 proc
⊢Σ p2 proc
⊢Σ p1 ⊗p2 proc
(40.1c)
⊢Σ,a~τ p proc
⊢Σ ν a ~ τ . p proc
(40.1d)
Processes are identified up to structural congruence, as described in Chapter 39.
Action formation is defined by the following rules:
⊢Σ ε action
(40.2a)

PREVIEW
40.1 Concurrent Algol
391
⊢Σ e : clsfd
e valΣ
⊢Σ e ! action
(40.2b)
⊢Σ e : clsfd
e valΣ
⊢Σ e ? action
(40.2c)
Messages are values of the type clsfd defined in Chapter 33.
The dynamics of CA is defined by transitions between processes, which represent the state of
the computation. More precisely, the judgment p
α7−→
Σ
p′ states that the process p evolves in one
step to the process p′ while undertaking action α.
m
α=⇒
Σ
ν Σ′ { m′ ⊗p }
run( m )
α7−→
Σ ν Σ′{run( m′ ) ⊗p}
(40.3a)
e valΣ
run( ret e )
ε7−→
Σ 1
(40.3b)
p1
α7−→
Σ
p′
1
p1 ⊗p2
α7−→
Σ
p′
1 ⊗p2
(40.3c)
p1
α7−→
Σ
p′
1
p2
α7−→
Σ
p′
2
p1 ⊗p2
ε7−→
Σ
p′
1 ⊗p′
2
(40.3d)
p
α
7−−−→
Σ,a~τ
p′
⊢Σ α action
ν a ~ τ . p
α7−→
Σ ν a ~ τ . p′
(40.3e)
Rule (40.3a) states that a step of execution of the atomic process run( m ) consists of a step of
execution of the command m, which may allocate some set Σ′ of symbols or create a concurrent
process p. This rule implements scope extrusion for classes (channels) by expanding the scope of
the channel declaration to the context in which the command m occurs. Rule (40.3b) states that a
completed command evolves to the inert (stopped) process; processes are executed solely for their
effect, and not for their value.
Executing a command in CA may, in addition to evolving to another command, allocate a new
channel or may spawn a new process. More precisely, the judgment1
m
α=⇒
Σ
ν Σ′ { m′ ⊗p′ }
1The right-hand side of this judgment is a triple consisting of Σ′, m′, and p′, not a process expression comprising these
parts.

PREVIEW
392
40.2 Broadcast Communication
states that the command m transitions to the command m′ while creating new channels Σ′ and
new processes p′. The action α specifies the interactions of which m is capable when executed.
As a notational convenience we drop mention of the new channels or processes when either are
trivial.
The following rules define the execution of the basic forms of command inherited from MA:
e 7−→
Σ
e′
ret e
ε=⇒
Σ
ret e′
(40.4a)
m1
α=⇒
Σ
ν Σ′ { m′
1 ⊗p′ }
bnd x ←cmd m1 ; m2
α=⇒
Σ
ν Σ′{bnd x ←cmd m′
1 ; m2 ⊗p′}
(40.4b)
e valΣ
bnd x ←cmd ( ret e ) ; m2
ε=⇒
Σ
[e/x]m2
(40.4c)
e1 7−→
Σ
e′
1
bnd x ←e1 ; m2
ε=⇒
Σ
bnd x ←e′
1 ; m2
(40.4d)
These rules are supplemented by rules governing communication and synchronization among
processes in the next two sections.
40.2
Broadcast Communication
In this section we consider a very general form of process synchronization called broadcast. Pro-
cesses emit and accept messages of type clsfd, the type of dynamically classified values consid-
ered in Chapter 33. A message consists of a channel, which is its class, and a payload, which is
a value of the type associated with the channel (class). Recipients may pattern match against a
message to determine whether it is of a given class, and, if so, recover the associated payload. No
process that lacks access to the class of a message may recover the payload of that message. (See
Section 33.4.1 for a discussion of how to enforce confidentiality and integrity restrictions using
dynamic classification).
The syntax of the commands pertinent to broadcast communication is given by the following
grammar:
Cmd
m
::=
spawn( e )
spawn( e )
spawn
emit( e )
emit( e )
emit message
acc
acc
accept message
newch{τ}
newch
new channel
The command spawn( e ) spawns a process that executes the encapsulated command given by e.
The commands emit( e ) and acc emit and accept messages, which are classified values whose

PREVIEW
40.2 Broadcast Communication
393
class is the channel on which the message is sent. The command newch{τ} returns a reference to a
fresh class carrying values of type τ.
The statics of broadcast communication is given by the following rules:
Γ ⊢Σ e : cmd( unit )
Γ ⊢Σ spawn( e ) ∼·· unit
(40.5a)
Γ ⊢Σ e : clsfd
Γ ⊢Σ emit( e ) ∼·· unit
(40.5b)
Γ ⊢Σ acc ∼·· clsfd
(40.5c)
Γ ⊢Σ newch{τ} ∼·· cls( τ )
(40.5d)
Execution of these commands is defined as follows:
spawn( cmd( m ) )
ε=⇒
Σ
ret ⟨⟩⊗run( m )
(40.6a)
e 7−→
Σ
e′
spawn( e )
ε=⇒
Σ
spawn( e′ )
(40.6b)
e valΣ
emit( e )
e !
==⇒
Σ
ret ⟨⟩
(40.6c)
e 7−→
Σ
e′
emit( e )
ε=⇒
Σ
emit( e′ )
(40.6d)
e valΣ
acc
e ?
==⇒
Σ
ret e
(40.6e)
newch{τ}
ε=⇒
Σ
ν a ~ τ { ret ( & a ) }
(40.6f)
Rule (40.6c) specifies that emit( e ) has the effect of emitting the message e. Correspondingly,
rule (40.6e) specifies that acc may accept (any) message that is being sent.
As usual, the preservation theorem for CA ensures that well-typed programs remain well-
typed during execution. The proof of preservation requires a lemma about command execution.
Lemma 40.1. If m
α=⇒
Σ
ν Σ′ { m′ ⊗p′ }, ⊢Σ m ∼·· τ, then ⊢Σ α action, ⊢Σ Σ′ m′ ∼·· τ, and ⊢Σ Σ′ p′ proc.

PREVIEW
394
40.3 Selective Communication
Proof. By induction on rules (40.4).
With this in hand the proof of preservation goes along familiar lines.
Theorem 40.2 (Preservation). If ⊢Σ p proc and p 7−→
Σ
p′, then ⊢Σ p′ proc.
Proof. By induction on transition, appealing to Lemma 40.1 for the crucial steps.
Typing does not, however, guarantee progress with respect to unlabeled transition, for the
simple reason that there may be no other process with which to communicate. By extending
progress to labeled transitions we may state that this is the only way for process exceution to
get stuck. But some care must be taken to account for allocating new channels.
Theorem 40.3 (Progress). If ⊢Σ p proc, then either p ≡1, or p ≡ν Σ′{p′} such that p′
α
7−−→
Σ Σ′
p′′ for
some ⊢Σ Σ′ p′′ and some ⊢Σ Σ′ α action.
Proof. By induction on rules (40.1) and (40.5).
The progress theorem says that no process can get stuck for any reason other than the inability
to communicate with another process. For example, a process that receives on a channel for which
there is no sender is “stuck”, but this does not violate Theorem 40.3.
40.3
Selective Communication
Broadcast communication provides no means of restricting acceptance to messages of a particu-
lar class (that is, of messages on a particular channel). Using broadcast communication we may
restrict attention to a particular channel a of type τ by running the following command:
fix loop : τ cmd is cmd {x ←acc ; match x as a · y ,→ret y ow ,→emit( x ) ; do loop}
This command is always capable of receiving a broadcast message. When one arrives, it is exam-
ined to see whether it is classified by a. If so, the underlying classified value is returned; otherwise
the message is re-broadcast so that another process may consider it. Polling consists of repeatedly
executing the above command until a message of channel a is successfully accepted, if ever it is.
Polling is evidently impractical in most situations. An alternative is to change the language
to allow for selective communication. Rather than accept any broadcast message, we may confine
attention to messages sent only on certain channels. The type event( τ ) of events consists of a
finite choice of accepts, all of whose payloads are of type τ.
Typ
τ
::=
event( τ )
τ event
events
Exp
e
::=
rcv[ a ]
? a
selective read
never{τ}
never
null
or( e1 ; e2 )
e1 or e2
choice
wrap( e1 ; x . e2 )
e1 as x in e2
post-composition
Cmd
m
::=
sync( e )
sync( e )
synchronize

PREVIEW
40.3 Selective Communication
395
Events in CA are similar to those of the asynchronous process calculus described in Chapter 39.
The chief difference is that post-composition is considered as a general operation on events, in-
stead of one tied to the receive event itself.
The statics of event expressions is given by the following rules:
Σ ⊢a ~ τ
Γ ⊢Σ rcv[ a ] : event( τ )
(40.7a)
Γ ⊢Σ never{τ} : event( τ )
(40.7b)
Γ ⊢Σ e1 : event( τ )
Γ ⊢Σ e2 : event( τ )
Γ ⊢Σ or( e1 ; e2 ) : event( τ )
(40.7c)
Γ ⊢Σ e1 : event( τ1 )
Γ, x : τ1 ⊢Σ e2 : τ2
Γ ⊢Σ wrap( e1 ; x . e2 ) : event( τ2 )
(40.7d)
The corresponding dynamics is defined by these rules:
Σ ⊢a ~ τ
rcv[ a ] valΣ
(40.8a)
never{τ} valΣ
(40.8b)
e1 valΣ
e2 valΣ
or( e1 ; e2 ) valΣ
(40.8c)
e1 valΣ
wrap( e1 ; x . e2 ) valΣ
(40.8d)
e1 7−→
Σ
e′
1
or( e1 ; e2 ) 7−→
Σ
or( e′
1 ; e2 )
(40.8e)
e1 valΣ
e2 7−→
Σ
e′
2
or( e1 ; e2 ) 7−→
Σ
or( e1 ; e′
2 )
(40.8f)
e1 7−→
Σ
e′
1
wrap( e1 ; x . e2 ) 7−→
Σ
wrap( e′
1 ; x . e′
2 )
(40.8g)
Event values are identified up to structural congruence as described in Chapter 39.
The statics of the synchronization command is given by the following rule:
Γ ⊢Σ e : event( τ )
Γ ⊢Σ sync( e ) ∼·· τ
(40.9a)

PREVIEW
396
40.4 Free Assignables as Processes
The type of the event determines the type of value returned by the synchronization command.
Execution of a synchronization command depends on the event.
e 7−→
Σ
e′
sync( e )
ε=⇒
Σ
sync( e′ )
(40.10a)
e valΣ
⊢Σ e : τ
Σ ⊢a ~ τ
sync( rcv[ a ] )
a·e ?
===⇒
Σ
ret( e )
(40.10b)
sync( e1 )
α=⇒
Σ
m1
sync( or( e1 ; e2 ) )
α=⇒
Σ
m1
(40.10c)
sync( e2 )
α=⇒
Σ
m2
sync( or( e1 ; e2 ) )
α=⇒
Σ
m2
(40.10d)
sync( e1 )
α=⇒
Σ
m1
sync( wrap( e1 ; x . e2 ) )
α=⇒
Σ
bnd( cmd( m1 ) ; x . ret( e2 ) )
(40.10e)
Rule (40.10b) states that an acceptance on a channel a may synchronize only with messages classi-
fied by a. When combined with structural congruence, Rules (40.10c) and (40.10d) state that either
event between two choices may engender an action. Rule (40.10e) yields the command that per-
forms the command m1 resulting from the action α taken by the event e1, then returns e2 with x
bound to the return value of m1.
Selective communication and dynamic events can be used together to implement a communi-
cation protocol in which a channel reference is passed on a channel in order to establish a com-
munication path with the recipient. Let a be a channel carrying values of type cls( τ ), and let b
be a channel carrying values of type τ, so that & b can be passed as a message along channel a.
A process that wishes to accept a channel reference on a and then accept on that channel has the
form
{x ←sync( ? a ) ; y ←sync( ?? x ) ; . . .}.
The event ? a specifies a selective receipt on channel a. Once the value x is accepted, the event ?? x
specifies a selective receipt on the channel referenced by x. So, if & b is sent along a, then the event
?? & b evaluates to ? b, which accepts selectively on channel b, even though the receiving process
may have no direct access to the channel b itself.
40.4
Free Assignables as Processes
Scope-free assignables are definable in CA by associating to each assignable a server process that
sets and gets the contents of the assignable. To each assignable a of type τ is associated a server
that selectively accepts a message on channel a with one of two forms:

PREVIEW
40.4 Free Assignables as Processes
397
1. get · ( & b ), where b is a channel of type τ. This message requests that the contents of a be
sent on channel b.
2. set · ( ⟨e, & b⟩), where e is a value of type τ, and b is a channel of type τ. This message
requests that the contents of a be set to e, and that the new contents be transmitted on channel
b.
In other words a is a channel of type τsrvr given by
[get ,→τ cls , set ,→τ × τ cls].
The server selectively accepts on channel a, then dispatches on the class of the message to satisfy
the request.
The server associated with the assignable a of type τ maintains the contents of a using recur-
sion. When called with the current contents of the assignable, the server selectively accepts on
channel a, dispatching on the associated request, and calling itself recursively with the (updated,
if necessary) contents:
λ ( u : τsrvr cls ) fix srvr : τ ⇀unit cmd is λ ( x : τ ) cmd {y ←sync( ?? u ) ; e(40.12)}.
(40.11)
The server is a procedure that takes an argument of type τ, the current contents of the assignable,
and yields a command that never terminates, because it restarts the server loop after each request.
The server selectively accepts a message on channel a, and dispatches on it as follows:
case y {get · z ,→e(40.13) | set · ⟨x′, z⟩,→e(40.14)}.
(40.12)
A request to get the contents of the assignable a is served as follows:
{ ←emit( inref( z ; x ) ) ; do srvr( x )}
(40.13)
A request to set the contents of the assignable a is served as follows:
{ ←emit( inref( z ; x′ ) ) ; do srvr( x′ )}
(40.14)
The type τ ref is defined to be τsrvr cls, the type of channels (classes) to servers providing a
cell containing a value of type τ. A new free assignable is created by the command ref e0, which
is defined to be
{x ←newch ; ←spawn( e(40.11)( x )( e0 ) ) ; ret x}.
(40.15)
A channel carrying a value of type τsrvr is allocated to serve as the name of the assignable, and a
new server is spawned that accepts requests on that channel, with initial value e0 of type τ0.
The commands ∗e0 and e0 ∗= e1 send a message to the server to get and set the contents of an
assignable. The code for ∗e0 is as follows:
{x ←newch ; ←emit( inref( e0 ; get · x ) ) ; sync( ?? ( x ) )}
(40.16)
A channel is allocated for the return value, the server is contacted with a get message specifying
this channel, and the result of receiving on this channel is returned. Similarly, the code for e0 ∗= e1
is as follows:
{x ←newch ; ←emit( inref( e0 ; set · ⟨e1, x⟩) ) ; sync( ?? ( x ) )}
(40.17)

PREVIEW
398
40.5 Notes
40.5
Notes
Concurrent Algol is a synthesis of process calculus and Modernized Algol; is essentially an “Algol-
like” formulation of Concurrent ML (Reppy, 1999). The design is influenced by Parallel Algol
(Brookes, 2002). Much work on concurrent interaction takes communication channels as a basic
concept, but see Linda (Gelernter, 1985) for an account similar to the one suggested here.
Exercises
40.1. In Section 40.2 channels are allocated using the command newch, which returns a channel
reference. Alternatively one may extend CA with a means of declaring channels just as
assignables are declared in MA. Formulate the syntax, statics, and dynamics of such a con-
struct, and derive newch using this extension.
40.2. Extend selective communication (Section 40.3) to account for channel references, which give
rise to a new form of event. Give the syntax, statics, and dynamics of this extension.
40.3. Adapt the implementation of an RS latch given in Exercise 39.3 to CA.

PREVIEW
Chapter 41
Distributed Algol
A distributed computation is one that takes place at many sites, each of which controls some re-
sources at that site. For example, the sites might be nodes on a network, and a resource might be
a device or sensor at that site, or a database controlled by that site. Only programs that execute at
a particular site may access the resources situated at that site. Consequently, command execution
always takes place at a particular site, called the locus of execution. Access to resources at a remote
site from a local site is achieved by moving the locus of execution to the remote site, running code
to access the local resource, and returning a value to the local site.
In this chapter we consider the language DA, which extends Concurrent Algol with a spatial
type system that mediates access to resources on a network. The type safety theorem ensures that
all accesses to a resource controlled by a site are through a program executing at that site, even
though references to local resources can be freely passed around to other sites on the network. The
main idea is that channels and events are located at a particular site, and that synchronization on an
event can only occur at the proper site for that event. Issues of concurrency, which are temporal,
are thereby separated from those of distribution, which are spatial.
The concept of location in DA is sufficiently abstract that it admits another useful interpretation
that can be useful in computer security settings. The “location” of a computation can be considered
to be the principal on whose behalf the computation is executing. From this point of view, a local
resource is one that is accessible to a particular principal, and a mobile computation is one that can
be executed by any principal. Movement from one location to another may then be interpreted as
executing a piece of code on behalf of another principal, returning its result to the principal that
initiated the transfer.
41.1
Statics
The statics of DA is inspired by the possible worlds interpretation of modal logic. Under that inter-
pretation the truth of a proposition is considered relative to a world, which determines the state
of affairs described by that proposition. A proposition may be true in one world, and false in an-
other. For example, one may use possible worlds to model counter-factual reasoning, where one

PREVIEW
400
41.1 Statics
postulates that certain facts that happen to be true in this, the actual, world, might be otherwise in
some other, possible, world. For instance, in the actual world you, the reader, are reading this book,
but in a possible world you may never have taken up the study of programming languages at all.
Of course not everything is possible: there is no possible world in which 2 + 2 is other than 4, for
example. Moreover, once a commitment has been made to one counter-factual, others are ruled
out. We say that one world is accessible from another when the second is a sensible counter-factual
relative to the first. So, for example, one may consider that relative to a possible world in which
you are the king, there is no further possible world in which someone else is also the king (there
being only one sovereign).
In DA we shall interpret possible worlds as sites on a network, with accessibility between
worlds expressing network connectivity. We postulate that every site is connected to itself (re-
flexivity); that if one site is reachable from another, then the second is also reachable from the first
(symmetry); and that if a site is reachable from a reachable site, then this site is itself reachable from
the first (transitivity). From the point of view of modal logics, the type system of DA is derived
from the logic S5, for which accessibility is an equivalence relation.
The syntax of DA derives from that of CA. The following grammar summarizes the important
changes:
Typ
τ
::=
cmd[ w ]( τ )
τ cmd[ w ]
commands
event[ w ]( τ )
τ event[ w ]
events
Cmd
m
::=
at[ w ]( m )
at w do m
change site
The command and event types are indexed by the site w at which they make sense. The command
at[ w ]( m ) changes the locus of execution from one site to another.
A signature Σ in DA consists of a finite set of declarations of the form a ~ τ @ w, where τ is a type
and w is a site. Such a declaration specifies that a is a channel at site w carrying a payload of type
τ. We may think of a signature Σ as a family of signatures Σw one for each world w, containing the
declarations of the channels at that world. Partitioning channels in this way corresponds to the
idea that channels are located at a particular site. They may be handled passively at other sites,
but their only active role is at the site at which they are declared.
The statics of DA is given by the following judgment forms:
Γ ⊢Σ e : τ
expression typing
Γ ⊢Σ m ∼·· τ @ w
command typing
Γ ⊢Σ p proc @ w
process formation
Γ ⊢Σ α action @ w
action formation
The expression typing judgment is independent of the site, expressing the requirement that the
values of a type be meaningful at any site. On the other hand commands can only be executed at
a particular site, because their meaning depends on the resources at a site. Processes are similarly
confined to execution at a site. Actions are site-specific; there is no inter-site synchronization.
The expressions of the command and event types of DA are defined by the following rules:
Γ ⊢Σ m ∼·· τ @ w
Γ ⊢Σ cmd( m ) : cmd[ w ]( τ )
(41.1a)

PREVIEW
41.1 Statics
401
Γ ⊢Σ never{τ} : event[ w ]( τ )
(41.1b)
Σ ⊢a ~ τ @ w
Γ ⊢Σ rcv[ a ] : event[ w ]( τ )
(41.1c)
Γ ⊢Σ e1 : event[ w ]( τ )
Γ ⊢Σ e2 : event[ w ]( τ )
Γ ⊢Σ or( e1 ; e2 ) : event[ w ]( τ )
(41.1d)
Γ ⊢Σ e1 : event[ w ]( τ1 )
Γ, x : τ1 ⊢Σ e2 : τ2
Γ ⊢Σ wrap( e1 ; x . e2 ) : event[ w ]( τ2 )
(41.1e)
Rule (41.1a) states that the type of an encapsulated command records the site at which the com-
mand is executed. Rules (41.1b) to (41.1e) specify that events are attached to a site because channels
are. Communication among processes is confined to a site; there is no inter-site synchronization.
The statics of the commands of DA is given by the following rules:
Γ ⊢Σ e : τ
Γ ⊢Σ ret( e ) ∼·· τ @ w
(41.2a)
Γ ⊢Σ e1 : cmd[ w ]( τ1 ) @ w
Γ, x : τ1 ⊢Σ m2 ∼·· τ2 @ w
Γ ⊢Σ bnd( e1 ; x . m2 ) ∼·· τ2 @ w
(41.2b)
Γ ⊢Σ e : cmd[ w ]( unit )
Γ ⊢Σ spawn( e ) ∼·· unit @ w
(41.2c)
Γ ⊢Σ e : τ
Σ ⊢a ~ τ @ w
Γ ⊢Σ snd[ a ]( e ) ∼·· unit @ w
(41.2d)
Γ ⊢Σ e : event[ w ]( τ )
Γ ⊢Σ sync( e ) ∼·· τ @ w
(41.2e)
Γ ⊢Σ m′ ∼·· τ′ @ w′
Γ ⊢Σ at[ w′ ]( m′ ) ∼·· τ′ @ w
(41.2f)
Rule (41.2a) states that an expression may be returned at any site, because its meaning is indepen-
dent of the site. Rule (41.2b) ensures that the sequential composition of commands is allowed only
within a site, and not across sites. Rule (41.2e) states that the sync command returns a value of
the same type as that of the event, and can be executed only at the site to which the given event
pertains. Rule (41.2d) states that a message can be sent along a channel available at the site from
which it is sent. Finally, rule (41.2f) states that to execute a command at a site w′ requires that the
command pertain to that site. The returned value is then passed to the original site.
Process formation is defined as follows:
⊢Σ 1 proc @ w
(41.3a)
⊢Σ m ∼·· unit @ w
⊢Σ run( m ) proc @ w
(41.3b)

PREVIEW
402
41.2 Dynamics
⊢Σ p1 proc @ w
⊢Σ p2 proc @ w
⊢Σ p1 ⊗p2 proc @ w
(41.3c)
⊢Σ,a~τ@w p proc @ w
⊢Σ ν a ~ τ . p proc @ w
(41.3d)
These rules state that processes are sited. In particular an atomic process consists of a command
suitable for the site at which the process is run, and a new channel is allocated at the site of the
process that allocates it.
Action formation is defined as follows:
⊢Σ ε action @ w
(41.4a)
⊢Σ e : τ
e valΣ
Σ ⊢a ~ τ @ w
⊢Σ a · e ! action @ w
(41.4b)
⊢Σ e : τ
e valΣ
Σ ⊢a ~ τ @ w
⊢Σ a · e ? action @ w
(41.4c)
Messages are values of type clsfd, and are meaningful only at the site at which the channel is
allocated. Locality of actions corresponds to confinement of communication to a single site.
41.2
Dynamics
The dynamics of DA is a labeled transition judgment between processes at a site. Thus, the judg-
ment
p
α @ w
7−−−→
Σ
p′
states that at site w the process p steps to the process p′, engendering the action α. It is defined by
the following rules:
m
α @ w
===⇒
Σ
ν Σ′ { m′ ⊗p }
run( m )
α @ w
7−−−→
Σ
ν Σ′{run( m′ ) ⊗p}
(41.5a)
e valΣ
run( ret e )
ε @ w
7−−−→
Σ
1
(41.5b)
p1
α @ w
7−−−→
Σ
p′
1
p1 ⊗p2
α @ w
7−−−→
Σ
p′
1 ⊗p2
(41.5c)
p1
α @ w
7−−−→
Σ
p′
1
p2
α @ w
7−−−→
Σ
p′
2
p1 ⊗p2
ε @ w
7−−−→
Σ
p′
1 ⊗p′
2
(41.5d)

PREVIEW
41.2 Dynamics
403
p
α @ w
7−−−−−→
Σ,a~τ@w
p′
⊢Σ α action @ w
ν a ~ τ . p
α @ w
7−−−→
Σ
ν a ~ τ . p′
(41.5e)
These rules are like Rules (40.3), but for the sensitivity to the site at which execution takes place.
The site comes into play in Rules (41.5a) and (41.5e).
Rule (41.5a) makes use of the command execution judgment
m
α @ w
===⇒
Σ
ν Σ′ { m′ ⊗p },
which states that the command m when executed at site w may engender the action α and in the
process create new channels, Σ′, and a new process p. (The result of the transition is not a process
expression, but a triple comprising the newly allocated channels, the newly created processes, and
a new command)
Command execution is defined by the following rules:
spawn( cmd( m ) )
ε @ w
===⇒
Σ
ret( ⟨⟩) ⊗run( at[ w ]( m ) )
(41.6a)
e valΣ
⊢Σ e : τ
Σ ⊢a ~ τ @ w
snd[ a ]( e )
a·e ! @ w
=====⇒
Σ
ret ⟨⟩
(41.6b)
e valΣ
⊢Σ e : τ
Σ ⊢a ~ τ @ w
sync( rcv[ a ] )
a·e ? @ w
=====⇒
Σ
ret( e )
(41.6c)
sync( e1 )
α @ w
===⇒
Σ
m1
sync( or( e1 ; e2 ) )
α @ w
===⇒
Σ
m1
(41.6d)
sync( e2 )
α @ w
===⇒
Σ
m2
sync( or( e1 ; e2 ) )
α @ w
===⇒
Σ
m2
(41.6e)
sync( e1 )
α @ w
===⇒
Σ
m1
sync( wrap( e1 ; x . e2 ) )
α @ w
===⇒
Σ
bnd( cmd( m1 ) ; x . ret( e2 ) )
(41.6f)
m
α @ w′
====⇒
Σ
ν Σ′ { m′ ⊗p′ }
at[ w′ ]( m )
α @ w
===⇒
Σ
ν Σ′ { at[ w′ ]( m′ ) ⊗p′ }
(41.6g)

PREVIEW
404
41.3 Safety
e valΣ
at[ w′ ]( ret( e ) )
ε @ w
===⇒
Σ
ret( e )
(41.6h)
Rule (41.6a) states that new processes created at a site stay at that site—the new process executes
the given command at the current site. Rule (41.6b) specifies that a send generates an event specific
to the site at which it occurs. Rules (41.6c) to (41.6f) specify that receive events occur only for chan-
nels allocated at the execution site. Rules (41.6g) and (41.6h) state that the command at[ w′ ]( m )
is executed at site w by executing m at site w′, and returning the result to the site w.
41.3
Safety
The safety theorem for DA ensures that synchronization on a channel may only occur at the site
on which the channel resides, even though channel references may be propagated from one site
to another during a computation. By the time the reference is resolved and synchronization is
attempted the computation will be executing at the right site.
Lemma 41.1 (Execution). If m
α @ w
===⇒
Σ
ν Σ′ { m′ ⊗p′ }, and ⊢Σ m ∼·· τ @ w, then ⊢Σ α action @ w,
⊢Σ Σ′ m′ ∼·· τ @ w, and ⊢Σ Σ′ p′ proc @ w.
Proof. By induction on rules (41.6).
Theorem 41.2 (Preservation). If p
α @ w
7−−−→
Σ
p′ and ⊢Σ p proc @ w, then ⊢Σ p′ proc @ w.
Proof. By induction on the statics of DA, appealing to Lemma 41.1 for atomic processes.
The progress theorem states that the only impediment to execution of a well-typed program is
synchronizing on an event that never occurs.
Theorem 41.3 (Progress). If ⊢Σ p proc @ w, then either p ≡1 or there exists α and p′ such that
p
α @ w
7−−−→
Σ
p′.
Proof. By induction on the dynamics of DA.
41.4
Notes
The use of a spatial modality to express locality and mobility constraints in a distributed program
was introduced in the experimental language ML5 (Murphy et al., 2004). Some languages for dis-
tributed computing consolidate concurrency with distribution by allowing cross-site interaction.
The idea of DA is to separate temporal from spatial considerations, limiting synchronization to a
single site, but allowing movement of the locus of execution from one site to another.

PREVIEW
41.4 Notes
405
Exercises
41.1. The definition of DA given in this chapter has no means of allocating new channels, or send-
ing and receiving on them. Remedy this shortcoming by adding a command to create chan-
nel references. Give the statics and dynamics of this extension, and of any associated exten-
sions needed to account for it. Hint: the type of channel references, chan[ w ]( τ ), should be
indexed by the site of the channel to which the reference refers.
41.2. Given a channel reference e : chan[ w′ ]( τ ), it is sensible to send a message asynchronously
from site w along this channel by providing a payload e′ : τ. It is also possible to implement a
synchronous remote send (also known as a remote procedure call) that sends a message e′ : τ on
a remote channel e : chan[ w′ ]( τ ) and returns a result of type τ′ in response to the message.
Implement both of these capabilities in DA. Hint: Implement synchronous communication
using a reply channel as described in Chapter 39.

PREVIEW
406
41.4 Notes

PREVIEW
Part XVII
Modularity

PREVIEW

PREVIEW
Chapter 42
Modularity and Linking
Modularity is the most important technique for controlling the complexity of programs. Programs
are decomposed into separate components with precisely specified, and tightly controlled, interac-
tions. The pathways for interaction among components determine dependencies that constrain
the process by which the components are integrated, or linked, to form a complete system. Differ-
ent systems may use the same components, and a single system may use multiple instances of a
single component. Sharing of components amortizes the cost of their development across systems,
and helps limit errors by limiting coding effort.
Modularity is not limited to programming languages. In mathematics the proof of a theorem
is decomposed into a collection of definitions and lemmas. References among the lemmas deter-
mine a dependency relation that constrains their integration to form a complete proof of the main
theorem. Of course, one person’s theorem is another person’s lemma; there is no intrinsic limit
on the depth and complexity of the hierarchies of results in mathematics. Mathematical structures
are themselves composed of separable parts, for example, a ring comprises a group and a monoid
structure on the same underlying set.
Modularity arises from the structural properties of the hypothetical and general judgments.
Dependencies among components are expressed by free variables whose typing assumptions state
the presumed properties of the component. Linking amounts to substitution to discharge the
hypothesis.
42.1
Simple Units and Linking
Decomposing a program into units amounts to exploiting the transitivity of the hypothetical judg-
ment (see Chapter 3). The decomposition may be described as an interaction between two parties,
the client and the implementor, mediated by an agreed-upon contract, an interface. The client assumes
that the implementor upholds the contract, and the implementor guarantees that the contract will
be upheld. The assumption made by the client amounts to a declaration of its dependence on the
implementor discharged by linking the two parties accordng to their agreed-upon contract.

PREVIEW
410
42.2 Initialization and Effects
The interface that mediates the interaction between a client and an implementor is a type. Link-
ing is the implementation of the composite structural rules of substitution and transitivity:
Γ ⊢eimpl : τintf
Γ, x : τintf ⊢eclient : τclient
Γ ⊢[eimpl/x]eclient : τclient
(42.1)
The type τintf is the interface type. It defines the operations provided by the implementor eimpl and
relied upon by the client eclient. The free variable x expresses the dependency of eclient on eimpl. That
is, the client accesses the implementation by using the variable x.
The interface type τintf is the contract between the client and the implementor. It determines
the properties of the implementation on which the client may depend and, at the same time, de-
termines the obligations that the implementor must fulfill. The simplest form of interface type is a
finite product type of the form ⟨f1 ,→τ1 , . . . , fn ,→τn⟩, specifying a component with components
fi of type τi. Such a type is an application program interface, or API, because it determines the op-
erations that the client (application) may expect from the implementor. A more advanced form of
interface is one that defines an abstract type of the form ∃( t.⟨f1 ,→τ1 , . . . , fn ,→τn⟩), which de-
fines an abstract type t representing the internal state of an “abstract machine” whose “instruction
set” consists of the operations f1, . . . , fn whose types may involve t. Being abstract, the type t is
not revealed to the client, but is known only to the implementor.1
Conceptually, linking is just substitution, but practically this can be implemented in many
ways. One method is separate compilation. The expressions eclient and eimpl, the source modules,
are translated (compiled) into another, lower-level, language, resulting in object modules. Linking
consists of performing the required substitution at the level of the object language in such a way
that the result corresponds to translating [eimpl/x]eclient. Another method, separate checking, shifts
the requirement for translation to the linker. The client and implementor units are checked for
type correctness with respect to the interface, but are not translated into lower-level form. Linking
then consists of translating the composite program as a whole, often resulting in a more efficient
outcome than would be possible when compiling separately.
The foregoing are all forms of static linking because the program is composed before it is ex-
ecuted. Another method, dynamic linking, defers program composition until run-time, so that a
component is loaded only if it is actually required during execution. This might seem to involve
executing programs with free variables, but it does not. Each client implemented by a stub that
forwards accesses to a stored implementation (typically, in an ambient file system). The difficulty
with dynamic linking is that it refers to components by name (say, a path in a file system), and the
binding of that name may change at any time, wreaking havoc on program behavior.
42.2
Initialization and Effects
Linking resolves the dependencies among the components of a program by substitution. This
view is valid so long as the components are given by pure expressions, those that evaluate to a
value without inducing any effects. For in such cases there is no problem with the replication, or
1See Chapters 17 and 48 for a discussion of type abstraction.

PREVIEW
42.2 Initialization and Effects
411
complete omission, of a component arising from repeated, or absent, uses of a variable represent-
ing it. But what if the expression dening the implementation of a component has an effect when
evaluated? At a minimum replication of the component implies replication of its effects. Worse,
effects introduce
implicit dependencies among components that are not apparent from their types.
For example, if each of two components mutates a shared assignable, the order in which they are
linked with a client program affects the behavior of the whole.
This may raise doubts about the treatment of linking as substitution, but on closer inspection it
becomes clear that implicit dependencies are naturally expressed by the modal distinction between
expressions and commands introduced in Chapter 34. Specically, a component that may have
an effect when executed does not have type
t intf of implementations of the interface type, but
rather the type
t intf cmd of encapsulated commands that, when executed, have effects and yield
implementations. Being encapsulated, a value of this type is itself free of effects, but it may have
effects when evaluated.
The distinction between the types
t intf and t intf cmd is mediated by the sequencing command
introduced in Chapter 34. For the sake of generality, let us assume that the client is itself an
encapsulated command of type
t client cmd, so that it may itself have effects when executed, and may
serve as a component of a yet larger system. Assuming that the client refers to the encapsulated
implementation by the variable
x, the command
bnd x  
x ; do eclient
rst determines the implementation of the interface by running the encapsulated command
x then
running the client code with the result bound to
x. The implicit dependencies of the client on
the implementor are made explicit by the sequencing command, which ensures that the imple-
mentor’s effects occur prior to those of the client, precisely because the client depends on the
implementor for its execution.
More generally, to manage such interactions in a large program it is common to isolate an
initialization procedure whose role is to stage the effects engendered by the various components
according to some policy or convention. Rather than attempt to survey all possible policies, let us
just note that the upshot of such conventions is that the initialization procedure is a command of
the form
f x1  
x1 ; . . . xn  
xn ; mmaing,
where x1, . . . , xn represent the components of the system and
mmain is the main (startup) routine.
After linking the initialization procedure has the form
f x1  
e1 ; . . . xn  
en ; mmaing,
where e1, . . . , en are the encapsulated implementations of the linked components. When the ini-
tialization procedure is executed, it results in the substitution
[v1, . . . , vn/ x1, . . . , xn]mmain,
where the expressions
v1, . . . , vn represent the values resulting from executing
e1, . . . , en, respec-
tively, and the implicit effects have occurred in the order specied by the initializer.

















































































PREVIEW
492
Chapter 3
3.1. As usual, give an inductive definition of the two-place judgment len(a ; n), where a comb and
n nat, and show that it relates every combinator a to a unique number n by induction on the
given rules C defining a comb.
3.2. Pick a renaming x′ of x, and extend rules C with the axiom x′ comb. Proceed by rule induction
on this extended rule set, replacing x′ comb by a1 comb at the base case, and otherwise
proceeding inductively.
3.3. The required derivation is suggested by the following equivalences:
s k k x ≡( k x ) ( k x )
≡x
The first is justified by the S axiom, the second by the K axiom.
3.4. The formulation of the question suggests to fix x and define a judgment absx a is a′ and show
that it defines a function:
absx x is s k k
(A.7a)
absx k is k k
(A.7b)
absx s is k s
(A.7c)
absx a1 is a′
1
absx a2 is a′
2
absx a1 a2 is s a′
1 a′
2
(A.7d)
It is easy to check that the required equivalence holds, noting that the axioms governing k
and s have been chosen precisely to make the proof go through without complication.
3.5. Simply redefine bracket abstraction so that [ x ] a ≜ap( k ; a ) when x /∈a. This formulation
generalizes the original case, where a = y ̸= x, to avoid altering any combinator in which
x does not occur. Then prove that [a/y][ x ] b = [ x ] [a/y]b under the stated conditions by
induction on the derivation of x y | x comb y comb ⊢b comb.
3.6. The following rules define the generalized form of the judgment:
(1 ≤i ≤n)
x1, . . . , xk | x1 closed, . . . , xk closed ⊢xi closed
(A.8a)
x1, . . . , xk | x1 closed, . . . , xk closed ⊢a1 closed
x1, . . . , xk | x1 closed, . . . , xk closed ⊢a2 closed
x1, . . . , xk | x1 closed, . . . , xk closed ⊢ap( a1 ; a2 ) closed
(A.8b)

PREVIEW
493
x1, . . . , xk, x | x1 closed, . . . , xk closed, x closed ⊢a closed
x1, . . . , xk | x1 closed, . . . , xk closed ⊢λ( x . a ) closed
(A.8c)
The “trick” is that the local variables x1, . . . , xk of the generality judgment are disjoint from
the ambient variables X that are also available. There being no hypotheses governing the
ambient variables, X , it is impossible to derive x closed for any x ∈X . But when descending
into the scope of an abstractor, it is temporarily postulated that the bound variable x is closed
so that its occurrences within the scope of the abstractor are properly regarded as closed.
This exercise drives home the principle that variables are pronouns, and are not nouns. The
assumption x closed does not say of a “thing in itself” x that is closed; were variables “things”
such a hypothesis would be senseless. But variables are not things, they refer to things. So
a hypothesis x closed expresses a constraint on to what the pronoun x refers—that is, it
constrains what can be substituted for it. It makes perfect sense to hypothesize that only
closed abts may be substituted for a given variable.
Chapter 4
4.1. Many variations are possible. Here is an illustrative fragment of a solution that incorporates
some of the suggestions given in Exercise 4.2.
Γ x ↑τ ⊢x ↑τ
(A.9a)
Γ ⊢e ↑τ
Γ ⊢e ↓τ
(A.9b)
Γ ⊢e ↓τ
Γ ⊢cast{τ}( e ) ↑τ
(A.9c)
Γ ⊢num[ n ] ↓num
(A.9d)
Γ ⊢e1 ↓num
Γ ⊢e2 ↓num
Γ ⊢plus( e1 ; e2 ) ↑num
(A.9e)
Γ ⊢str[ s ] ↓str
(A.9f)
Γ ⊢e1 ↑τ1
Γ, x ↑τ1 ⊢e2 ↓τ2
Γ ⊢let( e1 ; x . e2 ) ↓τ2
(A.9g)
The separation of synthetic from analytic typing resolves the difficulty with the type of the
defined term in a definition expression.
4.2. The main difficulty is to ensure that you do not preclude programs that ought to be allowed,
or that can only be expressed very awkwardly. Within these constraints there are many
possible variations on Solution 4.1.

PREVIEW
494
Chapter 5
5.1. Proceed by rule induction on Rules (5.10).
5.2. Proceed by rule induction on the first premise.
5.3. The definitions of multi-step and k-step transition are chosen so as to make this proof a
routine induction as indicated. Because it is obvious that if s 7−→k s′ and s′ 7−→k′ s′′, then
s 7−→k+k′ s′′, Solution 5.1 may be obtained as a corollary of this solution.
5.4. Proceed by rule induction on rules (5.10). The suggested strengthening ensures that rule (5.10f)
can be proved without complication. The assumptions on ei and e′
i are preserved when pass-
ing to the premises of rule (5.10f), and these assumptions are needed when considering re-
flexivity for a variable xi. The rest of the proof is routine.
Chapter 6
6.1. The remaining cases follow along the same lines as those given in the proof of Theorem 6.2.
6.2. The remaining cases follow along the same lines as those given in the proof of Theorem 6.4.
6.3. The suggested case analysis ensures that errors are propagated properly by each construct.
The proof as a whole ensures that there are no well-typed “stuck” expressions other than
values and checked errors.
Chapter 7
7.1. Proceed by a simultaneous rule induction on rules (7.1).
7.2. Proceed along the same lines as those steps already given.
7.3. The second part proceeds by a rule induction on rules (5.1), appealing to the lemma in the
inductive step.
7.4. The difficulty is that the progress theorem would allow an unchecked, as well as a checked,
error in its statement. Moreover, Theorem 7.5 is no longer valid in the presence of a checked
error, so safety is no longer a corollary of progress. The most obvious alternative is to intro-
duce two forms of error checks, one for unchecked errors (solely to express safety), and one
for checked errors (to allow for run-time errors arising from well-typed expressions). Such a
formulation becomes rather baroque.



































































