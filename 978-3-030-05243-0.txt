Emergence, Complexity and Computation ECC
Guanrong Chen
Yang Lou
Naming 
Game
Models, Simulations and Analysis

Emergence, Complexity and Computation
Volume 34
Series editors
Ivan Zelinka, Technical University of Ostrava, Ostrava, Czech Republic
e-mail: ivan.zelinka@vsb.cz
Andrew Adamatzky, University of the West of England, Bristol, UK
e-mail: adamatzky@gmail.com
Guanrong Chen, City University of Hong Kong, Hong Kong, China
e-mail: eegchen@cityu.edu.hk
Editorial Board
Ajith Abraham, MirLabs, USA
Ana Lucia C. Bazzan, Universidade Federal do Rio Grande do Sul, Porto
Alegre, RS, Brazil
Juan C. Burguillo, University of Vigo, Spain
Sergej Čelikovský, Academy of Sciences of the Czech Republic, Czech Republic
Mohammed Chadli, University of Jules Verne, France
Emilio Corchado, University of Salamanca, Spain
Donald Davendra, Technical University of Ostrava, Czech Republic
Andrew Ilachinski, Center for Naval Analyses, USA
Jouni Lampinen, University of Vaasa, Finland
Martin Middendorf, University of Leipzig, Germany
Edward Ott, University of Maryland, USA
Linqiang Pan, Huazhong University of Science and Technology, Wuhan, China
Gheorghe Păun, Romanian Academy, Bucharest, Romania
Hendrik Richter, HTWK Leipzig University of Applied Sciences, Germany
Juan A. Rodriguez-Aguilar, IIIA-CSIC, Spain
Otto Rössler, Institute of Physical and Theoretical Chemistry, Tübingen, Germany
Vaclav Snasel, Technical University of Ostrava, Czech Republic
Ivo Vondrák, Technical University of Ostrava, Czech Republic
Hector Zenil, Karolinska Institute, Sweden

The Emergence, Complexity and Computation (ECC) series publishes new
developments, advancements and selected topics in the ﬁelds of complexity,
computation and emergence. The series focuses on all aspects of reality-based
computation approaches from an interdisciplinary point of view especially from
applied sciences, biology, physics, or chemistry. It presents new ideas and inter-
disciplinary insight on the mutual intersection of subareas of computation, com-
plexity and emergence and its impact and limits to any computing based on
physical limits (thermodynamic and quantum limits, Bremermann’s limit, Seth
Lloyd limits…) as well as algorithmic limits (Gödel’s proof and its impact on
calculation, algorithmic complexity, the Chaitin’s Omega number and Kolmogorov
complexity, non-traditional calculations like Turing machine process and its con-
sequences,…) and limitations arising in artiﬁcial intelligence ﬁeld. The topics are
(but not limited to) membrane computing, DNA computing, immune computing,
quantum computing, swarm computing, analogic computing, chaos computing and
computing on the edge of chaos, computational aspects of dynamics of complex
systems (systems with self-organization, multiagent systems, cellular automata,
artiﬁcial life,…), emergence of complex systems and its computational aspects, and
agent based computation. The main aim of this series it to discuss the above
mentioned topics from an interdisciplinary point of view and present new ideas
coming from mutual intersection of classical as well as modern methods of com-
putation. Within the scope of the series are monographs, lecture notes, selected
contributions from specialized conferences and workshops, special contribution
from international experts.
More information about this series at http://www.springer.com/series/10624

Guanrong Chen
• Yang Lou
Naming Game
Models, Simulations and Analysis
123

Guanrong Chen
Department of Electronic Engineering
City University of Hong Kong
Hong Kong, China
Yang Lou
Centre for Chaos and Complex Networks
City University of Hong Kong
Hong Kong, China
ISSN 2194-7287
ISSN 2194-7295
(electronic)
Emergence, Complexity and Computation
ISBN 978-3-030-05242-3
ISBN 978-3-030-05243-0
(eBook)
https://doi.org/10.1007/978-3-030-05243-0
Library of Congress Control Number: 2018962759
© Springer Nature Switzerland AG 2019
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors, and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
Language is a unique hallmark of human beings among all species in nature, which
is emerged and developed by creation, acquisition, and maintenance. Humans use
language to communicate with each other, to exchange information, and to express
ideas, which in many ways have signiﬁcantly changed our world and ourselves.
In the study of language evolution and development, one important scientiﬁc
method is based on social or computer game simulations. In a typical language
game, a simple mathematical model is designed and used to simulate the process of
linguistic pattern formation and recognition, where a population of humans is
involved in the play and they follow some pre-set game rules leading to consensus
of the whole population on a new word, a new phrase, or a new sentence.
A representative computer game model for studying language creation and
development is the so-called naming game model which, along with several vari-
ants, is a simulation-based numerical study exploring the emergence and evolution
of shared information in a population of communicating agents. The shared
information could be, for example, a set of emerging names for an object observed
by the agents, or some social conventions, ideas, knowledge, etc. The population of
agents is connected in a certain communication topology, and thus each agent can
be considered as a node in the underlying communication network, with the mutual
interaction or acquaintance between two connected nodes represented by an edge.
As a result, naming game is formulated as a reaction–diffusion process on a graph,
which can be studied using tools from the graph theory in mathematics.
This monograph presents an introduction to the naming game in various ver-
sions, speciﬁcally the minimal naming game with agents having inﬁnite or ﬁnite
sizes of memories (Chaps. 2 and 3), naming game with group discussions (Chap. 4),
naming game with learning errors in communications (Chap. 5), naming game on
multi-community networks (Chap. 6), naming game with multiple words or sen-
tences (Chap. 7), and naming game with multiple languages (Chap. 8). It is the
authors’ hope that, after reading, the readers could have some fundamental
knowledge with a comprehensive understanding of the naming game and its
applications to future social and language studies.
v

The book is designed for self-studies by researchers and practitioners, graduate
and also undergraduate students, as well as social and linguistic scientists. The main
contents of the text are collected from the authors’ own research work, in a natural
combination with many others’ contributions, all being edited into a logical pre-
sentation of the notion as a self-contained technical book. In so doing, all simu-
lations have been re-checked with all simulation ﬁgures redrawn in a uniﬁed format.
A rather comprehensive list of main references is provided for the reader’s veriﬁ-
cation and future studies.
During the preparation of the manuscript, the authors received some helpful
assistance from several individuals, especially Dr. Zhengping Fan who shared the
source code of the multi-local network model and Mr. Jianfeng Zhou who shared
the source code of the multi-language naming game model. All of them are
acknowledged here with great appreciation.
Hong Kong, China
Guanrong Chen
September 2018
Yang Lou
vi
Preface

Contents
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Development Towards Applications . . . . . . . . . . . . . . . . . . . . . . .
4
1.3
Abbreviations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.4
Layout of the Monograph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2
Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
2.1
Complex Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
2.1.1
ER Random-Graph Network Model . . . . . . . . . . . . . . . . .
11
2.1.2
WS Small-World Network Model . . . . . . . . . . . . . . . . . . .
13
2.1.3
BA Scale-Free Network Model . . . . . . . . . . . . . . . . . . . . .
14
2.1.4
Multi-Local-World Network Model. . . . . . . . . . . . . . . . . .
15
2.2
Naming Game . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
2.2.1
Naming Game Framework . . . . . . . . . . . . . . . . . . . . . . . .
16
2.2.2
Minimal Naming Game . . . . . . . . . . . . . . . . . . . . . . . . . .
19
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
3
Finite-Memory Naming Game . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
3.2
Finite-Memory Naming Game . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
3.3
Simulation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
3.3.1
Simulation Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
3.3.2
Convergence Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
3.3.3
Memory Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
3.3.4
Convergence Process . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
3.4
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
vii

4
Naming Game with Multi-Hearers or Group Discussions . . . . . . . . .
43
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
4.2
Multi-Hearer Naming Game . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
4.3
Naming Game with Group Discussions . . . . . . . . . . . . . . . . . . . .
49
4.3.1
Group Formation and Transmitted-Words . . . . . . . . . . . . .
50
4.3.2
Words Transmission . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
4.4
Simulation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
4.4.1
General Convergence Process of MHNG . . . . . . . . . . . . .
55
4.4.2
Convergence Time of MHNG . . . . . . . . . . . . . . . . . . . . .
56
4.4.3
Peak of Convergence Curve of MHNG . . . . . . . . . . . . . . .
59
4.4.4
General Convergence Process of NGG . . . . . . . . . . . . . . .
61
4.4.5
Convergence Time Analysis . . . . . . . . . . . . . . . . . . . . . . .
64
4.5
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
5
Communications with Learning Errors . . . . . . . . . . . . . . . . . . . . . .
71
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
5.2
Communications with Learning Errors . . . . . . . . . . . . . . . . . . . . .
72
5.3
Simulation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
5.3.1
Simulation Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
5.3.2
Convergence Processes . . . . . . . . . . . . . . . . . . . . . . . . . .
78
5.3.3
Convergence Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
5.3.4
Maximum Number of Total and Different Words . . . . . . .
83
5.3.5
Convergence Thresholds . . . . . . . . . . . . . . . . . . . . . . . . .
87
5.4
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
6
Naming Game on Multi-Community Networks . . . . . . . . . . . . . . . . .
95
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
6.2
Multi-Local-World Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
6.3
Simulation Results and Analysis . . . . . . . . . . . . . . . . . . . . . . . . .
99
6.3.1
Convergence Time Versus Number and Sizes
of Local-Worlds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
101
6.3.2
Convergence Time Versus Rate of Initially
Assigned Nodes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
6.3.3
Convergence Process . . . . . . . . . . . . . . . . . . . . . . . . . . . .
107
6.3.4
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
111
6.4
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
112
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
112
7
Multi-Word Naming Game . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
115
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
115
7.2
Multi-Word Naming Game . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
118
viii
Contents

7.2.1
Conventional Sentence Patterns . . . . . . . . . . . . . . . . . . . .
119
7.2.2
Local Consensus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
7.3
Simulation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
123
7.3.1
Simulation Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
123
7.3.2
Conventional English Language Patterns . . . . . . . . . . . . . .
124
7.3.3
Man-Designed Language Patterns . . . . . . . . . . . . . . . . . . .
130
7.4
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
133
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
133
8
Multi-Language Naming Game . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
135
8.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
135
8.2
Multi-Language Naming Game . . . . . . . . . . . . . . . . . . . . . . . . . .
136
8.3
Simulation Results and Analysis . . . . . . . . . . . . . . . . . . . . . . . . .
140
8.3.1
Simulation Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
140
8.3.2
Convergence Process and Analysis . . . . . . . . . . . . . . . . . .
142
8.3.3
Communication Ratio . . . . . . . . . . . . . . . . . . . . . . . . . . .
145
8.3.4
Convergence Speed . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
8.3.5
Maximum Numbers of Total and Different Words . . . . . . .
150
8.4
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
153
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
154
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
155
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
157
Contents
ix

Chapter 1
Introduction
1.1
Background
Language is a unique hallmark of human beings among all species in nature, which is
developed by creation, acquisition, and maintenance. Humans use language to com-
municate in their society. With language, humans are able to exchange information
and to express ideas, which in many ways have totally changed our lives and the
social world [1].
To study the evolution of language, one of the most important scientiﬁc methods is
to simulate the so-called language game. In a language game, a simple mathematical
model is used to mimic the process of linguistic pattern formation, where a population
of humans (referred to as agents) is involved as the main body of the game [2].
Consensus among all agents in the population on a new word is of extreme importance
as a stepping stone towards the linguistic pattern formation [3, 4].
With the rapid development of the Internet and communication technologies,
people are much better connected to each other through peer-to-peer networks today,
whereas a much greater population size and a much faster communication speed
can be reached for a spontaneous global consensus via local information exchanges.
On the other hand, novel words, abbreviations and phrases are emerging very often,
either being accepted or discarded by the majority of the population eventually.
Recently, for the case of acceptance the phenomenon of spontaneous consensus has
been extensively discussed in various ﬁelds, ranging from linguistics [5, 6], biology
[7], social science [8–14], to artiﬁcial intelligence [15, 16]. The underlying principle
of such self-organized consensus in large populations has attracted growing research
interest from various scientiﬁc communities.
In social science studies, statistical physics is deemed important for understanding
various collective social behaviors, such as the origin and the evolution of human lan-
guages, opinion and convention formation, and the dynamics of evolutionary games
[17–20]. One representative example is that a collective agreement on naming an
object observed could be reached via local communications among multiple agents
without their global coordination [21–23]. Early studies have mainly focused on
© Springer Nature Switzerland AG 2019
G. Chen and Y. Lou, Naming Game, Emergence, Complexity and Computation 34,
https://doi.org/10.1007/978-3-030-05243-0_1
1

2
1
Introduction
the cases where agents either are able to interact with all others or occupy all node
positions of a regularly-structured network, thus the underlying networks employed
in those investigations are either fully-connected networks or regular lattices. These
however are not realistic in reﬂecting the real-world situation of human population
distributions. Recently, due to the rapid development of network science, a notion
commonly known as complex networks, more comprehensive relationships among
agents can be described and characterized by employing different underlying con-
nection topologies. Some nontrivial topological properties such as small-world and
scale-free are commonly observed in many real-world networks [24–26]. Conse-
quently, understanding the collective social behaviors over these typical topologies
has been considered an important task in many interdisciplinary ﬁelds in recent
years [27].
A typical language game model is commonly known as the naming game model.
The naming game model, along with several variants, is a simulation-based numerical
study exploring the emergence and evolution of shared information in a population
of communicating agents. The shared information could be, for example, a set of
emerging names for an object observed by the population, which could also be some
social conventions, ideas and knowledge, etc. The population of agents are connected
in a certain communication topology, thus each agent can be considered as a node in
the underlying communication network, with the mutual interaction or acquaintance
between two connected nodes characterized by an edge. As such, naming game is
formulated as a process on a graph and can be studied using tools from the graph
theory in mathematics.
The so-called minimal naming game, introduced in [15, 28], starts from a popu-
lation of agents with empty memories, connected in a certain topology. After imple-
menting some simple protocols of game rules, the model facilitates all agents to
achieve consensus on the name of an observed but unknown object through con-
versations among the agents. This is an information interaction-diffusion process.
Speciﬁcally, a pair of neighboring agents are chosen at random, one as speaker and
the other as hearer, for conversation about the observed object. Initially, if the speaker
has an empty memory, he would generate a new word from a vocabulary and then
transmit it to the hearer as the name of the object. But, if the speaker already had
some words in his memory, he would randomly choose a word from memory to
tell the hearer. If the transmitting word also exists in the hearer’s memory, then the
two agents reach consensus thereby only that word would be kept by both agents;
whereas if the hearer did not have the transmitting word in his memory, then this
conversation fails, so the hearer will learn that word and add it to his memory. This
process continues until ﬁnal convergence to a single word in the whole population,
or eventually fails to succeed after a sufﬁciently long time of communications.
The minimal naming game has been extensively studied and used for analyzing
behavioral consensus problems, such as language evolution [5, 6], opinion spreading
or negotiation [13, 29, 30], cultural development [8, 9], and community formation
[31–33].
The minimal naming game model, likewise its variants, describes linguistic con-
ventions via the above-described pair-wise local interactions among all agents in a

1.1 Background
3
population [16, 34]. Consensus on a linguistic term emerges through local interac-
tion between each pair of agents, so the topology of their communication network
plays a central role in both modeling and processing [35]. It has been shown that,
compared to regular lattices, the Erdös–Rényi (ER) random-graph network [36],
Watts–Strogatz (WS) small-world network [37], and Barabási–Albert (BA) scale-
free network [38] models signiﬁcantly affect the achievements of such linguistic
consensus [39–41]. Based on the aforementioned three representative network mod-
els, many studies of the naming game and its variants have been carried out in the
last two decades, for instances, on the ER random-graph networks [35, 39, 40, 42],
WS small-world networks [29, 30, 35, 40, 42–44] and BA scale-free networks [30,
39, 41, 45]. It has also been shown that the topological features of the network, such
as node degree, clustering coefﬁcient, and path distance, have signiﬁcant impacts
on consensus [39–41], revealing that higher degrees, lower clustering and shorter
distance usually promote network consensus.
Besides the network topology, agent’s memory size is another important factor
in naming game consensus [46]. In the above studies, and in many others to be
introduced below, each agent has an inﬁnite size of memory. A naming game model
where each agent has a ﬁnite size of memory is introduced in [42]. In the latter
model, when the hearer received a new word which is not in his memory but his
memory is already full of some other words, he will randomly discard one old word,
or overﬂow the newly received one. In this setting, the naming game processing
behavior turns out to be quite different from that with inﬁnite memories. Also, the
case when memory loss happens to some agents is discussed in [47], where each
agent has inﬁnite capacity of memory, but would loss some word(s) at random at
each time step.
In [48], a network with adaptive connecting weights is considered, where the
weightsdependonthesuccessrateofpastcommunications.In[49],anextendednam-
ing game with biased assimilation is considered, where a hearer accepts a received
word with a predeﬁned probability even the word is not in his memory. The work
in [49] also reveals that, by rewiring some edges in the network, the naming game
convergence process can be greatly accelerated. Moreover, a probability-controlled
model is proposed in [50], where the threshold of reaching consensus is speciﬁed.
A model with zealots (agents who are committed on one of the binary opinions) is
introduced in [51], for which the conditions under which an opinion would win the
majority are derived.
By combining complex networks and social dynamics, the naming game theory
provides an effective approach to studying self-organized consensus, which involves
some important issues in mathematical modeling and computer simulation. To ana-
lyze the role that each agent plays in the naming game process, a modiﬁed model
with weighted words is proposed in [45]. In this model, a tunable parameter based
on the network connectivity is introduced to govern the word weights, and an opti-
mal value of the parameter is found leading to the fastest convergence. It shows that
certain hub nodes in a network favor the achievement of consensus. A negotiation
strategy is proposed in [30, 44] to emphasize the role of geography on the dynamics
of the naming game, which depends upon the geographical distance between two

4
1
Introduction
agents, characterizing the correlation between the interaction strength and the geo-
graphical distance. A reputation parameter is introduced in [52] for a modiﬁed failure
interaction process; that is, if the reputation of the speaker is less than that of the
hearer at the step of failure, the speaker invents a new word taken from a vocabulary
but meanwhile decreases its reputation to a certain ﬁxed degree, while the hearer
does nothing. In [53], the success interaction is modiﬁed, such that only the speaker
(SO-NG) or only the hearer (HO-NG) performs the update of memory. In another
model [54], overhearing can take place by multiple overhearers, which performs the
HO-NG at success, but the speaker still interacts only with the original hearer. More
recently, naming game with multiple hearers (MHNG) is investigated in [43], where
one speaker interacts with multiple hearers simultaneously.
Many naming game models emphasize on the consensus of a word in a vocabulary
among the agents [29, 55–59]. Since vocabulary itself has no other meanings but is
related to the observed objects, it is excluded from some naming game models where
consensus is achieved through a learning process that solely relies on the interac-
tions among the agents [57]. Some more general settings have also been studied,
in which the agents possess special propensities such as commitment [56, 60–62]
and stubbornness [55, 58, 59], showing that the learning behaviors of these agents
can signiﬁcantly affect the consensus dynamics over the whole population. Rather
unusually, global consensus is not expected as the only outcome in the ﬂouted naming
game model [63], where either contention or convention can be formed, depending
on a basic asymmetry on the cognitive reward of two opposing ideas.
1.2
Development Towards Applications
The naming game model and its variants have been applied to some real-world
applications. Agents with artiﬁcial intelligence are able to learn an object, not only its
name but also its contents and features. This development is moving towards human
intelligence and cognitive ability in categorizing objects based on their contents and
features.
Various naming game models are employed for community detection in complex
networks [64], where three locally pair-wise interaction features, i.e., trust, uncer-
tainty and opinion preference, are abstracted from data by community detection
methods. A naming game model with adaptive weights is employed in [48] to study
the inﬂuence of trust. A naming game model with uncertain local exploration factor
is discussed in [65], and a naming game model with secondary memory is adopted in
[66] for studying opinion preference, showing how a social communication model
reveals the community structure in an emerging fashion.
Collective game model is also employed for categorization. For example, dis-
crimination and guessing games are proposed in [67] to accomplish the task of
categorization. In a discrimination game, the speaker is trained with ground truth so
that he can relate an object to the actual category. In a guessing game, which is similar
to a typical naming game except that a category rather than an object is named, the

1.2 Development Towards Applications
5
hearer not only acquires the name but also updates his classiﬁer for the category as
instructed by the speaker. Based on these two types of games, the problem of color
categorization is studied in [67], and a category game model is developed in [14]. In
this category game, a pair of objects are presented to a speaker and a hearer simul-
taneously, from which the target topic (the object to be learned) is selected by the
speaker. If both objects are distinguishable to the speaker, one is randomly chosen as
the target topic; otherwise, the speaker must discriminate the two objects by creating
a new boundary between them before selecting one as the target topic. The inter-
action between the speaker and the hearer then makes it feasible to learn and name
the target topic. In the collective game model, some factors such as language aging
[68], persistence [69] and individual biases [70] can be studied. In the real world,
it often occurs that a population can reach an agreement without being given the
ground truth, and this has consequences. For example, different languages can give
different color categorizations, and consensus of opinions among practitioners in a
ﬁnancial market can trigger a herding behavior that leads to the fat-tail phenomenon
seen in the distributions of prices and returns [71].
In case that the agents have a purpose of learning or performance evaluation,
a naming game model can be used as a multi-agent learning system. A domain
learning model is proposed in [72], where the population of agents work together to
accomplish the task of color categorization through eliminating prerequisites in the
learning process. The color perception process follows an algorithm based on the
majority rule. All agents are unaware of the task and thus their communications and
learning are self-motivated. There are external evaluations and adjustments assisting
in the achievement of the goal. During the color categorization process, if two colors
are as close to each other as non-differentiable, then they would be merged into the
same category. The distance of two colors is measured by their difference calculated
by the CIE2000 standard [73, 74]. The external evaluations and adjustments do not
affect agents’ communications and learning. Therefore, the color categorization task
is achieved via unsupervised social interactions.
The likelihood category game model is proposed in [75]. A population of self-
organized agents deﬁnes the category based on the acquired information and uses
a likelihood estimation method to classify the objects into different categories. The
transmitting information in this model is neither simple words nor absolute results,
but rather, is attached with agents’ subjective perceptions. Agents are able to classify
objects in terms of distinct likelihoods from their learned knowledge, and also their
knowledge can be updated when new information is learned. A trade-off between
learning knowledge and reaching local consensus is introduced into this model. By
nature, it is a naming game model with more intelligent agents, and it can be applied
to mode complicated scenarios.

6
1
Introduction
1.3
Abbreviations
All the abbreviations together with their full names used in the book (especially in
tables and ﬁgures) are listed as follows:
AS - Autonomous System
BA - Barabási–Albert
CW - Candidate Word
ER - Erdös–Rényi
FMNG - Finite-Memory Naming Game
HO-NG - Hearer-Only Naming Game
LM - Length of Memory
MHNG - Naming Game with Multiple Hearers
MLNG - Multi-Language Naming Game
MLW - Multi-Local-World
MNDW - Maximum Number of Different Words
MNTW - Maximum Number of Total Words
MWNG - Multi-Word Naming Game
NG - Naming Game
NGG - Naming Game in Groups
NGLE - Naming Game with Learning Errors
NH - Number of Hearers
NW-SW - Newman–Watts Small-World
RG - Random-Graph
RTM - Random Triangle Model
SF - Scale-Free
SO-NG - Speaker-Only Naming Game
SW - Small-World
SWNG - Single-Word Naming Games
WS - Watts–Strogatz
WS-SW - Watts–Strogatz Small-World
1.4
Layout of the Monograph
The book is organized as follows: Chap.2 presents preliminaries for both complex
networks and the minimal naming game model. Several versions of the naming game
models are studied through Chaps.3–8.
An overview of the book is shown in Fig. 1.1. Speciﬁcally, Chap.2 provides two
preliminaries, a brief review of the notion of complex networks and a brief descrip-
tion of the minimal naming game model, where every agent has an inﬁnite capacity
of memory. Chapter 3 introduces the ﬁrst variant of the minimal naming game model,

1.4 Layout of the Monograph
7
NG models
Chapter 1 – introduction
Chapter 2 – preliminaries for 
complex networks and naming 
game
Chapter 9 - conclusions
Chapter 7 – communicating in 
sentences
Chapter 6 – on multi-community 
networks
Chapter 5 – with learning errors 
in communications
Chapter 4 – in group discussions
Chapter 8 – communicating in 
different languages
Chapter 3 – finite memory size 
Fig. 1.1 An overview of the book
in which every agent has a ﬁnite size of memory therefore is unable to remember too
many words received through the gaming process. Chapter 4 introduces two other
naming game models, where agent communications are performed group-wise. The
ﬁrst is a group-broadcasting model, which has one speaker and multiple hearers,
while the second is a group-discussion model, where all the participating agents are
both speaker and hearer simultaneously. Chapter 5 studies the case when there are
learning errors during the communication process among agents. Chapter 6 puts the
naming gaming process onto a set of tunable multi-community networks. In Chap.7,
agents attempt to communicate with sentences, rather than single words; while in
Chap.8, agents communicate by transmitting a single word but multiple languages
are involved. Finally, conclusions are drawn in the last chapter with a research out-
look.
References
1. M.A. Nowak, Evolutionary biology of language. Philos. Trans. Royal Soc. Lond. B: Bio Sci.
355(1403), 1615–1622 (2000)
2. D.C. Krakauer, Robustness in biological systems: a provisional taxonomy, Complex Systems
Science in Biomedicine (Springer, Berlin, 2006), pp. 183–205
3. S. Edelman, B. Pedersen, Linguistic evolution through language acquisition: formal and com-
putational models. J. Linguist (2004)
4. J.R. Hurford, M. Studdert-Kennedy, C. Knight, Approaches to the Evolution of Language:
Social and Cognitive Bases (Cambridge University Press, Cambridge, 1998)
5. V. Loreto, L. Steels, Social dynamics: emergence of language. Nat. Phys. 3(11), 758–760
(2007)

8
1
Introduction
6. L. Steels, M. Loetzsch, The grounded naming game. Exp. Cult. Lang. Evol. 3, 41–59 (2012)
7. S.A. Rands, G. Cowlishaw, R.A. Pettifor, J.M. Rowcliffe, R.A. Johnstone, Spontaneous emer-
gence of leaders and followers in foraging pairs. Nature 423(6938), 432–434 (2003)
8. A. Baronchelli, T. Gong, A. Puglisi, V. Loreto, Modeling the emergence of universality in color
naming patterns. Proc. Natl. Acad. Sci. USA 107(6), 2403–2407 (2010)
9. A. Baronchelli, V. Loreto, L. Steels, In-depth analysis of the naming game dynamics: the
homogeneous mixing case. Int. J. Mod. Phys. C 19, 785–812 (2008). https://doi.org/10.1142/
S0129183108012522
10. C. Castellano, S. Fortunato, V. Loreto, Statistical physics of social dynamics. Rev. Mod. Phys.
81(2), 591–646 (2009)
11. F. Fu, L. Wang, Coevolutionary dynamics of opinions and networks: from diversity to unifor-
mity. Phys. Rev. E 78(1), 016104 (2008)
12. Q. Liu, X.F. Wang, Opinion dynamics with similarity-based random neighbors. Sci. Rep. 3,
2968 (2013)
13. S.K. Maity, T.V. Manoj, A. Mukherjee, Opinion formation in time-varying social networks:
the case of the naming game. Phys. Rev. E 86(3), 036110 (2012)
14. A. Puglisi, A. Baronchelli, V. Loreto, Cultural route to the emergence of linguistic categories.
Proc. Natl. Acad. Sci. USA 105(23), 7936–7940 (2008)
15. A. Baronchelli, M. Felici, V. Loreto, E. Caglioti, L. Steels, Sharp transition towards shared
vocabularies in multi-agent systems. J. Stat. Mech.: Theory Exp. 6, P06014 (2006). https://doi.
org/10.1088/1742-5468/2006/06/P06014
16. L. Steels, Self-organizing vocabularies. Artif. Life 2(3), 319–332 (1995)
17. A.M. Colman, Game Theory and its Applications: In the Social and Biological Sciences (Psy-
chology Press, Hove, 2013)
18. P. Garrido, J. Maroo, M. Muñoz, Modeling Cooperative Behavior in the Social Sciences (AIP,
College Park, 2005). ISBN-10:0735402663
19. D. Stauffer, S.M.M. De Oliveira, P.M.C. De Oliveira, J.S. De Sá Martins, Biology, Sociology,
Geology by Computational Physicists (Elsevier, New York, 2006). ISBN-10:0444521461
20. W. Weidlich, Sociodynamics: A Systematic Approach to Mathematical Modelling in the Social
Sciences (Courier Corporation, Courier Corporation, 2006)
21. D.J. Barr, Establishing conventional communication systems: is common knowledge neces-
sary? Cognit. Sci. 28(6), 937–962 (2004). https://doi.org/10.1016/j.cogsci.2004.07.002
22. S. Kirby, Natural language from artiﬁcial life. Artif. Life 8(2), 185–215 (2002). https://doi.org/
10.1162/106454602320184248
23. L. Steels, The synthetic modeling of language origins. Evol. Commun. 1(1), 1–34 (1997).
https://doi.org/10.1075/eoc.1.1.02ste
24. R. Albert, A.L. Barabási, Statistical mechanics of complex networks. Rev. Mod. Phys. 74(1),
47 (2002). https://doi.org/10.1103/RevModPhys.74.47
25. S.N. Dorogovtsev, J.F.F. Mendes, Evolution of networks. Adv. Phys. 51(4), 1079–1187 (2002).
https://doi.org/10.1080/00018730110112519
26. D.J. Watts, The “new” science of networks. Annu. Rev. Soc. 30, 243–270 (2004). https://doi.
org/10.1146/annurev.soc.30.020404.104342
27. M.E.J. Newman, The structure and function of complex networks. SIAM Rev. 45(2), 167–256
(2003). https://doi.org/10.1137/S003614450342480
28. A. Baronchelli, A gentle introduction to the minimal naming game. Belg. J. Linguist 30(1),
171–192 (2016)
29. R.R. Liu, W.X. Wang, Y.C. Lai, G.R. Chen, B.H. Wang, Optimal convergence in naming game
with geography-based negotiation on small-world networks. Phys. Lett. A 375(3), 363–367
(2011)
30. H.X. Yang, W.X. Wang, B.H. Wang, Asymmetric negotiation in structured language games.
Phys. Rev. E 77(2), 027103 (2008)
31. Q.Lu,G.Korniss,B.K.Szymanski,Thenaminggameinsocialnetworks:communityformation
and consensus engineering. J. Econ. Interact Coord 4(2), 221–235 (2009). https://doi.org/10.
1007/s11403-009-0057-7

References
9
32. J. Xie, S. Sreenivasan, G. Korniss, W. Zhang, C. Lim, B.K. Szymanski, Social consensus
through the inﬂuence of committed minorities. Phys. Rev. E 84(1), 011130 (2011)
33. W. Zhang, C.C. Lim, Noise in naming games, partial synchronization and community detection
in social networks (2010), arXiv:1008.4115
34. L. Steels, A self-organizing spatial vocabulary. Artif. Life 2(3), 319–332 (1995)
35. A. Baronchelli, L. Dall’Asta, A. Barrat, V. Loreto, The role of topology on the dynamics of
the naming game. Eur. Phys. J. Spec. Topic 143(1), 233–235 (2007). https://doi.org/10.1140/
epjst/e2007-00092-0
36. P. Erdös, A. Rényi, On the strength of connectedness of a random graph. Acta Mathematica
Academiae Scientiarum Hungarica 12(1–2), 261–267 (1964)
37. D.J. Watts, S.H. Strogatz, Collective dynamics of ‘small-world’ networks. Nature 393(6684),
440–442 (1998). https://doi.org/10.1038/30918
38. A.L. Barabási, R. Albert, Emergence of scaling in random networks. Science 286(5439), 509–
512 (1999)
39. A.Barrat,A.Baronchelli,L.Dall’Asta,V.Loreto,Agreementdynamicsoninteractionnetworks
with diverse topologies. Chaos 17(2), 026111 (2007)
40. L. Dall’Asta, A. Baronchelli, A. Barrat, V. Loreto, Agreement dynamics on small-world net-
works. EPL (Europhys Lett) 73(6), 969 (2006). https://doi.org/10.1209/epl/i2005-10481-7
41. L.Dall’Asta,A.Baronchelli,A.Barrat,V.Loreto,Nonequilibriumdynamicsoflanguagegames
on complex networks. Phys. Rev. E 74(3), 036105 (2006). https://doi.org/10.1103/PhysRevE.
74.036105
42. W.X. Wang, B.Y. Lin, C.L. Tang, G.R. Chen, Agreement dynamics of ﬁnite-memory language
gamesonnetworks.Eur.Phys.J.B60(4),529–536(2007).https://doi.org/10.1140/epjb/e2008-
00013-5
43. B. Li, G.R. Chen, T.W.S. Chow, Naming game with multiple hearers. Commun. Nonlinear Sci.
Numer. Simul. 18, 1214–1228 (2013). https://doi.org/10.1016/j.cnsns.2012.09.022
44. R.R. Liu, C.X. Jia, H.X. Yang, B.H. Wang, Naming game on small-world networks with
geographical effects. Phys. A 388, 3615–3620 (2009). https://doi.org/10.1016/j.physa.2009.
05.007
45. C.L. Tang, B.Y. Lin, W.X. Wang, M.B. Hu, B.H. Wang, Role of connectivity-induced weighted
words in language games. Phys. Rev. E 75(2), 027101 (2007)
46. L. Huang, K. Park, Y.C. Lai, Information propagation on modular networks. Phys. Rev. E 73(3),
035103 (2006)
47. G. Fu, Y. Cai, W. Zhang, Analysis of naming game over networks in the presence of memory
loss. Phys. A 479, 350–361 (2017)
48. D. Lipowska, A. Lipowski, Naming game on adaptive weighted networks. Artif. Life 18(3),
311–323 (2012)
49. A.M. Thompson, B.K. Szymanski, C.C. Lim, Propensity and stickiness in the naming game:
tipping fractions of minorities. Phys. Rev. E 90(4), 042809 (2014)
50. F. Palombi, S. Toti, Topological aspects of the multi-language phases of the naming game on
community-based networks. Games 8(1), 12 (2017)
51. G. Verma, A. Swami, K. Chan, The impact of competing zealots on opinion dynamics. Phys.
A 395, 310–331 (2014)
52. E. Brigatti, Consequence of reputation in an open-ended naming game. Phys. Rev. E 78(4),
046108 (2008)
53. A. Baronchelli, Role of feedback and broadcasting in the naming game. Phys. Rev. E 83,
046103 (2011). https://doi.org/10.1103/PhysRevE.83.046103
54. S.K. Maity, A. Mukherjee, F. Tria, V. Loreto, Emergence of fast agreement in an overhearing
population: the case of the naming game. EPL (Europhys Lett) 101(6), 68004 (2013)
55. J. Gao, B. Li, G. Schoenebeck, F.Y. Yu, Engineering agreement: the naming game with asym-
metric and heterogeneous agents, in AAAI (2017), pp. 537–543
56. D. Mistry, Q. Zhang, N. Perra, A. Baronchelli, Committed activists and the reshaping of status-
quo social consensus. Phys. Rev. E 92(4), 042805 (2015)

10
1
Introduction
57. L. Steels, Experiments in Cultural Language Evolution (John Benjamins Publishing, Amster-
dam, 2012)
58. Y. Treitman, C. Lim, W. Zhang, A. Thompson, Naming game with greater stubbornness and
unilateral zealots, in IEEE Netw Sci Workshop (NSW) (IEEE, 2013), pp. 126–130
59. C.W. Wu, Can stubbornness or gullibility lead to faster consensus? A study of various strategies
for reaching consensus in a model of the naming game, in IEEE International Symposium on
Circuits and Systems (ISCAS) (IEEE, 2011), pp. 2111–2114
60. A. Baronchelli, L. Dall’Asta, A. Barrat, V. Loreto, Nonequilibrium phase transition in negoti-
ation dynamics. Phys. Rev. E 76(5), 051102 (2007)
61. G. Fu, W. Zhang, Naming game with biased assimilation over adaptive networks. Phys. A 490,
260–268 (2018)
62. X. Niu, C. Doyle, G. Korniss, B.K. Szymanski, The impact of variable commitment in the
naming game on consensus formation. Sci. Rep. 7, 41750 (2017)
63. H.P.deVladar,Theﬂoutednaminggame:contentionsandconventionsinculture,inGECCO’18
(ACM, 2018), pp. 1307–1312
64. T.G. Uzun, C.H.C. Ribeiro, Detection of communities with naming game-based methods. PLoS
ONE 12(8), e0182737 (2017). https://doi.org/10.1371/journal.pone.0182737
65. T.G. Uzun, C.H.C. Ribeiro, Incorporation of social features in the naming game for community
detection, in Complex Networks VII (Springer, Berlin, 2016), pp. 119–131
66. T.G. Uzun, C.H.C. Ribeiro, A naming game with secondary memory for community detection,
in 2015 9th International Conference on Complex, Intelligent, and Software Intensive Systems
(CISIS) (IEEE, 2015), pp. 156–163. https://doi.org/10.1109/CISIS.2015.21
67. L. Steels, T. Belpaeme, Coordinating perceptually grounded categories through language: a
case study for colour. Behav. Brain Sci. 28(4), 469–488 (2005)
68. A. Mukherjee, F. Tria, A. Baronchelli, A. Puglisi, V. Loreto, Aging in language dynamics.
PLoS One 6(2), e16677 (2011)
69. P. Cui, M. Tang, Z.X. Wu, Message spreading in networks with stickiness and persistence:
Large clustering does not always facilitate large-scale diffusion. Sci. Rep. 4, 6303 (2014)
70. A. Baronchelli, V. Loreto, A. Puglisi, Individual biases, cultural evolution, and the statistical
nature of language universals: the case of colour naming systems. PLoS One 10(5), e0125019
(2015)
71. V.M. Eguiluz, M.G. Zimmermann, Transmission of information and herd behavior: An appli-
cation to ﬁnancial markets. Phys. Rev. Lett. 85(26), 5659 (2000)
72. D.J. Li, Z.Y. Fan, W.K.S. Tang, Domain learning naming game for color categorization. PLoS
ONE 12(11), e0188164 (2017). https://doi.org/10.1371/journal.pone.0188164
73. M.R. Luo, G.H. Cui, B. Rigg, The development of the CIE 2000 colour-difference formula:
CIEDE2000. Color Res. Appl. 26(5), 340–350 (2001). https://doi.org/10.1002/col.1049
74. R.D. Paravina, M. Kimura, J.M. Powers, Evaluation of polymerization-dependent changes
in color and translucency of resin composites using two formulae. Odontology 93(1), 46–51
(2005). https://doi.org/10.1007/s10266-005-0048-7
75. Z.Y. Fan, Y.C. Lai, W.K.S. Tang, Consensus in complex networks: The role of learning and
knowledge (2018), arXiv:1809.00297

Chapter 2
Preliminaries
2.1
Complex Networks
A complex network is a graph with nontrivial topological features that do not occur
in “simple” (“regular”) networks such as chains, trees, lattices, or fully-connected
graphs (also called complete graphs), but often occur in some irregular complex
forms from modeling real-world systems [1].
Typical undirected complex network models include Erdös-Rényi (ER) random-
graph networks, Watts–Strogatz (WS) small-world networks, and Barabási–Albert
(BA) scale-free networks, as visualized by Fig. 2.1. More detailed descriptions of
these three typical models are introduced in the following three subsections, respec-
tively, followed by an extension of the BA model, named the multi-local-world model
[2, 3].
Throughout the book, the terms agent and node are used interchangeably to rep-
resent an individual in a population, and the terms graph and network are used
interchangeably for verbal convenience.
2.1.1
ER Random-Graph Network Model
An ER random-graph network is generated as follows [4]:
(i) Initialization: Start with N isolated nodes.
(ii) Picks up all possible pairs of nodes, once and once only, from the N given nodes,
and connect each pair of nodes by an edge with probability p ∈[0, 1], referred
to as the connection probability.
Statistically, the expectation or mean of the number of edges of such a network is
pN(N −1)/2. Generally, the larger the p is, the denser the resultant network will
be. For p = 0, the initially isolate nodes remain to be isolated; for p = 1, one obtains
a fully-connected network.
© Springer Nature Switzerland AG 2019
G. Chen and Y. Lou, Naming Game, Emergence, Complexity and Computation 34,
https://doi.org/10.1007/978-3-030-05243-0_2
11

12
2
Preliminaries
Fig. 2.1 Examples of three
typical network topologies:
a Erdös-Rényi (ER)
random-graph network;
b Watts–Strogatz (WS)
small-world network;
c Barabási–Albert (BA)
scale-free network. All have
30 nodes, arranged in a ring
-
m
o
d
n
a
r
)
R
E
(
i
y
n
e´
R
-s
o¨
d
r
E
)
a
(
graph network, with N
= 30,
⟨k⟩= 5.8
(b) Watts-Strogatz (WS) small-
world network, with N
= 30,
⟨k⟩= 6.0
-
ela
c
s
)
A
B
(
t
r
e
b
l
A
-is
a´
b
a
r
a
B
)
c
(
free network, with N = 30, ⟨k⟩=
5.7

2.1 Complex Networks
13
Note that the above process of generating a random-graph network will not intro-
duce multiple edges, nor self-loop at any single node, therefore the results are all
simple graphs.
ER random-graph networks have the following basic structural properties, where
k is the node-degree variable (i.e., the number of connections of the node):
In an ER random-graph network, the average degree of nodes is
⟨k⟩= p(N −1) ≈pN ;
(2.1)
the average path length, or distance, is
⟨apl⟩≈ln N/ ln⟨k⟩;
(2.2)
and the average clustering coefﬁcient is
⟨cc⟩≈⟨k⟩/N = p .
(2.3)
It is a basic feature of the ER random-graph networks that they have relatively
short average path length given by formula (2.2) in a logarithmic scale in the order
of O(ln N). However, as seen from formula (2.3), the clustering coefﬁcient of an ER
random-graph network is fairly small: ⟨cc⟩≪1.
For ER random-graph networks, their node-degrees follow a Poisson distribution:
P(k) = μk
k! e−μ ,
(2.4)
where μ is a constant parameter, which is the expectation value, μ = pN = ⟨k⟩. For
more details, see [1].
2.1.2
WS Small-World Network Model
A WS small-world network is generated as follows [5]:
(i) Start from a ring-shaped network with N nodes, in which each node is connected
to its 2K neighbors symmetrically, K nodes on each side, where K > 0 is a small
integer.
(ii) For every pair of connected nodes in the ring-shaped network, rewire the edge
in such a way that the beginning end of the edge is kept (i.e., this edge-end
connection is unchanged) but the other end is disconnected with probability p,
referredtoastherewiringprobability,andthenisreconnectedtoanoderandomly
chosen from the network. This rewiring operation is performed edge by edge
on the original ring-shaped network, once and once only, either clockwise or
counterclockwise until it returns to the starting node.

14
2
Preliminaries
Here, at Step (ii), the random rewiring operations follow a uniform distribution in
the sense that every node has an equal probability to be picked, and self-loops and
multiple edges are avoided (ignored).
Clearly, Step (ii) will introduce some long-range connections, which are important
in that it signiﬁcantly shortens the average path length of the whole network, enabling
it to have some small-world properties, especially with a short average path length
⟨apl⟩= 2N
K
f (2N P/K) ,
(2.5)
with
f (x) =
c,
x ≪1
ln x/x, x ≫1 (typically, c = 1/4) ,
while having a large clustering coefﬁcient
⟨cc⟩= 3(K −1)
2(2K −1) (1 −p)3 .
(2.6)
The node-degree distribution of the WS small-world networks are nearly Poisson,
similar to the ER random-graph networks. For more detailed, see [1].
2.1.3
BA Scale-Free Network Model
A BA scale-free network is generated as follows [6]:
(i) Growth: Starting from a connected network of small size m0 ≥1, introduce one
new node to the existing network each time, and this new node is connected to
m existing nodes in the network simultaneously, where 1 ≤m ≤m0.
(ii) Preferential Attachment: The above-referred incoming new node is simultane-
ously connected to each of the m existing nodes, according to the following
probability: for node i of degree ki,
Πi =
ki
N
j=1 k j
.
(2.7)
Clearly, at the tth step of the node-adding process, the total number of existing
nodes is N = m0 + t, t = 0, 1, 2, . . .. Thus, after t steps, the network will have a total
of N = m0 + t nodes and M = mt + m∗edges, where m∗is the number of initial
edges. Thus, if the initial network is fully connected then m∗= m0(m0 −1)/2.
The average path length of the BA scale-free network is
⟨apl⟩≈
ln N
ln ln N .
(2.8)

2.1 Complex Networks
15
The average clustering coefﬁcient of the BA scale-free network model is
⟨cc⟩= m2(m + 1)2
4(m −1)

ln
m + 1
m

−1
m
 (ln t)2
t
.
(2.9)
The average node degree of the BA network is approximately equal to
P(k) ≈2m2k−3 ,
(2.10)
which is in a power-law form.
Since the node-degree distribution (2.10) is independent of the network scale
(sizes N and M), it is referred to as scale-free networks. Mathematically, the scale-
free property of a function f (·) is
f (ax) = bf (x) ,
where a and b are two constants. In fact, if f satisﬁes f (1) f ′(1) ̸= 0 then it is
uniquely determined by a power-law, f (x) = f (1)x−γ where the constant γ =
−f ′(1)/f (1). It is easy to verify that the distribution function (2.10) satisﬁes this
property, therefore is of scale-free.
In a large-scale network with a power-law degree distribution, most nodes have
very low degrees (i.e., with very few edges) and yet there are a few nodes having very
high degrees (i.e., with many edges). This kind of complex networks is referred to as
heterogeneous networks, where the high-degree nodes are called hubs. In contrast,
the ER random-graph networks and WS small-world networks are homogeneous
networks. For more detailed, see [1].
2.1.4
Multi-Local-World Network Model
In generating scale-free networks by means of preferential attachment, as described
above, the mechanism responsible for the emergence of the scale-free property has
a global feature; namely, the probability that an existing node receives a new edge is
with respect to the total number of existing edges in the whole network. However,
this is seldom the case in real life. For example, in the Internet, due to the technical
and economical limitations, a router in one autonomous system (AS) usually favors
a shortest-path connection within the same AS when placing new edges. Therefore,
the Internet can be divided into many sub-networks, and nodes in the same sub-
network are relatively densely connected while nodes in different sub-networks have
very few connections. This prominent localization effect motivates the following
more comprehensive model, named the Multi-Local-World (MLW) [2, 3] network
model is also considered as the underlying network of naming game communications.
An illustrative example of MLW network is given in Fig.2.2. Two generated MLW

16
2
Preliminaries
Fig. 2.2 An example of
MLW network
initial node
added node
initial link
added link within an MLW
added link between two MLWs
removed link
network instances, with different number and size of local communities, are shown in
Fig. 6.1.
The detailed generation algorithm of MLW networks is given in Sect.6.2.
It can be proved that this MLW network model is also a scale-free model with a
power-law node-degree distribution and yet with many communities (local worlds)
in the network. For more detailed, see [1].
2.2
Naming Game
The framework of a basic naming game model is shown in Fig. 2.3. A naming game
model can conceptually be considered as a social dynamics process, or a social
system, with input and output where the ‘Naming Game’ block (main loop) in the
ﬁgure composes of the main body of the system while the system requires a set of
‘inputs’ including: (1) a population of agents, (2) the underlying network topology
that represents the relationships among the agents, (3) an object to be named, and
(4) a large (ﬁnite or inﬁnite) external lexicon for the candidate name of the object.
Assuch,thenaminggameisamulti-agentmodelinwhichagentsperformpairwise
interactions to name an object that some of them observed. There is no central control,
or any other external control, available to assist their global acceptance of a common
name taken from the available lexicon to name the object, referred to as global
consensus. If this process converges, namely if all agents accept the same name in
the end, the game is successful and ﬁnished; otherwise, the game fails and stops.
2.2.1
Naming Game Framework
In the naming game model shown in Fig. 2.3, the population of agents could initially
be either with or without information about the object in their memories. Also,
the memory size of each agent could be ﬁnite or inﬁnite. Other characteristics like
commitment, forgetting, etc., could also be assigned to the agents if it makes sense
to the application. The relationships among agents are organized by an underlying

2.2 Naming Game
17
Fig. 2.3 Framework of the
naming game
Naming Game
 (main loop)
Input: 
1) agents
2) underlying network
3) external lexicon
4) object(s) to be named
terminate ?
perform one-time-step 
naming game
Output:
1) final state
2) converging process
communication network. Each agent is presented by a node in the network, and a pair
ofnodesareconnectedbyanedgeifthetwonodes(agents)areacquaintancesandthey
communicate to each other. Assume that only pair-wise communications can take
place on the network. Thus, only those directly-contented agents can communicate
with each other to exchange information locally. The external lexicon gives a certain
volume of possible names, which could be, for example, a vocabulary with ﬁnitely
many words. The input is an object to be named. The object is a conceptual input,
whether it has a real name or not is not important, but the naming process is the
main issue to be studied. In the naming process, the population of agents invent new
names, then they negotiate pair-wise iteratively, and ﬁnally reach a consent on the
name of the object if the process converges.
Of course, the real-world situations are far more complicated than the above-
described simple process, where there could be many objects to be named simul-
taneously, and different naming processes may have mutual inﬂuences with each
other, etc. For simplicity, it is assumed that the processes of naming different objects
are mutually independent, i.e., when there are multiple objects to be named simul-
taneously, the processes can be simply added up together. Therefore, throughout the
book, only a single object is considered to be named in the games.
In the naming game model shown in Fig. 2.3, the ‘Naming Game’ block is the
main loop. The ‘one-time-step process’ continues iteratively until the pre-set stop
criterion is met. The stop criterion could be: (1) the maximum number of iterations is
completed, or (2) a global consensus state is reached. Upon meeting the criterion, the
main loop is terminated. The one-time-step process is the core of all variants of the
basic naming game model, which may vary from model to model. For example, if the
one-time-step communication is performed group-wise, then the physical meaning
of the model changes to become an extension of the pair-wise communication model.

18
2
Preliminaries
Fig. 2.4 Example of
calculating the three basic
indexes of convergence:
a Given a population of ﬁve
agents, their
acquaintanceships are
denoted by edges. The
number of total words in the
population is 10; the number
of different words is 4.
b Given a time series that
records the local
communication results
(either local failure or local
success), the success rate at
the iteration i is calculated
by the average rate of local
consensus in the last 10
iterations; thus, the success
rate at the iteration i is
4/10 = 0.4
(a)
(b)
The ‘output’ of the naming game includes the ﬁnal state of the population of
agents, and the convergence process if it is needed to be examined. Generally, most
naming game models are able to reach a global consensus state eventually, showing
that all the agents agree to the same word for naming the object, and this converging
state would not change anymore once it is reached.
Throughout the book, the terms convergence and global consensus will be used
interchangeably to describe the ﬁnal state of the population. Also, the term local con-
sensus represents pair-wise (or group-wise) consensus locally or temporarily, which
may change later. The adjective local refers to two neighboring agents, or a small
group of neighboring agents, while global refers to the entire population of agents.
Thus, local consensus and local success are used interchangeably to describe a con-
sensus result of local communications after some iterations. Similarly, local failure
and local learning are used to describe a non-consensus result of local communica-
tions. Also, the terms word and name are used interchangeably, which both represent
a prospective name for the object, and each word/name occupies one memory unit
of an agent in the gaming process.
The convergence process, from (random) initialization to the ﬁnal state (global
consensus), will be recorded for reviewing and analysis. Basically, three important
indexes are recorded, i.e., the number of total words, the number of different words,
andtheconvergencecurveofsuccessrate.Figure2.4showsanexampleofcalculating
these three index values. In Fig. 2.4a, there is a population of ﬁve agents, where agents
#1 and #4 have three words in their memories, agents #2 and #3 have two, and agent

2.2 Naming Game
19
#5 has nothing in his memory. The number of total words is calculated by summing
up the number of words once remembered by all the agents. Thus, in this case, the
number of total words in the population is 10.
As can be seen from Fig. 2.4a, some words appear repeatedly in different agents’
memories; for example, the word ‘apple’ appears in both agent #1 and #3’s memories.
The number of different words is calculated by counting the size of the unique word
list remembered by all the agents. In Fig. 2.4a, the unique word list is {‘apple’,
‘banana’, ‘egg’, ‘tree’}, and thus the number of different words is 4.
In Fig. 2.4b, the results of local communications are logged into a time-series,
which are either a local failure or a local success. The success rate at iteration i is
calculated by the average rate of local consensus within the last 10 iterations from
iteration i −9 to iteration i. As shown in Fig. 2.4b, there are 4 local successes and 6
local failures, and thus the success rate at iteration i is 4/10 = 0.4.
The convergence process of naming game is not deterministic. Randomness
affects: (1) the generation of the underlying network; (2) the way to pick a speaker
and a hearer; and (3) the invention of a new word, when a speaker has nothing in his
memory. Due to randomness, the convergence process may vary from case to case,
so these data should be collected and processed in a statistical manner to ﬁlter out
(or reduce) the inﬂuence of the randomness. In this book, two statistical methods
are used. The ﬁrst method takes the average value of a number of independently
repeated runs, and the second also requires a number of independently repeated runs
but the results are displayed in a boxplot. A boxplot shows the statistical features
more clearly, such as the maximum, minimum, median, etc.
Besides the convergence curves that reﬂect the process behaviors, the index con-
vergence time reﬂects the convergence speed of the model. Generally, convergence
time is deﬁned as the number of iterations (time-steps) required by the population of
agents from initialization to reaching the global consensus state. Alternatively, this
term can be deﬁned as the computational and communication costs required by the
population of agents from initialization to reaching the global consensus state.
The above collective data and information provide meaningful insights to the
naming game model, which are the most valuable for well understanding not only
the naming game model but also the human language emergence and evolution,
especially from a network science perspective.
2.2.2
Minimal Naming Game
The minimal naming game [7] is the bare-bone version of the basic naming game
model. It also follows the framework shown in Fig. 2.3, but with the least additional
operations. In the minimal naming game, the agents are assumed with nothing in their
memories initially, but they all have an inﬁnite capacity of memory. The external
lexicon is also inﬁnite, such that a speaker could generate any name when he has
nothing in memory. Only local pair-wise communication is allowed, which is always
correct without any error or misunderstanding.

20
2
Preliminaries
Fig. 2.5 Flowchart of the
minimal naming game
Start of one 
time step
Speaker randomly 
pick a word w, and 
utters it
Speaker creates a 
word w, saves it in 
memory, and utters it
Hearer learns the 
word w and saves it in 
memory
Hearer receives the 
word w
End of one 
time step
Speaker and 
Hearer local 
consensus
Speaker has 
memory?
Hearer
has w?
yes
no
yes
no
Figure2.5 shows the one-time-step ﬂowchart of the minimal naming game. At the
beginning of each time step, a speaker is chosen globally at random, and then a hearer
is randomly picked locally from all the neighbors of the speaker. This is the direct
strategy [8, 9] of choosing a speaker-hearer pair. In contrast, the inverse strategy
[9] picks a hearer globally and randomly ﬁrst, and then a speaker is picked from
his neighbors. For the direct strategy, speakers are randomly chosen from the entire
population, thus the probability of being a speaker for each agent is the same (due
to the uniform random global selection). However, the inverse strategy gives equal
opportunity to each agent to be a hearer instead. Two strategies have no essential
difference when the underlying network is homogeneous. The difference between
the two strategies emerges when the underlying network is heterogeneous, where
there are many nodes connecting with the hub nodes so that a ﬁrst-picked agent has a
greater chance than the next-picked one if a hub node is involved. Clearly, the inverse
strategy gives higher probability for the hub nodes being speakers. The hub nodes
are more inﬂuential than the smaller ones, so when they have a higher probability
to be a speaker they will facilitate the information propagation speed and range as
well the convergence performance. Note that the speaker-hearer selection strategy
is not the only fact that inﬂuences the convergence speed of the naming game. For

2.2 Naming Game
21
Speaker
right
bite
signal
(a) failure (learning)
(b) success (consensus)
Hearer
bite
cream
Speaker
right
bite
signal
Hearer
bite
cream
signal
Speaker
right
bite
signal
Hearer
bite
light
cream
Speaker
bite
Hearer
bite
Fig. 2.6 Example of one-time-step communication between the speaker and the hearer with a result
of a local failure, b local success. The underlined italic word is the speaker-uttered word
example, if there is some operator that signiﬁcantly drags the convergence speed,
then the inﬂuence caused by the strategy of picking up the speaker and the hearer is
negligible.
As shown in Fig. 2.5, a pair-wise local communication starts immediately after
the speaker-hearer pair is picked. First, the speaker randomly chooses a name from
the vocabulary and tells it to the hearer. If the speaker has nothing in memory (which
happens only when he is in the initial state, or has not had any local communication),
he will randomly pick a name from the external lexicon, remembers it, and then
utters it. This name is denoted by w.
Since local communications are always noise-free and correctly performed,
the hearer receives w immediately, and then checks it throughout his memory. If
the hearer has no such a word w in his memory, then he learns it and saves it into
the memory. This situation is called a local failure (or, learning) step. Otherwise, if
the hearer has a same word w in his memory, then both the speaker and the hearer
clear out their memories except keeping this word w, reaching a local consent. This
situation is called a local success (or local consensus) step.
An example about both local failure and local success is shown in Fig. 2.6. Each
block represents an agent with his inventory. An arrow is used to distinguish the
states of agents’ memories before and after a local communication step.
In Fig. 2.6a, the speaker utters a word ‘signal’ to the hearer, but the hearer does
not have the same word in his memory. This process refers to a local failure (or local
learning), as mentioned above. The hearer will then keep the word ‘signal’ in his
memory.
In Fig. 2.6b, when the speaker utters a word ‘bite’ and the hearer also has the
same word in his memory, the two agents reach local consensus. As can be seen
from the right-hand scene in Fig. 2.6b, both the speaker and the hearer clear up their
previously remembered words but keep the word ‘bite’ alone. As mentioned above,
this scenario is a step of local success.

22
2
Preliminaries
References
1. G.R.Chen,X.F.Wang,X.Li,IntroductiontoComplexNetworks:Models,StructuresandDynam-
ics (High Education Press, Beijing, 2014). ISBN 978-1-118-71811-7
2. G.R. Chen, Z.P. Fan, X. Li, Modelling the complex internet topology, Complex Dynamics
in Communication Networks (Springer, Berlin, 2005), pp. 213–234. https://doi.org/10.1007/
10973509_9
3. Z.P. Fan, G.R. Chen, Y.N. Zhang, A comprehensive multi-local-world model for complex net-
works. Phys. Lett. A 373(18), 1601–1605 (2009). https://doi.org/10.1016/j.physleta.2009.02.
072
4. P. Erdös, A. Rényi, On the strength of connectedness of a random graph. Acta Mathematica
Academiae Scientiarum Hungarica 12(1–2), 261–267 (1964)
5. D.J. Watts, S.H. Strogatz, Collective dynamics of ‘small-world’ networks. Nature 393(6684),
440–442 (1998). https://doi.org/10.1038/30918
6. A.L. Barabási, R. Albert, Emergence of scaling in random networks. Science 286(5439), 509–
512 (1999)
7. A. Baronchelli, A gentle introduction to the minimal naming game. Belg. J. Linguist 30(1),
171–192 (2016)
8. L. Dall’Asta, A. Baronchelli, A. Barrat, V. Loreto, Nonequilibrium dynamics of language games
on complex networks. Phys. Rev. E 74(3), 036105 (2006). https://doi.org/10.1103/PhysRevE.
74.036105
9. R.R. Liu, C.X. Jia, H.X. Yang, B.H. Wang, Naming game on small-world networks with geo-
graphical effects. Phys. A 388, 3615–3620 (2009). https://doi.org/10.1016/j.physa.2009.05.007

Chapter 3
Finite-Memory Naming Game
3.1
Introduction
One crucial assumption on the minimal naming game model is that all agents have
inﬁnite capacity of memory, or practically sufﬁciently large volume of memory, for
storing all words that they learned. However, in reality this may not always be the
case. Information overﬂow due to lack of sufﬁcient memory is common in real-world
systems such as various communication networks. In this chapter, the ﬁnite-memory
naming game (FMNG) model [1] is introduced. Agents in FMNG have ﬁnite volumes
of memory, and the memory size of each agent is determined beforehand. FMNG
takes the bounded rationality of agents into account and mimics the situation that
there are only ﬁnite capability and resource for information storage in the game
system. The process of FMNG works under the same framework as the minimal
naming game, except that each agent has a limited memory capacity.
In FMNG, suppose that each agent has a memory limit, L (a positive integer),
before an agent reaches his memory limit, he would remember everything learned
from his neighboring agents through interactions. This behavior is exactly the same
as it is in the minimal naming game model. However, when an agent has fully used
his memory (i.e., when he has already remembered L different words), he cannot take
(remember)anymorenewword(differentfromthe L wordsthathehadremembered),
causing memory overﬂow. In this case, (at least) one old word must be discarded,
such that his memory can have room to take the new word if he decided to do so.
Information loss would essentially affect the local consensus process, and ultimately
affect the global convergence process. For example, the lost word may be a common
word of some neighboring agent(s), and by discarding it the agent may lose a chance
to consent to this word with his neighbor(s) later on the process. Note that in FMNG,
the memory loss is passively performed due to the limitation of memory size; while
in [2], there proposed a model where agents may forget some words during gaming,
which could be considered an active procedure of memory loss.
© Springer Nature Switzerland AG 2019
G. Chen and Y. Lou, Naming Game, Emergence, Complexity and Computation 34,
https://doi.org/10.1007/978-3-030-05243-0_3
23

24
3
Finite-Memory Naming Game
To introduce the notion of FMNG, Sect.3.2 will present the model, followed by
simulations with comparisons in Sect.3.3. Section3.4 concludes the chapter with
some discussions.
3.2
Finite-Memory Naming Game
Once again, the naming game is played by multiple agents with local interactions,
which is presented by a graph with nodes being the agents and edges representing
their pairwise communications. Thus, the population size of the agents is equal to
the size of the network, namely the number of nodes. Each edge in the underlying
network represents an acquaintanceship of the connected pair of agents. Two agents
can communicate only if they are connected. Each agent is endowed with a memory
(inventory) to store a ﬁnite number of words, referred to as ﬁnite memory. Initially,
the memory of each agent is set to be empty. Considering the memory length, denoted
by L, as a tunable parameter, when L is sufﬁciently large (assuming no information
loss during communications) or inﬁnite, the FMNG is degenerated to the minimal
naming game model discussed in the last chapter. Therefore, FMNG is a natural
extension of the minimal naming game model with the common memory size of the
agents being a ﬁxed positive integer. When this number is relatively small, there is
not enough room to store some new words during the naming process, such that some
words have to be discarded (information loss) during the pair-wise communications.
Compared to the minimal naming game, FMNG has one more parameter, i.e., the
memory length L, for control, thus one more situation to deal with, i.e., when the
memory of an agent is ﬁlled up to reach the limited length L, it needs to decide which
old word(s) should be discarded.
The one-time-step ﬂowchart of FMNG is shown in Fig.3.1. At the beginning
of each time step, a speaker-hearer pair is picked at random, say using the direct
strategy [3, 4]. Here, the focus is on the memory length (or memory limit), other
than the strategy of picking up agents, so only the direct strategy is discussed. The
pair-wise communication process is the same as the minimal naming game. If the
picked speaker has nothing in his memory to describe the object observed, then he
will invent a word arbitrarily, or takes one from the vocabulary; otherwise, if he
already has that particular word in memory, then he will replace a randomly-picked
word from his memory by the new word. Let this new word be denoted by w. The
speaker utters the word w, and the hearer receives it, both smoothly and immediately.
The hearer should check his memory as soon as he received the word w. If he
has already had the same word as w in his memory, then local consensus is reached
at this time step. In this case, both speaker and hearer clear up their memories but
only keep the consented word w; otherwise, if the hearer does not have the same
word w in memory, then he will learn this new word. In the latter case, if the hearer
has not used up all his memory, then he simply stores the word w into his memory;
otherwise, if the hearer reaches his memory limit, then a word must be removed from

3.2 Finite-Memory Naming Game
25
Fig. 3.1 Flowchart of the
ﬁnite-memory naming game
(FMNG) in one time step
Start of one 
time step
Speaker randomly 
pick a word w, and 
utters it
Speaker creates a word 
w, saves it in memory, 
and utters it
Hearer learns 
the word w
Hearer receives the 
word w
End of one 
time step
Speaker and 
Hearer local 
consensus
memory 
overflow?
prune a word
Speaker has 
memory?
Hearer has
w ?
yes
no
yes
no
yes
no
his memory, so that he can store the new word w into memory, or he simply rejects
the new word at this time step.
Speciﬁcally, suppose that each agent can remember at most L different words.
The hearer has already had L different words {w1, w2, . . . , wL} in memory, and then
he receives a new and different word wL+1 from the speaker. Since his memory limit
L, the hearer must discard one word from the union set {w1, w2, . . . , wL, wL+1}.
Two strategies are proposed here to deal with memory overﬂow, as shown in
Fig.3.2.
The ﬁrst strategy is shown in Fig.3.2a, which refers to a uniformly randomly
discarding rule, where all the words in {w1, w2, . . . , wL, wL+1} have an equal prob-
ability (i.e., 1/(L + 1)) to be discarded. The evenly-cut pie chart in Fig.3.2a shows
the equal probability in discarding each word. The gray-shaded pie in the chart rep-
resents the probability of discarding the newly learned word wL+1.
The second strategy is shown in Fig.3.2b, which is the strategy used in [1]. More
precisely, when the hearer has fully used his memory, then with probability 0.5, he
discards a word from {w1, w2, . . . , wL}, and then uses the room in memory to store
the new word wL+1; otherwise, with the same probability 0.5, he refuses to learn the
newwordwL+1 butsimplydiscardingit.AsshowninFig.3.2b,theoverallprobability
of discarding the new word wL+1 is 0.5, while the probability of discarding any other
word in {w1, w2, . . . , wL} is 0.5/L. Clearly, the strategy (b) has a higher probability
to discard the new word wL+1 than the strategy used in Fig.3.2a. This is represented
by the gray-shaded part in Fig.3.2b, which is larger than the gray-shaded part in
Fig.3.2a.

26
3
Finite-Memory Naming Game
Fig. 3.2 Two memory
overﬂow situations: a Every
word has an equal
probability to be discarded; b
with probability 0.5, the new
word transmitted from the
speaker will be lost, and with
probability 0.5, an old word
in the hearer’s memory will
be lost
(a) 
Speaker
ws,1
ws,2
…
w1
w2
wL+1
…
…
…
…
…
…
…
wL
w1
w2
wL+1
…
…
wL
(b) 
Hearer
w1
w2
…
wL
wL+1
Note that although a naming game model can also be regarded as an opinion-
formation model, they essentially differ from each other in the numbers of selectable
options of the agents. For example, in the voter models [5–11], each agent has only
two opinions to express; while in a naming game model, on the contrary, before
reaching the state of global consensus, each agent can have as many names as he
can remember (or, here in the FMNG model, at most L names) for the only object
to be named. It is also worth noting that the FMNG model considers only one single
object, while in reality, agents can observe and name many different objects, either
in parallel or sequentially. Meanwhile, considering multiple objects in parallel may
be one reason to reach the memory limit for the agents, this situation is not consid-
ered here. This is because, in naming game models, it is assumed that the semantic
correlation among different objects is negligible, therefore the process of naming
different objects can be decomposed into several independent processes, wherein
each process simply names only one object. This assumption signiﬁcantly simpliﬁes
the model for investigation.
3.3
Simulation Results
3.3.1
Simulation Settings
In the following, the FMNG model is investigated by comprehensive computer sim-
ulations. Four typical topologies of complex networks, namely, the Erdös–Rényi
random-graph (RG) networks [12, 13], Watts–Strogatz small-world (SW) networks
[14, 15], Barabási–Albert scale-free (SF) networks [16–18] and the fully-connected

3.3 Simulation Results
27
Table 3.1 Network settings for the naming game simulations
Notation
Network type and setting
N
⟨k⟩
⟨apl⟩
⟨cc⟩
RG/0.004
ER random-graph network with
P = 0.004
2000
8.0047
3.8855
0.0040
RG/0.005
ER random-graph network with
P = 0.005
2000
10.0174
3.5581
0.0050
RG/0.0075
ER random-graph network with
P = 0.0075
2000
15.0088
3.0764
0.0075
RG/0.01
ER random-graph network with
P = 0.01
2000
19.9731
2.8312
0.0100
RG/0.0125
ER random-graph network with
P = 0.0125
2000
24.9875
2.7121
0.0126
RG/0.025
ER random-graph network with
P = 0.025
2000
49.9933
2.2544
0.0250
RG/0.05
ER random-graph network with
P = 0.05
2000
100.0335
1.9563
0.0500
RG/0.25
ER random-graph network with
P = 0.25
2000
499.7311
1.7500
0.2500
RG/0.5
ER random-graph network with
P = 0.5
2000
999.5062
1.5000
0.5000
SW/4/0.1
WS small-world network with
K = 4 and RP = 0.1
2000
8
5.6546
0.4738
SW/4/0.2
WS small-world network with
K = 4 and RP = 0.2
2000
8
4.7898
0.3365
SW/5/0.1
WS small-world network with
K = 5 and RP = 0.1
2000
10
4.9427
0.4896
SW/5/0.2
WS small-world network with
K = 5 and RP = 0.2
2000
10
4.2782
0.3461
SW/7/0.1
WS small-world network with
K = 7 and RP = 0.1
2000
14
4.1657
0.5075
SW/7/0.2
WS small-world network with
K = 7 and RP = 0.2
2000
14
3.6892
0.3590
SW/10/0.1
WS small-world network with
K = 10 and RP = 0.1
2000
20
3.5648
0.5210
SW/10/0.2
WS small-world network with
K = 10 and RP = 0.2
2000
20
3.2452
0.3690
SW/12/0.1
WS small-world network with
K = 12 and RP = 0.1
2000
24
3.3302
0.5259
SW/12/0.2
WS small-world network with
K = 12 and RP = 0.2
2000
24
3.0345
0.3728
networks, are employed as the underlying networks to present different relationships
among the multiple agents. The population size is set to N = 2000 for all simula-
tions in this chapter. The detailed settings and statistics of the underlying networks
are summarized in Tables3.1 and 3.2.

28
3
Finite-Memory Naming Game
Table 3.2 Network settings for the naming game simulations (continued)
Notation
Network type and setting
N
⟨k⟩
⟨apl⟩
⟨cc⟩
SW/25/0.1
WS small-world network with
K = 25 and RP = 0.1
2000
50
2.7201
0.5389
SW/25/0.2
WS small-world network with
K = 25 and RP = 0.2
2000
50
2.5732
0.3828
SW/50/0.1
WS small-world network with
K = 50 and RP = 0.1
2000
100
2.2809
0.5471
SW/50/0.2
WS small-world network with
K = 50 and RP = 0.2
2000
100
2.0854
0.3932
SW/250/0.1
WS small-world network with
K = 250 and RP = 0.1
2000
500
1.7499
0.5783
SW/250/0.2
WS small-world network with
K = 250 and RP = 0.2
2000
500
1.7499
0.4572
SW/500/0.1
WS small-world network with
K = 500 and RP = 0.1
2000
1000
1.4997
0.6344
SW/500/0.2
WS small-world network with
K = 500 and RP = 0.2
2000
1000
1.4997
0.5695
SF/3
BA scale-free with 4 initial
nodes and 3 new edges added at
each step
2000
5.9920
3.7329
0.0229
SF/4
BA scale-free with 5 initial
nodes and 4 new edges added at
each step
2000
7.9869
3.4064
0.0255
SF/5
BA scale-free with 6 initial
nodes and 5 new edges added at
each step
2000
11.9747
3.0273
0.0311
SF/9
BA scale-free with 10 initial
nodes and 9 new edges added at
each step
2000
17.9482
2.7673
0.0391
SF/11
BA scale-free with 12 initial
nodes and 11 new edges added
at each step
2000
21.9249
2.6737
0.0445
SF/24
BA scale-free with 25 initial
nodes and 24 new edges added
at each step
2000
47.6822
2.2535
0.0742
SF/49
BA scale-free with 50 initial
nodes and 49 new edges added
at each step
2000
96.7420
1.9760
0.1201
SF/249
BA scale-free with 250 initial
nodes and 249 new edges
added at each step
2000
466.7577
1.7666
0.3524
SF/499
BA scale-free with 500 initial
nodes and 499 new edges
added at each step
2000
873.0679
1.5633
0.5411

3.3 Simulation Results
29
average degree k
101
102
103
convergence time
×105
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
LM=5
LM=10
LM=15
LM=20
LM=infinity
Fig. 3.3 Convergence time as a function of the average degree ⟨k⟩in RG networks for different
maximumlengthsofmemory(LM).Eachdata point isaveragedfrom500independent runs.Accord-
ing to Tables3.1 and 3.2, the average degree points are 8.0047 (RG/0.004), 10.0174 (RG/0.005),
15.0088 (RG/0.0075), 19.9731 (RG/0.01), 24.9875 (RG/0.0125), 49.9933 (RG/0.025), 100.0335
(RG/0.05), 499.7311 (RG/0.25), 999.5062 (RG/0.5), and 1999 (fully-connected network)
3.3.2
Convergence Time
The convergence time of FMNG is investigated ﬁrst. Figures3.3, 3.4, 3.5, and 3.6
show the convergence time as a function of the average degree ⟨k⟩with different
maximum lengths of memory (LM) in the four different network topologies, namely,
RG, SW, SF and fully-connected networks.
Figure3.3 shows that:
(1) As the average degree ⟨k⟩varies from 8.0047 to 1999, the convergence time of
FMNG on RG varies within a relatively small range, namely, within the range
between 1.2 × 105 and 3.0 × 105. Compared the ranges of FMNG on SW and
SF networks, as shown in Figs.3.4, 3.5, and 3.6, the variation of convergence
time in RG is relatively stable.
(2) When the maximum length of memory is not inﬁnite, the convergence time is
not a monotonic function of the average degree. In Fig.3.3, LM is set to 5,
10, 15, 20, and inﬁnity, respectively. As can be seen from in Sect.3.3.3, when
LM = {5, 10, 15, 20}, information loss happens, and so these curves are non-
monotonic. In contrast, when LM is set to inﬁnity, information loss does not
happen, and thus the corresponding curve is monotonically decreasing as the
average degree ⟨k⟩increases (the black dotted line with circles).
(3) For the same average degree, a greater value of LM facilitates the convergence
speed. When LM is set to be greater, information loss may be reduced or even
disappeared, and thus the convergence time is lowered.

30
3
Finite-Memory Naming Game
average degree k
101
102
103
convergence time
0.1
0.5
1
2
3
4
5
LM=5
LM=10
LM=15
LM=20
LM=infinity
average degree k
500
1000
2000
convergence time
×105
1.2
1.4
1.6
1.8
×105
Fig. 3.4 Convergence time as a function of the average degree ⟨k⟩in SW networks (with rewiring
probability RP = 0.1) for different maximum lengths of memory. Each data point is averaged from
500 independent runs. According to Tables3.1 and 3.2, the average degree points are 8 (SW/4/0.1),
10 (SW/5/0.1), 14 (SW/7/0.1), 20 (SW/10/0.1), 24 (SW/12/0.1), 50 (SW/25/0.1), 100 (SW/50/0.1),
500 (SW/250/0.1), 1000 (SW/500/0.1), and 1999 (fully-connected network)
average degree k
101
102
103
convergence time
×105
2
3
4
5
6
7
LM=5
LM=10
LM=15
LM=20
LM=infinity
average degree k
8
10
15
20 25
convergence time
×105
4.5
5
5.5
6
Fig. 3.5 Convergence time as a function of the average degree ⟨k⟩in SW networks (with rewiring
probability RP = 0.2) for different maximum length of memory. Each data point is averaged from
500 independent runs. According to Tables3.1 and 3.2, the average degree points are 8 (SW/4/0.2),
10 (SW/5/0.2), 14 (SW/7/0.2), 20 (SW/10/0.2), 24 (SW/12/0.2), 50 (SW/25/0.2), 100 (SW/50/0.2),
500 (SW/250/0.2), 1000 (SW/500/0.2), and 1999 (fully-connected network)

3.3 Simulation Results
31
Fig. 3.6 Convergence time
as a function of the average
degree ⟨k⟩in SF networks
for different maximum
length of memory. Each data
point is averaged from 500
independent runs. According
to Tables3.1 and 3.2, the
average degree points are
5.9920 (SF/3), 7.9869
(SF/4), 11.9747 (SF/5),
17.9482 (SF/9), 21.9249
(SF/11), 47.6822 (SF/24),
96.7420 (SF/49), 466.7577
(SF/249), 873.0679
(SF/499), and 1999
(fully-connected network)
average degree k
101
102
103
convergence time
1
2
5
12
LM=5
LM=10
LM=15
LM=20
LM=infinity
×105
(4) When the underlying network is fully-connected (with the maximum ⟨k⟩=
1999), the convergence speed is the fastest, despite the maximum length of
memory.
Figures3.4 and 3.5 show the convergence time versus the average degree ⟨k⟩in
SW networks with two different rewiring probability settings. Both ﬁgures show
more complicated relationships than what is shown in Figs.3.3 and 3.6. Figure3.4
(SW small-world with RP = 0.1) shows that:
(1) As the average degree ⟨k⟩varies, the convergence time varies much greater than
that in RG. The maximum convergence time is more than 10 times greater than
the minimum. Also, the convergence speed is slower than that of RG and SF.
(2) As the average degree increases, the convergence time has an up-down-up-down
process, with two local peaks (one is the ﬁfth point from left to right, where
⟨k⟩= 24.9875, and the other is at ⟨k⟩= 999.50622).
(3) The inset shows that, when the average degree is set ﬁxed, a greater value of LM
leads to a faster convergence speed, which is the same phenomenon as shown in
Fig.3.3.
(4) The population does not converge in the fastest speed in a fully-connected net-
work. Although the connectivity of the underlying network does not precisely
facilitate the convergence, the process does converge faster than many cases with
a poor connectivity.
Figure3.5 shows a similar phenomenon as Fig.3.4, except two differences: (1)
The curves basically have three local peaks (one local peak more than that in Fig.3.4).
(2) When LM = 5, it converges the fastest for ⟨k⟩= 24.9875, ⟨k⟩= 49.99334, and
⟨k⟩= 100.03348. The second observation slightly differs from what was observed
in Fig. 2 of [1], where the ﬁgures of convergence time vs ⟨k⟩in SW networks give a
neat formation. This indicates that not only the average degree and information loss

32
3
Finite-Memory Naming Game
will affect the convergence speed, but as well many other factors may inﬂuence both
the convergence speed and convergence process.
Figure3.6 shows a bunch of similar curves as that in Fig.3.3, except when SF
networks are employed as the underlying networks, wherein the convergence speed
is faster that that in a fully-connected network. This indicates that the heterogeneity
of the underlying network facilitates the convergence speed [19]. However, when
naming game models are performed on homogeneous networks, a better connectivity
would likely speed up the convergence.
3.3.3
Memory Loss
A distinguished feature of the FMNG model is that each agent has a ﬁxed size of
memory. In this subsection, simulations focus on the statistics of the lost words during
the naming process. When the memory size is not large enough, agents are unable to
remember all words that they received, and thus memory loss (or information loss) is
inevitable. For example, for an agent (denoted by A) with a memory size of LM = 5,
initially his memory is empty but eventually when reaching global consensus, he
would keep one and only one word in memory, and the same for all other agents.
At any time step of the process, when agent A has already remembered 5 words in
memory and then picked as the hearer, any non-consented word he received would
cause one unit of memory loss because he needs to memorize the new word. If this
event happens 100 times to agent A from the beginning (initialization) to the end
(global consensus), then his memory loss number (or score) is counted 100.
As shown in Fig.3.7, the distributions of memory loss are put in a 3D coordinate
system. The axis memory loss indicates the number of words lost during the entire
convergence process of the naming game. The vertical axis ratio of nodes shows
the ratio of nodes with different memory losses. The summation of ratios along the
ratio of nodes axis is 1. The third axis ⟨k⟩shows that the comparison of distributions
of memory losses is under the same conditions, except the average degree of the
underlying network.
As can be seen from Fig.3.7a, the distribution of memory loss varies when the
average degree ⟨k⟩increases. The peak of the curve moves towards the direction
with greater memory loss. There are two curves showing a decreasing distribution,
marked by ﬁlled-red squares and unﬁlled-black triangles, respectively. The peaks of
these two curves are located at the positions where the memory loss is zero, and the
nodes with the most memory loss is of the lowest ratio. The next two curves, namely
the ﬁlled-yellow circles and the unﬁlled-blue hexagrams, are transitional curves.
For the rest ﬁve curves for ⟨k⟩≥24.9875, each one has a clear peak that is not at
the lowest-memory-loss point. This suggests that, as the average degree increases
(while all the other settings and parameters remain the same), the average memory
loss increases and the distribution of memory loss becomes more concentrated. From
Fig.3.7a, b, the length of the maximum memory LM increases from 5 to 10; therefore,
memory loss is reduced in each case (with the same ⟨k⟩value). In Fig.3.7d, where the

3.3 Simulation Results
33
999.5062
499.7311
100.0335
k
49.9933
24.9875
19.9731
15.0088
10.0174
8.0047
102
memory loss
100
10-2
100
ratio of nodes
999.5062
499.7311
100.0335
k
49.9933
24.9875
19.9731
15.0088
10.0174
8.0047
102
memory loss
100
100
10-2
ratio of nodes
(a)
(b)
999.5062
499.7311
100.0335
k
49.9933
24.9875
19.9731
15.0088
10.0174
8.0047
102
memory loss
100
10-2
100
ratio of nodes
999.5062
499.7311
100.0335
k
49.9933
24.9875
19.9731
15.0088
10.0174
8.0047
102
memory loss
100
10-2
100
ratio of nodes
(c)
(d)
Fig. 3.7 Distribution of information loss when the average degree ⟨k⟩varies in RG networks: a
LM = 5; b LM = 10; c LM = 15; d LM = 20
length of the maximum memory is LM = 20, for ⟨k⟩= 8.0047 and ⟨k⟩= 10.0174,
there is no memory loss (see the only ﬁlled-red square and the only unﬁlled-black
triangle). This phenomenon is due to the fact that, when LM increases, the memory
loss decreases. This also suggests that LM = 20 is empirically large enough volume
of memory for a 2000-agent naming game simulation in a RG underlying network
with ⟨k⟩≤10.0174.
Figure3.8 shows similar distributions of memory losses in SW and SF networks.
When the average degree ⟨k⟩increases, the peak of memory loss curve moves towards
the direction with greater values. Consequently, it is reasonable to reason that, when
LM increases, the memory loss decreases.
3.3.4
Convergence Process
The convergence process of FMNG is investigated from three aspects: the number
of total words (see Figs.3.9, 3.12 and 3.15) and the number of different words (see
Figs. 3.10, 3.13 and 3.16) remembered by the population of agents, and the success
rate (see Figs.3.11, 3.14 and 3.17) of local communications.

34
3
Finite-Memory Naming Game
873.0679
466.7577
96.7420
k
47.6822
21.9249
17.9482
11.9747
7.9869
5.9920
102
memory loss
100
100
10-2
ratio of nodes
1000
500
100
k
50
25
20
15
10
8
102
memory loss
100
10-2
100
ratio of nodes
(a)
(b)
Fig. 3.8 Distribution of memory loss when the average degree ⟨k⟩varies: a in SW networks with
LM = 5; b in SF networks with LM = 5
iteration
(a)
104
106
number of total words
2000
5000
10000
20000
<k>=10
<k>=50
<k>=100
<k>=500
<k>=1000
iteration
(b)
104
106
number of total words
2000
5000
10000
20000
iteration
(c)
104
number of total words
2000
5000
10000
20000
iteration
(d)
104
number of total words
2000
5000
10000
20000
Fig. 3.9 Convergence process of the number of total words in a RG, b SW with RP = 0.1, c SW
with RP = 0.2, d SF networks. For all agents, their lengths of maximum memory are the same:
LM = 10

3.3 Simulation Results
35
iteration
(a)
104
106
number of different words
100
102
104
<k>=10
<k>=50
<k>=100
<k>=500
<k>=1000
iteration
(b)
104
106
number of different words
100
102
104
iteration
(c)
104
number of different words
100
102
104
iteration
(d)
104
number of different words
100
102
104
Fig. 3.10 Convergence process of the number of different words in a RG, b SW with RP = 0.1,
c SW with RP = 0.2, d SF networks. For all agents, their lengths of maximum memory are the
same: LM = 10
Figures3.9, 3.10 and 3.11 show the convergence process when the maximum
length of memory is LM = 10, and the average degree ⟨k⟩varies within the
ranges shown in the subplots of the ﬁgures (i.e., ⟨k⟩= {10, 50, 100, 500, 1000}). In
Figs.3.12, 3.13 and 3.14, agents have inﬁnite capacity of memory, namely LM = ∞,
and ⟨k⟩= {10, 50, 100, 500, 1000}. In Figs.3.15, 3.16 and 3.17, the average degree
is ﬁxed to ⟨k⟩= 20, while LM varies within {5, 10, 15, 20, ∞} in each subplot of
each ﬁgure.
Figures3.9, 3.12, and 3.15 show the curves of the number of total words remem-
bered by the whole population, which is initially zero and ﬁnally equal to the pop-
ulation size (2000 in this subsection) when reaching global consensus. As can be
seen from Figs.3.9 and 3.12, the curves vary when the average degree ⟨k⟩changes.
The peaks of the curves become lower when ⟨k⟩increases, the black pluses (for
⟨k⟩= 1000) has the highest peak, while the red squares (for ⟨k⟩= 10) has the lowest
peak. A higher peak of a curve means more words are remembered by the population,
and thus the memory usage of the agents is higher. It can also be seen that the red
squares converge in the slowest speeds in these ﬁgures.

36
3
Finite-Memory Naming Game
iteration
(a)
104
106
success rate
10-3
10-2
10-1
100
<k>=10
<k>=50
<k>=100
<k>=500
<k>=1000
iteration
(b)
104
106
success rate
10-3
10-2
10-1
100
iteration
(c)
104
success rate
10-3
10-2
10-1
100
iteration
(d)
104
success rate
10-3
10-2
10-1
100
Fig.3.11 Convergenceprocessofsuccessratein a RG,bSWwith RP = 0.1,c SWwith RP = 0.2,
d SF networks. For all agents, their lengths of maximum memory are the same: LM = 10
Figure3.15 shows the curves of the number of total words as LM changes. The
peaks of the curves become higher as LM increases. The red squares (for LM = 5) is
the lowest curve with the slowest convergence speed. In contrast, the black pluses (for
LM = ∞) has the highest peak but with fastest convergence speed. The difference
of convergence speeds shown in Fig.3.15 is more prominent than those in Figs.3.9
and 3.12, implying that the limitation on memory size affects the convergence speed
better than the change on the average degree of the underlying networks (or the
connectivities of the networks).
Figures3.10, 3.13, and 3.16 show the convergence processes in terms of the num-
ber of different words, which are initially zero and ﬁnally equal to one when reaching
global consensus. These curves are also inﬂuenced by the changes of the average
degrees and lengths of memory, similarly to the curves of total words. When ⟨k⟩= 10
(red squares), the curve has the lowest peak in each subplot, but converges in the
slowest speed. As to Fig.3.16, when LM increases, the convergence speed is clearly
lower, and also the peak becomes slightly lower.
Figures3.11, 3.14, and 3.17 show the convergence processes in terms of success
rate. The number of total words, the number of different words, and the success rate
essentially reﬂect the same convergence property, but from different perspectives.

3.3 Simulation Results
37
iteration
(a)
104
106
number of total words
2000
5000
10000
20000
40000
<k>=10
<k>=50
<k>=100
<k>=500
<k>=1000
iteration
(b)
104
106
number of total words
2000
5000
10000
20000
40000
iteration
(c)
104
number of total words
2000
5000
10000
20000
40000
iteration
(d)
104
number of total words
2000
5000
10000
20000
40000
Fig. 3.12 Convergence process of the number of total words in a RG, b SW with RP = 0.1, c SW
with RP = 0.2, d SF networks. For all agents, their lengths of maximum memory are the same:
LM = ∞
The convergence speed shown in the success rate curves is consistent with the curves
of the total words and different words. However, there is some feature that can only
be observed in the success rate convergence curve.
Figure3.11 shows that, when LM = 10, the success rate curves of ⟨k⟩= 10 (red
squares) keep the highest success rates before reaching global consensus. How-
ever, the curves of ⟨k⟩= 1000 (black pluses), although with the fastest convergence
speeds, basically keep the lowest success rates before they drastically increase and
climb up to one. Recall that, in Figs.3.9 and 3.10, when the average degree is low,
the numbers of total words and different words are relatively small. However, the
success rate is relatively high. This implies that, when the connectivity of the under-
lying network is poor, information cannot propagate globally. During a quite long
period, agents just remember a few words that are probably the same as their neigh-
bors, and local consensus are quite common between pairs of agents. However, due
to the poor connectivity, words cannot be passed around efﬁciently. The dominant
word comes up late, and thus the convergence speed in a poorly-connected net-
work is slow. In contrast, when the connectivities of the underlying networks are

38
3
Finite-Memory Naming Game
iteration
(a)
104
106
number of different words
100
102
104
<k>=10
<k>=50
<k>=100
<k>=500
<k>=1000
iteration
(b)
104
106
number of different words
100
102
104
iteration
(c)
104
number of different words
100
102
104
iteration
(d)
104
number of different words
100
102
104
Fig. 3.13 Convergence process of the number of different words in a RG, b SW with RP = 0.1,
c SW with RP = 0.2, d SF networks. For all agents, their lengths of maximum memory are the
same: LM = ∞
iteration
(a)
104
106
success rate
10-3
10-2
10-1
100
<k>=10
<k>=50
<k>=100
<k>=500
<k>=1000
iteration
(b)
104
106
success rate
10-3
10-2
10-1
100
iteration
(c)
104
success rate
10-3
10-2
10-1
100
iteration
(d)
104
success rate
10-3
10-2
10-1
100
Fig.3.14 Convergenceprocessofsuccessratein a RG,bSWwith RP = 0.1,c SWwith RP = 0.2,
d SF networks. For all agents, their lengths of maximum memory are the same: LM = ∞

3.3 Simulation Results
39
iteration
(a)
104
106
number of total words
2000
4000
8000
12000
LM=5
LM=10
LM=15
LM=20
LM=infinity
iteration
(b)
104
106
number of total words
2000
4000
6000
8000
iteration
(c)
104
number of total words
2000
4000
6000
8000
iteration
(d)
104
number of total words
2000
4000
6000
8000
Fig. 3.15 Convergence process of the number of total words in a RG, b SW with RP = 0.1, c
SW with RP = 0.2, d SF networks. The average degrees of the underlying networks are the same:
⟨k⟩= 20
iteration
(a)
104
106
number of different words
100
102
104
LM=5
LM=10
LM=15
LM=20
LM=infinity
iteration
(b)
104
106
number of different words
100
102
104
iteration
(c)
104
number of different words
100
102
104
iteration
(d)
104
number of different words
100
102
104
Fig. 3.16 Convergence process of the number of different words in a RG, b SW with RP = 0.1, c
SW with RP = 0.2, d SF networks. The average degrees of the underlying networks are the same:
⟨k⟩= 20

40
3
Finite-Memory Naming Game
iteration
(a)
104
106
success rate
0.1
0.2
0.4
1
LM=5
LM=10
LM=15
LM=20
LM=infinity
iteration
(b)
104
106
success rate
0.2
0.4
0.6
0.8
1
iteration
(c)
104
success rate
0.2
0.4
0.6
0.8
1
iteration
(d)
104
success rate
0.2
0.4
0.6
0.8
1
Fig.3.17 Convergenceprocessofsuccessratein a RG,bSWwith RP = 0.1,c SWwith RP = 0.2,
d SF networks. The average degrees of the underlying networks are the same: ⟨k⟩= 20
good, agents are learning new words, during a long period locally or globally. Good
connectivity facilitates the propagation of words, which becomes earlier than that
in poorly-connected networks; some dominant word comes up and leads the entire
population to convergence.
The only difference between Figs.3.14 and 3.11 is that, in Fig.3.14, the limit
of memory is LM = ∞, but in Fig.3.11, LM = 20. Figure3.14 shows a similar
phenomenon as Fig.3.11.
Figure3.17 shows a very interesting phenomenon in FMNG: increasing the value
of LM has a similar effect as increasing the value of ⟨k⟩of the underlying network.
Increasing either LM or ⟨k⟩will (1) increase the numbers of total words and different
words remembered by the population, (2) lower the success rate over a long period
before reaching global consensus, and (3) speed up the convergence rate. Note that
these observations are not contradictory to that shown in Fig.3.6. In Figs.3.9, 3.10,
3.11, 3.12, 3.13, 3.14, 3.15, 3.16 and 3.17, the increase of the average degree ⟨k⟩is
within a certain range from ⟨k⟩= 10 to ⟨k⟩= 1000. Whereas, the fully-connected
situation is not considered.

3.4 Conclusion
41
3.4
Conclusion
In this chapter, the evolution and dynamics of the ﬁnite-memory naming game
(FMNG) are studied. Four types of typical underlying networks, namely the Erdös–
Rényi random-graph networks, the Watts–Strogatz small-world networks, the
Barabási–Albertscale-freenetworks,andthefully-connectednetworks,areemployed
as the underlying relationship networks of the population of agents. The convergence
time, convergence performance, and the memory loss of agents in FMNG are stud-
ied. It is found that the distribution of memory loss is more concentrated when the
average degree of the underlying network increases. Both the increase of the average
degree and the maximum memory length of the agents can lead to faster conver-
gence speeds. It is revealed that the ﬁnite-memory effect plays an important role in
the naming game.
References
1. W.X. Wang, B.Y. Lin, C.L. Tang, G.R. Chen, Agreement dynamics of ﬁnite-memory language
gamesonnetworks.Eur.Phys.J.B60(4),529–536(2007).https://doi.org/10.1140/epjb/e2008-
00013-5
2. G. Fu, Y. Cai, W. Zhang, Analysis of naming game over networks in the presence of memory
loss. Physica A 479, 350–361 (2017)
3. L.Dall’Asta,A.Baronchelli,A.Barrat,V.Loreto,Nonequilibriumdynamicsoflanguagegames
on complex networks. Phys. Rev. E 74(3), 036105 (2006). https://doi.org/10.1103/PhysRevE.
74.036105
4. R.R. Liu, C.X. Jia, H.X. Yang, B.H. Wang, Naming game on small-world networks with
geographical effects. Physica A 388, 3615–3620 (2009). https://doi.org/10.1016/j.physa.2009.
05.007
5. D. Boyer, O. Miramontes, Interface motion and pinning in small-world networks. Phys. Rev.
E 67(3), 035102 (2003). https://doi.org/10.1103/PhysRevE.67.035102
6. C. Castellano, V. Loreto, A. Barrat, F. Cecconi, D. Parisi, Comparison of voter and glauber
ordering dynamics on networks. Phys. Rev. E 71(6), 066107 (2005). https://doi.org/10.1103/
PhysRevE.71.066107
7. C. Castellano, D. Vilone, A. Vespignani, Incomplete ordering of the voter model on small-world
networks. EPL (Europhys. Lett.) 63(1), 153–158 (2003)
8. I. Dornic, H. Chaté, J. Chave, H. Hinrichsen, Critical coarsening without surface tension: the
universality class of the voter model. Phys. Rev. Lett. 87(4), 045701 (2001). https://doi.org/
10.1103/PhysRevLett.87.045701
9. X. Niu, C. Doyle, G. Korniss, B.K. Szymanski, The impact of variable commitment in the
naming game on consensus formation. Sci. Rep. 7, 41750 (2017)
10. K. Suchecki, V.M. Eguíluz, M. San Miguel, Voter model dynamics in complex networks: Role
of dimensionality, disorder, and degree distribution. Phys. Rev. E 72(3), 036132 (2005). https://
doi.org/10.1103/PhysRevE.72.036132
11. D. Vilone, C. Castellano, Solution of voter model dynamics on annealed small-world networks.
Phys. Rev. E 69(1), 016109 (2004). https://doi.org/10.1103/PhysRevE.69.016109
12. P. Erdös, A. Rényi, On random graphs I. Publ. Math. Debrecen 6, 290–297 (1959)
13. P. Erdös, A. Rényi, On the strength of connectedness of a random graph. Acta Mathematica
Academiae Scientiarum Hungarica 12(1–2), 261–267 (1964)

42
3
Finite-Memory Naming Game
14. D.J. Watts, S.H. Strogatz, Collective dynamics of ‘small-world’ networks. Nature 393(6684),
440–442 (1998). https://doi.org/10.1038/30918
15. D.J. Watts, The “new” science of networks. Annu. Rev. Sociol 30, 243–270 (2004). https://doi.
org/10.1146/annurev.soc.30.020404.104342
16. R. Albert, A.L. Barabási, Statistical mechanics of complex networks. Rev. Mod. Phys. 74(1),
47 (2002). https://doi.org/10.1103/RevModPhys.74.47
17. A.L. Barabási, R. Albert, H. Jeong, Scale-free characteristics of random networks: the topol-
ogy of the world-wide web. Physica A 281(1), 69–77 (2000). https://doi.org/10.1016/S0378-
4371(00)00018-2
18. A.L. Barabási, R. Albert, Emergence of scaling in random networks. Science 286(5439), 509–
512 (1999)
19. J. Gao, B. Li, G. Schoenebeck, F.Y. Yu, Engineering agreement: the naming game with asym-
metric and heterogeneous agents, in AAAI, pp. 537–543 (2017)

Chapter 4
Naming Game with Multi-Hearers
or Group Discussions
4.1
Introduction
As global consensus is the ultimate state that the entire population are pursuing,
perhaps unintentionally and unconsciously, one possible direction to improve the
naming game models is to facilitate their convergence speeds. It is typically assumed
that in an naming game model there are only two agents, a speaker and a hearer,
involved in a local interaction at each time step of the iterative process. However,
this is not always the real case in human communications, where broadcasting and
group discussion are very common. Broadcasting means that there is one speaker
sending out a message to multiple hearers simultaneously, for example, in a TV show
or a conference presentation. In this chapter, therefore, a more realistic situation with
multiple hearers is considered in a naming game model; that is, when a speaker utters
a name, there are several hearers listening to it at the same time. Such a naming game
with multiple hearers (MHNG) extends the minimal naming game model from the
one-to-one local communication scenario to the one-to-many setting. In the one-to-
many broadcasting framework, a same word is sent to multiple hearers towards local
consensus simultaneously, which encourages the emergence of a dominant name in
the population, thus facilitates the information propagation within the population.
For the new scenario with multiple hearers, one might have an impression that the
convergencespeedwouldaccelerateasthenumberofhearersincreases.Thisintuition
actually has never been proved, however. This chapter investigates the relationship
between the convergence time and the number of hearers, in several different network
topologies respectively. It shows that, when the number of hearers is less than the
average (node) degree of the underlying network, the convergence time actually
decreases as the number of hearers increases. As the number of hearers further
increases, till the allowed number of hearers becomes greater than the average degree,
the inﬂuence on the convergence time is not obvious in homogeneous networks.
However, for heterogeneous scale-free networks, increasing the allowed number of
hearers will indeed accelerate the consensus speed (or reduce the convergence time)
in general. It is also revealed that, as the number of hearers increases, the agents
© Springer Nature Switzerland AG 2019
G. Chen and Y. Lou, Naming Game, Emergence, Complexity and Computation 34,
https://doi.org/10.1007/978-3-030-05243-0_4
43

44
4
Naming Game with Multi-Hearers or Group Discussions
are prone to learning fewer new words; that is, the maximal number of different
words learned by the population of agents is reduced as more hearers are involved.
Therefore, the number of hearers plays an important role in the linguistic evolution,
which favors the network consensus at the cost of having more learners. As for small-
world networks, local convergence within local communities actually hinders (slows
down) the global consensus process. With a small rewiring probability, the cluster
coefﬁcient of such a network remains to be large. In such an underlying network
with local communities, some agents tend to consent only locally. These agents are
unwilling to learn more new (different) words for having some old words learned
with his peers in the same community. As a result, this can increase the success rate
by increasing the rate of local consensus, but hinders the global convergence. This
chapter focuses on the multiple hearer broadcasting strategy, with the naming game
convergence affected by the multiple-local-community structure to be introduced in
Chap.6.
Group discussion or negotiation is another common situation in real-life, in which
multiple participants are peers to each other, and thus each participant is actually
both speaker and hearer simultaneously during group discussion. Recall that in the
minimal naming game model, the communication can only take place between two
directly connected agents, of which one is speaker and the other is hearer. Whereas
two indirectly connected agents are probably able to communicate through a (short
and direct) connected path (via the so-called word of mouth spreading). On the other
hand, in a group discussion, it is not necessary that all agents know each other in
the group, implying that not all the agents are directly connected in the group, i.e.,
the group is not a fully-connected sub-graph of the underlying network. Moreover,
friends of friends should be able to communicate (and become acquaintances) as
well. Since becoming acquaintances would change the topology of the underlying
network, here only the scenario with “friends of friends” is considered to be able to
mutually communicate. Meanwhile, multiple words should be allowed to be trans-
mitted in a group discussion. Therefore, the group discussion can include a group
of agents who are not necessarily fully-connected through an underlying subgraph,
while different agents could speak out their different ideas with multiple words. The
naming game in groups (NGG) model, therefore, is introduced to mimic this group
discussion scenario. Clearly, the NGG model further extends the one-to-many local
communication scenario of MHNG to the many-to-many setting.
Both MHNG and NGG facilitate the convergence speed of the naming game in
terms of time steps, due to the involvement of multiple agents in a local conversation
at each time step. However, it is also noted that neither MHNG nor NGG would
essentially reduce the total number of operations in the naming game process.
In the rest of the chapter, MHNG and NGG models are introduced in Sects.4.2
and 4.3, respectively. Experimental studies are performed and compared in Sect.4.4.
Finally, conclusion is drawn in Sect.4.5.

4.2 Multi-Hearer Naming Game
45
4.2
Multi-Hearer Naming Game
Recall the scenario of the minimal naming game, where a population of agents
connected through a communication network in a certain topology. At each time
step of the iterative process, a pair of agents are picked at random, referred to as
the speaker and the hearer respectively. The speaker tells the hearer one and only
one word from either his own memory or from an external vocabulary (equivalent
to inventing one within a pre-set limit). With a certain probability or in a certain
manner, the hearer learns and remembers the speaker-uttered word. If successes,
both drop all the other remembered words from their memories, but keep the only
speaker-uttered word. Thus, after a sufﬁciently large number of pair-wise iterations
performed in this way, the memories of all agents in the population trend to have one
and the same word. When this happens, the population is considered converging to
the global consensus state; namely, the population of agents reach consent to a name
of the object.
Compared with the minimal naming game model, MHNG has multiple hearers in
the communication of each time step, where there are one speaker and several hearers.
In MHNG, a speaker is uniformly randomly selected from the whole population,
while hearers are uniformly randomly selected from the neighbors of the speaker.
Thus,onlythedirectedstrategycanbeappliedtoMHNG,whereastheinversestrategy
cannot. The local communications in MHNG are actually performed in a star-shaped
sub-network of the entire underlying network. The speaker is located at the center of
the star-shaped sub-network, while the hearers are located at the leaves. Figure4.1
shows a sub-region of the underlying network. The selected speaker is denoted by
the circle, and a portion of the speaker’s neighbors are selected as the hearers (based
on a predeﬁned parameter, namely the number of hearers). Their links are drawn
by lines (referred to as effective links), which means that pair-wise communications
can directly take place between the linked agents. The dotted lines are referred to as
speaker 
hearer
unrelated agent
effective link
ineffective link
Fig. 4.1 An example of local communications. The star-shaped sub-network is connected by lines,
while the other links are denoted by dotted lines. The speaker is located at the center (circle), and
part of his neighbors are picked as hearers (triangles). The local communications are performed in
the form of the speaker (circle) sending out the same word to the hearers (triangles)

46
4
Naming Game with Multi-Hearers or Group Discussions
Fig. 4.2 Flowchart of the MHNG model in one time step. Nh is the total number of hearers; Nch
is the number of consented hearers
ineffective links, which are the links in the underlying network that are not relevant
to the current local pair-wise communication. A portion of the speaker’s neighbors
are not selected as hearers at this iteration (squares in Fig.4.1). These unselected
neighboring agents, together with all the other agents that are not directly linked to
the speaker, are irrelevant agents.
Figure4.2 illustrates the ﬂowchart of one-time-step learning process of MHNG.
Before the operations shown in Fig.4.2 can be proceeded, a local star-shaped speaker-
hearers sub-network (as shown in Fig.4.1) should have been formed. The local pair-
wise communications between the speaker and each hearer is performed. The total
number of hearers, Nh, is a predeﬁned parameter that indicates the scale of the
local communications. Note that, although one can assign an arbitrary integer to
Nh, the total number of hearers is also limited by the total number of neighbors of
the selected speaker. Strictly speaking, for an agent A, his total number of hearers,
Nh(A), should be deﬁned by Nh(A) = min{Nh, k(A)}, where Nh is a user-deﬁned
parameter and k(A) is the degree of agent A in the underlying network. Due to
the degree heterogeneity and randomness, the total number of hearers varies from
time to time. In this chapter, for the sake of convenience, Nh is used to denote the
approximate total number of hearers, without considering the variation caused by

4.2 Multi-Hearer Naming Game
47
Fig. 4.3 Flowchart of the
hearer-only naming game
model in one time step
the node degrees of the underlying network. Nevertheless, a speaker can speak to at
most min{Nh, k(A)} hearers simultaneously. On the other hand, Nch is the number
of consented hearers, with Nch ≤Nh. If all Nh hearers are consented to the speaker-
uttered word, then Nch = Nh. In this case, the speaker also reaches a local consensus,
i.e., the speaker clears up all his remembered old words but keeps the last new word
only. Otherwise, if Nch < Nh, meaning that at least one hearer is not consented to
the speaker, then the speaker does not reach local consensus in the current time step,
thus the speaker does nothing, and the process continues.
EachhearerinFig.4.2followsthehearer-onlynaminggame(HO-NG)process[1].
The ﬂowchart of HO-NG is shown in Fig.4.3. Note that HO-NG and MHNG mimic
different scenarios, thereby they have different meanings of operations, although the
operations are quite similar to each other. The MHNG model, which was proposed
later than HO-NG, could be considered as a variant of HO-NG. As suggested by their
different names, HO-NG considers only the consensus of the hearers (namely, agents
never reach consent when they play the roles as speakers), while MHNG considers
the consensus of both speakers and hearers together at last.
Figure4.3 shows the ﬂowchart of HO-NG. The HO-NG model is almost the same
as the minimal naming game, except for the box-shaded regions in gray in the ﬁgure.

48
4
Naming Game with Multi-Hearers or Group Discussions
Speaker
right
bite
signal
(a) failure (no hearer consensus)
(b) failure (one hearer consensus)
Hearer 1
bite
cream
right
(c) success
Hearer 2
kite
bite
Speaker
right
bite
signal
Hearer 1
bite
cream
signal
Hearer 2
kite
bite
signal
Speaker
right
bite
signal
Hearer 1
bite
cream
right
Hearer 2
kite
bite
Speaker
right
bite
signal
Hearer 1
right
Hearer 2
kite
bite
right
Speaker
right
bite
signal
Hearer 1
bite
cream
right
Hearer 2
kite
bite
Speaker
bite
Hearer 1
bite
Hearer 2
bite
Fig. 4.4 An example of MHNG in one-time-step communication. One speaker and two hearers are
connected. There are three possible outcomes after the speaker utters a words: a neither Hearer 1
nor Hearer 2 is consented; b Hearer 1 is consented but Hearer 2 is not; c both hearers are consented.
The ﬁrst two cases are failures, and the last one is success
When the hearer has the same word in memory as the speaker, in the minimal naming
game both speaker and hearer clear up their memories but keep the consented word
only. However, in the HO-NG, when this local consensus takes place, only the hearer
clears up his memory keeping only the consented word, while the speaker’s memory
remains unchanged.
There is also a speaker-only naming game (SO-NG) model introduced in [1].
Similarly to HO-NG, in SO-NG, when local consensus takes place, only the speaker
clears up his memory keeping only the consented word, while the hearer’s memory
remains unchanged.
Note that, in MHNG, there is only one speaker who utters only one single word to
all his hearers. Thus, if all the hearers received the word and all are consented, then the
speaker would naturally be consented to the same word he uttered. Figure4.4 shows
an example of local communications in MHNG. Three agents are connected, where
there is a speaker and two hearers. In Fig.4.4a, neither Hearer 1 nor Hearer 2 has the
speaker-uttered word ‘signal’, thus both hearers learned it in their memories. This
is clearly a failure in local communication. In Fig.4.4b, only Hearer 1 is consented,
while Hearer 2 is not. Hearer 1 clears up all other words and keeps the word ‘right’
only, whereas Hearer 2 learned the word ‘right’. The result is also a local failure.
Only if all the hearers are consented as shown in Fig.4.4c, it is a local consensus or
local success. In this case, all three agents clear up all their previous memories, but
keep only the consented word (i.e., ‘bite’) in their memories.

4.3 Naming Game with Group Discussions
49
4.3
Naming Game with Group Discussions
Despite the rapid development of various naming game models in the past few years,
there is also a lack of realistic naming game models that can better describe real-life
group discussion scenarios. In the following, the limitations of the existing models
are discussed in detail. There is also a naming game model introduced to overcome
some of such limitations. In very much the same way, a self-organized consensus
is treated as the result of negotiation on a peer-to-peer network, which represents
individuals as nodes and their interactions as edges.
First, in retrospect, various social negotiation properties were reported based on
naming game simulations. For example, in [2] the degree correlation of asymmet-
ric negotiations is studied on scale-free (SF) and small-world (SW) networks. It is
demonstrated that a moderate attempt to choose a higher-degree agent as the speaker
would facilitate global consensus by means of improving its convergence speed. In
[3], optimal convergence with geography-based negotiations is discussed on SW
networks, revealing that the fastest convergence could be achieved under a moderate
variation on the distances among agents. However, there exists several limitations on
these models and their variants. The single speaker and the single hearer are strictly
distinguished. Even in [4], there is one speaker, one hearer and multiple over-hearers,
and in [5] there is one speaker and multiple hearers, the roles of the speaker(s) and the
hearer(s) are strictly distinctive. At each iteration, an agent can only be a speaker or a
hearer, and as soon as the role is assigned, he cannot switch the role in this iteration.
There is only one direction of word spreading, i.e., from the speaker to the hearer.
However, in a real negotiation scenario, there are usually multiple participants who
are peers to each other, where each agent in the conversation is actually both speaker
and hearer at the same time.
Second, in most naming game models, communications can only take place
between (two) directly connected agents, but actually two indirectly connected agents
should be able to communicate through a (short) connected path (via the so-called
“word of mouth” spreading). Note that two indirectly connected agents are connected
through a common friend (agent), while the communication between any two irrel-
evant agents is not considered in the naming game models. Here, irrelevant means
there is a long path between to agents, though connected.
Third, in the existing models, multiple words are not allowed to be transmitted
within a group at the same time, and intra-group multiple consensuses are not applied
ineachiteration.However,inareal-worlddiscussion,therearemultiplewords(ideas)
communicated.
To overcome the above-mentioned limitations, herewith a model called naming
game in groups (NGG) is introduced. This model generalizes most of the aforemen-
tioned models, including MHNG [5]. Some distinguished characteristics of the NGG
model are summarized as follows:
(1) At each iteration, agents participating in group discussion are both speaker and
hearer simultaneously.

50
4
Naming Game with Multi-Hearers or Group Discussions
(2) Words are allowed to be transmitted from an agent to a indirectly-connected one,
as long as the two agents are connected through a common friend.
(3) Multiple words are allowed to be spread within the group in one iteration. Also,
intra-group multiple consensuses is allowed.
In the following, the model is described in detail. The NGG model combines the
fundamental framework of the minimal naming game [6, 7] with the above three
distinguished characteristics together. Basic elements, including the underlying net-
work, agent (node), communication (negotiation), word (name), memory, vocabu-
lary, as well as speaker and hearer, are the same as that in the minimal naming game.
Inﬁnite memory capacity is assumed for every agent, and agents cannot hear himself
(i.e., no self-loops exist in the underlying network).
Speciﬁcally, given a population of N agents as a connected network of N nodes
described by the adjacency matrix [Ai j], the NGG performs iteratively the group for-
mation, transmitted-words determination and words transmission, which are respec-
tively introduced and discussed in the following two subsections.
4.3.1
Group Formation and Transmitted-Words
A connected subset of the population, namely a group G of size NG, as a sub-network
of the underlying network, is formed as follows: (1) A node of degree ks is randomly
picked from the entire population as the seed agent; (2) a number of min(ks, NG −1)
neighboring agents (directly-connected to the seed) are randomly chosen. Then, a
group G is formed, containing min(ks + 1, NG) group members in total. Clearly
from the construction, the maximal path length between any two group members is
precisely 2. Figure4.5 shows an example of group formation. In Fig.4.5a, the seed
agent (gray-shaded) is picked at random from the population. The seed has degree 5.
Then, in Fig.4.5b, 4 neighboring agents are picked from the 5 neighbors of the seed
agent. In this example, the group size is NG = 5, and the degree of the seed is ks = 5,
and thus the number of picked neighbors is min(ks, NG −1) = min(5, 4) = 4. After
the group G is formed, as shown in Fig.4.5b, only the group members (gray-shaded
circles) and the internal links connecting group members (lines) are considered, while
the other agents (non-shaded circles) and links (dotted lines) are ignored in this time
step.
Every group member expresses his opinion by saying a word for negotiation. In
this scenario, every group member is a speaker. Meanwhile, every group member is
able to hear the words from other group members, so each member is a hearer as
well. All the words uttered by the group members would be sorted to a unique word
list, denoted by CW, for further negotiation, where CW is sorted candidate words.
Then, some words are chosen from the candidate word list CW to be transmitted
among directly- and indirectly-connected group members. For directly-connected
pairs, the words can be transmitted exactly the same as did in other naming game

4.3 Naming Game with Group Discussions
51
seed 
node/agent 
link
group member 
non-group member 
non-group link
(a) 
(b) 
Fig. 4.5 An example of group formation in the NGG model: a An agent is randomly picked from
the whole population as the seed agent of the group. b A number of neighbors are picked at random,
and then a group is formed. In the group, the path length between any two agents is either 1 or 2
models; while for an indirectly-connected pair of group members, since the path
length between them is 2, the transmission probability is set to 0.5.
To avoid possible ‘gabbling’ in the group negotiations, the transmitted-words are
assigned with weights. Thus, the selection of words to be transmitted depends on
their weights. The weight of word implies its importance. In real-life discussion,
some topics would draw more attentions than other. Words with higher weights
are more easily picked than lower-weighted words. The weight value of a word
in CW is assigned according to how many group members utter it and how many
group members hear it directly. So, here, the ‘importance of a word’ is measured
by how popular it is. This is reasonable according to the ‘plurality rule’ in social
negotiations [8].
In NGG, the probability is measured by a weight metric. Formally, the weight
metric in the NGG model is deﬁned on three levels: the pair-level Ip, the node-level
In, and the word-level Iw. The precise meanings of these three levels are introduced
as follows.
(1) The pair-level weight Ip is used to determine the transmission rate between two
agents, Ai and A j:
Ip(i, j) =
⎧
⎪⎨
⎪⎩
0,
if i = j
1,
if A(i, j) = 1
0.5, otherwise
(4.1)
(2) The node-level weight In is an intermediate value for calculating the word-level
weight Iw in Eq.(4.3). Given an agent Ai, its In(i) value is the summation of all
the pair-level weights between agent Ai and other group members A j, j ∈G:

52
4
Naming Game with Multi-Hearers or Group Discussions
In(i) =

j∈G
Ip(i, j)
(4.2)
The purpose of introducing In is to give different weights to the members in
the same group, depending on the underlying network topology, rather than
to distinguish the roles of the group members. For example, if the group is a
fully-connected sub-network, In for each group members is identical.
(3) It is possible that different group members utter the same word in an iteration.
The word-level weight Iw(w) for a word w is deﬁned as the summation of the
node-level weights for all the group members that uttered w:
Iw(w) =

i∈G
Ii(i)
if agent Ai speaks word w
(4.3)
A particular word w is chosen from the candidate word list CW for transmission
according to the following probability:
Pw(w) =
Iw(w)

w∈CW Iw(w)
(4.4)
Clearly, the probability of picking up the word w (Pw(w)) is determined only by
the value of Iw(w). Speciﬁcally, Pw(w) is determined by Iw(w), which represents
the total number of group members who utter w (calculated by Eq.(4.3)) and the
number of group members who hear the word w directly (calculated by Eqs.(4.1)
and (4.2)).
Thus, in the NGG model, βNG words are allowed for transmission, where β is a
pre-deﬁned proportion parameter determining the probability of word transmission,
and NG is the group size. The βNG transmitted-words compose a subset of the
candidate word list CW, denoted by W, in which the βNG words are chosen from
CW using Eq.(4.4). Note that the words with large values of Iw(w) could be chosen
more than once to ‘persuade’ the other group members during the negotiation, thus
the βNG transmitted-words in W are not necessarily unique. However, each word in
CW is unique.
4.3.2
Words Transmission
In group negotiation, an agent receives some words and he may agree with several
different words, meaning that he may have several words that are the same as the
received ones. It is assumed that, in NGG, each agent will be consented to the ﬁrst
consented-word only. This assumption is made because: (1) It is recommended that
an agent should not betray the recently earlier consensus in an iterative game [9]. (2)
The word transmission order follows the order where the words were picked, thus,
words with higher Pw values (implying higher importance or plurality) are picked

4.3 Naming Game with Group Discussions
53
and transmitted earlier. Thus, when an agent reaches local consensus only to the ﬁrst
consented-word he received, it also implies that the agent respects the fact that the
earlier received words are more ‘important’ or more ‘popular’ in the population. The
words are then transmitted through a ‘broadcasting and feedback’ process as follows.
As every agent can be both speaker and hearer in an iteration, when one agent
tells a word w to another, the former is referred to as a source agent of the word w,
so as to avoid possible confusion. For each transmitted-word w in W, according to
its selected order, ‘broadcasting’ occurs in the group from the source agent(s) of w.
Whether a group member can hear w from ‘broadcasting’ is determined by a hearing
probability Ph. Since there could be more than one source agent for the word w, a
group member will hear w as long as he is the nearest neighbor to the source agent.
Consequently,
Ph(i) = max j{Ip(i, j), j ∈S}
(4.5)
where S ⊆G denotes the set of source agents for the current transmitting-word w.
According to Eqs.(4.1) and (4.5), Ph(i) means that if agent Ai is directly connected
to at least one source agent, then it deﬁnitely hears w; otherwise, it has a probability
0.5 to hear w.
After calculating Ph for all the group members, the hearers who could hear w
can be determined. Denote the set of these hearers by H. For every agent in H, if
he already has the word w in memory, then it is a local success. Therefore, HO-NG
[1] consensus is performed. The consented hearer keeps only w in memory while
dropping all other words thereafter; otherwise, it is a local failure, consequently w
will be added into the hearer’s memory.
A ‘feedback’ scheme is designed for source agents if they reach a local consensus
on some word(s). When a source agent utters a word w, and there are one or more
hearers having consent to the word w, then all the hearers perform the HO-NG, where
the source agent himself should also be considered as a member with local consensus.
Speciﬁcally, the success probability of a source agent is calculated by nsuc/NG, where
nsuc is the number of agents (hearers) consented to the current transmitted-word w.
Note that, if there are multiple source agents, an agent can also be successful if he
hears w from other source agents. Thus, the ‘feedback’ scheme only takes effect on
the unsuccessful source agents after ‘broadcasting’.
The above procedure of word transmission repeats, until βNG words in W have
been transmitted.
The above-described algorithm iterates on the entire population of agents. Recall
that the groups are chosen uniformly randomly, so every group member can speak
words from his memory if it is not empty, and the intra-group local success leads
to eliminating those non-consented words from group members’ memories. As a
result, the iterations of this procedure will eventually lead to global convergence to
a common word. The iterations stop once the global consensus state is reached.
It is remarked that the NGG model will be degenerated to the MHNG model [5]
by two simpliﬁcations: (1) Let the seed agent be the only speaker, and allow only one
word be transmitted from this speaker to multiple hearers at each iteration. (2) Set a
ﬁxed threshold to determine the success of the speaker, i.e., if nsuc/NG < 1 then the

54
4
Naming Game with Multi-Hearers or Group Discussions
speaker does not consent; otherwise, if nsuc/NG = 1 then the speaker is consented
with all the hearers in this iteration.
Input: A, N, NG, and β
1 Initialize the memory of each agent to be null;
2 while Stopping criterion is not met do
3
Randomly pick a seed agent from the entire population, and randomly pick
min(ks, NG −1) agents from the neighboring agents of the seed agent, so that a group G
with min(ks + 1, NG) agents is formed;
4
Every agent in G utters a word, forming a unique list CW of candidate words;
5
For every word w ∈CW, calculate Pw according to Eqs.(4.1), (4.2), (4.3), and (4.4);
6
According to Pw, choose ⌊βNG⌋words from CW, forming the transmission word list W;
7
Initialize the set of unsuccessful group member U = G;
8
foreach word w ∈W according to its selected order do
9
Find the set of source agent S;
10
Initialize the set H = , which represents the agents who can hear w;
11
foreach agent Ai ∈U do
12
Check whether Ai can hear w by Ph according to Eq. (4.5);
13
Add Ai into H if Ai can hear w;
end
14
foreach agent Ai ∈H do
15
If w is in agent Ai’s memory, it is a local success, delete Ai from U; otherwise, it
is unsuccessful;
end
16
Count the number of successful agents nsuc;
17
foreach agent Ai ∈S do
18
If Ai ∈U, set it be successful with probability nsuc/N;
19
Delete Ai from U if Ai becomes successful;
end
end
end
Algorithm 1: The pseudo code for the NGG model.
The detailed operations in the NGG model is illustrated by the pseudo code of
Algorithm 1. The input includes the underlying network A, the population size N, the
group size NG, and the parameter β. Agents are initialized with nothing in memory,
but with unlimited capacity of memorization. The stopping criterion in NGG is
either a predeﬁned maximum number of iterations (e.g., 1 × 107), or the global
consensus state is reached. Moreover, CW represents all the words uttered by all the
group members (as a speaker respectively), while W represents the shortlisted words,
which can be heard by the group members. The words {w ∈CW|w /∈W} are not
discussed in this iteration, since they are lack of importance or popularity. In addition,
U represents the agents having not reached local consensus in the current iteration.
Local consensus is checked one word after another in CW, where S represents a list
of agents that utter the current word and H represents a list of agents that could hear
this word.

4.4 Simulation Results
55
4.4
Simulation Results
In this section, both MHNG and NGG models are investigated and compared. Three
underlying networks, namely random-graph (RG), small-world (SW), and scale-free
(SF) models are adopted as the underlying communication networks for simulations.
The detailed parameter settings and statistics of these networks are summarized in
Tables3.1 and 3.2, respectively. The population size in simulations is ﬁxed to be
2000 for both MHNG and NGG.
The rest of this section is organized as follows: Sects.4.4.1, 4.4.2 and 4.4.3 show
the simulation results of MHNG. Section4.4.4 studies the convergence process of
NGG. Section4.4.5 compares the convergence time of MHNG, NGG, and the mini-
mal naming game.
4.4.1
General Convergence Process of MHNG
Figures4.6, 4.7, and 4.8 show the convergence processes in terms of (1) the number of
total words held by the entire population, (2) the number of different words existing
in the population, and (3) the success rate of local communications. In the ﬁgures, the
horizontal axis iteration denotes the number of time steps. Note that in the minimal
naming game and FMNG models, the number of time steps is equivalent to the
number of pair-wise communications. However, in the broadcasting model (MHNG)
and group discussions model (NGG), the number of pair-wise communications is
more than the number of time steps, since at each time step there are several speaker-
hearer communications happening simultaneously, e.g., several independent HO-NG
operations, checking for consensus of the speaker, etc.
Figure4.6 shows the convergence process of the number of total words. The
general converging process is similar to that in the minimal naming game. It can be
seen from the ﬁgure that, when the number of hearers (NH) participating in local
communications is set to NH = 5, the curve has the highest peak, much higher
than the other two curves (with NH = 15 and NH = 50). Also, when NH = 5, the
population converges in the lowest speed to global consensus. In all four different
underlying networks, as the number of hearers increases, the peak of a curve becomes
lowered, but the convergence speed is improved meanwhile.
Figure4.7 shows the convergence process in terms of the number of different
words. The curves show a similar phenomenon as that in Fig.4.6: As the number of
hearers increases, the peak of a curve becomes lowered, but the convergence time is
shortened.
Figure4.8 shows the success rate varying against the number of iterations. The
success rate is calculated by the number of successes over the ﬁnal 10 iterations (an
example of success rate calculation is given in Fig.2.4). Here, success means that
all hearers and the speaker reached consensus, as the example shown in Fig.4.4c.
The success rate curve shows that, when the number of hearers increases, it becomes

56
4
Naming Game with Multi-Hearers or Group Discussions
iteration
(a)
104
105
number of total words
2000
3000
4000
5000
NH = 5
NH = 15
NH = 50
iteration
(b)
104
106
number of total words
2000
2500
3000
3500
4000
4500
iteration
(c)
104
106
number of total words
2000
2500
3000
iteration
(d)
104
106
number of total words
2000
2500
3000
3500
4000
Fig. 4.6 Number of total words remembered by the population versus iterations in MHNG on the
underlying network: a ER random-graph network RG/0.01, b BA scale-free SF/9, c WS small-world
SW/10/0.1, d WS small-world SW/10/0.2
earlier to reach a high success rate. When more agents are participating, the proba-
bility of all participating agents having a common word in memory is low. However,
even if the communications in the initial stage are unsuccessful, the speaker will
teach all the un-consented hearers a common word in a broadcasting manner, which
will help facilitate both local and global consensus in the following communications.
4.4.2
Convergence Time of MHNG
Convergence time reﬂects the learning speed of the agents in a naming game model.
In MHNG, convergence time could be counted in two ways: (1) The total num-
ber of time steps (iterations) to reach global consensus; or (2) the total number of
operations to reach global consensus, where an operation means a pair-wise com-
munication between a speaker-hearer pair. In the broadcasting model, although the
speaker utters a word once only, the multiple hearers receive and process the informa-
tion independently, and thus the computational cost is more than that in a pair-wise
communication. For example, if there are Nh participating hearers, and the speaker

4.4 Simulation Results
57
iteration
(a)
104
105
number of different words
100
101
102
103
NH = 5
NH = 15
NH = 50
iteration
(b)
104
106
number of different words
100
101
102
103
iteration
(c)
104
106
number of different words
100
101
102
103
iteration
(d)
104
106
number of different words
100
101
102
103
Fig. 4.7 Number of different words remembered by the population versus iterations in MHNG
on the underlying network: a ER random-graph network RG/0.01, b BA scale-free SF/9, c WS
small-world SW/10/0.1, d WS small-world SW/10/0.2
utters one word to all of them. By the ﬁrst counting method, it is counted as 1, since
all the communications, learning, and consensuses happen in one iteration. In con-
trast, by the second method, it is counted as Nh operations, where each operation is
considered as one procedure that the hearer hears the speaker-uttered word. Here,
the ﬁrst counting method is adopted, which cares more about the time steps the pop-
ulation of agents need to reach the global consensus state from their initial state. The
second counting method is used for reference, which reﬂects the true communication
overhead required to reach global consensus for the agents.
Figures4.9 and 4.10 show the convergence time varying against the changes in
the number of participating hearers, as well the average degree of the network. A bar
of lower height means faster convergence speed (i.e., less time to convergence). As
can be seen from Fig.4.9, (1) when the average degree of the underlying network
increases, the convergence time in terms of iterations reduces; (2) when the number
of hearers increases, the convergence time in terms of iterations reduces as well.
It can also be observed from Fig.4.9 that, when the number of hearers is not
greater than the average degree of the underlying network, the convergence time is
reduced when NH increases. The average degree of each network can be found in
Tables3.1 and 3.2, or estimated according to the statistics of the network (e.g., the

58
4
Naming Game with Multi-Hearers or Group Discussions
iteration
(a)
104
105
success rate
10-3
10-2
10-1
100
NH = 5
NH = 15
NH = 50
iteration
(b)
104
106
success rate
10-3
10-2
10-1
100
iteration
(c)
104
106
success rate
10-2
10-1
100
iteration
(d)
104
106
success rate
10-2
10-1
100
Fig. 4.8 Success rate in MHNG on the underlying network: a ER random-graph network RG/0.01,
b BA scale-free SF/9, c WS small-world SW/10/0.1, d WS small-world SW/10/0.2
approximateaveragedegreeofRG/0.01isabout⟨kGR/0.01⟩≈2000 × 0.01 = 20,and
its real average degree is 19.9731 according to Table3.1). This is because, when more
hearers are listening to the same speaker, it facilitates the propagation of common
words to the agents. The common words remembered by the agents is one of the
necessary conditions to reach global consensus.
For homogeneous networks (RG and SW), when the number of hearers is greater
than the average degree of the network, the decrease of convergence time is not
signiﬁcant. In contrast, for heterogeneous networks (SF), the convergence time con-
tinuously decreases as the number of hearers increases. As discussed earlier, for an
agent A,itstotalnumberofhearers Nh(A)isdeterminedby Nh(A) = min{Nh, k(A)},
where Nh is the number of hearers and k(A) is the degree of agent A in the underlying
network. In a homogeneous network, due to its homogeneity of degree distribution,
when Nh > ⟨k⟩, it is reasonable to assume that k(A) = min{Nh, k(A)}. However, in
a heterogeneous network, this cannot be assumed since there are quit a few hub nodes
whose degrees are much greater than ⟨k⟩. When a hub-node agent is selected to be
the speaker, it could be assumed that Nh = min{Nh, k(A)}, and thus a large number
of hearers would facilitate the propagation of a common word in an iteration.

4.4 Simulation Results
59
200
NH
50
20
15
10
5
RG/0.05
RG/0.025
RG/0.01
RG
RG/0.005
10
5
0
×104
convergence iteration
200
NH
50
20
15
10
5
SF/29
SF/24
SF
SF/9
SF/4
2
1
0
×105
200
NH
50
20
15
10
5
SW/50/0.1
SW/25/0.1
SW
(with RP = 0.1)
SW/10/0.1
SW/5/0.1
3
2
1
0
×105
200
NH
50
20
15
10
5
SW/50/0.2
SW/25/0.2
SW
(with RP = 0.2)
SW/10/0.2
SW/5/0.2
15
10
5
0
×104
(b)
(a)
(d)
(c)
Fig. 4.9 Convergence time counted by the number of iterations (one-to-many communications) to
reach global consensus on: a ER random-graph, b BA scale-free, c WS small-world with RP = 0.1,
d WS small-world with RP = 0.2. NH represents the number of participating hearers in a local
communication
Figure4.10 shows the convergence time in terms of communicating operations.
It is clear from Fig.4.10a, c, d that the convergence time generally increases when
the number of hearers increases. This means that the multi-hearer setting does not
essentially reduce the number of operations, but rather, reduces the time steps only.
In Fig.4.10b, it can be observed from networks SF/9, SF/24, and SF/29 that the
convergence time increases ﬁrst and then decreases, when the NH increases, while
for network SF/4, the convergence time decreases when the NH increases. This is due
to the heterogeneity of the SF degree distribution. When an agent at the hub position
of the underlying network is selected to be the speaker, a larger number of degrees
sufﬁces a large NH value, thus facilitating the broadcasting of common words.
4.4.3
Peak of Convergence Curve of MHNG
The peak of convergence curve in terms of the number of total words is denoted the
maximum number of total words (MNTW), while the peak of the difference-word

60
4
Naming Game with Multi-Hearers or Group Discussions
200
NH
50
20
15
10
5
RG/0.05
RG/0.025
RG/0.01
RG
RG/0.005
2
0
6
4
×105
convergence operation
200
NH
50
20
15
10
5
SF/29
SF/24
SF
SF/9
SF/4
10
5
0
×105
200
NH
50
20
15
10
5
SW/50/0.1
SW/25/0.1
SW
(with RP = 0.1)
SW/10/0.1
SW/5/0.1
2
1
0
×106
200
NH
50
20
15
10
5
SW/50/0.2
SW/25/0.2
SW
(with RP = 0.2)
SW/10/0.2
SW/5/0.2
15
10
5
0
×105
(b)
(a)
(c)
(d)
Fig. 4.10 Convergence time counted by the number of operations (pair-wise communications) to
reach global consensus on: a ER random-graph, b BA scale-free, c WS small-world with RP = 0.1,
d WS small-world with RP = 0.2. NH represents the number of participating hearers in a local
communication
curveisdenotedthemaximumnumberofdifferentwords(MNDW).Generally,ascan
be seen from Figs.4.11 and 4.12, MNTW and MNDW show a similar phenomenon as
that shown in Fig.4.9: (1) When the average degree increases, the maximum number
oftotalwordsincreases(seeFig.4.11),whilethemaximumnumberofdifferentwords
decreases (see Fig.4.12). (2) If the number of hearers is not greater than the average
degree of the network, when the value of NH increases, both MNTW and MNDW
decrease. Otherwise, if the number of hearers is greater than the average degree of
the network, when the number of NH increases, only in heterogeneous networks
(Figs.4.11b and 4.12b) both MNTW and MNDW decrease, while for homogeneous
networks (Figs.4.11a, c, d and 4.12a, c, d), MNTW and MNDW stop decreasing
when NH is just greater than the average degree of the network.
Recall that there is no interlacing in the convergence curves shown in Figs.4.6
and 4.7, thus, MNTW is able to reﬂect the same phenomenon as it is shown in the
convergence curve of total words, i.e., higher MNTW (higher peak) represents greater
convergence time, and vise versa. Similarly, MNDW reﬂects the same phenomenon
as it is shown in the convergence curve of different words.

4.4 Simulation Results
61
5
10
NH
15
20
50
200
RG/0.005
RG/0.01
RG/0.025
RG/0.05
RG
5000
10000
15000
0
5
10
NH
15
20
50
200
SF/4
SF/9
SF/24
SF
SF/29
0
5000
10000
15000
5
10
NH
15
20
50
200
SW/5/0.1
SW/10/0.1
SW/25/0.1
SW (with RP = 0.1)
SW/50/0.1
0
5000
10000
max number of total words
5
NH
10
15
20
50
200
SW/5/0.2
SW/10/0.2
SW/25/0.2
SW (with RP = 0.2)
SW/50/0.2
10000
5000
0
(c)
(d)
(b)
(a)
Fig. 4.11 The maximum number of total words on: a ER random-graph, b BA scale-free, c WS
small-world with RP = 0.1, d WS small-world with RP = 0.2. NH represents the number of
participating hearers in a local communication
4.4.4
General Convergence Process of NGG
Figures4.13, 4.14, and 4.15 show the convergence processes in terms of (1) the
number of total words, (2) the number of different words, and (3) the success rate of
local communications. Again, the same 4 underlying networks are used for simula-
tions, namely RG/0.01, SF/9, SW/10/0.1, and SW/10/0.2. The detailed settings and
statistics of these networks are shown in Tables3.1 and 3.2, respectively.
Figure4.13 shows the convergence process of the number of total words. The
parameters are set to β = 0.5 and NG = ⟨k⟩, where ⟨k⟩is the average degree of
the underlying network, thus βNG = 0.5⟨k⟩. As can be seen from Fig.4.13, the
population converges much faster in RG and SF than in SW. Also, when the average
degree of the network increases form ⟨k⟩= 10 to ⟨k⟩= 20 and then to ⟨k⟩= 50, the
convergence speed signiﬁcantly increases. However, when it is further increased to
⟨k⟩= 100, the convergence process is slowed down again.
In Fig.4.14, especially in Fig.4.14c, d, it is clear that as the network average degree
⟨k⟩increases, the convergence curve of the number of different words becomes ﬂatter
with a lower peak. Generally, when the underlying network has better connectivity,
the convergence process would likely be improved. Differently, when ⟨k⟩= 100,

62
4
Naming Game with Multi-Hearers or Group Discussions
RG/0.005
RG
RG/0.01
RG/0.025
RG/0.05
200
50
20
15
NH
10
5
200
100
0
400
300
SF/4
SF
SF/9
SF/24
SF/29
200
50
20
NH
15
10
5
200
400
600
0
max number of different words
SW/5/0.1
SW/10/0.1
SW/25/0.1
SW
(with RP = 0.1)
SW/50/0.1
200
50
20
15
NH
10
5
200
0
400
SW/5/0.2
SW/10/0.2
SW/25/0.2
SW
(with RP = 0.2)
SW/50/0.2
200
50
20
15
NH
10
5
400
0
200
(a)
(b)
(c)
(d)
Fig. 4.12 The maximum number of different words on: a ER random-graph, b BA scale-free, c
WS small-world with RP = 0.1, d WS small-world with RP = 0.2. NH represents the number of
participating hearers in a local communication
the convergence process seems slow down as shown in Fig.4.13, but speed up as in
Fig.4.14. This is due to the setting of βNG = 0.5⟨k⟩. When ⟨k⟩increases to 100,
the group size for local negotiation is also NG = 100, where βNG = 0.5⟨k⟩= 50
words are discussed at each iteration. Therefore, essentially, the convergence process
is facilitated. But, due to the large group size, many hearers are learning a new word
immediately after they were consented to another word in the last conversation.
Therefore, the number of total words is greater in ⟨k⟩= 100 than it is in the cases of
⟨k⟩= 10 and ⟨k⟩= 20.
The success rate displayed in Fig.4.15 also shows the phenomenon that, when
⟨k⟩increases from 10 to 20 and then to 50, the success rate curves converge (to 1)
faster, while when ⟨k⟩= 100, the success rate curves converge slower than before,
but still faster than the case of ⟨k⟩= 10.
In Figs.4.13, 4.14, and 4.15, the convergence processes are demonstrated when the
connectivity (or the average degree) of a network changes. Next, in Figs.4.16, 4.17,
and 4.18, the convergence processes are illustrated when the parameter β changes.
As shown in Figs.4.16 and 4.17, the population converges in the fastest speed on
RG and SF networks. When RG and SF are employed as the underlying networks,
the peaks of both curves of total-words and different-words increase as the value of

4.4 Simulation Results
63
iteration
(a)
500
103
2500
number of total words
2000
2200
2400
2600
2800
<k>=10
<k>=20
<k>=50
<k>=100
iteration
(b)
103
3000
number of total words
2000
2200
2400
2600
2800
iteration
(c)
103
104
number of total words
2000
2002
2004
2006
2008
iteration
(d)
103
104
number of total words
2000
2020
2040
2060
Fig. 4.13 Number of total words remembered by the population versus iterations in NGG on the
underlying network: a ER random-graph network RG/0.01, b BA scale-free SF/9, c WS small-world
SW/10/0.1, d WS small-world SW/10/0.2. The parameter β is set to 0.5, the group size NG is set
to ⟨k⟩. In each subplot, the 4 curves show the differences when the different average degrees of the
underlying networks change
β increases. However, when SW is employed as the underlying network, the peak
of the total-words curve is the highest at β = 0.25, but the lowest at β = 0.5. In
contrast, the change of the value of β makes no clear difference on the peaks of the
different-words curves.

64
4
Naming Game with Multi-Hearers or Group Discussions
iteration
(a)
500
103
2500
number of different words
0
50
100
150
200
250
<k>=10
<k>=20
<k>=50
<k>=100
iteration
(b)
103
3000
number of different words
0
100
200
300
400
500
iteration
(c)
103
104
number of different words
0
20
40
60
80
iteration
(d)
103
104
number of different words
0
20
40
60
80
100
Fig. 4.14 Number of different words remembered by the population versus iterations in NGG on
the underlying network: a ER random-graph network RG/0.01, b BA scale-free SF/9, c WS small-
world SW/10/0.1, d WS small-world SW/10/0.2. The parameter β is set to 0.5, the group size NG
is set to ⟨k⟩. In each subplot, the 4 curves show the differences when the different average degrees
of the underlying networks change
4.4.5
Convergence Time Analysis
A comparison of MHNG with two different measures for the convergence speed is
presented in Figs.4.9 and 4.10 in Sect.4.4.2. It is shown that MHNG reduces the
number of iterations for reaching global consensus, but increases the number of total
operations for reaching global consensus. In Fig.4.19, the two group-based naming
game models (MHNG and NGG) are compared to the minimal naming game model.
In the minimal naming game model, each iteration includes one operation only,
where an operation refers to a speaker-hearer pair-wise communication. In both
MHNG and NGG, however, each iteration may include several operations, depending

4.4 Simulation Results
65
iteration
   (a)
500
1000
1500 2000 2500
success rate
0.8
0.85
0.9
0.95
1
<k>=10
<k>=20
<k>=50
<k>=100
iteration
(b)
1000
2000
3000
success rate
0.7
0.8
0.9
1
iteration
(c)
103
104
success rate
0.98
0.985
0.99
0.995
1
iteration
(d)
103
104
success rate
0.95
0.96
0.97
0.98
0.99
1
Fig. 4.15 Success rate of local communications versus iterations in NGG on the underlying net-
work: a ER random-graph network RG/0.01, b BA scale-free SF/9, c WS small-world SW/10/0.1,
d WS small-world SW/10/0.2. The parameter β is set to 0.5, the group size NG is set to ⟨k⟩. In each
subplot, the 4 curves show the differences when the different average degrees of the underlying
networks change
on group size. Meanwhile, the number of operations at each iteration is also limited
by the connection degrees of the agents.
As shown in Fig.4.19a, b, NGG essentially reduces both the number of iterations
and the number of operations when the underlying network is RG or SF, compared
to the minimal naming game. It is also shown in Figs.4.13, 4.14, 4.15, 4.16, 4.17
and 4.18 that the convergence curves on RG and SF networks are pretty short. As
discussed in Sect.4.4.2, MHNG also needs a small number of iterations similarly to
NGG, but essentially the number of operations in MHNG is much more than that of
NGG and the minimal naming game. This is because in each group communication of
NGG, the transmitted-words are ordered according to the popularity, which facilitates
the emergence of dominant word. However, in MHNG, the transmitted-words are

66
4
Naming Game with Multi-Hearers or Group Discussions
iteration
(a)
500
600
700
800
900
number of total words
2000
2100
2200
2300
β = 0.25
β = 0.5
β = 0.75
iteration
(b)
500
600
700
800
900 1000
number of total words
2000
2010
2020
2030
iteration
(c)
103
104
number of total words
2000
2005
2010
iteration
(d)
103
number of total words
2000
2010
2020
2030
Fig. 4.16 Number of total words remembered by the population versus iterations in NGG on the
underlying network: a ER random-graph network RG/0.01, b BA scale-free SF/9, c WS small-world
SW/10/0.1, d WS small-world SW/10/0.2. Both the average degree ⟨k⟩and the group size NG are
set to 10. In each subplot, the 3 curves show the differences when β changes
always random, which forces the multiple hearers to learn some minority word in
one iteration, and to be forgotten in another iteration. Thus, MHNG requires more
operations than the minimal naming game model to reach global convergence.
In Fig.4.19c, d, the underlying network is SW. In this case, both NGG and MHNG
need larger numbers of operations than the minimal naming game to reach global con-
sensus. As for the number of iterations, especially shown by Fig.4.19d, both NGG
and MHNG need smaller numbers of iterations than the minimal naming game.
Note that, in Fig.4.19, the number of operations is equal to the number of iterations
in the minimal naming game, where NG-NO represents both NG-NO and NG-NI.
Compared to RG and SF, SW networks are highly clustered (see the clustering coef-
ﬁcients ⟨cc⟩in Tables3.1 and 3.2), thus, both MHNG and NGG naturally facilitate
the local-consensus in the clustered sub-regions.
Suppose that the population is converging towards the dominant word w1, while
there is a relatively clustered small group (with Nsg agents) holding an internal-
dominant word w2. Let us deﬁne the event “a w1-agent persuade a w2-agent” be E1,
and “a w2-agent persuade a w1-agent” be operation E2. Ideally, with E1 happening

4.4 Simulation Results
67
iteration
(a)
500
600
700
800
900
number of different words
0
50
100
150
β = 0.25
β = 0.5
β = 0.75
iteration
(b)
500
600
700
800
900 1000
number of different words
0
10
20
30
40
iteration
(c)
103
104
number of different words
0
5
10
15
iteration
(d)
103
number of different words
0
2
4
6
8
10
Fig. 4.17 Number of different words remembered by the population versus iterations in NGG on
the underlying network: a ER random-graph network RG/0.01, b BA scale-free SF/9, c WS small-
world SW/10/0.1, d WS small-world SW/10/0.2. Both the average degree ⟨k⟩and the group size
NG are set to 10. In each subplot, the 3 curves show the differences when β changes
Nsg times, the global convergence can be reached. However, E2 is inevitable, and the
number of occurrence is assumed to be nE2. For the minimal naming game, a roughly
expected number of events is nmin = E1 × Nsg + (E1 + E2) × nE2, while for a
group-based naming game, ngroup = E1 × Nsg + (E1 + E2) × nE2 × NG, where
NG is the group size in NGG or the number of hearers in MHNG. Further, simply
assume E = E1 = E2, then, nmin = E × (Nsg + 2nE2), and ngroup = E × (Nsg +
2nE2 × NG). For the minimal naming game, the required number of iterations and
number of operations are both nmin, while for the group-wise model, the required
number of iterations N Igroup = ngroup/Ng = E Nsg/Ng + 2nE2. Clearly, the group-
based model requires less number of iterations (N Igroup < nmin), but more number
of operations (ngroup > nmin). The above calculation of computational cost roughly
explains the phenomenon observed in Fig.4.19c, d.
In general, when more agents are involved in the communication of an iteration,
the number of iterations needed to reach global consensus of the population will be
reduced. The MHNG model does not reduce the number of operations for reaching
global consensus, however. The NGG model reduces both the number of iterations
and the number of operations when the underlying networks are not locally clustered.

68
4
Naming Game with Multi-Hearers or Group Discussions
iteration
(a)
500
600
700
800
900
success rate
0.996
0.997
0.998
0.999
1
β = 0.25
β = 0.5
β = 0.75
iteration
(b)
500
600
700
800
900 1000
success rate
0
0.5
1
iteration
(c)
103
104
success rate
0.96
0.97
0.98
0.99
1
iteration
(d)
103
success rate
0.94
0.96
0.98
1
Fig. 4.18 Success rate of local communications versus iterations in NGG on the underlying net-
work: a ER random-graph network RG/0.01, b BA scale-free SF/9, c WS small-world SW/10/0.1,
d WS small-world SW/10/0.2. Both the average degree ⟨k⟩and the group size NG are set to 10. In
each subplot, the 3 curves show the differences when β changes
When the underlying networks are locally clustered, i.e., the networks have higher
clustering coefﬁcient values (which can be found in Tables3.1 and 3.2), neither
MHNG nor NGG can reduce the number of operations by introducing more agents
into each local communication in an iteration.
The convergence time of NGG inﬂuenced by the parameter β is shown in Fig.4.20,
which can also be regarded as the inﬂuence of parameter β on global consensus. Here,
the convergence time is counted by the number of iterations from initialization to
reaching global consensus. Figure4.20c shows that the population is slower in reach-
ing global consensus on SW/10/0.2 than on RG/0.01 and SF/9 (shown in Fig.4.20a,
b, respectively). This is because, when the underlying network has local clustered
structures (with larger clustering coefﬁcients), the population reaches global con-
sensus slowly. On the other hand, when the value of β varies from 0.1 to 1.0, the
convergence time is basically not inﬂuenced.

4.4 Simulation Results
69
MH-NO
NGG-NO
MH-NI
NGG-NI
NG-NO
RG/0.05
RG/0.025
RG/0.01
RG/0.005
0
4
6
2
×105
convergence time
MH-NO
NGG-NO
MH-NI
NGG-NI
NG-NO
SF/29
SF/24
SF/9
SF/4
0
5
10
×105
MH-NO
NGG-NO
MH-NI
NGG-NI
NG-NO
SW/50/0.1
SW/25/0.1
SW/10/0.1
SW/5/0.1
3
2
1
0
×106
MH-NO
NGG-NO
MH-NI
NGG-NI
NG-NO
SW/50/0.2
SW/25/0.2
SW/10/0.2
SW/5/0.2
0
5
10
×105
(a)
(b)
(d)
(c)
Fig. 4.19 Comparison on convergence time of the minimal naming game, MHNG, and NGG on:
a ER random-graph network RG/0.01, b BA scale-free SF/9, c WS small-world SW/10/0.1, d WS
small-world SW/10/0.2. Here, ‘-NO’ represents the number of operations to reach global consensus,
while ‘-NI’ represents the number of iteration to reach global consensus
β
 (a)
0
0.2
0.4
0.6
0.8
1.0
convergence time
1000
2000
3000
4000
5000
β
 (b)
0
0.2
0.4
0.6
0.8
1.0
2000
3000
4000
5000
6000
β
 (c) 
0
0.2
0.4
0.6
0.8
1.0
×104
1
1.5
2
2.5
3
3.5
Fig. 4.20 The convergence time versus β in NGG on: a RG/0.01, b SF/9, c SW/10/0.2. The group
size is set to NG = ⟨k⟩, where ⟨k⟩is the average degree of the network

70
4
Naming Game with Multi-Hearers or Group Discussions
4.5
Conclusion
Group broadcasting and group discussion are common in human communications,
but in most naming game models only peer-to-peer communication is considered. In
this chapter, the naming game model is extended to a group conversation scenario.
First, the multi-hearer naming game (MHNG) is introduced in Sect.4.2, which mim-
ics the real-world scenario of information broadcasting. At each time step, a speaker
and multiple of his neighboring hearers are picked. The speaker utters a word to
all the hearers simultaneously, and the hearers independently decide whether they
are consented to the new word or not, based on their previously-learned words in
the memories. Only if all the hearers are consented, the process is successful, and
meanwhile the speaker also consents to the same word. In the second part of the
chapter, the naming game in groups (NGG) is introduced as a further extension of
the MHNG model. In NGG, each iteration is a group discussion, rather than a group
broadcasting. In such a group, every agent is both speaker and hearer. The scenario
is more complicated but more realistic. It is found that both MHNG and NGG signif-
icantly reduce the number of iterations for the population of agents to reach global
consensus as compared to the minimal naming game.
References
1. A. Baronchelli, Role of feedback and broadcasting in the naming game. Phys. Rev. E 83, 046103
(2011). https://doi.org/10.1103/PhysRevE.83.046103
2. H.X. Yang, W.X. Wang, B.H. Wang, Asymmetric negotiation in structured language games.
Phys. Rev. E 77(2), 027103 (2008)
3. R.R. Liu, W.X. Wang, Y.C. Lai, G.R. Chen, B.H. Wang, Optimal convergence in naming game
with geography-based negotiation on small-world networks. Phys. Lett. A 375(3), 363–367
(2011)
4. S.K. Maity, A. Mukherjee, F. Tria, V. Loreto, Emergence of fast agreement in an overhearing
population: The case of the naming game. EPL (Europhys. Lett.) 101(6), 68004 (2013)
5. B. Li, G.R. Chen, T.W.S. Chow, Naming game with multiple hearers. Comm. Nonl. Sci. Numer.
Simul. 18, 1214–1228 (2013). https://doi.org/10.1016/j.cnsns.2012.09.022
6. A. Baronchelli, A gentle introduction to the minimal naming game. Belgian J. Linguist 30(1),
171–192 (2016)
7. C. Castellano, S. Fortunato, V. Loreto, Statistical physics of social dynamics. Rev. Mod. Phys.
81(2), 591–646 (2009)
8. L. Conradt, C. List, Group decisions in humans and animals: a survey. Philos. Trans, R. Soc.
Lond. B: Bio. Sci. 364(1518), 719–742 (2009)
9. R. Axelrod, W.D. Hamilton, The evolution of cooperation. Science 211(4489), 1390–1396
(1981)

Chapter 5
Communications with Learning Errors
5.1
Introduction
In the real-life scenario of human communications, both the local communication
process of agents and the information propagation on the underlying network affect
the achievement and speed of global convergence. For example, if agents can learn
fast and correctly from local communications, and thereafter teach their neighbors to
effectively learn the same, then the entire population would be able to reach global
convergence efﬁciently; or, if the acquaintanceship of agents is simple, meaning
that the underlying network is simple, then the transmitted information would be
propagated efﬁciently over the entire network. Here, a simple underlying network
is one with good connectivity (large average degree), but with less local-clustered
structures (low clustering coefﬁcient). However, realistically, language acquisition
is always error-prone. This problem in human language leads to ambiguities with
learning errors in human conversations, thereby degrading the effectiveness of human
communications. Interestingly, it was suggested in [1] that learning errors can actu-
ally increase diversity of the linguistic system by introducing additional information.
Thus, learning errors are able to help prevent the linguistic system from being trapped
in sub-optimum states, beneﬁcial for the evolution of a more efﬁcient language. Here,
the linguistic system is evaluated by a function of payoff. It was also found in [1]
some thresholds of the learning error rate for certain models, where if the error rate
is below the threshold then the system gains advantage from learning errors; other-
wise, if the error rate is above the threshold then the errors or mistakes will impair
the system, e.g., reducing signiﬁcantly the payoff of the linguistic system. Moreover,
noise may lead to recurrently converging states of a Markov chain model, which is
considered beneﬁcial for better detecting social interactions [2]. Therefore, errors or
noise may affect the language system positively, to some extent, as in the two cases
mentioned above.
However, the naming game models studied in this book neither aim to form an
efﬁcient linguistic system as in [1], nor to reach a series of recurrently converging
states as in [2]. Instead, the issue of learning errors will be studied from the per-
© Springer Nature Switzerland AG 2019
G. Chen and Y. Lou, Naming Game, Emergence, Complexity and Computation 34,
https://doi.org/10.1007/978-3-030-05243-0_5
71

72
5
Communications with Learning Errors
spective of human communications in a common sense that learning errors likely
bring negative effects to communications. In real-life situations, everyone may make
mistakes, but it could be expected that if one is more experienced then he will prob-
ably make fewer mistakes or even know how to avoid making new mistakes. It is
therefore natural to consider naming game as a process of learning with errors. From
this consideration, a realistic scenario in naming game is studied, where agents are
all error-prone initially but gradually they learn how to reduce and even to avoid
making further errors, thereby eventually they are all error-free, so that the whole
population will reach consensus asymptotically.
Speciﬁcally, naming game is studied in an environment where learning errors are
common during pair-wise communications. Learning errors are characterized by the
error rates with a uniform probability distribution in [0, 1]. First, naming game model
with learning errors (NGLE) in communications is proposed. Then, a strategy to pre-
vent perpetual learning errors is suggested, by which agents are able to learn from
errors. To that end, the three typical networks, namely random-graph, small-world
and scale-free networks, are employed to investigate the effects of various learning
errors. Simulation results show that: (1) Learning errors marginally affect (neither
speed up nor hinder) the convergence time. (2) The existence of learning errors dis-
tinctively increases the required memory size of each agent during the gaming. (3)
The maximum number of different words held by the population increases polyno-
mially as the error rate increases. (4) Very small learning errors does not impair the
convergence at all, when no strategy is applied to prevent agents from making per-
petual learning errors. In this case, there exists some threshold, only if the error rate
is above which, the global convergence is hindered, which below which, the global
convergence is not impaired.
The rest of this chapter is organized as follows. Section5.2 introduces the NGLE
model. Section5.3 presents comprehensive simulation results with analysis and com-
parisons. Section5.4 draws a conclusion with some discussion.
5.2
Communications with Learning Errors
To introduce the scenario of the naming game with learning errors in communica-
tions, consider the information transmission path with the speaker (sender) end, the
communication media, and the hearer (receiver) end. In the communications, the
hearer is to learn the new information sent by the speaker, where learning errors may
occur. Indeed, there are many possibilities for errors to emerge from the learning
process, for example, ambiguous pronunciations of the speaker, hearing or vision
impaired of the hearer, and imperfect coding and decoding of the media, etc. An
example is shown in Fig.5.1, where learning errors may affect any entity in between
the speaker and hearer.
Here, in NGLE, the reason and source of generating errors are not examined. Any
learning error is regarded as noise to communication, in a uniform distribution. Only
the question how learning errors affect the convergence process is addressed.

5.2 Communications with Learning Errors
73
Fig. 5.1 Communication
methods between two agents,
where learning errors may
occur in any entity of the
speaker-hearer
communication system
coding
writing
speaking
cable
(e-)mail
air
decoding
reading
hearing
Speaker
Hearer
Fig. 5.2 Communications
between speaker and hearer
with learning errors
no
Speaker
utters ‘bit’
Hearer hears 
either ‘bit’ or ‘fit’
Learning 
error?
yes
‘bit’ becomes 
‘fit’, for example
It is assumed that the occurrence of an error is represented by a numerical value,
the error rate. Although different types of errors may have different inﬂuences on
communications, it is assumed that, in NGLE, all types of errors have the same
outcome, i.e., the hearer receives a wrong word that was not uttered by the speaker.
Neither the speaker nor the hearer is aware of the existence of the error. So, in NGLE,
the speaker takes the uttered-word as the true word and the hearer takes the heard-
word as the true one, despite the fact that they are different. The occurrence of such
learning errors is described and determined by the error rate. For example, given a
word ‘bit’ uttered by the speaker, the hearer would hear it correctly only with a certain
probability; while with its complementary probability the hearer would receive a
wrong word, say ‘ﬁt’, but the hearer believes this is the correct word that the speaker
uttered. Figure5.2 shows an illustrative example, where the conditional judgment
rhombus represents all the different communication media shown in Fig.5.1.
People make mistakes from time to time, also humans usually learn how to correct
mistakes thereby improving themselves so that they would not make mistakes per-
petually, especially on the same thing. Ideally, good experiences can be obtained and
improvement can be expected in human communication practice. In naming game
models, there is a population of agents and a great number of lexicons. Lexicons
are objective and would not be wrong themselves, but un-experienced agents are

74
5
Communications with Learning Errors
error-prone. In NGLE, agents are assigned the characteristics that they are able to
learn to be more reliable to prevent learning errors by themselves. There are many
ways for agents to learn to be more precise and reliable. Empirically and practically,
educating and training agents are effective to improve their abilities of understanding
and expressing in conversations. Double-checking before speaking out, or sending
out information, and the ﬁnding ways to verify it, provide good solutions to the prob-
lem as well. Sometimes, redundant medium or redundant information may also help,
e.g., sending out an email accompanied with an auxiliary voice message to assist the
hearer to verify and conﬁrm.
In the NGLE model, the simple strategy of agent self-learning to prevent errors is
adopted. Each agent becomes experienced after he has been a speaker in the commu-
nications, since such a local-communication could also be considered as a teaching
process for the speaker. It is assumed that one can obtain more experiences being a
speaker than being a hearer. After being a speaker, the agent is assumed experienced
and would not introduce mistake in the forthcoming pair-wise communications. This
also follows the old saying that teaching and learning grow hand in hand. This say-
ing implies that teaching offers experiences to the teacher (the speaker in a naming
game). It is also assumed that communicating correctly is the symbol of being experi-
enced. Therefore, in the early stage of the NGLE process, communications of agents
are essentially error-prone, while in the later stage, occurrence of errors would be
gradually decreased until all agents have been speaker at least once in the gaming,
such that the entire population become experienced, hence error-free.
The basic communication framework in the NGLE model is exactly the same as
that in the minimal naming game. The only difference is that, in NGLE, agents are
error-prone initially and then become error-free through learning.
Speciﬁcally, the model of NGLE is summarized as follows, where all the random
variables (operations) are referred to uniformly distributed.
(1) A population of N agents connected in a certain topology (the underlying com-
munication network) is initialized with empty memories. Each agent has inﬁnite
capacity of memory. An external vocabulary is initialized with a very large num-
ber of different words.
(2) At each iteration, a speaker is randomly picked from the population:
(2.1) If the speaker has nothing in memory, then he randomly picks a word, w,
from the vocabulary;
(2.2) Otherwise, the speaker randomly picks a word, w, from his memory.
(3) A hearer is randomly picked from the neighborhood of the speaker. The speaker
sends the word w to the hearer.
(3.1) If the hearer has never been a speaker before, then within the pre-set error
rate ρ > 0, he receives w′ other than w;
(3.2) Otherwise, the hearer receives the word w correctly.

5.2 Communications with Learning Errors
75
(4) The hearer checks if it has the same word w in his memory:
(4.1) If the w is already in his memory, then it is a local success (consensus),
so both the speaker and the hearer clear out their memories except keeping
only this word w;
(4.2) Otherwise, it is a local failure, and thus the hearer adds the new word w into
his memory.
(5) Repeat Step 2 to Step 4 iteratively, until all agents keep one and only one same
word, or until the number of iterations reaches a pre-set (large enough) number
for termination.
The speaker-hearer pair is picked using the direct strategy [3, 4]. In Step 3, all
types of possible errors are uniﬁed as a single numerical value, namely the error
rate. In the following, Step 3 and Step 4 are interpreted in details, which consist of
the main communication process in NGLE. Affected by the existence of learning
errors, the process becomes quite different from the minimal naming game [5]: In
the minimal naming game model there are only two possible outcomes (local success
and local failure) in each iteration; while in NGLE there are four possible outcomes
as described below.
Figure5.3 shows an example of four possible outcomes at one iteration in NGLE.
In Fig.5.3a, the ﬁrst case of NGLE is that when there is no learning error in the
communication, meaning that the word ‘signal’ is sent from the speaker to the hearer
directly and correctly. Then, the hearer learns and adds the word ‘signal’ into his
memory, since he does not have this word in memory before learning it. This case is a
local failure without any inﬂuence of learning errors. It is analogous to the situation
where the hearer directly learns a new word from the speaker as in the minimal
naming game. The second case shown in Fig.5.3b is a local success (consensus),
where the hearer has the speaker-uttered word ‘bite’ in memory, and thus both empty
their memories except keeping the common word ‘bite’ only. The two cases shown
in Fig.5.3a, b are exactly the same as the two possible outcomes of the minimal
naming game in one iteration [5].
Now, there are two more other possible outcomes in the NGLE. These two out-
comes are inﬂuenced by learning errors. The third case is shown in Fig.5.3c. The
speaker utters a word ‘right’ to the hearer, with a certain probability (error rate, see
Fig.5.2) the hearer receives a wrong word (‘night’). Since the word ‘night’ is not in
the hearer’s memory, he learned it. However, note that if the hearer has the speaker-
uttered word (in this example, the hearer has a word ‘right’ in memory), but due to
the inﬂuence of learning errors, they missed an opportunity to reach a local success
(consensus).
The fourth case shown in Fig.5.3d is an interesting one, which is opposite to
the third case shown in Fig.5.3c. In the third case, agents missed opportunities to
reach local consensus, but in the fourth case agents gain some more opportunities
for reaching local consensus. In Fig.5.3d, the speaker says ‘right’ but the hearer
hears ‘light’. Coincidentally, the hearer has the word ‘light’ in his memory. Thus,
an ambiguous consensus happens: from the consented feedback of the hearer, the

76
5
Communications with Learning Errors
Fig. 5.3 An example of four
outcomes in one iteration of
NGLE: a local failure
without being inﬂuenced by
learning errors; b local
success without being
inﬂuenced by learning errors;
c local failure inﬂuenced by
a learning error, where the
speaker-uttered word ‘right’
is wrongly received by the
hearer as a word ‘night’; d
local success inﬂuenced by a
learning error, where the
hearer consents to a wrong
word ‘light’, but the
speaker-uttered one is ‘right’
Speaker
right
bite
signal
 failure (without learning error)
 success (without learning error)
(b)
Hearer
bite
light
apple
Speaker
right
bite
signal
Hearer
bite
light
apple
signal
Speaker
right
bite
signal
Hearer
bite
light
apple
Speaker
bite
Hearer
bite
(c) failure (with learning error)
Speaker
right
bite
signal
Hearer
bite
light
apple
Speaker
right
bite
signal
Hearer
bite
light
apple
night
(d) success (with learning error)
Speaker
right
bite
signal
Hearer
bite
light
apple
Speaker
right
Hearer
light
(a)
speaker considers that the hearer agrees with his uttered-word ‘right’, but the hearer
actually agrees with ‘light’ and he believes that this word is what uttered by the
speaker. This situation is referred to as a local pseudo consensus, not a real one.
Consequently, in this example, both speaker and hearer empty their memories, while
the speaker keeps only the word ‘right’, and the hearer keeps only the word ‘light’.
Neither the speaker nor the hearer is aware of the existence of such a learning error.
In real life, this is common due to misunderstanding between the speaker and the
hearer, while neither of them realizes it.
5.3
Simulation Results
The process and performance of NGLE are examined by employing three typical
network topologies as the underlying networks, namely random-graph (RG), small-
world (SW) and scale-free (SF) networks. Extensive simulations are performed to
study the variability of NGLE with comparisons. When the picked speaker has noth-
ing in memory, he will randomly pick a word from the external vocabulary, which

5.3 Simulation Results
77
is inﬁnite in size. When a learning error occurs, the hearer randomly picks a word
from the same vocabulary and this word would be almost surely different from the
speaker-uttered word.
5.3.1
Simulation Setup
Twelve settings of the underlying networks with 2000 nodes are simulated (see
Table 6.1). Population sizes of 200, 500, 1000, and 3000 are also studied in some
networks for investigating their scaling property, with detailed results presented in
[6]. To reduce the randomness, 30 independent runs are performed for each type of
network, and then an average is taken as the ﬁnal result. Thus, the data shown in
Table5.1 and the convergence curves shown in Figs.5.4, 5.5 and 5.6 are all averaged
results from 30 independent simulation trials.
Table 6.1 shows the network settings. Three types of RG networks are employed
by altering the connection probability, including RG/0.005, RG/0.01 and RG/0.025.
Three types of SF networks are used by altering the number of adding nodes at each
step, including SF/4, SF/9 and SF/24. For each SW network, it has two parame-
ters, i.e., the number of neighborhoods and the rewiring probability, thus six com-
binations are examined by altering the two parameters, including SW/5/{0.1, 0.2},
SW/10/{0.1, 0.2}, and SW/25/{0.1, 0.2}.
Different initial states of agent memories, e.g., one-word-per-agent or no-word
initially, would lead to different convergence processes [7]. Here, in the simulation
study of NGLE, it is assumed that each agent has nothing in memory initially, but
each agent is able to remember as many words as he receives. The values of learning
error rate ρ are studied comprehensively. In [8], there are in total 24 different values
of learning error rates are studied, which varies from 0.001 to 0.009 with an increment
of 0.001, from 0.01 to 0.09 with an increment of 0.01, and from 0.1 to 0.5 with an
increment of 0.1, respectively. Note that generally the error rate of system should
be small, otherwise it means quit lot of incorrect information, implying that the
communication system may need improvement. Especially, when the error rate is
greater than 0.5, it means there is more incorrect information than correct information
in communications, which is out of question. However, here the learning error rate
setting is extended to 0.5 < ρ ≤1 for completing the picture of 0 ≤ρ ≤1, which
does not mean a communication system transmitting more noise than signals. The
reference group is one with ρ = 0, for comparison. Thus, more than 24 settings of
error rate are studied here. For clarity, in Sects.5.3.2 and 5.3.3, 5 settings of learning
error values are listed, which are 0, 0.001, 0.01, 0.1, and 1, respectively. Then, in
Sects.5.3.4 and 5.3.5, 21 error rate settings (from 0 to 1 with an increment of 0.05)
are presented.

78
5
Communications with Learning Errors
Table 5.1 Network settings for the NGLE simulations
Notation
Network type and setting
N
⟨k⟩
⟨apl⟩
⟨cc⟩
RG/0.005
ER random-graph network with
P = 0.005
2000
10.0174
3.5581
0.0050
RG/0.01
ER random-graph network with
P = 0.01
2000
19.9731
2.8312
0.0100
RG/0.025
ER random-graph network with
P = 0.025
2000
49.9933
2.2544
0.0250
SW/5/0.1
WS small-world network with
K = 5 and RP = 0.1
2000
10
4.9427
0.4896
SW/5/0.2
WS small-world network with
K = 5 and RP = 0.2
2000
10
4.2782
0.3461
SW/10/0.1
WS small-world network with
K = 10 and RP = 0.1
2000
20
3.5648
0.5210
SW/10/0.2
WS small-world network with
K = 10 and RP = 0.2
2000
20
3.2452
0.3690
SW/25/0.1
WS small-world network with
K = 25 and RP = 0.1
2000
50
2.7201
0.5389
SW/25/0.2
WS small-world network with
K = 25 and RP = 0.2
2000
50
2.5732
0.3828
SF/4
BA scale-free with 5 initial
nodes and 4 new edges added at
each step
2000
7.9869
3.4064
0.0255
SF/9
BA scale-free with 10 initial
nodes and 9 new edges added at
each step
2000
17.9482
2.7673
0.0391
SF/24
BA scale-free with 25 initial
nodes and 24 new edges added
at each step
2000
47.6822
2.2535
0.0742
The last parameter to introduce is the maximum number of iterations, which is
set to be 1 × 107 in all simulations. This value is empirically large enough for the
twelve networks with different error rate settings. In each single run of NGLE, the
population deﬁnitely reaches the global consensus state before reaching this pre-set
maximum number of iterations.
5.3.2
Convergence Processes
First, the relationship between the number of total words remembered by the popu-
lation vs. different values of the error rare is investigated.

5.3 Simulation Results
79
iteration
(a)
104
105
number of total words
2000
3000
4000
5000
6000
7000
Error Rate = 0
Error Rate = 0.001
Error Rate = 0.01
Error Rate = 0.1
Error Rate = 1
iteration
(b)
104
105
2000
4000
6000
8000
10000
12000
iteration
(c)
104
105
×104
0.5
1
1.5
2
2.5
iteration
(d)
104
105
2000
3000
4000
5000
iteration
(e)
104
105
2000
4000
6000
8000
iteration
(f)
104
105
5000
10000
15000
iteration
(g)
103
104
2000
2500
3000
3500
iteration
(h)
103
104
2000
2500
3000
3500
4000
4500
iteration
(i)
103
104
2000
4000
6000
8000
Fig. 5.4 The convergence process in terms of the number of total words in the population on the
underlying network of: a RG/0.005; b RG/0.01; c RG/0.025; d SF/4; e SF/9; f SF/24; g SW/5/0.2;
h SW/10/0.2; i SW/25/0.2
For clarity, only 5 settings of learning error values are listed. All agents are
memory-free initially. Then, many names are invented and accumulated by the pop-
ulation of agents in the early stage of the gaming, where the learning process (local
failures as in Fig.5.3a, c) is dominant. The numbers of both total and different words
are ascending during this process. After a short saturated stage, a descending feature
of both total and different words indicates that convergence process (local consensus
as in Fig. 5.3b, d) becomes dominant. Finally, the number of total words converges
to 2000, which is the exact number of agents in the population, while the number
of different words converges to 1, meaning that all the agents agree with one and
only one word to name the object. Meanwhile, all the agents are experienced, and
no more learning error existing in the system. This is the global consensus state.
As shown in Fig.5.4, the convergence curves, when the error rate is less than
or equal to 0.01 (red squares, yellow circles, and blue stars), cannot be visually
distinguished from the curves without learning errors. As the error rate increases to
0.1 (green triangles), the curve difference is recognizable. As the error rate becomes
1 (black pluses), meaning that every agent would make error once and only once,
the curve is clearly distinguished from the other curves. This means that, when the
error rate is less than or equal to 0.01, the inﬂuence of the learning error on the

80
5
Communications with Learning Errors
iteration
(a)
104
105
number of different words
0
500
1000
1500
2000
2500
Error Rate = 0
Error Rate = 0.001
Error Rate = 0.01
Error Rate = 0.1
Error Rate = 1
iteration
(b)
104
105
0
1000
2000
3000
iteration
(c)
104
105
0
1000
2000
3000
iteration
(d)
104
105
0
500
1000
1500
2000
iteration
(e)
104
105
0
500
1000
1500
2000
2500
iteration
(f)
104
105
0
1000
2000
3000
iteration
(g)
103
104
0
100
200
300
iteration
(h)
103
104
0
100
200
300
400
iteration
(i)
103
104
0
500
1000
Fig. 5.5 Convergence process in terms of the number of different words in the population on the
underlying network of: a RG/0.005; b RG/0.01; c RG/0.025; d SF/4; e SF/9; f SF/24; g SW/5/0.2;
h SW/10/0.2; i SW/25/0.2
number of total words is insigniﬁcant; when the error rate is 0.1, it becomes non-
negligible. If every agent introduces one learning error, the inﬂuence becomes quite
signiﬁcant, since more (different) words are introduced into the population, and thus
the agents require more memories to temporarily store these ‘wrong-words’ that will
be dropped eventually. Therefore, from the viewpoint of memory cost in the NGLE
system, when the error rate is small, no (or very little) extra memory is required for
storing extra words due to learning errors. However, when the error rate is equal to
or greater than 0.1, the extra memory cost is recognizable.
The relationship between the number of different words and different values of
the error rate is presented in Fig.5.5. The curves therein show a similar convergence
process as that of the number of total words displayed in Fig.5.4. A converging curve
of the number of total words starts with zero and then converges to the population
size, while a converging curve of the number of different words starts with zero and
then converges to 1. All agents in the population hold one and only one same word
to name the object, when reaching global convergence, but it is impossible to predict
which word from the external vocabulary it would be.

5.3 Simulation Results
81
iteration
(a)
104
105
success rate
0
1(0)
1(0)
1(0)
1(0)
1
Error Rate = 0
Error Rate = 0.001
Error Rate = 0.01
Error Rate = 0.1
Error Rate = 1
iteration
(b)
104
105
success rate
0
1(0)
1(0)
1(0)
1(0)
1
iteration
(c)
104
105
success rate
0
1(0)
1(0)
1(0)
1(0)
1
iteration
(d)
104
105
success rate
0
1(0)
1(0)
1(0)
1(0)
1
iteration
(e)
104
105
success rate
0
1(0)
1(0)
1(0)
1(0)
1
iteration
(f)
104
105
success rate
0
1(0)
1(0)
1(0)
1(0)
1
iteration
(g)
104
success rate
0
1(0)
1(0)
1(0)
1(0)
1
iteration
(h)
104
success rate
0
1(0)
1(0)
1(0)
1(0)
1
iteration
(i)
104
success rate
0
1(0)
1(0)
1(0)
1(0)
1
Fig. 5.6 Convergence process in terms of the success rate in the population on the underlying
network of: a RG/0.005; b RG/0.01; c RG/0.025; d SF/4; e SF/9; f SF/24; g SW/5/0.2; h SW/10/0.2;
i SW/25/0.2
As can be seen from Fig.5.5, for all the nine underlying networks, when the
error rate is less than or equal to 0.01 (red squares, yellow circles, and blue stars),
the differences among them are negligible. When the error rate is greater than or
equal to 0.1 (green triangles and black pluses), the difference becomes prominent.
Figure5.5 actually tells the same story as Fig.5.4 does; that is, an error rate less than
or equal to 0.01 increases an insigniﬁcant size of extra memory, but when the error
rate is greater than or equal to 0.1, the extra memory cost becomes signiﬁcant.
The explanation for the phenomena shown in Figs.5.4 and 5.5 is that a larger
learning error rate gives more chances to introduce (different) words from the external
vocabulary. Therefore, these words increase both the number of total words and the
number of different words. The greater the learning error, the more different words
will be introduced into the population.

82
5
Communications with Learning Errors
A similar deﬁnition of success rate as given in Sect.2.2.1 (see Fig. 2.4 for example)
is used here to evaluate the convergence process of NGLE. However, the case of local
pseudo consensus is not considered in the success rate deﬁnition in Sect.2.2.1, while
in NGLE it is considered as a local success. Thus, the success rate is calculated by
the number of local consensus and local pseudo consensus in the last successive 10
iterations, then divided by 10. The calculation is the same as that in the example
given in Fig.2.4b.
The5curves shownineachsubplot of Fig.5.6indicatethat learningerrors increase
or decrease the success rate, nor change the shape of the curves of the success rate.
The convergence processes with respect to (1) the number of total words, (2) the
number of different words, and (3) the success rate, are all scalable when the size of
the network varies from 200, 500, 1000, and 3000. The results are shown in [6]. The
larger the population size is, the slower it reaches the global consensus state, but the
basic proﬁles or shapes of the curves remain to be quite similar.
5.3.3
Convergence Time
The existence of learning errors in naming game leads to the conclusion that the
agents are required to use more memory for storing extra words.
Next, the inﬂuence of learning errors on the convergence time is examined, which
refers to the number of iterations, since in NGLE there is one conversation (operation)
at each iteration. Here, 3 types of topologies with different parameter settings are
investigated, as show in Table5.2. In this subsection, the error rate settings include
ρ = {0, 0.001, 0.01, 0.1, 0.5}, without ρ = 1. Note that in the study of the conver-
gence process, the curves with ρ = 1 are used for reference. In Table5.3, the incre-
ment of convergence time caused by learning errors is presented. A positive sign (‘+’)
means that the convergence times is increased due to learning errors, while a negative
sign (‘−’) means that the convergence times is reduced due to learning errors. If the
increase or decrease is statistically signiﬁcant, a star (‘*’) is marked following the
value. The signiﬁcance is detected by using the Mann-Whitney U-test (at α = 0.05
signiﬁcant level) [9, 10]. The results shown in Table5.3 show that, in 4 cases (includ-
ing the network is RG/0.05, RG/0.1, SF/75, and SF/50(3K), respectively), when the
learning error is 0.5 the increment of convergence time is statistically signiﬁcant.
However, in all the rest cases, the increment of convergence time caused by learning
errors is insigniﬁcant. In summary, there are only 4 out of 60 cases are detected with
signiﬁcant differences in Table5.3. A more detailed increment relationship, which
contains all 24 values of different error rates, can be found in [6].

5.3 Simulation Results
83
Table 5.2 Network settings for NGLE simulations. The notations with 1K and 3K in parentheses
denote the population sizes of 1000 and 3000, respectively. The notations without parentheses mean
that the corresponding population size is 2000 (Data taken from [8])
Notation
Network type
N
⟨k⟩
⟨apl⟩
⟨cc⟩
RG/0.03
Random-graph network with
P = 0.03
2,000
59.97
2.1305
0.0300
RG/0.05(1K)
Random-graph network with
P = 0.05
1,000
49.97
2.0280
0.0500
RG/0.05
Random-graph network with
P = 0.05
2,000
99.96
1.9564
0.0500
RG/0.05(3K)
Random-graph network with
P = 0.05
3,000
149.98
1.9505
0.0500
RG/0.1
Random-graph network with
P = 0.1
2,000
199.92
1.9000
0.1000
SW/20/0.1
Small-world network with
K = 20 and RP = 0.1
2,000
40.00
2.8251
0.5360
SW/20/0.2(1K)
Small-world network with
K = 20 and RP = 0.2
1,000
40.00
2.4657
0.3862
SW/20/0.2
Small-world network with
K = 20 and RP = 0.2
2,000
40.00
2.6963
0.3806
SW/20/0.2(3K)
Small-world network with
K = 20 and RP = 0.2
3,000
40.00
2.7936
0.3783
SW/20/0.3
Small-world network with
K = 20 and RP = 0.3
2,000
40.00
2.6133
0.2597
SF/25
Scale-free with 26 initial nodes
and 25 new edges added at each
step
2,000
49.66
2.2312
0.0760
SF/50(1K)
Scale-free with 51 initial nodes
and 50 new edges added at each
step
1,000
97.39
1.9044
0.1954
SF/50
Scale-free with 51 initial nodes
and 50 new edges added at each
step
2,000
98.69
1.9725
0.1217
SF/50(3K)
Scale-free with 51 initial nodes
and 50 new edges added at each
step
3,000
99.13
2.0237
0.0918
SF/75
Scale-free with 76 initial nodes
and 75 new edges added at each
step
2,000
147.10
1.9273
0.1602
5.3.4
Maximum Number of Total and Different Words
From the above studies on the convergence process and the convergence time, it is
revealed that learning error affects the required memory size, rather than the conver-
gence speed (neither speed up nor slow down the convergence). In the following, the

84
5
Communications with Learning Errors
Table 5.3 The average convergence time and increment relationship between average convergence
time with different values of the error rate (* indicates that the average convergence time is signiﬁ-
cantly delayed due to learning errors) (Data taken from [8])
Networks
Error rate
0
0.001
0.01
0.1
0.5
RG/0.03
(Increment)
1.32E+05
NA
1.34E+05
+0.0210
1.30E+05
−0.0097
1.39E+05
+0.0563
1.41E+05
+0.0704
RG/0.05
(Increment)
1.30E+05
NA
1.25E+05
−0.0407
1.29E+05
−0.0118
1.28E+05
−0.0185
1.48E+05
+0.1322(*)
RG/0.1
(Increment)
1.29E+05
NA
1.25E+05
−0.0328
1.31E+05
+0.0128
1.33E+05
+0.0298
1.42E+05
+0.1024(*)
SW/20/0.1
(Increment)
2.89E+06
NA
3.42E+06
+0.1832
2.81E+06
−0.0280
3.49E+06
+0.2086
3.12E+06
+0.0782
SW/20/0.2
(Increment)
5.67E+05
NA
5.01E+05
−0.1162
5.75E+05
+0.0133
6.65E+05
+0.1719
5.48E+05
−0.0332
SW/20/0.3
(Increment)
1.46E+05
NA
1.40E+05
−0.0428
1.49E+05
+0.0221
1.55E+05
+0.0602
1.55E+05
+0.0617
SF/25
(Increment)
1.58E+05
NA
1.59E+05
+0.0017
1.49E+05
−0.0600
1.56E+05
−0.0166
1.61E+05
+0.0197
SF/50
(Increment)
1.49E+05
NA
1.53E+05
+0.0285
1.51E+05
+0.0157
1.44E+05
−0.0321
1.57E+05
+0.0566
SF/75
(Increment)
1.40E+05
NA
1.46E+05
+0.0423
1.45E+05
+0.0387
1.45E+05
+0.0352
1.56E+05
+0.1191(*)
RG/0.05(1K)
(Increment)
5.32E+04
NA
5.34E+04
+0.0040
5.53E+04
+0.0401
5.20E+04
−0.0225
5.72E+04
+0.0764
RG/0.05(3K)
(Increment)
2.31E+05
NA
2.31E+05
−0.0009
2.34E+05
+0.0135
2.38E+05
+0.0317
2.53E+05
+0.0935
SW/20/0.2(1K)
(Increment)
1.18E+05
NA
1.14E+05
−0.0348
1.07E+05
−0.0958
9.98E+04
−0.1537
1.10E+05
−0.0673
SW/20/0.2(3K)
(Increment)
1.82E+06
NA
1.60E+06
−0.1182
1.57E+06
−0.1376
1.63E+06
−0.1048
1.77E+06
−0.0272
SF/50(1K)
(Increment)
5.77E+04
NA
6.20E+04
+0.0736
5.89E+04
+0.0215
5.84E+04
+0.0115
6.29E+04
+0.0897
SF/50(3K)
(Increment)
2.44E+05
NA
2.52E+05
+0.0323
2.53E+05
+0.0361
2.54E+05
+0.0394
2.76E+05
+0.1292(*)
relationship between the maximum numbers of total/different words and different
error rate values is discussed.
Figures5.7, 5.8, 5.9 and 5.10 show the curves of the maximum numbers of
total/different words, when the value of the error rate varies. For the NGLE model,
it is assumed that (1) all agents can introduce errors into communications, and (2)
an agent will stop making new errors after being a speaker. Next, the case when the
second assumption is waived is investigated. Figures5.11, 5.12, 5.13 and 5.14 show

5.3 Simulation Results
85
error rate
(a)
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of total words
× 104
0.6
0.8
1
1.2
1.4
1.6
1.8
2
2.2
RG/0.005
RG/0.01
RG/0.025
error rate
(b)
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of total words
4000
6000
8000
10000
12000
14000
SF/4
SF/9
SF/24
error rate
(c)
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of total words
2000
4000
6000
8000
10000
12000
SW/5/0.1
SW/10/0.1
SW/25/0.1
error rate
(d)
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of total words
2000
4000
6000
8000
10000
12000
14000
SW/5/0.2
SW/10/0.2
SW/25/0.2
Fig. 5.7 Relationship between the error rate and the maximum number of total words over the
underlying network: a RG; b SF; c SW with RP = 0.1; d SW with RP = 0.2. The population size
is 2000. The dotted lines are ﬁtted polynomial curves, for reference
the situation when agents are unable to stop making new errors in communications,
used for reference.
Figure5.7 shows a boxplot of the relationship between the learning error rate
and the maximum number of total words. In the 4 different underlying topologies:
(1) The maximum number of total words increases as the connectivity (number of
connections) of the network increases. In each subplot, the green curve has the least
connectivity, whiletheblackcurvehas thegreatest. It is clear that abetter connectivity
leads to a greater maximum number of total words. (2) The maximum number of
total words increases when the error rate increases from 0 to a small value, and then
becomes saturated. For example, in Fig.5.7a, for the underlying network RG/0.025,
the maximum number of total words increases when the error rate increases from 0
to 0.4, and then becomes saturated.

86
5
Communications with Learning Errors
error rate
(a)
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of different words
1000
1500
2000
2500
3000
RG/0.005
RG/0.01
RG/0.025
error rate
   (b)
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of different words
1000
1500
2000
2500
3000
SF/4
SF/9
SF/24
error rate
(c)
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of different words
1000
1500
2000
2500
3000
SW/5/0.1
SW/10/0.1
SW/25/0.1
error rate
   (d)
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of different words
1000
1500
2000
2500
3000
SW/5/0.2
SW/10/0.2
SW/25/0.2
Fig. 5.8 Relationship between the error rate and the maximum number of different words over the
underlying network: a RG; b SF; c SW with RP = 0.1; d SW with RP = 0.2. The population size
is 2000. The dotted lines are ﬁtted polynomial curves, for reference
Figure5.8 shows a boxplot of the relationship between the learning error rate
and the maximum number of different words. Similarly to Fig.5.7, the maximum
number of different words increases as the connectivity of the network increases.
The difference from Fig.5.7 is that, the curves continue to increase when the error
rate increases, but not becoming saturated.
Figures5.9 and 5.10 also show the relationship of the maximum numbers of
total/different words vs. the error rate, but are compared with different population
sizes. The relationship reﬂected by these two ﬁgures is simple: A greater population
leads to greater maximum numbers of total/different words.
Figures5.11, 5.12, 5.13 and 5.14 show the one-to-one corresponding plots to
Figs.5.7, 5.8, 5.9 and 5.10, but herein the agents are unable to stop making errors by
being speaker. Note that the populations in the simulations for Figs.5.11, 5.12, 5.13
and 5.14 may not eventually reach the global consensus state, due to the permanent
learning errors, so only the maximum numbers of total/different words are presented.
As can be seen from the ﬁgures, it is also true that either a better connectivity or
a greater population size will lead to greater maximum numbers of total/different

5.3 Simulation Results
87
error rate 
(a) RG/0.01
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of total words
2000
4000
6000
8000
10000
12000
NS=500
NS=1000
NS=2000
error rate
(b) SF/9
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of total words
1000
2000
3000
4000
5000
6000
7000
8000
NS=500
NS=1000
NS=2000
error rate
(c) SW/10/0.1
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of total words
1000
2000
3000
4000
5000
6000
7000
8000
NS=500
NS=1000
NS=2000
error rate
(d) SW/10/0.2
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of total words
1000
2000
3000
4000
5000
6000
7000
8000
9000
NS=500
NS=1000
NS=2000
Fig. 5.9 Relationship between error rate and the maximum number of total words over the under-
lying network: a RG/0.01; b SF/9; c SW/10/0.1; d SW/10/0.2. The population size is set to 500,
1000, and 2000 within each subplot. The dotted lines attached are ﬁtted polynomial curves, for
reference
words. Note that in [8], the relationship between the maximum numbers of different
words and the error rate is reported a linear-like shape, which is because (1) the
maximum error rate tested in [8] is 0.5, and (2) the tested error rates are sparse with
an increment of 0.1, but here, the whole picture is completed by choosing ρ ∈[0, 1]
with an increment 0.05.
5.3.5
Convergence Thresholds
In the above simulation results except that in Figs.5.11, 5.12, 5.13 and 5.14, global
convergence state is implicitly limited by the fact that the population of agents should
reach global consensus before the pre-set maximum number of iterations, 1 × 107,
is reached. The NGLE model also employs a rule to prevent learning errors being

88
5
Communications with Learning Errors
error rate 
(a) RG/0.01
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of different words
0
500
1000
1500
2000
2500
NS=500
NS=1000
NS=2000
error rate
(b) SF/9
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of different words
0
500
1000
1500
2000
2500
NS=500
NS=1000
NS=2000
error rate
(c) SW/10/0.1
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of different words
0
500
1000
1500
2000
2500
NS=500
NS=1000
NS=2000
error rate
(d) SW/10/0.2
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of different words
0
500
1000
1500
2000
2500
NS=500
NS=1000
NS=2000
Fig. 5.10 Relationship between error rate and the maximum number of different words over the
underlying network: a RG/0.01; b SF/9; c SW/10/0.1; d SW/10/0.2. The population size is set to
500, 1000, and 2000 within each subplot. The dotted lines are ﬁtted polynomial curves, for reference
introduced into the population permanently. However, if this rule is discarded, then
all agents might continuously make errors in future communications, so that the
population may not converge within the pre-set limit of maximal number of iterations,
or never converge.
Asintroducedin[1],thereisanoptimalerrorratethatmaximizestheperformances
of parental learning and role-model learning systems, but for the random learning
model this would not work at all when the error rate is greater than a certain (small)
threshold. This phenomenon is studied next.

5.3 Simulation Results
89
error rate
(a)
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of total words
×105
0
1
2
3
4
5
RG/0.005
RG/0.01
RG/0.025
error rate
(b)
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of total words
×104
2
4
6
8
10
SF/4
SF/9
SF/24
error rate
(c)
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of total words
×104
2
4
6
8
10
SW/5/0.1
SW/10/0.1
SW/25/0.1
error rate
(d)
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of total words
×104
2
4
6
8
10
SW/5/0.2
SW/10/0.2
SW/25/0.2
Fig. 5.11 The agents permanently make learning errors with a varying error rate. The relationship
between the error rate and the maximum number of total words over the underlying network: a RG;
b SF; c SW with RP = 0.1; d SW with RP = 0.2. The population size is 2000. The dotted lines
are ﬁtted polynomial curves, for reference
Figure 5.15 shows the statistical results of the simulations on the error rate thresh-
olds, or convergence thresholds. For each type of network, 30 independent runs are
performed. In each single sun, the initial error rate is set to be 0. If, with the current
error rate, the population converges within the pre-set maximal number of iterations
(i.e., 1 × 107), then the error rate is increased with an incremental step size of 0.0001.
This process repeats until the population cannot reach global consensus with a certain
error rate, and thus this error rate is recorded as the threshold in this run. It is worth
mentioning that, not only suggested by [1] but also by the trial-and-error simulations
here, a too small (e.g., less than 0.0001) incremental step for the error rate makes no
sense to the results for a population of 2000 agents, therefore is not discussed.

90
5
Communications with Learning Errors
error rate
(a)
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of different words
×105
0
1
2
3
4
5
RG/0.005
RG/0.01
RG/0.025
error rate
(b)
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of different words
×104
0
2
4
6
8
10
SF/4
SF/9
SF/24
error rate
(c)
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of different words
×104
0
2
4
6
8
10
SW/5/0.1
SW/10/0.1
SW/25/0.1
error rate
(d)
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of different words
×104
0
2
4
6
8
10
SW/5/0.2
SW/10/0.2
SW/25/0.2
Fig. 5.12 The agents permanently make learning errors with a varying error rate. The relationship
between the error rate and the maximum number of different words over the underlying network:
a RG; b SF; c SW with RP = 0.1; d SW with RP = 0.2. The population size is 2000. The dotted
lines are ﬁtted polynomial curves, for reference
For each boxplot shown in Fig. 5.15, a box represents that the central 50% data lie
in this section; the bar inside the box is the median value of all 30 datasets; the upper
and lower bars are the greatest and the least values, excluding outliers; and ﬁnally the
red pluses represent the outliers. As can be seen from the boxplot, all the different
RG and SW networks have similar thresholds for the error rate, located between
0.0060 and 0.0067. More precisely, in most cases, when the error rate increases to
around 0.0067 or above, the population would probably not converge within the pre-
set maximal number of iterations. For the 3 SF networks, this threshold is somewhere
from 0.0068 to 0.0074, implying that the tolerance of learning errors in SF networks
is higher than that in RG and SW networks in this case.

5.4 Conclusion
91
error rate
(a)
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of total words
×105
0
1
2
3
4
5
RG/0.01/NS=500
RG/0.01/NS=1000
RG/0.01/NS=2000
error rate
(b)
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of total words
×104
2
4
6
8
10
SF/9/NS=500
SF/9/NS=1000
SF/9/NS=2000
error rate
(c)
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of total words
×104
2
4
6
8
10
SW/10/0.1/NS=500
SW/10/0.1/NS=1000
SW/10/0.1/NS=2000
error rate
(d)
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of total words
×104
2
4
6
8
10
SW/10/0.2/NS=500
SW/10/0.2/NS=1000
SW/10/0.2/NS=2000
Fig. 5.13 The agents permanently make learning errors with a varying error rate. The relationship
between error rate and the maximum number of total words over the underlying network: a RG/0.01;
b SF/9; c SW/10/0.1; d SW/10/0.2. The population size is set to 500, 1000, and 2000 within each
subplot. The dotted lines are ﬁtted polynomial curves, for reference
5.4
Conclusion
In this chapter, the model of naming game with learning errors (NGLE) in commu-
nications is introduced and studied by means of extensive and comprehensive simu-
lation experiments. It is found that, if the agents have some learning errors but can
learntoavoidmakingfurthererrors,theconvergencewillbeonlymarginallyaffected,
without signiﬁcant acceleration or delay. The convergence speed is also affected by
the underlying network structures, when different topological and parameter settings
are simulated. However, during the convergence process from initialization to global
consensus, agents in NGLE learn and discard more words than that in minimal nam-
ing game, which means that more memory space for agents is required when learning

92
5
Communications with Learning Errors
error rate
(a)
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of different words
×105
0
1
2
3
4
5
RG/0.01/NS=500
RG/0.01/NS=1000
RG/0.01/NS=2000
error rate
(b)
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of different words
×104
0
2
4
6
8
10
SF/9/NS=500
SF/9/NS=1000
SF/9/NS=2000
error rate
(c)
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of different words
×104
0
2
4
6
8
10
SW/10/0.1/NS=500
SW/10/0.1/NS=1000
SW/10/0.1/NS=2000
error rate
(d)
0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
max number of different words
×104
0
2
4
6
8
10
SW/10/0.2/NS=500
SW/10/0.2/NS=1000
SW/10/0.2/NS=2000
Fig. 5.14 The agents permanently make learning errors with a varying error rate. The relationship
between error rate and the maximum number of different words over the underlying network: a
RG/0.01; b SF/9; c SW/10/0.1; d SW/10/0.2. The population size is set to 500, 1000, and 2000,
respectively within each subplot. The dotted lines are ﬁtted polynomial curves, for reference
errors are introduced into the naming game. The relationship between the maximum
numbers of total/different words throughout the convergence process against the
error rat is revealed, with or without applying a strategy to stop introducing further
learning errors. In addition, a statistical range of error rate thresholds is empirically
observed, above which the population would not converge if there is no strategy is
applied to prevent continuously introducing new learning errors.

References
93
Fig. 5.15 A boxplot of the
learning error rate threshold
on underlying networks of: a
RG/0.03; b RG/0.05; c
RG/0.1; d SW/10/0.1; e
SW/10/0.2; f SW/10/0.3; g
SW/20/0.1; h SW/20/0.2; i
SW/20/0.3; j SF/25; k SF/50;
l SF/75. The greed dotted
lines represent the average of
3 adjacent networks (Partial
data taken from [8])
networks
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
(j)
(k)
(l)
error tate
×10-3
5.8
6
6.2
6.4
6.6
6.8
7
7.2
7.4
7.6
References
1. M.A. Nowak, J.B. Plotkin, D.C. Krakauer, The evolutionary language game. J. Theor. Biol.
200(2), 147–162 (1999). https://doi.org/10.1006/jtbi.1999.0981
2. C.C. Lim, W.T. Zhang, Noisy naming games, partial synchronization and coarse-graining in
social networks, in Network Science Workshop (NSW), 2011 IEEE (IEEE, 2011), pp. 25–29.
https://doi.org/10.1109/NSW.2011.6004654
3. R.R. Liu, C.X. Jia, H.X. Yang, B.H. Wang, Naming game on small-world networks with
geographical effects. Physica A 388, 3615–3620 (2009). https://doi.org/10.1016/j.physa.2009.
05.007
4. L. Dall’Asta, A. Baronchelli, A. Barrat, V. Loreto, Agreement dynamics on small-world net-
works. EPL (Europhys. Lett.) 73(6), 969 (2006). https://doi.org/10.1209/epl/i2005-10481-7
5. A. Baronchelli, A gentle introduction to the minimal naming game. Belgian J. Linguist. 30(1),
171–192 (2016)
6. Y. Lou, G.R. Chen, Supplementary information for paper “analysis of the “naming game” with
learning errors in communications” (2015), http://www.ee.cityu.edu.hk/~gchen/pdf/NGLE-
SI.pdf
7. Q.Lu,G.Korniss,B.K.Szymanski,Thenaminggameinsocialnetworks:communityformation
and consensus engineering. J. Econ. Interac. Coord. 4(2), 221–235 (2009). https://doi.org/10.
1007/s11403-009-0057-7
8. Y. Lou, G.R. Chen, Analysis of the “naming game” with learning errors in communications.
Sci. Rep. 5, 12191 (2015). https://doi.org/10.1038/srep12191
9. H.B. Mann, D.R. Whitney, On a test of whether one of two random variables is stochastically
larger than the other. Annals Math. Stat. pp. 50–60 (1947)
10. M.P. Fay, M.A. Proschan, Wilcoxon-Mann-Whitney or t-test? On assumptions for hypothesis
tests and multiple interpretations of decision rules. Stat. Surveys 4, 1–39 (2010)

Chapter 6
Naming Game on Multi-Community
Networks
6.1
Introduction
Complex networks are commonly employed as the underlying communication struc-
tures in naming game studies, especially random-graph (RG), small-world (SW), and
scale-free (SF) networks [1–5]. As can be seen from the previous chapters, different
network topologies affect the naming game process signiﬁcantly in different ways.
Naming game simulations and analysis offer an effective computer-aided approach
to building sensible mathematical models and, more importantly, better understand-
ing of the evolution and development of human languages and social behaviors. The
convergence phenomena in naming game models are typically veriﬁed via numerical
simulations [6–8], theoretical proofs [9], and sometimes social experiments [10].
In a naming game model, agents are connected over the underlying network, which
represents the acquaintanceships and interactions among them. On the underlying
network, two agents can communicate with each other only if they are directly
connected. Recall that, in the naming game in group (NGG) model introduced in
Chap.4, two agents can also communicate through their common friends in group
discussions, but as a convention the group-wise communications in NGG are not
regarded as direct communications.
An isolated agent or isolated subgroup of agents is not considered in any naming
game studied in this book, thus no isolated node or isolated subgraph exists in the
underlying network. This is because, once being isolated, an agent or subgroup is
unable to send or receive information. As a result, they are not participating in the
game, therefore will be removed in the study. Information should be statistically
reachable between any pair of agents, sooner or later, so that the whole population
can eventually reach global consensus. As before, global consensus here means that
every agent keeps one and only one same name, which is used to name the observed
object.
Once again, in a naming game, every node in the underlying network represents an
agent and every edge indicates that the two connected end-nodes can communicate
to each other directly. The number of connections that a node has is referred to
© Springer Nature Switzerland AG 2019
G. Chen and Y. Lou, Naming Game, Emergence, Complexity and Computation 34,
https://doi.org/10.1007/978-3-030-05243-0_6
95

96
6
Naming Game on Multi-Community Networks
as its degree. The heterogeneity of a network can generally be reﬂected by the SF
network model, such as a social network [3, 11, 12], where a few agents have much
larger degrees than the other agents. On the other hand, human communications
are essentially community-based, in the sense that people belonging to the same
community are much more frequently and actively interacting and communicating
with each other as compared to those belonging to different communities. A naming
game model, on the other hand, mimics the social communication behaviors and
simulates the process of learning a name for a new object by a population of agents.
In a naming game, usually all the agents in a population can reach global consensus
asymptotically through pair-wise or group-wise conversations iteratively, if the game
rules are reasonably designed.
To study the above-described social phenomena, a multi-local-world (MLW) net-
work model [13, 14] is developed, as brieﬂy introduced in Chap.2. The MLW model
is a kind of SF network, capable of capturing the essential features of many real-
world networks with prominent community structures. The MLW model shows good
performances in capturing basic features of the Internet at the autonomous system
(AS) level [15]. It has been recognized that human social networks also have AS-like
structures [16]. Therefore, it is reasonable to study a naming game of a population on
an MLW network, where each local-world represents a community. In this chapter,
naming game is studied using the MLW network as its underlying network. Com-
paring to the three typical topologies (namely, RG, SW and SF networks) used as
the underlying networks, the naming game model on MLW has its distinctive char-
acteristics. In real life, communities are formed not only by natural barriers such as
rivers, mountains, and oceans, but also by folkways, dialects and cultures, etc. In the
MLW network model, both the number and the size of the communities are control-
lable as tunable parameters, and thus users are able to control the generation of the
communities properly. For each community, which has at least three nodes to make
sense of the community structure, an edge between two nodes therein is referred
to as an intra-community connection, while an edge between two nodes from two
different communities respectively is referred to as an inter-community connection.
These intra- and inter-community connections play a vital role in the information
propagation process of the naming game with the underlying MLW network, which
affect the global convergence signiﬁcantly.
In this chapter, naming game simulations are performed by employing the MLW
model as the underlying network. Comparisons are performed against RG, SW, and
SF networks. It shows that (1) when the intra-community connections increase while
the inter-community connections remain unchanged, the convergence to global con-
sensus is slow and sometimes might fail; (2) when the inter-community connections
are sufﬁciently dense, both the number and the sizes of the communities do not affect
the convergence process, at least not signiﬁcantly; and (3) for different topologies
with the same average node-degree, local clustering of agents obstructs or prohibits
global consensus to take place. The study of this chapter reveals the role of local
communities in a global naming game network.

6.1 Introduction
97
The rest of the chapter is organized as follows. Section6.2 describes the MLW
model, brieﬂy introduced in Chap.2, in detail. Section6.3 presents extensive simu-
lations with analysis and comparison. Section6.4 concludes the investigation.
6.2
Multi-Local-World Networks
Here and throughout the chapter, all random operations (e.g., random generation,
selection, addition or deletion) follow a uniform distribution.
The algorithm for generating an MLW network with N nodes [14] is detailed
below, in a way more conveniently to describe the following simulations than its
brief description given in Chap.2.
The initialization starts with NLW isolated local-worlds (communities). Within
each local-world, there are m0 nodes connected by e0 edges. At each time step, a
value r ∈(0, 1) is generated at random.
(a) If 0 < r < p1, perform addition of a new local-world of m0 nodes connected
by e0 edges, which is added to the existing network.
(b) If p1 ≤r < p2, perform addition of a new node to a randomly selected local-
worldviapreferentialattachment:thenewnodeisaddedintotheselectedlocal-world,
establishing e1 new connections (edges), according to the following preferential
probability:
Π(ki) =
ki + α

j∈LW(ki + α)
(6.1)
where ki is the degree of node i within the local-world LW, and α is a tunable
parameter.
(c) If p2 ≤r < p3, perform addition of edges within a randomly selected local-
world LW: e2 edges are added to this LW. For each new edge, one end is connected
to a randomly-picked node within LW, while the other end is connected to a node
selected also from the same LW according to a probability Π(ki) given by Eq.(6.1).
This process repeats e2 times.
(d) If p3 ≤r < p4, perform deletion of edges within a randomly selected local-
world LW: e3 edges are removed from LW. This will remove more edges that
connect to small-degree nodes. To do so, randomly select a node from LW. Remove
the edges of this node one by one, according to the following probability, where ki
is the degree of the node at the other end of the edge:
Π′(ki) =
1
NLW −1 · (1 −Π(ki))
(6.2)
where NLW is the number of nodes within LW, and Π(ki) is given by Eq.(6.1). This
process repeats e3 times.

98
6
Naming Game on Multi-Community Networks
(e) If p4 ≤r < 1, perform addition of edges among local-worlds: e4 edges are
added to connect different local-worlds pair-wise. First, two different local-worlds
are picked at random. Then, one node is selected within each local-world, according
to the probability given by Eq.(6.1). An edge is ﬁnally added between these two
nodes. This process repeats e4 times.
The generation algorithm stops when totally N nodes have been generated into
the network.
In this algorithm, the initial number of nodes is NLW · m0 and the termination
number is N > NLW · m0 (typically, much larger). Note also that, throughout the
generation process, repeated connections, self-loops and isolated nodes should be
avoided or removed.
More details about the generating algorithm of an MLW network as well as the
calculation of its degree distribution can be found in [14].
As shown above, there are totally 11 tunable parameters, among which 2 param-
eters are of particular interest, i.e., the number of local-worlds NLW and the initial
number m0 of nodes within each local-world. The former parameter basically deter-
mines the number of communities in the generated underlying network, while the
latter roughly determines the sizes of the communities.
It is known [17] that, in general, if the underlying network has multiple communi-
ties then it is hard for a population to reach global consensus. The underlying network
employed in [17] is a combination of several SF networks, where the combination is
generated by a reversed preferential attachment probability. Speciﬁcally, the intra-
connections within each community (an SF network) are based on a preferential
attachment probability given by Eq.(6.1); the inter-connections between two com-
munities (different SF networks) are generated according the following preferential
attachment probability:
Π(ki) =
1/ki + α

j∈LW(1/k j + α)
(6.3)
Note that only bi-community and tri-community networks are studied in [17].
In this chapter, the MLW model will be employed as the underlying network,
in which both the number of local worlds NLW and the initial size m0 are tunable
parameters. By simply adjusting these two parameters, the acquaintanceships among
agents can be adjusted easily. Either strong or weak community structures can be
generated, such that the resulting networks are more general and more realistic, so
as to represent the real human society and language development scenarios.
Two MLW examples with different parameters are shown in Fig.6.1. In Fig.6.1a,
eachlocal-worldhas20nodesinitially,andlaterafewextranodesarerandomlyadded
into the existing local-world networks. It is easy to distinguish the intra-connections
and the inter-connections. In contrast, the network shown in Fig.6.1b has a relatively
weak community structure in comparison.

6.3 Simulation Results and Analysis
99
(a) An MLW example with N =
100, NLW = 4, and m0 = 20
(b) An MLW example with N =
110, NLW = 16, and m0 = 6
Fig. 6.1 Two examples of MLW networks. Since e0 = m0 · (m0 −1)/2, all the local-worlds are
fully-connected initially, and some edges may be removed by operation d with probability 0.04 as
indicated in Table6.1
6.3
Simulation Results and Analysis
Minimal naming game is performed on MLW networks, for it realistically simu-
lates the Internet as well as some other social networks with prominent community
structures.
There are mainly 11 parameters in the MLW generation algorithm, detailed above,
among which 2 key parameters are tuned in the simulations, i.e., the number of local-
worlds NLW and the initial number m0 of nodes within each local-world. The other
9 parameters are ﬁxed, as suggested in [14], at least for simplicity since they are
relatively insigniﬁcant for the present study. The precise values of them, namely
p1, p2, p3, p4, e0, e1, e2, e3, and e4, are presented in Table6.1, along with their
correspondences or meanings in the settings. Within each local-world, the m0 notes
are initially fully-connected, thus e0 = m0 · (m0 −1)/2.
The size N of an MLW is also the population size, which should satisfy N >
NLW · m0; otherwise, NLW local-worlds will remain isolated, so that the network is
not connected [14]. A new parameter ρ (0 < ρ < 1) is introduced into the present
study, which is not a parameter of the MLW model, to represent the rate of initially
assigned nodes in the local-worlds: when ρ = 0, there is no local-world initially and
the network degenerates to an SF network where every node is added via preferential
attachment; when ρ = 1, it represents a number of isolated local-worlds without
any additional nodes or edges; when it 0 < ρ < 1, some ‘normal’ MLW networks
will be generated. The purpose of introducing ρ is to change the above inequality
(0 < ρ < 1) to an equality for the sake of determination:
ρ · N = NLW · m0
(6.4)

100
6
Naming Game on Multi-Community Networks
Table 6.1 Parameter values and their correspondences or meanings. (Data taken from [18])
Parameter Setting
Meaning
p1 = 0
Operation a (addition of new local-worlds) is not performed
p2 = 0.28
Operation b (addition of a new node to a local-world) is performed
with probability 0.28
p3 = 0.39
Operation c (addition of edges within a local-world) is performed with
probability 0.11 (=0.39 −0.28)
p4 = 0.43
Operation d (deletion of edges within a local-world) is performed with
probability 0.04 (=0.43 −0.39); meanwhile, operation e (addition of
edges among local-worlds) is performed with probability 0.57
(=1.00 −0.43)
e0 = m0 · (m0 −1)/2
Initially, local-worlds are isolated but in each of them the nodes are
fully-connected
e1 = e2 = e3 = e4 = 2
At each time step, when operations b, c, and d are performed, the
number of edges added or deleted is 2
Comprehensive simulations are performed by varying the values of ρ, m0, and
NLW.
It might be expected that the local community structure could make the achieve-
ment of global consensus more difﬁcult. Convergence time will be used as the main
measure, as usual, which is counted by the number of iterations needed from initial-
ization to achieving global convergence.
Three comparisons are performed: (1) ρ is ﬁxed and the convergence time affected
by the number and size of local-worlds is examined; (2) the convergence time is
examined when ρ varies, with ﬁxed values of m0 and NLW; and (3) the convergence
progress of the MLW network is compared to three typical networks, i.e., RG, SW,
and SF networks.
The population size is set to N = 1000 in all simulations. The maximum number
of iterations is set to 1 × 107 as the stopping criterion. To eliminate the effect of
randomness, all data are collected from 30 independent runs and then averaged. Here,
1 × 107 iterations are empirically large enough for the present numerical studies. The
scaling property of the population size is studied in [19], where the population size
is set to 500 and 1500, respectively. The number of different names at iteration t is
denoted by Ndi f f (t) in the following, with
1 ≤Ndi f f (107) ≤NLW
(6.5)
When Ndi f f (107) = 1, there is only one same word remembered by the whole
population, implying that the population is in the global convergence state. When
1 < Ndi f f (107) ≤NLW, it probably means that the population is partially converged
community-wise, and agents in different communities may converge to different
words respectively, as can be seen from Table6.2. In addition, it can be observed
that, with a long time period τ ≫0, one has

6.3 Simulation Results and Analysis
101
Table 6.2 The number of different words at iteration step 1 × 107, comparing to the number
of local-worlds. m0 is set to 26 different values. The number of local-worlds is calculated by
NLW = (ρ · N)/m0. It can be seen that when 3 ≤m0 ≤18 (for ρ = 0.5) and 3 ≤m0 ≤14 (for ρ =
0.7), the population reaches global convergence (Ndi f f (107) = 1); otherwise, 1 < Ndi f f (107) ≤
NLW . Putting together all the cases, one has 1 ≤Ndi f f (107) ≤NLW . Especially, when m0 ≥30,
Ndi f f (107) is approaching NLW , implying that each local-world converges, but to a different name
respectively. (Data taken from [18])
m0
3
4
5
6
7
8
9
10
11
12
13
14
15
ρ = 0.5
Ndi f f (107)
1
1
1
1
1
1
1
1
1
1
1
1
1
NLW
166
125
100
83
71
62
55
50
45
41
38
35
33
ρ = 0.7
Ndi f f (107)
1.1
1
1
1
1
1
1
1
1
1
1
1
1.4
NLW
233
175
140
116
100
87
77
70
63
58
53
50
46
m0
16
17
18
19
20
30
40
50
60
70
80
90
100
ρ = 0.5
Ndi f f (107)
1
1
1
3.2
5.7
15.0
11.7
9.7
7.9
6.9
5.8
4.9
4.9
NLW
31
29
27
26
25
16
12
10
8
7
6
5
5
ρ = 0.7
Ndi f f (107)
1.5
7.9
11.7
22.4
25.4
22.8
17.0
13.9
11.0
10.0
8.0
6.9
7.0
NLW
43
41
38
36
35
23
17
14
11
10
8
7
7
Ndi f f (107 −τ) = Ndi f f (107)
(6.6)
which means that the number of different words is not changed during a long dura-
tion of τ ≫0, before the number of iterations reaches 1 × 107. Note that Ndi f f is
monotonically non-increasing in the converging and converged stages, since there is
no learning error or other factors that would increase the number of different words
remembered by the population. This phenomenon can be observed from Fig.6.6.
Considering both Eqs.(6.5) and (6.6) together, by setting the maximum number of
iterations to 1 × 107, the population would converge sufﬁciently well.
6.3.1
Convergence Time Versus Number and Sizes
of Local-Worlds
The value m0 of initial nodes in each local-world is set to 26 different values, vary-
ing from 3 to 19 with an increment 1, and from 20 to 100 with an increment 10,
to create different scenarios. The rate of initially assigned nodes is set to ρ = 0.5
and ρ = 0.7, respectively, as shown in Fig.6.2a, b. As can be seen from Fig.6.2, a
relatively small size of community is beneﬁcial for achieving convergence. When
3 ≤m0 ≤18 (for ρ = 0.5) and 3 ≤m0 ≤14 (for ρ = 0.7), the population reaches
global convergence (Ndi f f (107) = 1) in all 30 independent runs. Since agents within
the same community are highly-connected, which was fully-connected initially but
with random connection-removal later on, this makes intra-community converging
rather fast. In contrast, a strong intra-connection of different communities makes the

102
6
Naming Game on Multi-Community Networks
inter-community more difﬁcult to converge, especially when different communities
had already converged to different words respectively.
In the boxplot shown in Fig.6.2, a box indicates that the central 50% data lie
in this section; the bar inside the box is the median value of all 30 datasets; the
upper and lower bars are the greatest and least values, excluding the outliers that
are represented by red pluses. The ratio of inter/intra edges per node is attached
for reference in Fig.6.2. The convergence time becomes longer when the ratio of
inter/intra edges per node decreases. In Fig.6.2a, when the ratio of inter/intra edges
per node is less than 0.0081, the non-converged behavior can be observed from the
corresponding simulations. In Fig.6.2b, the threshold value of the ratio is 0.0087.
Table6.3 shows the mean ratio of inter-connection versus intra-connection per
node,assoonasthepopulationstartstoshow(occasionally)non-convergedbehaviors
over 30 independent runs. It could be regarded as the threshold for non-convergence,
e.g., for ρ = 0.5 (also shown in Fig.6.4a), when m0 = 19, the population just starts
to show non-converged behaviors, while when m0 < 19, the population always con-
verges in the 30 runs. The ratio is set to 0.4 ≤ρ ≤0.7, since ρ is very small, ≤0.3,
the community structure is too weak. In this case, the resulting network is actually
a preferential attached SF network rather than an MWL. In contrast, if ρ ≥0.8,
the resulting network is lack of inter-connections. The R value is calculated by the
averaged ratio of the number of inter-connections divided by the number of intra-
connections within each community, and then divided by the number of nodes in
the community. An illustrative example is shown in Fig.6.3. As can be seen from
Fig.6.3b, the nodes n1, n2, n3 and n4 are clustered as a community, which has 6
intra-connections and 1 inter-connection, and thus the ratio for each node of this
community is (1/6)/4 = 0.0417. Here, ¯R is the mean value of the ratios of all com-
munities in the underlying network.
As showninTable6.3, whenthevalueof ρincreases, mmin
0
decreases, meaningthat
when more nodes are initially allocated in communities, they are sparsely distributed
in many small-sized communities, other than in a few large-sized communities.
Otherwise, theglobal convergencemaybehindered. Theratio( ¯R) of inter-connection
versus intra-connection stays relatively stably for different cases (around 0.0081–
0.0087), meaning that if the ratio of inter-connection versus intra-connection can be
set above such a value, then global consensus can be reached.
Table6.4 shows the average degree ⟨k⟩, average path length ⟨pl⟩, and average
clustering coefﬁcient ⟨cc⟩of all the MLW networks employed as the underlying
network in the simulations. It can be seen that, as m0 increases, both average degree
and average clustering coefﬁcient increase, while the average path length decreases.
This means that when m0 increases, the networks have better connectivity, yet more
clustered. It is known that better connectivity (greater ⟨k⟩but shorter ⟨pl⟩) facilitates
information propagation in a naming game [11, 20]. On the contrary, local clustering
and forming communities hinder global convergence, although local convergence
within communities is facilitated by clustering. Poor inter-connection and strong
intra-connection form barriers, which prevent the communities from achieving global
convergence together.

6.3 Simulation Results and Analysis
103
inter/intra links per node
0
0.032
0.064
0.096
0.128
0.16
0.192
0.224
0.256
0.288
0.32
3
4
5
6
7
8
9
10 11 12 13 14 15 16 17 18 19 20 30 40 50 60 70 80 90 100
convergence time
×106
0
1
2
3
4
5
6
7
8
9
10
0.0081
number of initial nodes per local-world
(a) An MLW example with N = 100, NLW = 4, and m0 = 20
inter/intra links per node
0
0.0254
0.0509
0.0763
0.102
0.127
0.153
0.178
0.204
0.229
0.254
3
4
5
6
7
8
9
10 11 12 13 14 15 16 17 18 19 20 30 40 50 60 70 80 90 100
convergence time
×106
0
1
2
3
4
5
6
7
8
9
10
0.0087
number of initial nodes per local-world
(b) An MLW example with N = 110, NLW = 16, and m0 = 6
Fig. 6.2 A boxplot of the convergence time versus the number m0 of initial nodes in each local-
world, with a ρ = 0.5 and b ρ = 0.7. The number of local-worlds is calculated by Eq.(6.4), i.e.,
NLW = ⌊ρ · N/m0⌋, where ⌊x⌋is the largest integer less than or equal to x. The mean value of the
convergence time in both subplots is concave: it ﬁrst decreases slightly when m0 increases from 3 to
5, and then increases as m0 continues to increase. When m0 = 4 and m0 = 5, it converges the fastest
in both subplots. In a, ρ = 0.5, when m0 ≥18, the population always converges; when m0 = 19, 20
it shows occasionally non-converged behaviors; and when m0 ≥30, it is always not convergent
within 1 × 107 iterations. In b, ρ = 0.7, when m0 ≥14, the population always converges; when
m0 = 15, 16, 17, 18, it shows occasionally non-converged behaviors; when m0 = 19, 20, it shows
occasionally converging behaviors, but mainly non-converged behaviors; and when m0 ≥30, it is
always not convergent within 1 × 107 iterations. The ratio of inter/intra edges per node is attached
for reference. (Data taken from [18])

104
6
Naming Game on Multi-Community Networks
Table 6.3 The mean ratio ( ¯R) of inter-connection versus intra-connection per node, when the
populationstartstoshow(occasionally)non-convergedbehaviorsover30runs.mmin
0
istheminimum
value of m0 when the population starts to become non-converged. mmin
0
is the threshold such that if
m0 = mmin
0
1 then the population always converges. Moreover, NLW = ρN/m0 and ¯R is the mean
ratio of inter-connection versus intra-connection over all nodes, averaged from 30 independent runs,
and Std is the standard deviation. (Data from [18])
mmin
0
NLW
¯R
Std
ρ = 0.4
20
20
0.0084
4.63 × 10−4
ρ = 0.5
19
26
0.0081
3.83 × 10−4
ρ = 0.6
17
35
0.0084
4.09 × 10−4
ρ = 0.7
15
46
0.0087
6.07 × 10−4
Table 6.4 The feature statistics of all MLW networks in simulations. ⟨k⟩is the average degree,
⟨pl⟩is the average path length, and ⟨cc⟩is the average clustering coefﬁcient. As m0 increases, both
⟨k⟩and ⟨cc⟩increase, while ⟨pl⟩decreases. (Data taken from [18])
m0
3
4
5
6
7
8
9
10
11
ρ = 0.5
⟨k⟩
6.09
6.82
7.24
7.58
8.09
8.56
9.2
9.54
9.95
⟨pl⟩
3.98
3.81
3.74
3.7
3.66
3.61
3.56
3.53
3.52
⟨cc⟩
0.33
0.35
0.36
0.4
0.42
0.43
0.44
0.49
0.52
ρ = 0.7
⟨k⟩
4.37
5.42
5.98
6.72
7.26
8.01
8.57
9.41
10.01
⟨pl⟩
5.21
4.56
4.48
4.24
4.23
4.08
4.02
3.9
3.87
⟨cc⟩
0.41
0.41
0.47
0.51
0.55
0.57
0.61
0.62
0.65
m0
12
13
14
15
16
17
18
19
20
ρ = 0.5
⟨k⟩
10.42
11.05
11.8
11.97
12.55
13.06
13.04
13.91
14.56
⟨pl⟩
3.49
3.46
3.37
3.42
3.38
3.35
3.4
3.33
3.31
⟨cc⟩
0.52
0.53
0.52
0.57
0.58
0.6
0.61
0.64
0.65
ρ = 0.7
⟨k⟩
10.61
11.66
12.09
12.78
13.48
14.19
14.83
15.61
16.43
⟨pl⟩
3.84
3.64
3.7
3.63
3.58
3.56
3.49
3.42
3.42
⟨cc⟩
0.68
0.66
0.7
0.72
0.72
0.74
0.75
0.74
0.77
m0
30
40
50
60
70
80
90
100
ρ = 0.5
⟨k⟩
19
23.78
29.78
33.64
39
43.34
45.94
54.61
⟨pl⟩
3.17
3.07
2.96
2.92
2.87
2.84
2.84
2.77
⟨cc⟩
0.73
0.78
0.79
0.82
0.86
0.86
0.86
0.9
ρ = 0.7
⟨k⟩
22.91
29.8
37.18
42.32
51.39
54.36
59.83
72.41
⟨pl⟩
3.29
3.14
3.09
2.97
2.91
2.79
2.75
2.72
⟨cc⟩
0.84
0.86
0.9
0.89
0.93
0.91
0.92
0.94
Figure6.3 shows an example illustrating how the intra-connections become
stronger as the community size increases. The number of intra-connections is getting
greater as the community size is getting larger, since initially all the local-worlds are
fully-connected to provide good local connectivity. The inter-connection is ﬁxed to
be 1 in the three subplots. Therefore, the number of intra-connection versus inter-

6.3 Simulation Results and Analysis
105
n2
n1
n3
nex
n3
n1
n4
nex
n2
n4
n2
n5
nex
n3
n1
n6
(a)
(b)
(c)
Fig. 6.3 An illustrative example for intra-community and inter-community connections: a a
3-node community with 1 external connection; b a 4-node community with 1 external connection;
c a 6-node community with 1 external connection. nex is an external node outside the community.
The number of intra-connection versus inter-connection is: a 3:1; b 6:1; and c 15:1
connection can be seen from Fig.6.3a as 3:1, from Fig.6.3b as 6:1, and from Fig.6.3c
as 15:1. However, if one wishes to keep the ratio constant, e.g., 3, then for a 4-node
community there should be one other node connected externally, so that there are 6
intra-connections versus 2 inter-connections, yielding a ratio of 3. As for a 6-node
community, the number of inter-connections should be 5 to maintain a ratio of 3.
The inter-connections are generated by the addition of ⌊N · (1 −ρ)⌋nodes repeat-
edly and randomly selecting operations from a to e (see Sect.6.2 for more details).
As shown in Fig.6.3, if the number of inter-community connections is ﬁxed, but the
size of the local community is growing, then the number of external connections is
becoming insufﬁcient for information propagation and transmission, thereby likely
hindering global convergence.
For convenience, the inter-community connections of each MLW network are
kept constant, and the number and sizes of the communities are varied, so that
the ratio of intra-connection and inter-connection can be controlled. As a result, as
intra-connections increase while inter-connections are kept constant, the convergence
process will be slowed down and eventually failed. This also explains more clearly
the incremental convergence time shown in Fig.6.2.
6.3.2
Convergence Time Versus Rate of Initially
Assigned Nodes
In this subsection, both the number and the sizes of all local-worlds are ﬁxed, while
the rate ρ varies from 0.1 to 0.9. Here, ρ controls the assignment of nodes in initial
communities.
As can be seen from Fig.6.4 a common ground is that, when ρ is small (e.g.,
ρ ≤0.6 in Fig.6.4a; ρ ≤0.5 in Fig.6.4b; and ρ ≤0.3 in Fig.6.4c), different settings
of ρ do not signiﬁcantly affect the convergence time, or even not at all. Note that the

106
6
Naming Game on Multi-Community Networks
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
convergence time
×106
0
2
4
6
8
10
initial rate
(a) An MLW example with N = 100,
NLW = 4, and m0 = 20
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
convergence time
×106
0
2
4
6
8
10
initial rate
(b) An MLW example with N = 110,
NLW = 16, and m0 = 6
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
convergence time
×106
0
2
4
6
8
10
initial rate
(c) An MLW example with N = 110,
NLW = 16, and m0 = 6
Fig. 6.4 A boxplot of the convergence time versus the rate ρ of initially assigned nodes in commu-
nities. The number m0 of initial nodes in each local-world is set to 4, 10, and 18, respectively; the
number of local-worlds is calculated by NLW = ⌊ρN/m0⌋. In each subplot, as ρ varies from 0.1
to 0.9, the convergence time shows oscillations slightly prior to a prominent ascending progress.
(Data taken from [18])
inter-community connections are generated by the addition of ⌊N · (1 −ρ)⌋nodes,
which are very important for information exchange inter-community-wise. When ρ
is small, the inter-community connections are substantial and probably sufﬁcient for
achieving global convergence.
As ρ continues to increase, the value of ⌊N · (1 −ρ)⌋deceases, meaning that inter-
connections are reducing and probably becoming insufﬁcient for achieving global
consensus. If ρ reaches certain large values (e.g., ρ > 0.6 in Fig.6.4a; ρ > 0.5 in
Fig.6.4b; and ρ > 0.3 in Fig.6.4c), non-convergence behaviors of the population
emerge. Denote the threshold value of ρ by ρth. When ρ ≤ρth, the convergence time

6.3 Simulation Results and Analysis
107
Table 6.5 Feature statistics of the 4 networks. ⟨k⟩is the average degree, ⟨pl⟩is the average path
length,and⟨cc⟩istheaverageclusteringcoefﬁcient.ThedataforMLWiscollectedfromexperiments
in Sect.6.3.1, while those for RG, SW and SF networks are generated using the ⟨k⟩values of the
MLW for reference. The 4 types of networks have very similar ⟨k⟩values. The size of population
is set to N = 1500. (Data taken from [18])
Reference ⟨k⟩
⟨pl⟩
⟨cc⟩
MLW RG
SW
SF
MLW RG
SW
SF
MLW RG
SW
SF
m0 = 10
9.22
9.05
10
9.98 4.25
3.57
4.07
3.12
0.64
0.01
0.33
0.03
m0 = 20
16.1
15.75
16
15.94 3.69
2.92
3.4
2.77
0.78
0.01
0.37
0.05
m0 = 30
23.36 23.39
22
21.91 3.38
2.67
3.02
2.61
0.84
0.02
0.39
0.05
m0 = 100 37.35 37.23
38
37.73 3.22
2.36
2.64
2.3
0.9
0.02
0.38
0.08
is generally not affected by different values of ρ, while when ρ > ρth, the convergence
time increases drastically as ρ increases.
As can be seen from Fig.6.4, as m0 increases (m0 = {4, 10, 18}), ρth decreases
(ρth = {0.6, 0.5, 0.3}). This phenomenon is also observable when the population size
is set to 500 and 1500, respectively [19]. The phenomenon can be partially explained
by the example shown in Fig.6.3, where the number of intra-community connections
is
m0
2

. Note that, in Fig.6.3, the number of intra-connections is exactly
m0
2

; but
in a real MLW network, it is roughly
m0
2

due to the edge-removal operations in
the MLW-generation algorithm. This means that, when m0 is small, the number of
intra-community connections is also small, so that the demanded inter-community
connections are not too many. Thus, a small ρ does not affect the convergence time,
until ρ becomes large. In contrast, when m0 is set to be large, the number of intra-
community connections is relatively large, so that the demanded inter-community
connections are many. Thus, even ρ is relatively small, the convergence time is clearly
affected, due to the large number of inter-community connections.
6.3.3
Convergence Process
The convergence progress of naming game in MLW networks is compared to three
typical underlying networks, i.e., RG, SW, and SF models.
Comparison is performed in terms of the number of total words, the number of dif-
ferent words, and the success rate. A total of 4 parameter settings are simulated, with
m0 = {10, 20, 30, 100}, of which the average degree is 9.22, 16.1, 23.36, and 37.35,
respectively. For fairness in comparisons, RG, SW, and SF networks are generated
using the same (or similar, if cannot be exact) average degree as in the corresponding
MLW network. The statistics of the 4 types of resulting networks are summarized
in Table6.5. Here, the population size is set to 1500, while the scaling property is
studies in [18, 19] when population size is set to 500 and 1000 respectively.

108
6
Naming Game on Multi-Community Networks
iteration
(a)
104
105
106
number of total words
1400
1600
1800
2000
2200
2400
2600
2800
MLW
RG
SW
SF
iteration
(b)
104
105
106
number of total words
1400
1600
1800
2000
2200
2400
2600
2800
iteration
(c)
104
105
106
number of total words
1400
1600
1800
2000
2200
2400
2600
2800
iteration
(d)
104
105
106
number of total words
1400
1600
1800
2000
2200
2400
2600
2800
3000
Fig. 6.5 Convergence process in terms of the number of total words, with: a ⟨k⟩≈9.22, b ⟨k⟩≈
16.1, c ⟨k⟩≈23.36, and d ⟨k⟩≈37.35. In each subplot, RG (blue triangle) curve has the highest
peak but the fastest convergence speed. In contrast, MLW (red square) curve has the lowest peak but
slowest convergence speed. SW (black pluses) and SF (green circles) behave somewhere between
RG and MLW. This phenomenon is similarly observed from the 4 subplots. (Data taken from [19])
As shown in Table6.5, the 4 types of networks have very similar average degrees,
with negligible differences. Among them, MLW has the largest average path length
and clustering coefﬁcient values, followed by SW which has the second greatest
average path length and the second greatest clustering coefﬁcient values. Both RG
and SF have smaller values of these two measures. Apparently, MLW networks are
the most clustered with the strongest community structure among the 4 topologies.
In Figs.6.5, 6.6, and 6.7, the 4 parameter settings are: (a) ⟨k⟩≈9.22, (b) ⟨k⟩≈
16.1, (c) ⟨k⟩≈23.36, and (d) ⟨k⟩≈37.35. The 4 types of topologies have the same
(or very similar) average degrees.
Figure6.5 shows two common phenomena that can be observed in all subplots:
(1) the population with RG underlying network converges the fastest, followed by
that of SF and SW, and MLW converges the slowest. Note that, in Fig.6.5a, naming
game on MLW converges, but in Fig.6.5b–d, the population does not converge; (2)
the curves with RG underlying network has the highest peak, followed by that of SF
and SW, and MLW has the lowest peak. Recall from Table6.5 that RG has the lowest

6.3 Simulation Results and Analysis
109
iteration
(a)
104
105
106
number of different words
0
100
200
300
400
500
600
MLW
RG
SW
SF
iteration
(b)
104
105
106
number of different words
0
100
200
300
400
500
600
iteration
(c)
104
105
106
number of different words
0
100
200
300
400
500
600
iteration
(d)
104
105
106
number of different words
0
100
200
300
400
500
600
Fig.6.6 Convergenceprocessintermsofthenumberofdifferentwords:a ⟨k⟩≈9.22,b⟨k⟩≈16.1,
c ⟨k⟩≈23.36, and d ⟨k⟩≈37.35. Differing from Fig.6.5, the peaks of all 4 curves in each subplot
are similar to each other, around 600. Similarly to Fig.6.5, the ranking of the convergence speeds
is: RG (blue triangles) converges the fastest; SW (black pluses) the second, followed by SF (green
circles); MLW (red squares) converges the slowest. (Data taken from [18])
clustering coefﬁcient values, followed by SF and SW, and MLW has the greatest,
meaning that MLW has the strongest structure in clustering and in communities
formation than the other three networks. Simply observed from Fig.6.5 and Table6.5,
a greater clustering coefﬁcient value in the underlying network leads to a slower
convergence speed, and a lower peak of the curve for the number of total words.
This also leads to the following two outcomes: (1) agents in the same community
reach convergence quickly, so that the number of total words in the entire population
decreases fast when there are communities in the underlying network; and (2) the
inter-community convergence process is delayed or even prevented by the multi-
community topology. This can be further summarized as follows: given the same
average degree of networks, the less clustered underlying network has a convergence
curve with a higher peak and sharper decline; in contrast, a more clustered underlying

110
6
Naming Game on Multi-Community Networks
iteration
(a)
104
105
106
success rate
0
0.2
0.4
0.6
0.8
1
MLW
RG
SW
SF
iteration
(b)
104
105
106
success rate
0
0.2
0.4
0.6
0.8
1
iteration
(c)
104
105
106
success rate
0
0.2
0.4
0.6
0.8
1
iteration
(d)
104
105
106
success rate
0
0.2
0.4
0.6
0.8
1
Fig. 6.7 Convergence process in terms of the success rate: a ⟨k⟩≈9.22, b ⟨k⟩≈16.1, c ⟨k⟩≈
23.36, and d ⟨k⟩≈37.35. The success rate curves of RG and SW are simple, while the other two
are ﬂuctuating. In particular, the curves of MLW are visually fuzzy and do not eventually reach the
value 1.0, in all subplots. (Data taken from [18])
network leads to a ﬂatter curve with a lower peak, which probably hinders the global
consensus.
In the extreme case where the underlying topology is a tree (with average degree
⟨k⟩= 2 −2
N and clustering coefﬁcient ⟨cc⟩= 0), or a globally-fully-connected net-
work (with average degree ⟨k⟩= N −1 and clustering coefﬁcient ⟨k⟩= 1), naming
game is not investigated in the above simulations. This is because, in these two spe-
cial cases with the given average degree value, the clustering coefﬁcient cannot be
adjusted for comparisons.
In Fig.6.6, although the ranking of the convergence speeds is exactly the same as
that shown in Fig.6.5, the peaks of the curves show different features. All the peaks
are around 600, because not only the lexicon but also the game rules are identical
for all types of underlying networks. Here, recall that the rule is that if the picked
speaker has nothing in his memory then he randomly picks a name from the external
lexicon. With this rule, around ⌊N/2⌋different words are picked in the population.
Since some words may be lost during local consensus, the peak of the different-word
curve is lower than ⌊N/2⌋.

6.3 Simulation Results and Analysis
111
Figure6.7 shows the success rate. It is obvious that, when a network has a small
clustering coefﬁcient value, its success rate curve is generally smooth. However, for
SW and MLW underlying networks, high clustering coefﬁcient values generate very
rough success rate curves. For SW, although rough, the success rate can eventually
reach 1.0; but for MLW, if the population does not converge to global consensus,
as shown in Figs.6.5 and 6.6, the success rate cannot reach 1.0. This is because, in
the later stage: (1) agents in the same community have already converged, so that
the success rate of intra-communication is as high as 1.0, but (2) agents in different
communities have converged to generally different names, so that the success rate of
inter-communication is likely to be as low as 0. As a result, the overall success rate
curves are ﬂuctuating and visually fuzzy.
6.3.4
Discussion
Consider a real-life situation where there are two different local communities, i.e.
local-worlds, denoted by LW: one is located in the suburb of a city, denoted by
LWm; the other is a primitive tribe, denoted by LWp. Presumably, LWm has many
connections to the city, as well as to other cities. The connections include roads,
telephone systems and the Internet, etc. However, LWp has perhaps only one path
to connect to the outside world without further communication means. Suppose also
that, within each community, people know each other therefore they have direct
communications, implying a fully-connected community.
Furthermore, suppose that the community size is relatively small, either LWm or
LWp, so that information can be easily transmitted within each community. Agents
in a community can be inﬂuenced by the outside world as well, and will reach
global consensus with the outside world if the naming game ﬁnally succeeds. On
the contrary, if the community size is relatively big, then a large number of external
edges are needed to achieve global consensus; otherwise, many agents cannot receive
information from outside their communities. In such a situation (e.g., with a large
value of LWp), they could only reach local consensus. When the external connections
in a big community are rare, very few agents can obtain information from the outside
world. Thus, before these agents can inﬂuence the entire community to converge,
they are inﬂuenced by the strong local consensus instead. As a result, it is very
difﬁcult if not impossible to reach global consensus for a big community with poor
external-connections.
In addition, given a ﬁxed number of inter- or intra-connections, the average degree
of a node is basically unchangeable, which is generally not good for global consensus.
For example, suppose that everyone has a ﬁxed number of friends to communicate
with. Then, people can only communicate with their local friends, so there is less
chance for them to communicate with other people globally. Consequently, vari-
ous communities are formed and so global consensus is hindered because global
consensus requires people to have sufﬁcient chances to communicate globally.

112
6
Naming Game on Multi-Community Networks
6.4
Conclusion
In this chapter, naming game is performed on the multi-local-world (MLW) model
as underlying communication networks. Its performance is compared to those on
three other typical models, namely RG, SW and SF networks. It is revealed that
community structures are essential for social communications, for which the MLW
model is more practical to be used as the underlying network than the other three
network topologies.
Extensive simulations are performed to study the effects of the number of com-
munities as well as the size of each community on the naming game convergence
speeds over different networks, with or without communities. Simulations are com-
pared by varying several key model parameters. The results suggest that: (1) suf-
ﬁciently many inter-community connections are crucial for speeding up the global
convergence; thus, given a constant number of inter-connections, when the num-
ber of intra-connections increases, meaning that the inter-connections are relatively
weakened, the convergence process will be slowed down and eventually may fail; (2)
for sufﬁciently many inter-community connections, both the number and the sizes
of the communities do not affect the convergence signiﬁcantly, or even not at all;
and (3) for the same average degree (the same number of connections) in different
underlying network topologies, different clustering coefﬁcients distinctively affect
the convergence speeds, which also change the shapes of the convergence curves.
Higher clustering hinders the global consensus in general.
The simulation results on MLW networks highlight the importance of communi-
ties in naming games.
References
1. A. Baronchelli, L. Dall’Asta, A. Barrat, V. Loreto, The role of topology on the dynamics of the
naming game. Eur. Phys. J. Spec. Top. 143(1), 233–235 (2007). https://doi.org/10.1140/epjst/
e2007-00092-0
2. A. Baronchelli, V. Loreto, L. Steels, In-depth analysis of the naming game dynamics: the
homogeneous mixing case. Int. J. Mod. Phys. C 19, 785–812 (2008). https://doi.org/10.1142/
S0129183108012522
3. L.Dall’Asta,A.Baronchelli,A.Barrat,V.Loreto,Nonequilibriumdynamicsoflanguagegames
on complex networks. Phys. Rev. E 74(3), 036105 (2006). https://doi.org/10.1103/PhysRevE.
74.036105
4. L. Dall’Asta, A. Baronchelli, A. Barrat, V. Loreto, Agreement dynamics on small-world net-
works. EPL (Europhys. Lett.) 73(6), 969 (2006). https://doi.org/10.1209/epl/i2005-10481-7
5. R.R. Liu, C.X. Jia, H.X. Yang, B.H. Wang, Naming game on small-world networks with
geographical effects. Phys. A 388, 3615–3620 (2009). https://doi.org/10.1016/j.physa.2009.
05.007
6. A. Baronchelli, M. Felici, V. Loreto, E. Caglioti, L. Steels, Sharp transition towards shared
vocabularies in multi-agent systems. J. Stat. Mech. Theory Exp. 6, P06014 (2006). https://doi.
org/10.1088/1742-5468/2006/06/P06014

References
113
7. Q.Lu,G.Korniss,B.K.Szymanski,Thenaminggameinsocialnetworks:communityformation
and consensus engineering. J. Econ. Interact. Coord. 4(2), 221–235 (2009). https://doi.org/10.
1007/s11403-009-0057-7
8. W.X. Wang, B.Y. Lin, C.L. Tang, G.R. Chen, Agreement dynamics of ﬁnite-memory language
gamesonnetworks.Eur.Phys.J.B60(4),529–536(2007).https://doi.org/10.1140/epjb/e2008-
00013-5
9. B.D. Vylder, K. Tuylsl, How to reach linguistic consensus: a proof of convergence for the
naming game. J. Theor. Biol. 242(4), 818–831 (2006). https://doi.org/10.1016/j.jtbi.2006.05.
024
10. D. Centola, A. Baronchelli, The spontaneous emergence of conventions: an experimental study
of cultural evolution. Proc. Natl. Acad. Sci. USA 112(7), 1989–1994 (2015). https://doi.org/
10.1073/pnas.1418838112
11. A. Baronchelli, Role of feedback and broadcasting in the naming game. Phys. Rev. E 83,
046103 (2011). https://doi.org/10.1103/PhysRevE.83.046103
12. D.J. Barr, Establishing conventional communication systems: Is common knowledge neces-
sary? Cogn. Sci. 28(6), 937–962 (2004). https://doi.org/10.1016/j.cogsci.2004.07.002
13. G.R. Chen, Z.P. Fan, X. Li, Modelling the complex internet topology, Complex Dynamics
in Communication Networks (Springer, Berlin, 2005), pp. 213–234. https://doi.org/10.1007/
10973509_9
14. Z.P. Fan, G.R. Chen, Y.N. Zhang, A comprehensive multi-local-world model for complex
networks. Phys. Lett. A 373(18), 1601–1605 (2009). https://doi.org/10.1016/j.physleta.2009.
02.072
15. G. Siganos, M. Faloutsos, P. Faloutsos, C. Faloutsos, Power laws and the AS-level inter-
net topology. IEEE/ACM Trans. Netw. 11(4), 514–524 (2003). https://doi.org/10.1109/TNET.
2003.815300
16. M.E.J. Newman, J. Park, Why social networks are different from other types of networks. Phys.
Rev. E 68(3), 036122 (2003)
17. D.W. Guo, X.Y. Meng, M. Liu, C.F. Hou, Naming game on multi-community network. J.
Comput. Res. Dev. 52(2), 487–498 (2015)
18. Y. Lou, G.R. Chen, Z.P. Fan, L.N. Xiang, Local communities obstruct global consensus: naming
game on multi-local-world networks. Phys. A 492, 1741–1752 (2018). https://doi.org/10.1016/
j.physa.2017.11.094
19. Lou, Y., Chen, G.R., Fan, Z.P., Xiang, L.N.: Supplementary information for paper “local com-
munities obstruct global consensus: naming game on multi-local-world networks” (2015),
http://www.ee.cityu.edu.hk/~gchen/pdf/MLW-SI.pdf
20. B. Li, G.R. Chen, T.W.S. Chow, Naming game with multiple hearers. Commun. Nonlinear Sci.
Numer. Simul. 18, 1214–1228 (2013). https://doi.org/10.1016/j.cnsns.2012.09.022

Chapter 7
Multi-Word Naming Game
7.1
Introduction
Naming game is a simulation-based numerical experiment exploring the emergence
of shared lexicon in a population of communicating agents about an object that they
all observed [1–3]. The single object in naming game is typically a simple entity that
can be described by a single word [4]. Here, the simplicity refers to the convention
that the object is always considered as a single entity without concerning its internal
components or detailed features. The entire population of agents are communicating
to each other in some way according to the game rules, where the relationships among
the agents are represented by a connected network in a certain topology, referred to
as the underlying network. The naming game models introduced and discussed in
the previous chapters, including the minimal naming game, ﬁnite-memory naming
game (FMNG), multiple hearers naming game (MHNG), naming game in groups
(NGG), and naming game with learning errors (NGLE), are all single-word naming
games (SWNG). Technically, there is only one single word being transmitted at every
iteration step through the whole communication process over the population.
SWNG models were studied with focus on two aspects: one is on the variation
of the agents in the population [2, 5–11] and the other is on the variation of the
transmitting information in the process [12–14].
Speciﬁcally, the scenario of variation of agents includes the following:
(1) The variation on the topological relationships among agents. For example, the
minimal naming game is investigated on random-graph and scale-free networks
in [6, 8], and on small-world networks in [7, 11].
(2) The selection of speaker and hearer. For example, the direct strategy is proposed
in [7, 11] and the inverse strategy, in [11].
(3) The communication method among the agents. For example, speaker-only nam-
ing game (SO-NG) and hearer-only naming game (HO-NG) are studied in [5].
In SO-NG, only the speaker will update his memory with a certain probability
© Springer Nature Switzerland AG 2019
G. Chen and Y. Lou, Naming Game, Emergence, Complexity and Computation 34,
https://doi.org/10.1007/978-3-030-05243-0_7
115

116
7
Multi-Word Naming Game
after reaching consensus, while in HO-NG only the hearer will do so. In the SO-
(HO-)NG model, the hearer (speaker) has no chance to reach local consensus.
The MHNG model studied in [10] includes an additional rule: only when all
hearers reached consensus with the same speaker, they reach local consensus.
The naming game in groups (NGG) is a multiple-speaker and multiple-hearer
model developed in [9], which allows every agent in a group selected from the
population to play the dual role as a speaker and also as a hearer simultaneously.
(4) The characteristics or personality of the agents. For example, some agents may
be forgetful [15], or committed on certain opinion [16].
The variation of transmitted information is another important aspect to study.
Naming game with learning errors (NGLE) in communications, investigated in [12],
reveals that learning errors with a certain small probability will not delay the con-
vergence speed of global consensus, with or without a remedial strategy to prevent
everlasting errors from happening.
All the aforementioned naming game models use atomic names for transmission.
An atomic name is a unique and independent unit that is distinguishable from any
other atomic name, e.g., ‘blue’, ‘sky’, and ‘print’ are three atomic names. On the
contrary, a combinational name is a permuted combination of some atomic names,
e.g., ‘sky-blue’, ‘blue-sky’, and ‘blueprint’ are three different combinational names.
An atomic name is considered as a whole, while a combinational name is associ-
ated with its atomic components. By these conventions, the so-called combinational
naming game is investigated in [13], for the case where the names are composed of
several atomic names. The blending game discussed in [14] uses similar combina-
tional forms, but with different communication rules, where a blending name will
be composed only if a speaker-uttered name is not agreed by the hearer. Although
a combinational name contains more information than an atomic name, it is never-
theless simpler than a sentence. Both atomic names and combinational names are
considered single names and, as thus, both atomic naming game and combinational
naming game are SWNG.
Practically, a word for an object might be atomic or combinational, which is based
not absolutely but only relatively on the coding method that maps from the set of
atomic words into the set of combinational words [13]. An illustrative example is
shown in Fig.7.1, where the integer coding could transform the atomic names to be
combinational, and then be further decomposed into binary strings.
In this chapter, multi-word naming game (MWNG) is introduced to study the
scenario where the names of some objects are described by neither atomic nor com-
binational words, but more complicatedly by sentences. It is assumed that the object
being named in MWNG is too complicated to be described by a single word. Again,
the case that there are multiple different objects to be named is not considered.
Thus, there is only one object to be named in both SWNG and MWNG models.
In these models, ﬁrst, the lexicon is divided into several categories, and then pat-
terns are deﬁned as sentential structures, so that a sentence can be composed strictly
following a certain pattern. Note that, due to the duality caused by coding, one can
consider a speciﬁc sentence as a whole, and in this case MWNG degenerates to

7.1 Introduction
117
Fig. 7.1 An example of coding. Two atomic names, ‘night’ and ‘light’, can be decomposed into
5 independent letters respectively, where the integers are ASCII code for each letter. These two
atomic names could be considered as ‘n’ + ‘ight’ and ‘l’ + ‘ight’, respectively. The only difference
between ‘night’ and ‘light’ is the initial words ‘n’ and ‘l’. They can also be further decomposed
into other components. When every letter in a name is encoded in binary, the letters can be further
decomposed into sequences of binary strings, e.g., ‘011’, ‘0’, ‘1’, and ‘00’. As a result, ‘n’ consists
of two ‘011’s, and one ‘1’ and one ‘0’ orderly, and ‘l’ consists of two ‘011’s followed by one ‘00’
SWNG, or a minimal naming game. In this sense, MWNG is a natural extension of
SWNG.
In the MWNG model, ﬁrstly several word categories are deﬁned, and for each
category a large and independent vocabulary is made available. Then, sentences
are composed by organizing some different words from different categories, in a
mechanical way. A formatted combination of several words is referred to as a pattern,
which is more complicated than a combinational name, but simpler than one that
satisﬁes a systematical grammar. A composed sentence is a basic unit of propagating
information in the MWNG model, while in SWNG models it is merely a single word.
Through a pair-wise conversation, the hearer is expected to achieve consensus with
the speaker about the entire sentence, thus both every single word in the sentence as
well as the sentence pattern must be learned. All these together guarantees the correct
meaning of the saying; otherwise, they fail reaching consensus in the conversation.
In the following study of the MWNG model, three typical topologies are used
as the underlying communication networks, namely the random-graph (RG), small-
world (SW) and scale-free (SF) networks. Both conventional English language pat-
terns and man-designed test sentence patterns are employed to test and verify the
MWNG model. The simulation results show that: (1) The new sentence-sharing
model is an extension of the classical word-sharing naming game models. (2) The
convergence processes is more complicated than that in the conventional single-word
naming game models; as a result, the propagating, learning and converging speeds
are signiﬁcantly slower. (3) The convergence time is generally not decreasing nor
increasing as the network becomes better connected. (4) The entire population is
prone to converging to short sentence patterns.
The rest of the chapter is organized as follows: The MWNG model is introduced in
Sect.7.2. Then, in Sect.7.3, simulations are presented on the three typical topologies
of communication networks, i.e. RG, SW and SF models, with different parameter

118
7
Multi-Word Naming Game
settings, together with analysis and comparisons. Conclusions are drawn with some
discussions in Sect.7.4.
7.2
Multi-Word Naming Game
The distinctive feature of MWNG is that each speaker utters a sentence rather than
a single word to describe an object (e.g., an opinion, an event, etc.), and accordingly
each hearer learns (or consents to) the whole sentence. This consensus process is
more realistic and common in human conversations.
From a network science perspective, a pattern of sentences in MWNG is kept
simple, as long as it is able to show the organizing structure of the words. The
simple patterns deﬁned in MWNG could also be considered as an simpliﬁed version
of a grammar. A practical and simple implementation is that a pattern is formed
by a combination of several words, where all single words have been labeled with
different categories in advance. Words from the same category are of equivalent
importance in a pattern. For example, let two word categories noun and verb be
deﬁned, where the set of word category noun includes two words {‘boy’, ‘girl’},
and verb includes three words {‘run’, ‘sing’, ‘jump’}. Given a pattern of sentence
composed by ‘noun+verb’, by picking one word from noun and another from verb,
one could get 6 resulting sentences in two-word combinations from {‘boy’, ‘girl’}
+ {‘run’, ‘sing’, ‘jump’}. A word has no priority to another if they are in the same
category.
Categorization of words directly affects the resolution of a pattern. The term
resolution here is concerned with the correctness and precision of a sentence. For
example, one may put all the words roughly into three categories, noun, verb, and
adjective. Given a pattern ‘noun + verb + noun’, a sentence like ‘boys play football’
could be generated. Likewise a sentence ‘football plays boys’ perfectly follows the
same pattern, but practically meaningless. This means that the pattern ‘noun + verb +
noun’ is of low resolution, which leads the resulting sentences to be of low precision
(either ambiguous or meaningless). On the contrary, the classiﬁcation of words could
be very detailed, while many word categories are arbitrarily deﬁned. Thus, the deﬁned
patterns would be of high resolution. For example, given a delicately deﬁned pattern
like ‘human + human-action-verb + sports-name’, following this pattern one could
generate a meaningful sentence like ‘boys play football’. But meaningless sentences
like ‘football plays boys’ will be excluded from such a pattern. Generally, too few
categories will probably lead to too many ambiguous or meaningless sentences, but
too many categories are clearly inefﬁcient in communications.
This chapter studies some simple and conventional patterns of the English lan-
guage, as well as some sets of man-designed sentence patterns. The category clas-
siﬁcation of vocabularies may be arbitrary [17]. In MWNG, it is assumed that the
associated category is an intrinsic property of a word, so that an agent identiﬁes
the category of the word as soon as he has it (either inventing it or receiving it).

7.2 Multi-Word Naming Game
119
For example, an agent would immediately map a newly learned word ‘boy’ into the
category human, and put ‘play’ into the category human-action.
For simplicity, two assumptions are imposed into MWNG: (1) The pattern of
a sentence is a unique ordering of word categories; (2) the tag indicating the word
category is associated with a word, so that an agent can identify the category immedi-
ately when he learned it. In the model, therefore, each sentence has one and only one
pattern without exception. As soon as an agent learned a sentence, he can abstract
the pattern according to the pre-set categories and the ordering of the composing
words in that sentence. In real-life communications, however, one would not offer
a category tag along with a word he said, but it is very common that a speaker will
provide some additional information if the word is new to the hearer. One example is,
say, to speak the unusual word ‘chakalaka’, associated with additional information
‘African food’, in order for the hearer to understand. To remember this word, the
hearer typically associates the word ‘chakalaka’ with some tags in mind, such as
‘noun’, ‘food’, or ‘exotic’. Note that, in the naming game, a hearer will learn a word
only if it is new, which means that the word was not in his memory. In MWNG,
therefore, it is assumed that the speaker is obligated to explain to the hearer whatever
needed, or equivalently the hearer is assumed smart enough to understand whatever
new words he learned. These natural assumptions make the model simpler to use and
easier to discuss.
Figure7.2 shows a ﬂowchart of one-time-step communication in MWNG. In the
beginning, a connected pair of speaker-hearer is randomly picked. Either direct strat-
egy [8, 11] or reverse strategy [11] can be applied. Here, in MWNG, the direct strat-
egy is employed, where the speaker is uniformly randomly selected from the entire
population, and then a hearer is uniformly randomly picked from all the neighbors
of the speaker. The study of MWNG with the reverse strategy is discussed in [18],
where with the reverse strategy a hearer is picked from the population ﬁrst, and then
a speaker is picked from the neighbors of the hearer, both uniformly randomly.
7.2.1
Conventional Sentence Patterns
First, ﬁve simple conventional English language patterns are chosen to study the
sentence propagation in MWNG. The vocabulary is classiﬁed into four categories,
namely subject, verb, object, and complement. Some basic permutation and combi-
nation of these four categories lead to ﬁve conventional sentence patterns, as shown
in Fig.7.3.
Real-life situations are more complicated, of course. A sentence is usually with
contents supported by some conversation and background information. The contents
help disambiguate the conversation, so the speaker and the hearer may sometimes
immediately reach a partial-consensus state. A partial-consensus state refers to the
result of a conversation that the hearer agrees part of the information received from
the speaker, but disagrees with the rest. For example, if the speaker utters a sentence
‘this is a book’, the hearer may agree or disagree with any part of the sentence; for

120
7
Multi-Word Naming Game
Fig. 7.2 Flowchart of one iteration in the MWNG. Iteration starts with a uniformly randomly
selected speaker-hearer pair from the population, using the direct strategy. Then, the speaker utters
a sentence taken from his memory, if any; otherwise, he generates a new sentence (together with its
pattern). The hearer receives the sentence and then checks if he has the same pattern stored in his
memory. Only if the hearer has the same pattern in memory, he will then verify the sentence word
by word; otherwise, he learns the pattern and/or (some) words in the received sentence. Only if the
hearer has the same sentence pattern and all words as the speaker uttered, he and the speaker reach
a local consensus. Then, both the speaker and the hearer keep the consented sentence (including the
same pattern and the same words) in their memories, and drop all other patterns and words therein
example, the hearer agrees with ‘this’ but disagrees with ‘book’ (he might believe
that ‘this is a pen’). Thus, it is easy for them to further negotiate about the disagreed
word(s), which eventually leads to a local consensus between them.
However, such useful contents are not offered to the agents in naming game
simulations. Also, an MWNG model with a partial-consensus rule is essentially
equivalent to a parallel combination of several minimal naming games. As such, the

7.2 Multi-Word Naming Game
121
Fig. 7.3 The 5 conventional English language patterns are denoted by P1–P5, respectively. Pattern
abbreviations are given inside the parentheses, and an example is shown below each pattern therein.
Note that, in P4, there are two components belonging to the category object. In simulation, the
category object is stored in two independent subsets, one for indirect object and the other for direct
object. When P4 is encountered, the two subsets are treated independently. When P2 and P5 (which
include object without distinguishing direct or indirect), the two subsets are treated as one union
set, thus a speaker will choose one object from the ensemble set of indirect object and direct object,
uniformly randomly
local partial-consensus situation is not discussed here. In MWNG, it requires the
hearer to achieve consensus with the speaker with respect to the sentence pattern as
well as every single word in the sentence, which ensures the correct meaning of the
saying; otherwise, they fail reaching consensus in the present interaction.
7.2.2
Local Consensus
Figure7.4 shows an example of pair-wise communications, in which Fig.7.4a shows
the situation where the hearer learns a new pattern together with the whole sentence
received from the speaker, Fig.7.4b shows that although the hearer has the same
pattern, he learns some new words from the speaker because the hearer does not have
some single word of the sentence in his memory, Fig.7.4c shows local consensus,
and Fig.7.4d is the result of local consensus.
The following compares the probabilities of local consensus in the minimal nam-
ing game and MWNG.
Suppose that, at a time step of the minimal naming game, two connected agents A
and B have been picked up as speaker and hearer, respectively. Agent A has L A words
in his memory while agent B has L B words. The number of common words that both
A and B have is IAB. The probability of reaching consensus within one iteration is
PNG = IAB/L A. As for MWNG, an agent has several separated parts of memory

122
7
Multi-Word Naming Game
Pattern
S+V
S+V+C
(a) Hearer learns the sentence and the pattern
(b) Hearer learns some word(s) of the sentence 
(c) Hearer agrees the sentence (and as well the pattern) 
(d) local consensus
Subject
girl
cream
cat
Verb
sing
taste
swim
Object
Complement
delicious
fast
Speaker
Pattern
S+V
S+V+C
Subject
girl
cream
cat
Verb
sing
taste
swim
Object
Complement
delicious
fast
Speaker
Pattern
S+V
S+V+C
Subject
girl
cream
cat
Verb
sing
taste
swim
Object
Complement
delicious
fast
Speaker
Pattern
S+V
Subject
girl
Verb
swim
Object
Complement
Speaker
Pattern
S+V
S+V+O
S+V+C
Subject
girl
student
cream
Verb
play
swim
taste
Object
football
tennis
Complement
delicious
Hearer
Pattern
S+V
S+V+O
Subject
girl
student
cat
Verb
play
swim
Object
football
tennis
Complement
Hearer
Pattern
S+V
S+V+O
Subject
girl
student
Verb
play
swim
Object
football
tennis
Complement
Hearer
Pattern
S+V
Subject
girl
Verb
swim
Object
Complement
Hearer
Fig. 7.4 An example of local pair-wise communication. a Speaker randomly chooses a pattern
‘S+V+C’ (P3), composes a sentence ‘cream taste(s) delicious’ accordingly, and utters it. Since
Hearer has no pattern P3 in memory, he learned the pattern, together with the three words in the
sentence. b Although Hearer has the same pattern P1 as Speaker uttered, he does not have an
identical sentence; so he learned the new words from the sentence. Since Hearer has already had the
word ‘swim’ in his memory, he would neither learn it again nor consent to this single word. Hearer
learned a new word ‘cat’. c Hearer has the same pattern P1, as well as all the words in the sentence,
therefore reaching local consensus. d The state when Speaker and Hearer reach a pair-wise local
consensus: both Speaker and Hearer have only pattern P1 and the agreed words in the sentence,
‘girl swim(s)’, in their memories
to store patterns and words of different categories. Denote the memory lengths for
patterns by L P
A (for agent A) and L P
B (for agent B). All the words W are evenly
divided into M categories, i.e., W = {W1, W2, . . . , WM}, and the numbers of words
remembered by agents A and B are denoted by LWi
A and LWi
B (i = 1, 2, . . . , M),
respectively. The number of common patterns that both A and B have is denoted by
I P
AB, and the numbers of common words are denoted by I Wi
AB, i = 1, 2, . . . , M. The
probability of reaching consensus within one communication is
PMW NG = ρ I P
AB
L P
A
(7.1)
where ρ = 
j∈R
I
W j
AB
L
W j
A
, and R (R ⊂W) represents the component word categories of
the picked pattern.
An illustrative example is given in Fig.7.5. The number of remembered words (or
patterns) can be seen at the lower-right corner of each agent’s memory box, shaded
in gray. Figure7.5a shows the case of the minimal naming game, where agents A and

7.2 Multi-Word Naming Game
123
(a)
(b)
Fig. 7.5 An example illustrating the probability of consensus within one communication in: a the
minimal naming game, and b MWNG. The memory length is shown at the lower-right corner of
each agent’s memory box
B have two words (‘girl’ and ‘apple’) in common (IAB = 2), thus the probability
of reaching consensus in one communication is PNG = IAB
L A = 0.4. In Fig.7.5b, two
agents have one pattern (P1) in common (I P
AB = 1). There are two word categories
in this pattern, namely R = {S, V }, R ⊂W = {S, V, O, C}. Both agents have one
common word in subject and verb, respectively. Thus, I S
AB = 1 and I V
AB = 1, so
ρ = 
j∈R
I
W j
AB
L
W j
A
= I S
AB
L S
A × I V
AB
LV
A = 1
9, thus PMW NG = ρ I P
AB
L P
A =
1
18.
The probability of one-iteration consensus in MWNG equals the probability of
reaching consensus of a pattern multiplies all the probabilities of reaching consensus
of the related words. The probability of reaching consensus in one communication in
MWNG is lower than that in the minimal naming game, but not preventing consensus
from happening.
7.3
Simulation Results
7.3.1
Simulation Setup
Numerical simulations are performed on three typical network topologies, namely,
random-graph (RG) [20], small-world (SW) [21] and scale-free (SF) [22] networks.
The performances of emergence, propagation and consensus of sentences and their
patterns are examined. Simulation results reﬂecting different aspects of MWNG are
collected with comparisons.
In the simulations, agents are initialized with empty memories, but their capaci-
ties of memory are inﬁnite. A total of 5 conventional English language patterns are
used, as shown in Fig.7.3, to form various sentences. A total of 12 different under-
lying networks are employed, each with 500 nodes (agents). The settings and basic
properties of the networks are summarized in Table7.1. The scaling property on the

124
7
Multi-Word Naming Game
Table 7.1 Network settings in the simulation study of MWNG. A total of 12 networks including
RG, SW, and SF are employed. The network size is set to 500. The networks are randomly generated
and basic properties including average node degree (⟨k⟩), average path length (⟨pl⟩), and average
clustering coefﬁcient (⟨cc⟩) are obtained by averaging over 30 independent runs. (Data taken from
[19])
Notation
Network type
N
⟨k⟩
⟨pl⟩
⟨cc⟩
RG/0.03
Random-graph network with
P = 0.03
500
14.98
2.5956
0.0302
RG/0.05
Random-graph network with
P = 0.05
500
24.97
2.2228
0.05
RG/0.1
Random-graph network with P = 0.1
500
49.98
1.9057
0.1002
SW/50/0.1
Small-world network with K = 50
and RP = 0.1
500
100
1.8049
0.5676
SW/50/0.2
Small-world network with K = 50
and RP = 0.2
500
100
1.7997
0.4382
SW/50/0.3
Small-world network with K = 50
and RP = 0.3
500
100
1.7996
0.3457
SW/60/0.1
Small-world network with K = 60
and RP = 0.1
500
120
1.7599
0.5725
SW/60/0.2
Small-world network with K = 60
and RP = 0.2
500
120
1.7595
0.4521
SW/60/0.3
Small-world network with K = 60
and RP = 0.3
500
120
1.7595
0.3672
SF/25
Scale-free with 26 initial nodes and 25
new edges added at each step
500
48.64
1.9272
0.1972
SF/50
Scale-free with 51 initial nodes and 50
new edges added at each step
500
94.81
1.8102
0.3088
SF/75
Scale-free with 76 initial nodes and 75
new edges added at each step
500
138.47
1.7228
0.3983
population size is investigated in [18], where the population size is set to 1000, 1200
and 2000, respectively, which conﬁrms that the ﬁndings shown in this chapter are
consistent with various population sizes.
7.3.2
Conventional English Language Patterns
The simulation results of MWNG are plotted in Figs.7.6, 7.7, 7.8, and 7.9. For easy
and direct comparison of MWNG with different underlying networks, all the subplots
in these ﬁgures are drawn in the same coordinates.
Figure7.6 shows the convergence process with the number of total words remem-
bered by the entire population. It can be seen that the convergence curve has a
ﬁrst-increase-then-decrease shape. For each single category, the convergence curve

7.3 Simulation Results
125
iteration
(a)
103
104
105
number of total words
0
5000
10000
15000
Subject
Verb
Complement
Object
Success Rate
iteration
(b)
103
104
105
0
5000
10000
15000
iteration
(c)
103
104
105
0
5000
10000
15000
iteration
(d)
103
104
105
0
5000
10000
15000
iteration
(e)
103
104
105
0
5000
10000
15000
iteration
(f)
103
104
105
0
5000
10000
15000
iteration
(g)
103
104
105
0
5000
10000
15000
iteration
(h)
103
104
105
0
5000
10000
15000
iteration
(i)
103
104
105
0
5000
10000
15000
iteration
(j)
103
104
105
0
5000
10000
15000
iteration
(k)
103
104
105
0
5000
10000
15000
iteration
(l)
103
104
105
0
5000
10000
15000
1.0
0.5
0.0
1.0
0.5
0.0
1.0
0.5
0.0
1.0
0.5
0.0
1.0
0.5
0.0
1.0
0.5
0.0
1.0
0.5
0.0
1.0
0.5
0.0
1.0
0.5
0.0
1.0
0.5
0.0
1.0
0.5
0.0
1.0
0.5
0.0
Fig. 7.6 Convergence curves in terms of the number of total words vs. iterations, accompanied
with the success rate curve used as reference: a RG/0.03; b RG/0.05; c RG/0.1; d SW/50/0.1; e
SW/50/0.2; f SW/50/0.3; g SW/60/0.1; h SW/60/0.2; i SW/60/0.3; j SF/25; k SF/50; l SF/75. In
each subplot, the converging process is plotted as 4 curves, representing the 4 categories of words,
respectively. The evolving process of the success rate is included as background for reference. The
comparison between the success rate curves is shown in Fig.7.9. In the tested conventional English
sentence (sub-)pattern, ‘subject’ and ‘verb’ always appear together, and their convergence curves
are consistent with each other. For each of the 4 types of underlying networks (on each row), the
parameters of (re-)connection probability as well as the average node degrees increase from left to
right, and the peaks of the curves become higher from left to right. The convergence time becomes
longer as the network parameter values increase. The numbers of total words for ‘complement’
and/or ‘object’ are zero when the population ﬁnally reaches a consent rule without these types of
words, while the numbers of both ‘subject’ and ‘verb’ eventually reach the population size, 500
is similar to that of SWNG, but with slight oscillations between the saturation phase
and the convergence phase. The peak of a curve of total words refers to as the maxi-
mum number of total words. Analytical studies on the minimal naming game in [2]

126
7
Multi-Word Naming Game
iteration
(a)
103
104
105
number of different words
0
100
200
300
Subject
Verb
Complement
Object
iteration
(b)
103
104
105
0
100
200
300
iteration
(c)
103
104
105
0
100
200
300
iteration
(d)
103
104
105
0
100
200
300
iteration
(e)
103
104
105
0
100
200
300
iteration
(f)
103
104
105
0
100
200
300
iteration
(g)
103
104
105
0
100
200
300
iteration
(h)
103
104
105
0
100
200
300
iteration
(i)
103
104
105
0
100
200
300
iteration
(j)
103
104
105
0
100
200
300
iteration
(k)
103
104
105
0
100
200
300
iteration
(l)
103
104
105
0
100
200
300
Fig. 7.7 Convergence curves of the number of different words vs. iterations: a RG/0.03; b RG/0.05;
c RG/0.1;d SW/50/0.1;e SW/50/0.2;f SW/50/0.3;g SW/60/0.1;h SW/60/0.2;i SW/60/0.3;jSF/25;
k SF/50; l SF/75. Differing from the curves of the number of total words shown in Fig.7.6, when the
network parameters are varied, the shapes of the curves are nearly unchanged but slightly shifted.
Moreover, since all the curves are plotted in identical coordinates, they can be compared vertically.
There is not much difference in the curves between different network types
show that, for RG and SW networks, the maximum number of total names N max
name
is given by N max
name = N
2 ×

1 + ⟨k⟩
2

, where N is the population size and ⟨k⟩is the
average degree of the underlying communication network. For example, for RG/0.03
(N = 500, ⟨k⟩= 14.98, as shown in Table7.1), the maximum number of total words
can be estimated as N max
name = 2123. Comparing with the simulation results shown in
Fig.7.6a, it is obvious that the peak of each word category is higher than 2000. The
maximum numbers of subject and verb are more than 5000. Therefore, the maxi-
mum number of total words (the summary of four categories) in MWNG is much

7.3 Simulation Results
127
iteration
(a)
103
104
105
number of total patterns
500
1000
1500
2000
2500
RG/0.03
RG/0.05
RG/0.1
iteration
(b)
103
104
105
number of total patterns
500
1000
1500
2000
2500
SW/50/0.1
SW/50/0.2
SW/50/0.3
iteration
(c)
103
104
105
number of total patterns
500
1000
1500
2000
2500
SW/60/0.1
SW/60/0.2
SW/60/0.3
iteration
(d)
103
104
105
number of total patterns
500
1000
1500
2000
2500
SF/25
SF/50
SF/75
Fig. 7.8 Convergence curves of the number of total patterns vs. iterations: a three RG networks; b
three SW networks with K = 50; c three SW networks with K = 60; d three SF networks. When
the (re-)connection probability is small, the peak of a curve is lower and the convergence takes place
earlier. This implies that a better connectivity leads to a larger number of total rules to propagate in
the population. The peaks of all curves are approximately near to or higher than 2000, but obviously
lower than 2500 (as can be further seen from Fig.7.10b, which is about 2400). This means that there
is a long time period where the agents have learned more than 4 patterns on average. Many agents
have learned all 5 patterns
more than that in the minimal naming game. The estimation of the maximum number
of total names in an SF network is presented in [2]. Notably, the sentence patterns
and multiple words here make the communications more complicated and require
the agents to store many more names throughout the process. During the saturation-
convergence transition phase, local consensus suffers more from disturbing when
the atomic names are divided into several categories. This complication produces
the oscillatory behavior, making local consensus as well as global consensus difﬁ-
cult to take place.
The success rate curve for each network is presented in a subplot of Fig.7.6. All
the 12 subplots give similar correlations between the success rate curve and the curve
of the number of total words. It can be observed that, when the number of iterations
is between 104 and 105, the success rate increases only slightly, but the curves of
total words pass the peaks and then begin to decrease drastically. This is because,
on the average, each agent has accumulated many patterns and names in memory,
and local consensus may require to clear up many (tens of) names therein. When

128
7
Multi-Word Naming Game
iteration
(a)
103
104
105
success rate
0
0.2
0.4
0.6
0.8
1
RG/0.03
RG/0.05
RG/0.1
iteration
   (b)
103
104
105
success rate
0
0.2
0.4
0.6
0.8
1
SW/50/0.1
SW/50/0.2
SW/50/0.3
iteration
(c)
103
104
105
success rate
0
0.2
0.4
0.6
0.8
1
SW/60/0.1
SW/60/0.2
SW/60/0.3
iteration
(d)
103
104
105
success rate
0
0.2
0.4
0.6
0.8
1
SF/25
SF/50
SF/75
Fig. 7.9 Curves of the success rate: a three RG networks; b three SW networks with K = 50;
c three SW networks with K = 60; d three SF networks. The success rate curves of MWNG are
simple as compared with the oscillatory success rate curves of SW networks in SWNG [12]. Before
global consensus takes place, the success rate stays below 0.2; then, in the convergence phase, the
success rate increases dramatically to reach 1.0. All these 12 success rate curves are plotted in
Fig.7.6 for reference
the tendency of global consensus is prominent, the success rate curve raises and the
number of total names drops, both drastically but smoothly.
Figure7.7 shows the convergence curves of the number of different words, where
all 12 subplots have very similar shapes to each other. The peaks of subject and verb
are both approximately equal to 250, the peak of complement is approximately equal
to 100, and the peak of object is approximately equal to 200. These match well the
situations shown in Fig.7.3, namely, subject and verb always appear together in each
of the 5 patterns, and complement appears twice in all 5 patterns. Although object
appears in 3 patterns, it appears 4 times over all 5 patterns. Therefore, the maximum
number of different complement words is 40% of the maximum number of different
subject (or verb) words, whereas for the maximum number of different object words,
it is 80% of the maximum number of different subject (or verb) words.
Figure7.8 shows the curves of the number of total patterns remembered by the
population. During a time period, the agents have learned more than 4 patterns on
average, which suggests I P
AB
L P
A > 0.8 in Eq. (7.1).

7.3 Simulation Results
129
(a)
(c)
(b)
(d)
Fig. 7.10 Study on the connection probability, varying from 0.02 to 1.0 (incremental step size
0.02), in RG with 500 nodes. The 4 subplots show: a convergence time; b maximum number of total
patterns; c maximum number of total words; d maximum number of different words, varying against
the changes of the connection probability. All the curves are averaged over 30 independent runs.
Whenthe connectionprobabilityis between0.02and nearly0.4, the convergence time andmaximum
numbers of total patterns and total words all increase as the connection probability increases. When
the connection probability is greater than 0.4, these three indexes become plateaued. However, the
maximum number of different works is not affected by the change of the connection probability
Figure7.9 shows the success rate curves. The success rate is calculated by the
number of local consensus in the most-recently 10 time steps, divided by 10. Since
the deﬁnition of local consensus avoids any local partial-consensus situation, the
consensus process is drastic rather than gradual.
Figure7.10 shows boxplots of 4 indexes, namely, the convergence time, maximum
number of total patterns, maximum number of total words, and maximum number
of different words, against the changes of the connection probability. The study is
implemented in a 500-node RG network. In each boxplot, the central box denotes
that the central 50% data lies within this section; the bar inside the central box is the
median value of all 30 independent data; the upper and lower bars are the greatest
and the least values, excluding outliers, which are indicated by pluses. Figure7.10a
shows that, in MWNG, the convergence time is non-decreasing as the connection
probability (and the average node degree) increases. This behavior is quite differ-

130
7
Multi-Word Naming Game
ent from the atomic naming game, as reported in [2], where the convergence time
decreases monotonously as the average node degree increases. A greater value of the
average degree actually introduces more information input to every agent, on aver-
age, i.e. more different words come from different neighbors. As a result, the agents
in a better connected network will accumulate more names than those in a poorly-
connected network in the early stage. Ultimately, the accumulated information will
facilitate the global convergence in the convergence stage.
Therefore, in a SWNG model, the number of accumulated names in the agents’
memories will directly inﬂuence the probability of reaching local consensus as well
as global consensus. This is because, ﬁrst, the number of different words is limited
and not affected by the average node degree. Only when an agent has nothing in
his memory will he invent a name. Figure7.10d veriﬁes this observation, albeit
empirically. Second, the more names the agents have accumulated, the more common
names they would have, so that a higher probability of achieving local consensus will
be gained. As a result, the convergence time decreases monotonously as the average
node degree increases.
However, in MWNG, accumulating more names will not directly inﬂuence the
probability of reaching local consensus. Its impact on the probability of reaching
local consensus is even lower as compared to SWNG due to the possible conﬂict in
pairing words from different categories, a phenomenon not existing in SWNG. As
can be seen from the simulation results to be reported in the next subsection, the more
components a pattern has, the more difﬁcult the consensus on this pattern will be.
As a result, in general, a good connection in the underlying communication network
may not facilitate the convergence in MWNG.
The above discussion on the network connectivity is about RG (or homogeneous)
networks, where the average degree would affect the accumulation of different words
in the agents’ memories. Note that, in a heterogeneous network, variation of the
average node degree would affect the receiving and accumulating of different words
in the agents’ memories, and thus would lead to different results in general.
7.3.3
Man-Designed Language Patterns
In simulations, 5 sets of man-designed language patterns are considered, with each
set includes 3–6 patterns respectively. The above-studied 5 conventional English
language patterns are natural and useful in real-life communications, but not effec-
tive in the experimental studies. For instance, the categories of subject and verb
always appear together in the beginning of all patterns, and also in the same ordering
(appeared as ‘subject + verb’). Experimentally, in all the above simulation results,
the population of agents always converges to the simplest ‘subject + verb’ (P1)
pattern. Pattern P1 is a subset of each of the other 4 patterns. To ﬁlter out the inef-
fectiveness of such conventional patterns, man-designed patterns are intentionally
designed to better observe the convergence of MWNG.

7.3 Simulation Results
131
In the literature, as said by William Shakespeare, ‘brevity is the soul of wit’.
Correspondingly in scientiﬁc research, as reported in [23], “papers with shorter
titles receive more citations per paper”. To some extent, (recognition of) ‘wit’ and
‘citation’ can also be regarded as a kind of one-sided consensus. Neither (recognition
of) ‘wit’ nor ‘citation’ is the result of mutual interactions in a naming game, but the
one who recognized or cited could be regarded as a hearer who consents to the
message (here, Shakespeare’s saying, or the observation reported in [23]).
Differing from the literature and also the citations in scientiﬁc research, for which
the reasons for popularity of short expressions are still unclear [23], the reason for
MWNG to converge to shorter sentence patterns is obvious, and indeed quite simple:
a shorter pattern has a higher probability to reach consensus than a longer pattern.
As an example, consider the situation where, in a time step, both speaker and hearer
store some identical Ns ‘subjects’, Nv ‘verbs’, No ‘objects’, and Nc ‘complements’
in their memories, and both have learned all 5 patterns (P1–P5). Let the speaker
and the hearer have the same memory size. Then, if the speaker utters a sentence
following pattern P1, the probability of achieving local consensus in this iteration
is
1
Ns×Nv . But, the probability of the speaker uttering a P5 sentence is
1
Ns×Nv×No×Nc ,
where obviously
1
Ns×Nv >
1
Ns×Nv×No×Nc . In addition, for instance, when an agent
has learned a sentence ‘boys play football’, he can compose or consent to a shorter
sentence like ‘boys play’, but not vice versa. As a result, a shorter pattern has a greater
probability to be learned and consented, thus eventually survives.
It should be noted that the above explanation about MWNG may not be able to
directly answer other questions, like why papers with shorter titles received more
citations in general. This is because, in the naming game simulations, a sentence is
mechanically generated as a combination of words, such that its (implicit) meaning
is not as clear and precise as a paper title. Furthermore, ‘citation’ in research is very
different to infer from ‘consensus’ in naming game. In naming game, the population
tries to name an object, uniformly randomly, but the paper titles bare signiﬁcant
information of research ideas and contents. Nevertheless, the study of MWNG sheds
some lights on the common phenomenon that, if the amount of information in a
sentence is uniform to describe an object, agents would tend to accepting a shorter
sentence.
The results of a statistical study on the eventually converged man-designed pat-
terns are shown in Table7.2. The 5 man-designed pattern sets are denoted as (a), (b),
(c), (d) and (e) in Fig.7.11. In each pattern set, several test patterns (TP) are deﬁned,
where each pattern composes of several test categories (Tc). The modiﬁer test (T)
is used to distinguish the conventional English language patterns (P1∼P5). The test
pattern sets are man-designed, used for testing the eventually converged pattern dis-
tributions. Sets (a) and (b) are uniformly distributed. The difference between (c) and
(d) is that, in (d), TP1 is a component (‘Tc1+Tc2’) of TP4, which does not exist
in (c). In (e), the longest pattern shares no common parts with the other (shorter)
patterns.
Simulations on MWNG with 500 and 1000 agents, respectively, are implemented
and the results are summarized in Table7.2. It can be observed that (1) from (a) and

132
7
Multi-Word Naming Game
Table 7.2 The number of eventually converged patterns in 5 test sets (these man-designed pattern
sets are deﬁned in Fig.7.11). There are 12 networks simulated over 30 independent runs, thus there
are 360 trials in total. Each integer represents a number of trials which led the population to converge
to that pattern, with its proportion indicated in the parentheses. (Data taken from [19])
Number
of nodes
Test
pattern
TP1
TP2
TP3
TP4
TP5
TP6
500
A
183(0.51)
177(0.49)
/
/
/
/
B
63(0.17)
69(0.19)
49(0.14)
61(0.17)
65(0.18)
53(0.15)
C
119(0.33)
139(0.39)
102(0.28)
0(0.00)
/
/
D
125(0.35)
123(0.34)
112(0.31)
0(0.00)
/
/
E
174(0.48)
167(0.47)
19(0.05)
/
/
/
1000
A
171(0.48)
189(0.52)
/
/
/
/
B
61(0.17)
66(0.18)
64(0.18)
57(0.16)
53(0.15)
59(0.16)
C
106(0.29)
132(0.37)
122(0.34)
0(0.00)
/
/
D
117(0.33)
124(0.34)
119(0.33)
0(0.00)
/
/
E
156(0.43)
189(0.53)
15(0.04)
/
/
/
TP1: Tc1 + Tc2
TP2: Tc2 + Tc1 
(a)
TP1: Tc1 + Tc2 
TP2: Tc2 + Tc1 
TP3: Tc1 + Tc3 
TP4: Tc3 + Tc1
TP5: Tc2 + Tc3
TP6: Tc3 + Tc2
(b)
TP1: Tc1 + Tc2 
TP2: Tc3 + Tc2 
TP3: Tc1 + Tc3 
TP4: Tc2 + Tc3 + Tc1
(c)
TP1: Tc1 + Tc2 
TP2: Tc3 + Tc2 
TP3: Tc1 + Tc3 
TP4: Tc3 + Tc1 + Tc2
(d)
TP1: Tc1 + Tc2 
TP1: Tc2 + Tc1 
TP1: Tc3 + Tc4 + Tc5
(e)
Fig. 7.11 A total of 5 sets of man-designed patterns are deﬁned: a 2 uniform patterns, which are
mutually reverse to each other. b 6 uniform patterns with 3 word categories; (C) and (D) 2 sets of
patterns with different word categories and different lengths; (E) 3 patterns with 5 word categories.
Tc1–Tc5 refer to 5 man-designed testing word categories. Pattern sets (a) and (b) are uniformly
distributed. In c, none of the shorter patterns is a sub-sequence of the longer pattern. In d, TP1 is a
sub-sequence of TP4. The pattern set in e includes 3 patterns, where the longest pattern TP3 shares
no common word categories with the shorts patterns
(b), the eventually converged patterns are uniformly distributed, when the patterns
are of equal length and the categories are uniformly distributed; (2) from (c) and (d),
the longer pattern (TP4) has no chance to be converged to, if it shares some common
word categories with the other shorter patterns, even if the longer pattern has a sub-
sequence of the shorter patterns; (3) from (e), when a longer pattern (TP3) shares no
common word categories with the other shorter patterns, it will be converged to, but
with a very small probability.

7.4 Conclusion
133
7.4
Conclusion
In this chapter, the multi-word naming game (MWNG) is introduced and studied via
extensive and comparative computer simulations. MWNG is a new model simulat-
ing the situation where a population of agents tries to invent, propagate and learn a
sentence of several words with a language pattern to describe a single object (e.g.,
opinion or event). MWNG is studied on ﬁve conventional English language patterns
and ﬁve man-designed test patterns. The simulation results show that: (1) the new
sentence-sharing model is an extension of the classical lexicon-sharing model, in
which their processes and features are basically similar; (2) the propagating, learn-
ing and converging processes are more complicated than that in the conventional
naming games, since a larger memory size and a longer convergence time are needed
in MWNG; (3) the convergence time is generally non-decreasing as the network
becomes better connected, while a greater value of the average node degree reduces
the convergence time in the single-word naming games (SWNG); (4) the agents
tend to accepting short sentence patterns, consistent with many known linguistic
phenomena in the real world.
References
1. A. Baronchelli, M. Felici, V. Loreto, E. Caglioti, L. Steels, Sharp transition towards shared
vocabularies in multi-agent systems. J. Stat. Mech. Theory Exp. 6, P06014 (2006). https://doi.
org/10.1088/1742-5468/2006/06/P06014
2. W.X. Wang, B.Y. Lin, C.L. Tang, G.R. Chen, Agreement dynamics of ﬁnite-memory language
gamesonnetworks.Eur.Phys.J.B60(4),529–536(2007).https://doi.org/10.1140/epjb/e2008-
00013-5
3. Q.Lu,G.Korniss,B.K.Szymanski,Thenaminggameinsocialnetworks:communityformation
and consensus engineering. J. Econ. Interact. Coord. 4(2), 221–235 (2009). https://doi.org/10.
1007/s11403-009-0057-7
4. D. Centola, A. Baronchelli, The spontaneous emergence of conventions: an experimental study
of cultural evolution. Proc. Natl. Acad. Sci. USA 112(7), 1989–1994 (2015). https://doi.org/
10.1073/pnas.1418838112
5. A. Baronchelli, Role of feedback and broadcasting in the naming game. Phys. Rev. E 83,
046103 (2011). https://doi.org/10.1103/PhysRevE.83.046103
6. A. Baronchelli, L. Dall’Asta, A. Barrat, V. Loreto, The role of topology on the dynamics of the
naming game. Eur. Phys. J. Spec. Top. 143(1), 233–235 (2007). https://doi.org/10.1140/epjst/
e2007-00092-0
7. L. Dall’Asta, A. Baronchelli, A. Barrat, V. Loreto, Agreement dynamics on small-world net-
works. EPL (Europhys. Lett.) 73(6), 969 (2006). https://doi.org/10.1209/epl/i2005-10481-7
8. L.Dall’Asta,A.Baronchelli,A.Barrat,V.Loreto,Nonequilibriumdynamicsoflanguagegames
on complex networks. Phys. Rev. E 74(3), 036105 (2006). https://doi.org/10.1103/PhysRevE.
74.036105
9. Y. Gao, G.R. Chen, R.H.M. Chan, Naming game on networks: let everyone be both speaker
and hearer. Sci. Rep. 4, 6149 (2014). https://doi.org/10.1038/srep06149
10. B. Li, G.R. Chen, T.W.S. Chow, Naming game with multiple hearers. Commun. Nonlinear Sci.
Numer. Simul. 18, 1214–1228 (2013). https://doi.org/10.1016/j.cnsns.2012.09.022

134
7
Multi-Word Naming Game
11. R.R. Liu, C.X. Jia, H.X. Yang, B.H. Wang, Naming game on small-world networks with
geographical effects. Phys. A 388, 3615–3620 (2009). https://doi.org/10.1016/j.physa.2009.
05.007
12. Y. Lou, G.R. Chen, Analysis of the “naming game” with learning errors in communications.
Sci. Rep. 5, 12191 (2015). https://doi.org/10.1038/srep12191
13. Stadler, K., Wellens, P., De Beule, J.: The combinatorial naming game, vol. 3 (IFAAMAS,
2012), p. 9
14. F. Tria, B. Galantucci, V. Loreto, Naming a structured world: a cultural route to duality of
patterning. PLoS ONE 7, e37744 (2012). https://doi.org/10.1371/journal.pone.0037744
15. G. Fu, Y. Cai, W. Zhang, Analysis of naming game over networks in the presence of memory
loss. Phys. A 479, 350–361 (2017)
16. X. Niu, C. Doyle, G. Korniss, B.K. Szymanski, The impact of variable commitment in the
naming game on consensus formation. Sci. Rep. 7, 41750 (2017)
17. J. Beule, K. Stadler, An evolutionary cybernetics perspective on language and coordination.
New Ideas Psychol. 32, 118–130 (2014). https://doi.org/10.1016/j.newideapsych.2013.03.003
18. Lou, Y, Chen, G.R., Hu, J.W.: Supplementary information for paper “communicating with
sentences: a multi-word naming game model” (2015), http://www.ee.cityu.edu.hk/~gchen/
pdf/MWNG-SI.pdf
19. Y. Lou, G.R. Chen, J.W. Hu, Communicating with sentences: a multi-word naming game model.
Phys. A 490, 857–868 (2018). https://doi.org/10.1016/j.physa.2017.08.066
20. P. Erdös, A. Rényi, On the strength of connectedness of a random graph. Acta Math. Acad.
Sci. Hung. 12(1–2), 261–267 (1964)
21. D.J. Watts, S.H. Strogatz, Collective dynamics of ‘small-world’ networks. Nature 393(6684),
440–442 (1998). https://doi.org/10.1038/30918
22. A.L. Barabási, R. Albert, Emergence of scaling in random networks. Science 286(5439), 509–
512 (1999)
23. A. Letchford, H.S. Moat, T. Preis, The advantage of short paper titles. R. Soc. Open Sci. 2(8),
150266 (2015). https://doi.org/10.1098/rsos.150266

Chapter 8
Multi-Language Naming Game
8.1
Introduction
Many realistic scenarios have been considered in the studies of naming games, for
example, misunderstandings [1], limited memory sizes [2], and group discussions
[3, 4]. Recently, a scenario with multiple languages is considered in [5, 6], useful
for community detection. In such a scenario, agents speak different languages, and
the population would reach a metastable state when different-language agents reach
consensus in their own group, when the underlying network is a community-based
one [5–7].
In [8], agents are initially assigned to be able to speak different languages, and the
aimofsimulationsistoinventnewspeciesoflanguagefromtheexistinglanguages.In
such a model, agents speaking different languages are assumed to be able to commu-
nicate with each other directly without the need of a translator. However, in real life,
people actually cannot communicate directly when they speak different languages.
This happens not only in human communications, but also in human-machine and
machine-machine communications. For example, some Bluetooth devices cannot
connect with each other if they use different operation systems or different proto-
cols, where different protocols are regarded as different languages. In this case, a
translator is needed for human-human communications, and an adapter is needed
for machine-machine communications, so that words or information can be sent and
received to complete a communication.
In a system with multiple languages, regarded as a multi-language self-organized
system, some features of the agreement process, such as communication pattern,
convergence speed, memory size, memory cost, and the proportion of different-
language agents, are quit different from those in a single-language system. Thus,
speciﬁc modeling and analysis on a multi-language system are necessary.
In this chapter, the multi-language naming game (MLNG) model is introduced. In
MLNG, agents are divided into different language groups. Differing from the model
discussed in [8], two agents speak different languages in MLNG here cannot com-
municate to each other directly, and a bi-lingual friend between them is needed. The
© Springer Nature Switzerland AG 2019
G. Chen and Y. Lou, Naming Game, Emergence, Complexity and Computation 34,
https://doi.org/10.1007/978-3-030-05243-0_8
135

136
8
Multi-Language Naming Game
Fig. 8.1 Agents speaking different languages: EN denotes an English speaker, CH denotes a
Chinese speaker, and TR denotes a translator who can speak both English and Chinese languages.
Both EN and CH have their own vocabularies, while the translator TR has both English and Chinese
vocabularies with their mutual translations
MLNG model can be generalized to the many-language scenario straightforwardly.
In this chapter, for simplicity, only the bi-language scenario is considered.
Consider ﬁve representative network topologies, namely random-graph (RG),
Watts–Strogatz small-world (WS-SW), Newman–Watts small-world (NW-SW),
scale-free (SF), and random triangle model (RTM) topologies, which are used as the
underlying communication networks. Simulation and analysis results show that: (1)
taking the network features and the proportion of translators together into account, the
probability of successfully establishing a conversation between two or three agents
can be theoretically estimated; (2) the relationship between the convergence speed
and the proportion of translators has a power-law-like correlation; (3) assuming that
different agents require different memory sizes, a local memory allocation rule is
recommended for saving memory resources.
The rest of the chapter is organized as follows: In Sect.8.2, the MLNG model
is introduced, followed by extensive simulations with analysis and comparisons in
Sect.8.3. Section 8.4 concludes the chapter.
8.2
Multi-Language Naming Game
The MLNG model consists of two parts: The underlying communication network
and the game rules.
As usual, an agent is represented by a node in the underlying network. Different
languages are uniformly-randomly assigned to the agents in advance. Agents (nodes)
speaking different languages are uniformly-randomly distributed over the network.
At each time step, a randomly-picked speaker ﬁrst makes an attempt to utter a word
to one of his neighbors, referred to as the hearer, either in a pair-wise conversation or
a three-agent talk with the help of a translator. If the population is able to converge
to a global consensus state iteratively, then all agents keep one and only one same
word (in the same language) in their memories.
First, a few terminologies are introduced.
Agent assignment: Agents in the MLNG model are labeled as different language
speakers. An example is shown in Fig.8.1, where CH represents a Chinese speaker,
EN represents an English speaker, TR represents a translator, who speaks both two

8.2 Multi-Language Naming Game
137
languages. Note that when there are NL > 2 languages in the system, the role of
the translators should be more delicately designed, since a translator is one who
speaks nL (2 ≤nL ≤NL) languages. For simplicity, only two languages (English
and Chinese) are considered in this chapter.
Pair-communication: Two agents who can have direct conversation should have
at least one common language. As shown by the example in Fig.8.1, an EN and a
CH has no common language, a TR has one common language with an EN and a
CH, respectively. Pair-communication could be the direct conversation between an
EN and an EN, or between a CH and a CH. A translator TR, as the label shows, is
able to communicate with a TR, a CH, or an EN, all directly.
Note that if all agents share at least one common language, then the model degen-
erates to the minimal naming game. Thus, the MLNG model is a natural extension
of the minimal naming game model.
Triangular-communication: It is the conversation between two agents who speak
different single-languages with the help of a translator. One example is the three-
agent talk among EN, CH, and TR, where EN is the speaker and CH is the hearer
respectively, and between them there is the translator TR.
Communication-unestablished: Two agents speaking different single-languages
cannot establish conversation, if they fail to ﬁnd a translator from among their com-
mon neighbors.
Local failure: When the speaker-uttered word is sent out to a hearer, there are two
paths to go through: (1) directly from the speaker to the hearer, and (2) via a translator.
Therefore, local failure could be caused accordingly by two factors: (1) the hearer
does not have the same word in his memory, and (2) the translator has no such word
in his memory. In Fig.8.2a, b, d, the uttered word successfully reaches the hearer, but
the hearer refused to consent to it. This is similar to the local failure in the minimal
naming game. In Fig.8.2c, the local failure is caused by the translator instead. In this
example, although the CH hearer has the same word as the EN speaker-uttered one
(‘egg’), they cannot reach local consensus because the translator cannot translate
this name from one to another. When a local failure occurs, the transmitted word
will be added to the memory of the one who caused the failure. For example, if the
local failure is caused by the translator, then he would learn it and then keep it in his
memory. However, the hearer has no chance to hear this word during this processing
stage, thus he would not know or learn this word at the current time step.
Local success (or local consensus): If there is a same word in the hearer memory
as the one transmitted from the speaker or translated by the translator, a local success
occurs. A local success could happen in both pair-communication (Fig.8.2e, f) and
triangular-communication (Fig.8.2g). When a local success occurs, both the speaker
and the hearer will clear out their memories except keeping this same word. Note
that the translator is included in a local failure, where he would learn a new word, but
is excluded from the local consensus, because his job is to translate the word only,
rather than to judge the name.
Communication establishment of agents with different languages is summarized
in Table 8.1. Throughout this chapter, the MLNG is studied in the context of two
different language-speaking agents. Thus, only one type of translators, who can speak

138
8
Multi-Language Naming Game
(a)
(b)
(c)
(d)
(e)
(f)
(g)
Fig. 8.2 An example of local failure and local success: a local failure between two ENs; b local
failure between two CHs; c local failure in a triangular-communication, where TR cannot translate
the transmitted word; d local failure in a triangular-communication; e local success between two
ENs; f local success between two CHs; g local success in a triangular-communication

8.2 Multi-Language Naming Game
139
Table 8.1 Communication establishment of agents with different languages. PC stands for direct
pair-communication, and TC for triangular-communication
Agent
EN
CH
TR
EN
PC
TC
PC
CH
TC
PC
PC
TR
PC
PC
PC
both languages, is needed. In an L (> 2)-language scenario, a translator may speak 2
to L different languages. This complicated situation is not considered in the present
study.
The game rule of the bi-language MLNG model is described by the following
scheme:
(1) Generate a population of N agents connected in a certain topology (the under-
lying network). Each agent is initialized to have an empty memory, with inﬁnite
capacity to remember all words he learns.
(2) Uniformly-randomly assign each agent into different language categories,
namely, E N, C H, and T R, with a certain probability depending on the pro-
portion of the different languages in the population. Set up their corresponding
external vocabularies.
(3) At each time step, randomly select a speaker and then a hearer from among the
speaker’s neighbors:
(3.1) If the speaker and the hearer speak at least one common language (if
both are translators, then they have two common languages), then a pair-
communication is established and they can communicate with each other:
(3.1.1) Randomly pick a word from the speaker’s memory if his memory is not
empty; otherwise, randomly pick a word from his external vocabulary.
(3.1.2) If the hearer has the same word in his memory, it is a local success (see
Fig.8.2e, f), so both the speaker and the hearer clear out their memories
except keeping the consented word; otherwise, it is a local failure and
the hearer adds this new word to his memory (see Fig.8.2a, b).
(3.2) Otherwise, this pair of agents cannot communicate with each other directly.
They start to search for a translator from among their common neighbors. If
there is no translator there, the communication between this pair of agents
cannot be established; otherwise, if there is at least one translator among
their common neighbors, then randomly select one translator from them, so
that a triangular-communication is established. Then:
(3.2.1) Randomly pick a word from the speaker’s memory if his memory is not
empty; otherwise, randomly pick a word from his external vocabulary.
(3.2.2) If the translator does not have the speaker-uttered word in his memory,
it is a local failure (see Fig.8.2c), so the translator learns this word and
adds it to his memory. The hearer does nothing. But, if the translator
has this word in his memory, while the hearer does not, then it is also

140
8
Multi-Language Naming Game
a failure (see Fig.8.2d), so the hearer learns this word and adds it to
his memory. If this word exists in both memories of the translator and
the hearer, it is a success (see Fig.8.2g), so both the speaker and the
hearer clear out their memories except keeping the consented word.
The translator does nothing.
(4) The process continues until all the agents (E N, C H, and T R) keep one and
only one same word in their memories, or the predeﬁned maximum number of
iterations is reached.
The job of a translator in a triangular-communication is only to translate words
for the two agents who speak different languages. Therefore, when a local success
occurs in a triangular-communication, the translator does nothing. However, in the
situation of a local failure caused by the translator (see Fig.8.2c), the translator has
a chance to learn a new word, but the hearer does nothing in this case.
8.3
Simulation Results and Analysis
8.3.1
Simulation Settings
The MLNG is simulated on ﬁve different underlying network topologies, so that
different effects of various complex networks can be examined, such as homogeneity,
heterogeneity, SW and SF features, etc. The population size is set to 500 and 1000,
respectively, to verify their consistency and scaling property.
The underlying networks include RG networks (denoted as RG/P/N, where
P is the connecting probability and N is the population size) [9]. Both the WS-
SW network [10] and the NW-SW network [11] are employed as two SW-types of
underlying networks, denoted by W S/K/RP/N and NW/K/RP/N, respectively.
Here, K is the half number of connected neighbors of each node in the initial ring-
shaped network, and thus 2K represents the total number of neighbors initially; RP
is the rewiring probability in the NW-SW model, or the link-adding probability in
the WS-SW model. The SF networks [12] are denoted as SF/m0/m/N, where m0
is the initial number of nodes and m is the number of edges being added at each step
in the network generation, and N is the ﬁnal population size. The random triangle
model (RTM) [13] is also employed for simulation, which is generated by starting
from a fully-connected network with m0 nodes and then adding new nodes into the
network step by step: When a new node is added, it connects to the existing nodes in
the network to form t new triangles, where the way to form a triangle is to connect the
new node to one existing node and one neighbor of the picked node, both at random.
Here, an RTM is denoted as RT M/m0/t/N, where N is the ﬁnal population size.
Network parameter settings of MLNG are presented in Table8.2. For fair compar-
isons, parameters of different networks are delicately chosen, such that the average
degrees of different networks are the same (with negligible difference due to ran-
domness).

8.3 Simulation Results and Analysis
141
Table 8.2 Parameter settings and feature statistics of the employed underlying networks in the
MLNG simulations. (Data taken from [14])
Notation
Network type
N
⟨k⟩
⟨cc⟩
⟨apl⟩
RG/0.1/0.5K
Random-graph network with
P = 0.1
500
49.90
0.1001
1.9061
RG/0.1/1K
Random-graph network with
P = 0.1
1000
99.95
0.1001
1.9000
W S/25/0.2/0.5K
WS small-word network with
K = 25 and RP = 0.2
500
50.00
0.4024
2.0074
W S/50/0.2/1K
WS small-word network with
K = 50 and RP = 0.2
1000
100.00
0.4076
1.9149
NW/20/0.02/0.5K
NW small-word network with
K = 20 and RP = 0.02
500
49.23
0.5054
2.0376
NW/40/0.02/1K
NW small-word network with
K = 40 and RP = 0.02
1000
98.33
0.5113
1.9251
SF/25/25/0.5K
BA scale-free network with
m0 = 25 initial nodes and
m = 25 new edges added at
each step
500
48.64
0.1978
1.9268
SF/50/50/1K
BA scale-free network with
m0 = 50 initial nodes and
m = 50 new edges added at
each step
1000
97.39
0.1948
1.9044
RT M/50/13/0.5K
RTM with m0 = 50 initial
nodes and m = 13 new
triangles added at each step
500
49.14
0.1963
1.9433
RT M/50/27/1K
RTM with m0 = 50 initial
nodes and m = 27 new
triangles added at each step
1000
98.55
0.1588
1.9049
In a bi-language MLNG, agents are classiﬁed into three groups, including group
C H (Chinese speakers), group E N (English speakers) and group T R (translators
who can speak both Chinese and English languages). In order to make it simpler with
better observations on T R, same proportions are assigned to C H and E N. The three
groups, C H, E N, and T R, are uniformly randomly distributed over the underlying
network.
The size of the external vocabulary is set to 1 × 104 for each language, respec-
tively. The maximum number of iterations is set to 1 × 107, which is empirically
large enough for global convergence, assuming that the proportion of translators in
the entire population is not too small which is denoted by PT . To reduce the random
inﬂuences caused by the random distributions of the three language groups and the
randomness in the naming game, every simulation is averaged over 30 independent
runs.

142
8
Multi-Language Naming Game
iteration
102
103
104
105
number of total words
0
2000
4000
6000
8000
10000
(a) PT = 0.1
iteration
102
103
104
105
number of different words
0
100
200
300
400
500
600
(b) PT = 0.1
iteration
102
103
104
105
number of total words
0
2000
4000
6000
8000
10000
(c) PT = 0.5
iteration
102
103
104
105
number of different words
0
100
200
300
400
500
600
(d) PT = 0.5
Entire Population
TR
CH
EN
C
D
A
B
Fig. 8.3 The convergence processes in the RG network RG/0.1/1K: a the number of total words,
with PT = 0.1; b the number of different words, with PT = 0.1; c the number of total words, with
PT = 0.5; d the number of different words, with PT = 0.5. (Data taken from [14])
8.3.2
Convergence Process and Analysis
The convergence process of MLNG is studied by observing the evolutions of the
numbers of total and different words, both global-wise and group-wise. Global-wise
behavior refers to the entire population, while group-wise to each language group
(i.e., C H, E N, and T R).
Theproportionoftranslators, PT ,issetto0.1and0.5,respectively.Here, PT = 0.1
represents a relatively low proportion of translators in the population, while PT = 0.5
is a medium-size portion. Note that, when PT = 1.0, all agents can communicate
directly and thus the naming game in the bi-language system is vanished. In this
case, the model is degenerated to the minimal naming game.
It turns out that the convergence processes of all the 10 networks, shown in
Table8.2), are quite similar. Thus, for clarity, only the RG network with 1000 agents
is considered here as an example for analysis and discussion. Results on the other
networks are shown in [15] (see Figs. S1–S9 in [15]).

8.3 Simulation Results and Analysis
143
The simulation results are shown in Fig.8.3. For both the number of total words
and the number of different words, every curve ﬁrst increases and then decreases. The
increase in a curve refers to a period of exploring, when agents explore the external
lexicon randomly for new words to name the object. Agents get a large number of
words from the external vocabulary, and put them into their memories. Similarly,
the decrease of a curve refers to a converging process, when agents move towards
consensus by deleting unmatched words from their memories.
Although the rough shapes of curves shown in Fig.8.3 are similar, there are some
subtle differences. The different-word curves reach peaks ahead of the total-word
curves. This is because the agents would invent new words (picked from external
vocabularies) only if their memories are empty. When an agent has at least one word
in his memory, the number of different words would not increase any further. But,
meanwhile, a local failure is inevitable, which increases of the number of total words,
keeping the number of different words unchanged. The peak of a different-word
curve is less than half of the population size. Before the peak of the curve is reached,
the agents mainly explore and learn, but after reaching the peak (approximately less
than N/2), local consensus becomes dominant and thus the curve goes down towards
global convergence.
Although the translators occupy only 10% of the population, they remember a
large number of different words, which is greater than that remembered in C H and
E N during a long time period in the process, as shown in Fig.8.3b. The number
of total words remembered by the translators is not less than that of the other two
language groups. Translators learn two languages and can communicate with any
neighboring agent, meaning that they have better exploring and learning abilities.
Moreover, agents in groups E N and C H can learn fast if the involved translators have
a large number of words in their memories (during the period between positions D
and B in Fig.8.3a, b). Capable translators are helpful for successful communications
between agents of different languages, while incapable translators would also learn.
Here, a capable translator means one that has the speaker-uttered word, so he can help
translate and send it to a hearer. More words remembered by the translators means
that they are more capable to offer translation service. The group of translators starts
to converge slightly earlier than the entire population, while the single language
groups (E N and C H) start to converge slightly later than the entire population, as
expected.
When the proportion of translators is set to 50%, the convergence processes are
shown in Fig.8.3c, d, which appear to be quite similar to the situation when the
proportion of translators is only 10%.
Figure 8.4 visualizes a single run of simulation on W S/50/0.2/1K with PT =
0.5 (the proportion of translators is set to 50%). As shown in Fig.8.4a, the central
agents are marked with lighter blue, meaning that they have learned more words
than the small-degree agents who are plotted at the rim. Then, the entire population
becomes lighter blue, meaning that roughly all the agents have dozens of words
in their memories (as shown in Fig.8.4b–d). Later, in Fig.8.4e, those small-degree
agents at the rim become deeper blue than the central agents, meaning that small-
degree agents reach convergence earlier than the large-degree agents. Finally, the

144
8
Multi-Language Naming Game
0
5
10
15
20
25
30
35
40
45
circle #- CH speaker
star #- EN speaker
triangle #- TR translator
(c)
(a)
(d)
(f)
(e)
(b)
Fig. 8.4 Visualization of the MLNG performed on SF/50/50/1K with PT = 0.5. Agents are put
in a circle, where central agents have larger degrees. Different colors indicate different numbers
of words remembered by an agent. a Iteration = 1000, the average number of remembered words
is 1.41; b Iteration = 10000, the average number of remembered words is 6.473; c Iteration =
20000, the average number of remembered words is 7.05; d Iteration = 30000, the average number
of remembered words is 6.457; e Iteration = 40000, the average number of remembered words is
2.925; f Iteration = 50000, the average number of remembered words is 1.116. (Data taken from
[14])

8.3 Simulation Results and Analysis
145
entire population becomes deep blue, meaning that all agents have very few words
in their memories.
8.3.3
Communication Ratio
There are two communication modes in the MLNG model: the pair-communication
(PC) and the triangular-communication (TC). Three indexes are now introduced to
measure the percentage of each communication mode in the naming game process.
The ratio of pair-communication, denoted by Ppc, is deﬁned as the proportion of
pair-communications over all the communications throughout the entire process.
Similarly, the ratio of triangular-communication, denoted by Ptc, is deﬁned as the
proportion of triangular-communications over all communications throughout the
entire process. Finally, Pcu is deﬁned to measure the percentage of communication-
unestablished interactions over all attempted communications through the entire
process.
The above three indexes are studied as the proportion of translators, PT , is var-
ied. Here, PT is the only parameter that inﬂuences the communication ratio of pair-
communication, while Ppc represents the probability that two randomly-picked adja-
cent agents can communicate directly. Here, Ppc can be calculated as follows:
Ppc = 1 −(1 −PT )2 + 2 ×
1 −PT
2
2
= 1 −(1 −PT )2
2
(8.1)
Moreover, Ptc is determined by three factors: (1) the proportion of translators, PT , in
the population; (2) the average degree ⟨k⟩of the underlying subnetwork; and (3) the
average clustering coefﬁcient ⟨cc⟩of the underlying subnetwork. Given these three
values, Ptc can be estimated as follows:
(1) Calculate the percentage of not running pair-communication, by (1−PT )2
2
.
(2) Randomly pick a speaker and ﬁnd all his neighbors to form a star-shaped sub-
network, excluding the speaker herself. The connecting probability ps of the
subnetwork can be estimated as follows: among all the neighbors, the number
of edges is about
⟨k⟩
2

× ⟨cc⟩, so ⟨cc⟩can be regarded as a good approximation
of ps.
(3) Randomly pick a hearer from the above subnetwork, and then calculate Ptc. To
ﬁnd a common translator, since the subnetwork is also the speaker’s neighbors,
one only needs to consider the neighborhood of the hearer. As for the hearer, there
are approximately ⟨cc⟩× (⟨k⟩−1) neighbors, among which the probability of
having at least one translator is
PET ≈1 −(1 −PT )⟨cc⟩×(⟨k⟩−1)
(8.2)

146
8
Multi-Language Naming Game
proportion of translators (PT)
communication ratio
0.4
0.6
0.8
1
(a) pair-communication
proportion of translators (PT)
communication ratio
0
0.1
0.2
0.3
0.4
(b) tri-communication
RG/0.1/0.5K
WS/25/0.2/0.5K
NW/20/0.02/0.5K
SF/25/25/0.5K
RTM/50/13/0.5K
proportion of translators (PT)
communication ratio
0
0.1
0.2
0.3
0.4
(c) communication-unestablished
proportion of translators (PT)
communication ratio
0.5
0.6
0.7
0.8
0.9
(d) pair-communication
proportion of translators (PT)
communication ratio
0
0.1
0.2
0.3
0.4
(e) tri-communication
proportion of translators (PT)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
communication ratio
0
0.1
0.2
0.3
0.4
(f) communication-unestablished
RG/0.1/1K
WS/50/0.2/1K
NW/40/0.02/1K
SF/50/50/1K
RTM/50/27/1K
Fig. 8.5 The values of communication ratios on 10 different underlying networks: a and d
communication ratios for pair-communication; b and e communication ratios for triangular-
communication; c and f communication ratios for communication-unestablished.
(Data taken
from [14])
Then, denote Ct = ⟨cc⟩× (⟨k⟩−1) and call it as the triangular-communication
coefﬁcient. Thus, Ptc can be well estimated by
Ptc = (1 −Ppc) × PET ≈(1 −PT )2
2
× (1 −(1 −PT )Ct)
(8.3)
After Ppc and Ptc are obtained, Pcu can be easily obtained from
Pcu = 1 −Ppc −Ptc ≈(1 −PT )2+Ct
2
(8.4)
Figure 8.5 shows the communication ratios tested on 10 different underlying net-
works. Figure8.6 compares the calculated (predicted) communication ratios and the
measured communication ratios. As shown in the ﬁgure, the calculated (predicted)

8.3 Simulation Results and Analysis
147
curves are very close to the measured ones on all 10 underlying networks. Therefore,
it conﬁrms the correctness of Eqs.(8.1), (8.3), and (8.4), for estimating the values of
Ppc, Ptc, and Pcu. It is valid not only for RG networks, but also for the other 4 topolo-
gies, namely SW, SF, and RTM networks. According to Eq.(8.4), the average degree
and the average clustering coefﬁcient are the two major factors that signiﬁcantly
affect the communication ratios.
As for triangular-communication, when the proportion of translators, PT , is very
small, it is more likely to encounter the communication-unestablished situation,
rather than to build a triangular-communication relationship. Since the probability of
ﬁnding a common translator is low, the percentage of communication-unestablished
situations becomes high. When PT is increased, accordingly the probability of con-
structing triangular-communications becomes higher. Meanwhile, when PT takes a
relatively large value, pair-communications can be easily established from T R to
T R, and also from T R to any single language agent. Thus, the communications
between two different single-language agents becomes less and less; consequently,
the proportion of triangular-communications is reduced.
8.3.4
Convergence Speed
Consider the relationship between the convergence speed and the proportion of trans-
lators, PT , where the convergence time reefers to the number of iterations from ini-
tialization to global consensus, since in each iteration of MLNG there is only one
operation.
From the simulation results, it can be observed that the convergence speed is
sensitive to the change of PT when PT is small, but not sensitive when PT is large.
If PT is set to be too small, the population cannot reach global consensus. For this
reason, PT is set to an empirical threshold, such that the population can always reach
global consensus in a reasonable number of steps, say 30. The minimal value of PT
is set to this empirical threshold, and is varied within several other small intervals.
Both the starting values and the resulting values of PT are given in Table8.3. The
situation with PT ≥0.3 is considered, since small variations of this value basically
do not affect the process.
SimulationresultsareshowninFig.8.7.Obviously,theconvergencetimedecreases
as PT increases. The existence of translators facilitates the convergence speed. Recall
that, if all the agents share one common language, it becomes the minimal naming
game. On the other hand, if the entire population consists of only translators, thus
no translation work is needed, it also becomes the minimal naming game.
Now, let Tc denote the convergence time, i.e., the number of iterations from ini-
tialization to global consensus. The relationship between log(Tc) and log(PT ) is
approximately linear. Thus, the relationship between the convergence time and PT
displays a power-law-like curve:
Tc ≈a × P−γ
T
(8.5)

148
8
Multi-Language Naming Game
Fig. 8.6 Comparison of calculated communication ratios and measured communication ratios
on 10 underlying networks: a RG/0.1/500, b RG/0.1/1000, c WS/25/0.2/500, d WS/25/0.2/1000,
e NW/2/0.02/500, f NW/2/0.02/1000, g SF/25/25/500, h SF/50/50/1000, i RTM/50/13/500, j
RTM/50/27/1000. (Data taken from [14])

8.3 Simulation Results and Analysis
149
Table 8.3 The settings for the convergence speed study. (Data taken from [14])
Network
Pre-set proportion of
translators
Resulting proportion of
translators
RG/0.1/0.5K
Setting 1
Unchanged
RG/0.1/1K
Setting 1
Unchanged
WS/25/0.2/0.5K
Setting 2
Unchanged
WS/50/0.2/1K
Setting 2
Unchanged
NW/20/0.02/0.5K
Setting 2
0.0151 ≤PT < 0.3: 33
intervals
NW/40/0.02/1K
Setting 2
Unchanged
SF/25/25/0.5K
Setting 2
0.0331 ≤PT < 0.3: 26
intervals
SF/50/50/1K
Setting 2
0.0280 ≤PT < 0.3: 28
intervals
RTM/50/13/0.5K
Setting 2
0.0383 ≤PT < 0.3: 24
intervals
RTM/50/27/1K
Setting 2
0.0151 ≤PT < 0.3: 33
intervals
Setting 1: 0.05 ≤PT < 0.3: 35 intervals and 0.3 ≤PT ≤1.0: 75 intervals
Setting 2: 0.01 ≤PT < 0.3: 35 intervals and 0.3 ≤PT ≤1.0: 75 intervals
Fig. 8.7 Relationships between the convergence time and the proportion of translators (PT ) (in
log–log form): a N = 500; b N = 1000. (Data taken from [14])
where a is a coefﬁcient determined by the convergence time of a corresponding
minimal naming game model.
For the same network topology, small-size population converges faster than a
large-sized one. But, for different topologies, this may not be true. There are more
factors affecting the convergence speed, especially the degree distribution of the
underlying network. As can be seen from Fig.8.7, when the underlying network is

150
8
Multi-Language Naming Game
Fig. 8.8 Relationships
between γ and the average
clustering coefﬁcient ⟨cc⟩.
The legends indicate the
topologies of the original
networks used to generate
the dk-graphs. (Data taken
from [14])
RG (red squares), SF (black pluses), and RTM (light blue circles), the convergence
speeds perform similarly. These three curves have similarly small gradients, meaning
that they have a small degree exponent γ in Eq.(8.5). As for SW networks (green
triangles and blue stars), their curve gradients are relatively large, with a large γ in
Eq.(8.5). Brieﬂy, a greater clustering coefﬁcient value leads to a larger γ in Eq.(8.5),
as can be revealed by considering Table8.2 and Fig.8.7 together.
Next, consider the relationship between γ and ⟨cc⟩, on several typical networks
with the same average degree but different ⟨cc⟩values. These networks, denoted
as dk-graphs, are generated using the dk-targeting rewiring algorithm with d = 2.1
[13] on all the network models of N = 500 nodes shown in Table8.3. The detailed
generating method for dk-graphs can be found in [15].
Figure 8.8 shows the relationship between γ and ⟨cc⟩of the dk-graphs. When ⟨cc⟩
is small, ⟨cc⟩≤0.3, γ changes only slightly. However, when ⟨cc⟩> 0.3, γ increases
drastically as ⟨cc⟩increases. To a certain extent, the exponent γ reﬂects how signiﬁ-
cantlytheincreaseof PT wouldspeeduptheconvergenceprocess.When⟨cc⟩issmall,
there may be lack of complete triangles to establish triangular-communications.
Consequently, small ⟨cc⟩would limit the ability of translators for speeding up the
convergence, thus yielding a small γ. On the contrary, if ⟨cc⟩is large, triangles can
be easily formed, and thus triangular-communications can be frequently established.
8.3.5
Maximum Numbers of Total and Different Words
The maximum number of total words roughly indicates an upper bound of the
required memory size for the population. Likewise, the maximum number of differ-
ent words indicates an upper bound of the required memory size for unique words.
These two indexes are examined by extensive simulations, with results summarized
below.

8.3 Simulation Results and Analysis
151
Proportion of T (PT)
0
0.5
1
500
550
600
(j) N=1000
Proportion of T (PT)
0
0.5
1
240
260
280
(i) N=500
Proportion of T (PT)
0
0.5
1
500
550
600
(h) N=1000
Proportion of T (PT)
0
0.5
1
260
280
300
(g) N=500
Proportion of T (PT)
0
0.5
1
400
500
600
(f) N=1000
Proportion of T (PT)
0
0.5
1
240
260
280
(e) N=500
Proportion of T (PT)
0
0.5
1
400
500
600
(d) N=1000
Proportion of T (PT)
0
0.5
1
240
260
280
(c) N=500
Proportion of T (PT)
0
0.5
1
400
500
600
(b) N=1000
Proportion of T (PT)
0
0.5
1
max number of different words
240
260
280
(a) N=500
Fig. 8.9 The maximum number of different words versus the proportion of translators (PT ) on
the underlying networks: a RG/0.1/0.5K; b RG/0.1/1K; c WS/25/0.2/0.5K; d WS/50/0.2/1K; e
NW/20/0.02/0.5K; f NW/40/0.02/1K; g SF/25/25/0.5K; h SF/50/50/1K; i RTM/50/13/0.5K; j
RTM/50/27/1K. (Data taken from [14])
First, it can be seen from the simulations that these two indexes are basically not
affected by different underlying network topologies.
Figure 8.9 shows the curves of the maximum number of different words against the
variations of PT . Generally, the curves are not monotonically increasing or decreasing
as PT is increased. The PT curves are similar to the Ptc curves shown in Fig.8.5.
Figure 8.10 shows the curves of the maximum number of different words versus
PT on the WS small-world network. When inventing a new word, both the maximum
number of total and different words increase. When PT is small, although the proba-
bility of establishing triangular-communication is low, if a triangular-communication
can be established, then the same translator has a high probability to be selected. With
a larger Ptc, the chance for the same translator to be selected is high. Thus, the max-
imum number of different words versus PT has a similar form as that of Ptc versus
PT . This can be clearly seen by comparing Figs.8.6 and 8.9.

152
8
Multi-Language Naming Game
proportion of translators (PT)
max number of different words
0
100
200
300
400
500
600
(b) 1000 nodes
Entire Population
TR
EN
CH
proportion of translators (PT)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
max number of different words
0
50
100
150
200
250
300
(a) N=500
Fig. 8.10 The maximum number of different words versus the proportion of translators (PT ) on
WS small-world networks: a WS/25/0.2/0.5K; b WS/50/0.2/1K. (Data from [14])
Figure 8.10 shows that the maximum numbers of different words in different
language groups are different. If the 3 language groups are evenly distributed, i.e.
each group occupies 33.3% of the population, the maximum number of different
words of the translator group is greater than that of the other two groups. This
implies that the translators require larger memory sizes than single-language agents.
Memory allocation is also important in naming games [16]. According to Fig.8.9,
memory costs for different agents are different. If all the agents are assigned the
same memory size, then memory waste or memory insufﬁciency is inevitable. The
proportion of translators PT signiﬁcantly inﬂuences the required memory sizes. On
the other hand, it is possible to assign different memory sizes to the agents in different
language groups according to Fig.8.10. Obviously, this allocation method is a more
reasonable choice for memory resource consumption.
Curvesforthemaximumnumberoftotalwordsversustheproportionoftranslators
(PT ) are shown in Fig.8.11. The red squares represent the maximum number of total
words existing in the entire population, which is also the sum of the other three
groups (T R in blue stars, E N in black pluses, and C H in green squares). Comparing
Fig.8.11a, b, it is clear that the population size basically does not inﬂuence the curve
behavior. As PT increases, the proportions of English and Chinese language speakers
are reduced.
When PT ≤0.1, the population lacks translators for triangular-communications,
and thus the convergence process, including the maximum number of total words,
is severely inﬂuenced by PT . In contrast, when PT ≥0.2, the maximum number
of total words in the group of T R (blue stars) increases linearly as PT increases.
Also, the maximum numbers of total words for the E N and C H groups (black
pluses and green circles) decreases linearly as PT increases. Note that the curves
for E N and C H are heavily overlapped, since the two single-language groups here

8.3 Simulation Results and Analysis
153
proportion of translators (PT)
max number of total words
0
1000
2000
3000
4000
5000
6000
7000
(b) N=1000
Entire Population
TR
EN
CH
proportion of translators (PT)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
max number of total words
0
500
1000
1500
2000
2500
(a) N=500
Fig. 8.11 The maximum number of words versus the proportion of translators (PT ) on WS small-
world networks: a WS/25/0.2/0.5K; b WS/50/0.2/1K. (Data from [14])
have the same proportion in the population. The maximum number of total words
existing in the entire population remains relatively stable when PT ≥0.2. As dis-
cussed in Sect.8.3.4, when PT is small, the convergence process is sensitive to the
slight changes of PT . But, if PT is sufﬁciently large, the change of PT affects very
slightly the convergence process. Here, the sufﬁciency of PT means that PT is not
the bottle-neck parameter for local communications and/or global consensus.
The curve for the maximum number of total words existing in the entire population
(red squares) does not change much if PT ≥0.2. The peaks of these curves, as shown
in Fig.8.3, become more synchronizing when PT becomes larger.
In a nutshell, if the total required memory size is ﬁxed, the translator group
demands more memory resources than the single-language groups.
8.4
Conclusion
In this chapter, the naming game with multiple languages, or multi-language naming
game (MLNG), is introduced and investigated. Speciﬁcally, a bi-language system is
studied as an illustrative example, in which there are two single-language agents and
a group of translators who can speak both languages. This model is analyzed through
extensive simulations on ﬁve different underlying networks, namely RG, WS-SW,
NW-SW, SF, and RT network topologies.
The convergence process is the focus in the investigation. Analysis reveals that
the probability of different communication modes can be estimated by network topo-
logical features such as average node-degree, average clustering coefﬁcient, and pro-
portion of translators in the population. It is also found that MLNG has an interesting
power-law-like relation between the convergence time and the proportion of trans-

154
8
Multi-Language Naming Game
lators. Different agents require different memory sizes in MLNG, thus a good local
memory assignment method is suggested for better memory resource consumption
in MLNG.
References
1. Y. Lou, G.R. Chen, Analysis of the “naming game” with learning errors in communications.
Sci. Rep. 5, 12191 (2015). https://doi.org/10.1038/srep12191
2. W.X. Wang, B.Y. Lin, C.L. Tang, G.R. Chen, Agreement dynamics of ﬁnite-memory language
gamesonnetworks.Eur.Phys.J.B60(4),529–536(2007).https://doi.org/10.1140/epjb/e2008-
00013-5
3. Y. Gao, G.R. Chen, R.H.M. Chan, Naming game on networks: let everyone be both speaker
and hearer. Sci. Rep. 4, 6149 (2014). https://doi.org/10.1038/srep06149
4. B. Li, G.R. Chen, T.W.S. Chow, Naming game with multiple hearers. Comm. Nonl. Sci. Numer.
Simul. 18, 1214–1228 (2013). https://doi.org/10.1016/j.cnsns.2012.09.022
5. D. Lipowska, Naming game and computational modelling of language evolution. Comput.
Methods Sci. Tech. 17(1–2), 41–51 (2011)
6. D. Lipowska, A. Lipowski, Naming game on adaptive weighted networks. Artif. Life 18(3),
311–323 (2012). https://doi.org/10.1162/artl_a_00067
7. A. Lipowski, D. Lipowska, Computational approach to the emergence and evolution of
language-evolutionary naming game model (2008), arXiv:0801.1658
8. L. Pucci, P. Gravino, V.D.P. Servedio. Modeling the emergence of a new language: naming
game with hybridization, in Proceedings of International Workshop Self-Organizing System
(Springer, 2013), pp. 78–89
9. P. Erd˝os, A. Rényi, On random graphs I. Publ. Math. Debrecen 6, 290–297 (1959)
10. D.J. Watts, S.H. Strogatz, Collective dynamics of ‘small-world’ networks. Nature 393(6684),
440–442 (1998). https://doi.org/10.1038/30918
11. M.E.J.Newman,D.J.Watts,Renormalizationgroupanalysisofthesmall-worldnetworkmodel.
Phys. Lett. A 263(4–6), 341–346 (1999)
12. A.L. Barabási, R. Albert, Emergence of scaling in random networks. Science 286(5439), 509–
512 (1999)
13. C. Orsini, M.M. Dankulov, P. Colomer-de Simón et al., Quantifying randomness in real net-
works. Nat. Comm. 6, 8627 (2015). https://doi.org/10.1038/ncomms9627
14. J. Zhou, Y. Lou, G.R. Chen, W.K.S. Tang, Multi-language naming game. Physica A 496,
620–634 (2018). https://doi.org/10.1016/j.physa.2017.12.124
15. J. Zhou, Y. Lou, G.R. Chen, W.K.S. Tang, Supplementary information for the paper “multi-
language naming game”. Physica A, pp. 1–15 (2018), https://ars.els-cdn.com/content/image/
1-s2.0-S0378437117313730-mmc1.pdf
16. A. Baronchelli, M. Felici, V. Loreto, E. Caglioti, L. Steels, Sharp transition towards shared
vocabularies in multi-agent systems. J. Stat. Mech. Theory Exp. 6, P06014 (2006). https://doi.
org/10.1088/1742-5468/2006/06/P06014

Conclusions
This monograph has presented the notion of naming game, in various versions,
speciﬁcally the minimal naming game with inﬁnite or ﬁnite size of agent memories
(Chaps.2 and 3), naming game with group discussions (Chap.4), naming game with
learning errors in communications (Chap.5), naming game on multi-community
networks (Chap.6), naming game with multiple words or sentences (Chap.7), and
naming game with multiple languages (Chap.8).
As a typical computer game model for studying language creation and develop-
ment, as well as opinion spreading and consensus alike, naming game provides a
powerful and efﬁcient model for simulation and analysis. Along with several vari-
ants, the naming game is useful for exploring the emergence and evolution of shared
information (e.g., object names, social conventions, personal opinions, individual
ideas, and human knowledge) within a population of communicating agents. Agents
in the population are connected in a certain communication topology, determined by
their social relationships, thus the mathematical graph theory provides useful tools
for the study.
Since human language is an extremely sophisticated and complicated system con-
sisting of creation, acquisition and maintenance, with the properties of productivity
and displacement, relying on social environments and human learning, and is evolv-
ing and diversifying over time, the various naming games studied in this treatise is
clearly taking only the very ﬁrst step and playing a very premature role in the study.
On one hand, it is not possible to expect being able to truly mimic the real language
development scenario, and yet, on the other hand, it paves the way to gain a basic
understanding with some fundamental knowledge about relevant social and language
studies. It also leaves the door widely open for more and better investigations towards
a comprehensive investigation on the important research subject of naming game.
© Springer Nature Switzerland AG 2019
G. Chen and Y. Lou, Naming Game, Emergence, Complexity and Computation 34,
https://doi.org/10.1007/978-3-030-05243-0
155

Index
A
Agent, 1
Autonomous System (AS), 6, 96
B
Barabási–Albert (BA), 6
BA scale-free network, 3, 14
C
Candidate Word (CW), 6
Clustering coefﬁcient, 13
Communication ratio, 146
Community structure, 96
Complex network, 11
Connection probability, 11
Consensus, 2
Conventional sentence pattern, 119
Convergence, 18
Convergence process, 33, 79, 124, 142
Convergence speed, 147
Convergence threshold, 89
Convergence time, 19, 29, 82, 147
D
Degree, 13
Distance, 13
E
Edge, 2
Erdös–Rényi (ER), 6
ER random-graph network, 3, 11
Error rate, 73, 75, 77
F
Finite memory, 24
Finite-Memory Naming Game (FMNG), 6,
23
G
Global consensus, 16
H
Hearer, 2
Hearer-Only Naming Game (HO-NG), 6, 47
Heterogeneous network, 15
Homogeneous network, 15
I
Inﬁnite memory, 19
L
Language game, 1
Length of Memory (LM), 6
Local consensus, 18, 121, 137
Local failure, 18, 137
Local learning, 18
Local pseudo consensus, 76
Local success, 18, 137
M
Maximum Number of Different Words
(MNDW), 6
Maximum
Number
of
Total
Words
(MNTW), 6
Memory loss, 32
Minimal naming game, 2, 19
© Springer Nature Switzerland AG 2019
G. Chen and Y. Lou, Naming Game, Emergence, Complexity and Computation 34,
https://doi.org/10.1007/978-3-030-05243-0
157

158
Index
Multi-Language Naming Game (MLNG), 6,
135
Multi-Local-World (MLW), 6, 96
Multi-Local-World (MLW) network, 15, 96
Multi-WordNamingGame(MWNG),6,116
N
Naming Game in Groups (NGG), 6, 44, 49
Naming game model with learning errors, 72
Naming Game (NG), 6, 16
Naming
Game
with
Learning
Errors
(NGLE), 6, 72, 74
Naming
Game
with
Multiple
Hearers
(MHNG), 6, 43
Newman–Watts Small-World (NW-SW), 6
Node, 2
Node-degree, 13
Node-degree distribution, 14
Number of different words, 18, 33, 55, 79,
107, 128, 143, 150
Number of Hearers (NH), 6
Number of iterations, 78
Number of total word, 18, 33, 55, 78, 107,
124, 143, 150
O
Object, 16
P
Path length, 13
Poisson distribution, 13
Power-Law, 15
Preferential attachment, 14, 98
R
Random-Graph (RG), 6
Random Triangle Model (RTM), 6, 136
Rewiring probability, 13
S
Scale-Free (SF), 6, 15
Single-Word Naming Games (SWNG), 6,
115
Small-World (SW), 6
Source agent, 53
Speaker, 2
Speaker-Only Naming Game (SO-NG), 6,
48
Success rate, 18, 33, 82, 107
T
Transmitted-word, 51
W
Watts–Strogatz Small-World (WS-SW), 6
Watts–Strogatz (WS), 6
Weight of word, 51
WS small-world network, 3, 13

