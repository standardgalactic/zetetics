arXiv:2304.06035v1  [cs.OH]  31 Mar 2023
Choose Your Weapon:
Survival Strategies for Depressed AI Academics
Julian Togelius and Georgios N. Yannakakis∗
April 14, 2023
Abstract
Are you an AI researcher at an academic institution? Are you anxious you are not coping
with the current pace of AI advancements?
Do you feel you have no (or very limited) access
to the computational and human resources required for an AI research breakthrough? You are
not alone; we feel the same way.
A growing number of AI academics can no longer ﬁnd the
means and resources to compete at a global scale. This is a somewhat recent phenomenon, but
an accelerating one, with private actors investing enormous compute resources into cutting edge
AI research. Here, we discuss what you can do to stay competitive while remaining an academic.
We also brieﬂy discuss what universities and the private sector could do improve the situation, if
they are so inclined. This is not an exhaustive list of strategies, and you may not agree with all of
them, but it serves to start a discussion.
1
Introduction
As someone who does AI research in a university, you develop a complicated relationship to the corpo-
rate AI research powerhouses, such as DeepMind, Open AI, Google Brain and Meta AI. Whenever you
see one of these papers that train some kind of gigantic neural net model to do something you weren’t
even sure a neural network could do, unquestionably pushing the state of the art and reconﬁguring
your ideas of what is possible, you get conﬂicting emotions. On the one hand: it’s very impressive.
Good on you for pushing AI forward. On the other hand: how could we possibly keep up? As an AI
academic, leading a lab with a few PhD students and (if you’re lucky enough) some postdocs, perhaps
with a few dozen GPUs in your lab, this kind of research is simply not possible to do.
To be clear, this was not always the case. As recently as ten years ago, if you had a decent desktop
computer and an internet connection you had everything you needed to compete with the best of
researchers out there. Ground-breaking papers were often written by one or two people who ran all
the experiments on their regular workstations. It is useful to point this out particularly for those
who’ve come into the research ﬁeld within the last decade, and for which the need for giant compute
resources is a given.
If we have learned one thing from deep learning, it is that scaling works. From the ImageNet [15]
competitions and their various winners to ChatGPT, Gato [13], and most recently to GPT-4 [1], we
have seen that more data and more compute yield quantitatively and often even qualitatively better
results. (By the time you are reading this, that list of very recent AI milestones might very well be
outdated.). Of course there are improvements to learning algorithms and network architectures as
well, but these improvements are only really useful in the context of the massive scale of experiments.
(Sutton talks about the “Bitter Pill”, referring to that simple methods that scale well always win
the day when more compute becomes available [18].)
A scale that is not achievable by academic
researchers nowadays. As far as we can tell, the gap between the amount of compute available to
ordinary researchers and the amount available to stay competitive is growing every year.
This goes a long way to explain the resentment that many AI researchers in academia feel towards
these companies. Healthy competition from your peers is one thing, but competition from someone
that has so much resources that they can easily do things you could never, no matter how good your
ideas are, is another thing. When you’ve been working on a research topic for a while and DeepMind or
∗JT is with New York University, and GNY is with the University of Malta.
1

OpenAI decides to work on the same thing, you’ll likely feel the same way as the owner of a small-town
general store feels when Walmart sets up a shop next door. Which is sad, because we want to believe
in research as an open and collaborative endeavor where everybody gets their contribution recognized,
don’t we?
So, if you are but a Professor, with a limited team size and limited compute resources, what can
you do to stay relevant in face of the onslaught of incredibly well-funded research companies? This
is a question that has been troubling us and many of our colleagues for years now. Recent events,
with models such as GPT-4 being shockingly capable and shockingly closed-sourced and devoid of
published details, has made the question even more urgent. We have heard from multiple researchers
at various levels of seniority, both in-person and via social media, who worry about the prospects
of doing meaningful research given the lack of resources and the unfair competition from big tech
companies.
Let us make this clear at the outset: both of us are secure. We hold tenured academic Professorships
and we rose up on the academic ladder pretty fast, in part, because we systematically pushed the
envelope of AI in the domain of video games. While we obviously care about continuing to do relevant
AI research ourselves, we are writing this mostly for our more junior colleagues, who may wonder
about which career path to choose. Is it worthwhile to go into academia, or is it better to join a big
tech company, or maybe start a startup? Is a career in AI a good idea, or is it better to become a
plumber? Should you be a cog in the machinery, or a rebel? (It’s usually easier to be a rebel when
you have nothing to lose, which is either at the beginning of your career or when you have tenure.)
As skilled as one can be, is this glorious battle to stay competitive lost already? Are we about to
lie here, obedient to our laws? This paper is partly meant as serious advice, and partly as emotional
encouragement, but perhaps most of all to start a discussion with all of you so we improve our position
as academics before the battle is long lost. We do not wish to stop the evolution of AI technology
(even if we could); quite the contrary: we wish to discuss the strategies that will equip as many as
possible to be part of this journey.
The following is a list of ideas for what to do if you are an AI academic despairing about your
options.
These options are presented in no particular order.
We also don’t make any particular
recommendations here. Towards the end of the document, however, we discuss what big tech companies
and universities can do to help the situation. There, we make some speciﬁc suggestions.
2
Give Up!
Giving up is always an option. Not giving up on doing research, but giving up on doing things that
are really impactful and pushing the envelope. There are still plenty of technical details and sub-sub-
questions to publish papers about in mid-tier journals and conferences. Note: (1) This works best if
you already have a secure permanent position and you do not care much about promotions, (2) this
wasn’t really what you came to do when you decided on a research career, right? Forcing yourself to
reframe your research agenda because of this ﬁerce competition is similar to adjusting your research
to the sometimes arbitrary priorities of funding bodies like the European Commission or US National
Science Foundation. At least going for the latter might secure some funding for your lab which can,
in turn, help you work with some talented AI researchers and doctoral students. It is important to
note that we both consider ourselves lucky enough as we have coordinated or have been part of several
small- and large-scale research projects1 that allowed us to support our research agendas and helped
us (in part) to secure our positions.
3
Try Scaling Anyway
Going head-to-head with an overwhelming competition is an admirable sentiment. If scaling works,
let’s do it in our university labs! Let’s go tilting at windmills (GPU fans)!
The most obvious problem is access to CPUs and GPUs. So, let’s say you secure $50k of funding
for cloud compute from somewhere and go ahead running your big experiment. But this is a very
small amount of money compared to what training something like GPT-3 costs. The recent open
1Examples
include
the
H2020
AI4Media
(https://www.ai4media.eu/)
and
the
FP7
C2Learn
(http://project.c2learn.eu/) projects.
2

AI agent that learned to craft a diamond pickaxe in Minecraft required training of 9 days on 720
V100 GPUs [2]; this amounts to a few hundred thousand dollars for a single experiment. Not even
prestigious ERC (EU) or NSF (US) grants can support such a level of investment. Still, spending
$50k on cloud compute will give you signiﬁcantly more compute than a bunch of gaming PCs taped
together, so you could scale at least a little bit. At least for that very experiment. But as we all know,
most experiments don’t work the ﬁrst time you try them. For every big experiment we see reported,
we have months or maybe years of prototypes, proofs of concept, debugging, parameter tuning, and
failed starts. You need this level of compute available constantly.
The less obvious problem is that you need the right kind of team to build experimental software
that scales, and that’s not easily compatible with academic job structures. Most of the members of a
typical academic research lab in computer science are PhD students that need to graduate within a few
years, and need to have an individual project to work on which results in multiple ﬁrst-author papers
so they can get a job afterwards. A large-scale AI project typically means that most members of the
team work for many months or years on the same project, where only one of them can be ﬁrst author
on the paper. The team will probably also include people who do “mundane” software engineering
tasks that are crucial to the success of the project, but which are not seen as AI research in themselves.
The structures needed for successful large scale projects are simply not compatible with the structures
of academia.
4
Scale Down
One popular way to bypass the issue is to focus on simple yet representative (toy) problems that will
either prove the beneﬁts of a new approach theoretically or showcase the comparative advantages of
a novel method. Indicatively, a recent paper on Behaviour Transformers [17] showcased the beneﬁts
of the method on a toy navigation task that only took a simple multi layer perceptron to solve. A
similar approach was later used in [11]. Both studies however will likely be impactful because they
demonstrated the capacity of the algorithms in popular game and robotic benchmark problems that
require large models and signiﬁcant compute to train. In [10] we observe the same pattern once again:
a case is made in a toy (gambling) environment but the impact, one would argue, comes from the
comparative advantages the algorithm shows in more complex but computationally heavy problems.
A downside with this approach is that people are wowed by pretty colors in high resolution, and
take a real car navigating a road more seriously than a toy car, even though the challenges may be the
same. So you will get less media exposure. There are also domains, such as language, which are very
hard to scale down further.
5
Reuse and Remaster
One of the core reasons AI has advanced so rapidly over the last decade is that researchers make
their code and models available to the scientiﬁc community. Model sharing and code accessibility was
neither the norm nor the priority of AI researchers back in the days. Having access to pretrained
large models like ImageNet [15], ViT [4] or variants of GPT [?] saves you time and eﬀort as you can
adopt large parts of them and ﬁne tune smaller parts of your model for your own speciﬁc problem.
Arguably, one needs to assume that the representations of those large models is general enough to be
able to perform well to your downstream task with limited training. Unfortunately the ﬁne-tuning
and post-hoc analysis of a large model is often not enough for good performance.
6
Analysis Instead of Synthesis
Another thing to do with the publicly available pretrained models is to analyze them. While this may
not directly contribute to new capabilities, it can still make scientiﬁc progress. The current state of
things is that we have great models for text and image generation publicly available, but we don’t
understand them very well. Actually, we barely understand them all. Let’s face it: a transformer
is not an intuitive thing to anyone, and the scale of data these models are trained on is almost
incomprehensible in itself. There is plenty of work to do in analyzing what happens, for example by
3

probing them in creative ways, and developing visualizations and conceptual machinery to help us
understand them.
One can do analysis with diﬀerent mindsets.
Trying to ﬁnd and describe speciﬁc circuits and
mechanisms that have been learnt is useful, and can help us (well, someone else, with resources) to
create better models in the future. But one can also play the role of the gadﬂy, incessantly ﬁnding
ways to break them! This is valuable, no matter what those who try to make a business out of large
models say.
7
RL! No Data!
One might scale down one’s requirements with respect to data and instead approach AI problems
through the lens of (online) reinforcement learning (RL). Following the RL path might allow you to
bypasses all issues related to data availability, its analysis, storage and handling; it does not however
minimize the computational eﬀort required necessarily. In fact, even the most eﬃcient RL methods
are known to be computationally heavy as the very process of exploration is costly. Moreover, shaping
a reward function often involves forms of black art (informally) or practical wisdom (more formally).
That is, a researcher often needs to continuously run lengthy experiments with diﬀerent types of reward
(among other hyperparameters) for a breakthrough result. So ultimately one has to downscale the
complexity of the problem once again. The bottom line is that if you want to break free from large
data sets you might be still faced with large compute unless you work on simple (toy) problems or
specialized domains; the next section is dedicated to the latter strategy.
8
Small Models! No Compute!
Another valid strategy is to compromise on model scale to save on compute. There are many cir-
cumstances where you want or need a smaller model. Think of the smallest possible models that are
capable of solving a problem or completing a task. This is particularly important to and relevant
for real-world applications. In-the-wild domains such as games, IoT, and autonomous vehicles could
allow AI to be deployed next to their end user and the data the user generates, i.e. at the edge of the
network. This is often called edge AI [8], the operation of AI applications in devices of the physical
world is possible when memory requirements are low and inference occurs rapidly. Neuroevolution and
neural architecture search [8], and knowledge distillation [5, 9] methods are only a few of the available
methods for edge AI. Note that beyond learning more from smaller models one could also attempt to
learn more from less data [6].
Following this research path may lead to signiﬁcant breakthroughs when it comes to our under-
standing of a model’s inner workings. Studying small AI models makes the analysis far easier and
increases the explainability of whatever the model does. Deploying models on device helps with pri-
vacy concerns. Importantly, it supports the green AI research initiative [16] advocating for an inclusive
AI that considers (and attempts to minimize) its environmental footprint. Obviously there are limits
to what a small model is capable of doing but the importance of this research direction, we feel, will
be growing drastically over the years.
9
Work on Specialized Application Areas or Domains
One rather eﬃcient strategy is to pick a niche but somewhat established area of research–that is likely
beyond the immediate interest of the industry and try to innovate within and through that area. It
is often a successful strategy to bring and test your ideas to an entirely new domain but it is less
often that the outcomes will have a large impact beyond that domain. There are plenty of examples
of niche areas eventually becoming dominant due to the push of a few dedicated researchers. We are
both currently mostly taking this strategy, have the AI for games community as primary scientiﬁc
community where we can perform state-of-art work, as few large companies put serious eﬀorts into
modern AI for games.
Think of video games as a domain that penetrated and dominated the research communities of
robotics and computer vision back in early 00s (e.g. the IJCAI and AAAI conference series at the
time). Think of neural networks and deep learning methods that dominated communities invested in
4

support vector machines and regression models (e.g. NeurIPS a decade ago). Also think of the ways
reinforcement learning and deep learning have altered the core principles of multi-agent learning and
cognitive/aﬀect modeling in communities represented by the AAMAS, ACII and IVA conferences, for
instance. A core downside to this strategy is the diﬃculty getting your paper accepted in the kind of
large venues that are most inﬂuential in AI, such as NeurIPS, AAAI, ICML and IJCAI. Your paper
and its results might end up sitting out-of-the-interest-distribution. It is, however, very possible to
start your own community with its own publication venues.
10
Solve Problems Few Care About (For Now!)
While focusing on an established niche or application ﬁeld is a relatively safe strategy, a somewhat
riskier one is to ﬁnd a niche or application that does not exist yet. Basically, focus on a problem that
almost no-one sees the importance of, or a method that nobody ﬁnds promising. One approach is to
go looking for applications that people have not seriously applied AI to.
A good idea is to look into a ﬁeld that is neither timely nor “sexy”. The bet here is that this
particular application domain will become important in the future, either in its own right or because
it enables something else. We both took this path. Procedural content generation for games was a
very niche topic 15 years ago which we both helped introduce to the research community [20, 23];
recently it has become more important not only for the games industry, but also as a way to help
generalize (deep) reinforcement learning [14, 19]. Research on reinforcement learning is a core AI topic
with thousands of papers published per year, lending more importance to this once somewhat obscure
topic. This high-risk high-gain mindset of might lead to a lonely path that nevertheless could end up
being highly rewarding in the long run.
So, look around you, and talk to people who are not AI researchers. What problem domains do
you see where AI is rarely applied, and which AI researchers seem to not know or care about? Might
someone care about these domains in the future? If so, you may want to dig deeper in one of those
domains.
11
Try Things that Shouldn’t Work
Another comparative advantage of small academic teams is the ability to try things that “shouldn’t
work”, in the sense that they are unsupported by theory or experimental evidence. The dynamics of
large industry research labs are typically such that researchers are incentivized to try things that are
likely to work; if not, money is lost. In academia, failure can be as instructive and valuable as success
and the stakes are overall lower. Many important inventions and ideas in AI come from trying the
“wrong” thing. In particular, all of deep learning stems from researchers stubbornly working on neural
networks even though there were good theoretical reasons why they shouldn’t work.
12
Do Things that Have Bad Optics
The larger and more important a company is, the more constrained it is by ethics and optics. Any
company is ultimately responsible to their shareholders, and if the shareholders perceive that the
company suﬀers “reputational damage” they can easily ﬁre the CEO. So large companies will try to
avoid to do anything that looks bad. To get around this, large companies sometimes fund startups to
do their more experimental work that might go wrong (think Microsoft and OpenAI). But even such
plays have limits, as bad PR can come washing back like the tide in San Francisco Bay.
As an individual researcher, who either has no position or who already has a secure position, you
have nothing to lose. You can do things that are as crazy as you like. You are only constrained
by the law and your own personality. Now, we are in no way arguing that you should do research
that is unethical. By all means, try to do the right thing. But what you ﬁnd objectionable might
be very diﬀerent from what a group of mostly-white liberal overeducated engineers in coastal USA
ﬁnd objectionable. The PR departments, ethics committees, and boards of directors of the rich tech
companies espouse a very particular set of values. But the world is large, and full of very diﬀerent
people and cultures. So there is a big opportunity to do research that these tech companies will not
do even though they could.
5

As an example of a project that exploits such an opportunity, one of us participated in a project
critically examining the normativity of the “neutral English” in current writing support systems by
creating an autocomplete system with a language model that assumes you write in the tone of Chuck
Tingle, the famous author of absurd sci-ﬁpolitical satire gay erotica [7]. Our guess is that this project
would not have been cleared for publication by Amazon or Google. Another example is this very
paper.
Similarly, you may ﬁnd that you deviate from the cultural consensus in big tech companies regarding
topics relating to nudity, sexuality, rudeness, religion, capitalism, communism, law and order, justice,
equality, welfare, representation, history, reproduction, violence, or something else. As all AI research
happens in and is inﬂuenced by a cultural and political context, see your deviation from the norm as
an opportunity. If you can’t do the research they couldn’t do, do the research they wouldn’t do.
13
Start it Up; Spin it Out!
By now it should be rather clear that academia is somewhat, paradoxically, limiting academic AI
research. Even if one manages to secure large-scale multimillion projects this covers only a fraction
of human and computational resources that are necessary for contemporary AI research. One popular
alternative among AI scientists is to spin out their idea from their university lab and found a company
that will gradually transfer AI research to a set of commercial-standard services or products. Both
authors have been part of this journey through co-founding modl.ai [12] and have learned a lot from
this.
Being part of the applied AI world oﬀers many beneﬁts. In principle you get access to rich data
from real-world applications that you wouldn’t be able to have otherwise. Moreover your AI algorithms
are tested on challenging commercial-standard, applications and have to be operational in the wild.
Finally, you usually gain access to more compute and, if the start-up scales up, growing access to
human resources.
This journey is far from being ideal, however, as there are several limiting factors to consider.
First, not all research ideas are directly applicable to a startup business model. Your best research
ideas might be brilliant in terms of understanding the world, or at least getting published in highly
prestigious venues, but that doe snot mean that one can easily make products out of them. Second,
many outstanding results one obtained in the lab today may have to go through long runway until
they turn into a business case of some sort. Most startups do development rather than research, as the
runways are short and you need to have a functioning product, preferably with some market traction,
before the next funding round in two years or so. Third, even if you do get some investment, this
does not mean you have an unlimited compute budget. With seed grants often in the range of a few
millions, this does not buy you the capacity to do OpenAI-level experiments, especially as you need
to pay real salaries (not PhD stipends) to your employees. Fourth, not every AI academic enjoys this
type of an adventure. At the end of the day most academics have long agreed on their priorities when
they opted to follow the academic career path. The security of an academic environment (given it is
both safe and creative), means to some far more than any potentially higher salary or other corporate
beneﬁts.
Here, we might point out that both of us publish many more papers with our academic research
teams than with the company we co-founded and work part-time at. On the other hand, we believe
we have more direct impact on the games industry through our company.
14
Collaborate or Jump Ship!
If none of the above options work for you and you still want to innovate though large scale methods
that are trained on lots of data you can always collaborate with those that have them both: compute
and data. There are several ways to move forward with this approach.
Universities in the vicinity of leading AI companies have a comparative advantage as local social
networks and in-person meetings make the collaboration easier. Researchers from remote universities
can still establish collaborations though research visits, placements and internships as part of a joint-
research project. More radically, some established AI professors decide to dedicate some (if not all) of
their research time to an industrial partner or even move their entire lab in there. Results from such
6

partnerships, placements or lab transfers can be astonishing [21, 22]. At a glance, this looks like the
best way forward for AI academics, however, 1) the generated IP cannot always be published and 2)
not everyone can or want to work in an AI industry-based lab.
One might even argue that innovation should be driven by public institutions as supported by
the industry; not the other way around. It is therefore, the University’s responsibility to maintain
(part of, or some of) the talented AI researchers it educates (academics and students) and the IP they
generate. Otherwise AI education and research will eventually become redundant within a University
environment. Next, let’s look at this relationship more closely and outline ways industrial corporations
and universities may be able to help.
15
How Can Large Players in Industry Help?
It is not clear that large companies with well-ﬁnanced AI labs actually want to help alleviate this situ-
ation. Individual researchers and managers might care about the depression of academic AI research,
but what the companies care about is the bottom line and shareholder value, and having a competitive
academic research community might or might not be in their best interest. However, to the extent
that large private sector actors do care, there are multiple things they can do.
At the most basic level, open-sourcing models, including both weights and training scripts, helps a
lot. It allows academic AI researchers to study the trained models, ﬁne-tune them, and build systems
around them. It still leaves academic researchers uncompetitive when it comes to training new models,
but it is a start. To their credit, several large industrial research organizations regularly release their
most capable models publicly. Others don’t, and could rightly be shamed for not doing so.
The next step for remedying this situation is to collaborate with academia. As discussed earlier (see
Section 14) some large institutions regularly do this, mostly through accepting current PhD students
as interns, allowing these students to do large-scale work. Some oﬀer joint appointments to certain
academic research, and a few even occasionally oﬀer research grants. All of this is good, but more can
be done. In particular there could be mechanisms where academics initiate collaborations by proposing
work they would do collaboratively.
Going even further, private companies that really wanted to help mend this academia-industry
divide could choose to work in public: post their plans, commit code, models, and development updates
to public repositories and allowing academics to contribute freely. This is not how most companies
work, and often they have good reasons for their secrecy. On the other hand, a lot could be gained
from having academics contributing to your code and training for free.
16
How Can Universities Help?
As much as industry might be willing to help, the primary initiative should come from those universities
that wish to drive innovation. It is worth noting that several of the most inﬂuential papers in the
broader AI scene involve a university department. Those papers are co-authored by researchers that
either collaborate with or are involved in the company. The successful examples are out there [21, 22, 3],
but more is needed from the University’s end to enable such partnerships. And actually, there are
many ways an academic institution can initiate and foster collaboration with the industry.
Universities can also help their faculty manage the changed competitive landscape by encouraging
and allowing them to be more risk-taking. The comparative advantage of academic researchers in AI
is to do more high-risk exploration, and incentive structures at universities must change to account for
this. For example, it is unreasonable to expect a steady stream of papers at top-tier conferences such as
NeurIPS and AAAI; large, well-funded industry research labs will have large advantages at writing such
papers. Similarly, the grant funding structure is such that it rewards safe and incremental research on
popular topics; this seems to be an inherent feature of the way grant applications are evaluated, and it
is unlikely to change however often funding agencies use words like “disruptive”. The kind of research
that is favored by some of the most traditional (closed-call) grant mechanisms is mostly the kind of
research where academic researchers will not be able to compete with industry. Therefore, universities
should probably avoid making grant funding a condition for hires and promotions. If universities are
serious about incentivising their faculty to leverage their competitive advantage, they should reward
7

trying and failing and promote high-risk high-gain funding schemes and research initiatives. It is then
likely that funding agencies will follow the trend and invest even more on basic and blue sky research.
17
Parting Words
We wrote this letter with several purposes in mind. First, to share our concerns with other fellow
AI researchers with a hope of ﬁnding a common cause (and a collective remedy?) as a community.
Second, to oﬀer a set of guidelines based on our own experiences but also the discussions we had in
the academic and industrial AI venues we participate or organize. Third, to spur an open dialogue
and solicit ideas for potentially more eﬃcient strategies for us all. Arguably, the list of strategies we
ended up discussing here are far from being inclusive of all possibilities that are available out there;
we believe, however, that they are seeds of a conversation that—in our opinion—is very timely.
References
[1] Rick Astley. Never Gonna Give You Up. RCA, New York, 1987.
[2] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoﬀet, Brandon
Houghton, Raul Sampedro, and JeﬀClune. Video pretraining (vpt): Learning to act by watching
unlabeled online videos. Advances in Neural Information Processing Systems, 35:24639–24654,
2022.
[3] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter
Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via
sequence modeling. Advances in neural information processing systems, 34:15084–15097, 2021.
[4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020.
[5] Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A
survey. International Journal of Computer Vision, 129:1789–1819, 2021.
[6] Souhila Kaci. Working with preferences: Less is more. Springer Science & Business Media, 2011.
[7] Ahmed Khalifa, Gabriella AB Barros, and Julian Togelius. Deeptingle. In International Confer-
ence on Computational Creativity, 2017.
[8] En Li, Liekang Zeng, Zhi Zhou, and Xu Chen.
Edge ai: On-demand accelerating deep neu-
ral network inference via edge computing.
IEEE Transactions on Wireless Communications,
19(1):447–457, 2019.
[9] Konstantinos Makantasis, David Melhart, Antonios Liapis, and Georgios N Yannakakis. Privileged
information for modeling aﬀect in the wild. In 2021 9th International Conference on Aﬀective
Computing and Intelligent Interaction (ACII), pages 1–8. IEEE, 2021.
[10] Keiran Paster, Sheila McIlraith, and Jimmy Ba. You can’t count on luck: Why decision trans-
formers fail in stochastic environments. arXiv preprint arXiv:2205.15967, 2022.
[11] Tim Pearce, Tabish Rashid, Anssi Kanervisto, Dave Bignell, Mingfei Sun, Raluca Georgescu,
Sergio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, et al. Imitating
human behaviour with diﬀusion models. arXiv preprint arXiv:2301.10677, 2023.
[12] Christoﬀer Holmg˚ard Pedersen, Benedikte Mikkelsen, Julian Togelius, Georgios N Yannakakis,
Sebastian Risi, and Lars Henriksen. Experience based game development and methods for use
therewith, May 17 2022. US Patent 11,331,581.
8

[13] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,
Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A
generalist agent. arXiv preprint arXiv:2205.06175, 2022.
[14] Sebastian Risi and Julian Togelius. Increasing generality in machine learning through procedural
content generation. Nature Machine Intelligence, 2(8):428–436, 2020.
[15] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International journal of computer vision, 115:211–252, 2015.
[16] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. Green ai. Communications of the
ACM, 63(12):54–63, 2020.
[17] Nur Muhammad Shaﬁullah, Zichen Cui, Ariuntuya Arty Altanzaya, and Lerrel Pinto. Behavior
transformers: Cloning k modes with one stone. Advances in neural information processing systems,
35:22955–22968, 2022.
[18] Richard Sutton. The bitter lesson. Incomplete Ideas (blog), 13(1), 2019.
[19] Adaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal Behbahani, Avishkar
Bhoopchand, Nathalie Bradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, et al.
Human-timescale adaptation in an open-ended task space.
arXiv preprint arXiv:2301.07608,
2023.
[20] Julian Togelius, Georgios N Yannakakis, Kenneth O Stanley, and Cameron Browne. Search-based
procedural content generation: A taxonomy and survey. IEEE Transactions on Computational
Intelligence and AI in Games, 3(3):172–186, 2011.
[21] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
 Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems, 30, 2017.
[22] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling
network architectures for deep reinforcement learning. In International conference on machine
learning, pages 1995–2003. PMLR, 2016.
[23] Georgios N Yannakakis and Julian Togelius. Experience-driven procedural content generation.
IEEE Transactions on Aﬀective Computing, 2(3):147–161, 2011.
9

