Theoretical bounds on estimation error for meta-learning
James Lucas
JLUCAS@CS.TORONTO.EDU
University of Toronto; Vector Institute
Mengye Ren
MREN@CS.TORONTO.EDU
University of Toronto; Vector Institute; Uber ATG
Irene Kameni
IKAMENI@AIMSAMMI.ORG
African Master for Mathematical Sciences; Vector Institute
Toniann Pitassi
TONI@CS.TORONTO.EDU
University of Toronto; Vector Institute; Canadian Institute for Advanced Research
Richard Zemel
ZEMEL@CS.TORONTO.EDU
University of Toronto; Vector Institute; Canadian Institute for Advanced Research
Abstract
Machine learning models have traditionally been developed under the assumption that the training
and test distributions match exactly. However, recent success in few-shot learning and related
problems are encouraging signs that these models can be adapted to more realistic settings where
train and test distributions differ. Unfortunately, there is severely limited theoretical support for
these algorithms and little is known about the difﬁculty of these problems. In this work, we provide
novel information-theoretic lower-bounds on minimax rates of convergence for algorithms that are
trained on data from multiple sources and tested on novel data. Our bounds depend intuitively
on the information shared between sources of data, and characterize the difﬁculty of learning in
this setting for arbitrary algorithms. We demonstrate these bounds on a hierarchical Bayesian
model of meta-learning, computing both upper and lower bounds on parameter estimation via
maximum-a-posteriori inference.
1. Introduction
Many practical machine learning applications deal with distributional shift from training to testing.
One example is few-shot classiﬁcation (Ravi and Larochelle, 2016; Vinyals et al., 2016), where new
classes need to be learned at test time based on only a few examples for each novel class. Recently,
few-shot classiﬁcation has seen increased success; however, the theoretical properties of this problem
remain poorly understood.
In this paper we analyze the meta-learning setting, where the learner is given access to samples
from a set of meta-training distributions, or tasks. At test-time, the learner is exposed to only a
small number of samples from some novel task. The meta-learner aims to uncover a useful inductive
1
arXiv:2010.07140v1  [stat.ML]  14 Oct 2020

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
bias from the original samples, which allows them to learn a new task more efﬁciently.1 While
some progress has been made towards understanding the generalization performance of speciﬁc
meta-learning algorithms (Amit and Meir, 2017; Bullins et al., 2019; Cao et al., 2019; Denevi et al.,
2019; Khodak et al., 2019), little is known about the difﬁculty of the meta-learning problem in
general. Existing work has studied generalization upper-bounds for novel data distributions (Amit
and Meir, 2017; Ben-David et al., 2010), yet to our knowledge, the inherent difﬁculty of these tasks
relative to the i.i.d case has not been characterized.
In this work, we derive novel bounds for meta learners. We ﬁrst present a general information
theoretic lower bound, Theorem 1, that we use to derive bounds in particular settings. Using this
result, we derive lower bounds in terms of the number of training tasks, data per training task, and
data available in a novel target task. Additionally, we provide a specialized analysis for the case
where the space of learning tasks is only partially observed, proving that inﬁnite training tasks or
data per training task are insufﬁcient to achieve zero minimax risk (Corollary 3).
We then derive upper and lower bounds for a particular meta-learning setting. In recent work,
Grant et al. (2018) recast the popular meta-learning algorithm MAML (Finn et al., 2017) in terms of
inference in a Bayesian hierarchical model. Following this, we provide a theoretical analysis of a
hierarchical Bayesian model for meta-linear-regression. We compute sample complexity bounds for
posterior inference under Empirical Bayes (Robbins, 1956) in this model and compare them to our
predicted lower-bounds in the minimax framework. Furthermore, through asymptotic analysis of
the error rate upper bound of the MAP estimator, we identify crucial features of the meta-learning
environment which are necessary for novel task generalization.
Our primary contributions can be summarized as follows:
• We introduce novel lower bounds on minimax risk of parameter estimation in meta-learning.
• Through these bounds, we compare the relative utility of samples from meta-training tasks and
the novel task and emphasize the importance of the relationship between the tasks.
• We provide novel upper bounds on the error rate for estimation in a hierarchical meta-linear-
regression problem, which we verify through an empirical evaluation.
2. Related work
Baxter (2000) introduced a formulation for inductive bias learning where the learner is embedded
in an environment of multiple tasks. The learner must ﬁnd a hypothesis space which enables good
generalization on average tasks within the environment, using ﬁnite samples. In our setting, the
learner is not explicitly tasked with ﬁnding a reduced hypothesis space but instead learns using a
two-stage approach, which matches the standard meta-learning paradigm (Vilalta and Drissi, 2002).
In the ﬁrst stage an inductive bias is extracted from the data, and in the second stage the learner
estimates using data from a novel task distribution. Further, we focus on bounding minimax risk
of meta learners. Under minimax risk, an optimal learner achieves minimum error on the hardest
learning problem in the environment. While average case risk of meta learners is more commonly
1. Note that this deﬁnition encompasses few-shot learning.
2

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
studied, recent work has turned attention towards the minimax setting (Hanneke and Kpotufe, 2019,
2020; Kpotufe and Martinet, 2018). The worst-case error in meta-learning is particularly important
in safety-critical systems, for example in medical diagnosis.
There is a large volume of prior work studying upper-bounds on generalization error in multi-task
environments (Amit and Meir, 2017; Ben-David and Borbely, 2008; Ben-David et al., 2010; Pentina
and Lampert, 2014). While the approaches in these works vary, one common factor is the need
to characterize task-relatedness. Broadly, these approaches either assume a shared distribution for
sampling tasks (Amit and Meir, 2017; Baxter, 2000; Pentina and Lampert, 2014), or a measure
of distance between distributions (Ben-David and Borbely, 2008; Ben-David et al., 2010; Mohri
and Medina, 2012). Our lower-bounds utilize a weak form of task relatedness, assuming that the
environment contains a ﬁnite set that is suitably separated in parameter space but close in KL
divergence—this set of assumptions also arises often when computing i.i.d minimax lower bounds.
One practical approach to meta-learning is learning a linear mapping on top of a learned feature
space. For example, Prototypical Networks (Snell et al., 2017) effectively learn a discriminative
embedding function and performs linear classiﬁcation on top using the novel task data. Analyzing
these approaches is challenging due to metric-learning inspired objectives (that require non-i.i.d
sampling) and the simultaneous learning of feature mappings and top-level linear functions yet some
progress has been made (Du et al., 2020; Jin et al., 2009; Saunshi et al., 2019; Wang et al., 2019).
Maurer (2009), for example, explores linear models ﬁtted over a shared linear feature map in a
Hilbert space. Our results can be applied in these settings if a suitable packing of the representation
space is deﬁned.
Other approaches to meta-learning aim to parameterize learning algorithms themselves. Tradi-
tionally, this has been achieved by hyper-parameter tuning (MacKay et al., 2019; Rasmussen and
Nickisch, 2010) but recent fully parameterized optimizers also show promising performance in deep
neural network optimization (Andrychowicz et al., 2016), few-shot learning (Ravi and Larochelle,
2016), unsupervised learning (Metz et al., 2019), and reinforcement learning (Duan et al., 2016). Yet
another approach learns the initialization of task-speciﬁc parameters, that are further adapted through
regular gradient descent. Model-Agnostic Meta-Learning (Finn et al., 2017), or MAML, augments
the global parameters with a meta-initialization of the weight parameters. Grant et al. (2018) recast
MAML in terms of inference in a Bayesian hierarchical model. In Section 5, we consider learning in
a hierarchical environment of linear models and provide both lower and upper bounds on the error of
estimating the parameters of a novel linear regression problem.
Lower bounding estimation error is a critical component of understanding learning problems
(and algorithms). Accordingly, there is a large body of literature producing such lower bounds
(Khas’ minskii, 1979; Loh, 2017; Yang and Barron, 1999). We focus on producing lower-bounds
for parameter estimation using local packing sets, but expect that extending these results to density
estimation or non-parametric estimation is feasible.
3. Novel task environment risk
Most existing theoretical work studying out-of-distribution generalization focuses on providing
upper-bounds on generalization performance (Amit and Meir, 2017; Ben-David et al., 2010; Pentina
3

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
and Lampert, 2014). We begin by instead exploring the converse: what is the best performance we
can hope to achieve on any given task in the environment? After introducing notation and minimax
risks, we then show how these ideas can be applied, using meta linear regression as an example.
A full reference table for notation can be found in Appendix A and a short summary is given
here. We consider algorithms that learn in an environment (Z, P), with data domain Z = X × Y
and P a space of distributions with support Z. In the typical i.i.d setting, the algorithm is provided
training data S ∈Zk, consisting of k i.i.d samples from P ∈P.
In the standard multi-task setting, we sample training data from a set of training tasks
{P1, . . . , PM+1} ⊂P. We extend this to a meta-learning, or novel-task setting by ﬁrst drawing
S1:M: n training data points from the ﬁrst M distributions, for a total of nM samples. We call this
the meta-training set. We then draw a small sample of novel data, called a support set, SM+1 ∈Zk,
from PM+1.
Consider a symmetric loss function ℓ(a, b) = ψ(ρ(a, b)) for non-decreasing ψ and arbitrary
metric ρ. We seek to estimate the output of θ : P →Ω, a functional that maps distributions to
a metric space Ω. For example, θ(P) may describe the coefﬁcient vector of a high-dimensional
hyperplane when P is a space of linear models, and ρ may be the Euclidean distance.
The i.i.d minimax risk
Before studying the meta-learning setting, we ﬁrst begin with a deﬁnition
of the i.i.d minimax risk that measures the worst-case error of the best possible estimator,
R∗= inf
ˆθ
sup
P∈P
ES∼P k
h
ℓ(ˆθ(S), θP )
i
.
(1)
For notational convenience, we denote the output of θ(P) by θP . The estimator for θ is denoted,
ˆθ : Zk →Ω, and maps k samples from P to an estimate of θP .
Novel-task minimax risk
In the novel-task setting, we wish to estimate θPM+1, the parameters of
the novel task distribution PM+1. We consider two-stage estimators for θPM+1. In the ﬁrst stage,
the meta-learner uses a learning algorithm f : S1:M 7→ˆθS1:M , that maps the meta-training set to an
estimation algorithm, ˆθS1:M : Zk →Ω. In the second stage, the learner computes ˆθS1:M (SM+1),
the estimate of θPM+1.
The novel-task minimax risk is given by,
R∗
P = inf
ˆθ
sup
P1,...,PM+1∈P
E S1:M∼P n
1:M
SM+1∼P k
M+1
h
ℓ(ˆθS1:M (SM+1), θPM+1)
i
(2)
The estimator for θM+1 now depends additionally on the Mn samples in S1:M, where only
k ≪Mn samples from PM+1 are available to the learner. This quantity addresses the domain shift
expected at test-time in the meta-learning setting and allows the learner to use data from multiple
tasks.
The goal of f in this setting is to learn an inductive bias from S1:M such that useful inferences
can be made with only k data points from the novel distribution, PM+1. In this setting, k is equivalent
to the number of shots in the few-shot learning setup.
4

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
P1
P2
1
0
1
P3
1
0
1
P4 (novel)
True fn
MAP
MLE
Samples
Figure 1: Meta-learning 1D-regression: The parameters of a 1D regression model are ﬁtted from a
small support set. The training distributions (P1, P2, P3) give a useful inductive bias for ﬁtting P4
using only 5 points. The MLE solution on the novel task for those 5 points is also displayed.
An example with meta-linear regression
We present here a short summary based on meta linear
regression, which we will analyze in more detail in Section 5.
In Figure 1, we show observed data samples from a family of polynomial regression models.
Our aim is to output an algorithm which recovers the parameters of a new polynomial function from
limited observations–we choose a MAP estimator which is described fully in Section 5. In the bottom
right, we are given only 5 data points from a novel task distribution and estimate the parameters of
the model with both the MLE and MAP estimators — the MLE overﬁts the support set while the
MAP estimator is close to the true function.
In terms of the terminology used above, the set,
P = {pθ(y) = N(x⊤θ, σ2) : θ ∈Rd, x = [1, x, . . . , xd−1]},
is the space of polynomial regression models, parameterized by θ. For this problem, we take
ℓ(ˆθ, θ) = ∥ˆθ −θ∥2
2. The tasks are generated with p(θ) = N(τ, σ2
θ), for unknown, sparse, τ ∈Rd.
Thus, each model is a polynomial function with few large coefﬁcients. The algorithm f, ﬁrst
takes samples from P1, P2, P3 and computes an estimate, ˆτ. This estimate of τ is then used to
compute ˆθ(SM+1; ˆτ) = argmaxθ4 p(θ4|ˆτ, SM+1). Note that this approach is able to learn the
correct inductive bias from the data, without requiring a carefully designed regularizer. The lower
bounds we derive in Section 4 can be applied to problems of this general type, and the upper and
lower bounds in Section 5 apply speciﬁcally to this setting.
5

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
4. Information theoretic lower bounds on novel task generalization
In this section, we ﬁrst present our most general result: Theorem 1. Using this, we derive Corollary 2
that gives a lower bound in terms of the sample size in the training and novel tasks. Corollary 2
recovers the well-known i.i.d lower bound (Theorem 4) when Mn = 0, and, importantly, highlights
that the novel task data is signiﬁcantly more valuable than the training task data. Additionally, we
provide a specialized bound that applies when the environment is partially observed — proving that
in this setting training task data is insufﬁcient to drive the minimax risk to zero.
In Theorem 1, we assume that P contains J distinct 2δ-separated distributions but the learner
observes data from only M + 1 ≤J of them. Intuitively, the learner observes the environment and
their error rate lower-bound shrinks as the amount of information shared between the training tasks
and the novel task grows. All proofs are given in Appendix B.1. Recall ℓ(a, b) = ψ(ρ(a, b)) for
non-decreasing ψ and arbitrary metric ρ.
Theorem 1 (Minimax novel task risk lower bound) Let J ⊂P contain J distinct distributions
such that ρ(θP , θP ′) ≥2δ for all P, P ′ ∈J . Let π be a random ordering of the J elements, and
Z|π be a vector of k i.i.d samples from PπM+1. Further, deﬁne W|π to be an n × M matrix whose
jth column consist of n i.i.d samples from Pπj. Then,
R∗
P ≥ψ(δ)

1 −I(πM+1; W) + I(πM+1; Z) + 1
log2 J

.
To derive this result, we bound the statistical estimation error by the error on a corresponding
decoding problem where we must predict the novel task index, given the meta-training set S1:M and
SM+1. Fano’s inequality provides best-case error probabilities for this problem.
Using Theorem 1, we derive our ﬁrst bound on the novel-task minimax risk that depends on
the number of training tasks and datapoints per training task. The following corollary implies that
if J of the previous meta-training tasks are close (in terms of their pairwise KL distance), then
learning a novel task from training samples drawn from the meta-training tasks requires signiﬁcantly
more examples; in particular, learning the novel task from samples drawn from the meta training set
requires Ω(J) times the sample complexity of the novel task. This matches our intuition that learning
the novel task implies the ability to distinguish it from all J well-separated meta-training tasks.
Corollary 2 Assume the same setting as in Theorem 1 and additionally that DKL (Pi∥Pj) ≤α for
all Pi, Pj ∈J , i ̸= j. Then,
R∗
P ≥ψ(δ)
 
1 −
1 + ( Mn
J−1 + k)α
log2 J
!
.
Typically, practical instances of this bound require ψ(δ) = O(1/k) or similar, as in Theorem 5
below.
A tighter bound on partially observed environments
We now consider the special case of
Theorem 1 when M < J −1, meaning that the meta-training tasks cannot cover the full environment.
In this setting, we prove that no algorithm can generalize perfectly to tasks in unseen regions of the
space with small k, regardless of the number of data points n observed in each meta-training task.
6

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
Corollary 3 Consider the setting given in Corollary 2, with M + 1 < J. Then,
R∗
P ≥ψ(δ)
log2(J −M) −kα −1
log2 J

.
In this work, we have focused on the setting where W contains an equal number of samples from
each of the meta-training tasks — this is the sampling scheme shown in Figure 2. However, it is
possible to extend these results to different sampling schemes for W. For example, in the appendix
we derive bounds with W|π as a mixture distribution. Surprisingly, despite the task identity being
hidden from the learner, the asymptotic rate for these two sampling schemes match.
4.1. Measuring Task-Relatedness
The use of local packing requires the design of an appropriate set of distributions whose corresponding
parameters are 2δ-separated but maintain small KL divergences. In the multi-task setting such an
assumption is intuitively reasonable: challenging tasks should require separated parameters for ideal
explanations (2δ-separated) but should satisfy some relatedness measure (small KL). As we will
see shortly, lower bounds on minimax risk in the i.i.d setting may also assume the same notion of
relatedness between the distributions in P.
Task relatedness is a necessary feature for upper-bounds on novel task generalization, but is
typically difﬁcult to deﬁne (see e.g. Ben-David and Borbely (2008)). Our lower bounds utilize a
relatively weak notion of task-relatedness, and thus may be overly pessimistic compared to the upper
bounds computed in existing work. Note however that task relatedness of the form utilized here
can be formulated in a representation space shared across tasks and thus can be applied in settings
like those explored by e.g. Du et al. (2020). Deriving lower bounds under other task relatedness
assumptions present in the literature would make for exciting future work.
4.2. Comparison to risk of i.i.d learners
From the statement of Theorem 1 it is not clear how this lower-bound compares to that of the i.i.d
learner which has access only to the k samples from SM+1. To investigate the beneﬁt of additional
meta-training tasks, we compare our derived minimax risk lower bounds to those achieved by i.i.d
learners. To do so, we revisit a standard result on minimax lower bounds that can be found in e.g.
Loh (2017).
Theorem 4 (IID minimax lower-bound) Suppose {P1, . . . , PJ} ⊆P satisfy ρ(θPi, θPj) ≥2δ for
all i ̸= j. Additionally assume that DKL (Pi∥Pj) ≤α, for all pairs i and j, then,
R∗≥ψ(δ)

1 −kα + 1
log2 J

.
We include a standard proof of this result in Appendix B.1. As hoped, Corollary 2 recovers
Theorem 4 when there are no training tasks available. Moreover, this i.i.d bound is strictly larger than
the one computed in Corollary 2 in general. Note that while this i.i.d minimax risk is asymptotically
7

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
tight for several learning problems (Loh, 2017; Raskutti et al., 2011), there is no immediate guarantee
that the same is true for our meta-learning minimax bounds. We investigate the quality of these
bounds by providing comparable upper bounds in the next section.
5. Analysis of a hierarchical Bayesian model of meta-learning
Our goal is to analyze the sample complexity of learning in a simple yet popular and practical model,
where samples are drawn from multiple meta-training tasks and we want to generalize to a new task
with only a few data points. After introducing the model, we will compute lower-bounds on the
minimax risk using our results from Section 4, revealing a 2d scaling on the meta-training sample
complexity. Following the lower bound, we derive an accompanying upper-bound on the risk of a
MAP estimator. Asymptotic analysis of this bound reveals that if the observed samples from the
novel task vary considerably more than the task parameters, then observing more meta-training
samples may signiﬁcantly improve convergence in the small k regime.
For i = 1...M + 1, where M + 1 is the total
number of tasks, we deﬁne,
yi = Xiθi + ϵi,
Xi ∈Rni×d, yi ∈Rni, ϵi ∈Rni
ϵi ∼N(0, σ2
i I),
σ2
i ∈R+
θi = τ + ξ,
τ ∈Rd, ξ ∈Rd
ξ ∼N(0, σ2
θI),
σ2
θ ∈R+
Each task has some design matrix Xi and un-
known parameters θi. For simplicity, we assume
known isotropic noise models and that ni = n
for all i ≤M, with nM+1 = k.
τ
θ1
θ2
y(1)
1
y(n)
1
y(1)
2
y(k)
2
...
...
Figure 2: A simple two-task hierarchical model.
We will consider the Maximum a Posterior estimator,
ˆθM+1 = argmax
θM+1
p(θM+1|y1, . . . , yM+1),
and will characterize its risk, E[∥ˆθM+1 −θM+1∥2
2].
We can derive the posterior distribution under the Empirical Bayes estimate for τ, which we
defer to Appendix C.2. The derivation is standard but dense and we recommend dedicated readers to
consult Gelman et al. (2013), or an equivalent text, for more details.
5.1. Minimax lower bounds
We now compute lower bounds for parameter estimation with meta-learning over multiple linear
regression tasks. Beginning with a deﬁnition of the space of data generating distributions,
PLR = {pθ(y) = N(Xθ, σ2I) : θ ∈B2(1), X ∈Rn×d},
8

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
where θ are the parameters to be learned, and X is the design matrix of each linear regression
task in the environment. We write γ = maxi σmax(Xi/√n), which we assume is bounded for all X
and n (an assumption that is validated for random Gaussian matrices by Raskutti et al. (2011)).
Theorem 5 (Meta linear regression lower bound) Consider PLR deﬁned as above and let ℓ(a, b) =
(∥a −b∥2)2. If d > 2, then,
R∗
PLR ≥O

dσ2
γ2(2−dnM + k)

The proof is given in Appendix B.5. We see that the size of the meta-training set has an
inverse exponential scaling in the dimension, d. This reﬂects the complexity of the space growing
exponentially in dimensions and the need for a matching growth in data size to cover the environment
sufﬁciently.
5.2. Minimax upper bounds
To compute upper bounds on the estimation error, we require an additional assumption. Namely,
we will assume that the design matrices also have bounded minimum singular values, 0 < s ≤
σmin(X/√n) (see Raskutti et al. (2011) for some justiﬁcation). For the upper-bounds, we allow the
bounds on the singular values of the design matrices and the observation noise in the novel task to be
different than those in the meta-training tasks. We note that we can still recover the setting assumed
in the lower bounds, where all tasks match on these parameters, as a special case.
We consider the setting where the learner observes n data points from each linear regression
model in {Pθ1, . . . , PθM } ⊂P. We then bound the error of estimating the parameters of some new
model, PθM+1, of which k samples are available.
The expected error rate of the MAP estimator can be decomposed as the posterior variance and
bias squared. In the appendix we provide a detailed derivation of these results. The bound depends
on dimensionality d, the observation noise in each task σ2
i , the number of tasks M, the number of
data points in each meta-training task n, and the number of data points in the novel task k.
Theorem 6 (Meta Linear Regression Upper Bound) Let ˆθM+1 be the maximum-a-posteriori es-
timator, µθM+1|Y1:M+1. Then,
R∗
PLR ≤
sup
θ1,...,θM+1∈B2(1)
E[∥ˆθM+1 −θM+1∥2] ≤O

dσ2
M+1C(M, n, k)−2D(M, n, k)

where,
C(M, n, k) =

k +
Mn
n(M+κ2)s2
2
α2
+ A

, and, D(M, n, k) =
"
k +
Mn
( n
L1 + A1)(Mn
L2 + A2)
#
.
Expectations are taken over the data conditioned on θ1, . . . , θM+1. Additional terms not depending
on d, M, n, k are deﬁned in Appendix C.2.
9

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
While the bounds presented in Theorem 6 are relatively complicated, we can probe the asymptotic
convergence of the MAP estimator to the true task parameters, θM+1. In the following section, we
will discuss some of the consequences of this result and its implications for our lower bounds.
5.3. Asymptotic behavior of the MAP estimator
We ﬁrst notice that when k (the number of novel task examples) is small, the risk cannot be reduced
to zero by adding more meta-training data. Recent work has suggested such a relationship may be
inevitable (Hanneke and Kpotufe, 2020). Our lower bound presented in Corollary 3 agrees that more
samples from a small number of meta-training tasks will not reduce the error to zero. However,
unlike our lower bounds based on local packing, the lower bounds presented in this section predict
that if the meta-training tasks cover the space sufﬁciently then an optimal algorithm might hope to
reduce the error entirely with enough samples. We hypothesize that this gap is due to limitations in
the standard proof techniques we utilize for the lower-bounds when the number of tasks grows, and
expect a sharper bound may be possible.
To emulate the few-shot learning setting where k is relatively small, we consider n →∞, with k
and M ﬁxed. In this case, the risk is bounded as,
sup
θ1,...,θM+1∈B2(1)
E[∥ˆθM+1 −θM+1∥2] ≤O
 
dσ2
M+1

k + 2α2M
M + κ2
−1!
,
where α2 = σ2
M+1/σ2
θ, is the ratio of the observation noise to the variance in sampling θ, and κ is
the condition number of the design matrices. This leads to a key takeaway: if the observed samples
from PM+1 vary considerably more than the parameters θ, then observing more samples in S1:M
will signiﬁcantly improve convergence towards the true parameters in the small k regime. Further,
adding more tasks (increasing M) also improves these constant factors by removing the dependence
on the condition number, κ.
6. Empirical Investigations
In this section, we provide additional quantitative exploration of the upper bound studied in Section 5.
The aim is to take steps towards relating the bounds to experimental results; we know of little
theoretical work in meta-learning that attempt to relate their results to practical empirical datasets.
6.1. Hierarchical Bayes polynomial regression
We ﬁrst focus on the setting of polynomial regression over inputs in the range [−1, 1]. Some examples
of these functions and samples are presented in Figure 1, alongside the MAP and MLE estimates for
the novel task. Full details of the data used can be found in Appendix D.
Figure 3 shows the analytical expected error rate (risk) under various environment settings. We
observe that even in this simple hierarchical model, the estimator exhibits complex behavior that is
correctly predicted by Theorem 6. In Figure 3A, we varied the novel task difﬁculty by increasing
the novel task observation noise (σ2
M+1). We plot three curves for three different dataset size
10

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
10
8
10
6
10
4
10
2
10
0
10
2
Novel task noise
10
6
10
4
10
2
10
0
10
2
10
4
Risk
Expected error over varying novel task difficulty
(M, n, k) = (50, 20, 5)
(M, n, k) = (50, 20, 1000)
(M, n, k) = (50, 200, 5)
(A)
0
2000
4000
6000
8000
10000
12000
14000
Total data samples (Mn + k)
10
2
10
1
10
0
10
1
10
2
10
3
10
4
Risk
Expected error over varying dataset sizes
M = 2, n = 20, k
[20, 8000]
k = 20, n = 20, M
[2, 800]
(B)
Figure 3: The expected error rate of the hierarchical MAP estimator, ˆθM+1, over different environ-
ment hyperparameter settings. A) The novel task observation noise is increased, making the novel
task harder to learn. B) We increase the size of the dataset, in one case adding new tasks (M) and in
the other adding new novel task data samples (k).
conﬁgurations. When the novel task is much noisier than the source tasks, it is greatly beneﬁcial to
add more meta-training data (blue vs. red). And while larger k made little difference when the novel
task was relatively difﬁcult (blue vs. green), the expected loss was orders of magnitude lower when
the novel task became easier. In Figure 3B, we ﬁxed the relative task difﬁculty and instead varied k
and M. The x-axis now indicates the total data Mn + k available to the learner. We observed that
adding more tasks has a large effect in the low-data regime but, as predicted, the error has a non-zero
asymptotic lower-bound — eventually it is more beneﬁcial to add more novel-task data samples.
These empirical simulations verify that our theoretical analysis is predictive of the behavior
of this meta learning algorithm, as each of these observations can be inferred from Theorem 6.
While this model is simple, it captures key features of and provides insight into the more general
meta-learning problem.
6.2. Sinusoid regression with MAML
Following the connections between MAML and hierarchical Bayes explored by Grant et al. (2018),
we also explored regression on sinusoids using MAML. Our aim was to investigate how predictive
our linear theory is for this highly non-linear problem setting. As in Finn et al. (2017), we sample
sinusoid functions by placing a prior over the amplitude and phase. In other works (Finn et al.,
2017; Grant et al., 2018) the same prior is used for the training and testing stages. However, to
better measure generalization to novel tasks we use different prior distributions when training versus
evaluating the model. Full details of the experimental set-up and data sampling procedure can be
found in Appendix D.
We display the risk averaged over 30 trials in Figure 4. We varied the novel task difﬁculty by
increasing the observation noise in the novel task. We plot separate curves for different dataset
size conﬁgurations, and observe that the empirical results align fairly well with the results derived
by sampling the hierarchical model (Figure 3A). Adding more meta-training data (increasing n) is
beneﬁcial (green vs. yellow) and adding more test data-points (higher k) is also beneﬁcial (red vs.
green). Here however, these relationships did not interact with the task difﬁculty, as the wins for
11

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
increased meta-training and meta-testing data were consistent, until task noise prevents any setting
of the model from performing the task.
7. Conclusion
10 7
10 5
10 3
10 1
Novel tasks noise
10 2
10 1
100
Risk
(M,n,k)=(50,20,100)
(M,n,k)=(50,20,1000)
(M,n,k)=(50,200,1000)
Figure 4: Average risk for regressing sinusoid
functions with MAML.
Meta-learning algorithms identify the inductive
bias from source tasks and make models more
adaptive towards unseen novel distribution. In this
paper, we take initial steps towards characterizing
the difﬁculty of meta-learning and understanding
how these limitations present in practice. We have
derived both lower bounds and upper bounds on
the error of meta-learners, which are particularly
relevant in the few-shot learning setting where k
is small. Our bounds capture key features of the
meta-learning problem, such as the effect of in-
creasing the number of shots or training tasks. We
have also identiﬁed a gap between our lower and
upper bounds when there are a large number of
training tasks, which we hypothesize is a limita-
tion of the proof technique that we applied to derive the lower bounds — suggesting an exciting
direction for future research.
8. Acknowledgements
This work beneﬁted greatly from the input of many other researchers. In particular, we extend our
thanks to Shai Ben-David, Karolina Dziugaite, Samory Kpotufe, and Daniel Roy for discussions
and feedback on the results presented in this work. We also thank Elliot Creager, Will Grathwohl,
Mufan Li, and many of our other colleagues at the Vector Institute for feedback that greatly improved
the presentation of this work. Resources used in preparing this research were provided, in part, by
the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the
Vector Institute (www.vectorinstitute.ai/partners).
References
Ron Amit and Ron Meir. Meta-learning by adjusting priors based on extended PAC-bayes theory.
arXiv preprint arXiv:1711.01244, 2017.
Marcin Andrychowicz, Misha Denil, Sergio Gomez Colmenarejo, Matthew W. Hoffman, David Pfau,
Tom Schaul, and Nando de Freitas. Learning to learn by gradient descent by gradient descent. In
Advances in Neural Information Processing Systems 29, pages 3981–3989, 2016.
Jonathan Baxter. A model of inductive bias learning. Journal of artiﬁcial intelligence research, 12:
149–198, 2000.
12

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
Shai Ben-David and Reba Schuller Borbely. A notion of task relatedness yielding provable multiple-
task learning guarantees. Machine learning, 73(3):273–287, 2008.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151–175, 2010.
Brian Bullins, Elad Hazan, Adam Kalai, and Roi Livni. Generalize across tasks: Efﬁcient algorithms
for linear representation learning. In Aur´elien Garivier and Satyen Kale, editors, Proceedings of
the 30th International Conference on Algorithmic Learning Theory, volume 98 of Proceedings of
Machine Learning Research, pages 235–246, Chicago, Illinois, 22–24 Mar 2019. PMLR. URL
http://proceedings.mlr.press/v98/bullins19a.html.
Tianshi Cao, Marc Law, and Sanja Fidler. A theoretical analysis of the number of shots in few-shot
learning. arXiv preprint arXiv:1909.11722, 2019.
Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.
Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil. Learning-to-learn stochas-
tic gradient descent with biased regularization. In Kamalika Chaudhuri and Ruslan Salakhutdinov,
editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of
Proceedings of Machine Learning Research, pages 1566–1575, Long Beach, California, USA,
09–15 Jun 2019. PMLR.
Simon S Du, Wei Hu, Sham M Kakade, Jason D Lee, and Qi Lei. Few-shot learning via learning the
representation, provably. arXiv preprint arXiv:2002.09434, 2020.
Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl$ˆ2$:
Fast reinforcement learning via slow reinforcement learning. CoRR, abs/1611.02779, 2016.
Robert Fano. Transmission of information. A Statistical Theory of Communication, 1961.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pages 1126–1135. JMLR. org, 2017.
Andrew Gelman, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin.
Bayesian data analysis. Chapman and Hall/CRC, 2013.
Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Grifﬁths. Recasting gradient-
based meta-learning as hierarchical Bayes. arXiv preprint arXiv:1801.08930, 2018.
Steve Hanneke and Samory Kpotufe. On the value of target data in transfer learning. In Advances in
Neural Information Processing Systems 32, pages 9871–9881. Curran Associates, Inc., 2019.
Steve Hanneke and Samory Kpotufe. A no-free-lunch theorem for multitask learning. arXiv preprint
arXiv:2006.15785, 2020.
Rong Jin, Shijun Wang, and Yang Zhou. Regularized distance metric learning:theory and algorithm.
In Advances in Neural Information Processing Systems 22, pages 862–870, 2009.
13

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
Rafail Z Khas’ minskii. A lower bound on the risks of non-parametric estimates of densities in the
uniform metric. Theory of Probability & Its Applications, 23(4):794–798, 1979.
Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Provable guarantees for gradient-based
meta-learning. arXiv preprint arXiv:1902.10644, 2019.
Samory Kpotufe and Guillaume Martinet. Marginal singularity, and the beneﬁts of labels in covariate-
shift. In S´ebastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, Proceedings of the
31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research,
pages 1882–1886. PMLR, 06–09 Jul 2018.
Po-Ling Loh. On lower bounds for statistical learning theory. Entropy, 19(11):617, 2017.
Matthew MacKay, Paul Vicol, Jonathan Lorraine, David Duvenaud, and Roger B. Grosse. Self-tuning
networks: Bilevel optimization of hyperparameters using structured best-response functions. In
7th International Conference on Learning Representations, 2019.
Andreas Maurer. Transfer bounds for linear feature learning. Machine learning, 75(3):327–350,
2009.
Luke Metz, Niru Maheswaranathan, Brian Cheung, and Jascha Sohl-Dickstein. Meta-learning update
rules for unsupervised representation learning. In 7th International Conference on Learning
Representations, 2019.
Mehryar Mohri and Andres Munoz Medina. New analysis and algorithm for learning with drifting
distributions. In International Conference on Algorithmic Learning Theory, pages 124–138.
Springer, 2012.
Anastasia Pentina and Christoph Lampert. A PAC-bayesian bound for lifelong learning. In Interna-
tional Conference on Machine Learning, pages 991–999, 2014.
Garvesh Raskutti, Martin J Wainwright, and Bin Yu.
Minimax rates of estimation for high-
dimensional linear regression over ℓq-balls. IEEE transactions on information theory, 57(10):
6976–6994, 2011.
Carl Edward Rasmussen and Hannes Nickisch. Gaussian processes for machine learning (GPML)
toolbox. J. Mach. Learn. Res., 11:3011–3015, 2010.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. International
Conference on Learning Representations, 2016.
Herbert Robbins. An empirical bayes approach to statistics. In Proceedings of the Third Berkeley
Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of
Statistics, pages 157–163, Berkeley, Calif., 1956. University of California Press.
Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar.
A theoretical analysis of contrastive unsupervised representation learning.
In International
Conference on Machine Learning, pages 5628–5637, 2019.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
Advances in Neural Information Processing Systems, pages 4077–4087, 2017.
14

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
Ricardo Vilalta and Youssef Drissi. A perspective view and survey of meta-learning. Artiﬁcial
intelligence review, 18(2):77–95, 2002.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one
shot learning. In Advances in neural information processing systems, pages 3630–3638, 2016.
John Von Neumann. Some matrix-inequalities and metrization of matric space. 1937.
Boyu Wang, Hejia Zhang, Peng Liu, Zebang Shen, and Joelle Pineau. Multitask metric learning:
Theory and algorithm. In Kamalika Chaudhuri and Masashi Sugiyama, editors, Proceedings of
Machine Learning Research, volume 89 of Proceedings of Machine Learning Research, pages
3362–3371. PMLR, 16–18 Apr 2019.
Yuhong Yang and Andrew Barron. Information-theoretic determination of minimax rates of conver-
gence. Annals of Statistics, pages 1564–1599, 1999.
15

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
Appendix A. Notation
Description
X
The domain of the data, e.g. Rd
Y
The range of the data, e.g. R
Z
The product space X × Y
P
A collection of distributions over Z
J
A (ﬁnite) subset of distributions in P
P
An element of P
P k
The product distribution, whose samples correspond to k independent draws from P
P1:M
The product distribution, ΠM
i=1Pi, for Pi ∈P
¯P1:M
The mixture distribution, 1
M
PM
i=1 Pi, for Pi ∈P
Ω
A metric space, containing parameters for each distribution
θ
A functional, mapping distributions in P to parameters in Ω
ˆθ
An estimator ˆθ : Zn →F
S, SM+1
Denotes training datasets drawn i.i.d from some P ∈P.
Typically S = {z1, . . . , zn},SM+1 = {z′
1, . . . , z′
k}
SP
Denotes a meta-training set drawn i.i.d from P1:M
[N], for N ∈N Indicates the set {1, . . . , N}
Bp(r)
The p-norm ball of radius r, centered at 0.
Table 1: Summary of notation used in this manuscript
Appendix B. Lower Bound Proofs
We will make use of several standard results below, which we present here.
Lemma 7 Fano’s Inequality (Cover and Thomas, 2012; Fano, 1961) For any estimator ˆY of a
random variable Y such that Y →Z →ˆY forms a Markov chain, it holds that,
P( ˆY ̸= Y ) ≥H(Y |Z) −1
log2 |Y |
= H(Y ) −I(Y ; Z) −1
log2 |Y |
.
Lemma 8 Mutual information equality (Khas’ minskii, 1979) Consider random variables Z1, Z2, Y ,
then,
I(Y ; (Z1, Z2)) + I(Z1; Z2) = I(Z1; (Z2, Y )) + I(Z2; Y )
Lemma 9 Local packing lemma (Loh, 2017) Consider distributions P1, . . . , PJ ∈P. Let Y be a
random variable distributed uniformly on [J] and let Z|{Y = j} be a vector of k i.i.d samples from
Pj. Then,
I(Y ; Z) ≤k
J2
X
1≤i,j≤J
DKL (Pi∥Pj) .
We will require a novel local packing bound for the novel-task risk, which we present in Lemma 12.
16

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
B.1. IID Lower Bound
We ﬁrst prove the i.i.d result, which will serve as a guide for our novel lower bounds.
Theorem 4 (IID minimax lower-bound) Suppose {P1, . . . , PJ} ⊆P satisfy ρ(θPi, θPj) ≥2δ for
all i ̸= j. Additionally assume that DKL (Pi∥Pj) ≤α, for all pairs i and j, then,
R∗≥ψ(δ)

1 −kα + 1
log2 J

.
Proof First, notice that,
sup
P∈P
ES∼P k
h
ℓ(ˆθ(S), θP )
i
≥1
J
J
X
i=1
ES∼P k
i
h
ℓ(ˆθ(S), θPi)
i
.
Now deﬁne the decision rule,
f(S) = argmin
1≤j≤J
ρ(ˆθ(S), θPj)],
with ties broken arbitrarily. We proceed by bounding the expected loss. First, using Markov’s
inequality,
ES∼P k
i
h
ℓ(ˆθ(S), θPi)
i
≥ψ(δ)PS∼P k
i
h
ψ(ρ(ˆθ(S), θPi)) ≥ψ(δ)
i
,
= ψ(δ)PS∼P k
i
h
ρ(ˆθ(S), θPi) ≥δ
i
.
Next, consider the case ρ(ˆθ(S), θi)) < δ. Through the triangle inequality,
ρ(ˆθ(S), θPj) ≥ρ(θPi, θPj) −ρ(ˆθ(S), θPi)
≥2δ −δ > ρ(ˆθ(S), θPi)
Thus, the probability that the distance is less than δ is at least as large as the probability that the
estimator is correct.
ψ(δ)PS∼P k
i
h
ℓ(ˆθ(S), θPi) ≥ψ(δ)
i
≥ψ(δ)P(f(S) ̸= i).
Now, using Fano’s inequality with Y = πM+1, and ˆY = f(S) (and the corresponding Markov chain
πM+1 →S →f(S)), we have,
1
J
J
X
i=1
P(f(S) ̸= i) ≥log2 J −I(πM+1; Z) −1
log2 J
.
Combining the above inequalities with the Local Packing Lemma gives the ﬁnal result.
17

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
B.2. Proof of Theorem 1
Theorem 1 (Minimax novel task risk lower bound) Let J ⊂P contain J distinct distributions
such that ρ(θP , θP ′) ≥2δ for all P, P ′ ∈J . Let π be a random ordering of the J elements, and
Z|π be a vector of k i.i.d samples from PπM+1. Further, deﬁne W|π to be an n × M matrix whose
jth column consist of n i.i.d samples from Pπj. Then,
R∗
P ≥ψ(δ)

1 −I(πM+1; W) + I(πM+1; Z) + 1
log2 J

.
Proof As in the i.i.d case, we ﬁrst bound the supremum from below with an average,
sup
P ′
1,...,P ′
M+1∈P
ES1:M∼(P ′
1:M)n
SM+1∼(P ′
i)k
h
ℓ(ˆθS1:M (SM+1), θ(P ′
i))
i
≥1
J
J
X
i=1
1
 J−1
M

X
π|{πM+1=i}
Ew∼W|π
z∼Z|π
h
ℓ(ˆθw(z), θi)
i
,
where the inner sum is over all length M orderings, π with πM+1 = i.
As before, we consider the following estimator,
f(W, Z) = argmin
1≤j≤J
ρ(ˆθW (Z), θ(Pj))
Using Markov’s inequality, and then following the proof of Theorem 4, we have,
1
J
J
X
i=1
1
 J−1
M

X
π|{πM+1=i}
Ew∼W|π
z∼Z|π
h
ℓ(ˆθw(z), θi)
i
≥1
J
J
X
i=1
1
 J−1
M

X
π|{πM+1=i}
ψ(δ)P[f(W, Z) ̸= i|π]
= ψ(δ)P[f(W, Z) ̸= πM+1]
with the use of Fano’s inequality, we arrive at,
ψ(δ)

1 −I(πM+1; (W, Z)) + 1
log2 J

Conditioned on Y , each element of W and Z are independent but they are not identically distributed.
Thus, with the application of Lemma 8,
I(πM+1; (W, Z)) ≤I(πM+1; Z) + I(πM+1; W)
The result follows by combining these inequalities.
Remark
In the above proof of Theorem 1, we did not need to make use of the form of the
distribution of W|Y = i, only that the correct graph structure was observed. This grants us some
ﬂexibility, which we utilize later in Section B.4 to prove lower bounds for mixture distributions.
We now proceed with proofs of the following corollaries.
Corollary 10 Consider the setting given in Corollary 2, with M + 1 < J. Then,
R∗
P ≥ψ(δ)
log2(J −M) −kα −1
log2 J

.
18

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
Proof This result follows as an application of the data processing inequality. Notice that πM+1 →
π1:M →W forms a Markov chain. Thus,
I(πM+1; W) ≤I(πM+1; π1:M),
by the data processing inequality. We can compute I(πM+1; π1:M) in closed form:
I(πM+1; π1:M) = log
J
J −M .
The proof is completed by plugging in the i.i.d local packing bound alongside the above.
B.3. Local packing results
Lemma 11 (Meta-learning local packing) Consider the same setting as in Theorem 1, then
I(πM+1; W) ≤
Mn
J2(J −1)
X
1≤i,j≤J
DKL (Pi∥Pj)
Proof There are (J −1)!/(J −M −1)! orderings on the ﬁrst M indices, given the (M + 1)th. We
introduce the following notation,
¯P−i := (J −M −1)!
(J −1)!
X
π|πM+1=i
p(W|π)
¯P := 1
J
J
X
i=1
¯P−i
As in previous proofs, we notice that we can write,
¯P = 1
J
¯P−i + J −1
J
1
J −1
X
j̸=i
¯P−j
First, note that we can upper bound I(πM+1; W) ≤nI(πM+1; w), where w denotes a single row in
W. Further,
I(πM+1; w) = 1
J
J
X
i=1
E log
¯P−i
¯P
= 1
J
J
X
i=1
DKL

¯P−i∥1
J
¯P−i + J −1
J
1
J −1
X
j̸=i
¯P−j


≤1
J
J
X
i=1
1
J DKL
  ¯P−i∥¯P−i

+ J −1
J
DKL

¯P−i∥
1
J −1
X
j̸=i
¯P−j


≤
1
J(J −1)
X
1≤i̸=j≤J
DKL
  ¯P−i∥¯P−j

19

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
We will use the convexity of the KL divergence to upper bound this quantity. Each distribution ¯P−i
is an average over a random selection of index orderings.
When applying convexity, all pairs of selections that exactly match will lead to a KL divergence
of zero. There are the same number of these in each component of ¯P−i. Thus we care only about
selections that contain either j or i such that matching pairs of distributions exactly is not possible.
Further, we need only consider pairs of product distributions who differ only in a single, identical
position.
Each of the above described pairs of distributions has KL divergence equal to DKL (Pj∥Pi). We
conclude by counting the total number of orderings producing such pairs. First, there are M choices
for the index of Pj and Pi. Then, there are (J −2)!/(J −M −1)! total orderings of the remaining
M −1 elements. Thus, we have,
I(πM+1; w) ≤
1
J(J −1)
X
1≤i̸=j≤J
DKL
  ¯P−i∥¯P−j

≤
1
J(J −1)
(J −M −1)!
(J −1)!
M(J −2)!
(J −M −1)!
X
1≤i̸=j≤J
DKL (Pj∥Pi)
=
M
J(J −1)2
X
1≤i̸=j≤J
DKL (Pj∥Pi)
B.4. Bounds using mixture distributions
In this section we introduce tools to lower bound the minimax risk when the meta-training set
is sampled from a mixture over the meta-training tasks, ¯P1:M =
1
M
PM
i=1 Pi. We note ﬁrst that
Theorem 1 can be reproduced exactly when W ∼¯P1:M. Thus, we need only provide a local packing
bound for the mixture distribution. In Lemma 12 we provide such a lower bound for the special case
where M = J −1, so that data is sampled from a mixture over the entire environment.
Lemma 12 (Leave-one-task-out mixture local packing) Let J ⊂P contain J distinct distribu-
tions such that ρ(θP , θP ′) ≥2δ for all P, P ′ ∈J and let ¯P−i =
1
J−1
P
j̸=i Pj. Let π be a random
ordering of the J elements, and deﬁne W|π to be a vector of n i.i.d samples from ¯P−πM+1. Then,
I(πM+1; W) ≤
1
(J −1)J2
X
1≤i,j≤J
DKL (Pi∥Pj) .
Proof From Lemma 9 (and some simple arithmetic) we have,
I(πM+1; W) = 1
J
J
X
i=1
DKL
  ¯P−i∥¯P1:J

.
20

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
Note that by the deﬁnition of the mixture distribution,
¯P1:J = J −1
J
¯P−i + 1
J Pi.
Using the convexity of the KL divergence,
I(πM+1; W) = 1
J
J
X
i=1
DKL

¯P−i
J −1
J
¯P−i + 1
J Pi

≤1
J
J
X
i=1
J −1
J
DKL
  ¯P−i∥¯P−i

+ 1
J DKL
  ¯P−i∥Pi

= 1
J2
J
X
i=1
DKL
  ¯P−i∥Pi

= 1
J2
J
X
i=1
DKL


1
J −1
X
1≤j≤J,j̸=i
Pj
Pi


≤
1
(J −1)J2
J
X
i=1
X
1≤j≤J,j̸=i
DKL (Pj∥Pi)
=
1
(J −1)J2
X
1≤i,j≤J
DKL (Pi∥Pj) .
Noting for the last step that the KL is zero if and only if the distributions are the same almost
everywhere.
B.5. Proof of Hierarchical linear model lower bound
Recall that the space of distributions we consider is given by,
PLR = {pθ(y) = N(Xθ, σ2I) : θ ∈B2(1), X ∈Rn×d.}
Theorem 5 (Meta linear regression lower bound) Consider PLR deﬁned as above and let ℓ(a, b) =
(∥a −b∥2)2. If d > 2, then,
R∗
PLR ≥O

dσ2
γ2(2−dnM + k)

Proof The proof consists of two steps, we ﬁrst construct a 2δ-packing of PLR. Then, we upper
bound the KL divergence between two distributions in this packing and use Corollary 2 to give the
desired bound.
The maximal packing number J for the unit 2-norm ball can be bounded by the following,
1
δ
d
≤J ≤

1 + 2
δ
d
.
21

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
We use a common scaling trick. First, through this bound, we can build a packing set, V, with
packing radius 1/2, giving 2d ≤J ≤5d. We deﬁne a new packing set of the same cardinality by
taking θi = 4δvi for all vi ∈V. Giving for all i ̸= j,
∥θi −θj∥= 4δ∥vi −vj∥≥2δ
similarly, ∥θi −θj∥≤4δ.
We now proceed with bounding the KL divergences.
DKL (Pi∥Pj) =
1
2σ2 ∥Xiθi −Xjθj∥2
2
=
1
2σ2

θ⊤
i X⊤
i Xiθi + θ⊤
j X⊤
j Xjθj −2θ⊤
i X⊤
i Xjθj

≤
1
2σ2

niγ2
i ∥θi∥2 + njγ2
j ∥θj∥2 −2θ⊤
i X⊤
i Xjθj

where γ2
i = supθ
∥Xiθ∥
√ni∥θ∥. We write n = maxk nk and γ = maxk γk, then,
DKL (Pi∥Pj) ≤nγ2
2σ2

∥θi∥2 + ∥θj∥2 −
2
nγ2 θ⊤
i X⊤
i Xjθj

≤nγ2
2σ2
 ∥θi∥2 + ∥θj∥2 + 2∥θi∥∥θj∥

= nγ2
2σ2 (∥θi∥+ ∥θj∥)2 ≤32nγ2δ2
σ2
The second line is derived using the Cauchy-Schwarz inequality, and the ﬁnal inequality uses
∥θi∥= ∥4δvi∥≤4δ.
Now, using Corollary 2,
R∗
PLR ≥δ2

1 −(nM2−d + k)32γ2δ2/σ2 + 1
d

,
Choosing δ2 = dσ2/64γ2(2−dMn + k) gives,
1 −(nM2−d + k)32δ2/σ2 + 1
d
= 1 −d/2 + 1
d
≥1/4,
for d ≥2. Thus,
R∗
P ≥O

dσ2
γ2(2−dnM + k)

Appendix C. Hierarchical Bayesian Linear Regression Upper Bounds
C.1. Some useful linear algebra results
Let smax(A) denotes the maximum singular value of A; smin(A) denotes the minimum singular
value of A.
22

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
Lemma 13 Singular value of sum of two matrices Let A, B ∈Rm×n, then smax(A) + smax(B) ≥
smax(A + B). Furthermore, if A, B are positive deﬁnite, smin(A) + smin(B) ≤smin(A + B).
Proof The ﬁrst result follows immediately from the triangle inequality of the matrix norm ∥·∥2.
For the second result, suppose that A and B are positive deﬁnite.
smin(A + B) = inf
∥u∥=1∥(A + B)u∥
(3)
=
r
inf
∥u∥=1∥(A + B)u∥2
(4)
=
r
inf
∥u∥=1∥Au∥2 + ∥Bu∥2 + 2⟨Au, Bu⟩
(5)
=
r
inf
∥u∥=1∥Au∥2 + ∥Bu∥2 + 2u⊤A⊤Bu
(6)
(7)
Now, notice that A⊤B is similar to the matrix A1/2BA1/2, which exists as A is positive deﬁnite.
This matrix is itself positive deﬁnite, and thus has non-negative eigenvalues, meaning A⊤B also has
all positive eigenvalues. Thus, u⊤A⊤Bu ≥0, for all u, and,
smin(A + B) ≥
r
inf
∥u∥=1∥Au∥2 + ∥Bu∥2
(8)
≥
r
inf
∥u∥=1∥Au∥2 + inf
∥v∥=1∥Bv∥2
(9)
≥
p
smin2(A) + smin2(B)
(10)
≥smin(A) + smin(B)
(Concavity of √·)
(11)
Lemma 14 Singular value of product of two matrices Let A, B ∈Cn×n, then smax(A)smax(B) ≥
smax(AB), and, smin(A)smin(B) ≤smin(AB).
First we prove the maximum singular value.
23

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
Proof
smax(AB) = sup
∥v∥=1
√
v∗B∗A∗ABv
(12)
= sup
∥v∥=1
p
∥Bv∥2u∗A∗Au for u =
Bv
∥Bv∥,
(13)
≤
sup
∥v∥=1,∥u∥=1
p
∥Bv∥2u∗A∗Au
(14)
=
r
sup
∥v∥=1
∥Bv∥2 sup
∥u∥=1
∥Au∥2
(15)
=
p
smax2(B)smax2(A)
(16)
= smax(A)smax(B).
(17)
The minimum singular value follows a similar structure. Suppose AB is full rank,
smin(AB) = inf
∥v∥=1
√
v∗B∗A∗ABv
(18)
= inf
∥v∥=1
p
∥Bv∥2u∗A∗Au for u =
Bv
∥Bv∥,
(19)
≥
inf
∥v∥=1,∥u∥=1
p
∥Bv∥2u∗A∗Au
(20)
=
r
inf
∥v∥=1∥Bv∥2 inf
∥u∥=1∥Au∥2
(21)
=
p
smin2(B)smin2(A)
(22)
= smin(A)smin(B).
(23)
If AB is not full rank, then smin(AB) = smin(A)smin(B) = 0.
Lemma 15 (Von Neumann’s Trace Inequality (Von Neumann, 1937)) Given two n × n complex
matrices A, B, with singular vales a1 ≥. . . ≥an and b1 ≥. . . ≥bn respectively. We have,
|Tr(AB)| ≤
n
X
i=1
aibi
This is a classic result whose proof we exclude.
As a direct consequence of Lemma 15, |Tr(AB)| ≤na1b1.
24

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
C.2. Posterior estimate
For reference, we reproduce the posterior estimate for the true parameters θM+1. As a shorthand, we
write Y1:M+1 = (y1, . . . , yM+1).
p(τ|Y1:M) = N(µτ|Y1:M , Στ|Y1:M ),
(24)
Σ−1
τ|Y1:M =
M
X
i=1
X⊤
i (σ2
θXiX⊤
i + σ2
1I)−1Xi,
(25)
µτ|Y1:M = Στ|Y1:M
M
X
i=1
X⊤
i (σ2
θXiX⊤
i + σ2
1I)−1yi
(26)
p(θM+1|Y1:M, yM+1) = N(µθM+1|Y1:M+1, ΣθM+1|Y1:M+1),
(27)
Σθ+τ|Y1:M = σ2
θI + Στ|Y1:M
(28)
Σ−1
θM+1|Y1:M+1 = σ−2
M+1X⊤
M+1XM+1 + Σ−1
θ+τ|Y1:M
(29)
µθM+1|Y1:M+1 = ΣθM+1|Y1:M+1(σ−2
M+1X⊤
M+1yM+1 + Σ−1
θ+τ|Y1:M µτ|Y1:M )
(30)
C.3. Upper bound for meta linear regression
In this section we prove the main upper bound result of our paper, Theorem 6.
Theorem 6 (Meta Linear Regression Upper Bound) Let ˆθM+1 be the maximum-a-posteriori es-
timator, µθM+1|Y1:M+1. Then,
R∗
PLR ≤
sup
θ1,...,θM+1∈B2(1)
E[∥ˆθM+1 −θM+1∥2] ≤O

dσ2
M+1C(M, n, k)−2D(M, n, k)

where,
C(M, n, k) =

k +
Mn
n(M+κ2)s2
2
α2
+ A

, and, D(M, n, k) =
"
k +
Mn
( n
L1 + A1)(Mn
L2 + A2)
#
.
Expectations are taken over the data conditioned on θ1, . . . , θM+1. Additional terms not depending
on d, M, n, k are deﬁned in Appendix C.2.
Before proceeding with the proof, we introduce some additional notation and technical results.
Additional notation
To alleviate (only a little of) the notational clutter, we will deﬁne the following
quantities,
• Σ′ = ΣθM+1|Y1:M+1
• Σ′
0 = Σθ+τ|Y1:M .
25

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
• smin(X/√n) = s1
• smin(XM+1/
√
k) = s2
• smax(X/√n) = γ1 = κs1
• smax(XM+1/
√
k) = γ2 = κM+1s2
• α1 = σ2
1/σ2
θ
• α2 = σ2
M+1/σ2
θ
• L =
α2
(M+κ2)s2
2
• L1 =
α1
s2
1s2
2κ2
M+1
• L2 =
˜κκτα2
2s2
2κ2
M+1
• A = s2
2α1
s2
1α2
• A1 = s2
2κ2
M+1
• A2 =
α1s2
2κ2
M+1
κ2τs2
1α2
As we have uniform bounds on the singular values of all design matrices, we introduced an auxil-
lary matrix X whose largest and smallest singular values are given by √nγ1 and √ns1 respectively.
We will also write S(A) = Cov[A, A], and κ(A) = smax(A)/smin(a) throughout.
Bias-Variance Decomposition
As is standard, we can decompose the risk into the bias and vari-
ance of the estimator:
E[∥ˆθM+1 −θM+1∥2] = E[Tr((ˆθM+1 −θM+1)(ˆθM+1 −θM+1)⊤)]
(31)
= Tr(E[(ˆθM+1 −θM+1)(ˆθM+1 −θM+1)⊤])
(32)
= Tr(Cov[ˆθM+1, ˆθM+1]) + Tr(E[(ˆθM+1 −θM+1)]E[(ˆθM+1 −θM+1)]⊤)
(33)
In the next two sections, we will derive upper bounds on the bias and variance terms above.
C.3.1. VARIANCE TECHNICAL LEMMAS
We ﬁrst decompose the variance into contributions from two sources: the variance from data in the
novel task and the variance from data in the source tasks.
Lemma 16 (Variance decomposition) Let ˆθM+1 = µθM+1|Y1:M+1 as deﬁned above. Then the
variance of the estimator can be written as
Tr(S(ˆθM+1)) = Tr(Σ′σ−2
M+1X⊤
M+1XM+1Σ′) + Tr(S(Σ′Σ′
0
−1µτ|Y1:M ))
26

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
Proof
Tr(Cov[ˆθM+1, ˆθM+1]) = Tr(S(Σ′(σ−2
M+1X⊤
M+1yM+1 + Σ′
0
−1µτ|Y1:M )))
(34)
= Tr(S(Σ′σ−2
M+1X⊤
M+1yM+1) + S(Σ′Σ′
0
−1µτ|Y1:M ))
(35)
= Tr(Σ′σ−2
M+1X⊤
M+1S(yM+1)σ−2
M+1XM+1Σ′ + S(Σ′Σ′
0
−1µτ|Y1:M ))
(36)
= Tr(Σ′σ−2
M+1X⊤
M+1XM+1Σ′) + Tr(S(Σ′Σ′
0
−1µτ|Y1:M ))
(37)
We will now work towards a bound for each of the two variance terms in Lemma 16 separately.
To do so, we will need to produce bounds on the singular values of terms appearing in Lemma 16.
We begin with the covariance term Σ′.
Lemma 17 (Novel task covariance singular value bound) Let L, A and s2 be as deﬁned above.
Then,
smax(Σ′) ≤σ2
M+1
s2
2
"
k +
n
Mn
L + A
#−1
.
Proof Using Lemma 13, we can bound smax(Σ′) as follows,
smax(Σ′) = smax(ΣθM+1|Y1:M+1)
(38)
= smax((σ−2
M+1X⊤
M+1XM+1 + Σ−1
θ+τ|Y1:M )−1)
(39)
= 1/smin(σ−2
M+1X⊤
M+1XM+1 + Σ−1
θ+τ|Y1:M )
(40)
≤1/(smin(σ−2
M+1X⊤
M+1XM+1) + smin(Σ−1
θ+τ|Y1:M ))
(41)
= 1/(smin(σ−2
M+1X⊤
M+1XM+1) + 1/smax(Σθ+τ|Y1:M ))
(42)
Now, using the auxillary matrix X,
smax(Σ′) ≤
"
smin(σ−2
M+1X⊤
M+1XM+1) +
1
σ2
θ + 1
M smax((X⊤˜C−1X)−1)
#−1
(43)
=

smin(X⊤
M+1XM+1)
σ2
M+1
+
1
σ2
θ + 1
M
1
smin(X⊤˜C−1X)


−1
(44)
≤

ksmin2(XM+1/
√
k)
σ2
M+1
+
1
σ2
θ + 1
M
1
smin(X⊤˜C−1X)


−1
(45)
≤

ks2
2
σ2
M+1
+
1
σ2
θ + 1
M
1
smin(X⊤(Xσ2
θIX⊤+σ2I)−1X)


−1
(46)
27

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
Above we have used Lemma 13 repeatedly, alongside the standard identity, smax(A−1) =
smin(A)−1. We continue now, additionally using Lemma 14,
smax(Σ′) ≤

ks2
2
σ2
M+1
+
1
σ2
θ + 1
M
1
smin(X⊤X)smin((Xσ2
θIX⊤+σ2
1I)−1)


−1
(47)
=

ks2
2
σ2
M+1
+
1
σ2
θ + smax(Xσ2
θIX⊤+σ2
1I)
smin(X⊤X)


−1
(48)
≤

ks2
2
σ2
M+1
+
1
σ2
θ + 1
M
smax(Xσ2
θIX⊤)+σ2
1
ns2
1


−1
(49)
≤

ks2
2
σ2
M+1
+
1
σ2
θ + σ2
θsmax(XX⊤)+σ2
1
ns2
1


−1
(50)
=

ks2
2
σ2
M+1
+
1
σ2
θ + 1
M
σ2
θns2
1κ2+σ2
1
ns2
1


−1
(51)
= σ2
M+1

ks2
2 +
Mn
n(M+κ2)
α2
+
α1
s2
1α2


−1
(52)
= σ2
M+1
s2
2
"
k +
n
Mn
L + A
#−1
.
(53)
Next, we deal with terms appearing corresponding to the data from the source tasks.
Lemma 18 (Source tasks covariance singular value bound) Let ˜C1 = σ2
θXX⊤+ σ2
1I, and write
˜κ = κ( ˜C1) and κτ = κ(Στ|Y1:M ). Then,
smax2(Σ−1
θ+τ|Y1:M Στ|Y1:M ) ≤
1
2Mσ2
θns2
1
smax( ˜C1)κτ + 1
κ2τ
=: D1
and,
smax(σ2
1 ˜C−1
1 ) ≤
1
ns2
1
α1 + 1
=: D2
Proof Using Lemma 13 and Lemma 14 we have,
28

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
smax2(Σ−1
θ+τ|Y1:M Στ|Y1:M ) = smax2(Σ−1
θ+τ|Y1:M Στ|Y1:M )
(54)
≤smax2(Σ−1
θ+τ|Y1:M )smax2(Στ|Y1:M )
(55)
= smin−2(Σθ+τ|Y1:M )smax2(Στ|Y1:M )
(56)
= smin−2(σ2
θI + Στ|Y1:M )smax2(Στ|Y1:M )
(57)
≤
smax(Στ|Y1:M )2
(σ2
θ + smin(Στ|Y1:M ))2
(58)
Now, using σ2
θ > 0,
smax(Στ|Y1:M )2
(σ2
θ + smin(Στ|Y1:M ))2 ≤
smax(Στ|Y1:M )2
2σ2
θsmin(Στ|Y1:M ) + smin(Στ|Y1:M )2
(59)
≤
1
2σ2
θ
smax(Στ|Y1:M )κτ + 1
κ2τ
(60)
Introducing the auxillary matrix X and using Lemma 13 and Lemma 14 on Στ|Y1:M , we have
1
2σ2
θ
smax(Στ|Y1:M )κτ + 1
κ2τ
≤
1
2Mσ2
θsmin(X⊤˜C−1X)
κτ
+ 1
κ2τ
,
(61)
where,
smin(X⊤˜C−1
1 X) ≥smin(X⊤X)
smax( ˜C1)
(62)
=
ns2
1
smax( ˜C1)
.
(63)
This gives the ﬁrst stated inequality,
smax2(Σ−1
θ+τ|Y1:M Στ|Y1:M ) ≤
1
2Mσ2
θns2
1
smax( ˜C1)κτ + 1
κ2τ
=: D1
(64)
The second follows as,
smax(σ2
1 ˜C−1
1 ) =
σ2
1
smin(σ2
θXX⊤+ σ2
1I)
(65)
≤
σ2
1
smin(σ2
θXX⊤) + smin(σ2
1I)
(66)
≤
σ2
1
ns2
1σ2
θ + σ2
1
(67)
=
1
ns2
1
α1 + 1
=: D2
(68)
29

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
In Lemma 18, we introduced additional condition numbers, which we can bound as follows,
˜κ = κ( ˜C1) = κ(σ2
θXX⊤+ σ2
1I) ≤κ(σ2
θXX⊤) ≤κ(σ2
θXX⊤)κ(σ2
θI) = κ2,
(69)
κτ = κ(Στ|Y1:M ) ≤κ(X⊤X)κ( ˜C1) = κ2˜κ ≤κ4.
(70)
C.3.2. VARIANCE UPPER BOUND
We are now ready to put the above technical results together to achieve a bound on the variance of
the estimator.
Lemma 19 (Variance bound)
Tr(S(ˆθM+1)) ≤κ2
M+1σ2
M+1
s2
2
d

k +
n
n
L + A
−2 "
k +
Mn
( n
L1 + A1)(Mn
L2 + A2)
#
Proof First, by Lemma 16 we can decompose the overall variance into two terms:
Tr(S(ˆθM+1)) = Tr(Σ′σ−2
M+1X⊤
M+1XM+1Σ′) + Tr(S(Σ′Σ′
0
−1µτ|Y1:M ))
We deal with the left term ﬁrst.
Using trace permutation invariance and the von Neumann trace inequality (Lemma 15). We can
upper bound the left variance term as follows,
Tr(Σ′σ−2
M+1X⊤
M+1XM+1Σ′) = σ−2
M+1Tr(Σ′Σ′X⊤
M+1XM+1)
(71)
≤dkσ−2
M+1smax(Σ′)2smax2(XM+1/
√
k)
(72)
= dkσ−2
M+1smax(Σ′)2s2
2κ2
M+1
(73)
For the second variance term, we observe that,
Tr(Σ′Σ′
0
−1S(µτ|Y1:M )Σ′
0
−1Σ′)
(74)
= Tr(Σ′Σ′
0
−1S(Στ|Y1:M
M
X
i=1
X⊤
i (σ2
θXiX⊤
i + σ2
1I)−1yi)Σ′
0
−1Σ′)
(75)
≤MTr(Σ′Σ′
0
−1Στ|Y1:M X⊤˜C−1S(y1) ˜C−1XΣτ|Y1:M Σ′
0
−1Σ′)
(76)
= MTr(Σ′Σ′
0
−1Στ|Y1:M X⊤˜C−1
1 σ2
1I ˜C−1
1 XΣτ|Y1:M Σ′
0
−1Σ′)
(77)
≤Msmax(Σ′)2smax2(Σ−1
θ+τ|Y1:M Στ|Y1:M )σ2
1Tr(X⊤˜C−1 ˜C−1X)
(78)
30

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
Using Lemma 18, we have,
Tr(Σ′Σ′
0
−1S(µτ|Y1:M )Σ′
0
−1Σ′) ≤smax(Σ′)2MD1D2Tr(X⊤X)smax( ˜C−1
1 )
(79)
≤smax(Σ′)2MD1D2 min(n, d)nsmax( ˜C−1
1 )
(80)
≤smax(Σ′)2D2
M min(n, d)n
2Mσ2
θns2
1smin( ˜C1)
smax( ˜C1)κτ
+ smin( ˜C1)
κ2τ
(81)
≤smax(Σ′)2D2
M min(n, d)n
2Mσ2
θns2
1smin( ˜C1)
smax( ˜C1)κτ
+ σ2
1
κ2τ
(82)
≤smax(Σ′)2D2
min(n, d)n
σ2
M+1
M
2Mn
˜κκτα2 +
α1
κ2τσ2
1
2α2
(83)
≤smax(Σ′)2
σ2
M+1
nd
ns2
1
α1 + 1
M
2Mn
˜κκτα2 +
α1
κ2τs2
1α2
(84)
Finally, rearranging and using Lemma 17, we can bound the sum of the two terms in the variance as
follows,
Tr(S(ˆθM+1)) ≤smax(Σ′)2
σ2
M+1
(kds2
2κ2
M+1 +
nd
ns2
1
α1 + 1
M
2Mn
˜κκτα2 +
α1
κ2τs2
1α2
)
(85)
≤smax(Σ′)2s2
2κ2
M+1
σ2
M+1
(kd +
nd
ns2
1s2
2κ2
M+1
α1
+ s2
2κ2
M+1
M
2Mns2
2κ2
M+1
˜κκτα2
+
α1s2
2κ2
M+1
κ2τs2
1α2
) (86)
≤κ2
M+1σ2
M+1
s2
2

k +
n
n
L + A
−2

kd +
Mnd
(
ns2
1s2
2κ2
M+1
α1
+ s2
2κ2
M+1)(
2Mns2
2κ2
M+1
˜κκτα2
+
α1s2
2κ2
M+1
κ2τs2
1α2 )


(87)
≤κ2
M+1σ2
M+1
s2
2
d

k +
n
n
L + A
−2 "
k +
Mn
( n
L1 + A1)(Mn
L2 + A2)
#
(88)
C.3.3. BOUNDING THE BIAS
Lemma 20 (Bias upper bound) Given θ1, . . . , θM+1 ∈B2(1), we have,
E[(ˆθM+1 −θM+1)] ≤O

d

k +
Mn
n(M+κ2)s2
2
α2
+ A


−2

31

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
Proof The bias can be computed as follows,
E[(ˆθM+1 −θM+1)] = EµθM+1|Y1:M+1 −θM+1
(89)
= ΣθM+1|Y1:M+1E(σ−2
M+1X⊤
M+1y2 + Σ−1
θ+τ|Y1:M µτ|Y1:M ) −θM+1
(90)
= ΣθM+1|Y1:M+1(σ−2
M+1X⊤
M+1XM+1θM+1 + Σ−1
θ+τ|Y1:M Eµτ|Y1:M ) −θM+1
(91)
= (σ−2
M+1X⊤
M+1XM+1 + Σ−1
θ+τ|Y1:M )−1(σ−2
M+1X⊤
M+1XM+1θM+1 + Σ−1
θ+τ|Y1:M Eµτ|Y1:M ) −θM+1
(92)
= (F + G)−1(FθM+1 + Gµτ|Y1:M ) −θM+1
(93)
= (F + G)−1FθM+1 −(F + G)−1(F + G)θM+1 + (F + G)−1GEµτ|Y1:M
(94)
= (F + G)−1G(Eµτ|Y1:M −θM+1),
(95)
where we wrote F = σ−2
M+1X⊤
M+1XM+1, and G = Σ−1
θ+τ|Y1:M . Thus,
E[(ˆθM+1 −θM+1)]⊤E[(ˆθM+1 −θM+1)] ≤∥(F + G)−1∥2
2∥G∥2
2∥Eµτ|Y1:M −θM+1∥2
2
We can bound each term in turn. First, note that ∥G∥2
2 ≤1/σ2
θ, and we have bounded ∥(F + G)−1∥2
2
above. We can write,
∥Eµτ|Y1:M −θM+1∥2
2 =
Στ|Y1:M
 M
X
i=1
X⊤
i ˜C−1
i
Xiθi
!
−θM+1

2
2
∥Eµτ|Y1:M −θM+1∥2
2 =
Στ|Y1:M
 M
X
i=1
X⊤
i ˜C−1
i
Xiθi
!
−Στ|Y1:M Σ−1
τ|Y1:M θM+1

2
2
=
Στ|Y1:M
M
X
i=1
X⊤
i ˜C−1
i
Xi(θi −θM+1)

2
2
≤
 M
X
i=1
∥Στ|Y1:M ∥2 ∥X⊤
i ˜C−1
i
Xi(θi −θM+1)∥2
!2
≤
 M
X
i=1
∥Στ|Y1:M ∥2 ∥X⊤
i ˜C−1
i
Xi∥2
!2
The last line follows from the fact that the parameters lie in a ball of unit radius. We now proceed by
bounding the sum by M times the supremum — with some light abuse of notation,
∥µτ|Y1:M −θM+1∥2
2 ≤(smax(X⊤˜C−1X)smax(X ˜C−1X))2
= smax(X⊤˜C−1X)4 ≤O(1)
32

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
Thus, overall the convergence of the bias is bounded by,
E[(ˆθM+1 −θM+1)]⊤E[(ˆθM+1 −θM+1)] ≤O(smax(Σ′)2) ≤O

d

k +
Mn
n(M+κ2)s2
2
α2
+ A


−2

The proof of Theorem 6 is given by the combination of Lemma 19 and Lemma 20, and the
bias-variance decomposition of the risk .
Appendix D. Additional Experiment Details
D.1. Hierarchical Bayes Evaluation
We sample M linear models according to the hierarchical model in Section 5, with design matrices
constructed by uniformly sampling points, x ∼U[−1, 1], and storing the vector xj = xj, for
i = 0, . . . , d in each row of Xi.
To produce the plots in Figure 3 we computed the average loss over 100 random draws of the
training data and labels from the same set of ﬁxed θ1:M+1 values. The θ values were sampled once
from the hierarchical model with τ = [0, 1, 2, 0, 0, 3, 1], and σ2
θ = 0.1
D.2. Sinusoid Regression with MAML
Hyper parameters
Description
σ
noise at test time.
M
number of tasks at the training tasks
Mq
number of tasks at the testing tasks
eps per batch
episode per batch
train ampl range
range of amplitude at training
train phase range
range of phase at training
val ampl range
range of amplitude at testing
val phase range
range of phase at testing
inner steps
number of steps of Maml
inner lr
learning rate used to optimize parameter of the model
meta lr
used to optimize parameter of the meta-learner
n
number of datapoints at training tasks(support set)
k
number of datapoints at testing tasks (support set)
nq
number of datapoints at training tasks (query set) .
kq
number of datapoints at testing tasks (query set).
For all of these experiments we used a fully connected network with 6 layers and 40 hidden units per
layer. The network is trained using the MAML algorithm (Finn et al., 2017) with 5 inner steps using
33

J. LUCAS, M. REN, I. KAMENI, T. PITASSI & R. ZEMEL.
SGD with an inner learning rate of 10−3. We used Adam for the outer loop learning with a learning
rate of 10−3.
The expected error was computed after 500 epochs of optimization and was averaged over 30
runs. We produced our results through a comprehensive grid search over 72 combinations of the
settings below and it required around 30 minutes to produce the output of each setting, using a system
with 1 gpu and 3 cpus. This experiment therefore lasted 20 hours in total.
M = 50, n ∈{20, 200}, k ∈{100, 1000}, σ ∈[10−8, 1.5], Mq = 100, eps per batch = 25, train ampl range =
[1, 4], train phase range = [0, π/2], val ampl range = [3, 5], val phase range = [0, π/2], inner steps =
5, inner lr = 10−3, meta lr = 10−3
34

