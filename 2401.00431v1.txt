Wild2Avatar: Rendering Humans Behind Occlusions
Tiange Xiang1*, Adam Sun1, Scott Delp1, Kazuki Kozuka2, Li Fei-Fei1, Ehsan Adeli1*
1Stanford University
2Panasonic
https://cs.stanford.edu/˜xtiange/projects/wild2avatar/
Figure 1. In this paper, we introduce Wild2Avatar, a method to render high fidelity human avatars from in-the-wild monocular videos
behind occlusions. To achieve photo-realistic rendering, we parameterize the scene into three parts: occlusion→human→background and
decouple the renderings through novel optimization objectives. Faces in the figure are blurred for anonymity.
Abstract
Rendering the visual appearance of moving hu-
mans from occluded monocular videos is a challenging
task. Most existing research renders 3D humans un-
der ideal conditions, requiring a clear and unobstructed
scene. Those methods cannot be used to render hu-
mans in real-world scenes where obstacles may block
the camera’s view and lead to partial occlusions. In
this work, we present Wild2Avatar, a neural rendering
approach catered for occluded in-the-wild monocular
videos. We propose occlusion-aware scene parameter-
ization for decoupling the scene into three parts - oc-
clusion, human, and background. Additionally, exten-
sive objective functions are designed to help enforce
the decoupling of the human from both the occlusion
and the background and to ensure the completeness of
the human model. We verify the effectiveness of our
approach with experiments on in-the-wild videos.
1. Introduction
Rendering humans from videos has a wide variety of ap-
plications in many fields, including AR/VR [9, 12], film
[3], and healthcare [6]. Videos from single cameras are
widespread and easy to acquire, so rendering humans from
monocular videos has been the target of copious research.
Methods to accomplish this task such as Vid2Avatar [9],
MonoHuman [51], and NeuMan [19] have achieved impres-
sive performance. Despite being provided with only one
*Correspondence to {xtiange,eadeli}@stanford.edu
camera view, these methods are able to render the human
accurately from novel views. However, most existing hu-
man rendering methods are designed for ideal experimental
scenes with little to no obstacles and a full view of the hu-
man in each frame. In real-world scenes, however, humans
might move behind objects and move them around, causing
the objects to block the camera’s view.
Most neural rendering methods face difficulty with real-
world occluded scenarios for one main reason – lack of su-
pervision. More specifically, in-the-wild scenarios usually
do not provide ground-truth supervision of the appearance,
shape, and pose of the human, making it necessary for the
model to infer this information based on sparse evidence.
This can be challenging, especially when a large portion of
the human body is invisible. Due to the point-based render-
ing scheme of many neural methods, two very close coordi-
nates can create a dramatic difference in rendering output
when one is occluded and one is not [28, 46]. As a re-
sult, methods that are not adapted to occluded scenarios fre-
quently display incomplete human bodies or render floaters
and other artifacts when they encounter occlusions.
In this work, we address these shortcomings by intro-
ducing Wild2Avatar (Figure 1), an improved method for
rendering humans under occlusion.
Our method models
the occlusions, human, and background as three separate
neural fields, allowing a clean 3D reconstruction of the hu-
man regardless of occlusion. To accomplish this, we uti-
lize scene self-decomposition.
Extending upon inverted
sphere parametrization [53], we propose occlusion-aware
1
arXiv:2401.00431v1  [cs.CV]  31 Dec 2023

scene parametrization. In addition to the first sphere de-
fined by inverted sphere parametrization, our parametriza-
tion introduces a second, inner sphere and defines the region
from the camera to the edge of the inner sphere as the oc-
clusion region. By rendering this region separately, we can
decouple the occlusion from the rest of the scene. To en-
sure a high-fidelity and complete rendering of humans, we
propose aggregating the three renderings through a combi-
nation of a pixel-wise photometric loss, a scene decompo-
sition loss, an occlusion decoupling loss, and a geometry
completeness loss. While there are past works designed to
render occluded humans [26, 46], our work is the first to de-
couple the human from the occlusion cleanly, allowing for
complete renderings of humans without floaters.
In summary, our contributions are: (i) We introduce
occlusion-aware scene parametrization, a method to decou-
ple a scene into three parts: occlusion, human body, and
background. (ii) We propose a new rendering framework
that renders each of these three parts separately, and design
novel optimization objectives to ensure a clean decoupling
of the occlusion and a more complete human rendering. (iii)
We evaluate our method on challenging occlusion-intensive
in-the-wild videos, demonstrating the effectiveness of our
approach in rendering occluded humans.
2. Related Work
3D human modeling and rendering. There has been an
abundance of recent work focused on free-viewpoint ren-
dering of humans. While past works were able to achieve
good quality renderings of humans from dense [5, 10, 54]
and sparse [14, 25, 27, 40, 41, 47, 55] camera views,
a recent research focus involves rendering a moving hu-
man from a single camera angle [1, 2, 9, 11, 15, 17, 18,
20, 44, 51].
To accomplish this task, Neural Radiance
Fields (NeRFs) [32] based methods have been more fa-
vored recently due to their high rendering quality. Origi-
nally used to model static scenes, NeRFs have been adapted
to model a dynamic human by parameterizing the human
body using the SMPL [31] prior and mapping from the
pose-independent canonical space to the observation space
[9, 15, 44, 51]. HumanNeRF was one of the first to work
on free-viewpoint rendering of humans in complex motion
from monocular video [44]. MonoHuman models the de-
formation field with bi-directional constraints for even bet-
ter rendering of humans [51]. While these methods per-
form well in rendering a high-fidelity human avatar from
a monocular video in a clean environment, they fail when
occlusions block the camera’s view of the human.
Rendering via scene decomposition.
Complex natural
scenes are usually composed of multiple sub-components.
So, a common approach to render these scenes is to model
the different 3D components of the scene individually
and then aggregate them together in the 2D image space
[9, 19, 36, 45, 52]. GIRAFFE [36] was one of the pioneer-
ing works that proposed using generative neural fields to
disentangle objects from the background. The same idea
can be also found in NeRF++ [53], which is specifically
designed for rendering unbounded surroundings, with two
neural fields being trained separately for foreground and
background. STNeRF [16] utilizes neural layers to render
humans and background separately from multiple cameras,
while NeuMan trains a separate human and scene radiance
field on a monocular in-the-wild video [19]. Vid2Avatar
uses inverse sphere parametrization from [53] to separate
the dynamic human from the static background, resulting
in clean and detailed human avatars [9]. Vid2Avatar’s high
fidelity yet robust renderings with minimal artifacts inspire
us to use it as the foundation of this work.
Occlusion handling.
While accounting for occlusions
in rendering has been a long-standing research problem
[21, 24, 33, 35, 48], occlusion handling for NeRFs is rela-
tively new. OCC-NeRF uses a scene MLP and a background
MLP to remove occlusions from the output [56]. NeuRay
utilizes feature vectors to track the visibility of of each 3D
point to determine whether it is occluded [29]. While these
approaches reduce artifacts caused by occlusion, they re-
quire multiple input views to predict visibility, and are thus
not generalizable to an in-the-wild monocular video of a
dynamic human.
HOSNeRF can render human, objects,
and scene separately from a monocular in-the-wild video
by introducing object bones that are attached to the human
skeleton [26]. However, this approach does not account for
object-based occlusions of the human. OccNeRF utilizes
geometry and visibility priors with surface-based rendering
to render occluded humans [46]. However, it renders nu-
merous artifacts and floaters characteristic of NeRF-based
methods (Figure 6). Our approach, on the other hand, ren-
ders the human body cleanly due to the separation of hu-
man, background, and occlusion.
3. Methods
In this section, we present Wild2Avatar, a model that ren-
ders 3D humans with complete geometry and high-fidelity
appearance for in-the-wild monocular videos with occlu-
sions. We start by reviewing the basics of neural radiance
fields and the kinetics of the human body (section 3.1). We
then present the key concepts of scene parameterization,
which is central to the success of occlusion-aware human
renderings (section 3.2). To follow, we outline the objective
functions needed for complete human modeling and crisp
scene decomposition (section 3.3). Lastly, we tie all the
components together and outline the overall framework of
Wild2Avatar in Figure 2.
2

ℒrgb
+
ℒdec
+
ℒcomp
+
ℒocc
+
Body pose
# ⊇{&!}
Scene MLP 
("#$%$
Backward 
Skinning
SDF-based
Volume Rendering
Scene MLP 
("#$%$
Volume
Rendering
Volume
Rendering
Human MLP 
(&'
ℒeik
Human branch
Backward/Update
Canonical Space
Sec. 3.1
Occlusion>Human->Background Scene Param.
Sec. 3.2
Sec. 3.1
Sec. 3.3
Sec. 3.3
Occlusion branch
Background branch
Figure 2. Wild2Avatar renders occluded humans from a monocular video. It parameterizes the 3D scene into three parts: occlusion,
human, and background, in order from closest to farthest from the camera. The human and the occlusions/background are individually
modeled via separate neural radiance fields. The human is parameterized in a bounded sphere Π by first deforming ray samples into a
canonical space with the help of the pre-computed body pose P. The canonical points x are passed into a rendering network F fg to
learn the radiance c and distance s to the surface of the human, which can then be rendered through SDF-based volume rendering. The
unbounded background is represented as coordinates on the surface of Π along with their inverted distances. Another rendering network
F scene is used to learn the radiance and density for the background ray samples. The space of occlusion is determined as the interval
between the camera and an inner sphere π. We parameterize ray samples as coordinates on the surface of π and the negation of the
inverted distances to the center of the inner sphere. We rely on the same network F scene to render the occlusions. The three renderings
are sequentially aggregated and supervised on a combination of losses, in which we specifically encourage the decoupling of the occlusion
from the human through Locc and penalize the incompleteness of human geometry through Lcomp.
3.1. Preliminaries and Background
Implicit neural radiance fields. Neural Radiance Fields
(NeRFs) [32] learn a neural network F to model the
mappings between (positionally embedded) 3D coordinates
{x ∈R3} and their radiance c(x) and density σ(x). This
representation is applicable to stationary one-object scenes
but usually suffers from unexpected rendering artifacts and
floaters in the wild. For rendering solid and continuous hu-
man bodies, using an implicit surface representation with a
Signed Distance Function (SDF) is preferred [9, 38, 43, 49].
Instead of learning density values for each x, F is trained
to output distances to the surface of the human body s(x).
The human geometry is then represented as a surface model
by the zero-level set: {s(x) = 0}.
Human body deformation. Following [4, 44], we model
an articulated human in a static canonical space {xc} and
deform its body from and to the observation space {xo} via
backward and forward skinning [4, 9]:
xo =
X
i
wi
cBixc,
xc = (
X
i
wi
oBi)−1xo,
(1)
where B is the bone transformation and {wi
(·)} are vertex-
wise skinning weights derived from SMPL [31] poses P.
With the above transformations, we are able to first optimize
a static neural field for the dynamic human in the canonical
space and then deform the ray samples to the observation
space for volume rendering [39, 40, 42].
3.2. Occlusion-aware Scene Parameterization
To decouple the human from the background, the scene is
usually parameterized separately for the human and back-
ground [9]. This kind of parameterization first introduced
by [53] uses a sphere Π to cover all of the space intended
to be occupied by the human. The space outside the sphere
is parameterized as the background by inverting the sphere.
Rendering is then achieved via ray composition.
Motivation. The above human→background parameteriza-
tion successfully renders the human and background when
the human can be clearly viewed by the camera with no ob-
stacles occluding the human. However, this ideal setting is
impractical for in-the-wild videos, which may contain unex-
pected foreground objects other than the human. The object
occlusions can interfere with human modeling. Considering
this, we propose to parameterize the scene into three parts:
occlusion→human→background.
Human. For each frame in the video, we transform the de-
formed humans into the common canonical space, which
is bounded by a sphere Π with pre-defined radius R. We
learn a neural radiance field to model the surface of the hu-
man body implicitly (section 3.1). We follow [9, 49]’s ap-
proach to SDF-based volume rendering: for each canonical
3

Human
Center
Outermost
Rays
Occlusion
Space
Human
Space
Background
Space
(A) Two-part scene param.
Human -> Background
(B) Occlusion-aware scene param.
Occlusion -> Human -> Background
Type equation here.
r = 0
r = 0
r = 1 !
"
"
Figure 3.
Comparisons of scene parameterizations.
(A)
The regular two-part paradigm [53] parameterizes the foreground
within a sphere Π with radius R and background outside the
sphere. (B) Our proposed occlusion-aware paradigm parameter-
izes the scene into three sequential parts. The occlusion is explic-
itly modeled within the interval space between the camera and an
inner sphere π with a radius r < R.
point xi, their signed distances si are first converted into
density values σi through scaled Laplace distribution’s Cu-
mulative Distribution Function (CDF) to the negated signed
distances. Then, we aggregate the radiance and density of
each sample along the rays for volume rendering [30]:
C =
X
i
ci(1 −exp(−σiδi))
Y
j<i
exp(−σjδj),
(2)
τ i = exp

−
X
j<i
σj  1 −exp(−σj)


,
(3)
where δi is the z-axis distance between two ray samples
and τ i is the transmittance that indicates the probability of
the ray not hitting anything within the interval [1, i]. To
eliminate ambiguity when parameterizing different parts of
the scene, we normalize the center of the human to O = ⃗0.
Background. The background of a scene includes the un-
bounded surroundings that cannot be naively parameterized
by a sphere.
We adopt the technique proposed by [53]
that inverts the sphere Π to represent every 3D point x =
{x, y, z} as a quadruple xbg = {xbg, ybg, zbg, 1
d}, where
||{xbg, ybg, zbg}|| indicates 3D coordinates on the surface of
Π and d is the Euclidean distance between the sphere ori-
gin O and x. This inverted sphere parameterization enables
sampling within the bounded interval 1
d ∈(0, 1]. Given any
1
d, {xbg, ybg, zbg} can be calculated by rotating the vector
[53]. Similar to [9], the rendering of the background is con-
ditioned on the combination of a per-frame latent code, 4D
parameterization, and embedded ray directions.
Occlusion. On top of the two-part scene parameterization,
in this work, we propose to cater for obstacles that come
between the human and the camera when filming the video.
We define the occlusion to lie inside the sphere Π and oc-
cupy a bounded sub-space within Π. To parameterize the
human and occlusion separately, a concentric inner sphere
π is introduced with radius r < R. Without any semantic
priors about the obstacles (e.g. size, appearance), we use the
entire space between the camera and π to learn the neural
radiance field for occlusion. To fully bound this sub-space,
π is built as an inscribed sphere to the outermost rays shot
from the camera, and its radius r is determined accordingly
to ensure every ray intersects π at least once. For any radius
r, the number of intersections can be determined via:
(o · d)2 −||o2|| + r2,
(4)
where o is the camera location, d is the ray direction, and
||·|| is L1 norm on the coordinate dimension. There is no in-
tersection between the rays and π when the above function
evaluates to negative, and there must be at least one inter-
section otherwise. Given the monotonicity of the function
w.r.t r, we can easily find the minimum possible r through
binary search within (0, R) during pre-processing.
Since we do not focus on rendering quality for the ob-
stacles and background, we improve network efficiency by
using the same rendering network FScene for both obstacles
and background while conditioning on different per-frame
latent codes. For the unbounded background, the quadru-
ple xbg must be computed through vector rotation which
is time-consuming. In the bounded occlusion space, the
quadruple xocc can be computed more efficiently by solving
the following quadratic equation w.r.t t:
||o + td|| = r,
||d||2t2 + 2(o · d)t + ||o||2 −r = ⃗0,
t = −
p
||o · d||2 + (||r2|| −||o2||) ||d2|| −||o · d||
||d2||
.
(5)
The parameterized 3D coordinates on the surface of π can
be then computed as {xocc, yocc, zocc} = o + td.
Sharing the same network Fscene for both background
and occlusion requires the inputs to be at the same
magnification scale.
To do this, we normalize both
{xocc, yocc, zocc} and {xbg, ybg, zbg} to the unit sphere.
Such normalization will not lead to overlapped surface vec-
tors since r is strictly smaller than R. To further decou-
ple the dependencies for occlusion/background rendering,
we determine the quadruple xocc as the concatenation of
{xocc, yocc, zocc} and negation of the inverted depth −
r
||x2||.
Scene Composition. We cast different sets of ray samples
on the neural fields individually and query their color via
Equation 3.2 separately.
For the bounded occlusion and
human, we sample xocc and xfg directly. For the back-
ground, we sample 1
d instead and compute xbg afterwards.
The ray opacity α for each neural field can be determined
via α = P
i τ i (Equation 3.2). The topology of the scene is
explicitly constrained through sequential composition:
C = Cocc+(1−αocc)Cfg +(1−αocc)(1−αfg)Cbg. (6)
4

Binary Cross 
Entropy
×
Inverted mask
Segmentation 
Model
Binary mask
‘occlusion’ mask
Occluded human
//
Human occupancy
Occlusion occupancy
Wild2Avatar
~
Figure 4. Workflow for the occlusion decoupling loss Locc. ‘∼’
indicates binary inversion and ‘//’ indicates gradient stopping.
3.3. Optimization Objectives
Rendering objectives.
The most common objective for
neural rendering models is the pixel-wise photometric loss
Lrgb [32] that forces renderings to reconstruct input im-
ages. For SDF-based rendering, a regularizer Leik [8] is
usually used to constrain the implicit geometry to satisfy the
Eikonal equation. Moreover, following the two-part scene
decomposition paradigm [53], we borrow the decomposi-
tion loss Ldec [9] for better decoupling of the dynamic hu-
man from the static background.
Occlusion decoupling. In videos with occlusions, optimiz-
ing towards the above objectives tends to merge obstacles
with the human and mistakenly render obstacle textures as
part of the human appearance (Figure 5). So, we propose
an additional objective that encourages the decoupling of
the occluding obstacles and the human. Without any exter-
nal knowledge of the obstacles occluding the human, it is
impossible to model them completely. Hence we only pa-
rameterize potential obstacles within the interval between
the camera and the human. To be more specific, we encour-
age the obstacles’ density to be high at the pixels of human
occupancy but are invisible from the camera. The visibility
of the human can be determined via a binary mask M from
an off-the-shelf segmentation model and the occupancy of
the human can be obtained from volume rendering:
Locc = BCE (αocc, (1 −M)(sg(αfg) > ϵ)) ,
(7)
where αocc, αfg ∈[0, 1] are the obstacle and human den-
sity maps respectively, ϵ = 0.1 is a threshold to determine
the occupancy of the human, BCE(·) is the binary cross en-
tropy function, and sg(·) indicates gradient stopping. The
workflow is outlined in Figure 4. We noticed that occlu-
sions only appear in a small area of most videos, and the
remaining areas usually have empty obstacle densities. To
deal with this data imbalance, we use weighted BCE(·) and
assign a higher weight to the pixels that tend to be occluded.
OcMotion
qualityvis
comp.
qualityllm
Vid2Avatar [9]
13.14
0.77
4.40
Wild2Avatar
13.07
0.81
8.20
In-the-wild
qualityvis
comp.
qualityllm
Vid2Avatar [9]
8.20
0.64
4.0
Wild2Avatar
9.45
0.73
7.5
Table 1. Quantitative comparisons on the 5 OcMotion and 2 in-
the-wild videos. We color cells that have the best metric values.
Geometry completeness regularization. In the decompo-
sition loss Ldec, rays that intersect the surface of the human
are encouraged to have high occupancy of the 2D space.
We observed that such regularization in 2D introduces am-
biguity in 3D and the rendered human geometry is usu-
ally incomplete under strong occlusions. Considering this,
we propose to regularize the completeness of 3D geometry
by enforcing the ray samples near the surface Xnear to be
closer to the implicit surface:
Lcomp =
1
|Xnear|
X
x∈Xnear
|s(x)|,
(8)
where Xnear can be determined by the Euclidean distances
to the human mesh, which was initialized as SMPL [31]
mesh and updated in the optimization process [9].
Our Wild2Avatar is trained on the combination of all of
the above objectives, each with a scale weight λ(·).
4. Experiments
4.1. Datasets
OcMotion [13]. This dataset consists of indoor scenes of
humans engaging with a variety of objects while being par-
tially occluded by them. We used 5 out of the 48 videos
at different levels of occlusion to evaluate the methods. In
particular, we drew only 100 frames from each of the videos
to train the models. We initialized the optimization pro-
cess with the dataset’s provided camera matrices, human
poses, and SMPL parameters. Frame-wise binary human
segmentation masks are obtained from the Segment Any-
thing Model (SAM) [23].
In-the-wild videos. We conducted additional experiments
on two real-world videos, one of which was downloaded
from YouTube while the other one was captured by our re-
search team using a cell phone camera. We sampled 150
frames from both of the videos for training. In these videos,
we obtained camera matrices, human poses, and SMPL pa-
rameters using SLAHMR [7, 50]. Since no ground truth
poses are provided, evaluations on these videos also indicate
the robustness of methods to inaccurately estimated priors.
4.2. Evaluations
Comparison.
Wild2Avatar is mainly compared against
Vid2Avatar [9], which is the state-of-the-art in human ren-
5

In-the-wild Videos
Occluded
Human
Ours
Ours
Vid2Avatar
Vid2Avatar
Occluded
Human
Input View
Novel View
Figure 5. Qualitative comparisons with Vid2Avatar [9] in the OcMotion dataset [13] (top 5) and in-the-wild videos (bottom 2). We
manually blurred the faces for the in-the-wild videos to maintain anonymity.
6

dering. We also compare against the most recent occlusion-
aware human rendering method OccNeRF [46]. For fair-
ness, all comparison methods also use the same binary
human segmentation masks either for supervision or ray
sampling.
Note that OccNeRF originally requires more
than 500 frames for stable training while Wild2Avatar and
Vid2Avatar only require ∼100. We conducted experiments
on both high and low frame count regimes for OccNeRF.
Metrics. We used qualitative and quantitative evaluations to
compare the methods. For qualitative evaluations, we ren-
der the human behind the obstacles as well as novel views
to assess the capability of occlusion handling. Furthermore,
we rely on three quantitative metrics to measure the capa-
bility of renderers from different perspectives. First, we
compute the commonly used Peak Signal-to-Noise Ratio
(PSNR) on the visible human parts (determined by the bi-
nary segmentation mask) to measure the rendering quality
on unoccluded human appearance. Then, we calculate the
Intersection over Union (IoU) between human occupancy
masks and GT SMPL mesh silhouettes to measure the com-
pleteness of rendering [46]. Lastly, we propose to assess
the complete rendering quality of occluded humans through
Large Language Models (LLMs). We prompt GPT4V [37]
to output a quality score at a scale of 0-10 (the higher the
better) for each rendering. The three metrics are denoted as
qualityvis, comp., and qualityllm respectively.
4.3. Results on Occluded Monocular Videos
We first compare the rendering results of Vid2Avatar and
Wild2Avatar on both datasets in Figure 5. Both methods
can reconstruct the visible human parts in high fidelity with-
out obvious artifacts or floaters, which verified the superi-
ority of representing humans via implicit SDF. However,
Vid2Avatar fails to recover the occluded body parts and
mistakenly renders parts of the occlusions as human appear-
ance. Wild2Avatar, on the other hand, not only decouples
most occlusions from the human body but recovers consis-
tent human appearance. In Table 1, we report the quantita-
tive results of the two methods and observed on-par render-
ing performances at the visible part. Wild2Avatar surpasses
Vid2Avatar consistently on body geometry and the render-
ing quality of the occluded parts.
4.4. Comparison Against OccNeRF [46]
We then compare Wild2Avatar against a recent occlusion-
aware human rendering counterpart OccNeRF [46] in Fig-
ure 6. For fair comparisons, we trained OccNeRF on 500
frames and 100 frames respectively. Without implicit SDF
representation, OccNeRF suffers from common defects of
floaters and artifacts. Although OccNeRF is also able to re-
cover occluded human parts, the body is usually unexpect-
edly twisted resulting in low rendering quality.
OcMotion
qualityvis
comp.
qualityllm
OccNeRF-100 [46]
13.77
0.73
3.50
OccNeRF-500 [46]
15.02
0.77
5.00
Wild2Avatar
15.92
0.82
8.50
Occluded Human
OccNeRF100
OccNeRF500
Ours100
Figure 6. Comparison against OccNeRF [46] trained on 100 and
500 frames. We color cells that have the best metric values.
4.5. Visualizations of Scene Decomposition
As introduced in section 3, Wild2Avatar renders three scene
parts compositionally. Human and background/occlusion
are separately modeled in two different neural fields. Here
we show individual renderings of the three scene parts in
Figure 7. Note that since this work focuses solely on hu-
man rendering, artifact-free rendering of background and
occlusions are outside the scope of this work.
4.6. Ablation Studies
Impact of occlusion-aware parameterization. One of the
major contributions that enables Wild2Avatar to achieve
human renderings under occlusions is the occlusion-aware
scene parameterization. Here we first ablate the effective-
ness of such a design by inheriting the two-stage param-
eterization but optimizing the rendering of the foreground
human on the visible parts only [46] while maintaining the
same optimization objectives. As shown in the first row of
Figure 8, even though Wild2Avatar can still recover the oc-
cluded appearances without the proposed parameterization,
the rendered results are corrupted with many artifacts.
Impact of Locc. In section 3.3, the occlusion decoupling
loss is proposed to further disentangle the rendering of hu-
man and occlusions. We validated the effect of this loss by
simply removing it from optimization objectives. Accord-
ing to the results in the second row of Figure 8, the occluded
areas cannot be fully recovered without the proposed loss.
Impact of Lcomp. As also noted in [46], the completeness
of 3D human geometry needs to be explicitly enforced dur-
ing training. When removing this loss, the 2D renderings
can easily degenerate. We also observed that the proposed
7

In-the-wild videos
Composition of renderings
occlusion->human->background
Rendering of occlusion
Rendering of human
Rendering of background
Figure 7. Visualizations of scene decomposition. Wild2Avatar separately renders occlusion, human, and background.
Occluded human
w/o  ℒdec
w/o  ℒcomp
Occluded human
Occluded human
w/o occlusion param.
Full Wild2Avatar
Full Wild2Avatar
Full Wild2Avatar
Figure 8. Ablation results. Major differences are highlighted.
loss serves as a regularizer to force the human geometry to
be consistent with the SMPL mesh prior, which prevents
from rendering incorrect poses (third row in Figure 8).
5. Discussion and Conclusion
Discussion. Training human neural radiance fields to fit in-
put frames while handling occlusions is challenging. With-
out knowing any semantic priors of the occlusions, it is
impractical to achieve occlusion-human decoupling in a
fully unsupervised manner.
Our proposed occlusion de-
coupling loss Locc serves as a weakly-supervised approach
with the help of precomputed binary human segmentation
masks.
Moreover, we noticed that both Vid2Avatar and
Wild2Avatar rely on accurate poses. Although the poses
can be jointly optimized along the training process, inac-
curate priors are still very likely to lead to poor or incom-
plete renderings, especially at body limbs. Moreover, the
need to render the occlusions causes the inference time of
Wild2Avatar to increase, resulting in a slower optimization
process. Future work may include adding additional pose
correction steps [50] and adopting efficient NeRF variants
[18, 22, 34] for faster training.
Conclusion.
We introduced Wild2Avatar, a method to
render a 3D human avatar from an occluded in-the-wild
monocular video. Unlike prior work, our method was able
to decouple the human body from both background and ob-
stacle, allowing for a complete rendering without artifacts.
We accomplished this through novel occlusion-aware scene
parametrization: modeling the occlusion, human, and back-
ground separately. Additionally, we introduced training ob-
jectives to improve the quality of our renderings. Our work
achieves high-fidelity state-of-the-art rendering of occluded
humans on challenging in-the-wild videos.
Acknowledgment.
This work was partially funded by the
Panasonic Holdings Corporation, the Gordon and Betty
Moore Foundation, the Jaswa Innovator Award, Stanford
HAI, and Stanford Wu Tsai Human Performance Alliance.
8

References
[1] Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian
Theobalt, and Gerard Pons-Moll. Detailed human avatars
from monocular video, 2018. 2
[2] Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian
Theobalt, and Gerard Pons-Moll. Video based reconstruction
of 3d people models, 2018. 2
[3] Joel Carranza, Christian Theobalt, Marcus A Magnor, and
Hans-Peter Seidel. Free-viewpoint video of human actors.
ACM transactions on graphics (TOG), 22(3):569–577, 2003.
1
[4] Xu Chen, Yufeng Zheng, Michael J Black, Otmar Hilliges,
and Andreas Geiger. Snarf: Differentiable forward skinning
for animating non-rigid neural implicit shapes. In Interna-
tional Conference on Computer Vision (ICCV), 2021. 3
[5] Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter
Duiker, Westley Sarokin, and Mark Sagar.
Acquiring the
reflectance field of a human face.
In Proceedings of the
27th Annual Conference on Computer Graphics and In-
teractive Techniques, page 145–156, USA, 2000. ACM
Press/Addison-Wesley Publishing Co. 2
[6] Beerend GA Gerats, Jelmer M Wolterink, and Ivo AMJ
Broeders. Depth-supervised nerf for multi-view rgb-d oper-
ating room images. arXiv preprint arXiv:2211.12436, 2022.
1
[7] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran,
Angjoo Kanazawa*, and Jitendra Malik*. Humans in 4D:
Reconstructing and tracking humans with transformers. In
International Conference on Computer Vision (ICCV), 2023.
5
[8] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and
Yaron Lipman. Implicit geometric regularization for learning
shapes. arXiv preprint arXiv:2002.10099, 2020. 5
[9] Chen Guo, Tianjian Jiang, Xu Chen, Jie Song, and Otmar
Hilliges. Vid2avatar: 3d avatar reconstruction from videos
in the wild via self-supervised scene decomposition. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 12858–12868, 2023. 1, 2, 3,
4, 5, 6
[10] Kaiwen Guo, Peter Lincoln, Philip Davidson, Jay Busch,
Xueming Yu, Matt Whalen, Geoff Harvey, Sergio Orts-
Escolano, Rohit Pandey, Jason Dourgarian, Danhang Tang,
Anastasia Tkach, Adarsh Kowdle, Emily Cooper, Ming-
song Dou, Sean Fanello, Graham Fyffe, Christoph Rhemann,
Jonathan Taylor, Paul Debevec, and Shahram Izadi. The re-
lightables: Volumetric performance capture of humans with
realistic relighting. 38(6), 2019. 2
[11] Marc Habermann, Weipeng Xu, Michael Zollhoefer, Gerard
Pons-Moll, and Christian Theobalt. Livecap: Real-time hu-
man performance capture from monocular video, 2019. 2
[12] Marc Habermann, Lingjie Liu, Weipeng Xu, Gerard Pons-
Moll, Michael Zollhoefer, and Christian Theobalt. Hdhu-
mans: A hybrid approach for high-fidelity digital humans.
Proceedings of the ACM on Computer Graphics and Inter-
active Techniques, 6(3):1–23, 2023. 1
[13] Buzhen Huang, Yuan Shu, Jingyi Ju, and Yangang Wang.
Occluded human body capture with self-supervised spatial-
temporal motion prior.
arXiv preprint arXiv:2207.05375,
2022. 5, 6
[14] Zeng Huang, Tianye Li, Weikai Chen, Yajie Zhao, Jun Xing,
Chloe LeGendre, Linjie Luo, Chongyang Ma, and Hao Li.
Deep volumetric video from very sparse multi-view perfor-
mance capture. In Computer Vision – ECCV 2018. Springer
International Publishing, 2018. 2
[15] Vinoj Jayasundara, Amit Agrawal, Nicolas Heron, Abhinav
Shrivastava, and Larry S Davis. Flexnerf: Photorealistic free-
viewpoint rendering of moving humans from sparse views.
In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 21118–21127, 2023.
2
[16] Zhang Jiakai, Liu Xinhang, Ye Xinyi, Zhao Fuqiang, Zhang
Yanshun, Wu Minye, Zhang Yingliang, Xu Lan, and Yu
Jingyi. Editable free-viewpoint video using a layered neu-
ral representation. In ACM SIGGRAPH, 2021. 2
[17] Boyi Jiang, Yang Hong, Hujun Bao, and Juyong Zhang. Sel-
frecon: Self reconstruction your digital avatar from monoc-
ular video, 2022. 2
[18] Tianjian Jiang, Xu Chen, Jie Song, and Otmar Hilliges. In-
stantavatar: Learning avatars from monocular video in 60
seconds, 2022. 2, 8
[19] Wei Jiang, Kwang Moo Yi, Golnoosh Samei, Oncel Tuzel,
and Anurag Ranjan. Neuman: Neural human radiance field
from a single video, 2022. 1, 2
[20] Yue Jiang, Marc Habermann, Vladislav Golyanik, and Chris-
tian Theobalt. Hifecap: Monocular high-fidelity and expres-
sive capture of human performances, 2022. 2
[21] Sankaraganesh Jonna, Sukla Satapathy, and Rajiv R Sahay.
Stereo image de-fencing using smartphones. In 2017 IEEE
international conference on acoustics, speech and signal
processing (ICASSP), pages 1792–1796. IEEE, 2017. 2
[22] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk¨uhler,
and George Drettakis.
3d gaussian splatting for real-time
radiance field rendering.
ACM Transactions on Graphics
(ToG), 42(4):1–14, 2023. 8
[23] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. arXiv preprint arXiv:2304.02643, 2023. 5
[24] Jingyuan Li, Ning Wang, Lefei Zhang, Bo Du, and Dacheng
Tao. Recurrent feature reasoning for image inpainting. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition, pages 7760–7768, 2020. 2
[25] Ruilong Li, Julian Tanke, Minh Vo, Michael Zollhofer, Jur-
gen Gall, Angjoo Kanazawa, and Christoph Lassner. Tava:
Template-free animatable volumetric actors, 2022. 2
[26] Jia-Wei Liu, Yan-Pei Cao, Tianyuan Yang, Eric Zhongcong
Xu, Jussi Keppo, Ying Shan, Xiaohu Qie, and Mike Zheng
Shou. Hosnerf: Dynamic human-object-scene neural radi-
ance fields from a single video, 2023. 2
[27] Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu
Sarkar, Jiatao Gu, and Christian Theobalt.
Neural actor:
Neural free-view synthesis of human actors with pose con-
trol, 2022. 2
9

[28] Yuan Liu, Sida Peng, Lingjie Liu, Qianqian Wang, Peng
Wang, Christian Theobalt, Xiaowei Zhou, and Wenping
Wang. Neural rays for occlusion-aware image-based render-
ing. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 7824–7833,
2022. 1
[29] Yuan Liu, Sida Peng, Lingjie Liu, Qianqian Wang, Peng
Wang, Christian Theobalt, Xiaowei Zhou, and Wenping
Wang. Neural rays for occlusion-aware image-based render-
ing. In 2022 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), pages 7814–7823, 2022. 2
[30] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel
Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural vol-
umes: Learning dynamic renderable volumes from images.
arXiv preprint arXiv:1906.07751, 2019. 4
[31] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-
ard Pons-Moll, and Michael J. Black.
SMPL: A skinned
multi-person linear model.
ACM Trans. Graphics (Proc.
SIGGRAPH Asia), 34(6):248:1–248:16, 2015. 2, 3, 5
[32] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. Communications of the ACM, 65(1):99–106, 2021. 2,
3, 5
[33] Yadong Mu, Wei Liu, and Shuicheng Yan. Video de-fencing.
IEEE Transactions on Circuits and Systems for Video Tech-
nology, 24(7):1111–1121, 2013. 2
[34] Thomas M¨uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a mul-
tiresolution hash encoding. ACM Transactions on Graphics
(ToG), 41(4):1–15, 2022. 8
[35] Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z Qureshi,
and Mehran Ebrahimi.
Edgeconnect: Generative image
inpainting with adversarial edge learning.
arXiv preprint
arXiv:1901.00212, 2019. 2
[36] Michael Niemeyer and Andreas Geiger. Giraffe: Represent-
ing scenes as compositional generative neural feature fields,
2021. 2
[37] OpenAI, 2023. 7
[38] Jeong Joon Park, Peter Florence, Julian Straub, Richard
Newcombe, and Steven Lovegrove. Deepsdf: Learning con-
tinuous signed distance functions for shape representation.
In Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition, pages 165–174, 2019. 3
[39] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien
Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo
Martin-Brualla. Nerfies: Deformable neural radiance fields.
In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pages 5865–5874, 2021. 3
[40] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan
Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Ani-
matable neural radiance fields for modeling dynamic human
bodies. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision, pages 14314–14323, 2021. 2,
3
[41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,
Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:
Implicit neural representations with structured latent codes
for novel view synthesis of dynamic humans, 2021. 2
[42] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and
Francesc Moreno-Noguer.
D-nerf: Neural radiance fields
for dynamic scenes. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
10318–10327, 2021. 3
[43] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku
Komura, and Wenping Wang. Neus: Learning neural im-
plicit surfaces by volume rendering for multi-view recon-
struction. Advances in Neural Information Processing Sys-
tems, 34:27171–27183, 2021. 3
[44] Chung-Yi Weng,
Brian Curless,
Pratul P Srinivasan,
Jonathan T Barron, and Ira Kemelmacher-Shlizerman. Hu-
mannerf: Free-viewpoint rendering of moving people from
monocular video.
In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern Recognition, pages
16210–16220, 2022. 2, 3
[45] Tianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, For-
rester Cole, and Cengiz Oztireli. D2nerf: Self-supervised
decoupling of dynamic and static objects from a monocular
video, 2022. 2
[46] Tiange Xiang, Adam Sun, Jiajun Wu, Ehsan Adeli, and Li
Fei-Fei. Rendering humans from object-occluded monocu-
lar videos. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 3239–3250, 2023. 1,
2, 7
[47] Hongyi Xu, Thiemo Alldieck, and Cristian Sminchisescu.
H-nerf: Neural radiance fields for rendering and temporal
reconstruction of humans in motion, 2021. 2
[48] Tianfan Xue, Michael Rubinstein, Ce Liu, and William T.
Freeman.
A computational approach for obstruction-free
photography. ACM Transactions on Graphics, 34(4):1–11,
2015. 2
[49] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman.
Volume rendering of neural implicit surfaces.
In Thirty-
Fifth Conference on Neural Information Processing Systems,
2021. 3
[50] Vickie Ye, Georgios Pavlakos, Jitendra Malik, and Angjoo
Kanazawa.
Decoupling human and camera motion from
videos in the wild. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2023. 5, 8
[51] Zhengming Yu, Wei Cheng, Xian Liu, Wayne Wu, and
Kwan-Yee Lin.
Monohuman: Animatable human neural
field from monocular video, 2023. 1, 2
[52] Wentao Yuan, Zhaoyang Lv, Tanner Schmidt, and Steven
Lovegrove. Star: Self-supervised tracking and reconstruc-
tion of rigid objects in motion with neural rendering, 2020.
2
[53] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen
Koltun. Nerf++: Analyzing and improving neural radiance
fields. arXiv preprint arXiv:2010.07492, 2020. 1, 2, 3, 4, 5
[54] Taotao Zhou, Kai He, Di Wu, Teng Xu, Qixuan Zhang, Kuix-
iang Shao, Wenzheng Chen, Lan Xu, and Jingyi Yu. Re-
lightable neural human assets from multi-view gradient illu-
minations, 2023. 2
10

[55] Tiansong Zhou, Jing Huang, Tao Yu, Ruizhi Shao, and Kun
Li.
Hdhuman: High-quality human novel-view rendering
from sparse views, 2023. 2
[56] Chengxuan Zhu, Renjie Wan, Yunkai Tang, and Boxin Shi.
Occlusion-free scene recovery via neural radiance fields. In
2023 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR). IEEE, 2023. 2
11

