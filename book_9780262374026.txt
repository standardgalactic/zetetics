Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Distributional Reinforcement Learning
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Adaptive Computation and Machine Learning
Francis Bach, editor
A complete list of books published in the Adaptive Computation and Machine
Learning series appears at the back of this book.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Distributional Reinforcement Learning
Marc G. Bellemare, Will Dabney, and Mark Rowland
The MIT Press
Cambridge, Massachusetts
London, England
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

c⃝2023 Marc G. Bellemare, Will Dabney, and Mark Rowland
This work is subject to a Creative Commons CC-BY-ND-NC license.
Subject to such license, all rights are reserved.
The MIT Press would like to thank the anonymous peer reviewers who provided 
comments on drafts of this book. The generous work of academic experts is essential for 
establishing the authority and quality of our publications. We acknowledge with 
gratitude the contributions of these otherwise uncredited readers.
This book was set in LATEX by the authors. 
Library of Congress Cataloging-in-Publication Data
Names: Bellemare, Marc G., author. | Dabney, Will, author. | Rowland, Mark 
(Research scientist), author.
Title: Distributional reinforcement learning / Marc G. Bellemare, Will Dabney, Mark 
Rowland.
Description: Cambridge, Massachusetts : The MIT Press, [2023] | Series: Adaptive 
computation and machine learning | Includes bibliographical references and index.
Identifiers: LCCN 2022033240 (print) | LCCN 2022033241 (ebook) | ISBN 
9780262048019 (hardcover) | ISBN 9780262374019 (epub) | ISBN 9780262374026 
(pdf)
Subjects: LCSH: Reinforcement learning. | Reinforcement learning–Statistical methods.
Classification: LCC Q325.6 .B45 2023 (print) | LCC Q325.6 (ebook) | DDC
006.3/1–dc23/eng20221102
LC record available at https://lccn.loc.gov/2022033240
LC ebook record available at https://lccn.loc.gov/2022033241
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Contents
Preface
ix
1
Introduction
1
1.1
Why Distributional Reinforcement Learning? . . . . . . . . .
2
1.2
An Example: Kuhn Poker . . . . . . . . . . . . . . . . . . . .
3
1.3
How Is Distributional Reinforcement Learning Diﬀerent? . . .
5
1.4
Intended Audience and Organization . . . . . . . . . . . . . .
7
1.5
Bibliographical Remarks . . . . . . . . . . . . . . . . . . . .
9
2
The Distribution of Returns
11
2.1
Random Variables and Their Probability Distributions . . . . .
11
2.2
Markov Decision Processes . . . . . . . . . . . . . . . . . . .
14
2.3
The Pinball Model
. . . . . . . . . . . . . . . . . . . . . . .
16
2.4
The Return
. . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.5
The Bellman Equation
. . . . . . . . . . . . . . . . . . . . .
25
2.6
Properties of the Random Trajectory . . . . . . . . . . . . . .
27
2.7
The Random-Variable Bellman Equation . . . . . . . . . . . .
30
2.8
From Random Variables to Probability Distributions
. . . . .
33
2.9
Alternative Notions of the Return Distribution* . . . . . . . .
40
2.10 Technical Remarks . . . . . . . . . . . . . . . . . . . . . . .
41
2.11 Bibliographical Remarks . . . . . . . . . . . . . . . . . . . .
43
2.12 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
3
Learning the Return Distribution
51
3.1
The Monte Carlo Method . . . . . . . . . . . . . . . . . . . .
52
3.2
Incremental Learning . . . . . . . . . . . . . . . . . . . . . .
54
3.3
Temporal-Diﬀerence Learning . . . . . . . . . . . . . . . . .
56
3.4
From Values to Probabilities . . . . . . . . . . . . . . . . . .
58
3.5
The Projection Step . . . . . . . . . . . . . . . . . . . . . . .
60
3.6
Categorical Temporal-Diﬀerence Learning . . . . . . . . . . .
65
v
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

vi
Contents
3.7
Learning to Control . . . . . . . . . . . . . . . . . . . . . . .
69
3.8
Further Considerations . . . . . . . . . . . . . . . . . . . . .
70
3.9
Technical Remarks . . . . . . . . . . . . . . . . . . . . . . .
70
3.10 Bibliographical Remarks . . . . . . . . . . . . . . . . . . . .
71
3.11 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
4
Operators and Metrics
77
4.1
The Bellman Operator
. . . . . . . . . . . . . . . . . . . . .
78
4.2
Contraction Mappings
. . . . . . . . . . . . . . . . . . . . .
79
4.3
The Distributional Bellman Operator . . . . . . . . . . . . . .
83
4.4
Wasserstein Distances for Return Functions . . . . . . . . . .
86
4.5
ℓp Probability Metrics and the Cramér Distance . . . . . . . .
92
4.6
Suﬃcient Conditions for Contractivity . . . . . . . . . . . . .
95
4.7
A Matter of Domain . . . . . . . . . . . . . . . . . . . . . . .
98
4.8
Weak Convergence of Return Functions*
. . . . . . . . . . . 102
4.9
Random-Variable Bellman Operators* . . . . . . . . . . . . . 104
4.10 Technical Remarks . . . . . . . . . . . . . . . . . . . . . . . 105
4.11 Bibliographical Remarks . . . . . . . . . . . . . . . . . . . . 106
4.12 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
5
Distributional Dynamic Programming
115
5.1
Computational Model . . . . . . . . . . . . . . . . . . . . . . 115
5.2
Representing Return-Distribution Functions . . . . . . . . . . 118
5.3
The Empirical Representation
. . . . . . . . . . . . . . . . . 120
5.4
The Normal Representation . . . . . . . . . . . . . . . . . . . 125
5.5
Fixed-Size Empirical Representations
. . . . . . . . . . . . . 128
5.6
The Projection Step . . . . . . . . . . . . . . . . . . . . . . . 130
5.7
Distributional Dynamic Programming . . . . . . . . . . . . . 135
5.8
Error Due to Diﬀusion
. . . . . . . . . . . . . . . . . . . . . 138
5.9
Convergence of Distributional Dynamic Programming
. . . . 141
5.10 Quality of the Distributional Approximation . . . . . . . . . . 145
5.11 Designing Distributional Dynamic Programming Algorithms . 147
5.12 Technical Remarks . . . . . . . . . . . . . . . . . . . . . . . 148
5.13 Bibliographical Remarks . . . . . . . . . . . . . . . . . . . . 154
5.14 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
6
Incremental Algorithms
161
6.1
Computation and Statistical Estimation . . . . . . . . . . . . . 161
6.2
From Operators to Incremental Algorithms
. . . . . . . . . . 163
6.3
Categorical Temporal-Diﬀerence Learning . . . . . . . . . . . 165
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Contents
vii
6.4
Quantile Temporal-Diﬀerence Learning . . . . . . . . . . . . 167
6.5
An Algorithmic Template for Theoretical Analysis
. . . . . . 171
6.6
The Right Step Sizes . . . . . . . . . . . . . . . . . . . . . . 174
6.7
Overview of Convergence Analysis . . . . . . . . . . . . . . . 176
6.8
Convergence of Incremental Algorithms*
. . . . . . . . . . . 179
6.9
Convergence of Temporal-Diﬀerence Learning* . . . . . . . . 183
6.10 Convergence of Categorical Temporal-Diﬀerence Learning*
. 185
6.11 Technical Remarks . . . . . . . . . . . . . . . . . . . . . . . 188
6.12 Bibliographical Remarks . . . . . . . . . . . . . . . . . . . . 190
6.13 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
7
Control
197
7.1
Risk-Neutral Control . . . . . . . . . . . . . . . . . . . . . . 197
7.2
Value Iteration and Q-Learning . . . . . . . . . . . . . . . . . 199
7.3
Distributional Value Iteration . . . . . . . . . . . . . . . . . . 202
7.4
Dynamics of Distributional Optimality Operators . . . . . . . 204
7.5
Dynamics in the Presence of Multiple Optimal Policies*
. . . 209
7.6
Risk and Risk-Sensitive Control
. . . . . . . . . . . . . . . . 213
7.7
Challenges in Risk-Sensitive Control . . . . . . . . . . . . . . 214
7.8
Conditional Value-At-Risk*
. . . . . . . . . . . . . . . . . . 218
7.9
Technical Remarks . . . . . . . . . . . . . . . . . . . . . . . 222
7.10 Bibliographical Remarks . . . . . . . . . . . . . . . . . . . . 227
7.11 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
8
Statistical Functionals
233
8.1
Statistical Functionals . . . . . . . . . . . . . . . . . . . . . . 234
8.2
Moments
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
8.3
Bellman Closedness . . . . . . . . . . . . . . . . . . . . . . . 239
8.4
Statistical Functional Dynamic Programming
. . . . . . . . . 244
8.5
Relationship to Distributional Dynamic Programming . . . . . 247
8.6
Expectile Dynamic Programming . . . . . . . . . . . . . . . . 248
8.7
Inﬁnite Collections of Statistical Functionals . . . . . . . . . . 250
8.8
Moment Temporal-Diﬀerence Learning* . . . . . . . . . . . . 252
8.9
Technical Remarks . . . . . . . . . . . . . . . . . . . . . . . 254
8.10 Bibliographical Remarks . . . . . . . . . . . . . . . . . . . . 255
8.11 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
9
Linear Function Approximation
261
9.1
Function Approximation and Aliasing . . . . . . . . . . . . . 262
9.2
Optimal Linear Value Function Approximations . . . . . . . . 264
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

viii
Contents
9.3
A Projected Bellman Operator for Linear Value Function
Approximation
. . . . . . . . . . . . . . . . . . . . . . . . . 266
9.4
Semi-Gradient Temporal-Diﬀerence Learning . . . . . . . . . 270
9.5
Semi-Gradient Algorithms for Distributional Reinforcement
Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274
9.6
An Algorithm Based on Signed Distributions* . . . . . . . . . 277
9.7
Convergence of the Signed Algorithm*
. . . . . . . . . . . . 281
9.8
Technical Remarks . . . . . . . . . . . . . . . . . . . . . . . 285
9.9
Bibliographical Remarks . . . . . . . . . . . . . . . . . . . . 287
9.10 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
10 Deep Reinforcement Learning
293
10.1 Learning with a Deep Neural Network . . . . . . . . . . . . . 294
10.2 Distributional Reinforcement Learning with Deep Neural
Networks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
10.3 Implicit Parameterizations
. . . . . . . . . . . . . . . . . . . 301
10.4 Evaluation of Deep Reinforcement Learning Agents . . . . . . 304
10.5 How Predictions Shape State Representations . . . . . . . . . 309
10.6 Technical Remarks . . . . . . . . . . . . . . . . . . . . . . . 311
10.7 Bibliographical Remarks . . . . . . . . . . . . . . . . . . . . 312
10.8 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 315
11 Two Applications and a Conclusion
319
11.1 Multiagent Reinforcement Learning . . . . . . . . . . . . . . 319
11.2 Computational Neuroscience . . . . . . . . . . . . . . . . . . 323
11.3 Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . 329
11.4 Bibliographical Remarks . . . . . . . . . . . . . . . . . . . . 330
Notation
333
References
337
Index
365
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Preface
The history of this book begins one evening in November 2016, when after an
especially unfruitful day of research, Will and Marc decided to try a diﬀerent
approach to reinforcement learning. The idea took inspiration from the earlier
“Compress and Control” algorithm (Veness et al. 2015) and recent successes in
using classiﬁcation algorithms to perform regression (van den Oord et al. 2016),
yet was unfamiliar, confusing, exhilarating. Working from one of the many
whiteboards in DeepMind oﬃces at King’s Cross, there were many false starts
and much reinventing the wheel. But eventually C51, a distributional reinforce-
ment learning algorithm, came to be. The analysis of the distributional Bellman
operator proceeded in parallel with algorithmic development, and by the ICML
2017 deadline, there was a theorem regarding the contraction of this operator in
the Wasserstein distance and state-of-the-art performance at playing Atari 2600
video games. These results were swiftly followed by a second paper that aimed
to explain the fairly large gap between the contraction result and the actual C51
algorithm. The trio was completed when Mark joined for a summer internship,
and at that point the ﬁrst real theoretical results came regarding distributional
reinforcement learning algorithms. The QR-DQN, Implicit Quantile Networks
(IQN), and expectile temporal-diﬀerence learning algorithms then followed. In
parallel, we also began studying how one could theoretically explained just why
distributional reinforcement learning led to better performance in large-scale
settings; the ﬁrst results suggested said that it should not, only deepening a
mystery that we continue to work to solve today.
One of the great pleasures of working on a book together has been to be able
to take the time to produce a more complete picture of the scientiﬁc ancestry
of distributional reinforcement learning. Bellman (1957b) himself expressed
in passing that quantities other than the expected return should be of interest;
Howard and Matheson (1972) considered the question explicitly. Earlier studies
focused on a single characteristic of the return distribution, often a criterion to be
optimized: for example, the variance of the return (Sobel 1982). Similarly, many
ix
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

x
Preface
results in risk-sensitive reinforcement learning have focused on optimizing a
speciﬁc measure of risk, such as variance-penalized expectation (Mannor and
Tsitsiklis 2011) or conditional-value-at-risk (Chow and Ghavamzadeh 2014).
Our contribution to this vast body of work is perhaps to treat these criteria
and characteristics in a more uniﬁed manner, focusing squarely on the return
distribution as the main object of interest, from which everything can be derived.
We see signs of this uniﬁed treatment paying oﬀin answering related questions
(Chandak et al. 2021). Of course, we have only been able to get there because
of relatively recent advances in the study of probability metrics (Székely 2002;
Rachev et al. 2013), better tools with which to study recursive distributional
relationships (Rösler 1992; Rachev and Rüschendorf 1995), and key results
from stochastic approximation theory.
Our hope is that, by providing a more comprehensive treatment of distri-
butional reinforcement, we may pave the way for further developments in
sequential decision-making and reinforcement learning. The most immediate
eﬀects should be seen in deep reinforcement learning, which has since that ﬁrst
ICML paper used distributional predictions to improve performance across a
wide variety of problems, real and simulated. In particular, we are quite excited
to see how risk-sensitive reinforcement learning may improve the reliability
and eﬀectiveness of reinforcement learning for robotics (Vecerik et al. 2019;
Bodnar et al. 2020; Cabi et al. 2020). Research in computational neuroscience
has already demonstrated the value of taking a distributional perspective, even
to explain biological phenomena (Dabney et al. 2020b). Eventually, we hope
that our work can generally help further our understanding of what it means for
an agent to interact with its environment.
In developing the material for this book, we have been immensely lucky to
work with a few esteemed mentors, collaborators, and students who were willing
to indulge us in the ﬁrst steps of this journey. Rémi Munos was instrumental in
shaping this ﬁrst project and helping us articulate its value to DeepMind and
the scientiﬁc community. Yee Whye Teh provided invaluable advice, pointers
to the statistics literature, and lodging and eventually brought the three of us
together. Pablo Samuel Castro and Georg Ostrovski built, distilled, removed
technical hurdles, and served as the voice of reason. Clare Lyle, Philip Amortila,
Robert Dadashi, Saurabh Kumar, Nicolas Le Roux, John Martin, and Rosie
Zhao helped answer a fresh set of questions that we had until then lacked the
formal language to describe, eventually creating more problems than answers –
such is the way of science. Yunhao Tang and Harley Wiltzer gracefully accepted
to be the ﬁrst consumers of this book, and their feedback on all parts of the
notation, ideas, and manuscript has been invaluable.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Preface
xi
We are very grateful for the excellent feedback – narrative and technical –
provided to us by Adam White and our anonymous reviewers, which allowed
us to make substantial improvements on the original draft. We thank Rich
Sutton, Andy Barto, Csaba Szepesvári, Kevin Murphy, Aaron Courville, Doina
Precup, Prakash Panangaden, David Silver, Joelle Pineau, and Dale Schuur-
mans, for discussions on book writing and serving as role models on taking
an eﬀort larger than anything else we had previously done. We appreciate the
technical and conceptual input of many of our colleagues at Google, DeepMind,
Mila, and beyond: Bernardo Avila Pires, Jason Baldridge, Pierre-Luc Bacon,
Yoshua Bengio, Michael Bowling, Sal Candido, Peter Dayan, Thomas Degris,
Audrunas Gruslys, Hado van Hasselt, Shie Mannor, Volodymyr Mnih, Derek
Nowrouzezahrai, Adam Oberman, Bilal Piot, Tom Schaul, Danny Tarlow, and
Olivier Pietquin. We further thank the many people who reviewed parts of
this book and helped ﬁll in some of the gaps in our knowledge: Yinlam Chow,
Erick Delage, Pierluca D’Oro, Doug Eck, Amir-massoud Farahmand, Jesse
Farebrother, Chris Finlay, Tadashi Kozuno, Hugo Larochelle, Elliot Ludvig,
Andrea Michi, Blake Richards, Daniel Slater, and Simone Totaro. We further
thank Vektor Dewanto, Tyler Kastner, Karolis Ramanauskas, Rylan Schaeﬀer,
Eugene Tarassov, and Jun Tian for their feedback on the online draft and the
COMP-579 students at McGill University for beta-testing our presentation of
the material. We were lucky to perform this research within DeepMind and
Google Brain, which provided support both moral and material and inspiration
to take on ever larger challenges. Finally, we thank Francis Bach, Elizabeth
Swayze, Matt Valades, and the team at MIT Press for championing this work
and making it a possibility.
Marc gives further thanks to Judy Loewen, Frédéric Lavoie, Jacqueline Smith,
Madeleine Fugère, Samantha Work, Damon MacLeod, and Andreas Fidjeland,
for support along the scientiﬁc journey, and to Lauren Busheikin, for being
an incredibly supportive partner. Further thanks go to CIFAR and the Mila
academic community for providing the fertile scientiﬁc ground from which the
writing of this book began.
Will wishes to additionally thank Zeb Kurth-Nelson and Matt Botvinick for
their patience and scientiﬁc rigor as we explored distributional reinforcement
learning in neuroscience; Koray Kavukcuoglu and Demis Hassabis for their
enthusiasm and encouragement surrounding the project; Rémi Munos for sup-
porting our pursuit of random, risky research ideas; and Blair Lyonev for being
a supportive partner, providing both encouragement and advice surrounding the
challenges of writing a book.
Mark would like to thank Maciej Dunajski, Andrew Thomason, Adrian
Weller, Krzysztof Choromanski, Rich Turner, and John Aston for their
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

xii
Preface
supervision and mentorship, and his family and Kristin Goﬀe for all their
support.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

1
Introduction
A hallmark of intelligence is the ability to adapt behavior to reﬂect external
feedback. In reinforcement learning, this feedback is provided as a real-valued
quantity called the reward. Stubbing one’s toe on the dining table or forgetting
soup on the stove are situations associated with negative reward, while (for
some of us) the ﬁrst cup of coﬀee of the day is associated with positive reward.
We are interested in agents that seek to maximize their cumulative reward –
or return – obtained from interactions with an environment. An agent maximizes
its return by making decisions that either have immediate positive consequences
or steer it into a desirable state. A particular assignment of reward to states and
decisions determines the agent’s objective. For example, in the game of Go,
the objective is represented by a positive reward for winning. Meanwhile, the
objective of keeping a helicopter in ﬂight is represented by a per-step negative
reward (typically expressed as a cost) proportional to how much the aircraft
deviates from a desired ﬂight path. In this case, the agent’s return is the total
cost accrued over the duration of the ﬂight.
Often, a decision will have uncertain consequences. Travelers know that it
is almost impossible to guarantee that a trip will go as planned, even though
a three-hour layover is usually more than enough to catch a connecting ﬂight.
Nor are all decisions equal: transiting through Chicago O’Hare may be a riskier
choice than transiting through Toronto Pearson. To model this uncertainty,
reinforcement learning introduces an element of chance to the rewards and to
the eﬀects of the agent’s decisions on its environment. Because the return is the
sum of rewards received along the way, it too is random.
Historically, most of the ﬁeld’s eﬀorts have gone toward modeling the mean
of the random return. Doing so is useful, as it allows us to make the right
decisions: when we talk of “maximizing the cumulative reward,” we typically
mean “maximizing the expected return.” The idea has deep roots in probability,
1
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

2
Chapter 1
the law of large numbers, and subjective utility theory. In fact, most reinforce-
ment learning textbooks axiomatize the maximization of expectation. Quoting
Richard Bellman, for example:
The general idea, and this is fairly unanimously accepted, is to use some average of
the possible outcomes as a measure of the value of a policy.
This book takes the perspective that modeling the expected return alone
fails to account for many complex, interesting phenomena that arise from
interactions with one’s environment. This is evident in many of the decisions
that we make: the ﬁrst rule of investment states that expected proﬁts should
be weighed against volatility. Similarly, lottery tickets oﬀer negative expected
returns but attract buyers with the promise of a high payoﬀ. During a snowstorm,
relying on the average frequency at which buses arrive at a stop is likely to lead
to disappointment. More generally, hazards big and small result in a wide range
of possible returns, each with its own probability of occurrence. These returns
and their probabilities can be collectively described by a return distribution, our
main object of study.
1.1
Why Distributional Reinforcement Learning?
Just as a color photograph conveys more information about a scene than a
black and white photograph, the return distribution contains more information
about the consequences of the agent’s decisions than the expected return. The
expected return is a scalar, while the return distribution is inﬁnite-dimensional;
it is possible to compute the expectation of the return from its distribution, but
not the other way around (to continue the analogy, one cannot recover hue from
luminance).
By considering the return distribution, rather than just the expected return,
we gain a fresh perspective on the fundamental problems of reinforcement
learning. This includes understanding of how optimal decisions should be
made, methods for creating eﬀective representations of an agent’s state, and
the consequences of interacting with other learning agents. In fact, many of the
tools we develop here are useful beyond reinforcement learning and decision-
making. We call the process of computing return distributions distributional
dynamic programming. Incremental algorithms for learning return distributions,
such as quantile temporal-diﬀerence learning (Chapter 6), are likely to ﬁnd uses
wherever probabilities need to be estimated.
Throughout this book, we will encounter many examples in which we use the
tools of distributional reinforcement learning to characterize the consequences
of the agent’s choices. This is a modeling decision, rather than a reﬂection of
some underlying truth about these examples. From a theoretical perspective,
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Introduction
3
ante
ante
1.raise
2.call
win (+2)
loss (-2)
(a)
(b)
check
raise
fold
call
-1
+1
+1
+1
+1
+2
+2
+2
+1
+1
+1
+1
+1
+2
+2
+2
-2
-2
-2
-2
-2
-2
-1
-1
-1
-1
-1
-1
-1
-1
Figure 1.1
(a) In Kuhn poker, each player is dealt one card and then bets on whether they hold the
highest card. The diagram depicts one particular play through; the house’s card (bottom)
is hidden until betting is over. (b) A game tree with all possible states shaded according
to their frequency of occurrence in our example. Leaf nodes depict immediate gains and
losses, which we equate with diﬀerent values of the received reward.
justifying our use of the distributional model requires us to make a number of
probabilistic assumptions. These include the notion that the random nature of
the interactions is intrinsically irreducible (what is sometimes called aleatoric
uncertainty) and unchanging. As we encounter these examples, the reader is
invited to reﬂect on these assumptions and their eﬀect on the learning process.
Furthermore, there are many situations in which such assumptions do not
completely hold but where distributional reinforcement learning still provides
a rich picture of how the environment operates. For example, an environment
may appear random because some parts of it are not described to the agent (it
is said to be partially observable) or because the environment changes over
the course of the agent’s interactions with it (multiagent learning, the topic
of Section 11.1, is an example of this). Changes in the agent itself, such as
a change in behavior, also introduce nonstationarity in the observed data. In
practice, we have found that the return distributions are a valuable reﬂection
of the underlying phenomena, even when there is no aleatoric uncertainty at
play. Put another way, the usefulness of distributional reinforcement learning
methods does not end with the theorems that characterize them.
1.2
An Example: Kuhn Poker
Kuhn poker is a simpliﬁed variant of the well-known card game. It is played
with three cards of a single suit (jack, queen, and king) and over a single round
of betting, as depicted in Figure 1.1a. Each player is dealt one card and must
ﬁrst bet a ﬁxed ante, which for the purpose of this example we will take to be
£1. After looking at their card, the ﬁrst player decides whether to raise, which
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

4
Chapter 1
Probability
Winnings
Probability
Winnings
Probability
Winnings
Probability
Winnings
10
5
0
5
10
0.0
0.2
0.4
0.6
0.8
10
5
0
5
10
0.0
0.1
0.2
0.3
10
5
0
5
10
0.0
0.1
0.2
0.3
10
5
0
5
10
0.0
0.1
0.2
0.3
Figure 1.2
Distribution over winnings for the player after playing T rounds. For T = 1, this corre-
sponds to the distribution of immediate gains and losses. For T = 5, we see a single mode
appear roughly centered on the expected winnings. For larger T, two additional modes
appear, one in which the agent goes bankrupt and one where the player has successfully
doubled their stake. As T →∞, only these two outcomes have nonzero probability.
doubles their bet, or check. In response to a raise, the second player can call
and match the new bet or fold and lose their £1 ante. If the ﬁrst player chooses
to check instead (keep the bet as-is), the option to raise is given to the second
player, symmetrically. If neither player folded, the player with the higher card
wins the pot (£1 or £2, depending on whether the ante was raised). Figure 1.1b
visualizes a single play of the game as a ﬁfty-ﬁve-state game tree.
Consider a player who begins with £10 and plays a total of up to T hands
of Kuhn poker, stopping early if they go bankrupt or double their initial stake.
To keep things simple, we assume that this player always goes ﬁrst and that
their opponent, the house, makes decisions uniformly at random. The player’s
strategy depends on the card they are dealt and also incorporates an element of
randomness. There are two situations in which a choice must be made: whether
to raise or check at ﬁrst and whether to call or fold when the other player raises.
The following table of probabilities describes a concrete strategy as a function
of the player’s dealt card:
Holding a ...
Jack
Queen
King
Probability of raising
1/3
0
1
Probability of calling
0
2/3
1
If we associate a positive and negative reward with each round’s gains or losses,
then the agent’s random return corresponds to their total winnings at the end of
the T rounds and ranges from −10 to 10.
How likely is the player to go bankrupt? How long does it take before the
player is more likely to be ahead than not? What is the mean and variance of the
player’s winnings after T = 15 hands have been played? These three questions
(and more) can be answered by using distributional dynamic programming
to determine the distribution of returns obtained after T rounds. Figure 1.2
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Introduction
5
shows the distribution of winnings (change in money held by the player) as a
function of T. After the ﬁrst round (T = 1), the most likely outcome is to have
lost £1, but the expected reward is positive. Consequently, over time, the player
is likely to be able to achieve their objective. By the ﬁfteenth round, the player is
much more likely to have doubled their money than to have gone broke, with a
bell-shaped distribution of values in between. If the game is allowed to continue
until the end, the player has either gone bankrupt or doubled their stake. In our
example, the probability that the player comes out a winner is approximately
85 percent.
1.3
How Is Distributional Reinforcement Learning Diﬀerent?
In reinforcement learning, the value function describes the expected return
that one would counterfactually obtain from beginning in any given state. It is
reasonable to say that its fundamental object of interest – the expected return
– is a scalar and that algorithms that operate on value functions operate on
collections of scalars (one per state). On the other hand, the fundamental object
of distributional reinforcement learning is a probability distribution over returns:
the return distribution. The return distribution characterizes the probability of
diﬀerent returns that can be obtained as an agent interacts with its environment
from a given state. Distributional reinforcement learning algorithms operate on
collections of probability distributions that we call return-distribution functions
(or simply return functions).
More than a simple type substitution, going from scalars to probability distri-
butions results in changes across the spectrum of reinforcement learning topics.
In distributional reinforcement learning, equations relating scalars become equa-
tions relating random variables. For example, the Bellman equation states that
the expected return at a state x, denoted Vπ(x), equals the expectation of the
immediate reward R, plus the discounted expected return at the next state X′:
Vπ(x) = Eπ
R + γVπ(X′) | X = x] .
Here π is the agent’s policy – a description of how it chooses actions in diﬀerent
states. By contrast, the distributional Bellman equation states that the random
return at a state x, denoted Gπ(x), is itself related to the random immediate
reward and the random next-state return according to a distributional equation:1
Gπ(x)
D= R + γGπ(X′) ,
X = x .
1. Later we will consider a form that equates probability distributions directly.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

6
Chapter 1
In this case, Gπ(x), R, X′, and Gπ(X′) are random variables, and the superscript
D indicates equality between their distributions. Correctly interpreting the distri-
butional Bellman equation requires identifying the dependency between random
variables, in particular between R and X′. It also requires understanding how
discounting aﬀects the probability distribution of Gπ(x) and how to manipulate
the collection of random variables Gπ implied by the deﬁnition.
Another change concerns how we quantify the behavior of learning algo-
rithms and how we measure the quality of an agent’s predictions. Because
value functions are real-valued vectors, the distance between a value function
estimate and the desired expected return is measured as the absolute diﬀerence
between those two quantities. On the other hand, when analyzing a distribu-
tional reinforcement learning algorithm, we must instead measure the distance
between probability distributions using a probability metric. As we will see,
some probability metrics are better suited to distributional reinforcement learn-
ing than others, but no single metric can be identiﬁed as the “natural” metric
for comparing return distributions.
Implementing distributional reinforcement learning algorithms also poses
some concrete computational challenges. In general, the return distribution
is supported on a range of possible returns, and its shape can be quite com-
plex. To represent this distribution with a ﬁnite number of parameters, some
approximation is necessary; the practitioner is faced with a variety of choices
and trade-oﬀs. One approach is to discretize the support of the distribution
uniformly and assign a variable probability to each interval, what we call the
categorical representation. Another is to represent the distribution using a ﬁnite
number of uniformly weighted particles whose locations are parameterized,
called the quantile representation. In practice and in theory, we ﬁnd that the
choice of distribution representation impacts the quality of the return function
approximation and also the ease with which it can be computed.
Learning return distributions from sampled experience is also more challeng-
ing than learning to predict expected returns. The issue is particularly acute
when learning proceeds by bootstrapping: that is, when the return function
estimate at one state is learned on the basis of the estimate at successor states.
When the return function estimates are deﬁned by a deep neural network, as is
common in practice, one must also take care in choosing a loss function that is
compatible with a stochastic gradient descent scheme.
For an agent that only knows about expected returns, it is natural (almost
necessary) to deﬁne optimal behavior in terms of maximizing this quantity.
The Q-learning algorithm, which performs credit assignment by maximizing
over state-action values, learns a policy with exactly this objective in mind.
Knowledge of the return function, however, allows us to deﬁne behaviors
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Introduction
7
that depend on the full distributions of returns – what is called risk-sensitive
reinforcement learning. For example, it may be desirable to act so as to avoid
states that carry a high probability of failure or penalize decisions that have
high variance. In many circumstances, distributional reinforcement learning
enables behavior that is more robust to variations and, perhaps, better suited to
real-world applications.
1.4
Intended Audience and Organization
This book is intended for advanced undergraduates, graduate students, and
researchers who have some exposure to reinforcement learning and are inter-
ested in understanding its distributional counterpart. We present core ideas from
classical reinforcement learning as they are needed to contextualize distribu-
tional topics but often omit longer discussions and a presentation of specialized
methods in order to keep the exposition concise. The reader wishing a more
in-depth review of classical reinforcement learning is invited to consult one
of the literature’s many excellent books on the topic, including Bertsekas and
Tsitsiklis (1996), Szepesvári (2010), Bertsekas (2012), Puterman (2014), Sutton
and Barto (2018), and Meyn (2022).
Already, an exhaustive treatment of distributional reinforcement learning
would require a substantially larger book. Instead, here we emphasize key
concepts and challenges of working with return distributions, in a mathematical
language that aims to be both technically correct but also easily applied. Our
choice of topics is driven by practical considerations (such as scalability in
terms of available computational resources), a topic’s relative maturity, and our
own domains of expertise. In particular, this book contains only one chapter
about what is commonly called the control problem and focuses on dynamic
programming and temporal-diﬀerence algorithms over Monte Carlo methods.
Where appropriate, in the bibliographical remarks, we provide references on
these omitted topics. In general, we chose to include proofs when they pertain
to major results in the chapter or are instructive in their own right. We defer the
proof of a number of smaller results to exercises.
Each chapter of this book is structured like a hiking trail.2 The ﬁrst sections
(the “foothills”) introduce a concept from classical reinforcement learning and
extend it to the distributional setting. Here, a knowledge of undergraduate-level
probability theory and computer science usually suﬃces. Later sections (the
“incline”) dive into more technical points: for example, a proof of convergence
or more complex algorithms. These may be skipped without aﬀecting the
2. Based on one of the author’s experience hiking around Banﬀ, Canada.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

8
Chapter 1
reader’s understanding of the fundamentals of distributional reinforcement
learning. Finally, most chapters end on a few additional results or remarks that
are interesting yet easily omitted (the “side trail”). These are indicated by an
asterisk (∗). For the latter part of the chapter’s journey, the reader may wish to
come equipped with tools from advanced probability theory; our own references
are Billingsley (2012) and Williams (1991).
The book is divided into three parts. The ﬁrst part introduces the building
blocks of distributional reinforcement learning. We begin by introducing our
fundamental objects of study, the return distribution and the distributional
Bellman equation (Chapter 2). Chapter 3 then introduces categorical temporal-
diﬀerence learning, a simple algorithm for learning return distributions. By
the end of Chapter 3, the reader should understand the basic principles of
distributional reinforcement learning and be able to use them in simple practical
settings.
The second part develops the theory of distributional reinforcement learn-
ing. Chapter 4 introduces a language for measuring distances between return
distributions and operators for transforming with these distributions. Chapter 5
introduces the notion of a probability representation, needed to implement
distributional reinforcement learning; it subsequently considers the problem of
computing and approximating return distributions using such representations,
introducing the framework of distributional dynamic programming. Chapter 6
studies how return distributions can be learned from samples and in a incre-
mental fashion, giving a formal construction of categorical temporal-diﬀerence
learning as well as other algorithms such as quantile temporal-diﬀerence learn-
ing. Chapter 7 extends these ideas to the setting of optimal decision-making
(also called the control setting). Finally, Chapter 8 introduces a diﬀerent perspec-
tive on distributional reinforcement learning based on the notion of statistical
functionals. By the end of the second part, the reader should understand the
challenges that arise when designing distributional reinforcement learning
algorithms and the available tools to address these challenges.
The third and ﬁnal part develops distributional reinforcement learning for
practical scenarios. Chapter 9 reviews the principles of linear value function
approximation and extends these ideas to the distributional setting. Chapter 10
discusses how to combine distributional methods with deep neural networks
to obtain algorithms for deep reinforcement learning. Chapter 11 discusses the
emerging use of distributional reinforcement learning in two further domains of
research (multiagent learning and neuroscience) and concludes.
Code for examples and exercises, as well as standard implementations of the
algorithms presented here, can be found at http://distributional-rl.org.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Introduction
9
1.5
Bibliographical Remarks
1.0. The quote is due to Bellman (1957b).
1.1. Kuhn poker is due to Kuhn (1950), who gave an exhaustive characterization
of the game’s Nash equilibria. The player’s strategy used in the main text forms
part of such a Nash equilibrium.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

2
The Distribution of Returns
Training for a marathon. Growing a vegetable garden. Working toward a piano
recital. Many of life’s activities involve making decisions whose beneﬁts are
realized only later in the future (whether to run on a particular Saturday morning;
whether to add fertilizer to the soil). In reinforcement learning, these beneﬁts
are summarized by the return received following these decisions. The return is a
random quantity that describes the sum total of the consequences of a particular
activity – measured in dollars, points, bits, kilograms, kilometers, or praise.
Distributional reinforcement learning studies the random return. It asks ques-
tions such as: How should it be described, or approximated? How can it be
predicted on the basis of past observations? The overarching aim of this book
is to establish a language with which such questions can be answered. By
virtue of its subject matter, this language is somewhere at the intersection of
probability theory, statistics, operations research, and of course reinforcement
learning itself. In this chapter, we begin by studying how the random return
arises from sequential interactions and immediate rewards. From this, we estab-
lish the fundamental relationship of random returns: the distributional Bellman
equation.
2.1
Random Variables and Their Probability Distributions
A quantity that we wish to model as random can be represented via a random
variable. For example, the outcome of a coin toss can be represented with a
random variable, which may take on either the value “heads” or “tails”. We
can reason about a random variable through its probability distribution, which
speciﬁes the probability of its possible realizations.
Example 2.1. Consider driving along a country road toward a railway crossing.
There are two possible states that the crossing may be in. The crossing may
be open, in which case you can drive straight through, or it may be closed,
in which case you must wait for the train to pass and for the barriers to lift
11
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

12
Chapter 2
before driving on. We can model the state of the crossing as a random variable
C with two outcomes, “open” and “closed.” The distribution of C is speciﬁed
by a probability mass function, which provides the probability of each possible
outcome:
P(C = “open”) = p ,
P(C = “closed”) = 1 −p ,
for some p ∈[0, 1].
△
Example 2.2. Suppose we arrive at the crossing described above and the barri-
ers are down. We may model the waiting time T (in minutes) until the barriers
are open again as a uniform distribution on the interval [0, 10]; informally,
any real value between 0 and 10 is equally likely. In this case, the probability
distribution can be speciﬁed through a probability density function, a function
f : R →[0, ∞). In the case of the uniform distribution above, this function is
given by
f(t) =

1
10
if t ∈[0, 10]
0
otherwise
.
The density then provides the probability of T lying in any interval [a, b]
according to
P(T ∈[a, b]) =
Z b
a
f(t)dt .
△
In this book, we will encounter instances of random variables – such as
rewards and returns – that are discrete, have densities, or in some cases fall in
neither category. To deal with this heterogeneity, one solution is to describe
probability distributions over R using their cumulative distribution function
(CDF), which always exists. The cumulative distribution function associated
with a random variable Z is the function FZ : R →[0, 1] deﬁned by
FZ(z) = P(Z ≤z) .
In distributional reinforcement learning, common operations on random
variables include summation, multiplication by a scalar, and indexing into col-
lections of random variables. Later in the chapter, we will see how to describe
these operations in terms of cumulative distribution functions.
Example 2.3. Suppose that now we consider the random variable T ′ describing
the total waiting time experienced at the railroad crossing. If we arrive at the
crossing and it is open (C = “open”), there is no need to wait and T ′ = 0. If,
however, the barrier is closed (C = “closed”), the waiting time is distributed
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

The Distribution of Returns
13
uniformly on [0, 10]. The cumulative distribution function of T ′ is
FT ′(t) =

0
t < 0
p
t = 0
p + (1−p)t
10
0 < t < 10
1
t ≥10 .
Observe that there is a nonzero probability of T ′ taking the value 0 (which
occurs when the crossing is open), so the distribution cannot have a density. Nor
can it have a probability mass function, as there are a continuum of possible
waiting times from 0 to 10 minutes.
△
Another solution is to treat probability distributions atomically, as elements of
the space P(R) of probability distributions. In this book we favor this approach
over the use of cumulative distribution functions, as it lets us concisely express
operations on probability distributions. The probability distribution that puts
all of its mass on z ∈R, for example, is the Dirac delta denoted δz; the uniform
distribution on the interval from a to b is U([a, b]). Both distributions belong to
P(R). The distribution of the random variable T ′ in Example 2.3 also belongs
to P(R). It is a mixture of distributions and can be written in terms of its
constituent parts:
pδ0 + (1 −p)U([0, 10]) .
More formally, these objects are probability measures: functions that associate
diﬀerent subsets of outcomes with their respective probabilities. If Z is a real-
valued random variable and ν is its distribution, then for a subset S ⊆R,3 we
write
ν(S ) = P(Z ∈S ) .
In particular, the probability assigned to S by a mixture distribution is the
weighted sum of probabilities assigned by its constituent parts: for ν1, ν2 ∈P(R)
and p ∈[0, 1], we have
 pν1 + (1 −p)ν2
(S ) = pν1(S ) + (1 −p)ν2(S ) .
With this language, the cumulative distribution function of ν ∈P(R) can be
expressed as
Fν(y) = ν((−∞, y]) .
This notation also extends to distributions over outcomes that are not real-valued.
For instance, P({“open”, “closed”}) is the set of probability distributions over
the state of the railroad crossing in Example 2.1. Probability distributions (as
3. Readers expecting to see the qualiﬁer “measurable” here should consult Remark 2.1.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

14
Chapter 2
Agent
Environment
action
state
reward
Figure 2.1
Top: The Markov decision process model of the agent’s interactions with its environment.
Bottom: The same interactions, unrolled to show the sequence of random variables
(Xt, At, Rt)t≥0, beginning at time t = 0 and up to the state XT.
elements of P(R)) make it possible to express some operations on distributions
that would be unwieldy to describe in terms of random variables.
2.2
Markov Decision Processes
In reinforcement learning, an environment is any of a wide variety of systems
that emit observations, can be inﬂuenced, and persist in one form or another
over time. A data center cooling system, a remote-controlled helicopter, a stock
market, and a video game console can all be thought of as environments. An
agent interacts with its environment by making choices that have consequences
in this environment. These choices may be implemented simply as an if state-
ment in a simulator or they may require a human to perform some task in our
physical world.
We assume that interactions take place or are recorded at discrete time
intervals. These give rise to a sequential process in which at any given time
t ∈N = {0, 1, 2, …}, the current situation is described by a state Xt from a ﬁnite
set X.4 The initial state is a random variable X0 with probability distribution
ξ0 ∈P(X).
4. Things tend to become more complicated when one considers inﬁnite state spaces; see
Remark 2.3.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

The Distribution of Returns
15
The agent inﬂuences its future by choosing an action At from a ﬁnite set of
actions A. In response to this choice, the agent is provided with a real-valued
reward Rt.5 This reward indicates to the agent the usefulness or worth of its
choice. The action also aﬀects the state of the system; the new state is denoted
Xt+1. An illustration of this sequential interaction is given in Figure 2.1. The
reward and next state are modeled by the transition dynamics P : X × A →
P(R × X) of the environment, which provides the joint probability distribution
of Rt and Xt+1 in terms of the state Xt and action At. We say that Rt and Xt+1 are
drawn from this distribution:
Rt, Xt+1 ∼P(·, · | Xt, At) .
(2.1)
In particular, when Rt is discrete, Equation 2.1 can be directly interpreted as
P(Rt = r, Xt+1 = x′ | Xt = x, At = a) = P(r, x′ | x, a).
Modeling the two quantities jointly is useful in problems where the reward
depends on the next state (common in board games, where the reward is asso-
ciated with reaching a certain state) or when the state depends on the reward
(common in domains where the state keeps track of past rewards). In this book,
however, unless otherwise noted, we make the simplifying assumption that
the reward and next state are independent given Xt and At, and separate the
transition dynamics into a reward distribution and transition kernel:
Rt ∼PR(· | Xt, At)
Xt+1 ∼PX(· | Xt, At) .
A Markov decision process (MDP) is a tuple (X, A, ξ0, PX, PR) that contains
all the information needed to describe how the agent’s decisions inﬂuence
its environment. These decisions are not themselves part of the model but
instead arise from a policy. A policy is a mapping π : X →P(A) from states to
probability distributions over actions such that
At ∼π(· | Xt) .
Such policies choose the action At solely on the basis of the immediately
preceding state Xt and possibly a random draw. Technically, these are a special
subset of decision-making rules that are both stationary (they do not depend
on the time t at which the decision is to be taken, except through the state Xt)
and Markov (they do not depend on events prior to time t). Stationary Markov
policies will be enough for us for most of this book, but we will study more
general policies in Chapter 7.
5. An alternative convention is to denote the reward that follows action At by Rt+1. Here we prefer
Rt to emphasize the association between action and reward.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

16
Chapter 2
Finally, a state x∅from which no other states can be reached is called
absorbing or terminal. For all actions a ∈A, its next-state distribution is
PX(x∅| x∅, a) = 1.
Terminal states correspond to situations in which further interactions are irrele-
vant: once a game of chess is won by one of the players, for example, or once a
robot has successfully accomplished a desired task.
2.3
The Pinball Model
The game of American pinball provides a useful metaphor for how the various
pieces of a Markov decision process come together to describe real systems.
A classic American pinball machine consists of a slanted, glass-enclosed play
area ﬁlled with bumpers of various shapes and sizes. The player initiates the
game by using a retractable spring to launch a metal ball into the play area, a
process that can be likened to sampling from the initial state distribution. The
metal ball progresses through the play area by bouncing oﬀthe various bumpers
(the transition function), which reward the player with a variable number of
points (the reward function). The game ends when the ball escapes through a
gap at the bottom of the play area, to which it is drawn by gravity (the terminal
state). The player can prevent this fate by controlling the ball’s course with
a pair of ﬂippers on either side of the gap (the action space). Good players
also use the ﬂippers to aim the ball toward the most valuable bumpers or other,
special high-scoring zones and may even physically shake the pinball cabinet
(called nudging) to exert additional control. The game’s state space describes
possible arrangements of the machine’s diﬀerent moving parts, including the
ball’s location.
Turning things around, we may think of any Markov decision process as an
abstract pinball machine. Initiated by the equivalent of inserting the traditional
quarter into the machine, we call a single play through the Markov decision
process a trajectory, beginning from the random initial state and lasting until a
terminal state is reached. This trajectory is the sequence X0, A0, R0, X1, A1, . . .
of random interleaved states, actions, and rewards. We use the notation
(Xt, At, Rt)t≥0 to express this sequence compactly.
The various elements of the trajectory depend on each other according to the
rules set by the Markov decision process. These rules can be summarized by a
collection of generative equations, which tell us how we might write a program
for sampling a trajectory one variable at a time. For a time step t ∈N, let us
denote by X0:t, A0:t, and R0:t the subsequences (X0, X1, . . . , Xt), (A0, A1, . . . , At),
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

The Distribution of Returns
17
and (R0, R1, . . . , Rt), respectively. The generative equations are
X0 ∼ξ0 ;
At | (X0:t, A0:t−1, R0:t−1) ∼π( · | Xt), for all t ≥0 ;
Rt | (X0:t, A0:t, R0:t−1) ∼PR(· | Xt, At), for all t ≥0 ;
Xt+1 | (X0:t, A0:t, R0:t) ∼PX( · | Xt, At), for all t ≥0 .
We use the notation Y | (Z0, Z1, . . . ) to indicate the basic dependency structure
between these variables. The equation for At, for example, is to be interpreted
as
P At = a | X0, A0, R0, · · · , Xt−1, At−1, Rt−1, Xt) = π(a | Xt
.
For t = 0, the notation A0:t−1 and R0:t−1 denotes the empty sequence. Because
the policy ﬁxes the “decision” part of the Markov decision process formalism,
the trajectory can be viewed as a Markov chain over the space X × A × R. This
model is sometimes called a Markov reward process.
By convention, terminal states yield no reward. In these situations, it is
sensible to end the sequence at the time T ∈N at which a terminal state is ﬁrst
encountered. It is also common to notify the agent that a terminal state has
been reached. In other cases (such as Example 2.4 below), the sequence might
(theoretically) go on forever.
We use the notion of a joint distribution to formally ask questions (and
give answers) about the random trajectory. This is the joint distribution over
all random variables involved, denoted Pπ(·). For example, the probability
that the agent begins in state x and ﬁnds itself in that state again after t time
steps when acting according to a policy π can be mathematically expressed
as Pπ(X0 = x, Xt = x). Similarly, the probability that a positive reward will be
received at some point in time is
Pπ(there exists t ≥0 such that Rt > 0) .
The explicit policy subscript is a convention to emphasize that the agent’s
choices aﬀect the distribution of outcomes. It also lets us distinguish statements
about random variables derived from the random trajectory from statements
about other, arbitrary random variables.
The joint distribution Pπ gives rise to the expectation Eπ over real-valued
random variables. This allows us to write statements such as
Eπ[2 × R0 + 1{X1 = X0}R1] .
Remark 2.1 provides additional technical details on how this expectation can be
constructed from Pπ.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

18
Chapter 2
Example 2.4. The martingale is a betting strategy popular in the eighteenth
century and based on the principle of doubling one’s ante until a proﬁt is made.
This strategy is formalized as a Markov decision process where the (inﬁnite)
state space X = Z is the gambler’s loss thus far, with negative losses denoting
gains. The action space A = N is the gambler’s bet.6 If the game is fair, then for
each state x and each action a,
PX(x + a | x, a) = PX(x −a | x, a) = 1/2.
Placing no bet corresponds to a = 0. With X0 = 0, the martingale policy is A0 = 1
and for t > 0,
At =
( Xt + 1
Xt > 0
0
otherwise.
Formally speaking, the policy maps each loss x > 0 to δx+1, the Dirac distribution
at x + 1, and all negative states (gains) to δ0. Simple algebra shows that for t > 0,
Xt =
( 2t −1
with probability 2−t,
−1
with probability 1 −2−t .
That is, the gambler is assured to eventually make a proﬁt (since 2−t →0), but
arbitrary losses may be incurred before a positive gain is made. Calculations
show that the martingale strategy has nil expected gain (Eπ[Xt] = 0 for t ≥
0), while the variance of the loss grows unboundedly with the number of
rounds played (Eπ[X2
t ] →∞); as a result, it is frowned upon in many gambling
establishments.
△
The notation Pπ makes clear the dependence of the distribution of the random
trajectory (Xt, At, Rt)t≥0 on the agent’s policy π. Often, we ﬁnd it also useful
to view the initial state distribution ξ0 as a parameter to be reasoned explicitly
about. In this case, it makes sense to more explicitly denote the joint distribution
by Pξ0,π. The most common situation is when we consider the same Markov
decision process but initialized at a speciﬁc starting state x. In this case, the
distribution becomes ξ0 = δx, meaning that Pξ0,π(X0 = x) = 1. We then use the
fairly standard shorthand
Pπ( · | X0 = x) = Pδx,π( · ) ,
Eπ[ · | X0 = x] = Eδx,π[ · ] .
Technically, this use of the conditioning bar is an abuse of notation. We are
directly modifying the probability distributions of these random variables, rather
than conditioning on an event as the notation normally signiﬁes.7 However,
this notation is convenient and common throughout the reinforcement learning
6. We are ignoring cash ﬂow issues here.
7. If we attempt to use the actual conditional probability distribution instead, we ﬁnd that it is
ill-deﬁned when ξ0(x) = 0. See Exercise 2.2.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

The Distribution of Returns
19
literature, and we therefore adopt it as well. It is also convenient to modify the
distribution of the ﬁrst action A0, so that rather than this random variable being
sampled from π( · | X0), it is ﬁxed at some action a ∈A. We use similar notation
as above to signify the resulting distribution over trajectories and corresponding
expectations:
Pπ( · | X0 = x, A0 = a) ,
Eπ[ · | X0 = x, A0 = a] .
2.4
The Return
Given a discount factor γ ∈[0, 1), the discounted return8 (or simply the return)
is the sum of rewards received by the agent from the initial state onward,
discounted according to their time of occurrence:
G =
∞
X
t=0
γtRt .
(2.2)
The return is a sum of scaled, real-valued random variables and is therefore
itself a random variable. In reinforcement learning, the success of an agent’s
decisions is measured in terms of the return that it achieves: greater returns are
better. Because it is the measure of success, the return is the fundamental object
of reinforcement learning.
The discount factor encodes a preference to receive rewards sooner than
later. In settings that lack a terminal state, the discount factor is also used to
guarantee that the return G exists and is ﬁnite. This is easily seen when rewards
are bounded on the interval [Rmin, Rmax], in which case we have
G ∈
" Rmin
1 −γ, Rmax
1 −γ
#
.
(2.3)
Throughout this book, we will often write Vmin and Vmax for the endpoints of the
interval above, denoting the smallest and largest return obtainable when rewards
are bounded on [Rmin, Rmax]. When rewards are not bounded, the existence of
G is still guaranteed under the mild assumption that rewards have ﬁnite ﬁrst
moment; we therefore adopt this assumption throughout the book.
Assumption 2.5. For each state x ∈X and action a ∈A, the reward distribution
PR( · | x, a) has ﬁnite ﬁrst moment. That is, if R ∼PR( · | x, a), then
E |R| < ∞.
△
8. In parts of this book, we also consider the undiscounted return P∞
t=0 Rt. We sometimes write
γ = 1 to denote this return, indicating a change from the usual setting. One should be mindful that
we then need to make sure that the sum of rewards converges.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

20
Chapter 2
Proposition 2.6. Under Assumption 2.5, the random return G exists and
is ﬁnite with probability 1, in the sense that
Pπ

G ∈(−∞, ∞)

= 1 .
△
In the usual treatment of reinforcement learning, Assumption 2.5 is taken
for granted (there is little to be predicted otherwise). We still ﬁnd Proposition
2.6 useful as it establishes that the assumption is suﬃcient to guarantee the
ﬁniteness of G and hence the existence of its probability distribution. The phrase
“with probability 1” allows for the fact that in some realizations of the random
trajectory, G is inﬁnite (for example, if the rewards are normally distributed) –
but the probability of observing such a realization is nil. Remark 2.2 gives
more details on this topic, along with a proof of Proposition 2.6; see also
Exercise 2.17.
We call the probability distribution of G the return distribution. The return
distribution determines quantities such as the expected return
Eπ
G = Eπ
h ∞
X
t=0
γtRt
i
,
which plays a central role in reinforcement learning, the variance of the return
Varπ
 G = Eπ
(G −Eπ[G])2 ,
and tail probabilities such as
Pπ
 ∞
X
t=0
γtRt ≥0

,
which arise in risk-sensitive control problems (discussed in Chapter 7). As the
following examples illustrate, the probability distributions of random returns
vary from simple to intricate, according to the complexity of interactions
between the agent and its environment.
Example 2.7 (Blackjack). The card game of blackjack is won by drawing
cards whose total value is greater than that of a dealer, who plays according to
a known, ﬁxed set of rules. Face cards count for 10, while aces may be counted
as either 1 or 11, to the player’s preference. The game begins with the player
receiving two cards, which they can complement by hitting (i.e., requesting
another card). The game is lost immediately if the player’s total goes over 21,
called going bust. A player satisﬁed with their cards may instead stick, at which
point the dealer adds cards to their own hand, until a total of 17 or more is
reached.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

The Distribution of Returns
21
If the player’s total is greater than the dealer’s or the dealer goes bust, they
are paid out their ante; otherwise, the ante is lost. In the event of a tie, the player
keeps the ante. We formalize this as receiving a reward of 1, −1, or 0 when play
concludes. A blackjack occurs when the player’s initial cards are an ace and a
10-valued card, which most casinos will pay 3 to 2 (a reward of 3/2) provided
the dealer’s cards sum to less than 21. After seeing their initial two cards, the
player may also choose to double down and receive exactly one additional card.
In this case, wins and losses are doubled (2 or −2 reward).
Since the game terminates in a ﬁnite number of steps T and the objective is
to maximize one’s proﬁt, let us equate the return G with the payoﬀfrom playing
one hand. The return takes on values from the set
{−2, −1, 0, 1, 3/2, 2} .
Let us denote the cards dealt to the player by C0,C1, . . . ,CT, the sum of these
cards by Yp, and the dealer’s card sum by Yd. With this notation, we can handle
the ace’s two possible values by adding 10 to Yp or Yd when at least one ace
was drawn, and the sum is 11 or less. We deﬁne Yp = 0 and Yd = 0 when the
player or dealer goes bust, respectively. Consider a player who doubles when
dealt cards whose total is 11. The probability distribution of the return is
Pπ(G = 3/2)
(a)= 2Pπ(C0 = 1,C1 = 10, Yd , 21)
Pπ(G = −2)
(b)= Pπ(C0 +C1 = 11,C0 , 1,C1 , 1,C0 +C1 +C2 < Yd)
Pπ(G = 2)
(c)= Pπ(C0 +C1 = 11,C0 , 1,C1 , 1,C0 +C1 +C2 > Yd)
Pπ(G = −1) = Pπ(Yp < Yd) −Pπ(G = −2)
Pπ(G = 0) = Pπ(Yp = Yd)
Pπ(G = 1) = Pπ(Yp > Yd) −Pπ(G = 3/2) −Pπ(G = 2) ;
(a) implements the blackjack rule (noting that either C0 or C1 can be an ace)
while (b) and (c) handle doubling (when there is no blackjack). For example, if
we assume that cards are drawn with replacement, then
P(Yd = 21) ≈0.12
and
Pπ(G = 3/2) = 2 ×  1 −Pπ(Yd = 21) × 1
13
4
13 ≈0.042 ,
since there are thirteen card types to drawn from, one of which is an ace and
four of which have value 10.
Computing the probabilities for other outcomes requires specifying the
player’s policy in full (when do they hit?). Were we to do this, we could
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

22
Chapter 2
then estimate these probabilities from simulation or, more tediously, calculate
them by hand.
△
Example 2.8. Consider a simple solitaire game that involves repeatedly throw-
ing a single six-sided die. If a 1 is rolled, the game ends immediately. Otherwise,
the player receives a point and continues to play. Consider the undiscounted
return
∞
X
t=0
Rt = 1 + 1 + · · · + 1
|           {z           }
Ttimes
,
where T is the time at which the terminating 1 is rolled. This is an integer-valued
return ranging from 0 to ∞. It has the geometric distribution
Pπ
 ∞
X
t=0
Rt = k

= 1
6
  5
6
k ,
k ∈N
corresponding to the probability of seeing k successes before the ﬁrst failure
(rolling a 1). Choosing a discount factor less than 1 (perhaps modeling the
player’s increasing boredom) changes the support of this distribution but not the
associated probabilities. The partial sums of the geometric series correspond to
k
X
t=0
γt = 1 −γk+1
1 −γ
and it follows that for k ≥0,
Pπ
 ∞
X
t=0
γtRt = 1−γk
1−γ

= 1
6
  5
6
k .
△
When a closed-form solution is tedious or diﬃcult to obtain, we can some-
times still estimate the return distribution from simulations. This is illustrated
in the next example.
Example 2.9. The Cliﬀs of Moher, located in County Clare, Ireland, are famous
for both their stunning views and the strong winds blowing from the Atlantic
Ocean. Inspired by the scenic walk from the nearby village of Doolin to the
Cliﬀs’ tourist center, we consider an abstract cliﬀenvironment (Figure 2.2)
based on a classic domain from the reinforcement learning literature (Sutton
and Barto 2018).9
The walk begins in the cell “S” (Doolin village) and ends with a positive
reward (+1) when the “G” cell (tourist center) is reached. Four actions are
available, corresponding to each of the cardinal directions – however, at each
step, there is a probability p = 0.25 that the strong winds take the agent in an
9. There are a few diﬀerences, which we invite the reader to discover.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

The Distribution of Returns
23
Cliffs
S
G
Safe policy
Quick policy
1.0
0.5
0.0
0.5
1.0
Return
0
2
4
6
8
10
Probability density
Quick policy
Safe policy
Figure 2.2
Left: The Cliﬀs environment, along with the path preferred by the quick and safe
policies. Right: The return distribution for these policies, estimated by sampling 100,000
trajectories from the environment. The ﬁgure reports the distribution as a probability
density function computed using kernel density estimation (with bandwidth 0.02).
unintended (random) direction. For simplicity, the edges of the grid act as walls.
We model falling oﬀthe cliﬀusing a negative reward of −1, at which point
the episode also terminates. Reaching the tourist center yields a reward of +1
and also ends the episode. The reward is zero elsewhere. A discount factor of
γ = 0.99 incentivizes the agent to not dally too much along the way.
Figure 2.2 depicts the return distribution for two policies: a quick policy that
walks along the cliﬀ’s edge and a safe policy that walks two cells away from
the edge.10 The corresponding returns are bimodal, reﬂecting the two possible
classes of outcomes. The return distribution of the faster policy, in particular,
is sharply peaked around −1 and 1: the goal may be reached quickly, but the
agent is more likely to fall.
△
The next two examples show how even simple dynamics that one might
reasonably encounter in real scenarios can result in return distributions that are
markedly diﬀerent from the reward distributions.
Example 2.10. Consider a single-state, single-action Markov decision process,
so that X = {x} and A = {a}. The initial distribution is ξ0 = δx and the transition
kernel is PX(x | x, a) = 1. The reward has a Bernoulli distribution, with
PR(0 | x, a) = PR(1 | x, a) = 1/2 .
Suppose we take the discount factor to be γ = 1/2. The return is
G = R0 + 1
2R1 + 1
4R2 + · · · .
(2.4)
10. More precisely, the safe policy always attempts to move up when it is possible to do so, unless
it is in one of the rightmost cells. The quick policy simply moves right until one of these cells is
reached; afterward, it goes down toward the “G” cell.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

24
Chapter 2
1.5
1.0
0.5
2.0
0.0
0.5
1.0
Cumulative Probability
Return
1.0
0.5
0.0
0.5
1.0
Cumulative Probability
Return
Figure 2.3
Left: Illustration of Example 2.10, showing cumulative distribution functions (CDFs)
for the truncated return G0:T = PT
t=0

1
2
t Rt as T increases. The number of steps in this
function doubles with each increment of T, reaching the uniform distribution in the limit
as T →∞. Right: The same illustration, now for Example 2.11.
We will show that G has a uniform probability distribution over the interval
[0, 2]. Observe that the right-hand side of Equation 2.4 is equivalent to the
binary number
R0.R1R2…
(2.5)
Since Rt ∈{0, 1} for all t, this implies that the support of G is the set of numbers
that have a (possibly inﬁnite) binary expansion as in Equation 2.5. The smallest
such number is clearly 0, while the largest number is 2 (the inﬁnite sequence of
1s). Now
Pπ(G ∈[0, 1]) = Pπ(R0 = 0)
Pπ(G ∈[1, 2]) = Pπ(R0 = 1) ,
so that it is equally likely that G falls either in the interval [0, 1] or [1, 2].11 If
we now subdivide the lower interval, we ﬁnd that
Pπ(G ∈[0.5, 1]) = Pπ(G ∈[0, 1])Pπ(G ≥0.5 | G ∈[0, 1])
= Pπ(R0 = 0)Pπ(R1 = 1 | R0 = 0)
= Pπ(R0 = 0)Pπ(R1 = 1) ,
and analogously for the intervals [0, 0.5], [1, 1.5], and [1.5, 2]. We can repeat this
subdivision recursively to ﬁnd that any dyadic interval [a, b] whose endpoints
belong to the set
Y =
n
n
X
j=0
  1
2
ja j : n ∈N , aj ∈{0, 1} for 0 ≤j ≤n
o
11. The probability that G = 1 is zero.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

The Distribution of Returns
25
has probability b−a
2 : that is,
Pπ
 G ∈[a, b] = b −a
2
for a, b ∈Y, a < b .
(2.6)
Because the uniform distribution over the interval [0, 2] satisﬁes Equation
2.6, we conclude that G is distributed uniformly on [0, 2]. A formal argument
requires us to demonstrate that Equation 2.6 uniquely determines the cumulative
distribution function of G (Exercise 2.6); this argument is illustrated (informally)
in Figure 2.3.
△
Example 2.11 (*). If we substitute the reward distribution of the previous
example by
PR(0 | x, a) = PR(2/3 | x, a) = 1/2
and take γ = 1/3, the return distribution becomes the Cantor distribution (see
Exercise 2.7). The Cantor distribution has no atoms (values with probability
greater than zero) or a probability density. Its cumulative distribution function is
the Cantor function (see Figure 2.3), famous for violating many of our intuitions
about mathematical analysis.
△
2.5
The Bellman Equation
The cliﬀ-walking scenario (Example 2.9) shows how diﬀerent policies can lead
to qualitatively diﬀerent return distributions: one where a positive reward for
reaching the goal is likely and one where high rewards are likely, but where
there is also a substantial chance of a low reward (due to a fall). Which should be
preferred? In reinforcement learning, the canonical way to answer this question
is to reduce return distributions to scalar values, which can be directly compared.
More precisely, we measure the quality of a policy by the expected value of its
random return,12 or simply expected return:
Eπ
h ∞
X
t=0
γtRt
i
.
(2.7)
Being able to determine the expected return of a given policy is thus central
to most reinforcement learning algorithms. A straightforward approach is to
enumerate all possible realizations of the random trajectory (Xt, Rt, At)t≥0 up to
length T ∈N. By weighting them according to their probability, we obtain the
approximation
Eπ
h T−1
X
t=0
γtRt
i
.
(2.8)
12. Our assumption that the rewards have ﬁnite ﬁrst moment (Assumption 2.5) guarantees that the
expectation in Expression 2.7 is ﬁnite.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

26
Chapter 2
However, even for reasonably small T, this is problematic, because the num-
ber of partial trajectories of length T ∈N may grow exponentially with T.13
Even for problems as small as cliﬀ-walking, enumeration quickly becomes
impractical. The solution lies in the Bellman equation, which provides a concise
characterization of the expected return under a given policy.
To begin, consider the expected return for a policy π from an initial state x.
This is called the value of x, written
Vπ(x) = Eπ
 ∞
X
t=0
γtRt
 X0 = x

.
(2.9)
The value function Vπ describes the value at all states. As the name implies, it
is formally a mapping from a state to its expected return under policy π. The
value function lets us answer counterfactual questions (“how well would the
agent do from this state?”) and also allows us to determine the expected return
in Equation 2.7, since (by the generative equations)
Eπ
h
Vπ(X0)
i
= Eπ
h ∞
X
t=0
γtRt
i
.
By linearity of expectations, the expected return can be decomposed into an
immediate reward R0 (which depends on the initial state X0) and the sum of
future rewards:
Eπ
h ∞
X
t=0
γtRt
i
= Eπ
h
R0 +
∞
X
t=1
γtRt
|   {z   }
future rewards
i
.
The Bellman equation expresses this relationship in terms of the value of
diﬀerent states; we give its proof in the next section.
Proposition 2.12 (The Bellman equation). Let Vπ be the value function
of policy π. Then for any state x ∈X, it holds that
Vπ(x) = Eπ
h
R0 + γVπ(X1) | X0 = x
i
.
(2.10)
△
The Bellman equation transforms an inﬁnite sum (Equation 2.7) into a recur-
sive relationship between the value of a state x, its expected reward, and the
value of its successor states. This makes it possible to devise eﬃcient algorithms
for determining the value function Vπ, as we shall see later.
13. If rewards are drawn from a continuous distribution, there is in fact an inﬁnite number of
possible sequences of length T.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

The Distribution of Returns
27
(b)
(a)
Figure 2.4
(a) An example Markov decision process with three states x, y, z and a terminal state
denoted by a double circle. Action a gives a reward of 0 or 1 depending on the state,
while action b gives a reward of 0 in state y and 2 in state z. In state x, action b
gives a random reward of either 0 or 2 with equal probability. Arrows indicate the
(deterministic) transition kernel from each state. (b) An illustration of the Markov
property: the subsequences rooted in state X1 = z have the same probability distribution;
and the time-homogeneity property: the subsequence rooted at X2 = x has the same
probability distribution as the full sequence beginning in X0 = x.
An alternative to the value function is the state-action value function Qπ. The
state-action value function describes the expected return when the action A0 is
ﬁxed to a particular choice a:
Qπ(x, a) = Eπ
 ∞
X
t=0
γtRt | X0 = x, A0 = a

.
Throughout this book, we will see a few situations for which this form is
preferred, either because it simpliﬁes the exposition or makes certain algorithms
possible. Unless otherwise noted, the results we will present hold for both value
functions and state-action value functions.
2.6
Properties of the Random Trajectory
The Bellman equation is made possible by two fundamental properties of the
Markov decision process, time-homogeneity and the Markov property. More
precisely, these are properties of the random trajectory (Xt, Rt, At)t≥0, whose
probability distribution is described by the generative equations of Section 2.3.
To understand the distribution of the random trajectory (Xt, At, Rt)t≥0, it is
helpful to depict its possible outcomes as an inﬁnite tree (Figure 2.4). The root
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

28
Chapter 2
of the tree is the initial state X0, and each level consists of a state-action-reward
triple, drawn according to the generative equations. Each branch of the tree
then corresponds to a realization of the trajectory. Most of the quantities that
we are interested in can be extracted by “slicing” this tree in particular ways.
For example, the random return corresponds to the sum of the rewards along
each branch, discounted according to their level.
In order to relate the probability distributions of diﬀerent parts of the ran-
dom trajectory, let us introduce the notation D(Z) to denote the probability
distribution of a random variable Z. When Z is real-valued we have, for S ⊆R,
D(Z)(S ) = P(Z ∈S ) .
One advantage of the D(·) notation is that it often avoids unnecessarily introduc-
ing (and formally characterizing) such a subset S and is more easily extended
to other kinds of random variables. Importantly, we write Dπ to refer to the
distribution of random variables derived from the joint distribution Pπ.14 For
example, Dπ(R0 + R1) and Dπ(X2) are the probability distribution of the sum
R0 + R1 and of the third state in the random trajectory, respectively.
The Markov property states that the trajectory from a given state x is inde-
pendent of the states, actions, and rewards that were encountered prior to x.
Graphically, this means that for a given level of the tree k, the identity of the
state Xk suﬃces to determine the probability distribution of the subtree rooted
at that state.
Lemma 2.13 (Markov property). The trajectory (Xt, At, Rt)t≥0 has the Markov
property. That is, for any k ∈N, we have
Dπ

(Xt, At, Rt)∞
t=k | (Xt, At, Rt)k−1
t=0 = (xt, at, rt)k−1
t=0 , Xk = x

= Dπ

(Xt, At, Rt)∞
t=k | Xk = x

whenever the conditional distribution of the left-hand side is deﬁned.
△
As a concrete example, Figure 2.4 shows that there are two realizations of
the trajectory for which X1 = z. By the Markov property, the expected return
from this point on is independent of whether the immediately preceding reward
(R0) was 0 or 2.
One consequence of the Markov property is that the expectation of the
discounted return from time k is independent of the trajectory prior to time k,
given Xk:
Eπ
h ∞
X
t=k
γtRt | (Xt, At, Rt)k−1
t=0 = (xt, at, rt)k−1
t=0 , Xk = x
i
= Eπ
h ∞
X
t=k
γtRt | Xk = x
i
.
14. Because the random trajectory is inﬁnite, the technical deﬁnition of Dπ requires some care and
is given in Remark 2.4.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

The Distribution of Returns
29
Time-homogeneity states that the distribution of the trajectory from a state x
does not depend on the time k at which this state is visited. Graphically, this
means that for a given state x, the probability distribution of the subtree rooted
in that state is the same irrespective of the level k at which it occurs.
Lemma 2.14 (Time-homogeneity). The trajectory (Xt, At, Rt)t≥0 is time-
homogeneous, in the sense that for all k ∈N,
Dπ

(Xt, At, Rt)∞
t=k | Xk = x

= Dδx,π

(Xt, At, Rt)∞
t=0

,
(2.11)
whenever the conditional distribution on the left-hand side is deﬁned.
△
While the left-hand side of Equation 2.11 is a proper conditional distribution,
the right-hand side is derived from Pπ( · | X0 = x) and by convention is not a
conditional distribution. With this is mind, this is why we write Dδx,π rather
than the shorthand Dπ( · | X0 = x).
Lemmas 2.13 and 2.14 follow as consequences of the deﬁnition of the trajec-
tory distribution. A formal proof requires some measure-theoretic treatment;
we provide a discussion of some of the considerations in Remark 2.4.
Proof of Proposition 2.12 (the Bellman equation). The result follows straight-
forwardly from Lemmas 2.13 and 2.14. We have
Vπ(x) = Eπ
h ∞
X
t=0
γtRt | X0 = x
i
(a)= Eπ
h
R0 | X0 = x
i
+ γ Eπ
h ∞
X
t=1
γt−1Rt | X0 = x
i
(b)= Eπ
h
R0 | X0 = x
i
+ γEπ
"
Eπ
h ∞
X
t=1
γt−1Rt | X0 = x, A0, X1
i
| X0 = x
#
(c)= Eπ
h
R0 | X0 = x
i
+ γEπ
"
Eπ
h ∞
X
t=1
γt−1Rt | X1
i
| X0 = x
#
(d)= Eπ
h
R0 | X0 = x
i
+ γEπ
h
Vπ(X1) | X0 = x
i
= Eπ
R0 + γVπ(X1) | X0 = x ,
where (a) is due to the linearity of expectations, (b) follows by the law of
total expectation, (c) follows by the Markov property, and (d) is due to time-
homogeneity and the deﬁnition of Vπ.
The Bellman equation states that the expected value of a state can be
expressed in terms of the immediate action A0, reward R0, and the succes-
sor state X1, omitting the rest of the trajectory. It is therefore convenient to
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

30
Chapter 2
deﬁne a generative model that only considers these three random variables along
with the initial state X0. Let ξ ∈P(X) be a distribution over states. The sam-
ple transition model assigns a probability distribution to the tuple (X, A, R, X′)
taking values in X × A × R × X according to
X ∼ξ ;
A | X ∼π( · | X) ;
R | (X, A) ∼PR( · | X, A) ;
X′ | (X, A, R) ∼PX( · | X, A) .
(2.12)
We also write Pπ for the joint distribution of these random variables. We will
often ﬁnd it useful to consider a single source state x, such that as before
ξ = δx. We write (X = x, A, R, X′) for the resulting random tuple, with probability
distribution and expectation
Pπ( · | X = x) and Eπ[ · | X = x] .
The sample transition model allows us to omit time indices in the Bellman
equation, which simpliﬁes to
Vπ(x) = Eπ
R + γVπ(X′) | X = x .
In the sample transition model, we call ξ the state distribution. It is generally
diﬀerent from the initial state distribution ξ0, which describes a property of the
environment. In Chapters 3 and 6, we will use the state distribution to model
part of a learning algorithm’s behavior.
2.7
The Random-Variable Bellman Equation
The Bellman equation characterizes the expected value of the random return
from any state x compactly, allowing us to reduce the generative equations
(an inﬁnite sequence) to the sample transition model. In fact, we can leverage
time-homogeneity and the Markov property to characterize all aspects of the
random return in this manner. Consider again the deﬁnition of this return as a
discounted sum of random rewards:
G =
∞
X
t=0
γtRt .
As with value functions, we would like to relate the return from the initial state
to the random returns that occur downstream in the trajectory. To this end, let
us deﬁne the return function
Gπ(x) =
∞
X
t=0
γtRt,
X0 = x ,
(2.13)
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

The Distribution of Returns
31
which describes the return obtained when acting according to π starting from a
given state x. Note that in this deﬁnition, the notation X0 = x again modiﬁes the
initial state distribution ξ0. Equation 2.13 is thus understood as “the discounted
sum of random rewards described by the generative equations with ξ0 = δx.”
Formally, Gπ is a collection of random variables indexed by an initial state x,
each generated by a random trajectory (Xt, At, Rt)t≥0 under the distribution Pπ(· |
X0 = x). Because Equation 2.13 is concerned with random variables, we will
sometimes ﬁnd it convenient to be more precise and call it the return-variable
function.15
The inﬁnite tree of Figure 2.4 illustrates the abstract process by which
one might generate realizations from the random variable Gπ(x). We begin
at the root, whose value is ﬁxed to X0 = x. Each level is drawn by sampling,
in succession, the action At, the reward Rt, and ﬁnally the successor state
Xt+1, according to the generative equations. The return Gπ(x) accumulates the
discounted rewards (γtRt)t≥0 along the way.
The nature of random variables poses a challenge to converting this generative
process into a recursive formulation like the Bellman equation. It is tempting to
try and formulate a distributional version of the Bellman equation by deﬁning a
collection of random variables ˜Gπ according to the relationship
˜Gπ(x) = R + γ ˜Gπ(X′),
X = x
(2.14)
for each x ∈X, in direct analogy with the Bellman equation for expected returns.
However, these random variables ˜Gπ do not have the same distribution as the
random returns Gπ. The following example illustrates this point.
Example 2.15. Consider the single-state Markov decision process of Example
2.10, for which the reward R has a Bernoulli distribution with parameter 1/2.
Equation 2.14 becomes
˜Gπ(x) = R + γ ˜Gπ(x) ,
X = x ,
which can be rearranged to give
˜Gπ(x) =
∞
X
t=0
γtR =
1
1 −γR .
Since R is either 0 or 1, we deduce that
˜Gπ(x) =

0
with probability 1/2
1
1−γ
with probability 1/2 .
15. One might wonder about the joint distribution of the random returns (Gπ(x) : x ∈X). In this
book, we will (perhaps surprisingly) not need to specify this joint distribution; however, it is valid
to conceptualize these random variables as independent for concreteness.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

32
Chapter 2
This is diﬀerent from the uniform distribution identiﬁed in Example 2.10, in the
case γ = 1/2.
△
The issue more generally with Equation 2.14 is that it eﬀectively reuses the
reward R and successor state X′ across multiple visits to the initial state X = x,
which in general is not what we intend and violates the structure of the MDP in
question.16 Put another way, Equation 2.14 fails because it does not correctly
handle the joint distribution of the sequence of rewards (Rt)t≥0 and states (Xt)t≥0
encountered along a trajectory. This phenomenon is one diﬀerence between
distributional and classical reinforcement learning; in the latter case, the issue
is avoided thanks to the linearity of expectations.
The solution is to appeal to the notion of equality in distribution. We say that
two random variables Z1, Z2 are equal in distribution, denoted
Z1
D= Z2 ,
if their probability distributions are equal. This is eﬀectively shorthand for
D(Z1) = D(Z2) .
Equality in distribution can be thought of as breaking up the dependency of
the two random variables on their sample spaces to compare them solely on
the basis of their probability distributions. This avoids the problem posed by
directly equating random variables.
Proposition 2.16 (The random-variable Bellman equation). Let Gπ
be the return-variable function of policy π. For a sample transition
(X = x, A, R, X′) independent of Gπ, it holds that for any state x ∈X,
Gπ(x)
D= R + γGπ(X′),
X = x .
(2.15)
△
From a generative perspective, the random-variable Bellman equation states
that we can draw a sample return by sampling an immediate reward R and
successor state X′ and then recursively generating a sample return from X′.
Proof of Proposition 2.16. Fix x ∈X and let ξ0 = δx. Consider the (partial)
random return
Gk:∞=
∞
X
t=k
γt−kRt,
k ∈N .
16. Computer scientists may ﬁnd it useful to view Equation 2.14 as simulating draws from R and
X′ with a pseudo-random number generator that is reinitialized to the same state after each use.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

The Distribution of Returns
33
In particular, Gπ(x)
D=G0:∞under the distribution Pπ( · | X0 = x). Following an
analogous chain of reasoning as in the proof of Proposition 2.12, we decompose
the return G0:∞into the immediate reward and the rewards obtained later in the
trajectory:
G0:∞= R0 + γG1:∞.
We can decompose this further based on the state occupied at time 1:
G0:∞= R0 + γ
X
x′∈X
1{X1 = x′}G1:∞.
Now, by the Markov property, on the event that X1 = x′, the return obtained from
that point on is independent of the reward R0. Further, by the time-homogeneity
property, on the same event, the return G1:∞is equal in distribution to the return
obtained when the episode begins at state x′. Thus, we have
R0 + γ
X
x′∈X
1{X1 = x′}G1:∞
D= R0 + γ
X
x′∈X
1{X1 = x′}Gπ(x′) = R0 + γGπ(X1) ,
and hence
Gπ(x)
D= R0 + γGπ(X1),
X0 = x.
The result follows by equality of distribution between (X0, A0, R0, X1) with the
sample transition model (X = x, A, R, X′) when X0 = x.
Note that Proposition 2.16 implies the standard Bellman equation, in the
sense that the latter is obtained by taking expectations on both sides of the
distributional equation:
Gπ(x)
D= R + γGπ(X′),
X = x
=⇒Dπ
 Gπ(x) = Dπ
 R + γGπ(X′) | X = x
=⇒Eπ[Gπ(x)] = Eπ
R + γGπ(X′) | X = x
=⇒Vπ(x) = Eπ
R + γVπ(X′) | X = x
where we made use of the linearity of expectations as well as the independence
of X′ from the random variables (Gπ(x) : x ∈X).
2.8
From Random Variables to Probability Distributions
Random variables provide an intuitive language with which to express how
rewards are combined to form the random return. However, a proper random-
variable Bellman equation requires the notion of equality in distribution to
avoid incorrectly reusing realizations of the random reward R and next-state X′.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

34
Chapter 2
As a consequence, Equation 2.15 is somewhat incomplete: it characterizes the
distribution of the random return from x, Gπ(x), but not the random variable
itself.
A natural alternative is to do away with the return-variable function Gπ and
directly relate the distribution of the random return at diﬀerent states. The result
is what we may properly call the distributional Bellman equation. Working
with probability distributions requires mathematical notation that is somewhat
unintuitive compared to the simple addition and multiplication of random
variables but is free of technical snags. With precision in mind, this is why we
call Equation 2.15 the random-variable Bellman equation.
For a real-valued random variable Z with probability distribution ν ∈P(R),
recall the notation
ν(S ) = P(Z ∈S ) ,
S ⊆R .
This allows us to consider the probability assigned to S by ν more directly,
without referring to Z. For each state x ∈X, let us denote the distribution of the
random variable Gπ(x) by ηπ(x). Using this notation, we have
ηπ(x)(S ) = P(Gπ(x) ∈S ) ,
S ⊆R .
We call the collection of these per-state distributions the return-distribution
function. Each ηπ(x) is a member of the space P(R) of probability distributions
over the reals. Accordingly, the space of return-distribution functions is denoted
P(R)X.
To understand how the random-variable Bellman equation translates to the
language of probability distributions, consider that the right-hand side of Equa-
tion 2.15 involves three operations on random variables: indexing into Gπ,
scaling, and addition. We use analogous operations over the space of probability
distributions to construct the Bellman equation for return-distribution functions;
these distributional operations are depicted in Figure 2.5.
Mixing. Consider the return-variable and return-distribution functions Gπ
and ηπ, respectively, as well as a source state x ∈X. In the random-variable
equation, the term Gπ(X′) describes the random return received at the successor
state X′, when X = x and A is drawn from π( · | X) – hence the idea of indexing
the collection Gπ with the random variable X′.
Generatively, this describes the process of ﬁrst sampling a state x′ from the
distribution of X′ and then sampling a realized return from Gπ(x′). If we denote
the result by Gπ(X′), we see that for a subset S ⊆R, we have
Pπ(Gπ(X′) ∈S | X = x) =
X
x′∈X
Pπ(X′ = x′ | X = x)Pπ(Gπ(X′) ∈S | X′ = x′, X = x)
=
X
x′∈X
Pπ(X′ = x′ | X = x)Pπ
 Gπ(x′) ∈S 
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

The Distribution of Returns
35
Return
Return
Return
(a)
(b)
(c)
(d)
Return
Mixing
Scaling
Translating
Figure 2.5
Illustration of the eﬀects of the transformations of the random return in distributional
terms, for a given source state. The discount factor is applied to the individual realizations
of the return distribution, resulting in a scaling (a) of the support of this distribution.
The arrows illustrate that this scaling results in a “narrower” probability distribution.
The reward r translates (b) the return distribution. We write these two operations as a
pushforward distribution constructed from the bootstrap function br,γ. Finally, the return
distributions of successor states are combined to form a mixture distribution (c) whose
mixture weights are the transition probabilities to these successor states. The combined
transformations are given in (d), which depicts the return distribution at the source state
(dark outline).
=
 X
x′∈X
Pπ(X′ = x′ | X = x)ηπ(x′)

(S ) .
(2.16)
This shows that the probability distribution of Gπ(X′) is a weighted combination,
or mixture of probability distributions from ηπ. More compactly, we have
Dπ
 Gπ(X′) | X = x =
X
x′∈X
Pπ(X′ = x′ | X = x)ηπ(x′)
= Eπ
ηπ(X′) | X = x .
Consequently, the distributional analogue of indexing into the collection Gπ of
random returns is the mixing of their probability distributions.
Although we prefer working directly with probability distributions, the
indexing step also has a simple expression in terms of cumulative distribu-
tion functions. In Equation 2.16, taking the set S to be the half-open interval
(−∞, z], we obtain
Pπ(Gπ(X′) ≤z | X = x) = Pπ(Gπ(X′) ∈(−∞, z] | X = x)
=
X
x′∈X
Pπ(X′ = x′ | X = x)Pπ(Gπ(x′) ≤z) .
Thus, the mixture of next-state return distributions can be described by the
mixture of their cumulative distribution functions:
FGπ(X′)(z) =
X
x′∈X
Pπ(X′ = x′ | X = x)FGπ(x′)(z) .
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

36
Chapter 2
Scaling and translation. Suppose we are given the distribution of the next-
state return Gπ(X′). What is then the distribution of R + γGπ(X′)? To answer this
question, we must express how multiplying by the discount factor and adding a
ﬁxed reward r transforms the distribution of Gπ(X′).
This is an instance of a more general question: given a random variable
Z ∼ν and a transformation f : R →R, how should we express the distribution
of f(Z) in terms of f and ν? Our approach is to use the notion of a pushforward
distribution. The pushforward distribution f#ν is deﬁned as the distribution of
f(Z):
f#ν = D f(Z) .
One can think of it as applying the function f to the individual realizations of this
distribution – “pushing” the mass of the distribution around. The pushforward
notation allows us to reason about the eﬀects of these transformations on
distributions themselves, without having to involve random variables.
Now, for two scalars r ∈R and γ ∈[0, 1), let us deﬁne the bootstrap function
br,γ : z 7→r + γz .
The pushforward operation applied with the bootstrap function scales each
realization by γ and then adds r to it. That is,
(br,γ)#ν = D(r + γZ) .
(2.17)
Expressed in terms of cumulative distribution functions, this is
Fr+γZ(z) = FZ
z −r
γ

.
We use the pushforward operation and the bootstrap function to describe
the transformation of the next-state return distribution by the reward and the
discount factor. If x′ is a state with return distribution ηπ(x′), then
(br,γ)#ηπ(x′) = D r + γGπ(x′) .
We ﬁnally combine the pushforward and mixing operations to produce a
probability distribution
 br,γ

# Eπ
ηπ(X′) | X = x = Eπ
h br,γ

#ηπ(X′) | X = x
i
,
(2.18)
by linearity of the pushforward (see Exercises 2.11–2.13).
Equation 2.18 gives the distribution of the random return for a speciﬁc
realization of the random reward R. By taking the expectation over R and X′,
we obtain the distributional Bellman equation.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

The Distribution of Returns
37
Proposition 2.17 (The distributional Bellman equation). Let ηπ be the
return-distribution function of policy π. For any state x ∈X, we have
ηπ(x) = Eπ[(bR,γ)#ηπ(X′) | X = x] .
(2.19)
△
Figure 2.6 illustrates the recursive relationship between return distributions
described by the distributional Bellman equation.
Example 2.18. In Example 2.10, it was determined that the random return at
state x is uniformly distributed on the interval [0, 2]. Recall that there are two
possible rewards (0 and 1) with equal probability, x transitions back to itself,
and γ = 1/2. As X′ = x, when r = 0, we have
Eπ
h br,γ

#ηπ(X′) | X = x
i
= (b0,γ)#ηπ(x)
= D γGπ(x)
= U([0, 2γ])
= U([0, 1]) .
Similarly, when r = 1, we have
(b1,γ)#ηπ(x) = D 1 + γGπ(x)
= U([1, 2]) .
Consequently,
Dπ
 R + γGπ(X′) | X = x = 1
2(b0,γ)#ηπ(x) + 1
2(b1,γ)#ηπ(x)
= 1
2U([0, 1]) + 1
2U([1, 2])
= U([0, 2])
= ηπ(x) .
This illustrates how the pushforward operation can be used to express, in
distributional terms, the transformations at the heart of the random-variable
Bellman equation.
△
Proposition 2.17 can also be stated in terms of cumulative distribution func-
tions. Halfway between Equation 2.19 and the random-variable equation, what
one might call the cumulative distribution function Bellman equation is
FGπ(x)(z) = Eπ
FR+γGπ(X′)(z) | X = x
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

38
Chapter 2
= Eπ
h
FGπ(X′)
z −R
γ

| X = x
i
.
The ﬁrst form highlights the relationship between the random-variable equation
and the distributional equation, whereas the second form more directly expresses
the relationship between the diﬀerent cumulative distribution functions; the
odd indexing reﬂects the process of transferring probability mass “backward,”
from Gπ(X′) to Gπ(x). Although the cumulative distribution function Bellman
equation is at ﬁrst glance simpler than Equation 2.19, in later chapters, we
will see that working with probability distributions is usually more natural and
direct.
Proof of Proposition 2.17. Let S ⊆R and ﬁx x ∈X. By the law of total
expectation, we may write
Pπ(R + γGπ(X′) ∈S | X = x)
=
X
a∈A
π(a | x)
X
x′∈X
PX(x′ | x, a)Pπ
 R + γGπ(X′) ∈S | X = x, A = a, X′ = x′)
(a)=
X
a∈A
π(a | x)
X
x′∈X
PX(x′ | x, a) ×
ER
h
Pπ
 R + γGπ(x′) ∈S | X = x, A = a, R | X = x, A = a
i
=
X
a∈A
π(a | x)
X
x′∈X
PX(x′ | x, a) ER
h (bR,γ)#ηπ(x′)(S ) | X = x, A = a
i
= Eπ
h (bR,γ)#ηπ(X′)(S ) | X = x
i
= Eπ
(bR,γ)#ηπ(X′) | X = x
i
(S ) .
The notation ER in (a) denotes the expectation with respect to R ∼PR(· | x, a);
the independence of R and X′ given A allows us to remove the conditional
X′ = x′ from the inner probability term. Now, by Proposition 2.16, we know that
Gπ(x)
D= R + γGπ(X′),
X = x .
By deﬁnition, ηπ(x) is the distribution of Gπ(x) and hence the distribution of
R + γGπ(X′) when X = x. But the derivation above shows that that distribution
is
Eπ
(bR,γ)#ηπ(X′) | X = x ,
which completes the proof.
Equation 2.19 directly relates the return distributions at diﬀerent states and
does away with the return-variable function Gπ. This makes it particularly useful
when mathematical rigor is required, such as to prove the more formal results
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

The Distribution of Returns
39
Return
Return
Return
Reward
Reward
Reward
Reward
Reward
Reward
Return
Figure 2.6
The distributional Bellman equation states that the return distribution at one state (far
left) is formed by scaling the return distributions at successor states, shifting them by the
realized rewards, and mixing over possible realizations.
in later chapters. The random variables X, R, and X′ remain, however, inside
the expectation on the right-hand side. Their role is to concisely express the
three operations of mixing, scaling, and translation. It is also possible to omit
these random variables and write Equation 2.19 purely in terms of probability
distributions, by making the expectation explicit:
ηπ(x) =
X
a∈A
π(a | x)
X
x′∈X
PX(x′ | x, a)
Z
R
PR(dr | x, a) br,γ)#ηπ(x′) ,
(2.20)
as was partially done in the proof above. We will revisit this form of the
distributional Bellman equation in Chapter 5 when we consider algorithms for
approximating the return distribution. In general, however, Equation 2.19 is
more compact and applies equally well to discrete and continuous distributions.
Observe that the random action A is implicit in the random-variable and
distributional Bellman equations but explicit in Equation 2.20. This is because
A is needed to determine the probability distributions of R and X′, which are
only independent conditional on the action. This is a useful reminder that we
need to treat the addition of R to γGπ(X′) with care – these are not in general
independent random variables, and their addition does not generally correspond
to a simple convolution of distributions. We will consider this issue in more
detail in Chapter 4.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

40
Chapter 2
2.9
Alternative Notions of the Return Distribution*
In reinforcement learning, the discount factor γ is typically presented as a scalar
multiplier that shrinks the magnitude of future rewards. This is the interpretation
we have used to present the return in this chapter. However, the discount factor
can be given a diﬀerent interpretation as a probability of continuing. Under this
alternate interpretation, the rewards received by the agent are undiscounted, and
instead there is a probability 1 −γ that the sequence terminates at each time
step.
Under this model, the agent’s interactions with its environment end after a
random termination time T, which is geometrically distributed with parameter
γ, independently of all other random variables in the sequence. Speciﬁcally,
we have Pπ(T = t) = γt(1 −γ) for all t ≥0. In this case, the agent’s return is now
given by
T
X
t=0
Rt .
We call this the random-horizon return.
Straightforward calculation allows us to show that the expected value of the
return is unchanged (see Exercise 2.14):
Eπ

T
X
t=0
Rt
= Eπ

∞
X
t=0
γtRt
.
Because of this equivalence, from an expected-return perspective, it does not
matter whether γ is interpreted as a scaling factor or probability. By contrast,
this change in interpretation of γ has a signiﬁcant impact on the structure of
the return distribution. For example, the random variable PT
t=0 Rt is unbounded
even when the rewards are bounded, unlike the discounted return.
Just as there exists a distributional Bellman equation for the discounted return,
there is also a recursive characterization of the return distribution for this variant
of the return. By introducing an auxiliary random variable I = 1{T > 0}, we can
write down a distributional Bellman equation for this alternative interpretation
of the return. Expressing this relationship in terms of random variables, we have
Gπ(x)
D= R + IGπ(X′),
X = x .
In terms of distributions themselves, we have
ηπ(x) = (1 −γ)Dπ(R | X = x) + γEπ
(bR,1)#ηπ(X′) | X = x ,
where Dπ(R | X = x) denotes the distribution of the reward R at x under π.
Notice that γ in the equation above, which previously scaled the support of the
distribution, now mixes two probability distributions. These results are proven
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

The Distribution of Returns
41
in the course of Exercise 2.15 along with similar results for a more general class
of return deﬁnitions. The fact that there exists a distributional Bellman equation
for these variants of the return means that much of the theory of distributional
reinforcement learning still applies. We encourage the reader to consider how
the algorithms and analysis presented in later chapters may be modiﬁed to
accommodate alternative notions of the return.
We see that there are multiple ways in which we can deﬁne distributional
Bellman equations while preserving the relationship between expectations.
Another approach, commonly known as n-step bootstrapping in reinforcement
learning, relates the return distribution at state x with the return distribution n
steps into the future. Exercise 2.16 describes another alternative based on ﬁrst
return times.
2.10
Technical Remarks
Remark 2.1. Our account of random variables in this chapter has not required
a measure-theoretic formalism, and in general this will be the case for the
remainder of the book. For readers with a background in measure theory, the
existence of the joint distribution of the random variables (Xt, At, Rt)t≥0 in the
Markov decision process model described in this chapter can be deduced from
constructing a consistent sequence of distributions for the random variables
(Xt, At, Rt)H
t=0 for each horizon H ∈N using the conditional distributions given in
the deﬁnition and then appealing to the Ionescu Tulcea theorem (Tulcea 1949).
See Lattimore and Szepesvári (2020) for further discussion in the context of
Markov decision processes.
We also remark that a more formal deﬁnition of probability measures restricts
the subsets of outcomes under consideration to be measurable with an under-
lying σ-algebra. Here, and elsewhere in the book, we will generally avoid
repeated mention of the qualiﬁer “measurable” in cases such as these. The issue
can be safely ignored by readers without a background in measure theory. For
readers with such a background, measurability is always implicitly with respect
to the power set σ-algebra on ﬁnite sets, the Borel σ-algebra on the reals, and
the corresponding product σ-algebra on products of such spaces.
△
Remark 2.2. The existence of the random return P∞
t=0 γtRt is often taken for
granted in reinforcement learning. In part, this is because assuming that rewards
are deterministic or have distributions with bounded support makes its existence
straightforward; see Exercise 2.17.
The situation is more complicated when the support of the reward distribu-
tions is unbounded, as there might be realizations of the random trajectory for
which the sequence (G0:T)T≥0 does not converge. If, for example, the rewards
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

42
Chapter 2
are normally distributed, then it is possible that R0 = 1, R1 = −γ−1, R2 = γ−2, R3 =
−γ−3, . . . , in which case G0:T oscillates between 0 and 1 indeﬁnitely. When the
truncated returns do not converge, it is not meaningful to talk about the return
obtained in this realization of the trajectory.
Under the assumption that all reward distributions have ﬁnite mean, we can
demonstrate convergence of the truncated return with probability 1:
P

lim
T→∞
T
X
t=0
γtRt exists

= 1 ,
(2.21)
and hence guarantee that we can meaningfully study the distributional properties
of the random return. Intuitively, problematic sequences of rewards such as the
sequence given above require reward samples that are increasingly “unlikely”.
Equation 2.21 is established by appealing to the martingale convergence
theorem, and the details of this derivation are explored in Exercise 2.19.
△
Remark 2.3. The assumption of ﬁnite state and action spaces makes several
technical aspects of the presentation in this book more straightforward. As a
simple example, consider a Markov decision process with countably inﬁnite
state space given by the nonnegative integers N, for which state x deterministi-
cally transitions to state x + 1. Suppose in addition that the trajectory begins in
state 0. If the reward at state x is (−γ)−x, then the truncated return up to time T
is given by
T−1
X
t=0
γtRt =
T−1
X
t=0
γt(−γ)−t =
T−1
X
t=0
(−1)−t .
In the spirit of Remark 2.2, when T is odd, this sum is 1, while for T even, it
is 0. Hence, the limit P∞
t=0 γtRt does not exist, despite the fact that all reward
distributions have ﬁnite mean.
Dealing with continuous state spaces requires more technical sophistication
still. Constructing the random variables describing the trajectory requires taking
into account the topology of the state space in question, and careful consider-
ation of technical conditions on policies (such as measurability) is necessary
too. For details on how these issues may be addressed, see Puterman (2014)
and Meyn and Tweedie (2012).
△
Remark 2.4 (Proof of time-homogeneity and the Markov property). The gener-
ative equations deﬁning the distribution of the random trajectory (Xt, At, Rt)t≥0
immediately give statements such as
Pπ(Xk+1 = x′ | X0:k, A0:k, R0:k, Xk) = Pπ(Xk+1 = x′ | Xk) ,
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

The Distribution of Returns
43
which shows that the next state in the trajectory enjoys the Markov property.
Lemmas 2.13 and 2.14 state something stronger: that the entire future trajectory
has this property. To prove these lemmas from the generative equations, the strat-
egy is to note that the distribution of (Xt, At, Rt)∞
t=k conditional on (Xt, At, Rt)k−1
t=0
and Xk is determined by its ﬁnite-dimensional marginal distributions: that is,
conditional probabilities of the form
Pπ(Xti = xi, Ati = ai, Rti ∈S i for i = 1, …, n | (Xt, At, Rt)k−1
t=0 , Xk)
for all n ∈N+ and sequences k ≤t1 < · · · < tn of time steps (see, e.g., Billingsley
2012, Section 36). Thus, if these conditional probabilities are equal to the
corresponding quantities
Pπ(Xti = xi, Ati = ai, Rti ∈S i for i = 1, …, n | Xk) ,
then the distributions are equal. Finally, the equality of these ﬁnite-dimensional
marginals can be shown by induction from the generative equations.
△
2.11
Bibliographical Remarks
An introduction to random variables and probability distributions may be found
in any undergraduate textbook on the subject. For a more technical presentation,
see Williams (1991) and Billingsley (2012).
2.2. Markov decision processes are generally attributed to Bellman (1957b). A
deeper treatment than we give here can be found in most introductory textbooks,
including those by Bertsekas and Tsitsiklis (1996), Szepesvári (2010), and
Puterman (2014). Our notation is most aligned with that of Sutton and Barto
(2018). Interestingly, while it is by now standard to use reward as an indicator
of success, Bellman’s own treatment does not make it an integral part of the
formalism.
2.3. The formulation of a trajectory as a sequence of random variables is central
to control as inference (Toussaint 2009; Levine 2018), which uses tools from
probabilistic reasoning to derive optimal policies. Howard (1960) used the
analogy of a frog hopping around a lily pond to convey the dynamics of a
Markov decision process; we ﬁnd our own analogy more vivid. The special
consideration owed to inﬁnite sequences is studied at length by Hutter (2005).
2.4. The work of Veness et al. (2015) makes the return (as a random variable)
the central object of interest and is the starting point of our own investigations
into distributional reinforcement learning. Issues regarding the existence of
the random return and a proper probabilistic formulation can be found in that
paper. An early formulation of the return-variable function can be found in
Jaquette (1973), who used it to study alternative optimality criteria. Chapman
and Kaelbling (1991) consider the related problem of predicting the discounted
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

44
Chapter 2
cumulative probability of observing a particular level of reward in order to
mitigate partial observability in a simple video game. The blackjack and cliﬀ-
walking examples are adapted from Sutton and Barto (2018) and, in the latter
case, inspired by one of the authors’ trip to Ireland. In both cases, we put
a special emphasis on the probability distribution of the random return. The
uniform distribution example is taken from Bellemare et al. (2017a); such
discounted sums of Bernoulli random variables also have a long history in
probability theory (Jessen and Wintner 1935; see Solomyak 1995; Diaconis and
Freedman 1999; Peres et al. 2000 and references therein).
2.5–2.6. The Bellman equation is standard to most textbooks on the topic. A
particularly thorough treatment can be found in the work of Puterman (2014)
and Bertsekas (2012). The former also provides a good discussion on the
implications of the Markov property and time-homogeneity.
2.7–2.8. Bellman equations relating quantities other than expected returns were
originally introduced in the context of risk-sensitive control, at varying levels
of generality. The formulation of the distributional Bellman equation in terms
of cumulative distribution functions was ﬁrst given by Sobel (1982), under the
assumption of deterministic rewards and policies. Chung and Sobel (1987) later
gave a version for random, bounded rewards. See also the work of White (1988)
for a review of some of these approaches and Morimura et al. (2010a) for a
more recent presentation of the CDF Bellman equation.
Other versions of the distributional Bellman equation have been phrased in
terms of moments (Sobel 1982), characteristic functions (Mandl 1971; Farah-
mand 2019), and the Laplace transform (Howard and Matheson 1972; Jaquette
1973, 1976; Denardo and Rothblum 1979), again at varying levels of general-
ity, and in some cases using the undiscounted return. Morimura et al. (2010b)
also present a version of the equation in terms of probability densities. The
formulation of the distributional Bellman equation in terms of pushforward dis-
tributions is due to Rowland et al. (2018); the pushforward notation is broadly
used in measure-theoretic probability, and our use of it is inﬂuenced by optimal
transport theory (Villani 2008).
Distributional formulations have also been used to design Bayesian methods
for reasoning about uncertainty regarding the value function (Ghavamzadeh et
al. 2015). Dearden et al. (1998) propose an algorithm that maintains a posterior
on the return distribution under the assumption that rewards are normally
distributed. Engel et al. (2003, 2007) use a random-variable equation to derive
an algorithm based on Gaussian processes.
Our own work in the ﬁeld is rooted in the theory of stochastic ﬁxed point
equations (Rösler 1991, 1992; Rachev and Rüschendorf 1995), also known as
recursive distributional equations (Aldous and Bandyopadhyay 2005), from
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

The Distribution of Returns
45
which we draw the notion of equality in distribution. Recursive distributional
equations have been used, among other applications, for complexity analysis of
randomized algorithms (Rösler and Rüschendorf 2001), as well as the study of
branching processes and objects in random geometry (Liu 1998). See Alsmeyer
(2012) for a book-length survey of the ﬁeld. In eﬀect, the random-variable
Bellman equation for a given state x is a recursive distributional equation in the
sense of Aldous and Bandyopadhyay (2005); however, in distributional rein-
forcement learning, we typically emphasize the collection of random variables
(the return-variable function) over individual equations.
2.9. The idea of treating the discount factor as a probability recurs in the
literature, often as a technical device within a proof. Our earliest source is
Derman (1970, Chap. 3). The idea is used to handle discounted, inﬁnite horizon
problems in the reinforcement learning as inference setting (Toussaint and
Storkey 2006) and is remarked on by Sutton et al. (2011). The probabilistic
interpretation of the discount factor has also found application in model-based
reinforcement learning (Sutton 1995; Janner et al. 2020), and is closely related
to the notion of termination probabilities in options (Sutton et al. 1999). The
extension to the distributional setting was worked out with Tom Schaul but
not previously published. See also White (2017) for a broader discussion on
unifying various uses of discount factors.
2.12
Exercises
Exercise 2.1. We deﬁned a state x as being terminal if
PX(x | x, a) = 1
for all a ∈A
and PR(0 | x, a) = 1.
(i) Explain why multiple terminal states are redundant from a return function
perspective.
(ii) Suppose you are given an MDP with k terminal states. Describe a procedure
that creates a new MDP that behaves identically but has a single terminal
state.
△
Exercise 2.2. Consider the generative equations of Section 2.3. Using Bayes’s
rule, explain why
Pπ(X1 = x′ | X0 = x, A0 = a)
needs to be introduced as a notational convention, rather than derived from the
deﬁnition of the joint distribution Pπ.
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

46
Chapter 2
Exercise 2.3. Suppose that γ < 1 and Rt ∈[Rmin, Rmax] for all t ∈N. Show that
∞
X
t=0
γtRt ∈
" Rmin
1 −γ, Rmax
1 −γ
#
.
△
Exercise 2.4. Find a probability distribution ν that has unbounded support and
such that
Pπ

∞
X
t=0
γtZt
 < ∞

= 1 ,
Zt
i.i.d.
∼ν for t ∈N .
△
Exercise 2.5. Using an open-source implementation of the game of blackjack
or your own, use simulations to estimate the probability distribution of the
random return under
(i) The uniformly random policy.
(ii) The policy that doubles when starting with 10 or 11, hits on 16 and lower,
and otherwise sticks.
(iii) The policy that hits on 16 and lower and never doubles. Is it a better
policy?
△
In all cases, aces should count as 11 whenever possible.
Exercise 2.6. Show that any random variable Z satisfying the condition in
Equation 2.6 must have CDF given by
Pπ
 Z ≤z = z
2, 0 ≤z ≤2 ,
and hence that Z has distribution U([0, 2]).
△
Exercise 2.7 (*). The Cantor distribution has a cumulative distribution function
F, deﬁned for z ∈[0, 1] by expressing z in base 3:
z =
∞
X
n=1
zn3−n,
zn ∈{0, 1, 2} ,
such that
F(z) =
∞
X
n=1
1{zn > 0 , zi , 1 for all 1 ≤i < n}2−n .
Additionally, F(z) = 0 for z ≤0 and F(z) = 1 for z ≥1. Prove that the return
distribution for the MDP in Example 2.11 is the Cantor distribution.
△
Exercise 2.8. The n-step Bellman equation relates the value function at a state
x with the discounted sum of n ∈N+ future rewards and the value of the nth
successor state. Prove that, for each n ∈N+,
Vπ(x) = Eπ
h n−1
X
t=0
γtRt + γnVπ(Xn) | X0 = x
i
.
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

The Distribution of Returns
47
Exercise 2.9. In Section 2.5, we argued that the number of partial trajectories
of length T can grow exponentially with T. Give examples of Markov decision
processes with NX states and NA actions where the number of possible length
T realizations (xt, at, rt)T
t=0 of the random trajectory is
(i) Bounded by a constant.
(ii) Linear in T.
△
Exercise 2.10. In Section 2.7, we argued that the equation
˜Gπ(x) = R + γ ˜Gπ(X′) ,
X = x
leads to the wrong probability distribution over returns. Are there scenarios in
which this is a sensible model?
△
Exercise 2.11. The purpose of this exercise is to familiarize yourself with the
transformations of the pushforward operation applied to the bootstrap function
b. Let ν ∈P(R) be a probability distribution and let Z be a random variable
with distribution ν. Let r = 1/2, γ = 1/3, and let R be a Bernoulli(1/4) random
variable independent of Z. For each of the following probability distributions:
(i) ν = δ1;
(ii) ν = 1/2δ−1 + 1/2δ1;
(iii) ν = N(2, 1),
express the probability distributions produced by the following operations:
(i) (br,1)#ν = D(r + Z);
(ii) (b0,γ)#ν = D(γZ);
(iii) (br,γ)#ν = D(r + γZ); and
(iv) E[(bR,γ)#ν] = D(R + γZ) .
△
Exercise 2.12. Repeat the preceding exercise, now with R ∼N(0, 1). Conclude
on the distribution of the random return
G =
∞
X
t=0
γtRt ,
where Rt ∼N(1, 1) for all t ≥0.
△
Exercise 2.13. Let η ∈P(R)X be a return-distribution function, and let X′
be a X-valued random variable. Show that for any r, γ ∈R, the pushforward
operation combined with the bootstrap function forms an aﬃne map, in the
sense that
 br,γ

# E η(X′) = E  br,γ

#η(X′) .
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

48
Chapter 2
Exercise 2.14. Consider the standard discounted return P∞
t=0 γtRt. Recall from
Section 2.9 the random-horizon return, PT
t=0 Rt, where T has a geometric
probability distribution with parameter γ:
Pπ(T = k) = γk(1 −γ) .
Show that
Eπ

∞
X
t=0
γtRt
= Eπ

T
X
t=0
Rt
.
△
Exercise 2.15. Consider again the random-horizon return described in
Section 2.9.
(i) Show that if the termination time T has a geometric distribution with
parameter γ, then T has the memoryless property: that is, Pπ(T ≥k + l | T ≥
k) = Pπ(T ≥l), for all k, l ∈N.
(ii) Hence, show that the return-variable function corresponding to this
alternative deﬁnition of return satisﬁes the Bellman equation
Gπ(x)
D= R + 1{T > 0}Gπ(X′),
X = x .
for each x ∈X.
Consider now a further alternative notion of return, given by considering a
sequence of identically and independent distributed (i.i.d.) nonnegative random
variables (It)t≥0, independent from all other random variables in the MDP.
Suppose Eπ[It] = γ. Deﬁne the return to be
∞
X
t=0
 t−1
Y
s=0
Is

Rt.
(iii) Show that both the standard deﬁnition of the return and the random-horizon
return can be viewed as special cases of this more general notion of return
by particular choices of the distribution of the (It)t≥0 variables.
(iv) Verify that
Eπ
h ∞
X
t=0
 t−1
Y
s=0
Is

Rt
i
= Eπ
h ∞
X
t=0
γtRt
i
.
(v) Show that the random-variable Bellman equation associated with this
notion of return is given by
Gπ(x) = R + I0Gπ(X′) ,
X = x
for each x ∈X.
△
Exercise 2.16. For a state x ∈X, write
Tx = min{t ≥1 : Xt = x}
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

The Distribution of Returns
49
with Tx = ∞if x is never reached from time t = 1 onward. When X0 = x, this
is the random time of ﬁrst return to x. Prove that the following alternative
random-variable Bellman equation holds:
Gπ(x)
D=
T−1
X
t=0
γtRt + γTGπ(x),
X0 = x .
△
Exercise 2.17. The aim of the following two exercises is to explore under what
conditions the random return G = P∞
t=0 γtRt exists. As in Remark 2.2, denote
the truncated return by
G0:T =
T
X
t=0
γtRt.
Show that if all reward distributions are supported on the interval [Rmin, Rmax],
then for any values of the random variables (Rt)t≥0, the sequence (G0:T)T≥0
converges, and hence G is well deﬁned.
△
Exercise 2.18 (*). The random-variable Bellman equation is eﬀectively a sys-
tem of recursive distributional equations (Aldous and Bandyopadhyay 2005).
This example asks you to characterize the normal distribution as the solution to
another such equation. Let Z be a normally distributed random variable with
mean zero and unit variance (that is, Z ∼N(0, 1)). Recall that the probability
density pZ and characteristic function χZ of Z are
pZ(z) =
1
√
2π
exp

−z2
2

,
χZ(s) = exp

−s2
2

.
Suppose that Z′ is an independent copy of Z. Show that
Z
D= 2−1/2Z + 2−1/2Z′
by considering
(i) the probability density of each side of the equation above;
(ii) the characteristic function of each side of the equation above.
△
Exercise 2.19 (*). Under the overarching assumption of this chapter and
the remainder of the book that there are ﬁnitely many states and all reward
distributions have ﬁnite ﬁrst moment, we can show that the limit
lim
T→∞
T
X
t=0
γtRt
(2.22)
exists with probability 1. First, show that the sequence of truncated returns
(G0:T)T≥0 forms a semi-martingale with respect to the ﬁltration (Ft)t≥0, deﬁned
by
Ft = σ(X0:t, A0:t, R0:t−1) ;
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

50
Chapter 2
the relevant decomposition is
G0:T =G0:T−1 + γt Eπ[RT | XT, AT]
|                 {z                 }
predictable increment
+ γt(RT −Eπ[RT | XT, AT])
|                          {z                          }
martingale noise
.
Writing
CT =
T
X
t=0
γt Eπ[Rt | Xt, At]
for the sum of predictable increments, show that this sequence converges for
all realizations of the random trajectory. Next, show that the sequence ( ¯CT)T≥0
deﬁned by ¯CT =G0:T −CT is a martingale, and by using the assumption of a
ﬁnite state space X, show that it satisﬁes
sup
T≥0
E[| ¯CT|] < ∞.
Hence, use the martingale convergence theorem to argue that ( ¯CT)T≥0 converges
with probability 1 (w.p. 1). From there, deduce that the limit of Equation 2.22
exists w.p. 1.
△
Exercise 2.20 (*). The second Borel–Cantelli lemma (Billingsley 2012,
Section 4) states that if (Et)t≥0 is a sequence of independent events and
P
t≥0 P(Et) = ∞, then
P(inﬁnitely many Et occur) = 1 .
Let ε > 0. Exhibit a Markov decision process and policy for which the events
{|γtRt| > ε} are independent, and satisfy
Pπ(|γtRt| > ε for inﬁnitely many t) = 1 .
Deduce that
Pπ

T
X
t=0
γtRt converges as T →∞

= 0
in this case. Hint. Construct a suﬃciently heavy-tailed reward distribution from
a suitable cumulative distribution function.
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

3
Learning the Return Distribution
Reinforcement learning provides a computational model of what and how an
animal, or more generally an agent, learns to predict on the basis of received
experience. Often, the prediction of interest is about the expected return under
the initial state distribution or, counterfactually, from a query state x ∈X. In the
latter case, making good predictions is equivalent to having a good estimate
of the value function Vπ. From these predictions, reinforcement learning also
seeks to explain how the agent might best control its environment: for example,
to maximize its expected return.
By learning from experience, we generally mean from data rather than from
the Markov decision process description of the environment (its transition ker-
nel, reward distribution, and so on). In many settings, these data are taken
from sample interactions with the environment. In their simplest forms, these
interactions are realizations of the random trajectory (Xt, At, Rt)t≥0. The record
of a particular game of chess and the ﬂuctuations of an investment account
throughout the month are two examples of sample trajectories. Learning from
experience is a powerful paradigm, because it frees us from needing a complete
description of the environment, an often impractical if not infeasible require-
ment. It also enables incremental algorithms whose run time does not depend on
the size of the environment and that can easily be implemented and parallelized.
The aim of this chapter is to introduce a concrete algorithm for learning
return distributions from experience, called categorical temporal-diﬀerence
learning. In doing so, we will provide an overview of the design choices that
must be made when creating distributional reinforcement learning algorithms.
In classical reinforcement learning, there are well-established (and in some
cases, deﬁnitive) methods for learning to predict the expected return; in the
distributional setting, choosing the right algorithm requires balancing multiple,
sometimes conﬂicting considerations. In great part, this is because of the unique
and exciting challenges that arise when one wishes to estimate sometimes
intricate probability distributions, rather than scalar expectations.
51
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

52
Chapter 3
3.1
The Monte Carlo Method
Birds such as the pileated woodpecker follow a feeding routine that regularly
takes them back to the same foraging grounds. The success of this routine can be
measured in terms of the total amount of food obtained during a ﬁxed period of
time, say a single day. As part of a ﬁeld study, it may be desirable to predict the
success of a particular bird’s routine on the basis of a limited set of observations:
for example, to assess its survival chances at the beginning of winter based
on feeding observations from the summer months. In reinforcement learning
terms, we view this as the problem of learning to predict the expected return
(total food per day) of a given policy π (the feeding routine). Here, variations in
weather, human activity, and other foraging animals are but a few of the factors
that aﬀect the amount of food obtained on any particular day.
In our example, the problem of learning to predict is abstractly a problem
of statistical estimation. To this end, let us model the woodpecker’s feeding
routine as a Markov decision process.17 We associate each day with a sample
trajectory or episode, corresponding to measurements made at regular intervals
about the bird’s location x, behavior a, and per-period food intake r. Suppose
that we have observed a set of K sample trajectories,
n
(xk,t, ak,t, rk,t)Tk−1
t=0
oK
k=1 ,
(3.1)
where we use k to index the trajectory and t to index time, and where Tk denotes
the number of measurements taken each day. In this example, it is most sensible
to assume a ﬁxed number of measurements Tk = T, but in the general setting, Tk
may be random and possibly dependent on the trajectory, often corresponding to
the time when a terminal state is ﬁrst reached. For now, let us also assume that
there is a unique starting state x0, such that xk,0 = x0 for all k. We are interested
in the problem of estimating the expected return
Eπ
h T−1
X
t=0
γtRt
i
= Vπ(x0) ,
corresponding to the expected per-day food intake of our bird.18
Monte Carlo methods estimate the expected return by averaging the outcomes
of observed trajectories. Let us denote by gk the sample return for the kth
17. Although this may be a reasonable approximation of reality, it is useful to remember that
concepts such as the Markov property are in this case modeling assumptions, rather than actual
facts.
18. In this particular example, a discount factor of γ = 1 is reasonable given that T is ﬁxed and we
should have no particular preference for mornings over evenings.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Learning the Return Distribution
53
trajectory:
gk =
T−1
X
t=0
γtrk,t .
(3.2)
The sample-mean Monte Carlo estimate is the average of these K sample
returns:
ˆVπ(x0) = 1
K
K
X
k=1
gk .
(3.3)
This a sensible procedure that also has the beneﬁt of being simple to implement
and broadly applicable. In our example above, the Monte Carlo method corre-
sponds to estimating the expected per-day food intake by simply averaging the
total intake measured over diﬀerent days.
Example 3.1. Monte Carlo tree search is a family of approaches that have
proven eﬀective for planning in stochastic environments and have been used
to design state-of-the-art computer programs that play the game of Go. Monte
Carlo tree search combines a search procedure with so-called Monte Carlo
rollouts (simulations). In a typical implementation, a computer Go algorithm
will maintain a partial search tree whose nodes are Go positions, rooted in the
current board conﬁguration. Nodes at the fringe of this tree are evaluated by
performing a large number of rollouts of a ﬁxed rollout policy (often uniformly
random) from the nodes’ position to the game’s end.
By deﬁning the return of one rollout to be 1 if the game is won and 0 if the
game is lost, the expected undiscounted return from a leaf node corresponds to
the probability of winning the game (under the rollout policy) from that position.
By estimating the expected return from complete rollouts, the Monte Carlo
method avoids the challenges associated with devising a heuristic to determine
the value of a given position.
△
We can use the Monte Carlo method to estimate the value function Vπ rather
than only the expected return from the initial state. Suppose now that the sample
trajectories (Equation 3.1) have diﬀerent starting states (that is, xk,0 varies across
trajectories). The Monte Carlo method constructs the value function estimate
ˆVπ(x) =
1
N(x)
K
X
k=1
1{xk,0=x}gk ,
where
N(x) =
K
X
k=1
1{xk,0 = x}
is the number of trajectories whose starting state is x. For simplicity of
exposition, we assume here that N(x) > 0 for all x ∈X.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

54
Chapter 3
Learning the value function is useful because it provides ﬁner-grained detail
about the environment and chosen policy compared to only learning about the
expected return from the initial state. Often, estimating the full value function
can be done at little additional cost, because sample trajectories contain infor-
mation about the expected return from multiple states. For example, given a
collection of K sample trajectories drawn independently from the generative
equations, the ﬁrst-visit Monte Carlo estimate is
ˆVπ
fv(x) =
1
Nfv(x)
K
X
k=1
Tk−1
X
t=0
1{xk,t = x, xk,0, . . . , xk,t−1 , x}
|                               {z                               }
ck,t
 Tk−1
X
s=t
γs−trk,s

Nfv(x) =
K
X
k=1
Tk−1
X
t=0
ck,t .
The ﬁrst-visit Monte Carlo estimate treats each time step as the beginning of a
new trajectory; this is justiﬁed by the Markov property and time-homogeneity
of the random trajectory (Xt, At, Rt)t≥0 (Section 2.6). The restriction to the ﬁrst
occurrence of x in a particular trajectory guarantees that ˆVπ
fv behaves like the
sample-mean estimate.
3.2
Incremental Learning
Both in practice and in theory, it is useful to consider a learning model under
which sample trajectories are processed sequentially, rather than all at once.
Algorithms that operate in this fashion are called incremental algorithms, as
they maintain a running value function estimate V ∈RX that they improve with
each sample.19 Under this model, we now consider an inﬁnite sequence of
sample trajectories

(xk,t, ak,t, rk,t)Tk−1
t=0

k≥0 ,
(3.4)
presented one at a time to the learning algorithm. In addition, we consider the
more general setting in which the initial states (xk,0)k≥0 may be diﬀerent; we
call these states the source states, as with the sample transition model (Section
2.6). As in the previous section, a minimum requirement for learning Vπ is that
every state x ∈X should be the source state of some trajectories.
The incremental Monte Carlo algorithm begins by initializing its value
function estimate:
V(x) ←V0(x),
for all x ∈X .
19. Incremental methods are also sometimes called stochastic or sample based. We prefer the term
“incremental” as these methods can be applied even in the absence of randomness, and because
other estimation methods – including the sample-mean method of the preceding section – are also
based on samples.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Learning the Return Distribution
55
Algorithm 3.1: Online incremental ﬁrst-visit Monte Carlo
Algorithm parameters:
step size α ∈(0, 1],
policy π : X →P(A),
initial estimate V0 ∈RX
V(x) ←V0(x) for all x ∈X
Loop for each episode:
Observe initial state x0
T ←0
Loop for t = 0, 1, …
Draw at from π( · | xt)
Take action at, observe rt, xt+1
T ←T + 1
until xt+1 is terminal
g ←0
for t = T −1, …, 0 do
g ←rt + γg
if xt is not in {x0, …, xt−1} then
V(xt) ←(1 −α)V(xt) + αg
end for
end
Given the kth trajectory, the algorithm computes the sample return gk (Equation
3.2), called the Monte Carlo target in this context. It then adjusts its value
function estimate for the initial state xk,0 toward this target, according to the
update rule
V(xk,0) ←(1 −αk)V(xk,0) + αkgk .
The parameter αk ∈[0, 1) is a time-varying step size that controls the impact
of a single sample trajectory on the value estimate V. Because the incremental
Monte Carlo update rule only depends on the starting state and the sample
return, we can more generally consider learning Vπ on the basis of the sequence
of state-return pairs
 xk, gk)k≥0 .
(3.5)
Under this simpliﬁed model, the sample return gk is assumed drawn from the
return distribution ηπ(xk) (rather than constructed from the sample trajectory; of
course, these two descriptions are equivalent). The initial state xk,0 is substituted
for xk to yield
V(xk) ←(1 −αk)V(xk) + αkgk .
(3.6)
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

56
Chapter 3
By choosing a step size that is inversely proportional to the number of times
Nk(xk) that the value of state xk has been previously encountered, we recover
the sample-mean estimate of the previous section (see Exercise 3.1). In practice,
it is also common to use a constant step size α to avoid the need to track Nk. We
will study the eﬀect of step sizes in greater detail in Chapter 6.
As given, the Monte Carlo update rule is agnostic to the mechanism by which
the sample trajectories are generated or when learning occurs in relation to the
agent observing these trajectories. A frequent and important setting in reinforce-
ment learning is when each trajectory is consumed by the learning algorithm
immediately after being experienced, which we call the online setting.20 Com-
plementing the abstract presentation given in this section, Algorithm 3.1 gives
pseudo-code for an incremental implementation of the ﬁrst-visit Monte Carlo
algorithm in the online, episodic21 setting.
In Equation 3.6, the notation A ←B indicates that the value B (which may
be a scalar, a vector, or a distribution) should be stored in the variable A. In the
case of Equation 3.6, V is a collection of scalars – one per state – and the update
rule describes the process of modifying the value of one of these variables “in
place.” This provides a succinct way of highlighting the incremental nature of
the process. On the other hand, it is often useful to consider the value of the
variable after a given number of iterations. For k > 0, we express this with the
notation
Vk+1(xk) = (1 −αk)Vk(xk) + αkgk
Vk+1(x) = Vk(x) for x , xk ,
(3.7)
where the second line reﬂects the fact that only the variable associated to state
xk is modiﬁed at time k.
3.3
Temporal-Diﬀerence Learning
Incremental algorithms are useful because they do not need to maintain the
entire set of sample trajectories in memory. In addition, they are often simpler
to implement and enable distributed and approximate computation. Temporal-
diﬀerence learning (TD learning, or simply TD) is particularly well suited to
the incremental setting, because it can learn from sample transitions, rather than
entire trajectories. It does so by leveraging the relationship between the value
20. The online setting is sometimes deﬁned in terms of individual transitions, which are to be
consumed immediately after being experienced. Our use of the term is broader and related to the
streaming setting considered in other ﬁelds of research (see, e.g., Cormode and Muthukrishnan
2005).
21. A environment is said to be episodic when all trajectories eventually reach a terminal state.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Learning the Return Distribution
57
Algorithm 3.2: Online temporal-diﬀerence learning
Algorithm parameters:
step size α ∈(0, 1],
policy π : X →P(A)
initial estimate V0 ∈RX
V(x) ←V0(x) for all x ∈X
Loop for each episode:
Observe initial state x0
Loop for t = 0, 1, …
Draw at from π( · | xt)
Take action at, observe rt, xt+1
if xt+1 is terminal then
V(xt) ←(1 −α)V(xt) + αrt
else
V(xt) ←(1 −α)V(xt) + α rt + γV(xt+1)
end if
until xt+1 is terminal
end
function at successive states – eﬀectively, it takes advantage of the Bellman
equation.
To begin, let us abstract away the sequential nature of the trajectory and
consider the sample transition model (X, A, R, X′) deﬁned by Equation 2.12.
Here, the distribution ξ of the source state X may correspond, for example, to
the relative frequency at which states are visited over the random trajectory. We
consider a sequence of sample transitions drawn independently according to
this model, denoted
(xk, ak, rk, x′
k)k≥0 .
(3.8)
As with the incremental Monte Carlo algorithm, the update rule of temporal-
diﬀerence learning is parameterized by a time-varying step size αk. For the kth
sample transition, this update rule is given by
V(xk) ←(1 −αk)V(xk) + αk
 rk + γV(x′
k) .
(3.9)
Algorithm 3.2 instantiates this update rule in the online, episodic setting. We
call the term rk + γV(x′
k) in Equation 3.9 the temporal-diﬀerence target. If we
again write the Bellman equation
Vπ(x) = Eπ
R + γVπ(X′) | X = x ,
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

58
Chapter 3
we see that the temporal-diﬀerence target can be thought of as a realization
of the random variable whose expectation forms the right-hand side of the
Bellman equation, with the exception that the TD target uses the estimated
value function V in place of the true value function Vπ. This highlights the fact
that temporal-diﬀerence learning adjusts its value function estimate to be more
like the right-hand side of the Bellman equation; we will study this relationship
more formally in Chapter 6.
By rearranging terms, we can also express the temporal-diﬀerence update
rule in terms of the temporal-diﬀerence error rk + γV(x′
k) −V(xk):
V(xk) ←V(xk) + αk
 rk + γV(x′
k) −V(xk);
this form highlights the direction of change of the value function estimate
(positive if the target is greater than our estimate, negative if it is not) and is
needed to express certain reinforcement learning algorithms, as we will see in
Chapter 9.
The incremental Monte Carlo algorithm updates its value function estimate
toward a ﬁxed (but noisy) target. By contrast, Equation 3.9 describes a recur-
sive process, without such a ﬁxed target. Temporal-diﬀerence learning instead
depends on the value function at the next state V(x′
k) being approximately
correct. As such, it is said to bootstrap from its own value function estimate.
Because of this recursive dependency, the dynamics of temporal-diﬀerence
learning are usually diﬀerent from those of Monte Carlo methods, and are more
challenging to analyze (Chapter 6).
On the other hand, temporal-diﬀerence learning oﬀers some important advan-
tages over Monte Carlo methods. One is the way in which value estimates
are naturally shared between states, so that once a value has been estimated
accurately at one state, this can often be used to improve the value estimates at
other states. In many situations, the estimates produced by temporal-diﬀerence
learning are consequently more accurate than their Monte Carlo counterparts.
A full treatment of the statistical properties of temporal-diﬀerence learning is
beyond the scope of this book, but we provide references on the topic in the
bibliographical remarks.
3.4
From Values to Probabilities
In distributional reinforcement learning, we are interested in understanding
the random return as it arises from interactions with the environment. In the
context of this chapter, we are speciﬁcally interested in how we can learn the
return-distribution function ηπ.
As a light introduction, consider the scenario in which rewards are binary
(Rt ∈{0, 1}) and where we are interested in learning the distribution of the
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Learning the Return Distribution
59
undiscounted ﬁnite-horizon return function
Gπ(x) =
H−1
X
t=0
Rt,
X0 = x.
(3.10)
Here, H ∈N+ denotes the horizon of the learner – how far it predicts into
the future. By enumeration, we see that Gπ(x) takes on one of H + 1 possible
values, integers ranging from 0 to H. These form the support of the probability
distribution ηπ(x). To learn ηπ, we will construct an incremental algorithm that
maintains a return-distribution function estimate η, the distributional analogue
of V from the previous sections. This estimate assigns a probability pi(x) to
each possible return i ∈{0, . . . , H}:
η(x) =
H
X
i=0
pi(x)δi ,
(3.11)
where pi(x) ≥0 and PH
i=0 pi(x) = 1. We call Equation 3.11 a categorical repre-
sentation, since each possible return can now be thought of as one of H + 1
categories. Under this perspective, we can think of the problem of learning the
return distribution for a given state x as a classiﬁcation problem – assigning
probabilities to each of the possible categories. We may then view the problem
of learning the return function ηπ as a collection of classiﬁcation problems (one
per state).
As before, let us consider a sequence of state-return pairs (xk, gk)k≥0, where
each gk is drawn from the distribution ηπ(xk). As in Section 3.2, the sample
return gk provides a target for an update rule except that now we want to adjust
the probability of observing gk rather than an estimate of the expected return.
For a step size αk ∈(0, 1], the categorical update rule is
pgk(xk) ←(1 −αk)pgk(xk) + αk
pi(xk) ←(1 −αk)pi(xk) ,
i , gk .
(3.12)
The adjustment of the probabilities for returns other than gk ensures that
the return-distribution estimate at xk continues to sum to 1 after the update.
Expressed as an operation over probability distributions, this update rule corre-
sponds to changing η(xk) to be a mixture between itself and a distribution that
puts all of its mass on gk:
η(xk) ←(1 −αk)η(xk) + αkδgk .
(3.13)
We call this the undiscounted ﬁnite-horizon categorical Monte Carlo algorithm.
It is instantiated in the online, episodic setting in Algorithm 3.3. Similar to the
other incremental algorithms presented thus far, it is possible to demonstrate
that under the right conditions, this algorithm learns a good approximation to
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

60
Chapter 3
the distribution of the binary-reward, undiscounted, ﬁnite-horizon return. In the
next section, we will see how this idea can be carried over to the more general
setting.
Algorithm 3.3: Undiscounted ﬁnite-horizon categorical Monte
Carlo
Algorithm parameters:
step size α ∈(0, 1],
horizon H,
policy π : X →P(A),
initial probabilities  (p0
i (x))H
i=0 : x ∈X
pi(x) ←p0
i (x) for all 0 ≤i ≤H, x ∈X
Loop for each episode:
Observe initial state x0
T ←0
Loop for t = 0, 1, . . .
Draw at from π( · | xt)
Take action at, observe rt, xt+1
T ←T + 1
until xt+1 is terminal
g ←0
for t = T −1, …, 0 do
g ←g + rt
if t < T −H then
g ←g −rt+H
if xt is not in {x0, …, xt−1} then
pg(xt) ←(1 −α)pg(xt) + α
pi(xt) ←(1 −α)pi(xt), for i , g
end for
end
3.5
The Projection Step
The ﬁnite, undiscounted algorithm of the previous section is a sensible approach
when the random return takes only a few distinct values. In the undiscounted
setting, we already saw that the number of possible returns is NG = H + 1 when
there are two possible rewards. However, this small number is the exception,
rather than the rule. If there are NR possible rewards, then NG can be as large as
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Learning the Return Distribution
61
NR+H−1
H

, already a potentially quite large number for NR > 2 (see Exercise 3.5).
Worse, when a discount factor γ is introduced, the number of possible returns
depends exponentially on H. To see why, recall the single-state, single-action
Markov decision process of Example 2.10 with reward distribution
Pπ(Rt = 0) = Pπ(Rt = 1) = 1/2 .
With a discount factor γ = 1/2, we argued that the set of possible returns for this
MDP corresponds to the binary expansion of all numbers in the [0, 2] interval,
from which we concluded that the random return is uniformly distributed on
that interval. By the same argument, we can show that the H-horizon return
G =
H−1
X
t=0
γtRt
has support on all numbers in the [0, 2] interval that are described by H binary
digits; there are 2H such numbers. Of course, if we take H to be inﬁnite, there
may be uncountably many possible returns.
To deal with the issue of a large (or even inﬁnite) set of possible returns,
we insert a projection step prior to the mixture update in Equation 3.13.22
The purpose of the projection step is to transform an arbitrary target return g
into a modiﬁed target taking one of m values, for m reasonably small. From
a classiﬁcation perspective, we can think of the projection step as assigning a
label (from a small set of categories) to each possible return.
We will consider return distributions that assign probability mass to m ≥2
evenly spaced values or locations. In increasing order, we denote these locations
by θ1 ≤θ2 ≤· · · ≤θm. We write ςm for the gap between consecutive locations, so
that for i = 1, . . . , m −1, we have
ςm = θi+1 −θi .
A common design takes θ1 = Vmin and θm = Vmax, in which case
ςm = Vmax −Vmin
m −1
.
However, other choices are possible and sometimes desirable. Note that the
undiscounted algorithm of the previous section corresponds to m = H + 1 and
θi = i −1.
22. When there are relatively few sample trajectories, a sensible alternative is to construct a
nonparametric estimate of the return distribution that simply puts equal probability mass on all
observed returns. The return distributions described in Section 1.2 and Example 2.9, in particular,
were estimated in this manner. See Remark 3.1 for further details.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

62
Chapter 3
We express the corresponding return distribution as a weighted sum of Dirac
deltas at these locations:
η(x) =
m
X
i=1
pi(x)δθi .
For mathematical convenience, let us write θ0 = −∞and θm+1 = ∞. Consider
a sample return g, and denote by i∗(g) the index of the largest element of the
support (extended with −∞) that is no greater than g:
i∗= arg max
i∈{0,...,m}
{θi : θi ≤g} .
For this sample return, we write
Π−(g) = θi∗
Π+(g) = θi∗+1
to denote the corresponding element of the support and its immediate successor;
when θ1, . . . , θm are consecutive integers and g ∈[θ1, θm], these are the ﬂoor and
ceiling of g, respectively.
The projection step begins by computing
ζ(g) =
g −Π−(g)
Π+(g) −Π−(g) ,
with the convention that ζ(g) = 1 if Π−(g) = −∞and ζ(g) = 0 if Π+(g) = ∞. The
ζ(g) term corresponds to the distance of g to the two closest elements of the
support, scaled to lie in the interval [0, 1] – eﬀectively, the fraction of the
distance between Π−(g) and Π+(g) at which g lies. The stochastic projection of
g is
Π±(g) =
( Π−(g)
with probability 1 −ζ(g)
Π+(g)
with probability ζ(g).
(3.14)
We use this projection to construct the update rule
η(x) ←(1 −α)η(x) + αδΠ±g ,
where as before, x is a source state and g a sample return.23 Similar to Equation
3.12, this update rule is implemented by adjusting the probabilities according to
pi±(x) ←(1 −α)pi±(x) + α
pi(x) ←(1 −α)pi(x) ,
i , i± .
where i± is the index of location Π±g.
23. From here onward, we omit the iteration index k from the notation as it is not needed in the
deﬁnition of the update rule. The algorithm proper should still be understood as applying this update
rule to a sequence of state-return pairs (xk, gk)k≥0, possibly with a time-varying step size αk.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Learning the Return Distribution
63
We can improve on the stochastic projection by constructing a modiﬁed target
that contains information about both Π−(g) and Π+(g). In classiﬁcation terms,
this corresponds to using soft labels: the target is a probability distribution over
labels, rather than a single label. This deterministic projection of g results in
the update rule
η(x) ←(1 −α)η(x) + α(1 −ζ(g))δΠ−(g) + ζ(g)δΠ+(g)
 .
(3.15)
We denote the deterministic projection by Πc. Statistically speaking, the deter-
ministic projection produces return-distribution estimates that are on average
the same as those produced by the stochastic projection but are comparatively
more concentrated around their mean. Going forward, we will see that it is
conceptually simpler to apply this projection to probability distributions, rather
than to sample returns. Rather than Πc(g), we therefore write
Πcδg = (1 −ζ(g))δΠ−(g) + ζ(g)δΠ+(g).
We call this method the categorical Monte Carlo algorithm. This algorithm can
be used to learn to predict inﬁnite-horizon, discounted returns and is applicable
even when there are a large number of possible rewards.
Example 3.2. Montezuma’s Revenge is a 1984 platform game designed by then-
sixteen-year-old Robert Jaeger. As part of the Arcade Learning Environment
(Bellemare et al. 2013a), the Atari 2600 version of the game poses a challenging
reinforcement learning problem due to the rare occurrence of positive rewards.
The very ﬁrst task in Montezuma’s Revenge consists in collecting a key, an
act that rewards the agent with 100 points. Let us consider the integer support
θi = i −1, for i = 1, 2, . . . , 101. For a discount factor γ = 0.99, the discounted
H-horizon return from the initial game state is
G =
H−1
X
t=0
γtRt
= 1{τ < H}0.99τ × 100,
where τ denotes the time at which the key is obtained. For a ﬁxed τ ∈R, write
g = 0.99τ × 100
ζ = g −⌊g⌋.
For τ < H, the deterministic projection of the return 0.99τ puts probability mass
1 −ζ on ⌊g⌋and ζ on ⌈g⌉. For example, if τ = 60, then 100 × 0.99τ ≈54.72 and
the deterministic projection is
Πcδg = 0.28 × δ54 + 0.72 × δ55 .
△
By introducing a projection step, we typically lose some information about
the target return. This is by necessity: we are asking the learning algorithm to
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

64
Chapter 3
0.4
0.2
0.0
0.2
0.4
Return
0.0
0.2
0.4
0.6
0.8
1.0
Probability of assignment
(g)
1
(g)
0.6
0.8
1.0
1.2
1.4
Return
0.0
0.2
0.4
0.6
0.8
1.0
Probability of assignment
(g)
1
(g)
1.6
1.8
2.0
2.2
2.4
Return
0.0
0.2
0.4
0.6
0.8
1.0
Probability of assignment
(g)
1
(g)
Figure 3.1
Illustration of the projection for the ﬁve-location grid used in Example 3.3. The middle
panel depicts the proportion of probability mass assigned to the location θ3 = 1 in terms
of the sample return g. The left and right panels illustrate this probability assignment at
the boundary locations θ1 = 0 and θm = 2.
approximate complex distributions using a small, ﬁnite set of possible returns.
Under the right conditions, Equation 3.15 gives rise to a convergent algorithm.
The point of convergence of this algorithm is the return function ˆηπ(x) for which,
for all x ∈X, we have
ˆηπ(x) = E[ΠcδGπ(x)] = E  1 −ζ(Gπ(x))δΠ−(Gπ(x)) + ζ(Gπ(x))δΠ+(Gπ(x))
 . (3.16)
In fact, we may write
E[ΠcδGπ(x)] = Πcηπ(x) ,
where Πcηπ(x) denotes distribution supported on {θ1, …, θm} produced by pro-
jecting each possible outcome under the distribution ηπ(x); we will discuss this
deﬁnition in further detail in Chapter 5.
Example 3.3. Recall the single-state Markov decision process of Example 2.10,
whose return is uniformly distributed on the interval [0, 2]. Suppose that we take
our support to be the uniform grid {0, 0.5, 1, 1.5, 2}. Let us write ˆp0, ˆp0.5, . . . , ˆp2
for the probabilities assigned to these locations by the projected distribution
ˆηπ(x) = Πcηπ(x), where x is the unique state of the MDP. The probability density
of the return on the interval [0, 2] is 0.5. We thus have
ˆp0
(a)= 0.5
Z
g∈[0,2]
1{Π−(g) = 0}(1 −ζ(g)) + 1{Π+(g) = 0}ζ(g)dg
= 0.5
Z
g∈[0,0.5]
(1 −ζ(g))dg
= 0.5
Z
g∈[0,0.5]
(1 −2g)dg
= 0.125.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Learning the Return Distribution
65
In (a), we reexpressed Equation 3.16 in terms of the probability assigned to
ˆp0. A similar computation shows that ˆp0.5 = ˆp1 = ˆp1.5 = 0.25, while ˆp2 = 0.125.
Figure 3.1 illustrates the process of assigning probability mass to diﬀerent
locations. Intuitively, the solution makes sense: there is less probability mass
near the boundaries of the interval [0, 2].
△
3.6
Categorical Temporal-Diﬀerence Learning
With the use of a projection step, the categorical Monte Carlo method allows us
to approximate the return-distribution function of a given policy from sample
trajectories and using a ﬁxed amount of memory. Like the Monte Carlo method
for value functions, however, it ignores the relationship between successive
states in the trajectory. To leverage this relationship, we turn to categorical
temporal-diﬀerence learning (CTD).
Consider now a sample transition (x, a, r, x′). Like the categorical Monte
Carlo algorithm, CTD maintains a return function estimate η(x) supported on
the evenly spaced locations {θ1, . . . , θm}. Like temporal-diﬀerence learning, it
learns by bootstrapping from its current return function estimate. In this case,
however, the update target is a probability distribution supported on {θ1, . . . , θm}.
The algorithm ﬁrst constructs an intermediate target by scaling the return
distribution η(x′) at the next state by the discount factor γ, then shifting it by
the sample reward r. That is, if we write
η(x′) =
m
X
i=1
pi(x′)δθi ,
then the intermediate target is
˜η(x) =
m
X
i=1
pi(x′)δr+γθi ,
which can also be expressed in terms of a pushforward distribution:
˜η(x) = (br,γ)#η(x′) .
(3.17)
Observe that the shifting and scaling operations are applied to each particle
in isolation. After shifting and scaling, however, these particles in general no
longer lie in the support of the original distribution. This motivates the use of the
projection step described in the previous section. Let us denote the intermediate
particles by
˜θi = br,γ(θi) = r + γθi .
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

66
Chapter 3
The CTD target is formed by individually projecting each of these particles
back onto the support and combining their probabilities. That is,
Πc(br,γ)#η(x′) =
m
X
j=1
p j(x′)Πcδr+γθ j .
More explicitly, this is
Πc(br,γ)#η(x′) =
m
X
j=1
p j(x′)(1 −ζ(˜θ j))δΠ−(˜θ j) + ζ(˜θ j)δΠ+(˜θ j)

=
m
X
i=1
δθi

m
X
j=1
pj(x′)ζi,j(r)

,
(3.18)
where
ζi, j(r) =  (1 −ζ(˜θ j))1{Π−(˜θ j) = θi} + ζ(˜θ j)1{Π+(˜θj) = θi}
.
In Equation 3.18, the second line highlights that the CTD target is a probability
distribution supported on {θ1, . . . , θm}. The probabilities assigned to speciﬁc
locations are given by the inner sum; as shown here, this assignment is obtained
by weighting the next-state probabilities p j(x′) by the coeﬃcients ζi,j(r).
The CTD update adjusts the return function estimate η(x) toward this target:
η(x) ←(1 −α)η(x) + α Πc(br,γ)#η(x′) .
(3.19)
Expressed in terms of the probabilities of the distributions η(x) and η(x′), this
is (for i = 1, . . . , m)
pi(x) ←(1 −α)pi(x) + α
m
X
j=1
ζi,j(r)pj(x′) .
(3.20)
With this form, we see that the CTD update rule adjusts each probability pi(x)
of the return distribution at state x toward a mixture of the probabilities of the
return distribution at the next state (see Algorithm 3.4).
The deﬁnition of the intermediate target in pushforward terms (Equation
3.17) illustrates that categorical temporal-diﬀerence learning relates to the
distributional Bellman equation
ηπ(x) = Eπ[(bR,γ)#ηπ(X′) | X = x] ,
analogous to the relationship between TD learning and the classical Bellman
equation. We will continue to explore this relationship in later chapters. How-
ever, the two algorithms usually learn diﬀerent solutions, due to the introduction
of approximation error from the bootstrapping process.
Example 3.4. We can study the behavior of the categorical temporal-diﬀerence
learning algorithm by visualizing how its predictions vary over the course
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Learning the Return Distribution
67
Algorithm 3.4: Online categorical temporal-diﬀerence learning
Algorithm parameters:
step size α ∈(0, 1],
policy π : X →P(A),
evenly spaced locations θ1, …, θm ∈R,
initial probabilities  (p0
i (x))H
i=0 : x ∈X
(pi(x))H
i=0 ←(p0
i (x))H
i=0 for all x ∈X
Loop for each episode:
Observe initial state x0
Loop for t = 0, 1, . . .
Draw at from π( · | xt)
Take action at, observe rt, xt+1
ˆpi ←0 for i = 1, …, m
for j = 1, …, m do
if xt+1 is terminal then
g ←rt
else
g ←rt + γθ j
if g ≤θ1 then
ˆp1 ←ˆp1 + p j(xt+1)
else if g ≥θm then
ˆpm ←ˆpm + pj(xt+1)
else
i∗←largest i s.t. θi ≤g
ζ ←
g−θi∗
θi∗+1−θi∗
ˆpi∗←ˆpi∗+ (1 −ζ)pj(xt+1)
ˆpi∗+1 ←ˆpi∗+1 + ζpj(xt+1)
end for
for i = 1, …, m do
pi(xt) ←(1 −α)pi(xt) + α ˆpi
end for
until xt+1 is terminal
end
of learning. We apply CTD to approximate the return function of the safe
policy in the Cliﬀs domain (Example 2.9). Learning takes place online (as per
Algorithm 3.4), using a constant step size of α = 0.05, and return distributions
are approximated with m = 31 locations evenly spaced between −1 and 1.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

68
Chapter 3
1.0
0.5
0.0
0.5
1.0
0.00
0.25
0.50
0.75
1.00
Probability
Return
Probability
Return
Probability
Return
Probability
Return
1.0
0.5
0.0
0.5
1.0
0.00
0.05
0.10
0.15
Probability
Return
1.0
0.5
0.0
0.5
1.0
0.00
0.05
0.10
0.15
Probability
Return
1.0
0.5
0.0
0.5
1.0
0.00
0.05
0.10
0.15
Probability
Return
1.0
0.5
0.0
0.5
1.0
0.00
0.05
0.10
0.15
Probability
Return
1.0
0.5
0.0
0.5
1.0
0.0
0.1
0.2
0.3
0.4
0.5
1.0
0.5
0.0
0.5
1.0
0.00
0.05
0.10
0.15
Probability
Return
1.0
0.5
0.0
0.5
1.0
0.00
0.05
0.10
0.15
Probability
Return
Figure 3.2
The return distribution at the initial state in the Cliﬀs domain, as predicted by categorical
temporal-diﬀerence learning over the course of learning. Top panels. The predictions
after e ∈{0, 1000, 2000, 10000} episodes (α = 0.05; see Algorithm 3.4). Here, the return
distributions are initialized by assigning equal probability to all locations. Bottom pan-
els. Corresponding predictions when the return distributions initially put all probability
mass on the zero return.
(a)
(b)
(c)
1.0
0.5
0.0
0.5
1.0
Return
0
2
4
6
8
10
Density
1.0
0.5
0.0
0.5
1.0
Return
0.0
0.1
0.2
0.3
0.4
0.5
Probability
1.0
0.5
0.0
0.5
1.0
Return
0.00
0.05
0.10
0.15
0.20
0.25
Probability
Figure 3.3
(a) The return distribution at the initial state in the Cliﬀs domain, visualized using
kernel density estimation (see Figure 2.2). (b) The return-distribution estimate learned
by the categorical Monte Carlo algorithm with m = 31. (c) The estimate learned by the
categorical temporal-diﬀerence learning algorithm.
Figure 3.2 illustrates that the initial return function plays a substantial role in
the speed at which CTD learns a good approximation. Informally, this occurs
because the uniform distribution is closer to the ﬁnal approximation than the
distribution that puts all of its probability mass on zero (what “close” means in
this context will be the topic of Chapter 4). In addition, we see that categorical
temporal-diﬀerence learning learns a diﬀerent approximation to the true return
function, compared to the categorical Monte Carlo algorithm (Figure 3.3). This
is due to a phenomenon we call diﬀusion, which arises from the combination of
the bootstrapping step and projection; we will study diﬀusion in Chapter 5.
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Learning the Return Distribution
69
3.7
Learning to Control
A large part of this book considers the problem of learning to predict the dis-
tribution of an agent’s returns. In Chapter 7, we will discuss how one might
instead learn to maximize or control these returns and the role that distribu-
tional reinforcement learning plays in this endeavor. By learning to control, we
classically mean obtaining (from experience) a policy π∗that maximizes the
expected return:
Eπ∗
h ∞
X
t=0
γtRt
i
≥Eπ
h ∞
X
t=0
γtRt
i
, for all π .
Such a policy is called an optimal policy. From Section 2.5, recall that the
state-action value function Qπ is given by
Qπ(x, a) = Eπ
h ∞
X
t=0
γtRt
 X0 = x, A0 = a
i
.
Any optimal policy π∗has the property that its state-action value function also
satisﬁes the Bellman optimality equation:
Qπ∗(x, a) = Eπ
R + γ max
a′∈A Qπ∗(X′, a′) | X = x, A = a .
Similar in spirit to temporal-diﬀerence learning, Q-learning is an incremental
algorithm that ﬁnds an optimal policy. Q-learning maintains a state-action value
function estimate, Q, which it updates according to
Q(x, a) ←(1 −α)Q(x, a) + α r + γ max
a′∈A Q(x′, a′) .
(3.21)
The use of the maximum in the Q-learning update rule results in diﬀerent
behavior than TD learning, as the selected action depends on the current value
estimate. We can think of this diﬀerence as constructing one target for each
action a′ and updating the value estimate toward the largest of such targets.
It can be shown that under the right conditions, Q-learning converges to the
optimal state-action value function Q∗, corresponding to the expected return
under any optimal policy. We extract an optimal policy from Q∗by acting
greedily with respect to Q∗: that is, choosing a policy π∗that selects maximally
valued actions according to Q∗.
The simplest way to extend Q-learning to the distributional setting is to
express the maximal action in Equation 3.21 as a greedy policy. Denote by
η a return-function estimate over state-action pairs, such that η(x, a) is the
return distribution associated with the state-action pair (x, a) ∈X × A. Deﬁne
the greedy action
aη(x) = arg max
a∈A
E
Z∼η(x,a)[Z] ,
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

70
Chapter 3
breaking ties arbitrarily. The categorical Q-learning update rule is
η(x, a) ←(1 −α)η(x, a) + α

Πc(br,γ)#η x′, aη(x′)
.
It can be shown that, under the same conditions as Q-learning, the mean of the
return-function estimates also converges to Q∗. The behavior of the distributions
themselves, however, may be surprisingly complicated. We can also put the
learned distributions to good use and make decisions on the basis of their
full characterization, rather than from their mean alone. This forms the topic
of risk-sensitive reinforcement learning. We return to both of these points in
Chapter 7.
3.8
Further Considerations
Categorical temporal-diﬀerence learning learns to predict return distributions
from sample experience. As we will see in subsequent chapters, the choices that
we made in designing CTD are not unique, and the algorithm is best thought of
as a jumping-oﬀpoint into a broad space of methods. For example, an important
question in distributional reinforcement learning asks how we should represent
probability distributions, given a ﬁnite memory budget. One issue with the
categorical representation is that it relies on a ﬁxed grid of locations to cover the
range [θ1, θm], which lacks ﬂexibility and is in many situations ineﬃcient. We
will take a closer look at this issue in Chapter 5. In many practical situations,
we also need to deal with a few additional considerations, including the use of
function approximation to deal with very large state spaces (Chapters 9 and 10).
3.9
Technical Remarks
Remark 3.1 (Nonparametric distributional Monte Carlo algorithm). In
Section 3.4, we saw that the (unprojected) ﬁnite-horizon categorical Monte
Carlo algorithm can in theory learn ﬁnite-horizon return-distribution functions
when there are only a small number of possible returns. It is possible to extend
these ideas to obtain a straightforward, general-purpose algorithm that can be
sometimes be used to learn an accurate approximation to the return distribution.
Like the sample-mean Monte Carlo method, the nonparametric distributional
Monte Carlo algorithm takes as input K ﬁnite-length trajectories with a com-
mon source state x0. After computing the sample returns (gk)K
k=1 from these
trajectories, it constructs the estimate
ˆηπ(x0) = 1
K
K
X
k=1
δgk
(3.22)
of the return distribution ηπ(x0). Here, nonparametric refers to the fact that
the approximating distribution in Equation 3.22 is not described by a ﬁnite
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Learning the Return Distribution
71
collection of parameters; in fact, the memory required to represent this object
may grow linearly with K. Although this is not an issue when K is relatively
small, this can be undesirable when working with large amounts of data and
moreover precludes the use of function approximation (see Chapters 9 and 10).
However, unlike the categorical Monte Carlo and temporal-diﬀerence learn-
ing algorithms presented in this chapter, the accuracy of this estimate is only
limited by the number of trajectories K; we describe various ways to quantify
this accuracy in Remark 4.3. As such, it provides a useful baseline for measuring
the quality of other distributional algorithms. In particular, we used this algo-
rithm to generate the ground-truth return-distribution estimates in Example 2.9
and in Figure 3.3.
△
3.10
Bibliographical Remarks
The development of a distributional algorithm in this chapter follows our own
development of the distributional perspective, beginning with our work on using
compression algorithms in reinforcement learning (Veness et al. 2015).
3.1. The ﬁrst-visit Monte Carlo estimate is studied by Singh and Sutton (1996),
where it is used to characterize the properties of replacing eligibility traces (see
also Sutton and Barto 2018). Statistical properties of model-based estimates
(which solve for the Markov decision process’s parameters as an intermediate
step) are analyzed by Mannor et al. (2007). Grünewälder and Obermayer (2011)
argue that model-based methods must incur statistical bias, an argument that
also extends to temporal-diﬀerence algorithms. Their work also introduces a
reﬁned sample-mean Monte Carlo method that yields a minimum-variance
unbiased estimator (MVUE) of the value function. See Browne et al. (2012) for
a survey of Monte Carlo tree search methods and Liu (2001), Robert and Casella
(2004), and Owen (2013) for further background on Monte Carlo methods more
generally.
3.2. Incremental algorithms are a staple of reinforcement learning and have
roots in stochastic approximation (Robbins and Monro 1951; Widrow and Hoﬀ
1960; Kushner and Yin 2003) and psychology (Rescorla and Wagner 1972). In
the control setting, these are also called optimistic policy iteration methods and
exhibit fairly complex behavior (Sutton 1999; Tsitsiklis 2002).
3.3. Temporal-diﬀerence learning was introduced by Sutton (1984, 1988). The
sample transition model presented here diﬀers from the standard algorithmic
presentation but allows us to separate concerns of behavior (data collection)
from learning. A similar model is used in Bertsekas and Tsitsiklis (1996) to
prove convergence of a broad class of TD methods and by Azar et al. (2012) to
provide sample eﬃciency bounds for model-based control.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

72
Chapter 3
3.4. What is eﬀectively the undiscounted ﬁnite-horizon categorical Monte Carlo
algorithm was proposed by Veness et al. (2015). There, the authors demonstrate
that by means of Bayes’s rule, one can learn the return distribution by ﬁrst
learning the joint distribution over returns and states (in our notation, Pπ(X,G))
by means of a compression algorithm and subsequently using Bayes’s rule
to extract Pπ(G | X). The method proved surprisingly eﬀective at learning to
play Atari 2600 games. Toussaint and Storkey (2006) consider the problem of
control as probabilistic inference, where the reward and trajectory length are
viewed as random variables to be optimized over, again obviating the need to
deal with a potentially large support for the return.
3.5–3.6. Categorical temporal-diﬀerence learning for both prediction and
control was introduced by Bellemare et al. (2017a), in part to address the
shortcomings of the undiscounted algorithm. Its original form contains both the
projection step and the categorical representation as given here. The mixture
update that we study in this chapter is due to Rowland et al. (2018).
3.7. The Q-learning algorithm is due to Watkins (1989); see also Watkins and
Dayan (1992). The explicit construction of a greedy policy is commonly found
in more complex reinforcement learning algorithms, including modiﬁed policy
iteration (Puterman and Shin 1978), λ-policy iteration (Bertsekas and Ioﬀe
1996), and nonstationary policy iteration (Scherrer and Lesner 2012; Scherrer
2014).
3.9. The algorithm introduced in Remark 3.1 is essentially an application of the
standard Monte Carlo method to the return distribution and is a special case of
the framework set out by Chandak et al. (2021), who also analyze the statistical
properties of the approach.
3.11
Exercises
Exercise 3.1. Suppose that we begin with the initial value function estimate
V(x) = 0 for all x ∈X.
(i) Consider ﬁrst the setting in which we are given sample returns for a single
state x. Show that in this case, the incremental Monte Carlo algorithm
(Equation 3.6), instantiated with αk =
1
k+1, is equivalent to computing the
sample-mean Monte Carlo estimate for x. That is, after processing the
sample returns g1, . . . , gK, we have
VK(x) = 1
K
K
X
i=1
gi.
(ii) Now consider the case where source states are drawn from a distribution
ξ, with ξ(x) > 0 for all x, and let Nk(xk) be the number of times xk has been
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Learning the Return Distribution
73
updated up to but excluding time k. Show that the appropriate step size to
match the sample-mean Monte Carlo estimate is αk =
1
Nk(xk)+1.
△
Exercise 3.2. Recall from Exercise 2.8 the n-step Bellman equation:
Vπ(x) = Eπ
h n−1
X
t=0
γtRt + γnVπ(Xn) | X0 = x.
Explain what a sensible n-step temporal-diﬀerence learning update rule might
look like.
△
Exercise 3.3. The Cart–Pole domain is a small, two-dimensional reinforcement
learning problem (Barto et al. 1983). In this problem, the learning agent must
balance a swinging pole that is a attached to a moving cart. Using an open-
source implementation of Cart–Pole, implement the undiscounted ﬁnite-horizon
categorical Monte Carlo algorithm and evaluate its behavior. Construct a ﬁnite
state space by discretising the four-dimensional state space using a uniform grid
of size 10 × 10 × 10 × 10. Plot the learned return distribution for a ﬁxed initial
state and other states of your choice when
(i) the policy chooses actions uniformly at random;
(ii) the policy moves the cart in the direction that the pole is leaning toward.
△
You may want to pick the horizon H to be the maximum length of an episode.
Exercise 3.4. Implement the categorical Monte Carlo (CMC), nonparametric
categorical Monte Carlo (Remark 3.1), and categorical temporal-diﬀerence
learning (CTD) algorithms. For a discount factor γ = 0.99, compare the return
distributions learned by these three algorithms on the Cart–Pole domain of the
previous exercise. For CMC and CTD, vary the number of particles m and the
range of the support [θ1, θm]. How do the approximations vary as a function of
these parameters?
△
Exercise 3.5. Suppose that the rewards Rt take on one of NR values. Consider
the undiscounted ﬁnite-horizon return
G =
H−1
X
t=0
Rt .
Denote by NG the number of possible realizations of G.
(i) Show that NG can be as large as
NR+H−1
H

.
(ii) Derive a better bound when Rt ∈{0, 1, . . . , NR −1}.
(iii) Explain, in words, why the bound is better in this case. Are there other sets
of rewards for which NG is smaller than the worst-case from (i)?
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

74
Chapter 3
Exercise 3.6. Recall from Section 3.5 that the categorical Monte Carlo
algorithm aims to ﬁnd the approximation
ˆηπ
m(x) = Πcηπ(x),
x ∈X ,
where we use the subscript m to more explicitly indicate that the quality of this
approximation depends on the number of locations.
Consider again the problem setting of Example 3.3. For m ≥2, suppose that
we take θ1 = 0 and θm = 2, such that
θi = 2 i −1
m −1
i = 1, . . . m .
Show that in this case, ˆηπ
m(x) converges to the uniform distribution on the
interval [0, 2], in the sense that
lim
m→∞ˆηπ
m(x) [a, b] →b −a
2
for all a < b; recall that ν([a, b]) denotes the probability assigned to the interval
[a, b] by a probability distribution ν.
△
Exercise 3.7. The purpose of this exercise is to demonstrate that the categorical
Monte Carlo algorithm is what we call mean-preserving. Consider a sequence
of state-return pairs (xk, gk)K
k=1 and an evenly spaced grid {θ1, . . . , θm}, m ≥2.
Suppose that rewards are bounded in [Rmin, Rmax].
(i) Suppose that Vmin ≥θ1 and Vmax ≤θm. For a given g ∈[Vmin, Vmax], show that
the distribution ν = Πcδg satisﬁes
E
Z∼ν[Z] = g .
(ii) Based on this, argue that if η(x) is a distribution with mean V, then after
applying the update
η(x) ←(1 −α)η(x) + αΠcδg ,
the mean of η(x) is (1 −α)V + αg.
(iii) By comparing with the incremental Monte Carlo update rule, explain why
categorical Monte Carlo can be said to be mean-preserving.
(iv) Now suppose that [Vmin, Vmax] ⊈[θ1, θm]. How are the preceding results
aﬀected?
△
Exercise 3.8. Following the notation of Section 3.5, consider the nearest
neighbor projection method
Πnng =
( Π−(g)
if ζ(g) ≤0.5
Π+(g)
otherwise.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Learning the Return Distribution
75
Show that this projection is not mean-preserving in the sense of the preceding
exercise. Implement and evaluate on the Cart–Pole domain. What do you
observe?
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

4
Operators and Metrics
Anyone who has learned to play a musical instrument knows that practice makes
perfect. Along the way, however, one’s ability at playing a diﬃcult passage
usually varies according to a number of factors. On occasion, something that
could be played easily the day before now seems insurmountable. The adage
expresses an abstract notion – that practice improves performance, on average or
over a long period of time – rather than a concrete statement about instantaneous
ability.
In the same way, reinforcement learning algorithms deployed in real sit-
uations behave diﬀerently from moment to moment. Variations arise due to
diﬀerent initial conditions, speciﬁc choices of parameters, hardware nonde-
terminism, or simply because of randomness in the agent’s interactions with
its environment. These factors make it hard to make precise predictions, for
example, about the magnitude of the value function estimate learned by TD
learning at a particular state x and step k, other than by extensive simulations.
Nevertheless, the large-scale behavior of TD learning is relatively predictable,
suﬃciently so that convergence can be established under certain conditions, and
convergence rates can be derived.
This chapter introduces the language of operators as an eﬀective abstrac-
tion with which to study such long-term behavior, characterize the asymptotic
properties of reinforcement learning algorithms, and eventually explain what
makes an eﬀective algorithm. In addition to being useful in the study of existing
algorithms, operators also serve as a kind of blueprint when designing new algo-
rithms, from which incremental methods such as categorical temporal-diﬀerence
learning can then be derived. In parallel, we will also explore probability metrics
– essentially distance functions between probability distributions. These metrics
play an immediate role in our analysis of the distributional Bellman operator,
and will recur in later chapters as we design algorithms for approximating return
distributions.
77
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

78
Chapter 4
4.1
The Bellman Operator
The value function Vπ characterizes the expected return obtained by following
a policy π, beginning in a given state x:
Vπ(x) = Eπ
h ∞
X
t=0
γtRt | X0 = x
i
.
The Bellman equation establishes a relationship between the expected return
from one state and from its successors:
Vπ(x) = Eπ
R + γVπ(X′) | X = x .
Let us now consider a state-indexed collection of real variables, written V ∈RX,
which we call a value function estimate. By substituting Vπ for V in the original
Bellman equation, we obtain the system of equations
V(x) = Eπ
R + γV(X′) | X = x , for all x ∈X .
(4.1)
From Chapter 2, we know that Vπ is one solution to the above.
Are there other solutions to Equation 4.1? In this chapter, we answer this
question (negatively) by interpreting the right-hand side of the equation as
applying a transformation on the estimate V. For a given realization (x, a, r, x′)
of the random transition, this transformation indexes V by x′, multiplies it by
the discount factor, and adds it to the immediate reward (this yields r + γV(x′)).
The actual transformation returns the value that is obtained by following these
steps, in expectation. Functions that map elements of a space onto itself, such
as this one (from estimates to estimates), are called operators.
Deﬁnition 4.1. The Bellman operator is the mapping T π : RX →RX deﬁned
by
(T πV)(x) = Eπ[R + γV(X′) | X = x] .
(4.2)
Here, the notation T πV should be understood as “T π, applied to V.”
△
The Bellman operator gives a particularly concise way of expressing the
transformations implied in Equation 4.1:
V = T πV .
As we will see in later chapters, it also serves as the springboard for the design
and analysis of algorithms for learning Vπ.
When working with the Bellman operator, it is often useful to treat V as a
ﬁnite-dimensional vector in RX and to express the Bellman operator in terms of
vector operations. That is, we write
T πV = rπ + γPπV ,
(4.3)
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Operators and Metrics
79
where rπ(x) = Eπ[R | X = x] and Pπ is the transition operator24 deﬁned as
(PπV)(x) =
X
a∈A
π(a | x)
X
x′∈X
PX(x′ | x, a)V(x′) .
Equation 4.3 follows from these deﬁnitions and the linearity of expectations.
A vector ˜V ∈RX is a solution to Equation 4.1 if it remains unchanged by the
transformation corresponding to the Bellman operator T π; that is, if it is a ﬁxed
point of T π. This means that the value function Vπ is a ﬁxed point of T π:
Vπ = T πVπ .
To demonstrate that Vπ is the only ﬁxed point of the Bellman operator, we will
appeal to the notion of contraction mappings.
4.2
Contraction Mappings
When we apply the operator T π to a value function estimate V ∈RX, we obtain
a new estimate T πV ∈RX. A characteristic property of the Bellman operator is
that this new estimate is guaranteed to be closer to Vπ than V (unless V = Vπ, of
course). In fact, as we will see in this section, applying the operator to any two
estimates must bring them closer together.
To formalize what we mean by “closer,” we need a way of measuring dis-
tances between value function estimates. Because these estimates can be viewed
as ﬁnite-dimensional vectors, there are many well-established ways of doing so:
the reader may have come across the Euclidean (L2) distance, the Manhattan
(L1) distance, and curios such as the British Rail distance. We use the term
metric to describe distances that satisfy the following standard deﬁnition.
Deﬁnition 4.2. Given a set M, a metric d : M × M →R is a function that
satisﬁes, for all U, V, W ∈M,
(a) d(U, V) ≥0,
(b) d(U, V) = 0 if and only if U = V,
(c) d(U, V) ≤d(U, W) + d(W, V),
(d) d(U, V) = d(V, U).
We call the pair (M, d) a metric space.
△
In our setting, M is the space of value function estimates, RX. Because we
assume that there are ﬁnitely many states, this space can be equivalently thought
of as the space of real-valued vectors with NX entries, where NX is the number
24. It is also possible to express Pπ as a stochastic matrix, in which case PπV describes a matrix–
vector multiplication. We will return to this point in Chapter 5.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

80
Chapter 4
of states. On this space, we measure distances in terms of the L∞metric, deﬁned
by
∥V −V′∥∞= max
x∈X |V(x) −V′(x)| ,
V, V′ ∈RX.
(4.4)
A key result is that the Bellman operator T π is a contraction mapping with
respect to this metric. Informally, this means that its application to diﬀerent
value function estimates brings them closer by at least a constant multiplicative
factor, called its contraction modulus.
Deﬁnition 4.3. Let (M, d) be a metric space. A function O : M →M is a con-
traction mapping with respect to d and with contraction modulus β ∈[0, 1), if
for all U, U′ ∈M,
d(OU, OU′) ≤βd(U, U′) .
△
Proposition 4.4. The operator T π : RX →RX is a contraction mapping
with respect to the L∞metric on RX with contraction modulus given by
the discount factor γ. That is, for any two value functions V, V′ ∈RX,
∥T πV −T πV′∥∞≤γ∥V −V′∥∞.
△
Proof. The proof is most easily stated in vector notation. Here, we make use of
two properties of the operator Pπ. First, Pπ is linear, in the sense that for any
V, V′,
PπV + PπV′ = Pπ(V + V′).
Second, because (PπV)(x) is a convex combination of elements from V, it must
be that
∥PπV∥∞≤∥V∥∞.
From here, we have
∥T πV −T πV′∥∞= ∥(rπ + γPπV) −(rπ + γPπV′)∥∞
= ∥γPπV −γPπV′∥∞
= γ∥Pπ(V −V′)∥∞
≤γ∥V −V′∥∞,
as desired.
The fact that T π is a contraction mapping guarantees the uniqueness of Vπ as a
solution to the equation V = T πV. As made formal by the following proposition,
because the operator T π brings any two value functions closer together, it cannot
keep more than one value function ﬁxed.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Operators and Metrics
81
Proposition 4.5. Let (M, d) be a metric space and O : M →M be a
contraction mapping. Then O has at most one ﬁxed point in M.
△
Proof. Let β ∈[0, 1) be the contraction modulus of O, and suppose U, U′ ∈M
are distinct ﬁxed points of O, so that d(U, U′) > 0 (following Deﬁnition 4.2).
Then we have
d(U, U′) = d(OU, OU′) ≤βd(U, U′) ,
which is a contradiction.
Because we know that Vπ is a ﬁxed point of the Bellman operator T π, fol-
lowing Proposition 4.5, we deduce that there are no other such ﬁxed points
– and hence no other solutions to the Bellman equation. As the phrasing of
Proposition 4.5 suggests, in some metric spaces, it is possible for O to be a
contraction mapping yet to not possess a ﬁxed point. This can matter when
dealing with return functions, as we will see in the second half of this chapter
and in Chapter 5.
Example 4.6. Consider the no-loop operator
 T π
nlV(x) = Eπ
R + γV(X′)1{X′ , x} | X = x ,
where the name denotes the fact that we omit the next-state value whenever a
transition from x to itself occurs. By inspection, we can determine that the ﬁxed
point of this operator is
Vπ
nl(x) = Eπ
h T−1
X
t=0
γtRt | X0 = x
i
,
where T denotes the (random) ﬁrst time at which XT = XT−1. In words, this
ﬁxed point describes the discounted sum of rewards obtained until the ﬁrst time
that an action leaves the state unchanged.25
Exercise 4.1 asks you to show that T π
nl is a contraction mapping with modulus
β = γ max
x∈X Pπ
 X′ , x | X = x .
Following Proposition 4.5, we deduce that this is the unique ﬁxed point to
T π
nl.
△
25. The reader is invited to consider the kind of environments in which policies that maximize the
no-loop return are substantially diﬀerent from those that maximize the usual expected return.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

82
Chapter 4
When an operator O is contractive, we can also straightforwardly construct a
mathematical approximation to its ﬁxed point.26 This approximation is given
by the sequence (Uk)k≥0, deﬁned by an initial value U0 ∈M, and the recursive
relationship
Uk+1 = OUk .
By contractivity, successive iterates of this sequence must come progressively
closer to the operator’s ﬁxed point. This is formalized by the following.
Proposition 4.7. Let (M, d) be a metric space, and let O : M →M be a
contraction mapping with contraction modulus β ∈[0, 1) and ﬁxed point
U∗∈M. Then for any initial point U0 ∈M, the sequence (Uk)k≥0 deﬁned
by Uk+1 = OUk is such that
d(Uk, U∗) ≤βkd(U0, U∗).
(4.5)
and in particular d(Uk, U∗) →0 as k →∞.
△
Proof. We will prove Equation 4.5 by induction, from which we obtain conver-
gence of (Uk)k≥0 in d. For k = 0, Equation 4.5 trivially holds. Now suppose for
some k ≥0, we have
d(Uk, U∗) ≤βkd(U0, U∗) .
Then note that
d(Uk+1, U∗)
(a)= d(OUk, OU∗)
(b)
≤βd(Uk, U∗)
(c)
≤βk+1d(U0, U∗) ,
where (a) follows from the deﬁnition of the sequence (Uk)k≥0 and the fact that
U∗is ﬁxed by O, (b) follows from the contractivity of O, and (c) follows from
the inductive hypothesis. By induction, we conclude that Equation 4.5 holds for
all k ∈N.
In the case of the Bellman operator T π, Proposition 4.7 means that repeated
application of T π to any initial value function estimate V0 ∈RX produces
a sequence of estimates (Vk)k≥0 that are progressively closer to Vπ. This
observation serves as the starting point for a number of computational
approaches that approximate Vπ, including dynamic programming (Chapter 5)
and temporal-diﬀerence learning (Chapter 6).
26. We use the term mathematical approximation to distinguish it from an approximation that can
be computed. That is, there may or may not exist an algorithm that can determine the elements of
the sequence (Uk)k≥0 given the initial estimate U0.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Operators and Metrics
83
4.3
The Distributional Bellman Operator
Designing distributional reinforcement learning algorithms such as categorical
temporal-diﬀerence learning involves a few choices – such as how to represent
probability distributions in a computer’s memory – that do not have an equiva-
lent in classical reinforcement learning. Throughout this book, we will make
use of the distributional Bellman operator to understand and characterize many
of these choices. To begin, recall the random-variable Bellman equation:
Gπ(x)
D= R + γGπ(X′),
X = x .
(4.6)
As in the expected-value setting, we construct a random-variable operator by
viewing the right-hand side of Equation 4.6 as a transformation of Gπ. In this
case, we break down the transformation of Gπ into three operations, each of
which produces a new random variable (Figure 4.1):
(a) Gπ(X′): the indexing of the collection of random variables Gπ by X′;
(b) γGπ(X′): the multiplication of the random variable Gπ(X′) with the scalar
γ;
(c) R + γGπ(X′): the addition of two random variables (R and γGπ(X′)).
More generally, we may apply these operations to any state-indexed collection
of random variables G =  G(x) : x ∈X), taken to be independent of the random
transitions used to deﬁne the transformation. With some mathematical caveats
discussed below, let us introduce the random-variable Bellman operator
(T πG)(x)
D= R + γG(X′),
X = x .
(4.7)
Equation 4.7 states that the application of the Bellman operator to G (evaluated
at x; the left-hand side) produces a random variable that is equal in distribution
to the random variable constructed on the right-hand side. Because this holds
for all x, we think of T π as mapping G to a new collection of random variables
T πG.
The random-variable operator is appealing because it is concise and easily
understood. In many circumstances, this makes it the tool of choice for reasoning
about distributional reinforcement learning problems. One issue, however, is that
its deﬁnition above is mathematically incomplete. This is because it speciﬁes
the probability distribution of (T πG)(x), but not its identity as a mapping from
some sample space to the real numbers. As discussed in Section 2.7, without
care we may produce random variables that exhibit undesirable behavior: for
example, because rewards at diﬀerent points in time are improperly correlated.
More immediately, the theory of contraction mappings needs a clear deﬁnition
of the space on which an operator is deﬁned – in the case of the random-variable
operator, this requires us to specify a space of random variables to operate
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

84
Chapter 4
(a)
(b)
(c)
Figure 4.1
The random-variable Bellman operator is composed of three operations: (a) indexing
into a collection of random variables, (b) multiplication by the discount factor, and (c)
addition of two random variables. Here, we assume that R and X′ take on a single value
for clarity.
in. Properly deﬁning such a space is possible but requires some technical
subtlety and measure-theoretic considerations; we refer the interested reader to
Section 4.9.
A more direct solution is to consider the distributional Bellman operator
as a mapping on the space of return-distribution functions. Starting with the
distributional Bellman equation
ηπ(x) = Eπ[ bR,γ

#ηπ(X′) | X = x] ,
we again view the right-hand side as the result of applying a series of
transformations, in this case to probability distributions.
Deﬁnition 4.8. The distributional Bellman operator T π : P(R)X →P(R)X is
the mapping deﬁned by
(T πη)(x) = Eπ[ bR,γ

#η(X′) | X = x] .
(4.8)
△
Here, the operations on probability distributions are expressed (rather com-
pactly) by the expectation in Equation 4.8 and the use of the pushforward
distribution derived from the bootstrap function b; these are the operations of
mixing, scaling, and translation previously described in Section 2.8.
We can gain additional insight into how the operator transforms a return
function η by considering the situation in which the random reward R and the
return distributions η(x) admit respective probability densities pR(r | x, a) and
px(z). In this case, the probability density of (T πη)(x), denoted p′
x, is
p′
x(z) = γ−1 X
a∈A
π(a | x)
Z
r∈R
pR(r | x, a)
X
x′∈X
PX(x′ | x, a)px′
z −r
γ

dr .
(4.9)
Expressed in terms of probability densities, the indexing of a collection of
random variables becomes a mixture of densities, while their addition becomes a
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Operators and Metrics
85
convolution; this is in fact what is depicted in Figure 4.1. In terms of cumulative
distribution functions, we have
F(T πη)(x)(z) = Eπ
h
Fη(X′)
z −R
γ

| X = x
i
.
However, we prefer the operator that deals directly with probability distributions
(Equation 4.8) as it can be used to concisely express more complex operations on
distributions. One such operation is the projection of a probability distribution
onto a ﬁnitely parameterized set, which we will use in Chapter 5 to construct
algorithms for approximating ηπ.
Using Deﬁnition 4.8, we can formally express the fact that the return-
distribution function ηπ is the only solution to the equation
η = T πη .
The proof is relatively technical and will be given in Section 4.8.
Proposition 4.9. The return-distribution function ηπ satisﬁes
ηπ = T πηπ
and is the unique ﬁxed point of the distributional Bellman operator T π.
△
When working with the distributional Bellman operator, one should be mind-
ful that the random reward R and next state X′ are generally not independent,
because they both depend on the chosen action A (we brieﬂy mentioned this
concern in Section 2.8). In Equation 4.9, this is explicitly handled by the outer
sum over actions. Analogously, we can make explicit the dependency on A by
introducing a second expectation in Equation 4.8:
(T πη)(x) = Eπ
Eπ[(bR,γ)#η(X′) | X = x, A] | X = x .
By conditioning the inner expectation on the action A, we make the random
variables R and γG(X′) conditionally independent in the inner expectation. We
will make use of this technique in proving Theorem 4.25, the main theoretical
result of this chapter.
In some circumstances, it is useful to translate between operations on proba-
bility distributions and those on random variables. We do this by means of a
representative set of random variables called an instantiation.
Deﬁnition 4.10. Given a probability distribution ν ∈P(R), we say that a ran-
dom variable Z is an instantiation of ν if its distribution is ν, written Z ∼ν.
Similarly, we say that a collection of random variables G = (G(x) : x ∈X) is an
instantiation of a return-distribution function η ∈P(R)X if for every x ∈X, we
have G(x) ∼η(x).
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

86
Chapter 4
Given a return-distribution function η ∈P(R)X, the new return-distribution
function T πη can be obtained by constructing an instantiation G of η, perform-
ing the transformation on the collection of random variables G as described
at the beginning of this section, and then extracting the distributions of the
resulting random variables. This is made formal as follows.
Proposition 4.11. Let η ∈P(R)X, and let G = (G(x) : x ∈X) be an instan-
tiation of η. For each x ∈X, let (X = x, A, R, X′) be a sample transition
independent of G. Then R + γG(X′) has the distribution (T πη)(x):
Dπ(R + γG(X′) | X = x) = (T πη)(x) .
△
Proof. The result follows immediately from the deﬁnition of the distributional
Bellman operator. For clarity, we step through the argument again, mirroring
the transformations set out at the beginning of the section. First, the indexing
transformation gives
Dπ(G(X′) | X = x) =
X
x′∈X
Pπ(X′ = x′ | X = x)η(x′)
= Eπ[η(X′) | X = x] .
Next, scaling by γ yields
Dπ(γG(X′) | X = x) = Eπ[(b0,γ)#η(X′) | X = x] ,
and ﬁnally adding the immediate reward R gives the result
Dπ(R + γG(X′) | X = x) = Eπ[(bR,γ)#η(X′) | X = x] .
Proposition 4.11 is an instance of a recurring principle in distributional rein-
forcement learning that “diﬀerent routes lead to the same answer.” Throughout
this book, we will illustrate this point as it arises with a commutative diagram;
the particular case under consideration is depicted in Figure 4.2.
4.4
Wasserstein Distances for Return Functions
Many desirable properties of reinforcement learning algorithms (for example,
the fact that they produce a good approximation of the value function) are
due to the contractive nature of the Bellman operator T π. In this section, we
will establish that the distributional Bellman operator T π, too, is a contraction
mapping – analogous to the value-based operator, the application of T π brings
return functions closer together.
One diﬀerence between expected-value and distributional reinforcement
learning is that the space of return-distribution functions P(R)X is substantially
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Operators and Metrics
87
η
η′
G
G′
T π
T π
Figure 4.2
A commutative diagram illustrating two perspectives on the application of the distri-
butional Bellman operator. The top horizontal line represents the direct application to
the return-distribution function η, yielding η′. The alternative path ﬁrst instantiates the
return-distribution function η as a collection of random variables G = (G(x) : (x) ∈X),
transforms G to obtain another collection of random variables G′, and then extracts the
distributions of these random variables to obtain η′.
diﬀerent from the space of value functions. To measure distances between value
functions, we can simply treat them as ﬁnite-dimensional vectors, taking the
absolute diﬀerence of value estimates at individual states. By contrast, it is
somewhat less intuitive to see what “close” means when comparing probability
distributions. Throughout this chapter, we will consider a number of probability
metrics that measure distances between distributions, each presenting diﬀer-
ent mathematical and computational properties. We begin with the family of
Wasserstein distances.
Deﬁnition 4.12. Let ν ∈P(R) be a probability distribution with cumulative
distribution function Fν. Let Z be an instantiation of ν (in particular, FZ = Fν).
The generalized inverse F−1
ν
is given by
F−1
ν (τ) = inf
z∈R{z : Fν(z) ≥τ} .
We additionally write F−1
Z = F−1
ν .
△
Deﬁnition 4.13. Let p ∈[1, ∞). The p-Wasserstein distance is a function wp :
P(R) × P(R) →[0, ∞] given by
wp(ν, ν′) =
 Z 1
0
F−1
ν (τ) −F−1
ν′ (τ)
pdτ
!1/p
.
The ∞-Wasserstein distance w∞: P(R) × P(R) →[0, ∞] is
w∞(ν, ν′) = sup
τ∈(0,1)
F−1
ν (τ) −F−1
ν′ (τ)
 .
△
Graphically, the Wasserstein distances between two probability distributions
measure the area between their cumulative distribution functions, with val-
ues along the abscissa taken to the pth power; see Figure 4.3. When p = ∞,
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

88
Chapter 4
this becomes the largest horizontal diﬀerence between the inverse cumulative
distribution functions. The p-Wasserstein distances satisfy the deﬁnition of a
metric, except that they may not be ﬁnite for arbitrary pairs of distributions
in P(R); see Exercise 4.6. Properly speaking, they are said to be extended
metrics, since they may take values on the real line extended to include inﬁnity.
Most probability metrics that we will consider are extended metrics rather than
metrics in the sense of Deﬁnition 4.2. We measure distances between return-
distribution functions in terms of the largest Wasserstein distance between
probability distributions at individual states.
Deﬁnition 4.14. Let p ∈[1, ∞]. The supremum p-Wasserstein distance wp
between two return-distribution functions η, η′ ∈P(R)X is deﬁned by27
wp(η, η′) = sup
x∈X
wp(η(x), η′(x)) .
△
The supremum p-Wasserstein distances fulﬁll all requirements of an extended
metric on the space of return-distribution functions P(R)X; see Exercise 4.7.
Based on these distances, we give our ﬁrst contractivity result regarding the
distributional Bellman operator; its proof is given at the end of the section.
Proposition 4.15. The distributional Bellman operator is a contraction
mapping on P(R)X in the supremum p-Wasserstein distance, for all p ∈
[1, ∞]. More precisely,
wp(T πη, T πη′) ≤γwp(η, η′) ,
for all η, η′ ∈P(R)X.
△
Proposition 4.15 is signiﬁcant in that it establishes a close parallel between
the expected-value and distributional operators. Following the line of reasoning
given in Section 4.2, it provides the mathematical justiﬁcation for the develop-
ment and analysis of computational approaches for ﬁnding the return function
ηπ. More immediately, it also enables us to characterize the convergence of the
sequence
ηk+1 = T πηk
(4.10)
to the return function ηπ.
27. Because we assume that there are ﬁnitely many states, we can equivalently write max in the
deﬁnition of supremum distance. However, we prefer the more generally applicable sup.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Operators and Metrics
89
4
2
0
2
4
Return
0.0
0.2
0.4
0.6
0.8
1.0
Cumulative Probability
′
4
2
0
2
4
Return
0.0
0.2
0.4
0.6
0.8
1.0
Cumulative Probability
′
Figure 4.3
Left: Illustration of the p-Wasserstein distance between a normal distribution ν = N(1, 1)
and a mixture of two normal distributions ν′ = 1
2N(−1, 0.5) + 1
2N(3, 0.5). Right: Illus-
tration of the ℓp metric for the same distributions (see Section 4.5). In both cases, the
shading indicates the axis along which the diﬀerences are taken to the pth exponent.
Proposition 4.16. Suppose that for each (x, a) ∈X × A, the reward distri-
bution PR(· | x, a) is supported on the interval [Rmin, Rmax]. Then for any
initial return function η0 whose distributions are bounded on the interval
h Rmin
1−γ, Rmax
1−γ
i
, the sequence
ηk+1 = T πηk
converges to ηπ in the supremum p-Wasserstein distance (for all p ∈[1, ∞]).
△
The restriction to bounded rewards in Proposition 4.16 is necessary to make
use of the tools developed in Section 4.2, at least without further qualiﬁcation.
This is because Proposition 4.7 requires all distances to be ﬁnite, which is not
guaranteed under our deﬁnition of a probability metric. If, for example, the
initial condition η0 is such that
wp(η0, ηπ) = ∞,
then Proposition 4.15 is not of much use. A less restrictive but more technically
elaborate set of assumptions will be presented later in the chapter. For now, we
provide the proof of the two preceding results. First, we obtain a reasonably
simple proof of Proposition 4.15 by considering an alternative formulation of
the p-Wasserstein distances in terms of couplings.
Deﬁnition 4.17. Let ν, ν′ ∈P(R) be two probability distributions. A coupling
between ν and ν′ is a joint distribution υ ∈P(R2) such that if (Z, Z′) is an
instantiation of υ, then also Z has distribution ν and Z′ has distribution ν′. We
write Γ(ν, ν′) ⊆P(R2) for the set of all couplings of ν and ν′.
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

90
Chapter 4
Proposition 4.18 (see Villani (2008) for a proof). Let p ∈[1, ∞).
Expressed in terms of an optimal coupling, the p-Wasserstein distance
between two distributions ν, ν′ ∈P(R) is
wp(ν, ν′) = min
υ∈Γ(ν,ν′)
E
(Z,Z′)∼υ[|Z −Z′|p]1/p .
The ∞-Wasserstein distance between ν and ν′ can be written as
w∞(ν, ν′) = min
υ∈Γ(ν,ν′) inf
n
z ∈R :
P
(Z,Z′)∼υ(|Z −Z′| > z) = 0
o
.
△
Informally, the optimal coupling ﬁnds an arrangement of the two probability
distributions that maximizes “agreement”: it produces outcomes that are as close
as possible. In Proposition 4.18, the optimal coupling takes on a very simple
form given by inverse cumulative distribution functions. For ν, ν′ ∈P(R), an
optimal coupling is the probability distribution of the random variable
 F−1
ν (τ), F−1
ν′ (τ),
τ ∼U [0, 1].
(4.11)
This can be understood by noting how the 1-Wasserstein distance between ν and
ν′ is obtained by measuring the horizontal distance between the two cumulative
distribution functions, at each level τ ∈[0, 1] (Figure 4.3).
Proof of Proposition 4.15. Let p ∈[1, ∞) be ﬁxed. For each x ∈X, consider the
optimal coupling between η(x) and η′(x) and instantiate it as the pair of ran-
dom variables  G(x),G′(x). Next, denote by (x, A, R, X′) the random transition
beginning in x ∈X, constructed to be independent from G(y) and G′(y), for all
y ∈X. With these variables, write
˜G(x) = R + γG(X′) ,
˜G′(x) = R + γG′(X′) .
By Proposition 4.11, ˜G(x) has distribution (T πη)(x) and ˜G′(x) has distribution
(T πη′)(x). The pair   ˜G(x), ˜G′(x) therefore forms a valid coupling of these
distributions. Now
wp
p

(T πη)(x), (T πη′)(x)
 (a)
≤Eπ
h R + γG(X′) − R + γG′(X′)p | X = x
i
(b)= γp E
hG(X′) −G′(X′)
p | X = x
i
(c)
≤γp X
x′∈X
Pπ(X′ = x′ | X = x) E
hG(x′) −G′(x′)
pi
(d)
≤γp sup
x′∈X
E
hG(x′) −G′(x′)
pi
(e)= γpwp
p(η, η′) .
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Operators and Metrics
91
Taking a supremum over x ∈X on the left-hand side and the pth root of both
sides yields the result. Here, (a) follows since the Wasserstein distance is
deﬁned as a minimum over couplings, (b) follows from algebraic manipulation
of the expectation, (c) follows from independence of the sample transition
(X = x, A, R, X′) and the random variables (G(x),G′(x) : x ∈X), (d) because the
maximum of nonnegative quantities is at least as great as their weighted average,
and (e) follows since (G(x′),G′(x′)) was deﬁned as an optimal coupling of η(x′)
and η′(x′). The proof for p = ∞is similar (see Exercise 4.8).
Proof of Proposition 4.16. Let us denote by PB(R) the space of distributions
bounded on [Vmin, Vmax], where as usual
Vmin = Rmin
1 −γ ,
Vmax = Rmax
1 −γ .
We will show that under the assumption of rewards bounded on [Rmin, Rmax],
(a) the return function ηπ is in PB(R), and
(b) the distributional Bellman operator maps PB(R) to itself.
Consequently, we can invoke Proposition 4.7 with O = T π and M = PB(R)
to conclude that for any initial η0 ∈PB(R)X, the sequence of iterates (ηk)k≥0
converges to ηπ with respect to d = wp, for any p ∈[1, ∞].
To prove (a), note that for any state x ∈X,
Gπ(x) =
∞
X
t=0
γtRt,
X0 = x ,
and since Rt ∈[Rmin, Rmax] for all t, then also Gπ(x) ∈[Vmin, Vmax]. For (b), let
η ∈PB(R)X and denote by G an instantiation of this return-distribution function.
For any x ∈X,
Pπ
 R + γG(X′) ≤Vmax | X = x = Pπ
 γG(X′) ≤Vmax −R | X = x
≥Pπ
 γG(X′) ≤Vmax −Rmax | X = x
= Pπ
 G(X′) ≤Vmax | X = x
= 1.
By the same reasoning,
Pπ
 R + γG(X′) ≥Vmin | X = x = 1.
Since R + γG(X′), X = x is an instantiation of (T πη)(x) for each x, we conclude
that if η ∈PB(R)X, then also T πη ∈PB(R)X.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

92
Chapter 4
4.5
ℓp Probability Metrics and the Cramér Distance
The previous section established that the distributional Bellman operator is well
behaved with respect to the family of Wasserstein distances. However, these
are but a few among many standard probability metrics. We will see in Chapter
5 that theoretical analysis sometimes requires us to study the behavior of the
distributional operator with respect to other metrics. In addition, many practical
algorithms directly optimize a metric (typically expressed as a loss function) as
part of their operation (see Chapter 10). The Cramér distance, a member of the
broader family of ℓp metrics, is of particular interest to us.
Deﬁnition 4.19. Let p ∈[1, ∞). The distance ℓp : P(R) × P(R) →[0, ∞] is a
probability metric deﬁned by
ℓp(ν, ν′) =
 Z
R
|Fν(z) −Fν′(z)|pdz
!1/p
.
(4.12)
For p = 2, this is the Cramér distance.28 The ℓ∞or Kolmogorov–Smirnov
distance is given by
ℓ∞(ν, ν′) = sup
z∈R
Fν(z) −Fν′(z)
 .
The respective supremum ℓp distances are given by (η, η′ ∈P(R)X)
ℓp(η, η′) = sup
x∈X
ℓp
 η(x), η(x′) .
These are extended metrics on P(R)X.
△
Where the p-Wasserstein distances measure diﬀerences in outcomes, the
ℓp distances measure diﬀerences in the probabilities associated with these
outcomes. This is because the exponent p is applied to cumulative probabilities
(this is illustrated in Figure 4.3). The distributional Bellman operator is also a
contraction mapping under the ℓp distances for p ∈[1, ∞), albeit with a larger
contraction modulus.29
28. Historically, the Cramér distance has been deﬁned as the square of ℓ2. In our context, it seems
unambiguous to use the word for ℓ2 itself.
29. For p = 1, the distributional Bellman operator has contraction modulus γ; this is sensible given
that ℓ1 = w1.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Operators and Metrics
93
Proposition 4.20. For p ∈[1, ∞), the distributional Bellman operator T π
is a contraction mapping on P(R)X with respect to ℓp, with contraction
modulus γ
1/p. That is,
ℓp(T πη, T πη′) ≤γ
1/pℓp(η, η′)
for all η, η′ ∈P(R)X.
△
The proof of Proposition 4.20 will follow as a corollary of a more general
result given in Section 4.6. One way to relate it to our earlier result is to consider
the behavior of the sequence deﬁned by
ηk+1 = T πηk.
(4.13)
As measured in the p-Wasserstein distance, the sequence (ηk)k≥0 approaches
ηπ at a rate of γ; but if we instead measure distances using the ℓp metric, this
rate is slower – only γ
1/p. Measured in terms of ℓ∞(the Kolmogorov–Smirnov
distance), the sequence of iterates may in fact not seem to approach ηπ at all.
To see this, it suﬃces to consider a single-state process with zero reward (that
is, PX(X′ = x | X = x) = 1 and R = 0) and a discount factor γ = 0.9. In this case,
ηπ(x) = δ0. For the initial condition η0(x) = δ1, we obtain
η1(x) = (T πη0)(x) = δγ.
Now, the (supremum) ℓ∞distance between ηπ and η0 is 1, because for any
z ∈(0, 1),
Fη0(x)(z) = 0
Fηπ(x)(z) = 1.
However, the ℓ∞distance between ηπ and η1 is also 1, by the same argument
(but now restricted to z ∈(0, γ)). Hence, there is no β ∈[0, 1) for which
ℓ∞(η1, ηπ) < βℓ∞(η0, ηπ).
Exercise 4.16 asks you to prove a similar result for a probability metric called
the total variation distance (see also Figure 4.4).
The more general point is that diﬀerent probability metrics are sensitive to
diﬀerent characteristics of probability distributions, and to varying degrees.
At one extreme, the ∞-Wasserstein distance is eﬀectively insensitive to the
probability associated with diﬀerent outcomes, while at the other extreme,
the Kolmogorov–Smirnov distance is insensitive to the scale of the diﬀerence
between outcomes. In Section 4.6, we will show that a metric’s sensitivity to
diﬀerences in outcomes determines the contraction modulus of the distributional
Bellman operator under that metric; informally speaking, this explains the “nice”
behavior of the distributional Bellman operator under the Wasserstein distances.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

94
Chapter 4
2
1
0
1
2
3
4
5
Return
0.0
0.2
0.4
0.6
0.8
Probability Density
2
1
0
1
2
3
4
5
Return
0.0
0.2
0.4
0.6
0.8
Probability Density
2
1
0
1
2
3
4
5
Return
0.0
0.2
0.4
0.6
0.8
1.0
Cumulative Probability
2
1
0
1
2
3
4
5
Return
0.0
0.2
0.4
0.6
0.8
1.0
Cumulative Probability
(a)
(b)
(c)
(d)
Figure 4.4
The distributional Bellman operator is not a contraction mapping in either the supremum
form of (a, b) total variation distance (dTV, shaded in the top panels; see Exercise
4.16 for a deﬁnition) or (c, d) Kolmogorov–Smirnov distance ℓ∞(vertical distance in
the bottom panels). The left panels show the density (a) and cumulative distribution
function (c) of two distributions η(x) (dashed line) and η′(x) (solid line). The right panels
show the same after applying the distributional Bellman operator (b, d), speciﬁcally
considering the transformation induced by the discount factor γ. The lack of contractivity
can be explained by the fact that neither dTV nor ℓ∞is a homogeneous probability metric
(Section 4.6).
Before moving on, let us summarize the results established thus far. By
combining the theory of contraction mappings with suitable probability metrics,
we were able to characterize the behavior of the iterates
ηk+1 = T πηk .
(4.14)
In the following chapters, we will use this as the basis for the design of
implementable algorithms that approximate the return distribution ηπ and will
appeal to contraction mapping theory to provide theoretical guarantees for these
algorithms. In particular, in Chapter 6, we will analyze the categorical temporal-
diﬀerence learning under the lens of the Cramér distance. While the results
presented until now suﬃce for most practical purposes, the following sections
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Operators and Metrics
95
deal with some of the more technical considerations that arise from study-
ing Equation 4.14 under general conditions, particularly the issue of inﬁnite
distances between distributions.
4.6
Suﬃcient Conditions for Contractivity
In the remainder of this chapter, we characterize in greater generality the
behavior of the sequence of return function estimates described by Equation
4.14, viewed under the lens of diﬀerent probability metrics. We begin with a
formal deﬁnition of what it means for a function d to be a probability metric.
Deﬁnition 4.21. A probability metric is an extended metric on the space of
probability distributions, written
d : P(R) × P(R) →[0, ∞] .
Its supremum extension is the function d : P(R)X × P(R)X →R deﬁned as
d(η, η′) = sup
x∈X
d(η(x), η′(x)) .
We refer to d as a return-function metric; it is an extended metric on P(R)X.
△
Our analysis is based on three properties that a probability metric should
possess in order to guarantee contractivity. These three properties relate closely
to the three fundamental operations that make up the distributional Bellman
operator: scaling, convolution, and mixture of distributions (equivalently: scal-
ing, addition, and indexing of random variables). In this analysis, we will ﬁnd
that some properties are more easily stated in terms of random variables, oth-
ers in terms of probability distributions. Accordingly, given two probability
distributions ν, ν′ with instantiations Z, Z′, let us overload notation and write
d(Z, Z′) = d(ν, ν′).
Deﬁnition 4.22. Let c > 0. The probability metric d is c-homogeneous if for any
scalar γ ∈[0, 1) and any two distributions ν, ν′ ∈P(R) with associated random
variables Z, Z′, we have
d(γZ, γZ′) = γcd(Z, Z′) .
In terms of probability distributions, this is equivalently given by the condition
d((b0,γ)#ν, (b0,γ)#ν′) = γcd(ν, ν′) .
If no such c exists, we say that d is not homogeneous.
△
Deﬁnition 4.23. The probability metric d is regular if for any two distributions
ν, ν′ ∈P(R) with associated random variables Z, Z′, and an independent random
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

96
Chapter 4
variable W, we have
d(W + Z, W + Z′) ≤d(Z, Z′) .
(4.15)
In terms of distributions, this is
d EW[(bW,1)#ν], EW[(bW,1)#ν′] ≤d(ν, ν′) .
△
Deﬁnition 4.24. Given p ∈[1, ∞), the probability metric d is p-convex30 if for
any α ∈(0, 1) and distributions ν1, ν2, ν′
1, ν′
2 ∈P(R), we have
dp αν1 + (1 −α)ν2, αν′
1 + (1 −α)ν′
2
 ≤αdp(ν1, ν′
1) + (1 −α)dp(ν2, ν′
2) .
△
Although this p-convexity property is given for a mixture of two distributions,
it implies an analogous property for mixtures of ﬁnitely many distributions.
Theorem 4.25. Consider a probability metric d. Suppose that d is regular,
c-homogeneous for some c > 0, and that there exists p ∈[1, ∞) such that d
is p-convex. Then for all return-distribution functions η, η′ ∈P(R)X, we
have
d(T πη, T πη′) ≤γcd(η, η′) .
△
Proof. Fix a state x ∈X and action a ∈A. For this state, consider the sample
transition (X = x, A = a, R, X′) (Equation 2.12), and recall that R and X′ are
independent given X and A, since
R ∼PR( · | X, A)
X′ ∼PX( · | X, A) .
Let G and G′ be instantiations of η and η′, respectively.31 We introduce a state-
action variant of the distributional Bellman operator, T : P(R)X →P(R)X×A,
given by
(T η)(x, a) = E[(bR,γ)#η(X′) | X = x, A = a].
Note that this operator is deﬁned independently of the policy π, since the action
a is speciﬁed as an argument. We then calculate directly, indicating where each
hypothesis of the result is used:
dp((T η)(x, a), (T η′)(x, a)) = dp R + γG(X′), R + γG′(X′)
(a)
≤dp γG(X′), γG′(X′)
(b)= γpcdp G(X′),G′(X′)
30. This matches the usual deﬁnition of convexity (for dp) if one treats the pair (ν1, ν2) as a single
argument from P(R) × P(R).
31. Note: the proof does not assume the independence of G(x) and G(y), x , y. See Exercise 4.12.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Operators and Metrics
97
(c)
≤γpcX
x′∈X
Pπ(X′ = x′ | X = x, A = a)dp G(x′),G′(x′)
≤γpc sup
x′∈X
dp G(x′),G′(x′)
= γpcd
p(η, η′) .
Here, (a) follows from regularity of d, (b) follows from the c-homogeneous
property of d, and (c) follows from p-convexity, where the mixture is over the
values taken by the random variable X′. We also note that
(T πη)(x) =
X
a∈A
π(a | x)(T η)(x, a),
and hence by p-convexity of d, we have
dp (T πη)(x), (T πη′)(x) ≤
X
a∈A
π(a | x)dp (T πη)(x, a), (T πη′)(x, a)
≤γpcd
p(η, η′) .
Taking the supremum over x ∈X on the left-hand side and taking pth roots then
yields the result.
Theorem 4.25 illustrates how the contractivity of the distributional Bellman
operator in the probability metric d (speciﬁcally, in its supremum extension)
follows from natural properties of d. We see that the contraction modulus is
closely tied to the homogeneity of d, which informally characterizes the extent
to which scaling random variables by a factor γ brings them “closer together.”
The theorem provides an alternative to our earlier result regarding Wasserstein
distances and enables us to establish the contractivity under the ℓp distances but
also under other probability metrics. Exercise 4.19 explores contractivity under
the so-called maximum mean discrepancy (MMD) family of distances.
Proof of Proposition 4.20 (contractivity in ℓp distances). We will apply Theo-
rem 4.25 to the probability metric ℓp, for p ∈[1, ∞). It is therefore suﬃcient to
demonstrate that ℓp is 1/p-homogeneous, regular, and p-convex.
1/p-homogeneity. Let ν, ν′ ∈P(R) with associated random variables Z, Z′.
We make use of the fact that for γ ∈[0, 1),
FγZ(z) = FZ
 z
γ

.
Writing ℓp
p for the pth power of ℓp, we have
ℓp
p(γZ, γZ′) =
Z
R
FγZ(z) −FγZ′(z)
pdz
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

98
Chapter 4
=
Z
R
FZ( z
γ) −FZ′( z
γ)
pdz
(a)=
Z
R
γ
FZ(z) −FZ′(z)
pdz
= γℓp
p(Z, Z′) ,
(4.16)
where (a) follows from a change of variables z/γ 7→z in the integral. Therefore,
we deduce that ℓp(γZ, γZ′) = γ1/pℓp(Z, Z′).
Regularity. Let ν, ν′ ∈P(R), with Z, Z′ independent instantiations of ν and
ν′, respectively, and let W be a random variable independent of Z, Z′. Then,
ℓp
p(W + Z, W + Z′) =
Z
R
|FW+Z(z) −FW+Z′(z)|pdz
=
Z
R
|EW[FZ(z −W)] −EW[FZ′(z −W)]|pdz
(a)
≤
Z
R
EW[|FZ(z −W) −FZ′(z −W)|p]dz
(b)= EW
 Z
R
|FZ(z −W) −FZ′(z −W)|p]dz

= ℓp
p(Z, Z′) ,
where (a) follows from Jensen’s inequality, and (b) follows by swapping the
integral and expectation (more formally justiﬁed by Tonelli’s theorem).
p-convexity. Let α ∈(0, 1), and ν1, ν′
1, ν2, ν′
2 ∈P(R). Note that
Fαν1+(1−α)ν2(z) = αFν1(z) + (1 −α)Fν2(z) ,
and similarly for the primed distributions ν′
1, ν′
2. By convexity of the function
z 7→|z|p on the real numbers and Jensen’s inequality,
|Fαν1+(1−α)ν2(z) −Fαν′
1+(1−α)ν′
2(z)|p
≤α|Fν1(z) −Fν′
1(z)|p + (1 −α)|Fν2(z) −Fν′
2(z)|p ,
for all z ∈R. Hence,
ℓp
p(αν1 + (1 −α)ν2, αν′
1 + (1 −α)ν′
2) ≤αℓp
p(ν1, ν′
1) + (1 −α)ℓp
p(ν2, ν′
2),
and ℓp is p-convex.
4.7
A Matter of Domain
Suppose that we have demonstrated, by means of Theorem 4.25, that the distri-
butional Bellman operator is a contraction mapping in the supremum extension
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Operators and Metrics
99
of some probability metric d. Is this suﬃcient to guarantee that the sequence
ηk+1 = T πηk
converges to the return function ηπ, by means of Proposition 4.7? In general,
no, because d may assign inﬁnite distances to certain pairs of distributions.
To invoke Proposition 4.7, we identify a subset of probability distributions
Pd(R) that are all within ﬁnite d-distance of each other and then ensure that
the distributional Bellman operator is well behaved on this subset. Speciﬁcally,
we identify a set of conditions under which
(a) the distributional Bellman operator T π maps Pd(R)X to itself, and
(b) the return function ηπ (the ﬁxed point of T π) lies in Pd(R)X.
For most common probability metrics and natural problem settings, these
requirements are easily veriﬁed. In Proposition 4.16, for example, we demon-
strated that under the assumption that the reward distributions are bounded,
then Proposition 4.7 can be applied with the Wasserstein distances. The aim of
this section is to extend the analysis to a broader set of probability metrics but
also to a greater number of problem settings, including those where the reward
distributions are not bounded.
Deﬁnition 4.26. Let d be a probability metric. Its ﬁnite domain Pd(R) ⊆P(R)
is the set of probability distributions with ﬁnite ﬁrst moment and ﬁnite d-
distance to the distribution that puts all of its mass on zero:
Pd(R) = ν ∈P(R) : d(ν, δ0) < ∞, E
Z∼ν[|Z|] < ∞	 .
(4.17)
△
By the triangle inequality, for any two distributions ν, ν′ ∈Pd(R), we are
guaranteed d(ν, ν′) < ∞. Although the choice of δ0 as the reference point is some-
what arbitrary, it is sensible given that many reinforcement learning problems
include the possibility of receiving no reward at all (e.g., Gπ(x) = 0). The ﬁnite
ﬁrst-moment assumption is made in light of Assumption 2.5, which guarantees
that return distributions have well-deﬁned expectations.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

100
Chapter 4
Proposition 4.27. Let d be a probability metric satisfying the conditions
of Theorem 4.25, with ﬁnite domain Pd(R). Let T π be the distribu-
tional Bellman operator corresponding to a given Markov decision process
(X, A, ξ0, PX, PR). Suppose that
(a) ηπ ∈Pd(R), and
(b) Pd(R) is closed under T π: for any η ∈Pd(R)X, we have that T πη ∈
Pd(R)X.
Then for any initial condition η0 ∈Pd(R), the sequence of iterates deﬁned
by
ηk+1 = T πηk
converges to ηπ with respect to d.
△
Proposition 4.27 is a specialization of Proposition 4.7 to the distributional
setting and generalizes our earlier result regarding bounded reward distributions.
Eﬀectively, it allows us to prove the convergence of the sequence (ηk)k≥0 for a
family of Markov decision processes satisfying the two conditions above. The
condition ηπ ∈Pd(R), while seemingly benign, does not automatically hold;
Exercise 4.20 illustrates the issue using a modiﬁed p-Wasserstein distance.
Example 4.28 (ℓp metrics). For a given p ∈[1, ∞), the ﬁnite domain of the
probability metric ℓp is
Pℓp(R) = {ν ∈P(R) : E
Z∼ν[|Z|] < ∞} ,
the set of distributions with ﬁnite ﬁrst moment. This follows because
ℓp
p(ν, δ0) =
Z 0
−∞
Fν(z)pdz +
Z ∞
0
(1 −Fν(z))pdz
≤
Z 0
−∞
Fν(z)dz +
Z ∞
0
(1 −Fν(z))dz
(a)= E[max(0, −Z)] + E[max(0, Z)]
= E[|Z|] ,
where (a) follows from expressing Fν(z) and 1 −Fν(z) as integrals and using
Tonelli’s theorem: Z ∞
0
(1 −Fν(z))dz =
Z ∞
0
E
Z∼ν[1{Z > z}]dz
= E
Z∼ν
h Z ∞
0
1{Z > z}dz
i
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Operators and Metrics
101
= E
Z∼ν[max(0, Z)] ,
and similarly for the integral from −∞to 0.
The conditions of Proposition 4.27 are guaranteed by Assumption 2.5
(rewards have ﬁnite ﬁrst moments). From Chapter 2, we know that under
this assumption, the random return has ﬁnite expected value and hence
ηπ(x) ∈Pℓp(R),
for all x ∈X.
Similarly, we can show from elementary operations that if R and G(x) satisfy
Eπ
|R|
 X = x] < ∞,
E[|G(x)| < ∞,
for all x ∈X ,
then also
Eπ
|R + γG(X′)|
 X = x < ∞,
for all x ∈X .
Following Proposition 4.27, provided that η0 ∈Pℓp(R)X, then the sequence
(ηk)k≥0 converges to ηπ with respect to ℓp.
△
When d is the p-Wasserstein distance (p ∈[1, ∞]), the ﬁnite domain takes on
a particularly useful form that we denote by Pp(R):
Pp(R) = ν ∈P(R) : E
Z∼ν[|Z|p] < ∞	 ,
p ∈[0, ∞) ,
P∞(R) = ν ∈P(R) : ∃C > 0 s.t. ν([−C,C]) = 1	 .
For p < ∞, this is the set of distributions with bounded pth moments; for p = ∞,
the set of distributions with bounded support. In particular, observe that the
ﬁnite domains of ℓ1 and w1 coincide: Pℓp(R) = P1(R).
As with the ℓp distances, we can satisfy the conditions of Proposition 4.27
for d = wp by introducing an assumption on the reward distributions. In this
case, we simply require that these be in Pp(R). As this assumption will recur
throughout the book, we state it here in full; Exercise 4.11 goes through the
steps of the corresponding result.
Assumption 4.29(p). For each state-action pair (x, a) ∈X × A, the reward
distribution PR( · | x, a) is in Pp(R).
△
Proposition 4.30. Let p ∈[1, ∞]. Under Assumption 4.29(p), the return
function ηπ has ﬁnite pth moments (p = ∞: is bounded). In addition, for
any initial return function η0 ∈Pp(R), the sequence deﬁned by
ηk+1 = T πηk
converges to ηπ with respect to the supremum p-Wasserstein metric.
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

102
Chapter 4
4.8
Weak Convergence of Return Functions*
Proposition 4.27 implies that if each distribution ηπ(x) lies in the ﬁnite domain
Pd(R) of a given probability metric d that is regular, c-homogeneous, and
p-convex, then ηπ is the unique solution to the equation
η = T πη
(4.18)
in the space Pd(R)X. It does not, however, rule out the existence of solutions
outside this space. This concern can be addressed by showing that for any
η0 ∈P(R)X, the sequence of probability distributions (ηk(x))k≥0 deﬁned by
ηk+1 = T πηk
converges weakly to the return distribution ηπ(x), for each state x ∈X. In addition
to giving an alternative perspective on the quantitative convergence results of
these iterates, the uniqueness of ηπ as a solution to Equation 4.18 (stated as
Proposition 4.9) follows immediately from Proposition 4.34 below.
Deﬁnition 4.31. Let (νk)k≥0 be a sequence of distributions in P(R), and let ν ∈
P(R) be another probability distribution. We say that (νk)k≥0 converges weakly
to ν if for every z ∈R at which Fν is continuous, we have Fνk(z) →Fν(z).
△
We will show that for each x ∈X, the sequence (ηk(x))k≥0 converges weakly
to ηπ(x). A simple approach is to consider the relationships between well-chosen
instantiations of ηk (for each k ∈N) and ηπ, by means of the following classical
result (see e.g., Billingsley 2012). Recall that a sequence of random variables
(Zk)k≥0 converges to Z with probability 1 if
P(lim
k→∞Zk = Z) = 1 .
Lemma 4.32. Let (νk)k≥0 be a sequence in P(R) and ν ∈P(R) be another
probability distribution. Let (Zk)k≥0 and Z be instantiations of these distributions
all deﬁned on the same probability space. If Zk →Z with probability 1, then
νk →ν weakly.
△
Lemma 4.32 is not a uniformly applicable approach to demonstrating
weak convergence; there always exists such instantiations by Skorokhod’s
representation theorem (Section 25), but ﬁnding such instantiations is not
always straightforward. However, in our case, there are a very natural set of
instantiations that work, constructed by the following result.
Lemma 4.33. Let η ∈P(R)X, and let G be an instantiation of η. For x ∈X, if
(Xt, At, Rt)t≥0 is a random trajectory with initial state X0 = x and generated by
following π, independent of G, then Pk−1
t=0 γtRt + γkG(Xk) is an instantiation of
((T π)kη)(x).
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Operators and Metrics
103
Proof. This follows by inductively applying Proposition 4.11.
Proposition 4.34. Let η0 ∈P(R)X, and for k ≥0 deﬁne
ηk+1 = T πηk .
Then we have ηk(x) →ηπ(x) weakly for each x ∈X, and consequently ηπ is
the unique ﬁxed point of T π in P(R)X.
△
Proof. Fix x ∈X, let G0 be an instantiation of η0, and on the same probability
space, let (Xt, At, Rt)t≥0 be a trajectory generated by π with initial state X0 = x,
independent of G0 (the existence of such a probability space is guaranteed by
the Ionescu–Tulcea theorem, as described in Remark 2.1). Then
Gk(x) =
k−1
X
t=0
γtRt + γkG0(Xk)
is an instantiation of ηk(x) by Lemma 4.33. Furthermore, P∞
t=0 γtRt is an
instantiation of ηπ(x). We have
Gk(x) −
∞
X
t=0
γtRt
 =
γkG0(Xk) −
∞
X
t=k
γtRt
 ≤γkG0(Xk)
 +

∞
X
t=k
γtRt
 →0 ,
with probability 1. The convergence of the ﬁrst term follows since |G0(Xk)| ≤
maxx∈X |G0(x)| is bounded with probability 1 (w.p. 1) and γk →0, and the
convergence of the second term follows from convergence of Pk−1
t=0 γtRt to
P∞
t=0 γtRt w.p. 1. Therefore, we have Gk(x) →P∞
t=0 γtRt w.p. 1, and so ηk(x) →
ηπ(x) weakly. Finally, this implies that there can be no other ﬁxed point of
T π in P(R)X; if η0 were such a ﬁxed point, then we would have ηk = η0 for
all k ≥0 and would simultaneously have ηk(x) →ηπ(x) weakly for all x ∈X, a
contradiction unless η0 = ηπ.
As the name indicates, the notion of weak convergence is not as strong as
many other notions of convergence of probability distributions. In general, it
does not even guarantee convergence of the mean of the sequence of distribu-
tions; see Exercise 4.21. In addition, we lose any notion of convergence rate
provided by the contractive nature of the distributional operator under speciﬁc
metrics. The need for stronger guarantees, such as those oﬀered by Proposition
4.27, motivates the contraction mapping theory developed in this chapter.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

104
Chapter 4
4.9
Random-Variable Bellman Operators*
In this chapter, we deﬁned the distributional Bellman operator T π as a mapping
on the space of return-distribution functions P(R)X. We also saw that the action
of the operator on a return function η ∈P(R)X can be understood both through
direct manipulation of the probability distributions or through manipulation of
a collection of random variables instantiating these distributions.
Viewing the operator through its eﬀect on the distribution of a collection of
representative random variables is a useful tool for understanding distributional
reinforcement learning and may prompt the reader to ask whether it is possible
to avoid referring to probability distributions at all, working instead directly
with random variables. We describe one approach to this below using the tools
of probability theory and then discuss some of its shortcomings.
Let G0 = (G0(x) : x ∈X) be an initial collection of real-valued random vari-
ables, indexed by state, supported on a probability space (Ω0, F0, P0). For each
k ∈N+, let (Ωk, Fk, Pk) be another probability space, supporting a collection of
random variables ((Ak(x), Rk(x, a), X′
k(x, a)) : x ∈X, a ∈A), with Ak(x) ∼π( · | x),
and independently Rk(x, a) ∼PR( · | x, a), Xk(x, a) ∼PX( · | x, a). We then con-
sider the product probability space on Ω= Q
k∈N Ωk. All random variables
deﬁned above can naturally be viewed as functions on this joint probability
space which depend on ω = (ω0, ω1, ω2, . . . ) ∈Ωonly through the coordinate
ωk that matches the index k on the random variable. Note that under this
construction, all random variables with distinct indices are independent.
Now deﬁne XN as the set of real-valued random variables on (Ω, F, P)
(where F is the product σ-algebra) that depend on only ﬁnitely many coordi-
nates of ω ∈Ω. We can deﬁne a Bellman operator T π : XN →XN as follows.
Given G = (G(x) : x ∈X) ∈X X
N , let K ∈N be the smallest integer such that the
random variables (G(x) : x ∈X) depend on ω = (ω0, ω1, ω2, …) ∈Ωonly through
ω0, …, ωK−1; such an integer exists due to the deﬁnition of XN and the ﬁniteness
of X. We then deﬁne T πG ∈XN by
(T πG)(x) = RK(x, AK(x)) + γG(X′
K(x, AK(x)) .
With this deﬁnition, we can obtain a sequence of collections of random vari-
ables (Gk)k≥0, deﬁned iteratively by Gk+1 = T πGk, for k ≥0.32 We have therefore
formalized an operator entirely within the realm of random variables, without
reference to the distributions of the iterates (Gk)k≥0. By construction, the distri-
bution of the random variables (Gk)k≥0 matches the sequence of distributions
that would be obtained by working directly on the space of probability dis-
tributions with the usual distributional Bellman operator. More concretely, if
32. This is real equality between random variables, rather than in distribution.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Operators and Metrics
105
η0 ∈P(R)X is such that η0(x) is the distribution of G0(x) for each x ∈X, then
we have that ηk, deﬁned by ηk = (T π)kη0, is such that ηk(x) is the distribution of
Gk(x), for each x ∈X. Thus, the random-variable Bellman operator constructed
above is consistent with the distributional Bellman operator that is the main
focus of this chapter.
One diﬃculty with this random variable operator is that it does not have a
ﬁxed point; while the distribution of the random variables Gk(·) converges to that
of Gπ(·), the random variables themselves, as functions on the probability space,
do not converge.33 Thus, while it is one way to view distributional reinforcement
learning purely in terms of random variables, it is much less natural to analyze
algorithms from this perspective, rather than through probability distributions
as described in this chapter.
4.10
Technical Remarks
Remark 4.1. Our exposition in this chapter has focused on the standard
distributional Bellman equation
Gπ(x)
D= R + γGπ(X′), X = x .
A similar development is possible with the alternative notions of random return
mentioned in Section 2.9, including the random-horizon return. In general, the
distributional operators that arise from these alternative notions of the random
return are still contraction mappings, although the metrics and contraction
moduli involved in these statements diﬀer. For example, while the standard
distributional Bellman operator is not a contraction in total variation distance,
the distributional Bellman operator associated with the random-horizon return
is; Exercise 4.17 explores this point in greater detail.
△
Remark 4.2. The ideas developed in this chapter, and indeed the other chapters
of the book, can also be applied to learning other properties related to the return.
Achab (2020) explores several methods for learning the distribution of the
random variables R + γVπ(X′), under the diﬀerent possible initial conditions X =
x; these objects interpolate between the expected return and the full distribution
of the return. Exercise 4.22 explores the development of contractive operators
for the distributions of these objects.
△
Remark 4.3. As well as allowing us to quantify the contractivity of the distribu-
tional Bellman operator, the probability metrics described in this chapter can be
used to measure the accuracy of algorithms that aim to approximate the return
33. This is a fairly subtle point – the reader is invited to consider what happens to Gk(x) and Gπ(x)
as mappings from Ωto R.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

106
Chapter 4
distribution. In particular, they can be used to give quantitative guarantees on the
accuracy of the nonparametric distributional Monte Carlo algorithm described
in Remark 3.1. These guarantees can then be used to determine how many
sample trajectories are required to approximate the true return distribution at a
required level of accuracy: for example, when evaluating the performance of
the algorithms in Chapters 5 and 6. We assume that K returns (Gk)K
k=1 beginning
at state x0 have been generated independently via the policy π and consider the
accuracy of the nonparametric distributional Monte Carlo estimator
ˆηπ(x0) = 1
K
K
X
k=1
δGk .
Diﬀerent realizations of the sampled returns will lead to diﬀerent estimates; the
following result provides a guarantee in the Kolmogorov–Smirnov distance ℓ∞
that holds with high probability over the possible realizations of the returns. For
any ε > 0, we have
ℓ∞(ˆηπ(x0), ηπ(x0)) ≤ε , with probability at least 1 −2 exp(−2Kε2) .
Thus, for a desired level of accuracy ε and a desired conﬁdence 1 −δ, K can
be selected so that 2 exp(−2Kε2) < δ, which then yields the guarantee that with
probability at least 1 −δ, we have ℓ∞(ˆηπ(x0), ηπ(x0)) ≤ε. This is in fact a key
result in empirical process theory known as the Dvoretzky–Kiefer–Wolfowitz
inequality (Dvoretzky et al. 1956; Massart 1990). Its uses speciﬁcally in rein-
forcement learning include the works of Keramati et al. (2020) and Chandak
et al. (2021). Similar concentration bounds are possible under other probability
metrics (such as the Wasserstein distances; see, e.g., Weed and Bach 2019;
Bobkov and Ledoux 2019), though typically some form of a priori information
about the return distribution, such as bounds on minimum/maximum returns, is
required to establish such bounds.
△
4.11
Bibliographical Remarks
4.1–4.2. The use of operator theory to understand reinforcement learning algo-
rithms is standard to most textbooks in the ﬁeld (Bertsekas and Tsitsiklis 1996;
Szepesvári 2010; Puterman 2014; Sutton and Barto 2018), and is commonly
used in theoretical reinforcement learning (Munos 2003; Bertsekas 2011; Scher-
rer 2014). Puterman (2014) provides a thorough introduction to vector notation
for Markov decision processes. Improved convergence results can be obtained
by studying the eigenspectrum of the transition kernel, as shown by, for exam-
ple, Morton (1971) and Bertsekas (1994, 2012). The elementary contraction
mapping theory described in Section 4.2 goes back to Banach (1922). Our
reference on metric spaces is the deﬁnitive textbook by Rudin (1976).
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Operators and Metrics
107
Not all reinforcement learning algorithms are readily analyzed using the
theory of contraction mappings. This is the case for policy-gradient algorithms
(Sutton et al. 2000; but see Ghosh et al. 2020; Bhandari and Russo 2021), but
also value-based algorithms such as advantage learning (Baird 1999; Bellemare
et al. 2016).
4.3. The random-variable Bellman operator presented here is from our earlier
work (Bellemare et al. 2017a), which provided an analysis in the p-Wasserstein
distances. There, the technical issues discussed in Section 4.9 were obviated
by declaring R, X′ and the collection (G(x) : x ∈X) to be independent. Those
issues were raised in a later paper (Rowland et al. 2018), which also introduced
the distributional Bellman equation in terms of probability measures and the
pushforward notation. The probability density equation (Equation 4.9) can be
found in Morimura et al. (2010b). Earlier instances of distributional operators
are given by Chung and Sobel (1987) and Morimura et al. (2010a), who provide
an operator on cumulative distribution functions and Jaquette (1976), who
provides an operator on Laplace transforms.
4.4. The Wasserstein distance can be traced back to Leonid Kantorovich (1942)
and has been rediscovered (and renamed) multiple times in its history. The name
we use here is common but a misnomer as Leonid Vaserstein (after whom the
distance is named) did not himself do any substantial work on the topic. Among
other names, we note the Mallows metric (Bickel and Freedman 1981) and the
Earth–Mover distance (Rubner et al. 1998). Much earlier, Monge (1781) was
the ﬁrst to study the problem of optimal transportation from a transport theory
perspective. See Vershik (2013) and Panaretos and Zemel (2020) for further
historical comments and Villani (2008) for a survey of theoretical properties.
A version of the contraction analysis in p-Wasserstein distances was given by
Bellemare et al. (2017a); we owe the proof of Proposition 4.15 in terms of
optimal couplings to Philip Amortila.
The use of contraction mapping theory to analyze stochastic ﬁxed-point equa-
tions was introduced by Rösler (1991), who analyzes the Quicksort algorithm
by characterizing the distributional ﬁxed points of contraction mappings in 2-
Wasserstein distance. Applications and generalization of this technique include
the analysis of further recursive algorithms, models in stochastic geometry, and
branching processes (Rösler 1992; Rachev and Rüschendorf 1995; Neininger
1999; Rösler and Rüschendorf 2001; Rösler 2001; Neininger 2001; Neininger
and Rüschendorf 2004; Rüschendorf 2006; Rüschendorf and Neininger 2006;
Alsmeyer 2012). Although the random-variable Bellman equation (Equation
2.15) can be viewed as a system of recursive distributional equations, the empha-
sis on a collection of eﬀectively independent random variables (Gπ) diﬀers from
the usual treatment of such equations.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

108
Chapter 4
4.5. The family of ℓp distances described in this chapter is covered at length
in the work of Rachev et al. (2013), which studies an impressive variety of
probability metrics. A version of the contraction analysis in Cramér distance
was originally given by Rowland et al. (2018). In two and more dimensions, the
Cramér distance is generalized by the energy distance (Székely 2002; Székely
and Rizzo 2013; Rizzo and Székely 2016), itself a member of the maximum
mean discrepancy (MMD) family (Gretton et al. 2012); contraction analysis
in terms of MMD metrics was undertaken by Nguyen et al. (2021) (see Exer-
cise 4.19 for further details). Another special case of the ℓp metrics considered in
this chapter is the Kolmogorov–Smirnov distance (ℓ∞), which features in results
in empirical process theory, such as the Glivenko–Cantelli theorem. Many of
these metrics are integral probability metrics (Müller 1997), which allows for a
dual formulation with appealing algorithmic consequences. Chung and Sobel
(1987) provide a nonexpansion result in total variation distance (without naming
it as such; the proof uses an integral probability metric formulation).
4.6. The properties of regularity, convexity, and c-homogeneity were introduced
by Zolotarev (1976) in a slightly more general setting. Our earlier work pre-
sented these in a modern context (Bellemare et al. 2017b), albeit with only a
mention of their potential use in reinforcement learning. Although that work
proposed the term “sum-invariant” as mnemonically simpler, this is only techni-
cally correct when Equation 4.15 holds with equality; we have thus chosen to
keep the original name. Theorem 4.25 is new to this book.
The characterization of the Wasserstein distance as an optimal transport prob-
lem in Proposition 4.18 is the standard presentation of the Wasserstein distance
in more abstract settings, which allows it to be applied to probability distri-
butions over reasonably general metric spaces (Villani 2003, 2008; Ambrosio
et al. 2005; Santambrogio 2015). Optimal transport has also increasingly found
application within machine learning in recent years, particularly in generative
modeling (Arjovsky et al. 2017). Optimal transport and couplings also arise
in the study of bisimulation metrics for Markov decision processes (Ferns et
al. 2004; Ferns and Precup 2014; Amortila et al. 2019) as well as analytical
tools for sample-based algorithms (Amortila et al. 2020). Peyré and Cuturi
(2019) provide an overview of algorithms, analysis, and applications associated
with optimal transport in machine learning and related disciplines.
4.7–4.8. Villani (2008) gives further discussion on the domain of Wasserstein
distances and on their relationship to weak convergence.
4.9. The usefulness of a random-variable operator has been a source of intense
debate between the authors of this book. The form we present here is inspired
by the “stack of rewards” model from Lattimore and Szepesvári (2020).
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Operators and Metrics
109
4.12
Exercises
Exercise 4.1. Show that the no-loop operator deﬁned in Example 4.6 is a
contraction mapping with modulus
β = γ max
x∈X Pπ
 X′ , x | X = x .
△
Exercise 4.2. For p ∈[1, ∞), let ∥· ∥p be the Lp norm over RX, deﬁned as
∥V∥p =
 X
x∈X
|V(x)|p1/p.
Show that T π is not a contraction mapping in the metric induced by the Lp
norm unless p = ∞(see Equation 4.4 for a deﬁnition of the L∞metric). Hint. A
two-state example suﬃces.
△
Exercise 4.3. In this exercise, you will use the ideas of Section 4.1 to study
several operators associated with expected-value reinforcement learning.
(i) For n ∈N+, consider the n-step evaluation operator T π
n : RX →RX deﬁned
by
(T π
nV)(x) = Eπ
 n−1
X
t=0
γtRt + γnV(Xn)
 X0 = x

.
Show that T π
n has Vπ as a ﬁxed point and is a contraction mapping with
respect to the L∞metric with contraction modulus γn. Hence, deduce that
repeated application of T π
n to any initial value function estimate converges
to Vπ. Show that in fact, T π
n = (T π)n.
(ii) Consider the λ-return operator T π
λ : RX →RX deﬁned by
(T π
λV)(x) = (1 −λ)
∞
X
n=1
λn−1Eπ
 n−1
X
t=0
γtRt + γnV(Xn)
 X0 = x

.
Show that T π
λ has Vπ as a ﬁxed point and is a contraction mapping with
respect to the L∞metric with contraction modulus
γ
 1 −λ
1 −γλ
!
.
Hence, deduce that repeated application of T π
λ to any initial value function
estimate converges to Vπ.
△
Exercise 4.4. Consider the random-variable operator (Equation 4.7). Given
a return-variable function G0, write G0(x, ω) =G0(x)(ω) for the realization
of the random return corresponding to ω ∈Ω. Additionally, for each x ∈X,
let (x, A(x), R(x), X′(x)) be an independent random transition deﬁned on the
same probability space, and write (A(x, ω), R(x, ω), X′(x, ω)) to denote the
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

110
Chapter 4
dependence of these random variables on ω ∈Ω. Suppose that for each k ≥0,
we deﬁne the return-variable function Gk+1 as
Gk+1(x, ω) = R(x, ω) + γGk(X′(x, ω), ω) .
For a given x, characterize the function
G∗(x, ω) = lim
k→∞Gk(x, ω) .
△
Exercise 4.5. Suppose that you are given a description of a Markov decision
process along with a policy π, with the property that the policy π reaches a
terminal state (i.e., one for which the return is zero) in at most T ∈N steps.
Describe a recursive procedure that takes in a scalar ω on [0, 1] and outputs
a return z ∈R such that P Gπ(X0) ≤z = ω. Hint. You may want to use the fact
that sampling a random variable Z can be emulated by drawing τ uniformly
from [0, 1], and returning F−1
Z (τ).
△
Exercise 4.6. Let p ∈[1, ∞].
(i) Show that for any ν, ν′ ∈P(R) with ﬁnite pth moments, we have wp(ν, ν) <
∞. Hint. Use the triangle inequality with intermediate distribution δ0. Hence,
prove that the p-Wasserstein metric is indeed a metric on Pp(R), for p ∈
[1, ∞].
(ii) Show that on the space P(R), the p-Wasserstein metric satisﬁes all require-
ments of a metric except ﬁniteness. For each p ∈[1, ∞], exhibit a pair of
distributions ν, ν′ ∈P(R) such that wp(ν, ν′) = ∞.
(iii) Show that if 1 ≤q < p < ∞, we have E[|Z|p] < ∞=⇒E[|Z|q] < ∞for any
random variable Z. Deduce that Pp(R) ⊆Pq(R). Hint. Consider applying
Jensen’s inequality with the function z 7→|z|p/q.
△
Exercise 4.7. Let d : P(R) × P(R) →[0, ∞] be a probability metric with ﬁnite
domain Pd(R).
(i) Prove that the supremum distance d is an extended metric on P(R)X.
(ii) Prove that it is a metric on Pd(R)X.
△
Exercise 4.8. Prove Proposition 4.15 for p = ∞.
△
Exercise 4.9. Consider two normally distributed random variables with mean
µ and µ′ and common variance σ2. Derive an expression for the ∞-Wasserstein
distance between these two distributions. Conclude that Assumption 4.29(∞) is
suﬃcient, but not necessary, for two distributions to have ﬁnite ∞-Wasserstein
distance. How does the situation change if the two normal distributions in
question have unequal variances?
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Operators and Metrics
111
Exercise 4.10. Explain, in words, why Assumption 4.29(p) is needed for Propo-
sition 4.30. By considering the deﬁnition of the p-Wasserstein distance, explain
why for p > 1 and 1 ≤q < p, Assumption 4.29(q) is not suﬃcient to guarantee
convergence in the p-Wasserstein distance.
△
Exercise 4.11. This exercise guides you through the proof of Proposition 4.30.
(i) First, show that under Assumption 4.29(p), the return distributions ηπ(x)
have ﬁnite pth moments, for all x ∈X. You may ﬁnd it useful to deal with
p = ∞separately, and in the case p ∈[1, ∞), you may ﬁnd it useful to rewrite
Eπ
h
∞
X
t=0
γtRt
p | X = x
i
= (1 −γ)−pEπ
h
∞
X
t=0
(1 −γ)γtRt
p | X = x
i
and use Jensen’s inequality on the function z 7→|z|p.
(ii) Let η ∈Pp(R)X, and let G be an instantiation of η. First letting p ∈
[1, ∞), use the inequality |z1 + z2|p ≤2p−1(|z1|p + |z2|p) to argue that if (X =
x, A, R, X′) is a sample transition independent of G, then
Eπ[|R + γG(X′)|p | X = x] < ∞.
Hence, argue that under Assumption 4.29(p), Pp(R)X is closed under T π.
Argue separately that this holds for p = ∞too.
Hence, argue that Proposition 4.27 applies, and hence conclude that Proposi-
tion 4.30 holds.
△
Exercise 4.12. In the proof of Theorem 4.25, we did not need to assume that
for the two distinct states x, y ∈X, their associated returns G(x) and G(y) are
independent. Explain why.
△
Exercise 4.13. The (1,1)-Pareto distribution νpar has cumulative distribution
Fνpar(z) =
0
if z < 1,
1 −1
z
if z ≥1 .
Justify the necessity of including Assumption 2.5 (ﬁnite-mean rewards) in
Deﬁnition 4.26 by demonstrating that
ℓ2(νpar, δ0) < ∞
yet E
Z∼ν[Z] = ∞.
△
Exercise 4.14. The purpose of this exercise is to contrast the p-Wasserstein and
ℓp distances. For each of the following, ﬁnd a pair of probability distributions
ν, ν′ ∈P(R) such that, for a given ε > 0,
(i) w2(ν, ν′) smaller than ℓ2(ν, ν′);
(ii) w2(ν, ν′) larger than ℓ2(ν, ν′);
(iii) w∞(ν, ν′) = ε and ℓ∞(ν, ν′) = 1;
(iv) w∞(ν, ν′) = 1 and ℓ∞(ν, ν′) = ε.
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

112
Chapter 4
Exercise 4.15. Show that the dependence on γ1/p is tight in Proposition 4.20.
△
Exercise 4.16. The total variation distance dTV : P(R) × P(R) →R is deﬁned
by
dTV(ν, ν′) = sup
U⊆R
|ν(U) −ν′(U)| ,
(4.19)
for all ν, ν′ ∈P(R).34 Show, by means of a counterexample, that the distribu-
tional Bellman operator is not a contraction mapping in the supremum extension
of this distance.
△
Exercise 4.17. Consider the alternative notion of the return introduced in
Section 2.9, the random-horizon return, for which γ is treated as the prob-
ability of continuing. Write down the distributional Bellman operator that
corresponds to this random-horizon return. For which metrics considered in this
chapter is this distributional Bellman operator a contraction mapping? Show in
particular that this distributional Bellman operator is a contraction with respect
to the supremum version of the total variation distance over return-distribution
functions, introduced in Exercise 4.16. What is its contraction modulus?
△
Exercise 4.18. Remark 2.3 describes some diﬀerences between Markov deci-
sion processes with ﬁnite state spaces (as we consider throughout the book) and
generalizations with inﬁnite state spaces. The contraction mapping theory in
this chapter is one case where stronger assumptions are required when moving
to larger state spaces. Using the example described in Remark 2.3, show that
Assumption 4.29(w1) is insuﬃcient to make P1(R) closed under the distribu-
tional Bellman operator T π when the state space is countably inﬁnite. How
could this assumption be strengthened to guarantee closedness?
△
Exercise 4.19 (*). The goal of this exercise is to explore the contractivity of
the distributional Bellman operator with respect to a class of metrics known
as maximum mean discrepancies; this analysis was undertaken by Nguyen et
al. (2021). A kernel on R is a function K : R × R →R with the property that for
any ﬁnite set {z1, …, zn} ⊆R, the m × m matrix with (i, j)th entry K(zi, zj) is pos-
itive semi-deﬁnite. A function K : R × R →R satisfying the weaker condition
that
n
X
i,j=1
cicjK(zi, z j) ≥0
34. For the reader with a measure-theoretic background, the supremum here is over measurable
subsets of R.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Operators and Metrics
113
whenever Pn
i=1 ci = 0 is called a conditionally positive-deﬁnite kernel. Condi-
tionally positive-deﬁnite kernels form a measure of similarity between pairs of
points in R and can also be used to deﬁne notions of distance over probability
distributions. The maximum mean discrepancy (MMD) associated with the
conditionally positive-deﬁnite kernel K is deﬁned by
MMDK(ν, ν′) =

E X∼ν
X′∼ν[K(X, X′)] + E Y∼ν′
Y′∼ν′[K(Y, Y′)] −2E X∼ν
Y∼ν′[K(X, Y)]
1/2
,
where each pair of random variables in the expectations above is taken to be
independent.
(i) Consider the function Kα(z1, z2) = −|z1 −z2|α, with α ∈(0, 2). Székely and
Rizzo (2013, Proposition 2) show that this deﬁnes a conditionally positive-
deﬁnite kernel. Show that MMDKa is regular, c-homogeneous (for some c >
0), and p-convex (for some p ∈[1, ∞)). Hence, use Theorem 4.25 to establish
that the distributional Bellman operator is a contraction with respect to
MMDKα, under suitable assumptions.
(ii) The Gaussian kernel, or squared exponential kernel, with variance σ2 > 0
and length scale λ > 0 is deﬁned by K(z1, z2) = σ2 exp(−(z1 −z2)2/(2λ2)).
Show, through the use of a counterexample, that the MMD corresponding
to the Gaussian kernel is not c-homogeneous for any c > 0, and so Theo-
rem 4.25 cannot be applied. Further, ﬁnd an MDP and policy π that serve as
a counterexample to the contractivity of the distributional Bellman operator
with respect to this MMD metric.
△
Exercise 4.20. Let p ∈[1, ∞], and consider a modiﬁcation of the p-Wasserstein
distance, ˜wp, such that ˜wp(ν, ν′) = wp(ν, ν′) if both ν, ν′ ∈P(R) are expressible
as ﬁnite mixtures of Dirac deltas, and ˜wp(ν, ν′) = ∞otherwise.
(i) Show that P ˜wp(R) is the set of distributions expressible as ﬁnite mixtures
of Dirac deltas.
(ii) Exhibit a Markov decision process and policy π for which all conditions of
Proposition 4.27 hold except for the condition ηπ ∈P ˜wp(R)X .
(iii) Show that the sequence of iterates (ηk)k≥0 does not converge to ηπ under
˜wp in this case.
△
Exercise 4.21. Consider the sequence of distributions (νk)∞
k=1 deﬁned by
νk = k −1
k
δ0 + 1
k δk .
Show that this sequence converges weakly to another distribution ν. From this,
deduce that weak convergence does not imply convergence of expectations.
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

114
Chapter 4
Exercise 4.22. Achab (2020) considers the random variables
˜Gπ(x) = R + γVπ(X′) ,
X = x .
What does the distribution of this random variable capture about the underlying
MDP and policy π? Using the tools of this chapter, derive an operator over
P(R)X that has the collection of distributions corresponding to this random
variable under each of the initial conditions in X as a ﬁxed point, and analyze
the properties of this operator.
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

5
Distributional Dynamic Programming
Markov decision processes model the dynamics of an agent exerting control
over its environment. Once the agent’s policy is selected, a Markov decision
process gives rise to a sequential system whose behavior we would like to char-
acterize. In particular, policy evaluation describes the process of determining
the returns obtained from following a policy π. Algorithmically, this translates
into the problem of computing the value or return-distribution function given
the parameters of the Markov decision process and the agent’s policy.
Computing the return-distribution function requires being able to describe
the output of the algorithm in terms of atomic objects (depending on the pro-
gramming language, these may be bits, ﬂoating point numbers, vectors, or even
functions). This is challenging because in general, return distributions take on a
continuum of values (i.e., they are inﬁnite-dimensional objects). By contrast,
the expected return from a state x is described by a single real number. Deﬁning
an algorithm that computes return-distribution functions ﬁrst requires us to
decide how we represent probability distributions in memory, knowing that
some approximation error must be incurred if we want to keep things ﬁnite.
This chapter takes a look at diﬀerent representations of probability distribu-
tions as they relate to the problem of computing return-distribution functions.
We will see that, unlike the relatively straightforward problem of computing
value functions, there is no obviously best representation for return-distribution
functions and that diﬀerent ﬁnite-memory representations oﬀer diﬀerent advan-
tages. We will also see that making eﬀective use of diﬀerent representations
requires diﬀerent algorithms.
5.1
Computational Model
As before, we assume that the environment is described as a ﬁnite-state, ﬁnite-
action Markov decision process. We write NX and NA for the size of the state
and action spaces X and A. When describing algorithms in this chapter, we will
115
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

116
Chapter 5
further assume that the reward distributions PR( · | x, a) are supported on a ﬁnite
set R of size NR; we discuss a way of lifting this assumption in Remark 5.1.
Of note, having ﬁnitely many rewards guarantees the existence of an interval
[Vmin, Vmax] within which the returns lie.35 We measure the complexity of a
particular algorithm in terms of the number of atomic instructions or memory
words it requires, assuming that these can reasonably be implemented in a
physical computer, as described by the random-access machine (RAM) model
of computation (Cormen et al. 2001).
In classical reinforcement learning, linear algebra provides a simple algorithm
for computing the value function of a policy π. In vector notation, the Bellman
equation is
Vπ = rπ + γPπVπ,
(5.1)
where the transition function Pπ is represented as an NX-dimensional square
stochastic matrix, and rπ is an NX-dimensional vector. With some matrix algebra,
we deduce that
Vπ = rπ + γPπVπ
⇐⇒(I −γPπ)Vπ = rπ
⇐⇒Vπ = (I −γPπ)−1rπ .
(5.2)
The computational cost of determining Vπ is dominated by the matrix inversion,
requiring O(N3
X) operations. The result is exact. The matrix Pπ and the vector
rπ are constructed entry-wise by writing expectations as sums:
Pπ(x′ | x) =
X
a∈A
π(a | x)PX(x′ | x, a)
rπ(x) =
X
a∈A
X
r∈R
π(a | x)PR(r | x, a) × r .
When the matrix inversion is undesirable, the value function can instead be
found by dynamic programming. Dynamic programming describes a wide vari-
ety of computational methods that ﬁnd the solution to a given problem by
caching intermediate results. In reinforcement learning, the dynamic program-
ming approach for ﬁnding the value function Vπ, also called iterative policy
evaluation, begins with an initial estimate V0 ∈RX and successively computes
Vk+1 = T πVk
for k = 1, 2, . . . , until some desired number of iterations K have been performed
or some stopping criterion is reached. This is possible because, when X, A, and
35. We can always take Rmin and Rmax to be the smallest and largest possible rewards, respectively.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Distributional Dynamic Programming
117
R are all ﬁnite, the Bellman operator can be written in terms of sums:
(T πVk)(x) =
X
a∈A
X
r∈R
X
x′∈X
Pπ(A = a, R = r, X′ = x′ | X = x)
|                                  {z                                  }
π(a | x)PX(x′ | x,a)PR(r | x,a)
r + γVk(x′) .
(5.3)
A naive implementation expresses these sums as nested for loops. Since the new
value function must be computed at all states, this naive implementation requires
on the order of N = N2
XNANR operations. We can do better by implementing it
in terms of vector operations:
Vk+1 = rπ + γPπVk ,
(5.4)
where Vk is stored in memory as an NX-dimensional vector. A single appli-
cation of the Bellman operator with vectors and matrices requires O(N2
XNA +
NXNANR) operations for computing rπ and Pπ, and O(N2
X) operations for the
matrix-vector multiplication. As rπ and Pπ do not need to be recomputed
between iterations, the dominant cost of this process comes from the successive
matrix multiplications, requiring O(KN2
X) operations.36
In general, the iterates (Vk)k≥0 will not reach Vπ after any ﬁnite number of
iterations. However, the contractive nature of the Bellman operator allows us to
bound the distance from any iterate to the ﬁxed point Vπ.
Proposition 5.1. Let V0 ∈RX be an initial value function and consider the
iterates
Vk+1 = T πVk .
(5.5)
For any ε > 0, if we take
Kε ≥
log   1
ε
 + log(∥V0 −Vπ∥∞)
log   1
γ

,
then for all k ≥Kε, we have that
∥Vk −Vπ∥∞≤ε .
For V0 = 0, the dependency on Vπ can be simpliﬁed by noting that
log(∥V0 −Vπ∥∞) = log(∥Vπ∥∞) ≤log   max(|Vmin|, |Vmax|) .
△
Proof. Since T π is a contraction mapping with respect to the L∞metric with con-
traction modulus γ (Proposition 4.4), and Vπ is its ﬁxed point (Proposition 2.12),
36. Assuming that the number of states NX is large compared to the number of actions NA and
rewards NR. For transition functions with special structure (sparsity, low rank, etc.), one can hope
to do even better.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

118
Chapter 5
we have for any k ≥1
∥Vk −Vπ∥∞= ∥T πVk−1 −T πVπ∥∞≤γ∥Vk−1 −Vπ∥∞,
and so by induction we have
∥Vk −Vπ∥∞≤γk∥V0 −Vπ∥∞.
Setting the right-hand side to be less than or equal to ε and rearranging gives
the required inequality for Kε.
From Proposition 5.1, we conclude that we can obtain an ε-approximation
to Vπ in O(KεN2
X) operations, by applying the Bellman operator Kε times to
an initial value function V0 = 0. Since the iterate Vk can be represented as an
NX-dimensional vector and is the only object that the algorithm needs to store in
memory (other than the description of the MDP itself), this shows that iterative
policy evaluation can approximate Vπ eﬃciently.
5.2
Representing Return-Distribution Functions
Now, let us consider what happens in distributional reinforcement learning.
As with any computational problem, we ﬁrst must decide on a data structure
that our algorithms operate on. The heart of our data structure is a scheme for
representing return-distribution functions in memory. We call such a scheme a
probability distribution representation.
Deﬁnition 5.2. A probability distribution representation F, or simply repre-
sentation, is a collection of probability distributions indexed by a parameter θ
from some set of allowed parameters Θ:
F = Pθ ∈P(R) : θ ∈Θ	 .
△
Example 5.3. The Bernoulli representation is the set of all Bernoulli distribu-
tions:
FB = (1 −p)δ0 + pδ1 : p ∈[0, 1]	 .
△
Example 5.4. The uniform representation is the set of all uniform distributions
on ﬁnite-length intervals:
FU = U([a, b]) : a, b ∈R, a < b	 .
△
We represent return functions using a table of probability distributions, each
associated with a given state and described in our chosen representation. For
example, a uniform return function is described in memory by a table of 2NX
numbers, corresponding to the upper and lower ends of the distribution at each
state. By extension, we call such a table a representation of return-distribution
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Distributional Dynamic Programming
119
functions. Formally, for a representation F, the space of representable return
functions is F X.
With this data structure in mind, let us consider the procedure (introduced by
Equation 4.10) that approximates the return function ηπ by repeatedly applying
the distributional Bellman operator:
ηk+1 = T πηk .
(5.6)
Because an operator is an abstract object, Equation 5.6 describes a mathematical
procedure, rather than a computer program. To obtain the latter, we begin by
expressing the distributional Bellman operator as a sum, analogous to Equation
5.3. Recall that the distributional operator is deﬁned by an expectation over the
random variables R and X′:
(T πη)(x) = Eπ[(bR,γ)#η(X′) | X = x] .
(5.7)
Here, the expectation describes a mixture of pushforward distributions. By
writing this expectation in full, we ﬁnd that this mixture is given by
 T πη(x) =
X
a∈A
X
r∈R
X
x′∈X
Pπ
 A = a, R = r, X′ = x′ | X = x(br,γ)#η(x′) .
(5.8)
The pushforward operation scales and then shifts (by γ and r, respectively)
the support of the probability distribution η(x′), as depicted in Figure 2.5.
Implementing the distributional Bellman operator therefore requires being able
to eﬃciently perform the shift-and-scale operation and compute mixtures of
probability distributions; we caught a glimpse of what that might entail when
we derived categorical temporal-diﬀerence learning in Chapter 3.
Cumulative distribution functions allow us to rewrite Equations 5.7–5.8 in
terms of vector-like objects, providing a nice parallel with the usual vector
notation for the expected-value setting. Let us write Fη(x, z) = Fη(x)(z) to denote
the cumulative distribution function of η(x). We can equally express Equation
5.7 as37
(T πFη)(x, z) = Eπ
h
Fη

X′, z−R
γ

| X = x
i
.
(5.9)
As a weighted sum of cumulative distribution functions, Equation 5.9 is
(T πFη)(x, z) =
X
a∈A
X
r∈R
X
x′∈X
Pπ
 A = a, R = r, X′ = x′ | X = xFη
 x′, z−r
γ
.
(5.10)
Similar to Equation 5.1, we can set Fη = Fηπ on both sides of Equation 5.10 to
obtain the linear system:
Fηπ(x, z) =
X
a∈A
X
r∈R
X
x′∈X
Pπ
 A = a, R = r, X′ = x′ | X = xFηπ x′, z−r
γ
 .
37. With Chapter 4 in mind, note that we are overloading operator notation when we apply T π to
collections of cumulative distribution functions, rather than return-distribution functions.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

120
Chapter 5
However, this particular set of equations is inﬁnite-dimensional. This is because
cumulative distribution functions are themselves inﬁnite-dimensional objects,
more speciﬁcally elements of the space of monotonically increasing functions
mapping R to [0, 1]. Because of this, we cannot describe them in physical
memory, at least not on a modern-day computer. This gives a concrete argument
as to why we cannot simply “store” a probability distribution but must instead
use a probability distribution representation as our data structure. For the same
reason, a direct algebraic solution to Equation 5.10 is not possible, in contrast to
the expected-value setting (Equation 5.2). This justiﬁes the need for a dynamic
programming method to approximate ηπ.
Creating an algorithm for computing the return-distribution function requires
us to implement the distributional Bellman operator in terms of our chosen
representation. Conversely, we should choose a representation that supports an
eﬃcient implementation. Unlike the value function setting, however, there is
no single best representation – making this choice requires balancing available
memory, accuracy, and the downstream uses of the return-distribution function.
The rest of this chapter is dedicated to studying these trade-oﬀs and developing
a theory of what makes for a good representation. Like Goldilocks faced with
her choices, we ﬁrst consider the situation where memory and computation are
plentiful, then the use of normal distributions to construct a minimally viable
return-distribution function, before ﬁnally introducing ﬁxed-size empirical
representations as a sensible and practical middle ground.
5.3
The Empirical Representation
Simple representations like the Bernoulli representation are ill-suited to describe,
say, the diﬀerent outcomes in blackjack (Example 2.7) or the variations in
the return distributions from diﬀerent policies (Example 2.9). Although there
are scenarios in which a representation with few parameters gives a reason-
able approximation, the most general-purpose algorithms for computing return
distributions should be based on representations that are suﬃciently expressive.
To understand what “suﬃciently expressive” might mean, let us consider
what it means to implement the iterative procedure
ηk+1 = T πηk .
(5.11)
The most direct implementation is a for loop over the iteration number k =
0, 1, . . . , interleaving
(a) determining (T πηk)(x) for each x ∈X, and
(b) expressing the outcome as a return-distribution function ηk+1 ∈F.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Distributional Dynamic Programming
121
The ﬁrst step of this procedure is an algorithm that emulates the operator T π,
which we may call the operator-algorithm. When used as part of the for loop,
the output of this operator-algorithm at iteration k becomes the input at iteration
k + 1. As such, it is desirable for the inputs and outputs of the operator-algorithm
to have the same type: given as input a return function represented by F, the
operator-algorithm should produce a new return function that is also represented
by F. A prerequisite is that the representation F be closed under the operator
T π, in the sense that
η ∈F X =⇒T πη ∈F X .
(5.12)
The empirical representation satisﬁes this desideratum.
Deﬁnition 5.5. The empirical representation is the set FE of empirical
distributions
FE =
(
m
X
i=1
piδθi : m ∈N+ , θi ∈R, pi ≥0,
m
X
i=1
pi = 1
)
.
△
An empirical distribution ν ∈FE can be stored in memory as a ﬁnite list of
pairs  θi, pi
m
i=1. We call individual elements of such a distribution particles,
each consisting of a probability and a location. Notationally, we extend the
empirical representation to return distributions by writing
η(x) =
m(x)
X
i=1
pi(x)δθi(x)
(5.13)
for the return distribution corresponding to state x.
The application of the distributional Bellman operator to empirical proba-
bility distributions has a particular convenient form that we formalize with the
following lemma and proposition.
Lemma 5.6. Let ν ∈FE be an empirical distribution with parameters m and
(θi, pi)m
i=1. For r ∈R and γ ∈R, we have
(br,γ)#ν =
m
X
i=1
piδr+γθi .
△
In words, the application of the bootstrap function to an empirical distribution
ν shifts and scales the locations of that distribution (see Exercise 5.7). This
property was implicit in our description of the pushforward operation in Chapter
2, and we made use of it (also implicitly) to derive the categorical temporal-
diﬀerence learning algorithm in Chapter 3.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

122
Chapter 5
Proposition 5.7. Provided that the set of possible rewards R is ﬁnite, the
empirical representation FE is closed under T π. In particular, if η ∈F X
E is
a return-distribution with parameters
 pi(x), θi(x)m(x)
i=1 : x ∈X

then
 T πη(x) =
X
a∈A
X
r∈R
X
x′∈X
Pπ(A = a, R = r, X′ = x′ | X = x)
m(x′)
X
i=1
pi(x′)δr+γθi(x′).
(5.14)
△
Proof. Pick a state x ∈X. For a triple (a, r, x′) ∈A × R × X, write Pa,r,x′ =
Pπ
 A = a, R = r, X′ = x′ | X = x. Then,
(T πη)(x)
(a)=
X
a∈A
X
r∈R
X
x′∈X
Pa,r,x′(br,γ)#η(x′)
=
X
a∈A
X
r∈R
X
x′∈X
Pa,r,x′(br,γ)#
 m(x′)
X
i=1
pi(x′)δθi(x′)

(b)=
X
a∈A
X
r∈R
X
x′∈X
Pa,r,x′
m(x′)
X
i=1
pi(x′)δr+γθi(x′)
≡
m′
X
j=1
p′
jδθ′
j
for some collection (θ′
j, p′
j)m′
j=1. Line (a) is Equation 5.8 and (b) follows from
Lemma 5.6. We conclude that (T πη)(x) ∈FE, and hence FE is closed under
T π.
Algorithm 5.1 uses Proposition 5.7 to compute the application of the distri-
butional Bellman operator to any η represented by FE. It implements Equation
5.14 almost verbatim, with two simpliﬁcations. First, it uses the fact that the
particle locations for a distribution (T πη)(x) only depend on r and x′ but not
on a. This allows us to produce a single particle for each reward-next-state
pair. Second, it also encodes the fact that the return is 0 from the terminal state,
making dynamic programming more eﬀective from this state (Exercise 5.3 asks
you to justify this claim). Since the output of Algorithm 5.1 is also a return
function from FE, we can use it to produce the iterates η1, η2, . . . from an initial
return function η0 ∈F X
E .
Because FE is closed under T π, we can analyze the behavior of this
procedure using the theory of contraction mappings (Chapter 4). Here we
bound the number of iterations needed to obtain an ε-approximation to the
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Distributional Dynamic Programming
123
Algorithm 5.1: Empirical representation distributional Bellman
operator
Algorithm parameters:
η, expressed as θ =   θi(x), pi(x)m(x)
i=1 : x ∈X
foreach x ∈X do
θ′(x) ←Empty List
foreach x′ ∈X do
foreach r ∈R do
αr,x′ ←P
a∈A π(a | x)PR(r | x, a)PX(x′ | x, a)
if x′ is terminal then
Append  r, αr,x′ to θ′(x)
else
for i = 1, …, m(x′) do
Append  r + γθi(x′), αr,x′ pi(x′) to θ′(x)
end for
end foreach
end foreach
end foreach
return θ′
return-distribution function ηπ, as measured by a supremum p-Wasserstein
metric.
Proposition 5.8. Consider an initial return function η0(x) = δ0 for all x ∈X
and the dynamic programming approach that iteratively computes
ηk+1 = T πηk
by means of Algorithm 5.1. Let ε > 0 and let
Kε ≥
log   1
ε
 + log   max(|Vmin|, |Vmax|)
log   1
γ

.
Then, for all k ≥Kε, we have that
wp(ηk, ηπ) ≤ε
∀p ∈[1, ∞] ,
where wp is the supremum p-Wasserstein distance.
△
Proof. Similar to the proof of Proposition 5.1, since T π is a contraction map-
ping with respect to the wp metric with contraction modulus γ (Proposition 4.15),
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

124
Chapter 5
and ηπ is its ﬁxed point (Propositions 2.17 and 4.9), we can deduce
wp(ηk, ηπ) ≤γkwp(η0, ηπ) .
Since η0 = δ0 and ηπ is supported on [Vmin, Vmax], we can upper-bound wp(η0, ηπ)
by max(|Vmin|, |Vmax|). Setting the right-hand side to be less than or equal to ε
and rearranging gives the required inequality for Kε.
Although we state Proposition 5.8 in terms of p-Wasserstein distance for
concreteness, we can also obtain similar results more generally for probability
metrics under which the distributional Bellman operator is contractive.
The analysis of this section shows that the empirical representation is suﬃ-
ciently expressive to support an iterative procedure for approximating the return
function ηπ to an arbitrary precision, due to being closed under the distributional
Bellman operator. This result is perhaps somewhat surprising: even if ηπ < FE,
we are able to obtain an arbitrarily accurate approximation within FE.
Example 5.9. Consider the single-state Markov decision process of Example
2.10, with Bernoulli reward distribution U({0, 1}) and discount factor γ = 1/2.
Beginning with η0(x), the return distributions of the iterates
ηk+1 = T πηk
are a collection of uniformly weighted, evenly spaced Dirac deltas:
ηk(x) = 1
2k
2k−1
X
i=0
δθi,
θi =
i
2k−1 .
(5.15)
As suggested by Figure 2.3, the sequence of distributions  ηk(x)
k≥0 converges
to the uniform distribution U([0, 2]) in the p-Wasserstein distances, for all
p ∈[1, ∞]. However, this limit is not itself an empirical distribution.
△
The downside is that the algorithm is typically intractable for anything but a
small number of iterations K. This is because the lists that describe ηk may grow
exponentially in length with K, as shown in the example above. Even when η0
is initialized to be δ0 at all states (as in Proposition 5.8), representing the kth
iterate requires O Nk
XNk
R
 particles per state, corresponding to all achievable
discounted returns of length k.38 This is somehow unavoidable: in a certain
sense, the problem of computing return functions is NP-hard (see Remark 5.2).
This motivates the need for a more complex procedure that forgoes closedness
in favor of tractability.
38. A smarter implementation only requires O Nk
R
 particles per state. See Exercise 5.8.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Distributional Dynamic Programming
125
5.4
The Normal Representation
To avoid the computational costs associated with unbounded memory usage, we
may restrict ourselves to probability distributions described by a ﬁxed number
of parameters. A simple choice is to model the return with a normal distribution,
which requires only two parameters per state: a mean µ and variance σ2.
Deﬁnition 5.10. The normal representation is the set of normal distributions
FN = N(µ, σ2) : µ ∈R, σ2 ≥0	 .
△
With this representation, a return function is described by a total of 2NX
parameters.
More often than not, however, the random returns are not normally distributed.
This may be because the rewards themselves are not normally distributed, or
because the transition kernel is stochastic. Figure 5.1 illustrates the eﬀect of
the distributional Bellman operator on a normal return-distribution function η:
mixing the return distributions at successor states results in a mixture of normal
distributions, which is not normally distributed except in trivial situations. In
other words, the normal representation FN is generally not closed under the
distributional Bellman operator.
Rather than represent the return distribution with high accuracy, as with the
empirical distribution, let us consider the more modest goal of determining the
best normal approximation to the return-distribution function ηπ. We deﬁne
“best normal approximation” as
ˆηπ(x) = N

Vπ(x), Var Gπ(x)
.
(5.16)
Given that a normal distribution is parameterized by its mean and variance, this
is an obviously sensible choice. In some cases, this choice can also be justiﬁed
by arguing that ˆηπ(x) is the normal distribution closest to ηπ(x) in terms of what
is called the Kullback–Leibler divergence.39 As we now show, this choice also
leads to a particularly eﬃcient algorithm for computing ˆηπ.
We will construct an iterative procedure that operates on return functions
from a normal representation and converges to ˆηπ. We start with the random
variable operator
 T πG(x)
D= R + γG(X′),
X = x ,
(5.17)
39. Technically, this is only true when the return distribution ηπ(x) has a probability density function.
When ηπ(x) does not have a density, a similar argument can be made in terms with the cross-entropy
loss; see Exercises 5.5 and 5.6.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

126
Chapter 5
Return
Return
Return
Figure 5.1
Applying the distributional Bellman oper-
ator to a return function η described by the
normal representation generally produces
return distributions that are not normally
distributed. Here, a uniform mixture of two
normal distributions (shown as probability
densities) results in a bimodal distribution
(in light gray). The best normal approxi-
mation ˆη to that distribution is depicted by
the solid curve.
and take expectations on both sides of the equation:40
E (T πG)(x) = Eπ
h
R + γE[G(X′) | X′] | X = x
i
= Eπ[R | X = x] + γ Eπ[E[G(X′) | X′] | X = x]
= Eπ[R | X = x] + γ
X
x′∈X
Pπ(X′ = x′ | X = x) E[G(x′)] ,
(5.18)
where the last step follows from the assumption that the random variables G are
independent of the next-state X′. When applied to the return-variable function
Gπ, Equation 5.18 is none other than the classical Bellman equation.
The same technique allows us to relate the variance of (T πG)(x) to the
next-state variances. For a random variable Z, recall that
Var(Z) = E (Z −E[Z])2 .
The random reward R and next-state X′ are by deﬁnition conditionally indepen-
dent given X and A. However, to simplify the exposition, we assume that they
are also conditionally independent given only X (in Chapter 8, we will see how
this assumption can be avoided).
Let us use the notation Varπ to make explicit the dependency of certain
random variables on the sample transition model, analogous to our use of Eπ.
Denote the value function corresponding to G by V(x) = E[G(x)]. We have
Var (T πG)(x) = Varπ
 R + γG(X′) | X = x
= Varπ
 R | X = x + Varπ
 γG(X′) | X = x
= Varπ
 R | X = x + γ2Varπ
 G(X′) | X = x
= Varπ
 R | X = x + γ2Varπ
 V(X′) | X = x
40. In Equation 5.18, some of the expectations are taken solely with respect to the sample transition
model, which we denote by Eπ as usual. The expectation with respect to the random variable G, on
the other hand, is not part of the model; we use the unsubscripted E to emphasize this point.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Distributional Dynamic Programming
127
+ γ2 X
x′∈X
Pπ(X′ = x′ | X = x)Var G(x′) ,
(5.19)
where the last line follows by the law of total variance:
Var(A) = Var E[A | B] + E Var[A | B].
Equation 5.19 shows how to compute the variances of the return function T πG.
Speciﬁcally, these variances depend on the reward variance, the variance in
next-state values, and the expected next-state variances. When G =Gπ, this is
the Bellman equation for the variance of the random return. Writing σ2
π(x) =
Var Gπ(x), we obtain
σ2
π(x) = Varπ(R | X = x) + γ2Varπ
 Vπ(X′) | X = x
+ γ2 X
x′∈X
Pπ(X′ = x′ | X = x)σ2
π(x) .
(5.20)
We now combine the results above to obtain a dynamic programming proce-
dure for ﬁnding ˆηπ. For each state x ∈X, let us denote by µ0(x) and σ2
0(x) the
parameters of a return-distribution distribution η0 with η0(x) = N µ0(x), σ2
0(x).
For all x, we simultaneously compute
µk+1(x) = Eπ[R + γµk(X′) | X = x]
(5.21)
σ2
k+1(x) = Varπ
 R | X = x + γ2Varπ
 µk(X′) | X = x + γ2 Eπ[σ2
k(X′) | X = x] .
(5.22)
We can view these two updates as the implementation of a bona ﬁde operator
over the space of normal return-distribution functions. Indeed, we can associate
to each iteration the return function
ηk(x) = N µk(x), σ2
k(x) ∈FN .
Analyzing the behavior of the sequence (ηk)k≥0 require some care, but we
shall see in Chapter 8 that the iterates converge to the best approximation ˆηπ
(Equation 5.16), in the sense that for all x ∈X,
µk(x) →Vπ(x)
σ2
k(x) →Var Gπ(x) .
This derivation shows that the normal representation can be used to create a
tractable algorithm for approximating the return distribution. However, in our
work, we have found that the normal representation rarely gives a satisfying
depiction of the agent’s interactions with its environment; it is not suﬃciently
expressive. In many problems, outcomes are discrete in nature: success or
failure, food or hunger, forward motion or fall. This arises in video games in
which the game ends once the player’s last life is spent. Timing is also critical:
to catch the diamond, key, or mushroom, the agent must press “jump” at just the
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

128
Chapter 5
right moment, again leading to discrete outcomes. Even in relatively continuous
systems, such as the application of reinforcement learning to stratospheric
balloon ﬂight, the return distributions tend to be skewed or multimodal. In short,
normal distributions are a poor ﬁt for the wide gamut of problems found in
reinforcement learning.
5.5
Fixed-Size Empirical Representations
The empirical representation is expressive because it can use more particles to
describe more complex probability distributions. This “blank check” approach
to memory and computation, however, results in an intractable algorithm. On
the other hand, the simple normal distribution is rarely suﬃcient to give a
good approximation of the return distribution. A good middle ground is to
preserve the form of the empirical representation while imposing a limit on its
expressivity. Our approach is to ﬁx the number and type of particles used to
represent probability distributions.
Deﬁnition 5.11. The m-quantile representation parameterizes the location of
m equally weighted particles. That is,
FQ,m =
( 1
m
m
X
i=1
δθi : θi ∈R
)
.
△
Deﬁnition 5.12. Given a collection of m evenly spaced locations θ1 < · · · < θm,
the m-categorical representation parameterizes the probability of m particles at
these ﬁxed locations:
FC,m =
(
m
X
i=1
piδθi : pi ≥0,
m
X
i=1
pi = 1
)
.
We denote the stride between successive particles by ςm = θm−θ1
m−1 .
△
This deﬁnition corresponds to the categorical representation used in Chapter
3. Note that because of the constraint that probabilities should sum to 1, a m-
categorical distribution is described by m −1 parameters. In addition, although
the representation depends on the choice of locations θ1, …, θm, we omit this
dependence in the notation FC,m to keep things concise.
In our deﬁnition of the m-categorical representation, we assume that the
locations (θi)m
i=1 are given a priori and not part of the description of a particular
probability distribution. This is sensible when we consider that algorithms such
as categorical temporal-diﬀerence learning use the same set of locations to
describe distributions at diﬀerent states and keep these locations ﬁxed across
the learning process. For example, a common choice is θ1 = Vmin and θm = Vmax.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Distributional Dynamic Programming
129
2
0
2
4
6
8
Return
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Probability
m-Categorical Representation
2
0
2
4
6
8
Return
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Probability
m-Quantile Representation
2
0
2
4
6
8
Return
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Probability
m-Particle Empirical Representation
Figure 5.2
A distribution ν (in light gray), as approximated with a m-categorical, m-quantile, or
m-particle representation, for m = 5.
When it is desirable to adjust both the locations and probabilities of diﬀerent
particles, we instead make use of the m-particle representation.
Deﬁnition 5.13. The m-particle representation parameterizes both the proba-
bility and location of m particles:
FE,m =
(
m
X
i=1
piδθi : θi ∈R, pi ≥0,
m
X
i=1
pi = 1
)
.
The m-particle representation contains both the m-quantile representation and
the m-categorical representation; a distribution from FE,m is deﬁned by 2m −1
parameters.
△
Mathematically, all three representations described above are subsets of
the empirical representation FE; accordingly, we call them ﬁxed-size empiri-
cal representations. Fixed-size empirical representations are ﬂexible and can
approximate both continuous and discrete outcome distributions (Figure 5.2).
The categorical representation is so called because it models the probability
of a set of ﬁxed outcomes. This is somewhat of a misnomer: the “categories”
are not arbitrary but instead correspond to speciﬁc real values. The quantile
representation is named for its relationship to the quantiles of the return distri-
bution. Although it might seem like the m-particle representation is a strictly
superior choice, we will see that committing to either ﬁxed locations or ﬁxed
probabilities simpliﬁes algorithmic design. For an equal number of parameters,
it is also not clear whether one should prefer m fully parameterized particles or,
say, 2m −1 uniformly weighted particles.
Like the normal representation, ﬁxed-size empirical representations are not
closed under the distributional Bellman operator T π. As discussed in Section
5.3, the consequence is that we cannot implement the iterative procedure
ηk+1 = T πηk
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

130
Chapter 5
with such a representation. To get around this issue, let us now introduce
the notion of a projection operator: a mapping from the space of probability
distributions (or a subset thereof) to a desired representation F.41 We denote
such an operator by
ΠF : P(R) →F .
Deﬁnitionally, we require that projection operators satisfy, for any ν ∈F,
ΠFν = ν .
We extend the notation ΠF to the space of return-distribution functions:
 ΠFη(x) = ΠF
 η(x) .
The categorical projection Πc, ﬁrst encountered in Chapter 3, is one such
operator; we will study it in greater detail in the remainder of this chapter.
We also made implicit use of a projection step in deriving an algorithm for
the normal representation: at each iteration, we kept track of the mean and
variance of the process but discarded the rest of the distribution, so that the
return function iterates could be described with the normal representation.
Algorithmically, we introduce a projection step following the application of
T π, leading to a projected distributional Bellman operator ΠFT π. By deﬁni-
tion, this operator maps F to itself, allowing for the design of distributional
algorithms that represent each iterate of the sequence
ηk+1 = ΠFT πηk
using a bounded amount of memory. We will discuss such algorithmic con-
siderations in Section 5.7, after describing particular projection operators for
the categorical and quantile representations. Combined with numerical integra-
tion, the use of a projection step also makes it possible to perform dynamic
programming with continuous reward distributions (see Exercise 5.9).
5.6
The Projection Step
We now describe projection operators for the categorical and quantile represen-
tations, correspondingly called categorical projection and quantile projection.
In both cases, these operators can be seen as ﬁnding the best approximation to a
given probability distribution, as measured according to a speciﬁc probability
metric.
To begin, recall that for a probability metric d, Pd(R) ⊆P(R) is the set of
probability distributions with ﬁnite mean and ﬁnite distance from the reference
41. In Section 4.1, we deﬁned an operator as mapping elements from a space to itself. The term
“projection operator” here is reasonable given that F ⊆P(R).
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Distributional Dynamic Programming
131
1
2
3
4
5
Return
0.0
0.2
0.4
0.6
0.8
1.0
Probability of assignment
1
2
3
4
5
Return
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Probability
Figure 5.3
Left: The categorical projection assigns probability mass to each location according to
a triangular kernel (central locations) and half-triangular kernels (boundary locations).
Here, m = 5. Right: The m = 5 categorical projection of a given distribution, shown in
gray.
distribution δ0 (Equation 4.26). For a representation F ⊆Pd(R), a d-projection
of ν ∈Pd(R) onto F is a function ΠF,d : Pd(R) →F that ﬁnds a distribution
ˆν ∈F that is d-closest to ν:
ΠF,dν ∈arg min
ˆν∈F
d(ν, ˆν) .
(5.23)
Although both the categorical and quantile projections that we present here
satisfy this deﬁnition, it is worth noting that in the most general setting, neither
the existence nor uniqueness of a d-projection ΠF,d is actually guaranteed (see
Remark 5.3). We lift the notion of a d-projection to return-distribution functions
in our usual manner; the d-projection of η ∈Pd(R)X onto F X is
 ΠF X,dη(x) = ΠF,d
 η(x) .
When unambiguous, we overload notation and write ΠF,dη to denote the
projection onto F X.
It is natural to think of the d-projection of the return-distribution function
η onto F X as the best achievable approximation within this representation,
measured in terms of d. We thus call ΠF,dν and ΠF X,dη the (d, F)-optimal
approximations to ν ∈P(R) and η, respectively.
Categorical projection. In Chapter 3, we deﬁned the categorical projection
Πc as assigning the probability mass q of a particle located at z to the two loca-
tions nearest to z in the ﬁxed support {θ1, . . . , θm}. Speciﬁcally, the categorical
projection assigns this mass q in (inverse) proportion to the distance to these
two neighbors. We now extend this idea to the case where we wish to more
generally project a distribution ν ∈P1(R) onto the m-categorical representation.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

132
Chapter 5
Given a probability distribution ν ∈P1(R), its categorical projection
Πcν =
m
X
i=1
piδθi
has parameters
pi = E
Z∼ν
h
hi

ς−1
m
 Z −θi
i
,
i = 1, . . . , m ,
(5.24)
for a set of functions hi : R →[0, 1] that we will deﬁne below. Here we write pi
in terms of an expectation rather than a sum, with the idea that this expectation
can be eﬃciently computed (this is the case when ν is itself a m-categorical
distribution).
When i = 2, . . . , m −1, the function hi is the triangular kernel
hi(z) = h(z) = max(0, 1 −|z|) .
We use this notation to describe the proportional assignment of probability mass
for the inner locations θ2, . . . , θm−1. One can verify that the triangular kernel
assigns probability mass from ν to the location θi in proportion to the distance
to its neighbors (Figure 5.3).
The parameters of the extreme locations are computed somewhat diﬀerently,
as these also capture the probability mass associated with values greater than
θm and smaller than θ1. For these, we use the half-triangular kernels
h1(z) =
( 1
z ≤0
max(0, 1 −|z|)
z > 0
hm(z) =
( max(0, 1 −|z|)
z ≤0
1
z > 0 .
Exercise 5.10 asks you to prove that the projection described here matches to
deterministic projection of Section 3.5.
Our derivation gives a mathematical formalization of the idea of assign-
ing proportional probability mass to the locations nearest to a given particle,
described in Chapter 3. In fact, it also describes the projection of ν in the Cramér
distance (ℓ2) onto the m-categorical representation. This is stated formally as
follows and proven in Remark 5.4.
Proposition 5.14. Let ν ∈P1(R). The m-categorical probability distri-
bution whose parameters are given by Equation 5.24 is the (unique)
ℓ2-projection onto FC,m.
△
Quantile projection. We call the quantile projection of a probability dis-
tribution of ν ∈P(R) a speciﬁc projection of ν in the 1-Wasserstein distance
(w1) onto the m-quantile representation (ΠFQ,m,w1). With this choice of distance,
this projection can be expressed in closed form and is easily implemented. In
addition, we will see in Section 5.9 that it leads to a well-behaved dynamic
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Distributional Dynamic Programming
133
programming algorithm. As with the categorical projection, we introduce the
shorthand Πq for the projection operator ΠFQ,m,w1.
Consider a probability distribution ν ∈P1(R). We are interested in a proba-
bility distribution Πqν ∈FQ,m that minimizes the 1-Wasserstein distance from
ν:
minimize w1(ν, ν′) subject to ν′ ∈FQ,m .
By deﬁnition, such a solution must take the form
Πqν = 1
m
m
X
i=1
δθi .
The following establishes that choosing (θi)m
i=1 to be a particular set of quantiles
of ν yields a valid w1-projection of ν.
Proposition 5.15. Let ν ∈P1(R). The m-quantile probability distribution
whose parameters are given by
θi = F−1
ν
 2i −1
2m
!
i = 1, . . . , m
(5.25)
is a w1-projection of ν onto FQ,m.
△
Equation 5.25 arises because the ith particle of a m-quantile distribution is
“responsible” for the portion of the 1-Wasserstein distance measured on the
interval [ i−1
m , i
m] (Figure 5.4). As formalized by the following lemma, the choice
of the midpoint quantile 2i−1
2m minimizes the 1-Wasserstein distance to ν on this
interval.
Lemma 5.16. Let ν ∈P1(R) with cumulative distribution function Fν. Let
0 ≤a < b ≤1. Then a solution to
min
θ∈R
Z b
a
F−1
ν (τ) −θ
dτ
(5.26)
is given by the quantile midpoint
θ = F−1
ν
 a + b
2
!
.
△
The proof is given as Remark 5.5.
Proof of Proposition 5.15. Let ν′ = 1
m
mP
i=1
δθi
be a m-quantile distribution.
Assume that its locations are sorted: that is, θ1 ≤θ2 ≤· · · ≤θm. For τ ∈(0, 1),
its inverse cumulative distribution function is
F−1
ν′ (τ) = θ⌈τm⌉.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

134
Chapter 5
2
0
2
4
6
8
Return
0.0
0.2
0.4
0.6
0.8
1.0
Cumulative Probability
2
0
2
4
6
8
Return
0.0
0.1
0.2
0.3
0.4
Probability
Figure 5.4
Left: The quantile projection ﬁnds the quantiles of the distribution ν (the dashed line
depicts its cumulative distribution function) for τi = 2i−1
2m , i = 1, . . . m. The shaded area
corresponds to the 1-Wasserstein distance between ν and its quantile projection Πqν
(solid line, m = 5). Right: The optimal (w1, FQ,m)-approximation to the distribution ν,
shown in gray.
This function is constant on the intervals (0, 1
m), [ 1
m, 2
m), . . . , [ m−1
m , 1). The 1-
Wasserstein distance between ν and ν′ therefore decomposes into a sum of m
terms:
w1(ν, ν′) =
Z 1
0
F−1
ν (τ) −F−1
ν′ (τ)
dτ
=
m
X
i=1
Z
i
m
(i−1)
m
F−1
ν (τ) −θi
dτ .
By Lemma 5.16, the ith term of the sum is minimized by the quantile midpoint
F−1
ν (τi), where
τi = 1
2
"i −1
m + i
m
#
= 2i −1
2m .
Unlike the categorical-Cramér case, in general, there is not a unique m-
quantile distribution ν′ ∈FQ,m that is closest in w1 to a given distribution ν ∈
P1(R). The following example illustrates how the issue might take shape.
Example 5.17. Consider the set of Dirac distributions
FQ,1 = {δθ : θ ∈R} .
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Distributional Dynamic Programming
135
Let ν = 1
2δ0 + 1
2δ1 be the Bernoulli(1/2) distribution. For any θ ∈[0, 1], δθ ∈FQ,1
is an optimal (w1, FQ,1)-approximation to ν:
w1(ν, δθ) = min
ν′∈FQ,1 w1(ν, ν′) ,
Perhaps surprisingly, this shows that the distribution δ1/2, halfway between the
two possible outcomes and an intuitive one-particle approximation to ν, is a no
better choice than δ0 when measured in terms of 1-Wasserstein distance.
△
5.7
Distributional Dynamic Programming
We embed the projected Bellman operator in an for loop to obtain an algorithmic
template for approximating the return function (Algorithm 5.2). We call this
template distributional dynamic programming (DDP),42 as it computes
ηk+1 = ΠFT πηk
(5.27)
by iteratively applying a projected distributional Bellman operator. A special
case is when the representation is closed under T π, in which case no projection
is needed. However, by contrast with Equation 5.6, the use of a projection allows
us to consider algorithms for a greater variety of representations. Summarizing
the results of the previous sections, instantiating this template involves three
parts:
Choice of representation. We ﬁrst need a probability distribution represen-
tation F. Provided that this representation uses ﬁnitely many parameters, this
enables us to store return functions in memory, using the implied mapping from
parameters to probability distributions.
Update step. We then need a subroutine for computing a single application
of the distributional Bellman operator to a return function represented by F
(Equation 5.8).
Projection step. We ﬁnally need a subroutine that maps the outputs of the
update step to probability distributions in F. In particular, when ΠF is a
d-projection, this involves ﬁnding an optimal (d, F)-approximation at each
iteration.
For empirical representations, including the categorical and quantile repre-
sentations, the update step can be implemented by Algorithm 5.1. That is, when
there are ﬁnitely many rewards, the output of T π applied to any m-particle
representation is a collection of empirical distributions:
η ∈FE,m =⇒T πη ∈FE .
42. More precisely, this is distributional dynamic programming applied to the problem of policy
evaluation. A sensible but less memorable alternative is “iterative distributional policy evaluation.”
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

136
Chapter 5
Algorithm 5.2: Distributional dynamic programming
Algorithm parameters:
representation F, projection ΠF
desired number of iterations K,
initial return function η0 ∈F X
Initialize η ←η0
for k = 1, . . . , K do
η′ ←T πη
▷Algorithm 5.1
foreach state x ∈X do
η(x) ←(ΠFη′)(x)
end foreach
end for
return η
As a consequence, it is suﬃcient to have an eﬃcient subroutine for projecting
empirical distributions back to FC,m, FQ,m, or FE,m.
For example, consider the projection of the empirical distribution
ν =
n
X
j=1
qjδz j
onto the m-categorical representation (Deﬁnition 5.12). For i = 1, . . . , m,
Equation 5.24 becomes
pi =
n
X
j=1
q jhi
 ς−1
m (z j −θi),
which can be implemented in linear time with two for loops (as was done in
Algorithm 3.4).
Similarly, the projection of ν onto the m-quantile representation (Deﬁni-
tion 5.11) is achieved by sorting the locations zi to construct the cumulative
distribution function from their associated probabilities qi:
Fν(z) =
n
X
j=1
q j1{z j ≤z},
from which quantile midpoints can be extracted.
Because the categorical-Cramér and quantile-w1 pairs recur so often through-
out this book, it is convenient to give a name to the algorithms that iteratively
apply their respective projected operators. We call these algorithms categorical
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Distributional Dynamic Programming
137
Algorithm 5.3: Categorical dynamic programming
Algorithm parameters:
representation parameters θ1, . . . , θm, m,
initial probabilities  (pi(x))m
i=1 : x ∈X,
desired number of iterations K
for k = 1, . . . , K do
η(x) =
mP
i=1
pi(x)δθi for x ∈X
  z′
j(x), p′
j(x)m(x)
j=1 : x ∈X ←T πη
▷Algorithm 5.1
foreach state x ∈X do
for i = 1, …, m do
pi(x) ←
m(x)
P
j=1
p′
j(x)hi
 ς−1
m (z′
j −θi)
end for
end foreach
end for
return  (pi(x))m
i=1 : x ∈X)
and quantile dynamic programming, respectively (CDP and QDP; Algorithms
5.3 and 5.4).43
For both categorical and quantile dynamic programming, the computational
cost is dominated by the number of particles produced by the distributional
Bellman operator, prior to projection. Since the number of particles in the
representation is a constant m, we have that per state, there may be up to
N = mNRNX particles in this intermediate step. Thus, the cost of K iterations
with the m-categorical representation is O(KmNRN2
X), not too dissimilar to the
cost of performing iterative policy evaluation with value functions. Due to the
sorting operation, the cost of K iterations with the m-quantile representation is
larger, at O KmNRN2
X log(mNRNX). In Chapter 6, we describe an incremental
algorithm that avoids the explicit sorting operation and is in some cases more
computationally eﬃcient.
In designing distributional dynamic programming algorithms, there is a good
deal of ﬂexibility in the choice of representation and in the projection step once
a representation has been selected. There are basic properties we would like
43. To be fully accurate, we should call these the categorical-Cramér and quantile-w1 dynamic
programming algorithms, given that they combine particular choices of probability representation
and projection. However, brevity has its virtues.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

138
Chapter 5
Algorithm 5.4: Quantile dynamic programming
Algorithm parameters:
initial locations  (θi(x))m
i=1 : x ∈X),
desired number of iterations K
for k = 1, . . . , K do
η(x) =
mP
i=1
1
mδθi(x) for x ∈X
η′ =   z′
j(x), p′
j(x)m(x)
j=1 : x ∈X ←T πη ▷Algorithm 5.1
foreach state x ∈X do
(z′
j(x), p′
j(x))m(x)
i=1 ←sort((z′
j(x), p′
j(x))m(x)
i=1 )
for j = 1, …, m do
Pj(x) ←
jP
i=1
p′
i(x)
end for
for i = 1, …, m do
j ←min{l : Pl(x) ≥τi}
θi(x) ←z′
j(x)
end for
end foreach
end for
return  (θi(x))m
i=1 : x ∈X
from the sequence deﬁned by Equation 5.27, such as a guarantee of convergence,
and further a limit that does not depend on our choice of initialization. Certain
combinations of representation and projection will ensure these properties hold,
as we explore in Section 5.9, while others may lead to very poorly behaved
algorithms (see Exercise 5.19). In addition, using a representation and projection
also necessarily incurs some approximation error relative to the true return
function. It is often possible to obtain quantitative bounds on this approximation
error, as Section 5.10 describes, but often judgment must be used as to what
qualitative types of approximation are the most acceptable for task in hand; we
return to this point in Section 5.11.
5.8
Error Due to Diﬀusion
In Section 5.4, we showed that the distributional algorithm for the normal
representation ﬁnds the best ﬁt to the return function ηπ, as measured in
Kullback–Leibler divergence. Implicit in our derivation was the fact that we
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Distributional Dynamic Programming
139
η0
η1
· · ·
ηk
ˆη0
ˆη1
· · ·
ˆηk
T π
ΠF
T π
ΠF
T π
ΠF
ΠFT π
ΠFT π
ΠFT π
Figure 5.5
A diﬀusion-free projection operator ΠF yields a distributional dynamic program-
ming procedure that is equivalent to ﬁrst computing an exact return function and then
projecting it.
could interleave projection and update steps to obtain the same solution as if
we had ﬁrst determined ηπ without approximation and then found its best ﬁt in
FN. We call a projection operator with this property diﬀusion-free (Figure 5.5).
Deﬁnition 5.18. Consider a representation F and a projection operator ΠF
for that representation. The projection operator ΠF is said to be diﬀusion-free
if, for any return function η ∈F X, we have
ΠFT πΠFη = ΠFT πη .
As a consequence, for any k ≥0 and any η ∈F X, a diﬀusion-free projection
operator satisﬁes
(ΠFT π)kη = ΠF(T π)kη .
△
Algorithms that implement diﬀusion-free projection operators are quite
appealing, because they behave as if no approximation had been made until the
ﬁnal iteration. Unfortunately, such algorithms are the exception, rather than the
rule. By contrast, without this guarantee, an algorithm may accumulate excess
error from iteration to iteration – we say that the iterates η0, η1, η2, . . . undergo
diﬀusion. Known projection algorithms for m-particle representations suﬀer
from this issue, as the following example illustrates.
Example 5.19. Consider a chain of n states with a deterministic left-to-right
transition function (Figure 5.6). The last state of this chain, xn, is terminal and
produces a reward of 1; all other states yield no reward. For t ≥0, the discounted
return at state xn−t is deterministic and has value γt; let us denote its distribution
by η(xn−t). If we approximate this distribution with m = 11 particles uniformly
spaced from 0 to 1, the best categorical approximation assigns probability to
the two particles closest to γt.
If we instead use categorical dynamic programming, we ﬁnd a diﬀerent
solution. Because each iteration of the projected Bellman operator must produce
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

140
Chapter 5
x1
x3
x5
x7
x9
State
Return
m-Categorical
Approximation
0.0
0.2
0.4
0.6
0.8
1.0
x1
x3
x5
x7
x9
State
Return
Categorical
Dynamic Programming
0.0
0.2
0.4
0.6
0.8
1.0
(a)
(b)
(c)
Figure 5.6
(a) An n-state chain with a single nonzero reward at the end. (b) The (ℓ2, FC,m)-optimal
approximation to the return function, for n = 10, m = 11, and θ1 = 0, . . . , θ11 = 1. The
probabilities assigned to each location are indicated in grayscale (white = 0, black =
1). (c) The approximation found by categorical dynamic programming with the same
representation.
a categorical distribution, the iterates undergo diﬀusion. Far from the terminal
state, the return distribution found by Algorithm 5.2 is distributed on a much
larger support than the best categorical approximation of η(xi).
△
The diﬀusion in Example 5.19 can be explained analytically. Let us asso-
ciate each particle with an integer j = 0, . . . , 10, corresponding to the location
j
10. For concreteness, let γ = 1 −ε for some 0 < ε ≪0.1, and consider the
return-distribution function obtained after N iterations of categorical dynamic
programming:
ˆη = (ΠcT π)nη0.
Because there are no cycles, one can show that further iterations leave the
approximation unchanged.
If we interpret the m particle locations as states in a Markov chain, then we
can view the return distribution ˆη(xn−t) as the probability distribution of this
Markov chain after t time steps (t ∈{0, . . . , n −1}). The transition function for
states j = 1 . . . 10 is
P(⌊γ j⌋| j) = ⌈γ j⌉−γ j
P(⌈γ j⌉| j) = 1 −⌈γ j⌉+ γ j .
For j = 0, we simply have P(0 | 0) = 1 (i.e., state 0 is terminal). When γ is
suﬃciently small compared to the gap ςm = 0.1 between neighboring particle
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Distributional Dynamic Programming
141
locations, the process can be approximated with a binomial distribution:
ˆG(xn−t) ∼1 −t−1Binom(1 −γ, t) ,
where ˆG is the return-variable function associated with ˆη. Figure 5.6c gives a
rough illustration of this point, with a bell-shaped distribution emerging in the
return distribution at state x1.
5.9
Convergence of Distributional Dynamic Programming
We now use the theory of operators developed in Chapter 4 to characterize the
behavior of distributional dynamic programming in the policy evaluation setting:
its convergence rate, its point of convergence, and also the approximation error
incurred. Speciﬁcally, this theory allows us to measure how these quantities
are impacted by diﬀerent choices of representation and projection. Although
the algorithmic discussion in previous sections has focused on implementa-
tions of distributional dynamic programming in the case of ﬁnitely supported
reward distributions, the results presented here apply without this assumption
(see Exercise 5.9 for indications of how distributional dynamic programming
algorithms may be implemented in such cases). As we will see in Chapter 6,
the theory developed here also informs incremental algorithms for learning the
return-distribution function.
Let us consider a probability metric d, possibly diﬀerent from the metric
under which the projection is performed (when applicable), and which we call
the analysis metric. We use the analysis metric to characterize instances of
Algorithm 5.2 in terms of the Lipschitz constant of the projected operator.
Deﬁnition 5.20. Let (M, d) be a metric space, and let O : M →M be a function
on this space. The Lipschitz constant of O under the metric d is
∥O∥d = sup
U,U′∈M
U,U′
d(OU, OU′)
d(U, U′)
.
△
When O is a contraction mapping, its Lipschitz constant is simply its con-
traction modulus. That is, under the conditions of Theorem 4.25 applied to a
c-homogeneous d, we have
∥T π∥d ≤γc .
Deﬁnition 5.20 extends the notion of a contraction modulus to operators, such
as projections, that are not contraction mappings.
Lemma 5.21. Let (M, d) be a metric space, and let O1, O2 : M →M be func-
tions on this space. Write O1O2 for the composition of these mappings.
Then,
∥O1O2∥d ≤∥O1∥d∥O2∥d .
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

142
Chapter 5
Proof. By applying the deﬁnition of the Lipschitz constant twice, we have
d(O1O2U, O1O2U′) ≤∥O1∥dd(O2U, O2U′) ≤∥O1∥d∥O2∥dd(U, U′) ,
as required.
Lemma 5.21 gives a template for validating and understanding diﬀerent
instantiations of Algorithm 5.2. Here, we consider the metric space deﬁned by
the ﬁnite domain of our analysis metric, (Pd(R)X, d), and use Lemma 5.21 to
characterize ΠFT π in terms of the Lipschitz constants of its parts (ΠF and
T π). By analogy with the conditions of Proposition 4.27, where needed, we
will assume that the environment (more speciﬁcally, its random quantities) is
reasonably behaved under d.
Assumption 5.22(d). The ﬁnite domain Pd(R)X is closed under both the
projection ΠF and the Bellman operator T π, in the sense that
η ∈Pd(R)X =⇒T πη, ΠFη ∈Pd(R)X.
△
Assumption 5.22(d) guarantees that distributions produced by the distri-
butional Bellman operator and the projection have ﬁnite distances from one
another. For many choices of d of interest, Assumption 5.22 can be easily shown
to hold when the reward distributions are bounded; contrast with Proposition
4.16.
The Lipschitz constant of projection operators must be at least 1, since for
any ν ∈F,
ΠFν = ν .
In the case of the Cramér distance ℓ2, we can show that it behaves much like a
Euclidean metric, from which we obtain the following result on ℓ2 projections.
Lemma 5.23. Consider a representation F ⊆Pℓ2(R). If F is complete with
respect to ℓ2 and convex, then the ℓ2-projection ΠF,ℓ2 : Pℓ2(R) →F is a
nonexpansion in that metric:
∥ΠF,ℓ2∥ℓ2 = 1 .
Furthermore, the result extends to return functions and the supremum extension
of ℓ2:
∥ΠF,ℓ2∥ℓ2 = 1 .
△
The proof is given in Remark 5.6. Exercise 5.15 asks you show that the
m-categorical representation is convex and complete with respect to the Cramér
distance. From this, we immediately derive the following.
Lemma 5.24. The projected distributional Bellman operator instantiated with
F = FC,m and d = ℓ2 has contraction modulus γ
1/2 with respect to ℓ2 on the
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Distributional Dynamic Programming
143
space Pℓ2(R)X. That is,
∥ΠcT π∥ℓ2 ≤γ
1/2 .
△
Lemma 5.24 gives a formal reason for why the use of the m-categorical
representation, coupled with a projection based on triangular kernel, produces a
convergent distributional dynamic programming algorithm.
The m-quantile representation, however, is not convex. This precludes a direct
appeal to an argument based on a quantile analogue to Lemma 5.23. Nor is
the issue simply analytical: the w1 Lipschitz constant of the projected Bellman
operator ΠqT π is greater than 1 (see Exercise 5.16). Instead, we need to use the
∞-Wasserstein distance as our analysis metric. As the w1-projection onto FQ,m
is a nonexpansion in w∞, we obtain the following result.
Lemma 5.25. Under Assumption 5.22(w∞), the projected distributional Bell-
man operator ΠqT π : P∞(R)X →P∞(R)X has contraction modulus γ in w∞.
That is,
∥ΠqT π∥w∞≤γ .
△
Proof. Since T π is a γ-contraction in w∞by Proposition 4.15, it is suﬃcient
to prove that Πq : P∞(R) →P∞(R) is a nonexpansion in w∞; the result then
follows from Lemma 5.21. Given two distributions ν1, ν2 ∈P∞(R), we have
w∞(ν1, ν2) = sup
τ∈(0,1)
|F−1
ν1 (τ) −F−1
ν2 (τ)| .
Now note that
Πqν1 = 1
m
m
X
i=1
δF−1
ν1 (τi) , Πqν2 = 1
m
m
X
i=1
δF−1
ν2 (τi) ,
where τi = 2i−1
2m . We have
w∞(Πqν1, Πqν2) = max
i=1,…,m
F−1
ν1 (τi) −F−1
ν2 (τi)

≤sup
τ∈(0,1)
F−1
ν1 (τ) −F−1
ν2 (τ)
 = w∞(ν1, ν2) ,
as required.
The derivation of a contraction modulus for a projected Bellman operator
provides us with two results. By Proposition 4.7, it establishes that if ΠFT π
has a ﬁxed point ˆη, then Algorithm 5.2 converges to this ﬁxed point. Second, it
also establishes the existence of such a ﬁxed point when the representation is
complete with respect to the analysis metric d, based on Banach’s ﬁxed point
theorem; a proof is provided in Remark 5.7.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

144
Chapter 5
Theorem 5.26 (Banach’s ﬁxed point theorem). Let (M, d) be a complete
metric space and let O be a contraction mapping on M, with respect to d.
Then O has a unique ﬁxed point U∗∈M.
△
Proposition 5.27. Let d be a c-homogeneous, regular probability metric
that is p-convex for some p ∈[1, ∞) and let F be a representation complete
with respect to d. Consider Algorithm 5.2 instantiated with a projection
step described by the operator ΠF, and suppose that Assumption 5.22(d)
holds. If ΠF is a nonexpansion in d, then the corresponding projected
Bellman operator ΠFT π has a unique ﬁxed point ˆηπ in Pd(R)X satisfying
ˆηπ = ΠFT π ˆηπ .
Additionally, for any ε > 0, if K the number of iterations is such that
K ≥
log   1
ε
 + log d(η0, ˆηπ)
c log   1
γ

with η0(x) ∈Pd(R) for all x, then the output ηK of Algorithm 5.2 satisﬁes
d(ηK, ˆηπ) ≤ε .
△
Proof. By the assumptions in the statement, we have that T π is a γc-contraction
on Pd(R)X by Theorem 4.25, and by Lemma 5.21, ΠFT π is a γc-contraction
on Pd(R)X. By Banach’s ﬁxed point theorem, there is a unique ﬁxed point ˆηπ
for ΠFT π in Pd(R)X. Now note
d(ηK, ˆηπ) = d(ΠFT πηK−1, ΠFT πˆηπ) ≤γcd(ηK−1, ˆηπ) ,
so by induction
d(ηK, ˆηπ) ≤γcKd(η0, ˆηπ) .
Setting the right-hand side to less than ε and rearranging yields the result.
Although Proposition 5.27 does not allow us to conclude that a particular
algorithm will fail, it cautions us against projections in certain probability
metrics. For example, because the distributional Bellman operator is only a non-
expansion in the supremum total variation distance, we cannot guarantee a good
approximation with respect to this metric after any ﬁnite number of iterations.
Because we would like the projection step to be computationally eﬃcient, this
argument also gives us a criterion with which to choose a representation. For
example, although the m-particle representation is clearly more ﬂexible than
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Distributional Dynamic Programming
145
either the categorical or quantile representations, it is currently not known how
to eﬃciently and usefully project onto it.
5.10
Quality of the Distributional Approximation
Having identiﬁed conditions under which the iterates produced by distributional
dynamic programming converge, we now ask: to what do they converge? We
answer this question by measuring how close the ﬁxed point ˆηπ of the projected
Bellman operator (computed by Algorithm 5.2 in the limit of the number of
iterations) is to the true return function ηπ. The quality of this approximation
depends on a number of factors: the choice and size of representation F, which
determines the optimal approximation of ηπ within F, as well as properties of
the projection step.
Proposition 5.28. Let d be a c-homogeneous, regular probability metric
that is p-convex for some p ∈[1, ∞), and let F ⊆Pd(R) be a representation
complete with respect to d. Let ΠF be a projection operator that is a
nonexpansion in d, and suppose Assumption 5.22(d) holds. Consider the
projected Bellman operator ΠFT π with ﬁxed point ˆηπ ∈Pd(R)X. Then,
d(ˆηπ, ηπ) ≤d(ΠFηπ, ηπ)
1 −γc
.
When ΠF is a d-projection in some probability metric d, ΠFηπ is a
(d, F)-optimal approximation of the return function ηπ.
△
Proof. We have
d(ηπ, ˆηπ) ≤d(ηπ, ΠFηπ) + d(ΠFηπ, ˆηπ)
= d(ηπ, ΠFηπ) + d(ΠFT πηπ, ΠFT π ˆηπ)
≤d(ηπ, ΠFηπ) + γcd(ηπ, ˆηπ) .
Rearranging then gives the result.
From this, we immediately derive a result regarding the ﬁxed point ˆηπ
c of the
projected operator ΠcT π.
Corollary 5.29. The ﬁxed point ˆηπ
c of the categorical-projected Bellman
operator ΠcT π : P1(R) →P1(R) satisﬁes
ℓ2(ˆηπ
c, ηπ) ≤ℓ2(Πcηπ, ηπ)
1 −γ
1/2
.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

146
Chapter 5
1.0
0.5
0.0
0.5
1.0
Return
0.0
0.2
0.4
0.6
Probability
m = 3
1.0
0.5
0.0
0.5
1.0
Return
0.0
0.2
0.4
0.6
Probability
m = 5
1.0
0.5
0.0
0.5
1.0
Return
0.0
0.2
0.4
0.6
Probability
m = 11
1.0
0.5
0.0
0.5
1.0
Return
0.0
0.2
0.4
0.6
Probability
m = 31
Figure 5.7
The return-distribution function estimates obtained by applying categorical dynamic
programming to the Cliﬀs domain (Example 2.9; here, with the safe policy). Each panel
corresponds to a diﬀerent number of particles.
If the return distributions (ηπ(x) : x ∈X) are supported on [θ1, θm], we further
have that, for each x ∈X,
min
ν∈FC,m ℓ2
2(ν, ηπ(x)) ≤ςm
m
X
i=1

Fηπ(x)
 θ1 + iςm
 −Fηπ(x)
 θ1 + (i −1)ςm
2 ≤ςm ,
(5.28)
and hence
ℓ2(ˆηπ
c, ηπ) ≤
1
1 −γ
1/2
θm −θ1
m −1 .
△
Proposition 5.28 suggests that the excess error may be greater when the
projection is performed in a metric for which c is small. In the particular case of
the Cramér distance, the constant in Corollary 5.29 can in fact be strengthened
to
1
√
1−γ by arguing about the geometry of the probability space under ℓ2
under certain conditions; see Remark 5.6 for a discussion of this geometry and
Rowland et al. (2018) for details on the strengthened bound.
Varying the number of particles in the categorical representation allows the
user to control both the complexity of the associated dynamic programming
algorithm and the error of the ﬁxed point ˆηπ
c (Figure 5.7). As discussed in the
ﬁrst part of this chapter, both the memory and time complexity of computing
with this representation increase with m. On the other hand, Corollary 5.29
establishes that increasing m also reduces the approximation error incurred from
dynamic programming.
Proposition 5.28 can be similarly used to analyze quantile dynamic pro-
gramming. This result, under the conditions of Lemma 5.25, shows that the
ﬁxed point ˆηπ
q of the projected operator ΠqT π for the m-quantile representation
satisﬁes
w∞(ˆηπ
q, ηπ) ≤w∞(Πqηπ, ηπ)
1 −γ
.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Distributional Dynamic Programming
147
Unfortunately, the distance w∞(Πqηπ, ηπ) does not necessarily vanish as the
number of particles m in the quantile representation increases. This is because
w∞is in some sense a very strict distance (Exercise 5.18 makes this point
precise). Nevertheless, the dynamic programming algorithm associated with the
quantile representation enjoys a similar trade-oﬀbetween complexity and accu-
racy as established for the categorical algorithm above. This can be shown by
instead analyzing the algorithm via the more lenient w1 distance; Exercise 5.20
provides a guide to a proof of this result.
Lemma 5.30. Suppose that for each (x, a) ∈X × A, the reward distribution
PR( · | x, a) is supported on the interval [Rmin, Rmax]. Then the m-quantile ﬁxed
point ˆηπ
q satisﬁes
w1(ˆηπ
q, ηπ) ≤3(Vmax −Vmin)
2m(1 −γ)
.
In addition, consider an initial return function η0 with distributions with support
bounded in [Vmin, Vmax] and the iterates
ηk+1 = ΠqT πηk
produced by quantile dynamic programming. Then we have, for all k ≥0,
w1(ηk, ηπ) ≤γk(Vmax −Vmin) + 3(Vmax −Vmin)
2m(1 −γ)
.
△
5.11
Designing Distributional Dynamic Programming Algorithms
Although both categorical and quantile dynamic programming algorithms arise
from natural choices, there is no reason to believe that they lead to the best
approximation of the return-distribution function for a ﬁxed number of parame-
ters. For example, can the error be reduced by using a m
2 -particle representation
instead? Are there measurable characteristics of the environment that could
guide the choice of distribution representation?
As a guide to further investigations, Table 5.1 summarizes the desirable
properties of representations and projections that were studied in this chapter,
as they pertain to the design of distributional dynamic programming algorithms.
Because these properties arise from the combination of a representation with
a particular projection, every such combination is likely to exhibit a diﬀerent
set of properties. This highlights how new DDP algorithms with desirable
characteristics may be produced by simply trying out diﬀerent combinations.
Because these properties arise from the combination of a representation with
a particular projection, one can naturally expect new algorithms for example,
one can imagine a new algorithm based on the quantile representation that is a
nonexpansion in the 1- or 2-Wasserstein distances.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

148
Chapter 5
Included in this table is whether a representation is mean-preserving. We say
that a representation F and its associated projection operator ΠF are mean-
preserving if for any distribution ν from a suitable space, the mean of ΠFν is
the same as that of ν. The m-categorical algorithm presented in this chapter is
mean-preserving provided the range of considered returns does not exceed the
boundaries of its support; the m-quantile algorithm is not. In addition to the
mean, we can also consider what happens to other aspects of the approximated
distributions. For example, the m-categorical algorithm produces probability
distributions that in general have more variance than those of the true return
function.
The choice of representation also has consequences beyond distributional
dynamic programming. In the next chapter, for example, we will consider the
design of incremental algorithms for learning return-distribution functions from
samples. There, we will see that the projected operator derived from the cate-
gorical representation directly translates into an incremental algorithm, while
the operator derived from the quantile representation does not. In Chapter 9,
we will also see how the choice of representation interplays with the use of
parameterized models such as neural networks to represent the return function
compactly. Another consideration, not listed here, concerns the sensitivity of the
algorithm to its parameters: for example, for small values of m, the m-quantile
representation tends to be a better choice than the m-categorical representation,
which suﬀers from large gaps between its particles’ locations (as an extreme
example, take m = 2).
The design and study of representations remains an active topic in distribu-
tional reinforcement learning. The representations we presented here are by no
mean an exhaustive portrait of the ﬁeld. For example, Barth-Maron et al. (2018)
considered using mixtures of m normal distributions in domains with vector-
valued action spaces. Analyzing representations like these is more challenging
because known projection methods suﬀer from local minima, which in turn
implies that dynamic programming may give diﬀerent (and possibly suboptimal)
solutions for diﬀerent initial conditions.
5.12
Technical Remarks
Remark 5.1 (Finiteness of R). In the algorithms presented in this chapter, we
assumed that rewards are distributed on a ﬁnite set R. This is not actually needed
for most of our analysis but makes it possible to compute expectations and
convolutions in ﬁnite time and hence devise concrete dynamic programming
algorithms. However, there are many problems in which the rewards are better
modeled using continuous or unbounded distributions. Rewards generated from
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Distributional Dynamic Programming
149
T π-closed
Tractable
Expressive
Diﬀusion-
free
Mean-
preserving
Empirical DP
✓
✓
✓
✓
Normal DP
✓
✓
✓
Categorical DP
✓
✓
*
Quantile DP
✓
✓
Table 5.1
Desirable characteristics of distributional dynamic programming algorithms. “Empirical
DP” and “Normal DP” refer to distributional dynamic programming with the empir-
ical and normal distributions, respectively (Sections 5.3 and 5.4). While no known
representation–projection pair satisﬁes all of these, the categorical-Cramér and quantile-
w1 choices oﬀer a good compromise. *The categorical representation is mean-preserving
provided its support spans the range of possible returns.
observations of a physical process are often well modeled by a normal distri-
bution to account for sensor noise. Rewards derived from a queuing process,
such as the number of customers who make a purchase at an ice cream shop in
a give time interval, can be modeled by a Poisson distribution.
△
Remark 5.2 (NP-hardness of computing return distributions). Recall that
a problem is said to be NP-hard if its solution can also be used to solve all
problems in the class NP, by means of a polynomial-time reduction (Cormen
et al. 2001). This remark illustrates how the problem of computing certain
aspects of the return-distribution function for a given Markov decision process
is NP-hard, by reduction from one of Karp’s original NP-complete problems,
the Hamiltonian cycle problem. Reductions from the Hamiltonian cycle problem
have previously been used to prove the NP-hardness of a variety of problems
relating to constrained Markov decision processes (Feinberg 2000).
Let G = (V, E) be a graph, with V = {1, . . . , n} for some n ∈N+. The Hamilto-
nian cycle problem asks whether there is a permutation of vertices σ : V →V
such that {σ(i), σ(i + 1)} ∈E for each i ∈[n −1], and {σ(n), σ(1)} ∈E. Now con-
sider an MDP whose state space is the set of integers from 1 to n, denoted
X = {1, . . . , n}, and with a singleton action set A = {a}, a transition kernel that
encodes a random walk over the graph G, and a uniform initial state distribution
ξ0. Further, specify reward distributions as PR(· | x, a) = δx for each x ∈X, and
set γ < 1/n+2. For such a discount factor, there is a one-to-one mapping between
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

150
Chapter 5
trajectories and returns. As the action set is a singleton, there is a single policy π.
It can be shown that there is a Hamiltonian cycle in G if and only if the support
of the return distribution has nonempty intersection with the set
[
σ∈Sn

n−1
X
t=0
γtσ(t + 1) + γnσ(1),
n−1
X
t=0
γtσ(t + 1) + γn(σ(1) + 1)
.
(5.29)
Since the MDP description is clearly computable in polynomial time from the
graph specifying the Hamiltonian cycle problem, it must therefore be the case
that ascertaining whether the set in Expression 5.29 has nonempty intersec-
tion with the return distribution support is NP-hard. The full proof is left as
Exercise 5.21.
△
Remark 5.3 (Existence of d-projections). As an example of a setting where
d-projections do not exist, let θ1, …, θm ∈R and consider the softmax categorical
representation
F =
n
m
X
i=1
eϕi
Pm
j=1 eϕj δθi : ϕ1, …, ϕm ∈R
o
.
For the distribution δθ1 and metric d = w2, there is no optimal (F, d)-
approximation to δθ1, since for any distribution in F, there is another
distribution in F that lies closer to δθ1 with respect to w2. The issue is that there
are elements of the representation which are arbitrarily close to the distribution
to be projected, but there is no distribution in the representation that dominates
all others as in Equation 5.23. An example of a suﬃcient condition for the
existence of an optimal (F, d)-approximation to ˆν ∈Pd(R) is that the metric
space (F, d) has the Bolzano–Weierstrass property, also known as sequential
compactness: for any sequence (νk)k≥0 in F, there is subsequence that con-
verges to a point in F with respect to d. When this assumption holds, we may
take the sequence (νk)k≥0 in F to satisfy
d(νk, ˆν) ≤inf
ν∈F d(ν, ˆν) +
1
k + 1 .
Using the Bolzano–Weierstrass property, we may pass to a subsequence (νkn)n≥0
converging to ν∗∈F. We then observe
d(ν∗, ˆν) ≤d(ν∗, νkn) + d(νkn, ˆν) →inf
ν∈F d(ν, ˆν) ,
showing that ν∗is an optimal (F, d)-approximation to ˆν. The softmax repre-
sentation (under the w2 metric) fails to have the Bolzano–Weierstrass property.
As an example, the sequence of distributions ( k
k+1δθ1 +
1
k+1δθ2)k≥0 in F has no
subsequence that converges to a point in F.
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Distributional Dynamic Programming
151
Remark 5.4 (Proof of Proposition 5.14). We will demonstrate the following
Pythagorean identity. For any ν ∈Pℓ2(R), and ν′ ∈FC,m, we have
ℓ2
2(ν, ν′) = ℓ2
2(ν, Πcν) + ℓ2
2(Πcν, ν′) .
From this, it follows that ν′ = Πcν is the unique ℓ2-projection of ν onto FC,m,
since this choice of ν′ uniquely minimizes the right-hand side. To show this
identity, we ﬁrst establish an interpretation of the cumulative distribution func-
tion (CDF) of Πcν as averaging the CDF values of ν on each interval (θi, θi+1)
for i = 1, …, m −1. First note that for z ∈(θi, θi+1], we have
hi
 ς−1
m (z −θi) = 1 −ς−1
m |z −θi|
=
1
θi+1 −θi
 θi+1 −θi + θi −z
= θi+1 −z
θi+1 −θi
.
Now, for i = 1, …, m −1, we have
FΠcν(θi) =
iX
j=1
E
Z∼ν[hj(Z)]
= E
Z∼ν
h
1{Z ≤θi} + 1{Z ∈(θi, θi+1]} θi+1 −Z
θi+1 −θi
i
=
1
θi+1 −θi
Z θi+1
θi
Fν(z)dz .
Now note
ℓ2
2(ν, ν′) =
Z ∞
−∞
(Fν(z) −Fν′(z))2dz
(a)=
Z ∞
−∞
(Fν(z) −FΠcν(z))2dz +
Z ∞
−∞
(FΠcν(z) −Fν′(z))2dz
+ 2
Z ∞
−∞
(Fν(z) −FΠcν(z))(FΠcν(z) −Fν′(z))dz
= ℓ2
2(ν, Πcν) + ℓ2
2(Πcν, ν′)
+ 2
 Z θ1
−∞
+
m−1
X
i=1
Z θi+1
θi
+
Z ∞
θm

(Fν(z) −FΠcν(z))(FΠcν(z) −Fν′(z))dz
(b)= ℓ2
2(ν, Πcν) + ℓ2
2(Πcν, ν′)
establishing the identity as required. Here, (a) follows by adding and subtracting
FΠcν inside the parentheses and expanding, and (b) follows by noting that on
(−∞, θ1) and (θm, ∞), FΠcν = Fv′, and on each interval (θi, θi+1), FΠcν −Fv′ is
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

152
Chapter 5
constant and FΠcν is constant and equals the average of Fν on the interval,
meaning that
Z θi+1
θi
(Fν(z) −FΠcν(z))(FΠcν(z) −Fν′(z))dz
= (FΠcν(θi) −Fν′(θi))
Z θi+1
θi
(Fν(z) −FΠcν(z))dz = 0 ,
as required.
△
Remark 5.5 (Proof of Lemma 5.16). Assume that F−1
ν
is continuous at τ∗=
a+b
2 ; this is not necessary but simpliﬁes the proof. For any τ,
F−1
ν (τ) −θ

(5.30)
is a convex function, and hence so is Equation 5.26. A subgradient44 for
Equation 5.30 is
gτ(θ) =

1
if θ < F−1
ν (τ)
−1
if θ > F−1
ν (τ)
0
if θ = F−1
ν (τ) .
A subgradient for Equation 5.26 is therefore
g[a,b](θ) =
Z b
τ=a
gτ(θ)dτ
=
Z Fν(θ)
τ=a
−1 dτ +
Z b
τ=Fν(θ)
1 dτ
= −(Fν(θ) −a) + (b −Fν(θ)).
Setting the subgradient to zero and solving for θ,
0 = a + b −2Fν(θ)
=⇒Fν(θ) = a + b
2
=⇒θ = F−1
ν
 a + b
2
!
.
△
Remark 5.6 (Proof of Lemma 5.23). The argument follows the standard proof
that ﬁnite-dimensional Euclidean projections onto closed convex sets are non-
expansions. Throughout, we write Π for ΠF,ℓ2 for conciseness. We begin with
44. For a convex function f : R →R, we say that g : R →R is a subgradient for f if for all z1, z2 ∈R,
we have f(z2) ≥f(z1) + g(z1)(z2 −z1).
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Distributional Dynamic Programming
153
the observation that for any ν ∈Pℓ2(R) and ν′ ∈F, we have
Z
R
(Fν(z) −FΠν(z))(Fν′(z) −FΠν(z))dz ≤0 ,
(5.31)
since if not, we have (1 −ε)Πν + εν′ ∈F for all ε ∈(0, 1) by convexity, and
ℓ2
2(ν, (1 −ε)Πν + εν′)
=
Z
R
(Fν(z) −(1 −ε)FΠν(z) −εFν′(z))2dz
= ℓ2
2(ν, Πν) −2ε
Z
R
(Fν(z) −FΠν(z))(Fν′(z) −FΠν(z))dz + O(ε2) .
This ﬁnal line must be at least as great as ℓ2
2(ν, Πν) for all ε ∈(0, 1), by deﬁnition
of Π. It must therefore be the case that Inequality 5.31 holds, since if not, we
could select ε > 0 suﬃciently small to make ℓ2
2(ν, (1 −ε)Πν + εν′) smaller than
ℓ2
2(ν, Πν).
Now take ν1, ν2 ∈Pℓ2(R). Applying the above inequality twice yields
Z
R
(Fν1(z) −FΠν1(z))(FΠν2(z) −FΠν1(z))dz ≤0 ,
Z
R
(Fν2(z) −FΠν2(z))(FΠν1(z) −FΠν2(z))dz ≤0 .
Adding these inequalities then yields
Z
R
(Fν1(z) −Fν2(z) + FΠν1(z) −FΠν2(z))(FΠν1(z) −FΠν2(z))dz ≤0
=⇒ℓ2
2(Πν1, Πν2) +
Z
R
(Fν1(z) −Fν2(z))(FΠν1(z) −FΠν2(z))dz ≤0
=⇒ℓ2
2(Πν1, Πν2) ≤
Z
R
(Fν2(z) −Fν1(z))(FΠν1(z) −FΠν2(z))dz .
Applying the Cauchy–Schwarz inequality to the remaining integral then yields
ℓ2
2(Πν1, Πν2) ≤ℓ2(Πν1, Πν2)ℓ2(ν1, ν2) ,
from which the result follows by rearranging.
△
Remark 5.7 (Proof of Banach’s ﬁxed point theorem (Theorem 5.26)). The
map O has at most one ﬁxed point by Proposition 4.5. It therefore suﬃces to
exhibit a ﬁxed point in M. Let β ∈[0, 1) be the contraction modulus of O, and
let U0 ∈M. Consider the sequence (Uk)k≥0 deﬁned by Uk+1 = OUk for k ≥0. For
any l > k, we have
d(Ul, Uk) ≤βkd(Ul−k, U0)
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

154
Chapter 5
≤βk
l−k−1
X
j=0
d(U j, U j+1)
≤βk
l−k−1
X
j=0
βjd(U1, U0)
≤
βk
1 −βd(U1, U0) .
Therefore, as k →∞, we have d(Ul, Uk) →0, so (Uk)k≥0 is a Cauchy sequence.
By completeness of (M, d), (Uk)k≥0 has a limit U∗∈M. Finally, for any k > 0,
we have
d(U∗, OU∗) ≤d(U∗, Uk) + d(Uk, OU∗) ≤d(U∗, Uk) + βd(Uk−1, U∗) ,
and as d(U∗, Uk) →0, we deduce that d(U∗, OU∗) = 0. Hence, U∗is the unique
ﬁxed point of O.
△
5.13
Bibliographical Remarks
5.1. The term “dynamic programming” and the Bellman equation are due to
Bellman (1957b). The relationship between the Bellman equation, value func-
tions, and linear systems of equations is studied at length by Puterman (2014)
and Bertsekas (2012). Bertsekas (2011) provides a treatment of iterative policy
evaluation generalized to matrices other than discounted stochastic matrices.
The advantages of the iterative process are well documented in the ﬁeld and
play a central role in the work of Sutton and Barto (2018), which is our source
for the term “iterative policy evaluation.”
5.2. Our notion of a probability distribution representation reﬂects the common
principle in machine learning of modeling distributions with simple parametric
families of distributions; see, for example, the books by MacKay (2003), Bishop
(2006), Wainwright and Jordan (2008), and Murphy (2012). The Bernoulli
representation was introduced in the context of distributional reinforcement
learning, mostly as a curio, by Bellemare et al. (2017a). Normal approximations
have been extensively used in reinforcement learning, often in Bayesian settings
(Dearden et al. 1998; Engel et al. 2003; Morimura et al. 2010b; Lee et al. 2013).
Similar to Equation 5.10, Morimura et al. (2010a) present a distributional
Bellman operator in terms of cumulative distribution functions; see also Chung
and Sobel (1987).
5.3. Li et al. (2022) consider what is eﬀectively distributional dynamic program-
ming with the empirical representation; they show that in the undiscounted,
ﬁnite-horizon setting with reward distributions supported on ﬁnitely many inte-
gers, exact computation is tractable. Our notion of the empirical representation
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Distributional Dynamic Programming
155
has roots in particle ﬁltering (Doucet et al. 2001; Robert and Casella 2004;
Brooks et al. 2011; Doucet and Johansen 2011) and is also a representation in
modern variational inference algorithms (Liu and Wang 2016). The NP-hardness
result (properly discussed in Remark 5.2) is new as given here, but Mannor
and Tsitsiklis (2011) give a related result in the context of mean-variance
optimization.
5.4. Sobel (1982) is usually noted as the source of the Bellman equation for the
variance and its associated operator. The equation plays an important role in
theoretical exploration (Lattimore and Hutter 2012; Azar et al. 2013). Tamar et
al. (2016) study the variance equation in the context of function approximation.
5.5. The m-categorical representation was used in a distributional setting by
Bellemare et al. (2017a), inspired by the success of categorical representations
in generative modeling (van den Oord et al. 2016). Dabney et al. (2018b)
introduced the quantile representation to avoid the ineﬃciencies in using a ﬁxed
set of evenly spaced locations, as well as deriving an algorithm more closely
grounded in the Wasserstein distances.
Morimura et al. (2010a) used the m-particle representation to design a risk-
sensitive distributional reinforcement learning algorithm. In a similar vein,
Maddison et al. (2017) used the same representation in the context of exponen-
tial utility reinforcement learning. Both approaches are closely related to particle
ﬁltering and sequential Monte Carlo methods (Gordon et al. 1993; Doucet et
al. 2001; Särkkä 2013; Naesseth et al. 2019; Chopin and Papaspiliopoulos 2020),
which rely on stochastic sampling and resampling procedures, by contrast to
the deterministic dynamic programming methods of this chapter.
5.6, 5.8. The categorical projection was originally proposed as an ad hoc solu-
tion to address the need to map the output of the distributional Bellman operator
back onto the support of the distribution. Its description as the expectation of a
triangular kernel was shown by Rowland et al. (2018), justifying its use from
a theoretical perspective and providing the proof of Proposition 5.14. Lemma
5.16 is due to Dabney et al. (2018b).
5.7, 5.9–5.10. The language and analysis of projected operators is inherited
from the theoretical analysis of linear function approximation in reinforcement
learning; a canonical exposition may be found in Tsitsiklis and Van Roy (1997)
and Lagoudakis and Parr (2003). Because the space of probability distributions
is not a vector space, the analysis is somewhat diﬀerent and, among other
things, requires more technical care (as discussed in Chapter 4). A version of
Theorem 5.28 in the special case of CDP appears in Rowland et al. (2018). Of
note, in the linear function approximation setting, the main technical argument
revolves around the noncontractive nature of the stochastic matrix Pπ in a
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

156
Chapter 5
weighted L2 norm, whereas here it is due to the c-homogeneity of the analysis
metric (and does not involve Pπ). See Chapter 9.
5.11. A discussion of the mean-preserving property is given in Lyle et al. (2019).
5.14
Exercises
Exercise 5.1. Suppose that a Markov decision process is acyclic, in the sense
that for any policy π and nonterminal state x ∈X,
Pπ
 Xt = x | X0 = x = 0 for all t > 0 .
Consider applying iterative policy evaluation to this MDP, beginning with the
initial condition V(x∅) = 0 for all terminal states x∅∈X. Show that it converges
to Vπ in a ﬁnite number of iterations K, and give a bound on K.
△
Exercise 5.2. Show that the normal representation (Deﬁnition 5.10) is closed
under the distributional Bellman operator T π under the following conditions:
(i) the policy is deterministic: π(a | x) ∈{0, 1};
(ii) the transition function is deterministic: PX(x′ | x, a) ∈{0, 1}; and
(iii) the rewards are normally distributed, PR(· | x, a) = N(µx,a, σ2
x,a).
△
Exercise 5.3. In Algorithm 5.1, we made use of the fact that the return Gπ(x∅)
from the terminal state x∅is 0, arguing that this is a more eﬀective procedure.
(i) Explain how this change aﬀects the output of categorical and quantile
dynamic programming, compared to the algorithm that explicitly maintains
and computes a return-distribution estimate for x∅.
(ii) Explain how this changes aﬀects the analysis given in Section 5.9 onward.
△
Exercise 5.4. Provide counterexamples showing that if any of the conditions
of the previous exercise do not hold, then the normal representation may not be
closed under T π.
△
Exercise 5.5. Consider a probability distribution ν ∈P2(R) with probability
density fν. For a normal distribution ν′ ∈FN with probability density fν′, deﬁne
the Kullback–Leibler divergence
KL(ν ∥ν′) =
Z
R
fν(z) log fν(z)
fν′(z)dz .
Show that the normal distribution ˆν = N(µ, σ2) minimizing KL(ν ∥ˆν) has
parameters given by
µ = E
Z∼ν[Z] ,
σ2 = E
Z∼ν[(Z −µ)2] .
(5.32)
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Distributional Dynamic Programming
157
Exercise 5.6. Consider again a probability distribution ν ∈P2(R) with instan-
tiation Z, and for a normal distribution ν′ ∈FN, deﬁne the cross-entropy
loss
CE(ν, ν′) = E
Z∼ν
 log fν′(z).
Show that the normal distribution ν′ that minimizes this cross-entropy loss has
parameters given by Equation 5.32. Contrasting with the preceding exercise,
explain why this result applies irrespective of whether ν′ has a probability
density.
△
Exercise 5.7. Prove Lemma 5.6 from the deﬁnition of the pushforward
distribution.
△
Exercise 5.8. The naive implementation of Algorithm 5.1 requires O Nk+1
X Nk
R

memory to perform the kth iteration. Describe an implementation that reduces
this cost to O NXNk
R
.
△
Exercise 5.9. Consider a Markov decision process in which the rewards are
normally distributed:
PR(· | x, a) = N µ(x, a), σ2(x, a) ,
for x ∈X, a ∈A .
Suppose that we represent our distributions with the m-categorical representa-
tion, with projection in Cramér distance Πc:
η(x) =
m
X
i=1
pi(x)δθi.
(i) Show that
(ΠcT πη)(x)
=
X
a∈A
X
x′∈X
π(a | x)PX(x′ | x, a)
m
X
i=1
pi(x′) Eπ
ΠcδR+γθi | X = x, A = a] .
(ii) Construct a numerical integration scheme that approximates the terms
Eπ
ΠcδR+γθi | X = x, A = a]
to ε precision on individual probabilities, for any x, a.
(iii) Use this scheme to construct a distributional dynamic programming algo-
rithm for Markov decision processes with normally distributed rewards.
What is its per-iteration computational cost?
(iv) Suppose that the support θ1, . . . , θm is evenly spaced on [θmin, θmax], that
σ2(x, a) = 1 for all x, a, and additionally, µ(x, a) ∈[(1 −γ)θmin, (1 −γ)θmax].
Analogous to the results of Section 5.10, bound the approximation error
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

158
Chapter 5
resulting from this algorithm, as a function of m, ε, the number of iterations
K.
△
Exercise 5.10. Prove that the deterministic projection of Section 3.5, deﬁned
in terms of a map to two neighboring particles, is equivalent to the triangular
kernel formulation presented in Section 5.6.
△
Exercise 5.11. Show that each of the representations of Deﬁnitions 5.11–5.13 is
complete with respect to wp, for p ∈[1, ∞]. That is, for F ∈{FQ,m, FC,m, FE,m},
show that if ν ∈Pp(R) is the limit of a sequence of probability distributions
(νk)k≥0 ∈F, then ν ∈F.
△
Exercise 5.12. Show that the empirical representation FE (Deﬁnition 5.5) is
not complete with respect to the 1-Wasserstein distance. Explain, concretely,
why this causes diﬃculties in deﬁning a distance-based projection onto FE.
△
Exercise 5.13. Explain what happens if the inverse cumulative distribution
function F−1
ν
is not continuous in Lemma 5.16. When does that arise? What are
the implications for the w1-projection onto the m-quantile representation?
△
Exercise 5.14. Implement distributional dynamic programming with the empir-
ical representation in the programming language of your choice. Apply it to the
deterministic Markov decision process depicted in Figure 5.6, for a ﬁnite num-
ber of iterations K, beginning with η0(x) = δ0. Plot the supremum 1-Wasserstein
distance between the iterates ηk and ηπ as a function of k.
△
Exercise 5.15. Prove that the m-categorical representation FC,m is convex and
complete with respect to the Cramér distance ℓ2, for all m ∈N+.
△
Exercise 5.16. Show that the projected Bellman operator, instantiated with the
m-quantile representation FQ,m and the w1-projection, is not a contraction in
w1.
△
Exercise 5.17. Consider the metric space given by the open interval (0, 1)
equipped with the Euclidean metric on the real line, d(x, y) = |x −y|.
(i) Show that this metric space is not complete.
(ii) Construct a simple contraction map on this metric space that has no ﬁxed
point, illustrating the necessity of the condition of completeness in Banach’s
ﬁxed-point theorem.
△
Exercise 5.18. Let p =
√
2
2 , and consider the probability distribution
ν = (1 −p)δ0 + pδ1 .
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Distributional Dynamic Programming
159
Show that, for all m ∈N+, the projection of ν onto the m-quantile representation,
written Πqν, is such that
w∞(Πqν, ν) = 1 .
Comment on the utility of the supremum-w∞metric in analyzing the conver-
gence and the quality of the ﬁxed point of quantile dynamic programming.
△
Exercise 5.19. Consider the Bernoulli representation:
{pδ0 + (1 −p)δ1 : p ∈[0, 1]} ;
this is also the categorical representation with m = 2, θ1 = 0, θ2 = 1. Consider the
distributional dynamic programming obtained by combining this representation
with a w∞-projection.
(i) Describe the projection operator mathematically.
(ii) Consider a two-state, single-action MDP with states x, y with transition
dynamics such that each state transitions immediately to the other and
rewards that are deterministically 0. Show that with γ > 1/2, the distribu-
tional dynamic programming operator deﬁned above is not a contraction
mapping and in particular has multiple ﬁxed points.
△
Exercise 5.20. The purpose of this exercise is to develop a guarantee of the
approximation error incurred by the ﬁxed point ˆηπ
q of the projected distributional
Bellman operator ΠqT π and the m-quantile representation; a version of this
analysis originally appeared in Rowland et al. (2019). Here, we write PB(R)
for the space of probability distributions bounded on [Vmin, Vmax].
(i) For a distribution ν ∈PB(R) and the quantile projection Πq : P1(R) →
FQ,m, show that we have
w1(Πqν, ν) ≤Vmax −Vmin
2m
.
(ii) Hence, using the triangle inequality, show that for any ν, ν′ ∈PB(R), we
have
w1(Πqν, Πqν′) ≤w1(ν, ν′) + Vmax −Vmin
m
.
Thus, while Πq is not an nonexpansion under w1, per Exercise 5.16, it is in
some sense “not far” from satisfying this condition.
(iii) Hence, using the triangle inequality with the return functions ˆηπ
q, Πqηπ, and
ηπ, show that if ηπ ∈PB(R)X, then we have
w1(ˆηπ
q, ηπ) ≤3(Vmax −Vmin)
2m(1 −γ)
.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

160
Chapter 5
(iv) Finally, show that w1(ν, ν′) ≤w∞(ν, ν′) for any ν, ν′ ∈PB(R). Thus, starting
from the observation that
w1(ηk, ηπ) ≤w1(ηk, ˆηπ
q) + w1(ˆηπ
q, ηπ) ,
deduce that
w1(ηk, ηπ) ≤γk(Vmax −Vmin) + 3(Vmax −Vmin)
2m(1 −γ)
.
△
Exercise 5.21. The aim of this exercise is to ﬁll out the details in the reduction
described in Remark 5.2.
(i) Consider an MDP where r(x, a) > 0 is the (deterministic) reward received
from choosing action a in state x (that is, the distribution PR(· | x, a) is a
Dirac delta at r(x, a)). Deduce that if the values r(x, a) are distinct across
state-action pairs and that the discount factor γ satisﬁes
γ <
Rmin
Rmin + Rmax
then there is an injective mapping from trajectories to returns.
(ii) Hence, show that for the class of MDPs described in Remark 5.2, the
trajectories whose returns lie in the set
[
σ∈Sn

n−1
X
t=0
γtσ(t + 1) + γnσ(1),
n−1
X
t=0
γtσ(t + 1) + γn(σ(1) + 1)

are precisely the trajectories whose initial n + 1 states correspond to a
Hamiltonian cycle.
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

6
Incremental Algorithms
The concept of experience is central to reinforcement learning. Methods such
as TD and categorical TD learning iteratively update predictions on the basis of
transitions experienced by interacting with an environment. Such incremental
algorithms are applicable in a wide range of scenarios, including those in
which no model of the environment is known, or in which the model is too
complex to allow for dynamic programming methods to be applied. Incremental
algorithms are also often easier to implement. For these reasons, they are key in
the application of reinforcement learning to many real-world domains.
With this ease of use, however, comes an added complication. In contrast to
dynamic programming algorithms, which steadily make progress toward the
desired goal, there is no guarantee that incremental methods will generate con-
sistently improving estimates of return distributions from iteration to iteration.
For example, an unusually high reward in a sampled transition may actually
lead to a short-term degrading of the value estimate for the corresponding state.
In practice, this requires making suﬃciently small steps with each update, to
average out such variations. In theory, the stochastic nature of incremental
updates makes the analysis substantially more complicated than contraction
mapping arguments.
This chapter takes a closer look at the behavior and design of incremen-
tal algorithms – distributional and not. Using the language of operators and
probability distribution representations, we also formalize what it means for an
incremental algorithm to perform well and discuss how to analyze its asymptotic
convergence to the desired estimate.
6.1
Computation and Statistical Estimation
Iterative policy evaluation ﬁnds an approximation to the value function Vπ by
successively computing the iterates
Vk+1 = T πVk ,
(6.1)
161
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

162
Chapter 6
deﬁned by an arbitrary initial value function estimate V0 ∈RX. We can also
think of temporal-diﬀerence learning as computing an approximation to the
value function Vπ, albeit with a diﬀerent mode of operation. To begin, recall
from Chapter 3 the TD learning update rule:
V(x) ←(1 −α)V(x) + α r + γV(x′) .
(6.2)
One of the aims of this chapter is to study the long-term behavior of the value
function estimate V (and, eventually, of estimates produced by incremental,
distributional algorithms).
At the heart of our analysis is the behavior of a single update. That is, for a
ﬁxed V ∈RX, we may understand the learning dynamics of temporal-diﬀerence
learning by considering the random value function estimate V′ deﬁned via the
sample transition model (X = x, A, R, X′):
V′(x) = (1 −α)V(x) + α R + γV(X′) ,
(6.3)
V′(y) = V(y) ,
y , x .
There is a close connection between the expected eﬀect of the update given by
Equation 6.3 and iterative policy evaluation. Speciﬁcally, the expected value of
the quantity R + γV(X′) precisely corresponds to the application of the Bellman
operator to V, evaluated at the source state x:
Eπ[R + γV(X′) | X = x] = (T πV)(x) .
Consequently, in expectation TD learning adjusts its value function estimate at
x in the direction given by the Bellman operator:
Eπ[V′(x) | X = x] = (1 −α)V(x) + α(T πV)(x) .
(6.4)
To argue that temporal-diﬀerence learning correctly ﬁnds an approximation to
Vπ, we must also be able to account for the random nature of TD updates. An
eﬀective approach is to rewrite Equation 6.3 as the sum of an expected target
and a mean-zero noise term:
V′(x) = (1 −α)V(x) + α

(T πV)(x)
|    {z    }
expected target
+ R + γV(X′) −(T πV)(x)
|                       {z                       }
noise

;
(6.5)
with this decomposition, we may simultaneously analyze the mean dynamics of
TD learning as well as the eﬀect of the noise on the value function estimates. In
the second half of the chapter, we will use Equation 6.5 to establish that under
appropriate conditions, these dynamics can be controlled so as to guarantee
the convergence of temporal-diﬀerence learning to Vπ and analogously the
convergence of categorical temporal-diﬀerence learning to the ﬁxed point ˆηπ
c.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Incremental Algorithms
163
6.2
From Operators to Incremental Algorithms
As illustrated in the preceding section, we can explain the behavior of temporal-
diﬀerence learning (an incremental algorithm) by relating it to the Bellman
operator. New incremental algorithms can also be obtained by following the
reverse process – by deriving an update rule from a given operator. This tech-
nique is particularly eﬀective in distributional reinforcement learning, where
one often needs to implement incremental counterparts to a variety of dynamic
programming algorithms. To describe how one might derive an update rule
from an operator, we now introduce an abstract framework based on what is
known as stochastic approximation theory.45
Let us assume that we are given a contractive operator O over some state-
indexed quantity and that we are interested in determining the ﬁxed point U∗of
this operator. With dynamic programming methods, we obtain an approximation
of U∗by computing the iterates
Uk+1(x) = (OUk)(x) ,
for all x ∈X .
To construct a corresponding incremental algorithm, we must ﬁrst identify what
information is available at each update; this constitutes our sampling model.
For example, in the case of temporal-diﬀerence learning, this is the sample
transition model (X, A, R, X′). For Monte Carlo algorithms, the sampling model
is the random trajectory (Xt, At, Rt)t≥0 (see Exercise 6.1). In the context of this
chapter, we assume that the sampling model takes the form (X, Y), where X is
the source state to be updated, and Y comprises all other information in the
model, which we term the sample experience.
Given a step size α and realizations x and y of the source state variable X and
sample experience Y, respectively, we consider incremental algorithms whose
update rule can be expressed as
U(x) ←(1 −α)U(x) + α ˆO(U, x, y) .
(6.6)
Here, ˆO(U, x, y) is a sample target that may depend on the current estimate
U. Typically, the particular setting we are in also imposes some limitation on
the form of ˆO. For example, when O is the Bellman operator T π, although
ˆO(U, x, y) = (OU)(x) is a valid instantiation of Equation 6.6, its implementation
might require knowledge of the environment’s transition kernel and reward
function. Implicit within Equation 6.6 is the notion that the space that the
estimate U occupies supports a mixing operation; this will indeed be the case
45. Our treatment of incremental algorithms and their relation to stochastic approximation theory is
far from exhaustive; the interested reader is invited to consult the bibliographical remarks.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

164
Chapter 6
for the algorithms we consider in this chapter, which work either with ﬁnite-
dimensional parameter sets or probability distributions themselves.
With this framework in mind, the question is what makes a sensible choice
for ˆO.
Unbiased update. An important case is when the sample target ˆO can be
chosen so that in expectation, it corresponds to the application of the operator
O:
E[ ˆO(U, X, Y) | X = x] = (OU)(x) .
(6.7)
In general, when Equation 6.7 holds, the resulting incremental algorithm is
also well behaved. More formally, we will see that under reasonable conditions,
the estimates produced by such an algorithm are guaranteed to converge to
U∗– a generalization of our earlier statement that temporal-diﬀerence learning
converges to Vπ.
Conversely, when the operator O can be expressed as an expectation over
some function of U, X, and Y, then it is possible to derive a sample target simply
by substituting the random variables involved with their realizations. In eﬀect,
we then use the sample experience to construct an unbiased estimate of (OU)(x).
As a concrete example, the TD target, expressed in terms of random variables,
is
ˆO(V, X, Y) = R + γV(X′) ;
the corresponding update rule is
V(x) ←(1 −α)V(x) + α (r + γV(x′))
|        {z        }
sample target
.
In the next section, we will show how to use this approach to derive categorical
temporal-diﬀerence learning (introduced in Chapter 3) from the categorical-
projected Bellman operator.
Example 6.1. The consistent Bellman operator is an operator over state-action
value functions based on the idea of making consistent choices at each state. At
a high level, the consistent operator adds the constraint that actions that leave
the state unchanged should be repeated. This operator is formalized as
T π
c Q(x, a) = Eπ
R + γ max
a′∈A Q(X′, a′)1{X′ , x} + γQ(x, a)1{X′ = x} | X = x .
Let (x, a, r, x′) be drawn according to the sample transition model. The update
rule derived by substitution is
Q(x, a) ←

(1 −α)Q(x, a) + α r + γ maxa′∈A Q(x′, a′)
if x′ , x
(1 −α)Q(x, a) + α(r + γQ(x, a))
otherwise.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Incremental Algorithms
165
Compared to Q-learning (Section 3.7), the consistent update rule increases the
action gap at each state, in the sense that its operator’s ﬁxed point Q∗
c has the
property that for all (x, a) ∈X × A,
max
a′∈A Q∗
c(x, a′) −Q∗
c(x, a) ≥max
a′∈A Q∗(x, a′) −Q∗(x, a) ,
with strict inequality whenever PX(x | x, a) > 0.
△
A general principle. Sometimes, expressing the operator O in the form of
Equation 6.7 requires information that is not available to our sampling model.
In this case, it is sometimes still possible to construct an update rule whose
repeated application approximates the operator O. More precisely, given a ﬁxed
estimate ˜U, with this approach we look for a sample target function ˆO such that
from a suitable initial condition, repeated updates of the form
U(x) ←(1 −α)U(x) + α ˆO( ˜U, x, y)
lead to U ≈O ˜U. In this case, a necessary condition for ˆO to be a suitable sample
target is that it should leave the ﬁxed point U∗unchanged, in expectation:
Eπ[ ˆO(U∗, X, Y) | X = x] = U∗(x) .
In Section 6.4, we will introduce quantile temporal-diﬀerence learning, an
algorithm that applies this principle to ﬁnd the ﬁxed point of the quantile-
projected Bellman operator.
6.3
Categorical Temporal-Diﬀerence Learning
Categorical dynamic programming (CDP) computes a sequence (ηk)k≥0 of
return-distribution functions, deﬁned by iteratively applying the projected dis-
tributional Bellman operator ΠcT π to an initial return-distribution function
η0:
ηk+1 = ΠcT πηk .
As we established in Section 5.9, the sequence generated by CDP converges
to the ﬁxed point ˆηπ
c. Let us express this ﬁxed point in terms of a collection of
probabilities  (pπ
i (x))m
i=1 : x ∈X associated with m particles located at θ1, . . . , θm:
ˆηπ
c(x) =
m
X
i=1
pπ
i (x)δθi .
To derive an incremental algorithm from the categorical-projection Bellman
operator, let us begin by expressing the projected distributional operator ΠcT π
in terms of an expectation over the sample transition (X = x, A, R, X′):
 ΠcT πη(x) = Πc Eπ
h bR,γ

#ηπ(X′) | X = x
i
.
(6.8)
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

166
Chapter 6
Following the line of reasoning from Section 6.2, in order to construct an
unbiased sample target by substituting R and X′ with their realizations, we
need to rewrite Equation 6.8 with the expectation outside of the projection Πc.
The following establishes the validity of exchanging the order of these two
operations.
Proposition 6.2. Let η ∈F X
C,m be a return function based on the m-
categorical representation. Then for each state x ∈X,
(ΠcT πη(x) = Eπ
Πc
 (bR,γ)#η(X′) | X = x .
△
Proposition 6.2 establishes that the projected operator ΠcT π can be written
in such a way that the substitution of random variables with their realizations
can be performed. Consequently, we deduce that the random sample target
ˆO η, x, (R, X′) = Πc(bR,γ)#η(X′)
provides an unbiased estimate of (ΠcT πη)(x). For a given realization (x, a, r, x′)
of the sample transition, this leads to the update rule46
η(x) ←(1 −α)η(x) + αΠc
 (br,γ)#η(x′)
|         {z         }
sample target
.
(6.9)
The last part of the CTD derivation is to express Equation 6.9 in terms of
the actual parameters being updated. These parameters are the probabilities
 (pi(x))m
i=1 : x ∈X of the return-distribution function estimate η:
η(x) =
m
X
i=1
pi(x)δθi .
The sample target in Equation 6.9 is given by the pushforward transformation of
a m-categorical distribution (η(x′)) followed by a categorical projection. As we
demonstrated in Section 3.6, the projection of a such a transformed distribution
can be expressed concisely from a set of coeﬃcients (ζi, j(r) : i, j ∈{1, . . . , m}).
In terms of the triangular and half-triangular kernels (hi)m
i=1 that deﬁne the
categorical projection (Section 5.6), these coeﬃcients are
ζi,j(r) = hi
 ς−1
m (r + γθ j −θi) .
(6.10)
With these coeﬃcients, the update rule over the probability parameters is
pi(x) ←(1 −α)pi(x) + α
m
X
j=1
ζi,j(r)p j(x′) .
46. Although the action a is not needed to construct the sample target, we include it for consistency.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Incremental Algorithms
167
Our derivation illustrates how substituting random variables for their real-
izations directly leads to an incremental algorithm, provided we have the right
operator to begin with. In many situations, this is simpler than the step-by-
step process that we originally followed in Chapter 3. Because the random
sample target is an unbiased estimate of the projected Bellman operator, it
is also simpler to prove its convergence to the ﬁxed point ˆηπ
c; in the second
half of this chapter, we will in fact apply the same technique to analyze both
temporal-diﬀerence learning and CTD.
Proof of Proposition 6.2. For a given r ∈R, x′ ∈X, let us write
˜η(r, x′) = (br,γ)#η(x′) .
Fix x ∈X. For conciseness, let us deﬁne, for z ∈R,
˜hi(z) = hi
 ς−1
m (z −θi) .
With this notation, we have
Eπ
Πc
 (bR,γ)#η(X′) | X = x (a)= Eπ
h
m
X
i=1
δθi
E
Z∼˜η(R,X′)[˜hi(Z)] | X = x
i
=
m
X
i=1
δθi Eπ
h
E
Z∼˜η(R,X′)[˜hi(Z)] | X = x
i
(b)=
m
X
i=1
δθi
E
Z′∼(T πη)(x)
˜hi(Z′)
= Πc(T πη)(x) ,
where (a) follows by deﬁnition of the categorical projection in terms of the
triangular and half-triangular kernels (hi)m
i=1 and (b) follows by noting that
if the conditional distribution of R + γG(X′) (where G is an instantiation of
η independent of the sample transition (x, A, R, X′)) given R, X′ is ˜η(R, X′) =
(bR,γ)#η(X′), then the unconditional distribution of G when X = x is (T πη)(x).
6.4
Quantile Temporal-Diﬀerence Learning
Quantile regression is a method for determining the quantiles of a probability
distribution incrementally and from samples.47 In this section, we develop an
47. More precisely, quantile regression is the problem of estimating a predetermined set of quantiles
of a collection of probability distributions. By extension, in this book, we also use “quantile
regression” to refer to the incremental method that solves this problem.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

168
Chapter 6
algorithm that aims to ﬁnd the ﬁxed point ˆηπ
q of the quantile-projected Bellman
operator ΠqT π via quantile regression.
To begin, suppose that given τ ∈(0, 1), we are interested in estimating the
τth quantile of a distribution ν, corresponding to F−1
ν (τ). Quantile regression
maintains an estimate θ of this quantile. Given a sample z drawn from ν, it
adjusts θ according to
θ ←θ + α(τ −1{z < θ}) .
(6.11)
One can show that quantile regression follows the negative gradient of the
quantile loss48
Lτ(θ) = (τ −1{z < θ})(z −θ)
= |1{z < θ} −τ| × |z −θ| .
(6.12)
In Equation 6.12, the term |1{z < θ} −τ| is an asymmetric step size that is either
τ or 1 −τ, according to whether the sample z is greater or smaller than θ,
respectively. When τ < 0.5, samples greater than θ have a lesser eﬀect on it
than samples smaller than θ; the eﬀect is reversed when τ > 0.5. The update
rule in Equation 6.11 will continue to adjust the estimate until the equilibrium
point θ∗is reached (Exercise 6.4 asks you to visualize the behavior of quantile
regression with diﬀerent distributions). This equilibrium point is the location at
which smaller and larger samples have an equal eﬀect in expectation. At that
point, letting Z ∼ν, we have
0 = E τ −1{Z < θ∗}

= τ −E 1{Z < θ∗}

= τ −P(Z < θ∗)
=⇒P(Z < θ∗) = τ
=⇒θ∗= F−1
ν (τ) .
(6.13)
For ease of exposition, in the ﬁnal line we assumed that there is a unique z ∈R
for which Fν(z) = τ; Remark 6.1 discusses the general case.
Now, let us consider applying quantile regression to ﬁnd a m-quantile approx-
imation to the return-distribution function (ideally, the ﬁxed point ˆηπ
q). Recall
that a m-quantile return-distribution function η ∈F X
Q,m is parameterized by the
locations  (θi(x))m
i=1 : x ∈X:
η(x) = 1
m
m
X
i=1
δθi(x).
48. More precisely, Equation 6.11 updates θ in the direction of the negative gradient of Lτ provided
that PZ∼ν(Z = θ) = 0. This holds trivially if ν is a continuous probability distribution.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Incremental Algorithms
169
Now, the quantile projection Πqν of a probability distribution ν is given by
Πqν = 1
m
m
X
i=1
δF−1
ν (τi) ,
τi = 2i −1
2m
for i = 1, …, m .
Given a source state x ∈X, the general idea is to perform quantile regression for
all location parameters (θi)m
i=1 simultaneously, using the quantile levels (τi)m
i=1
and samples drawn from (T πη)(x). To this end, let us momentarily introduce a
random variable J uniformly distributed on {1, . . . , m}. By Proposition 4.11, we
have
Dπ
 R + γθJ(X′) | X = x = (T πη)(x) .
(6.14)
Given a realized transition (x, a, r, x′), we may therefore construct m sample
targets  r + γθ j(x′)m
j=1. Applying Equation 6.11 to these targets leads to the
update rule
θi(x) ←θi(x) + α
m
m
X
j=1

τi −1{r + γθ j(x′) < θi(x)}

,
i = 1, . . . m .
(6.15)
This is the quantile temporal-diﬀerence learning (QTD) algorithm. A concrete
instantiation in the online case is summarized by Algorithm 6.1, by analogy with
the presentation of categorical temporal-diﬀerence learning in Algorithm 3.4.
Note that applying Equation 6.15 requires computing a total of m2 terms per
location; when m is large, an alternative is to instead use a single term from
the sum in Equation 6.15, with j sampled uniformly at random. Interestingly
enough, for m suﬃciently small, the per-step cost of QTD is less than the cost
of sorting the full distribution (T πη)(x) (which has up to NXNRm particles).
This suggests that the quantile regression approach to the projection step may
be useful even in the context of distributional dynamic programming.
The use of quantile regression to derive QTD can be seen as an instance of
the principle introduced at the end of Section 6.2. Suppose that we consider an
initial return function
η0(x) = 1
m
m
X
j=1
δθ0
j(x) .
If we substitute the sample target in Equation 6.15 by a target constructed from
this initial return function, we obtain the update rule
θi(x) ←θi(x) + α
m
m
X
j=1

τi −1{r + γθ0
j(x′) < θi(x)}

,
i = 1, . . . m .
(6.16)
By inspection, we see that Equation 6.16 corresponds to quantile regression
applied to the problem of determining, for each state x ∈X, the quantiles of
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

170
Chapter 6
Algorithm 6.1: Online quantile temporal-diﬀerence learning
Algorithm parameters:
step size α ∈(0, 1],
policy π : X →P(A),
number of quantiles m,
initial locations  (θ0
i (x))m
i=1 : x ∈X
θi(x) ←θ0
i (x) for i = 1, . . . , m, x ∈X
τi ←2i−1
2m for i = 1, . . . , m
Loop for each episode:
Observe initial state x0
Loop for t = 0, 1, . . .
Draw at from π(· | xt)
Take action at, observe rt, xt+1
for i = 1, …, m do
θ′
i ←θi(xt)
for j = 1, …, m do
if xt+1 is terminal then
g ←rt
else
g ←rt + γθ j(xt+1)
θ′
i ←θ′
i + α
m
 τi −1{g < θi(xt)}
end for
end for
for i = 1, …, m do
θi(xt) ←θ′
i
end for
until xt+1 is terminal
end
the distribution (T πη0)(x). Consequently, one may think of quantile temporal-
diﬀerence learning as performing an update that would converge to the quantiles
of the target distribution, if that distribution were held ﬁxed.
Based on this observation, we can verify that QTD is a reasonable distribu-
tional reinforcement learning algorithm by considering its behavior at the ﬁxed
point
ˆηπ
q = ΠqT π ˆηπ
q ,
the solution found by quantile dynamic programming. Let us denote the param-
eters of this return function by ˆθπ
i (x), for i = 1, . . . , m and x ∈X. For a given
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Incremental Algorithms
171
state x, consider the intermediate target
˜η(x) =  T π ˆηπ
q
(x) .
Now, by deﬁnition of the quantile projection operator, we have
ˆθπ
i (x) = F−1
˜η(x)
 2i−1
2m

.
However, by Equation 6.13, we also know that the quantile regression update
rule applied at ˆθπ
i (x) with τi = 2i−1
2m leaves the parameter unchanged in expec-
tation. In other words, the collection of locations  ˆθπ
i (x)m
i=1 is a ﬁxed point of
the expected quantile regression update, and consequently the return function
ˆηπ
q is a solution of the quantile temporal-diﬀerence learning algorithm. This
gives some intuition that is it indeed a valid learning rule for distributional
reinforcement learning with quantile representations.
Before concluding, it is useful to illustrate why the straightforward approach
taken to derive categorical temporal-diﬀerence learning, based on unbiased
operator estimation, cannot be applied to the quantile setting. Recall that the
quantile-projected operator takes the form
 ΠqT πη(x) = Πq Eπ
h bR,γ

#ηπ(X′) | X = x
i
.
(6.17)
As the following example shows, exchanging the expectation and projection
results in a diﬀerent operator, one whose ﬁxed point is not ˆηπ
q. Consequently,
we cannot substitute random variables for their realizations, as was done in the
categorical setting.
Example 6.3. Consider an MDP with a single state x, single action a, transition
dynamics so that x transitions back to itself, and immediate reward distribution
N(0, 1). Given η(x) = δ0, we have (T πη)(x) = N(0, 1), and hence the projec-
tion via Πq onto FQ,m with m = 1 returns a Dirac delta on the median of this
distribution: δ0.
In contrast, the sample target (bR,γ)#η(X′) is δR, and so the projection of this
target via Πq remains δR. We therefore have
Eπ[Πq(bR,γ)#η)(X′) | X = x] = Eπ[δR | X = x] = N(0, 1) ,
which is distinct from the result of the projected operator, (ΠqT πη)(x) = δ0.
△
6.5
An Algorithmic Template for Theoretical Analysis
In the second half of this chapter, we present a theoretical analysis of a class of
incremental algorithms that includes the incremental Monte Carlo algorithm
(see Exercise 6.9), temporal-diﬀerence learning, and the CTD algorithm. This
analysis builds on the contraction mapping theory developed in Chapter 4 but
also accounts for the randomness introduced by the use of sample targets in
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

172
Chapter 6
the update rule, via stochastic approximation theory. Compared to the analysis
of dynamic programming algorithms, the main technical challenge lies in
characterizing the eﬀect of this randomness on the learning process.
To begin, let us view the output of the temporal-diﬀerence learning algorithm
after k updates as a value function estimate Vk. Extending the discussion from
Section 6.1, this estimate is a random quantity because it depends on the
particular sample transitions observed by the agent and possibly the randomness
in the agent’s choices.49 We are particularly interested in the sequence of random
estimates (Vk)k≥0. From an initial estimate V0, this sequence is formally deﬁned
as
Vk+1(Xk) = (1 −αk)Vk(Xk) + αk
 Rk + γVk(X′
k)
Vk+1(x) = Vk(x)
if x , Xk ,
where (Xk, Ak, Rk, X′
k)k≥0 is the sequence of random transitions used to calculate
the TD updates. In our analysis, the object of interest is the limiting point of this
sequence, and we seek to answer the question: does the algorithm’s estimate
converge to the value function Vπ? We consider the limiting point because any
single update may or may not improve the accuracy of the estimate Vk at the
source state Xk. We will show that, under the right conditions, the sequence
(Vk)k≥0 converges to Vπ. That is,
lim
k→∞
Vk(x) −Vπ(x)
 = 0,
for all x ∈X .
More precisely, the above holds with probability 1: with overwhelming odds,
the variables X0, R0, X′
0, X1, . . . are drawn in such a way that Vk →Vπ.50
We will prove a more general result that holds for a family of incremental
algorithms whose sequence of estimates can be expressed by the template
Uk+1(Xk) = (1 −αk)Uk(Xk) + αk ˆO(Uk, Xk, Yk)
Uk+1(x) = Uk(x)
if x , Xk .
(6.18)
Here, Xk is the (possibly random) source state at time k, ˆO(Uk, Xk, Yk) is the
sample target, and αk is an (also possibly random) step size. As in Section 6.2,
the sample experience Yk describes the collection of random variables used
to construct the sample target: for example, a sample trajectory or a sample
transition (Xk, Ak, Rk, and X′
k).
Under this template, the estimate Uk describes the collection of variables
maintained by the algorithm and constitutes its “prediction.” More speciﬁcally,
49. In this context, we even allow the step size αk to be random.
50. Put negatively, there may be realizations of X0, R0, X′
0, X1, . . . for which the sequence (Vk)k≥0
does not converge, but the set of such realizations has zero probability.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Incremental Algorithms
173
it is a state-indexed collection of m-dimensional real-valued vectors, written
Uk ∈RX×m. In the case of the TD algorithm, m = 1 and Uk = Vk.
We assume that there is an operator O : RX×m →RX×m whose unique ﬁxed
point is the quantity to be estimated by the incremental algorithm. If we denote
this ﬁxed point by U∗, this implies that
OU∗= U∗.
We further assume the existence of a base norm ∥· ∥over Rm, extended to the
space of estimates according to
∥U∥∞= sup
x∈X
∥U(x)∥,
such that O is a contraction mapping of modulus β with respect to the metric
induced by ∥· ∥∞. For TD learning, O = T π and the base norm is simply the
absolute value; the contractivity of T π was established by Proposition 4.4.
Within this template, there is some freedom in how the source state Xk is
selected. Formally, Xk is assumed to be drawn from a time-varying distribution
ξk that may depend on all previously observed random variables up to but
excluding time k, as well as the initial estimate U0. That is,
Xk ∼ξk
 X0:k−1, Y0:k−1, α0:k−1, U0
 .
This includes scenarios in which source states are drawn from a ﬁxed distribu-
tion ξ ∈P(X), enumerated in a round-robin manner, or selected in proportion
to the magnitude of preceding updates (called prioritized replay; see Moore and
Atkeson 1993; Schaul et al. 2016). It also accounts for the situation in which
states are sequentially updated along a sampled trajectory, as is typical of online
algorithms.
We further assume that the sample target is an unbiased estimate of the
operator O applied to Uk and evaluated at Xk. That is, for all x ∈X for which
P(Xk = x) > 0,
E  ˆO(Uk, Xk, Yk) | X0:k−1, Y0:k−1, α0:k−1, U0, Xk = x] = (OUk)(x) .
This implies that Equation 6.18 can be expressed in terms of a mean-zero noise
wk, similar to our derivation in Section 6.1:
Uk+1(Xk) = (1 −αk)Uk(Xk) + αk
h
(OUk)(Xk) + ( ˆO(Uk, Xk, Yk) −(OUk)(Xk)
|                             {z                             }
wk
i
.
Because wk is zero in expectation, this assumption guarantees that, on average,
the incremental algorithm must make progress toward the ﬁxed point U∗. That
is, if we ﬁx the source state Xk = x and step size αk, then
E Uk+1(x) | X0:k−1, Y0:k−1, α0:k−1, Xk = x, αk]
(6.19)
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

174
Chapter 6
= (1 −αk)Uk(x) + αk E (OUk)(x) + wk | Xk = x
= (1 −αk)Uk(x) + αk(OUk)(x) .
By choosing an appropriate sequence of step sizes (αk)k≥0 and under a few
additional technical conditions, we can in fact provide the stronger guarantee
that the sequence of iterates (Uk)k≥0 converges to U∗w.p. 1, as the next section
illustrates.
6.6
The Right Step Sizes
To understand the role of step sizes in the learning process, consider an abstract
algorithm described by Equation 6.18 and for which
ˆO(Uk, Xk, Yk) = (OUk)(Xk) .
In this case, the noise term wk is always zero and can be ignored: the abstract
algorithm adjusts its estimate directly toward OUk. Here we should take the
step sizes (αk)k≥0 to be large in order to make maximal progress toward U∗. For
αk = 1, we obtain a kind of dynamic programming algorithm that updates its
estimate one state at a time and whose convergence to U∗can be reasonably
easily demonstrated; conversely, taking αk < 1 must in some sense slow down
the learning process.
In general, however, the noise term is not zero and cannot be neglected. In
this case, large step sizes amplify wk and prevent the algorithm from converg-
ing to U∗(consider, in the extreme, what happens when αk = 1). A suitable
choice of step size must therefore balance rapid learning progress and eventual
convergence to the right solution.
To illustrate what “suitable choice” might mean in practice, let us distill the
issue down to its essence and consider the process that estimates the mean of a
distribution ν ∈P1(R) according to the incremental update
Vk+1 = (1 −αk)Vk + αkRk ,
(6.20)
where (Rk)k≥0 are i.i.d. random variables distributed according to ν. For concrete-
ness, let us assume that ν = N(0, 1), so that we would like (Vk)k≥0 to converge
to 0.
Suppose that the initial estimate is V0 = 0 (the desired solution) and consider
three step size schedules: αk = 0.1, αk = 1/k+1, and αk = 1/(k+1)2. Figure 6.1 illus-
trates the sequences of estimates obtained by applying the incremental update
with each of these schedules and a single, shared sequence of realizations of the
random variables (Rk)k≥0.
The 1/k+1 schedule corresponds to the right step size schedule for the incre-
mental Monte Carlo algorithm (Section 3.2), and accordingly, we observe that it
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Incremental Algorithms
175
0
200
400
600
800
1000
Iteration
0.50
0.25
0.00
0.25
0.50
0.75
Estimated value
= 0.1
= 1/(k + 1)
= 1/(k + 1)2
Figure 6.1
The behavior of a simple incremental update rule 6.20 for estimating the expected value
of a normal distribution. Diﬀerent curves represent the sequence of estimates obtained
from diﬀerent step size schedules. The ground truth (V = 0) is indicated by the dashed
line.
is converging to the correct expected value.51 By contrast, the constant schedule
continues to exhibit variations over time, as the noise is not suﬃciently averaged
out. The quadratic schedule (1/(k+1)2) decreases too quickly and the algorithm
settles on an incorrect prediction.
To prove the convergence of algorithms that ﬁt the template described in
Section 6.5, we will require that the sequence of step sizes satisﬁes the Robbins–
Monro conditions (Robbins and Monro 1951). These conditions formalize the
range of step sizes that are neither too small nor too large and hence guarantee
that the algorithm must eventually ﬁnd the solution U∗. As with the source state
Xk, the step size αk at a given time k may be random, and its distribution may
depend on Xk, X0:k−1, α0:k−1, and Y0:k−1 but not the sample experience Yk. As in
the previous section, these conditions should hold with probability 1.
Condition 1: not too small. In the example above, taking αk = 1/(k+1)2 results
in premature convergence of the estimate (to the wrong solution). This is because
when the step sizes decay too quickly, the updates made by the algorithm may
not be of large enough magnitude to reach the ﬁxed point of interest. To avoid
this situation, we require that (αk)k≥0 satisfy
X
k≥0
αk1{Xk = x} = ∞,
for all x ∈X .
Implicit in this assumption is also the idea that every state should be updated
inﬁnitely often. This assumption is violated, for example, if there is a state x
51. In our example, Vk is the average of k i.i.d. normal random variables and is itself normally
distributed. Its standard deviation can be computed analytically and is equal to 1/
√
k (k ≥1). This
implies that after k = 1000 iterations, we expect Vk to be in the range ±3 × 1/
√
k = ±0.111, because
99.7 percent of a normal random variable’s probability is within three standard deviations of its
mean. Compare with Figure 6.1.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

176
Chapter 6
and time K after which Xk , x, for all k ≥K. For a reasonably well-behaved
distribution of source states, this condition is satisﬁed for constant step sizes,
including αk = 1: in the absence of noise, it is possible to make rapid progress
toward the ﬁxed point. On the other hand, it disallows αk = 1/(k+1)2, since
∞
X
k=0
1
(k + 1)2 = π2
6 < ∞.
Condition 2: not too large. Figure 6.1 illustrates how, with a constant step
size and in the presence of noise, the estimate Uk(x) continues to vary substan-
tially over time. To avoid this issue, the step size should be decreased so that
individual updates result in progressively smaller changes in the estimate. To
achieve this, the second requirement on the step size sequence (αk)k≥0 is
X
k≥0
α2
k1{Xk = x} < ∞,
for all x ∈X .
In reinforcement learning, a simple step size schedule that satisﬁes both of
these conditions is
αk =
1
Nk(Xk) + 1,
(6.21)
where Nk(x) is the number of updates to a state x up to but not including
algorithm time k. We encountered this schedule in Section 3.2 when deriving
the incremental Monte Carlo algorithm. As will be shown in the following
sections, this schedule is also suﬃcient for the convergence of TD and CTD
algorithms.52 Exercise 6.7 asks you to verify that Equation 6.21 satisﬁes the
Robbins–Monro conditions and investigates other step size sequences that also
satisfy these conditions.
6.7
Overview of Convergence Analysis
Provided that an incremental algorithm satisﬁes the template laid out in Section
6.5, with a step size schedule that satisﬁes the Robbins–Monro conditions,
we can prove that the sequence of estimates produced by this algorithm must
converge to the ﬁxed point U∗of the implied operator O. Before presenting the
proof in detail, we illustrate the main bounding-box argument underlying the
proof.
Let us consider a two-dimensional state space X = {x1, x2} and an incre-
mental algorithm for estimating a 1-dimensional quantity (m = 1). As per the
template, we consider a contractive operator O : RX →RX given by OU =
52. Because the process of bootstrapping constructs sample targets that are not in general unbiased
with regards to the value function Vπ, the optimal step size schedule for TD learning decreases at a
rate that is slower than 1/k. See bibliographical remarks.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Incremental Algorithms
177
 0.8U(x2), 0.8U(x2); note that the ﬁxed point of O is U∗= (0, 0). At each
time step, a source state (x1 or x2) is chosen uniformly at random and the
corresponding estimate is updated. The step sizes are αk = (k + 1)−0.7, satisfying
the Robbins–Monro conditions.
Suppose ﬁrst that the sample target is noiseless. That is,
ˆO(Uk, Xk, Yk) = 0.8Uk(x2).
In this case, each iteration of the algorithm contracts along a particular
coordinate. Figure 6.2a illustrates a sequence (Uk)k≥0 deﬁned by the update
equations
Uk+1(Xk) = (1 −αk)Uk(Xk) + αk ˆO(Xk, Uk, Yk)
Uk+1(x) = Uk(x),
x , Xk .
As shown in the ﬁgure, the algorithm makes steady (if not direct) progress
toward the ﬁxed point with each update. To prove that (Uk)k≥0 converges to
U∗, we ﬁrst show that the error ∥Uk −U∗∥∞is bounded by a ﬁxed quantity
for all k ≥0 (indicated by the outermost dashed-line square around the ﬁxed
point U∗= 0 in Figure 6.2a). The argument then proceeds inductively: if Uk lies
within a given radius of the ﬁxed point for all k greater than some K, then there
is some K′ ≥K for which, for all k ≥K′, it must lie within the next smallest
dashed-line square. We will see that this follows by contractivity of O and the
ﬁrst Robbins–Monro condition. Provided that the diameter of these squares
shrinks to zero, then this establishes convergence of Uk to U∗.
Now consider adding noise to the sample target, such that
ˆO(Uk, Xk, Yk) = 0.8Uk(y) + wk .
For concreteness, let us take wk to be an independent random variable with dis-
tribution U([−1, 1]). In this case, the behavior of the sequence (Uk)k≥0 is more
complicated (Figure 6.2b). The sequence (Uk)k≥0 no longer follows a neat path
to the ﬁxed point but can behave somewhat more erratically. Nevertheless, the
long-term behavior exhibited by the algorithm bears similarity to the noiseless
case: overall, progress is made toward the ﬁxed point U∗.
The proof of convergence follows the same pattern as for the noiseless case:
prove inductively that if ∥Uk −U∗∥∞is eventually bounded by some ﬁxed
quantity Bl ∈R, then ∥Uk −U∗∥∞is eventually bounded by a smaller quantity
Bl+1. As in the noiseless case, this argument is depicted by the concentric squares
in Figure 6.2c. Again, if these diameters shrink to zero, this also establishes
convergence of Uk to U∗.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

178
Chapter 6
Because noise can increase the error between the estimate Uk and the ﬁxed
point U∗at any given time step, to guarantee convergence we need to pro-
gressively decrease the step size αk. The second Robbins–Monro condition is
suﬃcient for this purpose, and with it the inductive step can be proven with a
more delicate argument. One additional challenge is that the base case (that
supk≥0 ∥Uk −U∗∥∞< ∞) is no longer immediate; a separate argument is required
to establish this fact. This property is called the stability of the sequence (Uk)k≥0
and is often one of the harder aspects of the proof of convergence of incremental
algorithms.
We conclude this section with a result that is crucial in understanding the
inﬂuence of noise in the algorithm. In the analysis carried out in this chapter, it
is the only result whose proof requires tools from advanced probability theory.53
Proposition 6.4. Let (Zk)k≥0 be a sequence of random variables taking
values in Rm and (αk)k≥0 be a collection of step sizes. Given ¯Z0 = 0, consider
the sequence deﬁned by
¯Zk+1 = (1 −αk) ¯Zk + αkZk .
Suppose that the following conditions hold with probability 1:
E[Zk | Z0:k−1, α0:k] = 0 , sup
k≥0
E[∥Zk∥2 | Z0:k−1, α0:k] < ∞,
∞
X
k=0
αk = ∞,
∞
X
k=0
α2
k < ∞.
Then ¯Zk →0 with probability 1.
△
The proof is given in Remark 6.2; here, we provide some intuition that can
be gleaned without consulting the proof.
First, parallels can be drawn with the strong law of large numbers. Expanding
the deﬁnition of ¯Zk yields
¯Zk = (1 −αk) · · · (1 −α1)α0Z0 + (1 −αk) · · · (1 −α2)α1Z1 + · · · + αkZk .
Thus, ¯Zk is a weighted average of the mean-zero terms (Zl)k
l=0. If αk = 1/k+1, then
we obtain the usual uniformly weighted average that appears in the strong law of
large numbers. We also note that unlike the standard strong law of large numbers,
the noise terms (Zl)k
l=0 are not necessarily independent. Nevertheless, it seems
reasonable that this sequence should exhibit similar behavior to the averages that
appear in the strong law of large numbers. This also provides further intuition
53. Speciﬁcally, the supermartingale convergence theorem; the result is a special case of the
Robbins–Siegmund theorem (Robbins and Siegmund 1971).
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Incremental Algorithms
179
(a)
(b)
(c)
1
0
1
U(x1)
1.0
0.5
0.0
0.5
1.0
U(x2)
1
0
1
U(x1)
1.0
0.5
0.0
0.5
1.0
U(x2)
0.0
0.5
1.0
U(x1)
0.0
0.5
1.0
U(x2)
Figure 6.2
(a) Example behavior of the iterates (Uk)k≥0 in the noiseless case. The shading indicates
the iteration number from light (k = 0) through to dark (k = 1000). (b) Example behavior
of the iterates (Uk)k≥0 in the general case. (c) Behavior of iterates for ten random seeds,
with the noiseless (expected) behavior overlaid.
for the conditions of Proposition 6.4. If the variance of individual noise terms
αkZk is too great, the weighted average may not “settle down” as the number of
terms increases. Similarly, if P∞
k=0 αk is too small, the initial noise term Z0 will
have too large an inﬂuence over the weighted average, even as k →∞.
Second, for readers familiar with stochastic gradient descent, we can rewrite
the update scheme as
¯Zk+1 = ¯Zk + αk(−¯Zk + Zk) .
This is a stochastic gradient update for the loss function L( ¯Zk) = 1/2∥¯Zk∥2 (the
minimizer of which is the origin). The negative gradient of this loss is −¯Zk, Zk
is a mean-zero perturbation of this gradient, and αk is the step size used in the
update. Proposition 6.4 can therefore be interpreted as stating that stochastic
gradient descent on this speciﬁc loss function converges to the origin, under
the required conditions on the step sizes and noise. It is perhaps surprising
that understanding the behavior of stochastic gradient descent in this speciﬁc
setting is enough to understand the general class of algorithms expressed by
Equation 6.18.
6.8
Convergence of Incremental Algorithms*
We now provide a formal run-through of the arguments of the previous section
and explain how they apply to temporal-diﬀerence learning algorithms. We
begin by introducing some notation to simplify the argument. We ﬁrst deﬁne a
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

180
Chapter 6
per-state step size that incorporates the choice of source state Xk:
αk(x) =

αk
if x = Xk
0
otherwise,
wk(x) =

ˆO(Uk, Xk, Yk) −(OUk)(Xk)
if x = Xk
0
otherwise.
This allows us to recursively deﬁne (Uk)k≥0 in a single equation:
Uk+1(x) = (1 −αk(x))Uk(x) + αk(x) (OUk)(x) + wk(x) .
(6.22)
Equation 6.22 encapsulates all of the random variables – (Xk)k≥0, (Yk)k≥0, (αk)k≥0
– which together determine the sequence of estimates (Uk)k≥0.
It is useful to separate the eﬀects of the noise into two separate cases: one
in which the noise has been “processed” by an application of the contractive
mapping O and one in which the noise has not been passed through this mapping.
To this end, we introduce the cumulative external noise vectors (Wk(x) : k ≥
0, x ∈X). These are random vectors, with each Wk(x) taking values in Rm,
deﬁned by
W0(x) = 0 ,
Wk+1(x) = (1 −αk(x))Wk(x) + αk(x)wk(x) .
We also introduce the sigma-algebras Fk = σ(X0:k, α0:k, Y0:k−1); these encode
the information available to the learning agent just prior to sampling Yk and
applying the update rule to produce Uk+1.
We now list several assumptions we will require of the algorithm to establish
the convergence result. Recall that ∥· ∥is the base norm identiﬁed in Section
6.5, which gives rise to the supremum extension ∥· ∥∞. In particular, we assume
that O is a β-contraction mapping in the metric induced by ∥· ∥∞.
Assumption 6.5 (Robbins–Monro conditions). For each x ∈X, the step sizes
(αk(x))k≥0 satisfy P
k≥0 αk(x) = ∞and P
k≥0 αk(x)2 < ∞with probability 1.
△
The second assumption encompasses the mean-zero condition described in
Section 6.5 and introduces an additional condition that the variance of this noise,
conditional on the state of the algorithm, does not grow too quickly.
Assumption 6.6. The noise terms (wk(x) : k ≥0, x ∈X) satisfy E[wk(x) | Fk] =
0 with probability 1, and E[∥wk(x)∥2 | Fk] ≤C1 +C2∥Uk∥2
∞w.p. 1, for some
constants C1,C2 ≥0, for all x ∈X and k ≥0.
△
We would like to use Proposition 6.4 to show that the cumulative external
noise (Wk(x))k≥0 is well behaved, and then use this intermediate result to estab-
lish the convergence of the sequence (Uk)k≥0 itself. The proposition is almost
applicable to the sequence (Wk(x))k≥0. The diﬃculty is that the proposition
stipulates that the individual noise terms Zk have bounded variance, whereas
Assumption 6.6 only bounds the conditional expectation of ∥wk(x)∥2 in terms of
∥Uk∥2
∞, which a priori may be unbounded. Unfortunately, in temporal-diﬀerence
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Incremental Algorithms
181
learning algorithms, the update variance typically does scale with the magnitude
of current estimates, so this is not an assumption that we can weaken. To get
around this diﬃculty, we ﬁrst establish the boundedness of the sequence (Uk)k≥0,
as described informally in the previous section, often referred to as stability in
the stochastic approximation literature.
Proposition 6.7. Suppose Assumptions 6.5 and 6.6 hold. Then there is a
ﬁnite random variable B such that supk≥0 ∥Uk∥∞< B with probability 1.
△
Proof. The idea of the proof is to work with a “renormalized” version of the
noises (wk(x))k≥0 to which Proposition 6.4 can be applied. First, we show that
the contractivity of O means that when U is suﬃciently far from 0, O contracts
the iterate back toward 0. To make this precise, we ﬁrst observe that
∥OU∥∞≤∥OU −U∗∥∞+ ∥U∗∥∞≤β∥U −U∗∥∞+ ∥U∗∥∞≤β∥U∥∞+ D ,
where D = (1 + β)∥U∗∥∞. Let ¯B > D/1−β so that ¯B > β ¯B + D, and deﬁne ψ ∈(β, 1)
by β ¯B + D = ψ ¯B. Now that that for any U with ∥U∥∞≥¯B, we have
∥OU∥∞≤β∥U∥∞+ D = β∥U∥∞+ (ψ −β) ¯B ≤β∥U∥∞+ (ψ −β)∥U∥∞= ψ∥U∥∞.
Second, we construct a sequence of bounds ( ¯Bk)k≥0 related to the iterates (Uk)k≥0
as follows. It will be convenient to introduce 1 + ε = ψ−1, the inverse of the
contraction factor ψ above. Take ¯B0 = max( ¯B, ∥U0∥∞), and iteratively deﬁne
¯Bk+1 =

¯Bk
if ∥Uk+1∥∞≤(1 + ε) ¯Bk
min{(1 + ε)l ¯B0 : l ∈N+, ∥Uk+1∥∞≤(1 + ε)l ¯B0}
otherwise .
Thus, the ( ¯Bk)k≥0 deﬁne a kind of soft “upper envelope” on (∥Uk∥∞)k≥0, which
are only updated when a norm exceeds the previous bound ¯Bk by a factor at
least (1 + ε). Note that (∥Uk∥∞)k≥0 is unbounded if and only if ¯Bk →∞.
We now use the ( ¯Bk)k≥0 to deﬁne a “renormalized” noise sequence ( ˜wk)k≥0
to which Proposition 6.4 can be applied. We set ˜wk = wk/ ¯Bk, and deﬁne ˜Wk
iteratively by ˜W0 = w0, and
˜Wk+1(x) = (1 −αk(x)) ˜Wk(x) + αk(x) ˜wk(x) .
By Assumption 6.6, we still have E[ ˜wk | Fk] = 0 and obtain that E[∥˜wk∥2
∞| Fk]
is uniformly bounded. Using Assumption 6.5, Proposition 6.4 now applies, and
we deduce that ˜Wk →0 with probability 1.
In particular, there is a (random) time K such that ∥˜Wk(x)∥< ε for all k ≥K
and x ∈X. Now supposing ¯Bk →∞, we may also take K so that ∥UK∥∞≤¯BK.
We will now prove by induction that for all k ≥K, we have both ∥Uk∥∞≤
(1 + ε) ¯BK and ∥Uk −Wk∥∞< ¯BK; the base case is clear from the above. For the
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

182
Chapter 6
inductive step, suppose for some k ≥K, we have both ∥Uk∥∞≤(1 + ε) ¯BK and
∥Uk −Wk∥∞< ¯BK. Now observe that
∥Uk+1(x) −Wk+1(x)∥
=∥(1 −αk(x))Uk(x) + αk(x)(OUk)(x) + αk(x)wk(x) −Wk+1(x)∥
≤(1 −αk(x))∥Uk(x) −Wk(x)∥+ αk(x)∥(OUk)(x)∥
≤(1 −αk(x)) ¯BK + αk(x)(β∥Uk∥∞+ D)
≤(1 −αk(x)) ¯BK + αk(x)ψ(1 + ε) ¯BK
≤¯BK .
And additionally,
∥Uk+1∥∞≤∥Uk+1 −Wk+1∥∞+ ∥Wk+1∥∞≤¯BK + ε ¯BK = (1 + ε) ¯BK
as required.
We can now establish the convergence of the cumulative external noise.
Proposition 6.8. Suppose Assumptions 6.5 and 6.6 hold. Then the external
noise Wk(x) converges to 0 with probability 1, for each x ∈X.
△
Proof. By Proposition 6.7, there exists a ﬁnite random variable B such that
∥Uk∥∞≤B for all k ≥0. We therefore have E[∥wk(x)∥2 | Fk] ≤C1 +C2B2 =: B′
w.p. 1 for all x ∈X and k ≥0, by Assumption 6.6. Proposition 6.4 therefore
applies to give the conclusion.
With this result in hand, we now prove the central result of this section,
using the stability result as the base case for the inductive argument intuitively
explained above.
Theorem 6.9. Suppose Assumptions 6.5 and 6.6 hold. Then Uk →U∗with
probability 1.
△
Proof. By Proposition 6.7, there is a ﬁnite random variable B0 such that ∥Uk −
U∗∥∞< B0 for all k ≥0 w.p. 1. Let ε > 0 such that β + 2ε < 1; we will show by
induction that if Bl = (β + 2ε)Bl−1 for all l ≥1, then for each l ≥0, there is a
(possibly random) ﬁnite time Kl such that ∥Uk −U∗∥∞< Bl for all k ≥Kl, which
proves the theorem.
To prove this claim by induction, let l ≥0 and suppose there is a random
ﬁnite time Kl such that ∥Uk −U∗∥∞< Bl for all k ≥Kl w.p. 1. Now let x ∈X, and
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Incremental Algorithms
183
k ≥Kl. We have
Uk+1(x) −U∗(x) −Wk+1(x)
= (1 −αk(x))Uk(x) + αk(x)((OUk)(x) + wk(x))
−U∗(x) −(1 −αk(x))Wk(x) −αk(x)wk(x)
= (1 −αk(x))(Uk(x) −U∗(x) −Wk(x)) + αk(x)((OUk)(x) −U∗(x)) .
Since O is a contraction mapping under ∥· ∥∞with ﬁxed point U∗and con-
traction modulus β, we have ∥(OUk)(x) −U∗(x)∥≤β∥Uk −U∗∥∞< βBl, and
so
∥Uk+1(x) −U∗(x) −Wk+1(x)∥≤(1 −αk(x))∥Uk(x) −U∗(x) −Wk(x)∥+αk(x)βBl .
Letting ∆k(x) = ∥Uk(x) −U∗(x) −Wk(x)∥, we then have
∆k+1(x) ≤(1 −αk(x))∆k(x) + αk(x)βBl
=⇒∆k+1(x) −βBl ≤(1 −αk(x))(∆k(x) −βBl) .
Telescoping this inequality from Kl to k yields
∆k+1(x) −βBl ≤

k
Y
s=Kl
(1 −αs(x))
(∆Kl(x) −βBl) .
If ∆Kl(x) −βBl ≤0, then ∆k(x) ≤βBl for all k ≥Kl. If not, then we can use the
inequality 1 −x ≤e−x (applied to x = αk ≥0) to deduce
∆k+1(x) −βBl ≤exp
−
k
X
s=Kl
αk(x)
(∆Kl(x) −βBl) ,
and since P
s≥0 αs(x) = ∞by assumption, the right-hand side tends to 0. There-
fore, there exists a random ﬁnite time after which ∆k(x) ≤(β + ε)Bl. Since X is
ﬁnite, there is a random ﬁnite time after which this holds for all x ∈X. Finally,
since Wk(x) →0 under ∥· ∥for all x ∈X w.p. 1 by Proposition 6.8, there is a
random ﬁnite time after which ∥Wk(x)∥≤εBl for all x ∈X. Letting Kl+1 ≥Kl be
the maximum of all these random times, we therefore have that for k ≥Kl+1, for
all x ∈X,
∥Uk(x) −U∗(x)∥≤∥Uk(x) −U∗(x) −Wk(x)∥+ ∥Wk(x)∥≤(β + ε)Bl + εBl = Bl+1 ,
as required.
6.9
Convergence of Temporal-Diﬀerence Learning*
We can now apply Theorem 6.9 to demonstrate the convergence of the sequence
of value function estimates produced by TD learning. Formally, we consider a
stream of sample transitions (Xk, Ak, Rk, X′
k)k≥0, along with associated step sizes
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

184
Chapter 6
(αk)k≥0, that satisfy the Robbins–Monro conditions (Assumption 6.5) and give
rise to zero-mean noise terms (Assumption 6.6). More precisely, we assume
there are sequences of functions (ξk)k≥0 and (νk)k≥0 such that our sample model
takes the following form (for k ≥0):
Xk|(X0:k−1, A0:k−1, R0:k−1, X′
0:k−1, α0:k−1)∼ξk
 X0:k−1, A0:k−1, R0:k−1, X′
0:k−1, α0:k−1
;
αk|(X0:k, A0:k−1, R0:k−1, X′
0:k−1, α0:k−1)∼νk
 X0:k, A0:k−1, R0:k−1, X′
0:k−1, α0:k−1
;
Ak| X0:k, A0:k−1, R0:k−1, X′
0:k−1, α0:k
∼π( · | Xk);
Rk| X0:k, A0:k, R0:k−1, X′
0:k−1, α0:k
∼PR( · | Xk, Ak);
X′
k| X0:k, A0:k, R0:k, X′
0:k−1, α0:k
∼PX( · | Xk, Ak) .
(6.23)
A generative, or algorithmic, perspective on this model is that at each update step
k, a source state Xk and step size αk are selected on the basis of all previously
observed random variables (possibly using an additional source of randomness
to make this selection), and the variables (Ak, Rk, X′
k) are sampled according
to π and the environment dynamics, conditionally independent of all random
variables already observed given Xk. Readers may compare this with the model
equations in Section 2.3 describing the joint distribution of a trajectory generated
by following the policy π. As discussed in Sections 6.5 and 6.6, this is fairly
ﬂexible model that allows us to analyze a variety of learning schemes.
Theorem 6.10. Consider the value function iterates (Vk)k≥0 deﬁned by
some initial estimate V0 and satisfying
Vk+1(Xk) = (1 −αk)Vk(Xk) + αk
 Rk + Vk(X′
k)
Vk+1(x) = Vk(x)
if x , Xk ,
where (Xk, Ak, Rk, X′
k)k≥0 is a sequence of transitions. Suppose that:
(a) The source states (Xk)k≥0 and step sizes (αk)k≥0 satisfy the Robbins–
Monro conditions: w.p. 1, for all x ∈X,
∞
X
k=0
αk1{Xk = x} = ∞,
∞
X
k=0
α2
k1{Xk = x} < ∞.
(b) The joint distribution of (Xk, Ak, Rk, X′
k)k≥0 is an instance of the
sampling model expressed in Equation 6.23.
(c) The reward distributions for all state-action pairs have ﬁnite variance.
Then Vk →Vπ with probability 1.
△
Theorem 6.10 gives formal meaning to our earlier assertion that the conver-
gence of incremental reinforcement learning algorithms can be guaranteed for a
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Incremental Algorithms
185
variety of source state distributions. Interestingly enough, the condition on the
source state distribution appears only implicitly, through the Robbins–Monro
conditions: eﬀectively, what matters is not so much when the states are updated
but rather the “total amount of step size” by which the estimate may be moved.
Proof. We ﬁrst observe that the temporal-diﬀerence algorithm described in
the statement is an instance of the abstract algorithm described in Section 6.5,
by taking Uk = Vk, m = 1, O = T π, Yk = (Ak, Rk, X′
k), and ˆO(Uk, Xk, Yk) = Rk +
γVk(X′
k). The base norm ∥· ∥is simply the absolute value on R. In this case, O is
a γ-contraction on RX by Proposition 4.4, with ﬁxed point Vπ, and the noise wk
is equal to Rk + γVk(X′
k) −(T πUk)(Xk), by the decomposition in Equation 6.5.
It therefore remains to check that Assumptions 6.5 and 6.6 hold; Theorem 6.9
then applies to give the result. Assumption 6.5 is immediate from the conditions
of the theorem. To see that Assumption 6.6 holds, ﬁrst note that
E[wk | Fk] = Eπ[Rk + γVk(X′
k) −(T πVk)(Xk) | Fk]
= Eπ[Rk + γVk(X′
k) −(T πVk)(Xk) | Xk, Vk]
= 0 ,
since conditional on (Xk, Vk), the expectation of R + γVk(X′
k) is (T πVk)(Xk).
Additionally, we note that
E|wk|2 | Fk
 = E|wk|2 | Xk, Vk

= E|R + γVk(X′
k) −(T πVk)(Xk)|2 | Xk, Vk

≤2

E|R + γVk(X′
k)|2 | Xk, Vk
 + (T πVk)(Xk)2
≤C1 +C2∥Vk∥2
∞,
for some C1,C2 > 0.
6.10
Convergence of Categorical Temporal-Diﬀerence Learning*
Let us now consider proving the convergence of categorical TD learning
by means of Theorem 6.9. Writing CTD in terms of a sequence of return
distribution estimates, we have
ηk+1(Xk) = (1 −αk)ηk(Xk) + αk
 Πc(bRk,γ)#η(X′
k)
ηk+1(x) = ηk(x)
if x , Xk .
(6.24)
Following the principles of the previous section, we may decompose the update
at Xk into an operator term and a noise term:
ηk+1(Xk) = (1 −αk)ηk(Xk)
(6.25)
+ αk

(ΠcT πη)(Xk)
|          {z          }
(OU)(Xk)
+ Πc(bRk,γ)#η(X′
k) −(ΠcT πη)(Xk)
|                                  {z                                  }
wk

.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

186
Chapter 6
Assuming that Rk and X′
k are drawn appropriately, this decomposition is sensible
by virtue of Proposition 6.2, in the sense that the expectation of the sample
target is the projected distributional Bellman operator:
Eπ
Πc
 (bRk,γ)#η(X′
k) | Xk = x = (ΠcT πη(x)
for all x .
(6.26)
With this decomposition, the noise term is not a probability distribution but
rather a signed distribution; this is illustrated in Figure 6.3. Informally speaking,
a signed distribution may assign negative “probabilities” and may not integrate
to one (we will revisit signed distributions in Chapter 9). Based on Proposition
6.2, we may intuit that wk is mean-zero noise, where “zero” here is to be
understood as a special signed distribution.
(a) Categorical TD target
(b) Expected update
(c) Mean-zero noise
Figure 6.3
The sample target in a categorical TD update (a) can be decomposed into an expected
update speciﬁed by the operator ΠcT π (b) and a mean-zero signed distribution (c).
However, expressing the CTD update rule in terms of signed distributions
is not suﬃcient to apply Theorem 6.9. This is because the theorem requires
that the iterates  Uk(x)
k≥0 be elements of Rm, whereas (ηk(x))k≥0 are proba-
bility distributions. To address this issue, we leverage the fact that categorical
distributions are represented by a ﬁnite number of parameters and view their
cumulative distribution functions as in vectors in Rm.
Recall that F X
C,m is the space of m-categorical return distribution functions.
To invoke Theorem 6.9, we construct an isometry I between F X
C,m and a certain
subset of RX×m. For a categorical return function η ∈F X
C,m, write
I(η) =

Fη(x)(θi) : x ∈X, i ∈{1, . . . , m}

∈RX×m ,
where as before θ1, . . . , θm denotes the locations of the m particles whose prob-
abilities are parameterized in FC,m. The isometry I maps return functions
to elements of RX×m describing the corresponding cumulative distribution
functions (CDFs), evaluated at these particles. The image of F X
C,m under I is
RX
I = {z ∈Rm : 0 ≤z1 ≤· · · ≤zm = 1}X.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Incremental Algorithms
187
The inverse map
I−1 : RX
I →F X
C,m
maps vectors describing the cumulative distribution functions of categorical
return functions back to their distributions.
With this construction, the metric induced by the L2 norm ∥· ∥2 over Rm is
proportional to the Cramér distance between probability distributions and is
readily extended to RX×m. That is, for η, η′ ∈F X
C,m, we have
I(η) −I(η′)
2,∞= sup
x∈X
(I(η))(x) −(I(η′))(x)
2 = 1
ςm
ℓ2(η, η′).
We will prove the convergence of the sequence (ηk)k≥0 deﬁned by Equation 6.24
to the ﬁxed point of the projected operator ΠcT π by applying Theorem 6.9 to
the sequence (I(ηk))k≥0 and the L2 metric and arguing (by isometry) that the
original sequence must also converge. An important additional property of I is
that it commutes with expectations, in the following sense.
Lemma 6.11. The isometry I : F X
C,m →RX
I is an aﬃne map. That is, for any
η, η′ ∈F X
C,m and α ∈[0, 1],
I(αη + (1 −α)η′) = αI(η) + (1 −α)I(η′) .
As a result, if η is a random return-distribution function, then we have
E[I(η)] = I(E[η]) .
△
Theorem 6.12. Let m ≥2 and consider the return function iterates (ηk)k≥0
generated by Equation 6.24 from some possibly random η0. Suppose that:
(a) The source states (Xk)k≥0 and step sizes (αk)k≥0 satisfy the Robbins–
Monro conditions: w.p. 1, for all x ∈X,
∞
X
k=0
αk1{Xk = x} = ∞,
∞
X
k=0
α2
k1{Xk = x} < ∞.
(b) The joint distribution of (Xk, Ak, Rk, X′
k)k≥0 is an instance of the
sampling model expressed in Equation 6.23.
Then, with probability 1, ηk →ˆηπ
c with respect to the supremum Cramér
distance ℓ2, where ˆηπ
c is the unique ﬁxed point of the projected operator
ΠcT π:
ˆηπ
c = ΠcT π ˆηπ
c .
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

188
Chapter 6
Proof. We begin by constructing a sequence (Uk)k≥0, Uk ∈RX
I that parallels the
sequence of return functions (ηk)k≥0. Write
O = I ◦ΠcT π ◦I−1
and deﬁne, for each k ∈N, Uk = I(ηk). By Lemma 5.24, O is a contraction with
modulus γ
1/2 in ∥· ∥2,∞and we have
Uk+1(Xk)
= (1 −αk)Uk(Xk) + αkI Πc(bRk,γ)#η(X′
k)
= (1 −αk)Uk(Xk) + αk
 (OUk)(Xk) + IΠc(bRk,γ)#(I−1Uk)(X′
k) −(OUk)(Xk)
|                                           {z                                           }
wk
.
To see that Assumption 6.6 (bounded, mean-zero noise) holds, ﬁrst note that
by Proposition 6.2 and aﬃneness of I and I−1 from Lemma 6.11, we have
E[wk | Fk] = 0. Furthermore, wk is a bounded random variable, because each
coordinate is a diﬀerence of two probabilities and hence in the interval [−1, 1].
Hence, we have E[∥wk∥2 | Fk] <C for some C > 0, as required.
By Banach’s theorem, the operator O has a unique ﬁxed point U∗. We can
thus apply Theorem 6.9 to conclude that the sequence (Uk)k≥0 converges to U∗,
satisfying
U∗= IΠcT πI−1U∗.
(6.27)
Because I is an isometry, this implies that (ηk)k≥0 converges to η∗= I−1U∗.
Applying I−1 to both sides of Equation 6.27, we obtain
I−1U∗= ΠcT πI−1U∗.
Since ΠcT π has a unique ﬁxed point, we conclude that η∗= ˆηπ
c.
The proof of Theorem 6.12 illustrates how the parameters of categorical
distributions are by design bounded, so that stability (i.e., Proposition 6.7) is
immediate. In fact, stability is also immediate for TD learning when the reward
distributions have bounded support.
6.11
Technical Remarks
Remark 6.1. Given a probability distribution ν ∈P(R) and a level τ ∈(0, 1),
quantile regression ﬁnds a value θ∗∈R such that
Fν(θ∗) = τ .
(6.28)
In some situations, for example when ν is a discrete distribution, there are
multiple values satisfying Equation 6.28. Let us write
S = {θ : Fν(θ) = τ} .
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Incremental Algorithms
189
Then one can show that S forms an interval. We can argue that quantile regres-
sion converges to this set by noting that, for τ ∈(0, 1) the expected quantile
loss
Lτ(θ) = E
Z∼ν
|1{Z<θ} −τ| × |Z −θ|
is convex in θ. In addition, for this loss, we have that for any θ, θ′ ∈S and θ′′ < S ,
Lτ(θ) = Lτ(θ′) < Lτ(θ′′) .
Convergence follows under appropriate conditions by appealing to standard
arguments regarding the convergence of stochastic gradient descent; see, for
example, Kushner and Yin (2003).
△
Remark 6.2 (Proof of Proposition 6.4). Our goal is to show that ∥¯Zk∥2
2
behaves like a nonnegative supermartingale, from which convergence would
follow from the supermartingale convergence theorem (see, e.g., Billingsley
2012). We begin by expressing the squared Euclidean norms of the sequence
elements recursively, writing Fk = σ(Z0:k−1, α0:k):
E[∥¯Zk+1(x)∥2
2 | Fk] = E[∥(1 −αk) ¯Zk + αkZk∥2
2 | Fk]
(a)= (1 −αk)2∥¯Zk∥2
2 + α2
kE[∥Zk∥2
2 | Fk]
(b)
≤(1 −αk)2∥¯Zk∥2
2 + α2
kB
≤(1 −αk)∥¯Zk∥2
2 + α2
kB .
(6.29)
Here, (a) follows by expanding the squared norm and using E[Zk | Fk] = 0, and
(b) follows from the boundedness of the conditional variance of the (Zk)k≥0,
where B is a bound on such variances.
This inequality does not establish the supermartingale property, due to the
presence of the additive term α2
kB on the right-hand side. However, the ideas
behind the Robbins–Siegmund theorem (Robbins and Siegmund 1971) can be
applied to deal with this term. The argument ﬁrst constructs the sequence
Λk = ∥¯Zk∥2
2 +
k−1
X
s=0
αk∥¯Zs∥2
2 −B
k−1
X
s=0
α2
s .
Inequality 6.29 above then shows that (Λk)k≥0 is a supermartingale but may
not be uniformly bounded below, meaning the supermartingale convergence
theorem still cannot be applied. Deﬁning the stopping times tq = inf{k ≥0 :
B′ Pk
s=0 α2
s > q} for each q ∈N (with the convention that inf ∅= ∞), each stopped
process (Λk∧tq)k≥0 is a supermartingale bounded below by −q, and hence each
such process converges w.p. 1 by the supermartingale convergence theorem.
However, B′ P∞
s=0 α2
s < ∞w.p. 1 by assumption, so w.p. 1 tq = ∞for suﬃciently
large q, and hence Λk converges w.p. 1. Since P∞
k=0 αk = ∞w.p. 1 by assumption,
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

190
Chapter 6
it must be the case that ∥¯Zk∥2
2 →0 as k →0, in order for Λk to have a ﬁnite
limit, and hence we are done. Although somewhat involved, Exercise 6.13
demonstrates the necessity of this argument.
△
6.12
Bibliographical Remarks
The focus of this chapter has been in developing and analyzing single-step
temporal-diﬀerence algorithms. Further algorithmic developments include
the use of multistep returns (Sutton 1988), oﬀ-policy corrections (Precup et
al. 2000), and gradient-based algorithms (Sutton et al. 2009; Sutton et al. 2008a);
the exercises in this chapter develop a few such approaches.
6.1–6.2. This chapter analyzes incremental algorithms through the lens of
approximating the application of dynamic programming operators. Temporal-
diﬀerence algorithms have a long history (Samuel 1959), and the idea of
incremental approximations to dynamic programming formed motivation for
several general-purpose temporal-diﬀerence learning algorithms (Sutton 1984,
1988; Watkins 1989).
Although early proofs of particular kinds of convergence for these algorithms
did not directly exploit this connection with dynamic programming (Watkins
1989; Watkins and Dayan 1992; Dayan 1992), later a strong theoretical connec-
tion was established that viewed these algorithms through the lens of stochastic
approximation theory, allowing for a uniﬁed approach to proving almost-sure
convergence (Gurvits et al. 1994; Dayan and Sejnowski 1994; Tsitsiklis 1994;
Jaakkola et al. 1994; Bertsekas and Tsitsiklis 1996; Littman and Szepesvári
1996). The unbiased estimation framework presented comes from these works,
and the second principle is based on the ideas behind two-timescale algorithms
(Borkar 1997, 2008). A broader framework based on asymptotically approxi-
mating the trajectories of diﬀerential equations is a central theme of algorithm
design and stochastic approximation theory more generally (Ljung 1977; Kusher
and Clark 1978; Borkar and Meyn 2000; Kushner and Yin 2003; Borkar 2008;
Benveniste et al. 2012; Meyn 2022).
In addition to the CTD and QTD algorithms described in this chapter, several
other approaches to incremental learning of return distributions have been
proposed. Morimura et al. (2010b) propose to update parametric density models
by taking gradients of the Kullback-Leibler divergence between the current
estimates and the result of applying the Bellman operator to these estimates.
Barth-Maron et al. (2018) also take this approach, using a representation based
on mixtures of Gaussians. Nam et al. (2021) also use mixtures of Gaussians
and minimize the Cramér distance from a multistep target, incorporating ideas
from TD(λ) (Sutton 1984, 1988). Gruslys et al. (2018) combine CTD with
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Incremental Algorithms
191
Retrace(λ), a multistep oﬀ-policy evaluation algorithm (Munos et al. 2016).
Nguyen et al. (2021) combine the quantile representation with a loss based
on the MMD metrics described in Chapter 4. Martin et al. (2020) propose a
proximal update scheme for the quantile representation based on (regularized)
Wasserstein ﬂows (Jordan et al. 1998; Cuturi 2013; Peyré and Cuturi 2019).
Example 6.1 is from Bellemare et al. (2016).
6.3. The categorical temporal-diﬀerence algorithm as a mixture update was
presented by Rowland et al. (2018). This is a variant of the C51 algorithm
introduced by Bellemare et al. (2017a), which uses a projection in a mixture of
Kullback–Leibler divergence and Cramér distance. Distributional versions of
gradient temporal-diﬀerence learning (Sutton et al. 2008a; Sutton et al. 2009)
based on the categorical representation have also been explored by Qu et
al. (2019).
6.4. The QTD algorithm was introduced by Dabney et al. (2018b). Quan-
tile regression itself is a long-established tool within statistics, introduced by
Koenker and Bassett (1978); Koenker (2005) is a classic reference on the sub-
ject. The incremental rule for estimating quantiles of a ﬁxed distribution was in
fact proposed by Robbins and Monro (1951), in the same paper that launched
the ﬁeld of stochastic approximation.
6.5. The discussion of sequences of learning rates that result in convergence
goes back to Robbins and Monro (1951), who introduced the ﬁeld of stochastic
approximation. Szepesvári (1998), for example, considers this framework in
their study of the asymptotic convergence rate of Q-learning. A ﬁne-grained
analysis in the case of temporal-diﬀerence learning algorithms, taking ﬁnite-
time concentration into account, was undertaken by Even-Dar and Mansour
(2003); see also Azar et al. (2011).
6.6–6.10. Our proof of Theorem 6.9 (via Propositions 6.4, 6.7, and 6.8) closely
follows the argument given by Bertsekas and Tsitsiklis (1996) and Tsitsiklis
(1994). Speciﬁcally, we adapt this argument to deal with distributional informa-
tion, rather than a single scalar value. Proposition 6.4 is a special case of the
Robbins–Siegmund theorem (Robbins and Siegmund 1971), and a particularly
clear exposition of this and related material is given by Walton (2021). We note
also that this result can also be established via earlier results in the stochastic
approximation literature (Dvoretzky 1956), as noted by Jaakkola et al. (1994).
Theorem 6.10 is classical, and results of this kind can be found in Bertsekas and
Tsitsiklis (1996). Theorem 6.12 was ﬁrst proven by Rowland et al. (2018), albeit
with a monotonicity argument based on that of Tsitsiklis (1994); the argument
here is based on a contraction mapping argument to match the analysis of the
temporal-diﬀerence algorithm. For further background on signed measures, see
Doob (1994).
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

192
Chapter 6
6.13
Exercises
Exercise 6.1. In this chapter, we argued for a correspondence between oper-
ators and incremental algorithms. This also holds true for the incremental
Monte Carlo algorithm introduced in Section 3.2. What is peculiar about the
corresponding operator?
△
Exercise 6.2. Exercise 3.2 asked you to derive an incremental algorithm from
the n-step Bellman equation
Vπ(x) = Eπ
 n−1
X
t=0
γtRt + γnVπ(Xn) | X0 = x

.
Describe this process in terms of the method where we substitute random
variables with their realizations, then derive the corresponding incremental
algorithm for state-action value functions.
△
Exercise 6.3 (*). The n-step random-variable Bellman equation for a policy π
is given by
Gπ(x)
D=
n−1
X
t=0
γtRt + γnGπ(Xn),
X0 = x,
where the trajectory (X0 = x, A0, R0, …, Xn, An, Rn) is distributed according to
Pπ(· | X0 = x).
(i) Write down the distributional form of this equation and the corresponding
n-step distributional Bellman operator.
(ii) Show that it is a contraction on a suitable subset of P(R)X with respect to
an appropriate metric.
(iii) Further show that the composition of this operator with either the categori-
cal projection or the quantile projection is also a contraction mapping in the
appropriate metric.
(iv) Using the approach described in this chapter, derive n-step versions of
categorical and quantile temporal-diﬀerence learning.
(v) In the case of n-step CTD, describe an appropriate set of conditions that
allow for Theorem 6.9 to be used to obtain convergence to the projected
operator ﬁxed point with probability 1. What happens to the ﬁxed points of
the projected operators as n →∞?
△
Exercise 6.4. Implement the quantile regression update rule (Equation 6.11).
Given an initial estimate θ0 = 0, visualize the sequence of estimates produced
by quantile regression for τ ∈{0.01, 0.1, 0.5} and a constant step size α = 0.01,
given samples from
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Incremental Algorithms
193
(i) a normal distribution N(1, 2) ;
(ii) a Bernoulli distribution U({0, 1}) ;
(iii) the mixture distribution 1
3δ1 + 2
3U([2, 3]) .
△
Exercise 6.5. Let η0 be a m-quantile return-distribution function, and let
(x, a, r, x′) denote a sample transition. Find a Markov decision process for
which the update rule
η(x) ←Πq
h
(1 −α)η(x) + αη0(x′)
i
does not converge.
△
Exercise 6.6. Implement the TD, CTD, and QTD algorithms, and use these
algorithms to approximate the value (or return) function of the quick policy on
the Cliﬀs domain (Example 2.9). Compare their accuracy to the ground-truth
value function and return-distribution function estimated using many Monte
Carlo rollouts, both in terms of an appropriate metric and by visually comparing
the approximations to the ground-truth functions.
Investigate how this accuracy is aﬀected by diﬀerent choices of constant step
sizes and sequences of step sizes that satisfy the requirements laid out in Section
6.6. What do you notice about the relative performance of these algorithms as
the degree of action noise p is varied?
Investigate what happens when we modify the TD algorithm by restricting
value function estimates on the interval [−C,C], for a suitable C ∈R+. Does
this restriction aﬀect the performance of the algorithm diﬀerently from the
restriction to [θ1, θm] that is intrinsic to CTD?
△
Exercise 6.7. Let (Xk, Ak, Rk, X′
k)k≥0 be a random sequence of transitions such
that for each x ∈X, we have Xk = x for inﬁnitely many k ≥0. Show that taking
αk =
1
Nk(Xk) + 1.
satisﬁes Assumption 6.5.
△
Exercise 6.8. Theorems 6.10 and 6.12 establish convergence for state-indexed
value functions and return-distribution functions under TD and CTD learning,
respectively. Discuss how Theorem 6.9 can be used to establish convergence of
the corresponding state-action-indexed algorithms.
△
Exercise 6.9 (*). Theorem 6.10 establishes that temporal-diﬀerence learning
converges for a reasonably wide parameterization of the distribution of source
states and step size schedules. Given a source state Xk, consider the incremental
Monte Carlo update
Vk+1(Xk) = (1 −αk)Vk(Xk) + αkGk
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

194
Chapter 6
Vk+1(x) = Vk(x)
if x , Xk ,
where Gk ∼ηπ(Xk) is a random return. Explain how Theorem 6.10 and its proof
should be adapted to prove that the sequence (Vk)k≥0 converges to Vπ.
△
Exercise 6.10 (Necessity of conditions for convergence of TD learning). The
purpose of this exercise is to explore the behavior of TD learning when the
assumptions of Theorem 6.10 do not hold.
(i) Write down an MDP with a single state x, from which trajectories immedi-
ately terminate, and a sequence of positive step sizes (αk(x))k≥0 satisfying
P
k≥0 αk(x) < ∞, with the property that the TD update rule applied with these
step sizes produces a sequence of estimates (Vk)k≥0 that does not converge
to Vπ.
(ii) For the same MDP, write down a sequence of positive step sizes (αk(x))k≥0
such that P
k≥0 α2
k(x) = ∞, and show that the sequence of estimates (Vk)k≥0
generated by TD learning with these step sizes does not converge to Vπ.
(iii) Based on your answers to the previous two parts, for which values of β ≥0
do step size sequences of the form
αk =
1
(Nk(Xk) + 1)β
lead to guaranteed TD converge, assuming all states are visited inﬁnitely
often?
(iv) Consider an MDP with a single state x, from which trajectories immedi-
ately terminate. Suppose the reward distribution at x is a standard Cauchy
distribution, with density
f(z) =
1
π(1 + z2) .
Show that if V0 also has a Cauchy distribution with median 0, then for any
positive step sizes (αk(x))k≥0, Vk has a Cauchy distribution with median
0, and hence the sequence does not converge to a constant. Hint. The
characteristic function of the Cauchy distribution is given by s 7→exp(−|s|).
(v) (*) In the proof of Theorem 6.9, the inequality 1 −u ≤exp(−u) was used
to deduce that the condition P
k≥0 αk(x) = ∞w.p. 1 is suﬃcient to guarantee
that
k
Y
l=0
(1 −αl(x)) →0
w.p. 1. Show that if αk(x) ∈[0, 1], the condition P
k≥0 αk(x) = ∞w.p. 1 is
necessary as well as suﬃcient for the sequence Qk
l=0(1 −αl(x)) to also
converge to zero w.p. 1.
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Incremental Algorithms
195
Exercise 6.11. Using the tools from this chapter, prove the convergence of
the undiscounted, ﬁnite-horizon categorical Monte Carlo algorithm (Algorithm
3.3).
△
Exercise 6.12. Recall the no-loop operator introduced in Example 4.6:
 T π
nlV(x) = Eπ
R + γV(X′)1{X′ , x} | X = x .
Denote its ﬁxed point by Vπ
nl. For a transition (x, a, r, x′) and time-varying step
size αk ∈[0, 1), consider the no-loop update rule:
V(x) ←

(1 −αk)V(x) + αk(r + γV(x′))
if x′ , x,
(1 −αk)V(x) + αkr
if x′ = x
.
(i) Demonstrate that this update rule can be derived by substitution applied to
the no-loop operator.
(ii) In words, describe how you would modify the online, incremental ﬁrst-visit
Monte Carlo algorithm (Algorithm 3.1) to learn Vπ
nl.
(iii) (*) Provide conditions under which the no-loop update converges to Vπ
nl,
and prove that it does converge under those conditions.
△
Exercise 6.13. Assumption 6.5 requires that the sequence of step sizes (αk(x) :
k ≥0, x ∈X) satisfy
∞
X
k=0
αk(x)2 < ∞
(6.30)
with probability 1. For k ≥0, let Nk(x) be the number of times that x has been
updated, and let uk(x) be the most recent time at which xl = x, l < k with the
convention that uk(x) = 1 if Nk(x) = 0. Consider the step size schedule
αk+1 =
(
1
Nk(Xk)+1
if uk(Xk) ≤k
2
1
otherwise.
This schedule takes larger steps for states whose estimate has not been recently
updated. Suppose that Xk ∼ξ for some distribution ξ that puts positive probabil-
ity mass on all states. Show that the sequence (αk)k≥0 satisﬁes Equation 6.30
w.p. 1, yet there is no B ∈R for which
∞
X
k=0
αk(x)2 < B
with probability 1. This illustrates the need for the care taken in the proof of
Proposition 6.4.
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

7
Control
A superpressure balloon is a kind of aircraft whose altitude is determined by the
relative pressure between its envelope and the ambient atmosphere and which
can be ﬂown high in the stratosphere. Like a submarine, a superpressure balloon
ascends when it becomes lighter and descends when it becomes heavier. Once
in ﬂight, superpressure balloons are passively propelled by the winds around
them, so that their direction of travel can be inﬂuenced simply by changing their
altitude. This makes it possible to steer such a balloon in an energy-eﬃcient
manner and have it operate autonomously for months at a time. Determining
the most eﬃcient way to control the ﬂight of a superpressure balloon by means
of altitude changes is an example of a control problem, the topic of this chapter.
In reinforcement learning, control problems are concerned with ﬁnding poli-
cies that achieve or maximize speciﬁed objectives. This is in contrast with
prediction problems, which are concerned with characterizing or quantifying
the consequences of following a particular policy. The study of control problems
involves not only the design of algorithms for learning optimal policies but
also the study of the behavior of these algorithms under diﬀerent conditions,
such as when learning occurs one sample at a time (as per Chapter 6), when
noise is injected into the process, or when only a ﬁnite amount of data is made
available for learning. Under the distributional perspective, the dynamics of
control algorithms exhibit a surprising complexity. This chapter gives a brief
and necessarily incomplete overview of the control problem. In particular, our
treatment of control diﬀers from most textbooks in that we focus on the distri-
butional component and for conciseness omit some traditional material such as
policy iteration and λ-return algorithms.
7.1
Risk-Neutral Control
The problem of ﬁnding a policy that maximizes the agent’s expected return is
called the risk-neutral control problem, as it is insensitive to the deviations of
197
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

198
Chapter 7
returns from their mean. We have already encountered risk-neutral control when
we introduced the Q-learning algorithm in Section 3.7. We begin this chapter
by providing a theoretical justiﬁcation for this algorithm.
Problem 7.1 (Risk-neutral control). Given an MDP (X, A, ξ0, PX, PR) and
discount factor γ ∈[0, 1), ﬁnd a policy π maximizing the objective function
J(π) = Eπ
h ∞
X
t=0
γtRt
i
.
△
A solution π∗that maximizes J is called an optimal policy.
Implicit in the deﬁnition of risk-neutral control and our deﬁnition of a policy
in Chapter 2 is the fact that the objective J is maximized by a policy that only
depends on the current state: that is, one that takes the form
π : X →P(A) .
As noted in Section 2.2, policies of this type are more properly called sta-
tionary Markov policies and are but a subset of possible decision rules. With
stationary Markov policies, the action At is independent of the random vari-
ables X0, A0, R0, . . . , Xt−1, At−1, Rt−1 given Xt. In addition, the distribution of At,
conditional on Xt, is the same for all time indices t.
By contrast, history-dependent policies select actions on the basis on the
entire trajectory up to and including Xt (the history). Formally, a history-
dependent policy is a time-indexed collection of mappings
πt : (X × A × R)t−1 × X →P(A) .
In this case, we have that
At | (X0:t, A0:t−1, R0:t−1) ∼πt(· | X0, A0, R0, . . . , At−1, Rt−1, Xt) .
(7.1)
When clear from context, we omit the time subscript to πt and write Pπ, Eπ,
Gπ, and ηπ to denote the joint distribution, expectation, return-variable function,
and return-distribution function implied by the generative equations but with
Equation 7.1 substituting the earlier deﬁnition from Section 2.2. We write πms
for the space of stationary Markov policies and πh for the space of history-
dependent policies.
It is clear that every stationary Markov policy is a history-dependent policy,
though the converse is not true. In risk-neutral control, however, the added
degree of freedom provided by history-dependent policies is not needed to
achieve optimality; this is made formal by the following proposition (recall that
a policy is deterministic if it always selects the same action for a given state or
history).
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Control
199
Proposition 7.2. Let J(π) be as in Problem 7.1. There exists a determinis-
tic stationary Markov policy π∗∈πms such that
J(π∗) ≥J(π)
∀π ∈πh .
△
Proposition 7.2 is a central result in reinforcement learning – from a computa-
tional point of view, for example, it is easier to deal with deterministic policies
(there are ﬁnitely many of them) than stochastic policies. Remark 7.1 discusses
some other beneﬁcial consequences of Proposition 7.2. Its proof involves a
surprising amount of detail; we refer the interested reader to Puterman (2014,
Section 6.2.4).
7.2
Value Iteration and Q-Learning
The main consequence of Proposition 7.2 is that when optimizing the risk-
neutral objective, we can restrict our attention to deterministic stationary Markov
policies. In turn, this makes it possible to ﬁnd an optimal policy π∗by computing
the optimal state-action value function Q∗, deﬁned as
Q∗(x, a) = sup
π∈πms
Eπ
h ∞
X
t=0
γtRt | X = x, A = a
i
.
(7.2)
Just as the value function Vπ for a given policy π satisﬁes the Bellman equation,
Q∗satisﬁes the Bellman optimality equation:
Q∗(x, a) = E R + γ max
a′∈A Q∗(X′, a′) | X = x, A = a .
(7.3)
The optimal state-action value function describes the expected return obtained
by acting so as to maximize the risk-neutral objective when beginning from
the state-action pair (x, a). Intuitively, we may understand Equation 7.3 as
describing this maximizing behavior recursively. While there might be multiple
optimal policies, they must (by deﬁnition) achieve the same objective value in
Problem 7.1. This value is
Eπ[V∗(X0)] ,
where V∗is the optimal value function:
V∗(x) = max
a∈A Q∗(x, a) .
Given Q∗, an optimal policy is obtained by acting greedily with respect to Q∗–
that is, choosing in state x any action a that maximizes Q∗(x, a).
Value iteration is a procedure for ﬁnding the optimal state-action value
function Q∗iteratively, from which π∗can then be recovered by choosing
actions that have maximal state-action values. In Chapter 5, we discussed a
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

200
Chapter 7
procedure for computing Vπ based on repeated applications of the Bellman
operator T π. Value iteration replaces the Bellman operator T π in this procedure
with the Bellman optimality operator
(TQ)(x, a) = Eπ
R + γ max
a′∈A Q(X′, a′) | X = x, A = a .
(7.4)
Let us deﬁne the L∞norm of a state-action value function Q ∈RX×A as
∥Q∥∞=
sup
x∈X,a∈A
|Q(x, a)|.
As the following establishes, the Bellman optimality operator is a contraction
mapping in this norm.
Lemma 7.3. The Bellman optimality operator T is a contraction in L∞norm
with modulus γ. That is, for any Q, Q′ ∈RX×A,
∥TQ −TQ′∥∞≤γ∥Q −Q′∥∞.
△
Corollary 7.4. The optimal state-action value function Q∗is the only value
function that satisﬁes the Bellman optimality equation. Further, for any Q0 ∈
RX×A, the sequence (Qk)k≥0 deﬁned by Qk+1 = TQk (for k ≥0) converges to
Q∗.
△
Corollary 7.4 is an immediate consequence of Lemma 7.3 and Proposition
4.7. Before we give the proof of Lemma 7.3, it is instructive to note that despite
a visual similarity and the same contractive property in supremum norm, the
optimality operator behaves somewhat diﬀerently from the ﬁxed-policy operator
T π, deﬁned for state-action value functions as
(T πQ)(x, a) = Eπ
R + γQ(X′, A′) | X = x, A = a] ,
(7.5)
where conditional on the random variables (X = x, A = a, R, X′), we have A′ ∼
π(· | X′). In the context of this chapter, we call T π a policy evaluation operator.
Such an operator is said to be aﬃne: for any two Q-functions Q, Q′ and α ∈[0, 1],
it satisﬁes
T π(αQ + (1 −α)Q′) = αT πQ + (1 −α)T πQ′ .
(7.6)
Equivalently, the diﬀerence between T πQ and T πQ′ can be expressed as
T πQ −T πQ′ = γPπ(Q −Q′) .
The optimality operator, on the other hand, is not aﬃne. While aﬃne operators
can be analyzed almost as if they were linear,54 the optimality operator is
generally a nonlinear operator. As such, its analysis requires a slightly diﬀerent
approach.
54. Consider the proof of Proposition 4.4.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Control
201
Proof of Lemma 7.3. The proof relies on a special property of the maximum
function. For f1, f2 : A →R, it can be shown that
 max
a∈A f1(a) −max
a∈A f2(a)
 ≤max
a∈A
 f1(a) −f2(a)
 .
Now let Q, Q′ ∈RX×A, and ﬁx x ∈X, a ∈A. Let us write Exa,π[·] = Eπ[· | X =
x, A = a]. By linearity of expectations, we have
|(TQ)(x, a) −(TQ′)(x, a)| =
 Exa,π[R + γ max
a′∈A Q(X′, a′) −R −γ max
a′∈A Q′(X′, a′)]

=
 Exa,π[γ max
a′∈A Q(X′, a′) −γ max
a′∈A Q′(X′, a′)]

= γ
 Exa,π[max
a′∈A Q(X′, a′) −max
a′∈A Q′(X′, a′)]

≤γ Exa,π
| max
a′∈A Q(X′, a′) −max
a′∈A Q′(X′, a′)|
≤γ Exa,π
 max
a′∈A |Q(X′, a′) −Q′(X′, a′)|
≤γ
max
(x′,a′)∈X×A |Q(x′, a′) −Q′(x′, a′)|
= γ∥Q −Q′∥∞.
Since the bound holds for any (x, a) pair, it follows that
∥TQ −TQ′∥∞≤γ∥Q −Q′∥∞.
Corollary 7.5. For any initial state-action value function Q0 ∈RX×A, the
sequence of iterates Qk+1 = TQk converges to Q∗in the L∞norm.
△
We can use the unbiased estimation method of Section 6.2 to derive an
incremental algorithm for learning Q∗, since the contractive operator T is
expressible as an expectation over a sample transition. Given a realization
(x, a, r, x′), we construct the sample target
r + γ max
a′∈A Q(x′, a′) .
We then incorporate this target to an update rule to obtain the Q-learning
algorithm ﬁrst encountered in Chapter 3:
Q(x, a) ←(1 −α)Q(x, a) + α(r + γ max
a′∈X Q(x′, a′)) .
Under appropriate conditions, the convergence of Q-learning to Q∗can be
established with Lemma 7.3 and a suitable extension of the analysis of Chapter
6 to the space of action-value functions.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

202
Chapter 7
7.3
Distributional Value Iteration
Analogous to value iteration, we can devise a distributional dynamic pro-
gramming procedure for the risk-neutral control problem; such a procedure
determines an approximation to the return function of an optimal policy. As
we will see, in some circumstances, this can be accomplished without compli-
cations, giving some theoretical justiﬁcation for the distributional algorithm
presented in Section 3.7.
As in Chapter 5, we perform distributional dynamic programming by imple-
menting the combination of a projection step with a distributional optimality
operator.55 Because it is not possible to “directly maximize” a probability
distribution, we instead deﬁne the operator via a greedy selection rule G.
We can view the expected-value optimality operator T as substituting the
expectation over the next-state action A′ in Equation 7.5 by a maximization
over a′. As such, it can be rewritten as a particular policy evaluation operator
T π whose policy π depends on the operand Q; the mapping from Q to π is what
we call a greedy selection rule.
Deﬁnition 7.6. A greedy selection rule is a mapping G : RX×A →πms with the
property that for any Q ∈RX×A, G(Q) is greedy with respect to Q. That is,
G(Q)(a | x) > 0 =⇒Q(x, a) = max
a′∈A Q(x, a′) .
We extend G to return functions by deﬁning, for η ∈P1(R)X×A, the induced
state-action value function
Qη(x, a) =
E
Z∼η(x,a)[Z] ,
and then letting
G(η) = G(Qη) .
△
A greedy selection rule may produce stochastic policies, for example when
assigning equal probability to two equally valued actions. However, it must
select actions that are maximally valued according to Q. Given a greedy
selection rule, we may rewrite the Bellman optimality operator as
TQ = T G(Q)Q .
(7.7)
In the distributional setting, we must make explicit the dependency of the oper-
ator on the greedy selection rule G. Mirroring Equation 7.7, the distributional
55. As before, this implementation is at its simplest when there are ﬁnitely many possible states,
actions, and rewards, and the projection step can be computed eﬃciently. Alternatives include a
sample-based approach and, as we will see in Chapter 9, function approximation.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Control
203
η
η′
Q
Q′
ΠFT G
T
Figure 7.1
When the projection step ΠF is mean-preserving, distributional value iteration produces
the same sequence of state-action value functions as standard value iteration.
Bellman optimality operator derived from G is
T Gη = T G(η)η .
We will see that, unlike the expected-value setting, diﬀerent choices of the
greedy selection rule result in diﬀerent operators with possibly diﬀerent dynam-
ics – we thus speak of the distributional Bellman optimality operators, in the
plural.
Distributional value iteration algorithms combine a greedy selection rule G
and a projection step (described by the operator ΠF and implying a particular
choice of representation F) to compute an approximate optimal return function.
Given some initial return function η0 ∈F X×A, distributional value iteration
implements the iterative procedure
ηk+1 = ΠFT G(ηk)ηk.
(7.8)
In words, distributional value iteration selects a policy that at each state x is
greedy with respect to the expected values of ηk(x, ·) and computes the return
function resulting from a single step of distributional dynamic programming
with that policy.
The induced value function Qηk plays an important role in distributional
value iteration as it is used to derive the greedy policy πk = G(ηk). When ΠF is
mean-preserving (Section 5.11), Qηk behaves as if it had been computed from
standard value iteration (Figure 7.1). That is,
Qηk+1 = TQηk .
By induction, distributional value iteration then produces the same sequence of
state-action value functions as regular value iteration. That is, given the initial
condition
Q0(x, a) =
E
Z∼η0(x,a)[Z] ,
(x, a) ∈X × A ,
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

204
Chapter 7
and the recursion
Qk+1 = TQk ,
we have
Qηk = Qk , for all k ≥0 .
As a consequence, these two processes also produce the same greedy policies:
πk = G(Qk) .
In the following section, we will use this equivalence to argue that distributional
value iteration ﬁnds an approximation to an optimal return function. When ΠF
is not mean-preserving, however, Qηk may deviate from Qk. If that is the case,
the greedy policy πk is likely to be diﬀerent from G(Qk), and distributional
value iteration may converge to the return function of a suboptimal policy. We
discuss this point in Remark 7.2.
Before moving on, let us remark on an alternative procedure for approximat-
ing an optimal return function. This procedure ﬁrst performs standard value
iteration to obtain an approximation ˆQ to Q∗. A greedy policy ˆπ is then extracted
from ˆQ. Finally, distributional dynamic programming is used to approximate
the return function ηˆπ. If ˆπ is an optimal policy, this directly achieves our stated
aim and suggests doing away with the added complexity of distributional value
iteration. In larger problems, however, it is diﬃcult or undesirable to wait until
Q∗has been determined before learning or computing the return function, or it
may not be possible to decouple value and return predictions. In these situations,
it is sensible to consider distributional value iteration.
7.4
Dynamics of Distributional Optimality Operators
In this section and the next, we analyze the behavior of distributional optimality
operators; recall from Section 7.3 that there is one such operator for each choice
of greedy selection rule G, in contrast to non-distributional value iteration.
Combined with a mean-preserving projection, our analysis also informs the
behavior of distributional value iteration. As we will see, even in the absence of
approximation due to a ﬁnite-parameter representation, distributional optimality
operators exhibit complex behavior.
Thus far, we have analyzed distributional dynamic programming algorithms
by appealing to contraction mapping theory. Demonstrating that an opera-
tor is a contraction provides a good deal of understanding about algorithms
implementing its repeated application: they converge at a geometric rate to the
operator’s ﬁxed point (when such a ﬁxed point exists). Unfortunately, distri-
butional optimality operators are not contraction mappings, as the following
shows.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Control
205
η
T Gη
η∗
x, ·
γε
±γ
γε
y, a
−ε
ε
ε
y, b
±1
±1
±1
Figure 7.2
Left: A Markov decision process for which no distributional Bellman optimality operator
is a contraction mapping. Right: The optimal return-distribution function η∗and initial
return-distribution function η used in the proof of Proposition 7.7 (expressed in terms of
their support). Given a c-homogeneous probability metric d, the proof chooses ε so as to
make d(T Gη, η∗) > d(η, η∗).
Proposition 7.7. Consider a probability metric d and let d be its supremum
extension. Suppose that for any z, z′ ∈R,
d(δz, δz′) < ∞.
If d is c-homogeneous, there exist a Markov decision process and two
return-distribution functions η, η′ such that for any greedy selection rule G
and any discount factor γ ∈(0, 1],
d(T Gη, T Gη′) > d(η, η′) .
(7.9)
△
Proof. Consider an MDP with two nonterminal states x, y and two actions a, b
(Figure 7.2). From state x, all actions transition to state y and yield no reward.
In state y, action a results in a reward of ε > 0, while action b results in a reward
that is −1 or 1 with equal probability; both actions are terminal. We will argue
that ε can be chosen to make T G satisfy Equation 7.9.
First note that any optimal policy must choose action a in state y, and all
optimal policies share the same return-distribution function η∗. Consider another
return-distribution function η that is equal to η∗at all state-action pairs except
that η(y, a) = δ−ε (Figure 7.2, right). This implies that the greedy selection rule
must select b in y, and hence
(T Gη)(x, a) = (T Gη)(x, b) = (b0,γ)#η(y, b) = (b0,γ)#η∗(y, b) .
Because both actions are terminal from y, we also have that
(T Gη)(y, a) = η∗(y, a)
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

206
Chapter 7
(T Gη)(y, b) = η∗(y, b).
Let us write ν = 1
2δ−1 + 1
2δ1 for the reward distribution associated with (y, b).
We have
d(η, η∗) = d η(y, a), η∗(y, a)
= d(δ−ε, δε) ,
(7.10)
and
d(T Gη, T Gη∗) = d (T Gη)(x, a), (T Gη∗)(x, a)
= d (b0,γ)#η∗(y, b), (b0,γ)#η∗(y, a)
= γcd(ν, δε) ,
(7.11)
where the last line follows by c-homogeneity (Deﬁnition 4.22). We will show
that for suﬃciently small ε > 0, we have
d(ν, δε) > γ−cd(δ−ε, δε) ,
from which the result follows by Equations 7.10 and 7.11. To this end, note that
d(δ−ε, δε) = εcd(δ−1, δ1).
(7.12)
Since d is ﬁnite for any pair of Dirac deltas, we have that d(δ−1, δ1) < ∞and so
lim
ε→0 d(δ−ε, δε) = 0 .
(7.13)
On the other hand, by the triangle inequality we have
d(ν, δε) + d(δ0, δε) ≥d(ν, δ0) > 0 ,
(7.14)
where the second inequality follows because ν and δ0 are diﬀerent distributions.
Again by c-homogeneity of d, we deduce that
lim
ε→0 d(δ0, δε) = 0 ,
and hence
lim inf
ε→0
d(ν, δε) ≥d(ν, δ0) > 0 .
Therefore, for ε > 0 suﬃciently small, we have
d(ν, δε) > γ−cd(δ−ε, δε) ,
as required.
Proposition 7.7 establishes that for any metric d that is suﬃciently well
behaved to guarantee that policy evaluation operators are contraction mappings
(ﬁnite on bounded-support distributions, c-homogeneous), optimality operators
are not generally contraction mappings with respect to d. As a consequence,
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Control
207
we cannot directly apply the tools of Chapter 4 to characterize distributional
optimality operators. The issue, which is implied in the proof above, is that it is
possible for distributions to have similar expectations yet diﬀer substantially,
for example, in their variance.
With a more careful line of reasoning, we can still identify situations in which
the iterates ηk+1 = T Gηk do converge. The most common scenario is when there
is a unique optimal policy. In this case, the analysis is simpliﬁed by the existence
of an action gap in the optimal value function.56
Deﬁnition 7.8. Let Q ∈RX×A. The action gap at a state x is the diﬀerence
between the highest-valued and second highest-valued actions:
gap(Q, x) = min Q(x, a∗) −Q(x, a) : a∗, a ∈A, a∗, a, Q(x, a∗) = max
a′∈A Q(x, a′)	 .
The action gap of Q is the smallest gap over all states:
gap(Q) = min
x∈X gap(Q, x) .
By extension, the action gap for a return function η is
gap(η) = min
x∈X gap(Qη, x) ,
Qη(x, a) =
E
Z∼η(x,a)[Z] .
△
Deﬁnition 7.8 is such that if two actions are optimal at a given state, then
gap(Q∗) = 0. When gap(Q∗) > 0, however, there is a unique action that is optimal
at each state (and vice versa). In this case, we can identify an iteration K from
which G(ηk) = π∗for all k ≥K. From that point on, any distributional optimality
operator reduces to the π∗evaluation operator, which enjoys now-familiar
convergence guarantees.
Theorem 7.9. Let T G be the distributional Bellman optimality operator
instantiated with a greedy selection rule G. Suppose that there is a unique
optimal policy π∗and let p ∈[1, ∞]. Under Assumption 4.29(p) (well-
behaved reward distributions), for any initial return-distribution function
η0 ∈P(R), the sequence of iterates deﬁned by
ηk+1 = T Gηk
converges to ηπ∗with respect to the metric wp.
△
Theorem 7.9 is stated in terms of supremum p-Wasserstein distances for
conciseness. Following the conditions of Theorem 4.25 and under a diﬀerent
set of assumptions, we can of course also establish the convergence in, say, the
supremum Cramér distance.
56. Example 6.1 introduced an update rule that increases this action gap.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

208
Chapter 7
Corollary 7.10. Suppose that ΠF is a mean-preserving projection for some
representation F and that there is a unique optimal policy π∗. If Assump-
tion 5.22(wp) holds and ΠF is a nonexpansion in wp, then under the conditions
of Theorem 7.9, the sequence
ηk+1 = ΠFT Gηk
produced by distributional value iteration converges to the ﬁxed point
ˆηπ∗= ΠFT π∗ˆηπ∗.
△
Proof of Theorem 7.9. As there is a unique optimal policy, it must be that the
action gap of Q∗is strictly greater than zero. Fix ε = 1
2gap(Q∗). Following the
discussion of the previous section, we have that
Qηk+1 = TQηk .
Because T is a contraction in L∞norm, we know that there exists a Kε ∈N after
which
∥Qηk −Q∗∥∞< ε
∀k ≥Kε .
(7.15)
For a ﬁxed x, let a∗be the optimal action in that state. From Equation 7.15, we
deduce that for any a , a∗,
Qηk(x, a∗) ≥Q∗(x, a∗) −ε
≥Q∗(x, a) + gap(Q∗) −ε
> Qηk(x, a) + gap(Q∗) −2ε
= Qηk(x, a) .
Thus, the greedy action in state x after time Kε is the optimal action for that
state. Thus, G(ηk) = π∗for k ≥Kε and
ηk+1 = T π∗ηk
k ≥Kε.
We can treat ηKε as a new initial condition η′
0, and apply Proposition 4.30 to
conclude that ηk →ηπ∗.
In Section 3.7, we introduced the categorical Q-learning algorithm in terms
of a deterministic greedy policy. Generalized to an arbitrary greedy selection
rule G, categorical Q-learning is deﬁned by the update
η(x, a) ←(1 −α)η(x, a) + α
X
a′∈A
G(η)(a′ | x′)

Πc(br,γ)#η x′, a′
.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Control
209
Because the categorical projection is mean-preserving, its induced state-value
function follows the update
Qη(x, a) ←(1 −α)η(x, a) + α
X
a′∈A
G(η)(a′ | x′) r + γQη(x′, a′)
|                                    {z                                    }
r+γ max
a′∈A Qη(x′,a′)
.
Using the tools of Chapter 6, one can establish the convergence of categorical
Q-learning under certain conditions, including the assumption that there is a
unique optimal policy π∗. The proof essentially combines the insight that, under
certain conditions, the sequence of greedy policies tracked by the algorithm
matches that of Q-learning and hence eventually converges to π∗, at which point
the algorithm is essentially performing categorical policy evaluation of π∗. The
actual proof is somewhat technical; we omit it here and refer the interested
reader to Rowland et al. (2018).
7.5
Dynamics in the Presence of Multiple Optimal Policies*
In the value-based setting, it does not matter which greedy selection rule is used
to represent the optimality operator: By deﬁnition, any greedy selection rule
must be equivalent to directly maximizing over Q(x, ·). In the distributional
setting, however, diﬀerent rules usually result in diﬀerent operators. As a con-
crete example, compare the rule “among all actions whose expected value is
maximal, pick the one with smallest variance” to “assign equal probability to
actions whose expected value is maximal.”
Theorem 7.9 relies on the fact that, when there is a unique optimal policy
π∗, we can identify a time after which the distributional optimality operator
behaves like a policy evaluation operator. When there are multiple optimal
policies, however, the action gap of the optimal value function Q∗is zero and
the argument cannot be used. To understand why this is problematic, it is useful
to write the iterates (ηk)k≥0 more explicitly in terms of the policies πk = G(ηk):
ηk+1 = T πkηk = T πkT πk−1ηk−1 = T πk · · · T π0η0 .
(7.16)
When the action gap is zero, the sequence of policies πk, πk+1, . . . may con-
tinue to vary over time, depending on the greedy selection rule. Although all
optimal actions have the same optimal value (guaranteeing the convergence
of the expected values to Q∗), they may correspond to diﬀerent distributions.
Thus, distributional value iteration – even with a mean-preserving projection –
may mix together the distribution of diﬀerent optimal policies. Even if (πk)k≥0
converges, the policy it converges to may depend on initial conditions (Exercise
7.5). In the worst case, the iterates (ηk)k≥0 might not even converge, as the
following example shows.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

210
Chapter 7
0
10
20
30
40
50
60
Iteration (k)
1.0
0.5
0.0
0.5
1.0
Gk(x, a)
Figure 7.3
Left: A Markov decision process in which the sequence of return function estimates
(ηk)k≥0 does not converge (Example 7.11). Right: The return-distribution function esti-
mate at (x, a) as a function of k and beginning with c0 = 1. At each k, the pair of dots
indicates the support of the distribution.
Example 7.11 (Failure to converge). Consider a Markov decision process with
a single nonterminal state x and two actions, a and b (Figure 7.3, left). Action
a gives a reward of 0 and leaves the state unchanged, while action b gives a
reward of −1 or 1 with equal probability and leads to a terminal state. Note that
the expected return from taking either action is 0.
Let γ ∈(0, 1). We will exhibit a sequence of return function estimates (ηk)k≥0
that is produced by distributional value iteration and does not have a limit. We
do so by constructing a greedy selection rule that achieves the desired behavior.
Suppose that
η0(x, b) = 1
2(δ−1 + δ1) .
For any initial parameter c0 ∈R, if
η0(x, a) = 1
2(δ−c0 + δc0) ,
then by induction, there is a sequence of scalars (ck)k≥0 such that
ηk(x, a) = 1
2(δ−ck + δck)
∀k ∈N .
(7.17)
This is because
ηk+1(x, a) = (b0,γ)#ηk(x, a)
or
ηk+1(x, a) = (b0,γ)#ηk(x, b), k ≥0 .
Deﬁne the following greedy selection rule: at iteration k + 1, choose a if ck ≥1
10;
otherwise, choose b. With some algebra, this leads to the recursion (k ≥0)
ck+1 =
( γ
if ck < 1
10,
γck
otherwise.
Over a period of multiple iterations, the estimate ηk(x, a) exhibits cyclical
behavior (Figure 7.3, right).
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Control
211
The example illustrates how, without additional constraints on the greedy
selection rule, it is not possible to guarantee that the iterates converge. However,
one can prove a weaker result based on the fact that only optimal actions must
eventually be chosen.
Deﬁnition 7.12. For a given Markov decision process M, the set of nonstation-
ary Markov optimal return-distribution functions is
ηnmo = {η¯π : ¯π = (πk)k≥0, πk ∈πms is optimal for M, ∀k ∈N}.
△
In particular, any history-dependent policy ¯π satisfying the deﬁnition above is
also optimal for M.
Theorem 7.13. Let T G be a distributional Bellman optimality operator
instantiated with some greedy selection rule, and let p ∈[1, ∞]. Under
Assumption 4.29(p), for any initial condition η0 ∈Pp(R)X the sequence
of iterates ηk+1 = T Gηk converges to the set ηnmo, in the sense that
lim
k→∞inf
η∈ηnmo wp(η, ηk) = 0.
△
Proof. Along the lines of the proof of Theorem 7.9, there must be a time K ∈N
after which the greedy policies πk, k ≥K are optimal. For l ∈N+, let us construct
the history-dependent policy
¯π = πK+l−1, πK+l−2, . . . , πK+1, πK, π∗, π∗, . . . ,
where π∗is some stationary Markov optimal policy. Denote the return of this
policy from the state-action pair (x, a) by G¯π(x, a) and its return-distribution
function by η¯π. Because ¯π is an optimal policy, we have that η¯π ∈ηnmo. Let
Gk be an instantiation of ηk, for each k ∈N. Now for a ﬁxed x ∈X, a ∈A,
let (Xt, At, Rt)l
t=0 be the initial segment of a random trajectory generated by
following ¯π beginning with X0 = x, A0 = a. More precisely, for t = 1, . . . , l, we
have
At | (X0:t, A0:t−1, R0:t−1) ∼πK+l−t( · | Xt) .
From this, we write
G¯π(x, a)
D=
l−1
X
t=0
γtRt + γlGπ∗(Xl, Al) .
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

212
Chapter 7
Because ηK+l = T πK+l−1 · · · T πKηK, by inductively applying Proposition 4.11, we
also have
GK+l(x, a)
D=
l−1
X
t=0
γtRt + γlGK(Xl, Al) .
Hence,
wp
 ηK+l(x, a), η¯π(x, a) = wp
 GK+l(x, a),G¯π(x, a)
= wp
 l−1
X
t=0
γtRt + γlGK(Xl, Al),
l−1
X
t=0
γtRt + γlGπ∗(Xl, Al)

.
Consequently,
wp
 ηK+l(x, a), η¯π(x, a) ≤wp(γlGK(Xl, Al), γlGπ∗(Xl, Al))
= γlwp(GK(Xl, Al),Gπ∗(Xl, Al))
≤γlwp(GK,Gπ∗) ,
following the arguments in the proof of Proposition 4.15. The result now follows
by noting that wp(GK,Gπ∗) < ∞under our assumptions, taking the supremum
over (x, a) on the left-hand side, and taking the limit with respect to l.
Theorem 7.13 shows that, even before the eﬀect of the projection step is
taken into account, the behavior of distributional value iteration is in general
quite complex. When the iterates ηk are approximated (for example, because
they are estimated from samples), nonstationary Markov return-distribution
functions may also be produced by distributional value iteration – even when
there is a unique optimal policy.
It may appear that the convergence issues highlighted by Example 7.11 and
Theorem 7.13 are consequences of using the wrong greedy selection rule. To
address these issues, one may be tempted to impose an ordering on policies (for
example, always prefer the action with the lowest variance, at equal expected
values). However, it is not clear how to do this in a satisfying way. One hurdle
is that, to avoid the cyclical behavior demonstrated in Example 7.11, we would
like a greedy selection rule that is continuous with respect to its input. This
seems problematic, however, since we also need this rule to return the correct
answer when there is a unique optimal (and thus deterministic) policy (Exercise
7.7). This suggests that when learning to control with a distributional approach,
the learned return distributions may simultaneously reﬂect the random returns
from multiple policies.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Control
213
7.6
Risk and Risk-Sensitive Control
Imagine being invited to interview for a desirable position at a prestigious
research institute abroad. Your plane tickets and the hotel have been booked
weeks in advance. Now the night before an early morning ﬂight, you make
arrangements for a cab to pick you up from your house and take you to the
airport. How long in advance of your plane’s actual departure do you request
the cab for? If someone tells you that, on average, a cab to your local airport
takes an hour – is that suﬃcient information to make the booking? How does
your answer change when the ﬂight is scheduled around rush hour, rather than
early morning?
Fundamentally, it is often desirable that our choices be informed by the
variability in the process that produces outcomes from these choices. In this
context, we call this variability risk. Risk may be inherent to the process or
incomplete knowledge about the state of the world (including any potential
traﬃc jams and the mechanical condition of the hired cab).
In contrast to risk-neutral behavior, decisions that take risk into account are
called risk-sensitive. The language of distributional reinforcement learning is
particularly well suited for this purpose, since it lets us reason about the full
spectrum of outcomes, along with their associated probabilities. The rest of
the chapter gives an overview of how one may account for risk in the decision-
making process and of the computational challenges that arise when doing so.
Rather than be exhaustive, here we take the much more modest aim of exposing
the reader to some of the major themes in risk-sensitive control and their relation
to distributional reinforcement learning; references to more extensive surveys
are provided in the bibliographical remarks.
Recall that the risk-neutral objective is to maximize the expected return from
the (possibly random) initial state X0:
J(π) = Eπ
h ∞
X
t=0
γtRt
i
= Eπ
h
Gπ(X0)
i
.
Here, we may think of the expectation as mapping the random variable Gπ(X0)
to a scalar. Risk-sensitive control is the problem that arises when we replace
this expectation by a risk measure.
Deﬁnition 7.14. A risk measure57 is a mapping
ρ : Pρ(R) →[−∞, ∞) ,
57. More precisely, this is a static risk measure, in that it is only concerned with the return from
time t = 0. See bibliographical remarks.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

214
Chapter 7
deﬁned on a subset Pρ(R) ⊆P(R) of probability distributions. By extension,
for a random variable Z instantiating the distribution ν, we write ρ(Z) = ρ(ν).
△
Problem 7.15 (Risk-sensitive control). Given an MDP (X, A, ξ0, PX, PR), a
discount factor γ ∈[0, 1), and a risk measure ρ, ﬁnd a policy π ∈πh maximizing58
Jρ(π) = ρ
 ∞
X
t=0
γtRt

.
(7.18)
△
In Problem 7.15, we assume that the distribution of the random return lies
in Pρ(R), similar to our treatment of probability metrics in Chapter 4. From a
technical perspective, subsequent examples and results should be interpreted
with this assumption in mind.
The risk measure ρ may take into account higher-order moments of the return
distribution, be sensitive to rare events, and even disregard the expected value
altogether. Note that according to this deﬁnition, ρ = E also corresponds to a
risk-sensitive control problem. However, we reserve the term for risk measures
that are sensitive to more than only expected values.
Example 7.16 (Mean-variance criterion). Let λ > 0. The variance-penalized
risk measure penalizes high-variance outcomes:
ρλ
mv(Z) = E[Z] −λVar(Z) .
△
Example 7.17 (Entropic risk). Let λ > 0. Entropic risk puts more weight on
smaller-valued outcomes:
ρλ
er(Z) = −1
λ log E[e−λZ] .
△
Example 7.18 (Value-at-risk). Let τ ∈(0, 1). The value-at-risk measure (Figure
7.4) corresponds to the τth quantile of the return distribution:
ρτ
VaR(Z) = F−1
Z (τ) .
△
7.7
Challenges in Risk-Sensitive Control
Many convenient properties of the risk-neutral objective do not carry over to
risk-sensitive control. As a consequence, ﬁnding an optimal policy is usually
signiﬁcantly more involved. This remains true even when the risk-sensitive
58. Typically, Problem 7.15 is formulated in terms of a risk to be minimized, which linguistically is
a more natural objective. Here, however, we consider the maximization of Jρ(π) so as to keep the
presentation uniﬁed with the rest of the book.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Control
215
2
0
2
4
6
Return
0.0
0.2
0.4
0.6
0.8
1.0
Cumulative Probability
VaR
CVaR
Figure 7.4
Illustration of value-at-risk (VaR) and conditional value-at-risk (CVaR). Depicted is the
cumulative distribution function of the mixture of normal distributions ν = 1
2N(0, 1) +
1
2N(4, 1). The dashed line corresponds to VaR; CVaR (τ = 0.4) can be determined from
a suitable transformation of the shaded area (see Section 7.8 and Exercise 7.10).
objective (Equation 7.18) can be evaluated eﬃciently: for example, by using
distributional dynamic programming to approximate the return-distribution func-
tion ηπ. In this section, we illustrate some of these challenges by characterizing
optimal policies for the variance-constrained control problem.
The variance-constrained problem introduces risk sensitivity by forbidding
policies whose return variance is too high. Given a parameter C ≥0, the
objective is to
maximize
Eπ[Gπ(X0)]
subject to
Varπ
 Gπ(X0) ≤C .
(7.19)
Equation 7.19 can be shown to satisfy our deﬁnition of a risk-sensitive control
problem if we express it in terms of a Lagrange multiplier:
Jvc(π) = min
λ≥0

Eπ
Gπ(X0) −λ Varπ
 Gπ(X0) −C
.
The variance-penalized and variance-constrained problems are related in that
they share the Pareto set πpar ⊆πh of possibly optimal solutions. A policy π is
in the set πpar if we have that for all π′ ∈πh,
(a) Var Gπ(X0) > Var Gπ′(X0) =⇒E[Gπ(X0)] > E[Gπ′(X0)], and
(b) Var Gπ(X0) = Var Gπ′(X0) =⇒E[Gπ(X0)] ≥E[Gπ′(X0)].
In words, between two policies with equal variances, the one with lower expecta-
tion is never a solution to either problem. However, these problems are generally
not equivalent (Exercise 7.8).
Proposition 7.1 establishes the existence of a solution of the risk-neutral
control problem that is (a) deterministic, (b) stationary, and (c) Markov. By
contrast, the solution to the variance-constrained problem may lack any or all
of these properties.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

216
Chapter 7
(a)
(b)
(c)
Figure 7.5
Examples demonstrating how the optimal policy for the variance-constrained control
problem might not be (a) deterministic, (b) Markov, or (c) stationary.
Example 7.19 (The optimal policy may not be deterministic). Consider the
problem of choosing between two actions, a and b. Action a always yields a
reward of 0, while action b yields a reward of 0 or 1 with equal probability
(Figure 7.5a). If we seek the policy that maximizes the expected return subject
to the variance constraint C = 3/16, the best deterministic policy respecting the
variance constraint must choose a, for a reward of 0. On the other hand, the
policy that selects a and b with equal probability achieves an expected reward
of 1/4 and a variance of 3/16.
△
Example 7.20 (The optimal policy may not be Markov). Consider the Markov
decision process in Figure 7.5b. Suppose that we seek a policy that maximizes
the expected return from state x, now subject to the variance constraint C = 0.
Let us assume γ = 1 for simplicity. Action a has no variance and is therefore
a possible solution, with zero return. Action b gives a greater expected return,
at the cost of some variance. Any policy π that depends on the state alone
and chooses b in state x must incur this variance. On the other hand, the
following history-dependent policy achieves a positive expected return without
violating the variance constraint: in state x, choose b; if the ﬁrst reward R0 is 0,
select action b in x; otherwise, select action a. In all cases, the return is 1, an
improvement over the best Markov policy.
△
Example 7.21 (The optimal policy may not be stationary). In general, the
optimal policy may require keeping track of time. Consider the problem of
maximizing the expected return from the unique state x in Figure 7.5c, subject
to Varπ(Gπ(X0)) ≤C, for C ≤1/4. Exercise 7.9 asks you to show that a simple
time-dependent policy that chooses a for TC steps and then selects b achieves an
expected return of up to
√
C. This is possible because the variance of the return
decays at a rate of γ2, while its expected value decays at the slower rate of γ.
By contrast, the best randomized stationary policy performs substantially worse
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Control
217
0.5
0.6
0.7
0.8
0.9
1.0
Discount factor 
0.05
0.10
0.15
0.20
0.25
0.30
Expected return
C = 0.10
C = 0.05
C = 0.01
Figure 7.6
Expected return as a function of the discount factor γ and variance constraint C in
Example 7.21. Solid and dashed lines indicate the expected return of the best stationary
and time-dependent policies, respectively. The peculiar zigzag shape of the curve for
the time-varying policy arises because the time TC at which action b is taken must be an
integer (see Exercise 7.9).
for a discount factor γ close to 1 and small values of C (Figure 7.6). Intuitively,
a randomized policy must choose a with a suﬃciently large probability to avoid
receiving the random reward early, which prevents it from selecting b quickly
beyond the threshold of TC time steps.
△
The last two examples establish that the variance-constrained risk measure
is time-inconsistent: informally, the agent’s preference for one outcome over
another at time t may be reversed at a later time. Compared to the risk-neutral
problem, the variance-constrained problem is more challenging because the
space of policies that one needs to consider is much larger. Among other things,
the lack of an optimal policy that is Markov with respect to the state alone
also implies that the dependency on the initial distribution in Equation 7.19 is
necessary to keep things well deﬁned. The variance-constrained objective must
be optimized for globally, considering the policy at all states at once; this is in
contrast with the risk-neutral setting, where value iteration can make overall
improvements to the policy by acting greedily with respect to the value function
at individual states (see Remark 7.1). In fact, ﬁnding an optimal policy for the
variance-constrained control problem is NP-hard (Mannor and Tsitsiklis 2011).
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

218
Chapter 7
7.8
Conditional Value-At-Risk*
In the previous section, we saw that solutions to the variance-constrained control
problem can take unintuitive forms, including the need to penalize better-than-
expected outcomes. One issue is that variance only coarsely measures what we
mean by “risk” in the common sense of the word. To reﬁne our meaning, we
may identify two types of risk: downside risk, involving undesirable outcomes
such as greater-than-expected losses, and upside risk, involving what we may
informally call a stroke of luck. In some situations, it is possible and useful to
separately account for these two types of risk.
To illustrate this point, we now present a distributional algorithm for optimiz-
ing conditional value-at-risk (CVaR), based on work by Bäuerle and Ott (2011)
and Chow et al. (2015). One beneﬁt of working with full return distributions
is that the algorithmic template we present here can be reasonably adjusted to
deal with other risk measures, including the entropic risk measure described in
Example 7.17. For conciseness, in what follows, we will state without proof a
few technical facts about conditional value-at-risk that can be found in those
sources and the work of Rockafellar and Uryasev (2002).
Conditional value-at-risk measures downside risk by focusing on the lower
tail behavior of the return distribution, speciﬁcally the expected value of this
tail. This expected value quantiﬁes the magnitude of losses in extreme scenarios.
Let Z be a random variable with cumulative and inverse cumulative distribution
functions FZ and F−1
Z , respectively. For a parameter τ ∈(0, 1), the CVaR of Z is
CVaRτ(Z) = 1
τ
Z τ
0
F−1
Z (u)du .
(7.20)
When the inverse cumulative distribution F−1
Z is strictly increasing, the right-
hand side of Equation 7.20 is equivalent to
E[Z | Z ≤F−1
Z (τ)] .
(7.21)
In this case, CVaR quantiﬁes the expected return, conditioned on the event that
this return is no greater than the return’s τth quantile – that is, is within the τth
fraction of lowest returns.59 In a reinforcement learning context, this leads to
the risk-sensitive objective
JCVaR(π) = CVaRτ
 ∞
X
t=0
γtRt

.
(7.22)
59. In other ﬁelds, CVaR is applied to losses rather than returns, in which case it measures the
expected loss subject that this loss is above the τth percentile. For example, Equation 7.21 becomes
E[Z | Z ≥F−1
Z (τ)], and the subsequent derivations need to be adjusted accordingly.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Control
219
In general, there may not be an optimal policy that depends on the state x alone;
however, one can show that optimality can be achieved with a deterministic,
stationary Markov policy on an augmented state that incorporates information
about the return thus far. In the context of this section, we assume that rewards
are bounded in [Rmin, Rmax]. At a high level, we optimize the CVaR objective by
(a) deﬁning the requisite augmented state;
(b) performing a form of risk-sensitive value iteration on this augmented state,
using a suitable selection rule; and
(c) extracting the resulting policy.
We now explain each of these steps.
Augmented state. Central to the algorithm and to the state augmentation
procedure is a reformulation of CVaR in terms of a desired minimum return or
target b ∈R. Let [x]+ denote the function that is 0 if x < 0 and x otherwise. For
a random variable Z and τ ∈(0, 1), Rockafellar and Uryasev (2002) establish
that
CVaRτ(Z) = max
b∈R

b −τ−1 E [b −Z]+
.
(7.23)
When F−1
Z is strictly increasing, the maximum-achieving b for Equation 7.23
is the quantile F−1
Z (τ). In fact, taking the derivative of the expression inside
the brackets with respect to α yields the quantile update rule (Equation 6.11;
see Exercise 7.11). The advantage of this formulation is that it is more easily
optimized in the context of a policy-dependent return. To see this, let us write
Gπ =
∞
X
t=0
γtRt
to denote the random return from the initial state X0, following some history-
dependent policy π ∈πh. We then have that
max
π∈πh JCVaR(π) = max
π∈πh max
b∈R

b −τ−1 E [b −Gπ]+
= max
b∈R

b −τ−1 min
π∈πh E [b −Gπ]+
.
(7.24)
In words, the CVaR objective can be optimized by jointly ﬁnding an optimal
target b and a policy that minimizes the expected shortfall E [b −Gπ]+. For a
ﬁxed target b, we will see that it is possible to minimize the expected shortfall
by means of dynamic programming. By adjusting b appropriately, one then
obtains an optimal policy.
Based on Equation 7.24, let us now consider an augmented state (Xt, Bt),
where Xt is as usual the current state and Bt takes on values in B = [Vmin, Vmax];
we will describe its dynamics in a moment. With this augmented state, we
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

220
Chapter 7
may consider a class of stationary Markov policies, πCVaR, which take the form
π : X × B →P(A).60
We use the variable Bt to keep track of the amount of discounted reward
that should be obtained from Xt onward in order to achieve a desired minimum
return of b0 ∈R over the entire trajectory. The transition structure of the Markov
decision process over the augmented state is deﬁned by modifying the generative
equations (Section 2.3):
B0 = b0
At | (X0:t, B0:t, A0:t−1, R0:t−1) ∼π(· | Xt, Bt)
Bt+1 | (X0:t+1, B0:t, A0:t, R0:t) = Bt −Rt
γ
;
we similarly extend the sample transition model with the variables B and B′.
This deﬁnition of the variables (Bt)t≥0 can be understood by noting, for example,
that a minimum return of b0 is achieved over the whole trajectory if
∞
X
t=1
γt−1Rt ≥b0 −R0
γ
.
If the reward Rt is small or negative, the new target Bt+1 may of course be larger
than Bt. Note that the value of b0 is a parameter of the algorithm, rather than
given by the environment.
Risk-sensitive value iteration. We next construct a method for optimizing
the expected shortfall given a target b0 ∈R. Let us write η ∈P(R)X×B×A for a
return-distribution function on the augmented state-action space, instantiated
as a return-variable function G. For ease of exposition, we will mostly work
with this latter form of the return function. As usual, we write Gπ for the return-
variable function associated with a policy π ∈πCVaR. With this notation in mind,
we write
JCVaR(π, b0) = max
b∈R

b −τ−1 E [b −Gπ(X0, b0, A0)]+
(7.25)
to denote the conditional value-at-risk obtained by following policy π from the
initial state (X0, b0), with A0 ∼π(· | X0, b0).
Similar to distributional value iteration, the algorithm constructs a series of
approximations to the return-distribution function by repeatedly applying the
distributional Bellman operator with a policy derived from a greedy selection
rule ˜G. Speciﬁcally, we write
aG(x, b) = arg min
a∈A
E [b −G(x, b, a)]+
(7.26)
60. Since Bt is a function of the trajectory up to time t, πCVaR is a strict subset of πh.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Control
221
for the greedy action at the augmented state (x, b), breaking ties arbitrarily. The
selection rule ˜G is itself given by
˜G(η)(a | x, b) = ˜G(G)(a | x, b) = 1{a = aG(x, b)}.
The algorithm begins by initializing η0(x, b, a) = δ0 for all x ∈X, b ∈B, and
a ∈A, and iterating
ηk+1 = T
˜Gηk,
(7.27)
as in distributional value iteration. Expressed in terms of return-variable
functions, this is
Gk+1(x, b, a)
D= R + γGk(X′, B′, aGk(X′, B′)),
X = x, B = b, A = a.
After k iterations, the policy πk = ˜G(ηk) can be extracted according to Equation
7.26. As suggested by Equation 7.25, a suitable choice of starting state is
b0 = arg max
b∈B

b −τ−1 E [b −Gk(X0, b, aGk(X0, b))]+
.
(7.28)
As given, there are two hurdles to producing a tractable implementation of
Equation 7.27: in addition to the usual concern that return distributions may
need to be projected onto a ﬁnite-parameter representation, we also have to
contend with a real-valued state variable Bt. Before discussing how this can be
addressed, we ﬁrst establish that Equation 7.27 is a sound approach to ﬁnding
an optimal policy for the CVaR objective.
Theorem 7.22. Consider the sequence of return-distribution functions
(ηk)k≥0 deﬁned by Equation 7.27. Then the greedy policy πk = ˜G(ηk) is such
that for all x ∈X, b ∈B, and a ∈A,
E [b −Gπk(x, b, a)]+ ≤min
π∈πCVaR E [b −Gπ(X0, b, a)]+ + γk Vmax −Vmin

1 −γ
.
Consequently, we also have that the conditional value-at-risk of this policy
satisﬁes (with b0 given by Equation 7.28)
JCVaR(πk, b0) ≥max
b∈B max
π∈πCVaR JCVaR(π, b) −γk Vmax −Vmin

τ(1 −γ)
.
△
The proof is somewhat technical and is provided in Remark 7.3.
Theorem 7.22 establishes that with suﬃciently many iterations, the policy
πk is close to optimal. Of course, when distribution approximation is intro-
duced, the resulting policy will in general only approximately optimize the
CVaR objective, with an error term that depends on the expressivity of the
probability distribution representation (i.e., the parameter m in Chapter 5). To
perform dynamic programming with the state variable B, one may use function
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

222
Chapter 7
approximation, the subject of the next chapter. Another solution is to consider a
discrete number of values for B and to extend the operator T ˜G to operate on
this discrete set (Exercise 7.12).
7.9
Technical Remarks
Remark 7.1. In Section 7.7, we presented some of the challenges involved with
ﬁnding an optimal policy for the variance-constrained objective. In some sense,
these challenges should not be too surprising given that that we are looking to
maximize a function J of an inﬁnite-dimensional object (a history-dependent
policy). Rather, what should be surprising is the relative ease with which one
can obtain an optimal policy in the risk-neutral setting.
From a technical perspective, this ease is a consequence of Lemma 7.3, which
guarantees that Q∗(and hence π∗) can be eﬃciently approximated. However,
another important property of the risk-neutral setting is that the policy can be
improved locally: that is, at each state simultaneously. To see this, consider a
state-action value function Qπ for a given policy π and denote by π′ a greedy
policy with respect to Qπ. Then,
TQπ = T π′Qπ ≥T πQπ = Qπ .
(7.29)
That is, a single step of value iteration applied to the value function of a policy
π results in a new value function that is at least as good as Qπ at all states
– the Bellman operator is said to be monotone. Because this single step also
corresponds to the value of a nonstationary policy that acts according to π′ for
one step and then switches to π, we can equivalently interpret it as constructing,
one step at a time, a deterministic history-dependent policy for solving the
risk-neutral problem.
By contrast, it is not possible to use a direct dynamic programming approach
over the objective J to ﬁnd the optimal policy for an arbitrary risk-sensitive
control problem. A practical alternative is to perform the optimization instead
with an ascent procedure (e.g., a policy gradient-type algorithm). Ascent algo-
rithms can often be computed in closed form, and tend to be simpler to
implement. On the other hand, convergence is typically only guaranteed to
local optima, seemingly unavoidable when the optimization problem is known
to be computationally hard.
△
Remark 7.2. When the projection ΠF is not mean-preserving, distributional
value iteration induces a state-action value function Qηk that is diﬀerent from
the value function Qk determined by standard value iteration under equivalent
initial conditions. Under certain conditions on the distributions of rewards, it is
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Control
223
possible to bound this diﬀerence as k →∞. To do so, we use a standard error
bound on approximate value iteration (see, e.g., Bertsekas 2012).
Lemma 7.23. Let (Qk)k≥0 be a sequence of iterates in RX×A satisfying
∥Qk+1 −TQk∥∞≤ε
for some ε > 0 and where T is the Bellman optimality operator. Then,
lim sup
k→∞
∥Qk −Q∗∥∞≤
ε
1 −γ .
△
In the context of distributional value iteration, we need to bound the diﬀerence
∥Qηk+1 −TQηk∥∞.
When the rewards are bounded on the interval [Rmin, Rmax] and the projection
step is Πq, the w1-projection onto the m-quantile representation, a simple bound
follows from an intermediate result used in proving Lemma 5.30 (see Exercise
5.20). In this case, for any ν bounded on [Vmin, Vmax],
w1(Πqν, ν) ≤Vmax −Vmin
2m
;
Conveniently, the 1-Wasserstein distance bounds the diﬀerence of means
between any distributions ν, ν′ ∈P1(R):
 E
Z∼ν[Z] −E
Z∼ν′[Z]
 ≤w1(ν, ν′) .
This follows from the dual representation of the Wasserstein distance (Villani
2008). Consequently, for any (x, a),
Qηk+1(x, a) −(T πkQηk)(x, a)
 ≤w1
 ηk+1(x, a), (T πkηk)(x, a)
= w1
 (ΠqT πkηk)(x, a), (T πkηk)(x, a)
≤Vmax −Vmin
2m
.
By taking the maximum over (x, a) on the left-hand side of the above and
combining with Lemma 7.23, we obtain
lim sup
k→∞
∥Qηk −Q∗∥∞≤Vmax −Vmin
2m(1 −γ) .
△
Remark 7.3. Theorem 7.22 is proven from a few facts regarding partial returns,
which we now give. Let us write ak(x, b) = aGk(x, b). We deﬁne the mapping
Uk : X × B × A →R as
Uk(x, b, a) = E [b −Gk(x, b, a)]+ ,
and for a policy π ∈πCVaR similarly write
Uπ(x, b, a) = E [b −Gπ(x, b, a)]+ .
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

224
Chapter 7
Lemma 7.24. For any x ∈X, a ∈A, b ∈B, and k ∈N, we have
Uk+1(x, b, a) = γ E
h
Uk
 X′, B′, ak(X′, B′) | X = x, B = b, A = a
i
.
In addition, if π ∈πCVaR is a stationary Markov policy on X × B, we have
Uπ(x, b, a) = γ Eπ
h
Uπ X′, B′, A′ | X = x, B = b, A = a
i
.
△
Proof. The result follows by time-homogeneity and the Markov property. Con-
sider the sample transition model (X, B, A, R, X′, B′, A′), with A′ = ak(X′, B′).
Simultaneously, consider the partial trajectory (Xt, Bt, At, Rt)k
t=0 for which
A0 = A′ and At ∼πk−t(· | Xt, Bt) for t > 0. As γB′ = B −R, we have
γ E
h
Uk
 X′, B′, A′ | X = x, B = b, A = a
i
= γ E
h
E
hB′ −Gk(X′, B′, A′)+i
| X = x, B = b, A = a
i
= γ E
h
E
hB′ −
k
X
t=0
γtRt
+ | X0 = X′, B0 = B′, A0 = A′i
| X = x, A = a
i
= E
h
E
hb −R −γ
k
X
t=0
γtRt
+ | X0 = X′, B0 = B′, A0 = A′i
| X = x, B = b, A = a
i
= E
h
E
hb −R −
k+1
X
t=1
γtRt
+ | X1 = X′, B1 = B′, A1 = A′i
| X = x, B = b, A = a
i
= E
hb −
k+1
X
t=0
γtRt
+ | X0 = x, B0 = b, A0 = a
i
.
The second statement follows similarly.
△
Lemma 7.25. Suppose that Vmin ≤0 and Vmax ≥0. Let (Rt)t≥0 be a sequence of
rewards in [Rmin, Rmax]. For any b ∈R and k ∈N,
E [b −
∞
X
t=0
γtRt]+] + γk+1Vmin ≤
E [b −
k
X
t=0
γtRt]+] ≤E [b −
∞
X
t=0
γtRt]+] + γk+1Vmax . △
Proof. First note that, for any b, z, z′ ∈R,
[b −z]+ ≤[b −z′]+ + [z′ −z]+.
(7.30)
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Control
225
To obtain the ﬁrst inequality in the statement, we set
[b −
∞
X
t=0
γtRt]+ ≤[b −
k
X
t=0
γtRt]+ + [−
∞
X
t=k+1
γtRt]+ .
Since rewards are bounded in [Rmin, Rmax], we have that
−
∞
X
t=k+1
γtRt ≤−γk+1 Rmin
1 −γ = −γk+1Vmin.
As we have assumed that Vmin ≤0, it follows that
[b −
∞
X
t=0
γtRt]+ ≤[b −
k
X
t=0
γtRt]+ −γk+1Vmin.
The second inequality in the statement is obtained analogously.
△
Lemma 7.26. The sequence (ηk)k≥0 deﬁned by Equation 7.27 satisﬁes, for any
x ∈X, b ∈B, and a ∈A,
Uk(x, b, a) = min
π∈πCVaR Eπ
[b −
k
X
t=0
γtRt]+ | X = x, B = b, A = a.
(7.31)
△
Proof (sketch). Our choice of G0(x, b, a) = 0 guarantees that the statement is
true for k = 0. The result then follows by Lemma 7.24, the fact that the policy
˜G(ηk) chooses the action minimizing the left-hand side of Equation 7.31, and
by induction on k.
△
Proof of Theorem 7.22. Let us assume that Vmin ≤0 and Vmax ≥0 so that Lemma
7.25 can be applied. This is without loss of generality, as otherwise we may
ﬁrst construct a new sequence of rewards shifted by an appropriate constant C,
such that Rmin = 0, Rmax ≥0; by inspection, this transformation does not aﬀect
the statement of the Theorem 7.22.
Let π∗∈πCVaR be an optimal deterministic policy, in the sense that
Uπ∗(x, b, a) = min
π∈πCVaR Uπ(x, b, a).
Combining Lemmas 7.25 and 7.26, we have
Uπ∗(x, b, a) + γk+1Vmin ≤Uk(x, b, a) ≤Uπ∗(x, b, a) + γk+1Vmax.
(7.32)
Write Exba[·] = Eπ[· | X = x, B = b, A = a]. By Lemma 7.24,
Uπk(x, b, a) −Uk(x, b, a)
(7.33)
= γ Exba
h
Uπk(X′, B′, ak(X′, B′)) −Uk−1(X′, B′, ak−1(X′, B′))
i
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

226
Chapter 7
= γ Exba
h
Uπk(X′, B′, ak(X′, B′)) −Uk(X′, B′, ak(X′, B′))
+ Uk(X′, B′, ak(X′, B′)) −Uk−1(X′, B′, ak−1(X′, B′))
i
≤γ Exba
h
Uπk(X′, B′, ak(X′, B′)) −Uk(X′, B′, ak(X′, B′)) + γk+1Vmax −γkVmin.
Now, the quantity
εk(x, b, a) = Uπk(x, b, a) −Uk(x, b, a)
is bounded above and hence
εk =
sup
x∈X,b∈B,a∈A
εk(x, b, a)
exists. Taking the supremum over x, b and a on both sides of Equation 7.33, we
have
εk ≤γεk + γk+1Vmax −γkVmin
and consequently for all x, b, a,
Uπk(x, b, a) −Uk(x, b, a) ≤γk(γVmax −Vmin)
1 −γ
.
(7.34)
Because Uk(x, b, a) ≤Uπ∗(x, b, a) + γk+1Vmax, then
Uπk(x, b, a) ≤Uπ∗(x, b, a) + γk+1Vmax + γk(γVmax −Vmin)
1 −γ
.
Now,
γk+1 + γk+1
1 −γ = γk γ + γ(1 −γ)
1 −γ
≤
γk
1 −γ.
Hence, we conclude that also
Uπk(x, b, a) ≤min
π∈πCVaR Uπ(x, b, a) + γk(Vmax −Vmin)
1 −γ
,
as desired.
For the second statement, following Equation 7.28 we have
b0 = arg max
b∈B

b −τ−1 E [b −Gk(X0, b, ak(X0, b))]+
.
The algorithm begins in state (X0, b0), selects action A0 = ak(X0, b0), and then
executes policy πk from there on; its return is Gπk(X0, b0, A0). In particular,
JCVaR(πk, b0) = max
b∈B

b −τ−1 E [b −Gπk(X0, b0, A0)]+
≥b0 −τ−1 E [b0 −Gπk(X0, b0, A0)]+ .
(7.35)
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Control
227
Write aπ∗(x, b) for the action selected by π∗in (x, b). Because Equation 7.32
holds for all x, b, and a, we have
min
a∈A Uk(x, b, a) ≤min
a∈A Uπ∗(x, b, a) + γk+1Vmax
=⇒Uk(x, b, ak(x, b)) ≤Uπ∗(x, b, aπ∗(x, b)) + γk+1Vmax ,
since ak(x, b) is the action a that minimizes Uk(x, b, a). Hence, for any state x,
we have
max
b∈B
 b −τ−1 min
a∈A Uk(x, b, a) ≥max
b∈B
 b −τ−1 min
a∈A Uπ∗(x, b, a) −γk+1Vmax
τ
,
and so
b0 −τ−1Uk(X0, b0, ak(X0, b0)) ≥max
b∈B
 b −τ−1Uπ∗(X0, b, aπ∗(x, b)) −γk+1Vmax
τ
= max
b∈B JCVaR(π∗, b) −γk+1Vmax
τ
.
Combined with Equations 7.34 and 7.35, this yields
JCVaR(πk, b0) ≥max
b∈B max
π∈πCVaR JCVaR(π, b) −γk(Vmax −Vmin)
τ(1 −γ)
.
△
7.10
Bibliographical Remarks
7.0. The balloon navigation example at the beginning of the chapter is from
Bellemare et al. (2020). Sutton and Barto (2018) separate “control problem”
from “prediction problem”; the latter ﬁgures more predominantly in this book.
In earlier literature, the control problem comes ﬁrst (see, e.g., Bellman 1957b)
and prediction is typically used as a subroutine for control (Howard 1960).
7.1. Time-dependent policies are common in ﬁnite-horizon scenarios and are
studied at length by Puterman (2014). The technical core of Proposition 7.2
involves demonstrating that any feasible value function can be attained by a
stationary Markov policy; see the results by Puterman (2014, Theorem 5.5.1),
Altman (1999) and the discussion by Szepesvári (2020).
In reinforcement learning, history-dependent policies are also used to deal
with partially observable environments, in which the agent receives an obser-
vation o at each time step rather than the identity of its state. For example,
McCallum (1995) uses a variable-length history to represent state-action values,
while Veness et al. (2011) use a history-based probabilistic model to learn a
model of the environment. History-dependent policies also play a central role
in the study of optimality in the fairly large class of computable environments
(Hutter 2005).
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

228
Chapter 7
7.2. The canonical reference for value iteration is the book by Bellman (1957b);
see also Bellman (1957a) for an asymptotic analysis in the undiscounted set-
ting. Lemma 7.3 is standard and can be found in most reinforcement learning
textbooks (Bertsekas and Tsitsiklis 1996; Szepesvári 2010; Puterman 2014).
State-action value functions were introduced along with the Q-learning algo-
rithm (Watkins 1989) and subsequently used in the development of SARSA
(Rummery and Niranjan 1994). Watkins and Dayan (1992) give a restricted
result regarding the convergence of Q-learning, which is more thoroughly estab-
lished by Jaakkola et al. (1994), Tsitsiklis (1994), and Bertsekas and Tsitsiklis
(1996).
7.3–7.5. The expression of the optimality operator as a ﬁxed-policy operator
whose policy varies with the input is common in the analysis of control algo-
rithms (see, e.g., Munos 2003; Scherrer 2014). The view of value iteration as
constructing a history-dependent policy is taken by Scherrer and Lesner (2012)
to derive more accurate value learning algorithms in the approximate setting.
The extension to distributional value iteration and Theorem 7.13 are from
Bellemare et al. (2017a). The correspondence between standard value iteration
and distributional value iteration with a mean-preserving projection is given by
Lyle et al. (2019).
The notion of action gap plays an important role in understanding the rela-
tionship between value function estimates and policies, in particular when
estimates are approximate. Farahmand (2011) gives a gap-dependent bound on
the expected return obtained by a policy derived from an approximate value
function. Bellemare et al. (2016) derive an algorithm for increasing the action
gap so as to improve performance in the approximate setting.
An example of a selection rule that explicitly incorporates distributional
information is the lexicographical rule of Jaquette (1973), which orders policies
according to the magnitude of their moments.
7.6. The notion of risk and risk-sensitive decisions can be traced back to
Markowitz (1952), who introduced the concept of trading oﬀexpected gains and
variations in those gains in the context of constructing an investment portfolio;
see also Steinbach (2001) for a retrospective. Artzner et al. (1999) propose a
collection of desirable characteristics that make a risk measure coherent in the
sense that it satisﬁes certain preference axioms. Of the risk measures mentioned
here, CVaR is coherent but the variance-constrained objective is not. Artzner et
al. (2007) discuss coherent risk measures in the context of sequential decisions.
Ruszczy´nski (2010) introduces the notion of dynamic risk measures for Markov
decision processes, which are amenable to optimization via Bellman-style recur-
sions; see also Chow (2017) for a discussion of static and dynamic risk measures
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Control
229
as well as time consistency. Jiang and Powell (2018) develop sample-based
optimization methods for dynamic risk measures based on quantiles.
Howard and Matheson (1972) considered the optimization of an exponential
utility function applied to the random return by means of policy iteration. The
same objective is given a distributional treatment by Chung and Sobel (1987).
Heger (1994) considers optimizing for worst-case returns. Haskell and Jain
(2015) study the use of occupancy measures over augmented state spaces as an
approach for ﬁnding optimal policies for risk-sensitive control; similarly, an
occupancy measure-based approach to CVaR optimization is studied by Carpin
et al. (2016). Mihatsch and Neuneier (2002) and Shen et al. (2013) extend
Q-learning to the optimization of recursive risk measures, where a base risk
measure is applied at each time step. Recursive risk measures are more easily
optimized than risk measures directly applied to the random return but are not
as easily interpreted. Martin et al. (2020) consider combining distributional
reinforcement learning with the notion of second-order stochastic dominance as
a means of action selection. Quantile criteria are considered by Filar et al. (1995)
in the case of average-reward MDPs and, more recently, by Gilbert et al. (2017)
and Li et al. (2022). Delage and Mannor (2010) solve a risk-constrained opti-
mization problem to handle uncertainty in a learned model’s parameters. See
Prashanth and Fu (2021) for a survey on risk-sensitive reinforcement learning.
7.7. Sobel (1982) establishes that an operator constructed directly from the
variance-penalized objective does not have the monotone improvement prop-
erty, making its optimization more challenging. The examples demonstrating
the need for randomization and a history-dependent policy are adapted from
Mannor and Tsitsiklis (2011), who also prove the NP-hardness of the problem
of optimizing the variance-constrained objective. Tamar et al. (2012) propose
a policy gradient algorithm for optimizing a mean-variance objective and for
the CVaR objective (Tamar et al. 2015); see also Prashanth and Ghavamzadeh
(2013) and Chow and Ghavamzadeh (2014) for actor-critic algorithms for these
criteria. Chow et al. (2018) augment the state with the return-so-far in order to
extend gradient-based algorithms to a broader class of risk measures.
7.8. The reformulation of the conditional value-at-risk (CVaR) of a random
variable in terms of the (convex) optimization of a function of a variable b ∈R
is due to Rockafellar and Uryasev (2000); see also Rockafellar and Uryasev
(2002) and Shapiro et al. (2009). Bäuerle and Ott (2011) provide an algorithm
for optimizing the CVaR of the random return in Markov decision processes.
Their work forms the basis for the algorithm presented in this section, although
the treatment in terms of return-distribution functions is new here. Another
closely related algorithm is due to Chow et al. (2015), who additionally provide
an approximation error bound on the computed CVaR. Brown et al. (2020) apply
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

230
Chapter 7
Rockafellar and Uryasev’s approach to design an agent that is risk-sensitive
with respect to a prior distribution over possible reward functions. Keramati et
al. (2020) combine categorical temporal-diﬀerence learning with an exploration
bonus derived from the Dvoretzky–Kiefer–Wolfowitz inequality to develop an
algorithm to optimize for conditional value-at-risk.
7.11
Exercises
Exercise 7.1. Find a counterexample that shows that the Bellman optimality
operator is not an aﬃne operator.
△
Exercise 7.2. Consider the Markov decision process depicted in Figure 2.4a.
For which values of the discount factor γ ∈[0, 1) is there more than one optimal
action from state x? Use this result to argue that the optimal policy depends on
the discount factor.
△
Exercise 7.3. Proposition 7.7 establishes that distributional Bellman optimality
operators are not contraction mappings.
(i) Instantiate the result with the 1-Wasserstein distance. Provide a visual
explanation for the result by drawing the relevant cumulative distribution
functions before and after the application of the operator.
(ii) Discuss why it was necessary, in the proof of Proposition 7.7, to assume
that the probability metric d is c-homogeneous.
△
Exercise 7.4. Suppose that there is a unique optimal policy π∗, as per Section
7.4. Consider the use of a projection ΠF for a probability representation F and
the iterates
ηk+1 = ΠFT ηk .
(7.36)
Discuss under what conditions the sequence of greedy policies (G(ηk))k≥0
converges to π∗when ΠF is
(i) the m-categorical projection Πc;
(ii) the m-quantile projection Πq.
Where necessary, provide proofs of your statements. Does your answer depend
on m or on θ1, . . . , θm for the case of the categorical representation?
△
Exercise 7.5. Give a Markov decision process for which the limit of the
sequence of iterates deﬁned by ηk+1 = T Gηk depends on the initial condition η0,
irrespective of the greedy selection rule G. Hint. Construct a scenario where the
implied policy πk is the same for all k but depends on η0.
△
Exercise 7.6. Consider the greedy selection rule that selects an action with
minimal variance among those with maximal expected value, breaking ties
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Control
231
uniformly at random. Provide an example Markov decision process in which
this rule results in a sequence of return-distribution functions that does not
converge, as per Example 7.11. Hint. Consider reward distributions of the form
1
3
P3
i=1 δθi.
△
Exercise 7.7 (*). Consider the 1-Wasserstein distance w1 and its supremum
extension w1. In addition, let d be a metric on P(A). Suppose that we are given
a mapping
˜G : P(R)X×A →πms
which is continuous at every state, in the sense that for any ε > 0, there exists a
δ > 0 such that for any return functions η, η′,
w1(η, η′) < δ =⇒max
x∈X d  ˜G(η)(· | x), ˜G(η′)(· | x) < ε.
Show that this mapping cannot be a greedy selection rule in the sense of
Deﬁnition 7.6.
△
Exercise 7.8. Consider the Markov decision process depicted in Figure 7.5a.
Show that there is no λ ≥0 such that the policy maximizing
Jmv(π) = Eπ
Gπ(X0) −λVarπ
 Gπ(X0)
is stochastic. This illustrates how the variance-constrained and variance-
penalized control problems are not equivalent.
△
Exercise 7.9. Consider the Markov decision process depicted in Figure 7.5c.
(i) Solve for the optimal stopping time TC maximizing the return of a time-
dependent policy that selects action a for TC time steps, then selects action
b (under the constraint that the variance should be no greater than C).
(ii) Prove that this policy can achieve an expected return of up to
√
C.
(iii) Based on your conclusions, design a policy that improves on this strategy.
(iv) Show that the expectation and variance of the return of a randomized
stationary policy that selects action b with probability p are given by
Eπ[Gπ(x)] =
p
2 1 −γ(1 −p)
Varπ
 Gπ(x) = p
4
 
2
1 −γ2(1 −p) −
p
(1 −γ(1 −p))2
!
.
(v) Using your favorite visualization program, chart the returns achieved by the
optimal randomized policy and the optimal time-dependent policy, for values
of C and γ diﬀerent from those shown in Figure 7.5c. What do you observe?
Hint. Use a root-ﬁnding algorithm to determine the maximum expected
return of a randomized policy under the constraint Varπ
 Gπ(x) ≤C.
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

232
Chapter 7
Exercise 7.10. Explain the relationship between the shaded area in Figure 7.4
and conditional value-at-risk for the depicted distribution.
△
Exercise 7.11. Following Equation 7.23, for a distribution ν ∈P(R), consider
the function
f(θ) = θ −τ−1 E
Z∼ν
[θ −Z]+ .
Show that for τ ∈(0, 1),
θ ←θ −ατ d
dθ f(θ)
is equivalent to the quantile update rule (Equation 6.11), in expectation.
△
Exercise 7.12 (*). Consider a uniform discretization Bε of the interval
[Vmin, Vmax] into intervals of width ε (endpoints included). For a return-
distribution function η on the discrete space X × Bε × A, deﬁne its extension to
X × [Vmin, Vmax] × A
˜η(x, b, a) = η(x, ε
jb
ε
k
, a).
Suppose that probability distributions can be represented exactly (i.e., without
needing to resort to a ﬁnite-parameter representation). For the CVaR objective
(Equation 7.22), derive an upper bound for the suboptimality
max
π∈πCVaR J(π) −J(˜π),
where ˜π is found by the procedure of Section 7.8 applied to the discrete space
X × Bε × A and using the extension η 7→˜η to implement the operator T ˜G.
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

8
Statistical Functionals
The development of distributional reinforcement learning in previous chapters
has focused on approximating the full return function with parameterized fami-
lies of distributions. In our analysis, we quantiﬁed the accuracy of an algorithm’s
estimate according to its distance from the true return-distribution function,
measured using a suitable probability metric.
Rather than try to approximate the full distribution of the return, we may
instead select speciﬁc properties of this distribution and directly estimate these
properties. Implicitly, this is the approach taken when estimating the expected
return. Other common properties of interest include quantiles of the distribu-
tions, high-probability tail bounds, and the risk-sensitive objectives described in
Chapter 7. In this chapter, we introduce the language of statistical functionals
to describe such properties.
In some cases, the statistical functional approach allows us to obtain accurate
estimates of quantities of interest, in a more straightforward manner. As a
concrete example, there is a low-cost dynamic programming procedure to
determine the variance of the return distribution.61 By contrast, categorical and
quantile dynamic programming usually under- or overestimate this variance.
This chapter develops the framework of statistical functional dynamic pro-
gramming as a general method for approximately determining the values of
statistical functionals. As we demonstrate in Section 8.4, it is in fact possible
to interpret both categorical and quantile dynamic programming as operating
over statistical functionals. We will see that while some characteristics of the
return (including its variance) can be accurately estimated by an iterative proce-
dure, in general, some care must be taken when estimating arbitrary statistical
functionals.
61. In fact, the return variance can be determined to machine precision by solving a linear system
of equations, similar to what was done in Section 5.1 for the value function.
233
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

234
Chapter 8
8.1
Statistical Functionals
A functional maps functions to real values. By extension, a statistical functional
maps probability distributions to the reals. In this book, we view statistical
functionals as measuring a particular property or characteristic of a probability
distribution. For example, the mapping
ν 7→PZ∼ν
 Z ≥0,
ν ∈P(R)
is a statistical functional that measures how much probability mass its argument
ν puts on the nonnegative reals. Statistical functionals express quantiﬁable
properties of probability distributions such as their mean and variance. The
following formalizes this point.
Deﬁnition 8.1. A statistical functional ψ is a mapping from a subset of
probability distributions Pψ(R) ⊆P(R) to the reals, written
ψ : Pψ(R) →R .
We call the particular scalar ψ(ν) associated with a probability distribution ν a
functional value and the set Pψ(R) the domain of the functional.
△
Example 8.2. The mean functional maps probability distributions to their
expected values. As before, let
P1(R) = {ν ∈P(R) : E
Z∼ν
|Z| < ∞}
be the set of distributions with ﬁnite ﬁrst moment. For ν ∈P1(R), the mean
functional is
µ1(ν) = E
Z∼ν[Z] .
The restriction to P1(R) is necessary to exclude from the deﬁnition distributions
without a well-deﬁned mean.
△
The purpose of this chapter is to study how functional values of the return
distribution can be approximated using dynamic programming procedures and
incremental algorithms. In general, we will be interested in a collection of
such functionals that exhibit desirable properties: for example, because they
can be jointly determined by dynamic programming or because they provide
complementary information about the return function. We call such a collection
a distribution sketch.
Deﬁnition 8.3. A distribution sketch (or simply sketch) ψ : Pψ(R) →Rm is a
vector-valued function speciﬁed by a tuple (ψ1, …, ψm) of statistical functionals.
Its domain is
Pψ(R) =
m
\
i=1
Pψi(R) ,
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Statistical Functionals
235
and it is deﬁned as
ψ(ν) = (ψ1(ν), …, ψm(ν)),
ν ∈Pψ(R) .
Its image is
Iψ = ψ(ν) : ν ∈Pψ(R)	 ⊆Rm .
We also extend this notation to return-distribution functions:
ψ(η) =  ψ(η(x)) : x ∈X),
η ∈Pψ(R)X .
△
Example 8.4. The quantile functionals are a family of statistical function-
als indexed by τ ∈(0, 1) and deﬁned over P(R). The τ-quantile functional is
deﬁned in terms of the inverse cumulative distribution function of its argument
(Deﬁnition 4.12):
ψq
τ(ν) = F−1
ν (τ) .
A ﬁnite collection of quantile functionals (say, for τ1, . . . , τm ∈(0, 1)) constitutes
a sketch.
△
Example 8.5. To prove the convergence of categorical temporal-diﬀerence
learning (Section 6.10), we introduced the isometry I : FC,m →RI deﬁned as
I(ν) =

Fν(θi) : i ∈{1, . . . , m}

,
(8.1)
where (θi)m
i=1 is the set of locations for the categorical representation. This
isometry is also a sketch in the sense of Deﬁnition 8.3. If we extend its domain
to be P(R), Equation 8.1 still deﬁnes a valid sketch but it is no longer an
isometry: it is not possible to recover the distribution ν from its functional
values I(ν).
△
8.2
Moments
Moments are an especially important class of statistical functionals. For an
integer p ∈N+, the pth moment of a distribution ν ∈Pp(R) is given by
µp(ν) = E
Z∼ν
Zp .
In particular, the ﬁrst moment of ν is its mean, while the variance of ν is the
diﬀerence between its second moment and squared mean:
µ2(ν) − µ1(ν)2 .
(8.2)
Moments are ubiquitous in mathematics. They form a natural way of capturing
important aspects of a probability distribution, and the inﬁnite sequence of
moments  µp(ν)∞
p=1 uniquely characterizes many probability distributions of
interest; see Remark 8.3.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

236
Chapter 8
Our goal in this section is to describe a dynamic programming approach to
determining the moments of the return distribution. Fix a policy π, and consider
a state x ∈X and action a ∈A. The pth moment of the return distribution ηπ(x, a)
is given by
Eπ
h Gπ(x, a)pi
,
where as before, Gπ(x, a) is an instantiation of ηπ(x, a). Although we can also
study dynamic programming approaches to learning the pth moment of state-
indexed return distributions,
Eπ
h Gπ(x)pi
,
this is complicated by a potential conditional dependency between the reward R
and next state X′ due to the action A. One solution is to assume independence
of R and X′, as we did in Section 5.4. Here, however, to avoid making this
assumption, we work with functions indexed by state-action pairs.
To begin, let us ﬁx m ∈N+. The m-moment function Mπ is
Mπ(x, a, i) = Eπ[(Gπ(x, a))i] = µi
 ηπ(x, a) ,
for i = 1, . . . , m .
(8.3)
As with value functions, we view Mπ as the function (or vector) in RX×A×m
describing the collection of the ﬁrst m moments of the random return. In
particular, Mπ(·, ·, 1) is the usual state-action value function. As elsewhere in
the book, to ensure that the expectation in Equation 8.3 is well deﬁned, we
assume that all reward distributions have ﬁnite pth moments, for p = 1, . . . , m.
In fact, it is suﬃcient to assume that this holds for p = m (Assumption 4.29(m)).
As with the standard Bellman equation, from the state-action random-
variable Bellman equation
Gπ(x, a) = R + γGπ(X′, A′),
X = x, A = a
we can derive Bellman equations for the moments of the return distribution. To
do so, we raise both sides to the ith power and take expectations with respect
to both the random return variables Gπ and the random transition (X = x, A =
a, R, X′, A′):
Eπ[(Gπ(x, a))i] = Eπ[(R + γGπ(X′, A′))i | X = x, A = a] .
From the binomial expansion of the term inside the expectation, we obtain
Eπ[(Gπ(x, a))i] = Eπ

iX
j=0
γi−j
 i
j
!
RjGπ(X′, A′)i−j
 X = x, A = a

.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Statistical Functionals
237
Since R and Gπ(X′, A′) are independent given X and A, we can rewrite the above
as
Mπ(x, a, i) =
iX
j=0
γi−j
 i
j
!
Eπ[Rj | X = x, A = a] Eπ
Mπ(X′, A′, i −j) | X = x, A = a,
where by convention we take Mπ(x′, a′, 0) = 1 for all x′ ∈X and a′ ∈A. This is a
recursive characterization of the ith moment of a return distribution, analogous
to the familiar Bellman equation for the mean. The recursion is cast into the
familiar framework of operators with the following deﬁnition.
Deﬁnition 8.6. Let m ∈N+. The m-moment Bellman operator T π
(m) : RX×A×m →
RX×A×m is given by
(T π
(m)M)(x, a, i) =
(8.4)
iX
j=0
γi−j
 i
j
!
Eπ[Rj | X = x, A = a] Eπ
M(X′, A′, i −j) | X = x, A = a . △
The collection of moments (Mπ(x, a, i) : (x, a) ∈X × A, i = 1, …, m) is a ﬁxed
point of the operator T π
(m). In general, the m-moment Bellman operator is not a
contraction mapping with respect to the L∞metric (except, of course, for m = 1;
see Exercise 8.1). However, with a more nuanced analysis, we can still show
that T π
(m) has a unique ﬁxed point to which the iterates
Mk+1 = T π
(m)Mk
(8.5)
converge.
Proposition 8.7. Let m ∈N+. Under Assumption 4.29(m), Mπ is the
unique ﬁxed point of T π
(m). In addition, for any initial condition M0 ∈
RX×A×m, the iterates of Equation 8.5 converge to Mπ.
△
Proof. We begin by constructing a suitable notion of distance between m-
moment functions RX×A×m. For M ∈RX×A×m, let
∥M∥∞,i =
sup
(x,a)∈X×A
|M(x, a, i)| ,
for i = 1, . . . , m
∥M∥∞,<i =
sup
j=1,…,i−1
∥M∥∞, j ,
for i = 2, . . . , m .
Each of ∥· ∥∞,i (for i = 1, …, m) and ∥· ∥∞,<i (for i = 2, …, m) is a semi-norm; they
fulﬁll the requirements of a norm, except that neither ∥M∥∞,i = 0 nor ∥M∥∞,<i = 0
implies that M = 0. From these semi-norms, we construct the pseudo-metrics
(M, M′) 7→∥M −M′∥∞,i,
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

238
Chapter 8
noting that it is possible for the distance between M and M′ to be zero even
when M is diﬀerent from M′.
The structure of the proof is to argue that T π
(m) is a contraction with modulus γ
with respect to ∥· ∥∞,1 and then to show inductively that it satisﬁes an inequality
of the form
∥T π
(m)M −T π
(m)M′∥∞,i ≤Ci∥M −M′∥∞,<i + γi∥M −M′∥∞,i ,
(8.6)
for each i = 2, …, m, and some constant Ci that depends on PR. Chaining these
results together then leads to the convergence statement, and uniqueness follows
as an immediate corollary.
To see that T π
(m) is a contraction with respect to ∥· ∥∞,1, let M ∈RX×A×m, and
write M(i) = (M(x, a, i) : (x, a) ∈X × A) for the function in RX×A corresponding
to the ith moment function estimates given by M. By inspecting Equation 8.4
with i = 1, it follows that
 T π
(m)M
(1) = T πM(1),
where T π is the usual Bellman operator. Furthermore, ∥M∥∞,1 = ∥M(1)∥∞, and
so the statement that T π
(m) is a contraction with respect to the pseudo-metric
implied by ∥· ∥∞,1 is equivalent to the contractivity of T π on RX×A with the
respect to the L∞norm, which was shown in Proposition 4.4.
To see that T π
(m) satisﬁes the bound of Equation 8.6 for i > 1, let L ∈R be such
that
 E[Ri | X = x, A = a]
 ≤L,
for all x, a ∈X × A and i = 1, …, m.
Observe that
(T π
(m)M)(x, a, i) −(T π
(m)M′)(x, a, i)

=

i−1
X
j=0
γi−j
 i
j
!
Eπ
R j | X = x, A = a×
X
x′∈X
a′∈A
PX(x′ | x, a)π(a′ | x′)(M −M′)(x′, a′, i −j)

≤
i−1
X
j=1
γi−j
 i
j
! Eπ
Rj | X = x, A = a × ∥M −M′∥∞,<i + γi∥M −M′∥∞,i
≤L
i−1
X
j=1
γi−j
 i
j
!
∥M −M′∥∞,<i + γi∥M −M′∥∞,i
≤(2i −2)L∥M −M′∥∞,<i + γi∥M −M′∥∞,i .
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Statistical Functionals
239
Taking Ci = (2i −2)L, we have
∥T π
(m)M −T π
(m)M′∥∞,i ≤Ci∥M −M′∥∞,<i + γi∥M −M′∥∞,i, for i = 2, . . . , m.
To chain these results together, ﬁrst observe that
∥Mk −Mπ∥∞,1 →0.
We next argue inductively that if, for a given i < m, (Mk)k≥0 converges to Mπ in
the pseudo-metric induced by ∥· ∥∞,<i, then also
∥Mk −Mπ∥∞,i →0,
and hence
∥Mk −Mπ∥∞,<(i+1) →0.
Let yk = ∥Mk −Mπ∥∞,<i and zk = ∥Mk −Mπ∥∞,i. Then the generalized contraction
result states that zk+1 ≤Ciyk + γizk. Taking the limit superior on both sides yields
lim sup
k→∞
zk ≤lim sup
k→∞
Ciyk + γizk
 = γi lim sup
k→∞
zk ,
where we have used the result yk →0. From this, we deduce lim supk→∞zk ≤0,
but since (zk)k≥0 is a nonnegative sequence, we therefore have zk →0. This
completes the inductive step, and we therefore obtain ∥Mk −Mπ∥∞,i →0, as
required.
In essence, Proposition 8.7 establishes that the m-moment Bellman operator
behaves in a similar fashion to the usual Bellman operator, in the sense that its
iterates converge to the ﬁxed point Mπ. From here, we may follow the deriva-
tions of Chapter 5 to construct a dynamic programming algorithm for learning
these moments62 or those of Chapter 6 to construct the corresponding incre-
mental algorithm (Section 8.8). Although the proof above does not demonstrate
the contractive nature of the moment Bellman operator, for m = 2, this can be
achieved using a diﬀerent norm and analysis technique (Exercise 8.4).
8.3
Bellman Closedness
In preceding chapters, our approach to distributional reinforcement learning
considered approximations of the return distributions that could be tractably
manipulated by algorithms. The m-moment Bellman operator, on the other
hand, is not directly applied to probability distributions – compared to say,
a m-categorical distribution, there is no immediate procedure for drawing a
sample from a collection of m moments. Compared to the categorical and
62. When the reward distributions take on a ﬁnite number of values, in particular, the expectations
of Deﬁnition 8.6 can be implemented as sums.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

240
Chapter 8
η
η′
s
s′
T π
ψ
ψ
T π
ψ
Figure 8.1
A sketch is Bellman closed if there is an operator T π
ψ such that in the diagram above, the
composite functions ψ ◦T π and T π
ψ ◦ψ coincide.
quantile projected operators, however, the m-moment operator yields an error-
free dynamic programming procedure – with suﬃciently many iterations and
under some ﬁniteness assumptions, we can determine the moments of the
return function to any degree of accuracy. The concept of Bellman closedness
formalizes this idea.
Deﬁnition 8.8. A sketch ψ = (ψ1, …, ψm) is Bellman closed if, whenever its
domain Pψ(R)X is closed under the distributional Bellman operator:
η ∈Pψ(R)X =⇒T πη ∈Pψ(R)X,
there is an operator T π
ψ : IX
ψ →IX
ψ such that
ψ T πη = T π
ψψ η
for all η ∈Pψ(R)X.
The operator T π
ψ is said to be the Bellman operator for the sketch ψ.
△
As was demonstrated in the preceding section, the collection of the m ﬁrst
moments  µ1, . . . , µm
 is a Bellman-closed sketch. Its associated operator is the
m-moment operator T π
(m).
When a sketch ψ is Bellman closed, the operator T π
ψ mirrors the application
of the distributional Bellman operator to the return-distribution function η; see
Figure 8.1. The concept of Bellman closedness is related to that of a diﬀusion-
free projection (Chapter 5), and we will in fact establish an equivalence between
the two in Section 8.4. In addition, Bellman-closed sketches are particularly
interesting from a computational perspective because they support an exact
dynamic programming procedure, as the following establishes.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Statistical Functionals
241
Proposition 8.9. Let ψ = (ψ1, …, ψm) be a Bellman-closed sketch and sup-
pose that Pψ(R)X is closed under T π. Then for any initial condition
η0 ∈Pψ(R)X, and sequences (ηk)k≥0, (sk)k≥0 deﬁned by
ηk+1 = T πηk ,
s0 = ψ(η0) ,
sk+1 = T π
ψ sk ,
we have, for k ≥0,
sk = ψ(ηk) .
In addition, the functional values sπ = ψ(ηπ) of the return-distribution
function are a ﬁxed point of the operator T π
ψ.
△
Proof. Both parts of the result follow immediately from the deﬁnition of the
operator T π
ψ. First suppose that sk = ψ(ηk), for some k ≥0. Then note that
sk+1 = T π
ψ sk = T π
ψψ(ηk) = ψ T πηk
 = ψ(ηk+1) .
Thus, by induction, the ﬁrst statement is proven. For the second statement, we
have
sπ = ψ(ηπ) = ψ T πηπ = T π
ψψ(ηπ) = T π
ψ sπ .
Of course, dynamic programming is only feasible if the operator T π
ψ can
itself be implemented in a computationally tractable manner. In the case of the
m-moment operator, we know this is possible under similar assumptions as
were made in Chapter 5.
Proposition 8.9 illustrates how, when the sketch ψ is Bellman closed, we can
do away with probability distributions and work exclusively with functional
values. However, many sketches of interest fail to be Bellman closed, as the
following examples demonstrate.
Example 8.10 (The median functional). A median of a distribution ν is its
0.5-quantile F−1
ν (0.5).63 Perhaps surprisingly, there is in general no way to
determine the median of a return distribution based solely on the medians
at the successor states. To see this, consider a state x that leads to state y1
with probability 1/3 and to state y2 with probability 2/3, with zero reward. The
following are two scenarios in which the median returns at y1 and y2 are the
same, but the median at x is diﬀerent (see Figure 8.2):
63. As usual, there might be multiple values of z for which PZ∼ν(Z ≤z) = 0.5; recall that F−1 takes
the smallest such value.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

242
Chapter 8
0.5
0.0
0.5
1.0
1.5
2.0
Return
0.00
0.25
0.50
0.75
1.00
Cumulative Probability
(x)
(y1)
(y2)
0.5
0.0
0.5
1.0
1.5
2.0
Return
0.00
0.25
0.50
0.75
1.00
Cumulative Probability
(x)
(y1)
(y2)
(c)
(b)
(a)
Figure 8.2
Illustration of Example 8.10. (a) A Markov decision process in which state x leads to
states y1 and y2 with probability 1/3 and 2/3, respectively. (b) Case 1, in which the median
of η(x) matches the median of η(y2). (c) Case 2, in which the median of η(x) diﬀers from
the median of η(y2).
Case 1. The return distributions at y1 and y2 are Dirac deltas at 0 and 1,
respectively, and these are also the medians of these distributions. The median
at x is also 1.
Case 2. The return distributions at y1 and y2 are a Dirac delta at 0 and the
uniform distribution on [0.5, 1.5], respectively, and have the same medians as
in Case 1. However, the median at x is now 0.75.
△
Example 8.11 (At-least functionals). For ν ∈P(R) and z ∈R, let us deﬁne
the at-least functional
ψ≥z(ν) = 1{PZ∼ν
 Z ≥z > 0} ,
measuring whether ν assigns positive probability to values in [z, ∞). Now
consider a state x that deterministically leads to y, with no reward, and suppose
that there is a single action a available. The statement “it is possible to obtain a
return of at least 10 at state y” corresponds to
ψ≥10(ηπ(y)) = 1 .
(8.7)
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Statistical Functionals
243
If Equation 8.7 holds, can we deduce whether or not a return of at least 10 is
possible at state x? The answer is no. Suppose that γ = 0.9, and consider the
following two situations:
Case 1. ηπ(y) = δ10. Then ψ≥10(ηπ(y)) = 1, ηπ(x) = δ9 and ψ≥10(ηπ(x)) = 0.
Case 2. ηπ(y) = δ20. Then ψ≥10(ηπ(y)) = 1 still. However, ηπ(x) = δ18 and
ψ≥10(ηπ(x)) = 1.
△
What goes wrong in the examples above is that we do not have suﬃcient
information about the return distribution at the successor states to compute the
functional values for the return distribution of state x. Consequently, we cannot
use an iterative procedure to determine the functional values of ηπ, at least not
without error.
As it turns out, m-moment sketches are somewhat special in being Bellman
closed. As the following theorem establishes, any sketch whose functionals
are expectations of functions must encode the same information as a moment
sketch.
Theorem 8.12. Let ψ = (ψ1, . . . , ψm) be a sketch. Suppose that ψ is Bell-
man closed and that for each i = 1, . . . , m, there is a function fi : R →R for
which
ψi(ν) = E
Z∼ν[ fi(Z)] .
Then, ψ is equivalent to the ﬁrst n-moment functionals for some n ≤m, in
the sense that there are real-valued coeﬃcients (bi j) and (ci j) such that for
any ν ∈Pψ(R) ∩Pm(R),
ψi(ν) =
n
X
j=1
bi jµj(ν) + bi0 ,
i = 1, . . . , m ;
µj(ν) =
m
X
i=1
ci jψi(ν) + c0 j ,
j = 1, . . . , n .
△
The proof is somewhat lengthy and is given in Remark 8.2 at the end of the
chapter.
As a corollary, we may deduce that any sketch that can be expressed as
an invertible function of the ﬁrst m moments is also Bellman closed. More
precisely, if ψ′ is a sketch that is an invertible transformation of the sketch ψ
corresponding to the ﬁrst m moments, say ψ′ = h ◦ψ, then ψ′ is Bellman closed
with corresponding Bellman operator h ◦T π
ψ ◦h−1. Thus, for example, we may
deduce that the sketch corresponding to the mean and variance functionals is
Bellman closed, since the mean and variance are expressible as an invertible
function of the mean and uncentered second moment. On the other hand, many
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

244
Chapter 8
other statistical functionals (including quantile functionals) are not covered by
Theorem 8.12. In the latter case, this is because there is no function f : R →R
whose expectation for an arbitrary distribution ν recovers the τth quantile of ν
(Exercise 8.5). Still, as established in Example 8.10, quantile sketches are not
Bellman closed.
8.4
Statistical Functional Dynamic Programming
When a sketch ψ is not Bellman closed, we lack an operator T π
ψ that emu-
lates the combination of the distributional Bellman operator and this sketch.
This precludes a dynamic programming approach that bootstraps its functional
value estimates directly from the previous estimates. However, approximate
dynamic programming with arbitrary statistical functionals is still possible if we
introduce an additional imputation step ι that reconstructs plausible probability
distributions from functional values. As we will now see, this allows us to apply
the distributional Bellman operator to the reconstructed distributions and then
extract the functional values of the resulting return function estimate.
Deﬁnition 8.13. An imputation strategy for the sketch ψ : Pψ(R) →Rm is a
function ι : Iψ →Pψ(R). We say that it is exact if for any valid functional values
(s1, …, sm) ∈Iψ, we have
ψi(ι(s1, …, sm)) = si,
i = 1, . . . , m .
Otherwise, we say that it is approximate.
By extension, we write ι(s) ∈Pψ(R)X for the return-distribution function
corresponding to the collection of functional values s ∈IX
ψ .
△
In other words, if ι is an exact imputation strategy for the sketch ψ =
(ψ1, …, ψm), then for any valid values s1, …, sm of the functionals ψ1, …, ψm,
we have that ι(s1, …, sm) is a probability distribution with the required values
under each functional. In a certain sense, ι is a pseudo-inverse to the vector-
valued map ψ : ν 7→(ψ1(ν), …, ψm(ν)). Note that a true inverse to ψ does not
exist, as ψ generally does not capture all aspects of the distribution ν.
Once an imputation strategy has been selected, it is possible to write down an
approximate dynamic programming algorithm for the functional values under
consideration. An abstract framework is given in Algorithm 8.1. In eﬀect, such
an algorithm recursively computes the iterates
sk+1 = ψ T πι(sk)
(8.8)
from an initial s0 ∈IX
ψ . Procedures that implement the iterative process described
by Equation 8.8 are referred to as statistical functional dynamic programming
(SFDP) algorithms. When the sketch ψ is Bellman closed and its imputation
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Statistical Functionals
245
strategy ι is exact, the sequence of iterates (sk)k≥0 converges to ψ(ηπ), so long
as ψ is continuous (with respect to a Wasserstein metric).
Algorithm 8.1: Statistical functional dynamic programming
Algorithm parameters:
statistical functionals ψ1, …, ψm,
imputation strategy ι,
initial functional values  (si(x))m
i=1 : x ∈X,
desired number of iterations K
for k = 1, . . . , K do
▷Impute distributions
η ← ι(s1(x), …, sm(x)) : x ∈X
▷Apply distributional Bellman operator
˜η ←T πη
foreach state x ∈X do
for i = 1, . . . , m do
▷Update statistical functional
values
si(x) ←ψi(˜η(x))
end for
end foreach
end for
return  (si(x))m
i=1 : x ∈X
Example 8.14. For the quantile functionals (ψQ
τi)m
i=1 with τi = 2i−1
2m
for i =
1, …, m, an exact imputation strategy is
(q1, …, qm) 7→1
m
m
X
i=1
δqi .
(8.9)
This follows because the 2i−1
2m -quantile of 1
m
mP
i=1
δqi is precisely qi.
Note that when τ1, . . . , τm ∈(0, 1) are arbitrary levels with quantile values
(q1, . . . , qm), however, it is generally not true that Equation 8.9 is an exact
imputation strategy for the corresponding quantile functionals.
△
Example 8.15. Categorical dynamic programming can be interpreted as an
SFDP algorithm. Indeed, the parameters p1, …, pm found by the categorical
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

246
Chapter 8
projection correspond to the values of the following statistical functionals:
ψC
i (ν) = E
Z∼ν
hi(ς−1
m (Z −θi),
i = 1, . . . , m
(8.10)
where (hi)m
i=1 are the triangular and half-triangular kernels deﬁning the cate-
gorical projection on (θi)m
i=1 (Section 5.6). An exact imputation strategy in this
case is the function that returns the unique distribution supported on (θi)m
i=1 that
matches the estimated functional values pi = ψC
i (ν), i = 1, . . . , m:
(p1, . . . , pm) 7→
m
X
i=1
piδθi .
△
Mathematically, an exact imputation strategy always exists, because we
deﬁned imputation strategies in terms of valid functional values. However, there
is no guarantee that an eﬃcient algorithm exists to compute the application of
this strategy to arbitrary functional values. In practice, we may favor approx-
imate strategies with eﬃcient implementations. For example, we may map
functional values to probability distributions from a representation F by opti-
mizing some notion of distance between functional values. The optimization
process may not yield an exact match in F (one may not even exist) but can
often be performed eﬃciently.
Example 8.16. Let ψc
1, . . . , ψc
m be the categorical functionals from Equation
8.10. Suppose we are given the corresponding functional values p1, . . . , pm of a
probability distribution ν:
pi = ψc
i (ν),
i = 1, . . . , m .
An approximate imputation strategy for these functionals is to ﬁnd the n-quantile
distribution (n possibly diﬀerent from m)
νθ = 1
n
n
X
i=1
δθi
that best ﬁts pi according to the loss
L(θ) =
m
X
i=1
pi −ψc
i (νθ)
 .
(8.11)
Exercise 8.7 asks you to demonstrate that this strategy is approximate for m > 2.
Although in this context, we know of an exact imputation strategy based on
categorical distributions, this illustrates that it is possible to impute distributions
from a diﬀerent representation.
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Statistical Functionals
247
8.5
Relationship to Distributional Dynamic Programming
In Chapter 5, we introduced distributional dynamic programming (DDP) as a
class of methods that operates over return-distribution functions. In fact, every
statistical functional dynamic programming is also a DDP algorithm (but not
the other way around; see Exercise 8.8). This relationship is established by
considering the implied representation
F = {ι(s) : s ∈Iψ} ⊆P(R)
and the projection ΠF = ι ◦ψ (see Figure 8.3).
η
˜η
η′
s
s′
T π
ΠF
ψ
ι
ι
Figure 8.3
The interpretation of SFDP algorithms as distributional dynamic programming algo-
rithms. Traversing along the diagram from η to η′ corresponds to dynamic programming
implementing a projected Bellman operator, while the path from s to s′ corresponds to
statistical functional dynamic programming (SFDP).
From this correspondence, we may establish the relationship between Bell-
man closedness and the notion of a diﬀusion-free projection developed in
Chapter 5.
Proposition 8.17. Let ψ be a Bellman-closed sketch. Then for any choice
of exact imputation strategy ι : Iψ →Pψ(R), the projection operator ΠF =
ιψ is diﬀusion-free.
△
Proof. We may directly check the diﬀusion-free property (omitting parentheses
for conciseness):
ΠFT πΠF = ιψT πιψ
(a)= ιT π
ψψιψ
(b)= ιT π
ψψ
(a)= ιψT π = ΠFT π ,
where steps marked (a) follow from the identity ψT π = T π
ψψ, and (b) follows
from the identity ψιψ = ψ for any exact imputation strategy ι for ψ.
Imputation strategies formalize how one might interpret functional values
as parameters of a probability distribution. Naturally, the chosen imputation
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

248
Chapter 8
strategy aﬀects the approximation artifacts from distributional dynamic pro-
gramming, the rate of convergence, and whether the algorithm converges at
all.
Compared with representation-based algorithms of the style introduced in
Chapter 5, working with statistical functionals allows us to design the projection
ΠF in two separate pieces: a sketch ψ and an imputation strategy ι. In particular,
this makes it possible to learn statistical functionals that would be diﬃcult to
directly capture in a probability distribution representation. As the next section
demonstrates, this allows us to create new kinds of distributional reinforcement
learning algorithms.
8.6
Expectile Dynamic Programming
Expectiles form a family of statistical functionals parameterized by a level
τ ∈(0, 1). They extend the notion of the mean of a distribution (τ = 0.5) similar
to how quantiles extend the notion of a median. Expectiles have classically
found application in econometrics and ﬁnance as a form of risk measure (see the
bibliographical remarks for further details). Based on the principles of statistical
functional dynamic programming, expectile dynamic programming64 uses an
approximate imputation strategy in order to iteratively estimate the expectiles
of the return function.
Deﬁnition 8.18. For a given τ ∈(0, 1), the τ-expectile of a distribution ν ∈
P2(R) is
ψe
τ(ν) = arg min
z∈R
ERτ(z; ν) ,
(8.12)
where
ERτ(z; ν) = E
Z∼ν
|1{Z<z} −τ| × (Z −z)2
(8.13)
is the expectile loss.
△
The loss appearing in Deﬁnition 8.18 is strongly convex (Boyd and Vanden-
berghe 2004) and bounded below by 0. As a consequence, Equation 8.12 has a
unique minimizer for a given τ; this veriﬁes that the corresponding expectile is
uniquely deﬁned.
To understand the relationship to the mean functional and develop some
intuition for the statistical property than an expectile encodes, observe that the
mean of a distribution ν ∈P2(R) can be expressed as
µ1(ν) = arg min
z∈R
E
Z∼ν[(Z −z)2] .
64. The incremental analogue is called expectile temporal-diﬀerence learning (Rowland et al. 2019).
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Statistical Functionals
249
Similar to how a quantile is derived from a loss that weights errors asymmetri-
cally (depending on whether the realization from Z is smaller or greater than
z), the expectile loss for τ ∈(0, 1) is the asymmetric version of the above. For τ
greater than 1/2, one can think of the expectile as an “optimistic” summary of
the distribution – a value that emphasizes outcomes that are greater than the
mean. Conversely, for τ smaller than 1/2, the corresponding expectile is in a
sense “pessimistic.”
Expectile dynamic programming (EDP) estimates the values of a ﬁnite set
of expectile functionals with values 0 < τ1 < · · · < τm < 1. For a distribution
ν ∈P2(R), let us write
ei = ψe
τi(ν) .
Given the collection of expectile values e1, …, em, EDP uses an imputation
strategy that outputs an n-quantile probability distribution that approximately
has these expectile values.65
The imputation strategy ﬁnds a suitable reconstruction by ﬁnding a solution to
a root-ﬁnding problem. To begin, this strategy outputs a n-quantile distribution
ˆν, with n possibly diﬀerent from m:
ˆν = 1
n
n
X
j=1
δθ j .
Following Deﬁnition 8.13, for this imputation to be exact, the expectiles of ˆν at
τ1, . . . , τm should be equal to e1, . . . , em:
ψe
τi(ˆν) = ei,
i = 1, . . . , m .
This constraint implies that the derivatives of the expectile loss, instantiated
with τ1, . . . , τm and evaluated with ˆν, should all be 0:
∂z ERτi

z; ˆν
z=ei
= 0,
i = 1, . . . , m .
(8.14)
Written out in full for the choice of ˆν above, these derivatives take the form
∂z ERτi

z; ˆν
z=ei
= 1
n
n
X
j=1
1
2(ei −θ j)|1{θj < ei} −τi|,
i = 1, . . . , m .
An alternative to the root-ﬁnding problem expressed in Equation 8.14 is the
following optimization problem:
minimise
m
X
i=1
 
∂z ERτi

z; ˆν
z=ei
!2
.
(8.15)
65. Of course, this particular form for the imputation strategy is a design choice; the reader is
invited to consider what other imputation strategies might be sensible here.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

250
Chapter 8
A practical implementation of this imputation strategy therefore applies an opti-
mization algorithm to the objective in Equation 8.15, or a root-ﬁnding method
to Equation 8.14, viewed as functions of θ1, …, θn. Because the optimization
algorithm may return a solution that does not exactly satisfy Equation 8.14,
this method is an approximate (rather than exact) imputation strategy. It can
be used in the impute distributions step of Algorithm 8.1, yielding a dynamic
programming algorithm that aims to approximately learn return-distribution
expectiles. If the root-ﬁnding algorithm is always able to ﬁnd ˆν exactly sat-
isfying Equation 8.14, then the imputation strategy is exact in this instance;
otherwise, it is approximate. A speciﬁc implementation is explored in detail in
Exercise 8.10.
8.7
Inﬁnite Collections of Statistical Functionals
Thus far, our treatment of statistical functionals has focused on ﬁnite collec-
tions of statistical functionals – what we call a sketch. From a computational
standpoint, this is sensible since, to implement an SFDP algorithm, one needs
to be able to operate on individual functional values. On the other hand, in
Section 8.3, we saw that many sketches are not Bellman closed and must be
combined with an imputation strategy in order to perform dynamic program-
ming. An alternative, which we will study in greater detail in Chapter 10, is to
implicitly parameterize an inﬁnite family of statistical functionals.
Many (though not all) inﬁnite families of functionals provide a lossless
encoding of probability distributions and are consequently Bellman closed
– that is, knowing the values taken on by these functionals is equivalent to
knowing the distribution itself. We encode this property with the following
deﬁnition.
Deﬁnition 8.19. Let Ψ be a set of statistical functionals. We say that Ψ char-
acterizes probability distributions over the real numbers if, for each ν ∈P(R),
there is a unique collection of functional values (ψ(ν) : ψ ∈Ψ).
△
The following families of statistical functionals all characterize probability
distributions over R.
The cumulative distribution function. The functionals mapping distribu-
tions ν to the probabilities PZ∼ν(Z ≤z), indexed by z ∈R. Closely related are
upper-tail probabilities,
ν 7→PZ∼ν(Z ≥z) ,
and the quantile functionals
ν 7→F−1
ν (τ) ,
indexed by τ ∈(0, 1).
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Statistical Functionals
251
The characteristic function. Functionals of the form
ν 7→E
Z∼ν[eiuZ] ∈C ,
indexed by u ∈R (and where i2 = −1). The corresponding collection of statistical
values is the characteristic function of ν, denoted χν.
Moments and cumulants. The inﬁnite collection of moment functionals
(µp)∞
p=1 does not unconditionally characterize the distribution ν: there are distinct
distributions that have the same sequence of moments. However, if the sequence
of moments does not grow too quickly, uniqueness is restored. In particular, a
suﬃcient condition for uniqueness is that the underlying distribution ν has a
moment-generating function
u 7→E
Z∼ν[euZ] ,
which is ﬁnite in an open neighborhood of u = 0; see Remark 8.3 for further
details. Under this condition, the moment-generating function itself also charac-
terizes the distribution, as does the cumulant-generating function, deﬁned as
the logarithm of the moment-generating function,
u 7→log

E
Z∼ν[euZ]

.
The cumulants (κp)∞
p=1 are deﬁned through a power series expansion of the
cumulant-generating function
log

E
Z∼ν[euZ]

=
∞
X
p=1
κpup
p! .
Under the condition that the moment-generating function is ﬁnite in an open
neighborhood of the origin, the sequences of cumulants and moments are
determined by one another, and so the sequence of cumulants is another
characterization of the distribution under this condition.
Example 8.20. Consider the return-variable Bellman equation
Gπ(x, a)
D= R + γGπ(X′, A′) ,
X = x , A = a .
If for each u ∈R we apply the functional ν 7→E
Z∼ν[eiuZ] to the distribution of the
random variables on each side, we obtain the characteristic function Bellman
equation:
χηπ(x,a)(u) = Eπ[eiu(R+γGπ(X′,A′)) | X = x, A = a]
= Eπ
eiuR | X = x, A = a Eπ
eiγuGπ(X′,A′) | X = x, A = a
= χPR(·|x,a)(u) Eπ
χηπ(X′,A′)(γu) | X = x, A = a .
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

252
Chapter 8
This is a diﬀerent kind of distributional Bellman equation in which the addi-
tion of independent random variables corresponds to a multiplication of their
characteristic functions. The equation highlights that the characteristic function
of ν evaluated at u depends on the next-state characteristic functions evaluated
at γu. This shows that for a set S ⊆R, the sketch (ν 7→χν(u) : u ∈S ) cannot be
Bellman closed unless S is inﬁnite or S = {0}. Exercise 8.12 asks you to give a
theoretical analysis of a dynamic programming approach based on characteristic
functions.
△
Another way to understand collections of statistical functionals that are
characterizing (in the sense of Deﬁnition 8.19) is to interpret them in light of
our deﬁnition of a probability distribution representation (Deﬁnition 5.2). Recall
that a representation F is a collection of distributions indexed by a parameter
θ:
F = νθ ∈P(R) : θ ∈Θ	 .
Here, the functional values associated with the set of statistical functionals Ψ
correspond to the (inﬁnite-dimensional) parameter θ, so that
FΨ = P(R) .
This clearly implies that FΨ is closed under the distributional Bellman operator
T π (Section 5.3) and hence that approximation-free distributional dynamic
programming is (mathematically) possible with FΨ.
8.8
Moment Temporal-Diﬀerence Learning*
In Section 8.2, we introduced the m-moment Bellman operator, from which an
exact dynamic programming algorithm can be derived. A natural follow-up is to
apply the tools of Chapter 6 to derive an incremental algorithm for learning the
moments of the return-distribution function from samples. Here, an algorithm
that incrementally updates an estimate M ∈RX×A×m of the m ﬁrst moments of
the return function can be directly obtained through the unbiased estimation
approach, as the corresponding operator can be written as an expectation. Given
a sample transition (x, a, r, x′, a′), the unbiased estimation approach yields the
update rule (for i = 1, . . . , m)
M(x, a, i) ←(1 −α)M(x, a, i) + α

iX
j=0
γi−j
 i
j
!
r jM(x′, a′, i −j)
,
(8.16)
where again we take M(·, ·, 0) = 1 by convention.
Unlike the TD and CTD algorithms analyzed in Chapter 6, this algorithm is
derived from an operator, T π
(m), which is not a contraction in a supremum-norm
over states. As a result, the theory developed in Chapter 6 cannot immediately
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Statistical Functionals
253
be applied to demonstrate convergence of this algorithm under appropriate
conditions. With some care, however, a proof is possible; we now give an
overview of what is needed.
The proof of Proposition 8.7 demonstrates that the behavior of T π
(m) is closely
related to that of a contraction mapping. Speciﬁcally, the behavior of T π
(m) in
updating the estimates of ith moments of returns is contractive if the lower
moment estimates are suﬃciently close to their correct values. To turn these
observations into a proof of convergence, an inductive argument on the moments
being learnt must be made, as in the proof of Proposition 8.7. Further, the
approach of Chapter 6 needs to be extended to deal with a vanishing bias term
in the update to account for this “near-contractivity” of T π
(m); to this end one
may, for example, begin from the analysis of Bertsekas and Tsitsiklis (1996,
Proposition 4.5).
Before moving on, let us remark that in practice, we are likely to be interested
in centered moments such as the variance (m = 2); these take the form
Eπ
h ∞
X
t=0
γtRt −Qπ(x, a)
m  X0 = x, A0 = a
i
,
These can be derived from their uncentered counterparts; for example, the vari-
ance of the return distribution ηπ(x, a) is obtained from the ﬁrst two uncentered
moments via Equation 8.2.
It is also possible to perform dynamic programming on centered moments
directly, as was shown in the context of the mean and variance in Section 5.4
(Exercise 8.14 asks you to derive the Bellman operators for the more general
case of the ﬁrst m centered moments). Given in terms of state-action pairs, the
Bellman equation for the return variances ¯Mπ(·, ·, 2) ∈RX×A is
¯Mπ(x, a, 2) = Varπ(R | X = x, A = a) +
(8.17)
γ2
Varπ(Qπ(X′, A′) | X = x, A = a) + Eπ[ ¯Mπ(X′, A′, 2) | X = x, A = a]

;
contrast with Equation 5.20.
One challenge with deriving an incremental algorithm for learning the vari-
ance directly is that unbiasedly estimating some of the variance terms on the
right-hand side requires multiple samples. For example, an unbiased estimator
of
Varπ(Qπ(X′, A′) | X = x, A = a)
in general requires two independent realizations of X′, A′ for a given source
state-action pair x, a. Consequently, unbiased estimation of the corresponding
operator application with a single transition is not feasible in this case. Despite
the fact that the ﬁrst m centered and uncentered moments of a probability
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

254
Chapter 8
distribution can be recovered from one another, there is a distinct advantage
associated with working with uncentered moments when learning from samples.
8.9
Technical Remarks
Remark 8.1. Theorem 8.12 illustrates how dynamic programming over func-
tional values must incur some approximation error, unless the underlying sketch
is Bellman closed. One way to avoid this error is to augment the state space
with additional information: for example, the return accumulated so far. We in
fact took this approach when optimizing the conditional value-at-risk (CVaR)
of the return in Chapter 7; in fact, risk measures are statistical functionals that
may also take on the value −∞(see Deﬁnition 7.14).
△
Remark 8.2 (Proof of Theorem 8.12). It is suﬃcient to consider a pair of
states, x and y, such that x deterministically transitions to y with reward r.
Because ψ is Bellman closed, we can identify an associated Bellman operator
T π
ψ. For a given return function η whose state-indexed collection of functional
values is s = ψ(η), let us write (T π
ψs)i(x) for the ith functional value at state x,
for i = 1, . . . , m. By construction and deﬁnition of the operator T π
ψ, (T π
ψs)i(x) is a
function of the functional values at y as well as the reward r and discount factor
γ, and so we may write
(T π
ψs)i(x) = gi
 r, γ, ψ1(η(y)), …, ψm(η(y))
for some function gi. We next argue that gi is aﬃne66 in the inputs
ψ1(η(y)), …, ψm(η(y)). This is readily observed as each functional ψ1, …, ψm is
aﬃne in its input distribution,
ψi(αν + (1 −α)ν′) =
E
Z∼αν+(1−α)ν′[ fi(Z)]
= α E
Z∼ν[ fi(Z)] + (1 −α) E
Z∼ν′[ fi(Z)]
= αψi(ν) + (1 −α)ψi(ν′) ,
and
(T π
ψs)i(x) =
E
Z∼η(y)[ fi(r + γZ)]
is also aﬃne as a function of η. This aﬃneness would be contradicted if gi were
not also aﬃne. Hence, there exist functions βi : R × [0, 1) →R for i = 1, …, m
66. Recall that a function h : M →M′ between vector spaces M and M′ is aﬃne if for u1, u2 ∈M,
λ ∈(0, 1), we have h(λu1 + (1 −λ)u2) = λh(u1) + (1 −λ)h(u2).
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Statistical Functionals
255
such that
gi
 r, γ, ψ1(η(y)), …, ψm(η(y)) = β0(r, γ) +
m
X
i=1
βi(r, γ)ψi(η(y)) ,
and therefore
E
Z∼η(y)[ fi(r + γZ)] =
E
Z∼η(y)

m
X
j=0
βi(r, γ)f j(Z)

,
where f0(z) = 1. Taking η(y) to be a Dirac delta δz then gives the following
identity:
fi(r + γz) =
m
X
j=0
βi(r, γ) f j(z) .
We therefore have that the ﬁnite-dimensional function space spanned by
f0, f1, …, fm (where f0 is the constant function equal to 1) is closed under
translation (by r ∈R) and scaling (by γ ∈[0, 1)). Engert (1970) shows that
the only ﬁnite-dimensional subspaces of measurable functions closed under
translation are contained in the span of ﬁnitely many functions of the form
z 7→zℓexp(λz), with ℓ∈N and λ ∈C. Since we further require closure under
scaling by γ ∈[0, 1), we deduce that we must have λ = 0 in any such function,
and the subspace must be equal to the space spanned by the ﬁrst n monomials
(and the constant function).
To conclude, since each monomial z 7→zi for i = 1, …, n is expressible as a
linear combination of f0, …, fm, the corresponding expectations EZ∼ν[Zi] are
expressible as linear combinations of the expectations EZ∼ν[f j(Z)], for any
distribution ν. The converse also holds, and so we conclude that the sketch ψ
encodes the same distributional information as the ﬁrst n moments.
△
Remark 8.3. The question of whether a distribution is characterized by its
sequence of moments has been a subject of study in probability theory for
over a century. The suﬃcient condition on the moment-generating function
described in Section 8.8 means that the characteristic function of such a dis-
tribution can be written as a power series with scaled moments as coeﬃcients,
ensuring uniqueness of the distribution; see, for example, Billingsley (2012)
for a detailed discussion. Lin (2017) gives a survey of known suﬃcient condi-
tions for characterization, as well as examples where characterization does not
hold.
△
8.10
Bibliographical Remarks
8.1. Statistical functionals are a core notion in statistics; see, for example,
the classic text by van der Vaart (2000). In reinforcement learning, speciﬁc
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

256
Chapter 8
functionals such as moments, quantiles, and CVaR have been of interest for
risk-sensitive control (more on this in the bibliographical remarks of Chapter 7).
Chandak et al. (2021) consider the problem of oﬀ-policy Monte Carlo policy
evaluation of arbitrary statistical functionals of the return distribution.
8.2, 8.8. Sobel (1982) gives a Bellman equation for return-distribution moments
for state-indexed value functions with deterministic policies. More recent work
in this direction includes that of Lattimore and Hutter (2012), Azar et al. (2013),
and Azar et al. (2017), who make use of variance estimates in combination with
Bernstein’s inequality to improve the eﬃciency of exploration algorithms, as
well as the work of White and White (2016), who use estimated return variance
to set trace coeﬃcients in multistep TD learning methods. Sato et al. (2001),
Tamar et al. (2012), Tamar et al. (2013), and Prashanth and Ghavamzadeh
(2013) further develop methods for learning the variance of the return. Tamar
et al. (2016) show that the operator T π
(2) is a contraction under a weighted
norm (see Exercise 8.4), develop an incremental algorithm with a proof of
convergence using the ODE method, and study both dynamic programming
and incremental algorithms under linear function approximation (the topic of
Chapter 9).
8.3–8.5. The notion of Bellman closedness is due to Rowland et al. (2019),
although our presentation here is a revised take on the idea. The noted connec-
tion between Bellman closedness and diﬀusion-free representations and the
term “statistical functional dynamic programming” are new to this book.
8.6. The expectile dynamic programming algorithm is new to this book but
is directly derived from expectile temporal-diﬀerence learning (Rowland et
al. 2019). Expectiles themselves were introduced by Newey and Powell (1987)
in the context of testing in econometric regression models, with the asymmetric
squared loss deﬁning expectiles already appearing in Aigner et al. (1976).
Expectiles have since found further application as risk measures, particularly
within ﬁnance (Taylor 2008; Kuan et al. 2009; Bellini et al. 2014; Ziegel
2016; Bellini and Di Bernardino 2017). Our presentation here focuses on the
asymmetric squared loss, requiring a ﬁnite second-moment assumption, but an
equivalent deﬁnition allows expectiles to be deﬁned for all distributions with a
ﬁnite ﬁrst moment (Newey and Powell 1987).
8.7. The study of characteristic functions in distributional reinforcement learn-
ing is due to Farahmand (2019), who additionally provides error propagation
analysis for the characteristic value iteration algorithm, in which value iteration
is carried out with characteristic function representations of return distributions.
Earlier, Mandl (1971) studied the characteristic function of the return in Markov
decision processes with deterministic immediate rewards and policies. Chow
et al. (2015) combine a state augmentation method (see Chapter 7) with an
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Statistical Functionals
257
inﬁnite-dimensional Bellman equation for CVaR values to learn a CVaR-optimal
policy. They develop an implementable version of the algorithm by tracking
ﬁnitely many CVaR values and using linear interpolation for the remainder, an
approach related to the imputation strategies described earlier in the chapter.
Characterization via the quantile function has driven the success of several large-
scale distributional reinforcement learning algorithms (Dabney et al. 2018a;
Yang et al. 2019), and is the subject of further study in Chapter 10.
8.11
Exercises
Exercise 8.1. Consider the m-moment Bellman operator T π
(m) (Deﬁnition 8.6).
For M ∈RX×A×m, deﬁne the norm
∥M∥∞,max = max
i∈{1,...,m} sup
x∈X
a∈A
M(x, a, i).
By means of a counterexample, show that T π
(m) is not a contraction mapping in
the metric induced by ∥· ∥∞,max.
△
Exercise 8.2. Let ε > 0. Determine a bound on the computational cost (in O(·)
notation) of performing iterative policy evaluation with the m-moment Bellman
operator to obtain an approximation ˆMπ such that
max
i∈{1,...,m} sup
x∈X
a∈A
| ˆMπ(x, a, i) −Mπ(x, a, i)| < ε.
You may ﬁnd it convenient to refer to the proof of Proposition 8.7.
△
Exercise 8.3. Equation 5.2 gives the value function Vπ as the solution of the
linear system of equations
V = rπ + γPπV.
Provide the analogous linear system for the moment function Mπ.
△
Exercise 8.4. The purpose of this exercise is to show that T π
(2) is a contraction
mapping on RX×A×2 in a weighted L∞norm, as shown by Tamar et al. (2016).
Let M ∈RX×A×2 be a moment function estimate (speciﬁcally, for the ﬁrst two
moments). For each α ∈(0, 1), deﬁne the α-weighted norm on RX×A×2 by
∥M∥α = α∥M(1)∥∞+ (1 −α)∥M(2)∥∞,
where M(i) = M(·, ·, i) ∈RX×A. For any M, M′ ∈RX×A×2, show that
∥T π
(2)(M−M′)∥α ≤α∥γPπ(M(1) −M′
(1))∥∞
+ (1 −α)∥2γCrPπ(M(1) −M′
(1)) + γ2Pπ(M(2) −M′
(2))∥∞,
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

258
Chapter 8
where Pπ is the state-action transition operator, deﬁned by
(PπQ)(x, a) =
X
(x′,a′)∈X×A
Pπ(x′|x, a)π(a′|x′)Q(x′, a′) ,
and Cr is the diagonal reward operator
(CrQ)(x, a) = E[R | X = x, A = a]Q(x, a) .
Writing λ ≥0 for the Lipschitz constant of CrPπ with respect to the L∞metric,
deduce that
∥T π
(2)(M −M′)∥α
≤(αγ + 2(1 −α)γλ)∥M(1) −M′
(1)∥∞+ (1 −α)γ2∥M(2) −M′
(2)∥∞.
Hence, deduce that there exist parameters α ∈(0, 1), β ∈[0, 1) such that
∥T π
(2)(M −M′)∥α ≤β∥M −M′∥α ,
as required.
△
Exercise 8.5. Consider the median functional
ν 7→F−1
ν (0.5) .
Show that there does not exist a function f : R →R such that, for any ν ∈R,
E
Z∼ν[ f(Z)] = F−1
ν (0.5) .
△
Exercise 8.6. Consider the subset of probability distributions endowed with a
probability density fν. Repeat the preceding exercise for the diﬀerential entropy
functional
ν 7→−
Z
z∈R
fν(z) log   fν(z)dz .
△
Exercise 8.7. For the imputation strategy of Example 8.16:
(i) show that for m = 2, the imputation strategy is exact, for any n ∈N+.
(ii) show that for m > 2, this imputation strategy is inexact. Hint. Find a
distribution ν for which ψc
i (ι(p1, . . . , pm)) , pi, for some i = 1, . . . , m.
△
Exercise 8.8. In Section 8.4, we argued that every statistical functional dynamic
programming algorithm is a distributional dynamic programming algorithm.
Explain why the converse is false. Under what circumstances may we favor
either an algorithm that operates on statistical functionals or one that operates
on probability distribution representations?
△
Exercise 8.9. Consider an imputation strategy ι for a sketch ψ. We say the (ψ, ι)
pair is mean-preserving if, for any probability distribution ν ∈Pψ(R),
ν′ = ιψ(ν)
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Statistical Functionals
259
satisﬁes
E
Z∼ν′[Z] = E
Z∼ν[Z] .
Show that in this case, the operator
ψ ◦T π ◦ι
is also mean-preserving.
△
Exercise 8.10. Using your favorite numerical computation software, implement
the expectile imputation strategy described in Section 8.6. Speciﬁcally:
(i) Implement a procedure for approximately determining the expectile values
e1, . . . , em of a given distribution. Hint. An incremental approach in the style
of quantile regression, or a binary search approach, will allow you to deal
with continuous distributions.
(ii) Given a set of expectile values, e1, . . . , em, implement a procedure that
imputes an n-quantile distribution
1
n
n
X
i=1
δθi
by minimizing the objective given in Equation 8.15.
Test your implementation on discrete and continuous distributions, and compare
it with the best m-quantile approximation of those distributions. Is one method
better suited to discrete distributions than the other? More generally, when
might one method be preferred over the other?
△
Exercise 8.11. Formulate a variant of expectile dynamic programming that
imputes n-quantile distributions and whose (possibly approximate) imputation
strategy is mean-preserving in the sense of Exercise 8.9.
△
Exercise 8.12 (*). This exercise applies the line of reasoning from Chapter 4
to characteristic functions and is based on Farahmand (2019). For a probability
distribution ν, recall that its characteristic function χν is
χν(u) = E
Z∼ν
eiuZ .
Now, for p ∈[1, ∞), deﬁne the probability metric
d1,p(ν, ν′) =
Z
u∈R
|χν(u) −χν′(u)|
|u|p
du
and its supremum extension to return functions
d1,p(η, η′) =
sup
(x,a)∈X×A
d1,p(η(x, a), η′(x, a)) .
(i) Determine a subset Pχ,p(R) ⊆P(R) on which d1,p is a proper metric.
(ii) Provide assumption(s) under which the return function ηπ lies in Pχ,p(R).
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

260
Chapter 8
(iii) Prove that for p ≥2, the distributional Bellman operator is a contraction
mapping in d1,p, with modulus γp−1.
△
Exercise 8.13 (*). Consider the probability metric
d2,2(ν, ν′) =
 Z
u∈R
 χν(u) −χν′(u)2
u2
du
1/2 .
Show that d2,2 is the Cramér distance ℓ2. Hint. Use the Parseval–Plancherel
identity.
△
Exercise 8.14. Let m ∈N+. Derive a Bellman operator on RX×A×m whose
unique ﬁxed point ˜Mπ is the collection of centered moments:
˜Mπ(x, a, i) = E
h Gπ(x, a) −Qπ(x, a)ii
,
i = 1, . . . , m .
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

9
Linear Function Approximation
A probability distribution representation is used to describe return functions
in terms of a collection of numbers that can be stored in a computer’s mem-
ory. With it, we can devise algorithms that operate on return distributions in a
computationally eﬃcient manner, including distributional dynamic program-
ming algorithms and incremental algorithms such as CTD and QTD. Function
approximation arises when our representation of the value or return function
uses parameters that are shared across states. This allows reinforcement learning
methods to be applied to domains where it is impractical or even impossible to
keep in memory a table with a separate entry for each state, as we have done
in preceding chapters. In addition, it makes it possible to make predictions
about states that have not been encountered – in eﬀect, to generalize a learned
estimate to new states.
As a concrete example, consider the problem of determining an approxima-
tion to the optimal value function for the game of Go. In Go, players take turns
placing white and black stones on a 19 × 19 grid. At any time, each location of
the board is either occupied by a white or black stone, or unoccupied; conse-
quently, there are astronomically many possible board states.67 Any practical
algorithm for this problem must therefore use a succinct representation of its
value or return function.
Function approximation is also used to apply reinforcement learning algo-
rithms to problems with continuous state variables. The classic Mountain Car
domain, in which the agent must drive an underpowered car up a steep hill, is
one such problem. Here, the state consists of the car’s position and velocity
(both bounded on some interval); learning to control the car requires being able
67. A naive estimate is 319×19. The real ﬁgure is somewhat lower due to symmetries and the
impossibility of certain states.
261
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

262
Chapter 9
Figure 9.1
A Markov decision process in which aliasing due to function approximation may result
in the wrong value function estimates even at the unaliased states x1 and x2.
to map two-dimensional points to a desired action, usually by predicting the
return obtained following this action.68
While there are similarities between the use of probability distribution rep-
resentations (parameterizing the output of a return function) and function
approximation (parameterizing its input), the latter requires a diﬀerent algorith-
mic treatment. When function approximation is required, it is usually because
it is infeasible to exhaustively enumerate the state space and apply dynamic
programming methods. One solution is to rely on samples (of the state space,
transitions, trajectories, etc.), but this results in additional complexities in the
algorithm’s design. Combining incremental algorithms with function approxi-
mation may result in instability or even divergence; in the distributional setting,
the analysis of these algorithms is complicated by two levels of approximation
(one for probability distributions and one across states). With proper care, how-
ever, function approximation provides an eﬀective way of dealing with large
reinforcement learning problems.
9.1
Function Approximation and Aliasing
By necessity, when parameters are shared across states, a single parameter usu-
ally aﬀects the predictions (value or distribution) at multiple states. In this case,
we say that the states are aliased. State aliasing has surprising consequences in
the context of reinforcement learning, including the unwanted propagation of
errors and potential instability in the learning process.
Example 9.1. Consider the Markov decision process in Figure 9.1, with four
nonterminal states x1, x2, y, and z, a single action, a deterministic reward
function, an initial state distribution ξ0, and no discounting. Consider an
68. Domains such as Mountain Car – which have a single initial state and a deterministic transition
function – can be solved without function approximation: for example, by means of a search
algorithm. However, function approximation allows us to learn a control policy that can in theory
be applied to any given state and has a low run-time cost.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Linear Function Approximation
263
approximation based on three parameters wx1, wx2, and wyz, such that
ˆV(x1) = wx1
ˆV(x2) = wx2
ˆV(y) = ˆV(z) = wyz .
Because the rewards from y and z are diﬀerent, no choice of wyz can yield
ˆV = Vπ. As such, any particular choice of wyz trades oﬀapproximation error at
y and z. When a reinforcement learning algorithm is combined with function
approximation, this trade-oﬀis made (implicitly or explicitly) based on the
algorithm’s characteristics and the parameters of the learning process. For
example, the best approximation obtained by the incremental Monte Carlo
algorithm (Section 3.2) correctly learns the value of x1 and x2:
ˆV(x1) = 2 ,
ˆV(x2) = 0 ,
but learns a parameter wyz that depends on the frequency at which states y and
z are visited. This is because wyz is updated toward 2 whenever the estimate
ˆV(y) is updated and toward 0 whenever ˆV(z) is updated. In our example, the
frequency at which this occurs is directly implied by the initial state distribution
ξ0, and we have
wyz = 2 × Pπ(X1 = y) + 0 × Pπ(X1 = z)
= 2ξ0(x1) .
(9.1)
When the approximation is learned using a bootstrapping procedure, aliasing
can also result in incorrect estimates at states that are not themselves aliased.
The solution found by temporal-diﬀerence learning, wyz, is as per Equation 9.1,
but the algorithm also learns the incorrect value at x1 and x2:
ˆV(x1) = ˆV(x2) = 0 + γ × ˆV(z)
= 2ξ0(x1) .
Thus, errors due to function approximation can compound in unexpected ways;
we will study this phenomenon in greater detail in Section 9.3.
△
In a linear value function approximation, the value estimate at a state x is
given by a weighted combination of features of x. This is in opposition to a
tabular representation, where value estimates are stored in a table with one
entry per state.69 As we will see, linear approximation is simple to implement
and relatively easy to analyze.
Deﬁnition 9.2. Let n ∈N+. A state representation is a mapping φ : X →Rn.
A linear value function approximation Vw ∈RX is parameterized by a weight
69. Technically, a tabular representation can also be expressed using the trivial collection of indicator
features. In practice, the two are used in distinct problem settings.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

264
Chapter 9
vector w ∈Rn and maps states to their expected return estimates according to
Vw(x) = φ(x)⊤w .
A feature φi(x) ∈R, i = 1, . . . , n is an individual element of φ(x). We call the
vectors φi ∈RX basis functions.
△
As its name implies, a linear value function approximation is linear in the
weight vector w. That is, for any w1, w2 ∈Rn, α, β ∈R, we have
Vαw1+βw2 = αVw1 + βVw2 .
In addition, the gradient of Vw(x) with respect to w is given by
∇wVw(x) = φ(x) .
As we will see, these properties aﬀect the learning dynamics of algorithms that
use linear value function approximation.
We extend linear value function approximation to state-action values in the
usual way. For a state representation φ : X × A →Rn, we deﬁne
Qw(x, a) = φ(x, a)⊤w .
A practical alternative is to use a distinct set of weights for each action and a
common representation φ(x) across actions. In this case, we use a collection of
weight vectors (wa : a ∈A), with wa ∈Rn, and write
Qw(x, a) = φ(x)⊤wa .
Remark 9.1 discusses the relationship between these two methods.
9.2
Optimal Linear Value Function Approximations
In this chapter, we will assume that there is a ﬁnite (but very large) number
of states. In this case, the state representation φ : X →Rn can expressed as a
feature matrix Φ ∈RX×n whose rows are the vectors φ(x), x ∈X. This yields the
approximation
Vw = Φw .
The state representation determines a space of value function approximations
that are constructed from linear combinations of features. Expressed in terms of
the feature matrix, this space is
Fφ = {Φw : w ∈Rn} .
We ﬁrst consider the problem of ﬁnding the best linear approximation to a value
function Vπ. Because Fφ is a n-dimensional linear subspace of the space of
value functions RX, there are necessarily some value functions that cannot be
represented with a given state representation (unless n = NX). We measure the
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Linear Function Approximation
265
discrepancy between a value function Vπ and an approximation Vw = Φw in
ξ-weighted L2 norm, for ξ ∈P(X):
∥Vπ −Vw∥ξ,2 =
 X
x∈X
ξ(x) Vπ(x) −Vw(x)21/2 .
The weighting ξ reﬂects the relative importance given to diﬀerent states. For
example, we may weigh states according to the frequency at which they are
visited, or we may put greater importance on initial states. Provided that ξ(x) > 0
for all x ∈X, the norm ∥· ∥ξ,2 induces the ξ-weighted L2 metric on RX:70
dξ,2(V, V′) = ∥V −V′∥ξ,2.
The best linear approximation under this metric is the solution to the
minimization problem
min
w∈Rn ∥Vπ −Vw∥ξ,2 .
(9.2)
One advantage of measuring approximation error in a weighted L2 norm, rather
than the L∞norm used in the analyses of previous chapters, is that a solution
w∗to Equation 9.2 can be easily determined by solving a least-squares system.
Proposition 9.3. Suppose that the columns of the feature matrix Φ are
linearly independent and ξ(x) > 0 for all x ∈X. Then, Equation 9.2 has a
unique solution w∗given by
w∗= (Φ⊤ΞΦ)−1Φ⊤ΞVπ ,
(9.3)
where Ξ ∈RX×X is a diagonal matrix with entries (ξ(x) : x ∈X).
△
Proof. By a standard calculus argument, any optimum w must satisfy
∇w
X
x∈X
ξ(x) Vπ(x) −φ(x)⊤w2 = 0
=⇒
X
x∈X
ξ(x) Vπ(x) −φ(x)⊤wφ(x) = 0 .
Written in matrix form, this is
Φ⊤Ξ(Φw −Vπ) = 0
=⇒Φ⊤ΞΦw = Φ⊤ΞVπ .
70. Technically, ∥· ∥ξ,2 is only a proper norm if ξ is strictly positive for all x; otherwise, it is
a semi-norm. Under the same condition, dξ,2 is proper metric; otherwise, it is a pseudo-metric.
Assuming that ξ(x) > 0 for all x addresses uniqueness issues and simpliﬁes the analysis.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

266
Chapter 9
Because Φ has rank n, then so does Φ⊤ΞΦ: for any u ∈Rn with u , 0, we have
Φu = v , 0 and so
u⊤Φ⊤ΞΦu = v⊤Ξv
=
X
x∈X
ξ(x)v(x)2 > 0 ,
as ξ(x) > 0 and v(x)2 ≥0 for all x ∈X, and P
x∈X v(x)2 > 0. Hence, Φ⊤ΞΦ is
invertible and the only solution w∗to the above satisﬁes Equation 9.3.
9.3
A Projected Bellman Operator for Linear Value Function
Approximation
Dynamic programming ﬁnds an approximation to the value function Vπ by
successively computing the iterates
Vk+1 = T πVk .
As we saw in preceding chapters, dynamic programming makes it easy to derive
incremental algorithms for learning the value from samples and also allows us to
ﬁnd an approximation to the optimal value function Q∗. Often, it is the de facto
approach for ﬁnding an approximation of the return-distribution function. It is
also particularly useful when using function approximation, where it enables
algorithms that learn by extrapolating to unseen states.
When dynamic programming is combined with function approximation, we
obtain a range of methods called approximate dynamic programming. In the case
of linear value function approximation, the iterates (Vk)k≥0 are given by linear
combinations of features, which allows us to apply dynamic programming to
problems with larger state spaces than can be described in memory. In general,
however, the space of approximations Fφ is not closed under the Bellman
operator, in the sense that
V ∈Fφ ̸=⇒T πV ∈Fφ .
Similar to the notion of a distributional projection introduced in Chapter 5, we
address the issue by projecting, for V ∈RX, the value function T πV back onto
Fφ. Let us deﬁne the projection operator Πφ,ξ : RX →RX as
(Πφ,ξV)(x) = φ(x)⊤w∗
such that
w∗∈arg min
w∈Rn
∥V −Vw∥ξ,2 .
This operator returns the approximation Vw∗= Φw∗that is closest to V ∈RX
in the ξ-weighted L2 norm. As established by Proposition 9.3, when ξ is fully
supported on X and the basis functions (φi)n
i=1 are linearly independent, then this
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Linear Function Approximation
267
projection is unique.71 By repeatedly applying the projected Bellman operator
Πφ,ξT π from an initial condition V0 ∈Fφ, we obtain the iterates
Vk+1 = Πφ,ξT πVk .
(9.4)
Unlike the approach taken in Chapter 5, however, it is usually impractical
to implement Equation 9.4 as is, as there are too many states to enumerate.
A simple solution is to rely on a sampling procedure that approximates the
operator itself. For example, one may sample a batch of K states and ﬁnd the
best linear ﬁt to T πVk at these states. In the next section, we will study the
related approach of using an incremental algorithm to learn the linear value
function approximation from sample transitions. Understanding the behavior
of the exact projected operator Πφ,ξT π informs us about the behavior of these
approximations, as it describes in some sense the ideal behavior that one expects
from both of these approaches.
Also diﬀerent from the setting of Chapter 5 is the presence of aliasing across
states. As a consequence of this aliasing, we have limited freedom in the choice
of projection if we wish to guarantee the convergence of the iterates of Equation
9.4. To obtain such a guarantee, in general, we need to impose a condition on
the distribution ξ that deﬁnes the projection Πφ,ξ. We will demonstrate that
the projected Bellman operator is a contraction mapping with modulus γ with
respect to the ξ-weighted L2 norm, for a speciﬁc choice of ξ. Historically, this
approach predates the analysis of distributional dynamic programming and is in
fact a key inspiration for our analysis of distributional reinforcement learning
algorithms as approximating projected Bellman operators (see bibliographical
remarks).
To begin, let us introduce the convention that the Lipschitz constant of an
operator with respect to a norm (such as L∞) follows Deﬁnition 5.20, applied to
the metric associated with the norm. In the case of L∞, this metric deﬁnes the
distance between u, u′ ∈RX by
∥u −u′∥∞.
Now recall that the Bellman operator T π is a contraction mapping in L∞norm,
with modulus γ. One reason this holds is because the transition matrix Pπ
satisﬁes
∥Pπu∥∞≤∥u∥∞,
for all u ∈RX ;
71. If only the ﬁrst of those two conditions hold, then there may be multiple optimal weight vectors.
However, they all result in the same value function, and the projection remains unique.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

268
Chapter 9
we made use of this fact in the proof of Proposition 4.4. This is equivalent to
requiring that the Lipschitz constant of Pπ satisfy
∥Pπ∥∞≤1 .
Unfortunately, the Lipschitz constant of Πφ,ξ in the L∞norm may be greater
than 1, precluding a direct analysis in that norm (see Exercise 9.5). We instead
prove that the Lipschitz constant of Pπ in the ξ-weighted L2 norm satisﬁes the
same condition when ξ is taken to be a steady-state distribution under policy π.
Deﬁnition 9.4. Consider a Markov decision process and let π be a policy
deﬁning the probability distribution Pπ over the random transition (X, A, R, X′).
We say that ξ ∈P(X) is a steady-state distribution for π if for all x′ ∈X,
ξ(x′) =
X
x∈X
ξ(x)Pπ(X′ = x′ | X = x) .
△
Assumption 9.5. There is a unique steady-state distribution ξπ, and it satisﬁes
ξπ(x) > 0 for all x ∈X.
△
Qualitatively, Assumption 9.5 ensures that approximation error at any state
is reﬂected in the norm ∥· ∥ξπ,2; contrast with the setting in which ξπ is nonzero
only at a handful of states. Uniqueness is not strictly necessary but simpliﬁes the
exposition. There are a number of practical scenarios in which the assumption
does not hold, most importantly when there is a terminal state. We discuss how
to address such a situation in Remark 9.2.
Lemma 9.6. Let π : X →P(A) be a policy and let ξπ be a steady-state distri-
bution for this policy. The transition matrix is a nonexpansion with respect to
the ξπ-weighted L2 metric. That is,
∥Pπ∥ξπ,2 = 1 .
△
Proof. A simple algebraic argument shows that if U ∈RX is such that U(x) = 1
for all x, then
PπU = U .
This shows that ∥Pπ∥ξπ,2 ≥1. Now for an arbitrary U ∈RX, write
∥PπU∥2
ξπ,2 =
X
x∈X
ξπ(x) (PπU)(x)2
=
X
x∈X
ξπ(x)
 X
x′∈X
Pπ(x′ | x)U(x′)
2
(a)
≤
X
x∈X
ξπ(x)
X
x′∈X
Pπ(x′ | x) U(x′)2
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Linear Function Approximation
269
=
X
x′∈X
 U(x′)2 X
x∈X
ξπ(x)Pπ(x′ | x)
(b)=
X
x′∈X
ξπ(x′) U(x′)2
= ∥U∥2
ξπ,2 ,
where (a) follows by Jensen’s inequality and (b) by Deﬁnition 9.4. Since
∥Pπ∥ξπ,2 = sup
U∈RX
∥PπU∥ξπ,2
∥U∥ξπ,2
≤1 ,
this concludes the proof.
Lemma 9.7. For any ξ ∈P(X) with ξ(x) > 0, the projection operator Πφ,ξ is a
nonexpansion in the ξ-weighted L2 metric, in the sense that
∥Πφ,ξ∥ξ,2 = 1 .
△
The proof constitutes Exercise 9.4.
Theorem 9.8. Let π be a policy and suppose that Assumption 9.5 holds;
let ξπ be the corresponding steady-state distribution. The projected Bellman
operator Πφ,ξπT π is a contraction with respect to the ξπ-weighted L2 norm
with modulus γ, in the sense that for any V, V′ ∈RX,
∥Πφ,ξπT πV −Πφ,ξπT πV′∥ξπ,2 ≤γ∥V −V′∥ξπ,2 .
As a consequence, this operator has a unique ﬁxed point
ˆVπ = Πφ,ξπT π ˆVπ,
(9.5)
which satisﬁes
∥ˆVπ −Vπ∥ξπ,2 ≤
1
p
1 −γ2 ∥Πφ,ξπVπ −Vπ∥ξπ,2 .
(9.6)
In addition, for an initial value function V0 ∈RX, the sequence of iterates
Vk+1 = Πφ,ξπT πVk
(9.7)
converges to this ﬁxed point.
△
Proof. The contraction result and consequent convergence of the iterates in
Equation 9.7 to a unique ﬁxed point follow from Lemmas 9.6 and 9.7, which,
combined with Lemma 5.21, allow us to deduce that
∥Πφ,ξπT π∥ξπ,2 ≤γ .
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

270
Chapter 9
Because Assumption 9.5 guarantees that ∥· ∥ξπ,2 induces a proper metric, we
may then apply Banach’s ﬁxed-point theorem. For the error bound of Equation
9.6, we use Pythagoras’s theorem to write
∥ˆVπ −Vπ∥2
ξπ,2 = ∥ˆVπ −Πφ,ξπVπ∥2
ξπ,2 + ∥Πφ,ξπVπ −Vπ∥2
ξπ,2
= ∥Πφ,ξπT π ˆVπ −Πφ,ξπT πVπ∥2
ξπ,2 + ∥Πφ,ξπVπ −Vπ∥2
ξπ,2
≤γ2∥ˆVπ −Vπ∥2
ξπ,2 + ∥Πφ,ξπVπ −Vπ∥2
ξπ,2 ,
since ∥Πφ,ξπT π∥ξπ,2 ≤γ. The result follows by rearranging terms and taking the
square root of both sides.
Theorem 9.8 implies that the iterates (Vk)k≥0 are guaranteed to converge
when the projection is performed in ξπ-weighted L2 norm. Of course, this does
not imply that a projection in a diﬀerent norm may not result in a convergent
algorithm (see Exercise 9.8), but divergence is a practical concern (we return to
this point in the next section). A sound alternative to imposing a condition on
the distribution ξ is to instead impose a condition on the feature matrix Φ; this
is explored in Exercise 9.10.
When the feature matrix Φ forms a basis of RX (i.e., it has rank NX), it is
always possible to ﬁnd a weight vector w∗for which
Φw∗= T πV ,
for any given V ∈RX. As a consequence, for any ξ ∈P(X) we have
Πφ,ξT π = T π ,
and Theorem 9.8 reduces to the analysis of the (unprojected) Bellman operator
given in Section 4.2. On the other hand, when n < NX, the ﬁxed point of Equation
9.5 is in general diﬀerent from the minimum-error solution Πφ,ξπVπ and is
called the temporal-diﬀerence learning ﬁxed point. Similar to the diﬀusion
eﬀect studied in Section 5.8, successive applications of the projected Bellman
operator result in compounding approximation errors. The nature of this ﬁxed
point is by now well studied in the literature (see bibliographical remarks).
9.4
Semi-Gradient Temporal-Diﬀerence Learning
We now consider the design of a sample-based, incremental algorithm for
learning the linear approximation of a value function Vπ. In the context of
domains with large state spaces, algorithms that learn from samples have an
advantage over dynamic programming approaches: whereas the latter require
some form of enumeration and hence have a computational cost that depends
on the size of X, the computational cost of the former instead depends on the
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Linear Function Approximation
271
size of the function approximation (in the linear case, on the number of features
n).
To begin, let us consider learning a linear value function approximation using
an incremental Monte Carlo algorithm. We are presented with a sequence of
state-return pairs (xk, gk)k≥0, with the assumption that the source states xk are
realizations of independent draws from the distribution ξ and that each gk is a
corresponding independent realization of the random return Gπ(xk). As before,
we are interested in the optimal weight vector w∗for the problem
min
w∈Rn ∥Vπ −Vw∥ξ,2,
Vw(x) = φ(x)⊤w .
(9.8)
Of note, the optimal approximation Vw∗is also the solution to the problem
min
w∈Rn E[∥Gπ −Vw∥ξ,2] .
(9.9)
Consequently, a simple approach for ﬁnding w∗is to perform stochastic gradient
descent with a loss function that reﬂects Equation 9.9. For x ∈X and z ∈R, let
us deﬁne the sample loss
L(w) =  z −φ(x)⊤w2
whose gradient with respect to w is
∇wL(w) = −2 z −φ(x)⊤wφ(x) .
Stochastic gradient descent updates the weight vector w by following the (nega-
tive) gradient of the sample loss constructed from each sample. Instantiating
the sample loss with x = xk and z = gk, this results in the update rule
w ←w + αk(gk −φ(xk)⊤wφ(xk) ,
(9.10)
where αk ∈[0, 1) is a time-varying step size that also subsumes the constant
from the loss. Under appropriate conditions, this update rule ﬁnds the optimal
weight vector w∗(see, e.g., Bottou 1998). Exercise 9.9 asks you to verify that
the optimal weight vector w∗is a ﬁxed point of the expected update.
Let us now consider the problem of learning the value function from a
sequence of sample transitions (xk, ak, rk, x′
k)k≥0, again assumed to be indepen-
dent realizations from the appropriate distributions. Given a weight vector w,
the temporal-diﬀerence learning target for linear value function approximation
is
rk + γVw(x′
k) = rk + γφ(x′
k)⊤w .
We use this target in lieu of gk in Equation 9.10 to obtain the semi-gradient
temporal-diﬀerence learning update rule
w ←w + αk
 rk + γφ(x′
k)⊤w −φ(xk)⊤w
|                            {z                            }
TD error
φ(xk) ,
(9.11)
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

272
Chapter 9
in which the temporal-diﬀerence (TD) error appears, now with value function
estimates constructed from linear approximation. By substituting the Monte
Carlo target gk for the temporal-diﬀerence target, the intent is to learn an
approximation to Vπ by a bootstrapping process, as in the tabular setting. The
term “semi-gradient” reﬂects the fact that the update rule does not actually
follow the gradient of the sample loss
 rk + γφ(x′
k)⊤w −φ(xk)⊤w2 ,
which contains additional terms related to φ(x′) (see bibliographical remarks).
We can understand the relationship between semi-gradient temporal-
diﬀerence learning and the projected Bellman operator Πφ,ξT π by way of an
update rule deﬁned in terms of a second set of weights ˜w, the target weights.
This update rule is
w ←w + αk(rk + γφ(x′
k)⊤˜w −φ(xk)⊤wφ(xk) .
(9.12)
When ˜w = w, this is Equation 9.11. However, if ˜w is a separate weight vector,
this is the update rule of stochastic gradient descent on the sample loss
 rk + γφ(x′
k)⊤˜w −φ(xk)⊤w2 .
Consequently, this update rule ﬁnds a weight vector w∗that approximately
minimizes
∥T πV ˜w −Vw∥ξ,2 ,
and Equation 9.12 describes an incremental algorithm for computing a single
step of the projected Bellman operator applied to V ˜w; its solution w∗satisﬁes
Φw∗= Πφ,ξT πV ˜w .
This argument suggests that semi-gradient temporal-diﬀerence learning tracks
the behavior of the projected Bellman operator. In particular, at the ﬁxed point
ˆVπ = Φ ˆw of this operator, semi-gradient TD learning (applied to realizations
from the sample transition model (X, A, R, X′), with X ∼ξ) leaves the weight
vector unchanged, in expectation.
Eπ
 R + γφ(X′)⊤ˆw −φ(X)⊤ˆwφ(X) = 0 .
In semi-gradient temporal-diﬀerence learning, however, the sample target
rk + γφ(x′
k)⊤w depends on w and is used to update w itself. This establishes
a feedback loop that, combined with function approximation, can result in
divergence – even when the projected Bellman operator is well behaved. The
following example illustrates this phenomenon.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Linear Function Approximation
273
(a)
(b)
0
20
40
60
80
100
Iteration
100
101
102
Approximation error (log)
Figure 9.2
(a) A Markov decision process for which semi-gradient temporal-diﬀerence learning can
diverge. (b) Approximation error (measured in unweighted L2 norm) over the course of
100 runs of the algorithm with α = 0.01 and the same initial condition w0 = (1, 0, 0) but
diﬀerent draws of the sample transitions. Dashed (top) and dotted (bottom) lines indicate
runs with ξ(x) = 1/2 and 1/11, respectively.
Example 9.9 (Baird’s counterexample (Baird 1995)). Consider the Markov
decision process depicted in Figure 9.2a and the state representation
φ(x) = (1, 3, 2)
φ(y) = (4, 3, 3).
Since the reward is zero everywhere, the value function for this MDP is Vπ = 0.
However, if Xk ∼ξ with ξ(x) = ξ(y) = 1/2, the semi-gradient update
wk+1 = wk + α(0 + γφ(X′
k)⊤wk −φ(Xk)⊤wk)φ(Xk)
diverges unless w0 = 0. Figure 9.2b depicts the eﬀect of this divergence on the
approximation error: on average, the distance to the ﬁxed point ∥Φwk −0∥ξ,2
grows exponentially with each update. Also shown is the approximation error
over time if we take ξ to be the steady-state distribution
ξ(x) = 1/11
ξ(y) = 10/11,
in which case the approximation error becomes close to zero as k →∞. This
demonstrates the impact of the relative update frequency of diﬀerent states on
the behavior of the algorithm.
On the other hand, note that the basis functions implied by φ span RX and
thus
Πφ,ξT πV = T πV
for any V ∈R2. Hence, the iterates Vk+1 = Πφ,ξT πVk in this case converge to
0 for any initial V0 ∈R2. This illustrates that requiring the convergence that
the sequence of iterates derived from the projected Bellman operator is not
suﬃcient to guarantee the convergence of the semi-gradient iterates.
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

274
Chapter 9
Example 9.9 illustrates how reinforcement learning with function approx-
imation is a more delicate task than in the tabular setting. If states are
updated proportionally to their steady-state distribution, however, a convergence
guarantee becomes possible (see bibliographical remarks).
9.5
Semi-Gradient Algorithms for Distributional Reinforcement
Learning
With the right modeling tools, function approximation can also be used to
tractably represent the return functions of large problems. One diﬀerence with
the expected-value setting is that it is typically more challenging to construct
an approximation that is linear in the true sense of the word. With linear
value function approximations, adding weight vectors is equivalent to adding
approximations:
Vw1+w2(x) = Vw1(x) + Vw2(x) .
In the distributional setting, the same cannot apply because probability dis-
tributions do not form a vector space. This means that we cannot expect a
return-distribution function representation to satisfy
ηw1+w2(x)
?= ηw1(x) + ηw2(x) ;
(9.13)
the right-hand side is not a probability distribution (it is, however, a signed
distribution: more on this in Section 9.6). An alternative is to take a slightly
broader view and consider distributions whose parameters depend linearly on
w. There are now two sources of approximation: one due to the ﬁnite parameter-
ization of probability distributions in F, another because those parameters are
themselves aliased. This is an expressive framework, albeit one under which
the analysis of algorithms is signiﬁcantly more complex.
Linear QTD. Let us ﬁrst derive a linear approximation of quantile temporal-
diﬀerence learning. Linear QTD represents the locations of quantile distri-
butions using linear combinations of features. If we write w ∈Rm×n for the
matrix whose columns are w1, . . . , wm ∈Rn, then the linear QTD return function
estimate takes the form
ηw(x) = 1
m
m
X
i=1
δφ(x)⊤wi .
One can verify that ηw(x) is not a linear combination of features, even though
its parameters are. We construct the linear QTD update rule by following the
negative gradient of the quantile loss (Equation 6.12), taken with respect to the
parameters w1, . . . , wm. We ﬁrst rewrite this loss in terms of a function ρτ:
ρτ(∆) = |1{∆< 0} −τ| × |∆|
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Linear Function Approximation
275
so that for a sample z ∈R and estimate θ ∈R, the loss of Equation 6.12 can be
expressed as
|1{z < θ} −τ| × |z −θ| = ρτ(z −θ) .
We instantiate the quantile loss with θ = φ(x)⊤wi and τ = τi = 2i−1
2m to obtain the
loss
Lτi(w) = ρτi(z −φ(x)⊤wi) .
(9.14)
By the chain rule, the gradient of this loss with respect to wi is
∇wiρτi(z −φ(x)⊤wi) = −(τi −1{z < φ(x)⊤wi})φ(x) .
(9.15)
As in our derivation of QTD, from a sample transition (x, a, r, x′), we construct
m sample targets:
gj = r + γφ(x′)⊤w j,
j = 1, . . . , m .
By instantiating the gradient expression in Equation 9.15 with z = g j and taking
the average over the m sample targets, we obtain the update rule
wi ←wi −α 1
m
m
X
j=1
∇wiρτi

g j −φ(x)⊤wi

,
which is more explicitly
wi ←wi + α 1
m
m
X
j=1

τi −1{gj < φ(x)⊤wi}

|                      {z                      }
quantile TD error
φ(x).
(9.16)
Note that, by plugging gj into the expression for the gradient (Equation 9.15),
we obtain a semi-gradient update rule. That is, analogous to the value-based
case, Equation 9.16 is not equivalent to the gradient update
wi ←wi −α 1
m
m
X
j=1
∇wiρτi

r + γφ(x′)⊤wj −φ(x)⊤wi

,
because in general
∇wiρτi

r + γφ(x′)⊤wi −φ(x)⊤wi

,

τi −1{gi < φ(x)⊤wi}

φ(x) .
Linear CTD. To derive a linear approximation of categorical temporal-
diﬀerence learning, we represent the probabilities of categorical distributions
using linear combinations of features. Speciﬁcally, we apply the softmax func-
tion to transform the parameters (φ(x)⊤wi)m
i=1 into a probability distribution. We
write
pi(x; w) =
eφ(x)⊤wi
mP
j=1
eφ(x)⊤w j
.
(9.17)
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

276
Chapter 9
Recall that the probabilities  pi(x; w))m
i=1 correspond to m locations (θi)m
i=1. The
softmax transformation guarantees that the expression
ηw(x) =
m
X
i=1
pi(x; w)δθi
describes a bona ﬁde probability distribution. We thus construct the sample
target ¯η(x):
¯η(x) = Πc(br,γ)#ηw(x′) = Πc
h
m
X
i=1
pi(x′; w)δr+γθi
i
=
m
X
i=1
¯piδθi ,
where ¯pi denotes the probability assigned to the location θi by the sample target
¯η(x). Expressed in terms of the CTD coeﬃcients (Equation 6.10), the probability
¯pi is
¯pi =
m
X
j=1
ζi,j(r)p j(x′; w) .
As with quantile regression, we adjust the weights w by means of a gradient
descent procedure. Here, we use the cross-entropy loss between ηw(x) and
¯η(x):72
L(w) = −
m
X
i=1
¯pi log pi(x; w).
(9.18)
Combined with the softmax function, Equation 9.18 becomes
L(w) = −
m
X
i=1
¯pi

φ(x)⊤wi −log
m
X
j=1
eφ(x)⊤wj
.
With some algebra and again invoking the chain rule, we obtain that the gradient
with respect to the weights wi is
∇wiL(w) = −  ¯pi −pi(x; w)φ(x) .
By adjusting the weights in the opposite direction of this gradient, this results
in the update rule
wi ←wi + α   ¯pi −pi(x; w)
|           {z           }
CTD error
φ(x).
(9.19)
72. The choice of cross-entropy loss is justiﬁed because it is the matching loss for the softmax
function, and their combination results in a convex objective (Auer et al. 1995; see also Bishop
2006, Section 4.3.6).
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Linear Function Approximation
277
While interesting in their own right, linear CTD and QTD are particularly
important in that they can be straightforwardly adapted to learn return-
distribution functions with nonlinear function approximation schemes such
as deep neural networks; we return to this point in Chapter 10. For now, it is
worth noting that the update rules of linear TD, linear QTD, and linear CTD
can all be expressed as
wi ←wi + αεiφ(x) ,
where εi is an error term. One interesting diﬀerence is that for both linear
CTD and linear QTD, εi lies in the interval [−1, 1], while for linear TD, it
is unbounded. This gives evidence that we should expect diﬀerent learning
dynamics from these algorithms. In addition, combining linear TD or linear
QTD to a tabular state representation recovers the corresponding incremen-
tal algorithms from Chapter 6. For linear CTD, the update corresponds to a
tabular representation of the softmax parameters rather than the probabilities
themselves, and the correspondence is not as straightforward.
Analyzing linear QTD and CTD is complicated by the fact that the return
functions themselves are not linear in w1, . . . , wm. One solution is to relax the
requirement that the approximation ηw(x) be a probability distribution; as we
will see in the next section, in this case the distributional approximation behaves
much like the value function approximation, and a theoretical guarantee can be
obtained.
9.6
An Algorithm Based on Signed Distributions*
So far, we made sure that the outputs of our distributional algorithms were valid
probability distributions (or could be as interpreted as such: for example, when
working with statistical functionals). This was done explicitly when using the
softmax parameterization in deﬁning linear CTD and implicitly in the mixture
update rule of CTD in Chapter 3. In this section, we consider an algorithm that
is similar to linear CTD but omits the softmax function. As a consequence of
this change, this modiﬁed algorithm’s outputs are signed distributions, which
we brieﬂy encountered in Chapter 6 in the course of analyzing categorical
temporal-diﬀerence learning.
Compared to linear CTD, this approach has the advantage of being both
closer to the tabular algorithm (it ﬁnds a best ﬁt in ℓ2 distance, like tabular
CTD) and closer to linear value function approximation (making it amenable
to analysis). Although the learned predictions lack some aspects of probability
distributions – such as well-deﬁned quantiles – the learned signed distributions
can be used to estimate expectations of functions, including expected values.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

278
Chapter 9
To begin, let us deﬁne a (ﬁnite) signed distribution ν as a weighted sum of
two probability distributions:
ν = λ1ν1 + λ2ν2
λ1, λ2 ∈R , ν1, ν2 ∈P(R) .
(9.20)
We write M (R) for the space of ﬁnite signed distributions. For ν ∈M (R)
decomposed as in Equation 9.20, we deﬁne its cumulative distribution function
Fν and the expectation of a function f : R →R as
Fν(z) = λ1Fν1(z) + λ2Fν2(z), z ∈R
E
Z∼ν[ f(Z)] = λ1 E
Z∼ν1[ f(Z)] + λ2 E
Z∼ν2[ f(Z)] .
(9.21)
Exercise 9.14 asks you to verify that these deﬁnitions are independent of the
decomposition of ν into a sum of probability distributions. The total mass of ν
is given by
κ(ν) = λ1 + λ2.
We make these deﬁnitions explicit because signed distributions are not proba-
bility distributions; in particular, we cannot draw samples from ν. In that sense,
the notation Z ∼ν in the deﬁnition of expectation is technically incorrect, but
we use it here for convenience.
Deﬁnition 9.10. The signed m-categorical representation parameterizes the
mass of m particles at ﬁxed locations (θi)m
i=1:
FS,m =
n
m
X
i=1
piδθi : pi ∈R for i = 1, . . . , m,
m
X
i=1
pi = 1
o
.
△
Compared to the usual m-categorical representation, its signed analogue
adds a degree of freedom: it allows the mass of its particles to be negative and
of magnitude greater than 1 (we reserve “probability” for values strictly in
[0, 1]). However, in our deﬁnition, we still require that signed m-categorical
distributions have unit total mass; as we will see, this avoids a number of
technical diﬃculties. Exercise 9.15 asks you to verify that ν ∈FS,m is a signed
distribution in the sense of Equation 9.20.
Recall from Section 5.6 that the categorical projection Πc : P(R) →FC,m is
deﬁned in terms of the triangular and half-triangular kernels hi : R →[0, 1], i =
1, . . . , m. We use Equation 9.21 to extend this projection to signed distributions,
written Πc : M (R) →FS,m. Given ν ∈M (R), the masses of Πcν = Pm
i=1 piδθi are
given by
pi = E
Z∼ν
hi
 ς−1
m (Z −θi) ,
where as before, ς−1
m = θi+1 −θi (i < m), and we write E
Z∼ν in the sense of Equation
9.21. We also extend the notation to signed return-distribution functions in the
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Linear Function Approximation
279
usual way. Observe that if ν is a probability distribution, then Πcν matches our
earlier deﬁnition. The distributional Bellman operator, too, can be extended
to signed distributions. Let η ∈M (R)X be a signed return function. We deﬁne
T π : M (R)X →M (R)X:
(T πη)(x) = Eπ[(bR,γ)#η(X′) | X = x] .
This is the same equation as before, except that now the operator constructs
convex combinations of signed distributions.
Deﬁnition 9.11. Given a state representation φ : X →Rn and evenly spaced
particle locations θ1, …, θm, a signed linear return function approximation
ηw ∈F X
S,m is parameterized by a weight matrix w ∈Rn×m and maps states to
signed return function estimates according to
ηw(x) =
m
X
i=1
pi(x; w)δθi,
pi(x; w) = φ(x)⊤wi + 1
m

1 −
m
X
j=1
φ(x)⊤wj

,
(9.22)
where wi is the ith column of w. We denote the space of signed return functions
that can be represented in this form by Fφ,S,m.
△
Equation 9.22 can be understood as approximating the mass of each particle
linearly and then adding mass to all particles uniformly to normalize the signed
distribution to have unit total mass. It deﬁnes a subset of signed m-categorical
distributions that are constructed from linear combinations of features. Because
of the normalization, and unlike the space of linear value function approxima-
tions constructed from φ, Fφ,S,m is not a linear subspace of M (R). However,
for each x ∈X, the mapping
w 7→ηw(x)
is said to be aﬃne, a property that is suﬃcient to permit theoretical analysis
(see Remark 9.3).
Using a linearly parameterized representation of the form given in Equa-
tion 9.22, we seek to build a distributional dynamic programming algorithm
based on the signed categorical representation. The complication now is that the
distributional Bellman operator takes the return function approximation away
from our linear parameterization in two separate ways: ﬁrst, the distributions
themselves may move away from the categorical representation, and second, the
distributions may no longer be representable by a linear combination of features.
We address these issues with a doubly projected distributional operator:
Πφ,ξ,ℓ2ΠcT π,
where the outer projection ﬁnds the best approximation to ΠcT π in Fφ,S,m. By
analogy with the value-based setting, we deﬁne “best approximation” in terms
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

280
Chapter 9
of a ξ-weighted Cramér distance, denoted ℓξ,2:
ℓ2
2(ν, ν′) =
Z
R
(Fν(z) −Fν′(z))2dz ,
ℓ2
ξ,2(η, η′) =
X
x∈X
ξ(x)ℓ2
2
 η(x), η′(x) ,
Πφ,ξ,ℓ2η = arg min
η′∈Fφ,S,m
ℓ2
ξ,2(η, η′) .
Because we are now dealing with signed distributions, the Cramér distance
ℓ2
2(ν, ν′) is inﬁnite if the two input signed distributions ν, ν′ do not have the same
total mass. This justiﬁes our restriction to signed distributions with unit total
mass in deﬁning both the signed m-categorical representation and the linear
approximation. The following lemma shows that the distributional Bellman
operator preserves this property (see Exercise 9.16).
Lemma 9.12. Suppose that η ∈M (R)X is a signed return-distribution function.
Then,
κ (T πη)(x) =
X
x′∈X
Pπ(X′ = x′ | X = x)κ η(x′).
In particular, if all distributions of η have unit total mass – that is, κ(η(x)) = 1
for all x – then
κ (T πη)(x) = 1 .
△
While it is possible to derive algorithms that are not restricted to outputting
signed distributions with unit total mass, one must then deal with added com-
plexity due to the distributional Bellman operator moving total mass from state
to state.
We next derive a semi-gradient update rule based on the doubly projected
Bellman operator. Following the unbiased estimation method for designing
incremental algorithms (Section 6.2), we construct the signed target
Πc ˜η(x) = Πc(br,γ)#ηw(x′) .
Compared to the sample target of Equation 6.9, here ηw is a signed return
function in F X
S,m.
The semi-gradient update rule adjusts the weight vectors wi to minimize the
ℓ2 distance between Πc ˜η(x) and the predicted distribution ηw(x). It does so by
taking a step in direction of the negative gradient of the squared ℓ2 distance73
wi ←wi −α∇wiℓ2
2(ηw(x), Πc ˜η(x)) .
73. In this expression, ˜η(x) is used as a target estimate and treated as constant with regards to w.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Linear Function Approximation
281
We obtain an implementable version of this update rule by expressing distri-
butions in FS,m in terms of m-dimensional vectors of masses. For a signed
categorical distribution
ν =
m
X
i=1
piδθi ,
let us denote by p ∈Rm the vector (p1, …, pm); similarly, denote by p′ the
vector of masses for a signed distribution ν′. Because the cumulative functions
of signed categorical distributions with unit total mass are constant on the
intervals [θi, θi+1] and equal outside of the [θ1, θm] interval, we can write
ℓ2
2(ν, ν′) =
Z
R
 Fν(z) −Fν′(z)2dz
= ςm
m−1
X
i=1
 Fν(θi) −Fν′(θi)2
= ςm∥Cp −Cp′∥2
2 ,
(9.23)
where ∥· ∥2 is the usual Euclidean norm and C ∈Rm×m is the lower-triangular
matrix
C =

1
0
· · ·
0
0
1
1
· · ·
0
0
...
...
...
1
1
· · ·
1
0
1
1
· · ·
1
1

.
Letting p(x) and ˜p(x) denote the vector of masses for the signed approximation
ηw(x) and the signed target Πc ˜η(x), respectively, we rewrite the above in terms
of matrix-vector operations (Exercise 9.17 asks you to derive the gradient of ℓ2
2
with respect to wi):
wi ←wi + αςm( ˜p(x) −p(x))⊤C⊤C˜eiφ(x) ,
(9.24)
where ˜ei ∈Rm is a vector whose entries are
˜ei j = 1{i = j} −1
m .
By precomputing the vectors C⊤C˜ei, this update rule can be applied in O(m2 +
mn) operations per sample transition.
9.7
Convergence of the Signed Algorithm*
We now turn our attention to establishing a contraction result for the doubly
projected Bellman operator, by analogy with Theorem 9.8. We write
Mℓ2,1(R) = {λ1ν1 + λ2ν2 : λ1, λ2 ∈R, λ1 + λ2 = 1, ν1, ν2 ∈Pℓ2(R)} ,
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

282
Chapter 9
for ﬁnite signed distributions with unit total mass and ﬁnite Cramér distance
from one another. In particular, note that FS,m ⊆Mℓ2,1(R).
Theorem 9.13. Suppose Assumption 4.29(1) holds and that Assump-
tion 9.5 holds with unique stationary distribution ξπ. Then the pro-
jected operator Πφ,ξπ,ℓ2ΠcT π : Mℓ2,1(R)X →Mℓ2,1(R)X is a contraction with
respect to the metric ℓξπ,2, with contraction modulus γ
1/2.
△
This result is established by analyzing separately each of the three operators
that composed together constitute the doubly projected operator Πφ,ξπ,ℓ2ΠcT π.
Lemma 9.14. Under the assumptions of Theorem 9.13, T π : Mℓ2,1(R) →
Mℓ2,1(R) is a γ
1/2-contraction with respect to ℓξπ,2.
△
Lemma 9.15. The categorical projection operator Πc : Mℓ2,1(R) →FS,m is a
nonexpansion with respect to ℓ2.
△
Lemma 9.16. Under the assumptions of Theorem 9.13, the function approxima-
tion projection operator Πφ,ξπ,ℓ2 : F X
S,m →F X
S,m is a nonexpansion with respect
to ℓξπ,2.
△
The proof of Lemma 9.14 essentially combines the reasoning of Proposi-
tion 4.20 and Lemma 9.6 and so is left as Exercise 9.18. Similarly, the proof of
Lemma 9.15 follows according to exactly the same calculations as in the proof
of Lemma 5.23, and so it is left as Exercise 9.19. The central observation is
that the corresponding arguments made for the Cramér distance in Chapter 5
under the assumption of probability distributions do not actually make use of
the monotonicity of the cumulative distribution functions and so extend to the
signed distributions under consideration here.
Proof of Lemma 9.16. Let η, η′ ∈F X
S,m and write p(x), p′(x) for the vector of
masses of η(x) and η′(x), respectively. From Equation 9.23, we have
ℓ2
ξπ,2(η, η′) =
X
x∈X
ξπ(x)ℓ2
2(η(x), η′(x))
=
X
x∈X
ξπ(x)ςm∥Cp(x) −Cp′(x)∥2
2
= ςm∥p−p′∥2
Ξ⊗C⊤C ,
where Ξ ⊗C⊤C is a positive semi-deﬁnite matrix in R(X×m)×(X×m), with Ξ =
diag(ξπ) ∈RX×X, and p, p′ ∈RX×m are the vectorized probabilities associated
with η, η′. Therefore, Πφ,ξπ,ℓ2 can be interpreted as a Euclidean projection under
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Linear Function Approximation
283
the norm ∥· ∥Ξ⊗C⊤C and hence is a nonexpansion with respect to the corre-
sponding norm; the result follows as this norm precisely induces the metric
ℓξπ,2.
Proof of Theorem 9.13. We use similar logic to that of Lemma 5.21 to com-
bine the individual contraction results established above, taking care that the
operators to be composed have several diﬀerent domains between them. Let
η, η′ ∈Mℓ2,1(R)X. Then note that
ℓξπ,2(Πφ,ξπ,ℓ2ΠcT πη, Πφ,ξπ,ℓ2ΠcT πη′)
(a)
≤ℓξπ,2(ΠcT πη, ΠcT πη′)
(b)
≤ℓξπ,2(T πη, T πη′)
(c)
≤γ
1/2ℓξπ,2(η, η′) ,
as required. Here, (a) follows from Lemma 9.16, since ΠcT πη, ΠcT πη′ ∈
F X
S,m. (b) follows from Lemma 9.15 and the straightforward corollary that
ℓξπ,2(Πcη, Πcη′) ≤ℓξπ,2(η, η′) for η, η′ ∈Mℓ2,1(R). Finally, (c) follows from
Lemma 9.14.
The categorical projection is a useful computational device as it allows Πφ,ξ,ℓ2
to be implemented strictly in terms of signed m-categorical representations,
and we rely on it in our analysis. Mathematically, however, it is not strictly
necessary as it is implied by the projection onto the Fφ,S,m; this is demonstrated
by the following Pythagorean lemma.
Lemma 9.17. For any signed m-categorical distribution ν ∈FS,m (for which
ν = Πcν) and any signed distribution ν′ ∈Mℓ2,1(R),
ℓ2
2(ν, ν′) = ℓ2
2(ν, Πcν′) + ℓ2
2(Πcν′, ν′) .
(9.25)
Consequently, for any η ∈Mℓ2,1(R),
Πφ,ξ,ℓ2ΠcT πη = Πφ,ξ,ℓ2T πη .
△
Equation 9.25 is obtained by a similar derivation to the one given in Remark
5.4, which proves the identity when ν and ν′ are the usual m-categorical
distributions.
Just as in the case studied in Chapter 5 without function approximation, it
is now possible to establish a guarantee on the quality of the ﬁxed point of the
operator Πφ,ξπ,ℓ2ΠCT π.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

284
Chapter 9
Proposition 9.18. Suppose that the conditions of Theorem 9.13 hold, and
let ˆηπ
s be the resulting ﬁxed point of the projected operator Πφ,ξπ,ℓ2ΠCT π.
We have the following guarantee on the quality of ˆηπ
s compared with the
(ℓ2,ξπ, Fφ,S,m)-optimal approximation of ηπ: namely, Πφ,ξπ,ℓ2ηπ:
ℓξπ,2(ηπ, ˆηπ
s) ≤ℓξπ,2(ηπ, Πφ,ξπ,ℓ2ηπ)
p
1 −γ
.
△
Proof. We calculate directly:
ℓ2
ξπ,2(ηπ, ˆηπ
s)
(a)= ℓ2
ξπ,2(ηπ, Πφ,ξπ,ℓ2Πcηπ) + ℓ2
ξπ,2(Πφ,ξπ,ℓ2Πcηπ, Πφ,ξπ,ℓ2Πc ˆηπ
s)
(b)= ℓ2
ξπ,2(ηπ, Πφ,ξπ,ℓ2Πcηπ) + ℓ2
ξπ,2(Πφ,ξπ,ℓ2ΠcT πηπ, Πφ,ξπ,ℓ2ΠcT π ˆηπ
s)
(c)
≤ℓ2
ξπ,2(ηπ, Πφ,ξπ,ℓ2Πcηπ) + γℓ2
ξπ,2(ηπ, ˆηπ
s)
=⇒ℓ2
ξπ,2(ηπ, ˆηπ
s) ≤
1
1 −γℓ2
ξπ,2(ηπ, Πφ,ξπ,ℓ2Πcηπ) ,
where (a) follows from the Pythagorean identity in Lemma 9.17 and a similar
identity concerning Πφ,ξπ,ℓ2, (b) follows since ηπ is ﬁxed by T π and ˆηπ
s is ﬁxed
by Πφ,ξπ,ℓ2ΠCT π, and (c) follows from γ
1/2-contractivity of Πφ,ξπ,ℓ2ΠCT π in
ℓ2,ξπ.
This result provides a quantitative guarantee on how much the approximation
error compounds when ˆηπ
s is computed by approximate dynamic programming.
Of note, here there are two sources of error: one due to the use of a ﬁnite
number m of distributional parameters and another due to the use of function
approximation.
Example 9.19. Consider linearly approximating the return-distribution func-
tion of the safe policy in the Cliﬀs domain (Example 2.9). If we use a
three-dimensional state representation φ(x) = [1, xr, rc] where xr, xc are the
row and column indices of a given state, then we observe aliasing in the
approximated return distribution (Figure 9.3). In addition, the use of a signed
distribution representation results in negative mass being assigned to some
locations in the optimal approximation. This is by design; given the limited
capacity of the approximation, the algorithms ﬁnd a solution that mitigates
errors at some locations by introducing negative mass at other locations. As
usual, the use of a bootstrapping procedure introduces diﬀusion error, here quite
signiﬁcant due to the low-dimensional state representation.
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Linear Function Approximation
285
1.0
0.5
0.0
0.5
1.0
Return
0
2
4
6
8
10
Density
1.0
0.5
0.0
0.5
1.0
Return
0.0
0.1
0.2
0.3
0.4
0.5
Probability
1.0
0.5
0.0
0.5
1.0
Return
0.2
0.0
0.2
0.4
Mass
1.0
0.5
0.0
0.5
1.0
Return
0.00
0.05
0.10
0.15
0.20
Mass
(a)
(b)
(c)
(d)
Figure 9.3
Signed linear approximations of the return distribution of the initial state in Example 2.9.
(a–b) Ground-truth return-distribution function and categorical Monte Carlo approxima-
tion, for reference. (c) Best approximation from Fφ,S,m based on the state representation
φ(x) = [1, xr, rc] where xr, xc are the row and column indices of a given state, with (0, 0)
denoting the top-left corner. (d) Fixed point of the signed categorical algorithm.
9.8
Technical Remarks
Remark 9.1. For ﬁnite action spaces, we can easily convert a state represen-
tation φ(x) to a state-action representation φ(x, a) by repeating a basic feature
matrix Φ ∈RX×n. Let NA be the number of actions. We build a block-diagonal
feature matrix ΦX,A ∈R(X×A)×(nNA) that contains NA copies of Φ:
ΦX,A :=

Φ
0
· · ·
0
0
Φ
· · ·
0
...
...
...
...
0
0
· · ·
Φ

.
The weight vector w is also extended to be of dimension nNA, so that
Qw(x, a) =  ΦX,Aw(x, a)
as before. This is equivalent to but somewhat more verbose than the use of
per-action weight vectors.
△
Remark 9.2. Assumption 9.5 enabled us to demonstrate that the projected
Bellman operator Πφ,ξT π has a unique ﬁxed point, by invoking Banach’s ﬁxed
point theorem. The ﬁrst part of the assumption, on the uniqueness of ξπ, is
relatively mild and is only used to simplify the exposition. However, if there
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

286
Chapter 9
is a state x for which ξπ(x) = 0, then ∥· ∥ξπ,2 does not deﬁne a proper metric on
Rn and Banach’s theorem cannot be used. In this case, there might be multiple
ﬁxed points (see Exercise 9.7).
In addition, if we allow ξπ to assign zero probability to some states, the
norm ∥· ∥ξπ,2 may not be a very interesting measure of accuracy. One common
situation where the issue arises is when there is a terminal state x∅that is
reached with probability 1, in which case
lim
t→∞Pπ(Xt = x∅) = 1 .
It is easy to see that in this case, ξπ puts all of its probability mass on x∅, so
that Theorem 9.8 becomes vacuous: the norm ∥· ∥ξπ,2 only measures the error at
the terminal state. A more interesting distribution to consider is the distribution
of states resulting from immediately resetting to an initial state when x∅is
reached. In particular, this corresponds to the distribution used in many practical
applications. Let ξ0 be the initial state distribution; without loss of generality,
let us assume that ξ0(x∅) = 0. Deﬁne the substochastic transition operator
PX,∅(x′ | x, a) =
(
0
if x′ = x∅,
PX(x′ | x, a)
otherwise.
In addition, deﬁne a transition operator that replaces transitions to the terminal
state by transition to one of the initial states, according to the initial distribution
ξ0:
PX,ξ0(x′ | x, a) = PX,∅(x′ | x, a) + 1{x′ , x∅}PX(x∅| x, a)ξ0(x′) .
One can show that the Bellman operator T π
∅(deﬁned from PX,∅) satisﬁes
T π
∅V = T πV
for any V ∈RX for which V(x∅) = 0 and that the steady-state distribution ξ∅
induced by PX,ξ0 and a policy π is such that
there exists t ∈N, Pπ(Xt = x) > 0 =⇒ξ∅(x) > 0 .
Let Pπ
∅be the transition matrix corresponding to PX,∅(x′ | x, a) and the policy
π. Exercise 9.21 asks you to prove that
∥Pπ
∅∥ξ∅,2 ≤1 ,
from which a modiﬁed version of Theorem 9.8 can be obtained.
△
Remark 9.3. Let M and M′ be vector spaces. A mapping O : M →M′ is said
to be aﬃne if, for any U, U′ ∈M and α ∈[0, 1],
O(αU + (1 −α)U′) = αOU + (1 −α)OU′ .
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Linear Function Approximation
287
When ηw is a signed linear return function approximation parameterized by w,
the map
w 7→ηw(x)
is aﬃne for each x ∈X, as we now show. It is this property that allows us to
express the ℓ2 distance between ν, ν′ ∈FS,m in terms of a diﬀerence of vectors
of probabilities (Equation 9.23), needed in the proof of Lemma 9.16.
Let w, w′ ∈Rn×m be the parameters of signed return functions of the form of
Equation 9.22. For these, write pw(x) for the vector of masses determined by w:
pw(x) = w⊤φ(x) + 1
me 1 −e⊤w⊤φ(x) ,
where e is the m-dimensional vector of ones. We then have
m
X
i=1
φ(x)⊤wi = e⊤w⊤φ(x) ,
and similarly for w′. Hence,
pαw+(1−α)w′(x) = (αw + (1 −α)w′)⊤φ(x) −1
mee⊤(αw + (1 −α)w′)⊤φ(x)
= αpw(x) + (1 −α)pw′(x) .
We conclude that w 7→ηw(x) is indeed aﬃne in w.
△
9.9
Bibliographical Remarks
9.1–9.2. Linear value function approximation as described in this book is in
eﬀect a special case of linear regression where the inputs are taken from a
ﬁnite set (φ(x))x∈X and noiseless labels; see Strang (1993), Bishop (2006),
and Murphy (2012) for a discussion on linear regression. Early in the history
of reinforcement learning research, function approximation was provided by
connectionist systems (Barto et al. 1983; Barto et al. 1995) and used to deal
with inﬁnite state spaces (Boyan and Moore 1995; Sutton 1996). An earlier-still
form of linear function approximation and temporal-diﬀerence learning was
used by Samuel (1959) to train a strong player for the game of checkers.
9.3. Bertsekas and Tsitsiklis (1996, Chapter 6) studies linear value function
approximation and its combination with various reinforcement learning meth-
ods, including TD learning (see also Bertsekas 2011, 2012). Tsitsiklis and Van
Roy (1997) establish the contractive nature of the projected Bellman operator
Πφ,ξT π under the steady-state distribution (Theorem 9.8 in this book). The
temporal-diﬀerence ﬁxed point can be determined directly by solving a least-
squares problem, yielding the least-squares TD algorithm (LSTD; Bradtke and
Barto 1996), which is extended to the control setting by least-squares policy
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

288
Chapter 9
iteration (LSPI; Lagoudakis and Parr 2003). In the control setting, the method is
also called ﬁtted Q-iteration (Gordon 1995; Ernst et al. 2005; Riedmiller 2005).
Bertsekas (1995) gives an early demonstration that the TD ﬁxed point is in
general diﬀerent from (and has greater error than) the best approximation to
Vπ, measured in ξ-weighted L2 norm. However, it is possible to interpret the
temporal-diﬀerence ﬁxed point as the solution to an oblique projection problem
that minimizes errors between consecutive states (Harmon and Baird 1996;
Scherrer 2010).
9.4. Barnard (1993) provides a proof that temporal-diﬀerence learning is not
a true gradient-descent method, justifying the term semi-gradient (Sutton and
Barto 2018). The situation in which ξ diﬀers from the steady-state (or sampling)
distribution of the induced Markov chain is called oﬀ-policy learning; Example
9.9 is a simpliﬁed version of the original counterexample due to Baird (1995).
Baird argued for the direct minimization of the Bellman residual by gradient
descent as a replacement to temporal-diﬀerence learning, but this method suﬀers
from other issues and can produce undesirable solutions even in mild scenarios
(Sutton et al. 2008a). The GTD line of work (Sutton et al. 2009; Maei 2011)
is a direct attempt at handling the issue by using a pair of approximations (see
Qu et al. 2019 for applications of this idea in a distributional context); more
recent work directly considers a corrected version of the Bellman residual (Dai
et al. 2018; Chen and Jiang 2019). The convergence of temporal-diﬀerence
learning with linear function approximation was proven under fairly general
conditions by Tsitsiklis and Van Roy (1997), using the ODE method from
stochastic approximation theory (Benveniste et al. 2012; Kushner and Yin 2003;
Ljung 1977).
Parr et al. (2007), Parr et al. (2008) and Sutton et al. (2008b) provide a domain-
dependent analysis of linear value function approximation, which is extended
by Ghosh and Bellemare (2020) to establish the existence of representations
that are stable under a greater array of conditions (Exercises 9.10 and 9.11).
Kolter (2011) studies the space of temporal-diﬀerence ﬁxed point as a function
of the distribution ξ.
9.5. Linear CTD and QTD were implicitly introduced by Bellemare et
al. (2017a) and Dabney et al. (2018b), respectively, in the design of deep
reinforcement learning agents (see Chapter 10). Their presentation as given
here is new.
9.6–9.7. The algorithm based on signed distributions is new to this book. It
improves on an algorithm proposed by Bellemare et al. (2019b) in that it adjusts
the total mass of return distributions to always be 1. Lyle et al. (2019) give
evidence that the original algorithm generally underperforms the categorical
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Linear Function Approximation
289
algorithm. They further establish that, in the risk-neutral control setting, distri-
butional algorithms cannot do better than value-based algorithms when using
linear approximation. The reader interested in the theory of signed measures is
referred to Doob (1994).
9.10
Exercises
Exercise 9.1. Use the update rules of Equation 9.10 and 9.11 to prove the
results from Example 9.1.
△
Exercise 9.2. Prove that for a linear value function approximation Vw = Φw,
we have, for any w, w′ ∈Rn and α, β ∈R,
Vαw+βw′ = αVw + βVw′, ∇wVw(x) = φ(x).
△
Exercise 9.3. In the statement of Proposition 9.3, we required that
(i) the columns of the feature matrix Φ be linearly independent;
(ii) ξ(x) > 0 for all x ∈X.
Explain how the result is aﬀected when either of these requirements is omitted.
△
Exercise 9.4. Prove Lemma 9.7. Hint. Apply Pythagoras’s theorem to a well-
chosen inner product.
△
Exercise 9.5. The purpose of this exercise is to empirically study the con-
tractive and expansive properties of the projection in L2 norm. Consider an
integer-valued state space x ∈X = {1, . . . , 10} with two-dimensional state rep-
resentation φ(x) = (1, x), and write Πφ for the L2 projection of a vector v ∈RX
onto the linear subspace deﬁned by φ. That is,
Πφv = Φ(Φ⊤Φ)−1Φ⊤v,
where Φ is the feature matrix.
With this representation, we can represent the vector u(x) = 0 exactly, and
hence Πφu = u. Now consider the vector u′ deﬁned by
u′(x) = log x.
With a numerical experiment, show that
∥Πφu′ −Πφu∥2 ≤∥u′ −u∥2
but
∥Πφu′ −Πφu∥∞> ∥u′ −u∥∞.
△
Exercise 9.6. Provide an example Markov decision process and state represen-
tation that result in the left- and right-hand sides of Equation 9.6 being equal.
Hint. A diagram might prove useful.
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

290
Chapter 9
Exercise 9.7. Following Remark 9.2, suppose that the steady-state distribution
ξπ is such that ξπ(x) = 0 for some state x. Discuss the implications on the analysis
performed in this chapter, in particular on the set of solutions to Equation 9.8
and the behavior of semi-gradient temporal-diﬀerence learning when source
states are drawn from this distribution.
△
Exercise 9.8. Let ξ0 be some initial distribution and π a policy. Deﬁne the
discounted state-visitation distribution
¯ξ(x′) = (1 −γ)ξ0(x′) + γ
X
x∈X
Pπ(X′ = x′ | X = x)¯ξ(x), for all x′ ∈X.
Following the line of reasoning leading to Lemma 9.6, show that the projected
Bellman operator Πφ,ξT π is a γ
1/2 contraction in the ¯ξ-weighted L2 metric:
∥T π∥¯ξ,2 ≤γ
1/2 .
△
Exercise 9.9. Let ξ ∈P(X). Suppose that the feature matrix Φ ∈RX×n has
linearly independent columns and ξ(x) > 0 for all x ∈X. Show that the unique
optimal weight vector w∗that is a solution to Equation 9.2 satisﬁes
E
h Gπ(X) −φ(X)⊤w∗φ(X)
i
= 0,
X ∼ξ.
△
Exercise 9.10 (*). This exercise studies the divergence of semi-gradient
temporal-diﬀerence learning from a dynamical systems perspective. Recall
that Ξ ∈RX×X is the diagonal matrix whose entries are ξ(x).
(i) Show that in expectation and in matrix form, Equation 9.11 produces the
sequence of weight vectors (wk)k≥0 given by
wk+1 = wk + αkΦ⊤Ξ rπ + γPπΦwk −Φwk).
(ii) Assume that αk = α > 0. Express the above as an update of the form
wk+1 = Awk + b,
(9.26)
where A ∈Rn×n and b ∈Rn.
(iii) Suppose that the matrix C = Φ⊤Ξ(γPπ −I)Φ has eigenvalues that are all
real. Show that if one of these is positive, then A has at least one eigenvalue
greater than 1.
(iv) Use the preceding result to conclude that under those conditions, there
exists a w0 such that ∥wk∥2 →∞.
(v) Suppose now that all of the matrix C’s eigenvalues are real and nonpositive.
Show that in this case, there exists an α ∈(0, 1) such that taking αk = α, the
sequence (Φwk)k≥0 converges to the temporal-diﬀerence ﬁxed point.
△
Exercise 9.11 (*). Suppose that the state representation is such that
(i) the matrix PπΦ ∈RX×n has columns that lie in the column span of Φ, and
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Linear Function Approximation
291
(ii) the matrix has columns that are orthogonal with regards to the ξ-weighted
inner product. That is,
Φ⊤ΞΦ = I.
Show that in this case, the eigenvalues of the matrix
Φ⊤Ξ(γPπ −I)Φ
are all nonpositive, for any choice of ξ ∈P(X). Based on the previous exercise,
conclude on the dynamical properties of semi-gradient temporal-diﬀerence
learning with this representation.
△
Exercise 9.12. Using your favorite numerical computation software, implement
the Markov decision process from Baird’s counterexample (Example 9.9) and
the semi-gradient update
wk+1 = wk + α γφ(x′
k)wk −φ(xk)wk
φ(xk) ,
for the state representation of the example and a small, constant value of α.
Here, it is assumed that Xk has distribution ξ and X′
k is drawn from the transition
kernel depicted in Figure 9.2.
(i) Vary ξ(x) from 0 to 1, and plot the norm of the value function estimate,
∥Vk∥ξ,2, as a function of k.
(ii) Now replace φ(X′
k)wk by φ(X′
k) ˜wk, where ˜wk = wk−k mod L for some integer
L ≥1. That is, ˜wk is kept ﬁxed for L iterations. Plot the evolution of ∥Vk∥ξ,2
for diﬀerent values of L. What do you observe?
△
Exercise 9.13. Repeat Exercise 3.3, replacing the uniform grid encoding the
state with a representation φ deﬁned by
φ(x) =  1, sin(x1), cos(x1), …, sin(x4), cos(x4) ,
where (x1, x2) denote the Cart–Pole state variables. Implement linear CTD and
QTD and use these with the state representation φ to learn a return-distribution
function approximation for the uniform and forward-leaning policies. Visualize
the learned approximations at selected states, including the initial state, and com-
pare them to ground-truth return distributions estimated by the nonparametric
Monte Carlo algorithm (Remark 3.1).
△
Exercise 9.14. Recall that given a ﬁnite signed distribution ν expressed as a
sum of probability distributions ν = λ1ν1 + λ2ν2 (with λ1, λ2 ∈R, ν1, ν2 ∈P(R)),
we deﬁned expectations under ν in terms of sums of expectations under ν1 and
ν2. Verify that this deﬁnition is not dependent on the choice of decomposi-
tion of ν into a sum of probability distributions. That is, suppose that there
also exist λ′
1, λ′
2 ∈R and ν′
1, ν′
2 ∈P(R) with ν = λ′
1ν′
1 + λ′
2ν′
2. Show that for any
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

292
Chapter 9
(measurable) function f : R →R with
E
Z∼ν1[| f(Z)|] < ∞, E
Z∼ν2[| f(Z)|] < ∞, E
Z∼ν′
1
[| f(Z)|] < ∞, E
Z∼ν′
2
[| f(Z)|] < ∞,
we have
λ1 E
Z∼ν1[ f(Z)] + λ2 E
X∼ν2[ f(Z)] = λ1 E
Z∼ν′
1
[ f(Z)] + λ2 E
Z∼ν′
2
[ f(Z)] .
△
Exercise 9.15. Show that any m-categorical signed distribution ν ∈FS,m can
be written as the weighted sum of two m-categorical (probability) distributions
ν1, ν2 ∈FC,m.
△
Exercise 9.16. Suppose that we consider a return function η ∈M (R)X deﬁned
over signed distributions (not necessarily with unit total mass). Show that
κ (T πη)(x) =
X
x′∈X
Pπ(X′ = x | X = x)κ(η(x′)) .
Conclude that if η ∈F X
S,m, then
κ (T πη)(x) = 1 .
△
Exercise 9.17. Consider two signed m-categorical distributions ν, ν′ ∈FS,m.
Denote their respective vectors of masses by p, p′ ∈Rm. Prove the correctness
of Equation 9.24. That is, show that
∇wiℓ2
2(ν, ν′) = −2ςm(p′ −p)⊤C⊤C˜eiφ(x) .
△
Exercise 9.18. Prove Lemma 9.14.
△
Exercise 9.19. Prove Lemma 9.15.
△
Exercise 9.20. Prove Lemma 9.17.
△
Exercise 9.21. Following the discussion in Remark 9.2, show that
∥Pπ
∅∥ξ∅,2 ≤1 .
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

10
Deep Reinforcement Learning
As a computational description of how an organism learns to predict, temporal-
diﬀerence learning reduces the act of learning, prediction, and control to
numerical operations and mostly ignores how the inputs and outputs to these
operations come to be. By contrast, real learning systems are but a component
within a larger organism or machine, which in some sense provides the inter-
face between the world and the learning component. To understand and design
learning agents, we must also situate their computational operations within a
larger system.
In recent years, deep reinforcement learning has emerged as a framework
that more directly studies how characteristics of the environment and the archi-
tecture of an artiﬁcial agent aﬀect learning and behavior. The name itself comes
from the combination of reinforcement learning techniques with deep neural
networks, which are used to make sense of low-level perceptual inputs, such
as images, and also structure complex outputs (such as the many degrees of
freedom of a robotic arm). Many of reinforcement learning’s recent applied
successes can be attributed to this combination, including superhuman perfor-
mance in the game of Go, robots that learn to grasp a wide variety of objects,
champion-level backgammon play, helicopter control, and autonomous balloon
navigation.
Much of the recent research in distributional reinforcement learning, includ-
ing our own, is rooted in deep reinforcement learning. This is no coincidence,
since return distributions are naturally complex objects, whose learning is facil-
itated by the use of deep neural networks. Conversely, predicting the return also
translates into practical beneﬁts, often in the guise of accelerated and more sta-
ble learning. In this context, the distributional method helps the system organize
its inputs into features that are useful for further learning, a process known as
representation learning. This chapter surveys some of these ideas and discusses
practical considerations in introducing distributional reinforcement learning to
a larger-scale system.
293
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

294
Chapter 10
Replay Buffer
Periodically
Updated
Target Network
Online Network
Pre-processing
Convolutional Layers
Fully-connected Layers
(a)
(b)
Figure 10.1
(a) The deep neural network at the heart of the DQN agent architecture. The network
takes as inputs four preprocessed images and outputs the predicted state-action value
estimates. (b) The replay buﬀer provides the sample transitions used to update the main
(online) network (Equation 10.2). A second target network is used to construct sample
targets and is periodically updated with the weights of the online network. Example input
to the network is visualized with frames from the Atari game Ms. Pac-Man (published
by Midway Manufacturing).
10.1
Learning with a Deep Neural Network
The term deep neural network (or simply deep network) designates a fairly
broad class of functions built from a composition of atomic elements (called
neurons or hidden units), typically organized in layers and parameterized by
weight vectors and one or many activation functions. The function’s inputs
are transformed by the ﬁrst layer, whose outputs become the inputs of the
second layer, and so on. Canonically, the weights of the network are adjusted to
minimize a given loss function by gradient descent. In that respect, one may
view linear function approximation as a simple neural network architecture that
maps inputs to outputs by means of a single, nonadjustable layer implementing
the chosen state representation.
More sophisticated architectures include: convolutional neural networks
(LeCun and Bengio 1995), particularly suited to learning functions that exhibit
some degree of translation invariance; recurrent networks (Hochreiter and
Schmidhuber 1997), designed to deal with sequences; and attention mechanisms
(Bahdanau et al. 2015; Vaswani et al. 2017). The reader interested in further
details is invited to consult the work of Goodfellow et al. (2016).
By virtue of their parametric ﬂexibility, deep neural networks have proven
particularly eﬀective at dealing with reinforcement learning problems with
high-dimensional structured inputs. As an example, the DQN algorithm (Deep
Q-Networks; Mnih et al. 2015) applies the tools of deep reinforcement learning
to achieve expert-level game play in Atari 2600 video games. More properly
speaking, DQN is a system or agent architecture that combines a deep neural
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Deep Reinforcement Learning
295
network with a semi-gradient Q-learning update rule and a few algorithmic
components that improve learning stability and eﬃciency. DQN also contains
subroutines to transform received observations into a useful notion of state and
to select actions to be enacted in the environment. We divide the computation
performed by DQN into four main subtasks:
(a) preprocessing observations (Atari 2600 images) into an input vector,
(b) predicting future outcomes given such an input vector,
(c) behaving on the basis of these predictions, and ﬁnally
(d) learning to improve the agent’s predictions from its experience.
Preprocessing. The Arcade Learning Environment or ALE (Naddaf 2010;
Bellemare et al. 2013a) provides a reinforcement learning interface to the Stella
Atari 2600 emulator (Mott et al. 1995–2023). This interface produces 210 × 160,
7-bit pixel images in response to one of eighteen possible joystick motions
(combinations of an up-down motion, a left-right motion, and a button press). A
single emulation step or frame lasts 1/60th of a second, but the agent only selects
a new action every four frames, what might be more appropriately called a
time step (that is, ﬁfteen times per emulated second; for further implementation
details, see Machado et al. (2018)).
In order to simplify the learning process and make it more suitable for rein-
forcement learning, DQN transforms or preprocesses the images and rewards it
receives from the ALE. Every time step, the DQN agent converts the most recent
frame to 7-bit grayscale (preserving luminance). It combines this frame with
the immediately preceding one, similarly converted, by means of a max-pooling
operation that takes the pixel-wise maximum of the two images. The result
is then downscaled to a smaller 84 × 84 size, and pixel values are normalized
from 0 (black) to 1 (white). The four most recent images (spanning a total of 16
frames, a little over 1/4th of a second) are concatenated together to form the
input vector, of size 84 × 84 × 4.74 In essence, DQN uses this input vector as a
surrogate for the state x.
The Arcade Learning Environment also provides an integer reward indicating
the change in score (positive or negative) between consecutive time steps. DQN
applies an operation called reward clipping, which keeps only the sign of the
provided reward (−1, 0, or 1). The result is what we denote by r.
Prediction. DQN uses a ﬁve layer neural network to predict the expected
return from the agent’s current state (see Figure 10.1a). The network’s input is
the stack of frames produced by the preprocessor. This input is successively
74. In many games, these images do not provide a complete picture of the game state. In this case,
the problem is said to be partially observable. See, for example, Kaelbling et al. (1998).
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

296
Chapter 10
transformed by three convolutional layers that extract features from the image,
and then by a fully connected layer that applies a linear transformation and
a nonlinearity to the output of the ﬁnal convolutional layer. The result is a
512-dimensional vector, which is then linearly transformed into NA = 18 values
corresponding to the predicted expected returns for each action. If we denote by
w the entire collection of weights of the neural network, then the approximation
Qw(x, a) denotes the resulting prediction made by DQN. Here, x is described by
the input image stack and a indexes into the network’s NA-dimensional output
vector.
Behavior and experience. DQN acts according to an ε-greedy policy derived
from its approximation Qw. More precisely, this policy assigns 1 −ε of its
probability mass to a greedy selection rule that breaks ties uniformly at random.
Provided that Qw assigns a higher value to better actions, acting greedily results
in good performance. The remaining ε probability mass is divided equally
among all actions, allowing for some exploration of unseen situations and
eventually improving the network’s value estimates from experience.
Similar to the online setting described by Algorithms 3.1 and 3.2, the sample
produced by the agent’s interactions with the Arcade Learning Environment is
used to improve the network’s value predictions (see Learning below). Here,
however, the transitions are not provided to the learning algorithm as they
are experienced but are instead stored in a circular replay buﬀer. The replay
buﬀer stores the last 1 million transitions experienced by the agent; as the
name implies, these are typically replayed multiple times as part of the learning
process. Compared to learning from each piece of experience once and then
discarding it, using a replay buﬀer has been empirically demonstrated to result
in greater sample eﬃciency and stability.
Learning. Every four time steps, DQN samples a minibatch of thirty-two
sample transitions from its replay buﬀer, uniformly at random. As with linear
approximation, these sample transitions are used to adjust the network weights
w with a semi-gradient update rule. Given a second target network with weights
˜w and a sample transition (x, a, r, x′), the corresponding sample target is
r + γ max
a′∈A Q ˜w(x′, a′) .
(10.1)
Canonically, the discount factor is chosen to be γ = 0.99.
In deep reinforcement learning, the update to the network’s parameters is
typically expressed ﬁrst and foremost in terms of a loss function L to be
minimized. This is because automatic diﬀerentiation can be used to eﬃciently
compute the gradient ∇wL(w) by means of the backpropagation algorithm
(Werbos 1982; Rumelhart et al. 1986). In the case of DQN, the sample target is
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Deep Reinforcement Learning
297
used to construct the squared loss
L(w) =  r + γ max
a′∈A Q ˜w(x′, a′) −Qw(x, a)2 .
Taking the gradient of L with respect to w, we obtain the semi-gradient update
rule:
w ←w + α r + γ max
a′∈A Q ˜w(x′, a′) −Qw(x, a)∇wQw(x, a) .
(10.2)
In practice, the actual loss to be minimized is the average loss over the sampled
minibatch. Consequently, the semi-gradient update rule adjusts the weight vector
on the basis of all sampled transitions simultaneously. After a speciﬁed number
of updates have been performed (10,000 in the original implementation), the
weights w of the main network (also called online network) are copied over to
the target network; the process is illustrated in Figure 10.1b. As discussed in
Section 9.4, the use of a target network allows the semi-gradient update rule
to behave more like the projected Bellman operator and mitigates some of the
convergence issues of semi-gradient methods.
Equation 10.2 generalizes the semi-gradient rule for linear value function
approximation (Equation 9.11) to nonlinear schemes; observe that if Qw(x, a) =
φ(x, a)⊤w, then
∇wQw(x, a) = φ(x, a) .
In practice, more sophisticated adaptive gradient descent methods (see,
e.g., Tieleman and Hinton 2012; Kingma and Ba 2015) are used to accel-
erate convergence. Additionally, the Huber loss is a popular alternative to the
squared loss. For κ ≥0, the Huber error function is deﬁned as
Hκ(u) =

1
2u2,
if |u| ≤κ
κ(|u| −1
2κ),
otherwise .
DQN’s Huber loss instantiates this error function with κ = 1:
L(w) = H1

r + γ max
a′∈A Q ˜w(x′, a′) −Qw(x, a)

.
The Huber loss corresponds to the squared loss for small errors but behaves like
the L1 loss for larger errors. In eﬀect, this puts a limit on the magnitude of the
updates to the weight vector, which in turn reduces instability in the learning
process.
Viewed as an agent interacting with its environment, DQN operates by acting
in the world according to its state-action value function estimates, collecting new
experience along the way. It then uses this experience to produce an improved
policy that is used to collect better experience and so on. Given suﬃcient
training time, this approach can learn fairly complex behavior from pixels and
rewards alone, including behavior that exploits computer bugs in some of Atari
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

298
Chapter 10
Pre-processing
Convolutional Layers
Fully-connected Layers
Distribution
Parameters
Figure 10.2
The extension of DQN to the distributional setting uses a deep neural network that outputs
m distribution parameters per action. The same network architecture is used for both
C51 (categorical representation) and QR-DQN (quantile representation). Example input
to the network is visualized with frames from the Atari game Ms. Pac-Man (published
by Midway Manufacturing).
2600 games. Although it is possible to achieve the same behavior by using a
ﬁxed state representation and learning a linear value approximation (Machado
et al. 2018), in practice, doing so without a priori knowledge of the speciﬁc
game being played is computationally demanding and eventually does not scale
as eﬀectively.
10.2
Distributional Reinforcement Learning with Deep Neural
Networks
We extend DQN to the distributional setting by mapping state-action pairs to
return distributions rather than to scalar values. A simple approach is to change
the network to output one m-dimensional parameter vector per action (equiva-
lently, one NA × m-dimensional parameter matrix, as illustrated in Figure 10.2).
These vectors are then interpreted as the parameters of return distributions,
by means of a probability distribution representation (Section 5.2). The target
network is modiﬁed in the same way.
By making diﬀerent choices of distribution representation or rule for updat-
ing the parameters of these distributions, we obtain diﬀerent distributional
algorithms. The C51 algorithm, in particular, is based on the categorical repre-
sentation, while the QR-DQN algorithm is based on the quantile representation.
Their respective update rules follow those of linear CTD and linear QTD in
Section 9.5, and here we emphasize the diﬀerences from the linear approach.
We begin with a description of the C51 algorithm.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Deep Reinforcement Learning
299
Prediction. C51 owes its name to its use of a 51-categorical representation
in its original implementation (Bellemare et al. 2017a), but the name generally
applies for any such agent with m ≥2. Given an input state x, its network outputs
a collection of softmax parameters ((ϕi(x, a))m
i=1 : a ∈A). These parameters are
used to deﬁne the state-action categorical distributions:
ηw(x, a) =
m
X
i=1
pi(x, a; w)δθi ,
where pi(x, a; w) is a probability given by the softmax distribution:
pi(x, a; w) =
eϕi(x,a;w)
mP
j=1
eϕj(x,a;w)
.
The standard implementation of C51 parameterizes the locations θ1, . . . , θm in
two speciﬁc ways: ﬁrst, θ1 and −θm are chosen to be negative of each other,
so that the support of the distribution is symmetric around 0. Second, it is
customary to take m to be an odd number so that the central location θ m−1
2 is zero.
The canonical choice is θ1 = −10 and θm = 10; we will study the eﬀect of this
choice in Section 10.4. Even though the largest theoretically achievable return
is Vmax = Rmax
1−γ = 100, the choice of θm = 10 is sensible because in most Atari
2600 video games, the player’s score only changes infrequently. Consequently,
the reward is zero on most time steps and the largest return actually observed
is typically much smaller than the analytical maximum (the same argument
applies to Vmin and θ1).
Behavior. C51 is designed to optimize for the risk-neutral objective. The
value function induced by its distributional predictions is
Qw(x, a) =
E
Z∼ηw(x,a)[Z] =
m
X
i=1
pi(x, a; w)θi ,
and similarly for Q ˜w. The agent acts according to an ε-greedy policy deﬁned
from the main network’s state-action value estimates Qw(x, a).
Learning. The sample target is obtained from the combination of an update
and projection step. Following Section 9.5, given a transition (x, a, r, x′), the
sample target is given by
¯η(x, a) =
m
X
j=1
¯pjδθ j = Πc

(br,γ)#η ˜w(x′, a ˜w(x′))

,
where
a ˜w(x′) = arg max
a′∈A
Q ˜w(x′, a′)
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

300
Chapter 10
is the greedy action for the induced state-action value function Q ˜w. As in the
linear setting, this sample target is used to formulate a cross-entropy loss to be
minimized (Equation 9.18):
L(w) = −
m
X
j=1
¯pj log p j(x, a; w) .
Updating the parameters w in the direction of the negative gradient of this loss
yields the (ﬁrst-order) semi-gradient update rule
w ←w + α
m
X
i=1
  ¯pi −pi(x, a; w)∇wθi(x, a; w) .
It is useful to contrast the above with the linear CTD semi-gradient update rule
(Equation 9.19):
wi ←wi + α  ¯pi −pi(x, a; w)φ(x, a) .
With linear CTD, the update rule takes a simple form where one computes the
per-particle categorical TD error   ˜pi −pi(x, a; w) and moves the weight vector
wi in the direction φ(x, a) in proportion to this error. This is possible because
the softmax parameters ϕi(x, a; w) = φ(x, a)⊤wi are linear in φ and consequently
∇wiϕi(x, a; w) = φ(x, a) ,
similar to the value-based setting. When using a deep network, however, the
weights at earlier layers aﬀect the entire predicted distribution and it is not
possible to perform per-particle weight updates independently.
QR-DQN. QR-DQN uses the same neural network as C51 but interprets
the output vectors as parameters of a m-quantile representation, rather than a
categorical representation:
ηw(x, a) = 1
m
m
X
i=1
δθi(x,a;w).
Its induced value function is simply
Qw(x, a) = 1
m
m
X
i=1
θi(x, a; w) .
Given a sample transition (x, a, r, x′) and levels τ1, . . . , τm, the parameters are
updated by performing gradient descent on the Huber quantile loss
L(w) = 1
m
m
X
i,j=1
ρH
τi
 r + γθ j(x′, a ˜w(x′); ˜w) −θi(x, a; w) ,
where ρH
τ (u) = |1{u < 0} −τ|H1(u) .
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Deep Reinforcement Learning
301
The Huber quantile loss behaves like the quantile loss (Equation 6.12) for
large errors but like the expectile loss (the single-sample equivalent of Equa-
tion 8.13) for suﬃciently small errors. This loss is minimized by a statistical
functional between a quantile and expectile. The Huber quantile loss can be
interpreted as a smoothed version of the quantile regression loss (Sections 6.4
and 9.5), which leads to more stable behavior when combined with the nonlinear
function approximation and adaptive gradient descent methods used in deep
reinforcement learning.
10.3
Implicit Parameterizations
The value function Qπ can be viewed as a mapping from state-action pairs to
real values. When there are ﬁnitely many actions and the state space is small
and ﬁnite, a tabular representation of this mapping is usually suﬃcient – in
this case, each entry in the table corresponds to the value at a given state and
action. Much like linear function approximation, neural networks improve on
the tabular representation by parameterizing the relationship between similar
states, allowing us to generalize the learned function to unseen states. It is
therefore useful to think of the inputs of DQN’s neural network as arguments
to a function and its outputs as the evaluation of this function. Under this
perspective, the network implements a function mapping X to RA, which can
be interpreted as the function Qw : X × A →R by a further indexing into the
output vector.
Similarly, we can represent probability distributions as diﬀerent kinds of
functions. For example, the probability density function fν of a suitable distri-
bution ν ∈P(R) is a mapping from R to [0, ∞) and its cumulative distribution
is a monotonically increasing function from R to [0, 1]. A distribution can also
be represented by its inverse cumulative distribution function F−1
ν : (0, 1) →R,
which we call quantile function in the context of this section. By extension,
a state-action return function can be viewed as a function of three arguments:
x ∈X, a ∈A, and a distribution parameter τ ∈(0, 1). That is, we may represent
η by the mapping
(x, a, τ) 7→F−1
η(x,a)(τ) .
Doing so gives rise to an implicit approach to distributional reinforcement
learning, which makes the distribution parameter an additional input to the
network.
To understand this idea, it is useful to contrast it with the two algorithms of
the previous section. Both C51 and QR-DQN take a half-and-half approach: the
state x is provided as input to the network, but the parameters of the distribution
(the probabilities pi or locations θi, accordingly) are given by the network’s
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

302
Chapter 10
Pre-processing
Convolutional Layers
Fully-connected Layers
Fully-connected Layer
Figure 10.3
The deep neural network used by IQN. IQN outputs a single scalar per action, corre-
sponding to the queried τth quantile. The input τ to the network is encoded using a
cosine embedding and multiplicatively combined with one of the network’s inner layers
(see Remark 10.1). Example input to the network is visualized with frames from the
Atari game Ms. Pac-Man (published by Midway Manufacturing).
outputs, one m-dimensional vector per action. This is conceptually simple and
has the advantage of leveraging probability distribution representations whose
behavior is theoretically well understood in the tabular setting. On the other
hand, the implied discretization of the return distributions incurs some cost,
for example, due to diﬀusion (Section 5.8). If instead we turn the distribution
parameter into an input to the network, we gain the beneﬁts of generalization,
and in principle we can learn an approximation to the return function that is
only limited by the capacity of the neural network.
Prediction. The implicit quantile network (IQN) algorithm instantiates the
argument-as-inputs principle by parameterizing the quantile function within
the neural network (Figure 10.3). As before, one of the network’s inputs is a
description of the state x: in the case of a DQN-type architecture, the stack
of preprocessed Atari 2600 images. In addition, the network also receives as
input a desired level τ ∈(0, 1). This level is encoded by the cosine embedding
ϕ(τ) ∈RM:
ϕ(τ) = (cos(πτi))M−1
i=0 .
In eﬀect, the cosine embedding represents τ as a real-valued vector, making
it easier for the neural network to work with. The network’s output is an NA-
dimensional vector describing the evaluation of the quantile function F−1
η(x,a)(·)
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Deep Reinforcement Learning
303
at this τ. With this construction, the neural network outputs the approximation
θw(x, a, τ) = φw(x, τ)⊤wa ≈F−1
η(x,a)(τ) ,
where φw(x, τ) ∈Rn are learned features for the (x, τ) pair and wa ∈Rn are per-
action weights. Remark 10.1 provides details on how φw(x, τ) is implemented
in the network.
Behavior. Because IQN represents the quantile function implicitly, the
induced state-action value functions Qw and Q ˜w are approximated by a sam-
pling procedure, where as before ˜w denotes the weights of the target network.
This approximation is obtained by drawing m levels τ1, . . . , τm uniformly and
independently from the (0, 1) interval and averaging the output of the network
at these levels:
Qw(x, a) ≈1
m
m
X
i=1
θw(x, a, τi) .
(10.3)
As with the other algorithms presented in this chapter, IQN acts according to an
ε-greedy policy derived from Qw, with greedy action aw(x).
Learning. Where QR-DQN aims to learn the quantiles of the return distri-
bution at a ﬁnite number of ﬁxed levels, IQN aims to approximate the entire
quantile function of this distribution.75 Because each query to the network
returns the network’s prediction evaluated for a single level, learning proceeds
somewhat diﬀerently. First, a pair of levels τ and τ′ is sampled uniformly from
(0, 1). These determine the level at which the quantile function is to be updated
and a level from which a sample target is constructed. For a given sample
transition (x, a, r, x′) and levels τ, τ′, the two-sample IQN loss is
Lτ,τ′(w) = ρH
τ
 r + γθ ˜w(x′, a ˜w(x′), τ′) −θw(x, a, τ) .
The variance of the sample gradient of this loss is reduced by averaging the
two-sample loss over many pairs of levels τ1, . . . , τm1 and τ′
1, . . . , τ′
m2:
L(w) = 1
m2
m1
X
i=1
m2
X
j=1
Lτi,τ′
j(w) .
Risk-sensitive control. An appealing side eﬀect of using an implicit param-
eterization is that many risk-sensitive objectives can be computed simply by
changing the sampling distribution for Equation 10.3. For example, given a pre-
dicted quantile function θw(x, a, ·) with instantiated random variable Gw(x, a),
recall that the CVaR of Gw(x, a) for a given level ¯τ ∈(0, 1) is, in the integral
75. Of course, in practice, the predictions made by IQN might not actually correspond to the return
distribution of any ﬁxed policy, because of approximation, bootstrapping, and issues arising in the
control setting (Chapter 7).
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

304
Chapter 10
form of Equation 7.20,
CVaR¯τ
 Gw(x, a) = 1
¯τ
Z ¯τ
0
θw(x, a, τ)dτ .
Similar to the procedure for estimating the expected value of Gw(x, a) (Equation
10.3), this integral can be approximated by sampling m levels τ1, . . . , τm, but
now from the (0, ¯τ) interval:
CVaR¯τ
 Gw(x, a) ≈1
m
m
X
i=1
θw(x, a, τi),
τi ∼U([0, ¯τ]) .
Treating the distribution parameter as a network input opens up the possibility
for a number of diﬀerent algorithms, of which IQN is but one instantiation.
In particular, in problems where actions are real-valued (for example, when
controlling a robotic arm), it is common to also make the action a an input to
the network.
10.4
Evaluation of Deep Reinforcement Learning Agents
To illustrate the practical value of deep reinforcement learning and the added
beneﬁts from predicting return distributions, let us take a closer look at how
well the algorithms presented in this chapter can learn to play Atari 2600 video
games. As the Arcade Learning Environment provides an interface to more
than sixty diﬀerent games, a standard evaluation procedure is to apply the same
algorithm across a large set of games and report its performance on a per-game
basis, as well as aggregated across games (see Machado et al. 2018 for a more
complete discussion). Here, performance is measured in terms of the in-game
score achieved during one play-through (i.e., an episode). One particularly
attractive feature of Atari 2600 games, from a benchmarking perspective, is that
almost all games explicitly provide such a score. Measuring performance in
terms of game score has the additional advantage that it allows us to numerically
compare the playing skill of learning agents to that of human players.
The goal of the learning agent is to improve its performance at a given game
by repeatedly playing that game – in more formal terms, to optimize the risk-
neutral control objective from sample interactions.76 The agent interacts with
the environment for a total of 200 million frames per game (about 925 hours of
game-play). Experimentally, we repeat this process multiple times in order to
evaluate the performance of the agent across diﬀerent initial network weights
76. Note, however, that the in-game score diﬀers from the agent’s actual learning objective, which
involves a discount factor (canonically, γ = 0.99) and clipped rewards. This metric-objective mis-
match is well studied in the literature (e.g., van Hasselt et al. 2016a), and exists in part because
optimizing for the undiscounted, unclipped return with DQN produces unstable behavior.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Deep Reinforcement Learning
305
and realizations of the various sources of randomness. Following common
usage, we call each repetition a trial and the process itself training the agent.
Figure 10.4 (left) illustrates the overall progress made by DQN, C51, QR-DQN,
and IQN over the course these interactions, evaluated every 1 million frames.
Quantitatively, we measure an agent’s performance in terms of an aggregate
metric called human-normalized interquartile mean score (HNIQM), which we
now deﬁne.
Deﬁnition 10.1. Denote by g the score achieved by a learning agent on a
particular game and by gh and gr the score achieved by two reference agents
(a human expert and the random policy). Assuming that gh > gr, we deﬁne the
human-normalized score as
hns(g, gh, gr) = g −gr
gh −gr
.
△
In the standard evaluation protocol described here, a learning agent’s perfor-
mance during training corresponds to its average per-episode score, measured
from 500,000 frames of additional, evaluation-only interactions with the
environment.
Deﬁnition 10.2. Let E be a set of E games on which we have evaluated an
agent for K trials. For 1 ≤k ≤K and e ∈E, denote by sk
e the human-normalized
score achieved on trial k:
sk
e = hns(gk
e, gh
e, gr
e) ,
where gk
e, gh
e, and gr
e are respectively the agent’s, human player’s, and random
policy’s score on game e. Suppose that (ˆsi)K×E
i=1
denotes the vector of these
human-normalized scores, sorted so that ˆsi ≤ˆsi+1, for all i = 1, . . . , K × E −1.
The human-normalized interquartile mean score is given by
1
E1 −E0
E1−1
X
i=E0
ˆsi ,
(10.4)
where E0 is the integer nearest to KE
4 and E1 = KE −E0.
△
The normalization to human performance makes it possible to compare scores
across games; although it is also possible to evaluate agents in terms of their
mean or median normalized scores, these aggregates tend to be less representa-
tive of the full distribution of scores. As shown in Figure 10.4, it is accepted
practice (see, e.g., Agarwal et al. (2021)) to measure the degree of variability in
performance across trials and games using some empirically-determined conﬁ-
dence interval. One should be mindful that such an interval, while informative,
does not typically guarantee statistical signiﬁcance – for example, because there
are too few samples to aggregate or because these samples are not identically
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

306
Chapter 10
0
50
100
150
200
Environment Frames (millions)
0.0
0.5
1.0
1.5
2.0
2.5
HNIQM
DQN
C51
QR-DQN
IQN
0
1
2
Score
1e4
Beam Rider
0
2
4
1e2
Breakout
0
2
4
1e4
Kung Fu Master
DQN
C51 QR-DQN IQN
0.0
2.5
5.0
Score
1e3
Ms Pacman
DQN
C51 QR-DQN IQN
0
1
2
1e4
Space Invaders
DQN
C51 QR-DQN IQN
0
1
2
1e4
Wizard Of Wor
0
50
100
150
200
Environment Frames (millions)
0.0
0.5
1.0
1.5
2.0
2.5
HNIQM
DQN
C51
QR-DQN
IQN
0
1
2
Score
1e4
Beam Rider
0
2
4
1e2
Breakout
0
2
4
1e4
Kung Fu Master
DQN
C51 QR-DQN IQN
0.0
2.5
5.0
Score
1e3
Ms Pacman
DQN
C51 QR-DQN IQN
0
1
2
1e4
Space Invaders
DQN
C51 QR-DQN IQN
0
1
2
1e4
Wizard Of Wor
Figure 10.4
Top and bottom: Evaluation using the deterministic-action and sticky-action versions of
the Arcade Learning Environment, respectively. Left: Human-normalized interquartile
mean score (HNIQM; see main text) across ﬁfty-seven Atari 2600 games during the
course of learning. A normalized score of 1.0 indicates human-level performance, on
aggregate. Per-game scores were obtained from ﬁve independent trials for each algorithm
× game conﬁguration. Shading indicates bootstrapped 95 percent conﬁdence intervals
(see main text). Right: Game scores obtained by diﬀerent algorithms and a human
expert (dashed line); the scale of these scores is indicated at the top left of each graph;
dots indicate individual per-trial scores. The reported scores are measured at the end of
training.
distributed. Consequently, it is also generally recommended to report individual
game scores in addition to aggregate performance. Figure 10.4 (right) illustrates
this for a selected subset of games.
These results show that reinforcement learning, combined with deep neural
networks, can achieve a high level of performance on a wide variety of Atari
2600 games. Additionally, this performance improves with more training. This
is particularly remarkable given that the learning algorithm is only provided
game images as input, rather than game-speciﬁc features such as the location of
objects or the number of remaining lives. It is worth noting that although each
agent uses the same network architecture and hyperparameters77 across Atari
2600 games, there are a few diﬀerences between the hyperparameters used by
77. Following common usage, we use the term “hyperparameter” to distinguish the learned
parameters (e.g., w) from the parameters selected by the user (e.g., m and θ1, . . . , θm).
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Deep Reinforcement Learning
307
these agents; these diﬀerences match what is reported in the literature and are
the default choices in the code used for these experiments (Quan and Ostrovski
2020). The question of how to account for hyperparameters in scientiﬁc studies
is an active area of research (see, e.g., Henderson et al. 2018; Ceron and Castro
2021; Madeira Auraújo et al. 2021).
Historically, all four agents whose performance is reported here were trained
on what is called the deterministic-action version of the Arcade Learning Envi-
ronment, in which arbitrarily complicated joystick motions can be performed.
For example, nothing prevents the agent from alternating between the “left”
and “right” actions every four frames (ﬁfteen times per emulated second). This
makes the comparison with human players somewhat unrealistic, as human play
involves a minimum reaction time and interaction with a mechanical device that
may not support such high-frequency decisions. In addition, some of the poli-
cies found by agents in the deterministic setting exploit quirks of the emulator
in ways that were clearly not intended by the designer.
To address this issue, more recent versions of the Arcade Learning Environ-
ment implement what is called sticky actions – a procedure that introduces a
variable delay in the environment’s response to the agent’s actions. Figure 10.4
(bottom panels) shows the results of the same experiment as above, but now
with sticky actions. The performance of the various algorithms considered here
generally remains similar, with some per-game diﬀerences (e.g., for the game
Space Invaders).
Although Atari 2600 games are fundamentally deterministic, randomness is
introduced in the learning process by a number of phenomena, including side
eﬀects of distributional value iteration (Section 7.4), state aliasing (Section 9.1),
the use of a stochastic ε-greedy policy, and the sticky-actions delay added by the
Arcade Learning Environment. In many situations, this results in distributional
agents making surprisingly complex predictions (Figure 10.5). A common
theme is the appearance of bimodal or skewed distributions when the outcome
is uncertain – for example, when the agent’s behavior in the next few time steps
is critical to its eventual success or failure. Informally, we can imagine that
because the agent predicts such outcomes, it in some sense “knows” something
more about the state than, say, an agent that only predicts the expected return.
We will see some evidence to this eﬀect in the next section.
Furthermore, incorporating distributional predictions in a deep reinforcement
learning agent provides an additional degree of freedom in deﬁning the number
and type of predictions that an agent makes at any given point in time. C51, for
example, is parameterized by the number of particles m used to represent prob-
ability distributions as well as the range of its support (described by θm). Figure
10.6 illustrates the change in human-normalized interquartile mean (measured
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

308
Chapter 10
11.5
12.0
12.5
13.0
13.5
14.5
14.0
Return
0.0
0.5
1.0
1.5
2.0
2.5
3.0
Density
17.5
20.0
22.5
25.0
27.5
30.0
32.5
35.0
Return
0.00
0.05
0.10
0.15
0.20
0.25
0.30
Density
0.5
0.6
0.7
0.8
0.9
1.0
1.1
1.2
1.3
Return
0
5
10
15
20
25
Density
1.0
1.5
2.0
2.5
3.0
3.5
Return
0
1
2
3
4
Density
(a)
(c)
(b)
(d)
Figure 10.5
Example return distributions predicted by IQN agents in four Atari 2600 games: (a)
Space Invaders (published by Atari, Inc.), (b) Pong (from Video Olympics, published by
Atari, Inc.), (c) Ms. Pac-Man (published by Midway Manufacturing), and (d) H.E.R.O.
(published by Atari, Inc.). In each panel, action-return distributions for the game state
shown are estimated for each action using kernel density estimation (over 1000 samples
τ). The outlined distribution corresponds to the action with the highest expected return
estimate (chosen by the greedy selection rule).
at the end of training) that results from varying both of these hyperparameters.
These results illustrate the commonly reported ﬁnding that predicting distribu-
tions leads to improved performance, with more accurate predictions generally
helping. More generally, this illustrates that an agent’s performance depends
to a good degree on the chosen distribution representation; this is an example
of what is called an inductive bias. In addition, as illustrated by the results for
m = 201 and larger values of θm, there is clearly a complex relationship between
an agent’s parameterization and its aggregate performance across Atari 2600
games.
The development of deep distributional reinforcement learning agent architec-
tures continues to be an active area of research. Recent advances have provided
further improvements to the game-playing performance of such agents in Atari
2600 video games, including fully parameterized quantile networks (FQF; Yang
et al. 2019), which extend the ideas underlying QR-DQN and IQN, and moment-
matching DQN (MM-DQN; Nguyen et al. 2021), which deﬁnes a training loss
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Deep Reinforcement Learning
309
3
11
51
101
201
Number of Particles (m)
0.0
0.5
1.0
1.5
HNIQM
3
5
7
10
15
Maximum Return (
m)
0.0
0.5
1.0
1.5
HNIQM
(a)
(b)
Figure 10.6
Aggregate performance (HNIQM) of C51 as a function of (a) the number of particles m
and (b) the largest return that can be predicted (θm), for m = 51. Performance is averaged
over the last 10 million frames of training, and error bars indicate bootstrapped 95
percent conﬁdence intervals (see main text).
via the MMD metric described in Chapter 4. Distributional reinforcement learn-
ing has also been combined with a variety of other deep reinforcement learning
techniques, as well as being used to improve exploration; we discuss some of
these techniques in the bibliographical remarks.
10.5
How Predictions Shape State Representations
Deep reinforcement learning algorithms adjust the weights w of their neural
network in order to minimize the error in their predictions. For example, DQN’s
semi-gradient update rule
w ←w + α r + γ max
a′∈A Q ˜w(x′, a′) −Qw(x, a)∇wQw(x, a)
adjusts the network weights w in proportion to the temporal-diﬀerence error and
the ﬁrst-order relationship between each weight and the action-value prediction
Qw(x, a). One way to understand how predictions inﬂuence the agent’s behavior
is to consider how these predictions aﬀect the parameters w.
In all agent architectures studied in this chapter, the algorithm’s predictions
are formed from linear combinations of the outputs of the hidden units at the
penultimate layer. Consequently, we may separate the network weights into
two sets: the weights that map inputs to the penultimate layer and the weights
that map this layer to predictions (Figures 10.1 and 10.2). In fact, we may think
of the output of the penultimate layer as a state representation φ(x), where the
mapping φ : X →Rn is implemented by the earlier layers. Viewed this way, the
learning process simultaneously modiﬁes the parameterized mapping φ and the
weights of the ﬁnal layer in order to make better predictions.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

310
Chapter 10
Figure 10.7
Left: Example frame from the game Freeway (published by Activision, Inc.). Center
and right: Reproduced from Such et al. (2019) with permission. Input images synthe-
sized to maximally activate individual hidden units in either a DQN or Rainbow (C51)
network. Each row represents a diﬀerent hidden unit; individual columns correspond to
one of the four images provided as input to the networks.
We should also expect that the type and number of predictions made by
the agent should inﬂuence the nature of the state representation. To test this
hypothesis, Such et al. (2019) studied the patterns detected by individual hidden
units in the penultimate layer of trained neural networks. In their experiments,
input images were synthesized to maximize the output of a given hidden unit
(see, e.g., Olah et al. 2018, for a discussion of how this can be done). For
example, Figure 10.7 compares the result of this process applied to networks that
either predict action-value functions (DQN) or categorical return distributions
(Rainbow (Hessel et al. 2018), a variant of C51 enhanced with additional
algorithmic components). Across multiple Atari 2600 games, Such et al. found
that making distributional predictions resulted in state representations that
better reﬂected the structure of the Atari 2600 game being played: for example,
identifying horizontal elements (car lanes) in the game Freeway (shown in the
ﬁgure).
Understanding how distributional reinforcement learning aﬀects the state
representation implied by the network and how that representation aﬀects per-
formance is an active area of research. One challenge is that deep reinforcement
learning architectures have a great deal of moving parts, many of which are
tuned by means of hyperparameters, and obtaining relevant empirical evidence
tends to be computationally demanding. In addition, changing the state repre-
sentation has a number of downstream eﬀects on the learning process, including
changing the optimization landscape, reducing or amplifying the parameter
noise due to the use of an incremental update rule, and of course aﬀecting the
quality of the best achievable value function approximation. Yet, the hope is that
understanding why making distributional predictions improves performance in
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Deep Reinforcement Learning
311
domains such as the Arcade Learning Environment should shed light on the
design of more eﬃcient deep reinforcement learning agents.
10.6
Technical Remarks
Remark 10.1. Figure 10.1 shows that the DQN neural network is composed of
three convolutional layers, a fully connected layer, and ﬁnally a linear mapping
to the output Qw(x, a). One appeal of neural networks is that these operations can
be composed fairly easily to add capacity to the learning algorithm or combine
diﬀerent inputs: for example, the cosine embedding discussed in Section 10.3.
Let us denote the input to the network by x ∈Rn0, where for simplicity of
exposition, we omit the structured nature of the input (in the case of DQN, it is
a 84 × 84 × 4 array of values; four square images). Thus, for i ≥0, we denote
by ni ∈N the size of the “ﬂattened” input vector to layer i + 1. For i > 0, the ith
layer transforms its inputs using a function fi : Rni−1 →Rni parameterized by a
weight vector wi:
xi = fi(xi−1; wi),
i > 0 .
In the case of DQN, f1, f2, and f3 are convolutional, while f4 is fully connected.
The latter is deﬁned by a weight matrix W4 and bias vector b4 (which in our
notation are part of w4):
f4(x; w4) = [W⊤
4 x + b4]+ ;
recall that for z ∈R, [z]+ = max(0, z). In the ﬁeld of deep learning, this function
is called a rectiﬁed linear transformation (ReLU; Nair and Hinton 2010).
IQN augments this network by transforming the cosine embedding ϕ(τ) with
another fully connected layer with the same number of dimensions as the output
of the last convolutional layer (n3):
x3b = f3b(ϕ(τ); w3b) .
The output of these two layers is then composed by element-wise multiplication:
x3c = x3 ⊙x3b ,
which becomes the input to the original network’s ﬁnal layer:
x4 = f4(x3c; w4) ,
itself linearly transformed into the quantile function estimate for (x, a, τ). The
reader interested in further details should consult the work of Dabney et
al. (2018a).
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

312
Chapter 10
10.7
Bibliographical Remarks
Beyond Atari 2600 games, the general eﬀectiveness of deep reinforcement
learning is well established in the literature. Perhaps the most famous example
to date is AlphaGo (Silver et al. 2016), a computer Go program that learned to
play better than the world’s best Go players. Further evidence that distributional
predictions improve the performance of deep reinforcement learning agents
can be found in experiments on a variety of domains, including the cooperative
card game Hanabi (Bard et al. 2020), stratospheric balloon ﬂight (Bellemare
et al. 2020), robotic manipulation (Bodnar et al. 2020; Cabi et al. 2020; Vecerik
et al. 2019), simulated race car control (Wurman et al. 2022), and magnetic
control of plasma (Degrave et al. 2022).
10.1. Early reinforcement learning research has close ties with the study of
connectionist systems; see, for example, Barto et al. (1983) and Bertsekas
and Tsitsiklis (1996). Tesauro (1995) combined temporal-diﬀerence learning
with a single-layer neural network to produce TD-Gammon, a grandmaster-
level Backgammon player and early deep reinforcement learning algorithm.
Neural Fitted Q-iteration (Riedmiller 2005) implements many of the ingredients
later found in DQN, including replaying past experience (Lin 1992) and the
use of a target network; the method has been successfully used to control a
soccer-playing robot (Riedmiller et al. 2009).
The Arcade Learning Environment (Bellemare et al. 2013a), itself based
on the Stella emulator (Mott et al. 1995–2023), introduced Atari 2600 game-
playing as a challenge domain for artiﬁcial intelligence. Early results on the
Arcade Learning Environment included both reinforcement learning (Bellemare
et al. 2012a, 2012b) and planning (Bellemare et al. 2013b; Lipovetzky et
al. 2015) solutions. The DQN algorithm demonstrated the ability of deep neural
networks to eﬀectively tackle this domain (Mnih et al. 2015). Since then, deep
reinforcement learning has been applied to produce high-performing policies
for a variety of video games and image-based control problems (e.g., Beattie
et al. 2016; Levine et al. 2016; Kempka et al. 2016; Bhonker et al. 2017; Cobbe
et al. 2020). Machado et al. (2018) study the relative performance of linear
and deep methods in the context of the Arcade Learning Environment. See
François-Lavet et al. (2018) and Arulkumaran et al. (2017) for reviews of deep
reinforcement learning and Graesser and Keng (2019) for a practical overview.
Montfort and Bogost (2009) give an excellent history of the Atari 2600 video
game console itself.
10.2–10.3. The C51, QR-DQN, and IQN agent architectures and algorithms
were respectively introduced by Bellemare et al. (2017a), Dabney et al. (2018b),
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Deep Reinforcement Learning
313
and Dabney et al. (2018a). Open-source implementations of these three algo-
rithms are available in the Dopamine framework (Castro et al. 2018) and the
DQN Zoo (Quan and Ostrovski 2020). The idea of implicitly parameterizing
other arguments of the prediction function has been used extensively to deal with
continuous actions; see, for example, Lillicrap et al. (2016b) and Barth-Maron
et al. (2018).
There is by now a wide variety of deep distributional reinforcement learn-
ing algorithms, many of which outperform IQN. FQF (Yang et al. 2019)
approximates the return distribution with a weighted combination of Diracs
by combining the IQN architecture with a method for selecting which values
of τ ∈(0, 1) to feed into the network. MM-DQN (Nguyen et al. 2021) use an
architecture based on QR-DQN in combination with an MMD-based loss as
described in Chapter 4; typically, the Gaussian kernel has been found to provide
the best empirical performance, despite a lack of theoretical guarantees. Both
Freirich et al. (2019) and Doan et al. (2018) propose the use of generative
adversarial networks (Goodfellow et al. 2014) to model the reward distribution.
Freirich et al. also extend this approach to the case of multivariate rewards.
There are also several recent modiﬁcations to the QR-DQN architecture that
seek to address the quantile-crossing problem – namely, that the outputs of
the QR-DQN network need not satisfy the natural monotonicity constraints of
distribution quantiles. Yue et al. (2020) propose to use deep generative mod-
els combined with a postprocessing sorting step to obtain monotonic quantile
estimates. Zhou et al. (2021) parameterize the diﬀerence between successive
quantiles, rather than the quantile locations themselves, to enforce monotonic-
ity; this approach was extended by Luo et al. (2021), who directly parameterize
the quantile function via rational-quadratic splines. Developing and improving
deep distributional reinforcement learning agents continues to be an exciting
direction of research.
Several agents also incorporate a distributional loss in combination with
a variety of other deep reinforcement learning techniques. Munchausen-IQN
(Vieillard et al. 2020) combines IQN with a form of entropy regularization, and
Rainbow (Hessel et al. 2018) combines C51 with a variety of modiﬁcations
to DQN, including double Q-networks (van Hasselt et al. 2016b), prioritized
experience replay (Schaul et al. 2016), a value-advantage dueling architecture
(Wang et al. 2016), parameter noise for exploration (Fortunato et al. 2018),
and multistep returns (Sutton 1988). There has also been a wide variety of
work combining distributional RL with the actor-critic framework, typically
by modifying the critic to include distributional predictions; see, for exam-
ple, Tessler et al. (2019), Kuznetsov et al. (2020), and Duan et al. (2021) and
Nam et al. (2021).
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

314
Chapter 10
Several recent deep reinforcement learning agents have also leveraged return
distribution estimates to improve exploration. Nikolov et al. (2019) combine
C51 with information-directed exploration to obtain an agent that outperforms
IQN, as judged by human-normalized mean and median performance on Atari
2600. Mavrin et al. (2019) extract an estimate of parametric uncertainty from
distributions learned under QR-DQN and use this information to specify an
exploration bonus. Clements et al. (2020) similarly build on QR-DQN to esti-
mate both aleatoric and epistemic uncertainties and use these for exploratory
action selection. Zhang and Yao (2019) use the quantile representation learned
by QR-DQN to form a set of options, with policies that vary in their risk-
sensitivity, to improve exploration. Further use cases continue to be developed,
with Lin et al. (2019) decomposing the reward signal into independent streams,
all of which are then predicted.
10.4. The training and evaluation protocol presented here, including the idea
of a human-normalized score, is due to Mnih et al. (2015); more generally,
Bellemare et al. (2013a) propose the use of a normalization scheme in order to
compare agents across Atari 2600 games. The sticky-actions mechanism was
proposed by Machado et al. (2018), who give evidence that a naive trajectory
optimization algorithm (see also Bellemare et al. 2015) can achieve scores
comparable to DQN when evaluated with the deterministic version of the ALE.
Our use of the interquartile mean to compare score follows the recommendations
of Agarwal et al. (2021), who also highlight reproducibility concerns when
evaluating across multiple-domain games. See also Henderson et al. (2018).
The experiments reported here were performed using the DQN Zoo (Quan and
Ostrovski 2020).
10.5. The success of distributional reinforcement learning algorithms on large
benchmarks such as the Arcade Learning Environment is often attributed to
their use as auxiliary tasks (Jaderberg et al. 2017). Auxiliary tasks are ancillary
predictions made by the neural network that stabilize learning, shape the state
representation implied by the neural network, and improve end performance
(Lample and Chaplot 2017; Kartal et al. 2019; Agarwal et al. 2020; Laskin
et al. 2020; Guo et al. 2020; Lyle et al. 2021). Their eﬀect on the penultimate
layer of the network is discussed more formally by Chung et al. (2018), Belle-
mare et al. (2019a), and Le Lan et al. (2022). Dabney et al. (2020a) argue that
representation learning plays a particularly acute role in deep reinforcement
learning when considering the control problem, in which the policy under eval-
uation changes over time. General value functions (GVFs) provide a language
for describing auxiliary tasks and expressing an agent’s knowledge (Sutton et
al. 2011). Schlegel et al. (2021) extend some of these ideas to the deep learning
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Deep Reinforcement Learning
315
setting, in particular considering how GVFs are useful features in the context of
sequential prediction.
As an alternative explanation for the empirical successes of distributional
reinforcement learning, Imani and White (2018) study the eﬀect of distributional
predictions on optimisation landscapes.
10.8
Exercises
Many of the exercises in this chapter are hands-on and open-ended in nature
and require programming or applying published learning algorithms to stan-
dard reinforcement learning domains. More generally, these exercises are
designed to give the reader some experience in performing deep reinforce-
ment learning experiments. As a starting point, we recommend that the reader
use open-source implementations of the algorithms covered in this chapter. The
Dopamine framework78 (Castro et al. 2018) provides implementations of the
DQN, C51, QR-DQN, and IQN agents and support for the Acrobot domain.
DQN Zoo79 (Quan and Ostrovski 2020) is a collection of open-source reference
implementations that were used to generate the results of this chapter.
Exercise 10.1. In this exercise, you will apply deep reinforcement learning
methods to the Acrobot domain (Sutton 1996). Acrobot is a two-link pendulum
where the state gives the joint angles of the two links and their corresponding
angular velocities. Because the inputs are vectors rather than images, the DQN
agent described in this chapter must be adapted to this domain by replacing the
convolutional layers by one or multiple fully connected layers and substituting a
simple state encoding for the image preprocessing (in Dopamine, this encoding
is given by what is called Fourier features (Konidaris et al. 2011)). Of course,
the reader is encouraged to think of possible alternatives to both of these choices.
(i) Train a DQN agent, varying the frequency at which the target network is
updated. Plot the value function over time, at the initial starting state and for
diﬀerent update frequencies.
(ii) Train a DQN agent, but now varying the size of the replay buﬀer. Describe
the eﬀect of the replay buﬀer size on the learning algorithm in this case.
(iii) Modify the DQN implementation to train from each transition as it is
received, rather than via the replay buﬀer. Compare this to the previous
results.
(iv) Starting from an implementation of the C51 agent, implement the signed
distributional algorithm from Section 9.6. Train both C51 and the signed
78. https://github.com/google/dopamine
79. https://github.com/deepmind/dqn_zoo
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

316
Chapter 10
algorithm on Acrobot. Visualize the return-distribution function predictions
made for the same initial state, over time. Plot the average undiscounted
return obtain by either algorithm over time. Explain your ﬁndings.
(v) Train both QR-DQN and IQN agents on Acrobot, and visualize the dis-
tributional predictions made for the same initial state. For IQN, this will
require querying the deep neural network for multiple values of τ. Visualize
those predictions at diﬀerent points during an episode; do they behave as
expected?
△
Exercise 10.2 (*). In this exercise, you will apply deep RL methods to the
MinAtar domain (Young and Tian 2019), which is a set of simpliﬁed versions
of Atari 2600 games (Asterix, Breakout, Freeway, Seaquest, Space Invaders).
(i) Using MinAtar’s built-in human-play example, play each game for ten
episodes and log your scores. How much variability do you see between
episodes? Plot the scores versus games played; does your performance
improve over time?
(ii) Implement a random agent that takes actions uniformly at random. Evaluate
the random agent on each MinAtar game for at least ten episodes and log
the agent’s scores for each game and episode.
(iii) Train a C51 agent to play Breakout in MinAtar for 2 million frames
(approximately 2.5 hours on a GPU). Use the default hyperparameters for
the range of predicted returns (θ1 = −10 and θm = 10). Use your recorded
scores from above (for human and random players) to compute C51’s human-
normalized score. How do these compare with those reported for C51 on
the corresponding games in Atari 2600?
(iv) Based upon your performance and that of the random agent, compute a
reasonable estimate of the maximum achievable discounted return in each
game. Train C51, again for 2 million frames, using this maximum return
to set the particle locations. Compare this agent’s performance in terms of
human-normalized score with that of the default C51 agent above. How do
they compare?
(v) Train a QR-DQN agent on the MinAtari Breakout game, evaluating in
terms of human-normalized score. Inspecting the learned return distributions,
how do these distributions compare with those learned by C51? Do any of
the quantile estimates exceed your estimated maximal discounted return?
Why might this happen?
(vi) Train DQN, C51, and QR-DQN agents with your preferred hyperparam-
eters, for 5 million frames with at least three seeds, on each of the ﬁve
MinAtar games. Compute the human-normalized interquartile mean score
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Deep Reinforcement Learning
317
(HNIQM) for each method versus training steps. How do your results com-
pare with those see in Figure 10.4? Note that this exercise will require
signiﬁcantly greater computational resources than others and will in general
require running multiple agents in parallel.
△
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

11
Two Applications and a Conclusion
We conclude by highlighting two applications of the core ideas covered in
earlier chapters, with the aim to give a sense of the range of domains to which
ideas from distributional reinforcement learning have been and may eventually
be applied.
11.1
Multiagent Reinforcement Learning
The core setting studied in this book is the interaction between an agent and its
environment. The model of the environment as an unchanging, static Markov
decision process is a good ﬁt for many problems of interest. However, a notable
exception is the case in which the agent ﬁnds itself interacting with other
learning agents. Such settings arise in games, both competitive and cooperative,
as well as real-world interactions such as in autonomous driving.
Interactions between distinct agents lead to an incredibly rich space of learn-
ing problems. What is possible is governed by considerations such as how
many agents there are, whether their interests are aligned or competing, whether
they have the same information about the environment, whether they must act
concurrently or sequentially, and whether they can directly communicate with
each other. We choose to focus here on just one of many models for cooperative
multiagent interactions.
Deﬁnition 11.1 (Boutilier 1996). A multiagent Markov decision process
(MMDP) is a Markov decision process (X, A, ξ0, PX, PR) in which the action
set A has a factorized structure A = QN
i=1 Ai, for some integer N ∈N+ and ﬁnite
nonempty sets Ai. We refer to N as the number of players in the MMDP.
△
An N-player MMDP describes N agents interacting with an environment. At
each stage, agent i selects an action ai ∈Ai (i = 1, …, N), knowing the current
state x ∈X of the MMDP, but without knowledge of the actions of the other
agents. All agents observe the reward resulting from the joint action (a1, …, aN)
319
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

320
Chapter 11
and share the joint goal of maximizing the discounted sum of rewards arising in
the MMDP; their interests are perfectly aligned.
To compute a joint optimal policy for the agents, one approach is to treat
the problem as an MDP and use either dynamic programming or temporal-
diﬀerence learning methods to compute an optimal policy. These methods
assume centralized computation of the policy, which is then communicated to
the agents to execute.
By contrast, the decentralized control problem is for the agents to arrive
at a joint optimal policy through direct interaction with the environment and
without any centralized or interagent communication; this is pertinent when
communication between agents is impossible or costly, and a model of the
environment is not known. Thus, the agents jointly interact with the environment,
producing transitions of the form (x, (a1, …, aN), r, x′); agent i observes only
(x, ai, r, x′) and must learn from transitions of this form, without observing the
actions of other agents that inﬂuenced the transition.
Example 11.2. The partially stochastic climbing game (Kapetanakis and
Kudenko 2002; Claus and Boutilier 1998) is an MMDP with a single non-
terminal state (also known as a matrix game), two players, and three actions per
player. The reward distributions for each combination of the players’ actions
are shown on the left-hand side of Figure 11.1; the ﬁrst player’s actions index
the rows of this matrix, and the second player’s actions index the columns. All
rewards are deterministic, except for the case of the central element, where
the distribution is uniform over the set {0, 14}. This environment represents a
coordination challenge for the two agents: the optimal strategy is for both to
take the ﬁrst action, but if either agent deviates from this strategy (by exploring
the value of other actions, for example), negative rewards of large magnitude
are incurred.
△
A concrete example of an approach to the decentralized control problem is
for each agent to independently implement Q-learning with these transitions
(Tan 1993). The center panels of Figure 11.1 show the result of the agents
using Q-learning to learn in the partially stochastic climbing game. Both agents
act using an ε-greedy policy, with ε decaying linearly during the interaction
(beginning at 1 and ending at 0), and use a step size of α = 0.001 to update their
action values. Due to the exploration the agents are undertaking, the ﬁrst action
is judged as worse than the third action by both agents, and both quickly move
to using the third action, hence not discovering the optimal behavior for this
environment.
The failure of the Q-learning agents to reach the optimal behavior stems from
the fact that from the point of view of an individual agent, the environment it is
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Two Applications and a Conclusion
321

11
−30
0
−30
U({0, 14})
6
0
0
5

Figure 11.1
Left: Table specifying reward distributions for the partially stochastic climbing game.
Right: Learned action values for each player–action combination under Q-learning (ﬁrst
column) and a distributional algorithm (second column).
interacting with is no longer Markov; it contains other learning agents, which
may adapt their behavior as time progresses, and in particular in response to
changes in the behavior of the individual agent itself. Redesigning learning
rules such as Q-learning to take into account the changing behavior of other
agents in the environment is a core means of encouraging better cooperation
between agents in such settings in multiagent reinforcement learning.
Hysteretic Q-learning (Matignon et al. 2007; HQL) is a modiﬁcation of
Q-learning that swaps the usual risk-neutral value update for a rule that instead
tends to learn an optimistic estimate of the value associated with an action.
Speciﬁcally, given an observed transition (x, a, r, x′), HQL performs the update
Q(x, a) ←Q(x, a) +

α1{∆> 0} + β1{∆< 0}

∆,
where ∆= r + γ max
a′∈X Q(x′, a) −Q(x, a) is the TD error associated with the tran-
sition. Here, 0 < β < α are asymmetric step size parameters associated with
negative and positive TD errors. By making larger updates in response to posi-
tive TD errors, the learnt Q-values end up placing more weight on high-reward
outcomes. In fact, this update can be shown to be equivalent to following the
negative gradient of the expectile loss encountered in Section 8.6:
Q(x, a) ←Q(x, a) + (α + β)|1{∆<0} −τ|∆,
with τ = α/α+β. The values learnt by HQL are therefore a kind of optimistic
summary of the agent’s observations. The motivation for learning values in this
way is that low-reward outcomes may be due to the exploratory behavior from
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

322
Chapter 11
other agents, which may be avoided as learning progresses, while rewarding
transitions may eventually occur more often, as other agents improve their
policies and are able to more reliably produce these outcomes. Matignon et
al. (2007) show that hysteretic Q-learning can lead to improved coordination
among decentralized agents compared to independent Q-learning in a range of
environments.
Distributional reinforcement learning provides a natural framework to build
optimistic learning algorithms of this form, by combining an algorithm for
learning representations of return distributions (Chapters 5 and 6) with a risk-
sensitive policy derived from these distributions (Chapter 7). To illustrate this
point, we compare the results of independent Q-learning on the partially stochas-
tic climbing game with the case where both agents use a distributional algorithm
in which distributions are updated using categorical TD updates. We take distri-
butions supported on {−30, −29, …, 30} and deﬁne greedy actions deﬁned in a
risk-sensitive manner; in particular, the greedy action is the one with the greatest
expectile at level τ, calculated from the categorical distribution estimates (see
Chapter 7), with τ linearly decaying from 0.9 to 0.7 throughout the course of
learning.
Figure 11.1 shows the learnt action values by both distributional agents in
this setting; the exploration schedule and step sizes are the same as for the
independent Q-learning agents. This level of optimism means that action values
are not overly inﬂuenced by the exploration of other agents and is also not too
high so as to be distracted by the (stochastic) outcome of fourteen available
when both agents play the second action, and indeed the agents converge to the
optimal joint policy in this case. We remark, however, that the optimism level
chosen here is tuned to illustrate the beneﬁcial eﬀects that are possible with
distributional approaches to decentralized cooperative learning, and in general,
other choices of risk-sensitive policies will not lead to optimal behavior in this
environment. This is illustrative of a broader tension: while we would like to be
optimistic about the behavior of other learning agents, the approach inevitably
leads to optimism in aleatoric environment randomness (in this example, the
randomness in the outcome when both players select the second action). With
both distributional and nondistributional approaches to decentralized multiagent
learning, it is diﬃcult to treat these sources of randomness diﬀerently from one
another.
The majority of work in distributional multiagent reinforcement learning
has focused on the case of large-scale environments, using deep reinforcement
learning approaches such as those described in Chapter 10. Lyu and Amato
(2020) introduce Likelihood Hysteretic IQN, which uses return distribution
learnt by an IQN architecture to adapt the level of optimism used in value
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Two Applications and a Conclusion
323
function estimates throughout training. Da Silva et al. (2019) also found beneﬁts
from using risk-sensitive policies based on learnt return distributions. In the
centralized training, decentralized execution regime (Oliehoek et al. 2008),
Sun et al. (2021) and Qiu et al. (2021) empirically explore the combination of
distributional reinforcement learning with previously established value function
factorization methods (Sunehag et al. 2017; Rashid et al. 2018; Rashid et
al. 2020). Deep distributional reinforcement learning agents have also been
successfully employed in cooperative multiagent environments without making
any use of learnt return distributions beyond expected values. The Rainbow
agent (Hessel et al. 2018), which makes use of the C51 algorithm described
in Chapter 10, forms a baseline for the Hanabi challenge (Bard et al. 2020).
Combinations of deep reinforcement learning with distributional reinforcement
learning have found application in a variety of multiagent problems to date;
we expect there to be further experimentally driven research in this area of
application and also remark that the theoretical understanding of how such
algorithms perform is largely open.
11.2
Computational Neuroscience
Machine learning and reinforcement learning often take inspiration from psy-
chology, neuroscience, and animal behavior. Examples include convolutional
neural networks (LeCun and Bengio 1995), experience replay (Lin 1992),
episodic control (Pritzel et al. 2017), and navigation by grid cells (Banino
et al. 2018). Conversely, algorithms developed for artiﬁcial agents have proven
useful as computational models for building theories regarding the mechanisms
of learning in humans and animals; some authors have argued, for example, on
the plausibility of backpropagation in the brain (Lillicrap et al. 2016a). As we
will see in this section, distributional reinforcement learning is also useful in this
regard and serves to explain some of the ﬁne-grained behavior of dopaminergic
neurons in the brain.
Dopamine (DA) is a neurotransmitter associated with learning, motivation,
motor control, and attention. Dopaminergic neurons, especially those concen-
trated in the ventral tegmental area (VTA) and substantia nigra pars compacta
(SNc) regions of the midbrain, release dopamine along several pathways pro-
jecting throughout the brain – in particular, to areas known to be involved in
reinforcement, motor function, executive functions (such as planning, decision-
making, selective attention, and working memory), and associative learning.
Furthermore, despite their relatively modest numbers (making up less than 0.001
percent of the neurons in the human brain), they are crucial to the development
and functioning of human intelligence. This can be seen especially acutely by
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

324
Chapter 11
dopamine’s implication in a range of neurological disorders such as Parkinson’s
disease, attention-deﬁcit hyperactivity disorder (ADHD), and schizophrenia.
The Rescorla–Wagner model (Rescorla and Wagner 1972) posits that the
learning of conditioned behavior in humans and animals is error-driven. That
is, learning occurs as the consequence of a mismatch between the learner’s
predictions and the observed outcome. The Rescorla–Wagner equation takes
the form of a familiar update rule:80
V ←V + α (r −V)
| {z }
error
,
(11.1)
where V is the predicted reward, r the observed reward, and α an asymmetric
step size parameter. Here, the term α plays the same role as the step size
parameter introduced in Chapter 3 but describes the modeled rate at which the
animal learns rather than a parameter proper.81
Rescorla and Wagner’s model explained, for example, classic experiments
in which rabbits learned to blink in response to a light cue predictive of an
unpleasant puﬀof air (an example of Pavlovian conditioning). The model also
explained a learning phenomenon called blocking (Kamin 1968): having learned
that the light cue predicts a puﬀof air, the rabbits did not become conditioned to
a second cue (an audible tone) when that cue was presented concurrently with
the light. This gave support to the theory of error-driven learning, as opposed to
associative learning purely based on co-occurrence (Pavlov 1927).
Temporal-diﬀerence learning is also a type of error-driven learning, one that
accounts for the temporally extended nature of prediction. In its simplest form,
TD learning is described by the equation
V ←V + α (r + γV′ −V)
|          {z          }
TD error
,
(11.2)
which improves on the Rescorla–Wagner model by decomposing the learning
target into an immediate reward (observed) and a prediction V′ about future
rewards (guessed). Just as the Rescorla–Wagner equation explains blocking,
temporal-diﬀerence learning explains how cues can themselves generate pre-
diction errors (by a process of bootstrapping). This in turn gives rise to the
phenomenon of second-order conditioning. Second-order conditioning arises
when a secondary cue is presented anterior to the main cue, which itself predicts
the reward. In this case, the secondary cue elicits a prediction of the future
reward, despite only being paired with the main cue and not the reward itself.
80. This notation resembles, but is not quite the same as, that of previous chapters, yet it is common
in the ﬁeld (see, e.g., Ludvig et al. 2011).
81. Admittedly, the diﬀerence is subtle.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Two Applications and a Conclusion
325
In one set of experiments, the dopaminergic (DA) neurons of macaque mon-
keys were recorded as they learned that a light is predictive of the availability
of a reward (juice, received by pressing a lever).82 In the absence of reward,
DA neurons exhibit a sustained level of activity, given by the baseline or tonic
ﬁring rate. Prior to learning, when a reward was delivered, the monkeys’ DA
neurons showed a sudden, short burst of activity, known as phasic ﬁring (Figure
11.2a, top). After learning, the DA neurons’ ﬁring rate no longer deviated from
the baseline when receiving the reward (Figure 11.2a, middle). However, pha-
sic activity was now observed following the appearance of the cue (CS, for
conditional stimulus).
One interpretation for these learning-dependent increases in ﬁring rate is
that they encode a positive prediction error. The increase in ﬁring rate at the
appearance of the cue, in particular, gives evidence that the cue itself eventually
induces a reward-based prediction error (RPE). Even more suggestive of an
error-driven learning process, omitting the juice reward following the cue
resulted in a decrease in ﬁring rate (a negative prediction error) at the time at
which a reward was previously received; simultaneously, the cue still resulted
in an increased ﬁring rate (Figure 11.2a, bottom).
The RPE interpretation was further extended when Montague et al. (1996)
showed that temporal-diﬀerence learning predicts the occurrence of a partic-
ularly interesting phenomenon found in an early experiment by Schultz et
al. (1993). In this experiment, macaque monkeys learned that juice could be
obtained by pressing one of two levers in response to a sequence of colored
lights. One of two lights (green, the “instruction”) ﬁrst indicated which lever to
press. Then, a second light (yellow, the “trigger”) indicated when to press the
lever and thus receive an apple juice reward – eﬀectively providing a ﬁrst-order
cue.
Figure 11.2b shows recordings from DA neurons after conditioning. When
the instruction light was provided at the same time as the trigger light, the
DA neurons responded as before: positively in response to the cue. When the
instruction occurred consistently one second before the trigger, the DA neurons
showed an increase in ﬁring only in response to the earlier of the two cues.
However, when the instruction was provided at a random time prior to the
trigger, the DA neurons now increased their ﬁring rate in response to both
events – encoding a positive error from receiving the unexpected instruction
and the necessary error from the unpredictable trigger. In conclusion, varying
the time interval between these two lights produced results that could not be
82. For a more complete review of reinforcement learning models of dopaminergic neurons and
experimental ﬁndings, see Schultz (2002), Glimcher (2011), and Daw and Tobler (2014).
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

326
Chapter 11
No prediction
Reward occurs
Reward predicted
Reward occurs
Reward predicted
No reward occurs
(a)
(b)
//
//
//
//
10
5
0
imp/s
10
5
0
imp/s
10
5
0
imp/s
Instruction
Trigger
-0.5
0.5
2.5
3.0
-0.5
0.5 s
0
3.5 s
0
Instruction
Trigger
-0.5
0
0.5
1.0
-0.5
0.5 s
0
1.5 s
Instruction + Trigger
-1.5
-1.0
0.5
0
-0.5
0.5 s
0
0.5 s
Figure 11.2
(a) DA activity when an unpredicted reward occurs, when a cue predicts a reward and it
occurs, and when a cue predicts a reward but it is omitted. The data are presented both
in raster plots showing ﬁring of a single dopaminergic neuron and as peri-stimulus time
histograms (PSTHs) – histograms capturing neuron ﬁring rate over time. Conditioned
stimulus (CS) marks the onset of the cue, with delivery or omission of reward indicated
by (R) or (no R). From Schultz et al. (1997). Reprinted with permission from AAAS. (b)
PSTHs averaged over a population of dopamine neurons for three conditions examining
temporal credit assignment. From Schultz et al. (1993), copyright 1993 Society for
Neuroscience.
completely explained by the Rescorla–Wagner model but were consistent with
TD learning.
The temporal-diﬀerence learning model of dopaminergic neurons suggests
that, in aggregate, these neurons modulate their ﬁring rate in response to unex-
pected rewards or in response to an anticipated reward failing to appear. In
particular, the model makes two predictions: ﬁrst, that deviations from the
tonic ﬁring rate should be proportional to the magnitude of the prediction error
(because the TD error in Equation 11.2 is linear in r), and second, that the tonic
ﬁring rate in a trained animal should correspond to the situation in which the
received reward matches the expected value (that is, r + γV′ = V, in which case
there is no prediction error).
For a given DA neuron, let us call reversal point the amount of reward r0
for which, if a reward r < r0 is received, the neuron expresses a negative error,
and if a reward r > r0 is received, it expresses a positive error.83 Under the
TD learning model, individual neurons should show approximately identical
reversal points (up to an estimation error) and should weigh positive and neg-
ative errors equally (Figure 11.3a). However, experimental evidence suggests
83. Assuming that the return is r (i.e., there is no future value V′). We can more generally deﬁne
the reversal point with respect to an observed return, but this distinction is not needed here.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Two Applications and a Conclusion
327
(a)
(b)
1.0
0.5
0.0
0.5
1.0
1.5
Firing Rate (variance normalized)
Cell (Sorted by reversal point)
3
2
1
0
1
2
Firing Rate (variance normalized)
Cell (Sorted by reversal point)
Figure 11.3
(a) The temporal-diﬀerence learning model of DA neurons predicts that, while individual
neurons may show small variations in reversal point (e.g., due to estimation error), their
response should be linear in the TD error and weight positive and negative errors
equally. Neurons are sorted from top to bottom in decreasing order of reversal point.
(b) Measurements of the change in ﬁring rate in response to each of the seven possible
reward magnitudes (indicated by marker and shading) for individual dopaminergic
neurons in mice (Eshel et al. 2015), sorted in decreasing order of imputed reversal point.
These measurements exhibit marked deviation from the linear error-response predicted
by the TD learning model.
otherwise – that individual neurons instead respond to the same cue in a man-
ner speciﬁc to each neuron and asymmetrically depending on the reward’s
magnitude (Figure 11.3b).
Eshel et al. (2015) measured the ﬁring rate of individual DA neurons of
mice in response to a random reward 0.1, 0.3, 1.2, 2.5, 5, 10, or 20 µL of juice,
chosen uniformly at random for each trial. Figure 11.4a shows the change
in ﬁring rate in response to each reward, after conditioning, as a function of
each neuron’s imputed reversal point (see Dabney et al. 2018 for details). The
analysis illustrates a marked asymmetry in the response of individual neurons
to reward; the neurons with the lowest reversal points, in particular, increase
their ﬁring rate for almost all rewards.
We may explain this phenomenon by considering a per-neuron update rule
that incorporates an asymmetric step size, known as the distributional TD model.
Because the neurons’ change in ﬁring rate does in general vary monotonically
with the magnitude of the reward, it is natural to consider an incremental
algorithm derived from expectile dynamic programming (Section 8.6). As
before, let (τi)m
1=1 be values in the interval (0, 1), and (θi)m
i=1 a set of adjustable
locations. Here, i corresponds to an individual DA neuron, such that θi denotes
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

328
Chapter 11
the predicted future reward for which this neuron computes an error, and τi
determines the asymmetry in its step size. For a sample reward r, the negative
gradient of the expectile loss (Equation 8.13) with respect to θi yields the update
rule
θi ←θi + α |1{r < θi} −τi|(r −θi)
|                 {z                 }
expectile error
,
(11.3)
Here, the term |1{r < θi} −τi| constitutes an asymmetric step size.
Under this model, the reversal point of a neuron corresponds to the prediction
θi, and therefore a neuron’s deviation from its tonic ﬁring rate corresponds to
the expectile error. In turn, the slope or rate at which the ﬁring rate is reduced
or increased as a function of the error reﬂects in some sense the neuron’s “step
size” α|1{g < θi} −τi|. By measuring the slope of a neuron’s change in ﬁring rate
for rewards smaller and larger than the imputed reversal point, one ﬁnds that
diﬀerent neurons indeed exhibit asymmetric slopes around their reversal point
(Figure 11.4a).
Given the slopes α+ and α−above and below the reversal point, respectively,
for an individual neuron, we can recover an estimate of the asymmetry parameter
τi according to
τi =
α+
α+ + α−.
With this change of variables, one ﬁnds a strong correlation between indi-
vidual neurons’ reversal points (θi) and their inferred asymmetries (τi); see
Figure 11.4b. This gives evidence that the diversity in responses to rewards of
diﬀerent magnitudes is structured consistent with an expectile representation of
the distribution learned through asymmetric scaling of prediction errors, that is,
evidence supporting the distributional TD model of dopamine.
As a whole, these results suggest that the behavior of dopaminergic neurons
is best modeled not with a single global update rule, such as in TD learning,
but rather a collection of update rules that together describe a richer prediction
about future rewards – a distributional prediction. While the downstream uses
of such a prediction remain to be identiﬁed, one can naturally imagine that
there should be behavioral correlates involving risk and uncertainty. Other open
questions around distributional RL in the brain include: What are the biological
mechanisms that give rise to the diverse asymmetric responses in DA neurons?
How, and to what degree, are DA neurons and those that encode reversal points
coupled, as required by the distributional TD model? Does distributional RL
confer representation learning beneﬁts in biological agents as it does in artiﬁcial
agents?
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Two Applications and a Conclusion
329
(a)
(b)
Figure 11.4
(a) Examples of the change in ﬁring rate in response to various reward magnitudes for
individual dopaminergic (DA) neurons showing asymmetry about the reversal point.
Each plot corresponds to an individual DA neuron, and each point within, with error bars
showing standard deviation over trials, shows that neuron’s change in ﬁring rate upon
receiving one of the seven reward magnitudes. Solid lines correspond to the piecewise
linear best ﬁt around the reversal point. (b) Estimated asymmetries strongly correlate
with reversal points as predicted by distributional TD. For all measured DA neurons
(n = 40), we show the estimated reversal point versus the cell’s asymmetry. We observe a
strong positive correlation between the two, as predicted by the distributional TD model.
11.3
Conclusion
The act of learning is fundamentally an anticipatory activity. It allows us to
deduce that eating certain kinds of foods might be hazardous to our health and
consequently avoid them. It helps the footballer decide how to kick the ball into
the opposite team’s net and the goalkeeper to prepare for the save before the
kick is even made. It informs us that studying leads to better grades; experience
teaches us to avoid the highway at rush hour. In a rich, complex world, many
phenomena carry a part of unpredictability, which in reinforcement learning
we model as randomness. In that respect, learning to predict the full range of
possible outcomes – the return distribution – is only natural: it improves our
understanding of the environment “for free,” in the sense that it can be done in
parallel with the usual learning of expected returns.
For the authors of this book, the roots of the distributional perspective lie
in deep reinforcement learning, as a technique for obtaining more accurate
representations of the world. By now, it is clear that this is but one potential
application. Distributional reinforcement learning has proven useful in settings
far beyond what was expected, including to model the behaviors of coevolving
agents and the dynamics of dopaminergic neurons. We expect this trend to
continue and look forward to seeing its greater application in mathematical
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

330
Chapter 11
ﬁnance, engineering, and life sciences. We hope this book will provide a sturdy
foundation on which these ideas can be built.
11.4
Bibliographical Remarks
11.1. Game theory and the study of multiagent interactions is a research disci-
pline that dates back almost a century (von Neumann 1928; Morgenstern and
von Neumann 1944). Shoham and Leyton-Brown (2009) provide a modern
summary of a wide range of topics relating to multiagent interactions, and
Oliehoek and Amato (2016) provide a recent overview from a reinforcement
learning perspective. The MMDP model described here was introduced by
Boutilier (1996), and forms a special case of the general class of Markov games
(Shapley 1953; van der Wal 1981; Littman 1994). A commonly encountered
generalization of the MMDP is the Dec-POMDP (Bernstein et al. 2002), which
also allows for partial observations of the state. Lauer and Riedmiller (2000)
propose an optimistic algorithm with convergence guarantees in deterministic
MMDPs, and many other (nondistributional) approaches to decentralized con-
trol in MMDPs have since been considered in the literature (see, e.g., Bowling
and Veloso 2002; Panait et al. 2003; Panait et al. 2006; Matignon et al. 2007,
2012; Wei and Luke 2016), including in combination with deep reinforcement
learning (Tampuu et al. 2017; Omidshaﬁei et al. 2017; Palmer et al. 2018;
Palmer et al. 2019). There is some overlap between certain classes of these
techniques and distribution reinforcement learning in stateless environments,
as noted by Rowland et al. (2021), on which the distributional example in this
section is based.
11.2. A thorough review of the research surrounding computational models
of DA neurons is beyond the scope of this book. For the machine learning
researcher, Niv (2009) and Sutton and Barto (2018) provide a broad discus-
sion and historical account of the connections between neuroscience and
reinforcement learning; see also the primer by Ludvig et al. (2011) for a
concise introduction to the topic and the work by Daw (2003) for a neuroscien-
tiﬁc perspective on computational models. Other recent, neuroscience-focused
overviews are provided by Shah (2012), Daw and Tobler (2014), and Lowet
et al. (2020). Here we highlight a few key works due to their historical rele-
vance, as well as those that provide context into both compatible and competing
hypotheses surrounding dopamine-based learning in the brain.
As discussed in Section 11.2, Montague et al. (1996) and Schultz et al. (1997)
provided the early experimental ﬁndings that led to the formulation of the
temporal-diﬀerence model of dopamine. These results followed mounting evi-
dence of limitations in the Rescorla–Wagner model (Schultz 1986; Schultz and
Romo 1990; Ljungberg et al. 1992; Miller et al. 1995).
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Two Applications and a Conclusion
331
Dopamine’s role in learning (White and Viaud 1991), motivation (Mogenson
et al. 1980; Cagniard et al. 2006), motor control (Barbeau 1974), and attention
(Nieoullon 2002) has been extensively studied and we recommend the interested
reader consult Wise (2004) for a thorough review.
We arrived at our claim of less than 0.001 percent of the brain’s neurons
being dopaminergic based upon the following two results. First, there are
approximately 86 ± 8 billion neurons in the adult human brain (Azevedo et
al. 2009), with only 400,000 to 600,000 dopaminergic neurons in the midbrain,
which itself contains approximately 75 percent of all DA neurons in the human
brain (Hegarty et al. 2013).
Much of the work untangling the role of DA in the brain was borne out of
studying associated neurological disorders. The loss of midbrain DA neurons is
seen as the neurological hallmark of Parkinson’s disease (Hornykiewicz 1966;
German et al. 1989), while ADHD is associated with reduced DA activity
(Olsen et al. 2021), and the connections between dysregulation of the dopamine
system and schizophrenia have continued to be studied and reﬁned for many
years (Braver et al. 1999; Howes and Kapur 2009).
Recently, Muller et al. (2021) used distributional RL to model reward-related
responses in the prefrontal cortex (PFC). This may suggest a more ubiquitous
role for distributional RL in the brain.
While the distributional TD model posits that DA neurons diﬀer in their
sensitivity to positive versus negative prediction errors, several alternative
models have been proposed to explain the observed diversity in dopaminergic
response. Kurth-Nelson and Redish (2009) propose that the brain encodes value
with a distributed representation over temporal discounts, with a multitude of
value prediction channels diﬀering in their discount factor. Such a model can
readily explain observations of purported hyperbolic discounting in humans
and animals. We also note that these neuroscientiﬁc models have themselves
inspired recent work in deep RL that combines multiple discount factors and
distributional predictions (Fedus et al. 2019).
Another line of research proposes to generalize temporal-diﬀerence learning
to prediction errors over reward-predictive features (Schultz 2016; Gardner et
al. 2018). These are motivated by ﬁndings in neuroscience, which have shown
that DA neurons may increase their ﬁring in response to unexpected changes
in sensory features, independent of anticipated reward (Takahashi et al. 2017;
Stalnaker et al. 2019). This generalization of the TD model is grounded in
the concept of successor representations (Dayan 1993), but is perhaps more
precisely characterized as successor features (Barreto et al. 2017), where the
features are themselves predictive of reward.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

332
Chapter 11
Tano et al. (2020) propose a temporal-diﬀerence learning algorithm for distri-
butional reinforcement learning which uses a variety of discount factors, reward
sensitivities, and multistep updates, allowing the population to make distribu-
tional predictions with a linear operator. The advantage of such a model is that it
is local, in the sense that there need not be any communication between the var-
ious value prediction channels, whereas distributional TD assumes signiﬁcant
communication among the DA neurons. Relatedly, Chapman and Kaelbling
(1991) consider estimating the value function by decomposing it into the total
discounted probability of individual reward outcomes.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Notation
Notation
Description
R
The set of real numbers
N, N+
The set of natural numbers including (excluding)
zero: {0, 1, 2, …}
P
The probability of one or many random variables
producing the given outcomes
P(Y)
The space of probability distributions over a set Y
Fν, F−1
ν
The cumulative distribution function (CDF) and
inverse CDF, respectively, for distribution ν
δθ
Dirac delta distribution at θ ∈R, a probability dis-
tribution that assigns probability 1 to outcome
θ
N(µ, σ2)
Normal distribution with mean µ and variance σ2
U([a, b])
Uniform distribution over [a, b], with a, b ∈R
U({a, b, …})
Uniform distribution over the set {a, b, …}
Z ∼ν
The random variable Z, with probability distribution
ν
z, Z
Capital letters generally denote random variables
and lowercase letters their realizations or expecta-
tions. Notable exceptions are V, Q, and P
x ∈X
A state x in the state space X
a ∈A
An action a in the action space A
r ∈R
A reward r from the set R
γ
Discount factor
Rt, Xt+1 ∼P(·, · | Xt, At)
Joint probability distribution of reward and next
states in terms of the current state and action
PX
Transition kernel
333
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

334
Notation
Notation
Description
PR
Reward distribution function
ξ0
Initial state distribution
x∅
A terminal state
NX, NA, NR
Size of the state, action, and reward spaces (when
ﬁnite)
π
A policy; usually stationary and Markov, mapping
states to distributions over actions
π∗
An optimal policy
k
Iteration number or index of a sample trajectory
t
Time step or time index
(Xt, At, Rt)t≥0
A trajectory of random variables for state, action,
and reward produced through interaction with a
Markov decision process
X0:t−1
A sequence of random variables
T
Length of an episode
Pπ
The distribution over trajectories induced by a
Markov decision process and a policy π
Eπ
The expectation operator for the distribution over
trajectories induced by Pπ
G
A random-variable function or random return
Var, Varπ
Variance of a distribution generally and variance
under the distribution Pπ
Vπ(x)
The value function for policy π at state x ∈X
Qπ(x, a)
The state-action value function for policy π at state
x ∈X and taking action a ∈A
Z
D= Z′
Equality in distribution of two random variables
Z, Z′
D(Z | Y)
The conditional probability distribution of a random
variable Z given Y
Gπ
The random-variable function for policy π
η
A return-distribution function
ηπ(x)
The return-distribution function for policy π at state
x ∈X
f#ν
Pushforward distribution passing distribution ν
through the function f
br,γ
Bootstrap function with reward r and discount γ
Rmin, Rmax, Vmin, Vmax
Minimum and maximum possible reward and return
within an MDP
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Notation
335
Notation
Description
Nk(x)
Number of visits to state x ∈X up to but excluding
iteration k
m
Number of particles or parameters of the distribu-
tion representation
{θ1, …, θm}
Support of a categorical distribution representation,
with θi < θj for i < j
ςm
The gap between consecutive locations for the
support of a categorical representation with m
locations
ˆVπ(x), ˆηπ(x)
An estimate of the value function or return distribu-
tion function at state x under policy π
α, αk
The step size in an update expression and the step
size used for iteration k
A ←B
Denotes updating the variable A with the contents
of variable B
Πc
The categorical projection (Sections 3.5 and 5.6)
Πq
The quantile projection (Section 5.6)
T π, T
The policy-evaluation Bellman operator and Bell-
man optimality operator, respectively
T π, T
The policy-evaluation distributional Bellman oper-
ator and distributional optimality operator, respec-
tively
OU
An operator O applied to a point U ∈M, where
(M, d) is a metric space.
∥· ∥∞
Supremum norm on a vector space
wp
p-Wasserstein distance
ℓp
ℓp distance between probability distributions
ℓ2
Cramér distance
¯d
The supremum extension of a probability metric d to
return-distribution functions, where the supremum
is taken over states
Γ(ν, ν′)
The set of couplings, joint probability distributions,
of ν, ν′ ∈P(R)
Pp(R)
The set of distributions with ﬁnite pth moments
Pd(R)
The set of distributions with ﬁnite d-distance to the
distribution δ0 and ﬁnite ﬁrst moment. Also referred
to as the ﬁnite domain of d
CVaR
Conditional value at risk
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

336
Notation
Notation
Description
F
A probability distribution representation
FE
Empirical probability distribution representation
FN
Normal probability distribution representation,
parameterized by mean and variance
FC,m
m-categorical probability distribution representation
FQ,m
m-quantile probability distribution representation
ΠF
A projection onto the probability distribution repre-
sentation F
⌊z⌋, ⌈z⌉
Floor and ceiling operations, mapping z ∈R to the
nearest integer that is less or equal (ﬂoor), or greater
or equal (ceiling), than z
d
A probability metric, typically used for the purposes
of contraction analysis
Lτ(θ)
Quantile regression loss function for target thresh-
old τ ∈(0, 1) and location estimate θ ∈R
1{u}
An indicator function that takes the value 1 when u
is true and 0 otherwise; also 1{·}
J(π)
Objective function for a control problem
G
Greedy policy operator, produces a policy that is
greedy with respect to a given action-value function
T G, T G
The Bellman and distributional Bellman optimality
operators derived from greedy selection rule G
Jρ(π)
A risk-sensitive control objective function, with risk
measure ρ
ψ
A statistical functional or sketch
ξπ
Steady-state distribution under policy π
φ(x)
State representation for state x, a mapping φ : X →
Rn
Πφ,ξ
Projection onto the linear subspace generated by φ
with state weighting ξ
M (R)
Space of signed probability measures over the reals
ℓξ,2
Weighted Cramér distance over return-distribution
functions, with state weighting given by ξ
Πφ,ξ,ℓ2
Projection onto the linear subspace generated by φ,
minimizing the ℓξ,2 distance
L
Loss function
Hκ
The Huber loss with threshold κ ≥0
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

References
Achab, Mastane. 2020. Ranking and risk-aware reinforcement learning. PhD diss.,
Institut Polytechnique de Paris.
Agarwal, Rishabh, Dale Schuurmans, and Mohammad Norouzi. 2020. An optimistic
perspective on oﬄine reinforcement learning. In Proceedings of the International
Conference on Machine Learning.
Agarwal, Rishabh, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G.
Bellemare. 2021. Deep reinforcement learning at the edge of the statistical precipice. In
Advances in Neural Information Processing Systems.
Aigner, D. J., Takeshi Amemiya, and Dale J. Poirier. 1976. On the estimation of pro-
duction frontiers: Maximum likelihood estimation of the parameters of a discontinuous
density function. International Economic Review 17 (2): 377–396.
Aldous, David J., and Antar Bandyopadhyay. 2005. A survey of max-type recursive
distributional equations. The Annals of Applied Probability 15 (2): 1047–1110.
Alsmeyer, Gerold. 2012. Random recursive equations and their distributional ﬁxed
points. Unpublished manuscript.
Altman, Eitan. 1999. Constrained Markov decision processes. Vol. 7. CRC Press.
Ambrosio, Luigi, Nicola Gigli, and Giuseppe Savaré. 2005. Gradient ﬂows: In metric
spaces and in the space of probability measures. Springer Science & Business Media.
Amortila, Philip, Marc G. Bellemare, Prakash Panangaden, and Doina Precup. 2019.
Temporally extended metrics for Markov decision processes. In SafeAI: AAAI Workshop
on Artiﬁcial Intelligence Safety.
Amortila, Philip, Doina Precup, Prakash Panangaden, and Marc G. Bellemare. 2020.
A distributional analysis of sampling-based reinforcement learning algorithms. In
Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics.
Arjovsky, Martin, Soumith Chintala, and Léon Bottou. 2017. Wasserstein GAN. In
Proceedings of the International Conference on Machine Learning.
Artzner, Philippe, Freddy Delbaen, Jean-Marc Eber, and David Heath. 1999. Coherent
measures of risk. Mathematical Finance 9 (3): 203–228.
337
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

338
References
Artzner, Philippe, Freddy Delbaen, Jean-Marc Eber, David Heath, and Hyejin Ku. 2007.
Coherent multiperiod risk adjusted values and Bellman’s principle. Annals of Operations
Research 152 (1): 5–22.
Arulkumaran, Kai, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath.
2017. A brief survey of deep reinforcement learning. IEEE Signal Processing Magazine,
Special Issue on Deep Learning for Image Understanding.
Auer, Peter, Mark Herbster, and Manfred K. Warmuth. 1995. Exponentially many local
minima for single neurons. In Advances in Neural Information Processing Systems.
Azar, Mohammad Gheshlaghi, Rémi Munos, Mohammad Ghavamzadeh, and Hilbert
J. Kappen. 2011. Speedy Q-learning. In Advances in Neural Information Processing
Systems.
Azar, Mohammad Gheshlaghi, Rémi Munos, and Hilbert J. Kappen. 2012. On the sample
complexity of reinforcement learning with a generative model. In Proceedings of the
International Conference on Machine Learning.
Azar, Mohammad Gheshlaghi, Rémi Munos, and Hilbert J. Kappen. 2013. Minimax
PAC bounds on the sample complexity of reinforcement learning with a generative
model. Machine Learning 91 (3): 325–349.
Azar, Mohammad Gheshlaghi, Ian Osband, and Rémi Munos. 2017. Minimax regret
bounds for reinforcement learning. In Proceedings of the International Conference on
Machine Learning.
Azevedo, Frederico A. C., Ludmila R. B. Carvalho, Lea T. Grinberg, José Marcelo
Farfel, Renata E. L. Ferretti, Renata E. P. Leite, Wilson Jacob Filho, Roberto Lent, and
Suzana Herculano-Houzel. 2009. Equal numbers of neuronal and nonneuronal cells
make the human brain an isometrically scaled-up primate brain. Journal of Comparative
Neurology 513 (5): 532–541.
Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine trans-
lation by jointly learning to align and translate. In Proceedings of the International
Conference on Learning Representations.
Baird, Leemon C. 1995. Residual algorithms: Reinforcement learning with function
approximation. In Proceedings of the International Conference on Machine Learning.
Baird, Leemon C. 1999. Reinforcement learning through gradient descent. PhD diss.,
Carnegie Mellon University.
Banach, Stefan. 1922. Sur les opérations dans les ensembles abstraits et leur application
aux équations intégrales. Fundamenta Mathematicae 3 (1): 133–181.
Banino, Andrea, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap,
Piotr Mirowski, Alexander Pritzel, Martin J. Chadwick, Thomas Degris, Joseph Modayil,
Greg Wayne, Hubert Soyer, Fabio Viola, Brian Zhang, Ross Goroshin, Neil Rabinowitz,
Razvan Pascanu, Charlie Beattie, Stig Petersen, Amir Sadik, Stephen Gaﬀney, Helen
King, Koray Kavukcuoglu, Demis Hassabis, Raia Hadsell, and Dharshan Kumaran. 2018.
Vector-based navigation using grid-like representations in artiﬁcial agents. Nature 557
(7705): 429–433.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

References
339
Barbeau, André. 1974. Drugs aﬀecting movement disorders. Annual Review of
Pharmacology 14 (1): 91–113.
Bard, Nolan, Jakob N. Foerster, Sarath Chandar, Neil Burch, Marc Lanctot, H. Francis
Song, Emilio Parisotto, Vincent Dumoulin, Subhodeep Moitra, Edward Hughes, Iain
Dunning, Shibl Mourad, Hugo Larochelle, Marc G. Bellemare, and Michael Bowling.
2020. The Hanabi challenge: A new frontier for AI research. Artiﬁcial Intelligence
280:103216.
Barnard, Etienne. 1993. Temporal-diﬀerence methods and Markov models. IEEE
Transactions on Systems, Man, and Cybernetics 23 (2): 357–365.
Barreto, André, Will Dabney, Rémi Munos, Jonathan J. Hunt, Tom Schaul, Hado van
Hasselt, and David Silver. 2017. Successor features for transfer in reinforcement learning.
In Advances in Neural Information Processing Systems.
Barth-Maron, Gabriel, Matthew W. Hoﬀman, David Budden, Will Dabney, Dan Horgan,
Dhruva TB, Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. 2018. Distributed dis-
tributional deterministic policy gradients. In Proceedings of the International Conference
on Learning Representations.
Barto, Andrew G., Steven J. Bradtke, and Satinder P. Singh. 1995. Learning to act using
real-time dynamic programming. Artiﬁcial Intelligence 72 (1): 81–138.
Barto, Andrew G., Richard S. Sutton, and Charles W. Anderson. 1983. Neuronlike
adaptive elements that can solve diﬃcult learning control problems. IEEE Transactions
on Systems, Man, and Cybernetics 13 (5): 834–846.
Bäuerle, Nicole, and Jonathan Ott. 2011. Markov decision processes with average-value-
at-risk criteria. Mathematical Methods of Operations Research 74 (3): 361–379.
Beattie, Charles, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright,
Heinrich Küttler, Andrew Lefrancq, Simon Green, Víctor Valdés, Amir Sadik, Julian
Schrittwieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton,
Stephen Gaﬀney, Helen King, Demis Hassabis, Shane Legg, and Stig Petersen. 2016.
DeepMind Lab. arXiv preprint arXiv:1612.03801.
Bellemare, Marc G., Salvatore Candido, Pablo Samuel Castro, Jun Gong, Marlos C.
Machado, Subhodeep Moitra, Sameera S. Ponda, and Ziyu Wang. 2020. Autonomous
navigation of stratospheric balloons using reinforcement learning. Nature 588 (7836):
77–82.
Bellemare, Marc G., Will Dabney, Robert Dadashi, Adrien Ali Taiga, Pablo Samuel
Castro, Nicolas Le Roux, Dale Schuurmans, Tor Lattimore, and Clare Lyle. 2019a. A
geometric perspective on optimal representations for reinforcement learning. In Advances
in Neural Information Processing Systems.
Bellemare, Marc G., Will Dabney, and Rémi Munos. 2017a. A distributional perspective
on reinforcement learning. In Proceedings of the International Conference on Machine
Learning.
Bellemare, Marc G., Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshmi-
narayanan, Stephan Hoyer, and Rémi Munos. 2017b. The Cramer distance as a solution
to biased Wasserstein gradients. arXiv preprint arXiv:1705.10743.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

340
References
Bellemare, Marc G., Yavar Naddaf, Joel Veness, and Michael Bowling. 2013a. The
Arcade Learning Environment: An evaluation platform for general agents. Journal of
Artiﬁcial Intelligence Research 47 (June): 253–279.
Bellemare, Marc G., Yavar Naddaf, Joel Veness, and Michael Bowling. 2015. The
Arcade Learning Environment: An evaluation platform for general agents, extended
abstract. In European Workshop on Reinforcement Learning.
Bellemare, Marc G., Georg Ostrovski, Arthur Guez, Philip S. Thomas, and Rémi
Munos. 2016. Increasing the action gap: New operators for reinforcement learning.
In Proceedings of the AAAI Conference on Artiﬁcial Intelligence.
Bellemare, Marc G., Nicolas Le Roux, Pablo Samuel Castro, and Subhodeep Moitra.
2019b. Distributional reinforcement learning with linear function approximation. In
Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics.
Bellemare, Marc G., Joel Veness, and Michael Bowling. 2012a. Investigating contingency
awareness using Atari 2600 games. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence.
Bellemare, Marc G., Joel Veness, and Michael Bowling. 2012b. Sketch-based linear
value function approximation. In Advances in Neural Information Processing Systems.
Bellemare, Marc G., Joel Veness, and Michael Bowling. 2013b. Bayesian learning of
recursively factored environments. In Proceedings of the International Conference on
Machine Learning.
Bellini, Fabio, and Elena Di Bernardino. 2017. Risk management with expectiles. The
European Journal of Finance 23 (6): 487–506.
Bellini, Fabio, Bernhard Klar, Alfred Müller, and Emanuela Rosazza Gianin. 2014.
Generalized quantiles as risk measures. Insurance: Mathematics and Economics 54:41–
48.
Bellman, Richard E. 1957a. A Markovian decision process. Journal of Mathematics and
Mechanics 6 (5): 679–684.
Bellman, Richard E. 1957b. Dynamic programming. Dover Publications.
Benveniste, Albert, Michel Métivier, and Pierre Priouret. 2012. Adaptive algorithms and
stochastic approximations. Springer Science & Business Media.
Bernstein, Daniel S., Robert Givan, Neil Immerman, and Shlomo Zilberstein. 2002.
The complexity of decentralized control of Markov decision processes. Mathematics of
Operations Research 27 (4): 819–840.
Bertsekas, Dimitri P. 1994. Generic rank-one corrections for value iteration in
Markovian decision problems. Technical report. Massachusetts Institute of Technology.
Bertsekas, Dimitri P. 1995. A counterexample to temporal diﬀerences learning. Neural
Computation 7 (2): 270–279.
Bertsekas, Dimitri P. 2011. Approximate policy iteration: A survey and some new
methods. Journal of Control Theory and Applications 9 (3): 310–335.
Bertsekas, Dimitri P. 2012. Dynamic programming and optimal control. 4th ed. Vol. 2.
Athena Scientiﬁc.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

References
341
Bertsekas, Dimitri P., and Sergey Ioﬀe. 1996. Temporal diﬀerences-based policy itera-
tion and applications in neuro-dynamic programming. Technical report. Massachusetts
Institute of Technology.
Bertsekas, Dimitri P., and John N. Tsitsiklis. 1996. Neuro-dynamic programming. Athena
Scientiﬁc.
Bhandari, Jalaj, and Daniel Russo. 2021. On the linear convergence of policy gradient
methods for ﬁnite MDPs. In Proceedings of the International Conference on Artiﬁcial
Intelligence and Statistics.
Bhonker, Nadav, Shai Rozenberg, and Itay Hubara. 2017. Playing SNES in the Retro
Learning Environment. In Proceedings of the International Conference on Learning
Representations.
Bickel, Peter J., and David A. Freedman. 1981. Some asymptotic theory for the bootstrap.
The Annals of Statistics 9 (6): 1196–1217.
Billingsley, Patrick. 2012. Probability and measure. 4th ed. John Wiley & Sons.
Bishop, Christopher M. 2006. Pattern recognition and machine learning. Springer.
Bobkov, Sergey, and Michel Ledoux. 2019. One-dimensional empirical measures, order
statistics, and Kantorovich transport distances. American Mathematical Society.
Bodnar, Cristian, Adrian Li, Karol Hausman, Peter Pastor, and Mrinal Kalakrishnan.
2020. Quantile QT-OPT for risk-aware vision-based robotic grasping. In Proceedings of
Robotics: Science and Systems.
Borkar, Vivek S. 1997. Stochastic approximation with two time scales. Systems &
Control Letters 29 (5): 291–294.
Borkar, Vivek S. 2008. Stochastic approximation: A dynamical systems viewpoint.
Cambridge University Press.
Borkar, Vivek S., and Sean P. Meyn. 2000. The ODE method for convergence of
stochastic approximation and reinforcement learning. SIAM Journal on Control and
Optimization 38 (2): 447–469.
Bottou, Léon. 1998. Online learning and stochastic approximations. On-line Learning in
Neural Networks 17 (9): 142.
Boutilier, Craig. 1996. Planning, learning and coordination in multiagent decision
processes. In Proceedings of the Conference on Theoretical Aspects of Rationality and
Knowledge.
Bowling, Michael, and Manuela Veloso. 2002. Multiagent learning using a variable
learning rate. Artiﬁcial Intelligence 136 (2): 215–250.
Boyan, Justin, and Andrew W. Moore. 1995. Generalization in reinforcement learning:
Safely approximating the value function. In Advances in Neural Information Processing
Systems.
Boyd, Stephen, and Lieven Vandenberghe. 2004. Convex optimization. Cambridge
University Press.
Bradtke, Steven J., and Andrew G. Barto. 1996. Linear least-squares algorithms for
temporal diﬀerence learning. Machine Learning 22 (1): 33–57.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

342
References
Braver, Todd S., Deanna M. Barch, and Jonathan D. Cohen. 1999. Cognition and
control in schizophrenia: A computational model of dopamine and prefrontal function.
Biological Psychiatry 46 (3): 312–328.
Brooks, Steve, Andrew Gelman, Galin Jones, and Xiao-Li Meng. 2011. Handbook of
Markov chain Monte Carlo. CRC Press.
Brown, Daniel, Scott Niekum, and Marek Petrik. 2020. Bayesian robust optimization
for imitation learning. In Advances in Neural Information Processing Systems.
Browne, Cameron B., Edward Powley, Daniel Whitehouse, Simon M. Lucas, Peter
I. Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samoth-
rakis, and Simon Colton. 2012. A survey of Monte Carlo tree search methods. IEEE
Transactions on Computational Intelligence and AI in Games 4 (1): 1–43.
Cabi, Serkan, Sergio Gómez Colmenarejo, Alexander Novikov, Ksenia Konyushkova,
Scott Reed, Rae Jeong, Konrad Zolna, Yusuf Aytar, David Budden, Mel Vecerik, Oleg
Sushkov, David Barker, Jonathan Scholz, Misha Denil, Nando de Freitas, and Ziyu
Wang. 2020. Scaling data-driven robotics with reward sketching and batch reinforcement
learning. In Proceedings of Robotics: Science and Systems.
Cagniard, Barbara, Peter D. Balsam, Daniela Brunner, and Xiaoxi Zhuang. 2006. Mice
with chronically elevated dopamine exhibit enhanced motivation, but not learning, for a
food reward. Neuropsychopharmacology 31 (7): 1362–1370.
Carpin, Stefano, Yinlam Chow, and Marco Pavone. 2016. Risk aversion in ﬁnite Markov
decision processes using total cost criteria and average value at risk. In Proceedings of
the IEEE International Conference on Robotics and Automation.
Castro, Pablo S., Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G.
Bellemare. 2018. Dopamine: A research framework for deep reinforcement learning.
arXiv preprint arXiv:1812.06110.
Ceron, Johan Samir Obando, and Pablo Samuel Castro. 2021. Revisiting Rainbow:
Promoting more insightful and inclusive deep reinforcement learning research. In
Proceedings of the International Conference on Machine Learning.
Chandak, Yash, Scott Niekum, Bruno Castro da Silva, Erik Learned-Miller, Emma
Brunskill, and Philip S. Thomas. 2021. Universal oﬀ-policy evaluation. In Advances in
Neural Information Processing Systems.
Chapman, David, and Leslie Pack Kaelbling. 1991. Input generalization in delayed
reinforcement learning: An algorithm and performance comparisons. In Proceedings of
the International Joint Conference on Artiﬁcial Intelligence.
Chen, Jinglin, and Nan Jiang. 2019. Information-theoretic considerations in batch
reinforcement learning. In Proceedings of the International Conference on Machine
Learning.
Chopin, Nicolas, and Omiros Papaspiliopoulos. 2020. An introduction to sequential
Monte Carlo. Springer.
Chow, Yinlam. 2017. Risk-sensitive and data-driven sequential decision making. PhD
diss., Stanford University.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

References
343
Chow, Yinlam, and Mohammad Ghavamzadeh. 2014. Algorithms for CVaR optimization
in MDPs. In Advances in Neural Information Processing Systems.
Chow, Yinlam, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. 2018.
Risk-constrained reinforcement learning with percentile risk criteria. Journal of Machine
Learning Research 18 (1): 6070–6120.
Chow, Yinlam, Aviv Tamar, Shie Mannor, and Marco Pavone. 2015. Risk-sensitive
and robust decision-making: A CVaR optimization approach. In Advances in Neural
Information Processing Systems.
Chung, Kun-Jen, and Matthew J. Sobel. 1987. Discounted MDPs: Distribution functions
and exponential utility maximization. SIAM Journal on Control and Optimization 25
(1): 49–62.
Chung, Wesley, Somjit Nath, Ajin Joseph, and Martha White. 2018. Two-timescale
networks for nonlinear value function approximation. In Proceedings of the International
Conference on Learning Representations.
Claus, Caroline, and Craig Boutilier. 1998. The dynamics of reinforcement learning in
cooperative multiagent systems. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence.
Clements, William R., Benoit-Marie Robaglia, Bastien Van Delft, Reda Bahi Slaoui, and
Sebastien Toth. 2020. Estimating risk and uncertainty in deep reinforcement learning.
In Workshop on Uncertainty and Robustness in Deep Learning at the International
Conference on Machine Learning.
Cobbe, Karl, Chris Hesse, Jacob Hilton, and John Schulman. 2020. Leveraging procedu-
ral generation to benchmark reinforcement learning. In Proceedings of the International
Conference on Machine Learning.
Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Cliﬀord Stein. 2001.
Introduction to algorithms. MIT Press.
Cormode, G., and S. Muthukrishnan. 2005. An improved data stream summary: The
count-min sketch and its applications. Journal of Algorithms 55 (1): 58–75.
Cuturi, Marco. 2013. Sinkhorn distances: Lightspeed computation of optimal transport.
In Advances in Neural Information Processing Systems.
Da Silva, Felipe Leno, Anna Helena Reali Costa, and Peter Stone. 2019. Distributional
reinforcement learning applied to robot soccer simulation. In Adaptive and Learning
Agents Workshop at the International Conference on Autonomous Agents and Multiagent
Systems.
Dabney, Will, André Barreto, Mark Rowland, Robert Dadashi, John Quan, Marc G.
Bellemare, and David Silver. 2020a. The value-improvement path: Towards better
representations for reinforcement learning. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence.
Dabney, Will, Zeb Kurth-Nelson, Naoshige Uchida, Clara Kwon Starkweather, Demis
Hassabis, Rémi Munos, and Matthew Botvinick. 2020b. A distributional code for value
in dopamine-based reinforcement learning. Nature 577 (7792): 671–675.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

344
References
Dabney, Will, Georg Ostrovski, David Silver, and Rémi Munos. 2018a. Implicit quantile
networks for distributional reinforcement learning. In Proceedings of the International
Conference on Machine Learning.
Dabney, Will, Mark Rowland, Marc G. Bellemare, and Rémi Munos. 2018b. Distribu-
tional reinforcement learning with quantile regression. In AAAI Conference on Artiﬁcial
Intelligence.
Dai, Bo, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and
Le Song. 2018. SBEED: Convergent reinforcement learning with nonlinear function
approximation. In Proceedings of the International Conference on Machine Learning.
Daw, Nathaniel D. 2003. Reinforcement learning models of the dopamine system and
their behavioral implications. Carnegie Mellon University.
Daw, Nathaniel D., and Philippe N. Tobler. 2014. Value learning through reinforcement:
The basics of dopamine and reinforcement learning. In Neuroeconomics, edited by
Paul W. Glimcher and Ernst Fehr, 283–298. Academic Press.
Dayan, Peter. 1992. The convergence of TD(λ) for general λ. Machine Learning 8 (3–4):
341–362.
Dayan, Peter. 1993. Improving generalization for temporal diﬀerence learning: The
successor representation. Neural Computation 5 (4): 613–624.
Dayan, Peter, and Terrence J. Sejnowski. 1994. TD(λ) converges with probability 1.
Machine Learning 14 (3): 295–301.
Dearden, Richard, Nir Friedman, and Stuart Russell. 1998. Bayesian Q-learning. In
Proceedings of the AAAI Conference on Artiﬁcial Intelligence.
Degrave, Jonas, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey,
Francesco Carpanese, Timo Ewalds, Roland Hafner, Abbas Abdolmaleki, Diego de
las Casas, Craig Donner, Leslie Fritz, Cristian Galperti, Andrea Huber, James Keeling,
Maria Tsimpoukelli, Jackie Kay, Antoine Merle, Jean-Marc Moret, Seb Noury, Federico
Pesamosca, David Pfau, Olivier Sauter, Cristian Sommariva, Stefano Coda, Basil Duval,
Ambrogio Fasoli, Pushmeet Kohli, Koray Kavukcuoglu, Demis Hassabis, and Martin
Riedmiller. 2022. Magnetic control of tokamak plasmas through deep reinforcement
learning. Nature 602:414–419.
Delage, Erick, and Shie Mannor. 2010. Percentile optimization for Markov decision
processes with parameter uncertainty. Operations Research 58 (1): 203–213.
Denardo, Eric V., and Uriel G. Rothblum. 1979. Optimal stopping, exponential utility,
and linear programming. Mathematical Programming 16 (1): 228–244.
Derman, Cyrus. 1970. Finite state Markovian decision processes. Academic Press.
Diaconis, Persi, and David Freedman. 1999. Iterated random functions. SIAM Review 41
(1): 45–76.
Doan, Thang, Bogdan Mazoure, and Clare Lyle. 2018. GAN Q-learning. arXiv preprint
arXiv:1805.04874.
Doob, J. L. 1994. Measure theory. Springer.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

References
345
Doucet, Arnaud, Nando De Freitas, and Neil Gordon. 2001. Sequential Monte Carlo
methods in practice. Springer.
Doucet, Arnaud, and Adam M. Johansen. 2011. A tutorial on particle ﬁltering and
smoothing: Fifteen years later. In The Oxford handbook of nonlinear ﬁltering, edited by
Dan Crisan and Boris Rozovskii. Oxford University Press.
Duan, Jingliang, Yang Guan, Shengbo Eben Li, Yangang Ren, Qi Sun, and Bo Cheng.
2021. Distributional soft actor-critic: Oﬀ-policy reinforcement learning for addressing
value estimation errors. IEEE Transactions on Neural Networks and Learning Systems.
Dvoretzky, Aryeh. 1956. On stochastic approximation. In Proceedings of the Berkeley
Symposium on Mathematical Statistics and Probability, 39–55.
Dvoretzky, Aryeh, Jack Kiefer, and Jacob Wolfowitz. 1956. Asymptotic minimax char-
acter of the sample distribution function and of the classical multinomial estimator. The
Annals of Mathematical Statistics 27 (3): 642–669.
Engel, Yaakov, Shie Mannor, and Ron Meir. 2003. Bayes meets Bellman: The Gaussian
process approach to temporal diﬀerence learning. In Proceedings of the International
Conference on Machine Learning.
Engel, Yaakov, Shie Mannor, and Ron Meir. 2007. Bayesian reinforcement learning
with Gaussian process temporal diﬀerence methods. Unpublished manuscript.
Engert, Martin. 1970. Finite dimensional translation invariant subspaces. Paciﬁc Journal
of Mathematics 32 (2): 333–343.
Ernst, Damien, Pierre Geurts, and Louis Wehenkel. 2005. Tree-based batch mode
reinforcement learning. Journal of Machine Learning Research 6:503–556.
Eshel, Neir, Michael Bukwich, Vinod Rao, Vivian Hemmelder, Ju Tian, and Naoshige
Uchida. 2015. Arithmetic and local circuitry underlying dopamine prediction errors.
Nature 525 (7568): 243–246.
Even-Dar, Eyal, and Yishay Mansour. 2003. Learning rates for Q-learning. Journal of
Machine Learning Research 5 (1): 1–25.
Farahmand, Amir-massoud. 2011. Action-gap phenomenon in reinforcement learning.
In Advances in Neural Information Processing Systems.
Farahmand, Amir-massoud. 2019. Value function in frequency domain and the char-
acteristic value iteration algorithm. In Advances in Neural Information Processing
Systems.
Fedus, William, Carles Gelada, Yoshua Bengio, Marc G. Bellemare, and Hugo
Larochelle. 2019. Hyperbolic discounting and learning over multiple horizons. In
Multi-Disciplinary Conference on Reinforcement Learning and Decision-Making.
Feinberg, Eugene A. 2000. Constrained discounted Markov decision processes and
Hamiltonian cycles. Mathematics of Operations Research 25 (1): 130–140.
Ferns, Norm, Prakash Panangaden, and Doina Precup. 2004. Metrics for ﬁnite Markov
decision processes. In Proceedings of the Conference on Uncertainty in Artiﬁcial
Intelligence.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

346
References
Ferns, Norman, and Doina Precup. 2014. Bisimulation metrics are optimal value
functions. In Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence.
Filar, Jerzy A., Dmitry Krass, and Keith W. Ross. 1995. Percentile performance crite-
ria for limiting average Markov decision processes. IEEE Transactions on Automatic
Control 40 (1): 2–10.
Fortunato, Meire, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband,
Alex Graves, Vlad Mnih, Rémi Munos, Demis Hassabis, Olivier Pietquin, Charles
Blundell, and Shane Legg. 2018. Noisy networks for exploration. In Proceedings of the
International Conference on Learning Representations.
François-Lavet, Vincent, Peter Henderson, Riashat Islam, Marc G. Bellemare, and Joelle
Pineau. 2018. An introduction to deep reinforcement learning. Foundations and Trends R⃝
in Machine Learning 11 (3–4): 219–354.
Freirich, Dror, Tzahi Shimkin, Ron Meir, and Aviv Tamar. 2019. Distributional multi-
variate policy evaluation and exploration with the Bellman GAN. In Proceedings of the
International Conference on Machine Learning.
Gardner, Matthew P. H., Geoﬀrey Schoenbaum, and Samuel J. Gershman. 2018. Rethink-
ing dopamine as generalized prediction error. Proceedings of the Royal Society B 285
(1891): 20181645.
German, Dwight C., Kebreten Manaye, Wade K. Smith, Donald J. Woodward, and Clif-
ford B. Saper. 1989. Midbrain dopaminergic cell loss in Parkinson’s disease: Computer
visualization. Annals of Neurology 26 (4): 507–514.
Ghavamzadeh, Mohammad, Shie Mannor, Joelle Pineau, and Aviv Tamar. 2015.
Bayesian reinforcement learning: A survey. Foundations and Trends R⃝in Machine
Learning 8 (5–6): 359–483.
Ghosh, Dibya, and Marc G. Bellemare. 2020. Representations for stable oﬀ-policy
reinforcement learning. In Proceedings of the International Conference on Machine
Learning.
Ghosh, Dibya, Marlos C. Machado, and Nicolas Le Roux. 2020. An operator view of
policy gradient methods. In Advances in Neural Information Processing Systems.
Gilbert, Hugo, Paul Weng, and Yan Xu. 2017. Optimizing quantiles in preference-
based Markov decision processes. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence.
Glimcher, Paul W. 2011. Understanding dopamine and reinforcement learning: The
dopamine reward prediction error hypothesis. Proceedings of the National Academy of
Sciences 108 (Suppl. 3): 15647–15654.
Goodfellow, Ian, Aaron Courville, and Yoshua Bengio. 2016. Deep learning. MIT Press.
Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets.
In Advances in Neural Information Processing Systems.
Gordon, Geoﬀrey. 1995. Stable function approximation in dynamic programming. In
Proceedings of the International Conference on Machine Learning.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

References
347
Gordon, Neil J., David J. Salmond, and Adrian F. M. Smith. 1993. Novel approach
to nonlinear/non-Gaussian Bayesian state estimation. IEE Proceedings F (Radar and
Signal Processing) 140 (2): 107–113.
Graesser, Laura, and Wah Loon Keng. 2019. Foundations of deep reinforcement learning:
Theory and practice in Python. Addison-Wesley Professional.
Gretton, Arthur, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, and Alexan-
der Smola. 2012. A kernel two-sample test. Journal of Machine Learning Research 13
(1): 723–773.
Grünewälder, Steﬀen, and Klaus Obermayer. 2011. The optimal unbiased value estimator
and its relation to LSTD, TD and MC. Machine Learning 83 (3): 289–330.
Gruslys, Audrunas, Will Dabney, Mohammad Gheshlaghi Azar, Bilal Piot, Marc Belle-
mare, and Rémi Munos. 2018. The Reactor: A fast and sample-eﬃcient actor-critic agent
for reinforcement learning. In Proceedings of the International Conference on Learning
Representations.
Guo, Zhaohan Daniel, Bernardo Avila Pires, Bilal Piot, Jean-Bastien Grill, Florent Altché,
Rémi Munos, and Mohammad Gheshlaghi Azar. 2020. Bootstrap latent-predictive
representations for multitask reinforcement learning. In Proceedings of the International
Conference on Machine Learning.
Gurvits, Leonid, Long-Ji Lin, and Stephen José Hanson. 1994. Incremental learning
of evaluation functions for absorbing Markov chains: New methods and theorems.
Technical report. Siemens Corporate Research.
Harmon, Mance E., and Leemon C. Baird. 1996. A response to Bertsekas’ “A
counterexample to temporal-diﬀerences learning”. Technical report. Wright Laboratory.
Haskell, William B., and Rahul Jain. 2015. A convex analytic approach to risk-aware
Markov decision processes. SIAM Journal on Control and Optimization 53 (3): 1569–
1598.
Hegarty, Shane V., Aideen M. Sullivan, and Gerard W. O’Keeﬀe. 2013. Midbrain
dopaminergic neurons: A review of the molecular circuitry that regulates their
development. Developmental Biology 379 (2): 123–138.
Heger, Matthias. 1994. Consideration of risk in reinforcement learning. In Proceedings
of the International Conference on Machine Learning.
Henderson, Peter, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and
David Meger. 2018. Deep reinforcement learning that matters. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence.
Hessel, Matteo, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will
Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. 2018. Rainbow:
Combining improvements in deep reinforcement learning. In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence.
Hochreiter, Sepp, and Jürgen Schmidhuber. 1997. Long short-term memory. Neural
Computation 9 (8): 1735–1780.
Hornykiewicz, Oleh. 1966. Dopamine (3-hydroxytyramine) and brain function. Pharma-
cological Reviews 18 (2): 925–964.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

348
References
Howard, R. 1960. Dynamic programming and Markov processes. MIT Press.
Howard, Ronald A., and James E. Matheson. 1972. Risk-sensitive Markov decision
processes. Management Science 18 (7): 356–369.
Howes, Oliver D., and Shitij Kapur. 2009. The dopamine hypothesis of schizophrenia:
Version III—the ﬁnal common pathway. Schizophrenia Bulletin 35 (3): 549–562.
Hutter, Marcus. 2005. Universal artiﬁcial intelligence: Sequential decisions based on
algorithmic probability. Springer.
Imani, Ehsan, and Martha White. 2018. Improving regression performance with
distributional losses. In Proceedings of the International Conference on Machine
Learning.
Jaakkola, Tommi, Michael I. Jordan, and Satinder P. Singh. 1994. On the convergence
of stochastic iterative dynamic programming algorithms. Neural Computation 6 (6):
1185–1201.
Jaderberg, Max, Volodymyr Mnih, Wojciech M. Czarnecki, Tom Schaul, Joel Z. Leibo,
David Silver, and Koray Kavukcuoglu. 2017. Reinforcement learning with unsuper-
vised auxiliary tasks. In Proceedings of the International Conference on Learning
Representations.
Janner, Michael, Igor Mordatch, and Sergey Levine. 2020. Generative temporal dif-
ference learning for inﬁnite-horizon prediction. In Advances in Neural Information
Processing Systems.
Jaquette, Stratton C. 1973. Markov decision processes with a new optimality criterion:
Discrete time. The Annals of Statistics 1 (3): 496–505.
Jaquette, Stratton C. 1976. A utility criterion for Markov decision processes. Manage-
ment Science 23 (1): 43–49.
Jessen, Børge, and Aurel Wintner. 1935. Distribution functions and the Riemann zeta
function. Transactions of the American Mathematical Society 38 (1): 48–88.
Jiang, Daniel R., and Warren B. Powell. 2018. Risk-averse approximate dynamic pro-
gramming with quantile-based risk measures. Mathematics of Operations Research 43
(2): 554–579.
Jordan, Richard, David Kinderlehrer, and Felix Otto. 1998. The variational formulation
of the Fokker–Planck equation. SIAM Journal on Mathematical Analysis 29 (1): 1–17.
Kaelbling, Leslie Pack, Michael L. Littman, and Anthony R. Cassandra. 1998. Planning
and acting in partially observable stochastic domains. Artiﬁcial Intelligence 101:99–134.
Kamin, Leon J. 1968. "Attention like" processes in classical conditioning. In Miami
Symposium on the Prediction of Behavior: Aversive Stimulation, 9–31.
Kantorovich, Leonid V. 1942. On the translocation of masses. Proceedings of the USSR
Academy of Sciences 37 (7–8): 227–229.
Kapetanakis, Spiros, and Daniel Kudenko. 2002. Reinforcement learning of coordination
in cooperative multi-agent systems. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

References
349
Kartal, Bilal, Pablo Hernandez-Leal, and Matthew E. Taylor. 2019. Terminal predic-
tion as an auxiliary task for deep reinforcement learning. In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence and Interactive Digital Entertainment.
Kempka, Michał, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech
Ja´skowski. 2016. Vizdoom: A Doom-based AI research platform for visual reinforce-
ment learning. In 2016 IEEE Conference on Computational Intelligence and Games,
1–8.
Keramati, Ramtin, Christoph Dann, Alex Tamkin, and Emma Brunskill. 2020. Being
optimistic to be conservative: Quickly learning a CVaR policy. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence.
Kingma, Diederik, and Jimmy Ba. 2015. Adam: A method for stochastic optimization.
In Proceedings of the International Conference on Learning Representations.
Koenker, Roger. 2005. Quantile regression. Cambridge University Press.
Koenker, Roger, and Gilbert Bassett Jr. 1978. Regression quantiles. Econometrica 46
(1): 33–50.
Kolter, J. Zico. 2011. The ﬁxed points of oﬀ-policy TD. In Advances in Neural
Information Processing Systems.
Konidaris, George D., Sarah Osentoski, and Philip S. Thomas. 2011. Value function
approximation in reinforcement learning using the Fourier basis. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence.
Kuan, Chung-Ming, Jin-Huei Yeh, and Yu-Chin Hsu. 2009. Assessing value at risk with
care, the conditional autoregressive expectile models. Journal of Econometrics 150 (2):
261–270.
Kuhn, Harold W. 1950. A simpliﬁed two-person poker. Contributions to the Theory of
Games 1:97–103.
Kurth-Nelson, Zeb, and A. David Redish. 2009. Temporal-diﬀerence reinforcement
learning with distributed representations. PLoS One 4 (10): e7362.
Kusher, Harold, and Dean Clark. 1978. Stochastic approximation methods for con-
strained and unconstrained systems. Springer.
Kushner, Harold, and G. George Yin. 2003. Stochastic approximation and recursive
algorithms and applications. Springer Science & Business Media.
Kuznetsov, Arsenii, Pavel Shvechikov, Alexander Grishin, and Dmitry Vetrov. 2020.
Controlling overestimation bias with truncated mixture of continuous distributional
quantile critics. In Proceedings of the International Conference on Machine Learning.
Lagoudakis, Michail G., and Ronald Parr. 2003. Least-squares policy iteration. Journal
of Machine Learning Research 4:1107–1149.
Lample, Guillaume, and Devendra Singh Chaplot. 2017. Playing FPS games with deep
reinforcement learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence.
Laskin, Michael, Aravind Srinivas, and Pieter Abbeel. 2020. CURL: Contrastive unsu-
pervised representations for reinforcement learning. In Proceedings of the International
Conference on Machine Learning.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

350
References
Lattimore, Tor, and Marcus Hutter. 2012. PAC bounds for discounted MDPs. In
Proceedings of the International Conference on Algorithmic Learning Theory.
Lattimore, Tor, and Csaba Szepesvári. 2020. Bandit algorithms. Cambridge University
Press.
Lauer, Martin, and Martin Riedmiller. 2000. An algorithm for distributed reinforce-
ment learning in cooperative multi-agent systems. In Proceedings of the International
Conference on Machine Learning.
Le Lan, Charline, Stephen Tu, Adam Oberman, Rishabh Agarwal, and Marc G. Belle-
mare. 2022. On the generalization of representations in reinforcement learning. In
Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics.
LeCun, Yann, and Yoshua Bengio. 1995. Convolutional networks for images, speech, and
time series. In The handbook of brain theory and neural networks, edited by Michael A.
Arbib. MIT Press.
Lee, Daewoo, Boris Defourny, and Warren B. Powell. 2013. Bias-corrected Q-learning
to control max-operator bias in Q-learning. In Symposium on Adaptive Dynamic
Programming And Reinforcement Learning.
Levine, Sergey. 2018. Reinforcement learning and control as probabilistic inference:
Tutorial and review. arXiv preprint arXiv:1805.00909.
Levine, Sergey, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. 2016. End-to-end
training of deep visuomotor policies. Journal of Machine Learning Research 17 (1):
1334–1373.
Li, Xiaocheng, Huaiyang Zhong, and Margaret L. Brandeau. 2022. Quantile Markov
decision processes. Operations Research 70 (3): 1428–1447.
Lillicrap, Timothy P., Daniel Cownden, Douglas B. Tweed, and Colin J. Akerman. 2016a.
Random synaptic feedback weights support error backpropagation for deep learning.
Nature Communications 7 (1): 1–10.
Lillicrap, Timothy P., Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez,
Yuval Tassa, David Silver, and Daan Wierstra. 2016b. Continuous control with deep
reinforcement learning. In Proceedings of the International Conference on Learning
Representations.
Lin, Gwo Dong. 2017. Recent developments on the moment problem. Journal of
Statistical Distributions and Applications 4 (1): 1–17.
Lin, L. J. 1992. Self-improving reactive agents based on reinforcement learning, planning
and teaching. Machine Learning 8 (3): 293–321.
Lin, Zichuan, Li Zhao, Derek Yang, Tao Qin, Tie-Yan Liu, and Guangwen Yang. 2019.
Distributional reward decomposition for reinforcement learning. In Advances in Neural
Information Processing Systems.
Lipovetzky, Nir, Miquel Ramirez, and Hector Geﬀner. 2015. Classical planning with
simulators: Results on the Atari video games. In Proceedings of International Joint
Conference on Artiﬁcial Intelligence.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

References
351
Littman, Michael L. 1994. Markov games as a framework for multi-agent reinforcement
learning. In Proceedings of the International Conference on Machine Learning.
Littman, Michael L., and Csaba Szepesvári. 1996. A generalized reinforcement-learning
model: Convergence and applications. In Proceedings of the International Conference
on Machine Learning.
Liu, Jun S. 2001. Monte Carlo strategies in scientiﬁc computing. Springer.
Liu, Qiang, and Dilin Wang. 2016. Stein variational gradient descent: A general purpose
Bayesian inference algorithm. In Advances in Neural Information Processing Systems.
Liu, Quansheng. 1998. Fixed points of a generalized smoothing transformation and
applications to the branching random walk. Advances in Applied Probability 30 (1):
85–112.
Ljung, Lennart. 1977. Analysis of recursive stochastic algorithms. IEEE Transactions
on Automatic Control 22 (4): 551–575.
Ljungberg, Tomas, Paul Apicella, and Wolfram Schultz. 1992. Responses of monkey
dopamine neurons during learning of behavioral reactions. Journal of Neurophysiology
67 (1): 145–163.
Lowet, Adam S., Qiao Zheng, Sara Matias, Jan Drugowitsch, and Naoshige Uchida.
2020. Distributional reinforcement learning in the brain. Trends in Neurosciences 43
(12): 980–997.
Ludvig, Elliot A., Marc G. Bellemare, and Keir G. Pearson. 2011. A primer on reinforce-
ment learning in the brain: Psychological, computational, and neural perspectives. In
Computational neuroscience for advancing artiﬁcial intelligence: Models, methods and
applications, edited by Eduardo Alonso and Esther Mondragón. IGI Global.
Luo, Yudong, Guiliang Liu, Haonan Duan, Oliver Schulte, and Pascal Poupart. 2021.
Distributional reinforcement learning with monotonic splines. In Proceedings of the
International Conference on Learning Representations.
Lyle, Clare, Pablo Samuel Castro, and Marc G. Bellemare. 2019. A comparative analysis
of expected and distributional reinforcement learning. In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence.
Lyle, Clare, Mark Rowland, Georg Ostrovski, and Will Dabney. 2021. On the eﬀect
of auxiliary tasks on representation dynamics. In Proceedings of the International
Conference on Artiﬁcial Intelligence and Statistics.
Lyu, Xueguang, and Christopher Amato. 2020. Likelihood quantile networks for
coordinating multi-agent reinforcement learning. In Proceedings of the International
Conference on Autonomous Agents and Multiagent Systems.
Machado, Marlos C., Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew
Hausknecht, and Michael Bowling. 2018. Revisiting the Arcade Learning Environ-
ment: Evaluation protocols and open problems for general agents. Journal of Artiﬁcial
Intelligence Research 61:523–562.
MacKay, David J. C. 2003. Information theory, inference and learning algorithms.
Cambridge University Press.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

352
References
Maddison, Chris J., Dieterich Lawson, George Tucker, Nicolas Heess, Arnaud Doucet,
Andriy Mnih, and Yee Whye Teh. 2017. Particle value functions. In Proceedings of the
International Conference on Learning Representations (Workshop Track).
Madeira Auraújo, João Guilherme, Johan Samir Obando Ceron, and Pablo Samuel
Castro. 2021. Lifting the veil on hyper-parameters for value-based deep reinforcement
learning. In NeurIPS 2021 Workshop: LatinX in AI.
Maei, Hamid Reza. 2011. Gradient temporal-diﬀerence learning algorithms. PhD diss.,
University of Alberta.
Mandl, Petr. 1971. On the variance in controlled Markov chains. Kybernetika 7 (1):
1–12.
Mannor, Shie, Duncan Simester, Peng Sun, and John N. Tsitsiklis. 2007. Bias and
variance approximation in value function estimates. Management Science 53 (2): 308–
322.
Mannor, Shie, and John Tsitsiklis. 2011. Mean-variance optimization in Markov decision
processes. In Proceedings of the International Conference on Machine Learning.
Markowitz, Harry M. 1952. Portfolio selection. Journal of Finance 7:77–91.
Martin, John, Michal Lyskawinski, Xiaohu Li, and Brendan Englot. 2020. Stochastically
dominant distributional reinforcement learning. In Proceedings of the International
Conference on Machine Learning.
Massart, Pascal. 1990. The tight constant in the Dvoretzky-Kiefer-Wolfowitz inequality.
The Annals of Probability 18 (3): 1269–1283.
Matignon, Laëtitia, Guillaume J. Laurent, and Nadine Le Fort-Piat. 2007. Hysteretic
Q-learning: An algorithm for decentralized reinforcement learning in cooperative multi-
agent teams. In IEEE International Conference on Intelligent Robots and Systems.
Matignon, Laëtitia, Guillaume J. Laurent, and Nadine Le Fort-Piat. 2012. Independent
reinforcement learners in cooperative Markov games: A survey regarding coordination
problems. The Knowledge Engineering Review 27 (1): 1–31.
Mavrin, Borislav, Hengshuai Yao, Linglong Kong, Kaiwen Wu, and Yaoliang Yu. 2019.
Distributional reinforcement learning for eﬃcient exploration. In Proceedings of the
International Conference on Machine Learning.
McCallum, Andrew K. 1995. Reinforcement learning with selective perception and
hidden state. PhD diss., University of Rochester.
Meyn, Sean. 2022. Control systems and reinforcement learning. Cambridge University
Press.
Meyn, Sean P., and Richard L. Tweedie. 2012. Markov chains and stochastic stability.
Cambridge University Press.
Mihatsch, Oliver, and Ralph Neuneier. 2002. Risk-sensitive reinforcement learning.
Machine Learning 49 (2): 267–290.
Miller, Ralph R., Robert C. Barnet, and Nicholas J. Grahame. 1995. Assessment of the
Rescorla-Wagner model. Psychological Bulletin 117 (3): 363.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

References
353
Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc
G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski,
Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan
Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. 2015. Human-level control
through deep reinforcement learning. Nature 518 (7540): 529–533.
Mogenson, Gordon J., Douglas L. Jones, and Chi Yiu Yim. 1980. From motivation to
action: Functional interface between the limbic system and the motor system. Progress
in Neurobiology 14 (2–3): 69–97.
Monge, Gaspard. 1781. Mémoire sur la théorie des déblais et des remblais. Histoire de
l’Académie Royale des Sciences de Paris: 666–704.
Montague, P. Read, Peter Dayan, and Terrence J. Sejnowski. 1996. A framework for
mesencephalic dopamine systems based on predictive Hebbian learning. Journal of
Neuroscience 16 (5): 1936–1947.
Montfort, Nick, and Ian Bogost. 2009. Racing the beam: The Atari video computer
system. MIT Press.
Moore, Andrew W., and Christopher G. Atkeson. 1993. Prioritized sweeping: Rein-
forcement learning with less data and less time. Machine Learning 13 (1): 103–
130.
Morgenstern, Oskar, and John von Neumann. 1944. Theory of games and economic
behavior. Princeton University Press.
Morimura, Tetsuro, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, and
Toshiyuki Tanaka. 2010a. Nonparametric return distribution approximation for rein-
forcement learning. In Proceedings of the International Conference on Machine
Learning.
Morimura, Tetsuro, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, and
Toshiyuki Tanaka. 2010b. Parametric return density estimation for reinforcement
learning. In Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence.
Morton, Thomas E. 1971. On the asymptotic convergence rate of cost diﬀerences for
Markovian decision processes. Operations Research 19 (1): 244–248.
Mott, Bradford W., Stephen Anthony, and the Stella team. 1995–2023. Stella: A multi-
platform Atari 2600 VCS Emulator. http://stella.sourceforge.net.
Müller, Alfred. 1997. Integral probability metrics and their generating classes of
functions. Advances in Applied Probability 29 (2): 429–443.
Muller, Timothy H., James L. Butler, Sebastijan Veselic, Bruno Miranda, Timothy
E. J. Behrens, Zeb Kurth-Nelson, and Steven W. Kennerley. 2021. Distributional
reinforcement learning in prefrontal cortex. bioRxiv 2021.06.14.448422.
Munos, Rémi. 2003. Error bounds for approximate policy iteration. In Proceedings of
the International Conference on Machine Learning.
Munos, Rémi, Tom Stepleton, Anna Harutyunyan, and Marc G. Bellemare. 2016. Safe
and eﬃcient oﬀ-policy reinforcement learning. In Advances in Neural Information
Processing Systems.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

354
References
Murphy, Kevin P. 2012. Machine learning: A probabilistic perspective. MIT Press.
Naddaf, Yavar. 2010. Game-independent AI agents for playing Atari 2600 console
games. Master’s thesis, University of Alberta.
Naesseth, Christian A., Fredrik Lindsten, and Thomas B. Schön. 2019. Elements of
sequential Monte Carlo. Foundations and Trends R⃝in Machine Learning 12 (3): 307–
392.
Nair, Vinod, and Geoﬀrey E. Hinton. 2010. Rectiﬁed linear units improve restricted
Boltzmann machines. In Proceedings of the International Conference on Machine
Learning.
Nam, Daniel W., Younghoon Kim, and Chan Y. Park. 2021. GMAC: A distributional
perspective on actor-critic framework. In Proceedings of the International Conference
on Machine Learning.
Neininger, Ralph. 1999. Limit laws for random recursive structures and algorithms.
PhD diss., University of Freiburg.
Neininger, Ralph. 2001. On a multivariate contraction method for random recursive
structures with applications to Quicksort. Random Structures & Algorithms 19 (3–4):
498–524.
Neininger, Ralph, and Ludger Rüschendorf. 2004. A general limit theorem for recursive
algorithms and combinatorial structures. The Annals of Applied Probability 14 (1): 378–
418.
Newey, Whitney K., and James L. Powell. 1987. Asymmetric least squares estimation
and testing. Econometrica 55 (4): 819–847.
Nguyen, Thanh Tang, Sunil Gupta, and Svetha Venkatesh. 2021. Distributional rein-
forcement learning via moment matching. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence.
Nieoullon, André. 2002. Dopamine and the regulation of cognition and attention.
Progress in Neurobiology 67 (1): 53–83.
Nikolov, Nikolay, Johannes Kirschner, Felix Berkenkamp, and Andreas Krause. 2019.
Information-directed exploration for deep reinforcement learning. In Proceedings of the
International Conference on Learning Representations.
Niv, Yael. 2009. Reinforcement learning in the brain. Journal of Mathematical
Psychology 53 (3): 139–154.
Olah, Chris, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Kather-
ine Ye, and Alexander Mordvintsev. 2018. The building blocks of interpretability.
Distill.
Oliehoek, Frans A., and Christopher Amato. 2016. A concise introduction to decentral-
ized POMDPs. Springer.
Oliehoek, Frans A., Matthijs T. J. Spaan, and Nikos Vlassis. 2008. Optimal and approxi-
mate Q-value functions for decentralized POMDPs. Journal of Artiﬁcial Intelligence
Research 32 (1): 289–353.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

References
355
Olsen, Ditte, Niels Wellner, Mathias Kaas, Inge E. M. de Jong, Florence Sotty, Michael
Didriksen, Simon Glerup, and Anders Nykjaer. 2021. Altered dopaminergic ﬁring
pattern and novelty response underlie ADHD-like behavior of SorCS2-deﬁcient mice.
Translational Psychiatry 11 (1): 1–14.
Omidshaﬁei, Shayegan, Jason Pazis, Christopher Amato, Jonathan P. How, and John Vian.
2017. Deep decentralized multi-task multi-agent reinforcement learning under partial
observability. In Proceedings of the International Conference on Machine Learning.
Owen, Art B. 2013. Monte Carlo theory, methods and examples.
Palmer, Gregory, Rahul Savani, and Karl Tuyls. 2019. Negative update intervals in deep
multi-agent reinforcement learning. In Proceedings of the International Conference on
Autonomous Agents and Multiagent Systems.
Palmer, Gregory, Karl Tuyls, Daan Bloembergen, and Rahul Savani. 2018. Lenient
multi-agent deep reinforcement learning. In Proceedings of the International Conference
on Autonomous Agents and Multiagent Systems.
Panait, Liviu, Keith Sullivan, and Sean Luke. 2006. Lenient learners in cooperative
multiagent systems. In Proceedings of the International Conference on Autonomous
Agents and Multiagent Systems.
Panait, Liviu, R. Paul Wiegand, and Sean Luke. 2003. Improving coevolutionary search
for optimal multiagent behaviors. In Proceedings of the International Joint Conference
on Artiﬁcial Intelligence.
Panaretos, Victor M., and Yoav Zemel. 2020. An invitation to statistics in Wasserstein
space. Springer Nature.
Parr, Ronald, Lihong Li, Gavin Taylor, Christopher Painter-Wakeﬁeld, and Michael
L. Littman. 2008. An analysis of linear models, linear value-function approximation,
and feature selection for reinforcement learning. In Proceedings of the International
Conference on Machine Learning.
Parr, Ronald, Christopher Painter-Wakeﬁeld, Lihong Li, and Michael Littman. 2007.
Analyzing feature generation for value-function approximation. In Proceedings of the
International Conference on Machine Learning.
Pavlov, Ivan P. 1927. Conditioned reﬂexes: An investigation of the physiological activity
of the cerebral cortex. Oxford University Press.
Peres, Yuval, Wilhelm Schlag, and Boris Solomyak. 2000. Sixty years of Bernoulli con-
volutions. In Fractal geometry and stochastics II, edited by Christoph Bandt, Siegfried
Graf, and Martina Zähle. Springer.
Peyré, Gabriel, and Marco Cuturi. 2019. Computational optimal transport: With applica-
tions to data science. Foundations and Trends R⃝in Machine Learning 11 (5–6): 355–
607.
Prashanth, L. A., and Michael Fu. 2021. Risk-sensitive reinforcement learning. arXiv
preprint arXiv:1810.09126.
Prashanth, L. A., and Mohammad Ghavamzadeh. 2013. Actor-critic algorithms for
risk-sensitive MDPs. In Advances in Neural Information Processing Systems.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

356
References
Precup, Doina, Richard S. Sutton, and Satinder P. Singh. 2000. Eligibility traces for
oﬀ-policy policy evaluation. In Proceedings of the International Conference on Machine
Learning.
Pritzel, Alexander, Benigno Uria, Sriram Srinivasan, Adria Puigdomenech Badia, Oriol
Vinyals, Demis Hassabis, Daan Wierstra, and Charles Blundell. 2017. Neural episodic
control. In Proceedings of the International Conference on Machine Learning.
Puterman, Martin L. 2014. Markov decision processes: Discrete stochastic dynamic
programming. John Wiley & Sons.
Puterman, Martin L., and Moon Chirl Shin. 1978. Modiﬁed policy iteration algorithms
for discounted Markov decision problems. Management Science 24 (11): 1127–1137.
Qiu, Wei, Xinrun Wang, Runsheng Yu, Xu He, Rundong Wang, Bo An, Svetlana
Obraztsova, and Zinovi Rabinovich. 2021. RMIX: Learning risk-sensitive policies
for cooperative reinforcement learning agents. In Advances in Neural Information
Processing Systems.
Qu, Chao, Shie Mannor, and Huan Xu. 2019. Nonlinear distributional gradient temporal-
diﬀerence learning. In Proceedings of the International Conference on Machine
Learning.
Quan, John, and Georg Ostrovski. 2020. DQN Zoo: Reference implementations of
DQN-based agents. Version 1.0.0. http://github.com/deepmind/dqn_zoo.
Rachev, Svetlozar T., Lev Klebanov, Stoyan V. Stoyanov, and Frank Fabozzi. 2013.
The methods of distances in the theory of probability and statistics. Springer Science &
Business Media.
Rachev, Svetlozar T., and Ludger Rüschendorf. 1995. Probability metrics and recursive
algorithms. Advances in Applied Probability 27 (3): 770–799.
Rashid, Tabish, Mikayel Samvelyan, Christian Schroeder de Witt, Gregory Farquhar,
Jakob N. Foerster, and Shimon Whiteson. 2020. Monotonic value function factorisation
for deep multi-agent reinforcement learning. Journal of Machine Learning Research 21
(1): 7234–7284.
Rashid, Tabish, Mikayel Samvelyan, Christian Schroeder de Witt, Gregory Farquhar,
Jakob Foerster, and Shimon Whiteson. 2018. QMIX: Monotonic value function factori-
sation for deep multi-agent reinforcement learning. In Proceedings of the International
Conference on Machine Learning.
Rescorla, Robert A., and Allan R. Wagner. 1972. A theory of Pavlovian conditioning:
Variations in the eﬀectiveness of reinforcement and nonreinforcement. In Classical
conditioning II: Current Research and Theory, edited by Abraham J. Black and William
F. Prosaky, 64–99. Appleton-Century-Crofts.
Riedmiller, M. 2005. Neural ﬁtted Q iteration – ﬁrst experiences with a data eﬃcient
neural reinforcement learning method. In Proceedings of the European Conference on
Machine Learning.
Riedmiller, Martin, Thomas Gabel, Roland Hafner, and Sascha Lange. 2009. Reinforce-
ment learning for robot soccer. Autonomous Robots 27 (1): 55–73.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

References
357
Rizzo, Maria L., and Gábor J. Székely. 2016. Energy distance. Wiley Interdisciplinary
Reviews: Computational Statistics 8 (1): 27–38.
Robbins, Herbert, and Sutton Monro. 1951. A stochastic approximation method. The
Annals of Mathematical Statistics 22 (3): 400–407.
Robbins, Herbert, and David Siegmund. 1971. A convergence theorem for non negative
almost supermartingales and some applications. In Optimizing methods in statistics,
edited by Jagdish S. Rustagi, 233–257. Academic Press.
Robert, Christian, and George Casella. 2004. Monte Carlo statistical methods. Springer
Science & Business Media.
Rockafellar, R. Tyrrell, and Stanislav Uryasev. 2000. Optimization of conditional value-
at-risk. Journal of Risk 2:21–42.
Rockafellar, R. Tyrrell, and Stanislav Uryasev. 2002. Conditional value-at-risk for
general loss distributions. Journal of Banking & Finance 26 (7): 1443–1471.
Rösler, Uwe. 1991. A limit theorem for “Quicksort.” RAIRO-Theoretical Informatics
and Applications 25 (1): 85–100.
Rösler, Uwe. 1992. A ﬁxed point theorem for distributions. Stochastic Processes and
Their Applications 42 (2): 195–214.
Rösler, Uwe. 2001. On the analysis of stochastic divide and conquer algorithms.
Algorithmica 29 (1): 238–261.
Rösler, Uwe, and Ludger Rüschendorf. 2001. The contraction method for recursive
algorithms. Algorithmica 29 (1–2): 3–33.
Rowland, Mark, Marc G. Bellemare, Will Dabney, Rémi Munos, and Yee Whye Teh.
2018. An analysis of categorical distributional reinforcement learning. In Proceedings of
the International Conference on Artiﬁcial Intelligence and Statistics.
Rowland, Mark, Robert Dadashi, Saurabh Kumar, Rémi Munos, Marc G. Bellemare,
and Will Dabney. 2019. Statistics and samples in distributional reinforcement learning.
In Proceedings of the International Conference on Machine Learning.
Rowland, Mark, Shayegan Omidshaﬁei, Daniel Hennes, Will Dabney, Andrew Jaegle,
Paul Muller, Julien Pérolat, and Karl Tuyls. 2021. Temporal diﬀerence and return
optimism in cooperative multi-agent reinforcement learning. In Adaptive and Learning
Agents Workshop at the International Conference on Autonomous Agents and Multiagent
Systems.
Rubner, Yossi, Carlo Tomasi, and Leonidas J. Guibas. 1998. A metric for distributions
with applications to image databases. In Sixth International Conference on Computer
Vision.
Rudin, Walter. 1976. Principles of mathematical analysis. McGraw-Hill.
Rumelhart, David E., Geoﬀrey E. Hinton, and Ronald J. Williams. 1986. Learning
representations by back-propagating errors. Nature 323 (6088): 533–536.
Rummery, Gavin A., and Mahesan Niranjan. 1994. On-line Q-learning using connec-
tionist systems. Technical report. Cambridge University Engineering Department.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

358
References
Rüschendorf, Ludger. 2006. On stochastic recursive equations of sum and max type.
Journal of Applied Probability 43 (3): 687–703.
Rüschendorf, Ludger, and Ralph Neininger. 2006. A survey of multivariate aspects of
the contraction method. Discrete Mathematics & Theoretical Computer Science 8:31–56.
Ruszczy´nski, Andrzej. 2010. Risk-averse dynamic programming for Markov decision
processes. Mathematical Programming 125 (2): 235–261.
Samuel, Arthur L. 1959. Some studies in machine learning using the game of checkers.
IBM Journal of Research and Development 11 (6): 601–617.
Santambrogio, Filippo. 2015. Optimal transport for applied mathematicians: Calculus
of variations, PDEs and modeling. Birkhäuser.
Särkkä, Simo. 2013. Bayesian ﬁltering and smoothing. Cambridge University Press.
Sato, Makoto, Hajime Kimura, and Shibenobu Kobayashi. 2001. TD algorithm for
the variance of return and mean-variance reinforcement learning. Transactions of the
Japanese Society for Artiﬁcial Intelligence 16 (3): 353–362.
Schaul, Tom, John Quan, Ioannis Antonoglou, and David Silver. 2016. Prioritized
experience replay. In Proceedings of the International Conference on Learning
Representations.
Scherrer, Bruno. 2010. Should one compute the temporal diﬀerence ﬁx point or mini-
mize the Bellman residual? The uniﬁed oblique projection view. In Proceedings of the
International Conference on Machine Learning.
Scherrer, Bruno. 2014. Approximate policy iteration schemes: A comparison. In
Proceedings of the International Conference on Machine Learning.
Scherrer, Bruno, and Boris Lesner. 2012. On the use of non-stationary policies for sta-
tionary inﬁnite-horizon Markov decision processes. In Advances in Neural Information
Processing Systems.
Schlegel, Matthew, Andrew Jacobsen, Zaheer Abbas, Andrew Patterson, Adam White,
and Martha White. 2021. General value function networks. Journal of Artiﬁcial
Intelligence Research (JAIR) 70:497–543.
Schultz, Wolfram. 1986. Responses of midbrain dopamine neurons to behavioral trigger
stimuli in the monkey. Journal of Neurophysiology 56 (5): 1439–1461.
Schultz, Wolfram. 2002. Getting formal with dopamine and reward. Neuron 36 (2):
241–263.
Schultz, Wolfram. 2016. Dopamine reward prediction-error signalling: A two-component
response. Nature Reviews Neuroscience 17 (3): 183–195.
Schultz, Wolfram, Paul Apicella, and Tomas Ljungberg. 1993. Responses of monkey
dopamine neurons to reward and conditioned stimuli during successive steps of learning
a delayed response task. Journal of Neuroscience 13 (3): 900–913.
Schultz, Wolfram, Peter Dayan, and P. Read Montague. 1997. A neural substrate of
prediction and reward. Science 275 (5306): 1593–1599.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

References
359
Schultz, Wolfram, and Ranulfo Romo. 1990. Dopamine neurons of the monkey midbrain:
Contingencies of responses to stimuli eliciting immediate behavioral reactions. Journal
of Neurophysiology 63 (3): 607–624.
Shah, Ashvin. 2012. Psychological and neuroscientiﬁc connections with reinforcement
learning. In Reinforcement learning, edited by Marco Wiering and Martijn Otterlo,
507–537. Springer.
Shapiro, Alexander, Darinka Dentcheva, and Andrzej Ruszczynski. 2009. Lectures on
stochastic programming: Modeling and theory. SIAM.
Shapley, Lloyd S. 1953. Stochastic games. Proceedings of the National Academy of
Sciences 39 (10): 1095–1100.
Shen, Yun, Wilhelm Stannat, and Klaus Obermayer. 2013. Risk-sensitive Markov control
processes. SIAM Journal on Control and Optimization 51 (5): 3652–3672.
Shoham, Yoav, and Kevin Leyton-Brown. 2009. Multiagent systems. Cambridge
University Press.
Silver, David, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George
van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam,
Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya
Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel,
and Demis Hassabis. 2016. Mastering the game of Go with deep neural networks and
tree search. Nature 529 (7587): 484–489.
Singh, Satinder P., and Richard S. Sutton. 1996. Reinforcement learning with replacing
eligibility traces. Machine Learning 22:123–158.
Sobel, Matthew J. 1982. The variance of discounted Markov decision processes. Journal
of Applied Probability 19 (4): 794–802.
Solomyak, Boris. 1995. On the random series Σ ± λn (an Erd˝os problem). Annals of
Mathematics 142 (3): 611–625.
Stalnaker, Thomas A., James D. Howard, Yuji K. Takahashi, Samuel J. Gershman,
Thorsten Kahnt, and Geoﬀrey Schoenbaum. 2019. Dopamine neuron ensembles signal
the content of sensory prediction errors. eLife 8:e49315.
Steinbach, Marc C. 2001. Markowitz revisited: Mean-variance models in ﬁnancial
portfolio analysis. SIAM Review 43 (1): 31–85.
Strang, Gilbert. 1993. Introduction to linear algebra. Wellesley-Cambridge Press.
Such, Felipe Petroski, Vashisht Madhavan, Rosanne Liu, Rui Wang, Pablo Samuel Castro,
Yulun Li, Ludwig Schubert, Marc G. Bellemare, JeﬀClune, and Joel Lehman. 2019. An
Atari model zoo for analyzing, visualizing, and comparing deep reinforcement learning
agents. In Proceedings of the International Joint Conference on Artiﬁcial Intelligence.
Sun, Wei-Fang, Cheng-Kuang Lee, and Chun-Yi Lee. 2021. DFAC framework: Factoriz-
ing the value function via quantile mixture for multi-agent distributional Q-learning. In
Proceedings of the International Conference on Machine Learning.
Sunehag, Peter, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius
Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls,
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

360
References
and Thore Graepel. 2017. Value-decomposition networks for cooperative multi-agent
learning. arXiv preprint arXiv:1706.05296.
Sutton, Richard S. 1984. Temporal credit assignment in reinforcement learning. PhD
diss., University of Massachusetts, Amherst.
Sutton, Richard S. 1988. Learning to predict by the methods of temporal diﬀerences.
Machine Learning 3 (1): 9–44.
Sutton, Richard S. 1995. TD models: Modeling the world at a mixture of time scales. In
Proceedings of the International Conference on Machine Learning.
Sutton, Richard S. 1996. Generalization in reinforcement learning: Successful examples
using sparse coarse coding. In Advances in Neural Information Processing Systems.
Sutton, Richard S. 1999. Open theoretical questions in reinforcement learning. In
European Conference on Computational Learning Theory.
Sutton, Richard S., and Andrew G. Barto. 2018. Reinforcement learning: An introduction.
MIT Press.
Sutton, Richard S., Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Sil-
ver, Csaba Szepesvári, and Eric Wiewiora. 2009. Fast gradient-descent methods for
temporal-diﬀerence learning with linear function approximation. In Proceedings of the
International Conference on Machine Learning.
Sutton, Richard S., David A. McAllester, Satinder P. Singh, and Yishay Mansour. 2000.
Policy gradient methods for reinforcement learning with function approximation. In
Advances in Neural Information Processing Systems.
Sutton, Richard S., Joseph Modayil, Michael Delp, Thomas Degris, Patrick M. Pilarski,
Adam White, and Doina Precup. 2011. Horde: A scalable real-time architecture for
learning knowledge from unsupervised sensorimotor interaction. In Proceedings of the
International Conference on Autonomous Agents and Multiagents Systems.
Sutton, Richard S., Doina Precup, and Satinder Singh. 1999. Between MDPs and semi-
MDPs: A framework for temporal abstraction in reinforcement learning. Artiﬁcial
Intelligence 112 (1–2): 181–211.
Sutton, Richard S., Csaba Szepesvári, and Hamid Reza Maei. 2008a. A convergent O(n)
temporal-diﬀerence algorithm for oﬀ-policy learning with linear function approximation.
In Advances in Neural Information Processing Systems.
Sutton, Richard S., Csaba Szespesvári, Alborz Geramifard, and Michael Bowling. 2008b.
Dyna-style planning with linear function approximation and prioritized sweeping. In
Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence.
Székely, Gabor J. 2002. E-statistics: The energy of statistical samples. Technical
report 02-16. Bowling Green State University, Department of Mathematics and Statistics.
Székely, Gábor J., and Maria L. Rizzo. 2013. Energy statistics: A class of statistics based
on distances. Journal of Statistical Planning and Inference 143 (8): 1249–1272.
Szepesvári, Csaba. 1998. The asymptotic convergence-rate of Q-learning. In Advances
in Neural Information Processing Systems.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

References
361
Szepesvári, Csaba. 2010. Algorithms for reinforcement learning. Morgan & Claypool
Publishers.
Szepesvári, Csaba. 2020. Constrained MDPs and the reward hypothesis. https://readin
gsml.blogspot.com/2020/03/constrained-mdps-and-reward-hypothesis.html. Accessed
June 25, 2021.
Takahashi, Yuji K., Hannah M. Batchelor, Bing Liu, Akash Khanna, Marisela Morales,
and Geoﬀrey Schoenbaum. 2017. Dopamine neurons respond to errors in the prediction
of sensory features of expected rewards. Neuron 95 (6): 1395–1405.
Tamar, Aviv, Dotan Di Castro, and Shie Mannor. 2012. Policy gradients with vari-
ance related risk criteria. In Proceedings of the International Conference on Machine
Learning.
Tamar, Aviv, Dotan Di Castro, and Shie Mannor. 2013. Temporal diﬀerence methods
for the variance of the reward to go. In Proceedings of the International Conference on
Machine Learning.
Tamar, Aviv, Dotan Di Castro, and Shie Mannor. 2016. Learning the variance of the
reward-to-go. Journal of Machine Learning Research 17 (1): 361–396.
Tamar, Aviv, Yonatan Glassner, and Shie Mannor. 2015. Optimizing the CVaR via
sampling. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence.
Tampuu, Ardi, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan
Aru, Jaan Aru, and Raul Vicente. 2017. Multiagent cooperation and competition with
deep reinforcement learning. PloS One 12 (4): e0172395.
Tan, Ming. 1993. Multi-agent reinforcement learning: Independent vs. cooperative
agents. In Proceedings of the International Conference on Machine Learning.
Tano, Pablo, Peter Dayan, and Alexandre Pouget. 2020. A local temporal diﬀerence code
for distributional reinforcement learning. In Advances in Neural Information Processing
Systems.
Taylor, James W. 2008. Estimating value at risk and expected shortfall using expectiles.
Journal of Financial Econometrics 6 (2): 231–252.
Tesauro, Gerald. 1995. Temporal diﬀerence learning and TD-Gammon. Communications
of the ACM 38 (3): 58–68.
Tessler, Chen, Guy Tennenholtz, and Shie Mannor. 2019. Distributional policy optimiza-
tion: An alternative approach for continuous control. In Advances in Neural Information
Processing Systems.
Tieleman, T., and G. Hinton. 2012. rmsprop: Divide the gradient by a running average
of its recent magnitude. COURSERA: Neural Networks for Machine Learning.
Toussaint, Marc. 2009. Robot trajectory optimization using approximate inference. In
Proceedings of the International Conference on Machine Learning.
Toussaint, Marc, and Amos Storkey. 2006. Probabilistic inference for solving discrete
and continuous state Markov decision processes. In Proceedings of the International
Conference on Machine Learning.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

362
References
Tsitsiklis, John N. 1994. Asynchronous stochastic approximation and Q-learning.
Machine Learning 16 (3): 185–202.
Tsitsiklis, John N. 2002. On the convergence of optimistic policy iteration. Journal of
Machine Learning Research 3:59–72.
Tsitsiklis, John N., and Benjamin Van Roy. 1997. An analysis of temporal-diﬀerence
learning with function approximation. IEEE Transactions on Automatic Control 42 (5):
674–690.
Tulcea, Cassius T. Ionescu. 1949. Mesures dans les espaces produits. Atti Accademia
Nazionale Lincei Rend 8 (7): 208–211.
van den Oord, Aäron, Nal Kalchbrenner, and Koray Kavukcuoglu. 2016. Pixel recurrent
neural networks. In Proceedings of the International Conference on Machine Learning.
van der Vaart, Aad W. 2000. Asymptotic statistics. Cambridge University Press.
van der Wal, Johannes. 1981. Stochastic dynamic programming: Successive approxima-
tions and nearly optimal strategies for Markov decision processes and Markov games.
Stichting Mathematisch Centrum.
van Hasselt, Hado, Arthur Guez, Matteo Hessel, Volodymyr Mnih, and David Sil-
ver. 2016a. Learning values across many orders of magnitude. In Advances in Neural
Information Processing Systems.
van Hasselt, Hado, Arthur Guez, and David Silver. 2016b. Deep reinforcement learning
with double Q-learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence.
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan
N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In
Advances in Neural Information Processing Systems.
Vecerik, Mel, Oleg Sushkov, David Barker, Thomas Rothörl, Todd Hester, and Jon
Scholz. 2019. A practical approach to insertion with variable socket position using deep
reinforcement learning. In IEEE International Conference on Robotics and Automation.
Veness, Joel, Marc G. Bellemare, Marcus Hutter, Alvin Chua, and Guillaume Desjardins.
2015. Compress and control. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence.
Veness, Joel, Kee Siong Ng, Marcus Hutter, William T. B. Uther, and David Silver.
2011. A Monte-Carlo AIXI approximation. Journal of Artiﬁcial Intelligence Resesearch
40:95–142.
Vershik, A. M. 2013. Long history of the Monge-Kantorovich transportation problem.
The Mathematical Intelligencer 35 (4): 1–9.
Vieillard, Nino, Olivier Pietquin, and Matthieu Geist. 2020. Munchausen reinforcement
learning. In Advances in Neural Information Processing Systems.
Villani, Cédric. 2003. Topics in optimal transportation. Graduate Studies in Mathematics.
American Mathematical Society.
Villani, Cédric. 2008. Optimal transport: Old and new. Springer Science & Business
Media.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

References
363
von Neumann, John. 1928. Zur Theorie der Gesellschaftsspiele. Mathematische Annalen
100 (1): 295–320.
Wainwright, Martin J., and Michael I. Jordan. 2008. Graphical models, exponential
families, and variational inference. Foundations and Trends R⃝in Machine Learning 1
(1–2): 1–305.
Walton, Neil. 2021. Lecture notes on stochastic control. Unpublished manuscript.
Wang, Ziyu, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando
Freitas. 2016. Dueling network architectures for deep reinforcement learning. In
Proceedings of the International Conference on Machine Learning.
Watkins, Christopher J. C. H. 1989. Learning from delayed rewards. PhD diss., King’s
College, Cambridge.
Watkins, Christopher J. C. H., and Peter Dayan. 1992. Q-learning. Machine Learning 8
(3–4): 279–292.
Weed, Jonathan, and Francis Bach. 2019. Sharp asymptotic and ﬁnite-sample rates of
convergence of empirical measures in Wasserstein distance. Bernoulli 25 (4A): 2620–
2648.
Wei, Ermo, and Sean Luke. 2016. Lenient learning in independent-learner stochastic
cooperative games. Journal of Machine Learning Research 17 (1): 2914–2955.
Werbos, Paul J. 1982. Applications of advances in nonlinear sensitivity analysis. In
System modeling and optimization, edited by Rudolph F. Drenick and Frank Kozin,
762–770. Springer.
White, D. J. 1988. Mean, variance, and probabilistic criteria in ﬁnite Markov decision
processes: A review. Journal of Optimization Theory and Applications 56 (1): 1–29.
White, Martha. 2017. Unifying task speciﬁcation in reinforcement learning. In
Proceedings of the International Conference on Machine Learning.
White, Martha, and Adam White. 2016. A greedy approach to adapting the trace param-
eter for temporal diﬀerence learning. In Proceedings of the International Conference on
Autonomous Agents and Multiagent Systems.
White, Norman M., and Marc Viaud. 1991. Localized intracaudate dopamine D2 receptor
activation during the post-training period improves memory for visual or olfactory
conditioned emotional responses in rats. Behavioral and Neural Biology 55 (3): 255–
269.
Widrow, Bernard, and Marcian E. Hoﬀ. 1960. Adaptive switching circuits. In WESCON
Convention Record Part IV.
Williams, David. 1991. Probability with martingales. Cambridge University Press.
Wise, Roy A. 2004. Dopamine, learning and motivation. Nature Reviews Neuroscience
5 (6): 483–494.
Wurman, Peter R., Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik
Subramanian, Thomas J. Walsh, Roberto Capobianco, Alisa Devlic, Franziska Eckert,
Florian Fuchs, Leilani Gilpin, Piyush Khandelwal, Varun Kompella, HaoChih Lin,
Patrick MacAlpine, Declan Oller, Takuma Seno, Craig Sherstan, Michael D. Thomure,
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

364
References
Houmehr Aghabozorgi, Leon Barrett, Rory Douglas, Dion Whitehead, Peter Dürr, Peter
Stone, Michael Spranger, and Hiroaki Kitano. 2022. Outracing champion Gran Turismo
drivers with deep reinforcement learning. Nature 602 (7896): 223–228.
Yang, Derek, Li Zhao, Zichuan Lin, Tao Qin, Jiang Bian, and Tie-Yan Liu. 2019. Fully
parameterized quantile function for distributional reinforcement learning. In Advances
in Neural Information Processing Systems.
Young, Kenny, and Tian Tian. 2019. MinAtar: An Atari-inspired testbed for thorough
and reproducible reinforcement learning experiments. arXiv preprint arXiv:1903.03176.
Yue, Yuguang, Zhendong Wang, and Mingyuan Zhou. 2020. Implicit distributional
reinforcement learning. In Advances in Neural Information Processing Systems.
Zhang, Shangtong, and Hengshuai Yao. 2019. QUOTA: The quantile option architec-
ture for reinforcement learning. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence.
Zhou, Fan, Zhoufan Zhu, Qi Kuang, and Liwen Zhang. 2021. Non-decreasing quantile
function network with eﬃcient exploration for distributional reinforcement learning. In
Proceedings of the International Joint Conference on Artiﬁcial Intelligence.
Ziegel, Johanna F. 2016. Coherence and elicitability. Mathematical Finance 26 (4):
901–918.
Zolotarev, Vladimir M. 1976. Metric distances in spaces of random variables and their
distributions. Sbornik: Mathematics 30 (3): 373–401.
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Index
Action, 15
Action gap, 207
Aﬃne map, 47, 287
Agent, 14
Agent architecture, 294
C51, 298
DQN, 294
FQF, 313
IQN, 302
MM-DQN, 313
QR-DQN, 300
Approximation error
due to diﬀusion, 69, 138, 284
linear TD, 270
Asymmetric step size
in dopaminergic neurons, 327
for multiagent reinforcement learning,
321
quantile regression, 168
Atari 2600
as a benchmark, 304
human-normalized score, 305
Montezuma’s Revenge, 63
Banach’s ﬁxed point theorem, 143, 188,
270
Basis function, 263
Bellman closedness, 240
Bellman equation, 26, 57, 78, 237
characteristic function, 251
distributional, 34, 37, 66, 85
as a linear system of equations, 116, 257
m-moment, 237
n-step, 46, 192
n-step random variable, 192
optimality, 69, 199
random-variable, 30, 32, 83, 236
state-action random-variable, 236
variance, 127, 253
Bellman operator
for centered moments, 260
consistent, 164
as a contraction mapping, 79, 267
doubly projected, 280
λ-return, 109
m-moment, 237
monotonicity of, 222
n-step, 41, 109
no-loop, 81, 109, 195
optimality, 200
policy evaluation, 200
projected, 267
random-variable, 83, 104, 109
vector notation, 78
see also Distributional Bellman operator
Bootstrap function, 36, 84
Bootstrapping, 58
Cart–pole environment, 73, 291
Categorical dynamic programming, 136,
165
error due to diﬀusion, 68, 145
Categorical projection
for signed distributions, 278
triangular kernel, 132, 166, 278
Categorical Q-learning, 70
365
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

366
Index
Categorical representation, 59, 128, 298
Categorical temporal-diﬀerence learning
(CTD), 51, 165
algorithm, 67
convergence of, 185
with deep neural networks, 298
linear, 275
from projected Bellman operator, 165
target, 66
Characteristic function, 49, 250
Cliﬀs environment, 22, 66, 284
Commutative diagram, 86
Conditional value-at-risk (CVaR), 218,
303
Contraction mapping, 79, 80
Bellman optimality operator, 200
distributional Bellman operator, 88
for linear value function approximation,
267
nonexpansion, 142
Contraction modulus, 80, 100
Control, 51, 69, 197
decentralized, 320
risk-neutral, 197
risk-sensitive, 214, 303, 322
variance-constrained, 215
Convergence of sequence of random
variables, 104
Coupling, 89
Cramér distance, 92, 260
ξ-weighted, 280
contraction modulus of distributional
Bellman operator, 97
Cumulants, 251
Cumulative distribution function, 12, 87,
250
generalized inverse, 87
Deep neural network, 294
Deep reinforcement learning, 293
evaluation, 304
Diﬀusion eﬀect, 139, 270
Dirac delta, 13
Discount factor, 19
Discounting
hyperbolic, 331
Distribution sketch, 234
Bellman closed, 240
Distributional Bellman equation, 34, 37,
66, 85
Distributional Bellman operator, 84
commutative diagram, 86
as a contraction mapping, 88
contraction modulus in supremum ℓp
distance, 97
for cumulative distribution functions, 85,
119
operations, 34, 83
optimality, 203
probability density version, 84
projected, 130
random-horizon, 105, 112
for signed distributions, 279
weak convergence of iterates, 102
see also Random-variable Bellman
operator
Distributional dynamic programming, 135
approximation error, 145
convergence of, 141
Distributional optimality operator, 203
lack of contractivity, 204
Distributional value iteration, 203
convergence of, 207, 211
Dopamine, 323
Dopaminergic neurons, 323
distributional TD model, 327
ﬁring rate, 325
reversal point, 326
Dvoretzky–Kiefer–Wolfowitz inequality,
106
Dynamic programming, 116
approximate, 266
distributional, 135
expectile, 248
Eligibility traces, 71
Environment, 14
Episode, 52
Equality in distribution, 32
Extended metric, 88
Feature matrix, 264
Fixed point, 79
distributional Bellman operator, 85
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Index
367
projected Bellman operator, 286
value function as a, 79
Function approximation, 261
deep neural network, 294
linear, 263
signed linear, 279
Generative equations, 16
Go, 53, 261
Greedy selection rule, 202, 230
Horizon, 59
Hysteretic Q-learning, 321
Implicit parameterization, 301
Imputation step, 244
Imputation strategy, 244
Incremental algorithm, 54, 163
Instantiation, 85
Iterative policy evaluation, 116
Kuhn poker, 3
Learning from experience, 51
Lipschitz constant, 141, 267
Loss
cross-entropy, 276, 300
IQN, 303
linear function approximation, 271
quantile, 168, 274, 300
Markov decision process, 15, 42
Markov property, 27, 42
multiagent, 319
time-homogeneity, 27, 42
Markov reward process, 17
Martingale, 18
Mean-preserving
probability distribution representation,
148
projection, 74
sketch and imputation strategy, 258
Metric, 79
L∞, 80
return-function, 95
ξ-weighted L2, 265
see also Probability metric
Metric space, 79
Mixture distribution, 13
Moment function, 236
Moments, 235, 251
Monte Carlo algorithms, 53
categorical, 63
ﬁrst-visit, 54
ﬁrst-visit online incremental, 55
incremental, 174, 192, 271
nonparametric distributional, 70, 105
undiscounted ﬁnite-horizon categorical,
59, 60, 195
Monte Carlo target, 55
Monte Carlo tree search, 53
Multiagent reinforcement learning, 319
Oﬀ-policy learning, 288
Online learning, 56
Operator, 77, 78, 80
aﬃne, 200
contraction, 173
linear, 80
nonexpansion, 142
see also Distributional Bellman operator,
Bellman operator
Optimal policy, 69
Optimal value function, 69, 199
Optimistic policy iteration, 71
Partially stochastic climbing game, 320
Policy, 15
ε-greedy, 303
greedy, 69, 202
history-dependent, 198
optimal, 198
risk-sensitive, 213
stationary Markov, 15, 198
Policy evaluation, 115
Prediction, 51
Probability of continuing, 40
Probability density function, 12
Probability distribution, 11
Bernoulli, 23
bimodal, 23
Cantor, 25
Cauchy, 194
characteristic function, 250
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

368
Index
cumulative distribution function, 12
density function, 12
geometric, 47
instantiation, 85
mixture, 59
moments, 235
uniform, 13, 24
Probability distribution representation,
118
Bernoulli, 118
categorical, 59, 128, 298
closure, 121
empirical, 121
local code for dopaminergic neurons,
331
location, 61
mean-preserving, 148
normal, 125
particle, 129
quantile, 128, 300
signed categorical, 278
uniform, 118
Probability mass function, 12
Probability metric, 87
analysis, 141
c-homogeneous, 95
Cramér distance, 92
ﬁnite domain, 99
Kolmogorov–Smirnov, 92
ℓp, 92
maximum mean discrepancy, 112
p-convex, 96
regular, 95
supremum extension, 95
supremum Wasserstein distance, 88
total variation, 112
Wasserstein distance, 87
ξ-weighted Cramér distance, 279
Projected Bellman operator, 267
categorical temporal-diﬀerence learning,
165
contraction modulus, 143
contraction modulus with linear function
approximation, 269
distributional dynamic programming,
135
ﬁxed point, 145, 286
for signed distributions, 280
from target network, 297
from target weights, 272
Projection
categorical, 61, 131
deterministic categorical, 63
diﬀusion-free, 139, 247
onto distribution representation, 63
mean-preserving, 74, 203
nearest neighbor, 74
in probability metric d, 131
quantile, 132
stochastic categorical, 62
Projection operator, 130
for linear value function approximation,
266
signed linear return-distribution function,
279
Pushforward distribution, 36, 65
Q-learning, 69, 201
categorical, 70, 208
in DQN, 297
for multiagent reinforcement learning,
320
Quantile dynamic programming, 170
error due to diﬀusion, 147
Quantile loss, 189, 275, 300
Huber, 300
Quantile regression, 167, 188
asymmetric step size, 168
Quantile representation, 128, 300
implicit, 301
Quantile temporal-diﬀerence learning
(QTD), 167
with deep neural networks, 300
linear, 274
from projected Bellman operator, 168
Random trajectory, 16
Random variable, 11
probability distribution D(Z), 28
Random-variable Bellman equation, 30,
32, 83, 236
Random-variable Bellman operator, 83,
104, 109
Representation learning, 293
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Index
369
Rescorla–Wagner model, 324
Return, 19
discounted, 19
existence, 19
expected, 25
random, 11
random-horizon, 40, 47, 105, 112
sample, 55
undiscounted, 19, 40
undiscounted ﬁnite-horizon, 59
Return distribution, 20, 62
Return-distribution function, 5, 34, 58
instantiation, 85
nonstationary Markov optimal, 211
signed, 279
Return-variable function, 30, 31
Reward, 15
Reward distribution, 15, 41
bounded moments assumption, 101
Risk measure, 213
coherent, 228
conditional value-at-risk, 218
entropic risk, 214
mean-variance criterion, 214
value-at-risk, 214
Risk-neutral control, 197
Risk-sensitive control, 214, 303, 322
Robbins–Monro conditions, 175, 180
Sample experience, 56, 163
Sample return, 52
Sample target, 55, 163
from substitution, 164
Sample transition model, 30
Sampling model, 163
Second-order conditioning, 324
Semi-gradient update rule, 270
C51, 300
categorical, 276
DQN, 297
quantile, 275
ξ-weighted Cramér distance, 280
Signed categorical dynamic programming,
279
convergence of, 281
Signed distribution, 186, 277
Softmax function, 150, 275, 298
Source state, 30, 54, 163, 173
State, 14
aliasing, 262
distribution, 30
feature, 263
terminal, 16, 45
State representation, 263
auxiliary tasks, 314
feature matrix, 264
learning, 293, 309
for state-action pairs, 285
tabular, 263
State-action value function, 27
Statistical functional, 234
at-least, 242
distribution sketch, 234
domain, 234
expectile, 248
functional value, 234
mean, 234
median, 241
quantile, 235
and risk measures, 254
Statistical functional dynamic
programming, 245
imputation step, 244
Steady-state distribution, 268
Step size, 55, 174
Stochastic approximation theory, 163, 172
stability, 178
Stochastic gradient descent, 271
analysis of TD algorithms, 179
Target network, 296
Target weights, 272
Temporal-diﬀerence (TD)
error, 58
target, 57
Temporal-diﬀerence (TD) learning, 56,
162
in the brain, 324
convergence of, 183
distributional, 327
as a dynamical system, 290
eigenstructure, 290
expectile, 248
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

370
Index
ﬁxed point with linear approximation,
270
for moments, 252
online, 57
semi-gradient, 270
Termination time, 40
Trajectory, 16
distribution of, 27
sample, 52
Transition kernel, 15
Unbiased update, 164, 173
Update rule, 55
abstract, 163
CTD, 166
incremental Monte Carlo, 55
QTD, 169
Rescorla–Wagner, 324
TD learning, 57
see also Semi-gradient update rule
Value function, 26
induced, 202
optimal, 69, 199
optimal state-action, 199
Q∗, 199
state-action, 27
Value iteration, 199
characteristic, 256
distributional, 203
risk-sensitive, 220
Wasserstein distance, 87
contraction modulus of distributional
Bellman operator, 88
Weak convergence, 102
With probability 1, 20
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Adaptive Computation and Machine Learning
Francis Bach, editor
Bioinformatics: The Machine Learning Approach, Pierre Baldi and Søren
Brunak, 1998
Reinforcement Learning: An Introduction, Richard S. Sutton and Andrew G.
Barto, 1998
Graphical Models for Machine Learning and Digital Communication, Brendan
J. Frey, 1998
Learning in Graphical Models, edited by Michael I. Jordan, 1999
Causation, Prediction, and Search, second edition, Peter Spirtes, Clark Gly-
mour, and Richard Scheines, 2000
Principles of Data Mining, David J. Hand, Heikki Mannila, and Padhraic Smyth,
2000
Bioinformatics: The Machine Learning Approach, second edition, Pierre Baldi
and Søren Brunak, 2001
Learning Kernel Classiﬁers: Theory and Algorithms, Ralf Herbrich, 2002
Learning with Kernels: Support Vector Machines, Regularization, Optimization,
and Beyond, Bernhard Schölkopf and Alexander J. Smola, 2002
Introduction to Machine Learning, Ethem Alpaydın, 2004
Gaussian Processes for Machine Learning, Carl Edward Rasmussen and
Christopher K. I. Williams, 2006
Semi-Supervised Learning, edited by Olivier Chapelle, Bernhard Schölkopf,
and Alexander Zien, 2006
The Minimum Description Length Principle, Peter D. Grünwald, 2007
Introduction to Statistical Relational Learning, edited by Lise Getoor and Ben
Taskar, 2007
Probabilistic Graphical Models: Principles and Techniques, Daphne Koller and
Nir Friedman, 2009
Introduction to Machine Learning, second edition, Ethem Alpaydın, 2010
Machine Learning in Non-Stationary Environments: Introduction to Covariate
Shift Adaptation, Masashi Sugiyama and Motoaki Kawanabe, 2012
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

Boosting: Foundations and Algorithms, Robert E. Schapire and Yoav Freund,
2012
Foundations of Machine Learning, Mehryar Mohri, Afshin Rostamizadeh, and
Ameet Talwalker, 2012
Machine Learning: A Probabilistic Perspective, Kevin P. Murphy, 2012
Introduction to Machine Learning, third edition, Ethem Alpaydın, 2014
Deep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville, 2017
Elements of Causal Inference: Foundations and Learning Algorithms, Jonas
Peters, Dominik Janzing, and Bernhard Schölkopf, 2017
Machine Learning for Data Streams, with Practical Examples in MOA, Albert
Bifet, Ricard Gavaldà, Geoﬀrey Holmes, Bernhard Pfahringer, 2018
Reinforcement Learning: An Introduction, second edition, Richard S. Sutton
and Andrew G. Barto, 2018
Foundations of Machine Learning, second edition, Mehryar Mohri, Afshin
Rostamizadeh, and Ameet Talwalker, 2019
Introduction to Natural Language Processing, Jacob Eisenstein, 2019
Introduction to Machine Learning, fourth edition, Ethem Alpaydın, 2020
Knowledge Graphs: Fundamentals, Techniques, and Applications, Mayank
Kejriwal, Craig A. Knoblock, and Pedro Szekely, 2021
Probabilistic Machine Learning: An Introduction, Kevin P. Murphy, 2022
Machine Learning from Weak Supervision: An Empirical Risk Minimization
Approach, Masashi Sugiyama, Han Bao, Takashi Ishida, Nan Lu, Tomoya Sakai,
and Gang Niu, 2022
Introduction to Online Convex Optimization, second edition, Elad Hazan, 2022
Distributional Reinforcement Learning, Marc G. Bellemare, Will Dabney, and
Mark Rowland, 2023
Downloaded from http://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf by guest on 04 July 2024

