24
Computational science: a new frontier for
computing
Andrew Herbert
Microsoft Research, Cambridge, United Kingdom
Abstract
In 2005 Gilles Kahn discussed with Rick Rashid, Stephen Emmott and
myself a proposal for Microsoft Research, Cambridge and INRIA to
establish a joint research laboratory in France, building on the long-term
informal collaboration between the two institutions. The research focus
of the joint laboratory was an important point of discussion. In addition
to building on our mutual strengths in areas such as software speciﬁ-
cation an important topic was a shared desire to create a programme
of researching the area of computational science – using the concepts
and methods of computer science to accelerate the pace of scientiﬁc
development and explore the potential for new approaches to science
exploiting computer science concepts and methods. This paper explores
what computational science is and the contribution it can make to
scientiﬁc progress. It is in large part abridged from a report “Towards
2020 Science” [1] published by a group of experts assembled by Microsoft
Research who met over three intense days to debate and consider the
role and future of science, looking towards 2020 and, in particular,
the importance and impact of computing and computer science in that
vision.
24.1 Introduction
Computers have played an increasingly important role in science for 50
years. At the end of the twentieth century there was a transition from
computers supporting scientists to do conventional science to computer
science itself becoming part of the fabric of science and how science is
done. From the traditional view of science comprising both a theoretical
From Semantics to Computer Science Essays in Honour of Gilles Kahn,
eds Yves
Bertot, G´erard Huet, Jean-Jacques L´evy and Gordon Plotkin. Published by Cambridge
University Press.
c
⃝Cambridge University Press 2009.
529
https://doi.org/10.1017/CBO9780511770524.025 Published online by Cambridge University Press

530
A. Herbert
and an experimental basis, increasing there is a third leg of “computa-
tional” science using computer science tools for modeling, experiment
sensing and control, data mining, information interaction and collabo-
ration in addition to pure numerical computation and data processing.
The history of science gives many examples of conceptual and
technological
tools
that
have
transformed
scientiﬁc
development
creating new approaches to science and indeed sometimes new sciences
themselves. For example, Fibonacci introduced algebra as a new branch
of mathematics when he publish Liver Abaci in 1202; in 1604 Galileo
invented the telescope creating the science of astronomy, transforming
our understanding of our world and the universe.
The developments in science being made possible now by computers
have the potential to be at least as transforming as these earlier tools.
24.2 Computational science
Science continually pushes the limits of what is possible in computing,
and in some areas is leading to computing advances, making possible
experiments that would have been impossible only 10 years ago, and
changing the way scientists do science.
There are experiments generating vast volumes of data: the Sanger
Centre in Cambridge, UK currently hosts 150 terabytes of genomic data
and clusters of computers totaling 2.5 Teraﬂops. Particle physics is set
to generate petabytes of data when the CERN Large Hadron Collider
(LHC) comes on-line in 2008. CERN’s solution is to use a computing
“Grid,” one of many being developed world-wide and an example of a
vision of e-Infrastructure (EU)/Cyber-infrastructure (USA).
Even with the relatively simple data collected by particle physics
experiments, data management is a major issue. The capabilities of
a ﬁle system to store and transmit bulk data from experiments has
to be extended to include indexing and structure to enable eﬃcient
query operations. Extensive metadata needs to be kept to describe
each experiment and the data it produces. The full power of relational
databases is needed to allow eﬀective interactions with the data and a
programmatic interface that can be used by tools for visualization and
analysis.
Other disciplines bring diﬀerent challenges. Astronomy has far more
emphasis on the collation of federated datasets held at disparate sites
and less demand for massive computation – modelling can be done
on departmental high-performance computing facilities. Chemistry is
https://doi.org/10.1017/CBO9780511770524.025 Published online by Cambridge University Press

Computational science: a new frontier for computing
531
diﬀerent again – teams are often small and there is little computational
infrastructure in use today. In the life sciences problems are mostly (but
not universally) related to heterogeneous, dispersed, and rich collections
of data rather than problems of scale and computation.
24.3 Systems architectures
For most of the twentieth century computing power was driven by
Moore’s law – as the number of transistors on a chip doubled more-or-less
every 18 months this was translated into a doubling of computational
throughput. Early in the twenty-ﬁrst century this is less clear and
many predict computers will no longer be exponentially growing in
computing power: memories and network bandwidth may be able to
continue on an exponential path; latency remains ﬁxed by the speed
of light. Thus individual computers may not get much faster, but
we will see the development of parallel computers, from multi-core
to many-core. From a programming point of view this will require a
paradigm shift away from the current sequential approaches to software
design to architectures that permit a mix of parallel and distributed
processing and are explicitly designed to cope with high latency when
a task have to be handed oﬀto a remote machine. This is likely to
bring two forms of systems architecture to the fore: “peer-to-peer” and
“service-orientated”.
Peer-to-peer architectures enable the construction of distributed
systems without any centralized control or hierarchical organization [2],
and can be designed to scale up to large systems and to use redundancy
to provide resilience and performance. Many peer-to-peer architectures
are able to automatically balance locality to overcome latency against
load-balancing to maximize utilization.
While peer-to-peer systems enable the re-use of memory and computa-
tional resources on a massive scale the paradigm of service-oriented
architectures [3] assist in the reuse of functionality. The fundamental
primitive of service-oriented architectures is the ability to locate and
access a computational service across machine and organizational
boundaries in both a synchronous (request–response) and asynchronous
fashion. The implementation of a service can be a wrapper for legacy
scientiﬁc applications and resource schedulers providing a migration
path. Computational scientists will be able to “orchestrate” these
services into scientiﬁc workﬂows to automate many currently manual
tasks.
https://doi.org/10.1017/CBO9780511770524.025 Published online by Cambridge University Press

532
A. Herbert
24.4 Semantics of data
A revolution is taking place in the scientiﬁc method: “hypothesize, design
and run experiment, analyze results” is being replaced by “hypothesize,
look up answer in database” [4]. Databases are an essential part of the
infrastructure of science: they may contain raw data, results of analyses
or simulations or the results of the annotation and organization of data.
The current trend is towards the worldwide publication of data.
A major issue is the distributions of data. It has long been known
that it is expensive or impossible to move large amounts of data –
it is better to take the program code (query) to the data and this
is the core of distributed query optimization. Traditionally queries
have been optimized for small numbers of databases, but how do we
optimize queries for say a network of several million sensors? How do
we extend query models developed for tabular business data (records)
to spatial queries, text-mining, stream of real-time data and other needs
of scientiﬁc processing?
These are questions of the base technology that has to be developed.
It must be supported by a programming environment that is easy for
scientists to use. First and foremost is the semantics of data. This
involves understanding the metadata (information about the organi-
zation of the data), the quality of the data, where and how it was
produced, who owns it and so forth. This data about data is not just
for human consumption it will primarily be used by tools that perform
data integration and use web services to share, transform or analyze the
data.
Attempting to solve the problems of scientiﬁc management by building
large centralized archival repositories is both dangerous and unworkable.
The danger comes from the dependence of ongoing administrative and
ﬁnancial support; unworkable because of scale and the natural desire of
scientists to be autonomous and keep control over their information. But
by moving to highly distributed and derived data there is the unsolved
problem of preserving the scientiﬁc record. How do we record the process
by which a dataset was derived? How do we record the history of a
dataset that is in continual ﬂux? How do we trace the origins of data that
has been replicated many times? These are as much social as technical
questions and require community standards for publishing metadata,
citations and provenance.
https://doi.org/10.1017/CBO9780511770524.025 Published online by Cambridge University Press

Computational science: a new frontier for computing
533
24.5 Intelligent interaction and information discovery
The demand for computational resources to perform scientiﬁc data
analysis is driven by three factors: more sophisticated algorithms
consume more instructions per byte of data processed; many algorithms
are polynomial in complexity, needing N 2 or N 3 steps to process N
data; I/O bandwidth has fallen behind storage capacity. Scientists need
better approximate algorithms with near-linear execution time, parallel
algorithms that apply many processors (and disks) and they need the
ability to steer long-running computations in order to prioritize the
production of data that is more likely to be of interest.
Many scientists use packages such as MATLAB to aid in data analysis
and hypothesis testing. To bring these symbolic computation tools closer
to the data and accessible from mainstream scientiﬁc programming
languages scientists need programming languages that can capture
mathematical models and compilers that turn them into deployable
executable software. By the same token, scientists need to extract valid,
authentic and actionable patterns, trends and knowledge from large
databases using algorithms such as automatic decision tree classiﬁers,
Bayesian prediction, sequence clustering, time series, linear regression
directly integrated into database engines.
It is increasingly compelling to integrate precision and accuracy into
type systems and to develop ﬁrst-class data types that perform scientiﬁc
error propagation and for these to be included in database query, search
and data mining tools.
Large observational datasets, the results of numerical computations
and high-dimensional analysis all require data visualization to aid
intuition and communication of results. Simulations depend on visual-
ization for the interpretation of results and hypothesis formation. Many
scientists need to create multi-dimensional aggregations to experiment
with correlations between measured and derived quantities. Today much
of this processing is done using home-brew software or simple spread-
sheets. There is a tremendous opportunity to exploit online analytical
processing (OLAP) add-ons to modern database engines. These allow
for the construction of “data cubes” serving as caches or replicas of pre-
computed, multi-dimensional aggregations that facilitate data analysis
from multiple perspectives. Second the support the visualization of data
over data partitions. Database technology will aid science ﬁrst through
the transformation of large-scale scientiﬁc data sets into schematized
small scale formats and then the transformation of small-scale formats
https://doi.org/10.1017/CBO9780511770524.025 Published online by Cambridge University Press

534
A. Herbert
into graphical data structures such as meshes, textures and voxels. The
ﬁrst is what OLAP can oﬀer; the second is a challenge for research and
development.
To bring together advances in data management, analysis, knowledge
discovery and visualization and empower the individual scientist another
important component is a truly smart (electronic) lab notebook. Such
a device would unlock access to data and make it extremely easy to
analyze, discover, visualize and publish new phenomena [5].
24.6 Transforming scientiﬁc communication
The web is reshaping scientiﬁc publishing and communication. Given
that science is a global endeavour and the web is perhaps the most
eﬀective global communications medium yet devised this is not a
surprise, yet the potential for the web to reshape scientiﬁc communi-
cation is underestimated. The challenge is not merely to adjust the
economics of publishing but to deﬁne the very nature of scientiﬁc
publication.
Online scientiﬁc publications will become interactive allowing readers
to explore visualizations and data, to search and navigate across all of an
author’s works, citations of that work and the author’s own use of others
work. Online pages will be generated at the moment they are requested
allowing customization according to a particular time and place, and
the reader’s access device – which could be a smart phone, a personal
computer or richly equipped collaboration environment. Responses will
be personalized to the reader: for example when looking outside their
main area of research, readers may prefer summaries or tutorials to full
articles. Researchers in the ﬁeld may want to read just the abstract
then directly access ﬁgures and data. Students may want to read the
whole paper including supplementary materials and full explanation of
experimental protocols, etc.
Modern scientiﬁc communication is dominated by databases and
journals, yet these are poorly integrated today. In the future many
scientiﬁc journals will become databases – the data will be peer reviewed
and the author credited for making the data available, even if no
immediate conclusions have been drawn from it. The technical challenges
are three-fold – managing the volume of data, tracking versions and
the provenance of data, and creating open structured machine readable
formats.
https://doi.org/10.1017/CBO9780511770524.025 Published online by Cambridge University Press

Computational science: a new frontier for computing
535
The web has evolved from a publishing medium to one which enables
discussion and dialogue using tools such as blogs and wikis. These allow
users to contribute content and for others to comment on their postings,
and automated services to feed notiﬁcation and summaries of new
content to a community of interest. These will become important ways
for scientists to organize, share and discover information building and
extending on-line collaborative social networks.
24.7 Exploiting computer science concepts
Concepts, theorems and tools developed with computer science are
ﬁnding wide-ranging application in sciences involving complex systems,
notably chemistry and biology. Computer science deals with dynamics
in a discrete and reactive sense. In most areas of science the discrete
is often not only more central but often harder to deal with – indeed
biological systems are the most exciting dynamic systems we know:
they are reactive, and they not only respond but also prescribe, cause
and indeed program other behavior.
One of the ﬁrst glimpses of the potential of computer science
techniques has been demonstrated in the Human Genome project
and the success in structural biology to routinely decipher the three-
dimensional structure of proteins. Biologists abstract genes as strings
and proteins as labelled graphs to code their knowledge in a form that
is amenable to computer processing and storing in computer databases.
The coding of knowledge empowers scientists by allowing them to share,
compare, criticize and correct scientiﬁc knowledge. It also changes
the way science is done: coded scientiﬁc knowledge can be analyzed
computationally before any experimentation is done. Coded knowledge
can be checked for consistency between code theories and for consistency
with collected experimental data. Inconsistencies may be resolved by
computer-designed experiments, analogous to automated testing for
computer hardware and software and computational analysis of theory
versus experimental data may suggest additional experiments to be
performed manually or even perhaps automatically.
Some computer science concepts are already familiar in other sciences.
Abstraction is a fundamental tool for looking at a system consistently
across diﬀerent levels of detail. It is a fundamental tool for managing
complexity. The concepts of concurrency theory, such as processes, non-
determinism, communication and synchronization may prove essential
for understanding inter- and intra-cellular biological processes. The
https://doi.org/10.1017/CBO9780511770524.025 Published online by Cambridge University Press

536
A. Herbert
interchangeability of program and data, universal computers, compilers,
interpreters, partial evaluation and compositional semantics may prove
essential to understanding the full role of DNA. Modularity and well-
deﬁned interfaces are key concepts in computer systems design – they
ensure that error in one component have a limited eﬀect on other
components and therefore can be tracked and ﬁxed. They also ensure
designs can be modiﬁed and evolved as requirements change. Modularity
and evolution appear to be key principles of biology – indeed we can
speculate that non-modular systems if they ever existed were not able
to evolve and withstand changes in external conditions and accordingly
have not survived.
24.8 Integrating theory, experiments and models
By bringing together modelling as a scientiﬁc tool and data as a scientiﬁc
resource we are able to tie together theories, models and experiments.
In simplistic terms a model is constructed as a “theory”; a set of inputs
are given to the model and behaviours observed. These behaviours are
compared to experimental data gathered under similar conditions and if
the correspondence between model and experiment holds over a range of
inputs, the theory is accepted. Once suﬃcient conﬁdence is established
in the model it can be used in place of experiment or as the input to
other models.
This is of course simplistic. In a real setting there are no clean sets
of data – experimental data are subject to noise, experiments may be
contested and models may be tainted with false assumptions. The model
may be congruent with a limited range of inputs but not all. There
may be other mechanisms that equally validly predict the experimental
outcome. We therefore need a framework in which data, models and
the dynamic relationships between them can be managed – this is what
a computer scientist would call conﬁguration control and introduces
the need for sophisticated version tracking and support for ﬁne-grained
scientiﬁc workﬂows alongside static data archiving and computational
modelling.
24.9 Complexity and coherence
One of the greatest challenges to science is to understand complexity
be it intra-cellular networks, organ systems, ecosystems, social systems
and indeed commerce. Paradoxically highly complex systems often
https://doi.org/10.1017/CBO9780511770524.025 Published online by Cambridge University Press

Computational science: a new frontier for computing
537
produce coherent behaviour and the question has to be how can we
understand and predict this? If approaches enabling coherence can
be made to emerge from complexity, all areas of applied science will
beneﬁt. Systematic approaches, enabled by an ability to predict always
trump heuristic ones, not least because they are less costly.
Complexity is grist to the mill of biology, computer science and
mathematics. There are opportunities to better reconcile the approaches
taken in these diﬀerent disciplines. In computer science space and time
complexity of algorithms is studied – complexity is represented as a
function of the size of a problem. Another computer science measure of
complexity is Kolmogorov or “descriptive” complexity – given a string,
what is the shortest program that can output that string? This metric
can perhaps help study biological descriptions such as lineage trees.
A natural representation of complex systems is as a matrix of
interconnected elements in which the interactions appear as the pattern
of connections in the matrix. In models of this kind the function or
failure of individual elements becomes less important than the statistical
dynamics or combinations of elements as in “system” or “network”
failures and the failure modes of such systems are often invisible. As
the number of elements and connections increase the behaviour of the
system may become less intuitive and indeed for very-large-scale systems
strikingly counter-intuitive. Computational tools can render these
counter-intuitive features visible and provide the basis for predictive
capability and provide the key for understanding complex systems
more successfully. The major problem is determining the structure and
topology of networks in complex systems. Studies have shown that
most important complex biological networks share elements of topology.
Connectivity is generally described by a power law distribution, which
gives these systems the robust property that network integrity is only
slowly degraded by random failure or deletion of notes, but is very
vulnerable to an intelligently targeted attack. Computer science models
for network complexity have focused largely on “scale free” topologies
and the identiﬁcation of “hubs” as key nodes on which the integrity
of a network depends. Further progress requires improved abilities
to measure complex systems; to model the eﬀect of perturbations or
designed changes upon them and to change complex systems in desired
directions.
https://doi.org/10.1017/CBO9780511770524.025 Published online by Cambridge University Press

538
A. Herbert
24.10 New tools
The invention of new tools, for example the telescope and the electron
microscope, typically form the building blocks of scientiﬁc revolutions
that historically have changed the course of history and society. New
conceptual tools are emerging from the intersection of computer science,
mathematics, biology, chemistry and engineering which could be equally
profound in their impact.
Codiﬁcation of scientiﬁc knowledge – representing scientiﬁc knowledge
in a code representation in terms of programs and data that are
executable and accessible to automatic analysis is a major activity in
many areas of science today. Codiﬁcation has an important property –
once obtained it can be right or wrong but importantly it is exactly
reproducible and analyzable.
Computer science can be viewed as the codiﬁcation of information
processing knowledge – algorithms and data structures are coded as
software or hardware realizations. In mathematics considerable progress
has been made in attempts to completely mechanize the production of
major mathematical proofs. Moreover many key numerical and symbolic
mathematical techniques are already encoded as libraries and tools.
Biology is an area where codiﬁcation is essential to progress. At the
lowest level we have the codiﬁcation of the genome as strings in a four
letter alphabet; at the next level we have proteomics which requires
representations of amino acids and three-dimensional structures. These
representations are now relatively standardized and tools exist to
exploit them. Further eﬀorts involve the coding of metabolic and
signalling pathways, where networks of biochemical interactions have to
be represented. This is still an open question, although many pathway
databases are being created. The hardest problem will be to store,
search, compare and analyze biological processes such as cell division.
This last example brings into focus that it is not in general suﬃcient to
represent facts as data, but rather as dynamic processes.
Codiﬁcation is therefore largely an engineering enterprise: there are
numerous issues about the best way of representing any particular
piece of information. The contribution of computer science is through
generally applicable principles to eﬀective codiﬁcation: abstraction,
composition, re-use and scalability.
The value of a scientiﬁc theory is its ability to make predictions. In
many areas of science the underlying equations are well understood and
in principle, predictions simply involve solving these using appropriate
https://doi.org/10.1017/CBO9780511770524.025 Published online by Cambridge University Press

Computational science: a new frontier for computing
539
numerical
or
symbolic
techniques.
Performance
improvements
in
computers enable predictions to be made for larger and more complex
systems and in turn these predictions are used by scientists as
simulations to explore phenomena where the cost of doing so experi-
mentally would be prohibitive or indeed impossible. In this respect we
can look at computers as “prediction machines”.
However in most areas of science the complexity of the domain, or
the absence of suﬃciently precise models prohibit direct simulation. In
such cases statistical approaches, in particular machine learning, have
proved to be very powerful. The goal of machine learning is to use
statistical methods to make predictions: for example, in a supervised
learning scenario a large number of input–output response pairs are used
to construct a probabilistic model of a system, capturing the underlying
trends and extracting them from the noise. The model is then used to
predict the responses to new inputs. Machine learning techniques are
also used for data visualization, data mining, data ﬁltering and a host
of other applications relevant to science.
In biology, Inductive Logic Programming – a form of machine learning
which represents hypotheses using logic – has been demonstrated on
tasks including the discovery of structural principles for major families
of protein folds [6], predictions of structure–activity relations in drugs
[7] and predictions of toxicity of small molecules [8]. Bayesian networks,
whose structure is inferred from observed data, have been used to model
the eﬀect of toxins on networks of metabolic reactions within cells.
Research into algorithms and methods for machine learning provide
insights into biological information processing systems, including the
human brain. For example in low-level visual processing by the brain’s
cortex there are marked similarities to wavelet feature bases, a technique
used in computer vision “object recognition”. More speculatively,
current research in hybrids of generative and discriminative models
of machine learning may shed light on the human brain’s remarkable
ability to accurately generalize training data which is almost entirely
unlabelled.
Machine learning is not limited to batch computation – with active
learning techniques the adaptation to the data and the prediction process
are intimately linked with the model pointing to new regions in the
space of variables in which to collect or label data so as to be maximally
informative.
Machine
learning
systems
that
produce
human-comprehensible
hypotheses from data will increasingly be used for knowledge discovery
https://doi.org/10.1017/CBO9780511770524.025 Published online by Cambridge University Press

540
A. Herbert
with science. Today such systems are open loop with no connection
between the machine learning system and the collection of data. A
more direct closed loop approach was investigated in the 1990s in
work on automating chemistry experiments [9] through the estimation
of chemical parameters. Recent advances are at the threshold of
“autonomous experimentation” in which artiﬁcial intelligence techniques
are used to carry out the whole cycle of scientiﬁc experimentation
including the origination of hypotheses to explain observations, the
devising of experiments to test the hypotheses and physical implemen-
tation of the experiments using robots to falsify hypotheses. Such a
system has already been demonstrated in the “Robot Scientist” project
[10], where laboratory robots conducted experiments selected by “active
learning”.
One exciting development we might expect to see in this area in
the next 10 years is the construction of a micro-ﬂuidic robot scientist.
The key eﬀects of miniaturizing the technology would be the reduction
of the experimental cycle time from hours to milliseconds, with a
corresponding increase in the robustness of experimental outcomes.
With the conﬂuence of wireless networking of “lab on a chip” sensor
technology we can anticipate large-scale applications to environmental
monitoring – each sensor node is likely be endowed with a limited supply
of wet chemistry and accordingly can only perform a limited number of
experiments – the network of such sensors must collectively decide how
these resources are spent.
Another key area for robot scientists is where experiments necessarily
occur in a place inaccessible to humans and where there is limited
bandwidth for remote control – for example, on the seabed, in outer
space or other hostile environments. An autonomous robot scientist
would decide for itself the next experimental step and data reduction.
These advances will increasingly blur the distinction between the
“artiﬁcial” and the “living”: living cells are the most sophisticated non-
systems known. Recent progress in micro-ﬂuidics and nanotechnology
has opened this environment to practical engineering experience. Simple
laboratory workﬂows such as sequences of reactions followed by product
analysis can be implemented on a single chip for mass production.
Speciﬁc macromolecules or cells can be speciﬁcally separated enabling
new types of experiments to be studied, and since small volumes of
supplies are consumed more extensive studies can be conducted.
Although lab-on-chip technology allows for control of molecular
interactions it pales in comparison to the production capabilities of
https://doi.org/10.1017/CBO9780511770524.025 Published online by Cambridge University Press

Computational science: a new frontier for computing
541
the living cell. For the past two decades eﬀorts have been underway
to modify cells for factory production of chemicals that are either too
complex for classical synthesis or can be produced much more eﬃciently
by micro-organisms. So far examples have been relatively simple, but
as genome-wide computational models become more complex, these
models will enable the construction of cell factories that could change
the nature of production in the chemical and pharmaceutical industries.
The self-replication capability of cells lends itself to mass production
and a shift from static metabolic engineering towards interfacing with
the control structure of cells [11] opens the possibility to engineer novel,
living biomaterials. Cells tuned for a particular process can be grown
in bio-ﬁlms of communication elements extending on the idea of DNA
computers [12] and this is leading the ﬁeld of Synthetic Biology.
An alternative engineering approach would be to design a self-
replicating von Neumann computer – in essence a chemical universal
Turing machine. Such a device would, in simplistic terms, be an
automaton with a reaction ﬂask and a conveyor belt with an arrangement
of chemicals (the program). The automaton would have instructions to
add and remove chemicals from the ﬂask and control the temperature
of the reaction. As a theoretical concept the chemical universal Turing
machine uniﬁes lab-on-a-chip and artiﬁcial cell concepts: research is
required to establish its viability. But, if successful, it could have a
dramatic eﬀect on combining theoretical, experimental and modelling
approaches to understanding and engineering biology.
24.11 Solving global challenges
The twenty-ﬁrst century presents some of the most important questions,
challenges and opportunities in human history. Some have solutions
in scientiﬁc advance (e.g. health). Others require political or economic
solutions (e.g. poverty). Some require signiﬁcant scientiﬁc advance in
order to provide the evidence necessary to make economic and political
decisions (e.g. our environment).
There is an urgent need to understand the Earth’s life support
systems – the biosphere (biodiversity, ecosystems and atmosphere) to
the extent we are able to model and predict the eﬀects of human activity
on them, and the consequent eﬀect on the ability of life, including
human life, to be sustained on the planet. Several areas of science are
beginning to tackle this through integrating theory, remote sensing
experiments, traditional studies and computational models. It is not
https://doi.org/10.1017/CBO9780511770524.025 Published online by Cambridge University Press

542
A. Herbert
just a matter of studying climate – we need to understand biological
systems at the geographical scale and how they inter-relate with climate
and natural phenomena such as volcanoes and earthquakes. The key
trends in this area of computational science are autonomous experi-
mentation, particular remote sensing, distributed data management
to mine heterogeneous databases from wide range of ecological and
environmental sciences and new algorithms for the analysis, modelling
and simulation of large, complex systems.
A recurrent theme in this paper is the role of computational science
in understanding biology – particular the emerging discipline of systems
biology looking at cell level and higher interactions using models taken
from theoretical computer science. It is reasonable to hope that within a
decade we will have models of a person’s immune system and computa-
tional approaches to the design of preventative and therapeutic vaccines.
Some go as far as to posit a new approach to personal health care –
theranostics, in which diagnostic testing is used to diagnose a disease,
choose a treatment and monitor the patient response to the therapy. This
requires the ability not just to understand the structure of biological
systems at the cell level but also to be able to fully understand the
dynamics by simulation and analysis.
Computational approaches to systems biology may also help address
the challenge of ﬁnding sustainable forms of energy, for example from
bio-energy crops with near carbon neutrality of grown, harvested and
converted eﬃciently with predictable performance.
At a higher level, understanding the human brain remains a major
challenge. Neuroscience has made dramatic strides in recent years –
computational circuits are expressed in terms of connectivity between
neurons and synapses; complex large-scale connectional systems have
been associated with individual brain functions and correlated with
physiological properties. It is increasingly understood how neuron
function relates to human perception and in turn how this relates to
models of regularities and other statistical properties of the world. A
complete model of the brain remains intractable, but experimental work
has shown that all central sensory systems are somewhat discrete, with
each being hierarchical and this opens the opportunity to use computa-
tional models for elaborating how complex brain processes interact.
The key computer science technology in developing such understanding
is likely to be machine learning and inference. It is intriguing that
almost all theories of brain function suppose there is no meaningful
concept of “software” that applies, so there may also be the potential
https://doi.org/10.1017/CBO9780511770524.025 Published online by Cambridge University Press

Computational science: a new frontier for computing
543
for neuroscience to propose new computational artefacts and in turn
enable greater capabilities for future computational science.
Computational science has several roles to play in confronting
global epidemics of infectious disease such as “severe acute respiratory
syndrome” (SARS) and avian inﬂuenza (“bird ’ﬂu”). First, collection
and analysis of genomic and proteomic data for pathogen identiﬁcation
and diagnostic screening. Second, real-time epidemic surveillance at
local and regional scale. Third, predictive modelling of disease spread
and control measures, both in civil preparedness and medical response
to an unfolding outbreak. Fourth, facilitating eﬀective communication
and management in ad-hoc global inter-disciplinary teams responding
to a new outbreak. Computational models of disease transmission
will integrate epidemiological and biological data to give insights into
patterns of spread and the eﬀects of interventions. These models will
necessarily be at a range of scales from local to global, and a key
question will be to discover which controls are the most eﬀect at
each scale. Techniques such as agent-based simulation driven by data
collected from the ﬁeld, and integration of location information from
transport and mobile telephones to track population behaviour would
represent a truly global resource for planning for and responding to
future epidemics.
Understanding the origin, workings and ultimate fate of the Universe
is one of the great questions. With modern computational tools and
methods being used on an unprecedented scale we may be able to
answer these questions within a generation. Satellite data has moved
cosmology into the realm of precision science. The data are in agreement
with the consensus “hot Big Bang” model, however they indicate
only 4% of the Universe consists of ordinary matters, and the rest
comprises dark matter (23%) and dark energy (73%). It is therefore
necessary to investigate the nature of dark energy and dark matter and
to understand the properties of the very high gravitational ﬁeld which
would have occurred during the Bang. The data to do this will come
from particle physics and experiments in this ﬁeld are totally reliant
on advanced computation, indeed particle physics experiments already
reject 99.999% of all collisions before writing them to mass storage,
using real-time pattern recognition algorithms running on the detectors
themselves. The next generation will require computational tools on an
unprecedented scale. For example, the CERN Large Hadron Collider
which started operation in 2008 will generate several petabytes of data
each year, feed out to a worldwide federation of national “computing
https://doi.org/10.1017/CBO9780511770524.025 Published online by Cambridge University Press

544
A. Herbert
grids” linking 100 000 CPUs. More challenging again will be the tools to
enable thousands of end users to analyse the physics objects. Usability
will be a key since all aspects of data cataloguing and code handling will
necessarily have to be automated. A user cannot be expected to know
the location of the data required, or the resources required to process it.
A completely transparent interface between the scientist’s desktop and
the computational resources will be needed to allow the user to focus
on the science rather than the infrastructure. It will require a move
away from batch processing to a more interactive, distributed method of
working. This will require data storage solutions combining the features
of relational databases and conventional ﬁle systems with advanced
caching features across wide area network links to avoid intolerable
latency issues.
Understanding the large-scale structure of the Universe requires that
data from surveys are compared to models which predict the distribution
of galaxies. Ideally such comparisons would range over many model
parameters and indeed competing models, providing an opportunity for
adopting Bayesian machine learning methods to search the parameter
spaces eﬃciently and make robust statistical inferences. There is great
potential for tools that steer computation, ﬁt models and automate data
analysis.
24.12 Conclusion
From this overview of the state-of-the-art and future prospects for
computational science it is evident that cutting edge computer science
will play a pivotal role. Across the sciences there are common themes of
handling federations of heterogeneous data, modelling complex systems
at multiple scales, simulation and visualization and automation of
scientiﬁc workﬂows in experimental science, in scientiﬁc analysis and in
scientiﬁc publishing. Seeing this challenge was one of the motivations
that led Gilles Kahn to propose a joint laboratory with Microsoft with
working on these topics as a key focus.
Bibliography
[1]
S. Emmott (ed.). Towards 2020 Science, available from http://www.
research.microsoft.com/towards2020science.
[2]
A. Rowstron and P. Druschel. Pastry: scalable, distributed object location
and routing for large scale peer-to-peer systems. IFIP/ACM International
https://doi.org/10.1017/CBO9780511770524.025 Published online by Cambridge University Press

Computational science: a new frontier for computing
545
Conference on Distributed System Platforms (Middleware), Heidelberg,
Germany pp. 329–350, November 2001.
[3]
C. Ferris and J. Farrell. What are web services? CACM 46(6):31, 2003.
[4]
M. Lesk. Online Data and Scientiﬁc Progress: Content in Cyberin-
frastructure. http://archiv.twoday.net/stories/337419/, 2004 [Accessed 6
December 2005].
[5]
J. Gray, D. T. Liu, M. Nieto-Santisteban, A. S. Szalay, D. De Wiit and
G. Heber. Scientiﬁc Data Management in the Coming Decade. Technical
Report MSR-TR-2005-10, Microsoft Research, 2005.
[6]
A. P. Cootes, S. H. Muggleton and M. J. Sternberg. The automatic
discovery of structural principles describing protein fold space. J. Mol. Biol.
330(4):839–850, 2003.
[7]
M. J. E. Stermberg and S. H. Muggleton. Structure Activity Relationships
(SAR) and pharmacophore discovery using inductive logic programming
(ILP). QSAR Comb. Sci. 22:527–532, 2003.
[8]
S. H. Muggleton, H. Lodhi, A. Amini and M. J. E. Sternberg. Support
vector inductive logic programming. In Proc. 8th International Conference
on Discovery Science, volume 3735, Lecture Notes in Artiﬁcial Intelligence,
pp. 163–175. Springer Verlag, 2005.
[9]
J. M. Zytkow, J. Zhu and A. Hussam. Automated discovery in a chemistry
laboratory. In Proc. 8th National Conference on Artiﬁcial Intelligence,
Boston, MA, USA, pp. 889–894, AAAI Press, MIT Press, 1990.
[10]
R. D. King, K. E. Whelan, F. M. Jones, et al. Functional genomic
hypothesis generation and experimentation by a robot scientist. Nature
427:247–252, 2004.
[11]
H. Kobayashi, M. Kaern, M. Araki, et al. Programmable cells: interfacing
natural and engineered gene networks. Proc. Nat. Acad. Sci., USA,
101(22):8414–8419, 2004.
[12]
S.
Basu,
R.
Mehreja,
S.
Thiberge,
M.-T.
Chen
and
R.
Weiss.
Spatiotemporal control of gene expression with pulse generating networks.
Proc. Nat. Acad. Sci., USA 101(17):6355–6360, 2004.
https://doi.org/10.1017/CBO9780511770524.025 Published online by Cambridge University Press

https://doi.org/10.1017/CBO9780511770524.025 Published online by Cambridge University Press

