1
Introduction
This document summarizes an approach to driving the emotional experience and 
expression of embodied agents controlled by OpenCog.   
The two motivating examples here are:
A humanoid robot with the ability to express emotion via its choice of animations, 
modulation of animations, and tone of voice; and to “experience” emotions via 
(in the OpenCog system controlling it) modulating its action selection and cognitive 
processes based on emotional factors.
A humanoid avatar living in a 3D virtual world, such as Sophiaverse, with similar 
properties to the above robot.
The central idea presented here is to integrate the Component Process Model (CPM) with 
the Psi Model (the latter underlying OpenCog's OpenPsi framework for motivated action), 
two previously distinct models for describing and simulating human emotion. This 
integration leads to other useful things almost automatically, since close connections 
between CPM and facial expressions and other physiological parameters already exist 
and are documented.
The Component Process Model
We will not review the Psi model here as it has already been discussed and written about 
extensively in an OpenCog context (see e.g., Cai Zhenhua’s paper from his PhD thesis).   
We will brieﬂy review the CPM before getting started with our original suggestions.
The CPM has been selected for use here for several reasons, one of which is that it has 
already been explicitly connected with physiological parameters and facial expressions. 
Connecting CPM to Psi and unraveling the various details then gives us a holistic frame-
work interrelating cognitive/perceptual control parameters (what Psi calls “modulators”), 
simulated-physiological parameters (e.g., heart rate), emotions (primary and second-
ary), animations and gestures and modulations of thereof, and evaluative processes 
assessing aspects of the system's state (what the CPM calls SECs, Stimulus Evaluation 
Checks).
Emotion, Physiology,
Motivated Action and Gesture/Expression 
for Humanoid Robots and Avatars
Mike Duncan  & Ben Goertzel

2
For background on the CPM model of emotion and associated cognitive and biological 
dynamics, see the two papers:
The dynamic architecture of emotion: Evidence for the component process model,
by Klaus R. Scherer , 
http://www.aﬀective-sciences.org/system/ﬁles/biblio/2009_Scherer_C&hE.pdf 
(see especially Table 1)
A systems approach to appraisal mechanisms in emotion, by David Sander*, Didier 
Grandjean, Klaus R. Scherer, 
http://cms2.unige.ch/fapse/EmotionLab/pdf/SanderGrandjeanScherer_2005_neu
ralnetw.pdf (see especially Table 3)
See the Table below for a concrete ﬂavor of the CPM:

3
The ﬁrst column includes SECs. The table cells indicate the degree to which each of 
these SECs leads to the expression (or in some cases inhibition) of a certain emotion.
See also the spreadsheet “SEC to Expression Table”  which contains (as well as some 
other stuﬀ) an editable version of Table 1 from the ﬁrst CPM paper referenced above. 
This table maps SEC values into physiological expressions, e.g. “novelty” corresponds to 
possibilities like “brows and lids up, frown, jaw drop, gaze directed.”
What we can get from the CPM in a practical robot emotion control context, it seems, is 
two things:
Direct mapping from SEC evaluations to physical expressions
Mapping from SEC evaluations to emotions
Note here that
Physical expressions emanating from multiple sources can be enacted concurrently 
or simultaneously and then blended together.
The level to which a certain emotion is active may be the combination of multiple 
factors, e.g. the contributions of the many concurrent SEC factors may add up.
The Psi modulators play a diﬀerent role than the CPM SECs. Speciﬁcally, the Psi 
modulators are parameters intended to modulate the behavior of e.g. action selection 
or motivated inference. The values of the modulators may then be modulated 
themselves by the SEC values.  
Psi contains a theory of emotion, via which the nature of the emotions experienced by a 
cognitive system corresponds to the vector of modulator values. (So the modulators are 
considered as dimensions of an n-dimensional space, and commonplace emotions are 
mapped into regions in this space.)    If we connect CPM and Psi and let the 
implementations of these models in OpenCog act in the most obvious ways, we then 
have a situation where
CPM SEC values drive emotion directly, and also drive Psi modulators
Psi modulators drive emotion directly as well
One could view this as redundant, and view the relation between Psi modulators and 
emotion as implicit and descriptive rather than as something that should directly drive 
modiﬁcation of emotion-representing Atoms in OpenCog. So long as the emotions 
determined by SEC values are consistent with the modulator values determined by 
these SEC values, according to Psi theory, this would be no problem. On the other hand, 
all these mappings are suﬃciently imprecise that it seems unproblematic and perhaps 
better to allow Psi modulators and CPM SECs to both guide the truth values of 
emotion-representing Atoms directly, and have their contributions combined via the 
revision rule.   

4
Types of Variables/Entities Involved
Based on the integration of the CPM and Psi, what is proposed here is to formulate a set 
of hand-coded rules (ImplicationLinks) embodying relationships between the following 
types of entities.  
1. SECs: Stimulus Evaluation Checks. Examples would be “Novelty” or “Urgency” or 
“Degree of control (over the situation)” or “intrinsic pleasure.”  In OpenCog each of 
these would be a GroundedPredicateNode which, when evaluated, would tell you the 
degree to which the given SEC holds at the current moment. 
2. Modulators. These are parameters that guide the behavior of cognition and per-
ception, e.g. “arousal”, “securing threshold”, “selection threshold”, “resolution level.”  In 
Zhenhua's old OpenPsi implementation, these were updated ongoingly by modula-
tor-update functions. 
3. Emotions. While emotions are complex and multidimensional and there is no way to 
make a comprehensive list of all emotions that a complex cognitive system will experi-
ence, still for the immediate term it is useful to create Atoms in OpenCog correspond-
ing to a list of basic emotions (the ones in Table 3 of the second CPM paper mentioned 
above may be a good start).
4. Physiological variables. These are simulations of physiological variables that vary 
in human beings in ways associated with emotions. An example is HeartRate. A subtler 
example is ergotrophic/tropotrophic balance (very roughly, the internal body balance 
between energetically doing new stuﬀ, and focusing on rest and body maintenance)
5. Facial expression animations & gestures. At the moment these come from a ﬁxed 
list of animations, though some animations have parameters associated with them.
One source of emotional facial expressions is this paper, which claims that human 
facial expression recognition boils down to expression of 22 elementary and compos-
ite emotions:  http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3992629/ﬁgure/ﬁg01/
6. Animation and gesture modulations. An example would be “start this animation or 
gesture extra-fast.”
7.  Voice modulations and emotions. Vocal expression can be made to express basic 
emotions like happiness, sadness and surprise. Diﬀerent types of voice modulation 
can also be done, e.g. wide, narrow, relaxed, tense, lazy, thin, full voice (each of these 
has diﬀerent frequency characteristics).
Actually, nothing new needs to be done here, except to create new animations for the 
remainder of the emotions required (as already suggested above).

