Salience Allocation as Guidance for Abstractive Summarization
Fei Wang‚Ä†‚àó, Kaiqiang Song‚Ä°‚àó, Hongming Zhang‚Ä°, Lifeng Jin‚Ä°, Sangwoo Cho‚Ä°
Wenlin Yao‚Ä°, Xiaoyang Wang‚Ä°, Muhao Chen‚Ä† and Dong Yu‚Ä°
‚Ä†University of Southern California; ‚Ä°Tecent AI Lab, Seattle
{fwang598,muhaoche}@usc.edu
{riversong,hongmzhang,lifengjin,swcho,wenlinyao,shawnxywang,dyu}@global.tencent.com
Abstract
Abstractive summarization models typically
learn to capture the salient information from
scratch implicitly. Recent literature adds ex-
tractive summaries as guidance for abstrac-
tive summarization models to provide hints
of salient content and achieves better perfor-
mance.
However, extractive summaries as
guidance could be over strict, leading to in-
formation loss or noisy signals. Furthermore,
it cannot easily adapt to documents with var-
ious abstractiveness. As the number and al-
location of salience content pieces varies, it
is hard to Ô¨Ånd a Ô¨Åxed threshold deciding
which content should be included in the guid-
ance.
In this paper, we propose a novel
summarization approach with a Ô¨Çexible and
reliable salience guidance, namely SEASON
(SaliencE Allocation as Guidance for Abstrac-
tive SummarizatiON). SEASON utilizes the al-
location of salience expectation to guide ab-
stractive summarization and adapts well to ar-
ticles in different abstractiveness. Automatic
and human evaluations on two benchmark
datasets show that the proposed method is ef-
fective and reliable. Empirical results on more
than one million news articles demonstrate a
natural Ô¨Åfteen-Ô¨Åfty salience split for news ar-
ticle sentences, providing a useful insight for
composing news articles.1
1
Introduction
Abstractive summarization seeks to generate con-
cise descriptions about synoptic information of
longer documents (Rush et al., 2015; Nallapati
et al., 2016; See et al., 2017). Tackling this task
can provide users with improved dissemination and
acquisition of more readable content in long doc-
uments. More concretely, it allows for enhanced
selection, compression and retrieval of Web-scale
*Work done during Fei Wang‚Äôs internship at Tencent AI
Lab Seattle. The Ô¨Årst two authors contributed equally.
1Code and model weights are available at https://
github.com/tencent-ailab/season.
Between iPhones, flat-screens and ‚Ä¶ 
A new report from Suncorp Bank ‚Ä¶
The report found Australians spent ‚Ä¶      
Men spent twice as much as women ‚Ä¶            
On average, men spent $2618 over ‚Ä¶      
The report also found that families ‚Ä¶     
'The report found adults without ‚Ä¶       
Despite the mounting costs, the ‚Ä¶
Mobile phone bills were the biggest ‚Ä¶
'Call and data plans for phones ‚Ä¶
'A quarter of Australians who use ‚Ä¶
Document Sentences
Extractive 
Summary
Salience 
Allocation
Figure 1: Illustration of different guidance. Extractive
summary is a strict guidance consisting of extracted
sentences labeled with check-mark. Salience allocation
is a Ô¨Çexible guidance mapping sentences to different
salience degrees shown as a bar chart.
textual information that beneÔ¨Åts other NLP tasks
such as machine reading comprehension (Inoue
et al., 2021), mention linking (Cheng et al., 2015),
claim veriÔ¨Åcation (Yin et al., 2021), and informa-
tion extraction (Lu et al., 2022).
Abstractive summarization models are typically
trained end-to-end using large collections of paired
corpora of raw documents and human-written sum-
maries to directly perform sequence-to-sequence
generation. In terms of deciding what to include
in the generated summaries, these models im-
plicitly learn to capture the salient information
from scratch. Accordingly, recent literature has
attempted to add auxiliary extractive salience guid-
ance for abstractive summarization models to give
them a higher-level understanding of input docu-
ments, among which, extractive summaries appear
to provide the most effective guidance (Li et al.,
2020; Jin et al., 2020; Dou et al., 2021). Methods
following this strategy learn to Ô¨Årst perform extrac-
tive summarization, then perform abstraction on
top of the extractive summaries (Hsu et al., 2018;
Pilault et al., 2020; Dou et al., 2021).
However, incorporating extractive summaries as
a form of guidance is evidently imperfect, even
arXiv:2210.12330v1  [cs.CL]  22 Oct 2022

though it improves the overall performance of ab-
stractive summarization in some cases (Dou et al.,
2021): 1) Extractive summaries are not reliable
guidance. When there are too many summary-
worthy sentences in the document, selecting a part
of them may prone to information loss. When
there are too few or no summary-worthy sentences,
using the selected extractive summaries could be
noisy and confusing to the model. 2) Extractive
summaries are not Ô¨Çexible to adapt to different
cases. The number and allocation of salience con-
tent pieces can vary by documents. Rather than
extracting a Ô¨Åxed number of sentences, a Ô¨Çexible
guidance should select salient content based on
document properties. An imperfect selection pro-
cess may also lead to further model biases, such
as positional biases or length biases (Zhong et al.,
2019). As the summarization process can differ for
distinct documents (Grusky et al., 2018; Koupaee
and Wang, 2018), a reliable guidance should al-
low Ô¨Çexible content selection, and be adaptive to
documents with different abstractiveness.
In this paper, we propose a novel summariza-
tion approach with a Ô¨Çexible and reliable salience
guidance, namely SEASON (SaliencE Allocation
as Guidance for Abstractive SummarizatiON).
Salience is the degree to which a sentence con-
tributes to the central idea of a document, and
its allocation means how salience is distributed
among all sentences in a document. To estimate
the salience allocation, a linear classiÔ¨Åer is trained
on top of the encoder. This estimation is incorpo-
rated into the decoder with Salience-Aware Cross-
Attention (SACA). It provides the Ô¨Çexibility to de-
cide how much signal to accept from the salience
guidance to supervise the abstractive summariza-
tion. The ground-truth salience label is assigned
to each sentence based on its similarity with the
ground-truth summary. Meanwhile, the number
of salience degrees and their cut-off thresholds are
decided based on the corpus to balance informative-
ness and prediction accuracy. To further improve
the robustness of the summarization model, we
apply label smoothing between adjacent salience
degrees during training, and use the expectation of
salience as a more robust salience estimation.
The technical contributions of this work are
three-fold. First, we develop a new method for
abstractive summarization on Transformer-based
encoder-decoder architecture with the allocation
of salience expectation as Ô¨Çexible guidance (¬ß3).
Our method provides reliable guidance that adapts
well to articles in different abstractiveness (¬ß5.1).
Second, we show the effectiveness and reliability
of our proposed method comparing to the existing
methods in both automatic (¬ß4.2) and human evalu-
ation (¬ß5.3). Third, empirical results on more than
one million news articles show a natural Ô¨Åfteen-Ô¨Åfty
salience split for news article sentences (¬ß4.3), pro-
viding a useful insight for composing news articles.
2
Related Work
Joint extractive and abstractive summarization.
Extractive summarization and abstractive summa-
rization are two general paradigms of text summa-
rization (See et al., 2017; Grusky et al., 2018). Ex-
tractive summarization ensures the faithfulness of
the generated summary but is not able to properly
summarize documents when rephrasing is needed
(Liu and Liu, 2009). Abstractive summarization,
comparatively, is more Ô¨Çexible but may suffer from
hallucination (Maynez et al., 2020).
A series of studies attempt to beneÔ¨Åt from the
advantages of both paradigms by combining them.
Hsu et al. (2018) encourage the word-level atten-
tion of an abstractive summarization model and the
relative sentence-level extraction probability from
an extractive summarization model to be consistent.
More recent studies show that conducting abstrac-
tive summarization with extractive summaries as a
part of the input leads to better performance (Saito
et al., 2020; Pilault et al., 2020; Dou et al., 2021).
Extractive summarization can also work as an ef-
fective content selector for abstractive summariza-
tion when summarizing long documents (Manakul
and Gales, 2021). Some studies (Gehrmann et al.,
2018; Li et al., 2020; Saito et al., 2020) also con-
sider to extract key words or phrases instead of
summary worthy sentences as guidance, but their
performances are not as good as those using sen-
tences (Dou et al., 2021).
Our work extends the strict extractive summary
guidance to a soft guidance of salience allocation.
The proposed guidance is more Ô¨Çexible, reliable
and adaptive, leading to better performance.
Selective attention. Selective attention is a psy-
chological concept referring to the differential pro-
cessing of simultaneous sources of information
(Johnston and Dark, 1986). Incorporating prior
knowledge through selective attention is widely ex-
plored in natural language processing, especially in

Self
Attention
Feed
Forward
Add & Norm
N√ó
Input
Embedding
Add & Norm
Self
Attention
Feed
Forward
Add & Norm
√óN
Output
Embedding
Add & Norm
Cross
Attention
Add & Norm
Linear
Softmax
Salience 
Probabilities
Softmax
Output
Probabilities
Linear
Salience
Embedding
Q
K
‚äï
V
ùùµ(x)
Figure 2:
Model architecture of SEASON.
The
proposed modules are highlighted with bold lines.
SEASON adds a salience predictor on top of the en-
coder, maps (the expectation of) salience degrees to
corresponding embeddings, and adds these salience em-
beddings to the key vectors of cross attention.
recent NLP models with attention mechanism (Lin
et al., 2016; Sukhbaatar et al., 2019; Pruthi et al.,
2020; Beltagy et al., 2020; Wang et al., 2022). To
modify the summarization process with selective
attention, previous studies either adjust the atten-
tion scores based on content selection probabilities
directly (Hsu et al., 2018; Saito et al., 2020; Li
et al., 2021), or appending selected content in the
input (Saito et al., 2020; Dou et al., 2021). Recent
studies show that the latter method with sentence-
level content selection performs better (Dou et al.,
2021).
Different from prior studies, SEASON maps
salience degrees to distinct embeddings and adds
them to the encoder outputs as key vector for cross-
attention. This gives our model the Ô¨Çexibility to
decide how much signal to accept from the salience
guidance for supervising the abstractive summa-
rization process. This strategy achieves better per-
formance in comparison with previous salience-
guided selective attention methods.
3
SEASON
In this work, we employ a Transformer-based
encoder-decoder model for abstractive summariza-
tion. As shown in Fig. 2, our model SEASON en-
capsulates salience prediction and text summariza-
tion in a single network. We perform multi-task
end-to-end training, and inference via one forward
pass. During training, the model jointly learns to
predict the degree of salience for each sentence
and is guided with ROUGE-based ground-truth
salience allocation to generate the abstractive sum-
mary. During inference, SEASON predicts the ex-
pected salience allocation intermediately with the
encoder outputs, and uses this predicted informa-
tion to guide the decoder to generate the summary.
3.1
Problem Formulation
Our assumption comes from an intuition that know-
ing the content salience allocation helps the model
to pay attention to important content and generate
more informative summaries. Although the con-
tent salience allocation is a built-in attribute of the
source document, it is hard for the model to lever-
age this attribute without direct supervision (Li
et al., 2020; Saito et al., 2020; Dou et al., 2021).
Let x be the sequence of input tokens in the
source document, and y be the sequence of the sum-
mary tokens, where every token xi or yi is in the
vocabulary V. We use zj, where j ‚àà{1, . . . , N},
to represent the salience degree of the j-th sen-
tence in the input document.
We deÔ¨Åne oi as
the sentence index for the i-th token, where oi ‚àà
{1, . . . , N}. The salience allocation is deÔ¨Åned as
Œ∂(x) = [f(zo1), . . . , f(zo|x|)].2 The problem can
be formulated as follows:
P(y|x) =
|y|
Y
k=1
pŒ∏(yk|y<k, x, Œ∂(x)).
(1)
In Eq. 1, each token prediction is conditioned on
the previously decoded summary tokens, the input
tokens in the source document, and the allocation
of salience of the source document.
3.2
Salience Allocation Prediction
To predict salience degrees of input sentences, we
slightly modify the encoder input sequence by
adding a special token at the beginning of each
sentence, obtaining their last-layer hidden states as
sentence representations:
[hsent
1
, . . . , hsent
n
] = Encoder(ÀÜx),
(2)
2f(¬∑) is a function that maps the sentence salience degree
to an embedding vector. In our implementation, we use the
ground-truth salience embedding for training, and the expected
embedding over the inferred salience distribution for testing.

where hsent
j
, j ‚àà{1, . . . , N}, is the contextualized
embedding of the j-th sentence, and ÀÜx is the modi-
Ô¨Åed input sequence. Then, sentence representations
are fed into a single-layer classiÔ¨Åcation head:
P(zj = l|x) ‚àùexp(
wT
l hsent
j
+ bu
œÑ
),
(3)
where œÑ is a sharpening coefÔ¨Åcient for the salience
degree distribution, l ‚àà{1, . . . , L} is the index
of salience degree, L is the number of salience
degrees, wl and bl are trainable parameters. We
provide discussions on L and œÑ in ¬ß4.3 and ¬ß5.4
respectively. The design above allows the model
predict salience allocation with minimal modiÔ¨Åca-
tions on the architecture.
3.3
Salience-Aware Cross-Attention
To explicitly incorporate the salience allocation
into the model , we develop a salience-aware cross-
attention (SACA) module. SACA Ô¨Årst maps the
salience degrees to trainable salience embeddings:
f(zj) = Emb(zj).
(4)
This operation is intuitive when using ground-
truth salience degrees. For predicted salience de-
grees, SACA needs to perform an estimation on
the salience embedding with the inferred salience
distribution.
A simple hard estimation can be
achieved by directly taking the embedding of de-
gree l that maximizes the probability:
f(zj) = Emb(argmax
l
P(zj = l|x)).
(5)
However, this direct estimation does not take the
uncertainty of prediction into consideration, so we
propose the soft estimation that calculates the ex-
pectation for the salience embedding:
f(zj) =
L
X
l=1
Emb(zj = l)P(zj = l|x).
(6)
We compare these two estimation methods com-
prehensively in ¬ß5.4. Next, SACA incorporates
the salience allocation in the cross-attention layer
to guide summary generation on the decoder side.
SACA adds the sentence salience embedding to the
encoder hidden state of each token belonging to the
sentence as the key state for cross-attention. The
cross-attention is formulated as:
CrossAttn(Q, K, V ) = MultiheadAttn(Q, K, V ),
where the attention query Q = hdecoder thereof
corresponds to the hidden state of the decoder, the
attention key K = hencoder+Œ∂(x) is the sum of the
encoder hidden state and the salience embedding,
and the value V = hencoder is composed of the
original encoder hidden state. In comparison with
adding salience scores to cross-attention scores di-
rectly, SACA allows the model to learn how much
signal to take from the salience guidance.
3.4
Learning Objectives
In training, SEASON learns to predict the salience
allocation and generate the summary simultane-
ously. For salience prediction, we use the averaged
cross-entropy loss on each predicted sentence:
Lcls = ‚àí1
N
N
X
j=1
log P(zj|x).
(7)
In addition, we apply label smoothing (Diaz and
Marathe, 2019) to the salience degrees for denois-
ing. SpeciÔ¨Åcally, a probability Œ≤ is evenly assigned
to salience degrees adjacent to the ground-truth de-
gree. Analysis in ¬ß5.4 shows its effectiveness com-
paring with common label smoothing. For sum-
mary generation, we use the ground-truth salience
allocation as input, and apply the averaged cross-
entropy loss on each predicted token as below:
Llm = ‚àí1
|y|
|y|
X
k=1
log pŒ∏(yk|y<k, x, Œ∂(x)).
(8)
We further combine two loss functions together
with a coefÔ¨Åcient Œ± that balances the two:
Ltotal = Llm + Œ±Lcls.
(9)
¬ß5.4 shows that SEASON is not sensitive to Œ±.
4
Experiment
In this section, we Ô¨Årst describe our experimental
setting, including datasets, baselines, evaluation
metrics and implementation details (¬ß4.1). Then,
we show the model performance on two summa-
rization datasets (¬ß4.2), and provide an insight on
salience threshold selection (¬ß4.3).
4.1
Experimental Setup
Datasets. We evaluate our method on two news
summarization datasets. For both datasets, we use
the original news article as input and the human-
written summary as the ground-truth output. CN-
NDM (See et al., 2017) consists of news articles and
their human-written abstracts from CNN and Daily

Mail websites, including 287,226/13,368/11,490
training/validation/test pairs. On average, each ar-
ticle has 781 words and each abstract contains 56
words. Newsroom (Grusky et al., 2018) contains
news articles and summaries written by authors
and editors from 38 major newsrooms published
between 1998 and 2017.
The dataset includes
995,041/108,837/108,862 training/validation/test
pairs. On average, each article has 659 words and
each summary has 27 words.
Metrics.
We report widely used ROUGE met-
rics (Lin, 2004), including ROUGE-1 (R-1),
ROUGE-2 (R-2), and sentence-level ROUGE-L
(R-L) F1 scores with rouge-score python package.3
Baselines.
We compare our system with three
types of strong baselines, including extractive,
abstractive, and mixed summarization methods.
LEAD-3 (See et al., 2017) is a common extractive
summarization baseline that extracts the Ô¨Årst three
sentences as the document summary. BertSum-
Ext (Liu, 2019) Ô¨Åne-tunes BERT for extractive
summarization. MatchSum (Zhong et al., 2020)
learns the semantic matching between candidate
summary and source document using contrastive
learning. HAHSum (Jia et al., 2020) incorporates a
hierarchical attentive heterogeneous graph network
in Albert (Lan et al., 2019) to perform redundancy-
aware sentence embedding for extractive summa-
rization. Point-Generator (See et al., 2017) Ô¨Årst
introduces the copy mechanism with coverage reg-
ularization for attentive seq2seq models in abstrac-
tive summarization. BART (Lewis et al., 2020) is
an encoder-decoder Transformer model with de-
noising seq2seq pre-training. PEGASUS (Zhang
et al., 2020) uses gap-sentence generation for pre-
training an encoder-decoder Transformer on ab-
stractive summarization tasks. CIT+SE (Saito et al.,
2020) uses a key word extractor and selective en-
coding mechanism to guide abstractive summariza-
tion. GSum (Dou et al., 2021) uses the extracted
summary from MatchSum as guidance to supervise
BART for abstractive summarization.
Implementation details.
We Ô¨Åne-tune BART-
large on CNNDM and Newsroom datasets. For
training data, we prepend a special token in front
of each sentence for calculating its sentence rep-
resentation. Each input sequence is truncated to
1024 tokens (including special tokens), to Ô¨Åt the
maximum input length for BART. According to the
3https://pypi.org/project/rouge-score/
System
R-1
R-2
R-L
CNNDM
LEAD-3
40.34
17.70
36.57
MatchSum
44.41
20.86
40.55
HAHSum
44.68
21.30
40.75
Point-Generator
39.53
17.28
36.38
BART
44.16
21.28
40.90
PEGASUS
44.17
21.47
41.11
CIT + SE
45.80
22.53
42.48
GSum
45.94
22.32
42.48
BART*
44.21
21.23
41.17
SEASON
46.27
22.64
43.08
Newsroom
LEAD-3
30.49
21.27
28.42
Point-Generator
26.02
13.25
22.43
PEGASUS
45.15
33.51
41.33
BART*
45.50
33.05
41.69
SEASON
46.00
33.37
42.03
Table 1: Results on CNNDM and Newsroom test sets. Best
scores are in bold. Scores signiÔ¨Åcantly better than the best
baseline model are underlined (p < 0.001). Results with *
are reproduced by us. Other numbers are from prior papers.
summary length distribution of CNNDM and News-
room datasets, we truncated the reference summary
to be 128 and 256 tokens respectively to ensure
more than 99% of the reference summaries are fully
preserved. For inference, we use the predicted soft
estimation for allocation of expected salience. The
predicted probability of salience degree is sharp-
ened with a temperature œÑ = 0.5. We use beam
search with beam size of 5, length penalty of 1.5
and 3-gram blocking. According to their ROGUE-
L F1 scores against the ground-truth summary, we
split sentences into L = 3 categories of salience
degrees: 1) The most important top 15% sentences,
2) the bottom 50% least important ones, and 3) ev-
erything in between. More discussions regarding
this setup is in ¬ß4.3).
4.2
Main Results
Tab. 1 shows the results on the two summarization
datasets. For baselines on CNNDM, joint extrac-
tive and abstractive summarization methods (i.e.
CIT+SE and GSum) perform better than indepen-
dent extractive (i.e. LEAD-3, MatchSum and HAH-
Sum) and abstractive summarization methods (i.e.
Point-Generator, BART and PEGASUS) when us-
ing the same backbone models. Among the joint
summarization baselines, using extractive summary
as guidance (i.e. GSum) performs better than using
key words as guidance (i.e. CIT+SE), which agrees

#degree
T-1
T-2
T-3
R-1
R-2
R-L
2
85%
-
-
45.94
22.52
42.74
3
50%
85%
-
46.38
22.83
43.18
4
30%
50%
85%
46.37
22.73
43.15
Table 2: Number of salience degrees, their best percentile
thresholds (T-x), and achieved results on CNNDM dev set.
0
0.1
0.2
0.3
abstractive
mixed
extractive
BART
GSum
SEASON
Figure 3: R-2 scores by density on CNNDM test set.
with the observation by Dou et al. (2021). Our
method promisingly improves the original BART
by 2.06/1.41/1.91 points in terms of ROUGE-1/2/L
F1 scores, indicating the multi-degree salience ex-
pectation allocation effectively guides the model to
generate better-quality summaries. In comparison
with GSum, our method achieves improvements
of 0.33/0.32/0.60 points in terms of ROUGE-1/2/L
F1 scores, w/o the help of SOTA extractive sum-
marization system and with less additional param-
eters, indicating the proposed guidance is more
effective than extractive summaries. Results on
Newsroom dataset further verify that our method
can achieve consistent improvements on different
datasets. In comparison with the vanilla BART
model, our method achieves 0.50/0.32/0.34 points
improvements in terms of ROUGE-1/2/L F1 scores.
4.3
The Fifteen-Fifty Phenomenon
The number of salience degrees and thresholds to
delimit them are the important hyper-parameters
to discretize the proposed guidance. We apply a
greedy search algorithm to Ô¨Ånd the best thresholds.
First, we compute salience scores of all sentences
in the corpus. In this work, we use ROUGE-L F1
between each document sentence and correspond-
ing reference summary to represent salience, and
Ô¨Ånd the best threshold for two salience degrees.
Then we gradually add one more salience degree
and search the additional threshold. The results on
CNNDM is shown in Tab. 2. Splitting all sentences
into three salience degrees by top 15% and bot-
tom 50% salience scores leads to the best ROUGE-
L F1. As the number of salience degrees L in-
creases, the model performance Ô¨Årst increases and
Informativeness Faithfulness Fluency
BART
87.21
76.77
85.86
GSum
78.45
79.46
26.94*
SEASON
88.89
78.11
87.88
Ground-Truth
77.78
75.76
72.39
Table 3: Percentage of positive votes (‚ÄôYes‚Äô) on informative-
ness, faithfulness and Ô¨Çuency of summaries. *GSum predic-
tions provided by the authors are lower-cased and lemmatized,
which hinders the Ô¨Çuency.
1st
2nd
3rd
4th
avg.
BART
34.68
30.64
21.21
13.47
2.13
GSum
11.11
15.49
24.92
48.48
3.11
SEASON
35.02
29.63
24.24
11.11
2.11
Ground-Truth
19.19
24.24
29.63
26.94
2.64
Table 4: Percentage of ranking and the average rank by hu-
man evaluation.
then decreases. We attribute this phenomenon to
the trade-off between informativeness and predic-
tion accuracy of the guidance. Although a more
Ô¨Åne-grained salience guidance is more informative,
our model generates summaries based on predicted
guidance during inference, where error propagation
exists. Increasing the number of salience degrees
to predict also increases the risk of misclassiÔ¨Åca-
tion. Furthermore, we Ô¨Ånd the best number and
thresholds of salience degrees for summarization
is consistent on Newsroom, indicating the salience
split by top Ô¨Åfteen and bottom Ô¨Åfty percentile is a
nature property of news articles. This phenomenon
may provide a useful insight for composing news
articles by journalists.
5
Analysis
To gain further insights on the proposed method,
we perform additional analyses on CNNDM to
comprehensively investigate performance by ab-
stractiveness (¬ß5.1), summary length (¬ß5.2), human
evaluation (¬ß5.3), and the impact of different model
components (¬ß5.4). A case study is also presented
in ¬ß5.5.
5.1
Performance by Abstractiveness
To understand how adaptive our method is on doc-
uments with different abstractiveness, we split all
documents into three subsets of equal sizes based
on their density scores following Grusky et al.
(2018). Results are shown in Fig. 3. SEASON
performs better than baselines on all subsets, indi-
cating that our method is adaptive to documents
with different abstractiveness. The improvements
on abstractive and mixed subsets are slightly higher

SACA
MTL
R-1
R-2
R-L


44.21
21.23
41.17


44.57
21.55
41.49
(pred)

46.27
22.64
43.08
(gold)

54.85
31.36
52.14
Table 5: Results of models w/ or w/o salience-aware cross-
attention (SACA) and multi-task leanring (MTL) on CNNDM
test set. For SACA, we provide results with both predicted
and gold salience information.
Œ±
R-1
R-2
R-L
0.5
46.49
22.77
43.47
1.0
46.50
22.74
43.46
1.5
46.48
22.81
43.44
Table 6: Results on CNNDM dev set with different loss
weights Œ± for salience prediction loss.
than that on the extractive subset, indicating ab-
stract documents beneÔ¨Åt more than extractive ones
from a Ô¨Çexible salience guidance.
5.2
Summary Length
As SEASON achieves better performance under dif-
ferent abstractiveness with a more Ô¨Çexible salience
guidance, a followup research question is: Does
the Ô¨Çexible salience guidance help predict summary
length more accurately? To answer this question,
we compute the average lengths of Ground-Truth
summaries and summaries generated by SEASON
and baseline systems. The average summary length
of Ground-Truth, SEASON, BART, and GSum are
respectively 54.8, 59.0, 60.7, and 72.0. Among
these methods, SEASON gave the closest average
summary length to Ground-Truth. Moreover, while
both of SEASON and GSum introduce sentence-
level salience guidance to BART, they change the
summary length in opposite directions.
5.3
Human Evaluation
We further evaluate the system outputs of SEASON,
BART, Gsum and the Ground-Truth with human
subjective evaluation. We randomly pick 100 in-
stances from CNNDM test set. For each raw docu-
ment in those instances, we provide the summary
generated by each system and the ground-truth sum-
mary. We hire human evaluators on Amazon Me-
chanical Turk to answer three Yes/No questions for
the four summaries and rank them. Each instance is
assigned to 3 different human evaluators to answer
the following three questions (Song et al., 2021).
a) Informativeness: Does the summary include the
major information from the news? b) Faithfulness:
Does the summary give any additional informa-
Strategy
Œ≤
R-1
R-2
R-L
-
-
46.38
22.83
43.18
all
0.1
46.47
22.71
43.42
0.2
46.40
22.68
43.38
adjacent
0.1
46.44
22.79
43.38
0.2
46.48
22.81
43.44
Table 7: Results on CNNDM dev set with different label
smoothing strategies.
Strategy
œÑ
R-1
R-2
R-L
hard
-
46.48
22.81
43.44
soft
0.1
46.76
23.08
43.57
0.2
46.94
23.24
43.75
0.5
46.98
23.20
43.78
1.0
46.59
22.75
43.44
Table 8: Results on CNNDM dev set with different salience
estimation methods. œÑ is the sharpening coefÔ¨Åcient in softmax.
tion not covered by the news? c) Fluency: Is the
summary grammatical and well-formed?
Tab. 3 reports the average percentage for each
method to get a positive answer on the correspond-
ing question. Among the three systems, SEASON
performs the best on informativeness and Ô¨Çuency,
while GSum performs the best on faithfulness. This
indicates that our Ô¨Çexible guidance helps the model
to identify salient content accurately and rephrase
them properly. Not surprisingly, systems with guid-
ance (i.e. SEASON and Gsum) are more faithful to
original content than a system without any guid-
ance (i.e. BART). Tab. 4 shows the ranking results.
SEASON has the highest percentage of the highest-
ranked summaries and the lowest percentage of
the lowest-ranked summaries. It also has the best
average rank. These results further demonstrate
that summaries generated by SEASON are of high
quality. Interestingly, we Ô¨Ånd that the ground-truth
is not always the best choice in human evaluation.
This observation aligns with the Ô¨Åndings in prior
studies (Maynez et al., 2020; Song et al., 2020; Fab-
bri et al., 2021). It could happen since both human
composition of summaries and human justiÔ¨Åcation
on their qualities could be subjective. Thus, ground-
truth news summaries written by editors may not
always be the Ô¨Årst choice of readers. It also indi-
cates that human evaluators could not distinguish
between the real human writer and our automatic
summarizer, and actually prefer our system outputs
more than the ground-truth summaries.

BART
New York-based writer Danielle Page set out to ask every cabbie she came across to dispense their best piece of relationship advice. The drivers,
many of whom are married themselves, revealed their personal tips, life lessons and cultural anecdotes all in the name of love.
GSum
New York-based writer Danielle Page set out to ask every cabbie she came across to dispense their best piece of relationship advice in hopes of
unlocking the key to a successful union. The drivers, many of whom are married themselves, revealed their personal tips, life lessons and cultural
anecdotes all in the name of love. A 60-year-old named Michael revealed that his trick to marital bliss is putting his wife‚Äôs happiness above his own
‚Äì but insists that what really makes a relationship work is Ô¨Ånding a partner who will do the same for you.
SEASON
New York-based writer Danielle Page set out to ask every cabbie she came across to dispense their best piece of relationship advice.
Gold
New York-based writer Danielle Page set out to ask every cabbie she came across to share their tips on Ô¨Ånding - and keeping - a partner.
BART
Researchers from Texas A&M School of Public Health found that hospitalizations from car crashes dropped 7 percent between 2003 and 2010 in
the 45 states with texting bans. Arizona, Texas, Montana, Missouri, and Oklahoma are the only Ô¨Åve states in America that do not have texting at
the wheel bans for all drivers.
GSum
Researchers from Texas A&M School of Public Health found that hospitalizations from car crashes dropped 7 percent between 2003 and 2010 in
the 45 states with texting bans when compared to states with no restrictions. Drivers between the ages of 25 and 40 are the most likely group of
people to get in an accident related to texting and driving.
SEASON
Researchers from Texas A&M School of Public Health found that hospitalizations from car crashes dropped 7 percent between 2003 and 2010 in
the 45 states with texting bans. Arizona, Texas, Montana, Missouri, and Oklahoma are the only Ô¨Åve states in America that do not have texting at
the wheel bans for all drivers. The study found that older drivers were more likely to make a texting and driving mistake than a younger driver.
Gold
Study found that hospitalizations from car crashes dropped 7 percent between 2003 and 2010 in the 45 states with texting bans. Arizona, Texas,
Montana, Missouri, and Oklahoma are the only Ô¨Åve states in America that do not have texting at the wheel bans for all drivers. The study also
found that older drivers were more likely to make a texting and driving mistake than a younger driver.
Table 9: Case Study. Continuous word spans overlapped with the gold summary of more than 3 words are in blue. Continuous
word spans in the gold summary not covered by any prediction are in red. Baselines may suffer from extra details or information
loss due to no or imperfect salience guidance.
5.4
Ablation Study
For all the experiments in this section, we use the
default setting introduced in ¬ß4.1, unless discussed
otherwise with different hyper-parameter values.
Multi-Task Learning. We Ô¨Årst investigate the ef-
fectiveness of salience prediction as an auxiliary
task by removing salience-aware cross-attention. In
this setting, the model jointly predicts the salience
allocation and the abstractive summary, but does
not feed the gold or predicted salience allocation
to the decoder. That means the salience allocation
is only used as supervision signals but not (inter-
mediate) input features. As shown in Tab. 5, with
MTL solely, the model can achieve 0.36/0.32/0.32
points improvements in terms of R-1/2/L. This indi-
cates that the salience prediction task can not only
provide effective guidance for abstractive summa-
rization, but also act as supervision for learning
more robust representations.
Salience-Aware Cross-Attention. We examine
the effectiveness of the proposed salience-aware
cross-attention module from two perspectives in
Tab. 5.
First, we provide the gold salience
labels instead of predicted ones to explore its
upper bound.
The performance increases by
8.58/8.72/9.06 points with a perfect salience pre-
dictor. This result indicates that a better estimation
of the salience can be helpful for further improving
the abstractive summarization performance. Sec-
ond, we compare it with the original cross-attention
module while keeping the auxiliary task and ob-
serve a performance drop by 1.70/1.09/1.59 points
in terms of R-1/2/L. This indicates that salience-
aware cross-attention is essential for selecting im-
portant content accurately.
CoefÔ¨Åcient of Multi-Task Learning. We further
examine the inÔ¨Çuence of the coefÔ¨Åcient Œ± of multi-
task learning in Tab. 6. We test three different Œ±
values and observe that the largest difference of
R1/2/L are within 0.02/0.07/0.03. According to the
results, SEASON is not sensitive to Œ±, indicating
our model architecture is robust.
Adjacent Label Smoothing. We compare differ-
ent label smoothing strategies in Tab. 7. In gen-
eral, label smoothing improves model generaliza-
tion and calibration (M√ºller et al., 2019), therefore
beneÔ¨Åts the overall performance. Given the same
smoothing probability Œ≤, adding label smoothing
to adjacent salience degrees performs better than
adding label smoothing on all other salient degrees.
Salience Estimation. We compare the effective-
ness of using soft (Eq. 6) and hard (Eq. 5) strategies
for salience estimation in Tab. 8. Computing the
expectation with raw probabilities (i.e., œÑ = 1.0)
brings 0.11 points improvements on ROUGE-1. By
adjusting the sharpness of probability distribution
with the sharpening coefÔ¨Åcient œÑ, ROUGE-1/2/L
improvements become 0.50/0.39/0.34 points, re-
spectively. As deÔ¨Åned in Eq. 3, œÑ represents the
conÔ¨Ådence on predictions, and a lower œÑ leads to
sharper probability distribution. In our experiments,
œÑ = 0.5 performs the best.
5.5
Case Study
We present a case study in Tab. 9 with two repre-
sentative examples to illustrate the advantage of
SEASON. In the Ô¨Årst case, BART tends to generate
extra details without the help of proper guidance

when only one sentence is enough to summarize the
document. GSum is guided by an extractive sum-
mary consisting of three sentences, so not surpris-
ingly it provides even more details. In the second
case, BART infers without any salience guidance
and ignores an important Ô¨Ånding of the research.
GSum selects exactly three sentences as guidance,
thus it misses key information when multiple sen-
tences are similarly important but some of them are
not included in the guidance. SEASON performs
well for both cases, indicating it is adaptive to doc-
uments of different properties.
6
Conclusion
In this paper, we propose SEASON, an abstrac-
tive summarization approach guided with salience
allocation expectation. In SEASON, the salience
guidance is adaptive to documents with differ-
ent abstractiveness, and the salience-aware cross-
attention module is Ô¨Çexible to decide how much
signal to accept from the salience guidance. Auto-
matic and human evaluation further demonstrate
the effectiveness and reliability of our proposed
method. Comparing to the strong baseline model
(i.e. BART), our method achieves 2.06/1.41/1.91
ROUGE-1/2/L performance gain on CNNDM, and
0.33/0.32/0.60 performance gain on Newsroom. Fi-
nally, the empirical results on more than one mil-
lion news articles demonstrate a natural Ô¨Åfteen-Ô¨Åfty
salience split for news article sentences providing
a useful insight for composing news articles.
Ackonwledgement
We thank all reviewers for their valuable sugges-
tions. We also thank Zi-Yi Dou and Pengfei Liu
for providing summaries generated by GSum and
stored at DataLab (Xiao et al., 2022)4. This work
was done when Fei Wang was doing an internship
at Tencent AI Lab Seattle. Fei Wang is partially
supported by Annerberg Fellowship. Muhao Chen
is supported by the National Science Foundation
of United States Grant IIS 2105329.
Limitation
In this study we have experimented with using
ROUGE-L F1 as the salience measurement. How-
ever, other metrics for text summarization can serve
as alternatives, such as BLEU (Papineni et al.,
2002) and BARTScore (Yuan et al., 2021). Choos-
ing among more metrics for salience measurement
4https://datalab.nlpedia.ai/
can be explored in future work. The number of
salience degrees and thresholds to delimit them
could be language-dependent. The Ô¨Åfty-Ô¨Åfteen phe-
nomenon is observed on two of the most represen-
tative English news summarization datasets. Fu-
ture work on other languages may need to search
the best salience degrees and thresholds of each
language.
Despite we use BART as our base
model and maximum likelihood estimation (MLE)
as the learning objective in this study, the proposed
method can also be applied to other backbones and
learning objectives (Zhang et al., 2020; Liu and
Liu, 2021; Liu et al., 2022). While we have limited
the proposed technique to abstractive summariza-
tion on news articles, future research can extend
SEASON to other domains, such as scientiÔ¨Åc pub-
lications (Cohan et al., 2018) and podcasts (Song
et al., 2022). In terms of evaluation, we focus
on the supervised and in-domain setting. Future
work may also consider to extend our method to
zero-shot, few-shot, cross-domain, or cross-dataset
settings. In addition to abstractive summarization,
future research can also extend SEASON to other
NLP tasks requiring salience-awareness, such as
fact veriÔ¨Åcation (Wang et al., 2021), information re-
trieval (Xiong et al., 2018) and distantly supervised
relation extraction (Lin et al., 2016).
Ethical Consideration
A general issue of automatic text summarization
is intellectual property problem caused by copying
content from the raw document to the generated
summary. This work seeks to improve abstractive
summarization models with salience allocation as
guidance. As the proposed guidance is more Ô¨Çexi-
ble than extractive summaries, it is likely to reduce
copying content. Although we create salience guid-
ance based on ground-truth summaries, the doc-
uments and ground-truth summaries remains the
same as it is in the original dataset, ensuring no
further social bias is introduced.
References
Iz Beltagy, Matthew E. Peters, and Arman Cohan.
2020. Longformer: The long-document transformer.
arXiv:2004.05150.
Gong Cheng, Danyun Xu, and Yuzhong Qu. 2015.
Summarizing entity descriptions for effective and ef-
Ô¨Åcient human-centered entity linking. In Proceed-
ings of the 24th International Conference on World
Wide Web, pages 184‚Äì194.

Arman Cohan, Franck Dernoncourt, Doo Soon Kim,
Trung Bui, Seokhwan Kim, Walter Chang, and Nazli
Goharian. 2018. A discourse-aware attention model
for abstractive summarization of long documents. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 2 (Short Papers), pages 615‚Äì621.
Raul Diaz and Amit Marathe. 2019. Soft labels for or-
dinal regression. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recogni-
tion, pages 4738‚Äì4747.
Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao
Jiang, and Graham Neubig. 2021. Gsum: A general
framework for guided neural abstractive summariza-
tion. In Proceedings of the 2021 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 4830‚Äì4842.
Alexander
Richard
Fabbri,
Wojciech
Kry¬¥sci¬¥nski,
Bryan McCann, Caiming Xiong, Richard Socher,
and Dragomir Radev. 2021.
Summeval:
Re-
evaluating summarization evaluation. Transactions
of the Association for Computational Linguistics,
9:391‚Äì409.
Sebastian Gehrmann, Yuntian Deng, and Alexander M
Rush. 2018. Bottom-up abstractive summarization.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
4098‚Äì4109.
Max Grusky, Mor Naaman, and Yoav Artzi. 2018.
Newsroom: A dataset of 1.3 million summaries with
diverse extractive strategies. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long Pa-
pers), pages 708‚Äì719.
Wan-Ting Hsu, Chieh-Kai Lin, Ming-Ying Lee, Kerui
Min, Jing Tang, and Min Sun. 2018.
A uniÔ¨Åed
model for extractive and abstractive summarization
using inconsistency loss.
In Proceedings of the
56th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
132‚Äì141.
Naoya Inoue, Harsh Trivedi, Steven Sinha, Niran-
jan Balasubramanian,
and Kentaro Inui. 2021.
Summarize-then-answer: Generating concise expla-
nations for multi-hop reading comprehension.
In
Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing, pages
6064‚Äì6080, Online and Punta Cana, Dominican Re-
public. Association for Computational Linguistics.
Ruipeng Jia, Yanan Cao, Hengzhu Tang, Fang Fang,
Cong Cao, and Shi Wang. 2020. Neural extractive
summarization with hierarchical attentive heteroge-
neous graph network. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 3622‚Äì3631.
Hanqi Jin, Tianming Wang, and Xiaojun Wan. 2020.
Semsum: Semantic dependency guided neural ab-
stractive summarization.
In Proceedings of the
AAAI Conference on ArtiÔ¨Åcial Intelligence, vol-
ume 34, pages 8026‚Äì8033.
William A Johnston and Veronica J Dark. 1986. Selec-
tive attention. Annual review of psychology.
Mahnaz Koupaee and William Yang Wang. 2018. Wik-
ihow:
A large scale text summarization dataset.
arXiv preprint arXiv:1810.09305.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
Kevin Gimpel, Piyush Sharma, and Radu Soricut.
2019. Albert: A lite bert for self-supervised learning
of language representations. In International Con-
ference on Learning Representations.
Mike
Lewis,
Yinhan
Liu,
Naman
Goyal,
Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020.
Bart: Denoising sequence-to-sequence pre-
training for natural language generation, translation,
and comprehension. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, pages 7871‚Äì7880.
Haoran Li, Arash Einolghozati, Srinivasan Iyer, Bhar-
gavi Paranjape, Yashar Mehdad, Sonal Gupta, and
Marjan Ghazvininejad. 2021.
Ease:
Extractive-
abstractive summarization end-to-end using the in-
formation bottleneck principle. In Proceedings of
the Third Workshop on New Frontiers in Summariza-
tion, pages 85‚Äì95.
Haoran Li, Junnan Zhu, Jiajun Zhang, Chengqing
Zong, and Xiaodong He. 2020. Keywords-guided
abstractive sentence summarization. In Proceedings
of the AAAI Conference on ArtiÔ¨Åcial Intelligence,
volume 34, pages 8196‚Äì8203.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries.
In Text summarization
branches out, pages 74‚Äì81.
Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,
and Maosong Sun. 2016. Neural relation extraction
with selective attention over instances. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 2124‚Äì2133.
Fei Liu and Yang Liu. 2009. From extractive to abstrac-
tive meeting summaries: Can it be done by sentence
compression?
In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, pages 261‚Äì264.
Yang Liu. 2019. Fine-tune bert for extractive summa-
rization. arXiv preprint arXiv:1903.10318.
Yixin Liu and Pengfei Liu. 2021.
Simcls: A sim-
ple framework for contrastive learning of abstractive
summarization. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference

on Natural Language Processing (Volume 2: Short
Papers), pages 1065‚Äì1072.
Yixin Liu, Pengfei Liu, Dragomir Radev, and Graham
Neubig. 2022.
Brio: Bringing order to abstrac-
tive summarization. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 2890‚Äì
2903.
Ilya Loshchilov and Frank Hutter. 2018.
Decoupled
weight decay regularization. In International Con-
ference on Learning Representations.
Keming Lu, I Hsu, Wenxuan Zhou, Mingyu Derek Ma,
Muhao Chen, et al. 2022. Summarization as indirect
supervision for relation extraction. arXiv preprint
arXiv:2205.09837.
Potsawee Manakul and Mark Gales. 2021. Long-span
summarization via local attention and content se-
lection.
In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers),
pages 6026‚Äì6041.
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and
Ryan McDonald. 2020. On faithfulness and factu-
ality in abstractive summarization. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 1906‚Äì1919.
Rafael M√ºller, Simon Kornblith, and Geoffrey E Hin-
ton. 2019. When does label smoothing help?
Ad-
vances in neural information processing systems, 32.
Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,
√áaÀòglar GÀôul√ßehre, and Bing Xiang. 2016. Abstrac-
tive text summarization using sequence-to-sequence
rnns and beyond.
In Proceedings of The 20th
SIGNLL Conference on Computational Natural Lan-
guage Learning, pages 280‚Äì290.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th annual meeting of the Association for Compu-
tational Linguistics, pages 311‚Äì318.
Jonathan Pilault, Raymond Li, Sandeep Subramanian,
and Christopher Pal. 2020.
On extractive and ab-
stractive neural document summarization with trans-
former language models. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 9308‚Äì9319.
Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Gra-
ham Neubig, and Zachary C Lipton. 2020. Learn-
ing to deceive with attention-based explanations. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 4782‚Äì
4793.
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,
and Yuxiong He. 2020.
Deepspeed: System opti-
mizations enable training deep learning models with
over 100 billion parameters. In Proceedings of the
26th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, pages 3505‚Äì
3506.
Alexander M Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 379‚Äì389.
Itsumi Saito,
Kyosuke Nishida,
Kosuke Nishida,
and Junji Tomita. 2020.
Abstractive summariza-
tion with combination of pre-trained sequence-to-
sequence and saliency models.
arXiv preprint
arXiv:2003.13028.
Abigail See, Peter J Liu, and Christopher D Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1073‚Äì
1083.
Kaiqiang Song, Chen Li, Xiaoyang Wang, Dong Yu,
and Fei Liu. 2022.
Towards abstractive grounded
summarization of podcast transcripts. In Proceed-
ings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 4407‚Äì4418.
Kaiqiang Song, Bingqing Wang, Zhe Feng, and Fei Liu.
2021. A new approach to overgenerating and scor-
ing abstractive summaries.
In Proceedings of the
2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1392‚Äì1404, On-
line. Association for Computational Linguistics.
Kaiqiang Song, Bingqing Wang, Zhe Feng, Ren Liu,
and Fei Liu. 2020. Controlling the amount of ver-
batim copying in abstractive summarization. In Pro-
ceedings of the AAAI Conference on ArtiÔ¨Åcial Intel-
ligence, volume 34, pages 8902‚Äì8909.
Sainbayar Sukhbaatar,
√âdouard Grave,
Piotr Bo-
janowski, and Armand Joulin. 2019.
Adaptive at-
tention span in transformers. In Proceedings of the
57th Annual Meeting of the Association for Compu-
tational Linguistics, pages 331‚Äì335.
Fei Wang, Kexuan Sun, Jay Pujara, Pedro Szekely, and
Muhao Chen. 2021.
Table-based fact veriÔ¨Åcation
with salience-aware learning. In Findings of the As-
sociation for Computational Linguistics: EMNLP
2021, pages 4025‚Äì4036.
Fei Wang, Zhewei Xu, Pedro Szekely, and Muhao
Chen. 2022. Robust (controlled) table-to-text gen-
eration with structure-aware equivariance learning.
arXiv preprint arXiv:2205.03972.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander Rush. 2020. Trans-
formers: State-of-the-art natural language process-
ing. In Proceedings of the 2020 Conference on Em-
pirical Methods in Natural Language Processing:
System Demonstrations, pages 38‚Äì45, Online. Asso-
ciation for Computational Linguistics.
Yang
Xiao,
Jinlan
Fu,
Weizhe
Yuan,
Vijay
Viswanathan, Zhoumianze Liu, Yixin Liu, Gra-
ham Neubig, and Pengfei Liu. 2022.
DataLab:
A platform for data analysis and intervention.
In
Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics: System
Demonstrations, pages 182‚Äì195, Dublin, Ireland.
Association for Computational Linguistics.
Chenyan Xiong, Zhengzhong Liu, Jamie Callan, and
Tie-Yan Liu. 2018. Towards better text understand-
ing and retrieval through kernel entity salience mod-
eling. In The 41st International ACM SIGIR Con-
ference on Research & Development in Information
Retrieval, pages 575‚Äì584.
Wenpeng Yin, Dragomir Radev, and Caiming Xiong.
2021. DocNLI: A large-scale dataset for document-
level natural language inference.
In Findings of
the Association for Computational Linguistics: ACL-
IJCNLP 2021, pages 4913‚Äì4922, Online. Associa-
tion for Computational Linguistics.
Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.
Bartscore: Evaluating generated text as text gener-
ation. Advances in Neural Information Processing
Systems, 34:27263‚Äì27277.
Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-
ter Liu. 2020. Pegasus: Pre-training with extracted
gap-sentences for abstractive summarization. In In-
ternational Conference on Machine Learning, pages
11328‚Äì11339. PMLR.
Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang,
Xipeng Qiu, and Xuan-Jing Huang. 2020. Extrac-
tive summarization as text matching.
In Proceed-
ings of the 58th Annual Meeting of the Association
for Computational Linguistics, pages 6197‚Äì6208.
Ming Zhong, Pengfei Liu, Danqing Wang, Xipeng Qiu,
and Xuanjing Huang. 2019.
Searching for effec-
tive neural extractive summarization: What works
and what‚Äôs next. In Proceedings of the 57th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1049‚Äì1058, Florence, Italy. Associa-
tion for Computational Linguistics.

A
Implementation Details
Our Implementation is based on Huggingface
Transformers
(Wolf
et
al.,
2020),
Pytorch-
lightning5 and Lightning-Transformers6. We fur-
ther use DeepSpeed (Rasley et al., 2020) Stage2
and half-precision to speed up the training pro-
cess. We applied the BART-large model consist-
ing of 400M parameters and Ô¨Åne-tuned them on
CNNDM7 and Newsroom8 dataset with 8√óV100
GPU(32GB) for 10 epochs. It takes about 5 hours
for training on CNNDM and 32 hours on NEWS-
ROOM. We set our batch size to be 96 to maximize
the utilization of the GPU memory. For Optimiza-
tion, we use AdamW (Loshchilov and Hutter, 2018)
with learning rates of 3e ‚àí5. The momentum pa-
rameters are 0.9 and 0.99. Our weight decay is
set to be 0.01 for parameters other than bias term
and LayerNorm layers. We uses a linear warmup
strategy, the number of warmup steps are 1,500
and 5,000 on CNNDM and Newsroom respectively.
The dropout rate is 10%. We follow the BART
(Lewis et al., 2020) Ô¨Åne-tuning and use gradient
clipping of 0.1. The coefÔ¨Åcient of multi-task learn-
ing Œ± is 1.5. We uses adjacent label smoothing and
the smoothing probability Œ≤ is 20%. For inference,
we use the predicted soft estimation for allocation
of expected salience. The predicted probability of
salience degree is sharpened with a temperature
œÑ = 0.5. We use beam search with beam size of
5 with length penalty of 1.5 and 3-gram blocking.
The minimum and maximum decoding length is
20 and 256. For human evaluation, we choose
master workers on Amazon Mechanical Turk9 with
more than 90% approve rates and more than 100
approved HIT.
5https://www.pytorchlightning.ai
6https://github.com/Lightning-AI/
lightning-transformers
7https://huggingface.co/datasets/ccdv/
cnn_dailymail
8https://lil.nlp.cornell.edu/newsroom/
download/index.html
9https://www.mturk.com

