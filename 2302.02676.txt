Chain of Hindsight Aligns Language Models with Feedback
Hao Liu Carmelo Sferrazza Pieter Abbeel
University of California, Berkeley
hao.liu@cs.berkeley.edu
Abstract
Learning from human preferences is important for
language models to be helpful and useful for hu-
mans, and to align with human and social values.
Prior work have achieved remarkable successes
by learning from human feedback to understand
and follow instructions. Nonetheless, these meth-
ods are either founded on hand-picked model gen-
erations that are favored by human annotators,
rendering them ineffective in terms of data uti-
lization and challenging to apply in general, or
they depend on reward functions and reinforce-
ment learning, which are prone to imperfect re-
ward function and extremely challenging to opti-
mize. In this work, we propose a novel technique,
Chain of Hindsight, that is easy to optimize and
can learn from any form of feedback, regardless
of its polarity. Our idea is inspired by how hu-
mans learn from extensive feedback presented in
the form of languages. We convert all types of
feedback into sentences, which are then used to
ﬁne-tune the model, allowing us to take advan-
tage of the language comprehension capabilities
of language models. We condition the model on a
sequence of model generations paired with feed-
back. By doing so, models are trained to generate
outputs based on feedback, and models can learn
to identify and correct negative attributes or errors.
Applying our method to large language models,
we observed that Chain of Hindsight signiﬁcantly
surpasses previous methods in aligning language
models with human preferences. We observed
signiﬁcant improvements on summarization and
dialogue tasks and our approach is markedly pre-
ferred in human evaluations.
1. Introduction
Large neural network based models have drawn continu-
ously increasing attention in recent years, with applications
in everything from natural language understanding (Brown
Preprint. Under review.
et al., 2020; Devlin et al., 2018) to protein structure predic-
tion (Jumper et al., 2021). However, in order to ensure that
these technologies have a positive impact on society, it is of
paramount importance for them to be aligned with human
values. One of the most critical elements in achieving this
is the use of human feedback.
Human feedback allows us to evaluate the performance of
such models in a way that is both objective and subjective. It
can help to identify issues with accuracy, fairness, and bias,
and provide insights into how the model can be improved,
in order to ensure that a model’s outputs align with societal
norms and expectations.
Driven by the importance of incorporating human feedback
into language models, researchers have been developing
and testing various methods for human-in-the-loop systems.
These methods aim to make the process of incorporating
human feedback more efﬁcient, resulting in models that are
able to achieve improved performance and accuracy, while
also providing more fairness and ethical outputs (Hancock
et al., 2019; Perez et al., 2019; Yi et al., 2019; Ouyang et al.,
2022). For example, InstructGPT (Ouyang et al., 2022) and
ChatGPT (OpenAI, 2022).
These successes in language modeling have been largely at-
tributed to the utilization of supervised ﬁnetuning (SFT) and
Reinforcement Learning with Human Feedback (RLHF)
techniques. While these approaches have demonstrated
promising results in enhancing the performance of language
models on speciﬁc tasks, they also suffer from notable limi-
tations.
SFT relies on human-annotated data and positive-rated
model generation to ﬁne-tune a pre-trained language model.
However, this approach is heavily reliant on the availability
of labeled data, which may entail signiﬁcant expenses and
time investments. Moreover, relying solely on positive-rated
data may constrain the model’s ability to identify and correct
negative attributes or errors, thus reducing its generalizabil-
ity to new and unseen data.
Alternatively, RLHF enables learning from all data, regard-
less of feedback rating. Nonetheless, this method requires
the acquisition of a reward function, which may be sub-
arXiv:2302.02676v5  [cs.LG]  5 Mar 2023

Chain of Hindsight aligns Language Models with Feedback
Explain neural networks to a child 
=
<
A
D
C
is less preferred compared with
B
is equally good as
B
C
is less informative than
For response to explain neural networks to a child,
A
B
C
Internet is …
They don’t 
understand …
Neural networks 
are used in …
A neural network 
is like …
D
Model Completion
A
B
C
D
<
Rank by Human
, and
,
Add Hindsight Feedback
GPT
Figure 1. Forming chain of hindsight from multiple model outputs and ratings. An illustrative example of how to construct a training
sequence from multiple rated model generations. To improve diversity, different formats of feedback and ordering of outputs can be used.
For instance, instead of concatenating generations with feedback ratings in a chronological order, one can consider reversing the order or
shufﬂe the order randomly. Additionally, one can use different formats of feedback, such as open-ended or human-in-the-loop feedback,
to further increase diversity.
ject to misalignment and imperfections. In addition, the
optimization of reinforcement learning algorithms can be
challenging, presenting signiﬁcant difﬁculties in its applica-
tion.
Our research aims to overcome the limitations of SFT and
RLHF by combining their strengths to leverage all feedback,
without resorting to reinforcement learning. We hypothe-
size that by conditioning language models on a sequence of
model generations that are paired with feedback, and subse-
quently training these models to generate outputs based on
such feedback, the models can develop the ability to identify
and correct negative attributes or errors.
Our key idea is that humans are capable of learning from
rich and detailed feedback in the form of language. Given
that recent studies that have demonstrated the impressive
capacity of pre-trained language models to learn effectively
in context (see e.g. Brown et al., 2020; Chowdhery et al.,
2022, among others), it is possible to turn all feedback into
a sentence and ﬁnetune models to understand the feedback.
Based on this insight, we propose the notion of converting
all feedback into a sentence format and subsequently ﬁne-
tuning models to comprehend and effectively utilize such
feedback. Speciﬁcally, we propose ﬁnetuning the model to
predict outputs while conditioning on one or more model
outputs, and their corresponding feedback in the form of
comparisons, akin to the process of hindsight experience
replay (Andrychowicz et al., 2017).
During training, the model is presented with feedback in the
form of sentences such as ”The following is a bad summary”
and ”The following summary is better”, and the model is
conditioned to predict outputs that better match the latter
feedback. At inference time, positive feedback instructs the
model to generate the desired outputs.
Our proposed approach enables models to learn from both
positive and negative feedback, allowing the identiﬁcation
and correction of negative attributes or errors. This method
leverages the power of language to comprehend and learn
from feedback, ultimately enhancing the models’ capability
to perform a diverse range of tasks with greater accuracy
and efﬁciency. We name our method the Chain of Hindsight
(CoH) as it conditions on a sequence of hindsight feedback.
We evaluate our approach on summarization and dialogue
tasks, demonstrating its signiﬁcant improvements over su-
pervised ﬁnetuning (SFT) and reinforcement learning from
human feedback (RLHF), both in automatic evaluation and
human evaluation. Our main contributions are twofold:
• We propose a novel learning framework that leverages
all available feedback data to improve model perfor-
mance.
• We conduct extensive experiments to showcase the
effectiveness of our method in comparison to existing
baselines.

Chain of Hindsight aligns Language Models with Feedback
A
D
C
is less preferred compared with
B
is equally good as
B
C
is less informative than
For response to explain neural networks to a child,
, and
,
GPT
For response to explain neural networks to a child,
generate a good, informative, and preferred 
answer.
Training
Inference
generate a more informative answer.
generate a better answer.
GPT
Figure 2. Training and prediction phases using chain-of-hindsight. (Left): During the training phase, the model is presented with a
sequence of model outputs denoted as A, B, C, and D. These outputs are labeled by a human labeler who provides feedback to the model
about the quality of the output. The labeler ranks the outputs based on their quality, and the feedback is incorporated into a chain of
hindsight. This chain of hindsight provides information to the model about the positiveness and/or relative rankings of the model outputs.
The feedback can take various forms, such as ”A is less preferred compared to B” and ”An example of not good response is A”. (Right):
During the prediction phase, the model is instructed to generate a desired output. The model can receive multiple rounds of instructions, if
needed.
2. Chain of Hindsight
Our goal is to improve the Transformer model’s perfor-
mance by leveraging human-rated data and feedback, and to
achieve this, we propose a novel approach that goes beyond
conventional SFT methods and RLHF methods.
Turning all feedback into a sequence. Our approach con-
siders all feedback and instructions provided by humans.
To achieve this, we present the model with a sequence of
model generations, along with corresponding feedback and
explanations provided by humans.
Our approach uses a conventional Transformer model ar-
chitecture that is causal and decoder-only, as proposed in
the work of (Brown et al., 2020; Vaswani et al., 2017) on
attention mechanisms. This means that at each timestep,
the model can only attend to the past timesteps and itself.
Given an input text x = [x1, · · · , xn], the standard causal
language modeling objective is deﬁned to maximize the log
likelihood of x autoregressively:
log p(x) = log
n
Y
i=1
p(xi|x1, x2, . . . , xi−1)
= log
n
Y
i=1
p(xi|x<i)
:= log
n
Y
i=1
p(xi|[xj]i−1
j=0).
(1)
In CoH, we construct x by combining multiple model out-
puts with feedback together. For instance, model generates
multiple responses to the question of explaining neural net-
works to baby, these responses are combined together into
a sequence by generating feedback instructions based on
ratings. An example is illustrated in Figure 1.
To enable models to learn from feedback such as ”the fol-
lowing is a better summary,” we require the model to predict
each token xi ∈x that is not part of the feedback. This is
achieved through the CoH objective, which can be expressed
as:
log p(x) = log
n
Y
i=1
1O(x)(xi) p(xi|[xj]i−1
j=0)
(2)
Here, 1O(x)(xi) denotes whether token xi is not part of the
feedback tokens. In other words, it is 1 if xi is not part of
the feedback and 0 if it is part of the feedback. The model
is trained to predict each non-feedback token xi given the
previous tokens [xj]i−1
j=0.
The type of feedback utilized can differ based on the spe-
ciﬁc task at hand. In theory, one could employ open-ended
feedback from humans in the loop. However, for this study,
we chose to generate feedback using pre-determined tem-
plates based on ratings. These templates can be found in
the Appendix A. It is important to note that our approach
is compatible with various forms of feedback, including

Chain of Hindsight aligns Language Models with Feedback
binary feedback as well as more complex multi-scale and
multi-dimensional feedback.
In the following, we remark two techniques that help ﬁne-
tuning using human feedback.
Prevent overﬁtting. The performance of language models
can suffer if they overﬁt to the limited diversity of human
annotations and model-generated data. To mitigate this
issue, we adopt a strategy similar to Ouyang et al. (2022)
and minimize the negative log likelihood of the pre-training
dataset. This is achieved through the following equation:
L = Ex∼F [log p(x)] + λEx∼P [log p(x)] ,
(3)
here, F and P refer to the human feedback dataset and
pre-training dataset, respectively.
The model is trained to minimize the negative log likelihood
of both datasets, with λ serving as a hyperparameter that
determines the weight given to the pretraining and human
feedback losses. By incorporating the pre-training dataset,
we can prevent overﬁtting and improve the performance of
the language model.
Prevent shortcut. Human preferences for model-generated
data are often complex, and small differences can have a
signiﬁcant impact on the perceived quality of the output.
Even if a dialogue is negative-rated, it may still be mostly
coherent and accurate, but might have missed a few critical
words. Directly ﬁne-tuning the model on these missed words
could lead to the model shortcutting and copying tokens,
which ultimately reduces its ability to generate high-quality
outputs.
To overcome this challenge, we adopt a strategy similar
to Liu et al. (2022) and randomly mask between 0% and 15%
of past tokens during training. This approach is designed to
regularize the model and prevent it from overﬁtting to the
speciﬁc examples seen during training. In our experiments,
we found that this regularization technique signiﬁcantly
improves the quality of the generated sentences.
Training. We work with a dataset of model outputs and their
corresponding human preference feedback, from which we
sample minibatches of model outputs. To generate hindsight
feedback in natural language, we randomly sample a feed-
back format and incorporate the human ratings. We combine
the hindsight feedback and model outputs into a chain of
hindsight, which serves as the input for our autoregressive
model. The objective is to predict the input sequence au-
toregressively, and we use cross-entropy loss to optimize
the model. We average the loss over each timestep in the
last model output sequence. Our approach is summarized in
Algorithm 1.
Algorithm 1 Hindsight Finetuning from Human Feedback.
training time: models are trained using a variety
of feedback in the chain-of-hindsight sequences.
At
inference time: models are directed to generate out-
puts based on the feedback that is considered ”good”.
Required: Pretrain Language Model, Human Feedback
Dataset
Required: Maximum training terations m, Maximum
length of chain-of-hindsight n
Initialize
for i = 1 to m do
Randomly sample j between 1 and n
Randomly sample j model outputs from dataset
Sort the j outputs based on their ratings and organize
them using pre-determined feedback templates.
Finetune models on the sequence in order to predict
model outputs by considering other outputs and feed-
back.
end for
3. Evaluation Setup
Training Datasets. We use a combination of three datasets
for ﬁnetuning. The three datasets are:
WebGPT comparisons. This dataset is from Nakano et al.
(2021).1 It includes a total of 19,578 comparisons where
each example comprises a question, a pair of model answers,
and metadata. The answers are rated by humans with a
preference score, which helps to identify the better of the
two answers.
Human Preference.
This dataset is from Ganguli et al.
(2022); Bai et al. (2022a) and contains human rated
dialogues4. Each example in this dataset consists of a pair
of conversations between a human and a chatbot, and one
of the two conversations is more preferred by humans.
Summarize from feedback. The source of this dataset is Sti-
ennon et al. (2020), and it consists of feedback from humans
regarding the summarizations generated by a model3. The
dataset is divided into two parts: ”comparisons” and ”axis.”
In the ”comparisons” section, human evaluators were re-
quested to choose the superior summary from two options
presented to them. In contrast, in the ”axis” section, hu-
man evaluators assigned scores to the quality of a summary
using a Likert scale. The ”comparisons” part is split into
training and validation sets, whereas the ”axis” part is split
into validation and test sets. The reward model used in the
research was trained on summaries from the TL;DR dataset,
and additional validation and test data were obtained from
CNN and Daily Mail articles.
1https://huggingface.co/datasets/openai/
webgpt_comparisons

Chain of Hindsight aligns Language Models with Feedback
Evaluation Tasks and Metrics. We consider both auto-
matic evaluation on various tasks and human evaluation on
summarization and dialogue tasks.
Few-shot tasks. For automatic evaluation, we fol-
low prior works Brown et al. (2020); Wang & Komat-
suzaki (2021) and consider a diverse suite of standard
NLP tasks, including SuperGLUE (Sarlin et al., 2020),
ANLI (Nie et al., 2019), LAMBADA (Paperno et al., 2016),
StoryCloze (Mostafazadeh et al., 2016), PIQA (Bisk et al.,
2019), and more. The full task list is shown in Table 3. We
use Language Model Evaluation Harness2 for evaluation.
Summarization task.
Following prior works on
learning from human feedback (Stiennon et al., 2020;
Nakano et al., 2021; Bai et al., 2022a), we also consider
two tasks that are best evaluated with human preference.
The ﬁrst one is summarization on TL;DRs dataset (V¨olske
et al., 2017). The original TL;DR dataset contains about 3
million posts from reddit.com across a variety of top-
ics (subreddits), as well summaries of the posts written by
the original poster (TL;DRs). We use the ﬁltered version
provided by Stiennon et al. (2020), which contains 123,169
posts3. We evaluate the performance on the validation set.
For evaluation metrics, labelers rated summaries for cov-
erage (how much important information from the original
post is covered), accuracy (to what degree the statements
in the summary are part of the post), coherence (how easy
the summary is to read on its own), and overall quality.
More details about evaluation dimensions and instructions
for human labelers are available in Appendix B.
Dialogue task.
We use a dataset from Bai et al.
(2022a)4, where each example comprises a pair of conver-
sations between a human and a large language model, with
one conversation preferred by the human and the other not.
To collect data for our evaluation, it would be too costly and
time-consuming to deploy our ﬁnetuned model to chat with
humans. Instead, we construct ”pseudo” dialogues using
positive examples. We replace each model response from
a previous dialogue with our model’s output, generated by
conditioning the model on the human response and past
model outputs.
For evaluating the dialogue, we consider metrics such as
helpfulness and harmlessness. A helpful model should fol-
low instructions and infer intention from a few-shot prompt
or another interpretable pattern. Since the intention of a
given prompt can be unclear or ambiguous, we rely on
2https://github.com/EleutherAI/
lm-evaluation-harness
3
https://huggingface.co/datasets/openai/
summarize_from_feedback
4https://huggingface.co/datasets/
Anthropic/hh-rlhf
judgment from our labelers, and the main metric we use
is the labelers’ preference ratings. However, there may be
a divergence between what a user actually intended and
what the labeler thought was intended from only reading the
prompt, since the labelers are not the users who generated
the prompts.
Model Architectures. We use the same model and archi-
tecture as GPT-J (Wang & Komatsuzaki, 2021), including
the modiﬁed activation (Shazeer, 2020), multi-query atten-
tion (Shazeer, 2019), parallel layers (Wang & Komatsuzaki,
2021) and RoPE embeddings (Su et al., 2021) described
therein.
Baselines. Our baselines are pretrained model, supervised
ﬁnetuning (SFT) and reinforcement learning from human
feedback (RLHF). In our experiments, the pretrained model
is GPT-J 6B (Wang & Komatsuzaki, 2021), which is also
the base model of SFT, RLHF, and CoH.
Supervised finetuning (SFT).
The
SFT
method ﬁnetunes the model on data with positive feedback,
e.g., that is, only on human preferred summarization or
dialogue.
Prior works have shown its effectiveness in
learning from human feedback (see e.g., Ouyang et al.,
2022; Stiennon et al., 2020; Bai et al., 2022a). The SFT
objective is altered from our approach by limiting it to
positive rated data only and eliminating the use of feedback
input. We also introduce three additional variations of SFT:
(1) conditioning on feedback, but without the feedback
chain used in our approach, (2) applying the loss function
to all data, and (3) adding an unlikelihood loss to negative
rated data.
RLHF. The RLHF method involves learning a reward func-
tion based on human preference and using reinforcement
learning to maximize this reward. In our study, we adopt
the PPO algorithm, as previously used in related work. To
ensure a fair comparison, we apply identical training tech-
niques (including continued training on pretraining data)
and hyperparameters to all baselines and our own method.
To ensure a fair comparison to SFT, we apply identical train-
ing techniques (including continued training on pretraining
data) and hyperparameters to all SFT variations and our own
method. In order to make a fair comparison to RLHF, we
adjust the hyperparameters of the reinforcement learning
algorithm and reward function to obtain the best possible
results.
4. Main Results
4.1. Better Summarization
In Figure 3, we present the ROUGE scores of our models
on the TL;DR dataset, following the setting of Stiennon

Chain of Hindsight aligns Language Models with Feedback
Score
10.0
15.0
20.0
25.0
30.0
35.0
Rouge 1
Rouge 2
Rouge L
Avg
Pretrained
Supervised Finetuning
RLHF
Chain of Hindsight
Figure 3. Evaluation on summarization.
The metrics are
ROUGE score on TL;DR summary task. CoH outperforms base-
lines.
Table 1. Human evaluation on summarization. Pair-wise com-
parison between CoH and baselines on summarization task using
human evaluation. TL;DR summarization task is based on Stien-
non et al. (2020). The metrics used in human evaluation follow
deﬁnitions from prior works. We use 15 human labelers. Labelers
are encouraged to select neutral if two outputs are similar. Im-
provement column denotes the relative improvement of CoH over
baseline.
Summary (%)
Pretrained
Neutral
CoH
Improvement
Accuracy
24.5
23.6
51.9
27.4
Coherence
18.9
18.5
62.6
43.7
Coverage
31.8
20.5
47.7
15.9
Average
25.1
20.9
54.1
29.0
Summary (%)
SFT
Neutral
CoH
Improvement
Accuracy
29.5
32.6
37.9
8.4
Coherence
21.7
25.6
52.7
31.0
Coverage
30.5
25.4
44.1
13.6
Average
27.2
27.9
44.9
17.7
Summary (%)
RLHF
Neutral
CoH
Improvement
Accuracy
31.8
29.5
38.7
6.9
Coherence
23.6
20.5
55.9
32.3
Coverage
34.9
21.9
43.2
8.3
Average
30.1
24.0
45.9
15.8
et al. (2020). Our proposed approach, CoH, signiﬁcantly
outperforms the pretrained model, supervised ﬁnetuning,
and RLHF.
To further evaluate the performance of our proposed ap-
proach, we conducted human evaluation as shown in Table 1.
We conducted pairwise comparisons between CoH and the
baselines because we found that this approach was easier
for human labelers to evaluate compared to multiple options.
We hired 15 human labelers who were proﬁcient in English
from a third-party platform to provide ratings.
In the pairwise comparison, human labelers were presented
with two summaries, one generated by RLHF and the other
generated by CoH. They were instructed to select the best
(or neutral) among the two according to the three metrics
mentioned above. The results show that CoH was substan-
Accuracy
0.00
20.00
40.00
60.00
80.00
Pretrained
Supervised Finetuning
RLHF
CoH
Figure 4. Evaluation on dialogue. The metrics are the accuracy
of classifying preference label of dialogue on dataset from Bai
et al. (2022a). CoH outperforms baselines.
Table 2. Human evaluation on dialogue. Pair-wise comparison
between CoH and baselines on dialogue using human evaluation.
Dialogue task is based on Bai et al. (2022a). The metrics used
in human evaluation follow deﬁnitions from prior works. We use
15 human labelers. Labelers are encouraged to select neutral if
two outputs are similar. Improvement column denotes the relative
improvement of CoH over baseline.
Dialogue (%)
Pretrained
Neutral
CoH
Improvement
Helpful
15.8
34.8
49.4
33.6
Harmless
14.5
35.9
49.6
35.1
Average
15.2
35.3
49.5
34.4
Dialogue (%)
SFT
Neutral
CoH
Improvement
Helpful
19.6
55.3
25.1
5.5
Harmless
12.5
57.4
30.1
17.6
Average
16.1
56.3
27.6
11.6
Dialogue (%)
RLHF
Neutral
CoH
Improvement
Helpful
25.8
38.5
35.7
9.9
Harmless
16.9
49.5
33.6
16.7
Average
21.4
44.0
34.6
13.3
tially more preferred by our human labelers across multiple
metrics. This indicates that our proposed approach is more
effective at learning from human feedback. More details on
the human evaluation results can be found in the appendix.
4.2. Better Dialogue
Using the test split of dialogue datasets from Bai et al.
(2022a), we evaluate models’ ability to classify which of a
dialogue pair is more preferred. Although this is a simple
multiple choice problem, it tests the model’s understanding
of human preference. The accuracy of different models is
shown in Figure 4. While all baselines outperform the pre-
trained model, our model (CoH) achieves the highest accu-
racy and substantially outperforms the second-best baseline
RLHF.

Chain of Hindsight aligns Language Models with Feedback
Model
Rouge Avg
5.00
10.00
15.00
20.00
25.00
30.00
GPT-2 0.5B
GPT 1.5B
OPT 2.7B
GPT-J 6B
Pretrained
Supervised Finetuning
RLHF
CoH
Figure 5. Model scaling trend. Comparison of supervised ﬁne-
tuning, RLHF, and chain of hindsight on summarization task with
different model sizes. CoH scales better than baseline.
To further evaluate our model’s performance, we replace the
model response parts from each dialogue in the data with
new generations from our model. For example, if the origi-
nal dialogue is a two-turn dialogue as [human-1][chatbot-
1][human-2][chatbot-2], the new dialogue would be [human-
1][newbot-1][human-2][newbot-2], where [newbot-1] is
generated by feeding the model with [human-1] and
[newbot-2] is generated by feeding the model with [human-
1][newbot-1][human-2].
We take this approach instead of having humans directly
chat with the ﬁnetuned model to reuse human-generated
data, as collecting interactive data can be very costly and is
prone to low data quality issues.
The results are presented in Table 2. Although more than
50% of the labelers are neutral between SFT and our model
(CoH), our model is still more favorable to human label-
ers compared to SFT. Furthermore, compared with RLHF,
CoH is similarly substantially more preferred by our human
labelers.
4.3. Model Scaling Trend
The ﬁndings in Figure 5 demonstrate the impact of varying
model sizes on the performance of the CoH method relative
to supervised ﬁne-tuning (SFT) and reinforcement learn-
ing with hindsight feedback (RLHF). Notably, for smaller
model sizes, CoH exhibits a marginal decrement in per-
formance compared to SFT. However, as the model size
increases, CoH consistently surpasses both SFT and RLHF
and displays a positive scaling trend, indicating its efﬁcacy
in enhancing model performance as model complexity in-
creases.
4.4. Controllable Generation
The controllable generation results are presented in Figure 6.
The models are provided with three instructions to generate
summaries of desired quality. The ﬁrst instruction asks for
a standard summary, while the second and third instructions
ask for improved summaries conditioned on the previous
summary generated by the model. We compare the perfor-
mance of CoH with that of the RLHF model. The results
indicate that while RLHF performs well in modeling hu-
man preferences and generates high-scoring summaries by
following the ﬁrst instruction, it fails to follow the second
and third instructions, which implies that it cannot compre-
hend human intentions. On the other hand, the CoH-trained
model is capable of understanding the intention of the in-
structions and generates better summaries in the second
and third trials. We must note that although we used a sin-
gle evaluation trial and the same task prompt across the
experiments, the controllable generation technique can be
further investigated in various evaluation settings to enhance
performance (Andreas, 2022; Keskar et al., 2019).
4.5. Alignment Tax
We conducted an evaluation on a diverse set of NLP tasks
that are commonly used in previous studies (Brown et al.,
2020; Wang & Komatsuzaki, 2021) to assess the effective-
ness of aligning models with human preferences. The re-
sults are reported in Table 3. Interestingly, we found that
the average performance of models that were supervised
ﬁne-tuned decreased after alignment. This decrease could
be attributed to the well-known issue of alignment tax in lan-
guage models (Ouyang et al., 2022), which underscores the
importance of human evaluation (Lee et al., 2022). On the
other hand, our proposed method, CoH, showed moderate
improvements over both the pretrained model and super-
vised ﬁne-tuned model. This result suggests that CoH is less
susceptible to the alignment tax issue.
4.6. Model Variations
To evaluate the importance of the different components
of CoH, we varied our default conﬁguration in differ-
ent ways, measuring the change in performance on mul-
tiple metrics.
We present these results in Table 4,
where Summarization Avg denotes ROUGE scores
on the ﬁltered TL;DR dataset from Stiennon et al. (2020).
Dialogue Avg
denotes model performance of model
prompted to classify human preference on the Human Pref-
erence dataset (Bai et al., 2022a) validation split.
In Table 4 rows (A), we vary the mask ratio. Performance
decreases when using a large mask ratio, and decreases
signiﬁcantly when mask ratio equals 0, this suggests using
causal masking is crucial for preventing model from simply
copying similar tokens.
In Table 4 rows (B), we vary the number of feedback tem-
plates to investigate the signiﬁcance of diversiﬁed feedback.
Our ﬁndings indicate that incorporating a wider variety of
feedback templates results in better outcomes.

Chain of Hindsight aligns Language Models with Feedback
Controllable Generation
Score
0.00
10.00
20.00
30.00
Rouge 1
Rouge 2
Rouge L
Avg
Instruction #1
Instruction #2
Instruction #3
Controllable Generation
Score
0.00
10.00
20.00
30.00
40.00
Rouge 1
Rouge 2
Rouge L
Avg
Instruction #1
Instruction #2
Instruction #3
Instruction
Content
Instruction #1
[Article]
”Generate
a
sum-
mary of the article”
Instruction #2
[Article]
”Generate
a
sum-
mary of the article”[Summary]
”Generate a better, more pre-
cise and accurate summary”
Instruction #3
[Article]
”Generate
a
sum-
mary of the article”[Summary]
”Generate a better, more pre-
cise and accurate summary”
[Summary] ”Generate a bet-
ter, more precise and accurate
summary”.
Figure 6. Controllable generation. (a): RLHF cannot follow instructions to generate improved summary. (b): After ﬁnetuning on CoH,
model follows instructions to achieve controllable generations. (c): First instruction is standard, second and third instructions ask for
better summaries.
Table 3. Alignment Tax on Few-Shot Benchmark: The results
of our experiments on few-shot NLP benchmarks using the
Language Model Evaluation Harness are presented in
Table 3. We follow the same setup as in previous work (Brown
et al., 2020; Wang & Komatsuzaki, 2021), including the splits for
each task. The reported numbers for GPT-J are taken from its
original paper, while the numbers for other models are reported by
us. We average the results over 5 random seeds.
Zero-shot
One-shot
Few-shot
Task
GPT-J
SFT
CoH
GPT-J
SFT
CoH
GPT-J
SFT
CoH
ANLI R1
34.00
33.50 33.80
33.50
33.50 33.60
32.70
32.60 32.70
ANLI R2
32.00
32.00 32.10
34.40
34.10 34.20
33.90
34.20 34.10
ANLI R3
34.00
34.30 36.80
34.80
34.60 36.90
35.40
35.60 36.80
ARC-C
27.00
26.80 27.60
32.20
32.50 33.80
33.10
33.50 34.20
ARC-E
54.30
54.20 54.40
62.80
62.50 62.50
66.50
66.50 66.50
BoolQ
58.50
61.50 61.30
57.20
57.10 58.10
42.50
42.30 42.90
CB
41.10
41.00 40.50
41.10
41.10 40.50
42.90
42.10 42.00
COPA
71.00
70.50 69.90
80.00
80.10 80.50
82.00
82.20 81.50
HeadQA
23.50
23.00 23.80
24.00
23.80 24.30
23.90
22.50 22.80
HellaSwag
42.60
42.30 42.00
46.20
46.10 46.10
46.10
46.00 46.70
MultiRC
3.00
3.10
4.10
6.50
6.70
7.40
6.60
6.90
7.50
ReCORD
85.80
85.60 85.60
86.20
86.00 86.40
58.60
58.80 58.60
RTE
51.20
50.50 50.00
55.60
55.50 55.90
52.00
52.00 52.00
WiC
45.00
45.00 45.00
44.50
44.20 44.10
50.00
50.50 50.00
WSC
36.50
36.90 42.80
37.50
38.10 43.70
35.80
37.60 41.30
LAMBADA
(openai)
5.50
5.70
5.70
5.30
5.40
5.40
2.50
2.70
3.60
LAMBADA
(standard)
2.10
0.90
0.90
3.00
2.20
1.90
3.20
3.30
3.30
LogiQA
21.50
20.00 20.00
20.70
20.90 20.90
19.00
20.60 20.10
WinoGrande
49.70
50.40 51.20
50.70
51.80 53.50
50.70
51.10 52.80
SciQ
86.40
86.00 86.00
89.10
89.10 89.10
54.00
55.00 55.00
OpenBookQA 16.00
16.20 15.40
16.80
16.70 16.70
20.80
20.90 21.10
PIQA
72.40
72.40 72.00
73.60
73.70 73.50
74.20
74.00 74.00
Average
40.60
40.54 40.95
42.53
42.53 43.14
39.38
39.59 39.98
In Table 4 rows (C), we depart from the use of variable
length chains of hindsight in favor of employing a ﬁxed
maximum length. The resulting scores for both summa-
rization and dialogue tasks are observed to be lower than
those obtained under variable length chains. We hypothe-
size that this decrease in performance is likely due to the
fact that variable length chains of hindsight serve to reduce
the discrepancy between training/ﬁnetuning and inference
stages. Speciﬁcally, the current inference paradigm only al-
lows for one-turn generation, whereas variable length chains
of hindsight more closely resemble the generation patterns
employed during training/ﬁnetuning, thereby improving the
model’s ability to generalize to new, unseen data.
In Table 4 rows (D), we set λ = 0 which disables pretraining
dataset regularization, we observe strong overﬁtting to ﬁne-
tuning dataset with NLP Avg score decreasing signiﬁcantly.
We further observe HH score decreases a lot, suggesting
that the generalization is worse without pretraining datset
regularization.
In Table 4 rows (E), we experiment with adversarial hind-
sight feedback, where a more preferred model generation
is described as less preferred in the chain of hindsight. We
observe a signiﬁcant decrease in both summarization and
dialogue scores. This suggests that the model can follow
adversarial instructions encoded in the chain of hindsight.
In Table 4 rows (F), we experiment with two variants of
SFT.
SFT on all data denotes applying SFT not only on human-
preferred examples but also on human-rejected examples.
The SFT with feedback as input is a variant of the SFT on
all data approach, but it takes the feedback (e.g., good’,
neutral’, etc.) as an additional input. This ablative baseline
is similar to the chain of hindsight approach, but it only
conditions on one output-feedback pair, in contrast to a
chain of multiple output-feedback pairs. Therefore, this
baseline is complementary to the variable HF approach,
which comprises only multiple output-feedback pairs.
SFT with unlikelihood denotes adding an unlikelihood of
human-rejected examples to standard SFT.
We observe that among the four baselines, SFT with unlike-
lihood performs worse than the other three, especially on
the dialogue task, indicating that unlikelihood training on
human feedback data may hurt generation ability. SFT on

Chain of Hindsight aligns Language Models with Feedback
Table 4. Variations on CoH. Unlisted values are identical to those of the default model. NLP Avg: average performance across the
same set of diverse tasks from Table 3. Summarization Avg: average rouge scores on summarization dataset. Dialogue Avg: average
classiﬁcation accuracy on human feedback dataset.
Variants
HF Number
Drop Tokens
Variable HF
Mix Pretrain
Adv HF
Summarization Avg
Dialogue Avg(%)
Default
all
0.15
true
true
false
26.90
78.8
(A)
0
25.35
75.2
0.3
25.40
70.5
(B)
1
24.78
60.6
5
25.79
64.5
15
26.90
73.8
(C)
false
23.35
74.7
(D)
false
20.89
60.8
(E)
true
11.27
18.8
(F)
SFT with unlikelihood
19.97
41.5
SFT on all data
20.35
49.8
SFT with feedback as input
25.51
64.8
Supervised Finetuning (SFT)
24.87
62.3
(G)
Pretrained Model
20.23
43.8
all data performs substantially worse than SFT, conﬁrming
that ﬁne-tuning on negative data hurts performance. SFT
with feedback as input performs the best, showing the effec-
tiveness of having models learn from feedback.
5. Related Work
Learning from Hindsight. In this paper we explore learn-
ing from chain of hindsight with human feedback, an ap-
proach that enables a model to learn from errors and revise
generations. The key idea of learning from hindsight experi-
ence was explored in goal conditioned RL (Kaelbling, 1993;
Andrychowicz et al., 2017; Schaul et al., 2015). Andrychow-
icz et al. (2017) proposes hindsight experience replay (HER)
to relabel rewards and transitions retroactively to learn from
sparse feedback. While HER relies on reinforcement learn-
ing and a distance function to learn from hindsight experi-
ence, we propose a new method called CoH that constructs
a chain of hindsight experience using human feedback and
ﬁne-tunes the model directly. Our approach offers several
advantages over other methods, such as HIR (Zhang et al.,
2023), which also makes use of incorrect model outputs.
HIR can be seen as a special case of CoH with a length
of one and relies on binary feedback generated by a script
function. In contrast, CoH can learn from both binary and
multi-scale feedback by constructing a chain-of-hindsight
trajectory based on relative ordering. Unlike HIR, which
employs a complex training process involving likelihood
loss, contrastive loss, and entropy loss, our approach is
straightforward and easy to implement.
Learning from Human Feedback. Prior work have ex-
plored using human feedback to improve various tasks, such
as summarization (B¨ohm et al., 2019; Ziegler et al., 2019;
Stiennon et al., 2020), dialogue (Yi et al., 2019; Hancock
et al., 2019; Bai et al., 2022a;b; Askell et al., 2021; Scheurer
et al., 2022), translation (Kreutzer et al., 2018; Bahdanau
et al., 2016), semantic parsing (Lawrence & Riezler, 2018),
story generation (Zhou & Xu, 2020), review generation (Cho
et al., 2018), evidence extraction (Perez et al., 2019), and in-
struction following (Ouyang et al., 2022; Bai et al., 2022a).
The main techniques behind them can be categorized as
supervised ﬁnetuning or training (SFT) on ﬁltered human
annotations and learning a reward function from human
feedback for reinforcement learning, which is often dubbed
as RLHF (Christiano et al., 2017; MacGlashan et al., 2017;
Lee et al., 2021; Warnell et al., 2017) and has been used
to train RL agents without the need for hand-designed re-
wards. Ouyang et al. (2022) demonstrates improved lan-
guage model alignment performance by training models
with SFT and RLHF using human feedback. Our work be-
longs to the category of SFT, and differs from SFT in that
our method conditions on feedback and can learn from exam-
ples without positive ratings. Our method is complementary
to RLHF and can be directly combined together for further
improvement. Using instructions to provide models with
human preference and desired behaviors is demonstrated
in Bai et al. (2022b), where models are prompted with a
set of statements/principles and are trained with RLHF. In
our work, we provide models with a sequence of model out-
puts and their feedback and train models to generate desired
outputs condition on feedback/control tokens.
Instruction Finetuning and Conditional Training. Fine-
tuning on chain of hindsight using human feedback is akin to
instruction ﬁnetuning. Driven by the impressive in-context

Chain of Hindsight aligns Language Models with Feedback
learning ability of large language models, ﬁnetuning pre-
trained models on instructions has been shown to improve
language models in many benchmarks (see e.g. Wang et al.,
2022; Mishra et al., 2021; Ye et al., 2021; Chung et al.,
2022; Wei et al., 2021; Sanh et al., 2021; Zelikman et al.,
2022; Huang et al., 2022, inter alia). Mostly the instruc-
tions are reformatted examples from NLP benchmarks (e.g.
Wei et al., 2021; Chung et al., 2022). CoT prompt (Wei
et al., 2022) are widely considered as instructions in prior
works (Chung et al., 2022; Wei et al., 2021), speciﬁcally in
the form of step by step explanations written by humans.
In relation to these, our chain of hindsight consists of hu-
man written hindsight feedback and ranked model outputs.
Conditional training (Keskar et al., 2019; Ficler & Gold-
berg, 2017; Laskin et al., 2022; Chen et al., 2021; Fan et al.,
2018; Lu et al., 2022) explore conditioning model on some
control tokens for controllable generations. In relation to
them, CoH generalizes to condition on a sequence of control
tokens instead of one control token. By doing so, CoH en-
ables model to understand the differences between control
tokens and their corresponding outputs. Our work suggests a
promising direction of using hindsight feedback to construct
instructions from model outputs, and can be combined with
prior instruction ﬁnetuning and conditional training works
for further improvements.
6. Conclusion
Our approach, Chain-of-Hindsight (CoH), is inspired by
how humans learn from rich feedback in the form of lan-
guage. We condition language models on a sequence of
hindsight feedback, allowing them to effectively leverage
all examples regardless of their preference score. This en-
ables the model to effectively learn from rich feedback,
aligning the model’s output with the feedback it receives. In
our experiments on summarization and dialogue tasks, CoH
signiﬁcantly outperforms all baselines, including supervised
ﬁnetuning and reinforcement learning with human feedback.
We believe that CoH has great potential for future applica-
tions in other forms of feedback, including automatic and
numeric feedback.
Acknowledgments
We thank the members of RLL for their helpful discussions
and feedback about this work. We thank Google TPU Re-
search Cloud for TPU access.
References
Andreas, J. Language models as agent models. Conference
On Empirical Methods In Natural Language Processing,
2022. doi: 10.48550/arXiv.2212.01681.
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,
R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,
and Zaremba, W. Hindsight experience replay. Advances
in neural information processing systems, 30, 2017.
Aroca-Ouellette, S., Paik, C., Roncone, A., and Kann,
K. PROST: Physical reasoning about objects through
space and time. In Findings of the Association for Com-
putational Linguistics: ACL-IJCNLP 2021, pp. 4597–
4608, Online, August 2021. Association for Computa-
tional Linguistics. doi: 10.18653/v1/2021.ﬁndings-acl.
404. URL https://aclanthology.org/2021.
findings-acl.404.
Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D.,
Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma,
N., et al. A general language assistant as a laboratory for
alignment. arXiv preprint arXiv:2112.00861, 2021.
Bahdanau, D., Brakel, P., Xu, K., Goyal, A., Lowe, R.,
Pineau, J., Courville, A., and Bengio, Y.
An actor-
critic algorithm for sequence prediction. arXiv preprint
arXiv:1607.07086, 2016.
Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., Das-
Sarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T.,
et al. Training a helpful and harmless assistant with rein-
forcement learning from human feedback. arXiv preprint
arXiv:2204.05862, 2022a.
Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J.,
Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKin-
non, C., et al. Constitutional ai: Harmlessness from ai
feedback. arXiv preprint arXiv:2212.08073, 2022b.
Bisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y.
Piqa: Reasoning about physical commonsense in natural
language. arXiv preprint arXiv: Arxiv-1911.11641, 2019.
Bisk, Y., Zellers, R., Le bras, R., Gao, J., and Choi,
Y.
PIQA: Reasoning about physical commonsense
in natural language.
In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence, volume 34, pp.
7432–7439, 04 2020.
doi:
10.1609/aaai.v34i05.
6239.
URL https://ojs.aaai.org/index.
php/AAAI/article/view/6239.
B¨ohm, F., Gao, Y., Meyer, C. M., Shapira, O., Dagan, I.,
and Gurevych, I. Better rewards yield better summaries:
Learning to summarise without references. arXiv preprint
arXiv:1909.01214, 2019.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
Advances in neural information processing systems, 33:
1877–1901, 2020.

Chain of Hindsight aligns Language Models with Feedback
Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A.,
Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. De-
cision transformer: Reinforcement learning via sequence
modeling. Advances in neural information processing
systems, 34:15084–15097, 2021.
Cho, W. S., Zhang, P., Zhang, Y., Li, X., Galley, M.,
Brockett, C., Wang, M., and Gao, J. Towards coherent
and cohesive long-form text generation. arXiv preprint
arXiv:1811.00511, 2018.
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
G., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,
Gehrmann, S., et al. Palm: Scaling language modeling
with pathways. arXiv preprint arXiv:2204.02311, 2022.
Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg,
S., and Amodei, D. Deep reinforcement learning from
human preferences. In Advances in Neural Information
Processing Systems, pp. 4299–4307, 2017.
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,
Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma,
S., et al. Scaling instruction-ﬁnetuned language models.
arXiv preprint arXiv:2210.11416, 2022.
Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,
A., Schoenick, C., and Tafjord, O.
Think you have
solved question answering?
try ARC, the AI2 Rea-
soning Challenge.
Computing Research Repository,
arXiv:1803.05457, 2018. version 1.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:
Pre-training of deep bidirectional transformers for lan-
guage understanding. arXiv preprint arXiv:1810.04805,
2018.
Fan, A., Lewis, M., and Dauphin, Y. Hierarchical neu-
ral story generation.
arXiv preprint arXiv:
Arxiv-
1805.04833, 2018.
Ficler, J. and Goldberg, Y. Controlling linguistic style as-
pects in neural language generation. arXiv preprint arXiv:
Arxiv-1707.02633, 2017.
Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y.,
Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse,
K., et al. Red teaming language models to reduce harms:
Methods, scaling behaviors, and lessons learned. arXiv
preprint arXiv:2209.07858, 2022.
Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T.,
Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,
Presser, S., and Leahy, C. The Pile: An 800GB dataset of
diverse text for language modeling. Computing Research
Repository, arXiv:2101.00027, 2020. version 1.
Hancock, B., Bordes, A., Mazare, P.-E., and Weston, J.
Learning from dialogue after deployment: Feed yourself,
chatbot! arXiv preprint arXiv:1901.05415, 2019.
Huang, J., Gu, S. S., Hou, L., Wu, Y., Wang, X., Yu, H., and
Han, J. Large language models can self-improve. arXiv
preprint arXiv:2210.11610, 2022.
Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. Trivi-
aQA: A large scale distantly supervised challenge dataset
for reading comprehension. In Proceedings of the 55th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 1601–1611,
Vancouver, Canada, July 2017. Association for Compu-
tational Linguistics. doi: 10.18653/v1/P17-1147. URL
https://aclanthology.org/P17-1147.
Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M.,
Ronneberger, O., Tunyasuvunakool, K., Bates, R., ˇZ´ıdek,
A., Potapenko, A., et al. Highly accurate protein structure
prediction with alphafold. Nature, 596(7873):583–589,
2021.
Kaelbling, L. P. Learning to achieve goals. In IJCAI, vol-
ume 2, pp. 1094–8. Citeseer, 1993.
Keskar, N. S., McCann, B., Varshney, L. R., Xiong, C.,
and Socher, R. Ctrl: A conditional transformer language
model for controllable generation. PREPRINT, 2019.
Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.
Kreutzer, J., Khadivi, S., Matusov, E., and Riezler, S. Can
neural machine translation be improved with user feed-
back? arXiv preprint arXiv:1804.05958, 2018.
Laskin, M., Wang, L., Oh, J., Parisotto, E., Spencer, S.,
Steigerwald, R., Strouse, D., Hansen, S., Filos, A.,
Brooks, E., Gazeau, M., Sahni, H., Singh, S., and Mnih,
V. In-context reinforcement learning with algorithm dis-
tillation. arXiv preprint arXiv: Arxiv-2210.14215, 2022.
Lawrence, C. and Riezler, S. Improving a neural seman-
tic parser by counterfactual learning from human bandit
feedback. arXiv preprint arXiv:1805.01252, 2018.
Lee, K., Smith, L., and Abbeel, P.
Pebble: Feedback-
efﬁcient interactive reinforcement learning via relabeling
experience and unsupervised pre-training. International
Conference On Machine Learning, 2021.
Lee, M., Srivastava, M., Hardy, A., Thickstun, J., Durmus,
E., Paranjape, A., Gerard-Ursin, I., Li, X. L., Ladhak,
F., Rong, F., et al. Evaluating human-language model
interaction. arXiv preprint arXiv:2212.09746, 2022.

Chain of Hindsight aligns Language Models with Feedback
Liu, H., Geng, X., Lee, L., Mordatch, I., Levine, S., Narang,
S., and Abbeel, P. Fcm: Forgetful causal masking makes
causal language models better zero-shot learners. arXiv
preprint arXiv:2210.13432, 2022.
Liu, J., Cui, L., Liu, H., Huang, D., Wang, Y., and Zhang,
Y. LogiQA: A challenge dataset for machine reading
comprehension with logical reasoning. In Bessiere, C.
(ed.), Proceedings of the Twenty-Ninth International
Joint Conference on Artiﬁcial Intelligence, IJCAI-20, pp.
3622–3628. International Joint Conferences on Artiﬁ-
cial Intelligence Organization, 7 2020. doi: 10.24963/
ijcai.2020/501.
URL https://www.ijcai.org/
proceedings/2020/501.
Lu, X., Welleck, S., Hessel, J., Jiang, L., Qin, L., West,
P., Ammanabrolu, P., and Choi, Y. QUARK: Control-
lable text generation with reinforced unlearning. In Oh,
A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
Advances in Neural Information Processing Systems,
2022. URL https://openreview.net/forum?
id=5HaIds3ux5O.
MacGlashan, J., Ho, M. K., Loftin, R., Peng, B., Wang,
G., Roberts, D. L., Taylor, M. E., and Littman, M. Inter-
active learning from policy-dependent human feedback.
International Conference On Machine Learning, 2017.
Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can
a suit of armor conduct electricity?
A new dataset
for open book question answering. In Proceedings of
the 2018 Conference on Empirical Methods in Natu-
ral Language Processing, pp. 2381–2391, Brussels, Bel-
gium, October-November 2018. Association for Compu-
tational Linguistics. doi: 10.18653/v1/D18-1260. URL
https://aclanthology.org/D18-1260.
Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H. Cross-
task generalization via natural language crowdsourcing
instructions. arXiv preprint arXiv:2104.08773, 2021.
Mostafazadeh, N., Chambers, N., He, X., Parikh, D., Batra,
D., Vanderwende, L., Kohli, P., and Allen, J. A cor-
pus and evaluation framework for deeper understanding
of commonsense stories. arXiv preprint arXiv: Arxiv-
1604.01696, 2016.
Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,
C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et al.
Webgpt: Browser-assisted question-answering with hu-
man feedback. arXiv preprint arXiv:2112.09332, 2021.
Nie, Y., Williams, A., Dinan, E., Bansal, M., Weston, J.,
and Kiela, D. Adversarial nli: A new benchmark for
natural language understanding. arXiv preprint arXiv:
Arxiv-1910.14599, 2019.
Nie, Y., Williams, A., Dinan, E., Bansal, M., Weston,
J., and Kiela, D.
Adversarial NLI: A new bench-
mark for natural language understanding. In Proceed-
ings of the 58th Annual Meeting of the Association
for Computational Linguistics, pp. 4885–4901, Online,
July 2020. Association for Computational Linguistics.
doi: 10.18653/v1/2020.acl-main.441.
URL https:
//aclanthology.org/2020.acl-main.441.
OpenAI. ChatGPT, OpenAI. https://openai.com/
blog/chatgpt/, 2022.
[Online; accessed 2-Feb-
2023].
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama,
K., Ray, A., et al.
Training language models to fol-
low instructions with human feedback. arXiv preprint
arXiv:2203.02155, 2022.
Paperno, D., Kruszewski, G., Lazaridou, A., Pham, Q. N.,
Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and
Fern´andez, R. The lambada dataset: Word prediction
requiring a broad discourse context. arXiv preprint arXiv:
Arxiv-1606.06031, 2016.
Pe˜nas, A., Hovy, E., Forner, P., Rodrigo, ´A., Sutcliffe, R.,
and Morante, R. QA4MRE 2011-2013: Overview of
question answering for machine reading evaluation. In
Forner, P., M¨uller, H., Paredes, R., Rosso, P., and Stein,
B. (eds.), Information Access Evaluation. Multilinguality,
Multimodality, and Visualization, pp. 303–320, Berlin,
Heidelberg, 2013. Springer Berlin Heidelberg. ISBN 978-
3-642-40802-1. doi: 10.1007/978-3-642-40802-1 29.
Perez, E., Karamcheti, S., Fergus, R., Weston, J., Kiela,
D., and Cho, K.
Finding generalizable evidence by
learning to convince q&a models.
arXiv preprint
arXiv:1909.05863, 2019.
Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y.
Winogrande: An adversarial winograd schema challenge
at scale. In The Thirty-Fourth AAAI Conference on Ar-
tiﬁcial Intelligence, AAAI 2020, The Thirty-Second In-
novative Applications of Artiﬁcial Intelligence Confer-
ence, IAAI 2020, The Tenth AAAI Symposium on Edu-
cational Advances in Artiﬁcial Intelligence, EAAI 2020,
New York, NY, USA, February 7-12, 2020, pp. 8732–8740.
AAAI Press, 2020. URL https://ojs.aaai.org/
index.php/AAAI/article/view/6399.
Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L.,
Alyafeai, Z., Chafﬁn, A., Stiegler, A., Scao, T. L., Raja,
A., et al. Multitask prompted training enables zero-shot
task generalization. arXiv preprint arXiv:2110.08207,
2021.

Chain of Hindsight aligns Language Models with Feedback
Sarlin, P., DeTone, D., Malisiewicz, T., and Rabinovich,
A. Superglue: Learning feature matching with graph
neural networks.
In 2020 IEEE/CVF Conference on
Computer Vision and Pattern Recognition, CVPR 2020,
Seattle, WA, USA, June 13-19, 2020, pp. 4937–4946.
Computer Vision Foundation / IEEE, 2020.
doi:
10.1109/CVPR42600.2020.00499.
URL
https:
//openaccess.thecvf.com/content_CVPR_
2020/html/Sarlin_SuperGlue_Learning_
Feature_Matching_With_Graph_Neural_
Networks_CVPR_2020_paper.html.
Schaul, T., Horgan, D., Gregor, K., and Silver, D. Universal
value function approximators. In International conference
on machine learning, pp. 1312–1320. PMLR, 2015.
Scheurer, J., Campos, J. A., Chan, J. S., Chen, A., Cho, K.,
and Perez, E. Training language models with language
feedback. arXiv preprint arXiv: Arxiv-2204.14146, 2022.
Shazeer, N. Fast transformer decoding: One write-head is
all you need. arXiv preprint arXiv: Arxiv-1911.02150,
2019.
Shazeer, N.
Glu variants improve transformer.
arXiv
preprint arXiv: Arxiv-2002.05202, 2020.
Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R.,
Voss, C., Radford, A., Amodei, D., and Christiano,
P. F. Learning to summarize with human feedback. Ad-
vances in Neural Information Processing Systems, 33:
3008–3021, 2020.
Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu,
Y. Roformer: Enhanced transformer with rotary position
embedding. arXiv preprint arXiv: Arxiv-2104.09864,
2021.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention
is all you need. In Advances in Neural Information Pro-
cessing Systems, volume 30, pp. 5998–6008. Curran As-
sociates, Inc., 2017. URL https://proceedings.
neurips.cc/paper/2017/hash/
3f5ee243547dee91fbd053c1c4a845aa-Abstract.
html.
Vilares, D. and G´omez-Rodr´ıguez, C.
HEAD-QA: A
healthcare dataset for complex reasoning. In Proceed-
ings of the 57th Annual Meeting of the Association
for Computational Linguistics, pp. 960–966, Florence,
Italy, July 2019. Association for Computational Lin-
guistics. doi: 10.18653/v1/P19-1092. URL https:
//aclanthology.org/P19-1092.
V¨olske, M., Potthast, M., Syed, S., and Stein, B. Tl; dr: Min-
ing reddit to learn automatic summarization. In Proceed-
ings of the Workshop on New Frontiers in Summarization,
pp. 59–63, 2017.
Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A.,
Michael, J., Hill, F., Levy, O., and Bowman, S. Super-
GLUE: A stickier benchmark for general-purpose lan-
guage understanding systems. In Wallach, H., Larochelle,
H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E., and
Garnett, R. (eds.), Advances in Neural Information Pro-
cessing Systems, volume 32, pp. 3266–3280. Curran As-
sociates, Inc., 2019. URL https://proceedings.
neurips.cc/paper/2019/hash/
4496bf24afe7fab6f046bf4923da8de6-Abstract.
html.
Wang,
B.
and
Komatsuzaki,
A.
GPT-J-6B:
A
6
Billion
Parameter
Autoregressive
Language
Model.
https://github.com/kingoflolz/
mesh-transformer-jax, May 2021.
Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y.,
Mirzaei, A., Arunkumar, A., Ashok, A., Dhanasekaran,
A. S., Naik, A., Stap, D., et al. Super-naturalinstructions:
Generalization via declarative instructions on 1600+ nlp
tasks. URL https://arxiv. org/abs/2204.07705, 2022.
Warnell, G., Waytowich, N. R., Lawhern, V., and Stone,
P.
Deep tamer:
Interactive agent shaping in high-
dimensional state spaces. Aaai Conference On Artiﬁcial
Intelligence, 2017. doi: 10.1609/aaai.v32i1.11485.
Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester,
B., Du, N., Dai, A. M., and Le, Q. V. Finetuned lan-
guage models are zero-shot learners.
arXiv preprint
arXiv:2109.01652, 2021.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.,
Le, Q., and Zhou, D. Chain of thought prompting elic-
its reasoning in large language models. arXiv preprint
arXiv:2201.11903, 2022.
Welbl, J., Liu, N. F., and Gardner, M.
Crowdsourcing
multiple choice science questions. In Proceedings of the
3rd Workshop on Noisy User-generated Text, pp. 94–106,
Copenhagen, Denmark, September 2017. Association for
Computational Linguistics. doi: 10.18653/v1/W17-4413.
URL https://aclanthology.org/W17-4413.
Ye, Q., Lin, B. Y., and Ren, X. Crossﬁt: A few-shot learn-
ing challenge for cross-task generalization in nlp. arXiv
preprint arXiv:2104.08835, 2021.
Yi, S., Goel, R., Khatri, C., Cervone, A., Chung, T., Heday-
atnia, B., Venkatesh, A., Gabriel, R., and Hakkani-Tur, D.
Towards coherent and engaging spoken dialog response

Chain of Hindsight aligns Language Models with Feedback
generation using automatic conversation evaluators. arXiv
preprint arXiv:1904.13015, 2019.
Zelikman, E., Mu, J., Goodman, N. D., and Wu, Y. T. Star:
Self-taught reasoner bootstrapping reasoning with reason-
ing. 2022.
Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi,
Y. HellaSwag: Can a machine really ﬁnish your sen-
tence? In Proceedings of the 57th Annual Meeting of
the Association for Computational Linguistics, pp. 4791–
4800, Florence, Italy, July 2019. Association for Compu-
tational Linguistics. doi: 10.18653/v1/P19-1472. URL
https://aclanthology.org/P19-1472.
Zhang, T., Liu, F., Wong, J., Abbeel, P., and Gonzalez,
J. E. The wisdom of hindsight makes language models
better instruction followers. arXiv preprint arXiv: Arxiv-
2302.05206, 2023.
Zhou, W. and Xu, K. Learning to compare for better training
and evaluation of open domain natural language genera-
tion models. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, volume 34, pp. 9717–9724, 2020.
Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford,
A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning
language models from human preferences. arXiv preprint
arXiv:1909.08593, 2019.

Chain of Hindsight aligns Language Models with Feedback
A. Feedback Templates
Table 5, Table 6 and Table 7 show the templates used in forming chain-of-hindsight training sequences. In order to increase
diversity, as demonstrated in Algorithm 1, the training sequences can also consist of a single positive or negative example,
in which cases, there is no need for providing comparative feedback. In such cases, the data can be formatted simply by
pre-appending positive or negative words to the input, the templates for such cases are also shown in the same Tables.
It is important to note that our use of scripted feedback in this study is not meant to preclude the use of open-ended or
human-in-the-loop feedback. Indeed, we believe that incorporating such forms of feedback could potentially yield even
more nuanced and informative results, and we encourage future researchers to explore these avenues further.
Table 5. Feedback templates for summarization data. We have omitted task prompts and other context-speciﬁc information, such as
cited sources. This information can be accessed by simply adding it as a preﬁx to the input sequence. We have excluded templates for the
opposite direction of the sequence for simplicity, but they are readily available from the templates listed.
Data Source
Feedback Templates
Summary
a good summary is {pos}
Summary
a bad summary is {neg}
Summary
the following is a good summary {pos}
Summary
the following is a bad summary {neg}
Summary
generate a good summary: {pos}
Summary
generate a bad summary: {neg}
Summary
good summary: {pos}
Summary
bad summary: {neg}
Summary
good: {pos}
Summary
bad: {neg}
Summary
let’s generate a good summary {pos}
Summary
let’s generate a bad summary: {neg}
Summary
a good summary is given by {pos}
Summary
a bad summary is given by {neg}
Summary
a less preferred summary is {neg}, a more preferred summary is {pos}
Summary
a more preferred summary is {pos}, a less preferred summary is {neg}
Summary
a not so good summary is {neg}, a better summary is {pos}
Summary
a good summary is {pos}, a worse summary is {neg}
Summary
an summary is {neg}, a better summary is {pos}
Summary
an summary is {pos}, a worse summary is {neg}
Summary
the following is not the best summary {neg}, a better summary can be {pos}
Summary
the following is a good summary {pos}, a worse summary can be {neg}
Summary
let’s generate a not very good summary {neg}, let’s generate a better summary {pos}
Summary
let’s generate a very good summary {pos}, let’s generate a worse summary {neg}
Summary
For the following two summaries {pos} and {neg}, which one is better? The answer is {1st}
Summary
For the following two summaries {neg} and {pos}, which one is better? The answer is {2nd}
Summary
{pos} and {neg} are two summaries, let’s think about which one is better? The answer is {1st}
Summary
{neg} and {pos} are two summaries, let’s think about which one is better? The answer is {2nd}
Summary
For the following two summaries {pos} and {neg}, which one is worse? The answer is {2nd}
Summary
For the following two summaries {neg} and {pos}, which one is worse? The answer is {1st}
Summary
{pos} and {neg} are two summaries, let’s think about which one is worse? The answer is {2nd}
Summary
{neg} and {pos} are two summaries, let’s think about which one is worse? The answer is {1st}
B. Human Evaluation Instructions
For human evaluations, we instruct human labelers to select preferred output. We follow prior work Stiennon et al. (2020);
Bai et al. (2022a) and resue their instructions and deﬁnitions of helpful, useful, etc. The instructions we use in summarization
task are from Stiennon et al. (2020) which is publicly available at https://docs.google.com/document/d/
1MJCqDNjzD04UbcnVZ-LmeXJ04-TKEICDAepXyMCBUb8/edit#. The instructions we use for dialogue task is
from Bai et al. (2022a), we refer the readers to original paper for details.

Chain of Hindsight aligns Language Models with Feedback
Table 6. Feedback templates for dialogue data. We have omitted task prompts and other context-speciﬁc information, such as cited
sources. This information can be accessed by simply adding it as a preﬁx to the input sequence. We have excluded templates for the
opposite direction of the sequence for simplicity, but they are readily available from the templates listed.
Data Source
Feedback Templates
Dialogue
a good dialogue is {pos}
Dialogue
a bad dialogue is {neg}
Dialogue
the following is a good dialogue {pos}
Dialogue
the following is a bad dialogue {neg}
Dialogue
generate a good dialogue: {pos}
Dialogue
generate a bad dialogue: {neg}
Dialogue
good dialogue: {pos}
Dialogue
bad dialogue: {neg}
Dialogue
good: {pos}
Dialogue
bad: {neg}
Dialogue
let’s generate a good dialogue {pos}
Dialogue
let’s generate a bad dialogue: {neg}
Dialogue
a good dialogue is given by {pos}
Dialogue
a bad dialogue is given by {neg}
Dialogue
a less preferred dialogue is {neg}, a more preferred dialogue is {pos}
Dialogue
a more preferred dialogue is {pos}, a less preferred dialogue is {neg}
Dialogue
a not so good dialogue is {neg}, a better dialogue is {pos}
Dialogue
a good dialogue is {pos}, a worse dialogue is {neg}
Dialogue
an dialogue is {neg}, a better dialogue is {pos}
Dialogue
an dialogue is {pos}, a worse dialogue is {neg}
Dialogue
the following is not the best dialogue {neg}, a better dialogue can be {pos}
Dialogue
the following is a good dialogue {pos}, a worse dialogue can be {neg}
Dialogue
let’s generate a not very good dialogue {neg}, let’s generate a better dialogue {pos}
Dialogue
let’s generate a very good dialogue {pos}, let’s generate a worse dialogue {neg}
Dialogue
For the following two dialogues {pos} and {neg}, which one is better? The answer is {1st}
Dialogue
For the following two dialogues {neg} and {pos}, which one is better? The answer is {2nd}
Dialogue
{pos} and {neg} are two dialogues, let’s think about which one is better? The answer is {1st}
Dialogue
{neg} and {pos} are two dialogues, let’s think about which one is better? The answer is {2nd}
Dialogue
For the following two dialogues {pos} and {neg}, which one is worse? The answer is {2nd}
Dialogue
For the following two dialogues {neg} and {pos}, which one is worse? The answer is {1st}
Dialogue
{pos} and {neg} are two dialogues, let’s think about which one is worse? The answer is {2nd}
Dialogue
{neg} and {pos} are two dialogues, let’s think about which one is worse? The answer is {1st}
C. Version Control
V4 →V5. Included a comprehensive list of templates employed in the experiments; provided an implementation link.
V3 →V4. Added the results of SFT with feedback input and SFT on all data baselines.
V2 →V3. Added the results of RLHF.
V1 →V2. Corrected typos in introduction and title.
D. Hyperparameters
All models are trained with the Adam (Kingma & Ba, 2014) optimizer, with β1 = 0.9, β2 = 0.95, and an epsilon of
1.0e−8. The batch size for human feedback data is set to 512, while for pretraining data it is set to 2048. The value of λ
is 1.5, which determines the relative strength of gradients from the human feedback dataset and pretraining dataset. The
pretraining regularization term is computed using the Pile dataset (Gao et al., 2020). Since we applied random past token
masking, dropout is not used in our experiments, as suggested by Liu et al. (2022). When ﬁnetuning, we combined three
human feedback datasets, and the data was sampled proportionally to their size to ensure balance across the datasets. The
implementation is available at https://github.com/lhao499/CoH.

Chain of Hindsight aligns Language Models with Feedback
Figure 7. Screenshots of our labeling interface for rating dialog. For each metric, labelers are asked to choose preferred dialog.
E. Task List and Prompt Format
For an automatic evaluation of model’s ability on diverse NLP tasks, we evaluate our model on a diverse collection of
standard language model evaluation datasets: ANLI (Nie et al., 2020), ARC (Clark et al., 2018), HeadQA (English) (Vilares
& G´omez-Rodr´ıguez, 2019), HellaSwag (Zellers et al., 2019), LAMBDADA (Paperno et al., 2016), LogiQA (Liu et al.,
2020), OpenBookQA (Mihaylov et al., 2018), PiQA (Bisk et al., 2020), PROST (Aroca-Ouellette et al., 2021), QA4MRE
(Pe˜nas et al., 2013) (2013), SciQ (Welbl et al., 2017), TriviaQA (Joshi et al., 2017), Winogrande (Sakaguchi et al., 2020),
and the SuperGlue version of the Winograd Schemas Challenge (WSC) (Wang et al., 2019).
Two other tasks are summarization (Stiennon et al., 2020) and dialogue (Bai et al., 2022a). In our ablation study, we consider
prompting model to evaluate model on whether it knows an example dialogue is preferred or not preferred by human using
the dialogue dataset (Bai et al., 2022a).
The majority of prompt formats follow GPT-3 (Brown et al., 2020) which are made available by https://github.com/
EleutherAI/lm-evaluation-harness. We follow the prompt formats used in Bai et al. (2022a) and Stiennon
et al. (2020) for dialogue and summarization tasks.
F. Web UI
In Figure 8 and Figure 7, we show screenshots of our labeling interface, that all of our labelers use to rate data. Labelers can
choose preferred model output or choose neutral in cases where two outputs seem to be of similar quality.

Chain of Hindsight aligns Language Models with Feedback
Figure 8. Screenshots of our labeling interface for rating summary. For each metric, labelers are asked to choose preferred summary.

Chain of Hindsight aligns Language Models with Feedback
Table 7. Feedback templates for webgpt data. We have omitted task prompts and other context-speciﬁc information, such as cited
sources. This information can be accessed by simply adding it as a preﬁx to the input sequence. We have excluded templates for the
opposite direction of the sequence for simplicity, but they are readily available from the templates listed.
Data Source
Feedback Templates
WebGPT
a good webgpt is {pos}
WebGPT
a bad webgpt is {neg}
WebGPT
the following is a good webgpt {pos}
WebGPT
the following is a bad webgpt {neg}
WebGPT
generate a good webgpt: {pos}
WebGPT
generate a bad webgpt: {neg}
WebGPT
good webgpt: {pos}
WebGPT
bad webgpt: {neg}
WebGPT
good: {pos}
WebGPT
bad: {neg}
WebGPT
let’s generate a good webgpt {pos}
WebGPT
let’s generate a bad webgpt: {neg}
WebGPT
a good webgpt is given by {pos}
WebGPT
a bad webgpt is given by {neg}
WebGPT
a less preferred webgpt is {neg}, a more preferred webgpt is {pos}
WebGPT
a more preferred webgpt is {pos}, a less preferred webgpt is {neg}
WebGPT
a not so good webgpt is {neg}, a better webgpt is {pos}
WebGPT
a good webgpt is {pos}, a worse webgpt is {neg}
WebGPT
an webgpt is {neg}, a better webgpt is {pos}
WebGPT
an webgpt is {pos}, a worse webgpt is {neg}
WebGPT
the following is not the best webgpt {neg}, a better webgpt can be {pos}
WebGPT
the following is a good webgpt {pos}, a worse webgpt can be {neg}
WebGPT
let’s generate a not very good webgpt {neg}, let’s generate a better webgpt {pos}
WebGPT
let’s generate a very good webgpt {pos}, let’s generate a worse webgpt {neg}
WebGPT
For the following two answers {pos} and {neg}, which one is better? The answer is {1st}
WebGPT
For the following two answers {neg} and {pos}, which one is better? The answer is {2nd}
WebGPT
{pos} and {neg} are two answers, let’s think about which one is better? The answer is {1st}
WebGPT
{neg} and {pos} are two answers, let’s think about which one is better? The answer is {2nd}
WebGPT
For the following two answers {pos} and {neg}, which one is worse? The answer is {2nd}
WebGPT
For the following two answers {neg} and {pos}, which one is worse? The answer is {1st}
WebGPT
{pos} and {neg} are two answers, let’s think about which one is worse? The answer is {2nd}
WebGPT
{neg} and {pos} are two answers, let’s think about which one is worse? The answer is {1st}
WebGPT (tie)
a good answer is {pos}, a good answer is {neg}
WebGPT (tie)
the following is a good answer {pos}, the following is a good answer {neg}
WebGPT (tie)
generate a good answer: {pos}
WebGPT (tie)
generate a good answer: {neg}
WebGPT (tie)
the following are two equally good answers {pos} and {neg}
WebGPT (tie)
the following are two equally preferred answers {pos} and {neg}
WebGPT (tie)
two equally good answers are the following {pos} and {neg}
WebGPT (tie)
two equally preferred answers are the following {pos} and {neg}

