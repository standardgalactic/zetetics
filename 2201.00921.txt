Neurons as hierarchies of quantum reference
frames
Chris Fieldsa∗, James F. Glazebrookb,c and Michael Levind
a 23 Rue des Lavandi`eres, 11160 Caunes Minervois, FRANCE
ORCID: 0000-0002-4812-0744
b Department of Mathematics and Computer Science,
Eastern Illinois University, Charleston, IL 61920 USA
c Adjunct Faculty, Department of Mathematics,
University of Illinois at Urbana-Champaign, Urbana, IL 61801 USA
d Allen Discovery Center at Tufts University, Medford, MA 02155 USA
ORCID: 0000-0001-7292-8084
January 5, 2022
Abstract
Conceptual and mathematical models of neurons have lagged behind empirical understand-
ing for decades. Here we extend previous work in modeling biological systems with fully
scale-independent quantum information-theoretic tools to develop a uniform, scalable rep-
resentation of synapses, dendritic and axonal processes, neurons, and local networks of
neurons. In this representation, hierarchies of quantum reference frames act as hierarchical
active-inference systems. The resulting model enables speciﬁc predictions of correlations
between synaptic activity, dendritic remodeling, and trophic reward. We summarize how
the model may be generalized to nonneural cells and tissues in developmental and regener-
ative contexts.
Keywords
Activity-dependent remodeling; Bayesian inference; Bioelectricity; Computation; Learning;
Memory
∗Corresponding author at: 23 Rue des Lavandi`eres, 11160 Caunes Minervois, FRANCE; E-mail address:
ﬁeldsres@gmail.com
1
arXiv:2201.00921v1  [q-bio.NC]  4 Jan 2022

1
Introduction
Neurons are canonical biological information processors.
Theoretical and, particularly,
conceptual models of neural information processing, however, lag increasingly far behind our
developing empirical understanding of neurons as electrically-excitable cells. Experimental
work over the past two decades has, for example, ﬁrmly established that dendrites undergo
activity-dependent remodeling [1, 2, 3], particularly alterations of spine location, density,
and function [4], even in adult humans. This ontogenic process is functionally analogous
to the evolution of structural and positional diversity of dendrites as they have adapted to
a spectrum of functional roles [5], e.g. the implementation of deep learning via synaptic
plasticity [6, 7]. Neurons are not, therefore, static structures, but rather can be regarded
as undergoing continuous development throughout the life cycle. This dynamic process has
signiﬁcant consequences for both neuron-level and organism-level function. For example, in
organisms (such as caterpillars transitioning into butterﬂies or moths) that exhibit drastic
remodeling and reconstruction of their brains, some of their learned memories remain and
survive the process [8]. In other contexts, memories can be imprinted on newly regenerating
brains from other tissues [9, 10], underscoring the plasticity of large-scale neural structure
and of the information stored within it. These eﬀects of remodeling are not just an issue for
so-called lower animals, as applications in regenerative medicine are likely to soon produce
human patients in whom some portion of the brain has been replaced by the progeny of
na¨ıve stem cells to treat degenerative disease or brain damage.
It has also been shown that subnetworks of dendrites can compute local logical operations
including exclusive-or (XOR) [11]. Nonetheless, the idea that dendritic trees eﬀectively com-
pute weighted sums, dating back to [12], continues not only to dominate artiﬁcial neural
network (ANN) development, but also to guide biological thinking. Explicit cable-theory
modeling can reproduce time-dependent signal propagation and processing in simpliﬁed
representations of dendritic trees [13], but rapidly becomes intractable with increasing geo-
metric complexity. Increasingly sophisticated hardware models of neurons allow exploration
of phase coding, frequency modulation, and other forms of hybrid analog-digital computing
[14, 15], but are not readily mapped to biological neural networks and are not standard
components of a theoretical neuroscientist’s toolkit.
Conceptualizing neurons as biological wires – even functionally sophisticated wires – pro-
vides, moreover, no insight into the fundamental question of cognitive neuroscience: the
question of how information present in the external world is encoded, via some combination
of evolution, development, remodeling, and learning, into the functional architecture of a
nervous system. Brains are not constructed, as ANNs are, in the absense of functional
activity, nor do they “begin learning” from a default initial state of uniform connectivity.
Brains are not “wired up” into some starting conﬁguration, after which they are turned on
and exposed to the world. Brains instead develop from neural primordia that are already
functionally complex multicellular microenvironments. The function of the nervous system,
from its earliest phylogenetic and ontogenetic role in directing morphogenesis [16] to adult
cognition, depends on the ability of individual neurons to negotiate this microenvironment,
2

both structurally and functionally. The local microenvironment, with its diverse cell types,
biochemical and (micro)anatomical complexity, and network of biomolecular and bioelectric
signaling, is the “world” with which each individual neuron interacts, and is each neuron’s
sole source of nongenomic information. There are many interesting scenarios supporting
this viewpoint; some are of the ‘socialization’ and decision making type.
For instance,
assortative neural networks demonstrate collective resilience, whereby nodes of a certain
degree have an aﬃnity to team up with those of the same topological type, and thus often
contribute to formation of a small world network structure [17, 18]. Neural network model-
ing of independent tasks in the prefrontal cortex in [19], for example, reveals how seemingly
separate neuronal groups engaged in their respective tasks may sometimes be drawn into a
coalition, casting votes for a stimulus, and then enacting a committee-like decision.
In line with the Free Energy Principle (FEP) and the idea of Bayesian active inference
[20, 21, 22, 23, 24], individual neurons can be considered cognitive agents that minimize
aggregate uncertainty about their future states (variational free energy or VFE) by actively
exploring and developing predictive models of their local microenvironments. It is the joint
activity of hundreds (in C. elegans [25]) to tens of billions (in primates [26]) of neurons within
this jointly-constructed environment that encodes information sourced from the external
world. Hence an adequate conceptual model of neurons as biological systems must explain
how each neuron’s construction of a predictive model of its local microenvironment results
in the joint encoding of a predictive model of the external environment at the scale of the
entire brain. The utility of FEP based models of cell sorting in morphogenesis [27, 28],
cortical minicolumn and local-circuit organization [29, 30, 31, 32, 33], and network and
whole-brain function [21, 34, 35, 36, 37] suggests that a scale-free conceptual framework is
the right way to approach this question of functional coherence between microenvironment
models constructed by individual neurons and external environment models constructed by
brains. The FEP is, however, fundamentally a statement about error minimization; it is an
implementation-independent principle of thermodynamics [24]. It does not tell us anything
speciﬁc about neuronal functional architecture, or about how neurons interact with either
each other or the non-neuronal components of their microenvironments.
Here we suggest that the idea of a quantum reference frame (QRF) developed within quan-
tum information theory [38, 39] is usefully applied to characterize neurons both architec-
turally and functionally.
While quantum information theory employs the formalism of
standard quantum theory, it makes no scale-dependent assumptions and is applicable from
sub-microscopic to cosmological scales. The formalism of QRFs, in particular, is applicable
at any scale. A QRF is a physical system that assigns units of measurement, and hence
an operational semantics, to each observational outcome, enabling outcomes obtained at
diﬀerent times or places, or with diﬀerent QRFs, to be compared. Macroscopic systems
such as meter sticks, clocks, gyroscopes, and the Earth with its gravitational and magnetic
ﬁelds are canonical examples of QRFs. While reference frames are typically thought of as
fully-speciﬁable abstractions in classical physics, this is not the case in quantum theory:
QRFs are physical systems that encode unmeasurable but functionally relevant quantum
phase information. Every QRF is, therefore, in an important sense unique. A QRF cannot,
3

in particular, be fully speciﬁed either structurally or functionally by any ﬁnite bit string;
it is “nonfungible” in the terminology of [39]. Alice can, in this case, share a QRF with a
distant observer Bob only by physically transferring the QRF to Bob; sending any ﬁnite de-
scription is provably insuﬃcient. Alice can, moreover, transfer her QRF to Bob successfully
only if Bob already possesses a functionally equivalent QRF [40]; Alice cannot, for example,
successfully communicate the meaning of “1 meter” to Bob unless Bob already has a QRF
that eﬀectively functions as a meter stick. From an information processing point of view,
a QRF is a quantum computer: it outputs an outcome value that is reproducible within
some ﬁxed resolution, and that is assigned standardized units that confer an operational
semantics, when given a “raw” measurement, or simply a physical interaction, as input.
We propose, in particular, that neurons are usefully regarded as hierarchies of QRFs. Neu-
rons in this representation are hierarchical measurement devices that sample their sur-
rounding cellular microenvironments at multiple scales and encode scale-speciﬁc expecta-
tions about how those microenvironments will behave. The output of a particular neuron
encodes, for each cell that receives it, a speciﬁc, nonfungible representation of the trans-
mitting neuron’s local measurements. These representations are encoded in the “units of
measurement” deﬁned by the sending neuron’s particular combination of QRFs. They im-
pose operational semantic constraints on all downstream processing. These constraints are
inherited by, and give a particular operational meaning to, the activity patterns of the
neural system as a whole.
We develop this proposal using a general formalism, that of Barwise-Seligman classiﬁers
[41] organized into networks with well-deﬁned limits and colimits (cone-cocone diagrams
or CCCDs) developed in [42] and applied to human cognitive architecture in [43, 44, 45].
Barwise-Seligman classiﬁers are naturally interpreted as representing either logical or prob-
abilistic constraints; hence CCCDs represent maximal collections of mutually-consistent
constraints, and indeed generalize hierarchical Bayesian inference [45]. To model neurons
as hierarchies of QRFs is to represent them as 1) quantum computers that 2) implement
hierarchical Bayesian inferences determined by 3) their speciﬁc three-dimensional (3d) ge-
ometries and 4) the diﬀerential signal-transduction velocities that their geometries impose.
This formalism extends naturally to networks of neurons, at any scale from a local circuit
– e.g. a minicolumn – up to an entire brain.
We begin in §2 by brieﬂy reviewing the implementation of QRFs by generic quantum sys-
tems [46, 47, 48] and their representations by CCCDs. We then turn to biological systems,
reviewing the representation of simple homeostatic setpoints as QRFs [49, 50] and extending
this treatment to perisynaptic processes in neurons in §3. This allows us in §4 to represent
the hierarchical structures of dendrites as hierarchical QRFs that detect the states – activ-
ity patterns – of particular “objects” – spatially-organized aggregates of presynaptic axon
terminals – in their microenvironments. We then extend this representation to neurons
in §5, showing how the integration of signals from multiple dendritic branches implements
an eﬀectively tomographic computation. This view of neurons as state identiﬁers scales
upwards to functional networks, the global activity of which provide coarse-grained repre-
sentations of particular components of the external environment. We then brieﬂy review
4

in §6 results from a variety of systems showing that with suitable spatial and temporal
scaling, this model applies to electrically-excitable cells generally. We consider simulation
modeling and experimental approaches suggested by this framework in §7.
2
QRFs and their representation by CCCDs
2.1
QRFs as generic representations of measurement
Information processing in biological systems has traditionally been represented as classi-
cal; despite theoretical arguments from a variety of perspectives [51, 52, 53, 54, 55] and
experimental evidence for the functional relevance of quantum coherence in photoreception
and magnetoreception [56, 57, 58], quantum biology remains in its infancy [59, 60, 61, 62].
Motivated in part by recent analyses showing that cellular bioenergetic resources fall orders
of magnitude short of those needed for fully-classical computation at macromolecular scales
[63], here we adopt a quantum-theoretic perspective from the start. This perspective allows
full use of the quantum information toolkit, including the concept of a QRF; it oﬀers the
advantages of full generality and applicability at any scale [50].
Consider an isolated, ﬁnite, bipartite system AB for which the interaction HAB is weak
enough and the time interval of interest short enough that considering AB to be separable,
i.e. having a joint state |AB⟩that factors as |AB⟩= |A⟩|B⟩, is a good approximation. In
this case, it is always possible to choose a basis in which the interaction can be written:
HAB = βkkBT k
N
X
i
αk
i M k
i ,
(1)
where k =
A or B, the M k
i are N Hermitian operators with eigenvalues in {−1, 1},
the αk
i ∈[0, 1] are such that PN
i αk
i = 1, and βk ≥ln 2 is an inverse measure of k’s
thermodynamic eﬃciency that depends on the internal dynamics Hk [40, 46, 49]. In the
form given by Eq.
(1), the interaction can, without loss of generality, be regarded as
deﬁned at a holographic screen B separating A from B; the operators M k
i can in this
case be regarded as “preparation” and “measurement” operators that alternately write and
read bit values encoded on B [46, 64]. The screen B can be realized as an ancillary qubit
array as in Fig. 1. The thermodynamic factor βkkBT k in Eq. (1) assures compliance with
Landauer’s Principle [65, 66, 67], i.e. assures that the per-bit free-energy cost of classical
bit erasure is paid on each cycle (see [44, 50] for discussion).
5

Figure 1: A holographic screen B separating systems A and B with an interaction HAB
given by Eq. (1) can be realized by an ancillary array of noninteracting qubits that are
alternately prepared by A (B) and then measured by B (A). There is no requirement that
A and B share preparation and measurement bases, i.e. QRFs. Adapted from [45] Fig. 1,
CC-BY license.
Now consider A to be an “observer” that decomposes B into a “system of interest” S and its
surrounding “environment” E. As a familiar example, S may be some item of laboratory
apparatus and E the surrounding laboratory.
We suppose further that A is primarily
interested in the state |P⟩of some proper component P of S, conventionally called the
“pointer” of S, that is separable from the remaining proper component R (the “reference”
component), i.e. S = PR and |PR⟩= |P⟩|R⟩over the observation times of interest. Under
these conditions, repeated observations of a ﬁxed state |R⟩of R allow the identiﬁcation of S
as a system; in our example, repeated observations of the ﬁxed size, shape, location, etc. of
an item of laboratory apparatus allow its identiﬁcation over time and hence unambiguous
observations of its pointer state [47, 48]. These repeated observations eﬀect decoherence
of S, and hence enforce separability of S from E [46]. Nothing in the above changes when
pure states |P⟩and |R⟩are replaced by densities ρP and ρR, interpreted as distributions
over time series of pure states, with the separability condition ρPR = ρPρR.
Assigning mutually-exclusive subsets of bits encoded on B to P, R, and E is clearly a
computational process that must be implemented by the internal interaction HA of A.
Indeed this process is completely independent of HB or, equivalently, of the decompositional
6

structure of B [47]. Tracking the states of P, R, and E over time requires an adequate
memory resource and a comparison function capable of detecting changes at some suitably
coarse-grained resolution. As shown in [47, 48], these computations can be regarded as
implementing QRFs for P, R, and E, the functions of which can, without loss of generality,
be speciﬁed by networks (formally, cocones) of Barwise-Seligman classiﬁers as described in
§2.3 below.
It is worth emphasizing here that the decomposition B = PRE is not “objective” in any
sense; the components P, R, and E are deﬁned, relative to and hence “for” A, by their re-
spective QRFs. These QRFs can, therefore, themselves be labeled ‘P’, ‘R’, and ‘E’ without
ambiguity. Similarly, the states |P⟩, |R⟩, and |E⟩encoded on B in the basis speciﬁed by Eq.
(1) are not “objective” but rather deﬁned as the domains, respectively, of the QRFs P, R,
and E. They are, therefore, also strictly relative to A (cf. the characterization of quantum
states as personal in [68, 69] or as observer-relative in [70]). The computations implemented
by P, R, and E result in classical, i.e. irreversible encodings of state information to A’s
memory; therefore they require free energy as discussed in [47]. This free energy can only
come from B; hence some fraction of the bits encoded on B must be viewed as supplying
the free energy required for computation. The values of these bits cannot, therefore, aﬀect
the computational outcomes of P, R, and E; they are eﬀectively traced over. This obligate
trace operation renders the outputs of P, R, and E coarse-grained representations ρP, ρR,
and ρE that can be viewed as probability distributions, and eﬀectively as weighted aver-
ages, in the basis speciﬁed by Eq. (1), or as pure states in a “computational basis” with
reduced dimension. The dimension reduction or coarse-graining induced by the action of
any QRF, solely in consequence of its free-energy requirements, enables the construction of
QRF hierarchies with distinguishable layers as discussed in the case of neurons in §4 and
§5 below.
2.2
Commutativity constraints on QRFs
A system S is only useful, in practice, as a measurement apparatus if it can be identiﬁed over
macroscopic time, i.e. has a stable coarse-grained reference state ρR. “Stability” requires,
in particular, that observing the environment E cannot aﬀect ρR; hence the QRFs R and
E must commute. The reference state ρR that allows identiﬁcation of S must, moreover,
remain stable when the pointer component P is observed; hence R and P must commute.
Similarly P must commute with E. These commutativity conditions are, eﬀectively, con-
sequences of separability: they follow directly from the assumption that P, R, and E are
each distinguishable from the others.
These commutativity conditions on the QRFs P, R, and E can be summarized in diagram-
7

matic form by:
ρP
MP
/ ρ′
P
ρR
MR
/ ρ′
R
ρE
ME
/ ρ′
E
|B⟩
P
O
PU / |B⟩′
P
O
|B⟩
R
O
PU / |B⟩′
R
O
|B⟩
E
O
PU / |B⟩′
E
O
(2)
where |B⟩is the bit string encoded on B by the action of HAB, PU is the time propagator
of the joint system U = AB, and MP, MR, and ME are Markov kernels on the state
spaces of ρP, ρR, and ρE, respectively. The boundary B functions as a Markov Blanket
(MB) [71, 72] that renders the system state |A⟩conditionally independent of |B⟩at the
“microscale” deﬁned by HAB [73], while ρP, ρR, and ρE are eﬀective MB states at the coarse-
grained “computational scale” at which A writes observed state information to memory.
The probabilities encoded by the MP, MR, and ME are posterior probabilities for A from
a Bayesian perspective; the task of A as an observer implementing active inference [20, 21,
22, 23, 24] is to learn priors, or equivalently, to implement actions on B, that predict MP,
MR, and ME as accurately as possible.
The operators MP, MR, and ME are, clearly, Markovian if but only if the joint density
ρPRE factors as ρPRE = ρPρRρE, i.e.
if but only if P, R, and E can be assumed to
be separable.
As noted above, separability can be assumed only in the limit of weak
interactions and/or short observation times.
Hence Markovian evolution of P, R, and
E is an assumption valid in this limiting case, not a fact.
Evolution being Markovian
corresponds, by deﬁnition, to probabilities satisfying the Kolmogorov axioms and hence
to the Bell [74] and Leggett-Garg [75] inequalities being satisﬁed, not violated.
Hence
physically, the Markovian evolution corresponds to phase correlations being negligible, i.e.
the absence of quantum entanglement or changes of “intrinsic” measurement context [76,
77, 78] as discussed further below. These conditions can be summarized as observations
being “suﬃciently coarse-grained” at the computational scale to be treated as classical.
The commutativity conditions given by Eq.
(2) are instances of the general condition
required to interpret any physical process as computation [79]. We can, therefore, regard
ρP, ρR, and ρE as encoding a “semantic” representation of the microscale states |P⟩, |R⟩,
and |E⟩encoded on B. It is with respect to this completely-general notion of semantics
that hierarchies of QRFs can be considered semantic, and hence virtual machine (VM)
hierarchies [80], in §4 - 5 below.
2.3
QRFs as constraint networks
In recent work [42, 43, 44, 45] we have drawn extensively upon the semantically enriched
Channel Theory of information ﬂow developed in [41], outlining many examples and appli-
cations (see also in particular [47, 48]). Here we summarize the basic ideas, starting with
8

that of a (Barwise-Seligman) “classiﬁer” (or “classiﬁcation”), which categorically accom-
modates a “context” in terms of its constituent “tokens” in some language and the “types”
to which they belong.
Deﬁnition 1. A classiﬁer A is a triple ⟨Tok(A), Typ(A), |=A⟩where Tok(A) is a set of
“tokens”, Typ(A) is a set of “types”, and |=A is a “classiﬁcation” relation between tokens
and types.
Note that this deﬁnition speciﬁes a classiﬁer/classiﬁcation as an object in the category of
Chu spaces [81, 82, 83] where ‘|=A’ is realizable by a satisfaction relation valued in some
set K having no structure assumed.
Morphisms between classiﬁers are speciﬁed by the following:
Deﬁnition 2. Given two classiﬁers A = ⟨Tok(A), Typ(A), |=A⟩and B = ⟨Tok(B), Typ(B), |=B
⟩, an infomorphism f : A →B is a pair of maps −→f
: Tok(B) →Tok(A) and ←−f
:
Typ(A) →Typ(B) such that ∀b ∈Tok(B) and ∀a ∈Typ(A), −→f (b) |=A a if and only if
b |=B
←−f (a).
This last deﬁnition can be represented schematically as the requirement that the following
diagram commutes:
Typ(A)
−
→
f / Typ(B)
|=B
Tok(A)
|=A
Tok(B)
←
−
f
o
(3)
Following these deﬁnitions, Channel Theory can be represented as a category comprising
classiﬁers as objects and infomorphisms as arrows, which can be seen to be equivalent to
the category comprising Chu spaces as objects and Chu morphisms as arrows [41].
Much of the formulism of [41] revolves around the idea of a distributed system in which the
classiﬁers and infomorphisms between them function as ‘logical gates’ as further exempliﬁed
in [42, 43, 44, 45]. Signiﬁcantly instrumental in this schemata is a ﬁnite, commuting cocone
diagram (CCD) depicting a ﬂow of infomorphisms sending inputs to a core C′ that is the
colimit of the underlying classiﬁers if such exists:
C′
A1
f1
8
g12
/ A2
f2
O
g23
/ . . . Ak
fk
g
(4)
There is a dual construction to this CCD, namely a commuting ﬁnite cone diagram (CD)
of infomorphisms on the same classiﬁers, where all arrows are reversed. In this case the
core of the (dual) channel is the limit of all possible downward-going structure-preserving
9

maps to the classiﬁers Ai. Hence we can deﬁne the central idea of a ﬁnite, commuting
cone-cocone diagram (CCCD) as consisting of both a cone and a cocone on a single ﬁnite
set of classiﬁers Ai linked by infomorphisms as depicted below:
C′
A1
f1
6
g12
g21
/ A2
o
f2
O
g23
g32
/ . . . Ak
o
fk
i
D′
h1
h
h2
O
hk
5
(5)
Signiﬁcantly, the CD functions as a “memory-write system” in Eq. (4); this is used to
represent the time-stamped “write” operations of a QRF in [47, 48]. These diagrams can
be extended to obtain a “bow tie” diagram as below in (6) which represents a coarse-graining
of the semantics of Ai via C′, into a compressed representation A′
i.
A′
1
g′
12
g′
21
/ A′
2
o
g′
23
g′
32
/ . . . A′
j
o
C′
h′
1
h
h′
2
O
h′
j
5
A1
f1
6
g12
g21
/ A2
o
f2
O
g23
g32
/ . . . Ak
o
fk
i
(6)
Diagrams such as (4) (6) can be further extended into hierarchical networks by adding
intermediate layers of classiﬁers and infomorphisms. In this way they resemble artiﬁcial
neural networks (ANNs), and in the “bowtie” form of Diagram (6), they resemble variational
autoencoders (VAEs) [42]. The core C′ in (6) can be viewed as both an “answer” computed
by the fi from inputs to the Ai and, dually, as an “instruction” propagated by the hi (or in
Diagram (6), the h′
i), to drive outputs from the Ai (or in Diagram (6), the A′
i). This kind
of dual input/output is precisely that of a QRF. Crucially, a “bow tie” diagram within the
distributed systems described above, is a descriptive mechanism for broadcasting control
signals to multiple recipients. The Global Neuronal Workspace, as a massively parallel,
distributed system, performs precisely this function for the mammalian central nervous
system [84, 85, 86].
We refer the reader to the concepts of a (regular) theory and local logic as introduced in
[41, Chs 9 12] (and extensively reviewed in [42, 45]) to specify the logical structure of a
given situation. Basically, a local logic is a classiﬁer with a theory, along with a subset of
tokens as speciﬁed by a sequent; namely, a classiﬁcation M |=A N of a classiﬁer given by
a pair of subsets M, N of Typ(A), such that ∀x ∈Tok(A), x |=A M ⇒x |=A N. In fact,
any classiﬁer generates its own natural local logic. Below, we will adopt the sequent as a
‘conditional’ when deﬁning probabilities.
10

2.4
CCCDs implement Bayesian inferences in deﬁned contexts
In general, given an information ﬂow channel:
−→Aα−1 −→Aα −→Aα+1 −→· · ·
(7)
the semantic content can be extended by postulating local logics Lα = L(Aα) generated by
the corresponding classiﬁers Aα (assumed, in principle, to be in relationship to a (regular)
theory associated to the individual Aα, as speciﬁed in [41, Ch 9] and reviewed in [45,
Appendix A]), thus postulating a ﬂow of logic infomorphisms:
· · · −→Lα−1 −→Lα −→Lα+1 −→· · ·
(8)
which may then comprise a CCD as in Eq. (4).
A probabilistic interpretation of information ﬂow in Eq.
(8) results when the sequent
relation is weakened to require only that if x |=A M, there is some probability P(N|M)
that x |=A N. 1 In eﬀect, |=A becomes a conditional probability that inherits the semantics
of the local logic [87] (cf. [88]); that is, given a sequent in a classiﬁer A, along with its
satisfaction relation satisfying:
M |=A N ,
∀x(x |=A M ⇒x |=A N),
(9)
we can deﬁne the conditional probability as:
M |=P
A N := P(M|N).
(10)
In fundamental Bayesian terms, M above can be regarded as an unobserved (or as be-
low, unobservable) event, and N an observable datum, in which case P(M) becomes the
prior, and P(N) the evidence, that together generate a prediction. Given the likelihood
P(N|M) as the conditional obtained from weakening the sequent, Bayes’ theorem speciﬁes
this conditional as the posterior:
P(M|N) = P(N|M)P(M)
P(N)
.
(11)
We introduce the idea of “contexts” of observation, following [45, §7.1] (see also [48, Ap-
pendix A.1]), by considering the following sets: i) X is a set of events in a very general sense
(e.g. a set of observed value combinations or atomic events); ii) Y is a set of conditions
(specifying objects/contents or inﬂuences; iii) Z is a set of contexts (e.g. detectors, mea-
surements, or methods); iv) W := Y ×Z, where X, Y and Z are taken to be subsets of some
(very) large probability space. Note that here Y can be decomposed as Y = Y + ∪Y −(dis-
joint union) where Y + consists of observed objects/contents/characteristics of the context
1We use the upper case ‘P’ to denote a probability in relationship to classiﬁers, and a lower case ‘p’ for
that pertaining to e.g. events or states as below.
11

Y , and Y −consists of what is not observed about Y . Hence W := Y ×Z = (Y + ∪Y −)×Z.
The classiﬁer in question is then A = ⟨X, W, |=A⟩and represents an observable in context.
Among several possible interpretations for the classiﬁcation relation ‘|=A’ is valuation by
the conditional probability p(a|x) = p(a|{b, c}), whenever deﬁned, for a ∈X, b ∈Y and
c ∈Z [45] (see also below).
As a brief example, consider arbitrary classiﬁers A(a)
1 , . . . , A(e)
5
in some part of an infor-
mation channel where the classiﬁers correspond to events a, b, c, d, e, respectively, together
with logic infomorphisms f13, . . . , f45 between them, and where the sequents are relaxed to
become conditional probabilities as above:
A(a)
1
f13
}
f14
!
A(b)
2
f24
}
A(c)
3
A(d)
4
f45

A(e)
5
(12)
Following e.g. [89], this particular channel then generates a joint probability distribution
given by:
p(abcde) = p(a)p(b)p(c|a)p(d|ab)p(e|d).
(13)
As developed in detail in [45], the above constructions provide a formal basis for classical
Bayesian inference. Speciﬁcally, the diagrams (4) and (5) depict channels of logic infomor-
phisms, when on relaxing the sequent in each case, the classiﬁcations |=Aα are now explicitly
realized as conditional probabilities pα(·|·), whenever these are deﬁned. Putting the above
details concerning conditionals into the diagrammatic framework reveals a typical portion
of a CCD computing a hierarchical Bayesian inference from a set of (posterior) observations
Ai to an outcome C′, as having the form:
C′
A1
p10(·|·)
8
p12(·|·)
/ A2
p20(·|·)
O
p23(·|·)
/ . . . Ak
pk0(·|·)
g
(14)
The requirement that diagrams (4) and (14) commute amounts to the requirement that
branching “upward” to an overall logic L along any one of the fα, is equivalent to following
the inferential sequence (8) up to its termination, and then following the last of the fα to
L. Thus the fα can be seen as shortcuts to reaching L: “insights” that allow the rest of
the inferences in (8) to be bypassed. The local logic L can be seen as the “answer” to the
problem (8) addresses: formally, it is the logic that solves the problem in one step. The
probability that the answer is “right” is the product of the probabilities along (8). The
12

commutativity requirement on (14) requires, in addition, that this overall probability is
conserved; hence the probability associated with each “insight” fα must be the combined
probability of the inferential steps it replaces.
Commutativity of a CCD thus enforces inferential coherence, and the same clearly applies
when the dual objects and maps of a CD are combined in constructing a CCCD as in
diagram (5) to implement a recurrent Bayesian network (see also [43] where CCCDs are
employed to model the interaction between bottom-up and top-down processing in visual
object categorization).
In contrast, non-commutativity of (4) implies that there is no
consistently deﬁnable (conditional) probability distribution across the system in question,
and characterises intrinsic contextuality [76, 77, 78] for that system [45, Th 7.1 and Coroll
7.1].
2.5
QRFs as memory resources
A QRF is only useful operationally, and hence only meaningful as a reference frame, if it
can be deployed consistently over multiple episodes of observation. A meter stick or a volt-
meter is, clearly, useful only if yields consistent measurements; the notion of a “standard”
to which a device can be periodically recalibrated captures this consistency requirement
operationally.
Recalling that a QRF is by deﬁnition a physical system, not merely an abstraction [39],
the above condition can be stated as a requirement that a QRF have stable dynamics
over the relevant time period, or if it is a subsystem of a larger system A, that it is an
(or a component of an) attractor of the dynamics of A, i.e. of the self-interaction HA.
In computational language, a QRF must be a memory structure, one that is active (and
hence executed as a computation) at all times, or one that is executable on demand via
some higher-level control structure. We can interpret a CCD identifying S = RP that is
implemented by A as encoding an expectation, and hence a prior probability for A, that
bit patterns specifying S will be encoded on the holographic screen B separating A from
its complement B. As emphasized earlier, neither the existence of this expectation not its
satisfaction in practice imply that S exists in any “ontic” sense as an observer-independent
component of B (cf. [90]).
Operational utility and hence meaningfulness also require that a QRF write its outcomes to
a classical memory in a way that allows comparison of outcomes obtained at diﬀerent times.
Following [47, 48], we consider the transition from writing an observed pointer outcome Pi
with timestamp τi to writing Pk with timestamp τj to be implemented by an operator Gij,
the complete set of which forms a groupoid under composition. “Timestamping” here can be
considered merely to be imposition of an order on the recorded data; a separate, observation
independent clock is not assumed (cf. [50] for a general discussion of time QRFs). As
compliance with Eq. (1) requires all classical data to be written on the holographic screen
B, the memory-write CD must write on B as shown in Fig.
2.
Comparing previous
with current data, in this case, requires reversing the arrows of the relevant memory-write
13

CD to reconstruct the previously-written data. The comparison itself becomes a “meta”
operation, i.e. one deﬁned at the next-higher hierarchical level as described in the case of
neurons below (cf. [43, 44, 45] for examples from human cognitive processing).
Figure 2: A CD Wkj (green triangle) speciﬁes a memory-write operation of the time-
stamped state (ρRPk, τj) to B. The timestamp τj is generated by the groupoid action Gij
on the previous (at τi) output from the CCD PR. Adapted from [48] Fig. 9, CC-BY license.
We can, therefore, consider QRFs to provide two distinct memory resources: 1) the stable
QRF itself as an executable computation, and 2) the ordered sequence of classical data
accumulated by deploying the QRF to make measurements. The executable QRF is, in
general, a “quantum” memory, as it is a quantum computation implemented by the operator
HA. The classical data, in contrast, are written on B and subject to both space and free-
energy constraints. As discussed in [50], these latter memories are eﬀectively stigmergic.
Any representation of the “self” that is readable as a classical memory, in particular, must
be written on B, and read from B when recalled [50]. These distinct memory resources
support distinct expectations: QRFs as executable computations encode expectations about
how the “world” faced by an agent is (or more properly, usefully can be) decomposed into
identiﬁable “systems” while the classical data encode expectations about the states in
which these systems will be encountered. These expectations can be represented as Markov
kernels that are learned incrementally; the diﬀerence between these “expected” kernels and
the “true” kernels given by Diagram (2) provides a representation of VFE to be minimized
14

by active inference, i.e. by exploration (novelty seeking that enlarges the “training set”) and
further learning. This deﬁnition of VFE for generic quantum systems allows a formulation
of the FEP [20, 21, 22, 23, 24] for such systems that is developed in detail elsewhere [91].
3
QRFs at the macromolecular scale
We now turn from generalities to the speciﬁc case of mammalian cortical neurons, and
show how these can be conceptualized as hierarchies of QRFs, i.e. hierarchies of physically-
implemented computations, each of which can be regarded as a measurement, at a deﬁned
scale and with respect to deﬁned units, of some aspect of its local environment. We then
generalize this treatment to arbitrary cells in §6.
3.1
QRFs as homeostatic setpoints
As discussed in [49, 50], the simplest biological QRFs are switches that induce opposing
behaviors whenever some parameter value is above or below some threshold value. The
CheY system regulating bacterial chemotaxis provides a familiar example: local concen-
trations of the phosphorylated form CheY-P above or below a essentially ﬁxed (at the
relevant timescales) default concentration induce swimming or tumbling behavior, respec-
tively [92, 93]. From a computational perspective, the CheY system implements a switch
between depth-ﬁrst and breadth-ﬁrst search, and hence a generic mechanism for avoiding lo-
cal maxima in a search space. More biologically, it implements both approach to resources
and avoidance of toxins and other irritants, and hence serves as a homeostatic setpoint.
Such setpoints are, from the present perspective, expectations consistent with continuing
structural and functional integrity, i.e. continuing separability from the surrounding world.
They can, therefore, be expected to arise in any system that maintains separability, i.e.
any system to which Eq. (1) applies.
A second familiar QRF is the default or “resting” membrane voltage V 0
mem across any local
patch of biological membrane. Fluctuations above or below this setpoint induce actions, e.g.
opening or closing voltage-gated ion channels. This QRF is obviously relevant to neuron
function as discussed below; at larger scales, it becomes a critical regulator of multicellular
morphology; see e.g. [94, 95, 96] and further discussion in §6.
Guided by these examples, it is straightforward to consider any system implementing linear
threshhold or sigmoid kinetics as a QRF that switches between “negative” and “positive”
responses as its input ﬂuctuates below or above a default value (Fig. 3). With the ad-
vent of two-component signaling pathways, early enough in prokaryotic evolution that they
appear in all analyzed lineages [97], the default value becomes adjustable, i.e. context-
dependent via a classical “direct inﬂuence” mechanism [78] (but see [98] for evidence that
lactose-glucose interference in E. coli [99] exhibits nonclassical contextuality). The multi-
component systems typical of eukaryotes, e.g. the Wnt [100], ERK [101], notch [102], BMP
15

[103] pathways and the interactions between them, introduce further, multifactorial context
dependence (see e.g. [104, 105] for general reviews). Such systems embody expectations
about the relationships between multiple, simultaneously-measured values, each eﬀectively
calibrated to an underlying standard, e.g. a molecular concentration or local value of Vmem.
Figure 3: a) A generic system executing sigmoid kinetics. b) Reﬂecting the response curve
at its inﬂection point redescribes the system as a QRF that switches behavior at a default
value.
As noted earlier, cellular-scale biochemical and bioelectric signaling pathways have tradi-
tionally been conceptualized and depicted as fully classical, with each component occupying
some determinate state at all relevant times. Cellular energy budgets cannot, however, sup-
port the thermodynamic cost of classicality, even at ms timescales, indicating that quantum
coherence and hence quantum computation may be signiﬁcant up to timescales of seconds
[63]. Classical data can, in this case, only be encoded on intra- or intercellular boundaries
as illustrated schematically in Fig. 1. “Chunking” the cell into functional components that
process and exchange classical information or serve as classical memory structures (e.g. the
genome, proteome, transcriptome, and “architectome” [106]) is, eﬀectively, identifying the
boundaries at which classical information is encoded, i.e. the boundaries that function as
MBs. A signal transduction pathway that measures some environmental parameter at the
cell membrane and transfers a context-dependent representation of this information to the
genome is, in this picture, a single quantum computation, a QRF that detects, calibrates,
and reports an observational outcome. It can be treated as a black box characterized by
inputs and outputs, including free energy inputs and dissipated heat outputs. We adopt
this approach to characterize the functional architecture of neurons in what follows.
3.2
The postsynaptic complex as a QRF
As discussed in [107, 108, 109], the MB of a single neuron includes the pre- and postsynaptic
membranes that mediate interactions with other neurons, as well as the overall cell mem-
16

brane that mediates interactions with the neuron’s local microenvironment. Single neurons
are, however, large, spatially-extended, computationally-complex structures (Fig. 4); it is
the need for improved models of such structures that motivates this paper. Hence we will
consider how the neuron’s MB is constructed from those of its components, identifying in
this process sites at which intermediate steps in neuronal computations may be classically
encoded.
Figure 4: Cartoon structure of a mammalian cortical neuron; cf. [110, 111] for anatom-
ical details. a) Voltage gated channels and pumps are the primary input (sensory) and
output (active) MB components; b) Specialized postsynaptic structures, e.g. on dendritic
spines, collect incoming information from other neurons; c) dendritic subtrees integrate and
process this incoming information; d) dendritic information is integrated at the soma and
distributed to outgoing axonal channels. Postsynaptic receptors may also appear on the
soma or on axons; this level of detail is neglected here.
Starting from the input side, the smallest cellular-scale component is the specialized post-
synaptic area, typically located on a spine morphologically. Its computational role is to
contribute a positive (excitatory) or negative (inhibitory) time-dependent Vmem gradient to
the overall electrical activity of its local dendritic branch. Performing this function requires
17

energetic and molecular ﬂows in addition to ∆Vmem(t), as illustrated in Fig. 5. At this level
of abstraction, the presynaptic specialization at the axonal bouton is essentially equivalent,
up to reversing the ﬂow of small molecules from the cell membrane.
Figure 5: Cartoon structure of a mammalian postsynaptic specialization; cf [4] for anatom-
ical and functional details.
Primary energy (red), molecular (blue) and Vmem gradient
(orange) ﬂows are shown. Considered as a system, the spine’s MB includes the spine mem-
brane (solid line) and the cytoplasmic/cytoskeletal interface between the spine and the bulk
dendritic cytoplasm (dashed line).
Making the reasonable assumption of balanced mass ﬂows, i.e. assuming that the volume
and density of the postsynaptic specialization, or in quantum terms, its Hilbert-space di-
mension remains constant, we can focus on information ﬂows only, and treat the interaction
between the postsynaptic specialization and its surroundings using Eq. (1). The bound-
ary B in this case has two components, B = CD, where C separates the postsynaptic
specialization from the external microenvironment and D separates it from the bulk den-
dritic cytoplasm. Both components support both inward sensory ﬂows and outward active
ﬂows. As these information ﬂows are both physically implemented and referenced to de-
fault homeostatic/allostatic setpoints, they can be considered to be implemented by QRFs.
Hence they can be represented as input – output or perception – action ﬂows as in Fig. 6.
In this representation, the boundary B is decomposed functionally into B = IO, where I
and O are “input” and “output” components respectively, each of which includes regions of
18

both anatomical components C and D. While each ﬂow Q has its own characteristic time
constant τ Q as in Fig. 2, the cross-modulatory couplings between pathways, and hence mu-
tual dependence between setpoints, requires that they be mutually coherent. Hence we can
represent the postsynaptic specialization as executing a single QRF with a vector output
and a time constant τ PS = maxQτ Q. Presynaptic specializations can be treated similarly.
Figure 6: Information ﬂows abstracted to QRFs (X, Y , Z; colored triangles) that read from
an input boundary component I (right hand blue ellipse) and write to a output boundary
component O (left hand blue ellipse), where the overall system boundary B = IO. Each
QRF Q has an associated time constant τ Q and a groupoid-structured set of operators GQ
ij
as depicted in Fig. 2.
As discussed in §2 above, all classical information in the system must be encoded on B.
Hence Fig. 6 depicts the ideal case in which the boundary component O encodes all out-
put information, i.e. the computations implemented by the QRFs X, Y , and Z have no
classically-encoded intermediate states. In this ideal case, the free-energy cost of compu-
tation per unit time is given by the output bandwidth in bits, i.e. the area in bits of O.
Assuming a minimal thermal energy ∆Eth = ln2kBT, the minimum dissipation timescale
is given by the time-energy uncertainty relation [112] as ∆tdiss ≥πℏ/(2∆Eth) ≈50 fs.
Neuronal energy budgets, however, are insuﬃcient for classical encoding at this timescale;
assuming 30,000 synapses per cortical neuron [111] and a maximum power consumption
of 250 Gbits/sec (where as a power unit 1 bit =def ln2kBT ≈3 · 10−21 J at T = 310 K)
[26], each synapse could encode at most 8.3 Mbits/sec if the cell’s metabolic resources were
devoted entirely to synaptic encoding. In fact the bulk of neuron energy usage is devoted to
19

action potential (AP) generation and transmission, reducing the coding capacity of synapses
by up to two orders of magnitude (see [63, 113, 114, 115] for further discussion of neuronal
classical encoding costs and capacity). We can expect, therefore, that individual postsynap-
tic specializations can realistically encode at most about 100 kbits/sec, or 100 bits per unit
time at the ms scales of dendritic postsynaptic potentials. Hence the idealization of Fig. 6
is not unrealistic from an energetic perspective, suggesting that consistent with the general
considerations in [63], QRFs implemented at the scale of cellular compartments perform
essentially pure quantum computations, i.e. employ quantum coherence as a short-term
memory resource.
The form of Fig. 6 suggests a cobordism with I and O as boundaries. If the QRFs X,
Y , and Z perform pure quantum computation without classical intermediate steps, we
can treat them in a path-integral formalism [116], and hence as deﬁning a manifold of
mappings from I to O. This suggests that information processing in neurons is amenable
to a treatment using topological quantum ﬁeld theory [117] and the emerging theory of
topological quantum neural networks [118]. We defer this possibility to future work.
4
Dendrites as spatially-organized QRFs
Dendritic branches are traditionally viewed as lossy but otherwise passive integrators of rel-
atively slow (10s of ms) postsynaptic potentials (PSPs), though faster spike-like activations
and backwards propagation of APs are also possible [110]; see [119] for detailed dendritic
spike models.
This passive view of dendritic activity renders the execution of complex
logical operations like XOR [11] surprising and activity-dependent remodeling [2, 3, 4] mys-
terious. It decouples the processes required to maintain a successful synapse: learning is
largely localized to the postsynaptic side, while target search and synapse formation are lo-
calized to the presynaptic side. Finally, it decouples dendritic function from the dendrite’s
trophic requirements. Branches that transmit only low-amplitude noise signals cannot, on
the passive model, be regarded as less successful or less worthy of continued metabolic
maintenance in comparison to those branches that consistently transmit high-amplitude,
highly informative signals.
Dendritic branches satisfy the physical requirements for implementing QRFs and hence
engaging in active inference. Viewing dendrites in this way raises two immediate questions:
1. What is a dendritic branch measuring?
2. What is the VFE function that a dendritic branch is minimizing?
However, while dendrites can be described as coincidence and hence activity-correlation
detectors [110] even on the passive view, the second of these questions is diﬃcult to even
formulate from a passive perspective. A partial answer to both questions is discussed in
[29, 30]: basically, dendrites can self-organize to minimize the VFE on surprise of their
20

presynaptic inputs, showing that postsynaptic gain is itself optimized with respect to VFE.
Extending the modeling approach of the previous section to dendritic branches suggests an-
swers to these questions that can be summarized by the hypothesis that dendritic branches
identify “objects” with relatively small numbers of states, and execute approximately binary
logic on the measured state vectors.
4.1
Colocating consistently correlated synapses minimizes VFE
While both dendritic arborization and spine density vary with neuron type and location
[120, 121, 122, 123], both cortical and hippocampal pyramidal cells can exhibit dendritic
branches with on the order of 1,000 spines and full dendritic trees with up to 30,000 spines
[111]. Consider such a branch, neglecting information and resource ﬂows through the non-
spine membrane to focus on ﬂows to and from the spines as modeled in Fig.
5.
As
the magnitudes of resource ﬂows correlate with activity and hence with the magnitude of
∆Vmem, we can further focus on the latter. In this case, we can think of the branch as
“observing” on the order of 1,000 points of active signaling, analogous to 1,000 points of
light in a visual ﬁeld.
There are clearly two limiting cases for the activity observed by a branch. If every spine
receives input from a distinct, independently-activated neuron, the correlation Cij between
inputs i and j can be expected to be small, with Cij →0 in the limit. If all spines receive
input from a single neuron, Cij →1 in the limit. Assuming roughly constant frequencies
(though random phases) of presynaptic activity, the former limit yields noise with an input-
frequency dependent amplitude as an output PSP from the branch; the latter yields an
essentially digital signal encoding the activity of the single input neuron. An essentially
constant noise signal is eﬀectively a “dark room” from a VFE perspective, while a variable
digital signal poses a well-deﬁned, potentially high-information prediction problem.
Microconnectome methods [124] allow, in principle, counting the numbers of presynaptic
(i.e. incoming) and postsynaptic (outgoing) partners for any neuron. Measured numbers
of presynaptic partners range up to 50 in C. elegans [25] and from tens to several hundred
in mammalian cortical cells [125, 126] to thousands for cerebellar Purkinje cells [124]. If
as estimated [127] neocortical pyramidal cells typically receive 4 or 5 synaptic connections
from each presynaptic partner, these cells may also have several thousand presynaptic
partners. Such cells would be operating very close to the noise limit of Cij →0 unless
the activity of the presynaptic partners is already highly correlated. Perhaps signiﬁcantly,
most microscale connectome mapping has been achieved in sensory cortices in which highly
correlated incoming information can be expected.
Following [110] and considering each dendritic branch to be a coincidence detector, the
fundamental problem faced by a branch is distinguishing between i) correlated signals from
a given presynaptic partner, ii) “true” correlates from multiple partners, and iii) “ran-
dom” correlates. We can treat this uncertainty as a VFE function, and ask how it can
be minimized. For a relatively short, distal, terminal branch exhibiting all three kinds of
21

correlates, the answer is shown in Fig. 7: localizing partially-correlated regular inputs to
distinct, non-branched branches and moving random inputs as far as possible from branch
junctions disambiguates signals from diﬀerent synaptic partners, allows “true” correlations
to be identiﬁed speciﬁcally at branch junctions, and reduces the amplitude of noise signals.
Figure 7: a) Least and b) most eﬃcient arrangements of synapses with partially-correlated
(red and blue) and random (green) activity on a two-sided symmetrical, terminal dendritic
branch.
By minimizing the ambiguity of correlation, arrangement b) allows maximally
digital processing at the branch junction.
A terminal branch in an initial state similar to Fig. 7a has synapse-level learning and spine
remodeling available as tools for moving toward a state similar to Fig. 7b, with trophic re-
ward as the imposed quality function. This is active inference, with spine remodeling as the
“action” that alters environmental input, i.e. synaptic activity, independently of synapse-
level priors. Success (given enough time) can be expected if trophic reward increases with
the overall informativeness of the branch’s activity, i.e. if trophic reward to dendrites as
functional units has the same activity-dependence as trophic reward to neurons as func-
tional units [1, 128, 129], as studies of dendritic remodeling already suggest [3, 4]. This
is mechanistically reasonable, as it transfers both the task of determining informativeness
and that of adjusting trophic reward to the proximal part of the dendritic tree, and hence
eﬀectively to the soma itself. Both are, from the branch’s perspective, outside of its MB
and hence part of the environment.
We can, therefore, state the following:
Prediction: Trophic reward to dendritic branches correlates with informative-
ness of the signal from the branch for more proximal dendritic branches, with
informativeness to the soma as a limit.
What the soma needs to know is whether to ﬁre an AP. A high-amplitude signal on a
low-amplitude background, i.e. an eﬀectively digital PSP, provides this information most
22

eﬃciently. Hence we can predict that dendritic trees will remodel in the direction of maxi-
mizing digital processing.
4.2
Branches as object detectors
A branch that has segregated its non-noise inputs into diﬀerent compartments as in Fig.
7b is, eﬀectively, an object detector. The object it detects is a presynaptic partner, or a
spatially-localized collection of axonal processes from a presynaptic partner. It measures
the detectable state of this object, its binned-average activity at some binning timescale,
typically a few ms. Hence we can think of it on the model of Eq. (2) or Fig. 2, with the
input correlation as the reference R, the binned-average activity as the pointer P, and any
background noise as the environment E. Hence a compartmentalized branch is a canonical
collection of QRFs.
From this perspective, we can see spine remodeling within dendritic branches as a process of
assembling QRFs that identify objects and hence enable measurements of their states. Spine
remodeling is thus an instance of the second, relatively neglected, form of active inference:
the minimization of uncertainty about the decomposition of an agent’s world. A branch
with maximally-ambiguous inputs as in Fig. 7a decomposes its world only into “points of
light”; a branch that has remodeling to approach Fig. 7b sees instead two objects with
well-deﬁned states that can be compared in both amplitude and time dimensions. Spine
remodeling is, as are the processes of object segregation and object persistence during infant
development, learning what to see, and hence learning what coherent things populate the
perceived world.
5
Neurons as measurement devices
5.1
Dendritic geometry imposes salience
As the “cut” and hence the boundary imposed to render a branch “terminal” is moved
toward the soma, the branch accumulates junctions separating sub-branches, and object-
identity information is replaced by coincidence information. At each such sub-branch junc-
tion, the post-junction sector of the dendrite is faced with the task of reducing VFE by
distinguishing “true” from “random” coincidences.
Learning and remodeling driven by
trophic reward remain the tools available for amplifying the former at the expense of the
latter, thus increasing the signal-to-noise ratio and rendering signal processing eﬀectively
more digital. Hence one can expect neurons to remodel in the direction of segregating inputs
to their dendritic trees in a way that maximizes true coincidences at branch junctions. To
the extent that they do this, they increasingly operate as hierarchies of spatially-segregated
QRFs
Dendritic geometry gives proximal synapses more inﬂuence over somatic activity than distal
ones; the gating function of proximal inhibitory synapses, in particular, is well known [110].
23

Distal signals can only be processed with high (Bayesian) precision in periods of proximal
silence. Hence it is natural to think of dendritic geometry as imposing a salience gradient
from proximal to distal synapses. Stereotypical diﬀerences in arborization pattern between
pyramidal cells from diﬀerent cortical or hippocampal layers reﬂect diﬀerent salience as-
signments, with cortical cells typically giving distal inputs, and hence distal presynaptic
partners, higher salience than do hippocampal cells [110]; see [130, 131, 132] for speciﬁc
diﬀerences between cortical cells from diﬀerent layers. Higher distal salience is only useful
from an information processing perspective if it is low-noise, i.e. if PSPs from distal den-
dritic branches, e.g. from the elaborate apical tufts of Layer V cortical pyramidal cells, are
(primarily) time-convolutions of true coincidence signals. As remodeling takes time, one
can expect neurons that compute the same function over long periods, i.e. that are required
to exhibit only slow learning, to assign the greatest salience to distal inputs.
In the spiking neurons of interest here, somatic signal integration results, given suﬃcient
amplitude, in AP generation. While APs are relatively high-amplitude, low-noise signals,
outside of myelinated axonal segments diﬀerential AP degradation can be expected to
contribute to diﬀerential presynaptic signal strength. Weak presynaptic signals are weak
actions on the signaling neuron’s environment, and cannot be expected to yield strong
conﬁrmatory signals in return.
Hence relatively weak synapses can be expected to be
clustered, consistent with Fig. 7b.
5.2
Neurons as tomographic computers
As discussed in §2, a QRF corresponds to a speciﬁc choice of basis for a subset of the
operators M A
i deployed by an agent A to measure the state of its MB, or boundary B. We
can, therefore, think of the hierarchy of QRFs implemented by a neuron’s dendritic tree as
a hierarchy of basis choices, with the “low-level” basis components corresponding to detec-
tions of correlated inputs from local clusters of synapses from single presynaptic partners
and the “high-level” basis components corresponding to coincidences between increasingly
more highly “chunked” aggregates of input signals. Each neuron can thus be thought of
as deploying a speciﬁc hierarchy of basis vectors to measure the presynaptic activity of its
environment.
Consider now a set N of neurons sampling the “same” environment, e.g. a set of Purkinje
cells each sampling the axonal outputs of a set of cerebellar granule cells.
The causal
antecedents of the activity of the input neurons are clearly “unknowns” from the perspective
of the neurons in N; the input activities can, therefore, be considered from this perspective
an ensemble E of samples from an unknown, time-varying state. We can, in this case,
think of N as making a set of measurements, each in a distinct basis, of the ensemble E of
time-varying states.
If we think of the states in E as quantum states, then the set N of neurons can be regarded
as performing quantum state tomography on E, and hence quantum process tomography on
the process generating time variation in E [133]. Quantum state tomography generalizes
24

classical tomography by generalizing the choice of basis away from “slices” of a three-
dimensional geometry in which the system of interest is embedded.
It is expensive in
terms of basis dimensionality, requiring d2 basis vectors for quantum states of (binary)
dimension d, and ∼d4 basis vectors for processes on states of dimension d. This dimensional
cost translates well to the case of ensembles of neural activity, since as in the case of
quantum states, it is phase correlation information that the tomographic measurement
aims to extract.
Viewing neural computation as tomographic provides an immediate explanation for the
enormously high input and output bandwidths and apparent redundancy of neuronal ar-
chitectures, which are diﬃcult to understand on the basis of simple logic-circuit models
[127, 134]. An input space with 100-bit states (d = 100) would require ∼1004 = 108 binary
dimensions for process tomography, or roughly 105 neurons at 1,000 distinct presynaptic
partners per neuron. Following [135] and assuming ∼108 minicolumns with 100 neurons
each in human neocortex, analyzing a 100-bit state would require a minimum of ∼103
minicolumns or 0.001% of neocortical capacity. We can infer from this that tomographic
computation is approximate, with substantial reduction of the input dimension by salience
systems, e.g. active attention.
5.3
Scaling upwards: minicolumns to functional networks
The stereotypically layered, only sparsely interconnected minicolumns of mammalian cor-
tex, and in particular, human neocortex, are widely viewed as computational as well as
neuroanatomical units, with modeling studies increasingly suggesting that minicolumns ex-
ecute Bayesian predictive coding [30, 107, 109, 123]. Extending the model outlined above,
we can think of minicolumns as functionally analogous to neurons, gathering input from and
sending output to other minicolumns. As does a neuron, a minicolumn “sees” a spatially-
distributed collection of (positive or negative) excitations, spatial and temporal correlations
between which are informative to the extent that they are non-random. Hence a minicol-
umn is faced with a coarse-grained version of the VFE minimization problem faced by
neuron: that of disambiguating “true” coincidences from random ones. Minicolumns can
be expected to functionally remodel to increase signal-to-noise ratio for the same reasons
neurons can, with trophic reward from the environment as the selective criterion.
Functional networks spanning multiple cortical regions, e.g.
sensory pathways, impose
a higher-level hierarchical organization.
Predictive coding across layers of this higher-
level hierarchy have been analyzed in terms of typical connections between minicolumns in
adjacent layers; see e.g. [109, Fig. 4] or [123, Fig. 2] for interlayer connection maps. Top-
down connections encoding likelihood at pathway layer i arise mostly from minimcolumn
Layer V pyramidal cells and target minicolumn Layer III cells at pathway layer i −1.
Reciprocally, bottom-up connections arise mainly in minicolumn Layer II at pathway layer
i, and project to networks of Layer IV cells at pathway layer i + 1. Hence at any level
of the pathway, empirical priors are localized in minicolumn Layer III, and likelihoods (or
predictions) in minicolumn Layer V. This is further analyzed in [107] in terms of how MBs
25

inﬂuence connectivity of microcircuits. Internal and external states are implemented by
spiny stellate cells and interneurons of each minicolumn; via connections from and to these,
respectively, superﬁcial pyramidal cells of the next minicolumn become the (blanket) active
states, while the deep pyramidal cells of the previous minicolumn become the (blanket)
sensory states. Eﬀectively, the minicolumns are the functional units comprising an MB of
networks (either seen as e.g. a MB of MBs, or a Matryoshka of MBs). Once the MBs
are functional, the overriding principle is that neurons, microcolumns and networks appear
to dynamically self-organize thanks to the FEP. There are many interesting outcomes:
consider for instance the visual network as composed of internal states inﬂuencing the
dorsal and ventral attention networks, while the default-mode network plays the role of a
sensory system mediating the inﬂuence between these former and external, sensorimotor
states. Just as in the case of lower-level structures, such high-level, multi-layer processing
pathways exhibit activity-dependent trophic reward and remodel as necessary to achieve it,
as shown e.g. by studies of large-scale pathway remodeling following injuries that remove
expected inputs [136] (cf. [109, 137]).
Since Friston’s proposal that the mammalian brain is an active-inference device [20], numer-
ous studies have explored the implementation of active inference by large-scale networks,
up to and including the global neuronal workspace [44, 45]. What we have shown here
is that these principles extend downward to the scale of the individual synapse. Trophic
rewards select for informative activity, i.e. high signal-to-noise ratios, at every scale. We
can expect these considerations to generalize from neurons to non-neural cells, and from
neural communication to communications between biological structures at every scale.
6
Generalizing neural computation to cellular compu-
tation
The prior discussion was framed in the context of neurons; however, it becomes more
generally applicable when we note that neurons did not appear de novo but in fact evolved
slowly from other cell types which already shared many of their features (reviewed in
[16]). Not only are the basic molecular components of neurons (ion channels, electrical
synapses, neurotransmitter machinery) already present in most cells including unicellulars,
but all cells produce bioelectric gradients and comprise tissues with propagating changes
in resting potential [95]. It has been suggested that developmental bioelectricity is the
evolutionary precursor to neural dynamics, and indeed that evolution speed-optimized the
slow bioelectrical signaling that was ﬁrst used to solve problems in morphospace (exerting
anatomical control over body shape by regulating cell behaviors) before it was exapted to
solve problems in 3-dimensional behavior space (by regulating muscle function) [138, 139].
Consistent with this hypothesis, bioelectric signaling in non-neural cells has been implicated
in control of morphogenetic decisions in embryogenesis, regeneration, and cancer [140, 141,
142]. Recent work targeting the native ion channels and gap junctions in tissue has shown
that bioelectric states can be readily modulated to predictably alter organ identity, induce
26

regeneration of limbs, control the axial patterning of whole bodies, and repair complex
structures such as craniofacial birth defects [143]. Indeed, it has been suggested that parallel
to the architecture of brains, non-neural bioelectricity is the medium implementing the
information processing that enables collective intelligence of cellular swarms during body
construction and remodeling [144].
Morphological change is a deeply computational process that relies on calibrated measure-
ment, and hence QRFs, at multiple organizational levels. While the DNA determines the
micro-level hardware that each cell gets to have (e.g., its complement of ion channels),
genomes do not directly encode morphology. Instead, they specify machines that have to
deploy considerable intelligence, using William James’ deﬁnition of intelligence as the abil-
ity to reach the same outcome from diﬀerent starting conditions and despite perturbations.
The physiology of cellular collectives implements very robust problem-solving capacities
by making important decisions about collective macrostates that are not deﬁned at the
level of individual cells. For example, mammalian embryos cut in half do not result in two
half-embryos – the system regulates to make complete, normal bodies of monozygotic twins
(regulative development). Tadpoles that are developmentally disrupted to have all of their
craniofacial organs in the wrong positions still result in normal frogs after metamorphosis
because the eyes, nostrils, mouth, etc. move through novel paths to get to the same invari-
ant outcome – a correct frog face [145, 146, 147]. Salamanders whose arms are amputated
at the shoulder or wrist regenerate precisely what is needed and then stop when a correct
limb is complete. These are just a few examples of a nearly ubiquitous property of morpho-
genetic systems: anatomical homeostasis toward a speciﬁc pattern memory, and the ability
to reach that state despite sometimes drastic, unpredictable perturbations. This degree of
anatomical control requites fundamentally computational functions by cell collectives: they
need to be able to measure the current state (e.g. the length of a limb, conﬁguration of the
face, etc.), remember the correct state (represent aspects of the correct target morphology),
and execute a kind of means-ends analysis to reduce error by controlling cell proliferation,
migration, and diﬀerentiation. Trophic reward plays a critical role in such processes: struc-
tures or bodies that are functionally insuﬃcient to obtain nutrients and other resources
from their environments do not survive.
Consistent with a functional continuity between neural and non-neural dynamics, morpho-
genesis shares a number of major features with behavior in addition to the reliance on bio-
electric networks to implement large-scale coordination. The ﬁrst is the hardware/software
distinction: genomes do not encode ﬁnal outcomes, they encode the structure of a system
with plasticity, context-sensitivity, and the ﬂexibility to produce diﬀerent outcomes from
the exact same hardware. This is why genetically wild-type ﬂatworm cells can generate
head structures appropriate to other species when their bioelectric signaling is shifted to
diﬀerent attractors [148], why normal skin cells liberated from frogs spontaneously self-
assemble into diﬀerent, motile proto-organisms (“Xenobots”) without any genomic editing
[149, 150], and why embryos with severe genetic defects can be bioelectrically coaxed to nor-
mal brain morphogenesis by reinforcing speciﬁc voltage patterns [151, 152]. Developmental
bioelectric circuits also feature a kind of re-writable memory. In addition to the default
27

voltage patterns (like the “electric face” [153, 154]), new ones can be written into tissues
and maintained by the circuit, such as the two-headed planaria that result in genetically
wild-type worms when a diﬀerent Vmem pattern memory is temporarily incepted into the
tissue [155, 156]. These two-headed worms continue to regenerate as two-headed in perpe-
tuity (without additional treatment) or can be bioelectrically switched back to one-headed
[155, 157], illustrating the stable but re-writable aspects of the information in the tissue
that guides behavior.
Bioelectric signaling in all tissues, like in nervous systems, integrates information across
distance to enable decisions and behavior toward adaptive outcomes in novel circumstances.
This basic scheme was already discovered by evolution as far back as the time of bacterial
bioﬁlms [158, 159], and is exploited very widely across the web of life from microbes to
humans [96]. Many aspects of cognitive neuroscience have clear parallels in developmental
biology [160], suggesting that much of the reasoning in §3–5 above applies not just to
nervous systems but in fact to all cells solving problems in various spaces. Indeed, all living
systems are deeply hierarchical, exhibiting aspects of basal cognition and decision-making
at levels including molecular pathways [161, 162], physiological problem-solving by cells
[163, 164], and tissue and organ memory [165, 166, 167] among others.
7
Conclusions and future directions
Neurons, networks of neurons, and biological structures generally exchange information
with their environments via physical interactions interpretable in terms of measurement (or
the gathering of sensory input) and manipulative action. This interpretation of biological
activity forms the basis of the active-inference principle [20, 21] and has achieved wide
currency in the biophysics, evo-devo, and neuroscience communities. We have shown here
how to formulate this view of biological activity in the very general yet powerful language
of quantum information theory, employing in particular the idea of a QRF as a calibrated
measurement, and in the output direction, a calibrated action. These results extend the
analysis of QRFs as hierarchical Bayesian systems developed in [43, 44, 45, 47].
They
allow us to model processes implemented by neurons and networks of neurons, from the
biomolecular pathway scale upward, as hierarchies of QRFs. This representation makes
explicit where and how classical information is encoded on successive layers of MBs, enabling
models that explicitly comply with energy-budget considerations. Such models require that
cells employ quantum coherence as a resource for bulk computation [63]. Hence the current
framework represents neurons as explicitly quantum devices.
Our goal here has been to develop a framework for building detailed models of speciﬁc
neuronal cell types in speciﬁc environments; these will be pursued in future work. Even
at the current abstract level, however, we are able to predict generically that dendritic
trees will remodel in the direction of maximal informativeness, with trophic reward as the
selection criterion. While this prediction is generally supported by the phenomenology of
dendritic remodeling [3, 4], it is speciﬁcally testable by correlating measures of branch level
28

activity and branch level active transport. As branch level remodeling is now known to be
involved in neurodegenerative conditions [3, 4], an understanding of the relations between
architecture, computation, and trophic reward at this scale may be useful in ameliorating
these conditions.
From a more general perspective, treating neurons in explicitly quantum-theoretic terms
introduces new possibilities for mathematical modeling, e.g. with topological ﬁeld theory
as discussed in §3. Neuroscience and computer science have both beneﬁtted from the ab-
straction of neurons to sum-threshold units connected in layered ANNs. As interest in and
tractable architectures for quantum computing continue to develop, we may expect a com-
parable, but considerably deeper, connection between biological and artiﬁcial computing
systems.
Acknowledgements
ML gratefully acknowledges funding from the Guy Foundation and the Finding Genius
Foundation.
Conﬂict of interest
The authors declare no competing, ﬁnancial, or commercial interests in this research.
References
[1] Butz M, W¨org¨otter F, van Ooyen A. 2009 Activity-dependent structural plasticity.
Brain Res. Rev. 60, 287–305.
[2] Carulli, D.; Foscarin, S.; Rossi, F. 2011 Activity-dependent plasticity and gene ex-
pression modiﬁcations in the adult CNS. Front. Mol. Neurosci. 4, 50.
[3] Hogan, M.K.; Hamilton, G. F.; Horner, P .J. 2020 Neural stimulation and molecular
mechanisms of plasticity and regeneration: A review. Front. Cell. Neurosci. 14, 271.
[4] Runge K; Cardoso C; de Chevigne A. 2020 Dendritic spine plasticity: Function and
mechanisms. Front. Synaptic Neurosci. 12, 36.
[5] Wittenberg, G. M.; Wang, S. S.-H. 2016. Evolution and scaling of dendrites. In (G.
Stuart, N. Spuston and M. H´’auser , eds.) Dendrites. Oxford University Press, Oxford
UK.
[6] Guerguiev, J; Lillicrap, T. P., Richards, B. A. 2017. Towards deep learning with
segregated dendrites. eLife 6:e22901, 37 pages.
29

[7] Sardi, S.; Vardi, R.; Goldental, A.; Tugendhaft, Y.; Uzan, H.; Kanter, I. 2018 Den-
dritic learning as a paradigm shift in brain learning. ACS Chem. Neurosci. 9, 1230–
1232.
[8] Blackiston, D.; Shomrat, T.; Levin, M. 2015 The stability of memories during brain
remodeling: A perspective. Commun. Integr. Biol. 8, e1073424.
[9] Shomrat, T.; Levin, M. 2013 An automated training paradigm reveals long-term
memory in planarians and its persistence through head regeneration. J. Expt. Biol.
216, 3799–3810.
[10] McConnell, J. V. 1967 A Manual of Psychological Experimentation on Planarians.
Journal of Biological Psychology (Publisher), Ann Arbour, USA.
[11] Gidon, A.; Zolnik, T.A.; Fidzinski, P.; Bolduan, F.; Papoutsi, A.; Poirazi, P.;
Holtkamp, M.; Vida, I.; Larkum, M. E. 2020 Dendritic action potentials and compu-
tation in human layer 2/3 cortical neurons. Science 367, 83–87.
[12] McCulloch, W. S.; Pitts, W. 1943 A logical calculus of the ideas immanent in nervous
activity. Bull. Math. Biophys 5, 115–133.
[13] Segev, I; London, M. 2000 Untangling dendrites with quantitative models. Science
290, 744–750.
[14] Schuman, C.D.; Potok, T. E.; Patton, R. M.; Birdwell, D.; Dean, M. E.; Rose, G.
S.; Plank, J. S. 2017 A survey of neuromorphic computing and neural networks in
hardware. Preprint arXiv:1705.06963v1 [cs.NE].
[15] Tang, J.; Yuan, F.; Shen, X.; Wang, Z.; Rao, M.; He. Y.; Sun, Y.; Li, X.; Zhang, W.;
Li, Y.; Gao, B.; Qian, H.; Bi, G.; Song, S.; Yang, J.; Wu, H. 2019 Bridging biological
and artiﬁcial neural networks with emerging neuromorphic devices: Fundamentals,
progress, and challenges. Adv. Mater. 31, 1902761.
[16] Fields, C.; Bischof, J.; Levin, M. 2020 Morphological coordination: A common an-
cestral function unifying neural and non-neural signaling. Physiology (Bethesda) 35,
16–30.
[17] Barrat, A.; Barth´elemy, M.; Vespignani, A. 2008 Dynamical processes on complex
networks. Cambridge University Press, Cambridge UK.
[18] Rubinov, M.; Sporns, O. 2010 Complex network measures of brain connectivity: Uses
and interpretations. NeuroImage 52, 1059–1069.
[19] Latham, P.; Dayan, P. 2005 Touch´e: the feeling of choice. Nature Neuroscience 8(4),
408–409.
[20] Friston, K. J. 2010 The free-energy principle: A uniﬁed brain theory? Nature Reviews
Neuroscience 11, 127–138.
30

[21] Friston, K. J. 2013 Life as we know it. Journal of The Royal Society Interface 10,
20130475.
[22] Friston, K. J.; Stephan, K. E. 2007 Free-energy and the brain Synthese 159(3), 417–
458
[23] Ramstead, M.J.D.; Friston, K. J.; Hip´olito, I. 2020 Is the free energy principle a formal
theory of semantics? From variational density dynamics to neural and phenotypic
representations. Entropy 22, 889.
[24] Friston, K. J. 2019 A free energy principle for a particular physics. Preprint
arXiv:1906.10184 [q-bio.NC]. https://arxiv.org/abs/1906.10184
[25] Varshney, L. R.; Chen, B. L.; Paniagua, E.; Hall, D. H.; Chklovskii, D. B. 2011
Structural properties of the Caenorhabditis elegans neuronal network. PLoS Comp.
Biol. 7, e1001066.
[26] Herculano-Houzel, S. 2011 Scaling of brain metabolism with a ﬁxed energy budget
per neuron: Implications for neuronal activity, plasticity and evolution. PLoS One 6,
e17514.
[27] Friston, K.; Levin, M.; Sengupta, B.; Pezzulo, G. 2015 Knowing one’s place: A free-
energy approach to pattern regulation. J. R. Soc. Interface 12, 20141383.
[28] Kuchling, F.; Friston, K.; Georgiev, G.; Levin, M. 2020 Morphogenesis as Bayesian
inference: A variational approach to pattern formation and control in complex bio-
logical systems. Phys. Life Rev. 33, 88–108.
[29] Kiebel, S. J., Friston, K, J. 2011 Free energy and dendritic self-organization. Frontiers
in Systems Neuroscience 5, 80 (13 pp).
[30] Bastos AM; Usrey WM; Adams RA; Mangun GR; Fries P; Friston KJ. 2012 Canonical
microcircuits for predictive coding. Neuron 76, 695–711.
[31] Shipp, S., Adams, R. A., Friston, K. J. (2013). Reﬂections on agranular architecture:
Predictive coding in the motor cortex. Trends in Neuroscience 36, 706–716.
[32] Kanai, R., Komura, Y., Shipp, S., Friston, K. (2015). Cerebral hierarchies: Predictive
processing, precision and the pulvinar. Philosophical Transactions of the Royal Society
B 370, 20140169.
[33] Adams, R. A., Friston, K. J., Bastos, A. M. (2015). Active inference, predictive coding
and cortical architecture. In M. F. Casanova, I. Opris (Eds.), Recent advances in the
modular organization of the cortex (pp. 97–121). Berlin: Springer.
[34] Clark A. 2013 Whatever next? Predictive brains, situated agents, and the future of
cognitive science. Behav Brain Sci 36, 181–204.
31

[35] Hohwy J. 2013 The predictive mind. Oxford University Press, Oxford, UK.
[36] Seth AK. 2013 Interoceptive inference, emotion, and the embodied self. Trends Cogn
Sci 17(11), 565–573.
[37] Friston KJ, Rigoli F, Ognibene D, Mathys C, FitzGerald T, Pezzulo G. 2015 Active
inference and epistemic value. Cognit Neurosci 6, 187–214.
[38] Aharonov, Y.; Kaufherr, T. 1984 Quantum frames of reference. Phys. Rev. D 30,
368–385.
[39] Bartlett, S.D.; Rudolph, T.; Spekkens, R.W. 2007 Reference frames, superselection
rules, and quantum information. Rev. Mod. Phys. 79, 555–609.
[40] Fields, C.; Marcian`o, A. 2019 Sharing nonfungible information requires shared non-
fungible information. Quant. Rep. 1, 252–259.
[41] Barwise, J.; Seligman, J. 1997 Information Flow: The Logic of Distributed Systems
(Cambridge Tracts in Theoretical Computer Science 44). Cambridge University Press,
Cambridge, UK.
[42] Fields, C.; Glazebrook, J. F. 2019 A mosaic of Chu spaces and Channel Theory I:
Category-theoretic concepts and tools. J. Expt. Theor. Artif. intell. 31, 177–213.
[43] Fields, C.; Glazebrook, J. F. 2019 A mosaic of Chu spaces and Channel Theory II:
Applications to object identiﬁcation and mereological complexity. J. Expt. Theor.
Artif. intell. 31, 237–265.
[44] Fields, C.; Glazebrook, J. F. 2020 Do Process-1 simulations generate the epistemic
feelings that drive Process-2 decision making? Cogn. Proc. 21, 533–553.
[45] Fields,
C.;
Glazebrook,
J.
F.
2021
Information
ﬂow
in
context-dependent
hierarchical Bayesian inference. J. Expt. Theor. Artif. intell. in press (doi:
10.1080/0952813X.2020.1836034).
[46] Fields, C.; Marcian`o, A. 2019 Holographic screens are classical information channels.
Quant. Rep. 2, 326–336.
[47] Fields, C.; Glazebrook, J. F. 2020 Representing measurement as a thermodynamic
symmetry breaking. Symmetry 12, 810.
[48] Fields, C.; Glazebrook, J. F.; Marcian`o, A. 2021 Reference frame induced symmetry
breaking on holographic screens. Symmetry 13, 408.
[49] Fields, C; Levin, M. 2020 How do living systems create meaning? Philosophies 5, 36.
[50] Fields, C.; Glazebrook, J. F.; Levin, M. 2021 Minimal physicalism as a scale-free
substrate for cognition and consciousness. Neurosci. Cons. 7(2), niab013.
32

[51] Schr¨odinger, E. 1944 What is Life? Cambridge, UK: Cambridge University Press.
[52] Hameroﬀ, S.; Penrose, R. 1996 Orchestrated reduction of quantum coherence in brain
microtubules: A model for consciousness Math. Comput. Simul. 40, 453–480. (doi:
10.1016/0378-4754(96)80476-9)
[53] Bordonaro,
B.;
Ogryzko,
V.
2013
Quantum
biology
at
the
cellular
level
–
Elements
of
the
research
program.
BioSystems
112,
11–30.
(doi:
10.1016/j.biosystems.2013.02.008)
[54] Tononi, G.; Koch, C. 2015 Consciousness here, there and everywhere? Philos. Trans.
R. Soc. B 215, 216–242.
[55] Georgiev, D.D. 2020 Quantum information theoretic approach to the mind-brain
problem. Prog. Biophys. Mol. Biol. 18, 16–32.
[56] Arndt M. T.; Juﬀmann, T.; Vedral, V. 2009 Quantum physics meets biology. HFSP
J. 3, 386–400. (doi: 10.2976/1.3244985)
[57] Lambert, N.; Chen, Y.-N.; Cheng, Y.-C.; Li C.-M.; Chen, G.-Y.; Nori, F. 2012
Quantum biology. Nat. Phys. 9, 10–18. (doi: 10.1038/NPHYS2474)
[58] Melkikh, A. V.; Khrennikov, A. 2015 Nontrivial quantum and quantum-like eﬀects
in biosystems: Unsolved questions and paradoxes. Prog. Biophys. Mol. Biol. 119,
137–161. (doi: 10.1016/j.pbiomolbio.2015.07.001)
[59] Marais, A. et al. 2018 The future of quantum biology. J. R. Soc. Interface 15,
20180640. (doi: 10.1098/rsif.2018.0640)
[60] Cao, J. et al. 2020 Quantum biology revisited. Science Adv. 6 eaaz4888 (doi:
10.1126/sciadv.aaz4888)
[61] Brookes J. C. 2017 Quantum eﬀects in biology: Golden rule in enzymes, olfaction,
photosynthesis and magnetodetection. Proc. R. Soc. A 473, 20160822.
[62] McFadden J, Al-Khalili J. 2018 The origins of quantum biology. Proc. R. Soc. A 474,
20180674.
[63] Fields, C.; Levin, M. 2021 Metabolic limits on classical information processing by
biological cells. BioSystems 209, 104513.
[64] Addazi, A.; Chen, P.; Fabrocini, F.; Fields, C.; Greco, E.; Lulli, M.; Marcian`o, A.;
Pasechnik, R. 2021 Generalized holographic principle, gauge invariance and the emer-
gence of gravity `a la Wilczek. Front. Astron. Space Sci. 8, 563450. (doi: 10.3389/fs-
pas.2021.563450)
[65] Landauer, R. 1961 Irreversibility and heat generation in the computing process. IBM
J. Res. Dev. 5, 183–195.
33

[66] Landauer, R. 1999 Information is a physical entity. Physica A 263, 63–67.
[67] Bennett, C.H. 1982 The thermodynamics of computation. Int. J. Theor. Phys. 21,
905–940.
[68] Fuchs, C. A.; Schack, R. 2013 Quantum-Bayesian coherence. Rev. Mod. Phys. 85,
1693–1715.
[69] Mermin, N. D. 2019 Making better sense of quantum mechanics. Rep. Preg. Phys. 82,
012002.
[70] Rovelli, C. 1996 Relational quantum mechanics. Int. J. Mod. Phys. 35, 1637–1678.
[71] Pearl, J. 1988 Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. San Mateo CA: Morgan Kaufmann.
[72] Clark A. 2017 How to knit your own Markov blanket: Resisting the second law with
metamorphic minds. In (T. Wetzinger and W. Wiese, eds.) Philosophy and Predictive
Processing 3, 19pp. Frankfurt am Mainz Mind Group.
[73] Fields, C.; Marcian`o, A. 2020 Markov blankets are general physical interaction sur-
faces. Phys. Life Rev. 33, 109–111.
[74] Bell, J. S. 1964 On the Einstein–Podolsky–Rosen paradox. Physics 1, 195–200.
[75] Emary, C.; Lambert, N.; Nori, F. 2014 Leggett–Garg inequalities. Rep. Prog. Phys.
77, 039501.
[76] Kochen, S.; Specker, E.P. 1967 The problem of hidden variables in quantum mechan-
ics. J. Math. Mech. 17, 59–87.
[77] Mermin, D. 1993 Hidden variables and the two theorems of John Bell. Rev. Mod.
Phys. 65, 803–815.
[78] Dzharfarov, E.N.; Kon, M. 2018 On universality of classical probability with contex-
tually labeled random variables. J. Math. Psych. 85, 17–24.
[79] Horsman C; Stepney S; Wagner RC; Kendon V. 2014 When does a physical system
compute? Proc. R. Soc. A 470, 20140182.
[80] Smith, J. E.; Nair, R. 2005 The architecture of virtual machines. IEEE Comp. 38(5),
32–38.
[81] Barr, M. 1979 *-Autonomous Categories, with an Appendix by Po Hsiang Chu; Lec-
ture Notes in Mathematics 752; Springer: Berlin, Germany.
[82] Pratt, V. 1999 Chu spaces. In School on Category Theory and Applications (Coimbra
1999); Volume 21 of Textos Mat. S´er. B; University of Coimbra: Coimbra, Portugal,
pp. 39–100.
34

[83] Pratt, V. 1999 Chu spaces from the representational viewpoint. Ann. Pure Appl. Log.
96, 319–333.
[84] Baars, B. J.;Franklin, S. 2003 How conscious experience and working memory interact.
Trends in Cognitive Science 7, 166–172.
[85] Dehaene, S.; Naccache, L. 2001 Towards a cognitive neuroscience of consciousness:
Basic evidence and a workspace framework. Cognition 79, 1–37.
[86] Mashour, G. A.;Roelfsema, P.; Changeux, J.-P.; Dehaene, S. 2020 Conscious process-
ing and the Global Neuronal Workspace hypothesis. Neuron 105, 776–798.
[87] Allwein, G.; Moskowitz, I.S.; Chang, L.-W. 2004 A New Framework for Shannon
Information Theory. Technical Report A801024 Naval Research Laboratory, Wash-
ington, DC, USA, 17p.
[88] Barwise, J. 1997 Information and impossibilities. Notre Dame Journal of Formal Logic
38(4), 488–515.
[89] Cherniak, E. 1991 Bayesian networks without tears. AI Magazine 12(4), 50–63.
[90] Prakash, C.; Fields, C.; Hoﬀman, D. D.; Prentner, R.; Singh, M. 2020 Fact, ﬁction,
and ﬁtness. Entropy 22, 514.
[91] Fields C, Friston K, Glazebrook JF, Levin M 2021 A free energy principle for generic
quantum systems. In review (available as arXiv:2112.15242 [quant-ph]).
[92] Lyon, P. 2015 The cognitive cell: Bacterial behavior reconsidered. Front. Microbiol.
6, 264.
[93] Micali, G.; Endres, R.G. 2016 Bacterial chemotaxis: Information processing, thermo-
dynamics, and behavior. Curr. Opin. Microbiol. 30, 8–15.
[94] Levin, M. 2012 Morphogenetic ﬁelds in embryogenesis, regeneration, and cancer: Non-
local control of complex patterning. Biosystems 109, 243–261.
[95] Levin, M.; Pezzulo, G.; Finkelstein, J. M. 2017 Endogenous bioelectric signaling
networks: Exploiting voltage gradients for control of growth and form. Annu. Rev.
Biomed. Eng. 19, 353–387.
[96] Srivastava, P.; Kane, A.; Harrison, C.; Levin, M. 2021 A meta-analysis of bioelectric
data in cancer, embryogenesis, and regeneration. Bioelectricity 3, 42–67.
[97] Wuichet, K.; Cantwell, B. J.; Zhulin, I. B. 2010 Evolution and phyletic distribution
of two-component signal transduction systems. Curr. Opin. Microbiol. 13, 219–225.
[98] Basieva, I; Khrennikov, A; Ohya, M; Yamato, O. 2011 Quantum-like interference
eﬀect in gene expression: glucose-lactose destructive interference. Syst. Synth. Biol.
5, 59–68.
35

[99] Inada, T; Kimata, K; Aiba, H. 1996 Mechanism responsible for glucose-lactose diauxie
in Eschericha coli challenge to the cAMP model. Genes Cell 1, 293–301.
[100] Loh, K. M.; van Amerongen, R.; Nusse, R. 2016 Generating cellular diversity and
spatial form: Wnt signaling and the evolution of multicellular animals. Dev. Cell. 38,
643–655.
[101] Kolch, W. 2005 Coordinating ERK/MAPK signaling through scaﬀolds and inhibitors.
Nat. Rev. Mol. Cell Biol. 6, 827–838.
[102] Schwanbeck, R.; Martini, S.; Bernoth, K.; Just, U. 2011 The notch signaling pathway:
molecular basis of cell context dependency. Eur Cell Biol 90(6-7), 572–581.
[103] Guo, X.; Wang, X.-F. 2009 Signaling cross-talk between TGF-β/BMP and other
pathways. Cell Res. 19, 71–88.
[104] Hunter, T. 2000 Signaling and beyond. Cell 100, 113–127.
[105] Adamska, M. 2015 Developmental signalling and emergence of animal multicellularity.
In: Evolutionary Transitions to Multicellular Life, edited by Ruiz-Trillo I, Nedelcu
AM. Dordrecht: Springer, pp. 425–450
[106] Fields, C.; Levin, M. 2018 Multiscale memory and bioelectric error correction in the
cytoplasm-cytoskeleton-membrane system. WIRES Syst. Biol. Med. 10, e1410.
[107] Hip´olito, I; Ramstead, M. J. D.; Convertino, L.; Bhat. A.; Friston, K.; Parr, T. 2021
Markov blankets in the brain. Neuroscience and Biobehavioral Reviews 125, 88–97.
[108] Palacios, E. R.; Razi, A.; Parr, T.; Kirchoﬀ, M.; Friston, K. 2020 On Markov blankets
and hierarchical self-organization. J. Theoretical Biology 486, 110089.
[109] Peters, A.; McEwen, B. S.; Friston, K. 2017 Uncertainty and stress: why it causes
diseases and how it can be mastered by the brain. Progress in Neurobiology 156,
164–188.
[110] Spruston, N. 2008 Pyramidal neurons: Dendritic structure and synaptic integration.
Nat. Rev. Neurosci. 9, 206–221.
[111] Rasia-Filho, A. A.; Guerra, K. T. K.; V´asquez, C. E.; Dall’Oglio, A.; Reberger,
R.; Jung, C. R.; Calcagnotto, M. E. 2021 The subcortical-allocortical-neocortical
continuum for the emergence and morphological heterogeneity of pyramidal neurons
in the human brain. Front. Synapt. Neurosci. 13, 616607.
[112] Lloyd, S. 2000 Ultimate physical limits to computation. Nature 406, 1047–1054.
[113] Attwell, D.; Laughlin S. B. 2001 An energy budget for signaling in the grey matter
of the brain. J. Cereb. Blood Flow Metab. 21, 1133–1145
36

[114] Sengupta, B.; Stemmler, M. B.; Friston, K. J. 2013 Information and eﬃciency in the
nervous system: A synthesis. PLoS Comput Biol 9(7), e1003157.
[115] Georgiev, D; Kolev, S; Cohen, E; Glazebrook, JF. 2020 Computational capacity of
pyramidal neurons in the cerebral cortex. Brain Research 1748, 147069.
[116] Deutsch, D. 2002 The structure of the multiverse. Proc. R. Soc. A 458, 2911–2923.
[117] Atiyah, M. 1988 Topological quantum ﬁeld theory. Pub. Math. IH`ES 68, 175–186.
[118] Marcian`o, A.; Chen, D.; Fabrocini, F.; Fields, C.; Greco, E.; Gresnigt, N.; Jinklub,
K.; Lulli, M., Terzidis, K.; Zappala, E. 2021 Deep neural networks as the semi-classical
limit of quantum neural networks. Preprint arXiv:2007.00142v2 [cond-mat.diss-nn].
https://arxiv.org/abs/2007.00142
[119] Eyal, G.; Verhoog, M. B.; Testa-Silva, G.; Deitcher, Y.; Benavides-Piccione, R.;
DeFelipe, J.; de Kock, C. P.J.; Mansvelder, H. D.; Segev, I. 2018 Human cortical
pyramidal neurons:from spines to spikes via models. Front. Cellular Neurosci. 12,
181.
[120] Galloni, A. R.; Laﬀere, A.; Rancz, E. 2020 Apical length governs computational
diversity of layer 5 pyramidal neurons. eLife 9, e55761.
[121] Major, G; Larkum, M. E.; Schiller, J. 2013 Active properties of neocortical pyramidal
neuron dendrites. Annu. Rev. Neurosci. 36, 1–24.
[122] Shipp, S. 2007 Structure and function of the cerebral cortex. Curr. Biol. 17, R443–
R449.
[123] Shipp, S. 2016 Neural elements for predictive coding. Front. Psychol. 7, 1792.
[124] Swanson, L. W.; Lichtman, J. W. 2016 From Cajal to connectome and beyond. Annu.
Rev. Neurosci. 39, 197–216.
[125] V´elez-Fort, M.; Rousseau, C. V.; Niedworok, C. J.; Wickersham, I. R.; Rancz, E.
A.; Brown, A. P. Y.; Strom, M.; Margrie, T. W. 2014 The stimulus selectivity and
connectivity of Layer Six principal cells reveals cortical microcircuits underlying visual
processing. Neuron 83, 1431–1443.
[126] De Nardo, L. A.; Berns, D. C.; DeLoach, K.; Luo, L. 2015 Connectivity of mouse
somatosensory and prefrontal cortex examined with trans-synaptic tracing. Nat. Neu-
rosci. 18, 1687–1697.
[127] Harris, K. D.; Shepherd, G. M. G. 2015 The neocortical circuit: Themes and varia-
tions. Nat. Neurosci. 18, 170–181.
[128] Mennerick S, Zorumsky CF. 2000 Neural activity and survival in the developing
nervous system. Mol. Neurobiol. 22, 41–54.
37

[129] Faust TE, Gunner G, Schafer DP. 2021 Mechanisms governing activity-dependent
synaptic pruning in the developing mammalian CNS. Nature Rev. Neurosci. 22, 657–
673.
[130] Deitcher Y.; Eyal G.; Kanari L.; et al. 2007 Comprehensive morpho-electronic analysis
shows 2 distinct classes of L2 and L3 pyramidal neurons in human temporal cortex.
Cereb. Cortex 27, 5398–5414.
[131] Mohan, H.; Verhoog, M. B.; Doreswamy, K. K.; et al. 2015 Dendritic and axonal
architecture of individual pyramidal neurons across layers of adult human neocortex.
Cereb. Cortex 25, 4839–4853.
[132] Beaulieu-Laroche, L; Toloza, E. H. S.; van der Goes, M. S. et al. 2018 Enhanced
dendritic compartmentalization in human cortical neurons. Cell 175, 643–651.
[133] Nielsen, M. A.; Chuang, I. L. 2000 Quantum Computation and Quantum Information.
New York, Cambridge University Press.
[134] Luo, L. 2021 Architectures of neuronal circuits. Science 373, eabg7285.
[135] Johansson, C.; Lansner, A. 2007 Towards cortex sized artiﬁcial neural systems. Neural
Networks 20, 48–61.
[136] Chen, R.; Cohen, L. G.; Hallett, M. 2002 Nervous system reorganization following
injury. Neuroscience 111, 761–773.
[137] Demekas, D.; Parr, T.; Friston, K. J. 2020 An ivestigation of the free energy principle
for emotional recognition. Frontiers in Computational Neuroscience 14, 30.
[138] Levin, M. 2019 The computational boundary of a “self”: Developmental bioelectricity
drives multicellularity and scale-free cognition. Front. Psychol. 10, 1688.
[139] Levin, M. 2014 Endogenous bioelectrical networks store non-genetic patterning infor-
mation during development and regeneration. J. Physiol. 592, 2295–2305.
[140] Levin, M. 2021 Bioelectric signaling: Reprogrammable circuits underlying embryoge-
nesis, regeneration, and cancer. Cell 184, 1971–1989.
[141] Bates, E. 2015 Ion channels in development and cancer. Annu. Rev. Cell. Devel. Biol.
31, 231–247.
[142] Harris, M. P. 2021 Bioelectric signaling as a unique regulator of development and
regeneration. Development 148, dev180794.
[143] Mathews, J.; Levin, M. 2018 The body electric 2.0: Recent advances in developmental
bioelectricity for regenerative and synthetic bioengineering. Curr. Opin. Biotechnol.
52, 134–144.
38

[144] Levin, M. 2021 Life, death, and self: Fundamental questions of primitive cognition
viewed through the lens of body plasticity and synthetic organisms. Biochem. Biophys.
Res. Commun. 564, 114–133.
[145] Vandenberg, L. N.; Adams, D. S.; Levin, M. 2012 Normalized shape and location of
perturbed craniofacial structures in the Xenopus tadpole reveal an innate ability to
achieve correct morphology. Devel. Dyn. 241, 863–878.
[146] Pinet, K.; McLaughlin, K. A. 2019 Mechanisms of physiological tissue remodeling
in animals: Manipulating tissue, organ, and organism morphology. Devel. Biol 451,
134–145.
[147] Pinet, K.; Deolankar, M.; Leung, B.; McLaughlin, K. A. 2019 Adaptive correction
of craniofacial defects in pre-metamorphic Xenopus laevis tadpoles involves thyroid
hormone-independent tissue remodeling. Development 146, dev175893.
[148] Emmons-Bell, M.; Durant, F.; Hammelman, J.; Bessonov, M.; Volpert, V.; Mo-
rokuma, J.; Pinet, K.; Adams, D. S.; Pietak, A.; Lobo, D.; Levin, M. 2015 Gap
junctional blockade stochastically induces diﬀerent species-speciﬁc head anatomies in
genetically wild-type Girardia dorotocephala ﬂatworms. Int. J. Mol. Sci. 16, 27865–
27896.
[149] Kriegman, S.; Blackiston, D.; Levin, M.; Bongard, J. 2020 A scalable pipeline for
designing reconﬁgurable organisms. Proc. Natl. Acad. Sci. USA 117, 1853–1859.
[150] Blackiston, D.; Lederer, E.; Kriegman, S.; Garnier, S.; Bongard, J.; Levin, M. 2021
A cellular platform for the development of synthetic living machines. Sci. Robot. 6,
eabf1571.
[151] Pai, V. P.; Pietak, A.; Willocq, V.; Ye, B.; Shi, N.-Q.; Levin, M. 2018 HCN2 Rescues
brain defects by enforcing endogenous voltage pre-patterns. Nat. Comms. 9, 998.
[152] Pai, V. P.; Lemire, J. M.; Par´e, J.-F.; Lin, G.; Chen,Y.; Levin, M. 2015 Endogenous
gradients of resting potential instructively pattern embryonic neural tissue via Notch
signaling and regulation of proliferation. J. Neurosci 35, 4366–4385.
[153] Vandenberg, L. N.; Morrie, R. D.; Adams, D. S. 2011 V-ATPase-dependent ecto-
dermal voltage and pH regionalization are required for craniofacial morphogenesis.
Devel. Dyn. 240, 1889–1904.
[154] Pezzulo, G.; Lapalme, J.; Durant, F.; Levin, M. 2021 Bistability of somatic pattern
memories: Stochastic outcomes in bioelectric circuits underlying regeneration. Phil.
Proc. R. Soc. B 376, 20190765.
[155] Durant F.; Morokuma, J.; Fields, C.; Williams, K.; Adams, D. S.; Levin, M. 2017
Long-term, stochastic editing of regenerative anatomy via targeting endogenous bio-
electric gradients. Biophys. J. 112, 2231–2243.
39

[156] Oviedo, N. J.; Morokuma, J.; Walentek, P.; Kema, I. P.; Gu, M. B.; Ahn, J.-M.;
Hwang, J. S.; Gojobori, T.; Levin, M. 2010 Long-range neural and gap junction
protein-mediated cues control polarity during planarian regeneration. Devel. Biol.
339, 188–199.
[157] Durant, F.; Bischof, J.; Fields, C.; Morokuma, J.; LaPalme, J.; Hoi, A.; Levin,
M. 2019 The role of early bioelectric signals in the regeneration of planarian ante-
rior/posterior polarity. Biophys. J. 116, 948–961.
[158] Koshland, D. E. 1983 The bacterium as a model neuron. Trends Neurosci 6, 133–137.
[159] Prindle, A.; Liu, J.; Asally, M.; Ly, S.;Garcia-Ojalvo, J.; S¨uel, G. M. 2015 Ion channels
enable electrical communication in bacterial communities. Nature 527, 59–63.
[160] Pezzulo, G.; Levin, M. 2015 Re-membering the body: Applications of computational
neuroscience to the top-down control of regeneration of limbs and other complex
organs. Integr. Biol. (Cambridge) 7, 1487–1517.
[161] Watson, R. A.; Buckley, C. L.; Mills, R.; Davies, A. 2010 In Artiﬁcial Life Conference
XII. (Odense, Denmark, 2010), pp. 194–201.
[162] Biswas, S.; Manicka, S.; Hoel, E.; Levin, M. 2021 Gene regulatory networks exhibit
several kinds of memory: Quantiﬁcation of memory in biological and random tran-
scriptional networks. iScience 24, 102131.
[163] Emmons-Bell, M., Durant, F.; Tung, A.; Pietak, A.; Miller, K.; Kane, A.; Martyniuk,
C. J.; Davidian, D.; Morokuma, J.; Levin, M. 2019 Regenerative adaptation to elec-
trochemical perturbation in planaria: A molecular analysis of physiological plasticity.
iScience 22, 147–165.
[164] Jacob, E. B.; Aharonov, Y.; Shapira, Y. 2004 Bacteria harnessing complexity. Bioﬁlms
1, 239–263.
[165] Goel, P.; Mehta, A. 2013 Learning theories reveal loss of pancreatic electrical connec-
tivity in diabetes as an adaptive response. PLoS One 8, e70366.
[166] Zoghi, M. 2004 Cardiac memory: Do the heart and the brain remember the same? J.
Interv. Card. Electrophysiol. 11, 177–182.
[167] Chakravarthy, S. V.; Ghosh, J. 1997 On Hebbian-like adaptation in heart muscle: A
proposal for ‘cardiac memory’. Biol. Cybern. 76, 207–215.
40

