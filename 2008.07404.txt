1
Computer Vision and Image Understanding
journal homepage: www.elsevier.com
Skeleton-based Action Recognition via Spatial and Temporal Transformer Networks
Chiara Plizzaria,b,∗∗, Marco Cannicia, Matteo Matteuccia
aPolitecnico di Milano, Via Giuseppe Ponzio 34/5, Milan 20133, Italy
bPolitecnico di Torino, Corso Duca degli Abruzzi, 24, Turin 10129, Italy
ABSTRACT
Skeleton-based Human Activity Recognition has achieved great interest in recent years as skeleton
data has demonstrated being robust to illumination changes, body scales, dynamic camera views,
and complex background. In particular, Spatial-Temporal Graph Convolutional Networks (ST-GCN)
demonstrated to be eﬀective in learning both spatial and temporal dependencies on non-Euclidean
data such as skeleton graphs. Nevertheless, an eﬀective encoding of the latent information underlying
the 3D skeleton is still an open problem, especially when it comes to extracting eﬀective information
from joint motion patterns and their correlations. In this work, we propose a novel Spatial-Temporal
Transformer network (ST-TR) which models dependencies between joints using the Transformer self-
-attention operator. In our ST-TR model, a Spatial Self-Attention module (SSA) is used to understand
intra-frame interactions between diﬀerent body parts, and a Temporal Self-Attention module (TSA) to
model inter-frame correlations. The two are combined in a two-stream network, whose performance is
evaluated on three large-scale datasets, NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics Skeleton
400, consistently improving backbone results. Compared with methods that use the same input data,
the proposed ST-TR achieves state-of-the-art performance on all datasets when using joints’ coordi-
nates as input, and results on-par with state-of-the-art when adding bones information.
© 2021 Elsevier Ltd. All rights reserved.
1. Introduction
Human Action Recognition is achieving increasing inter-
est in recent years for the progress achieved in deep learn-
ing and computer vision and for the interest of its applica-
tions in human-computer interaction, eldercare and healthcare
assistance, as well as video surveillance. Recent advances in
3D depth cameras such as Microsoft Kinect (Zhang (2012))
and Intel RealSense (Keselman et al. (2017)) sensors, and ad-
vanced human pose estimation algorithms (Cao et al. (2019))
made it possible to estimate 3D skeleton coordinates quickly
and accurately with cheap devices. Nevertheless, several as-
pects of skeleton-based action recognition still remain open
(Zhang et al. (2019); Aggarwal and Ryoo (2011); Ren et al.
(2020)). The most widespread method to perform skeleton-
based action recognition has nowadays become Graph Neu-
ral Networks (GNNs), and in particular, Graph Convolutional
∗∗Corresponding author:
e-mail: chiara.plizzari@mail.polimi.it (Chiara Plizzari)
Networks (GCNs) since, being an eﬃcient representation of
non-Euclidean data, they are able to eﬀectively capture spatial
(intra-frame) and temporal (inter-frame) information. Models
making use of GCN were ﬁrst introduced in skeleton-based ac-
tion recognition by Yan et al. (2018) and they are usually re-
ferred to as Spatial-Temporal Graph Convolutional Networks
(ST-GCNs). These models process spatial information by op-
erating on skeleton bone-connections along space, and tempo-
ral information by considering additional time-connections be-
tween each skeleton joint along time. Despite being proven to
perform very well on skeleton data, ST-GCN models have some
structural limitations, some of them already addressed by Shi
et al. (2019a); Simonyan and Zisserman (2014); Cheng et al.
(2020); Liu et al. (2020).
First of all, the topology of the graph representing the hu-
man body is ﬁxed for all layers and all the actions; this may
prevent the extraction of rich representations for the skeleton
movements during time, especially if graph links are directed
and information can only ﬂow along a predeﬁned path. Sec-
ondly, both Spatial and Temporal Convolution are implemented
starting from a standard 2D convolution. As such, they are lim-
arXiv:2008.07404v4  [cs.CV]  22 Jun 2021

2
q
k
v
(1)
(2)
(3)
(4)
Fig. 1. Self-attention on skeleton joints. (1) For each body joint, a query
q, a key k and a value vector v are calculated. (2) Then, the dot product
(⊙) between the query of the joint and the key of all the other nodes is per-
formed, representing the connection strength between each pair of nodes.
(3) Finally, each node is scaled by its correlation w.r.t. the current node, (4)
whose new features are obtained summing the weighted nodes together.
ited to operate in a local neighborhood, somehow restricted by
the convolution kernel size. And ﬁnally, as a consequence of the
previous, correlations between body joints not linked in the hu-
man skeleton, e.g., the left and right hands, are underestimated
even if relevant in actions such as “clapping”.
In this paper, we face all these limitations by employing
a modiﬁed Transformer self-attention operator, as depicted in
Figure 1. Despite being originally designed for Natural Lan-
guage Processing (NLP) tasks, Transformer self-attention has
shown remarkable results on a broad range of computer vi-
sion tasks, spanning from classical classiﬁcation and detection
(Dosovitskiy et al. (2020); Bello et al. (2019a); Wang et al.
(2018); Carion et al. (2020)), to more complex tasks such as
those involving point clouds (Zhao et al. (2020)), generative
modeling (Oord et al. (2016); Parmar et al. (2018)) and caption-
ing (He et al. (2020)). In our setting, the sequentiality and hier-
archical structure of human skeleton sequences, as well as the
ﬂexibility of Transformer self-attention (Vaswani et al. (2017))
in modeling long-range dependencies, make the Transformer
a perfect solution to tackle ST-GCN weaknesses. In our work,
we aim to apply Transformer to spatial-temporal skeleton-based
architectures, and in particular to joints representing the hu-
man skeleton, with the goal of modeling long-range interac-
tions within human actions both in space, through a Spatial
Self-Attention (SSA) module, and time, through a Temporal
Self-Attention (TSA) module. Main contributions of this pa-
per are summarized as follows:
• We propose a novel two-stream Transformer-based model
for skeleton activity recognition tasks, employing self-
attention on both the spatial and the temporal dimensions
• We design a Spatial Self-Attention (SSA) module to dy-
namically build links between skeleton joints, representing
the relationships between human body parts, conditionally
on the action and independently from the natural human
body structure. On the temporal dimension, we introduce
a Temporal Self-Attention (TSA) module to study the dy-
namics of a joint along time. We made both layers publicly
available for experiments replication and further use 1
• Our model outperforms ST-GCN (Yan et al. (2018)) and
1Code at https://github.com/Chiaraplizz/ST-TR
A-GCN (Shi et al. (2019b)) consistently improving back-
bone results on all datasets and achieving state-of-the-art
performance when using joint information, and results on-
par with state-of-the-art when bones information is used.
2. Related Works
2.1. Skeleton-based Action Recognition
Most of the early studies in skeleton-based action recogni-
tion relied on handcrafted features (Hu et al. (2015); Vemula-
palli et al. (2014); Hussein et al. (2013)) exploiting relative 3D
rotations and translations between joints. Deep learning revolu-
tionized activity recognition by proposing methods capable of
increased robustness (Wang et al. (2019)) and able to achieve
unprecedented performance. Methods that fall into this cate-
gory rely on diﬀerent aspects of skeleton data: (1) Recurrent
neural network (RNN) based methods (Lev et al. (2016); Wang
and Wang (2017); Liu et al. (2017b); Du et al. (2015)) leverage
on the sequentiality of joint coordinates, treating input skeleton
data as time series. (2) Convolutional neural network (CNN)
based methods (Ch´eron et al. (2015); Simonyan and Zisserman
(2014); Ding et al. (2017); Liu et al. (2017c); Li et al. (2017))
leverage spatial information, in a complementary way to RNN-
based ones. Indeed, 3D skeleton sequences are mapped into
a pseudo-image, representing temporal dynamics and skeleton
joints respectively in rows and columns. (3) Graph neural net-
work (GNN) based methods (Yan et al. (2018); Li et al. (2019);
Shi et al. (2019b,a); Cheng et al. (2020); Liu et al. (2020)), make
use of both spatial and temporal data by exploiting information
contained in the natural topological graph structure of the hu-
man skeleton. These latter methods have demonstrated to be
the most expressive among the three, and among these, the ﬁrst
model capturing the balance between spatial and temporal de-
pendencies has been the Spatio-Temporal Graph Convolutional
Network (ST-GCN) (Yan et al. (2018)).
In this work, we used ST-GCN as the baseline model; its
functioning is presented in details in Section 3.2. Speciﬁcally,
we propose to substitute regular graph convolutions on both
space and time with the transformer self-attention operator. Cho
et al. (2020) also proposed a Self-Attention Network (SAN) in
which embeddings are extracted by segmenting the action se-
quence in temporal clips, and self-attention is applied among
them in order to model long-term semantic information. How-
ever, since it applies self-attention on course-grained embed-
dings rather than skeleton joints, it hardly captures low-level
joints’ correlations within and between frames. Instead, by ap-
plying self-attention directly on nodes in the graph, both intra-
and inter-frame, we eﬃciently model both spatial and temporal
dependencies of the skeleton sequence.
2.2. Graph Neural Networks
Geometric deep learning (Bronstein et al. (2017)) refers to
all emerging techniques attempting to generalize deep learning
models to non-Euclidean domains such as graphs. The notion
of Graph Neural Network (GNN) was initially outlined by Gori
et al. (2005) and further elaborated by Scarselli et al. (2008).

3
The intuitive idea underlying GNNs is that nodes in a graph rep-
resent objects or concepts while edges represent their relation-
ships. Due to the success of Convolutional Neural Networks,
the concept of convolution has later been generalized from grid
to graph data. GNNs iteratively process the graph, each time
representing nodes as the result of applying a transformation to
nodes’ and their neighbors’ features. The ﬁrst formulation of
CNNs on graphs is due to Bruna et al. (2014), who general-
ized convolution to signals using a spectral construction. This
approach had computational drawbacks that have been subse-
quently addressed by Henaﬀet al. (2015) and Deﬀerrard et al.
(2016). The latter has been further simpliﬁed and extended by
Kipf and Welling (2017). A complementary approach is the
spatial one, where graph convolution is deﬁned as information
aggregation (Micheli (2009); Niepert et al. (2016); Such et al.
(2017)). In this work we make use of the spectral construc-
tion proposed by Kipf and Welling (2017), whose formulation
is provided in Section 3.2.
2.3. Transformers in Computer Vision
The Transformer is the leading neural model for Natural Lan-
guage Processing (NLP), proposed by Vaswani et al. (2017)
as an alternative to recurrent networks. It has been designed
to face two key problems: (i) the processing of very long
sequences, which are often intractable both for LSTMs and
RNNs, and (ii) the limitations in parallelizing sentence process-
ing, which is usually performed sequentially, word by word,
in standard RNNs architectures. The Transformer follows a
usual encoder-decoder structure, but it relies solely on multi-
head self-attention (Vaswani et al. (2017)). Recently, Trans-
former self-attention has been applied in many popular com-
puter vision tasks.
Wang et al. (2018) proposed a diﬀeren-
tiable non-local operator based on self-attention, which allows
to capture long-range dependencies both in space and time for
a more accurate video classiﬁcation. After the ﬁrst attempt of
Bello et al. (2019a) to use self-attention as an alternative to
convolutional operators, Dosovitskiy et al. (2020) proposed a
Vision Transformer (ViT), which shows how Transformers can
eﬀectively replace standard convolutions on images. He et al.
(2020) proposed a novel image transformer architecture for the
image captioning task. Carion et al. (2020) made the ﬁrst at-
tempt to use a Transformer model to tackle detection problems,
namely the Detection Transformer (DeTR). Zhao et al. (2020)
proposed Point Transformer, a model which uses transformer
self-attention to encode relations between point clouds, ex-
ploiting their permutation invariant nature. Other applications
of Transformers in segmentation tasks (Huang et al. (2019)),
multi-modal tasks (Lee et al. (2020)) and generative modeling
(Oord et al. (2016); Parmar et al. (2018)) have been recently
developed, showing the potential of Transformer models on a
broad range of tasks.
3. Background
In this section, Spatial-Temporal Graph Convolutional Net-
works (ST-GCN) by Yan et al. (2018) and the original Trans-
former self-attention by Vaswani et al. (2017) are summarized,
being the basic blocks of the model we propose in this paper.
3.1. Skeleton Sequences Representation
Given a sequence of skeletons, we deﬁne V as the number
of joints representing each skeleton and T as the total num-
ber of skeletons composing the sequence, also named frames
in the following. In order to represent the sequence, a spatial
temporal graph is built, i.e., G = (N, E), where N = {vti|t =
1, ..., T, i = 1, ..., V} represents the set of all the nodes vti of
the graph, i.e., the body joints of the skeleton along all the
time sequence, and E represents the set of all the connections
between nodes.
E consists of two subsets; the ﬁrst subset
ES = {(vti, vt j) | i, j = 1, . . . , V, t = 1, . . . , T} is composed by
the intra-skeleton connections at each time interval t, for any
pair of joints (i, j) connected by a bone in the human skeleton.
The subset ES of intra-skeleton connections is commonly fur-
ther divided into K disjoint partitions, based on some criterion
(Yan et al. (2018)) (e.g., distance from the center of gravity),
and encoded using a set of adjacency matrices ˜Ak ∈{0, 1}V×V.
The second subset ET = {(vti, v(t+1)i) | i = 1, . . . , V, t = 1, . . . , T}
consists of all the inter-frame connections between joints along
consecutive time frames. The result is a graph extending on
both the spatial and the temporal dimension.
3.2. Spatial Temporal Graph Convolutional Networks
Spatial Temporal Graph Convolutional Networks (ST-GCN)
have been introduced by Yan et al. (2018). A ST-GCN is struc-
tured as a hierarchy of stacked spatial-temporal blocks, which
are internally composed of a spatial convolution (GCN) fol-
lowed by a temporal convolution (TCN).
The spatial sub-module uses the Graph Convolution formu-
lation proposed by Kipf and Welling (2017), which can be sum-
marized as it follows:
fout =
Ks
X
k
(finAk)Wk,
(1)
Ak = D
−1
2
k ( ˜Ak + I)D
−1
2
k , Dii =
Ks
X
k
( ˜Ai j
k + Iij),
(2)
where Ks is the kernel size on the spatial dimension, ˜Ak is the
adjacency matrix of the undirected graph representing intra-
body connections, I is the identity matrix and Wk is a trainable
weight matrix. The temporal convolution sub-module (TCN)
is implemented as a 1 × Kt 2D convolution operating on (V, T)
dimensions of the (Cin, V, T) input volume, where Kt is the num-
ber of frames considered within the kernel receptive ﬁeld.
As shown in Equation 1, the graph structure is predeﬁned,
being the adjacency matrix ﬁxed. In order to make it adapative,
Shi et al. (2019b) introduced the Adaptive Graph Convolutional
Network (A-GCN), where the GCN formulation in Equation 1
is replaced by the following:
fout =
Ks
X
k
fin(Ak + Bk + Ck)Wk,
(3)
where Ak is the same as the one in Equation 1, Bk is learned
during training, and Ck determines whether two vertices are
connected or not through a similarity function.

4
(a) Spatial Self-Attention
(b) Temporal Self-Attention
Fig. 2. Spatial Self-Attention (SSA) and Temporal Self-Attention (TSA). Self-attention operates on each pair of nodes, by computing a weight for each of
them which represents the strength of their correlation. Those weights are then used to score the contribution of each body joint vti, proportionally to how
relevant the node is w.r.t. to all the others. Please notice that on SSA (a), the procedure is illustrated only of a group of ﬁve nodes for simplicity, while in
practice it operates on all the nodes.
3.3. Transformer Self-Attention
The original Transformer model of Vaswani et al. (2017) em-
ploys self-attention, i.e., a non-local operator originally de-
signed to operate on words in NLP tasks with the goal of en-
riching the embedding of each word based on the surrounding
context. In the Transformer, new word embeddings are com-
puted by comparing pairs of words an then mixing their embed-
dings together based on how much a word is relevant w.r.t. the
others. By gathering clues from the surrounding context, self-
attention enables to extract a better meaning from each word,
dynamically building relations within and between phrases.
In particular, for each word embedding wi
∈
W
=
{w1, ..., wn}, a query q ∈Rdq, a key k ∈Rdk and a value vector
v ∈Rdv are computed through trainable linear transformations,
independently. Then, a score for each word embedding is ob-
tained by taking the dot product αij = qi · kT
j ∀i, j = 1, ..., n,
where n is the total number of nodes being considered. This
score represents how much the word j is relevant for word i.
To compute the ﬁnal embedding for word i, a weighted sum
is computed by ﬁrst multiplying the value vector of each other
word v j by the corresponding score αij, scaled through the soft-
max function, and then summing these vectors together. This
process, also called scaled dot-product attention, can be written
in matrix form as it follows:
Attention(Q, K, V) = softmax
 QKT
√dk
!
V,
(4)
where Q, K, and V are matrices containing the predicted query,
key and value vectors, respectively, packed together and dk is
the channel dimension of the key vectors. The division by √dk
is performed in order to increase gradients stability during train-
ing. In order to obtain better performance, a mechanism called
multi-headed attention is usually applied, which consists in ap-
plying attention, i.e., a head, multiple times with diﬀerent learn-
able parameters and then ﬁnally combining the results.
4. Spatial Temporal Transformer Network
We propose the Spatial Temporal Transformer (ST-TR) net-
work, an architecture which uses Transformer self-attention to
operate on both space and time. We propose to achieve this
goal using two modules, the Spatial Self-Attention (SSA) and
the Temporal Self-Attention (TSA) modules, each one focusing
on extracting correlations on one of the two dimensions.
4.1. Motivation
The idea behind the original Transformer self-attention is to
allow the encoding of both short- and long-range correlations
between words in the sentence. Our intuition is that the same
approach can be applied to skeleton-based action recognition
as well, as correlations between nodes are crucial both on the
spatial and on the temporal dimension. We consider the joints
comprising the skeleton as a bag-of-words and make use of the
Transformer self-attention to extract node embeddings encod-
ing the relation between surrounding joints, just like words in a
phrase in NLP. Contrary to a standard graph convolution, where
only the adjacent nodes are compared, we discard any prede-
ﬁned skeleton structure and instead let the Transformer self-
attention automatically discover joint relations which are rele-
vant for predicting the current action. The resulting operation
acts similarly to a graph convolution, but in which the kernel
values are dynamically predicted based on the discovered joint
relations. The same idea is also applied at the sequence level, by
analyzing how each joint changes during the action and build-
ing long-range relations that span diﬀerent frames, similarly to
how relations between phrases are built in NLP. The resulting
operator is capable of obtaining a dynamical representation ex-
tending both on the spatial and the temporal dimension.
4.2. Spatial Self-Attention (SSA)
The Spatial Self-Attention module applies self-attention in-
side each frame to extract low-level features embedding the re-
lations between body parts. This is achieved by computing cor-
relations between each pair of joints in every single frame in-
dependently, as depicted in Figure 2a. Given the frame at time
t, for each node vti of the skeleton, a query vector qt
i ∈Rdq, a
key vector kt
i ∈Rdk and a value vector vt
i ∈Rdv are ﬁrst com-
puted by applying trainable linear transformations to the node
features nt
i ∈RCin with parameters Wq ∈RCin×dq, Wk ∈RCin×dk,
Wv ∈RCin×dv, shared across all nodes. Then, for each pair of

5
S-TR
T-TR
feature extraction
3,64,1
64,64,1
64,128,2
64,64,1
128,128,1
128,128,1
128,256,2
256,256,1
256,256,1
T-TR modules
S-TR modules
S-TR i-th module
T-TR i-th module
S-TR stream
T-TR stream
stream
stream
(ST-GCN layers)
Fig. 3. Illustration of two 2s-ST-TR architecture. On each stream, the ﬁrst
three layers extract low level features through standard ST-GCN (Yan et al.
(2018)) layers. At each successive layer, on the S-TR stream (coloured in
green), SSA is used to extract spatial information, followed by a 2D con-
volution on time dimension (TCN), while on the T-TR stream (coloured in
blue), TSA is used to extract temporal information, while spatial features
are extracted by a standard graph convolution (GCN).
body nodes (vti, vt j), a query-key dot product is applied to obtain
a weight αt
ij = qt
i · kt
j
T ∈R, ∀t ∈T representing the correlation
strength between the two nodes. The resulting score αt
i j is used
to weight each joint value vt
j, and a weighted sum is computed
to obtain a new embedding zt
i for node vti, as in the following:
zt
i =
X
j
softmaxj

αt
ij
√dk
vt
j,
(5)
where zt
i ∈RCout (with Cout the number of output channels) con-
stitutes the new embedding of node vti.
Multi-head attention is applied by repeating this embedding
extraction process Nh times, each time with a diﬀerent set of
learnable parameters. The set (zt
i1, ..., zt
iH) of node embeddings
thus obtained, all referring to the same node vti, is then com-
bined with a learnable transformation, i.e., concat(zt
i1, ..., zt
iH) ·
Wo, and constitutes the output features of SSA.
As shown in Figure 2a, the relations between nodes (i.e., the
αt
i j scores) are dynamically predicted in SSA; the correlation
structure in the skeleton is then not ﬁxed for all the actions, but
it changes adaptively for each sample. SSA operates similar to
a graph convolution on a fully connected graph where, however,
the kernel values (i.e., the αt
ij scores) are predicted dynamically
based on the skeleton pose.
4.3. Temporal Self-Attention (TSA)
With the Temporal Self-Attention (TSA) module, the dynam-
ics of each joint is studied separately along all the frames, i.e.,
each single joint is considered as independent and correlations
between frames are computed by comparing the change in the
embeddings of the same body joint along the temporal dimen-
sion (see Figure 2b). The formulation is symmetrical to the one
reported in Equation (5) for SSA:
αv
tu = qv
t · kv
u
∀v ∈V,
zv
t =
X
j
softmaxu
 αv
tu
√dk
!
vv
u,
(6)
where vti, vui indicate the same joint v in two diﬀerent instants
t, u, αi
tu ∈R is the correlation score, qi
t ∈Rdq is the query
Reshape
Reshape
BatchNorm
Conv1D
Conv1D
Fig. 4. Illustration of a SSA module (the implementation of TSA is the
same, with the only diﬀerence that the dimension V corresponds to T and
viceversa). The input fin is reshaped by moving T in the batch dimension,
such that self-attention operates on each time frame separately. SSA is
implemented as a matrix multiplication, where Q, K and V are the query,
key and value matrix respectively, and ⊗denotes the matrix multiplication.
associated to vti, ki
u ∈Rdk and vi
u ∈Rdv are the key and value
associated to joint vui (all computed using trainable linear trans-
formations as in SSA), and zi
t ∈RCout is the resulting node em-
bedding. Note that the notation used in this section is opposite
w.r.t. the one used in Section 4.2; subscripts indicate time while
superscripts indicate the joint. Multi-head attention is applied
in TSA as in SSA. An example of TSA is depicted in Figure 2b.
The TSA module, by extracting inter-frame relations be-
tween nodes in time, can learn how to correlate frames apart
from each other (e.g., nodes in the ﬁrst frame with those in the
last one), capturing discriminant features that are not otherwise
possible to capture with a standard ST-GCN convolution, being
this limited by the kernel size.
4.4. Two-Stream Spatial Temporal Transformer Network
To combine the SSA and TSA modules, a two-stream ar-
chitecture named ST-TR is used, as similarly proposed by Shi
et al. (2019b) and Shi et al. (2019a). In our formulation, the
two streams diﬀerentiate on the way the proposed self-attention
mechanisms are applied: SSA operates on the spatial stream
(named S-TR), while TSA on the temporal one (named T-TR).
On both streams, node features are ﬁrst extracted by a three-
layers residual network, where each layer processes the input
on the spatial dimension through graph convolution (GCN), and
on the temporal dimension through a standard 2D convolution
(TCN), as done by Yan et al. (2018)2. SSA and TSA are then
applied on the S-TR and on the T-TR streams in the subsequent
layers in substitution to the GCN and TCN feature extraction
modules respectively (Figure 3). The S-TR stream and T-TR
2In principle other features, e.g., visual features, could be added here but
we want in this paper to focus on pure skeleton base action recognition and we
leave this option for future investigations.

6
stream are end-to-end trained separately along with their corre-
sponding feature extraction layers. The sub-networks outputs
are eventually fused together by summing up their softmax out-
put scores to obtain the ﬁnal prediction, as proposed by Shi et al.
(2019b) and Shi et al. (2019a).
Spatial Transformer Stream (S-TR). In the spatial stream,
self-attention is applied at the skeleton level through a SSA
module, which focuses on spatial relations between joints. The
output of the SSA module is passed to a 2D convolutional mod-
ule with kernel Kt on the temporal dimension (TCN), as done
by Yan et al. (2018), in order to extract temporally relevant fea-
tures, as shown in Figure 3 and expressed in the following:
S-TR(x) = Conv2D(1×Kt)(SSA(x)).
(7)
Following the original Transformer, the input is passes through
a Batch Normalization layer (Ioﬀe and Szegedy (2015);
Nguyen and Salazar (2019)), and skip connections are used to
sum the input to the output of the SSA module (see Figure 4).
Temporal Transformer Stream (T-TR). The temporal stream,
instead, focuses on discovering inter-frame temporal relations.
Similarly to the S-TR stream, inside each T-TR layer, a standard
graph convolution sub-module (Yan et al. (2018)) is followed
by the proposed Temporal Self-Attention module:
T-TR(x) = TSA(GCN(x)).
(8)
TSA operates on graphs linking the same joint along all the time
dimension (e.g., all left feet, or all right hands).
4.5. Implementation of SSA and TSA
The matrix implementation of SSA (and of TSA) is based
on the implementation of Transformer on pixels by Bello et al.
(2019b). As shown in Figure 4, given an input tensor of shape
(Cin, T, V), where Cin is the number of input features, T is
the number of frames and V is the number of nodes, a matrix
XV ∈RT×Cin×V is obtained by rearranging the input. Here the
T dimension is moved inside the batch dimension, eﬀectively
implementing parameter sharing along the temporal dimension
and applying the transformation separately on each frame:
headh(XV) = S o ftmax

(XVWq)(XVWk)T
q
dh
k

(XVWv)
S el f AttentionV = Concat(head1, ..., headNh)Wo,
(9)
where the product with Wq ∈RCin×Nh×dh
q, Wk ∈RCin×Nh×dh
k and
Wv ∈RCin×Nh×dh
v gives rise respectively to Q ∈RT×Nh×dh
q×V,
K ∈RT×Nh×dh
k×V and V ∈RT×Nh×dh
v×V, being Nh the number of
heads, and Wo a learnable linear transformation combining the
heads outputs. The output of the Spatial Transformer is then
rearranged back into RCout×T×V. The TSA matrix implementa-
tion has the same expression as Equation (9), diﬀering only in
the way the input X is processed. Indeed, in order to be pro-
cessed by each TSA module, the input is reshaped into a ma-
trix XT ∈RV×Cin×T, where the V dimension has been moved
in the ﬁrst position and aggregated to the batch dimension, not
reported here explicitly, in order to operate separately on each
joint along the time dimension. The formulation is analogous
to Equation (9), diﬀering only in the shape of matrices, which
become Q ∈RV×Nh×dh
q×T, K ∈RV×Nh×dh
k×T and V ∈RV×Nh×dh
v×T.
5. Model Evaluation
To understand the impact of both the Spatial and Temporal
Transformer streams, we analyze their performance separately
and in diﬀerent conﬁgurations through extensive experiments
on NTU-RGB+D 60 (Shahroudy et al. (2016)) (see Table 1-
3). Then, for a comparison with the state-of-the-art, we test
the resulting best conﬁgurations on the Kinetics dataset (Kay
et al. (2017)) and on the NTU-RGB+D 120 dataset (Liu et al.
(2019)), which represents to date one of the most complex
skeleton-based action recognition benchmarks (see Table 4-5).
5.1. Datasets
NTU RGB+D 60 and NTU RGB+D 120. The NTU RGB+D
60 (NTU-60) dataset is a large-scale benchmark for 3D hu-
man action recognition collected using Microsoft Kinect v2 by
Shahroudy et al. (2016). Skeleton information consists of 3D
coordinates of 25 body joints and a total of 60 diﬀerent ac-
tion classes. The NTU-60 dataset follows two diﬀerent criteria
for evaluation. The ﬁrst one, called Cross-View Evaluation (X-
View), uses 37, 920 training and 18, 960 test samples, split ac-
cording to the camera views from which the action is taken. The
second one, called Cross-Subject Evaluation (X-Sub), is com-
posed instead of 40, 320 training and 26, 560 test samples. Data
collection has been performed with 40 diﬀerent subjects per-
forming actions and divided into two groups, one for training
and the other for testing. NTU RGB+D 120 (Liu et al. (2019))
(NTU-120) is an extension of NTU-60, which adds 57, 367 new
skeleton sequences representing 60 new actions. To perform the
evaluation, the extended dataset follows two criteria: the ﬁrst
one is the Cross-Subject Evaluation (X-Sub), the same used for
NTU-60, while the second one is called Cross-Setup Evaluation
(X-Set), which substitutes Cross-View by splitting training and
testing samples based on the parity of the camera setup IDs.
Kinetics. The Kinetics skeleton dataset (Yan et al. (2018)) is
obtained by extracting skeleton annotations from videos com-
posing the Kinetics 400 dataset (Kay et al. (2017)), by using the
OpenPose toolbox (Cao et al. (2019)). It consists of 240, 436
training and 19, 796 testing samples, representing a total of 400
action classes. Each skeleton is composed by 18 joints, each
one provided with the 2D coordinates and a conﬁdence score.
For each frame, a maximum of 2 people are selected based on
the highest conﬁdence scores.
5.2. Model Complexity
We perform an analysis on the complexity of the diﬀerent
self-attention modules we designed, and compare them to ST-
GCN modules (Yan et al. (2018)), based on standard convolu-
tion, and to 1s-AGCN (Shi et al. (2019b)) modules, based on
adaptive graph convolution. First, we compare in Figure 5a,

7
64 128
256
512
0
1
2
·106
channels
params
TC
GC
AGC
SSA, TSA
(a)
64 128
256
512
0
1
2
3
·106
channels
1s-AGCN
ST-GCN
S-TR
T-TR
(b)
Fig. 5. (a) Diﬀerence in terms of parameters between Graph Convolution
(GC), Adaptive Convolution (AGC), Spatial Self-Attention (SSA) modules
of Cin = Cout channels, and between Temporal Convolution (TC) and Tem-
poral Self-Attention (TSA) modules; (b) parameters comparison between
ST-GCN, 1s-AGCN and our novel S-TR and T-TR. Best viewed in colors.
singularly, a layer of standard convolution with our transformer
mechanism, setting Cin = Cout channels. This results in the
same number of parameters for both TSA and SSA, since the
convolutions performed internally have the same kernel dimen-
sions and both the query-key dot product and the logit-value
product are parameter free. It can be seen that SSA introduces
less parameters than GC, especially when dealing with a large
number of channels, where the maximum ∆GC−S S A, i.e., the
decrease in terms of parameters, is 1.1 × 105. When dealing
with adaptive modules (AGC), an additional number of param-
eters has to be considered, resulting in a diﬀerence with respect
to SSA of ∆AGC−S S A = 5 × 105. On the temporal dimension
∆TC−TS A reaches a value of 16.8 × 105. Temporal convolution
in Yan et al. (2018) is implemented as a 2D convolution with
ﬁlter 1 × F, where F is the number of frames considered along
the time dimension, and it is usually set to 9, striding along
T = 300 frames. Thus, substituting it with a self-attention
mechanism results in a great complexity reduction, in addition
to better performance, as reported in the next sections.
Finally, in Figure 5b we also compare the entire stream ar-
chitectures, i.e., ST-GCN (Yan et al. (2018)) and 1s-AGCN
(Shi et al. (2019b)) with the proposed S-TR and T-TR streams
in terms of parameters. As expected from the considerations
above, the biggest improvement in parameters reduction is
achieved by substituting temporal convolution with TSA, i.e.,
in T-TR, with a ∆S T−GCN−−T−TR = 16.7 × 105. On the spatial
dimension the diﬀerence in terms of parameters is not as pro-
nounced as in temporal dimension, but it is still signiﬁcant, with
a ∆S T−GCN−−S −TR = 1.07×105 and ∆1s−AGCN−−S −TR = 5.0×105.
5.3. Experimental Settings
Using PyTorch (Paszke et al. (2019)) framework, we trained
our models for a total of 120 epochs with batch size 32 and
SGD as optimizer on NTU-60 and NTU-120, while on Kinet-
ics we trained our models for a total of 65 epochs, with batch
size 128. The learning rate is set to 0.1 at the beginning and
then reduced by a factor of 10 at the epochs {60, 90} and {45,
55} for NTU and Kinetics respectively. These schedulings have
been selected as they have been shown to provide good results
on ST-GCN networks used by Shi et al. (2019a). When us-
ing adaptive AGCN modules, we performed a linear warmup
Table 1. Comparison between the baseline and our self-attention modules
in terms of both performance (accuracy (%)) and eﬃciency (number of
parameters) on NTU-60 (X-View)
Method
GCN
TCN Params [×105] Top-1
ST-GCN
✓
✓
31.0
92.7
ST-GCN-fc
Afc
✓
26.5
93.7
1s-AGCN
Ak, Bk, Ck
✓
34.7
93.7
1s-AGCN w/o A
Bk, Ck
✓
33.1
93.4
S-TR
SSA
✓
30.7
94.0
T-TR
✓
TSA
17.6
93.6
of the learning rate during the ﬁrst epoch. Moreover, we pre-
processed the data with the same procedure used by Shi et al.
(2019b) and Shi et al. (2019a). In order to avoid overﬁtting, we
also used DropAttention, a particular dropout technique intro-
duced by Zehui et al. (2019) for regularizing attention weights
in Transformer networks, that consists in randomly dropping
columns of the attention logits matrix. In all of these experi-
ments, the number of heads for multi-head attention is set to 8,
and dq, dk, dv embedding dimensions to 0.25×Cout in each layer,
as done in Bello et al. (2019b). We did not perform grid search
on these parameters. As far as it concerns the model architec-
ture, each stream is composed by 9 layers, of channel dimen-
sion 64, 64, 64, 128, 128, 128, 256, 256 and 256. Batch normal-
ization is applied to input coordinates, a global average pooling
layer is applied before the softmax classiﬁer and each stream is
trained using the standard cross-entropy loss.
5.4. Results
To verify in a fair way the eﬀectiveness of our SSA and TSA
modules, we compare the S-TR and T-TR streams individually
against the ST-GCN (Yan et al. (2018)) baseline (whose results
are reported using our learning rate scheduling) and other mod-
els that modify its basic GCN module (see Table 1): (i) ST-GCN
(fc): we implemented a version of ST-GCN whose adjacency
matrix is composed of all ones (referred as Afc), to simulate the
fully-connected skeleton structure underlying our SSA module
and verify the superiority of self-attention over graph convolu-
tion on the spatial dimension; (ii) 1s-AGCN: Adaptive Graph
Convolutional Network (AGCN) (Shi et al. (2019b)) (see Sec-
tion 3.2), as it demonstrated in the literature to be more robust
than standard ST-GCN, in order to remark the robustness of
our SSA module over more recent methods; (iii) 1s-AGCN w/o
A: 1s-AGCN without the static adjacency matrix, to verify the
eﬀectiveness of our SSA over graph convolution in a similar
setting where all the links between joints are exclusively learnt.
All these methods use the same implementation of convolution
on the temporal dimension (TCN). We make a comparison both
in terms of model accuracy and number of parameters.
Regarding SSA, the performance of S-TR is superior to all
methods mentioned above, demonstrating that self-attention
can be used in place of graph convolution, increasing the net-
work performance while also decreasing the number of param-
eters. In fact, as it can be seen from Table 1, S-TR introduces
0.3×105 parameters less then ST-GCN and 4×105 less than 1s-
AGCN, with a performance increment w.r.t. all GCN conﬁgu-

8
Table 2. a) Comparison of S-TR and T-TR streams, and their combination
(ST-TR) on NTU-60, w and w/o bones. b) Ablations on diﬀerent model
conﬁgurations
Method
BonesX-SubX-View
S-TR
86.4
94.0
T-TR
86.0
93.6
T-TR-agcn
86.9
94.7
ST-TR
88.7
95.6
ST-TR-agcn
89.2
95.8
S-TR
✓
87.9
94.9
T-TR
✓
87.3
94.1
T-TR-agcn
✓
88.6
94.7
ST-TR
✓
89.9
96.1
ST-TR-agcn
✓
90.3
96.3
(a)
Ablation
X-View
S-TR-all-layers
93.3
T-TR-all-layers
91.3
ST-TR-all-layers
95.0
S-TR-augmented
94.5
T-TR-augmented
90.2
ST-TR-augmented 94.9
ST-TR-1s
93.3
ST-TR (k = 0)
95.4
ST-TR (k = 1)
95.6
ST-TR (k = 2)
95.7
(b)
rations. Similarly, regarding TSA, what emerges from the com-
parison between T-TR and the ST-GCN baseline adopting stan-
dard convolution, is that by using self-attention on the temporal
dimension the model is signiﬁcantly lighter (13.4 × 105 less pa-
rameters), and achieves an increment in accuracy of 0.9%.
In Table 2 we ﬁrst analyze the performance of the S-TR
stream, T-TR stream and their combination by using input data
consisting of joint information only. As it can be seen from
Table 2a, on NTU-60 the S-TR stream achieves slightly better
performance (+0.4%) than the T-TR stream, on both X-View
and X-Sub. This can be motivated by the fact that SSA in S-
TR operates on 25 joints only, while on temporal dimension
the number of correlations is proportional to the huge number
of frames. Again, as shown in Table 1a, applying self-attention
instead of convolution clearly beneﬁts the model on both spatial
and temporal dimensions. The combination of the two streams
achieves 88.7% of accuracy on X-Sub and 95.6% of accuracy
on X-View, outperforming the baseline ST-GCN and surpassing
other two-stream architectures (see Table 3).
As adding the diﬀerential of spatial coordinates (bones in-
formation) demonstrated to lead to better results in previous
works (Shi et al. (2019a); Simonyan and Zisserman (2014)),
we also studied our Transformer modules on combined joint
and bones information. For each node v1 = (x1, y1, z1) and
v2 = (x2, y2, z2), the bone connecting the two is calculated as
bv1,v2 = (x2 −x1, y2 −y1, z2 −z1). Both joint and bone infor-
mation are concatenated along the channel dimension, and then
fed to the network. At each layer, the dimension of the input
and output channels are doubled as done by Shi et al. (2019a)
and Simonyan and Zisserman (2014). Results are shown again
in Table 2a, where all previous conﬁgurations improve when
bones information is added as input. This highlights the ﬂexi-
bility of our method, which is capable of adapting to diﬀerent
input types and network conﬁgurations.
To further test its ﬂexibility, we also perform experiments in
which the GCN module is substituted by the AGCN adaptive
module on the temporal stream. As it can be seen from Table
2a, these conﬁgurations (T-TR-agcn) achieve better results than
the one using standard GCN on both X-Sub and X-View.
5.5. Eﬀect of Applying Self-Attention since Feature Extraction
We designed our streams to operate starting from high-level
features, rather than directly from coordinates, extracted us-
ing a sequence of residual GCN and TCN modules as re-
ported in Section 4.4. This set of experiments validates our
design choice.
In these experiments SSA (TSA) substitutes
GCN (TCN) on the S-TR (T-TR) stream, from the very ﬁrst
layer. The conﬁgurations reported in Table 2b (named S-TR-all-
layers), performs worse than the corresponding ones in Table
2a, while still outperforming the baseline ST-GCN (Shi et al.
(2019a)) (see Table 3). Indeed, self-attention has demonstrated
being more eﬃcient when incorporated in later stages of the
network (Carion et al. (2020); Huang et al. (2019); Wang et al.
(2018)). Notice that on T-TR, in order to deal with the great
number of frames in the very ﬁrst layers (T = 300), we divided
them into blocks within which SSA is applied, and then gradu-
ally reduce the number of blocks (dblock = 10 where Cout = 64,
dblock = 10 where Cout = 128, and a single block of dblock = T l
on layers l with Cout = 256).
The standard protocol used in recent works (Cheng et al.
(2020); Shi et al. (2019b)) that propose alternative modules for
ST-GCN based networks is to keep the original ST-GCN back-
bone architecture ﬁxed in terms of layers composition. Follow-
ing these works, we kept the three original feature extraction
layers for a fair comparison. We further conduct some targeted
experiments in which we vary their number k (Table 2b). As
it can be seen, performance is not sensitive to variations of k,
conﬁrming the eﬀectiveness of the proposed approach.
5.6. Eﬀect of Augmenting Convolution with Self-Attention
Motivated by the results in Bello et al. (2019b), we studied
the eﬀect of applying the proposed Transformer mechanism as
an augmentation procedure to the original ST-GCN modules. In
this conﬁguration, 0.75 × Cout features result from GCN (TCN)
and they are concatenated to the remaining 0.25 × Cout fea-
tures from SSA (TSA), a setup that has proven to be eﬀective in
Bello et al. (2019b). To compensate the reduction of attention
channels, wide attention is used, i.e., half of the attention chan-
nels are assigned to each head, then recombined together while
merging heads. The results are reported in Table 2b (referred
as ST-TR-augmented). Graph convolution is the one that ben-
eﬁts the most from SSA attention (S-TR-augmented, 94.5%),
to be compared with S-TR’s 94% in Table 2a. Nevertheless,
the lower number of output features assigned to self-attention
prevent temporal convolution improving on T-TR stream.
5.7. Eﬀect of combining SSA and TSA in a single stream
We tested the eﬃciency of the model when SSA and TSA are
combined in a single stream architecture (see Table 2b, referred
as S-TR-1s). In this conﬁguration, feature extraction is still per-
formed by the original GCN and TCN modules, while from the
4th layer on, each layer is composed by SSA followed by TSA,
i.e., ST-TR-1s(x) = TSA(SSA(x)).
We also tested this conﬁguration on NTU-60, obtaining an
accuracy of 93.3%, slightly lower than the 95.6% accuracy of
2s-ST-TR (see Table 1a, ST-TR). However, it should be noted

9
Table 3. Comparison with state-of-the-art accuracy (%) on NTU-60. Best
for both conﬁgurations w/ and w/o bones in bold.
NTU-60
Method
Bones
X-Sub
X-View
STA-LSTM (Song et al. (2017))
73.4
81.2
VA-LSTM (Zhang et al. (2017))
79.4
87.6
AGC-LSTM (Si et al. (2019))
89.2
95.0
ST-GCN (Yan et al. (2018))
81.5
88.3
AS-GCN (Li et al. (2019))
86.8
94.2
1s-AGCN (Shi et al. (2019b))
86.0
93.7
SAN (Cho et al. (2020))
87.2
92.7
1s Shift-GCN (Cheng et al. (2020))
87.8
95.1
ST-TR (Ours)
88.7
95.6
ST-TR-agcn (Ours)
89.2
95.8
2s-AGCN (Shi et al. (2019b))
✓
88.5
95.1
DGNN (Shi et al. (2019a))
✓
89.9
96.1
2s Shift-GCN (Cheng et al. (2020))
✓
89.7
96.0
4s Shift-GCN (Cheng et al. (2020))
✓
90.7
96.5
MS-G3D (Liu et al. (2020))
✓
91.5
96.2
ST-TR (Ours)
✓
89.9
96.1
ST-TR-agcn (Ours)
✓
90.3
96.3
Table 4. Comparison with state-of-the-art accuracy (%) of S-TR, T-TR,
and their combination (ST-TR) on NTU-120. Best for both conﬁgurations
w/ and w/o bones in bold.
NTU-120
Method
Bones X-Sub X-Set
ST-LSTM (Liu et al. (2016))
55.7
57.9
GCA-LSTM (Liu et al. (2017a))
61.2
63.3
RotClips+MTCNN (Ke et al. (2018))
62.2
61.8
Pose Evol. Map (Liu and Yuan (2018))
64.6
66.9
1s Shift-GCN (Cheng et al. (2020))
80.9
83.2
S-TR (Ours)
78.6
80.7
T-TR (Ours)
78.4
80.5
T-TR-agcn (Ours)
80.1
82.1
ST-TR (Ours)
81.9
84.1
ST-TR-agcn (Ours)
82.7
85.0
2s-AGCN (Shi et al. (2019b))
✓
82.9
84.9
2s Shift-GCN (Cheng et al. (2020))
✓
85.3
86.6
4s Shift-GCN (Cheng et al. (2020))
✓
85.9
87.6
MS-G3D (Liu et al. (2020))
✓
86.9
88.4
S-TR (Ours)
✓
81.0
83.6
T-TR (Ours)
✓
80.4
83.0
T-TR-agcn (Ours)
✓
82.7
84.9
ST-TR (Ours)
✓
84.3
86.7
ST-TR-agcn (Ours)
✓
85.1
87.1
that S-TR-1s presents 17.4 × 105 parameters, drastically reduc-
ing the complexity of the baseline ST-GCN which consists in
31 × 105 parameters. Moreover, it outperforms the ST-GCN
baseline by 0.6% using half of the parameters.
Table 5. Comparison with state-of-the-art accuracy (%) of S-TR, T-TR,
and their combination (ST-TR) on Kinetics. Best for both conﬁgurations
w/ and w/o bones in bold.
Kinetics
Method
Bones Top-1 Top-5
ST-GCN (Yan et al. (2018))
30.7
52.8
AS-GCN (Li et al. (2019))
34.8
56.5
SAN (Cho et al. (2020))
35.1
55.7
S-TR (Ours)
32.4
55.3
T-TR (Ours)
32.4
55.2
T-TR-agcn (Ours)
34.4
57.1
ST-TR (Ours)
34.5
57.6
ST-TR-agcn (Ours)
36.1
58.7
2s-AGCN (Shi et al. (2019b))
✓
36.1
58.7
DGNN (Shi et al. (2019a))
✓
36.9
59.6
MS-G3D (Liu et al. (2020))
✓
38.0
60.9
S-TR (Ours)
✓
35.4
57.9
T-TR (Ours)
✓
33.1 55.86
T-TR-agcn (Ours)
✓
34.7
56.4
ST-TR (Ours)
✓
37.0
59.7
ST-TR-agcn (Ours)
✓
38.0
60.5
6. Comparison with State-Of-The-Art Results
In addition to NTU-60, we compare our methods on NTU-
120 and Kinetics. For a fair comparison, we compare the ST-
TR conﬁgurations on methods trained on the same input data
(either with joint information only, or both joint and bones in-
formation). On NTU-60 (Table 3), the proposed ST-TR, when
using joint information only, outperforms all the state-of-the-
art models using the same type of information. In particular, it
outperforms SAN (Cho et al. (2020)), another method employ-
ing self-attention in skeleton-based action recognition, by up to
3%. When using bones information, the proposed transformer
based architecture outperforms 2s-AGCN (Shi et al. (2019b))
on both X-Sub and X-Set and our best conﬁguration making
use of the AGCN backbone (ST-TR-agcn) reaches performance
on-par with state-of-the-art. We further compare ST-TR against
4s Shift-GCN (Cheng et al. (2020)) which, in addition to the
joint and bones streams, also comprises two extra streams mak-
ing use of additional temporal information, and DGNN (Shi
et al. (2019a)), which also makes use of motion information
besides joint and bones. We report 4s Shift-GCN and DGNN
in Table 3-5 with a diﬀerent color to highlight the diﬀerence
in the input. ST-TR-agcn outperforms DGNN on both X-Sub
and X-View and crucially, although 4s Shift-GCN uses extra
input data and combines two additional streams, ST-TR-agcn
still achieves on-par results but with a simpler design.
On NTU-120 (Table 4), the model only based on joints out-
performs all state-of-the-art methods that use the same infor-
mation.
When adding bones, both ST-TR and ST-TR-agcn
outperform 2s-AGCN by up to 3% on both X-Sub and X-Set.
Moreover, ST-TR-agcn’s results are on-par with 2s Shift-GCN
(Cheng et al. (2020)) on X-Sub, while they improve on X-
Set.
Our network has only slightly lower performance than
MS-G3D (Liu et al. (2020)), which represents to date a very

10
strong baseline in skeleton-based action recognition. Consider-
ing that MS-G3D features a multi-path design with multi-scale
graph convolutions, the performance obtained by ST-TR is re-
markable given that the latter is based on a simpler backbone.
Finally, on Kinetics (Table 5), our model using only joints out-
performs the ST-GCN baseline by 5% and all previous methods
using only joint information. When bones information is added,
it outperforms both 2s-AGCN and DGNN, and achieves results
on-par with the very recent state-of-the-art method MS-G3D.
7. Qualitative Results
In Figure 6, we report some actions and the corresponding
Spatial Self-Attention maps. On the top we draw the skeleton
of the subjects, where the radius of the circles in correspon-
dence to each joint is proportional its relevance predicted by
the self-attention. The heatmaps on the bottom represent the at-
tention scores of the last layer; these are 25×25 matrices, where
each row and each column represents a body joint. An element
in position (i, j) represents the predicted correlation between
joint i and joint j in the same frame. As it can be observed,
depending on the action, diﬀerent parts of the body are acti-
vated. In Figure 7 are shown the same heatmaps at each layer.
In the ﬁrst layers, self-attention captures low-level correlations
between body joints, as highlighted by the sparsity of the ac-
tivations. While going deeper through the network, the global
importance of each node emerges instead, as highlighted by the
vertical lines corresponding to the most relevant joints.
8. Conclusions
In this paper we propose a novel approach that introduces
Transformer self-attention in skeleton activity recognition as
an alternative to graph convolution. Through extensive exper-
iments on NTU-60, NTU-120 and Kinetics, we demonstrated
that our Spatial Self-Attention module (SSA) can replace graph
convolution, enabling more ﬂexible and dynamic representa-
tions. Similarly, Temporal Self-Attention module (TSA) over-
comes the strict locality of standard convolution, enabling the
extraction of long-range dependencies across the action. More-
over, our ﬁnal Spatial-Temporal Transformer network (ST-TR)
achieves state-of-the-art performance on all dataset w.r.t. meth-
ods using same input joint information and stream setup, and
results on-par with state-of-the-art methods when bones infor-
mation is added. As conﬁgurations only involving self-attention
modules revealed to be sub-optimal, a possible future work is
to search for a uniﬁed Transformer architecture able to replace
graph convolution in a variety of tasks.
References
Aggarwal, J.K., Ryoo, M.S., 2011. Human activity analysis: A review. ACM
Computing Surveys (CSUR) 43, 1–43.
Bello, I., Zoph, B., Vaswani, A., Shlens, J., Le, Q.V., 2019a. Attention aug-
mented convolutional networks, in: Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision (ICCV).
Bello, I., Zoph, B., Vaswani, A., Shlens, J., Le, Q.V., 2019b. Attention aug-
mented convolutional networks. Proceedings of the IEEE International Con-
ference on Computer Vision , 3286–3295.
Reading
Hand Waving
Touch Chest
B
A
B
A
C
C
D
D
E
E
F
F
B
A
C
D
E
F
B
A
C
D
E
F
B
C
D
E
F
A
Fig. 6. Skeleton of the subjects performing the action (top) and the corre-
sponding SSA heatmaps (bottom). We display with red boxes the joints
which the network identiﬁes as the most relevant, while the corresponding
spatial self-attention scores are highlighted in the attention maps.
Layer 5
Layer 4
Layer 3
Layer 8
Layer 7
Layer 6
Fig. 7. Layer visualization. Each column represent the k-th layer, along
with the corresponding spatial self-attention heatmap on top, and the
skeleton of the subjects performing the action on the bottom.
Bronstein, M.M., Bruna, J., LeCun, Y., Szlam, A., Vandergheynst, P., 2017.
Geometric deep learning: going beyond euclidean data. IEEE Signal Pro-
cessing Magazine 34, 18–42.
Bruna, J., Zaremba, W., Szlam, A., Lecun, Y., 2014. Spectral networks and
locally connected networks on graphs. International Conference on Learning
Representations (ICLR2014), CBLS, April 2014 .
Cao, Z., Hidalgo Martinez, G., Simon, T., Wei, S., Sheikh, Y.A., 2019. Open-
pose: Realtime multi-person 2d pose estimation using part aﬃnity ﬁelds.
IEEE Transactions on Pattern Analysis and Machine Intelligence .
Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.,
2020. End-to-end object detection with transformers, in: European Confer-
ence on Computer Vision, Springer. pp. 213–229.
Cheng, K., Zhang, Y., He, X., Chen, W., Cheng, J., Lu, H., 2020. Skeleton-
based action recognition with shift graph convolutional network. Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition , 183–192.
Ch´eron, G., Laptev, I., Schmid, C., 2015. P-cnn: Pose-based cnn features for
action recognition. Proceedings of the IEEE international conference on
computer vision , 3218–3226.
Cho, S., Maqbool, M., Liu, F., Foroosh, H., 2020. Self-attention network for
skeleton-based human action recognition, in: Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision, pp. 635–644.
Deﬀerrard, M., Bresson, X., Vandergheynst, P., 2016. Convolutional neural
networks on graphs with fast localized spectral ﬁltering. Advances in neural
information processing systems , 3844–3852.
Ding, Z., Wang, P., Ogunbona, P.O., Li, W., 2017. Investigation of diﬀerent
skeleton features for cnn-based 3d action recognition. 2017 IEEE Interna-
tional Conference on Multimedia & Expo Workshops (ICMEW) , 617–622.
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Un-
terthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.,
2020. An image is worth 16x16 words: Transformers for image recognition
at scale. arXiv preprint arXiv:2010.11929 .
Du, Y., Wang, W., Wang, L., 2015. Hierarchical recurrent neural network for
skeleton based action recognition. Proceedings of the IEEE conference on
computer vision and pattern recognition , 1110–1118.
Gori, M., Monfardini, G., Scarselli, F., 2005. A new model for learning in
graph domains. Proceedings. 2005 IEEE International Joint Conference on
Neural Networks, 2005. 2, 729–734.
He, S., Liao, W., Tavakoli, H.R., Yang, M., Rosenhahn, B., Pugeault, N., 2020.

11
Image captioning through image transformer, in: Proceedings of the Asian
Conference on Computer Vision.
Henaﬀ, M., Bruna, J., LeCun, Y., 2015. Deep convolutional networks on graph-
structured data. arXiv preprint arXiv:1506.05163 .
Hu, J.F., Zheng, W.S., Lai, J., Zhang, J., 2015. Jointly learning heterogeneous
features for rgb-d activity recognition. Proceedings of the IEEE conference
on computer vision and pattern recognition , 5344–5352.
Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y., Liu, W., 2019.
Cc-
net: Criss-cross attention for semantic segmentation, in: Proceedings of the
IEEE/CVF International Conference on Computer Vision, pp. 603–612.
Hussein, M.E., Torki, M., Gowayyed, M.A., El-Saban, M., 2013. Human ac-
tion recognition using a temporal hierarchy of covariance descriptors on 3d
joint locations. Twenty-third international joint conference on artiﬁcial in-
telligence .
Ioﬀe, S., Szegedy, C., 2015.
Batch normalization:
Accelerating deep
network training by reducing internal covariate shift.
arXiv preprint
arXiv:1502.03167 .
Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan,
S., Viola, F., Green, T., Back, T., Natsev, P., et al., 2017. The kinetics human
action video dataset. arXiv preprint arXiv:1705.06950 .
Ke, Q., Bennamoun, M., An, S., Sohel, F., Boussaid, F., 2018. Learning clip
representations for skeleton-based 3d action recognition. IEEE Transactions
on Image Processing 27, 2842–2855.
Keselman, L., Iselin Woodﬁll, J., Grunnet-Jepsen, A., Bhowmik, A., 2017. Intel
realsense stereoscopic depth cameras, in: Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition Workshops, pp. 1–10.
Kipf, T.N., Welling, M., 2017. Semi-supervised classiﬁcation with graph con-
volutional networks. 5th International Conference on Learning Representa-
tions, ICLR .
Lee, S., Yu, Y., Kim, G., Breuel, T., Kautz, J., Song, Y., 2020. Parameter
eﬃcient multimodal transformers for video representation learning. arXiv
preprint arXiv:2012.04124 .
Lev, G., Sadeh, G., Klein, B., Wolf, L., 2016. Rnn ﬁsher vectors for action
recognition and image annotation. European Conference on Computer Vi-
sion , 833–850.
Li, B., Dai, Y., Cheng, X., Chen, H., Lin, Y., He, M., 2017. Skeleton based ac-
tion recognition using translation-scale invariant image mapping and multi-
scale deep cnn. ICMEW .
Li, M., Chen, S., Chen, X., Zhang, Y., Wang, Y., Tian, Q., 2019. Actional-
structural graph convolutional networks for skeleton-based action recogni-
tion, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 3595–3603.
Liu, J., Shahroudy, A., Perez, M.L., Wang, G., Duan, L.Y., Chichung, A.K.,
2019. Ntu rgb+d 120: A large-scale benchmark for 3d human activity un-
derstanding. IEEE transactions on pattern analysis and machine intelligence
.
Liu, J., Shahroudy, A., Xu, D., Wang, G., 2016. Spatio-temporal lstm with trust
gates for 3d human action recognition. European conference on computer
vision , 816–833.
Liu, J., Wang, G., Duan, L.Y., Abdiyeva, K., Kot, A.C., 2017a.
Skeleton-
based human action recognition with global context-aware attention lstm
networks. IEEE Transactions on Image Processing 27, 1586–1599.
Liu, J., Wang, G., Hu, P., Duan, L.Y., Kot, A.C., 2017b. Global context-aware
attention lstm networks for 3d action recognition. Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition , 1647–1656.
Liu, M., Liu, H., Chen, C., 2017c. Enhanced skeleton visualization for view
invariant human action recognition. Pattern Recognition 68, 346–362.
Liu, M., Yuan, J., 2018. Recognizing human actions as the evolution of pose
estimation maps. Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , 1159–1168.
Liu, Z., Zhang, H., Chen, Z., Wang, Z., Ouyang, W., 2020. Disentangling and
unifying graph convolutions for skeleton-based action recognition. Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition , 143–152.
Micheli, A., 2009. Neural network for graphs: A contextual constructive ap-
proach. IEEE Transactions on Neural Networks 20, 498–511.
Nguyen, T.Q., Salazar, J., 2019. Transformers without tears: Improving the
normalization of self-attention. arXiv preprint arXiv:1910.05895 .
Niepert, M., Ahmed, M., Kutzkov, K., 2016. Learning convolutional neural
networks for graphs. International conference on machine learning , 2014–
2023.
Oord, A.v.d., Kalchbrenner, N., Vinyals, O., Espeholt, L., Graves, A.,
Kavukcuoglu, K., 2016. Conditional image generation with pixelcnn de-
coders. arXiv preprint arXiv:1606.05328 .
Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., Tran, D.,
2018. Image transformer, in: International Conference on Machine Learn-
ing, PMLR. pp. 4055–4064.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
T., Lin, Z., Gimelshein, N., Antiga, L., et al., 2019. Pytorch: An imperative
style, high-performance deep learning library, pp. 8026–8037.
Ren, B., Liu, M., Ding, R., Liu, H., 2020. A survey on 3d skeleton-based action
recognition using learning method. arXiv preprint arXiv:2002.05907 .
Scarselli, F., Gori, M., Tsoi, A.C., Hagenbuchner, M., Monfardini, G., 2008.
The graph neural network model. IEEE Transactions on Neural Networks
20, 61–80.
Shahroudy, A., Liu, J., Ng, T.T., Wang, G., 2016. Ntu rgb+d: A large scale
dataset for 3d human activity analysis. Proceedings of the IEEE conference
on computer vision and pattern recognition , 1010–1019.
Shi, L., Zhang, Y., Cheng, J., Lu, H., 2019a. Skeleton-based action recognition
with directed graph neural networks. Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , 7912–7921.
Shi, L., Zhang, Y., Cheng, J., Lu, H., 2019b. Two-stream adaptive graph con-
volutional networks for skeleton-based action recognition. Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition , 12026–
12035.
Si, C., Chen, W., Wang, W., Wang, L., Tan, T., 2019.
An attention en-
hanced graph convolutional lstm network for skeleton-based action recog-
nition. Proceedings of the IEEE conference on computer vision and pattern
recognition , 1227–1236.
Simonyan, K., Zisserman, A., 2014. Two-stream convolutional networks for
action recognition in videos. Advances in neural information processing
systems , 568–576.
Song, S., Lan, C., Xing, J., Zeng, W., Liu, J., 2017. An end-to-end spatio-
temporal attention model for human action recognition from skeleton data.
Proceedings of the Thirty-First AAAI Conference on Artiﬁcial Intelligence
, 4263–4270.
Such, F.P., Sah, S., Dom´ınguez, M., Pillai, S., Zhang, C., Michael, A., Cahill,
N.D., Ptucha, R.W., 2017. Robust spatial ﬁltering with graph convolutional
neural networks. IEEE Journal of Selected Topics in Signal Processing .
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
Kaiser, Ł., Polosukhin, I., 2017. Attention is all you need. Advances in
neural information processing systems , 5998–6008.
Vemulapalli, R., Arrate, F., Chellappa, R., 2014. Human action recognition by
representing 3d skeletons as points in a lie group. Proceedings of the IEEE
conference on computer vision and pattern recognition , 588–595.
Wang, H., Wang, L., 2017. Modeling temporal dynamics and spatial conﬁgura-
tions of actions using two-stream recurrent neural networks. Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition , 499–
508.
Wang, L., Huynh, D.Q., Koniusz, P., 2019. A comparative review of recent
kinect-based action recognition algorithms. IEEE Transactions on Image
Processing 29, 15–28.
Wang, X., Girshick, R., Gupta, A., He, K., 2018. Non-local neural networks, in:
2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 7794–7803. doi:10.1109/CVPR.2018.00813.
Yan, S., Xiong, Y., Lin, D., 2018. Spatial temporal graph convolutional net-
works for skeleton-based action recognition. Thirty-second AAAI confer-
ence on artiﬁcial intelligence .
Zehui, L., Liu, P., Huang, L., Fu, J., Chen, J., Qiu, X., Huang, X., 2019.
Dropattention: A regularization method for fully-connected self-attention
networks. arXiv preprint arXiv:1907.11065 .
Zhang, H.B., Zhang, Y.X., Zhong, B., Lei, Q., Yang, L., Du, J.X., Chen, D.S.,
2019. A comprehensive survey of vision-based human action recognition
methods. Sensors 19, 1005.
Zhang, P., Lan, C., Xing, J., Zeng, W., Xue, J., Zheng, N., 2017. View adaptive
recurrent neural networks for high performance human action recognition
from skeleton data. Proceedings of the IEEE International Conference on
Computer Vision , 2117–2126.
Zhang, Z., 2012. Microsoft kinect sensor and its eﬀect. IEEE multimedia 19,
4–10.
Zhao, H., Jiang, L., Jia, J., Torr, P., Koltun, V., 2020. Point transformer. arXiv
preprint arXiv:2012.09164 .

