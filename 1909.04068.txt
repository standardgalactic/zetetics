Adversarial Robustness Against the Union of Multiple Perturbation Models
Pratyush Maini 1 Eric Wong 2 J. Zico Kolter 3 4
Abstract
Owing to the susceptibility of deep learning sys-
tems to adversarial attacks, there has been a great
deal of work in developing (both empirically and
certiﬁably) robust classiﬁers. While most work
has defended against a single type of attack, re-
cent work has looked at defending against mul-
tiple perturbation models using simple aggrega-
tions of multiple attacks. However, these methods
can be difﬁcult to tune, and can easily result in
imbalanced degrees of robustness to individual
perturbation models, resulting in a sub-optimal
worst-case loss over the union. In this work, we
develop a natural generalization of the standard
PGD-based procedure to incorporate multiple per-
turbation models into a single attack, by taking
the worst-case over all steepest descent directions.
This approach has the advantage of directly con-
verging upon a trade-off between different pertur-
bation models which minimizes the worst-case
performance over the union. With this approach,
we are able to train standard architectures which
are simultaneously robust against ℓ∞, ℓ2, and ℓ1
attacks, outperforming past approaches on the
MNIST and CIFAR10 datasets and achieving ad-
versarial accuracy of 47.0% against the union of
(ℓ∞, ℓ2, ℓ1) perturbations with radius = (0.03,
0.5, 12) on the latter, improving upon previous
approaches which achieve 40.6% accuracy.
1. Introduction
Machine learning algorithms have been shown to be suscep-
tible to adversarial examples (Szegedy et al., 2014) through
1Department of Computer Science and Engineering, IIT
Delhi, India 2Machine Learning Department, Carnegie Mel-
lon University, Pittsburgh, Pennsylvania, USA 3Computer Sci-
ence Department, Carnegie Mellon University, Pittsburgh, Penn-
sylvania, USA 4Bosch Center for Artiﬁcial Intelligence, Pitts-
burgh, Pennsylvania, USA. Correspondence to: Pratyush Maini
<pratyush.maini@gmail.com>.
Proceedings of the 37 th International Conference on Machine
Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by
the author(s).
the existence of data points which can be adversarially per-
turbed to be misclassiﬁed, but are “close enough” to the
original example to be imperceptible to the human eye.
Methods to generate adversarial examples, or “attacks”,
typically rely on gradient information, and most commonly
use variations of projected gradient descent (PGD) to max-
imize the loss within a small perturbation region, usually
referred to as the adversary’s perturbation model. A number
of heuristic defenses have been proposed to defend against
this phenomenon, e.g. distillation (Papernot et al., 2016) or
logit-pairing (Kannan et al., 2018). However, as time goes
by, the original robustness claims of these defenses typi-
cally don’t hold up to more advanced adversaries or more
thorough attacks (Carlini & Wagner, 2017; Engstrom et al.,
2018; Mosbach et al., 2018). One heuristic defense that
seems to have survived (to this day) is to use adversarial
training against a PGD adversary (Madry et al., 2018), and
remains quite popular due to its simplicity and apparent em-
pirical robustness. The method continues to perform well
in empirical benchmarks even when compared to recent
work in provable defenses, though it comes with no formal
guarantees.
While adversarial training has primarily been used to learn
models robust to a single perturbation model, some recent
work has looked at empirically defending against multiple
perturbation models simultaneously. Schott et al. (2019)
proposed a variational autoencoder based architecture to
learn an MNIST classiﬁer which was robust to multiple per-
turbation models, while Tram`er & Boneh (2019) proposed
simple aggregations of different adversaries for adversarial
training against multiple perturbation models.
While these approaches can achieve varying degrees of ro-
bustness to the considered adversarial perturbation models,
in practice it is quite difﬁcult to achieve an optimal trade-off
which minimizes the worst-case error in the union of pertur-
bation models. Rather, these approaches tend to converge
to suboptimal local minima, resulting in a model that is
highly robust to certain perturbation models while failing to
defend against others, and the robust performance can often
vary substantially across datasets. This results in poor and
unpredictable robust performance against the worst-case at-
tack, and indicates that the optimization procedure actually
fails to minimize the worst-case loss in the union of the
perturbation models.
arXiv:1909.04068v2  [cs.LG]  28 Jul 2020

Adversarial Robustness Against the Union of Multiple Perturbation Models
We believe that achieving robustness to multiple perturba-
tions is an essential step towards the eventual objective of
universal robustness and our work further motivates research
in this area. In this work, we make three main contributions
towards learning models which are adversarially robust to
multiple perturbation models. First, we demonstrate the in-
consistency of previous approaches across datasets, showing
that they converge to suboptimal tradeoffs which may not
actually minimize the robust objective of worst-case loss
over the combined perturbation model. Second, we propose
a modiﬁed PGD-based algorithm called “Multi Steepest
Descent” (MSD) for adversarial training, which naturally
incorporates different gradient-based perturbation models
into a single uniﬁed adversary to directly solve the inner
optimization problem of ﬁnding the worst-case loss. Third,
we show empirically that our approach improves upon past
work by ﬁnding trade-offs between the perturbation models
which signiﬁcantly improve the worst-case robust perfor-
mance against multiple perturbation models on both MNIST
and CIFAR10. Speciﬁcally, on MNIST, our model achieves
58.4% adversarial accuracy against the union of all three
attacks (ℓ∞, ℓ2, ℓ1) for ϵ = (0.3, 2.0, 10) respectively, sub-
stantially improving upon both the ABS models and also
simpler aggregations of multiple adversarial attacks, which
at best achieve 42.1% robust accuracy. Additionally, un-
like past work, we also train a CIFAR10 model against the
union of all three attacks (ℓ∞, ℓ2, ℓ1), which achieves 47.0%
adversarial accuracy for ϵ = (0.03, 0.5, 12) and improves
upon the simpler aggregations of multiple attacks which
can achieve 40.6% robust accuracy under this perturbation
model. In all cases, we ﬁnd that our approach is able to
consistently reduce the worst-case error under the uniﬁed
perturbation model. Code for reproducing all the results can
be found at: https://github.com/locuslab/robust union.
2. Related work
After their original introduction, one of the ﬁrst widely-
considered attacks against deep networks had been the Fast
Gradient Sign Method (Goodfellow et al., 2015), which
showed that a single, small step in the direction of the sign
of the gradient could sometimes fool machine learning clas-
siﬁers. While this worked to some degree, the Basic Itera-
tive Method (Kurakin et al., 2017) (now typically referred
to as the PGD attack) was signiﬁcantly more successful
at creating adversarial examples, and now lies at the core
of many papers. Since then, a number of improvements
and adaptations have been made to the base PGD algorithm
to overcome heuristic defenses and create stronger adver-
saries. Adversarial attacks were thought to be safe under
realistic transformations (Lu et al., 2017) until the attack
was augmented to be robust to them (Athalye et al., 2018b).
Adversarial examples generated using PGD on surrogate
models can transfer to black box models (Papernot et al.,
2017). Utilizing core optimization techniques such as mo-
mentum can greatly improve the attack success rate and
transferability, and was the winner of the NIPS 2017 compe-
tition on adversarial examples (Dong et al., 2018). Uesato
et al. (2018) showed that a number of ImageNet defenses
were not as robust as originally thought, and Athalye et al.
(2018a) defeated many of the heuristic defenses submitted
to ICLR 2018 shortly after the reviewing cycle ended, all
with stronger PGD variations.
Throughout this cycle of attack and defense, some defenses
were uncovered that remain robust to this day. The afore-
mentioned PGD attack, and the related defense known as
adversarial training with a PGD adversary (which incor-
porates PGD-attacked examples into the training process)
has so far remained empirically robust (Madry et al., 2018).
Veriﬁcation methods to certify robustness properties of net-
works were developed, utilizing techniques such as SMT
solvers (Katz et al., 2017), SDP relaxations (Raghunathan
et al., 2018b), and mixed-integer linear programming (Tjeng
et al., 2019), the last of which has recently been successfully
scaled to reasonably sized networks. Other work has folded
veriﬁcation into the training process to create provably ro-
bust networks (Wong & Kolter, 2018; Raghunathan et al.,
2018a), some of which have also been scaled to even larger
networks (Wong et al., 2018; Mirman et al., 2018; Gowal
et al., 2018). Although some of these could potentially be
extended to apply to multiple perturbations simultaneously,
most of these works have focused primarily on defending
against and verifying only a single type of adversarial per-
turbation at a time.
Last but most relevant to this work are adversarial defenses
that are robust against multiple types of attacks simultane-
ously. Schott et al. (2019) used multiple variational autoen-
coders to construct a complex architecture called analysis by
synthesis (ABS) for the MNIST dataset that is not as easily
attacked by ℓ∞, ℓ2, and ℓ0 adversaries. The ABS model has
two variations, one which is robust to ℓ0 and ℓ2 but not ℓ∞
attacks and other which is robust to ℓ∞and ℓ0 but not ℓ2
attacks. Similarly, Tram`er & Boneh (2019) study the the-
oretical and empirical trade-offs of adversarial robustness
in various settings when defending against aggregations of
multiple adversaries, however they ﬁnd that the ℓ∞pertur-
bation model interferes with other perturbation models on
MNIST (ℓ1 and ℓ2) and they study a rotation and translation
adversary instead of an ℓ2 adversary for CIFAR10. Croce &
Hein (2019) propose a provable adversarial defense against
all ℓp norms for p ≥1 using a regularization term. Finally,
while not studied as a defense, Kang et al. (2019) study
the transferability of adversarial robustness between models
trained against different perturbation models, while Jordan
et al. (2019) study combination attacks with low perceptual
distortion.

Adversarial Robustness Against the Union of Multiple Perturbation Models
Figure 1: A depiction of the steepest descent directions
for ℓ∞, ℓ2, and ℓ1 norms. The gradient is the black arrow,
and the α radius step sizes and their corresponding steepest
descent directions ℓ∞, ℓ2, and ℓ1 are shown in blue, red, and
green respectively.
3. Overview of adversarial training
Adversarial training is an approach to learn a classiﬁer which
minimizes the worst-case loss within some perturbation re-
gion (the perturbation model). Speciﬁcally, for some net-
work fθ parameterized by θ, loss function ℓ, and training
data {xi, yi}i=1...n, the robust optimization problem of min-
imizing the worst-case loss within ℓp norm-bounded pertur-
bations with radius ϵ is
min
θ
X
i
max
δ∈∆p,ϵ ℓ(fθ(xi + δ), yi),
(1)
where ∆p,ϵ = {δ : ∥δ∥p ≤ϵ} is the ℓp ball with radius ϵ
centered around the origin. To simplify the notation, we
will abbreviate ℓ(fθ(x + δ), y) = ℓ(x + δ; θ).
3.1. Solving the inner optimization problem
We ﬁrst look at solving the inner maximization problem,
namely
max
δ∈∆p,ϵ ℓ(x + δ; θ).
(2)
This is the problem addressed by the “attackers” in the space
of adversarial examples, hoping that the classiﬁer can be
tricked by the optimal perturbed image, x + δ⋆. Typical
solutions solve this problem by running a form of projected
gradient descent, which iteratively takes steps in the gradient
direction to increase the loss followed by a projection step
back onto the feasible region, the ℓp ball. Since the gradients
at the example points themselves (i.e., δ = 0) are typically
too small to make efﬁcient progress, more commonly used
is a variation called projected steepest descent.
Steepest descent
For some norm ∥· ∥p and step size α,
the direction of steepest descent on the loss function ℓfor a
perturbation δ is
vp(δ) = arg max
∥v∥p≤α
vT ∇ℓ(x + δ; θ).
(3)
Then, instead of taking gradient steps, steepest descent uses
the following iteration
δ(t+1) = δ(t) + vp(δ(t)).
(4)
In practice, the norm used in steepest descent is typically
taken to be the same ℓp norm used to deﬁne the perturbation
region ∆p,ϵ. However, depending on the norm used, the
direction of steepest descent can be quite different from
the actual gradient (Figure 1). Note that a single steep-
est descent step with respect to the ℓ∞norm reduces to
v∞(x) = α · sign(∇ℓ(x + δ; θ)), better known in the adver-
sarial examples literature as the Fast Gradient Sign Method
(Goodfellow et al., 2015).
Projections
The second component of projected steepest
descent for adversarial examples is to project iterates back
onto the ℓp ball around x. Speciﬁcally, projected steepest
descent performs the following iteration
δ(t+1) = P∆p,ϵ

δ(t) + vp(δ(t))

(5)
where P∆p,ϵ(δ) is the standard projection operator that ﬁnds
the perturbation δ′ ∈∆p,ϵ that is “closest” in Euclidean
space to the input δ, deﬁned as
P∆p,ϵ(δ) = arg min
δ′∈∆p,ϵ
∥δ −δ′∥2
2.
(6)
Visually, a depiction of this procedure (steepest descent
followed by a projection onto the perturbation region) for an
ℓ2 adversary can be found in Figure 1. If we instead project
the steepest descent directions with respect to the ℓ∞norm
onto the ℓ∞ball of allowable perturbations, the projected
steepest descent iteration reduces to
δ(t+1) = P∆∞,ϵ(δ(t) + v∞(δ(t)))
= clip
[−ϵ,ϵ]

δ(t) + α · sign(∇ℓ(x + δ(t); θ))

(7)
where clip[−ϵ,+ϵ] “clips” the input to lie within the range
[−ϵ, ϵ]. This is exactly the Basic Iterative Method used in
Kurakin et al. (2017), typically referred to in the literature
as an ℓ∞PGD adversary.
3.2. Solving the outer optimization problem
We next look at how to solve the outer optimization problem,
or the problem of learning the weights θ that minimize the
loss of our classiﬁer. While many approaches have been
proposed in the literature, we will focus on a heuristic called
adversarial training, which has generally worked well in
practice.

Adversarial Robustness Against the Union of Multiple Perturbation Models
Adversarial training
Although solving the min-max op-
timization problem may seem daunting, a classical result
known as Danskin’s theorem (Danskin, 1967) says that the
gradient of a maximization problem is equal to the gradient
of the objective evaluated at the optimum. For learning
models that minimize the robust optimization problem from
Equation (1), this means that
∇θ
 X
i
max
δ∈∆p,ϵ ℓ(xi + δ; θ)
!
=
X
i
∇θℓ(xi + δ∗(xi); θ)
(8)
where δ∗(xi) = arg maxδ∈∆p,ϵ ℓ(xi+δ; θ). In other words,
this means that in order to backpropagate through the robust
optimization problem, we can solve the inner maximization
and backpropagate through the solution. Adversarial train-
ing does this by empirically maximizing the inner problem
with a PGD adversary. Note that since the inner problem is
not solved exactly, Danskin’s theorem does not strictly ap-
ply. However, in practice, adversarial training does seem to
provide good empirical robustness, at least when evaluated
against the ℓp perturbation model it was trained against.
4. Adversarial training for multiple
perturbation models
We can now consider the core of this work, adversarial train-
ing procedures against multiple perturbation models. More
formally, let S represent a set of perturbation models, such
that p ∈S corresponds to the ℓp perturbation model ∆p,ϵ,
and let ∆S = S
p∈S ∆p,ϵ be the union of all perturbation
models in S. Note that the ϵ chosen for each ball is not
typically the same, but we still use the same notation ϵ for
simplicity, since the context will always make clear which
ℓp-ball we are talking about. Then, the generalization of
the robust optimization problem in Equation (1) to multiple
perturbation models is
min
θ
X
i
max
δ∈∆S ℓ(xi + δ; θ).
(9)
The key difference is in the inner maximization, where the
worst-case adversarial loss is now taken over multiple ℓp per-
turbation models. In order to perform adversarial training,
using the same motivational idea from Danskin’s theorem,
we can backpropagate through the inner maximization by
ﬁrst ﬁnding (empirically) the optimal perturbation,
δ∗= arg max
δ∈∆S
ℓ(x + δ; θ).
(10)
To ﬁnd the optimal perturbation over the union of perturba-
tion models, we begin by discussing simple generalizations
of standard adversarial training, which will use aggrega-
tions of PGD solutions for individual adversaries to approx-
imately solve the inner maximization over multiple adver-
saries. The computational complexity of these approaches
are a constant factor times than the complexity of standard
adversarial training, where the constant is equal to the num-
ber of adversaries. We will focus the exposition primarily
on adversarial training based approaches as these are most
related to our proposed method, and we refer the reader
to Schott et al. (2019) for more detail on the analysis by
synthesis approach.
4.1. Simple combinations of multiple perturbations
First, we study two simple approaches to generalizing ad-
versarial training to multiple perturbation models, which
can learn robust models and do not rely on complicated
architectures. While these methods work to some degree,
we later ﬁnd empirically that these methods do not necessar-
ily minimize the worst-case performance, can converge to
unexpected tradeoffs between multiple perturbation models,
and can have varying dataset-dependent performance.
MAX: Worst-case perturbation
One way to generalize
adversarial training to multiple perturbation models is to
use each perturbation model independently, and train on
the adversarial perturbation that achieved the maximum
loss. Speciﬁcally, for each adversary p ∈S, we solve the
innermost maximization with an ℓp PGD adversary to get
an approximate worst-case perturbation δp,
δp = arg max
δ∈∆p,ϵ
ℓ(x + δ; θ),
(11)
and then approximate the maximum over all adversaries as
δ∗≈arg max
δp
ℓ(x + δp; θ).
(12)
When |S| = 1, then this reduces to standard adversarial
training. Note that if each PGD adversary solved their sub-
problem from Equation (11) exactly, then this is the optimal
perturbation δ⋆. This method corresponds to the “max”
strategy from Tram`er & Boneh (2019).
AVG: Augmentation of all perturbations
Another way
to generalize adversarial training is to train on all the adver-
sarial perturbations for all p ∈S to form a larger adversarial
dataset. Speciﬁcally, instead of solving the robust problem
for multiple adversaries in Equation (9), we instead solve
min
θ
X
i
X
p∈S
max
δ∈∆p,ϵ ℓ(xi + δ; θ)
(13)
by using individual ℓp PGD adversaries to approximate
the inner maximization for each perturbation model. This
reduces to standard adversarial training when |S| = 1 and
corresponds to the “avg” strategy from Tram`er & Boneh
(2019).
While these methods work to some degree, (which is shown
later in Section 5), both of these approaches solve the inner

Adversarial Robustness Against the Union of Multiple Perturbation Models
Algorithm 1 Multi steepest descent for learning classiﬁers
that are simultaneously robust to ℓp attacks for p ∈S
Input: classiﬁer fθ, data x, labels y
Parameters: ϵp, αp for p ∈S, maximum iterations T,
loss function ℓ
δ(0) = 0
for t = 0 . . . T −1 do
for p ∈S do
δ(t+1)
p
= P∆p,ϵ(δ(t) + vp(δ(t)))
end for
δ(t+1) = arg maxδ(t+1)
p
ℓ(fθ(x + δ(t+1)
p
), y)
end for
return δ(T )
maximization problem independently for each adversary.
Consequently, each individual PGD adversary is myopic to
its own perturbation model and does not take advantage of
the fact that the perturbation region is enlarged by other per-
turbation models. To leverage the full information provided
by the union of perturbation regions, we propose a modi-
ﬁcation to standard adversarial training, which combines
information from all considered perturbation models into a
single PGD adversary that is potentially stronger than the
combination of independent adversaries.
4.2. Multi Steepest Descent
To create a PGD adversary with full knowledge of the per-
turbation region, we propose an algorithm that incorporates
the different perturbation models within each step of pro-
jected steepest descent. Rather than generating adversarial
examples for each perturbation model with separate PGD
adversaries, the core idea is to create a single adversarial
perturbation by simultaneously maximizing the worst-case
loss over all perturbation models at each projected steep-
est descent step. We call our method multi steepest descent
(MSD), which can be summarized as the following iteration:
δ(t+1)
p
= P∆p,ϵ(δ(t) + vp(δ(t))) for p ∈S
δ(t+1) = arg max
δ(t+1)
p
ℓ(x + δ(t+1)
p
)
(14)
The key difference here is that at each iteration of MSD,
we choose a projected steepest descent direction that maxi-
mizes the loss over all attack models p ∈S, whereas stan-
dard adversarial training and the simpler approaches use
comparatively myopic PGD subroutines that only use one
perturbation model at a time. The full algorithm is in Al-
gorithm 1, and can be used as a drop in replacement for
standard PGD adversaries to learn robust classiﬁers with
adversarial training. We direct the reader to Appendix A for
a complete description of steepest descent directions and
projection operators for ℓ∞, ℓ2, and ℓ1 norms.1
5. Results
In this section, we present experimental results on using gen-
eralizations of adversarial training to achieve simultaneous
robustness to ℓ∞, ℓ2, and ℓ1 perturbations on the MNIST
and CIFAR10 datasets. Our primary goal is to show that
adversarial training can be used to directly minimize the
worst-case loss over the union of perturbation models to
achieve competitive results by avoiding any trade-off that
biases one particular perturbation model at the cost of the
others. Our results improve upon the state-of-the-art in three
key ways. First, we can continue to use simple, standard
architectures for image classiﬁers, without relying on com-
plex architectures or input binarization as done by Schott
et al. (2019). Second, our method is able to learn a single
model (on both MNIST and CIFAR10) which optimizes the
worst-case performance over the union of all three perturba-
tion models, whereas previous approaches are only robust
against two at a time, or have performance which is dataset
dependent. Finally, we provide the ﬁrst CIFAR10 model
trained to be simultaneously robust against ℓ∞, ℓ2, and ℓ1
adversaries, in comparison to previous work which trained
a model robust to ℓ∞, ℓ1, and rotation/translation attacks
(Tram`er & Boneh, 2019).
We train models using MSD, MAX and AVG approaches
for both MNIST and CIFAR10 datasets. We additionally
train models against individual PGD adversaries to measure
the changes and tradeoffs in universal robustness. Since
the analysis by synthesis model is not scalable, we do not
include it in our experimentation for CIFAR10. We perform
an extensive evaluation of these models with a broad suite of
both gradient and non-gradient based attacks using Foolbox2
(the same attacks used by Schott et al. (2019)), and also
incorporate all the PGD-based adversaries discussed in this
paper. All aggregate statistics that combine multiple attacks
compute the worst-case error rate over all attacks for each
example, in order to reﬂect the worst-case loss over the
combined perturbation model.
Summaries of these results at speciﬁc thresholds can be
found in Tables 1 and 2, where B-ABS and ABS refer to
binarized and non-binarized versions of the analysis by
synthesis models from Schott et al. (2019), Pp refers to
a model trained against a PGD adversary with respect to
the p-norm, MAX and AVG refer to models trained using
the worst-case and data augmentation generalizations of
adversarial training, and MSD refers to models trained using
1The pure ℓ1 steepest descent step is inefﬁcient since it only
updates one coordinate at a time. It can be improved by taking
steps on multiple coordinates, similar to that used in Tram`er &
Boneh (2019), and is also explained in Appendix A.
2https://github.com/bethgelab/foolbox (Rauber et al., 2017)

Adversarial Robustness Against the Union of Multiple Perturbation Models
multi steepest descent. Full tables containing the complete
breakdown of these numbers over all individual attacks
used in the evaluation are in Appendix B. We report the
results against individual attacks and perturbation models
for completeness, however we note that the original goal
and motivation of all these algorithms is to minimize the
robust optimization objective from Equation (9). While
there may be different implicit tradeoffs between individual
perturbation models that can be difﬁcult to compare, the
robust optimization objective, or the performance against
the union of all attacks, provides a single common metric
that all approaches are optimizing.
5.1. Experimental setup
Architectures and hyperparameters
For MNIST, we
use a four layer convolutional network with two convo-
lutional layers consisting of 32 and 64 5 × 5 ﬁlters and 2
units of padding, followed by a fully connected layer with
1024 hidden units, where both convolutional layers are fol-
lowed by 2 × 2 Max Pooling layers and ReLU activations
(this is the same architecture used by Madry et al. (2018)).
This is in contrast to past work on MNIST, which relied
on per-class variational autoencoders to achieve robustness
against multiple perturbation models (Schott et al., 2019),
which was also not easily scalable to larger datasets. Since
our methods have the same computational complexity as
standard adversarial training, they also easily apply to stan-
dard CIFAR10 architectures, and in this paper we use the
well known pre-activation version of the ResNet18 architec-
ture consisting of nine residual units with two convolutional
layers each (He et al., 2016).
A complete description of the hyperparameters used is in
Appendix C. All reported ϵ are for images scaled to be be-
tween the range [0, 1]. All experiments were run on modest
amounts of GPU hardware (e.g. a single 1080ti).
Attacks used for evaluation
To evaluate the model, we
incorporate the attacks from Schott et al. (2019) along with
our PGD based adversaries, and provide a short descrip-
tion of the same here. Note that we exclude attacks based
on gradient estimation, since the gradient for the standard
architectures used here are readily available.
For ℓ∞attacks, although we ﬁnd the ℓ∞PGD adversary
to be quite effective, for completeness, we additionally use
the Foolbox implementations of Fast Gradient Sign Method
(Goodfellow et al., 2015), PGD attack (Madry et al., 2018),
and Momentum Iterative Method (Dong et al., 2018).
For ℓ2 attacks, in addition to the ℓ2 PGD adversary, we use
the Foolbox implementations of the same PGD adversary,
the Gaussian noise attack (Rauber et al., 2017), the boundary
attack (Brendel et al., 2017), DeepFool (Moosavi-Dezfooli
et al., 2016), the pointwise attack (Schott et al., 2019), DDN
based attack (Rony et al., 2018), and C&W attack (Carlini
& Wagner, 2017).
For ℓ1 attacks, we use both the ℓ1 PGD adversary as well
as additional Foolbox implementations of ℓ0 attacks at the
same radius, namely the salt & pepper attack (Rauber et al.,
2017) and the pointwise attack (Schott et al., 2019). Note
that an ℓ1 adversary with radius ϵ is strictly stronger than
an ℓ0 adversary with the same radius, and so we choose to
explicitly defend against ℓ1 perturbations instead of the ℓ0
perturbations considered by Schott et al. (2019).
We make 10 random restarts for each of the results men-
tioned hereon for both MNIST and CIFAR10 3. We en-
courage future work in this area to incorporate the same,
since the success of all attacks, specially decision based or
gradient free ones, is observed to increase signiﬁcantly over
restarts.
5.2. MNIST
We ﬁrst present results on the MNIST dataset, which are
summarized in Table 1 (a more detailed breakdown over
each individual attack is in Appendix B.1). Complete robust-
ness curves over a range of epsilons over each perturbation
model can be found in Figure 2. Although we reproduce
the simpler approaches here, a more detailed discussion of
how these results compare with those presented by Tram`er
& Boneh (2019) can be found in Appendix D.
Suboptimal trade-offs
While considered an “easy”
dataset, we ﬁrst note that most of the previous approaches
for multiple perturbation models on MNIST are only able to
defend against two out of three perturbation models at a time,
resulting in a suboptimal trade-off between different pertur-
bation models which has poor overall performance against
the worst-case attack in the combined perturbation model.
Despite relying on a signiﬁcantly more complex architec-
ture, the B-ABS model is weak against ℓ2 attacks while the
ABS model is weak against ℓ∞attacks. Meanwhile, the
AVG model is weak against strong ℓ1 decision-based attacks.
The MAX and MSD models achieve relatively better trade-
offs, with the MSD model performing the best with a robust
accuracy rate of 58.4% against the union of (ℓ∞, ℓ2, ℓ1)
perturbations with radius ϵ = (0.3, 2.0, 10), which is over a
15% improvement in comparison to the MAX model.
Gradient Masking in MNIST models
We ﬁnd that even
though models trained via the MAX and AVG approaches
provide reasonable robustness against ﬁrst-order attacks
3All attacks were run on a subset of the ﬁrst 1000 test examples
with 10 random restarts, with the exception of Boundary Attack,
which by default makes 25 trials per iteration and DDN based At-
tack which does not beneﬁt from the same owing to a deterministic
initialization of δ.

Adversarial Robustness Against the Union of Multiple Perturbation Models
Table 1: Summary of adversarial accuracy results for MNIST (higher is better)
P∞
P2
P1
B-ABS4
ABS4
MAX
AVG
MSD
Clean Accuracy
99.1%
99.2%
99.3%
99%
99%
98.6%
99.1%
98.3%
ℓ∞attacks (ϵ = 0.3)
90.3%
0.4%
0.0%
77%
8%
51.0%
65.2%
62.7%
ℓ2 attacks (ϵ = 2.0)
13.6%
69.2%
38.5%
39%
80%
61.9%
60.1%
67.9%
ℓ1 attacks (ϵ = 10)
4.2%
43.4%
70.0%
82%
78%
52.6%
39.2%
65.0%
All Attacks
3.7%
0.4%
0.0%
39%
8%
42.1%
34.9%
58.4%
Figure 2: Robustness curves showing the adversarial accuracy for the MNIST model trained with MSD, AVG, MAX against
ℓ∞(left), ℓ2 (middle), and ℓ1 (right) perturbation models over a range of epsilon.
Figure 3: A view of each of the (5x5) learned ﬁlters of the
ﬁrst layer of a CNN robust to ℓ∞attacks. The singular
sharp values are characteristic features of models robust to
ℓ∞attacks.
(breakdown of attacks in Appendix B.1), they can be vulner-
able to gradient-free attacks like the Pointwise Attack and
Boundary Attack. This indicates the presence of masked
gradients that prevent ﬁrst-order adversaries from ﬁnding
the optimal steepest descent direction (Athalye et al., 2018a),
similar to how ℓ∞trained models are weak against decision-
based attacks in other norms as also observed by Schott
et al. (2019) and Tram`er & Boneh (2019). We analyze the
4Results are reported directly from Schott et al. (2019), which
used epsilon balls of radii (0.3,1.5,12) for (ℓ∞, ℓ2, ℓ0) adversaries.
They used an ℓ0 perturbation region of a higher radius and evalu-
ated against ℓ0 attacks. So the reported number is a near estimate
of the ℓ1 adversarial accuracy. They used an ℓ2 perturbation model
of a lower radius = 1.5. Further, they do not perform attack restarts
and the adversarial accuracy against all attacks is an upper bound
learned weights of the ﬁrst layer ﬁlters of the CNN models
trained on the MNIST, and observe a strong correlation of
the presence of thresholding ﬁlters (Figure 3) with the sus-
ceptibility to decision-based ℓ1 and ℓ2 adversaries. Further
analysis of the learned ﬁlter weights for all the models can
be found in Appendix E, where we observe that by reducing
the number of thresholding ﬁlters, the MSD model is able to
perform better against decision based adversaries, whereas
learning ﬁlter patterns similar to that of an ℓ∞robust model
correlates with susceptibility of MAX and AVG training
methods to gradient-free adversaries.
Unreliable training of MAX and AVG
To give the MAX
and AVG approaches the best chance at succeeding, we
searched over a wide range of hyperparameters (which are
described in Appendix C.2). However, we frequently ob-
serve that these training runs result in masked gradients as
described earlier, and are seemingly unable to balance the
right trade-off between multiple attacks. In Figure 4, we
show the sensitivity of different training methods to training
time hyperparameter choices. The worst case accuracy is
evaluated using the worst case over three gradient based
attacks (PGD attacks in ℓ∞, ℓ2, ℓ1 space) and one gradient-
free attack (pointwise attack in ℓ1 space). The MAX training
method achieves greater than 40% robust accuracy in only
10% of all the hyperparameter conﬁgurations tried. The sen-
sitivity was even higher for the AVG method on the MNIST
based on the reported accuracies for individual perturbation models.
Finally, all ABS results were computed using numerical gradient
estimation, since gradients are not readily available.

Adversarial Robustness Against the Union of Multiple Perturbation Models
Table 2: Summary of adversarial accuracy results for CIFAR10 (higher is better)
P∞
P2
P1
MAX
AVG
MSD
Clean accuracy
83.3%
90.2%
73.3%
81.0%
84.6%
81.1%
ℓ∞attacks (ϵ = 0.03)
50.7%
28.3%
0.2%
44.9%
42.5%
48.0%
ℓ2 attacks (ϵ = 0.5)
57.3%
61.6%
0.0%
61.7%
65.0%
64.3%
ℓ1 attacks (ϵ = 12)
16.0%
46.6%
7.9%
39.4%
54.0%
53.0%
All attacks
15.6%
27.5%
0.0%
34.9%
40.6%
47.0%
>10
>20
>30
>40
>50
Worst Case Accuracy (in %)
0
20
40
60
80
100
% of successful runs
MSD
MAX
AVG
Figure 4: Among all the models trained using the MSD,
MAX and AVG methods during our hyperparameter search,
we plot the percentage of models for each method that
achieve robust accuracies greater than a particular threshold
(against the union of ℓ∞, ℓ1, ℓ2 attacks).
dataset. Also, note that nearly all models attain greater than
50% robust accuracy when only attacked by gradient-based
adversaries, and the performance drop is largely attributed
to the gradient-free attack.
However, MSD is comparatively much easier to tune and
achieves greater than 50% accuracy in around 40% of the
runs. Moreover, we ﬁnd that MSD offers a natural way to
counteract any unwanted bias towards one perturbation type
by adjusting the relative step-sizes of individual descent
directions, whereas doing the same for the MAX and AVG
approaches does not help.
We note that in order to train the MAX and AVG approaches
reasonably well on the MNIST dataset (Table 1), we had
to set the radius of the ℓ1 ball to 12 for AVG and increase
the number of PGD ℓ1 attack restarts during training for
MAX. These methods help make the PGD ℓ1 attack rel-
atively stronger by changing the perturbation model, and
re-aligns the optimal trade-offs when the training process
is unable to naturally capture them. We observe that small
starts, as employed by Tram`er & Boneh (2019) to make their
models work better, may have a similar effect of re-aligning
the strength of various perturbation models. Rather than
“ﬁxing” the balance between different perturbation models
by changing the individual attacks used for training, MSD
is able to achieve the right trade-off by directly balancing
them, leading to greater reliability and consistency when
compared to the MAX and AVG approaches.
5.3. CIFAR10
Next, we present results on the CIFAR10 dataset, which
are summarized in Table 2 (a more detailed breakdown
over each individual attack is in Appendix B.2).
Our
MSD approach reaches the best performance against
the union of attacks, and achieves 47.0% (individu-
ally 48.0%, 64.3%, 53.0%) adversarial accuracy against
the union of (ℓ∞, ℓ2, ℓ1) perturbations of size ϵ
=
(0.03, 0.5, 12). We note that the P1 model trained against
an ℓ1 PGD adversary is not very robust when evaluated
against decision-based attacks, even though it can defend
reasonably well against the ℓ1 PGD attack in isolation (Ta-
ble 4 in Appendix B.2). Complete robustness curves over
a range of epsilons over each perturbation model can be
found in Figure 5. The speciﬁc heuristic adjustments made
to obtain the best-performing MAX and AVG models are de-
tailed in Appendix C.2. Although we reproduce the simple
adversarial training approaches here, a direct comparison
of how these results compare to those reported by (Tram`er
& Boneh, 2019) can be found in Appendix D. Furthermore,
while adversarial defenses are generally not intended to
be robust to attacks outside of the perturbation model, we
show some experiments exploring this aspect in Appendix
F, namely the performance on the CIFAR10-C dataset (CI-
FAR10 with common corruptions) as well as exploring what
happens when one defends against only two adversaries and
evaluates on a third, unseen adversary.
Dataset variability
In addition to converging to subop-
timal trade-offs between different adversaries as seen on
MNIST, we ﬁnd that the performance of simpler versions of
adversarial training for multiple perturbations can also vary
signiﬁcantly based on the dataset. While the MAX approach
performed better than AVG on MNIST, in the CIFAR10
setting we ﬁnd that these roles are swapped: the MAX ap-
proach converged to a suboptimal local minima which is
5.7% less robust against the union of perturbation models
than AVG. Once again, this highlights the inconsistency of
the simpler generalizations of adversarial training: depend-
ing on the problem setting, they may converge to suboptimal

Adversarial Robustness Against the Union of Multiple Perturbation Models
Figure 5: Robustness curves showing the adversarial accuracy for the CIFAR10 model trained with MSD, AVG, MAX
against ℓ∞(left), ℓ2 (middle), and ℓ1 (right) perturbation models over a range of epsilon.
local optima which do not minimize the robust optimization
objective from Equation (9). On the other hand, in both
problem settings, we ﬁnd MSD consistently converges to
a local optimum which is better at minimizing the worst-
case loss in the union of the perturbation models, achieving
47.0% robust accuracy, improving upon the best-performing
simpler method of AVG by 6.4%.
6. Conclusion
In this paper, we showed that previous approaches aimed
towards learning models which are adversarially robust to
multiple perturbation models can be highly variable (across
parameters and datasets), and difﬁcult to tune, thereby con-
verging to suboptimal local minima with trade-offs which do
not defend against the union of multiple perturbation models.
On the other hand, by incorporating the different perturba-
tion models directly into the direction of steepest descent,
our proposed approach of MSD consistently outperforms
past approaches across both MNIST and CIFAR10. The ap-
proach inherits the scalability and generality of adversarial
training, without relying on speciﬁc complex architectures,
and is able to better accomplish the robust optimization ob-
jective. We recommend using MSD to directly minimize
the worst-case performance among multiple perturbation
models.
Acknowledgements
Eric Wong was funded by support from the Bosch Center
for AI, under contract 0087016732PCR, and a fellowship
from the Siebel Scholars Foundation. Pratyush Maini was
supported by a fellowship from the Khorana Program for
Scholars, aided jointly by the Department of Science &
Technology Govt. of India and the US Department of State.
References
Athalye, A., Carlini, N., and Wagner, D. Obfuscated gra-
dients give a false sense of security: Circumventing de-
fenses to adversarial examples. In Proceedings of the 35th
International Conference on Machine Learning, ICML
2018, July 2018a. URL https://arxiv.org/abs/
1802.00420.
Athalye, A., Engstrom, L., Ilyas, A., and Kwok, K. Syn-
thesizing robust adversarial examples.
In Dy, J. and
Krause, A. (eds.), Proceedings of the 35th International
Conference on Machine Learning, volume 80 of Pro-
ceedings of Machine Learning Research, pp. 284–293,
Stockholmsmssan, Stockholm Sweden, 10–15 Jul 2018b.
PMLR. URL http://proceedings.mlr.press/
v80/athalye18b.html.
Brendel, W., Rauber, J., and Bethge, M. Decision-based ad-
versarial attacks: Reliable attacks against black-box ma-
chine learning models. arXiv preprint arXiv:1712.04248,
2017.
Carlini, N. and Wagner, D. Towards evaluating the robust-
ness of neural networks. In Security and Privacy (SP),
2017 IEEE Symposium on, pp. 39–57. IEEE, 2017.
Chen, P.-Y., Sharma, Y., Zhang, H., Yi, J., and Hsieh, C.-
J. Ead: Elastic-net attacks to deep neural networks via
adversarial examples, 2017.
Croce, F. and Hein, M.
Provable robustness against
all adversarial lp-perturbations for p≥1.
CoRR,
abs/1905.11213, 2019.
URL http://arxiv.org/
abs/1905.11213.
Danskin, J. M. The theory of max-min and its application
to weapons allocation problems, volume 5. Springer
Science & Business Media, 1967.
Dong, Y., Liao, F., Pang, T., Su, H., Zhu, J., Hu, X., and
Li, J. Boosting adversarial attacks with momentum. In
The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Duchi, J., Shalev-Shwartz, S., Singer, Y., and Chandra, T.
Efﬁcient projections onto the l1-ball for learning in high

Adversarial Robustness Against the Union of Multiple Perturbation Models
dimensions. In Proceedings of the 25th International Con-
ference on Machine Learning, ICML ’08, pp. 272–279,
New York, NY, USA, 2008. ACM. ISBN 978-1-60558-
205-4. doi: 10.1145/1390156.1390191. URL http:
//doi.acm.org/10.1145/1390156.1390191.
Engstrom, L., Ilyas, A., and Athalye, A. Evaluating and
understanding the robustness of adversarial logit pairing.
arXiv preprint arXiv:1807.10272, 2018.
Goodfellow, I., Shlens, J., and Szegedy, C. Explaining
and harnessing adversarial examples. In International
Conference on Learning Representations, 2015. URL
http://arxiv.org/abs/1412.6572.
Gowal, S., Dvijotham, K., Stanforth, R., Bunel, R., Qin,
C., Uesato, J., Arandjelovic, R., Mann, T. A., and
Kohli, P. On the effectiveness of interval bound prop-
agation for training veriﬁably robust models.
CoRR,
abs/1810.12715, 2018.
URL http://arxiv.org/
abs/1810.12715.
He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings
in deep residual networks. In European conference on
computer vision, pp. 630–645. Springer, 2016.
Jordan, M., Manoj, N., Goel, S., and Dimakis, A. G. Quanti-
fying perceptual distortion of adversarial examples. arXiv
preprint arXiv:1902.08265, 2019.
Kang, D., Sun, Y., Brown, T., Hendrycks, D., and Steinhardt,
J. Transfer of adversarial robustness between perturbation
types. arXiv preprint arXiv:1905.01034, 2019.
Kannan, H., Kurakin, A., and Goodfellow, I. J. Adversarial
logit pairing. CoRR, abs/1803.06373, 2018. URL http:
//arxiv.org/abs/1803.06373.
Katz, G., Barrett, C., Dill, D., Julian, K., and Kochenderfer,
M. Reluplex: An efﬁcient smt solver for verifying deep
neural networks. arXiv preprint arXiv:1702.01135, 2017.
Kurakin, A., Goodfellow, I., and Bengio, S. Adversarial
examples in the physical world. ICLR Workshop, 2017.
URL https://arxiv.org/abs/1607.02533.
Lu, J., Sibai, H., Fabry, E., and Forsyth, D. No need to
worry about adversarial examples in object detection in
autonomous vehicles. arXiv preprint arXiv:1707.03501,
2017.
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and
Vladu, A.
Towards deep learning models resistant
to adversarial attacks.
In International Conference
on Learning Representations, 2018. URL https://
openreview.net/forum?id=rJzIBfZAb.
Mirman,
M.,
Gehr,
T.,
and Vechev,
M.
Differ-
entiable
abstract
interpretation
for
provably
ro-
bust neural networks.
In International Confer-
ence on Machine Learning (ICML), 2018.
URL
https://www.icml.cc/Conferences/2018/
Schedule?showEvent=2477.
Moosavi-Dezfooli, S.-M., Fawzi, A., and Frossard, P. Deep-
fool: a simple and accurate method to fool deep neural
networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 2574–2582,
2016.
Mosbach, M., Andriushchenko, M., Trost, T., Hein, M.,
and Klakow, D. Logit pairing methods can fool gradient-
based attacks. arXiv preprint arXiv:1810.12042, 2018.
Papernot, N., McDaniel, P., Wu, X., Jha, S., and Swami,
A. Distillation as a defense to adversarial perturbations
against deep neural networks. In Security and Privacy
(SP), 2016 IEEE Symposium on, pp. 582–597. IEEE,
2016.
Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Ce-
lik, Z. B., and Swami, A. Practical black-box attacks
against machine learning. In Proceedings of the 2017
ACM on Asia Conference on Computer and Commu-
nications Security, ASIA CCS ’17, pp. 506–519, New
York, NY, USA, 2017. ACM. ISBN 978-1-4503-4944-
4.
doi: 10.1145/3052973.3053009.
URL http://
doi.acm.org/10.1145/3052973.3053009.
Raghunathan, A., Steinhardt, J., and Liang, P.
Cer-
tiﬁed defenses against adversarial examples.
In
International Conference on Learning Representa-
tions, 2018a.
URL https://openreview.net/
forum?id=Bys4ob-Rb.
Raghunathan, A., Steinhardt, J., and Liang, P. S. Semideﬁ-
nite relaxations for certifying robustness to adversarial
examples.
In Bengio, S., Wallach, H., Larochelle,
H., Grauman, K., Cesa-Bianchi, N., and Garnett, R.
(eds.), Advances in Neural Information Processing
Systems 31, pp. 10900–10910. Curran Associates, Inc.,
2018b.
URL http://papers.nips.cc/paper/
8285-semidefinite-relaxations-for-
certifying-robustness-to-adversarial-
examples.pdf.
Rauber, J., Brendel, W., and Bethge, M. Foolbox: A python
toolbox to benchmark the robustness of machine learning
models. arXiv preprint arXiv:1707.04131, 2017. URL
http://arxiv.org/abs/1707.04131.
Rony, J., Hafemann, L. G., Oliveira, L. S., Ayed, I. B.,
Sabourin, R., and Granger, E. Decoupling direction and
norm for efﬁcient gradient-based L2 adversarial attacks

Adversarial Robustness Against the Union of Multiple Perturbation Models
and defenses. CoRR, abs/1811.09600, 2018. URL http:
//arxiv.org/abs/1811.09600.
Schott, L., Rauber, J., Bethge, M., and Brendel, W. Towards
the ﬁrst adversarially robust neural network model on
MNIST. In International Conference on Learning Repre-
sentations, 2019. URL https://openreview.net/
forum?id=S1EHOsC9tX.
Smith, L. N.
A disciplined approach to neural net-
work hyper-parameters:
Part 1–learning rate, batch
size, momentum, and weight decay.
arXiv preprint
arXiv:1803.09820, 2018.
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Er-
han, D., Goodfellow, I., and Fergus, R.
Intriguing
properties of neural networks. In International Confer-
ence on Learning Representations, 2014. URL http:
//arxiv.org/abs/1312.6199.
Tjeng, V., Xiao, K. Y., and Tedrake, R. Evaluating ro-
bustness of neural networks with mixed integer program-
ming. In International Conference on Learning Repre-
sentations, 2019. URL https://openreview.net/
forum?id=HyGIdiRqtm.
Tram`er, F. and Boneh, D.
Adversarial training and ro-
bustness for multiple perturbations.
arXiv preprint
arXiv:1904.13000, 2019.
Uesato, J., O’Donoghue, B., Kohli, P., and van den
Oord, A.
Adversarial risk and the dangers of eval-
uating against weak attacks.
In Dy, J. and Krause,
A. (eds.), Proceedings of the 35th International Con-
ference on Machine Learning, volume 80 of Proceed-
ings of Machine Learning Research, pp. 5025–5034,
Stockholmsmssan, Stockholm Sweden, 10–15 Jul 2018.
PMLR. URL http://proceedings.mlr.press/
v80/uesato18a.html.
Wong, E. and Kolter, Z. Provable defenses against adver-
sarial examples via the convex outer adversarial polytope.
In International Conference on Machine Learning, pp.
5283–5292, 2018.
Wong, E., Schmidt, F., Metzen, J. H., and Kolter,
J. Z. Scaling provable adversarial defenses. In Ben-
gio, S., Wallach, H., Larochelle, H., Grauman, K.,
Cesa-Bianchi, N., and Garnett, R. (eds.), Advances
in Neural Information Processing Systems 31, pp.
8410–8419. Curran Associates, Inc., 2018. URL http:
//papers.nips.cc/paper/8060-scaling-
provable-adversarial-defenses.pdf.
Zagoruyko, S. and Komodakis, N. Wide residual networks,
2016.

Adversarial Robustness Against the Union of Multiple Perturbation Models
Adversarial Robustness Against the Union of
Multiple Perturbation Models
(Supplementary Material)
A. Steepest descent and projections for ℓ∞, ℓ2,
and ℓ1 adversaries
In this section, we describe the steepest descent and projec-
tion steps for ℓp adversaries for p ∈{∞, 2, 1}; these are
standard results, but are included for a complete description
of the algorithms. Note that this differs slightly from the
adversaries considered in Schott et al. (2019): while they
used an ℓ0 adversary, we opted to use an ℓ1 adversary. The
ℓ0 ball with radius ϵ is contained within an ℓ1 ball with the
same radius, so achieving robustness against an ℓ1 adversary
is strictly more difﬁcult.
ℓ∞space
The direction of steepest descent with respect
to the ℓ∞norm is
v∞(δ) = α · sign(∇l(x + δ; θ))
(15)
and the projection operator onto ∆∞,ϵ is
P∆∞,ϵ(δ) = clip[−ϵ,ϵ](δ)
(16)
ℓ2 space
The direction of steepest descent with respect to
the ℓ2 norm is
v2(δ) = α ·
∇ℓ(x + δ; θ)
∥∇ℓ(x + δ; θ)∥2
(17)
and the projection operator onto the ℓ2 ball around x is
P∆2,ϵ(δ) = ϵ ·
δ
max{ϵ, ∥δ∥2}
(18)
ℓ1 space
The direction of steepest descent with respect to
the ℓ1 norm is
v1(δ) = α · sign
∂ℓ(x + δ; θ)
∂δi⋆

· ei⋆
(19)
where
i⋆= arg max
i
|∇l(x + δ; θ)i|
(20)
and ei∗is a unit vector with a one in position i∗. Finally, the
projection operator onto the ℓ1 ball,
P∆1,ϵ(δ) = arg min
δ′:∥δ′∥1≤ϵ
∥δ −δ′∥2
2,
(21)
can be solved with Algorithm 2, and we refer the reader to
Duchi et al. (2008) for its derivation.
Algorithm 2 Projection of some perturbation δ ∈Rn onto
the ℓ1 ball with radius ϵ. We use | · | to denote element-wise
absolute value.
Input: perturbation δ, radius ϵ
Sort |δ| into γ : γ1 ≥γ2 ≥· · · ≥γn
ρ := max
n
j ∈[n] : γj −1
j
Pj
r=1 γr −ϵ

> 0
o
η := 1
ρ (Pρ
i=1 γi −ϵ)
zi := sign(δi) max {γi −η, 0} for i = 1 . . . n
return z
A.1. Enhanced ℓ1 steepest descent step
Note that the steepest descent step for ℓ1 only updates a
single coordinate per step. This can be quite inefﬁcient,
as pointed out by Tram`er & Boneh (2019). To tackle this
issue, and also empirically improve the attack success rate,
Tram`er & Boneh (2019) instead select the top k coordinates
according to Equation 20 to update. In this work, we adopt a
similar but slightly modiﬁed scheme: we randomly sample
k to be some integer within some range [k1, k2], and update
each coordinate with step size α′ = α/k. We observe in our
experimentation that the randomness induced by varying the
number of coordinates aids in reducing the gradient masking
problem observed by Tram`er & Boneh (2019).
A.2. Restricting the steepest descent coordinate
The steepest descent direction for both the ℓ0 and ℓ1 norm
end up selecting a single coordinate direction to move the
perturbation. However, if the perturbation is already at the
boundary of pixel space (for MNIST, this is the range [0,1]
for each pixel), then it’s possible for the PGD adversary to
get stuck in a loop trying to use the same descent direction
to escape pixel space. To avoid this, we only allow the
steepest descent directions for these two attacks to choose
coordinates that keep the image in the range of real pixels.
B. Extended results
Here, we show the full break down of adversarial error rates
over individual attacks for both MNIST and CIFAR10.
B.1. MNIST results
Expanded table of results
Table 3 contains brak down of
adversarial accuracies against all attacks for all models on
the MNIST dataset. All attacks were run on a subset of the
ﬁrst 1000 test examples with 10 random restarts, with the
exception of Boundary Attack, which by default makes 25
trials per iteration, and DDN attack, which does not beneﬁt
from restarts owing to a deterministic starting point. The
results for B-ABS and ABS models are reported directly
from Schott et al. (2019), which uses gradient estimation

Adversarial Robustness Against the Union of Multiple Perturbation Models
Table 3: Summary of adversarial accuracy results for MNIST
P∞
P2
P1
B-ABS
ABS
MAX
AVG
MSD
Clean Accuracy
99.1%
99.2%
99.3%
99%
99%
98.6%
99.1%
98.3%
PGD-ℓ∞
90.3%
0.4%
0.0%
-
-
51.0%
65.2%
62.7%
FGSM
94.9%
68.3%
6.4%
85%
34%
81.4%
85.5%
82.8%
PGD-Foolbox
92.1%
8.5%
0.1%
86%
13%
65.8%
73.5%
69.2%
MIM
92.3%
11.2%
0.1%
85%
17%
70.7%
76.7%
71.0%
ℓ∞attacks (ϵ = 0.3)
90.3%
0.4%
0.0%
77%
8%
51.0%
65.2%
62.7%
PGD-ℓ2
68.8%
69.2%
38.7%
-
-
64.1%
67.9%
70.2%
PGD-Foolbox
88.9%
77.9%
48.7%
63%
87%
75.6%
80.3%
78.4%
Gaussian Noise
98.9%
98.6%
98.9%
89%
98%
97.7%
98.6%
97.2%
Boundary Attack
18.2%
81.4%
62.1%
91%
83%
73.6%
71.8%
72.4%
DeepFool
93.0%
86.8%
59.5%
41%
83%
81.7%
87.3%
80.7%
Pointwise Attack
40.6%
95.1%
96.7%
87%
94%
90.8%
85.9%
89.6%
DDN
63.9%
70.5%
40.0%
-
-
62.5%
64.6%
69.5%
CWL2
79.6%
74.5%
44.8%
-
-
72.1%
72.4%
74.5%
ℓ2 attacks (ϵ = 2.0)
13.6%
69.2%
38.5%
39%
80%
61.9%
60.1%
67.9%
PGD-ℓ1
61.8%
51.1%
74.6%
-
-
61.2%
66.5%
70.4%
Salt & Pepper
62.1%
96.4%
97.7%
96%
95%
94.6%
90.6%
89.1%
Pointwise Attack
5.3%
83.3%
89.1%
82%
78%
65.3%
45.4%
70.7%
ℓ1 attacks (ϵ = 10)
4.2%
43.4%
70.0%
82%
78%
52.6%
39.2%
65.0%
All attacks
3.7%
0.4%
0.0%
39%
8%
42.1%
34.9%
58.4%
techniques whenever a gradient is needed, and the robust-
ness against all attacks for B-ABS and ABS is an upper
bound based on the reported results. Further, they used ep-
silon balls of radii (0.3,1.5,12) for (ℓ∞, ℓ2, ℓ0) adversaries.
Moreover, they used an ℓ0 perturbation model of a higher
radius and evaluated against ℓ0 attacks. So the reported
number is a near estimate of the ℓ1 adversarial accuracy.
B.2. CIFAR10 results
Expanded table of results
Table 4 contains the full table
of results for all attacks on all models on the CIFAR10
dataset. All attacks were run on a subset of the ﬁrst 1000
test examples with 10 random restarts, with the exception
of Boundary Attack, which by default makes 25 trials per
iteration, and DDN attack, which does not beneﬁt from
restarts owing to a deterministic starting point. Further note
that salt & pepper and pointwise attacks in the ℓ1 section are
technically ℓ0 attacks, but produce perturbations in the ℓ1
ball. Finally, it is clear here that while the training against
an ℓ1 PGD adversary defends against said PGD adversary, it
does not seem to transfer to robustness against other attacks.
C. Experimental details
C.1. Hyperparameters for PGD adversaries
In this section, we describe the parameters used for all PGD
adversaries in this paper.
MNIST
The ℓ∞adversary used a step size α = 0.01
within a radius of ϵ = 0.3 for 50 iterations.
The ℓ2 adversary used a step size α = 0.1 within a radius
of ϵ = 2.0 for 100 iterations.
The ℓ1 adversary used a step size of α = 0.8 within a radius
of ϵ = 10 for 50 iterations. By default the attack is run with
two restarts, once starting with δ = 0 and once by randomly
initializing δ in the allowable perturbation ball. k1 = 5, k2 =
20 as described in A.1.
At test time, we increase the number of iterations to
(100, 200, 100) for (ℓ∞, ℓ2, ℓ1).
CIFAR10
The ℓ∞adversary used a step size α = 0.003
within a radius of ϵ = 0.03 for 40 iterations.
The ℓ2 adversary used a step size α = 0.05 within a radius
of ϵ = 0.5 for 50 iterations.
The ℓ1 adversary used a step size α = 1.0 with ϵ = 12 for
50 iterations. k1 = 5, k2 = 20 as described in A.1.
At test time, we increase the number of iterations to
(100, 500, 100) for (ℓ∞, ℓ2, ℓ1).
C.2. Training hyperparameters
In this section, we describe the parameters used for adver-
sarial training.

Adversarial Robustness Against the Union of Multiple Perturbation Models
Table 4: Summary of adversarial accuracy results for CIFAR10
P∞
P2
P1
MAX
AVG
MSD
Clean accuracy
83.3%
90.2%
73.3%
81.0%
84.6%
81.1%
PGD-ℓ∞
50.3%
48.4%
29.8%
44.9%
42.8%
48.0%
FGSM
57.4%
43.4%
12.7%
54.9%
51.9%
53.7%
PGD-Foolbox
52.3%
28.5%
0.6%
48.9%
44.6%
53.5%
MIM
52.7%
30.4%
0.7%
49.9%
46.1%
50.7%
ℓ∞attacks (ϵ = 0.03)
50.7%
28.3%
0.2%
44.9%
42.5%
48.0%
PGD-ℓ2
59.0%
62.1%
28.9%
64.1%
66.9%
66.6%
PGD-Foolbox
61.6%
64.1%
4.9%
65.0%
68.0%
68.2%
Gaussian Noise
82.2%
89.8%
62.3%
81.3%
84.3%
80.9%
Boundary Attack
65.5%
67.9%
2.3%
64.4%
69.2%
69.4%
DeepFool
62.2%
67.3%
0.9%
64.4%
67.4%
66.1%
Pointwise Attack
80.4%
88.6%
46.2%
78.9%
83.8%
79.8%
DDN
60.0%
63.5%
0.1%
64.5%
67.7%
67.0%
CWL2
62.0%
71.6%
0.1%
66.9%
71.5%
64.7%
ℓ2 attacks (ϵ = 0.05)
57.3%
61.6%
0.0%
61.7%
65.0%
64.3%
PGD-ℓ1
16.5%
49.2%
69.1%
39.5%
54.0%
53.4%
Salt & Pepper
63.4%
74.2%
35.5%
75.2%
80.7%
73.9%
Pointwise Attack
49.6%
62.4%
8.4%
63.3%
77.0%
69.7%
ℓ1 attacks (ϵ = 12)
16.0%
46.6%
7.9%
39.4%
54.0%
53.0%
All attacks
15.6%
27.5%
0.0%
34.9%
40.6%
47.0%
MNIST
For all the models, we used the Adam optimizer
without weight decay, and used a variation of the learning
rate schedule from Smith (2018), which is piecewise linear
from 0 to 10−3 over the ﬁrst 6 epochs, and down to 0 over
the last 9 epochs.
We perform a large hyperparameter search for each of the
MAX, AVG, MSD models, by training them for 15 epochs
on all combinations of the following step sizes: α1 = {0.75,
0.8, 1.0, 2.0}, α2 = {0.1, 0.2}, α∞= {0.01, 0.02, 0.03}.
Also, we ﬁnd that setting the maximum value of learning
rate to 10−3 works best among other values that we experi-
ment on.
The MSD adversary used step sizes of α = (0.01, 0.1, 0.8)
for the (ℓ∞, ℓ2, ℓ1) directions within a radius of ϵ =
(0.3, 2.0, 10) for 100 iterations.
The MAX approach used step sizes of α = (0.01, 0.1, 1.0)
for the (ℓ∞, ℓ2, ℓ1) directions within a radius of ϵ =
(0.3, 2.0, 12) for (50, 100, 100) iterations respectively. We
had to make an early stop at the end of the fourth epoch,
since further training made the model biased towards ℓ∞
robustness. We also had to increase the number of restarts
and attack iterations for the ℓ1 PGD attack.
The AVG approach used step sizes of α = (0.01, 0.2, 1.0)
for the (ℓ∞, ℓ2, ℓ1) directions within a radius of ϵ =
(0.3, 2.0, 12) for (50, 100, 50) iterations respectively. Note
that we had to change the perturbation model for the ℓ1 ad-
versary to make it relatively stronger in-order to “balance”
the trade-offs between different perturbation models.
Finally, we train the standard P1, P2, P∞models for an
extended period till 20 epochs with respective step sizes α1
= 1.0, α2 = 0.1, and α∞= 0.01.
CIFAR10
For all the models, we used the SGD optimizer
with momentum 0.9 and weight decay 5 · 10−4. We used a
variation of the learning rate schedule from Smith (2018) to
achieve superconvergence in 50 epochs, which is piecewise
linear from 0 to 0.1 over the ﬁrst 20 epochs, down to 0.005
over the next 20 epochs, and ﬁnally back down to 0 in the
last 10 epochs.
The MSD adversary used step sizes of α = (0.003, 0.02,
1.0) for the (ℓ∞, ℓ2, ℓ1) directions within a radius of ϵ =
(0.03, 0.5, 12) for 50 iterations.
The MAX adversary used step sizes of α = (0.005, 0.05,
1.0) for the (ℓ∞, ℓ2, ℓ1) directions within a radius of ϵ =
(0.03, 0.3, 12) for (40, 50, 50) iterations respectively. We
do an early stop at epoch 45 for best accuracy.
The AVG adversary used step sizes of α = (0.003, 0.05, 1.0)
for the (ℓ∞, ℓ2, ℓ1) directions within a radius of ϵ =
(0.03, 0.3, 12) for (40, 50, 50) iterations respectively.
Note: For obtaining the best-performing MAX and AVG
models, we artiﬁcially balance the size of the ℓ2 perturbation
region, reducing its radius to 0.3 from the actual threat
model of radius 0.5.

Adversarial Robustness Against the Union of Multiple Perturbation Models
Table 5: Comparison with Tram`er & Boneh (2019) on MNIST (higher is better). Results for all models except MSD are
taken as is from Tram`er & Boneh (2019)
Vanilla
Adv∞
Adv1
Adv2
AdvAVG
AdvMAX
MSD
Clean accuracy
99.4%
99.1%
98.9%
98.5%
97.3%
97.2%
98.3%
ℓ∞attacks (ϵ = 0.3)
0.0%
91.1%
0.0%
0.4%
76.7%
71.7%
75.9%
ℓ2 attacks (ϵ = 2.0)
12.4%
12.1%
50.6%
71.8%
58.3%
56.0%
67.9%
ℓ1 attacks (ϵ = 10)
8.5%
11.3%
78.5%
68.0%
53.9%
62.6%
74.8%
All attacks
0.0%
6.8%
0.0%
0.4%
49.9%
52.4%
65.2%
Table 6: Comparison with Tram`er & Boneh (2019) on CIFAR10 (higher is better). Results for all models except MSD are
taken as is from (Tram`er & Boneh, 2019)
Vanilla
Adv∞
Adv1
AdvAVG
AdvMAX
MSD
Clean accuracy
95.7%
92.0%
90.8%
91.1%
91.2%
92.0%
ℓ∞attacks (ϵ =
4
255)
0.0%
71.0%
53.4%
64.1%
65.7%
66.8%
ℓ1 attacks (ϵ = 2000
255 )
0.0%
16.4%
66.2%
60.8%
62.5%
65.3%
All attacks
0.0%
16.4%
53.1%
59.4%
61.1%
63.2%
D. Comparison with Tram`er & Boneh (2019)
In this section, we compare the results of our trained MSD
model with that of Tram`er & Boneh (2019), who study the
theoretical and empirical trade-offs of adversarial robust-
ness in various settings when defending against multiple
adversaries. Training methods presented by them in their
comparisons, namely AdvAVG and AdvMAX closely resem-
ble the simpler approaches discussed in this paper: AVG and
MAX respectively. We use the results as is from their work,
and additionally compare the position of our MSD models
at the revised thresholds used by Tram`er & Boneh (2019).
We make our best attempt at replicating the same attack
strengths as of those used in the evaluation in Tram`er &
Boneh (2019). We use all attacks from the Foolbox library,
apart from the PGD ℓ1 or SLIDE attack (Tram`er & Boneh,
2019). Further, we do not make multiple random restarts for
these comparisons, which is in line with their evaluation.
The results of Tables 5 and 6 show that the relative advan-
tage of MSD over simpler techniques does hold up. The
MSD model was not retrained for the comparison on the
MNIST dataset since it was trained to be robust to the same
perturbation region in the main paper as well.
In case of CIFAR10, we train a model using the WideRes-
Net architecture (Zagoruyko & Komodakis, 2016) with 5
residual blocks and a widening factor of 10, as used by
Tram`er & Boneh (2019). It may be noted that this model
has 4 times more parameters than the pre-activation version
of ResNet which was used for the comparisons in the main
paper. Further, for the CIFAR10 results in Table 6, the mod-
els are trained and tested only for ℓ∞and ℓ1 adversarial
perturbations with ϵ = ( 4
255, 2000
255 ) ∼(0.0157, 7.84). Note
that the size of the perturbation regions considered in the
main paper is strictly larger than these perturbation regions.
We emphasize that the evaluation method adopted in the
main paper is stronger than that in this comparison. This
may also be noted from the results in Table 5, where the
same MSD model (without retraining) achieves nearly 7%
higher accuracy of 65.2% against all attacks that were con-
sidered by Tram`er & Boneh (2019), while the same model
achieved an overall robust accuracy of 58.4% in our evalua-
tion in Table 1 in the main paper. These differences can be
largely attributed to:
1. Use of random restarts: We observe in our experi-
ments that using up to 10 restarts for all our attacks
leads to a decrease in model accuracy from 5 to 10%
across all models. Tram`er & Boneh do not mention
restarting their attacks for these models and so the ro-
bust accuracies for their models in Tables 5, 6 could
potentially be lowered with random restarts.
2. Larger Suite of Attacks Used: The attacks used by
Tram`er & Boneh in case of the CIFAR10 dataset are
PGD, EAD (Chen et al., 2017) and Pointwise Attack
(Schott et al., 2019) for ℓ1; PGD, C&W (Carlini &
Wagner, 2017) and Boundary Attack (Brendel et al.,
2017) for ℓ2; and PGD for ℓ∞. We use a more expan-
sive suite of attacks as shown in Appendix B. Some
attacks like DDN, which proved to be strong adver-
saries in most cases, were not considered by them.
Our observations re-emphasize the importance of perform-
ing multiple restarts and using a broad suite of attacks in
order to be able to best determine the robust performance of
a proposed algorithm.

Adversarial Robustness Against the Union of Multiple Perturbation Models
E. Analyzing learned Filters for MNIST
As described in § 5.1, we use a simple 4 layer CNN model
to classify MNIST digits. Each of the two convolutional
layers has 5x5 ﬁlters. Speciﬁcally, the ﬁrst layer contains 32
such ﬁlters. We begin our analysis by observing the learned
ﬁlters of an ℓ∞robust model. We observe that many of the
learned ﬁlters are extremely sparse with only one non-zero
element as shown in Figure 6a. Interestingly, such a view
is unique to the case of the ℓ∞robust model and is not
observed in ℓ2 (Figure 6b) and ℓ1 (Figure 6c) robust models.
The presence of such learned ﬁlters that act as thresholding
ﬁlters, due to the immediately followed activation layer, has
been hypothesized to be the reason for gradient masking
in such models by Madry et al. (2018); Tram`er & Boneh
(2019). The hypothesis is in line with our experimental cor-
relations of ℓ∞model being the only standard model that
performs poorly against decision-based adversaries while
being signiﬁcantly robust to ﬁrst-order adversaries. There-
fore, we go beyond this preliminary analysis to observe the
initial layers of MSD (Figures 7a, 7b), MAX (Figures 8a,
8b), AVG (Figures 9a, 9b) models. In all the three cases,
we have two models that are almost identically trained, but
with different ℓ∞step sizes: α∞= 0.01 on the left and
α∞= 0.03 on the right. While we display results only on
two extreme settings of relative attack step-sizes, we ﬁnd
that changing the relative step size of different PGD adver-
saries can help reduce the number of thresholding ﬁlters in
the MSD approach, which also leads to better accuracies
against decision-based attacks like the Pointwise Attack.
However, the MAX and AVG models are nearly invariant to
the individual attack step-sizes.
As a result, in order to achieve reasonable performance
in case of MAX and AVG models against decision-based
attacks, we had to employ methods to manipulate the per-
turbation models in an ‘ad-hoc’ manner. More speciﬁcally,
in case of MAX we had to increase the number of restarts
of the ℓ1 attack during training, and perform an early stop
at the end of the fourth epoch (Figure 10a) since further
training biased the model towards ℓ∞robustness, and made
it susceptible to decision-based attacks. In case of AVG, we
had to increase the maximum radius of the ℓ1 attack to 12
(Figure 10b). It is worth noting that both the approaches
help cosmetically strengthen the relative effect of the ℓ1
attack and help reduce the number of sparse ﬁlters. We ob-
serve that these models perform signiﬁcantly better against
decision-based attacks as opposed to those in Figures 8, 9.
Finally, we emphasize that while learning sparse convolu-
tion ﬁlters and the susceptibility to gradient-free attacks is
often correlated, there is no consistent relation between the
“number” of such ﬁlters and the ﬁnal model performance
or the presence of gradient masking. We perform this em-
pirical analysis for completeness to follow up on previous
Table 7: Performance on CIFAR-10-C
Accuracy
Standard model
66.0%
P∞
75.0%
P2
82.7%
P1
57.8%
MAX
70.8%
AVG
76.8%
MSD
74.2%
work by Madry et al. (2018), and it comes with no formal
statements. In fact, a model may perform better against
decision-based attacks even if it has more sparse ﬁlters than
another model. We hope that these preliminary observations
encourage further exploration around the phenomenon of
gradient masking in adversarially robust models.
F. Attacks outside the perturbation model
In this section, we present some additional experiments ex-
ploring the performance of our model on attacks which lie
outside the perturbation model. Note that this is presented
only for exploratory reasons and there is no principled rea-
son why the adversarial defenses should generalize beyond
the perturbation model defended against.
Common corruptions
We measure the performance of
all the models on CIFAR-10-C, which is a CIFAR10 bench-
mark which has had common corruptions applied to it (e.g.
noise, blur, and compression). We report the results in Ta-
ble 7. We ﬁnd that that, apart from the P1 model, the rest
achieve some improved robustness against these common
corruptions above the standard CIFAR10 model.
Defending against ℓ1 and ℓ∞and evaluating on ℓ2
We
also brieﬂy study what happens when one trains against
ℓ1 and ℓ∞perturbation models, while evaluating against
the ℓ2 adversary. Speciﬁcally, we take the MSD approach
on MNIST and simply remove the ℓ2 adversary from the
perturbation model. This results in a model which has its ℓ1
and ℓ∞robust performance against a PGD adversary drop by
1% and its ℓ2 robust performance against a PGD adversary
(which it was not trained for) drops by 2% in comparison to
the original MSD approach on all three perturbation models.
As a result, we empirically observe that including the ℓ2
perturbation model in this setting actually improved overall
robustness against all three perturbation models. Unsurpris-
ingly, the ℓ2 performance drops to some degree, but the
model does not lose all of its robustness.

Adversarial Robustness Against the Union of Multiple Perturbation Models
(a) P∞Model
(b) P2 Model
(c) P1 Model
Figure 6: A view of each of the (5x5) learned ﬁlters of the ﬁrst layer of P∞, P2, P1 models trained on the MNIST dataset.
While there are many learned ﬁlters in the P∞model that have only one non-zero element (rest of the values are nearly
zero), such a phenomenon is absent in P2, P1 models.
(a) MSD Model (α1 = 0.8, α2 = 0.1, α∞= 0.01)
(b) MSD Model (α1 = 0.8, α2 = 0.1, α∞= 0.03)
Figure 7: A view of each of the (5x5) learned ﬁlters of the ﬁrst layer of MSD models trained on the MNIST dataset. The
training hyper-parameters for the left and right images only differ in the step-size for the ℓ∞attack, where α∞= 0.01 for
the left and α∞= 0.03 for the right image. The ﬁgure suggests how adjusting the relative step-sizes can help reduce the
occurrence of sparse ﬁlters in case of MSD models.
(a) MAX Model (α1 = 0.8, α2 = 0.1, α∞= 0.01)
(b) MAX Model (α1 = 0.8, α2 = 0.1, α∞= 0.03)
Figure 8: A view of each of the (5x5) learned ﬁlters of the ﬁrst layer of MAX models trained on the MNIST dataset. The
training hyper-parameters for the left and right images only differ in the step-size for the ℓ∞attack, where α∞= 0.01 for
the left and α∞= 0.03 for the right image. The learned ﬁlters are nearly identical for both models and indicate how there
may not be a natural way of balancing the trade-offs between different perturbation models in the training schedule for MAX
models.

Adversarial Robustness Against the Union of Multiple Perturbation Models
(a) AVG Model (α1 = 0.8, α2 = 0.1, α∞= 0.01)
(b) AVG Model (α1 = 0.8, α2 = 0.1, α∞= 0.03)
Figure 9: A view of each of the (5x5) learned ﬁlters of the ﬁrst layer of AVG models trained on the MNIST dataset. The
training hyper-parameters for the left and right images only differ in the step-size for the ℓ∞attack, where α∞= 0.01 for
the left and α∞= 0.03 for the right image. The learned ﬁlters are nearly identical for both models and indicate how there
may not be a natural way of balancing the trade-offs between different perturbation models in the training schedule for AVG
models.
(a) Final MAX Model
(b) Final AVG Model
Figure 10: A view of each of the (5x5) learned ﬁlters of the ﬁrst layer of MAX and AVG models trained on the MNIST
dataset. These models are not susceptible to decision-based attacks as opposed to those in Figures 8, 9. Notably, we had to
employ ‘ad-hoc’ techniques to manipulate the individual perturbation models to be able to train these models. However,
even after such manipulations, the accuracy against the worst-case adversary in the union of ℓ∞, ℓ2, ℓ1 perturbation models
for MAX, AVG approaches is considerably worse than the MSD approach.

