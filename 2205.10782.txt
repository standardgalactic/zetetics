Instruction Induction: From Few Examples
to Natural Language Task Descriptions
Or Honovichτ
Uri Shahamτ
Samuel R. Bowmanν
Omer Levyµτ
τ Tel Aviv University
ν New York University
µ Meta AI
{or.honovich,uri.shaham1}@gmail.com
Abstract
Large language models are able to perform a task by conditioning on a few input-
output demonstrations – a paradigm known as in-context learning. We show that
language models can explicitly infer an underlying task from a few demonstrations
by prompting them to generate a natural language instruction that ﬁts the examples.
To explore this ability, we introduce the instruction induction challenge, compile
a dataset consisting of 24 tasks, and deﬁne a novel evaluation metric based on
executing the generated instruction. We discover that, to a large extent, the ability
to generate instructions does indeed emerge when using a model that is both large
enough and aligned to follow instructions; InstructGPT achieves 65.7% of human
performance in our execution-based metric, while the original GPT-3 model reaches
only 9.8% of human performance. This surprising result suggests that instruction
induction might be a viable learning paradigm in and of itself, where instead of
ﬁtting a set of latent continuous parameters to the data, one searches for the best
description in the natural language hypothesis space.1
1
Introduction
Large language models (LMs) can perform unseen tasks by conditioning on a few labeled examples,
effectively inferring the underlying tasks through a process known as in-context learning [Brown
et al., 2020]. However, task inference is implicit, and the ability of models to explicitly reason about
it remains unexplored. In this work, we show that LMs can explicitly describe an underlying task, in
natural language, given a few labeled examples.
We introduce the instruction induction challenge, in which a model is provided with a few input-
output demonstrations, and is requested to generate a natural language instruction describing the
connection between the input-output pairs. In our experiments, inducing instructions is done in a
zero-shot manner by simply prompting the models to explain a small set of given demonstrations, as
shown in Figure 1; we do not perform ﬁne-tuning or use any labeled instruction induction data.
We examine instruction induction on 24 tasks, ranging from morphosyntactic tasks (e.g., pluralization)
to style transfer (e.g., formality) and sentiment analysis. As a basic evaluation protocol, we collect
human annotations and use them as gold-standard references; the generated instructions are then
compared to these references using BERTScore [Zhang et al., 2020]. Moreover, we suggest a
novel evaluation metric for instruction induction: execution accuracy. The execution accuracy of
a generated instruction is measured by testing whether LMs can correctly perform the task in a
zero-shot manner by using the generated instruction alone, without any demonstrations.
1Our code and data are publicly available at
https://github.com/orhonovich/instruction-induction
Preprint. Under review.
arXiv:2205.10782v1  [cs.CL]  22 May 2022

In-Context Learning
Instruction Induction
I gave a friend an instruction and five inputs. 
The friend read the instruction and wrote an 
output for every one of the inputs.  
Here are the input-output pairs:
Input: Sorry I messed up.
Output: I apologise for my wrongdoings.
Input: As soon as you can.
Output: At your earliest convenience.
Input: I can't stand his temper.
Output: I cannot tolerate his temper.
Input: Sorry I messed up.
Output: I apologise for my wrongdoings.
Input: As soon as you can.
Output: At your earliest convenience.
…
…
The instruction was translate the inputs 
into more formal language.
Figure 1: An example of instruction induction for the task of formality style transfer. Left: the
standard in-context learning setting; given ﬁve demonstrations, complete the sixth. Right: instruction
induction; the language model is prompted to generate a natural language instruction that describes
the demonstrations. Model completions are in blue, prompt templates are in pink.
Our experiments reveal a surprising ability at generating correct instructions. The best-performing
model, InstructGPT [Ouyang et al., 2022], achieves an average BERTScore of 44.4, compared to
human performance of 60.0; when measuring execution accuracy, the model reaches 43.6, with
human-written instructions reaching 66.4. For some tasks, the model’s performance is on par or
even better than human performance. When qualitatively examining the generated instructions, we
often observe accurate instructions, even for some of the more challenging tasks. For instance, in
the task of formality style transfer, generated instructions include “Translate the inputs into more
formal language” and “Use formal language”. For semantic text similarity, the generated instructions
include “For each input, rate the similarity of the two sentences on a scale of 0 to 5, with 5 being a
perfect match” and “Determine whether the two sentences are about the same thing”.
Despite these impressive results, we ﬁnd that this ability is currently unique to InstructGPT [Ouyang
et al., 2022], which is both very large (175B parameters) and was especially ﬁne-tuned to follow
instructions. Ablations on smaller versions of InstructGPT as well as the original 175B-parameter
GPT-3 [Brown et al., 2020] yield dramatically weaker performance. These ﬁndings are in line
with recent work showing that increasing model size unlocks new capabilities [Chowdhery et al.,
2022, Ganguli et al., 2022], and serves as additional evidence for the strength of instruction tuning
[Sanh et al., 2022, Wei et al., 2022a, Ouyang et al., 2022], perhaps even pointing to the necessity of
complementing standard next-word prediction with additional objectives.
The fact that models can induce natural language instructions suggests that instruction-induction may
serve as a learning paradigm of its own, where the goal is to ﬁnd the best description in the natural
language hypothesis space. While we currently provide a proof-of-concept for that idea, extending it
by grounding models in natural language has the immediate beneﬁt of human interpretability, and
might also help alleviate overﬁtting and other issues associated with spurious correlations.
2
Instruction Induction
We begin by formulating the task of instruction induction. Given a sequence of n demonstrations
{xk, yk}k∈{1,...,n}, the goal is to generate a single natural language instruction, such that for each
xk, following the instruction results in yk. This format is similar to in-context learning [Brown et al.,
2020], only here the desired output is an instruction describing the relation between the inputs and
outputs of the demonstrations. We require models to perform this in a zero-shot setting, without any
ﬁne-tuning on labeled data. Figure 1 illustrates the difference between standard in-context prompting
and instruction-induction prompting.
2

To elicit models to generate instructions, we consider prompts that would elicit humans to do so. We
design a meta-prompt presenting instruction induction as a challenge puzzle and verify its clarity in a
human study (§3.3). The prompt is presented in Figure 1 (right side, in pink).2
While prior work already shows that large LMs are often able to infer a latent task from a given set of
demonstrations, this has been largely based on their ability to execute the task on a held-out example.
Instruction induction requires that the model describe the underlying task in natural language.
3
Data
We evaluate on 24 tasks, listed in Table 1. We select these tasks as they vary in difﬁculty and represent
different aspects of language understanding, ranging from surface-level spelling to sentence similarity
and causality detection.3 We review the dataset’s format, the annotation and veriﬁcation processes we
conducted to ensure that the tasks are viable, and ﬁnally discuss a theoretical limitation of this setup.
3.1
Format
In every task, each single demonstration (xk, yk) is formatted as follows:
Input: xk
Output: yk
For instance, one demonstration in the pluralization task is “Input: cat” followed by “Output: cats”
in a new line. We split each task’s demonstrations into two sets: an induce set, which we use for
generating instructions, and an execute set, which is held out for the execution accuracy evaluation
metric (see §4.2). Each instruction induction example is composed of 5 demonstrations sampled
randomly without replacement from the induce set, concatenated with new-line separators; we
create 100 examples for each task. When generating instructions, each example is placed inside the
instruction induction prompt, and fed to the model (Figure 1, right).
3.2
Annotating Reference Instructions
We collect 10 gold-reference human-annotated instructions via college-graduate English-speaking
annotators. For each task, we provide the annotators with the exact same input we intend to provide a
model: 5 input-output demonstrations wrapped by the instruction-induction prompt (Figure 1). We
manually verify each annotation and discard ones that do not correctly describe the task. We refer to
this set of annotations as the gold annotations, and use them for reference-based evaluation (see §4).
3.3
Veriﬁcation
Prior to the instruction induction experiments, we conduct two tests to ensure that either models or
humans can infer the underlying task given 5 demonstrations. We ﬁrst verify that models can indeed
execute our tasks given 5 demonstrations using in-context learning. Secondly, we conduct a human
study to conﬁrm that 5 demonstrations are enough for humans to describe the latent tasks.
In-Context Learning
We prompt models with 5 input-output demonstrations and concatenate an
additional test input xk+1, and verify that the models are able to correctly predict yk+1 (Figure 1,
left). For each task, we repeat this experiment 100 times, each with a different set of demonstrations
and test inputs. We do not provide the model with any instruction beyond the “Input: xk Output:
yk” format. We evaluate each task using its predeﬁned evaluation metric.4 The in-context results
for GPT-3 [Brown et al., 2020] and InstructGPT [Ouyang et al., 2022] (see model details in §5) are
reported in Table 4 in Appendix B, which shows that in-context learning can reach 80% accuracy and
above on most tasks.
2We found this prompt informative for both humans and models in preliminary experiments.
3See Appendix A for the full details of each task.
4All metrics are variants of simple string matching, with some task-speciﬁc heuristics, for example, to allow
for multiple correct answers. See Appendix A for exact details.
3

Category
Task
Instruction
Demonstration
Spelling
First Letter
Extract the ﬁrst letter of the input word.
cat →c
Second Letter
Extract the second letter of the input word.
cat →a
List Letters
Break the input word into letters, sepa-
rated by spaces.
cat →c a t
Starting With
Extract the words starting with a given
letter from the input sentence.
The man whose car I hit last week
sued me. [m] →man, me
Morpho-
syntax
Pluralization
Convert the input word to its plural form.
cat →cats
Passivization
Write the input sentence in passive form.
The artist introduced the scientist.
→The scientist was introduced
by the artist.
Syntax
Negation
Negate the input sentence.
Time is ﬁnite →Time is not ﬁnite.
Lexical
Semantics
Antonyms
Write a word that means the opposite of
the input word.
won →lost
Synonyms
Write a word with a similar meaning to
the input word.
alleged →supposed
Membership
Write all the animals that appear in the
given list.
cat, helicopter, cook, whale, frog,
lion →frog, cat, lion, whale
Phonetics
Rhymes
Write a word that rhymes with the input
word.
sing →ring
Knowledge
Larger Animal
Write the larger of the two given animals.
koala, snail →koala
Semantics
Cause Selection
Find which of the two given cause and
effect sentences is the cause.
Sentence 1: The soda went ﬂat.
Sentence 2: The bottle was left
open. →The bottle was left open.
Common
Concept
Find a common characteristic for the given
objects.
guitars, pendulums, neutrinos →
involve oscillations.
Style
Formality
Rephrase the sentence in formal language.
Please call once you get there →
Please call upon your arrival.
Numerical
Sum
Sum the two given numbers.
22 10 →32
Difference
Subtract the second number from the ﬁrst.
32 22 →10
Number to Word
Write the number in English words.
26 →twenty-six
Multi-
lingual
Translation
Translate the word into German / Spanish
/ French.
game →juego
GLUE
Sentiment
Analysis
Determine whether a movie review is pos-
itive or negative.
The ﬁlm is small in scope, yet per-
fectly formed. →positive
Sentence
Similarity
Rate the semantic similarity of two input
sentences on a scale of 0 - deﬁnitely not
to 5 - perfectly.
Sentence 1: A man is smoking.
Sentence 2: A man is skating. →
0 - deﬁnitely not
Word in Context
Determine whether an input word has the
same meaning in the two input sentences.
Sentence 1: Approach a task. Sen-
tence 2: To approach the city.
Word: approach →not the same
Table 1: The tasks in our instruction-induction benchmark. For each task, we show a corresponding
instruction and demonstration, with →separating the input from the output.
4

Human Study
To assess the human ability to induce instructions, we collect human-written
instructions, using annotators that did not participate in the gold references collection. As in the
gold-reference annotation process, we provide annotators with the same input we intend to provide
to models. We refer to this set of annotations as the control annotations. We then manually count,
for each task, the number of annotators that provided a correct instruction, and report the correct
instructions percentage in Table 4 (Appendix B). In all but one task (Larger Animal), at least 4 out of
5 annotators were able to produce correct task descriptions.
We also use the control group’s annotations to establish a human baseline for automatic evaluation
metrics. For reference-based evaluation (§4.1), we treat the control annotations as generated instruc-
tions and compare them against the gold annotations, while for execution accuracy (§4.2), we use the
control annotations to measure human performance, and the gold references as a ceiling metric.
3.4
Ambiguity
A theoretical challenge in inducing instructions is ambiguity. For example, when given the single
demonstration “Input: The coffee is too hot. Output: The, too, hot”, one could infer that the
underlying task is either “write all the words containing the letter T” or “write all the three-lettered
words”, both valid interpretations. Ambiguity might confuse models tasked with instruction induction
while also making evaluation less reliable. In practice, providing 5 demonstrations typically resolves
the ambiguity in our set of tasks. As evident from the data veriﬁcation process, our tasks can typically
be inferred by models and/or humans.
Inducing more complex task descriptions, such as predicting detailed annotation guidelines, may pose
a greater challenge in terms of ambiguity. We hypothesize that providing more than 5 demonstrations
could mitigate some of that challenge, and leave further exploration of this avenue to future work.
4
Evaluating Generated Instructions
As a standard text generation metric, we report BERTScore [Zhang et al., 2020]. However, the
instruction induction challenge has a unique property, which does not usually hold for other text
generation tasks: the instructions are executable. Their correctness can therefore be measured directly
by utilizing them as prompts.
4.1
Reference-Based Evaluation
We use BERTScore [Zhang et al., 2020] to compare the model-generated instructions against the
collected gold annotations. As mentioned in §3.2, we use only the correct, veriﬁed annotations as
references. We take the maximal BERTScore-F1 over all gold-reference annotations to account for
natural variations in instruction formulation.5 We also establish a human baseline for each task using
the control annotations, which were collected from a separate control group of annotators (§B), which
we compare against the gold annotations in exactly the same way as model-generated instructions.
4.2
Execution Accuracy
We introduce execution accuracy, a new metric unique to the instruction induction task. To measure
the execution accuracy of a predicted instruction I (e.g., “Write the plural form of the given word.”)
for a task T (pluralization), we prompt a model with I and an input x (“cat”). We then test, given I
and x, whether the model can correctly predict y, the output of performing T on the input x (cats).
To obtain meaningful results, we measure execution accuracy on the 100 held-out execute examples
for each task. The execution accuracy of an instruction I is therefore computed by taking the average
over ScoreT (I(xn), yn) for all xn in the execute set, where ScoreT denotes the task’s corresponding
metric (see Appendix A), and I(xn) is the result of prompting a predeﬁned language model with the
instruction I and the input xn. As recent models are trained to follow instructions [Sanh et al., 2022,
Wei et al., 2022a, Ouyang et al., 2022], and due to the relative clarity of our tasks, we expect correct
instructions to yield high execution accuracy when using a sufﬁciently powerful execution model.
5We use BERTScore version 0.3.11 with the DeBERTa-xl-MNLI model [He et al., 2021, Nangia et al., 2017].
5

First Letter
Second Letter
List Letters
Starting With
Pluralization
Passivization
0
1
GPT-3
InstructGPT
Human
Negation
Antonyms
Synonyms
Membership
Rhymes
Larger Animal
0
1
Cause Selection Common Concept
Formality
Sum
Diff
Number To Word
0
1
Translation en-de Translation en-es Translation en-fr
Sentiment
Sentence SimilarityWord in Context
0
1
Average BERTScore
Figure 2: Average BERTScores of model-generated instructions for each task, compared to the
performance of the control group’s manually-authored instructions. The BERTScore for each
instruction is computed using the human gold annotations as references.
First Letter
Second Letter
List Letters
Starting With
Pluralization
Passivization
0
1
GPT-3
InstructGPT
Human
Gold
Negation
Antonyms
Synonyms
Membership
Rhymes
Larger Animal
0
1
Cause Selection Common Concept
Formality
Sum
Diff
Number To Word
0
1
Translation en-de Translation en-es Translation en-fr
Sentiment
Sentence SimilarityWord in Context
0
1
Average Execution Acuracy
Figure 3: Average execution accuracy of model-generated instructions for each task, compared to
the execution accuracy measured for human-written instructions. The Human baseline is measured
by taking the control group’s annotations, while the Gold ceiling metric is based on the separately-
annotated and veriﬁed gold annotations.
6

Model
BERTScore
Execution
GPT-3
Ada
-7.7
4.0
Babbage
4.1
3.2
Curie
13.9
7.9
DaVinci
14.6
6.5
InstructGPT
Ada
5.9
4.4
Babbage
-0.5
3.8
Curie
10.7
8.8
DaVinci
44.4
43.6
Human (Control)
60.0
66.4
Table 2: Average BERTScore and execution accuracy across tasks. BERTScore is measured against
the gold references. The execution accuracy for all generated instructions is measured using Instruct-
GPT as the execution model. Human performance is measured using the human control group’s
instructions.
5
Results
Baseline Models
We experiment with eight versions of GPT-3 [Brown et al., 2020], a Transformer
decoder language model. First, we experiment with the most current version available in the OpenAI
API, for each of the four available model sizes. Though not stated explicitly in the API, we assume
these models are those reported by Ouyang et al. [2022], and we therefore refer to them as Instruct
models.6 We also experiment with the four originally published GPT-3 versions.7 By default, we
refer to the largest Instruct model as InstructGPT, and the original 175B-parameter model as GPT-3.
All model generations were produced using the greedy decoding algorithm.
5.1
Comparing to Gold Annotations
Figure 2 presents the average BERTScore per task (see §4.1). Results show that the InstructGPT
model has, to some extent, the ability to induce instructions from a few demonstrations; in 13 out of
24 tasks it achieves at least 75% of human performance. GPT-3, on the other hand, is quite far from
human performance across the board.
Table 2 shows the average scores across all tasks. We observe the same trend; while InstructGPT’s
BERTScore is 15.6 points lower than human performance, the gap between GPT-3 and humans is
45.4 points. Moreover, we observe that smaller models – even those ﬁne-tuned to follow instructions
– do not exhibit any instruction-induction abilities. Scores are slightly higher for larger models of the
same family (except for the InstructGPT-Babbage outlier), but are overall low. Excluding the largest
models, there does not appear to be a signiﬁcant advantage for Instruct models over the originals
when controlling for model size.
5.2
Execution Accuracy
We compute the execution accuracy as detailed in §4.2, and report the average over 100 generated
instructions for each task. As an execution model, we use the largest InstructGPT model. We also use
this model to induce instructions, and while using it as an execution model might bias results towards
its own generations, preliminary experiments show that no other model is as good at following
instructions as InstructGPT. As a point of reference, we apply the execution accuracy evaluation
protocol to human-written instructions. First, to compare models with human performance, we
measure the execution accuracy of the control annotation set. Second, to account for limitations in the
execution model, we measure execution accuracy of the correct (manually veriﬁed) gold annotations,
which acts as an approximated ceiling metric.
6Concretely, we use: text-davinci-002, text-curie-001, text-babbage-001, text-ada-001.
7davinci, curie, babbage, ada.
7

Task
GPT-3
InstructGPT
First letter
The friend’s output was:
Write the ﬁrst letter of each word.
Sentence Similarity
The friend wrote the following output:
For each input, rate the similarity of the two
sentences on a scale of 0 to 5, with 5 being a
perfect match.
Pluralization
The friend’s output was:
Add ‘s’ to the end of each word.
Passivization
The friend wrote the following output:
Reverse the order of the subject and the object
in the sentence.
Antonyms
The friend’s output was:
Reverse the input.
Table 3: Examples of the instructions generated by GPT-3 and InstructGPT for ﬁve of our tasks.
Figure 3 presents the execution accuracy per task. In 12 out of 24 tasks, InstructGPT achieves at least
75% of the execution accuracy measured for the human-written instructions. GPT-3 shows much
weaker execution accuracy, scoring less than 10% on 20 of the 24 tasks. In fact, only in the cases of
formality, passivization, and cause selection does it approach human performance, and that is largely
an artifact of a more lenient evaluation metric in the case of formality and cause selection, or due
to the execution model being right for the wrong reasons in the case of passivization (see §6). In
some tasks, the control annotations are of high quality and reach a higher score than the veriﬁed gold
annotations, likely due to variance of the execution model in such cases.
Table 2 shows the same trends. On average, InstructGPT achieves 65.7% of human performance,
while GPT-3 reaches only 9.8% of human performance. When considering different model families or
sizes, we do not see any substantial improvements when increasing model size or adding instruction
tuning, with the exception of the largest InstructGPT model. The ability to generate instructions
seems to only emerge when a model is both large enough and aligned to follow instructions. Overall,
even the best-performing model still does not reach human performance, leaving room for future
improvement.
6
Analysis
To gain further insight into the successes and failures of instruction induction prompting, we manually
analyze the model-generated instructions of 5 tasks. Table 3 shows the most common predictions of
GPT-3 and InstructGPT for each of these tasks.
InstructGPT obtains high, or close to human execution accuracy scores for three of these tasks
(First Letter, Sentence Similarity, Pluralization). Indeed, the instructions for both First Letter and
Sentence Similarity accurately describe the task. However, the instruction generated for Pluralization
is not entirely precise, since it dismisses other forms of pluralization such as -es, -ies, and irregulars.
Although the instruction only asks to add an “s”, the execution model often ignores the speciﬁcs and
produces the correct plural form; in one case, the input word was “life” and the output was “lives”.
While this particular instruction accounts for 24% of the induced instructions in the pluralization task,
some predictions do explicitly mention pluralization, though not always accurately, e.g., “Add -s to
the end of each word to make it plural”.
For some tasks, InstructGPT fails to produce accurate instructions, even if it is able to solve via
in-context learning (see Table 4). In Passivization, 98% of the predicted instructions were to simply
“reverse the order of the subject and object”, while ignoring additional surface-form manipulations
needed to convert the given sentence into passive form; e.g., for the input “The authors supported the
scientist”, following the instructions produces the output “The scientist supported the authors”, while
the correct passive form is “The scientist was supported by the authors”. Surprisingly, the instructions
generated by GPT-3 obtained higher execution accuracy than the InstructGPT, even though they were
entirely unrelated. In 24% of the cases, GPT-3 predicted “The friend wrote the following output:”
– an instruction that apparently prompts the execution model to often rephrase the input in passive
form. Lastly, in Antonyms, 60% of InstructGPT’s predictions were “Reverse the input”, and another
11% were “Reverse the word”. While one could imagine an interpretation of these instructions that
8

reﬂects the task (reversing the meaning of the word), the execution model interprets them literally,
and reverses the input words’ letters.
Overall, GPT-3 did not exhibit any instruction induction abilities, although it did often phrase outputs
in imperative language. One relatively common prediction was the generic instruction “Write an
output for every input”. Because these empty instructions are in the right format, they tend to have
some overlap with the reference instructions, which inﬂates their BERTScore. Execution accuracy, on
the other hand, is robust to this phenomenon, and typically assigns GPT-3’s outputs very low scores.
7
Related Work
In-Context Learning
Brown et al. [2020] suggest that models can learn a task by conditioning on
few input-output demonstration pairs, without any ﬁne-tuning or gradient updates. This paradigm,
known as in-context learning or prompt-based learning [Liu et al., 2021], has been the focus of many
research efforts lately: Du et al. [2021] suggest methods for more efﬁcient in-context learning, Zhao
et al. [2021] study methods for improving the stability and accuracy of prompt-based models, Chen
et al. [2021] and Min et al. [2022a] conduct meta-training with an in-context learning objective, while
other work studies the effect of the provided prompts [Reynolds and McDonell, 2021, Webson and
Pavlick, 2021, Min et al., 2022b], or suggests prompt reframing techniques [Mishra et al., 2021]
and prompt retrieval methods [Rubin et al., 2021]. To the best of our knowledge, all previous work
study in-context learning through the lens of executing a latent task, while we focus on the ability to
explicitly describe it.
The Instruction Paradigm
Efrat and Levy [2020] propose to learn new tasks from natural language
instructions. Mishra et al. [2022] and Wang et al. [2022b] collect crowdsourcing instructions used to
create NLP datasets into a benchmark for measuring the ability to solve tasks by reading instructions.
Recent work shows that ﬁne-tuning on task instructions (instruction tuning) improves the zero-shot
learning abilities of LMs [Sanh et al., 2022, Wei et al., 2022a, Ouyang et al., 2022]. This work
focuses on models’ ability to generate instructions, rather than their ability to execute instructions
written by humans.
Intermediate Reasoning Steps
Nye et al. [2022] show that LMs can perform complex computa-
tions by writing intermediate steps on a “scratchpad”. In chain of thought prompting [Wei et al.,
2022b], input-output demonstrations are enriched with sentences elaborating intermediate task rea-
soning steps, improving the performance of LMs on tasks requiring reasoning skills. Subsequent
work further improves the performance on such tasks using a self-consistency ensemble [Wang
et al., 2022a], which samples a set of diverse chain-of-thought reasoning paths, taking the majority
vote over all generated answers. Zelikman et al. [2022] utilize a small set of examples labeled
with chain-of-thought rationales and a large set of unlabeled data to iteratively bootstrap automatic
rationale generation, thus creating a large dataset labeled with such rationales to enable ﬁne-tuning.
In contrast, we study the ability of LMs to generate a description of the task, rather than generating
intermediate reasoning steps as a means of executing complex tasks.
8
Discussion
This work demonstrates that large LMs can not only infer new tasks based on a handful of demonstra-
tions, but also describe them in natural language. We provide evidence of this ability on a diverse
set of language tasks, and show that while instruction induction abilities are limited to a single
state-of-the-art model, this model does indeed approach human performance on about half the tasks.
It is not unreasonable to assume that models in the near future will be even better at processing
human-generated instructions, and it is therefore interesting to discuss the potential applications of
instruction induction. In particular, we envision a use case in which instruction induction serves as
a machine learning approach; instead of converting a dataset into a set of continuous parameters,
we could produce a natural language instruction that best describes the data. Grounding the model
in concise natural language has the advantage of interpretability, and has the potential to solve
fundamental issues pertaining to spurious correlations. While it is still too early to determine whether
this approach is viable, we view it as an intriguing direction for future research.
9

References
BIG-bench collaboration. Beyond the imitation game: Measuring and extrapolating the capabilities
of language models., 2021. URL https://github.com/google/BIG-bench/.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato,
R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems,
volume 33, pages 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.
neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.
Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1:
Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the
11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1–14, Vancouver,
Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/S17-2001.
URL https://aclanthology.org/S17-2001.
Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. Meta-learning via language
model in-context tuning, 2021. URL https://arxiv.org/abs/2110.07814.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,
Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam
Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James
Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev-
skaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin
Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph,
Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.
Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon
Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean,
Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022. URL
https://arxiv.org/abs/2204.02311.
Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim
Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma,
Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathy
Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen,
and Claire Cui. Glam: Efﬁcient scaling of language models with mixture-of-experts, 2021. URL
https://arxiv.org/abs/2112.06905.
Avia Efrat and Omer Levy. The turking test: Can language models understand instructions?, 2020.
URL https://arxiv.org/abs/2010.11982.
Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Frederique
Laforest, and Elena Simperl. T-REx: A large scale alignment of natural language with knowledge
base triples. In Proceedings of the Eleventh International Conference on Language Resources and
Evaluation (LREC 2018), Miyazaki, Japan, May 2018. European Language Resources Association
(ELRA). URL https://aclanthology.org/L18-1544.
C. Fellbaum. WordNet: An Electronic Lexical Database. MIT Press, 1998.
Deep Ganguli, Danny Hernandez, Liane Lovitt, Nova DasSarma, Tom Henighan, Andy Jones,
Nicholas Joseph, Jackson Kernion, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom
Conerly, Dawn Drain, Nelson Elhage, Sheer El Showk, Stanislav Fort, Zac Hatﬁeld-Dodds, Scott
Johnston, Shauna Kravec, Neel Nanda, Kamal Ndousse, Catherine Olsson, Daniela Amodei, Dario
Amodei, Tom Brown, Jared Kaplan, Sam McCandlish, Chris Olah, and Jack Clark. Predictability
and surprise in large generative models, 2022. URL https://arxiv.org/abs/2202.07785.
10

Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert
with disentangled attention. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=XPZIaotutsD.
Nora Kassner and Hinrich Schütze. Negated and misprimed probes for pretrained language models:
Birds can talk, but cannot ﬂy. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 7811–7818, Online, July 2020. Association for Computational
Linguistics. doi: 10.18653/v1/2020.acl-main.698. URL https://aclanthology.org/2020.
acl-main.698.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.
Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language
processing, 2021. URL https://arxiv.org/abs/2107.13586.
Tom McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuris-
tics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics, pages 3428–3448, Florence, Italy, July 2019. Association for Computa-
tional Linguistics. doi: 10.18653/v1/P19-1334. URL https://aclanthology.org/P19-1334.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
Pointer sentinel mix-
ture models. In International Conference on Learning Representations, 2017. URL https:
//openreview.net/forum?id=Byj72udxe.
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to learn
in context. In NAACL-HLT, 2022a.
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke
Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv
preprint, 2022b.
Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. Reframing
instructional prompts to gptk’s language, 2021. URL https://arxiv.org/abs/2109.07830.
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization
via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pages 3470–3487, Dublin,
Ireland, May 2022. Association for Computational Linguistics. URL https://aclanthology.
org/2022.acl-long.244.
Nikita Nangia, Adina Williams, Angeliki Lazaridou, and Samuel Bowman. The RepEval 2017 shared
task: Multi-genre natural language inference with sentence representations. In Proceedings of the
2nd Workshop on Evaluating Vector Space Representations for NLP, pages 1–10, Copenhagen, Den-
mark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-5301.
URL https://aclanthology.org/W17-5301.
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David
Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and
Augustus Odena. Show your work: Scratchpads for intermediate computation with language
models. In Deep Learning for Code Workshop, 2022. URL https://openreview.net/forum?
id=HBlx2idbkbq.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,
Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and
Ryan Lowe. Training language models to follow instructions with human feedback, 2022. URL
https://arxiv.org/abs/2203.02155.
Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,
and Alexander Miller. Language models as knowledge bases?
In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th International Joint
Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463–2473, Hong Kong,
China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250.
URL https://aclanthology.org/D19-1250.
11

Mohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for
evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pages 1267–1273, Minneapolis, Minnesota,
June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1128. URL https:
//aclanthology.org/N19-1128.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions
for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 2383–2392, Austin, Texas, November 2016. Association
for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://aclanthology.
org/D16-1264.
Laria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond
the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors
in Computing Systems, CHI EA ’21, New York, NY, USA, 2021. Association for Computing
Machinery. ISBN 9781450380959. doi: 10.1145/3411763.3451760. URL https://doi.org/
10.1145/3411763.3451760.
Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context
learning, 2021. URL https://arxiv.org/abs/2112.08633.
Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chafﬁn, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker,
Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, De-
bajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,
Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen,
Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao,
Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training
enables zero-shot task generalization. In International Conference on Learning Representations,
2022. URL https://openreview.net/forum?id=9Vrb9D0WI4.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.
In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,
pages 1631–1642, Seattle, Washington, USA, October 2013. Association for Computational
Linguistics. URL https://aclanthology.org/D13-1170.
Robyn Speer and Catherine Havasi. Representing general relational knowledge in ConceptNet 5.
In Proceedings of the Eighth International Conference on Language Resources and Evaluation
(LREC’12), pages 3679–3686, Istanbul, Turkey, May 2012. European Language Resources As-
sociation (ELRA). URL http://www.lrec-conf.org/proceedings/lrec2012/pdf/1072_
Paper.pdf.
Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. oLMpics-on what language model
pre-training captures. Transactions of the Association for Computational Linguistics, 8:743–758,
2020. doi: 10.1162/tacl_a_00342. URL https://aclanthology.org/2020.tacl-1.48.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE:
A multi-task benchmark and analysis platform for natural language understanding. In Proceedings
of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,
pages 353–355, Brussels, Belgium, November 2018. Association for Computational Linguistics.
doi: 10.18653/v1/W18-5446. URL https://aclanthology.org/W18-5446.
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer
Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language
understanding systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox,
and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Cur-
ran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
4496bf24afe7fab6f046bf4923da8de6-Paper.pdf.
12

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-
ery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models,
2022a. URL https://arxiv.org/abs/2203.11171.
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, et al. Benchmarking
generalization via in-context instructions on 1,600+ language tasks. arXiv, 2022b.
Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.
arXiv preprint arXiv:1805.12471, 2018.
Albert Webson and Ellie Pavlick. Do prompt-based models really understand the meaning of their
prompts?, 2021.
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International
Conference on Learning Representations, 2022a. URL https://openreview.net/forum?id=
gEZrGCozdqR.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou.
Chain of thought prompting elicits reasoning in large language models, 2022b.
Eric Zelikman, Yuhuai Wu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning,
2022. URL https://arxiv.org/abs/2203.14465.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating
text generation with bert. In ICLR 2020, 2020.
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving
few-shot performance of language models. In ICML, pages 12697–12706, 2021. URL http:
//proceedings.mlr.press/v139/zhao21c.html.
13

A
Dataset Details
This appendix details each task’s dataset (§A.1). Some datasets rely on a set of common English
nouns (CEN), described at §A.2.
A.1
Tasks
We elaborate on each task’s data source, preprocessing protocol, and evaluation metric used in the
in-context learning and execution accuracy experiments. As mentioned in §3, each task has induce
and execute sets; unless stated otherwise, we sample 100 examples as the execute set for each task.
When evaluating outputs, the generated text is ﬁrst normalized; we take only the ﬁrst generated
sentence and lowercase it. We apply exact string match as the evaluation metric where applicable,
elaborating only where alternative metrics are used.
First Letter
In each demonstration, xk is a noun, and yk is the ﬁrst letter of that noun. We construct
the demonstrations by extracting the ﬁrst letter of each word in CEN.
Second Letter
Identical to the First Letter task, only here yk is the second letter of xk.
List Letters
xk is a noun from CEN, and yk is a list of xk’s letters, separated by spaces.
Starting With
xk contains a sentence and a letter in brackets, and yk lists the words in xk that start
with the given letter. We avoid cases in which yk is empty, i.e., there is always at least one word in
the input sentence starting with the given letter. Sentences are taken from the CoLA dataset [Warstadt
et al., 2018]. For the induce set, we create all (sentence, letter) pairs using CoLA’s train set, and then
sample 3,000 pairs. For the execute set, we create all (sentence, letter) pairs from CoLA’s in-domain
and out-of-domain dev sets, and then sample 50 in-domain and 50 out-of-domain examples. We
evaluate using exact set match, by treating the output (and yk) as a set of strings.
Pluralization
Given a singular noun xk, produce the plural form yk. We take noun inputs from the
CEN set, ﬁltering out mass nouns using a predeﬁned list.8 To create the plural forms, we apply an
automatic pluralization engine9 and exclude nouns for which the engine’s output did not appear at
least 50 times in the Wikitext-103 corpus. This results in 2,043 singular-plural noun pairs.
Passivization
Given a simple active sentence xk, rephrase the sentence in passive voice yk. We
use the 1,000 HANS [McCoy et al., 2019] evaluation set active-passive entailed sentence pairs.
Negation
yk is the negation of the input sentence xk. We use the negated LAMA dataset [Petroni
et al., 2019, Kassner and Schütze, 2020], taking the 304 negated SQuAD [Rajpurkar et al., 2016]
sentences, 300 ConceptNet [Speer and Havasi, 2012] sentences, 200 T-REx [Elsahar et al., 2018]
sentences and 200 Google-RE10 sentences. For ConceptNet and T-REx, we manually select these
sentences to ensure their quality. For Google-RE, we automatically sample 100 sentences from the
place of birth relation, and 100 from the place of death relation.
Antonyms
yk is the antonym of the input word xk. We use the antonym pairs from oLMpics
[Talmor et al., 2020], which were extracted from ConceptNet [Speer and Havasi, 2012] and WordNet
[Fellbaum, 1998]. For uniformity, we verify that all pairs are indeed antonyms according to WordNet.
Synonyms
xk is a word and yk is its synonym. As in the antonyms task, we use the synonym
pairs of Talmor et al. [2020]. Since there can be multiple synonyms for each input word, the task’s
in-context and execution accuracy are evaluated by testing whether the gold answer (a single word) is
contained in the predicted answer (which may be a list of words).
8https://gist.github.com/sudodoki/b5408fa4ba752cc22597250fc58a5970
9https://pypi.org/project/inflect/
10https://code.google.com/archive/p/relation-extraction-corpus/
14

Membership
xk is a list of words, where some of the words represent animals, and yk lists the
animals from xk. To construct the task’s data, we ﬁrst select 6 word categories: animals, clothing,
colors, food, vehicles, and professions. We then take 10-50 words from each category, using only
words that are categorized at the A1 or A2 levels according to the Common European Framework of
Reference for Languages (CEFR).11 Using these words, we create random lists containing between 5
to 7 words, where 3 or 4 are animals and the rest belong to one of the other 5 categories. The induce
split is constructed by sampling 3,000 such combinations, using 80% of each category’s words. The
execute split is constructed by sampling 100 such combinations, using the remaining 20% of each
category’s words. The task’s in-context and execution accuracy are evaluated using an exact set
match, by treating the output (and yk) as a set of strings.
Rhymes
yk is a rhyme of the input word xk. The data was constructed by taking words categorized
at the A1, A2, or B1 levels according to CEFR. We then use CMU’s pronouncing dictionary12 to ﬁnd
rhyming groups for these words. The execute split is constructed by sampling 30 rhyming groups,
each containing two or more words, and sampling 100 unique words. The induce split is constructed
using the rest of the rhyming groups. We evaluate this task by checking whether the predicted word
is contained in the rhyming group of xk.
Larger Animal
xk is two animals, and yk is the (physically) larger one. We use the object
comparison data from oLMpics [Talmor et al., 2020], taking the train split, which only contains
animals. We construct the induce set using a sample of 80% of the animals and the execute set by
sampling 100 pairs out of the remaining 20% animals.
Cause Selection
xk contains two sentences describing related events, where one event caused the
other; yk contains the cause sentence. As data source, we use the 50 examples from the BIG-bench
[BIG-bench collaboration, 2021] Cause and Effect task, randomly splitting them to equally-sized
induce and execute sets. In each of the induce demonstrations, we randomly sample the position of
the cause sentence (either the ﬁrst or the second sentence in xk). For examples in the execute set, we
take both options for each cause and effect pair, doubling the data.
Common Concept
xk contains a few entities that share a non-trivial common underlying concept,
while yk describes that common concept. We use the 32 examples from Novel Concepts in BIG-bench
[BIG-bench collaboration, 2021], using half for induce and half for execute. As the BIG-bench
answers usually contain clear “task markers” (e.g., answers that start with “They all have...”, indicating
that the task was to ﬁnd a common concept), we remove them from our demonstrations. The task’s
in-context and execution accuracy are evaluated using unigram overlap (F1).
Formality
xk is a sentence in informal English, and yk is its paraphrase in more formal language.
We write 30 sentence pairs ourselves, following existing guidelines for converting informal sentences
into formal ones.13 The task’s in-context and execution accuracy are evaluated using unigram overlap
(F1).
Sum
xk contains two numbers separated by a space, and yk is their sum. For each number in the
range [0, 99], we enumerate over all pairs.
Difference
xk contains two numbers separated by a space, and yk is the difference between them.
We use all number pairs such that both input numbers are in the range [0, 198], and always subtract
the smaller number from the bigger number.
Number to Word
xk is a number written in digits (e.g., 28), and yk is the same number written in
words (e.g, twenty-eight). We use all numbers in range [0,9999].
11https://languageresearch.cambridge.org/american-english
12https://github.com/cmusphinx/cmudict
13https://www.niu.edu/writingtutorial/style/formal-and-informal-style.shtml,
https://www.uts.edu.au/current-students/support/helps/self-help-resources/grammar/
formal-and-informal-language
15

Translation
xk is an English word and yk is its translation to some target language – either German,
Spanish, or French. We use CEN as input words, and obtain their translations via Wiktionary.14
For evaluation, we check whether the predicted answer is contained in the set of the possible gold
answers.
Sentiment Analysis
xk is a movie review and yk is a binary label, either “positive” or “negative”,
marking the review’s sentiment. We use the Stanford Sentiment Treebank dataset [Socher et al.,
2013] from GLUE [Wang et al., 2018], taking the train split as our induce set and the dev split as
the execute set. We consider only full sentences, discarding sentence constituents and sentences
containing more than 10 words. This leaves us with an induce set of 1,167 examples. To create
label-balanced instruction induction examples, we sample each sequence of 5 demonstrations such
that there are at least 2 demonstrations for each label.
Sentence Similarity
xk contains two sentences, and yk reﬂects the semantic similarity of the two
input sentences. The similarity is measured on a scale of 0 to 5, and the labels contain an additional
short textual description of the numerical label, e.g., “5 - perfectly”. We use the Semantic Textual
Similarity Benchmark dataset [Cer et al., 2017] from GLUE, rounding the similarity scores and
taking the train split as the induce set and the dev split as the execute set. We discard examples in
which at least one of the sentences contains more than 10 words, which leaves us with an induce set
of 3,716 examples. In each instruction induction example, we sample at least one pair with a score
of 0 and one with a score of 5, so that models will be exposed to the minimal and maximal scores
when generating an instruction. We evaluate whether the predicted answer matches one of three valid
outputs for each label: the numerical label (“5”), the verbal label (“perfectly”), or the combined label
(“5 - perfectly”).
Word in Context
xk contains a target word and two contexts (sentences) for that word, and yk is a
binary label reﬂecting whether the word has the same meaning in both contexts. We use the Word
in Context dataset [Pilehvar and Camacho-Collados, 2019] from SuperGLUE [Wang et al., 2019],
taking the train split as the induce set and the dev split as the execute set. We discard examples in
which at least one of the sentences contains more than 10 words, which leaves us with an induce
set of 4,084 examples. To create label-balanced instruction induction examples, we sample each
sequence of 5 demonstrations such that there are at least 2 demonstrations for each label. We evaluate
whether the predicted label matches one of several possible outputs: “same”, “yes”, or “true” for an
identical meaning, and “not the same”, “no”, or “false” for a different meaning.
A.2
Common English Nouns
We create a dataset of common English nouns (CEN) by ﬁltering high-frequency nouns from the
Wikitext-103 corpus [Merity et al., 2017]. We ﬁrst create a vocabulary of the 10,000 most frequent
words in the corpus, from which we will later select the nouns. We then process the corpus with
SpaCy’s part-of-speech tagger and lemmatizer,15 and retain only nouns that appear in their singular
form by verifying that their part-of-speech tag is “NN” and testing whether the word’s lemma is
identical to the word itself. We additionally ﬁlter nouns that have less than 3 letters. Overall, this
leaves us with a set of 3,406 nouns.
B
Data Veriﬁcation
Table 4 shows the results for the data veriﬁcation experiments (§3.3). As evident by these results,
most of our tasks can be inferred in-context by models. Moreover, all tasks but one can be accurately
described by at least 4 out 5 human annotators.
14https://github.com/open-dsl-dict/wiktionary-dict
15https://spacy.io/
16

Task
In-Context Learning
Human Study
GPT-3
InstructGPT
First Letter
97
98
100
Second Letter
25
34
100
List Letters
98
100
100
Starting With
33
46
80
Pluralization
95
99
100
Passivization
100
100
80
Negation
94
93
100
Antonyms
84
83
100
Synonyms
9
12
80
Membership
13
36
100
Rhymes
46
39
100
Larger Animal
58
82
40
Cause Selection
47
82
100
Common Concept
23
15
100
Formality
54
56
80
Sum
87
100
100
Diff
69
95
100
Number To Word
85
100
100
Translation en-de
80
85
100
Translation en-es
91
88
100
Translation en-fr
80
84
80
Sentiment
95
99
100
Sentence Similarity
3
15
80
Word in Context
56
61
80
Table 4: Data veriﬁcation results. The in-context learning scores show how well models can infer our
tasks, and the human study scores show how often humans write the correct instruction given the
instruction induction prompt. All scores above or equal to 80% are in bold.
17

