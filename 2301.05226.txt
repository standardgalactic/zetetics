See, Think, Conﬁrm: Interactive Prompting Between
Vision and Language Models for Knowledge-based Visual Reasoning
Zhenfang Chen1*
Qinhong Zhou2∗
Yikang Shen1
Yining Hong3
Hao Zhang4
Chuang Gan1,4
1MIT-IBM Watson AI Lab
2Tsinghua University
3University of California, Los Angeles
4UMass Amherst
Abstract
Large pre-trained vision and language models have
demonstrated remarkable capacities for various tasks. How-
ever, solving the knowledge-based visual reasoning tasks
remains challenging, which requires a model to comprehen-
sively understand image content, connect external world
knowledge, and perform step-by-step reasoning to an-
swer the questions correctly. To this end, we propose a
novel framework named Interactive Prompting Visual Rea-
soner (IPVR) for few-shot knowledge-based visual reasoning.
IPVR contains three stages, see, think and conﬁrm. The see
stage scans the image and grounds the visual concept candi-
dates with a visual perception model. The think stage adopts
a pre-trained large language model (LLM) to attend to the
key concepts from candidates adaptively. It then transforms
them into text context for prompting with a visual captioning
model and adopts the LLM to generate the answer. The con-
ﬁrm stage further uses the LLM to generate the supporting
rationale to the answer, verify the generated rationale with a
cross-modality classiﬁer and ensure that the rationale can in-
fer the predicted output consistently. We conduct experiments
on a range of knowledge-based visual reasoning datasets.
We found our IPVR enjoys several beneﬁts, 1). it achieves
better performance than the previous few-shot learning base-
lines; 2). it enjoys the total transparency and trustworthiness
of the whole reasoning process by providing rationales for
each reasoning step; 3). it is computation-efﬁcient compared
with other ﬁne-tuning baselines.
1. Introduction
We study the problem of knowledge-based visual rea-
soning (KB-VQA) [49, 53], which requires models to rec-
ognize the image content, recall open-world knowledge,
and perform logical reasoning to arrive at an answer. KB-
VQA is more challenging than traditional visual question
*indicates equal contributions
This is a 
coffee 
table. 
Question:
What is the name 
of the room?
1. See
2. Think
3. Confirm
There is a 
long sofa
with brown 
pillows.
…
The answer is 
living room.
Because sofa and 
coffee table are 
usually located in 
the living room.
✓
✓
✓
Figure 1. The human process to handle knowledge-based visual
reasoning. Given an image-question pair, a human is able to see
all the objects in the image, think of the related visual concepts to
get the answer, and ﬁnally conﬁrm the answer is correct based on
visual observation and knowledge.
answering [5, 25] since the models need to understand ex-
ternal knowledge besides perceiving visual content. It has
many real-world applications such as chatbots [54], assis-
tive robots [6], and intelligent tutoring [3,63]. As depicted
in Fig 1, to answer the question “What is the name of the
room?”, humans need ﬁrst to see the image and extract vi-
sual concepts such as “frame”, “sofa”, and“lamp”. We then
attend to the key concepts that are semantically related to the
question and think that “This is a coffee table” and “There
is a long sofa with brown pillows” to get the answer “living
room”. We can ﬁnally conﬁrm the answer is correct because
“Sofa and coffee table are usually located in the living room”.
Inspired by the success of large language models (LLMs)
in few-shot natural language processing reasoning [7, 70],
arXiv:2301.05226v1  [cs.CV]  12 Jan 2023

there are several works to adopt LLMs for reasoning in
vision and language tasks. The dominant approaches are
mainly divided into two categories. The ﬁrst category adds
additional visual perception modules to transform the vi-
sual inputs into latent inputs for LLMs, and ﬁnetunes the
models with massive vision-language data [1, 32, 56]. Al-
though such a pipeline could achieve high performance on
the downstream visual reasoning tasks, it requires a large
vision-language dataset to ﬁnetune the LLM and the new
visual modules for each downstream task [32, 56], which
are typically computational-intensive and time-consuming.
The second category uses prompt-based methods for visual
reasoning. For example, PICa [64] translates images into
captions, which can then be used as textual prompt inputs
for GPT3 models [7] to answer the questions. Despite their
high accuracy on knowledge-based visual question answer-
ing [49], their model has several limitations. First, the cap-
tioning processing in PICa is independent of the question’s
semantics, limiting the caption to focus only on the image’s
general aspects instead of the question-related objects. Sec-
ond, their pipeline can not provide a step-by-step reasoning
trace, leaving the question-answering a black-box process.
The key question we would like to investigate is how we
can leverage the large vision and language models to achieve
high accuracy for knowledge-based VQA, while maintaining
transparency and efﬁciency of the reasoning process. The
ideas of neural-symbolic reasoning models [12,46,65,66]
offer a promising step-by-step transparent visual reasoning
approach. They typically transform the input question into
a program consisting of a series of operations, and execute
these operations with a set of network modules to get the
answer. However, their success in both performance and
transparency has mainly been limited to speciﬁc domains
with simple visual primitive concepts (e.g., simulated physi-
cal scenes with only three shapes [33]). Moreover, they often
need to manually design and implement symbolic logic for
each operation, limiting their applications to open-world
knowledge-based visual reasoning.
Towards more effective and interpretable complex knowl-
edge reasoning in the visual world, we propose a novel frame-
work named Interactive Prompting Visual Reasoner (IPVR)
to mimic the human reasoning process in Fig. 1. Our model
also contains three key modules, a see module, a think mod-
ule, and a conﬁrm module. Given an image-question pair,
the see module ﬁrst uses a scene parser to extract all the
candidate visual concepts in the image. The think module
adopts an LLM to select relevant visual concepts (e.g. “Sofa”
in Fig. 1) extracted by the see module corresponding to
the given task, and uses a captioning model to transform
them into textual descriptions(e.g. “There is a long sofa
with brown pillows”). The LLM predicts the answer to the
question (“living room”) based on the attended visual con-
text. Moreover, we introduce a conﬁrm module for ratio-
nale veriﬁcation to provide more transparent and trustworthy
reasoning. Speciﬁcally, we require the LLM to generate ra-
tionales (e.g. “Sofa and coffee table are usually located in
the living room” for the predicted answer in Fig. 1). We
then estimate the matching similarity between these ratio-
nales and the given visual input with a neural cross-modality
classiﬁer. Finally, the selected rationale is fed to the LLM’s
prompt to ensure that the rationale can infer the same output
consistently. We repeat the process of think and conﬁrm
iteratively until the answers from two consequent iterations
are the same.
To summarize, we introduce IPVR, a novel modularized,
interactive and iterative framework for knowledge-based
visual reasoning, which is able to iteratively attend to the
related visual concepts in the image and provide consistent
supporting rationales for the answer prediction. IPVR enjoys
several advantages. First, it is effective. Extensive experi-
ments on knowledge-based benchmarks found that IPVR
achieves better performance than previous few-shot base-
lines. Moreover, IPVR is more transparent and interpretable
since it maintains the whole step-by-step reasoning trace that
leads to the prediction. IPVR is also more computationally
efﬁcient compared with ﬁnetuning methods.
2. Related Work
Prompting Large Pre-trained Models. Large language
models like GPT-3 [7] have popularized few-shot prompting
in natural language processing (NLP), where several input-
output pairs are used as context for the language model to
understand the task and generate predictions for a new ex-
ample. Some prompting techniques [16,47,60,73] have also
been developed for more effective or transparent reasoning
in NLP. Later, prompting was brought to the vision commu-
nity [24,31,34,59,74,75]. CLIP [51] and RegionCLIP [72]
enable zero-shot classiﬁcation and detection by replacing
the class labels with natural language supervision during
training. UnitedIO [44] speciﬁes each vision task with a lan-
guage prompt to perform multi-task learning. Differently,
we aim to use interactive prompting for knowledge-based
visual reasoning. It requires frequent interaction between lan-
guage models and vision models, such as extracting visual
concepts related to the task, recalling corresponding external
knowledge, and verifying the text prediction consistent with
the visual context, which has not been well studied before.
Large Pre-trained Models for Visual Reasoning. Large
pre-trained models have also been used in reasoning over
vision and language [21, 22, 32, 61, 69]. Most works [10,
32,37,39,56,68] learn large pre-trained models from mas-
sive vision-language data and ﬁnetune them for downstream
tasks, which are usually extremely computationally expen-
sive and time-consuming. For example, Flamingo [1] needs
to be ﬁnetuned on 1536 TPUv4 for 15 days with 185 million
images and 182 GB of text. It has also been observed that

meat
knife
beans
plate
plate
napkin
…
Detect
Input Image & Question
(A) See Module
knife
meat
Attend
Predict
(B) Think Module
(C) Confirm Module
“the knife is 
for cutting 
the meat.”
Explain
Verify
Vision Model
Language Model
Which food item is 
the knife for?
Image
Question
Meat
Q
Q
I
I
“A close up of 
a plate of food 
on a table”
Describe !"#!
2.“There are some pieces of meat on the plate.”
1. “Plates with food and a knife.”
Describe !"#"
{!#}
!"#!
!"#"
!"
"" =
'" =
Loop
Figure 2. The framework of our IPVR. Given an image-question pair, we ﬁrst use the see module to detect all object candidates in the
image and translate the whole image into a global description. Then, the think module adopts an LLM to attend to the key visual concepts,
transforms the selected concept into a language description with a captioner, and leverages the LLM to predict an answer. The conﬁrm
module requires the LLM to continue the generate the supporting rationale, verify whether the rationale is consistent with the image content,
and ensure that the same answer can be produced when the rationale is added to the prompt in the next iteration.
many of these pre-trained models (e.g., [35, 55]) contain
limited open-world knowledge. They achieve inferior perfor-
mance on KB-VQA datasets, compared with models with
LLMs for external knowledge [26,64]. Recently, Yang et al.
converted images into textual descriptions and treated them
as prompts for LLMs, which achieves high performance on
knowledge-based visual question answering. However, its
text-based visual context is independent of the query ques-
tion and leaves the question-answering process a black box.
Knowledge-based Visual Reasoning. Our work is also re-
lated to knowledge-based visual question answering (KB-
VQA) [49, 53, 57], which requires both understanding the
image content and retrieving external knowledge to answer
the questions. Most early methods [20,23,29,40,58,71,76]
use deep neural networks to understand images and retrieve
relevant knowledge from explicit knowledge bases. Recent
methods [26,36,41,64] found that LLMs like GPT-3 could
serve as a knowledge base and use the LLM to answer the
question directly. While our model also retrieves relevant
knowledge from LLMs, it provides the step-by-step reason-
ing process besides the ﬁnal answer prediction.
Neural-Symbolic Visual Reasoning. Our work could also
be regarded as neural module networks [2, 4, 9, 13, 18, 19,
46,66], which provides transparent step-by-step reasoning.
They typically decompose the query question into a set of op-
erations, model each operation with a network module, and
iteratively execute these operations to get a transparent result.
While these models are more interpretable, they either focus
on simple physical scenes [33,65] or only achieve inferior
performance on real-world datasets [5,30], compared with
end-to-end neural network methods [11,38]. Different from
them, we would like to build a system that can achieve rea-
sonable performance on open-world knowledge-based visual
reasoning while maintaining the system’s interoperability.
3. Method
3.1. Overall
In this section, we introduce a new model called Inter-
active Prompting Visual Reasoner (IPVR) for knowledge-
based visual reasoning, which can understand the query ques-
tion, attend to key visual concepts in the image, retrieve
supporting evidence, and ﬁnally get the answer in a step-by-
step manner. IPVR consists of three modules, see, think and
conﬁrm and run these modules in an iterative manner. As
illustrated in Fig. 2, given an image and a question about its
content, the see module uses a scene parser [28] to detect all
the candidate objects (concepts) in the image and represents
them with their predicted class names. It also generates a
global description for the whole image. Then, think module
attends to the key concepts that are semantically related to
the query question with an LLM [70] and describes them in
the form of natural language with an image captioner [39].
Based on the attended visual context, the LLM predicts the

answer to the question. The conﬁrm module requires the
LLM to continue to generate the answer’s supporting ratio-
nale and verify them with a cross-modality classiﬁer [51].
To ensure the rationale is consistent with the answer, we
add the veriﬁed supporting rationale back into the prompting
context and begin a new think-conﬁrm iteration. We itera-
tively generate the answer and the rationale until the answer
predictions in two consequent iterations are consistent. We
summarize the whole algorithm ﬂow in Algorithm 1.
Compared with existing learning-in-context methods [60,
64], our framework has two advantages, effectiveness and
interpretability. It is effective since it can adaptively attend
to the related visual regions and output consistent answer-
rationale predictions. It is interpretable as it is able to perform
a step-by-step investigation of the whole reasoning process.
Visualized examples of such a step-by-step reasoning process
can be found in Fig. 4.
3.2. Model Details
See Module.
Given a query image, we use a Faster-
RCNN [52] to detect all the object candidates in the im-
age. Speciﬁcally, we use the detection model released by
Yang et al. [28] to predict object locations and their categor-
ical labels such as “knife”, “plate” and “napkin”. It also
provides a global caption for the whole image with an image
captioner [39]. These visual concepts will be selected and
further described in the think module to provide valuable
visual context to get the answer.
Think module.
The second module of our framework is
the think module, which attends to the corresponding regions
in the image and transforms them into the textual description
for the LLM to predict the answer. To achieve this goal, we
use an attend-describe-predict approach in the think module,
as shown in Fig. 2 (B).
The ﬁrst step is attend, where we use prompting meth-
ods [7,14] to help the LLM to attend to the key concepts in
the image that is semantically related to the query question.
We show the prompting template for the LLM to attend to
key visual concepts in Fig. 3 (A). We feed some input-output
pairs from the training set into the LLM’s prompt and ask
the LLM to select based on the given context.
Speciﬁcally, the question is shown on the top of the tem-
plate. Objects detected in the see module are represented
by their category names, such as “knife” and “beans”, and
the vocabulary of LLM output words is constrained to these
category names. The LLM selects the most related object to
provide further visual context to handle the query question.
As shown in Fig. 2 (B), such attended concepts (e.g. “meat”
and “knife”) are important to get the answer “meat” to the
question “Which food item is the knife for?”.
The next step of the think module is describe. The region
in the image corresponding to the selected concept is cropped
Algorithm 1: Pipeline of the proposed IPVR
Input: Input Image and Question {I, Q}.
Output: Answer and the reasoning process {a∗, R}
Require: cn is the n-th concept detected in the image; ci
and capi are the concept and the regional caption
attended at the the i-th iteration; capg denotes a caption
for the global image. mIter: the maximal iteration;
Pthk and Pcon are the prompt text for concept selection
and question answering.
1: # the see module
2: {cn}N
n=1 ←ImageParser(I)
3: capg ←GlobalCaptioner(I)
4: i starts from 0. a0 is an empty string; Pcon,0 is the
in-context examples of the question, answer, and
rationales; Pthk is the in-context examples of the
question and object selection.
5: repeat
6:
i = i + 1
7:
# the think module
8:
ci ←LLMAttend({cn}N
n=1, Q, Pthk)
9:
capi ←Captioner(ci, I)
10:
Pcon,i = Pcon,i−1 + capi
11:
ai ←LLMPredict (Pcon,i, Q)
12:
# the conﬁrm module
13:
ri ←LLMConﬁrm (Pcon,i, Q, ai)
14:
if V erify(ri, I) > thre then
15:
Pcon,i = Pcon,i + ri
16:
end if
17: until ai == ai−1 or i == mIter
18: a∗←ai ;
R = ({cj, capj}i
j=1, ri)
19: return {a∗, R}
and fed to an image captioner [39] to generate a regional
description for the new concept. The generated regional
descriptions will be added to the LLM’s prompt to provide
ﬁne-grained visual context to predict an answer. Note that a
regional description for an object is usually more informative
than the object class. For example, the caption of “some
pieces of meat on the plate” in Fig. 2 additionally describes
the relationship between “meat” and “plate”.
The last step of the think module is predict. We add
multiple question-answering examples from the training set
to the LLM’s prompt and ask it to predict an answer to the
question. The answer prediction is based on the attended
visual context and the rationale predicted by the conﬁrm
module in the previous iterations, which we will discuss in
the conﬁrm module.
Conﬁrm Module.
The last module of our framework is
the conﬁrm module, as shown in Fig. 2 (C), which aims
to generate a consistent supporting rationale for the answer

Question: What is located on the 
shelves?
The most related option is shelf.
Question : Which object is used for 
warmth in this room?
The most related option is fireplace.
Question: What is the cabinet to the 
left called?
The most related option is cabinet.
Context: A fully cooked pizza sitting on a tray with a spatula digging.
Question: What is another tool used to cut this type of food?
Answer: The answer is knife. A pizza cutter cuts pizza.
Context: Sandwich in paper on counter with man in background.
Question: Where is this meal being eaten?
Answer: The answer is restaurant. The meal is at a restaurant.
Context: A couple of men preparing food inside of a kitchen. The 
restaurant is a pizza restaurant. Someone making a pizza with 
cheese, bacon, and cheese. Someone holding some food on a plate. 
Question: What type of restaurant is this?
Answer: The answer is pizza. The restaurant is a pizza restaurant.
…
…
(A) Prompting for concept attention.
(B) Prompting for question-answering and rationale.
Figure 3. Examples of prompting templates for visual concept attention and the full question-answer-rationale reasoning in Fig. 3 (A)-(B).
Outputs in the in-context examples and test examples are marked with blue and green colors. The attentive regional captions and the
rationale in the previous iterations are marked with red and bronze colors. We regard the most related option in the in-context examples as
the option (concept) closest to the ground-truth answer by CLIP similarity.
prediction and verify the prediction’s correctness. Given the
few-shot example context and the interactive prompt gener-
ated by the think module, we require the LLM to continue
to predict the supporting rationale after the answer predic-
tion. A prompt example of such a question-answer-rationale
template can be found in Fig. 3 (B). A signiﬁcant problem
of the LLM’s prediction is that the generation procedure is a
black box, and it is difﬁcult to verify the correctness of the
predicted answer and rationale. We believe a correct ratio-
nale should have two distinct features. First, the rationale
should be consistent with the answer. Given the predicted
supporting rationale (“The knife is for cutting the meat” in
Fig. 2) for the answer (“meat”), we should be able to predict
the same answer when it is added to the context. Second, the
rationale should be consistent with the visual input.
To ensure that the rationale supports the answer pre-
diction, we feed the generated textual rationale into the
LLM’s prompt in the next iteration. We repeat this answer-
to-rationale and rationale-to-answer procedure until the two
consequent predicted answers are the same (Line 4 to Line
17 of the Algorithm 1). To ensure that the generated rationale
is consistent with the given visual context, we use a large
pre-trained cross-modality classiﬁer [51] to verify whether
the textual rationale matches the given images or not (Line
13 to Line 15 of the Algorithm 1). Only the rationale with
the high matching similarity will be accepted and added to
the prompt for the answer prediction in the next iteration.
4. Experiments
In this section, we demonstrate the advantages of the
proposed IPVR with extensive experiments. We ﬁrst intro-
duce our experimental setting. Then, we compare our IPVR
with other methods on knowledge-based visual reasoning
benchmarks [49,53] to demonstrate our method’s effective-
ness and interpretability. We also evaluate each module’s
effectiveness with an ablation study.
Implementation Details. The effectiveness of the proposed
IPVR relies on the interaction of several pre-trained vision
models [28, 39, 51] and language models [70]. We choose
the faster R-CNN model [52] released by Han et al. [28]
to detect visual concepts in images, which was trained on
Visual Genome [36] with 1,595 diverse classes. We select
BLIP [39] as the regional captioning model for the attended
objects. We crop the object candidate from the image and
expand its bounding box 1.5 times to provide additional
visual context for captioning. To make the regional captions
more consistent with the question, we generate the regional
caption with constrained decoding [45]. We provide more
implementation details in the supplementary material. We
use the OPT-66B as the large pre-trained language model
to prompt since it is effective, publicly available, and easy
to run with 6 NVIDIA V100 PCIe 32 GB. We verify the
matching similarity between the rationale and the image
with the CLIP model (ViT-B/16) [51]. We do not use GPT-
3 [7] for experiments due to its expensive API and limited
visit frequency to the public.
The think module of Section 3.2 requires in-context ex-
amples to inform the LLM of the task to make predictions.

We ﬁx the number of the in-context examples to 8 since it
is the largest number we could efﬁciently run on our hard-
ware conﬁguration. Following Yang et al. [64], we prompt
the LLM with in-context example selection and multi-query
ensemble. For in-context examples, we select the examples
most similar to the current image-question pair in training
set with their clip features. For multi-query ensemble, we
feed our models and the baselines 5 times and select the one
with the highest log-probability as previous methods [8,64]
except the aligned baseline models in Table 3, where we
ensemble 14 times for baselines to make them have similar
computation cost as our model.
Datasets and Evaluation Metric. We evaluate our mod-
els on standard KB-VQA benchmarks, OK-VQA [49]
and A-OKVQA [53]. OK-VQA is the most popular
knowledge-based question-answering dataset with 14,055
image-question pairs, asking questions of diverse knowledge
such as transportation, food, and weather. A-OKVQA is
the current largest KB-VQA dataset, which not only asks
knowledge-related questions but also provides supporting
rationales, making it a better testing bed for step-by-step rea-
soning. We do not conduct experiments on other KB-VQA
datasets like F-VQA [57] and KB-VQA [58], since they
assume the question knowledge could be retrieved from
pre-deﬁned knowledge bases (e.g. ConceptNet [42] and
Wikipedia). Pre-deﬁned knowledge makes them less practi-
cal and not representative enough of the complex real-world
knowledge visual reasoning. We adopt the widely-used soft
accuracy [25] as the evaluation metric.
Baselines. We mainly compare our methods with two
strong learning-in-context baselines, PICa [64] and Chain-
of-Thought (CoT) [60].
• PICa. PICa with GPT-3 is the current state-of-the-art few-
shot model on OK-VQA, which prompts the LLM with
only the image caption and object tags.
• CoT. CoT is a popular few-shot prompting method, which
asks the model to perform step-by-step reasoning to solve
the task rather than directly output the answer. We imple-
ment CoT by asking the LLM to generate the reasoning
step (rationale) ﬁrst and then predict the answer.
We carefully implement these two methods with the OPT-
66B model for a fair comparison. We also include other fully-
supervised methods in the tables for performance reference.
Prompt examples of the baseline methods can be found in
the supplementary.
4.1. Compared with Other Methods.
Quantitative Results. We compare our IPVR with baselines
on the validation and test sets of the A-OKVQA dataset in
Table 1. Few-shot learning-in-context methods are marked
with ⋆in the table, and our method is marked with gray back-
ground for easy reference. According to the results, we have
Methods
A-OKVQA
OK-VQA
Val
Test
Test
MAVEx [62]
-
-
41.37
UnifER [27]
-
-
42.13
Pythia [67]
25.2
21.9
-
ViLBERT [43]
30.6
25.9
-
LXMERT [55]
30.7
25.9
-
KRISP [48]
33.7
27.1
38.4
PICa⋆[64]-GPT-3 [7]
-
-
48.0
GPV-2 [35]
48.6
40.7
-
KAT [26]-GPT-3 [7]
-
-
54.4
CoT⋆[60]
41.5
43.7
38.1†
PICa⋆[64]
42.4
43.8
42.9
Ours⋆
46.4
46.0
44.6‡
Table 1. Performance comparison of our model and other base-
lines on A-OKVQA and OK-VQA datasets. ⋆denotes learning-in-
context methods. ‡ denotes our model’s conﬁrm module is removed
since there are no available examples with rationales in OK-VQA.
† denotes in-context examples with the rationales for CoT are from
A-OKVQA dataset. Our model performs better than baselines.
the following observations. First, we have constant gains
compared with the learning-in-context baselines, PICa and
CoT and even achieve better performance than the previous
full-supervised pre-trained method GPV-2 on the test split
of A-OKVQA. These gains show our method’s effective-
ness in answer predictions. We have also noticed that fully-
supervised methods have a more signiﬁcant performance dis-
parity between the validation and test splits than the learning-
in-context methods. For example, GPV-2 has an accuracy
drop of 7.9, while our method only drops 0.4. We believe
the reason is that the validation set has frequently been eval-
uated in fully-supervised methods in different epochs and is
somewhat overﬁtted.
We also evaluated our method in the OK-VQA in Table 1.
The training set of the OK-VQA does not provide rationales
for reasoning, which is needed in CoT and our method. For
our method, we develop a variant that only uses examples
in OK-VQA as prompting for the think module without
rationale reasoning. The model stops when the predicted
answers from two consecutive iterations become the same.
We observe that our model performs better than the baseline
methods, PICa and CoT, which is consistent with our ﬁnding
in the A-OKVQA dataset. We can also see that the state-
of-the-art methods (KAT-GPT-3 and PICa-GPT-3) rely on
much more powerful LLM, GPT-3 (175B), to achieve great
performance. In contrast, our method achieves reasonable
performance by integrating the publicly-available OPT-66B
LLM.
Qualitative Results. The step-by-step reasoning nature of
our IPVR provides better transparency and interoperability,

What is the fence 
meant to block?
Input
Global Caption
Caption: A man 
running across 
a tennis court 
holding a 
racquet.
Ball: Someone has a tennis 
racket and is about to hit a ball.
Attend
Tennis court: Someone playing 
a sport on a tennis court.
Explain
The fence is meant 
to block the ball.
Prediction
ball
The fence is meant 
to block cars.
Ours
CoT
PICa
ball
Image
Question
GT
cars
cars
All tags: fence, ball, car, house, 
man, roof, bat, tennis court, 
shirt, window…
What is this wall 
used for?
Caption: A 
living room 
with a couch a 
table and a 
lamp.
Picture: Someone is taking a 
picture of some art.
Picture: There's a lamp and a 
picture hanging on the wall.
The wall is used for 
displaying art.
art
The wall is used for 
a sofa.
Ours
CoT
PICa
art
Image
Question
GT
sofa
wall
All tags: lamp, wall, door, 
couch, pillow, pillow, picture, 
picture, table…
ball
art
Figure 4. Qualitative results of our IPVR and baselines. Besides better answer accuracy, our method also enjoys better transparency by
providing a step-by-step reasoning trace with related visual concepts, regional descriptions, and a supporting rationale. →denotes our
reasoning ﬂow.
which makes it easy to understand how our model works.
We provide a qualitative comparison with baselines in Fig. 4.
The →in Fig. 4 shows the ﬂows of our method’s reasoning
process to get related visual concepts, regional descriptions,
and the supporting rationale.
Compared with PICa, our method can adaptively attend
to key visual concepts (e.g. “ball” and “tennis court” in
Fig. 4) in the image that are semantically important to the
question (e.g. What is the fence meant to block?) and describe
that in the form of the natural language (e.g. “Someone
has a tennis racket and is about to hit the ball”) to get the
answer. Besides, as shown in the explain column of Fig. 4,
our method could generate better supporting rationale (“The
wall is used for displaying art”), which matches with the
visual context in images better. In contrast, the CoT might
generate a rationale inconsistent with visual context (“The
wall is used for a sofa.”), which leads to a wrong answer.
Rationale Evaluation. To further evaluate the reasoning
process of our method, we compare the quality of rationales
between our method and CoT in Table 2 on the validation
set of A-OKVQA, where the rationales are publicly avail-
able. We use widely-used BLEU scores and CLIP sentence
similarity as the metrics. We measure BLEU calculated by
multi-bleu.perl 1 and averaged cosine similarity of sentence
representations calculated by CLIP (ViT-B/16) [51] text en-
coder. Both BLEU and similarity results show that the ratio-
nales generated by our method are closer to the ground truth
than the rationales generated by CoT.
Computation Analysis. Although we have suggested that
1https : / / github . com / moses - smt / mosesdecoder /
blob/master/scripts/generic/multi-bleu.perl
Methods
BLEU
Sentence Similarity
CoT
14.19
80.92
Ours
14.34
81.22
Table 2. Rationale performance comparison of our model and CoT
baseline on A-OKVQA validation set. The rationales generated by
our IPVR are closer to ground truth than those generated by CoT.
Methods
A-OKVQA
OK-VQA
PICa
42.40
42.94
PICa-aligned
41.90
42.84
CoT
41.53
38.13
CoT-aligned
42.10
38.15
Ours
46.41
44.62
Table 3. Analysis of computational cost on A-OKVQA validation
set and OK-VQA set. Our method still outperforms PICa and CoT
after considering the computational cost issue.
.
our method is more efﬁcient than ﬁnetuning methods (e.g. [1]
and [32]), it isn’t entirely obvious that this is still the case
when compared with other in-context learning methods.
After all, the step-by-step manner of IPVR requires more
queries to large models. To better understand the efﬁciency of
our method, we make baselines have similar computational
costs as our model. Speciﬁcally, we increase the number
of queries to ensemble k (5 in all the experiments except
PICa-aligned and CoT-aligned below) for PICa and CoT, and

Why are the wood 
platforms strapped 
to the elephants?
Input
Caption & Object
Caption: An 
elephant with a 
wooden chair on its 
back.
Object: elephant, 
trunk, tree, ear, leaf, 
blanket, ground, 
rope, eye, seat, 
cloth, head, leg, face, 
bench, dirt, pole
Seat: The 
elephant has a 
wooden seat 
strapped on to 
its back with a 
seat.
Attend
Trunk: A small 
elephant with a 
long trunk.
Explain & Predict
The elephants 
are being used 
for transporta-
tion.
protect 
elephants
Transportation
What indicates a 
baby is present?
pacifier
Caption: A man 
holding a birthday 
cake with lit candles.
Object: cup, key, 
table, man, flame, 
candle, jacket, cake, 
straw, hair, shirt, tie, 
ear, hand, zipper, 
eye, picture, ceiling, 
nose, cell phone
Man: Guy with 
cake and a tie 
with a candle.
Picture: An 
image is shown 
in a picture 
with the picture 
in front.
A birthday cake 
usaully 
indicates via 
candles how 
old the person 
is.
baby
GT
GT
Figure 5. Failure cases on A-OKVQA validation set. Our model
will collapse when the scene parser fails to detect the key concept
(“paciﬁer” at the bottom) or when the LLM fails to predict the
answer (prediction on the top) based on the correct visual context.
make their overall amount of queries to LLM the same as
our method. Considering it takes 2.27 rounds on average for
our method to get ﬁnal answers and one additional query in
each round introduced by think module, the mean amount of
queries is 13.62 for our method. Therefore, the cost-aligned
PICa and CoT have an ensemble amount k = 14 for each
sample, denoted as PICa-aligned and CoT-aligned.
We compare their results with our method in Table 3,
which indicates that our method still outperforms PICa and
CoT signiﬁcantly after considering the computational cost is-
sue. We also notice a slight performance drop for PICa when
its ensemble amount k increases. We suggest that this drop
is related to the in-context sample selection method, which
chooses the top n ∗k questions with the highest similarities,
where n is the number of the in-context examples (8 in our
case). When k increases, the in-context questions of PICa
and CoT become more irrelevant to the current question.
Failure Cases. One distinct advantage of our method is
transparency and interoperability, enabling us to analyze
how the model fails. In Fig. 5, we provide two typical failure
cases of our method from the A-OKVQA validation set. Our
method fails at the predict step in the ﬁrst case. Although
the information from context is enough to ﬁnd the answer,
OPT-66B fails to predict it correctly. In the second case, our
method fails at the detect step. The key object name (“paci-
ﬁer”) is not in the object list provided by R-CNN. The failure
cases indicate that our method can perform better when more
powerful vision and language models are provided.
Ablation Study. We conduct a series of ablation studies to
answer the following questions. Q1: Is the attend and de-
scribe components in the think module necessary to achieve
Figure 6. Ablation study on A-OKVQA validation set. Each ablated
component has its contribution to the overall performance.
good performance? Q2: Does the rationale reasoning in con-
ﬁrm module improve the answer accuracy? Q3: Is it nec-
essary to verify the rationale’s consistency with the input
image? As shown in Fig. 4, Full denotes our models with all
the component integration. “w/o attend” denotes the IPVR
model without the attend and describe components in the
think module. “w/o rationale” shows the model without
generating the rationale in the think module. “w/o verify”
denotes the model without CLIP veriﬁcation in the conﬁrm
module and simply adds the rationales back into the next
answer prediction step.
We ﬁnd that the performance will drop without any ab-
lated component, showing each component has its contribu-
tion to the overall performance. We observe that the “attend”
module has the most signiﬁcant effect on the model’s overall
performance, where we think the reason is that the “attend”
component has provided essential visual context for the LLM
to infer the correct answer as shown in the examples in Fig-
ure 4 (answering Q1). The “w/o rationale” ablation shows
that adding the reasoning rationale into the model not only
increases the model’s transparency but also has positive ef-
fects on the answer prediction (answering Q2). From the
result of w/o “verify” ablation, we can observe that further
gain could be achieved if we verify the matching similarity
between the rationale and the input image (answering Q3).
5. Conclusion
In this paper, we develop a novel model named Interactive
Prompting Visual Reasoner (IPVR) for knowledge-based vi-
sual reasoning, which requires the model to understand both
the image content and external knowledge to answer the
query question. IPVR can adaptively focus on the related
visual concepts in the image, transform them into natural
language and provide consistent rationales to support the an-
swer prediction. Compared with existing prompting methods,
it not only achieves better performance but also maintains
high transparency by keeping the whole trace of each reason-
ing step. We hope that our IPVR can motivate more future

research on interactive prompting between pre-trained mod-
els of different modalities for building more effective and
interpretable visual commonsense reasoning systems.
References
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual
language model for few-shot learning. arXiv, 2022. 2, 7
[2] Saeed Amizadeh, Hamid Palangi, Alex Polozov, Yichen
Huang, and Kazuhito Koishida. Neuro-symbolic visual rea-
soning: Disentangling. In ICML. PMLR, 2020. 3
[3] John R Anderson, C Franklin Boyle, and Brian J Reiser. In-
telligent tutoring systems. Science, 1985. 1
[4] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan
Klein. Neural module networks. In CVPR, 2016. 3
[5] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.
Vqa: Visual question answering. In ICCV, 2015. 1, 3
[6] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol
Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex
Irpan, Eric Jang, Ryan Julian, et al. Do as i can, not as i say:
Grounding language in robotic affordances. In CoRL. 1
[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language
models are few-shot learners, 2020. 1, 2, 4, 5, 6
[8] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Hen-
rique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,
Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Eval-
uating large language models trained on code. arXiv, 2021.
6
[9] Wenhu Chen, Zhe Gan, Linjie Li, Yu Cheng, William Wang,
and Jingjing Liu. Meta module network for compositional
visual reasoning. In WACV, 2021. 3
[10] Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and
William W. Cohen. Murag: Multimodal retrieval-augmented
generator for open question answering over images and text.
In EMNLP, 2022. 2
[11] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,
Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam
Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-
scaled multilingual language-image model. arXiv, 2022. 3
[12] Zhenfang Chen, Jiayuan Mao, Jiajun Wu, Kwan-Yee K Wong,
Joshua B. Tenenbaum, and Chuang Gan. Grounding phys-
ical concepts of objects and events through dynamic visual
reasoning. In ICLR, 2021. 2
[13] Zhenfang Chen, Kexin Yi, Yunzhu Li, Mingyu Ding, Anto-
nio Torralba, Josh Tenenbaum, and Chuang Gan. Comphy:
Compositional physical reasoning of objects and events from
videos. In ICLR, 2022. 3
[14] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,
Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,
et al. Palm: Scaling language modeling with pathways. arXiv,
2022. 4
[15] Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm´an,
Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin
Stoyanov. Unsupervised cross-lingual representation learning
at scale. In ACL, 2020. 12
[16] Antonia Creswell, Murray Shanahan, and Irina Higgins.
Selection-inference: Exploiting large language models for
interpretable logical reasoning. arXiv, 2022. 2
[17] Maxime De Bruyn, Ehsan Lotﬁ, Jeska Buhmann, and Walter
Daelemans. Mfaq: a multilingual faq dataset, 2021. 12
[18] Mingyu Ding, Zhenfang Chen, Tao Du, Ping Luo, Josh Tenen-
baum, and Chuang Gan. Dynamic visual reasoning by learn-
ing differentiable physics models from video and language.
34:887–899, 2021. 3
[19] Mingyu Ding, Yan Xu, Zhenfang Chen, David Daniel Cox,
Ping Luo, Joshua B. Tenenbaum, and Chuang Gan. Embodied
concept learner: Self-supervised learning of concepts and
mapping through instruction following. In CoRL, 2022. 3
[20] Yang Ding, Jing Yu, Bang Liu, Yue Hu, Mingxin Cui, and
Qi Wu. Mukea: Multimodal knowledge extraction and accu-
mulation for knowledge-based visual question answering. In
CVPR, 2022. 3
[21] Zi-Yi Dou, Aishwarya Kamath, Zhe Gan, Pengchuan Zhang,
Jianfeng Wang, Linjie Li, Zicheng Liu, Ce Liu, Yann Le-
Cun, Nanyun Peng, et al. Coarse-to-ﬁne vision-language
pre-training with fusion in the backbone. arXiv, 2022. 2
[22] Feng Gao, Qing Ping, Govind Thattai, Aishwarya Reganti,
Ying Nian Wu, and Prem Natarajan.
Transform-retrieve-
generate: Natural language-centric outside-knowledge visual
question answering. In CVPR, 2022. 2
[23] Franc¸ois Gard`eres, Maryam Ziaeefard, Baptiste Abeloos, and
Freddy Lecue. Conceptbert: Concept-aware representation
for visual question answering. In Findings of EMNLP, 2020.
3
[24] Chunjiang Ge, Rui Huang, Mixue Xie, Zihang Lai, Shiji Song,
Shuang Li, and Gao Huang. Domain adaptation via prompt
learning. 2022. 2
[25] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra,
and Devi Parikh. Making the V in VQA matter: Elevating the
role of image understanding in Visual Question Answering.
In CVPR, 2017. 1, 6
[26] Liangke Gui, Borui Wang, Qiuyuan Huang, Alex Hauptmann,
Yonatan Bisk, and Jianfeng Gao. Kat: A knowledge aug-
mented transformer for vision-and-language. In NAACL, 2022.
3, 6
[27] Yangyang Guo, Liqiang Nie, Yongkang Wong, Yibing Liu,
Zhiyong Cheng, and Mohan Kankanhalli. A uniﬁed end-to-
end retriever-reader framework for knowledge-based vqa. In
ACM MM, 2022. 6
[28] Xiaotian Han, Jianwei Yang, Houdong Hu, Lei Zhang, Jian-
feng Gao, and Pengchuan Zhang. Image scene graph genera-
tion (sgg) benchmark. arXiv, 2021. 3, 4, 5
[29] Yu-Jung Heo, Eun-Sol Kim, Woo Suk Choi, and Byoung-
Tak Zhang. Hypergraph Transformer: Weakly-supervised
multi-hop reasoning for knowledge-based visual question
answering. In ACL. 3

[30] Drew A Hudson and Christopher D Manning. Gqa: A new
dataset for real-world visual reasoning and compositional
question answering. In CVPR, pages 6700–6709, 2019. 3
[31] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,
Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual
prompt tuning. In ECCV, 2022. 2
[32] Woojeong Jin, Yu Cheng, Yelong Shen, Weizhu Chen, and
Xiang Ren. A good prompt is worth millions of parame-
ters: Low-resource prompt-based learning for vision-language
models. In ACL, 2022. 2, 7
[33] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten,
Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A
diagnostic dataset for compositional language and elementary
visual reasoning. In CVPR, 2017. 2, 3
[34] Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and Weidi
Xie. Prompting visual-language models for efﬁcient video
understanding. arXiv, 2021. 2
[35] Amita Kamath, Christopher Clark, Tanmay Gupta, Eric Kolve,
Derek Hoiem, and Aniruddha Kembhavi. Webly supervised
concept expansion for general purpose vision models. In
ECCV, 2022. 3, 6, 12, 13
[36] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-
tidis, Li-Jia Li, David A Shamma, et al. Visual genome:
Connecting language and vision using crowdsourced dense
image annotations. IJCV, 2017. 3, 5
[37] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg,
Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for
video-and-language learning via sparse sampling. In CVPR,
2021. 2
[38] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin
Jiang. Unicoder-vl: A universal encoder for vision and lan-
guage by cross-modal pre-training. In AAAI, 2020. 3
[39] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip:
Bootstrapping language-image pre-training for uniﬁed vision-
language understanding and generation. In ICML, 2022. 2, 3,
4, 5, 12
[40] Weizhe Lin and Bill Byrne. Retrieval augmented visual ques-
tion answering with outside knowledge. In EMNLP, 2022.
3
[41] Yuanze Lin, Yujia Xie, Dongdong Chen, Yichong Xu, Chen-
guang Zhu, and Lu Yuan. Revive: Regional visual representa-
tion matters in knowledge-based visual question answering.
arXiv, 2022. 3
[42] Hugo Liu and Push Singh. Conceptnet—a practical com-
monsense reasoning tool-kit. BT technology journal, 2004.
6
[43] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:
Pretraining task-agnostic visiolinguistic representations for
vision-and-language tasks. In NeurIPS, 2019. 6, 12
[44] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mot-
taghi, and Aniruddha Kembhavi. Uniﬁed-io: A uniﬁed model
for vision, language, and multi-modal tasks. arXiv, 2022. 2
[45] Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo
Kasai, Daniel Khashabi, Ronan Le Bras, Lianhui Qin, Young-
jae Yu, Rowan Zellers, et al. Neurologic a* esque decoding:
Constrained text generation with lookahead heuristics. arXiv,
2021. 5, 12
[46] Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenen-
baum, and Jiajun Wu. The Neuro-Symbolic Concept Learner:
Interpreting Scenes, Words, and Sentences From Natural Su-
pervision. In ICLR, 2019. 2, 3
[47] Ana Marasovic, Iz Beltagy, Doug Downey, and Matthew
Peters. Few-shot self-rationalization with natural language
prompts. In Findings of NAACL, 2022. 2
[48] Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta,
and Marcus Rohrbach. Krisp: Integrating implicit and sym-
bolic knowledge for open-domain knowledge-based vqa. In
CVPR, 2021. 6, 12
[49] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and
Roozbeh Mottaghi. Ok-vqa: A visual question answering
benchmark requiring external knowledge. In CVPR, 2019. 1,
2, 3, 5, 6
[50] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam
Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari,
Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru
Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak,
Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff,
and Colin Raffel. Crosslingual generalization through multi-
task ﬁnetuning, 2022. 13
[51] Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh,
Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda
Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and
Ilya Sutskever. Learning transferable visual models from
natural language supervision. In ICML, 2021. 2, 4, 5, 7, 12
[52] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. NIPS, 2015. 4, 5
[53] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,
Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A bench-
mark for visual question answering using world knowledge.
2022. 1, 3, 5, 6
[54] Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael
Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal
Arora, Joshua Lane, et al. Blenderbot 3: a deployed conver-
sational agent that continually learns to responsibly engage.
arXiv, 2022. 1
[55] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality
encoder representations from transformers. In EMNLP, 2019.
3, 6, 12
[56] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Es-
lami, Oriol Vinyals, and Felix Hill. Multimodal few-shot
learning with frozen language models. In NeurIPS, 2021. 2
[57] Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and Anton
Van Den Hengel. Fvqa: Fact-based visual question answering.
PAMI, 2017. 3, 6
[58] Peng Wang, Qi Wu, Chunhua Shen, Anthony R Dick, and
Anton van den Hengel. Explicit knowledge-based reasoning
for visual question answering. In IJCAI, 2017. 3, 6
[59] Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou,
Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chenguang
Zhu, Derek Hoiem, et al. Language models with image de-
scriptors are strong few-shot video-language learners. arXiv,
2022. 2, 13

[60] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma,
Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompt-
ing elicits reasoning in large language models. In NeurIPS,
2022. 2, 4, 6, 12, 13
[61] Keyu Wen, Jin Xia, Yuanyuan Huang, Linyang Li, Jiayan Xu,
and Jie Shao. Cookie: Contrastive cross-modal knowledge
sharing pre-training for vision-language representation. In
ICCV, 2021. 2
[62] Jialin Wu, Jiasen Lu, Ashish Sabharwal, and Roozbeh Mot-
taghi. Multi-modal answer validation for knowledge-based
vqa. In AAAI, 2022. 6
[63] Chunsheng Yang, Feng-Kuang Chiang, Qiangqiang Cheng,
and Jun Ji. Machine learning-based student modeling method-
ology for intelligent tutoring systems. Journal of Educational
Computing Research, 2021. 1
[64] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu,
Yumao Lu, Zicheng Liu, and Lijuan Wang. An empirical
study of gpt-3 for few-shot knowledge-based vqa. In AAAI,
2022. 2, 3, 4, 6, 12, 13
[65] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun
Wu, Antonio Torralba, and Joshua B Tenenbaum. Clevrer:
Collision events for video representation and reasoning. In
ICLR, 2020. 2, 3
[66] Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Push-
meet Kohli, and Joshua B Tenenbaum. Neural-Symbolic
VQA: Disentangling Reasoning from Vision and Language
Understanding. In NIPS, 2018. 2, 3
[67] Yu Jiang*, Vivek Natarajan*, Xinlei Chen*, Marcus
Rohrbach, Dhruv Batra, and Devi Parikh. Pythia v0.1: the
winning entry to the vqa challenge 2018. arXiv, 2018. 6, 12
[68] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng
Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack Hessel,
Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script
knowledge through vision and language and sound. In CVPR,
2022. 2
[69] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choro-
manski, Adrian Wong, Stefan Welker, Federico Tombari,
Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee,
Vincent Vanhoucke, and Pete Florence. Socratic models:
Composing zero-shot multimodal reasoning with language.
arXiv, 2022. 2
[70] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,
Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained trans-
former language models. arXiv, 2022. 1, 3, 5, 12
[71] Yifeng Zhang, Ming Jiang, and Qi Zhao. Query and attention
augmentation for knowledge-based explainable reasoning. In
CVPR, 2022. 3
[72] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li,
Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai,
Lu Yuan, Yin Li, and Jianfeng Gao. Regionclip: Region-based
language-image pretraining. In CVPR, 2022. 2
[73] Denny Zhou, Nathanael Sch¨arli, Le Hou, Jason Wei, Nathan
Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet,
Quoc Le, and Ed Chi. Least-to-most prompting enables com-
plex reasoning in large language models. arXiv, 2022. 2
[74] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei
Liu. Conditional prompt learning for vision-language models.
In CVPR, 2022. 2
[75] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei
Liu. Learning to prompt for vision-language models. IJCV,
2022. 2
[76] Zihao Zhu, Jing Yu, Yujing Wang, Yajing Sun, Yue Hu, and Qi
Wu. Mucko: multi-layer cross-modal knowledge reasoning
for fact-based visual question answering. In IJCAI, 2021. 3

A. Overview.
In this supplementary material, we back up our claims by
supplementing the main paper with more implementation
details (Section B), more quantitative performance analy-
sis (Section C.1), and more qualitative visualization (Sec-
tion C.2).
B. More Implementation Details.
Parameter Setting. As shown in Algorithm 1 of the main
paper, we stop iteratively attending to the new concept and
predicting a new answer when the current predicted answer
ai equals to the last predicted answer ai−1, or it reaches the
maximal iteration number mIter. In our implementation,
we simply set mIter to 5. We ﬁnd that our method often
converges to the same answer and jumps out of the loop after
the second or the third iteration. It takes 2.27 iterations on
average to get the consistent answer prediction. We add the
generated rationale back into the prompt context when the
cosine similarity between the generated rationale and the
input image is larger than a threshold thre. We estimate the
cross-modality similarity with CLIP [51]. We simply set the
thre to be 0 and found it performs well. Our code and data
will be released publicly upon acceptance.
Prompting templates We also provide prompting template
examples of baseline methods PICa [64] and CoT [60] in
Fig. 7 and Fig. 8, respectively. Comparing the baselines’
examples in Fig. 7-8 with our method’s example in Fig. 3 of
the main paper, we can ﬁnd that our method’s prompt could
provide additional visual context and rationales predicted in
previous iterations to the large language model for a better
and more consistent prediction.
Regional Captions. Our model requires a regional caption-
ing model to extract captions for each visual region (concept)
selected by the attend stage of think module. We use the
pre-trained model released by Li et al. [39] to extract re-
gional captions. To provide additional visual context for
captioning, we expand the width and length of the candidate
object bounding box by 1.5 times. To make the generated
caption focus more on the query question and the provided
concept, we use the guided decoding strategy introduced
in [45] for guided captioning decoding. Speciﬁcally, we
deﬁne the lookahead heuristics as the cosine similarity be-
tween the generated captions and the question estimated by
a RoBERTa model [15,17] 2. We provide an ablation study
between the performance of such guided captions and the
“captions” generated by naive scene graph transformation
(e.g. “a silver car” for “car” with the “silver” attribute) in
table 4. As could be seen in table 4, such guided caption
decoder (Ours-Guided) provides captions of higher quality
for knowledge-based visual question answering.
2https://huggingface.co/clips/mfaq
Methods
A-OKVQA
Ours-SG
44.7
Ours-Guided
46.4
Table 4. Regional Caption generation comparison on A-OKVQA
validation set with the direct-answer setting. Our method achieves
consistent improvements compared with baselines.
Methods
Multiple-Choice
val
test
Pythia [67]
49.0
40.1
ViLBERT [43]
49.1
41.5
LXMERT [55]
51.4
41.6
KRISP [48]
51.9
42.2
GPV-2 [35]
60.3
53.7
CoT [60]
48.1
45.6
Pica [64]
46.1
44.2
Ours (OPT-66B)
48.6
47.6
Ours (GPT-3) w/o ensemble
58.7
57.5
Table 5. Performance comparison of our model and other baselines
on the A-OKVQA dataset with the multiple-choice setting. Our
method with GPT-3 (175B) as the language model achieves better
performance than previous fully-supervised methods like GPV-
2 [35].
C. More Analysis of the Proposed Model.
C.1. More Quantitative Analysis.
In this section, we provide more experimental analysis
of our method and baselines. Besides the direct answer that
generates free-form answer predictions for questions, AOK-
VQA dataset also provides a simpliﬁed and less challenging
setting, where the model only needs to select an option can-
didate to output the answer prediction. We also compare our
method with baselines in this setting in table 5. We add the
option candidates into the context of the prompt to inform
the LLM what to select. Based on the experimental result, we
have the following observation, our method still outperforms
the baselines in this simpliﬁed setting, showing the effec-
tiveness of our method. We also ﬁnd that GPV-2 performs
better than our method with the OPT-66B LLM [70], which
is inconsistent with our observation in table 1 of the main
paper on the more challenging direct answer situations.
We analyze the failure cases of our method and base-
lines. We found that the released OPT-66B model has weak
capabilities for selecting a correct answer from the candi-
dates. To further prove our investigation, we replace the
OPT-66B LLM of our model with the current state-of-the-
art LLM engine (GPT-3 175B). We found that our model
has an accuracy of 57.5 on the test split of A-OKVQA,

achieving better performance than previous fully-supervised
methods like GPV-2 [35]. This shows the potential of our
model that performance could be further improved with a
stronger LLM like GPT-3 175B [59] or bloomz [50]. Due to
GPT-3 175B’s expensive API cost and limitation on query
frequency, we could not conduct extensive experiments with
multi-query ensemble [64]. We expect better performance
could be achieved with multi-query ensemble. We noticed
that fully-supervised methods like (GPV-2 and KRISP) have
a more signiﬁcant performance disparity between the vali-
dation and test splits than the learning-in-context methods,
which is consistent with what we have observed in the direct-
answer setting of the A-OKVQA dataset in the main paper.
KRISP and GPV-2 have an accuracy drop of 9.7 and 6.6,
respectively, while our model only drops 1.2. Since the vali-
dation set has frequently been evaluated in fully-supervised
methods in different epochs, it may suffer from overﬁtting,
while learning-in-context methods do not have such overﬁt-
ting problems.
C.2. More Qualitative Analysis.
Qualitative Examples. We provide more examples of our
methods in Fig. 9. As observed in the ﬁgure, we can further
conﬁrm that our method is able to additionally attend to
the visual context that are semantically related to the task
(e.g. “pizza”) and “food” in the second row of Fig. 9. Our
method can also generate consistent rationales like (e.g. “The
restaurant is a pizza restaurant.”) for the answer prediction.
Moreover, our method can provide a modularized, step-by-
step investigation of the answer prediction, leading to better
model transparency and interpretability.
Failure Case Analysis. The step-by-step reasoning trace
of our model also provides intermediate results for us to
analyze how the proposed IPVR fails and suggests further
directions on how to improve the model’s performance. For
example, in the ﬁrst row of the Fig. 10, we found that our
IPVR fails because our OPT-66B LLM fails to connect the
“colorful balloons” and the “lgbtq” social movement. It shows
that a better LLM with broader open-world knowledge and
stronger reasoning ability could further our method achieves
better performance. The failure case in the second row of
Fig. 10 shows that our model fails when the vision models
fail to provide essential visual context (i.e. the shape of
the donut). This indicates that our model could be further
improved with models that have stronger perception abilities.
Context: A fully cooked pizza sitting on a tray with a spatula digging.
Question: What is another tool used to cut this type of food?
Answer: The answer is knife.
Context: Sandwich in paper on counter with man in background.
Question: Where is this meal being eaten?
Answer: The answer is restaurant.
Context: A couple of men preparing food inside of a kitchen. apron, 
board, bottle, bowl, cap, chair, chef, chef, food, hair, picture, pizza.
Question: What type of restaurant is this?
Answer: The answer is fastfood. 
…
Figure 7. Exemplar prompting templates of the PICa [64] base-
line. Outputs in the in-context examples and test examples are
marked with blue and green colors. Object tags are marked with
bronze colors.
Context: A fully cooked pizza sitting on a tray with a spatula digging.
Question: What is another tool used to cut this type of food?
Answer: The answer is knife. A pizza cutter cuts pizza.
Context: Sandwich in paper on counter with man in background.
Question: Where is this meal being eaten?
Answer: The answer is restaurant. The meal is at a restaurant.
Context: A couple of men preparing food inside of a kitchen. apron, 
board, bottle, bowl, cap, chair, chef, chef, food, hair, picture, pizza.
Question: What type of restaurant is this?
Answer: The answer is fast food. The restaurant is a fast food
restaurant.
…
Figure 8. Exemplar prompting templates of the CoT [60] baseline.
Outputs in the in-context examples and test examples are marked
with blue and green colors. The “thoughts” (rationales) of the
training example is marked with red colors. Object tags are marked
with bronze colors.

Input
Global Caption
Attend
Explain
Prediction
What is located 
on the shelves?
Caption: A vase 
with flowers 
and a bust on a 
table.
Book: Several stacks of books 
sit on a table.
Book: The red book sitting on a 
shelf.
The books are on 
the shelves.
books
There are flowers 
on the self.
Ours
CoT
PICa
books, books, books, 
books, books, books, 
books, books, books, 
books
Image
Question
GT
flowers
flowers
All tags: binder, block, flower, 
head, lid, man, nose, shelf, 
wall, vase, table, book, book…
books
What type 
of restaurant 
is this?
Caption: A 
couple of men 
preparing food 
inside of a 
kitchen.
Pizza: Someone making a pizza 
with cheese, bacon, and 
cheese.
Food: Someone holding some 
food on a plate.
The restaurant is a 
pizza restaurant.
pizza
The restaurant is a 
fast food restaurant
Ours
CoT
PICa
modern restraunt, pizza 
parlor, pizzeria, pizza joint, 
pizza place, pizzeria, italian, 
pizza, pizza, breakfast
Image
Question
GT
Fast food
All tags: apron, board, bottle, 
bowl, cap, chair, chef, chef, 
food, hair, picture, pizza…
What type of 
boat is on the 
beach?
Caption: A 
seagull flying 
over a beach 
with people in 
the 
background.
Boat: A blue and white small 
boat on sand and water.
Water: Someone flying in the 
air above the water.
The boat is a 
sailboat and is on 
the beach.
sailboat
The boat is on the 
beach and the people 
are on the beach
Ours
CoT
PICa
sailboat, sail boat, rowboat, 
sailboat, sail boat, sail boat, 
sailboat, sail, rowboat, 
sailboat
Image
Question
GT
boat
fishing
All tags: ball, beach, bird, child, 
boat, cloud, foot, person, 
shore, sky, wave, water, sky…
pizza
sailboat
Fast food
Figure 9. More Qualitative examples of the proposed IPVR and baselines. Besides better answer accuracy, our method also enjoys better
transparency by providing a step-by-step reasoning trace with related visual concepts, regional descriptions, and a supporting rationale. →
denotes our reasoning ﬂow.

This social 
demographic or 
social movement 
is represented by 
this gathering?
Input
Caption & Object
Caption: A group of 
people with a 
umbrella on a street.
Object: balloon, 
bandana, crowd, 
boy, building, man, 
person, window, 
woman, shirt, pole, 
hook, sign, string…
Crowd: Students walking 
down a crowded street 
holding colorful balloons.
Attend
Man: Someone holding 
onto an upside-down 
umbrella and cell phones.
Explain & Predict
The people are 
protesting.
protest
lgbtq, pride, 
parade, pride, 
lgbtq, lgbt, 
lgbtq, pride, 
lgbtq, lgbt
GT
Person: Someone on the 
phone in the middle of a 
crowd.
What shape is the 
donut in?
Caption: A person 
holding a glass of 
beer and a 
chocolate dessert.
Object: arm, dessert,  
chocolate, finger, 
hand, food, frosting, 
toy, tag, thumb, 
person, glass…
Chocolate: Someone 
holding some kind of 
chocolate treat.
Finger: Someone has a red 
apple in their hand.
The donut is 
round and has a 
hole in the 
middle.
round
robot, person, 
person, 
gingerbread 
man, 
gingerbread 
man, person, 
man, 
rectangle, lost 
arm, person
GT
Frosting: Someone holds up 
a piece of food with white 
frosting.
The donut is 
round.
What is the 
favorite color of 
the person who 
lives here?
Caption: A bed and 
a table in a room.
Object: bed, table, 
bedroom, blanket, 
door, fence, table, 
window, wall, stone, 
cord, chair, carpet, 
building, star, top, 
house, room…
Bed: There's a bedroom 
with a bed and a table.
Wall: The room that looks 
like someone's bedroom.
The person 
who lives here 
has a blue 
blanket and 
pillow.
blue
purple, 
purple, 
purple, brown, 
purple, 
purple, brown, 
purple, 
purple, 
purple
GT
Figure 10. Typical Failure cases of the proposed method and baselines. our model provides intermediate results for failure cases. In the ﬁrst
row of the Figure, we show an example that the OPT-66B LLM in our think module module fails to connect the “colorful balloons” and the
“lgbtq” social movement. In the second row of Figure, we show that our model fails when the vision models fail to capture essential visual
context (i.e. the shape of the donut).

