Combining Fast and Slow Thinking
for
Human-like and Efﬁcient Navigation in Constrained Environments
Marianna B. Ganapini1 , Murray Campbell2 , Francesco Fabiano3 , Lior Horesh2 , Jon
Lenchner2 , Andrea Loreggia4 , Nicholas Mattei5 , Taher Rahgooy6 , Francesca Rossi2 , Biplav
Srivastava7 , Brent Venable8
1Union College
2IBM Research
3University of Udine
4University of Brescia
5Tulane University
6 University of West Florida
7Univ. of South Carolina
8Institute for Human and Machine Cognition
bergamam@union.edu, {mcam,lhoresh,lenchner,Francesca.Rossi2}@us.ibm.com
francesco.fabiano@uniud.it, andrea.loreggia@gmail.com, nsmattei@tulane.edu,
trahgooy@students.uwf.edu, biplav.s@sc.edu, bvenable@ihmc.org
Abstract
Current AI systems lack several important hu-
man capabilities, such as adaptability, generaliz-
ability, self-control, consistency, common sense,
and causal reasoning. We believe that existing cog-
nitive theories of human decision making, such
as the thinking fast and slow theory, can provide
insights on how to advance AI systems towards
some of these capabilities. In this paper, we propose
a general architecture that is based on fast/slow
solvers and a metacognitive component. We then
present experimental results on the behavior of an
instance of this architecture, for AI systems that
make decisions about navigating in a constrained
environment. We show how combining the fast
and slow decision modalities allows the system
to evolve over time and gradually pass from slow
to fast thinking with enough experience, and that
this greatly helps in decision quality, resource con-
sumption, and efﬁciency.
1
Introduction
AI systems have seen great advancement in recent years, on
many applications that pervade our everyday life. However,
we are still mostly seeing instances of narrow AI that are typi-
cally focused on a very limited set of competencies and goals,
e.g., image interpretation, natural language processing, clas-
siﬁcation, prediction, and many others. Moreover, while these
successes can be accredited to improved algorithms and tech-
niques, they are also tightly linked to the availability of huge
datasets and computational power [Marcus, 2020]. State-of-
the-art AI still lacks many capabilities that would naturally be
included in a notion of (human) intelligence, such as general-
izability, adaptability, robustness, explainability, causal anal-
ysis, abstraction, common sense reasoning, ethical reasoning
[Rossi and Mattei, 2019], as well as a complex and seamless
integration of learning and reasoning supported by both im-
plicit and explicit knowledge [Littman and et al., 2021].
We believe that a better study of the mechanisms that al-
low humans to have these capabilities can help [Booch et
al., 2021]. We focus especially on D. Kahneman’s theory
of thinking fast and slow [Kahneman, 2011], and we pro-
pose a multi-agent AI architecture (called SOFAI, for SlOw
and Fast AI) where incoming problems are solved by either
System 1 (or “fast”) agents (also called “solvers”), that re-
act by exploiting only past experience, or by System 2 (or
“slow”) agents, that are deliberately activated when there
is the need to reason and search for optimal solutions be-
yond what is expected from the System 1 agents. Given
the need to choose between these two kinds of solvers, a
meta-cognitive agent is employed, performing introspection
and arbitration roles, and assessing the need to employ Sys-
tem 2 solvers by considering resource constraints, abilities
of the solvers, past experience, and expected reward for a
correct solution of the given problem [Shenhav et al., 2013;
Thompson et al., 2011]. Different approaches to the design
of AI systems inspired by the dual-system theory have also
been published recently [Bengio, 2017; Goel et al., 2017;
Chen et al., 2019; Anthony et al., 2017; Mittal et al., 2017;
Noothigattu and et al., 2019; Gulati et al., 2020], showing that
this theory inspires many AI researchers.
In this paper we describe the SOFAI architecture, char-
acterizing the System 1 and System 2 solvers and the role
arXiv:2201.07050v2  [cs.AI]  12 Feb 2022

of the meta-cognitive agent, and provide motivations for the
adopted design choices. We then focus on a speciﬁc instance
of the SOFAI architecture, that provides the multi-agent plat-
form for generating trajectories in a grid environment with
penalties over states, actions, and state features. In this in-
stance, decisions are at the level of each move from one grid
cell to another. We show that the combination of fast and
slow decision modalities allows the system to create trajec-
tories that are similar to human-like ones, compared to using
only one of the modalities. Human-likeness is here exempli-
ﬁed by the trajectories built by a Multi-alternative Decision
Field Theory model (MDFT) [Roe et al., 2001], that has been
shown to mimick the way humans decide among several al-
ternatives. In our case, the possible moves in a grid state,
take into account non-rational behaviors related to alterna-
tives’ similarity. Moreover, the SOFAI trajectories are shown
to generate a better reward and to require a shorter decision
time overall. We also illustrate the evolution of the behav-
ior of the SOFAI system over time, showing that, just like in
humans, initially the system mostly uses the System 2 deci-
sion modality, and then passes to using mostly System 1 when
enough experience over moves and trajectories is collected.
2
Background
We introduce the main ideas of the thinking fast and slow
theory. We also describe the main features of the Multi-
alternative Decision Field Theory (MDFT) [Roe et al., 2001],
that we will use in the experiments (Section 4 and 5) to gen-
erate human-like trajectories in the grid environment.
2.1
Thinking Fast and Slow in Humans
According to Kahneman’s theory, described in his book
“Thinking, Fast and Slow” [Kahneman, 2011], human’s de-
cisions are supported and guided by the cooperation of two
kinds of capabilities, that for the sake of simplicity are called
systems: System 1 (“thinking fast”) provides tools for intu-
itive, imprecise, fast, and often unconscious decisions, while
System 2 (“thinking slow”) handles more complex situations
where logical and rational thinking is needed to reach a com-
plex decision.
System 1 is guided mainly by intuition rather than deliber-
ation. It gives fast answers to simple questions. Such answers
are sometimes wrong, mainly because of unconscious bias or
because they rely on heuristics or other short cuts [Gigerenzer
and Brighton, 2009], and usually do not provide explanations.
However, System 1 is able to build models of the world that,
although inaccurate and imprecise, can ﬁll knowledge gaps
through causal inference, allowing us to respond reasonably
well to the many stimuli of our everyday life.
When the problem is too complex for System 1, System
2 kicks in and solves it with access to additional computa-
tional resources, full attention, and sophisticated logical rea-
soning. A typical example of a problem handled by System 2
is solving a complex arithmetic calculation, or a multi-criteria
optimization problem. To do this, humans need to be able to
recognize that a problem goes beyond a threshold of cogni-
tive ease and therefore see the need to activate a more global
and accurate reasoning machinery [Kahneman, 2011]. Hence,
introspection and meta-cognition is essential in this process.
When a problem is new and difﬁcult to solve, it is han-
dled by System 2 [Kim et al., 2019]. However, certain prob-
lems, over time as more experience is acquired, pass on to
System 1. The procedures System 2 adopts to ﬁnd solutions
to such problems become part of the experience that System
1 can later use with little effort. Thus, over time, some prob-
lems, initially solvable only by resorting to System 2 reason-
ing tools, can become manageable by System 1. A typical
example is reading text in our own native language. However,
this does not happen with all tasks. An example of a problem
that never passes to System 1 is ﬁnding the correct solution
to complex arithmetic questions.
2.2
Multi-Alternative Decision Field Theory
Multi-alternative Decision Field Theory (MDFT) [Roe et al.,
2001] models human preferential choice as an iterative cumu-
lative process. In MDFT, an agent is confronted with multiple
options and equipped with an initial personal evaluation for
them along different criteria, called attributes. For example, a
student who needs to choose a main course among those of-
fered by the cafeteria will have in mind an initial evaluation
of the options in terms of how tasty and healthy they look.
More formally, MDFT comprises of:
Personal Evaluation: Given set of options O
=
{o1, . . . , ok} and set of attributes A = {A1, . . . , AJ}, the
subjective value of option oi on attribute Aj is denoted by
mij and stored in matrix M. In our example, let us assume
that the cafeteria options are Salad (S), Burrito (B) and Veg-
etable pasta (V). Matrix M, containing the student’s prefer-
ences, could be deﬁned as shown in Figure 1 (left), where
rows correspond to the options (S, B, V ) and the columns to
the attributes Taste and Health.
Figure 1: Evaluation (M), Contrast (C), and Feedback (S) matrix.
Attention Weights: Attention weights are used to express
the attention allocated to each attribute at a particular time
t during the deliberation. We denote them by vector W(t)
where Wj(t) represents the attention to attribute j at time t.
We adopt the common simplifying assumption that, at each
point in time, the decision maker attends to only one attribute
[Roe et al., 2001]. Thus, Wj(t) ∈{0, 1} and P
j Wj(t) = 1,
∀t, j. In our example, we have two attributes, so at any point
in time t we will have W(t) = [1, 0], or W(t) = [0, 1], repre-
senting that the student is attending to, respectively, Taste or
Health. The attention weights change across time according
to a stationary stochastic process with probability distribution
w, where wj is the probability of attending to attribute Aj. In
our example, deﬁning w1 = 0.55 and w2 = 0.45 would mean
that at each point in time, the student will be attending Taste
with probability 0.55 and Health with probability 0.45.
Contrast Matrix: Contrast matrix C is used to compute
the advantage (or disadvantage) of an option with respect to
the other options. In the MDFT literature [Busemeyer and
Townsend, 1993; Roe et al., 2001; Hotaling et al., 2010], C is
deﬁned by contrasting the initial evaluation of one alternative
2

against the average of the evaluations of the others, as shown
for the case with three options in Figure 1 (center).
At any moment in time, each alternative in the choice set
is associated with a valence value. The valence for option oi
at time t, denoted vi(t), represents its momentary advantage
(or disadvantage) when compared with other options on some
attribute under consideration. The valence vector for k op-
tions o1, . . . , ok at time t, denoted by column vector V(t) =
[v1(t), . . . , vk(t)]T , is formed by V(t) = C × M × W(t). In
our example, the valence vector at any time point in which
W(t) = [1, 0], is V(t) = [1 −7/2, 5 −3/2, 2 −6/2]T .
Preferences for each option are accumulated across the it-
erations of the deliberation process until a decision is made.
This is done by using Feedback Matrix S, which deﬁnes
how the accumulated preferences affect the preferences com-
puted at the next iteration. This interaction depends on how
similar the options are in terms of their initial evaluation ex-
pressed in M. Intuitively, the new preference of an option is
affected positively and strongly by the preference it had ac-
cumulated so far, while it is inhibited by the preference of
similar options. This lateral inhibition decreases as the dis-
similarity between options increases. Figure 1 (right) shows
S for our example [Hotaling et al., 2010].
At any moment in time, the preference of each alternative
is calculated by P(t+1) = S×P(t)+V(t+1) where S×P(t)
is the contribution of the past preferences and V(t + 1) is the
valence computed at that iteration. Starting with P(0) = 0,
preferences are then accumulated for either a ﬁxed number of
iterations (and the option with the highest preference is se-
lected) or until the preference of an option reaches a given
threshold. In the ﬁrst case, MDFT models decision making
with a speciﬁed deliberation time, while, in the latter, it mod-
els cases where deliberation time is unspeciﬁed and choice
is dictated by the accumulated preference magnitude. In gen-
eral, different runs of the same MDFT model may return dif-
ferent choices due to the attention weights’ distribution. In
this way, MDFT induces choice distributions over set of op-
tions and is capable of capturing well know behavioral ef-
fects such as the compromise, similarity, and attraction effects
that have been observed in humans and that violate rationality
principles [Busemeyer and Townsend, 1993].
3
Thinking Fast and Slow in AI
SOFAI is a multi-agent architecture (see Figure 2) where in-
coming problems are initially handled by those System 1 (S1)
solvers that possess the required skills to tackle them, analo-
gous to what is done by humans who ﬁrst react to an external
stimulus via their System 1.
3.1
Fast and Slow Solvers
As mentioned, incoming problems trigger System 1 (S1)
solvers. We assume such solvers act in constant time, i.e.,
their running time is not a function of the size of the input
problem instance, by relying on the past experience of the
system, which is maintained in the model of self. The model
of the world contains the knowledge accumulated by the sys-
tem over the external environment and the expected tasks,
while the model of others contains the knowledge and beliefs
Meta-cognition 
Module
•
Chooses between S1 solution and 
S2 activation
•
Assesses value of success, 
resources, trustworthiness of 
solvers
•
Adopts a two-phase assessment
Task/problem
System 1 
Solver
•
Based on past 
experiences
•
Acts in O(1) 
•
Activates 
autonomously
System 2 Solver
•
Employs reasoning 
•
Consumes more resources
•
Activated by meta-
cognitive module
Proposed
solution 
and confidence
Activation
Model of 
World
Knowledge about 
environment 
impacted by agent’s 
decisions
Model of 
Self
Past decisions and 
their reward
Model of 
Others
Knowledge and beliefs 
about other agents 
impacting the same 
environment
Adoption of 
system 1 solution
Solution/decision
OR
Model 
/Solver 
Updater
•
Updates the 
models
•
(Re)trains the 
S1 solvers
•
Autonomous 
activation
Figure 2: The SOFAI architecture.
about other agents who may act in the same environment. The
model updater agent acts in the background to keep all mod-
els updated as new knowledge of the world, of other agents,
or new decisions are generated and evaluated.
Once an S1 solver has solved the problem (for the sake of
simplicity, assume a single S1 solver), the proposed solution
and the associated conﬁdence level are available to the meta-
cognitive (MC) module. At this point the MC agent starts its
operations, with the task of choosing between adopting the
S1 solver’s solution or activating a System 2 (S2) solver. S2
agents use some form of reasoning over the current problem
and usually consume more resources (especially time) than
S1 agents. Also, they never work on a problem unless they
are explicitly invoked by the MC module.
To make its decision, the MC agent assesses the current
resource availability, the expected resource consumption of
the S2 solver, the expected reward for a correct solution for
each of the available solvers, as well as the solution and
conﬁdence evaluations coming from the S1 solver. In order
to not waste resources at the meta-cognitive level, the MC
agent includes two successive assessment phases, the ﬁrst one
faster and more approximate, related to rapid unconscious
assessment in humans [Ackerman and Thompson, 2017;
Proust, 2013], and the second one (to be used only if needed)
more careful and resource-costly, analogous to the conscious
introspective process in humans [Carruthers, 2021]. The next
section will provide more details about the internal steps of
the MC agent.
This architecture and ﬂow of tasks allows for minimizing
time to action when there is no need for S2 processing since
S1 solvers act in constant time. It also allows the MC agent
to exploit the proposed action and conﬁdence of S1 when de-
ciding whether to activate S2, which leads to more informed
and hopefully better decisions by the MC.
Notice that we do not assume that S2 solvers are always
better than S1 solvers, analogously to what happens in hu-
man reasoning [Gigerenzer and Brighton, 2009]. Take for ex-
ample complex arithmetic, which usually requires humans to
employ System 2, vs perception tasks, which are typically
handled by our System 1. Similarly, in the SOFAI architec-
ture we allow for tasks that might be better handled by S1
solvers, especially once the system has acquired enough ex-
perience on those tasks.
3

3.2
The Role of Meta-cognition
We focus on the concept of meta-cognition as initially de-
ﬁned by Flavell; Nelson [1979; 1990], that is, the set of pro-
cesses and mechanisms that could allow a computational sys-
tem to both monitor and control its own cognitive activi-
ties, processes, and structures. The goal of this form of con-
trol is to improve the quality of the system’s decisions [Cox
and Raja, 2011]. Among the existing computational mod-
els of meta-cognition [Cox, 2005; Kralik and et al., 2018;
Posner, 2020], we propose a centralized meta-cognitive mod-
ule that exploits both internal and external data, and arbitrates
between S1 and S2 solvers in the process of solving a single
task. Notice however that this arbitration is different from an
algorithm portfolio selection, which is already successfully
used to tackle many problems [Kerschke et al., 2019], be-
cause of the characterization of S1 and S2 solvers and the
way the MC agent controls them.
The MC module exploits information coming from two
main sources: 1) the system’s internal models of self, world,
and others; 2) the S1 solver(s), providing a proposed decision
for a task, and their conﬁdence in the proposed decision.
The ﬁrst meta-cognitive phase (MC1) activates automat-
ically as a new task arrives and a solution for the problem
is provided by an S1 solver. MC1 decides between accept-
ing the solution proposed by the S1 solver or activating the
second meta-cognitive phase (MC2). MC2 then makes sure
that there are enough resources for running S2. If not, MC2
adopts the S1 solver’s proposed solution. MC1 also compares
the conﬁdence provided by the S1 solver with the risk attitude
of the system: if the conﬁdence is high enough, MC1 adopts
the S1 solver’s solution. Otherwise, it activates the next as-
sessment phase (MC2) to make a more careful decision. The
rationale for this phase of the decision process is that we envi-
sion that often the system will adopt the solution proposed by
the S1 solver, because it is good enough given the expected
reward for solving the task, or because there are not enough
resources to invoke more complex reasoning.
Contrarily to MC1, MC2 decides between accepting the
solution proposed by the S1 solver or activating an S2 solver
for the task. To do this, MC2 evaluates the expected reward
of using the S2 solver in the current state to solve the given
task, using information contained in the model of self about
past actions taken by this or other solvers to solve the same
task, and the expected cost of running this solver. MC2 then
compares the expected reward for the S2 solver with the ex-
pected reward of the action proposed by the S1 solver: if the
expected additional reward of running the S2 solver, as com-
pared to using the S1 solution, is large enough, then MC2
activates the S2 solver. Otherwise, it adopts the S1 solution.
To evaluate the expected reward of the action proposed by
S1, MC2 retrieves from the model of self the expected imme-
diate and future reward for the action in the current state (ap-
proximating the forward analysis to avoid a too costly com-
putation), and combines this information with the conﬁdence
the S1 solver has in the action. The rationale for the behavior
of MC2 is based on the design decision to avoid costly reason-
ing processes unless the additional cost is compensated by an
even greater additional expected reward for the solution that
the S2 solver will identify for this task. This is analogous to
Figure 3: Example of the constrained grid decision scenario. Black
squares represents states with penalties. Penalties are generated also
when the agent moves left or bottom-right, or when it moves to a
blue or a green state. The red lines describe a set of trajectories
generated by the agent (all with the same start and end point). The
strength of the red color for each move corresponds to the amount
of trajectories employing such move.
what happens in humans [Shenhav et al., 2013].
4
Instantiating SOFAI on Grid Navigation
In the SOFAI instance that we consider and evaluate in this
paper, the decision environment is a 9 × 9 grid and the task is
to generate a trajectory from an initial state S0 to a goal state
SG, by making moves from one state to an adjacent one in a
sequence, while minimizing the penalties incurred.
Such penalties are generated by constraints over moves
(there are 8 moves for each state), speciﬁc states (grid cells),
and state features (in our setting, these are colors associated to
states). For example, there could be a penalty for moving left,
for going to the cell (1,3), and for moving to a blue state. In
our speciﬁc experimental setting, any move brings a penalty
of −4, each constraint violation gives a penalty of −50, and
reaching the goal state gives a reward of 10.
This decision environment is non-deterministic: there is a
10% chance of failure, meaning that the decision of moving
to a certain adjacent state may result in a move to another
adjacent state chosen at random. Figure 3 shows an example
of our grid decision environment.
Given this decision environment, we instantiate the SOFAI
architecture as follows:
• one S1 solver, that uses information about the past tra-
jectories to decide the next move (see below for details);
• one S2 solver, that uses MDFT to make the decision
about the next move;
• MC agent: its behavior is described by Algorithm 1;
• model of the world: the grid environment;
• model of self: it includes past trajectories and their fea-
tures (moves, reward, length, time);
• no model of others.
In Algorithm 1:
• nTraj(sx, {S, ALL}) returns the number of times in
state sx an action computed by solver S (ALL means
any solver) has been adopted by the system; if they are
below t1 (a natural number), it means that we don’t have
enough experience yet.
4

Algorithm 1 The MC agent
Input (Action a, Conﬁdence c, State sx, Partial Trajectory T)
1: if nTraj(sx, ALL) ≤t1 or partReward(T )
avgReward(sx) ≤t2 or
c ≤t3 then
2:
if nTraj(sx, S2) ≤t6 then
3:
randomly adopt S1 decision or activate S2 solver
4:
else
5:
expCostS2 ←expT imeS2
remT ime
6:
if expCostS2 ≤1 and
(expRewardS2(sx)–expReward(sx,a))
expCostS2
> t4 then
7:
Set the attention weights in W
8:
Activate the S2 solver
9:
else
10:
Adopt S1 decision
11:
end if
12:
end if
13: else
14:
Adopt S1 decision
15: end if
• partReward(T) and avgReward(sx) are respectively
the partial reward of the trajectory T and the average
partial reward that the agent gets when it usually reaches
state sx: if we are below t2 (between 0 and 1), it means
that we are performing worse than past experience.
• c is the conﬁdence of the S1 solver: if it is below t3 (be-
tween 0 and 1) it means that our attitude to risk does not
tolerate the conﬁdence level.
If any of the tests in line 1 are passed (meaning, the condi-
tion is not satisﬁed), the MC system (MC1) adopts the S1
decision. Otherwise, it performs a more careful evaluation
(MC2):
• t6 checks that the S2 solver has enough experience. If
not, a random choice between S1 and S2 is made (line
3).
• Otherwise, it checks if it is convenient to activate S2
(line 6), comparing the expected gain in reward nor-
malize by its cost. t4 gives the tolerance for this gain.
If it is convenient, MC activates the S2 solver (line 8),
otherwise it adopts S1’s decision. In this evaluation,
expTimeS2 and remTime are respectively the aver-
age amount of time taken by S2 to compute an ac-
tion and the remaining time to complete the trajectory;
expRewardS2(sx) and expReward(sx, a) are the ex-
pected reward using S2 in state sx and the expected re-
ward of adopting action a (computed by S1) in state sx.
The expected reward for an action a in a state sx is:
E(R|sx, a) =
X
ri∈Rsx,a
P(ri|sx, a) ∗ri
where Rsx,a is the set of all the rewards in state sx tak-
ing the action a that are stored in the model of self;
P(ri|sx, a) is the probability of getting the reward ri in
state sx taking the action a. As the expected reward de-
pends on the past experience stored in the model of self,
it is possible to compute a conﬁdence as follows:
c(sx, a) = sigmoid(
(r −0.5)
(σ + 1e −10)))
where σ is the standard deviation of the rewards in sx
taking an action a, r is the probability of taking action a
in state sx.
MC1 and MC2 bear some resemblance to UCB and model-
based learning in RL [Sutton and Barto, 2018]. However, in
SOFAI we decompose some of these techniques to make de-
cisions in a more ﬁne grained manner.
The S1 agent, given a state sx, chooses the action that max-
imizes the expected reward based on the past experience. That
is: argmaxa(E(R|sx, a) ∗c(sx, a)).
The S2 agent, instead, employs the MDFT machinery (see
Section 2.2) to make a decision, where the M matrix has
two columns, containing the Q values of a nominal and con-
strained RL agents, and the attention weights W are set in
three possible ways: 1) attention to satisfying the constraints
only if we have already violated many of them (denoted by
01), 2) attention to reaching the goal state only if the current
partial trajectory is too long (denoted by 10), and 3) attention
to both goal and constraints (denoted by 02). We will call the
three resulting versions SOFAI 01, 10, and 02.
5
Experimental Results
We generated at random 10 grids, and for each grid we ran-
domly chose: the initial and ﬁnal states, 2 constrained actions,
6 constrained states, 12 constrained state features (6 green
and 6 blue). For each grid, we run the following agents:
• two reinforcement learning agents: one that tries to avoid
the constraint penalties to reach the goal (called RL Con-
strained), and the other that just tries to reach the goal
with no attention to the constraints (called RL Nominal).
These agents will provide the baselines;
• the S1 solver;
• the S2 solver (that is, MDFT): this agent will be both
a component of SOFAI and the provider of human-like
trajectories;
• SOFAI 01, SOFAI 10, and SOFAI 02.
Each agent generates 1000 trajectories. We experimented
with many combinations of values for the parameters. Here,
we report the results for the following conﬁguration: t1 =
200, t2 = 0.8, t3 = 0.4, t4 = 0, t6 = 1.
We ﬁrst checked which agent generates trajectories that are
more similar to the human ones (exempliﬁed by MDFT). Fig-
ure 4 reports the average JS-divergence between the set of
trajectories generated by MDFT and the other systems. It is
easy to see that SOFAI agents perform much better than S1,
especially in the 01 conﬁguration.
We then compared the three versions of SOFAI to S1 alone,
S2 alone, and the two RL agents, in terms of the length of the
generated paths, total reward, and time to generate the trajec-
tories, see Figure 5. It is easy to see that S1 performs very
badly on all three criteria, while the other systems are compa-
rable. Notice that RL Nominal represents a lower bound for
the length criteria and an upper bound for the reward, since it
5

Figure 4: Average JS divergence between the set of trajectories gen-
erated by MDFT and the other systems.
gets to the goal with no attention to satisfying the constraints.
For both reward and time, SOFAI (which combines S1 and
S2) performs better than using only S1 or only S2.
We then passed from the aggregate results over all 1000
trajectories to checking the behavior of SOFAI and the other
agents over time, from trajectory 1 to 1000. The goal is to
see how SOFAI methods evolve in their behavior and their
decisions on how to combine its S1 and S2 agents. Given that
SOFAI 01 performs comparably or better than the other two
versions, in the following experimental results we will only
show the behavior of this version and will denote it simply as
SOFAI.
Figure 6 shows the length, reward, and time for each of
the 1000 trajectories, comparing SOFAI to S1 and to S2. In
terms of length and reward, S1 does not perform well at all,
while SOFAI and S2 are comparable. However, the time chart
shows that SOFAI is much faster than S2 and over time it
also becomes faster than S1, even if it uses a combination
of S1 and S2. This is due to the fact that S1 alone cannot
exploit the experience gathered by S2 within SOFAI, so it
generates much worse and longer trajectories, which require
much more time. Perhaps the most interesting is Figure 7.
The left ﬁgure shows the average time spent by S1 and S2
within SOFAI in taking a single decision (thus a single move
in the trajectory): S2 always takes more time than S1, and this
is stable over time. The center ﬁgure shows the average re-
ward for a single move: S2 is rather stable in generating high
quality moves, while S1 at ﬁrst performs very badly (since
there is not enough experience yet) and later generates bet-
ter moves (but still worse than S2). The question is now: how
come S1 improves so much over time? The answer is given
by the right ﬁgure which shows the percentage of usage of
S1 and S2 in each trajectory. As we can see, at the beginning
SOFAI uses mostly S2, since the lack of experience makes S1
not trustable (that is, the MC algorithm does not lead to the
adoption of the S1 decision). After a while, with enough tra-
jectories built by (mostly) S2 and stored in the model of self,
SOFAI (more precisely, the MC agent) can trust S1 enough
to use it more often when deciding the next move, so much
that after about 450 trajectories S1 is used more often than
S2. This allows SOFAI to be faster while not degrading the
reward of the generated trajectories. This behavior is similar
to what happens in humans (as described in Section 2.1): we
ﬁrst tackle a non-familiar problem with our System 2, until
we have enough experience that it becomes familiar and we
pass to using System 1.
6
Future Work
We presented SOFAI, a conceptual architecture inspired by
the thinking fast and slow theory of human decision making,
and we described its behavior over a grid environment, show-
ing that it is able to combine S1 and S2 decision modalities
to generate high quality decisions faster than using just S1 or
S2. We plan to generalize our work to allow for several S1
and/or S2 solvers and several problems for the same architec-
ture, thus tackling issues of ontology and similarity.
7
Acknowledgements
We would like to thank Daniel Kahneman for his continuous
support for our work, and many enlightening discussions. We
also would like to thank Aanya Khandelwal (Georgia Tech)
for her contributions to the design of the metacognition mod-
ule for the grid environment during her 2021 internship at
IBM, as well as all the other project team members (Grady
Booch, Kiran Kate, Nick Linck, Keerthiram Murugesan, Mat-
tia Rigotti, at IBM) for extensive discussions on both the the-
oretical and the experimental part of this work.
6

Figure 5: Average length (left), reward (center), and time (right) for each trajectory, aggregated over 10 grids and 1000 trajectories.
Figure 6: Average length (left), reward (center), and time to compute each trajectory (right), aggregated over 10 grids.
Figure 7: Time to compute a move (left), average reward for a move (center), and average fraction of times each sub-system is used (right),
over 10 grids.
References
[Ackerman and Thompson, 2017] Rakefet
Ackerman
and
Valerie A Thompson.
Meta-reasoning: Monitoring and
control of thinking and reasoning. Trends in Cognitive Sci-
ences, 21(8):607–617, 2017.
[Anthony et al., 2017] Thomas Anthony, Zheng Tian, and
David Barber. Thinking fast and slow with deep learn-
ing and tree search. In Advances in Neural Information
Processing Systems, pages 5360–5370, 2017.
[Bengio, 2017] Yoshua Bengio.
The consciousness prior.
arXiv preprint arXiv:1709.08568, 2017.
[Booch et al., 2021] Grady Booch, Francesco Fabiano, Lior
Horesh, Kiran Kate, Jonathan Lenchner, Nick Linck, An-
dreas Loreggia, Keerthiram Murgesan, Nicholas Mattei,
Francesca Rossi, and Biplav Srivastava. Thinking fast and
slow in AI. In Proceedings of the AAAI Conference on Ar-
tiﬁcial Intelligence, volume 35, pages 15042–15046, 2021.
[Busemeyer and Townsend, 1993] Jerome R Busemeyer and
James T Townsend.
Decision ﬁeld theory: a dynamic-
cognitive approach to decision making in an uncertain en-
vironment. Psychological review, 100(3):432, 1993.
[Carruthers, 2021] Peter Carruthers. Explicit nonconceptual
metacognition. Philosophical Studies, 178(7):2337–2356,
2021.
[Chen et al., 2019] Di Chen, Yiwei Bai, Wenting Zhao, Se-
bastian Ament, John M Gregoire, and Carla P Gomes.
Deep reasoning networks: Thinking fast and slow. arXiv
preprint arXiv:1906.00855, 2019.
[Cox and Raja, 2011] Michael T Cox and Anita Raja. Metar-
easoning: Thinking about thinking. MIT Press, 2011.
[Cox, 2005] Michael T Cox.
Metacognition in computa-
tion: A selected research review.
Artiﬁcial intelligence,
169(2):104–141, 2005.
[Flavell, 1979] John H Flavell.
Metacognition and cogni-
tive monitoring: A new area of cognitive–developmental
inquiry. American psychologist, 34(10):906, 1979.
[Gigerenzer and Brighton, 2009] Gerd
Gigerenzer
and
Henry Brighton.
Homo heuristicus: Why biased minds
7

make better inferences.
Topics in Cognitive Science,
1(1):107–143, 2009.
[Goel et al., 2017] Gautam Goel, Niangjun Chen, and Adam
Wierman. Thinking fast and slow: Optimization decompo-
sition across timescales. In IEEE 56th Conference on De-
cision and Control (CDC), pages 1291–1298. IEEE, 2017.
[Gulati et al., 2020] Aditya
Gulati,
Sarthak
Soni,
and
Shrisha Rao. Interleaving fast and slow decision making.
arXiv preprint arXiv:2010.16244, 2020.
[Hotaling et al., 2010] Jared M Hotaling, Jerome R Buse-
meyer, and Jiyun Li. Theoretical developments in decision
ﬁeld theory: Comment on tsetsos, usher, and chater (2010).
Psychological Review, 2010.
[Kahneman, 2011] Daniel Kahneman.
Thinking, Fast and
Slow. Macmillan, 2011.
[Kerschke et al., 2019] Pascal Kerschke, Holger H Hoos,
Frank Neumann, and Heike Trautmann.
Automated al-
gorithm selection: Survey and perspectives. Evolutionary
computation, 27(1):3–45, 2019.
[Kim et al., 2019] Dongjae
Kim,
Geon
Yeong
Park,
PO John, Sang Wan Lee, et al.
Task complexity in-
teracts with state-space uncertainty in the arbitration
between model-based and model-free learning.
Nature
communications, 10(1):1–14, 2019.
[Kralik and et al., 2018] Jerald D Kralik and et al. Metacog-
nition for a common model of cognition. Procedia com-
puter science, 145:730–739, 2018.
[Littman and et al., 2021] Michael L. Littman and et al.
Gathering Strength, Gathering Storms: The One Hundred
Year Study on Artiﬁcial Intelligence (AI100) 2021 Study
Panel Report. Stanford University, 2021.
[Marcus, 2020] Gary Marcus. The next decade in AI: Four
steps towards robust artiﬁcial intelligence. arXiv preprint
arXiv:2002.06177, 2020.
[Mittal et al., 2017] Sudip Mittal, Anupam Joshi, and Tim
Finin. Thinking, fast and slow: Combining vector spaces
and knowledge graphs. arXiv preprint arXiv:1708.03310,
2017.
[Nelson, 1990] Thomas O Nelson. Metamemory: A theoret-
ical framework and new ﬁndings. In Psychology of learn-
ing and motivation, volume 26, pages 125–173. Elsevier,
1990.
[Noothigattu and et al., 2019] R.
Noothigattu
and
et
al.
Teaching AI agents ethical values using reinforcement
learning and policy orchestration.
IBM J. Res. Dev.,
63(4/5):2:1–2:9, 2019.
[Posner, 2020] Ingmar Posner.
Robots thinking fast and
slow: On dual process theory and metacognition in em-
bodied AI. 2020.
[Proust, 2013] Jo¨elle Proust. The philosophy of metacogni-
tion: Mental agency and self-awareness.
OUP Oxford,
2013.
[Roe et al., 2001] Robert M Roe, Jermone R Busemeyer, and
James T Townsend. Multialternative decision ﬁeld theory:
A dynamic connectionst model of decision making. Psy-
chological review, 108(2):370, 2001.
[Rossi and Mattei, 2019] F. Rossi and N. Mattei. Building
ethically bounded AI. In Proceedings of the 33rd AAAI
Conference on Artiﬁcial Intelligence (AAAI), 2019.
[Shenhav et al., 2013] Amitai
Shenhav,
Matthew
M
Botvinick, and Jonathan D Cohen.
The expected
value of control: an integrative theory of anterior cingulate
cortex function. Neuron, 79(2):217–240, 2013.
[Sutton and Barto, 2018] Richard S. Sutton and Andrew G.
Barto. Reinforcement Learning: An Introduction, 2nd Edi-
tion. A Bradford Book, Cambridge, MA, USA, 2018.
[Thompson et al., 2011] Valerie
A
Thompson,
Jamie
A Prowse Turner, and Gordon Pennycook.
Intuition,
reason, and metacognition.
Cognitive psychology,
63(3):107–140, 2011.
8

