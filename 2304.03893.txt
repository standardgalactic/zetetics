ChatGPT Empowered Long-Step Robot Control in Various
Environments: A Case Application
Naoki Wake1, Atsushi Kanehira1, Kazuhiro Sasabuchi1, Jun Takamatsu1, and Katsushi Ikeuchi1
Abstract
This paper aims to provide a speciﬁc example of how OpenAI’s ChatGPT can be used in a few-shot setting to convert natural
language instructions into an executable robot action sequence (Fig. 1). Generating programs for robots from natural language
instructions is an attractive goal, but the practical application using ChatGPT is still in its early stages, and there is no established
methodology yet. Here, we have designed easy-to-customize input prompts for ChatGPT that meet common requirements in many
practical applications, including: 1) easy integration with robot execution systems or visual recognition programs, 2) applicability
to various environments, and 3) the ability to provide long-step instructions while minimizing the impact of ChatGPT’s token
limit. Speciﬁcally, the prompts encourage ChatGPT to 1) output a sequence of predeﬁned robot actions with explanations in a
readable JSON format, 2) represent the operating environment in a formalized style, and 3) infer and output the updated state of
the operating environment as the result of each operation, which will be input with the next instruction to allow ChatGPT to work
based solely on the memory of the latest operations. Through experiments, we conﬁrmed that the proposed prompts allow ChatGPT
to act in accordance with the requirements in various environments. Additionally, we observed that ChatGPT’s conversational
ability allows users to adjust its output with natural language feedback, which is crucial for developing an application that is both
safe and robust while providing a user-friendly interface. Users can easily customize the prompts as templates. The contribution
of this paper is to provide and publish the prompts, which are generic enough to be easily modiﬁed to ﬁt the requirements of
each experimenter, thereby providing practical knowledge to the robotics research community. Our prompts and source code for
using them are open-source and publicly available at https://github.com/microsoft/ChatGPT-Robot-Manipulation-Prompts.
Fig. 1. This paper shows practical prompts for ChatGPT to generate for translating executable robot action sequences from multi-step human instructions in
various environments.
I. INTRODUCTION
Recent advances in natural language processing have enabled the development of large language models (LLMs) that can
understand and generate human-like language. As a result of learning vast amount of data, some LLMs can be ﬁne-tuned for
speciﬁc tasks in a few-shot manner through conversations. ChatGPT [1] is a prime example of such an LLM. One exciting
1Applied Robotics Research, Microsoft, Redmond, WA 98052, USA naoki.wake@microsoft.com
arXiv:2304.03893v2  [cs.RO]  11 Apr 2023

application of ChatGPT is in the ﬁeld of robotics, where it can be used to convert natural language instructions into executable
programs for controlling robots.
Generating programs for robots from natural language instructions is an attractive goal, and many existing works exist (for
example, [2]–[4]), some of which are built on top of LLMs (for example, [5]–[7]). However, most of them were developed
within a limited scope, hardware-dependent, or lack the functionality of human-in-the-loop. Additionally, most of these studies
rely on speciﬁc datasets, which necessitate data recollection and model retraining when transferring or extending them to other
robotic settings. From a practical application standpoint, an ideal robotic solution would be one that can be easily applied to
other applications or operational settings without requiring extensive data collection or model retraining.
The advantage of using ChatGPT for robotic applications is that we can utilize its language recognition and interaction
capabilities as an interface, and begin with a small amount of sample data to tune up the model for speciﬁc applications. While
the potential of ChatGPT for robotic application is attracting attention [8], the practical application is still in its early stages,
and there is no established methodology yet.
In this paper, we provide a speciﬁc example of how ChatGPT can be used in a few-shot setting to convert natural language
instructions into a sequence of actions that a robot can execute (Fig.1). In designing the prompts, we tried to ensure that they
meet the requirements common to many practical applications while also being structured in a way that they can be easily
customizable. The requirements we deﬁned for this paper are:
1) Easy integration with robot execution systems or visual recognition programs.
2) Applicability to various home environments.
3) The ability to provide an arbitrary number of natural language instructions while minimizing the impact of ChatGPT’s
token limit.
To meet these requirements, we designed input prompts to encourage ChatGPT to:
1) Output a sequence of predeﬁned robot actions with explanations in a readable JSON format.
2) Represent the operating environment in a formalized style.
3) Infer and output the updated state of the operating environment, which can be reused as the next input, allowing ChatGPT
to operate based solely on the memory of the latest operations.
We conducted experiments to test the effectiveness of our proposed prompts in inferring appropriate actions for multi-stage
language instructions in various environments. Additionally, we observed that ChatGPT’s conversational ability allows users
to adjust its output with natural language feedback, which is crucial for developing an application that is both safe and robust
while providing a user-friendly interface.
The proposed prompts can be used as templates and are easily customizable, including the set of robot operations, rep-
resentation of environments, and object names. The contribution of this paper is to provide and publish generic prompts
that can be easily modiﬁed to ﬁt the requirements of each experimenter, thereby providing practical knowledge to the
robotics research community. Our prompts and source code for using them are open-source and publicly available at https:
//github.com/microsoft/ChatGPT-Robot-Manipulation-Prompts.
II. CHATGPT PROMPTS
In this section, we introduce the actual prompts that we have designed. The prompts consist of 1) an explanation of the role
of ChatGPT, 2) a deﬁnition of roboti actions, 3) an explanation of how to represent the environment, 4) an explanation of how
to format the output, 5) examples of input and output, and 6) speciﬁc input from the user.
In the sixth prompt, the user inputs a pair of environment information and language instructions. Note that we considered
including these prompts as a single prompt, but it worked more robustly when we input them through a conversation consisting
of six turns (see Fig. 2 for how to make ChatGPT process several prompts before the actual operation begins). We describe
each prompt below. All the prompts and their output examples are available online, and anyone can try them out through
OpenAI’s API or a web browser: https://github.com/microsoft/ChatGPT-Robot-Manipulation-Prompts.
Please note that the speciﬁc prompts are designed under the assumption that the robot has at least one arm, sufﬁcient degrees
of freedom, and reachability to execute the desired task in the working environment. Additionally, we assume that a user’s
instructions are given at the granularity of grasp-manipulation-release, which involves handling a single object from grasping
to releasing. Challenges and discussions on extending our approach to more general-purpose robotic systems will be presented
in Section IV.
A. The role of ChatGPT
At the ﬁrst prompt, we provided ChatGPT with context for this task by explaining the role that ChatGPT should play. The
speciﬁc details are shown in Fig. 2. To accommodate multiple prompts, we included a sentence instructing ChatGPT to wait
for the next prompt before beginning the actual operation.

Fig. 2. Prompt for explaining the role of ChatGPT.
B. The deﬁnition of robot actions
In the next prompt, we deﬁne and describe the set of robot actions. The appropriate set of robot actions should depend
on the application and the implementation of robotic software, and thus should be customized by experimenters. In the case
of our in-house learning-from-observation application [9], [10], we deﬁne robot actions as functions that change the motion
constraints on the objects being manipulated, based on the Kuhn-Tucker theory [11]. This application enables us to prepare
a necessary and sufﬁcient set of robot actions for object manipulation in theory. Among the action set, we deﬁned the most
commonly appearing robot actions (e.g., move-hand, pick-up, or rotate) in the prompt as shown in Fig.3. Again, users can add
other functions, including non-manipulative functions as needed.
Fig. 3. Prompt explaining the robotic functions.
C. Representation of the environments
In the next prompt, we deﬁne the representation method for the environment (Fig. 4). We describe the environment by
dividing it into non-manipulable obstacles, which we refer to as assets, such as shelves and tables, and manipulable objects,
which we refer to as objects, such as cans and handles. The environment was represented as a dictionary containing the lists
of assets, objects, and their states. The states explained the spatial relationships between assets and objects. The environment
information presented in this prompt is merely an example, and the actual environment will be provided with text instructions
in the sixth prompt. The environment name can be arbitrary, but we deﬁne labels that are compatible with general object
recognition to facilitate the integration with visual recognition systems. The spatial relationships, represented as the ”STATE
LIST” in Fig. 4, are described in language that is easy to specify, rather than using numerical values. However, in practice,
this level of expression has proven to be sufﬁcient for the system to function effectively. Of course, the contents of this state
list can also be modiﬁed to meet the user’s needs.

Fig. 4. Prompt for the representation of environments.
D. The format of ChatGPT’s output
In the following prompt, we explain how to format the output (Fig. 5). To facilitate easy integration with other processes,
such as robot control systems and visual recognition programs, we encourage ChatGPT to output a Python dictionary, which
can be saved as a JSON ﬁle. As a key point, we also encourage ChatGPT to output not only the sequence of robot actions
but also the linguistic descriptions of each action and supplementary information on environmental changes before and after
each operation. This information helps the user to debug whether ChatGPT is correctly understanding the input language and
inferring environmental operations. Furthermore, these supplementary pieces of information are also useful for integration as
an application. For example, the system can leverage the linguistic descriptions of each action to ask the user to perform
the speciﬁc actions required to collect visual information needed to complete the task (see [12] for an actual application, an
interactive learning-from-observation system). Additionally, the system can reuse the updated environmental information in the
next input to enable multi-step instructions.
Fig. 5. Prompt explaining the format of ChatGPT’s output.

E. Examples of input and output
In the following prompt, we provide examples of the expected inputs and outputs (Fig. 6) to achieve effective few-shot
results [13]. We found that providing more examples helps ChatGPT generate the desired sequence and minimizes the effort
required by users to correct the output during conversations.
Fig. 6. Prompt providing examples of desired input and output. A part of the prompt is shown.

F. Speciﬁc input from the user
Lastly, we format the user input that contains the environmental and natural language instructions speciﬁc to the use case (Fig.
7). We have provided a template for the user input, as shown in (a) of Fig. 7, which can be reused by replacing [INSTRUCTION]
with the user’s text input and [ENVIRONMENT] with the user’s speciﬁed environment, as illustrated in (b) of Fig. 7. It is
worth noting that since ChatGPT’s outputs the post-operation environment as the result of the former action sequence, the user
only needs to input the environment at the beginning. For the second and subsequent instructions, ChatGPT’s next response
was created including all previous turns of conversation in order to allow ChatGPT to make corrections, if requested by the
user, based on its own previous output and user feedback. If the number of input tokens exceeded the allowable number that
ChatGPT could process, we adjusted the token size by truncating the ﬁrst part of the sixth prompt (Fig. 8).
Fig. 7. The user input template and examples of actual input used. The user is assumed to deﬁne the environment information in advance in some way. By
inputting the environmental information after the operation output by ChatGPT as the next input, continuous environmental operations can be realized.

Fig. 8. The entire structure of the conversation that will be inputted into ChatGPT for generating a response.
III. EXPERIMENTS
In this section, we tested the prompts to check if ChatGPT can operates as expected. We used the ﬁxed GPT model provided
by Azure OpenAI (gpt-3.5-turbo) in our experiments. Due to space limitations, some experimental results are not presented
here as ﬁgures. However, all results including instructions and environment deﬁnitions can be found at https://github.com/
microsoft/ChatGPT-Robot-Manipulation-Prompts.
A. Multi-step manipulation of the environment
We tested the applicability of the proposed prompts for multi-step instructions in various environments. Speciﬁcally, we
conducted role-plays instructing the rearrangement and disposal of objects placed on tables and shelves, retrieving objects
from refrigerators and drawers, and cleaning tables and windows with a sponge, while assuming the operation in a household
environment. To ensure usability, we provided ChatGPT with instructions and feedback in natural language that resembled
the way humans communicate with each other. The results suggest that ChatGPT can effectively convert natural language
instructions into robot actions. Furthermore, by reusing the state of the environment after each manipulation, which was output
by ChatGPT, as the initial state for the subsequent manipulation (see Fig. 8), we were able to manipulate objects along the
instruction sequences without losing track of the environment’s state. Note that some of the results shown in this section contain
the outcomes generated by ChatGPT after receiving user feedback.
1) Relocation of objects on a table: A juice can placed on the bottom shelf of two shelves and a Spam can placed on the
table are manipulated by a multi-step natural language (Fig.9). First, the juice can on the bottom shelf is moved to the top
shelf. Next, the Spam can is thrown into the trash. Next, the juice can is moved to the table top. Finally, the juice can is placed
in the trash.

Fig. 9. Example of ChatGPT’s output for step-by-step natural language teaching, including multi-step manipulation of the environment. (Left panel) Robot
actions broken down for each natural language. (Right panel) The state of the environment that is output by ChatGPT. A part of JSON output is shown for each
ﬁle. All the results, including the representation of the environment can be found here: https://github.com/microsoft/ChatGPT-Robot-Manipulation-Prompts.
2) Open a fridge/drawer door: Next, we tested the following actions: rotating the refrigerator door open, opening the
refrigerator a little wider, removing the juice from the refrigerator and placing it on the ﬂoor, and ﬁnally closing the refrigerator
(Fig.9). Similarly, the task could be broken down by ChatGPT with the action of sliding the drawer open (Figure is not shown)

Fig. 10. Example of ChatGPT’s output for opening fridge and taking out a juice. (Left panel) Robot actions broken down for each natural language. (Right
panel) The state of the environment that is output by ChatGPT. A part of JSON output is shown for each ﬁle. All the results, including the representation of
the environment can be found here: https://github.com/microsoft/ChatGPT-Robot-Manipulation-Prompts.
3) Wipe a window with a sponge, and throw it away: Next, we tested the action of taking the sponge on the desk, wiping
the window, and returning it to the table. Following the operation, a robot throws the sponge into the trash (Fig.11). Similarly,
the task could be broken down by ChatGPT with the action of wiping the table with a sponge (Figure is not shown).
Fig. 11. Example of ChatGPT’s output for wiping a window with a sponge. (Left panel) Robot actions broken down for each natural language. (Right panel)
The state of the environment that is output by ChatGPT. A part of JSON output is shown for each ﬁle. All the results, including the representation of the
environment can be found here: https://github.com/microsoft/ChatGPT-Robot-Manipulation-Prompts.

B. Correction of ChatGPT’s output through user feedback
Since ChatGPT does not always generate complete action sequences, it is important for users to be able to review and
correct any errors in order to ensure safe and robust operation. With this in mind, We tested the ability to adjust ChatGPT’s
output by providing natural language feedback (see Fig. 12).
Fig. 12. Example of adjusting an output sequence with natural language feedback. The initial instruction to ChatGPT is to move a juice from the bottom
shelf to the upper shelf. (Left panel) After the feedback of ”Insert another move object() to in moving the the juice upward.” an action of move-object()
was added to the sequence. (Right panel) After the feedback of ”In this case, you can omit one move object() that moves the juice upward.” an action of
move object() was deleted from the sequence.
IV. DISCUSSION: TOWARDS MORE GENERAL ROBOTIC APPLICATIONS
In this paper, we focused on generating a sequence of basic robot actions from natural language commands given by a
human. We designed prompts for ChatGPT to meet three requirements: 1) easy integration with robot execution systems or
visual recognition programs, 2) applicability to various environments, and 3) the ability to provide long-step instructions while
minimizing the impact of ChatGPT’s token limit. Using these prompts, we conducted experiments to evaluate the effectiveness
of ChatGPT in generating robot actions from natural language commands. Through the experiments, we conﬁrmed that the
proposed prompts work as expected. Additionally, we checked that ChatGPT enables the user to correct the output interactively,
making the interface robust and useful. Based on these results, we consider that the proposed prompts provide a practical
example that can be widely used in the robotics research community.
It seems not trivial that ChatGPT is able to decompose given instructions without experiencing actual object manipulation,
only based on few-shot data. We consider that this ability may be attributed to the fact that the model learns about knowledge
related to object manipulation and the temporal relationships between cohesively occurring actions within the vast amount of
data it is trained on. In fact, ChatGPT can generate recipes when provided with a menu, which suggests that procedural steps
are learned within the language model.
The speciﬁc prompts are designed under the assumption that the robot has at least one arm, sufﬁcient degrees of freedom, and
reachability to execute the desired task in the given environment. Additionally, we assume that a user’s instructions are given

at the granularity of grasp-manipulation-release, which involves handling a single object from grasping to releasing. However,
these assumptions may be too strong when considering more general robot manipulations. We will discuss several issues
that arise when integrating the proposed approach into more general robotic applications, such as dealing with multi-object
manipulations, and propose ideas such as incorporating visual feedback to address these challenges.
1) Handling of higher-level logic: Some manipulations may require changing actions depending on the recognition results
or the environment, or repeating actions until certain conditions are met. For example, when performing a task such as throwing
away trash, it may be necessary to use different garbage cans depending on the object or to wipe the table until it is clean. It is
known that tasks such as generating programs from natural language, including high-level logic such as conditional branching,
can be achieved by large-scale language models such as Codex ( [14]). In robot applications, it is also suggested that logic
can be constructed based on abstracted action sequences in a similar manner ( [8]). Consistent with these ideas, we conﬁrmed
that small modiﬁcations to the prompts enabled ChatGPT to generate Python-style source codes that include logic (see Fig.
13). Additionally, we checked that it is also possible to prepare higher-level logic that controls the generated JSON ﬁles by
preparing a separate ChatGPT interface responsible for logic and reading these JSON ﬁles (see Fig. 14 for an example). These
results suggest that the proposed approach can be relatively easily connected to applications requiring higher-order logic.
Fig. 13. An example demonstrating the feasibility of using ChatGPT to generate control programs that include conditional branching. A part of the prompts
is shown. Note that we encouraged ChatGPT to track the state of objects at every line, as the ﬁnal state can change according to the conditional branching.
We also added a non-manipulative function in the robot action set. All the results, including the representation of the environment, can be found here:
https://github.com/microsoft/ChatGPT-Robot-Manipulation-Prompts.

Fig.
14. An example showing the feasibility of using ChatGPT to generate control programs that include conditional branching. Such logic generation is
outside the scope of this paper, which focuses on the generation of a sequence of robot actions for a single object manipulation (corresponding to the JSON
ﬁles in the ﬁgure).
2) Collaboration of multiple arms/robots: When a robot has multiple arms, it is possible to coordinate them to achieve a
task. The planner can treat the left and right arms as independent manipulators and read separate JSON ﬁles to manipulate a
single object. Although this is beyond the scope of this study, any planner, including ChatGPT, can achieve this. We conﬁrmed
that small modiﬁcations to the prompts enabled ChatGPT to generate a task sequence that involves both arms (Fig. 15).
Additionally, we checked that it is also possible to prepare another logic that controls multiple arms by calling the generated
JSON ﬁles through a separate ChatGPT interface (Fig. 16). These results suggest that the proposed approach can be relatively
easily connected to applications requiring multiple arms/robots to coordinate.

Fig. 15. An example demonstrating the feasibility of ChatGPT in generating control programs that involve multiple arms or robots cooperating. Note that we
included hand laterality in every function and outputted all the objects to be manipulated, as multiple objects can be handled during the grasp-manipulation-
release operations of both hands. All the results, including the representation of the environment, can be found here: https://github.com/microsoft/ChatGPT-
Robot-Manipulation-Prompts.
Fig. 16. An example demonstrating the feasibility of ChatGPT in generating control programs that involve multiple arms or robots coordinating. This type
of planning is beyond the scope of this paper.
3) Optimization of motion: Optimizing the motion of a robot is desirable for performing tasks safely and efﬁciently. For
instance, the number of ”move-hand()” calls and the trajectory of the hand can be optimized during manipulation. We recognize
that language inference has limitations in inferring parameters related to the interaction between the environment and the robot
hardware, since it requires physical experience of object interaction. For example, humans switch among a variety of grasping

strategies depending on the type of object to manipulate [15]. This is rooted in the symbol grounding problem. Our system
is built on the philosophy of 90% AI, where humans always check the operation to ensure safe, predictable, and robust
performance. Possible solutions include: 1) teaching the system optimal strategies through verbal correction (Section III-B) or
visual demonstration (e.g., using our in-house learning-from-observation vision systems, where humans visually demonstrate
how to move the body [9], [16] and where to focus [17], and how to grasp efﬁciently [18], [19]), and 2) designing robots to
learn optimal policies with the help of humans (e.g., through reinforcement learning [20]).
4) Adapting to environmental changes: The working environment of a robot may change while a task is being performed.
For example, objects may move within the workspace. By outputting these environmental changes to ChatGPT, along with the
robot’s action sequence and self-referencing prompts, the robot can track the changes it causes to its environment. However,
a limitation of this paper is that humans need to provide ChatGPT with the environmental information in a formatted way.
In future studies, we plan to consider integrating a vision (or multimodal) encoder to output the formatted environmental
information.
5) Connection with vision systems and robot controllers: Among recent experimental attempts to generate robot manipulation
from natural language using ChatGPT, our work is unique in its focus on the generation of robot action sequences (i.e., ”what-
to-do”), while avoiding redundant language instructions to obtain visual and physical parameters (i.e., ”how-to-do”), such as
how to grab, how high to lift, and what posture to adopt. Although both types of information are essential for operating a robot
in reality [21], the latter is often better presented visually than explained verbally. Therefore, we have focused on designing
prompts for ChatGPT to recognize what-to-do, while obtaining the how-to-do information from human visual demonstrations
and a vision system during robot execution.
As part of our efforts to develop a realistic robotic operation system, we have integrated the proposed system with a
learning-from-observation system (Fig. 17) that includes a speech interface [22], [23], a visual teaching interface [12], a
reusable library of robot actions [24], and a simulator for testing robot execution [25]. Please refer to the respective papers
for the results of robot execution, as it is beyond the scope of this paper. The code for the teaching interface is available at:
https://github.com/microsoft/cohesion-based-robot-teaching-interface.
Fig. 17. An example of integrating the proposed ChatGPT prompts into a robot teaching system. The system breaks down natural language input instructions
into a sequence of robot actions, and then obtains the necessary parameters for robot execution (i.e., how to perform the actions) by prompting a human to
visually demonstrate each step of the decomposed action sequence.
V. CONCLUSION
This paper presents a practical application of OpenAI’s ChatGPT for converting natural language instructions into executable
robot action sequences. We designed input prompts to meet common requirements in practical applications, speciﬁcally
encouraging ChatGPT to output a sequence of robot actions in a readable format, represent the operating environment in
a formalized style, and output the updated state of the environment. Through experiments, we tested the effectiveness of our
proposed prompts in various environments. Additionally, we observed that ChatGPT’s conversational ability allows users to
adjust its output with natural language feedback, which is crucial for developing an application that is both safe and robust
while providing a user-friendly interface. Our prompts and source code are open-source and publicly available. We hope this
paper provides practical knowledge to the robotics research community and inspires further development in this research area.
VI. NOTE
This paper was written by the authors, and ChatGPT was used for proofreading.

REFERENCES
[1] OpenAI, “Chatgpt.” https://openai.com/blog/chatgpt.
[2] P. Pramanick, H. B. Barua, and C. Sarkar, “Decomplex: Task planning from complex natural instructions by a collocating robot,” in 2020 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS), pp. 6894–6901, IEEE, 2020.
[3] S. G. Venkatesh, R. Upadrashta, and B. Amrutur, “Translating natural language instructions to computer programs for robot manipulation,” in 2021
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1919–1926, IEEE, 2021.
[4] I. Yanaokura, N. Wake, K. Sasabuchi, R. Arakawa, K. Okada, J. Takamatsu, M. Inaba, and K. Ikeuchi, “A multimodal learning-from-observation towards
all-at-once robot teaching using task cohesion,” in 2022 IEEE/SICE International Symposium on System Integration (SII), pp. 367–374, IEEE, 2022.
[5] Y. Jiang, A. Gupta, Z. Zhang, G. Wang, Y. Dou, Y. Chen, L. Fei-Fei, A. Anandkumar, Y. Zhu, and L. Fan, “Vima: General robot manipulation with
multimodal prompts,” arXiv preprint arXiv:2210.03094, 2022.
[6] M. Shridhar, L. Manuelli, and D. Fox, “Perceiver-actor: A multi-task transformer for robotic manipulation,” in Conference on Robot Learning, pp. 785–
799, PMLR, 2023.
[7] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, et al., “Do as i can, not as i
say: Grounding language in robotic affordances,” arXiv preprint arXiv:2204.01691, 2022.
[8] S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor, “Chatgpt for robotics: Design principles and model abilities,” preprint, 2023.
[9] N. Wake, R. Arakawa, I. Yanokura, T. Kiyokawa, K. Sasabuchi, J. Takamatsu, and K. Ikeuchi, “A learning-from-observation framework: One-shot robot
teaching for grasp-manipulation-release household operations,” in SII, IEEE, 2021.
[10] K. Ikeuchi, N. Wake, R. Arakawa, K. Sasabuchi, and J. Takamatsu, “Semantic constraints to represent common sense required in household actions for
multi-modal learning-from-observation robot,” arXiv preprint arXiv:2103.02201, 2021.
[11] H. T. Kuhn and W. L. Inequalities, “Related systems,” Annals of Mathematic Studies, Princeton Univ. Press. EEUU, 1956.
[12] N. Wake, A. Kanehira, K. Sasabuchi, J. Takamatsu, and K. Ikeuchi, “Interactive learning-from-observation through multimodal human demonstration,”
arXiv preprint arXiv:2212.10787, 2022.
[13] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., “Language models are
few-shot learners,” Advances in neural information processing systems, vol. 33, pp. 1877–1901, 2020.
[14] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al., “Evaluating large language
models trained on code,” arXiv preprint arXiv:2107.03374, 2021.
[15] N. Wake, K. Sasabuchi, and K. Ikeuchi, “Grasp-type recognition leveraging object affordance,” HOBI–RO-MAN Workshop, 2020.
[16] K. Sasabuchi, N. Wake, and K. Ikeuchi, “Task-oriented motion mapping on robots of various conﬁguration using body role division,” IEEE Robotics
and Automation Letters, vol. 6, no. 2, pp. 413–420, 2020.
[17] N. Wake, I. Yanokura, K. Sasabuchi, and K. Ikeuchi, “Verbal focus-of-attention system for learning-from-demonstration,” in ICRA, IEEE, 2021.
[18] N. Wake, D. Saito, K. Sasabuchi, H. Koike, and K. Ikeuchi, “Object affordance as a guide for grasp-type recognition,” arXiv preprint arXiv:2103.00268,
2021.
[19] D. Saito, N. Wake, K. Sasabuchi, H. Koike, and K. Ikeuchi, “Contact web status presentation for freehand grasping in mr-based robot-teaching,” in
Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction, pp. 167–171, 2021.
[20] D. Saito, K. Sasabuchi, N. Wake, J. Takamatsu, H. Koike, and K. Ikeuchi, “Task-grasping from a demonstrated human strategy,” in 2022 IEEE-RAS 21st
International Conference on Humanoid Robots (Humanoids), pp. 880–887, IEEE, 2022.
[21] K. Ikeuchi, J. Takamatsu, K. Sasabuchi, N. Wake, and A. Kanehira, “Applying learning-from-observation to household service robots: three common-sense
formulations,” arXiv preprint, 2023.
[22] N. Wake, M. Fukumoto, H. Takahashi, and K. Ikeuchi, “Enhancing listening capability of humanoid robot by reduction of stationary ego-noise,” IEEJ
Transactions on Electrical and Electronic Engineering, vol. 14, no. 12, pp. 1815–1822, 2019.
[23] J. Jaroslavceva, N. Wake, K. Sasabuchi, and K. Ikeuchi, “Robot ego-noise suppression with labanotation-template subtraction,” IEEJ Transactions on
Electrical and Electronic Engineering, vol. 17, no. 3, pp. 407–415, 2022.
[24] J. Takamatsu, K. Sasabuchi, N. Wake, A. Kanehira, and K. Ikeuchi, “Learning-from-observation system considering hardware-level reusability,” arXiv
preprint arXiv:2212.09242, 2022.
[25] K. Sasabuchi, D. Saito, A. Kanehira, N. Wake, J. Takamatsu, and K. Ikeuchi, “Task-sequencing simulator: Integrated machine learning to execution
simulation for robot manipulation,” arXiv preprint arXiv:2301.01382, 2023.

