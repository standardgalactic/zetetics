Calibrate Before Use:
Improving Few-Shot Performance of Language Models
Tony Z. Zhao * 1 Eric Wallace * 1 Shi Feng 2 Dan Klein 1 Sameer Singh 3
Abstract
GPT-3 can perform numerous tasks when pro-
vided a natural language prompt that contains a
few training examples. We show that this type of
few-shot learning can be unstable: the choice of
prompt format, training examples, and even the
order of the training examples can cause accuracy
to vary from near chance to near state-of-the-art.
We demonstrate that this instability arises from
the bias of language models towards predicting
certain answers, e.g., those that are placed near
the end of the prompt or are common in the pre-
training data. To mitigate this, we ﬁrst estimate
the model’s bias towards each answer by asking
for its prediction when given the training prompt
and a content-free test input such as “N/A”. We
then ﬁt calibration parameters that cause the pre-
diction for this input to be uniform across answers.
On a diverse set of tasks, this contextual calibra-
tion procedure substantially improves GPT-3 and
GPT-2’s average accuracy (up to 30.0% absolute)
and reduces variance across different choices of
the prompt.
1. Introduction
Few-shot learning—the ability to learn tasks with limited
examples—is an important aspect of intelligence (Lake et al.,
2015; Yogatama et al., 2019). Recent work shows that large
neural language models can perform few-shot learning with-
out ﬁnetuning (Radford et al., 2019; Brown et al., 2020).
Speciﬁcally, GPT-3 (Brown et al., 2020) can perform nu-
merous tasks when provided a few examples in a natural
language prompt. For example, to perform sentiment analy-
sis one can condition GPT-3 on a prompt such as:
Input: Subpar acting. Sentiment: Negative
Input: Beautiful ﬁlm. Sentiment: Positive
Input: Amazing.
Sentiment:
*Equal contribution
1UC Berkeley
2University of Mary-
land 3UC Irvine. Correspondence to: Eric Wallace <ericwal-
lace@berkeley.edu>.
where the ﬁrst two lines correspond to two training examples
and the last line is a test example. To make predictions, the
model predicts whether the subsequent token is more likely
to be the word “Positive” or “Negative”.
This style of few-shot “in-context” learning is interesting
because it shows that the model can learn without parameter
updates. And, more importantly, it has numerous practi-
cal advantages over the now-standard approach of ﬁnetun-
ing (Radford et al., 2018; Devlin et al., 2019). First, it allows
practitioners to “rapidly prototype” NLP models: changing
the prompt immediately leads to a new model. Second, it
provides a fully natural language interface to a machine
learning model, which allows users—even those without
technical expertise—to create NLP systems. Finally, since
in-context learning reuses the same model for each task, it
reduces memory requirements and system complexity when
serving many different tasks.
However, despite these promises, we show that GPT-3’s
accuracy can be highly unstable across different prompts
(Section 3). A prompt contains three components: a format,
a set of training examples, and a permutation (ordering) for
those examples. We show that different choices for these
factors can lead to highly different accuracies, e.g., changing
the permutation of the training examples in a sentiment
analysis prompt can change accuracy from near chance
(54%) to near state-of-the-art (93%). This instability implies
that GPT-3 users, who typically design prompts manually,
cannot expect to consistently obtain good accuracy.
We next analyze what causes this instability. We identify
three pitfalls of language models that lead them to be bi-
ased toward certain answers during few-shot learning. In
particular, they suffer from majority label bias, recency bias,
and common token bias (Section 4). The majority label and
recency biases lead the model to predict training answers
that appear frequently or near the end of the prompt. For
example, a prompt that ends with a Negative training ex-
ample may cause a bias towards the Negative class. On
the other hand, the common token bias leads the model to
prefer answers that are frequent in its pre-training data, e.g.,
it prefers “United States” over “Saint Lucia”, which is likely
suboptimal for the task of interest.
We identify that these biases typically result in a shift in
the output distribution of the model. We can thus coun-
arXiv:2102.09690v2  [cs.CL]  10 Jun 2021

Calibrate Before Use: Improving Few-Shot Performance of Language Models
0 1
4
8
16
Number of Training Examples
40
50
60
70
80
90
AGNews Accuracy (%)
GPT-3 175B
With Calibration
0
1
4
8
Number of Training Examples
40
50
60
70
80
MIT Director Accuracy (%)
GPT-3 13B
With Calibration
0 1
4
8
16
Number of Training Examples
30
40
50
60
70
80
DBPedia Accuracy (%)
GPT-3 2.7B
With Calibration
Figure 1. Few-shot learning can be highly unstable across different choices of the prompt. Above, we plot the mean accuracy (± one
standard deviation) across different choices of the training examples for three different datasets and model sizes. We show that our method,
contextual calibration, improves accuracy, reduces variance, and overall makes tools like GPT-3 more effective for end users.
teract these biases by “calibrating” the output distribution.
Concretely, we estimate the model’s bias towards certain an-
swers by feeding in a dummy test input that is content-free.
In the prompt above for example, if we replace “Amazing.”
with the string “N/A”, the model predicts 62% Positive. We
then ﬁt the calibration parameters so that the content-free
input has uniform scores for each answer. This contextual
calibration procedure provides a good setting of the calibra-
tion parameters without additional training data.
We test the effectiveness of contextual calibration on a range
of tasks (Section 5). Contextual calibration consistently
improves GPT-3 and GPT-2’s accuracy (up to 30.0% ab-
solute) across different choices of the prompt format and
examples (e.g., Figure 1). It also makes the accuracy more
stable across different prompts, thus mitigating the need
for prompt engineering. Overall, contextual calibration is a
simple method that makes language models better few-shot
learners: it enables end users to obtain higher accuracy with
considerably less effort.
2. Background and Experimental Setup
Neural autoregressive language models (LMs) take as input
a sequence of tokens and output a probability distribution
over the next token. Large neural LMs can perform tasks in a
zero- or few-shot manner using in-context learning (Radford
et al., 2019; Brown et al., 2020). To do so, a natural language
prompt is fed into the model. This prompt contains three
components: a format, a set of training examples, and a
permutation (ordering) of the training examples.
Prompt Format The prompt format is a template which
consists of placeholders for the training and test example(s)
and possibly a natural language description of the task. For
example, the format of the prompt in Section 1 is a template
with the style: “Input:” input “Sentiment:” label. Many
alternate formats exist, e.g., one could frame the task as
question answering.
Prompt Training Examples The prompt’s training exam-
ples are used to teach the LM how to solve the task at
hand. The prompt from Section 1 consists of two training
examples; we refer to this as “two-shot” learning. We also
consider “zero-shot” learning, where no training examples
are present.
Training Example Permutation When training examples
are used, they have a particular permutation, e.g., the “Sub-
par acting” example comes ﬁrst in the prompt from Sec-
tion 1. The permutation matters because neural language
models update their hidden states in a left-to-right-fashion.
To make predictions on an input, we slot it into the test
placeholder and generate from the LM. For example, see the
“Amazing.” test example in the prompt from Section 1. For
generation tasks, we generate greedily from the LM until
it produces a newline character. For classiﬁcation tasks,
the probability for each class is given by the probability
assigned to its associated label name, e.g., the words “Nega-
tive” and “Positive” for sentiment classiﬁcation.
2.1. Datasets and Prompt Formats
We use datasets for three tasks: text classiﬁcation, fact
retrieval, and information extraction. We use a ﬁxed prompt
format for each dataset unless otherwise speciﬁed. We show
the format and examples from each dataset in Appendix B.
Text Classiﬁcation We study text classiﬁcation using six
datasets: sentiment analysis using SST-2 (Socher et al.,
2013), 6-way question classiﬁcation using TREC (Voorhees
& Tice, 2000), textual entailment using 3-way CB (de Marn-
effe et al., 2019) and binary RTE (Dagan et al., 2005) from
SuperGLUE (Wang et al., 2019), and topic classiﬁcation

Calibrate Before Use: Improving Few-Shot Performance of Language Models
1
2
3
4
5
6
7
8
9
10
Training Set ID
50
60
70
80
90
SST-2 Accuracy (%)
Accuracy Across Training Sets and Permutations
Figure 2. There is high variance in GPT-3’s accuracy as we change
the prompt’s training examples, as well as the permutation of the
examples. Here, we select ten different sets of four SST-2 training
examples. For each set of examples, we vary their permutation and
plot GPT-3 2.7B’s accuracy for each permutation (and its quartiles).
1
2
3
4
5
6
7
8
9
10
Format ID
50
60
70
80
90
SST-2 Accuracy (%)
Accuracy Across Formats and Training Sets
Figure 3. There is high variance in GPT-3’s accuracy as we change
the prompt format. In this ﬁgure, we use ten different prompt
formats for SST-2. For each format, we plot GPT-3 2.7B’s accuracy
for different sets of four training examples, along with the quartiles.
using the 4-way AGNews (Zhang et al., 2015) and 14-way
DBPedia (Zhang et al., 2015) datasets. The prompt in Sec-
tion 1 shows an example of the sentiment analysis task.
Fact Retrieval We evaluate fact retrieval with LAMA
(Petroni et al., 2019). The dataset consists of knowledge
base triples that are placed into templates with missing ob-
jects, e.g. “Obama was born in”. We use these templates
as our prompts, and remove the relations where the missing
answer is not at the end of the template (left-to-right LMs
cannot solve these). The answers are always single tokens,
and we report average accuracy across all triples.
Information Extraction We consider information extrac-
tion using two slot ﬁlling datasets, ATIS (Hemphill et al.,
1990) and MIT Movies trivia10k13 (Liu et al., 2012). We
use two random slots for each dataset, airline and departure
date for ATIS, and director name and movie genre for MIT
Movies. The answer for both datasets is a span of text from
the input, e.g., the ATIS airline task is to predict “american
airlines” when given the sentence “list a ﬂight on american
airlines from toronto to san diego”. We use Exact Match
between the model’s generated output and the ground-truth
span as our evaluation metric.
2.2. Model Details
We run our experiments on three sizes of GPT-3 (2.7B, 13B,
and 175B parameters) as well as GPT-2 (1.5B parameters).
We access GPT-3 using the OpenAI API. We release code
to replicate our experiments.1
1https://www.github.com/tonyzhaozh/few-shot-learning
3. Accuracy Varies Highly Across Prompts
This section studies how GPT-3’s accuracy changes as we
vary each aspect of the prompt (training examples, permu-
tation, format). We focus on a subset of the datasets to
simplify our analysis; in Section 5 we show that our ﬁnd-
ings hold across all of the datasets we study.
GPT-3’s accuracy depends highly on both selection and
permutation of training examples. Concretely, we use a
ﬁxed prompt format and choose different random sets of
training examples. For each set of training examples, we
evaluate the accuracy for all possible permutations.
Figure 2 shows the results for SST-2 (4-shot, GPT-3 2.7B).
Surprisingly, varying the permutation can be as important,
or even more important, than which training examples are
chosen. For example, varying the permutation of the train-
ing examples can cause accuracy to go from near chance
(54.3%) to near state-of-the-art (93.4%). For a qualitative
example of the sensitivity to permutations, see Table 2 in
Appendix A. This high importance on example order is in
contrast to standard machine learning, where the ordering
of examples during training is typically an afterthought.
The variance persists with more data and larger models.
Adding more training examples into the prompt does not
necessarily reduce the variance in accuracy. We sweep over
the number of training examples for three different datasets
in Figure 1 (red curves). The variance remains high even
when we use 16 training examples. Moreover, adding more
training examples can sometimes hurt accuracy (e.g., mean
accuracy drops from 36.0% to 25.9% for DBPedia 0-shot
to 1-shot). The variance in accuracy can also remain high
when using larger models, e.g., the left of Figure 1.

Calibrate Before Use: Improving Few-Shot Performance of Language Models
0
0.2
0.4
0.6
0.8
1.0
Probability
p(Positive)
Figure 4. Majority label and recency biases cause GPT-3 to become biased towards certain answers and help to explain the high variance
across different examples and orderings. Above, we use 4-shot SST-2 with prompts that have different class balances and permutations,
e.g., [P P N N] indicates two positive training examples and then two negative. We plot how often GPT-3 2.7B predicts Positive on the
balanced validation set. When the prompt is unbalanced, the predictions are unbalanced (majority label bias). In addition, balanced
prompts that have one class repeated near the end, e.g., end with two Negative examples, will have a bias towards that class (recency bias).
GPT-3’s accuracy depends highly on prompt format.
We next keep the set of training examples and permutations
ﬁxed but vary the prompt format. We focus on SST-2, and
we manually design an additional 14 prompt formats. The
formats include question-answer templates, conversation-
style templates, prompts that resemble Web pages, and vari-
ations on the label names (all formats available in Table 7 in
Appendix B). The accuracy for ten of the formats is shown
in Figure 3. We ﬁnd that some of the formats are better than
others on average. However, all of the formats still suffer
from high variance across different training sets.
4. What Causes the High Variance?
We next analyze why GPT-3’s accuracy varies across differ-
ent training examples, permutations, and prompt formats.
Concretely, we show that the variance arises because LMs
are biased towards outputting answers that are (1) frequent
in the prompt (majority label bias), (2) towards the end of the
prompt (recency bias), and (3) common in the pre-training
data (common token bias).
Majority Label Bias We ﬁnd that GPT-3 is biased towards
answers that are frequent in the prompt. A trivial case is
when a text classiﬁcation prompt has a class imbalance, e.g.,
more Positive than Negative sentiment examples. This is
demonstrated in the “unbalanced” region of Figure 4: when
one class is more common, GPT-3 2.7B is heavily biased
towards predicting that class. Since the SST-2 sentiment
analysis dataset is balanced, this bias causes large accuracy
degradations. The majority label bias also explains why we
frequently observe a drop in accuracy when moving from
0-shot to 1-shot—we found that the drop is due to the model
frequently repeating the class of the one training example.
The majority label bias also occurs for generation tasks. On
the validation set for 4-shot LAMA with GPT-3 2.7B, 50.2%
of the model predictions are a repeat of one of the four train-
ing answers (the correct repeat rate is 24.7%). Overall, the
majority label bias helps to explain why different choices for
the training examples heavily inﬂuence GPT-3’s accuracy—
it shifts the distribution of model predictions.
Recency Bias The model’s majority label bias is aggravated
by its recency bias: the tendency to repeat answers that ap-
pear towards the end of the prompt. The “balanced” region
of Figure 4 demonstrates this. For instance, when two Neg-
ative examples appear at the end (P P N N), the model will
heavily prefer the Negative class. Moreover, the recency
bias can outweigh the majority label bias, e.g., the “P P P
N” training set leads to nearly 90% of predictions being
Negative, despite 3
4 of the training examples being Positive.
Recency bias also affects generation tasks.
For 4-shot
LAMA, the training answers that are closer to the end of the
prompt are more likely to be repeated by the model. Con-
cretely, the model “overpredicts” the answer from the 1st,
2nd, 3rd, and 4th training example by 8.5%, 8.3%, 14.3%,
and 16.1%, respectively.2 Overall, recency bias helps to
explain why the permutation of the training examples is
important—the ordering of the examples heavily inﬂuences
the distribution of the model predictions.
Common Token Bias Finally, we ﬁnd that GPT-3 is bi-
ased towards outputting tokens that are common in its pre-
training distribution, which is likely suboptimal for the dis-
tribution of answers on the downstream task. A simple case
of this occurs for the LAMA fact retrieval dataset, where
the model often predicts common entities such as “America”
when the ground-truth answer is instead a rare entity.
A more nuanced case of the common token bias occurs for
2Over all relations, as well as three different sets of training
examples, the model repeats the training example at a rate of
20.7%, 19.8%, 29.9%, and 26.8%. The ground-truth repeat rate is
12.2%, 11.5%, 15.6%, and 10.7%. We deﬁne “overpredicts” as the
model’s repeat rate minus the ground-truth repeat rate.

Calibrate Before Use: Improving Few-Shot Performance of Language Models
text classiﬁcation. Recall that the model makes predictions
by generating the label name associated with each class.
Because certain label names appear more frequently in the
pre-training data, the model will be inherently biased to-
wards predicting certain classes. For example, on DBPedia
(a balanced 14-way topic classiﬁcation dataset), GPT-3 pre-
dicts the “book” class 11× more often than the “artist” class.
In fact, there is a moderate correlation (r = 0.67) between
the frequency of a DBPedia label name and the rate at which
GPT-3 predicts its class.3 Overall, the common token bias
helps to explain why the choice of label names is important,
and why the model struggles on rare answers.
The Impact of Biases on Model Predictions We ﬁnd that
the end result of the above three biases is typically a sim-
ple shift in the model’s output distribution. For example,
Figure 5 visualizes this shift for a SST-2 sentiment prompt.
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
p(Positive)
Figure 5. The Positive class probability for 25 random test inputs
for a particular sentiment analysis prompt. Negative ground-truth
examples are marked with G and Positive are marked with G.
The prompt used in Figure 5 and the model’s intrinsic biases
cause it to frequently predict high conﬁdence for the Positive
class. Since the default 50% threshold is used to make pre-
dictions, this results in frequent false positives. Importantly,
note that if we could optimally set the classiﬁcation thresh-
old (p(Positive) = 0.68 in this case), the classiﬁer would be
highly accurate (94% on the validation set).
5. Contextual Calibration
Thus far, we have shown that GPT-3 is biased towards cer-
tain answers due to the prompt and the model’s intrinsic
biases. Here, we look to correct this by “calibrating” the
model’s output probabilities.4 A common technique for
adjusting output probabilities is to apply an afﬁne transfor-
mation (Platt, 1999; Guo et al., 2017):
ˆq = softmax(Wˆp + b),
(1)
where a weight matrix W and a bias vector b are applied
to the original probabilities ˆp to get the new probabilities
3The frequency of a token on the web is calculated using
Google Ngrams https://books.google.com/ngrams. The pre-
dictions are from the 0-shot setting on the validation set.
4The output of GPT-3 is biased (its outputs are shifted), similar
to how measurement devices such as voltage meters or weighing
scales are biased. Just like how these devices require “calibration
before use”, where the devices’ outputs are scaled/zeroed-out, we
hope to apply a similar calibration procedure to LMs. This goal is
distinct from statistical calibration (Brier, 1950; Guo et al., 2017),
i.e., aligning a model’s conﬁdence estimate with its true accuracy.
ˆq.5 For classiﬁcation tasks, ˆp is the set of probabilities that
are associated with each label name, renormalized to one.
For generation tasks, ˆp is the entire set of probabilities for
the ﬁrst token.6 In this paper, we restrict the matrix W to
be diagonal, known as vector scaling (Guo et al., 2017), to
prevent the parameters from growing quadratically in the
size of ˆp (which is ≈50, 000 for generation tasks).
The main challenge in the zero- or few-shot setting is that
we do not have data to learn W and b. We thus propose a
novel data-free procedure to infer a good setting of these
parameters. The key idea is that the model’s bias towards
certain answers can be estimated by feeding in a content-
free input such as the string “N/A”. For example, consider
the two-shot prompt:
Input: Subpar acting. Sentiment: Negative
Input: Beautiful ﬁlm. Sentiment: Positive
Input: N/A
Sentiment:
where “N/A” serves as the test input. Ideally, GPT-3 would
score this test input as 50% Positive and 50% Negative.
However, the model’s biases cause it to score this input as
61.8% Positive. Note that this error is contextual: a different
choice of the training examples, permutation, and format
will lead to different predictions for the content-free input.
We can correct this error by setting W and b so that the
class scores for the content-free input are uniform. We ﬁrst
obtain ˆp for the content-free input, denoted ˆpcf. We then set
W = diag(ˆpcf)−1 and b to the all-zero vector.7 To make
test predictions, we compute Wˆp + b and take the argmax.
Implementation Details This contextual calibration proce-
dure adds trivial amounts of computational overhead and
is implemented in a few lines of code (compute and save
ˆpcf, adjust output probabilities). For the content-free in-
put, many good choices exist, including “N/A”, the empty
string, and gibberish tokens. In all our experiments, we aver-
age the probabilities from three content-free inputs: “N/A”,
“[MASK]”, and the empty string.8 One could also craft the
content-free input in a task-speciﬁc manner. We explore this
for LAMA, where we replace the subject with the content-
free input, e.g., we use “N/A was born in” as the input.
5This afﬁne transformation is usually applied to the logits, i.e.,
prior to the softmax. However, we only have access to GPT-3’s
output probabilities in the OpenAI API.
6We only calibrate the prediction of the ﬁrst output token for
generation tasks. This is reasonable because, for the tasks we
consider, we found that the model’s predictions are highly deter-
ministic after generating the ﬁrst token.
7An alternate solution is to set b to −ˆpcf and W to the identity.
Empirically, this alternate solution yields higher accuracy for gen-
eration tasks (where the dimensionality of ˆp is large). The solution
in the main text performs better for classiﬁcation.
8We found this simple ensemble to achieve the best results for
AGNews, and we reuse it for all other datasets. See Section 5.2 for
an ablation on the choice of content-free input.

Calibrate Before Use: Improving Few-Shot Performance of Language Models
Dataset
LM
0-shot
1-shot
4-shot
8-shot
Baseline
Ours
Baseline
Ours
Baseline
Ours
Baseline
Ours
Text Classiﬁcation
AGNews
2.7B
44.7 0.0
63.2 0.0
33.0 5.1
59.6 6.4
43.3 8.3
71.1 8.5
50.8 7.8
72.7 5.8
175B
43.9 0.0
73.9 0.0
62.1 6.3
77.1 3.8
61.0 10.9
85.9 1.3
79.1 2.6
84.3 2.5
TREC
2.7B
31.0 0.0
38.8 0.0
24.3 6.4
36.8 7.7
25.8 11.5
38.6 13.2
29.3 8.0
44.3 11.4
175B
47.4 0.0
57.4 0.0
57.7 6.0
75.7 1.4
60.2 7.6
69.7 1.4
45.6 4.0
66.9 6.5
CB
2.7B
44.6 0.0
50.0 0.0
33.8 16.6
33.0 7.3
43.5 11.9
54.2 4.7
43.9 8.4
53.0 7.7
175B
30.4 0.0
48.2 0.0
50.9 6.7
51.8 7.2
45.2 19.4
60.7 6.7
59.6 11.3
65.0 7.9
RTE
2.7B
44.8 0.0
49.5 0.0
49.6 2.9
50.4 2.7
44.0 1.4
54.5 4.7
49.2 1.9
54.8 2.8
175B
57.8 0.0
57.8 0.0
62.9 2.7
62.8 2.3
58.7 11.9
60.4 8.1
66.2 5.8
65.5 2.5
SST-2
2.7B
57.2 0.0
71.4 0.0
67.3 7.9
79.1 8.3
59.1 10.2
79.9 7.8
54.0 4.3
82.0 5.5
175B
71.6 0.0
75.8 0.0
93.3 2.8
94.7 1.4
93.6 3.3
94.3 1.0
95.6 1.0
95.3 0.7
DBPedia
2.7B
36.0 0.0
38.7 0.0
25.9 4.4
61.6 2.9
61.0 12.8
66.0 7.5
72.6 4.5
74.8 5.0
175B
22.0 0.0
59.7 0.0
79.3 3.0
85.3 2.2
84.6 5.8
86.9 4.0
82.3 7.8
86.9 1.9
Fact Retrieval
LAMA
2.7B
14.0 0.0
22.7 0.0
29.7 1.8
31.6 1.3
35.8 3.8
37.4 3.4
42.5 1.3
42.5 1.4
175B
23.5 0.0
30.1 0.0
48.9 2.3
49.0 1.4
62.0 2.4
61.8 2.9
63.8 1.0
63.6 1.3
Information Extraction
MIT-G
2.7B
5.0 0.0
5.7 0.0
26.7 11.4
37.9 5.7
53.1 7.8
54.7 6.0
59.0 4.7
59.1 4.8
13B
15.0 0.0
18.7 0.0
47.3 3.9
52.0 7.9
57.9 4.8
58.9 4.0
59.0 4.7
59.1 4.8
MIT-D
2.7B
46.3 0.0
47.0 0.0
42.0 13.0
53.5 13.5
73.5 4.9
74.1 5.0
75.3 1.0
75.1 1.3
13B
36.3 0.0
38.7 0.0
58.6 21.4
72.8 4.0
75.4 1.9
75.9 2.1
77.8 0.5
77.8 0.5
ATIS-A
2.7B
10.8 0.0
14.0 0.0
29.8 12.8
33.1 9.4
43.0 26.2
47.3 21.3
55.6 5.0
58.8 4.0
13B
49.5 0.0
52.7 0.0
69.6 17.4
71.8 17.1
67.5 10.4
69.6 13.4
63.4 4.6
64.5 4.0
ATIS-D
2.7B
6.4 0.0
12.9 0.0
42.3 28.8
65.6 20.8
75.0 6.7
83.4 4.2
81.0 8.8
88.3 3.7
13B
4.0 0.0
5.0 0.0
97.9 0.6
95.5 4.6
98.0 0.6
97.8 0.7
98.8 0.3
98.8 0.3
Table 1. Contextual calibration improves accuracy across a range of tasks. We show the mean and standard deviation across different
choices of the training examples (the prompt format is ﬁxed). The LM column indicates the GPT-3 size (see Appendix A for GPT-2
results). The Baseline column shows the standard approach of greedy decoding (Brown et al., 2020) and Ours corresponds to greedy
decoding after modifying the output probabilities using contextual calibration. We bold the better result of the baseline and ours. MIT-G,
MIT-D, ATIS-A, and ATIS-D indicate the MIT Genre, MIT Director, ATIS Airline, and ATIS Departure Date datasets.
5.1. Results for Contextual Calibration
Here, we evaluate the effectiveness of contextual calibra-
tion across all of our datasets and LMs. We ﬁrst use a
ﬁxed prompt format and select ﬁve different random sets
of training examples, placing them in an arbitrary order in
the prompt. We do not artiﬁcially balance the labels of the
training examples for the classiﬁcation tasks. We use the
same sets of training examples for the baseline (standard de-
coding without calibration) and contextual calibration. We
use labeling budgets of 0–8 examples; using more than 8-
shots causes the cost of querying the OpenAI API to become
prohibitively expensive.
Table 1 shows the results and Figure 1 in Section 1 plots the
same data for a subset of the tasks.
Improves Mean And Worst-Case Accuracy Contextual
calibration dramatically improves GPT-3’s average and
worst-case accuracy, by up to 30.0% absolute. These gains
hold for both classiﬁcation and generation tasks. Contextual
calibration also sometimes allows GPT-3 2.7B to outper-
form the GPT-3 175B baseline—by up to 19.3%—despite
being over 50x smaller.
Can Reduce Variance Across Training Sets Figure 6
plots the difference in the standard deviation between the
baseline and contextual calibration for all tasks from Table 1.
Contextual calibration reduces the variance considerably in
a majority of cases, and it does not increase variance by
much in the remaining cases.
Reduces Drop from 0-shot to 1-shot For the baseline,
there are four cases where there is a drop in accuracy when

Calibrate Before Use: Improving Few-Shot Performance of Language Models
15
10
5
0
5
Std Dev of Contextual Calibration  Baseline
Figure 6. Aside from improving mean accuracy, contextual cal-
ibration also reduces the standard deviation of accuracy across
different choices of the training examples. We plot the differ-
ence in standard deviation between contextual calibration and the
baseline from Table 1.
moving from 0-shot to 1-shot (TREC, AGNews, DBpedia,
SST-2). We attribute this drop to the majority label bias (see
discussion in Section 4). Calibration removes this drop in
three out of four cases.
Improves GPT-2 We also test GPT-2 1.5B (see Table 4 in
Appendix A). We ﬁnd that like GPT-3, GPT-2’s accuracy
also highly varies across different prompts. This suggests
that the variance that we observe for few-shot in-context
learning is a general problem for LMs. Second, contextual
calibration works out-of-the-box for GPT-2—it improves
the mean accuracy and reduces variance for most tasks.
Improves Accuracy Across Formats In our next set of ex-
periments, we use a ﬁxed set of training examples and vary
the prompt format. We use the 15 prompt formats for SST-2
discussed in Section 3. We also create 15 prompt formats
for each of three random relations in LAMA (P20, P159,
P19) by using the paraphrases of the original LAMA tem-
plates generated by Jiang et al. (2020b). Figure 7 shows the
results before and after calibration for SST-2, and Figure 9
in Appendix A show the results for LAMA. Contextual cali-
bration improves the average and worst-case accuracy for
both datasets, and reduces the variance for SST-2.
5.2. Ablations on Contextual Calibration
We ﬁnally conduct two analyses/ablations on contextual
calibration. We ﬁrst analyze how effective contextual cal-
ibration is at inferring a good setting of W. To do so, we
compare its accuracy to an “oracle calibration” method that
uses the validation set to ﬁnd the best possible diagonal W.
We evaluate this oracle on AGNews, and ﬁnd that contextual
calibration is surprisingly close to it (Figure 8).
We also study how the choice of content-free input affects
accuracy. In Table 3 in Appendix A, we show the accu-
racy for SST-2 and AGNews for different choices of the
content-free input. The choice of content-free input matters,
however, many good choices exist.
0
1
4
8
Number of Training Examples
40
50
60
70
80
90
SST-2 Accuracy (%)
Accuracy Over Diff. Formats
GPT-3 2.7B
With Calibration
Figure 7. GPT-3 has high variance across different prompt formats;
contextual calibration reduces this variance and improves mean
accuracy. We show the mean accuracy (± standard deviation) over
15 different prompt formats for SST-2.
6. Discussion
Does Calibration Eliminate the Need to Engineer
Prompts? The motivation behind “prompt engineering”
is that not all prompts lead to the same accuracy. Thus, one
should tune the prompt’s format and examples to achieve the
best possible performance (Brown et al., 2020; Gao et al.,
2020). Contextual calibration does not eliminate the need to
engineer prompts, however, it does mitigate it: contextual
calibration makes the accuracy of the best, average, and
worst-case prompts more similar (and higher).
Should You Finetune in the Few-shot Setting? We use a
ﬁxed LM with no ﬁnetuning. As mentioned in Section 1,
there are numerous reasons not to ﬁnetune: it enables rapid
prototyping, provides a fully natural language interface, and
is more efﬁcient in terms of memory requirements and sys-
tem complexity when serving many different tasks. More-
over, like in-context learning without contextual calibration,
ﬁnetuning can be unstable in the few-shot setting (Schick
& Sch¨utze, 2021). Nevertheless, if these disadvantages are
acceptable or avoidable, ﬁnetuning can improve accuracy
over in-context learning in some cases (Schick & Sch¨utze,
2020; Gao et al., 2020). An interesting direction for future
work is to study the interplay between contextual calibration
and ﬁnetuning, e.g., does contextual calibration alleviate the
need to ﬁnetune, or vice versa?
7. Related Work
Few-shot Learning with Language Models Recent work
uses LMs to solve NLP tasks, e.g., for story cloze pre-
diction (Schwartz et al., 2017), knowledge base comple-
tion (Petroni et al., 2019), and Winograd schemas (Trinh &
Le, 2018). Radford et al. (2019) and Brown et al. (2020)

Calibrate Before Use: Improving Few-Shot Performance of Language Models
0 1
4
8
16
Number of Training Examples
40
50
60
70
80
AGNews Accuracy (%)
Accuracy Over Diff. Training Sets
Oracle Calibration
Contextual Calibration
Uncalibrated Baseline
Figure 8. Contextual calibration, despite using no training data,
achieves similar accuracy to an “oracle” calibration that ﬁnds the
best W using the validation set. The plot shows GPT-3 175B’s
mean accuracy (± standard deviation) on AGNews over different
choices of the training examples.
show that large LMs can be used to solve a myriad of tasks
in a few-shot manner via in-context learning. Our paper
provides a simple modiﬁcation to their setting that improves
performance. Asking LMs to complete natural language
prompts is also used as a method to “probe” LMs, e.g., ana-
lyzing their factual (Petroni et al., 2019; Jiang et al., 2020b;
Shin et al., 2020) or commonsense knowledge (Bosselut
et al., 2019). Our results suggest that these probing methods
may underestimate model accuracy, and we recommend that
future work take advantage of contextual calibration.
Volatility of Few-shot Learning in NLP Recent work
shows that when using masked language models such as
BERT for zero-shot learning, the prompt format can impact
accuracy (Petroni et al., 2019; Jiang et al., 2020b; Shin et al.,
2020). Independent and concurrent work also shows that
when ﬁnetuning masked language models on few examples,
the choice of training examples can impact results (Schick
& Sch¨utze, 2020; Gao et al., 2020). We show that similar
instabilities occur for in-context learning (i.e., no ﬁnetuning)
with left-to-right language models. We also show a surpris-
ing instability associated with example ordering. Moreover,
unlike past work, we analyze why these instabilities occur,
and we use insights from this analysis to mitigate the issues.
Failures of Language Models We identify failures when
LMs are used for in-context learning (e.g., recency bias).
Past work identiﬁes similar failures when LMs are used
for text generation. For example, neural LMs often repeat
themselves (Holtzman et al., 2020), suffer from overconﬁ-
dence (Braverman et al., 2020; Jiang et al., 2020a), suffer
from recency bias (Khandelwal et al., 2018; Ravfogel et al.,
2019), and prefer generic responses instead of rare text (Li
et al., 2016; Logan et al., 2019). Past work mitigates these
degeneracies by modifying the model’s output probabilities
or generation schemes, e.g., explicitly preventing repeti-
tions (Paulus et al., 2018) or using sampling instead of
greedy decoding (Holtzman et al., 2020).
8. Conclusion and Future Work
We show that few-shot learning can be highly volatile across
different choices of the prompt. Through a detailed analysis,
we identify that this volatility arises from biases in LMs, e.g.,
their tendency to output recent or common tokens. We use
these insights to develop contextual calibration—a simple
procedure to adjust the model’s output probabilities—which
improves accuracy, reduces variance, and overall makes
tools like GPT-3 more effective for end users.
Looking at the bigger picture, our results inspire two future
research directions in few-shot learning for NLP. First, on
the methods side, we show that good few-shot learning re-
quires attention to detail: small but non-trivial decisions
such as calibration can greatly inﬂuence results. This makes
it difﬁcult to correctly develop and compare new methods
(e.g., pretraining schemes or model architectures). We thus
hope to make other few-shot learning methods more robust,
and also expand our techniques to cover a wider ranger of
tasks (e.g., calibration for open-ended generation). Second,
on the analysis side, our results highlight the need to under-
stand what GPT-3 learns from the prompt. The model has an
impressive ability to improve with more training examples,
however, we show that the model learns some superﬁcial
patterns such as repetition of common answers. We hope to
better understand and analyze the dynamics of in-context
learning in future work.
Acknowledgements
We thank OpenAI for providing academic access to the GPT-
3 API. We thank Sewon Min, Nikhil Kandpal, Nelson Liu,
Girish Sastry, Marco Tulio Ribeiro, and the members of
Berkeley NLP for valuable feedback on the paper.
This work was supported by DARPA under the LwLL pro-
gram/Grant No. FA8750-19-1-0504, DARPA MCS program
under Contract No. N660011924033 with the United States
Ofﬁce Of Naval Research, DARPA and the Air Force Re-
search Laboratory (AFRL), and NSF award #IIS-1756023.
References
Bosselut, A., Rashkin, H., Sap, M., Malaviya, C., Celiky-
ilmaz, A., and Choi, Y. COMET: Commonsense trans-
formers for automatic knowledge graph construction. In
ACL, 2019.
Braverman, M., Chen, X., Kakade, S., Narasimhan, K.,

Calibrate Before Use: Improving Few-Shot Performance of Language Models
Zhang, C., and Zhang, Y. Calibration, entropy rates, and
memory in language models. In ICML, 2020.
Brier, G. W. Veriﬁcation of forecasts expressed in terms of
probability. Monthly Weather Review, 1950.
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,
J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,
Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,
J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,
Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,
S., Radford, A., Sutskever, I., and Amodei, D. Language
models are few-shot learners. In NeurIPS, 2020.
Dagan, I., Glickman, O., and Magnini, B. The PASCAL
recognising textual entailment challenge. In Machine
Learning Challenges Workshop, 2005.
de Marneffe, M.-C., Simons, M., and Tonhauser, J. The
CommitmentBank: Investigating projection in naturally
occurring discourse. In Sinn und Bedeutung, 2019.
Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT:
Pre-training of deep bidirectional transformers for lan-
guage understanding. In NAACL, 2019.
Gao, T., Fisch, A., and Chen, D. Making pre-trained lan-
guage models better few-shot learners. arXiv preprint
arXiv:2012.15723, 2020.
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. On
calibration of modern neural networks. In ICML, 2017.
Hemphill, C. T., Godfrey, J. J., and Doddington, G. R. The
ATIS spoken language systems pilot corpus. In Speech
and Natural Language Workshop, 1990.
Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y.
The curious case of neural text degeneration. In ICLR,
2020.
Jiang, Z., Araki, J., Ding, H., and Neubig, G. How can
we know when language models know? arXiv preprint
arXiv:2012.00955, 2020a.
Jiang, Z., Xu, F. F., Araki, J., and Neubig, G. How can we
know what language models know? In TACL, 2020b.
Khandelwal, U., He, H., Qi, P., and Jurafsky, D. Sharp
nearby, fuzzy far away: How neural language models use
context. In ACL, 2018.
Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B.
Human-level concept learning through probabilistic pro-
gram induction. In Science, 2015.
Li, J., Galley, M., Brockett, C., Gao, J., and Dolan, B. A
diversity-promoting objective function for neural conver-
sation models. In NAACL, 2016.
Liu, J., Cyphers, S., Pasupat, P., McGraw, I., and Glass, J. A
conversational movie search system based on conditional
random ﬁelds. In INTERSPEECH, 2012.
Logan, R. L., Liu, N. F., Peters, M. E., Gardner, M., and
Singh, S. Barack’s wife Hillary: Using knowledge-graphs
for fact-aware language modeling. In ACL, 2019.
Paulus, R., Xiong, C., and Socher, R. A deep reinforced
model for abstractive summarization. In ICLR, 2018.
Petroni, F., Rockt¨aschel, T., Lewis, P., Bakhtin, A., Wu,
Y., Miller, A. H., and Riedel, S. Language models as
knowledge bases? In EMNLP, 2019.
Platt, J. C. Probabilistic outputs for support vector machines
and comparisons to regularized likelihood methods. In
Advances in Large Margin Classiﬁers, 1999.
Radford, A., Narasimhan, K., Salimans, T., and Sutskever,
I. Improving language understanding by generative pre-
training. Technical Report, 2018.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and
Sutskever, I. Language models are unsupervised multitask
learners. Technical Report, 2019.
Ravfogel, S., Goldberg, Y., and Linzen, T. Studying the
inductive biases of RNNs with synthetic variations of
natural languages. In NAACL, 2019.
Schick, T. and Sch¨utze, H. It’s not just size that matters:
Small language models are also few-shot learners. arXiv
preprint arXiv:2009.07118, 2020.
Schick, T. and Sch¨utze, H. Exploiting cloze questions for
few-shot text classiﬁcation and natural language infer-
ence. In EACL, 2021.
Schwartz, R., Sap, M., Konstas, I., Zilles, L., Choi, Y., and
Smith, N. A. The effect of different writing tasks on
linguistic style: A case study of the ROC story cloze task.
In ACL, 2017.
Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., and
Singh, S. AutoPrompt: Eliciting knowledge from lan-
guage models with automatically generated prompts. In
EMNLP, 2020.
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning,
C. D., Ng, A., and Potts, C. Recursive deep models for
semantic compositionality over a sentiment treebank. In
EMNLP, 2013.
Trinh, T. H. and Le, Q. V. A simple method for common-
sense reasoning. arXiv preprint arXiv:1806.02847, 2018.
Voorhees, E. M. and Tice, D. M. Building a question an-
swering test collection. In SIGIR, 2000.

Calibrate Before Use: Improving Few-Shot Performance of Language Models
Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A.,
Michael, J., Hill, F., Levy, O., and Bowman, S.
Su-
perGLUE: A stickier benchmark for general-purpose lan-
guage understanding systems. In NeurIPS, 2019.
Yogatama, D., d’Autume, C. d. M., Connor, J., Kocisky,
T., Chrzanowski, M., Kong, L., Lazaridou, A., Ling, W.,
Yu, L., Dyer, C., et al. Learning and evaluating general
linguistic intelligence. arXiv preprint arXiv:1901.11373,
2019.
Zhang, X., Zhao, J., and LeCun, Y. Character-level con-
volutional networks for text classiﬁcation. In NeurIPS,
2015.

Calibrate Before Use: Improving Few-Shot Performance of Language Models
A. Additional Results on Variance and
Calibration
Table 2 shows an example of the sensitivity to ordering.
Prompt (test input not shown)
Acc.
Review: the whole thing ’s fairly lame , making it par for
the course for disney sequels .
Answer: Negative
Review: this quiet , introspective and entertaining indepen-
dent is worth seeking .
Answer: Positive
88.5%
Review: this quiet , introspective and entertaining indepen-
dent is worth seeking .
Answer: Positive
Review: the whole thing ’s fairly lame , making it par for
the course for disney sequels .
Answer: Negative
51.3%
Table 2. Top: a prompt consisting of two training examples (the test
input is not shown) that leads to good test accuracy for GPT-3 2.7B
(88.5%). Bottom: simply reversing the order of the two examples
causes the accuracy to drop to near random chance (51.3%).
Table 3 demonstrates that the choice of content-free input
does affect accuracy, however, many good choices exist.
Content-free Input
SST-2
AGNews
Uncalibrated Baseline
66.5
48.5
N/A
74.2
64.5
[MASK]
74.5
63.8
‘’
72.9
64.7
N/A, [MASK], ‘’
79.0
66.5
the
69.1
59.0
abc
77.5
57.3
the man.
79.4
62.0
dasjhasjkdhjskdhds
79.3
64.5
nfjkhdvy84tr9bpuirvwe
78.4
65.5
Table 3. We show the accuracy for 1-shot SST-2 and 0-shot AG-
News over different choices for the content-free input. The choice
of content-free input matters, however, many good choices exist.
The token ‘’ indicates the empty string. Recall that in our experi-
ments, we ensemble over N/A, [MASK], and the empty string.
Figure 9 shows how GPT-3 accuracy changes as the prompt
format is varied for LAMA, with and without calibration.
Table 4 shows the effect of calibration for GPT-2.
B. Prompt Formats Used
Tables 5 and 6 show the default prompt format used for all
tasks. Table 7 shows the 15 different formats used when
studying the effect of prompt format for SST-2.

Calibrate Before Use: Improving Few-Shot Performance of Language Models
0
1
4
8
Number of Training Examples
0
10
20
30
40
LAMA Accuracy (%)
Accuracy Over Diff. Formats (P20)
GPT-3 2.7B
With Calibration
0
1
4
8
Number of Training Examples
10
20
30
LAMA Accuracy (%)
Accuracy Over Diff. Formats (P159)
GPT-3 2.7B
With Calibration
0
1
4
8
Number of Training Examples
0
10
20
LAMA Accuracy (%)
Accuracy Over Diff. Formats (P19)
GPT-3 2.7B
With Calibration
Figure 9. Contextual calibration improves GPT-3’s accuracy across various prompt formats for LAMA. We plot GPT-2 2.7B’s mean
accuracy over 15 different formats for the LAMA “place of death” relation (P20), “Headquarter Location” relation (P159), and “place of
birth” relation (P19).
Dataset
LM
0-shot
1-shot
4-shot
8-shot
Baseline
Ours
Baseline
Ours
Baseline
Ours
Baseline
Ours
Text Classiﬁcation
AGNews
GPT-2
44.0 0.0
60.0 0.0
45.4 8.4
67.9 5.7
44.6 12.2
58.0 13.6
57.1 11.6
63.1 7.3
TREC
GPT-2
24.0 0.0
37.3 0.0
21.5 5.2
41.1 2.6
23.1 5.9
44.2 2.2
32.7 7.5
44.1 3.6
CB
GPT-2
44.6 0.0
17.9 0.0
49.6 10.0
47.1 12.2
40.0 8.3
55.4 7.3
48.9 5.7
63.2 1.4
RTE
GPT-2
51.0 0.0
48.5 0.0
57.6 2.1
56.3 2.4
53.2 6.0
57.5 1.8
54.9 3.0
57.7 1.29
SST-2
GPT-2
60.0 0.0
82.0 0.0
66.7 17.9
73.0 11.4
64.9 8.4
73.8 10.9
54.5 4.6
64.6 8.8
DBPedia
GPT-2
64.3 0.0
58.3 0.0
33.6 18.9
69.5 9.4
53.0 14.8
75.3 8.1
66.0 3.6
74.3 8.7
Fact Retrieval
LAMA
GPT-2
14.0 0.0
22.7 0.0
29.7 1.8
31.6 1.3
35.8 3.8
37.4 3.4
42.5 1.3
42.5 1.4
Information Extraction
MIT-G
GPT-2
7.7 0.0
10.0 0.0
32.9 10.0
41.2 4.1
44.3 6.5
47.7 5.8
56.9 2.5
59.5 2.5
MIT-D
GPT-2
29.3 0.0
41.7 0.0
26.2 10.5
58.8 4.8
70.5 2.5
75.4 1.8
77.1 4.4
78.1 3.9
ATIS-A
GPT-2
15.1 0.0
35.5 0.0
41.5 11.7
51.4 7.5
55.1 18.9
65.8 11.7
63.4 10.6
69.9 10.4
ATIS-D
GPT-2
1.0 0.0
2.5 0.0
62.3 9.2
68.7 4.3
81.1 3.6
83.2 7.2
81.8 4.5
83.9 5.0
Table 4. Contextual calibration improves accuracy for GPT-2. This table is analogous to Table 1 but shows results for GPT-2 XL.

Calibrate Before Use: Improving Few-Shot Performance of Language Models
Task
Prompt
Label Names
SST-2
Review: This movie is amazing!
Sentiment: Positive
Review: Horriﬁc movie, don’t see it.
Sentiment:
Positive, Negative
AGNews
Article: USATODAY.com - Retail sales bounced back a bit in July, and new claims for
jobless beneﬁts fell last week, the government said Thursday, indicating the economy is
improving from a midsummer slump.
Answer: Business
Article: New hard-drive based devices feature color screens, support for WMP 10.
Answer:
World, Sports, Business, Technology
TREC
Classify the questions based on whether their answer type is a Number, Location, Person,
Description, Entity, or Abbreviation.
Question: How did serfdom develop in and then leave Russia?
Answer Type: Description
Question: When was Ozzy Osbourne born?
Answer Type:
Number, Location, Person, Description,
Entity, Abbreviation
DBPedia
Classify the documents based on whether they are about a Company, School, Artist, Ath-
lete, Politician, Transportation, Building, Nature, Village, Animal, Plant, Album, Film,
or Book.
Article: Geoffrey D. Falksen (born July 31 1982) is an American steampunk writer.
Answer: Artist
Article: The Perrin River is a 1.3-mile-long (2.1 km) tidal river in the U.S. state of Vir-
ginia. It is a small inlet on the north shore of the York River near that river’s mouth at
Chesapeake Bay.
Answer:
Company, School, Artist, Athlete, Politi-
cian, Transportation, Building, Nature,
Village, Animal, Plant, Album, Film,
Book
CB
But he ended up eating it himself. I was reluctant to kiss my mother, afraid that somehow
her weakness and unhappiness would infect me. Naturally I didn’t think for a minute that
my life and spirit could stimulate her.
question: her life and spirit could stimulate her mother. True, False, or Neither?
answer: Neither
Valence the void-brain, Valence the virtuous valet. Why couldn’t the ﬁgger choose his
own portion of titanic anatomy to shaft? Did he think he was helping?
question: Valence was helping. True, False, or Neither?
answer:
True, False, Neither
RTE
Others argue that Mr. Sharon should have negotiated the Gaza pullout - both to obtain
at least some written promises of better Palestinian behavior, and to provide Mr. Abbas
with a prime prize to show his people that diplomacy, not violence, delivered Gaza.
question: Mr. Abbas is a member of the Palestinian family. True or False?
answer: False
The program will include Falla’s ”Night in the Gardens of Spain,” Ravel’s Piano Concerto
in G, Berlioz’s Overture to ”Beatrice and Benedict,” and Roy Harris’ Symphony No. 3.
question: Beatrice and Benedict is an overture by Berlioz. True or False?
answer:
True, False
Table 5. The prompts used for text classiﬁcation. We show one training example per task for illustration purposes. The right column
shows the label names (to make predictions, we check the LM’s probability for these tokens).

Calibrate Before Use: Improving Few-Shot Performance of Language Models
Task
Prompt
LAMA
Alexander Berntsson was born in Sweden
Khalid Karami was born in
ATIS
(Airline)
Sentence: what are the two american airlines ﬂights that leave from dallas to san francisco in the evening
Airline name: american airlines
Sentence: list a ﬂight on american airlines from toronto to san diego
Airline name:
ATIS
(Depart Date)
Sentence: please list any ﬂight available leaving oakland california tuesday arriving philadelphia wednesday
Depart date - Day name: tuesday
Sentence: show me all all ﬂights from pittsburgh to atlanta on wednesday which leave before noon and serve
breakfast
Depart date - Day name:
MIT Movies
(Genre)
Sentence: last to a famous series of animated movies about a big green ogre and his donkey and cat friends
Genre: animated
Sentence: what is a great comedy featuring the talents of steve carell as a loser looking for a friend
Genre:
MIT Movies
(Director)
Sentence: in 2005 director christopher nolan rebooted a legendary dc comics superhero with a darker grittier edge
in which movie
Director: christopher nolan
Sentence: what 1967 mike nichols ﬁlm features dustin hoffman in romantic interludes with anne bancroft as mrs
robinson
Director:
Table 6. The prompts used for generation tasks. We show one training example per task for illustration purposes.

Calibrate Before Use: Improving Few-Shot Performance of Language Models
Format ID Prompt
Label Names
1
Review: This movie is amazing!
Answer: Positive
Review: Horriﬁc movie, don’t see it.
Answer:
Positive, Negative
2
Review: This movie is amazing!
Answer: good
Review: Horriﬁc movie, don’t see it.
Answer:
good, bad
3
My review for last night’s ﬁlm: This movie is amazing! The critics agreed that this movie was good
My review for last night’s ﬁlm: Horriﬁc movie, don’t see it. The critics agreed that this movie was
good, bad
4
Here is what our critics think for this month’s ﬁlms.
One of our critics wrote ”This movie is amazing!”. Her sentiment towards the ﬁlm was positive.
One of our critics wrote ”Horriﬁc movie, don’t see it”. Her sentiment towards the ﬁlm was
positive, negative
5
Critical reception [ edit ]
In a contemporary review, Roger Ebert wrote ”This movie is amazing!”. Entertainment Weekly agreed, and
the overall critical reception of the ﬁlm was good.
In a contemporary review, Roger Ebert wrote ”Horriﬁc movie, don’t see it”. Entertainment Weekly agreed, and
the overall critical reception of the ﬁlm was
good, bad
6
Review: This movie is amazing!
Positive Review? Yes
Review: Horriﬁc movie, don’t see it.
Positive Review?
Yes, No
7
Review: This movie is amazing!
Question: Is the sentiment of the above review Positive or Negative?
Answer: Positive
Review: This movie is amazing!
Question: Is the sentiment of the above review Positive or Negative?
Answer:
Positive, Negative
8
Review: This movie is amazing!
Question: Did the author think that the movie was good or bad?
Answer: good
Review: This movie is amazing!
Question: Did the author think that the movie was good or bad?
Answer:
good, bad
9
Question: Did the author of the following tweet think that the movie was good or bad?
Tweet: This movie is amazing!
Answer: good
Question: Did the author of the following tweet think that the movie was good or bad?
Tweet: Horriﬁc movie, don’t see it
Answer:
good, bad
10
This movie is amazing! My overall feeling was that the movie was good
Horriﬁc movie, don’t see it. My overall feeling was that the movie was
good, bad
11
This movie is amazing! I liked the movie.
Horriﬁc movie, don’t see it. I
liked, hated
12
This movie is amazing! My friend asked me if I would give the movie 0 or 5 stars, I said 5
Horriﬁc movie, don’t see it. My friend asked me if I would give the movie 0 or 5 stars, I said
0, 5
13
Input: This movie is amazing!
Sentiment: Positive
Input: Horriﬁc movie, don’t see it.
Sentiment:
Positive, Negative
14
Review: This movie is amazing!
Positive: True
Review: Horriﬁc movie, don’t see it.
Positive:
True, False
15
Review: This movie is amazing!
Stars: 5
Review: Horriﬁc movie, don’t see it.
Stars:
5, 0
Table 7. The different prompt formats used when studying the effect of format for SST-2. We show one training example for illustration.

