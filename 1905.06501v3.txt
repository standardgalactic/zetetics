The Kernel Interaction Trick: Fast Bayesian
Discovery of Pairwise Interactions in High
Dimensions
Raj Agrawal
CSAIL
Massachusetts Institute of Technology
r.agrawal@csail.mit.edu
Jonathan H. Huggins
Department of Biostatistics
Harvard University
jhuggins@mit.edu
Brian Trippe
CSAIL
Massachusetts Institute of Technology
btrippe@mit.edu
Tamara Broderick
CSAIL
Massachusetts Institute of Technology
tbroderick@csail.mit.edu
Abstract
Discovering interaction effects on a response of interest is a fundamental problem
faced in biology, medicine, economics, and many other scientiﬁc disciplines. In
theory, Bayesian methods for discovering pairwise interactions enjoy many beneﬁts
such as coherent uncertainty quantiﬁcation, the ability to incorporate background
knowledge, and desirable shrinkage properties. In practice, however, Bayesian
methods are often computationally intractable for even moderate-dimensional
problems. Our key insight is that many hierarchical models of practical interest
admit a particular Gaussian process (GP) representation; the GP allows us to
capture the posterior with a vector of O(p) kernel hyper-parameters rather than
O(p2) interactions and main effects. With the implicit representation, we can
run Markov chain Monte Carlo (MCMC) over model hyper-parameters in time
and memory linear in p per iteration. We focus on sparsity-inducing models
and show on datasets with a variety of covariate behaviors that our method: (1)
reduces runtime by orders of magnitude over naive applications of MCMC, (2)
provides lower Type I and Type II error relative to state-of-the-art LASSO-based
approaches, and (3) offers improved computational scaling in high dimensions
relative to existing Bayesian and LASSO-based approaches.
1
Introduction
Many decision-making and scientiﬁc tasks require understanding how a set of covariates relate to a
target response. For example, in clinical trials and precision medicine, researchers seek to characterize
how individual-level traits impact treatment effects, and in modern genomic studies, researchers seek
to identify genetic variants that are risk factors for particular diseases. While linear regression is a
default method for these tasks and many others due to its ease of interpretability, its simplicity often
comes at the cost of failing to learn more nuanced information from the data. A common way to
increase ﬂexibility, while still retaining the interpretability of linear regression, is to augment the
covariate space. For instance, two genes together might be highly associated with a disease even
though individually they exhibit only moderate association; thus, an analyst might want to consider
the multiplicative effect of pairs of covariates co-occurring.
Unfortunately, augmenting the covariate space by including all possible pairwise interactions means
the number of parameters to analyze grows quadratically with the number of covariates p. This
arXiv:1905.06501v3  [stat.CO]  13 Nov 2022

growth leads to many statistical and computational difﬁculties that are only made worse in the
high-dimensional setting, where p is much larger than the number of observations N. And p ≫N
is often exactly the case of interest in genomic and medical applications. To address the statistical
challenges, practitioners often enforce a sparsity constraint on the model, reﬂecting an assumption
that only a small subset of all covariates affect the response. The problem of identifying this subset
is a central problem in high-dimensional statistics and many different LASSO-based approaches
have been proposed to return sparse point estimates. However, these methods do not address how to
construct valid conﬁdence intervals or adjust for multiple comparisons1 (Bien et al., 2013; Lim &
Hastie, 2015; Wu et al., 2009; Nakagawa et al., 2016; Shah, 2016).
Fortunately, hierarchical Bayesian methods have a shrinkage effect, naturally handle multiplicity,
can provide better statistical power than multiple comparison corrections Gelman et al. (2012),
and can leverage background knowledge. However, naive approaches to Bayesian inference are
computationally intractable for even moderate-dimensional problems. This intractability has two
sources. The ﬁrst source can be seen even in the simple case of conjugate linear regression with
a multivariate Gaussian prior. Let ˜X denote the augmented data matrix including all pairwise
interactions, Σ the multivariate Gaussian prior covariance on parameters, and σ2 the noise variance.
Given N observations, computing the posterior requires inverting Σ−1 +
1
σ2 ˜XT ˜X, which takes
O(p2N 2 + N 3) time. The second source is that reporting on O(p2) parameters simply has O(p2)
cost.
We propose to speed up inference in Bayesian linear regression with pairwise interactions by address-
ing both problems. In the ﬁrst case, we show how to represent the original model using a Gaussian
process (GP). We use the GP kernel in our kernel interaction sampler to take advantage of the special
structure of interactions and avoid explicitly computing or inverting Σ−1 +
1
σ2 ˜XT ˜X. In the second
case, we develop a kernel interaction trick to compute posterior summaries exactly for main effects
and interactions between selected main effects to avoid the full O(p2) reporting cost. In sum, we
show that we can recover posterior means and variances of regression coefﬁcients in O(pN 2 + N 3)
time, a p-fold speed-up. We demonstrate the utility and efﬁcacy of our general-purpose computational
tools for the sparse kernel interaction model (SKIM), which we propose in Section 6 for identifying
sparse interactions. In Section 7 we empirically show (1) improved Type I and Type II error relative to
state-of-the-art LASSO-based approaches and (2) improved computational scaling in high dimensions
relative to existing Bayesian and LASSO-based approaches. Our methods extend naturally beyond
pairwise interactions to higher-order multi-way interactions, as detailed in Appendix A.
2
Preliminaries and Related Work
Suppose we observe data D = {(x(n), y(n))}N
n=1 with covariates x(n) ∈Rp and responses y(n) ∈R.
Let X ∈RN×p denote the design matrix and Y ∈RN denote the vector of responses. Linear
models assume that each y(n) is a (noisy) linear function of the covariates x(n). A common strategy
to increase the expressivity of linear models is to augment the original covariates x(n) with their
pairwise interactions
ΦT
2 (x) := [1, x1, · · · , xp, x1x2, · · · , xp−1xp, x2
1, · · · , x2
p].
That is, for a parameter θ ∈Rp(p+1)/2 and zero-mean i.i.d. errors ϵ(n), we assume the data are
generated according to
y(n) = θT Φ2(x(n)) + ϵ(n).
(1)
Our goal is to identify which interaction terms have a signiﬁcant effect on the response. Detecting
such interactions is important for many applications. For example, in genomics, two-way interaction
terms are needed to detect possible epistasis between genes (Aschard, 2016; Slim et al., 2018) and to
appropriately account for the site- and sample-speciﬁc effects of GC content on genomic and other
types of sequencing data (Benjamini & Speed, 2012; Risso et al., 2011). In economics and clinical
trials, pairwise interactions between covariates and treatment are used to estimate the heterogeneous
effect a treatment has across different subgroups (Lipkovich et al., 2017, Section 6). Unfortunately,
having O(p2) parameters creates statistical and computational challenges when p is large.
1While the knockoff ﬁlter introduced in Barber & Cand`es (2015) is a promising way to control the false
discovery rate, such a method has not been evaluated theoretically or empirically for interaction models.
2

To address the statistical issues, practitioners often assume that θ is sparse (i.e., contains only a
few non-zero values), and that θ satisﬁes strong hierarchy. That is, an interaction effect θxixj
is present only if both of the main effects θxi and θxj are present, where θxixj and θxi are the
regression coefﬁcients of the variables xixj and xi respectively (Bien et al., 2013; Lim & Hastie,
2015; Wu et al., 2009; Nakagawa et al., 2016; Chipman, 1996). By assuming such low-dimensional
structure, inference tasks such as parameter estimation and variable selection become more tractable
statistically. However, sparsity constraints create computational difﬁculties. For example, ﬁnding the
maximum-likelihood estimator (MLE) subject to ∥θ∥0 ≤s requires searching over Θ(p2s) active
parameter subsets. To avoid the combinatorial issues resulting from an L0 penalty, recent works (Bien
et al., 2013; Lim & Hastie, 2015) have instead used L1 penalties to encourage parameter sparsity
for interaction models; L1 penalties have a long history in high-dimensional linear regression (Chen
et al., 1998; Candes & Tao, 2007; Tibshirani, 1994),
Maximizing the likelihood with an added L1 penalty is a convex problem. But each iteration of a
state-of-the-art solver for methods given by Bien et al. (2013) and Lim & Hastie (2015) still takes
O(Np2) time. To handle larger p, Wu et al. (2009); Nakagawa et al. (2016); Shah (2016) have
proposed various pruning heuristics for ﬁnding locally optimal solutions. However, since these
methods do not provide an exact solution to the optimization problem, any statistical guarantees (such
as the statistical rate at which these estimators converge to the true parameter as a function of N and
p) are weaker than those for exact methods.
L1-based methods face a number of additional challenges: constructing valid conﬁdence intervals,
incorporating background knowledge, and controlling for the issue of multiple comparisons when
testing many parameters for statistical signiﬁcance. In many applications such as genome-wide
association studies, controlling for multiplicity is critical to prevent wasting resources on false
discoveries. Moreover, since dim(Φ2) = p(p + 1)/2, θ can be very high dimensional even when p is
moderately large. Hence, there will typically be nontrivial uncertainty when attempting to estimate
θ. Fortunately, hierarchical Bayesian methods have (1) a natural shrinkage or regularization effect
such that multiple testing corrections are no longer necessary, (2) better statistical power than using
multiple comparison correction terms such as Bonferroni (Gelman et al., 2012), and (3) naturally
provide calibrated uncertainties. Bayesian methods can also incorporate expert information.
Though they offer desirable statistical properties, Bayesian approaches are computationally expensive.
Previous efforts (Grifﬁn & Brown, 2017; Chipman, 1996) have focused on developing hierarchical
sparsity priors that promote strong hierarchy, analogous to the LASSO-based approaches (Bien et al.,
2013; Lim & Hastie, 2015; Wu et al., 2009; Nakagawa et al., 2016). But these methods do not address
the computational intractability of inference for even moderate-dimensional problems.
We address the computational challenges of inference by developing the kernel interaction trick
(Section 5), which allows us to access posterior marginals of θ without ever representing θ explicitly.
Note that while some previous works have used a degree-two polynomial kernel to implicitly generate
all pairwise interactions (Morota & Gianola, 2014; Weissbrod et al., 2016; Su et al., 2012), those
works have focused on prediction or estimating the cumulative proportion of variance explained by
interactions rather than our present focus on posterior inference.
3
Bayesian Models with Interactions
Our goal is to estimate and provide uncertainties for the parameter θ ∈Rdim(Φ2). To take a Bayesian
approach, we encode the state of knowledge before observing the data D in a prior π0(θ). We
express the likelihood as L(Y | θ, X) = QN
n=1 L(y(n) | θ, x(n)). Applying Bayes’ theorem yields
the posterior distribution π(θ | D) ∝L(Y | θ, X)π0(θ), which describes the state of knowledge
about θ after observing the data D. For a function f of interest, we wish to compute the posterior
expectation
Eπ(θ|D)[f(θ)] =
Z
f(θ)π(θ | D)dθ.
(2)
Typically, f(θ) = θj or f(θ) = θ2
j, which together allow us to compute the posterior mean and
variance of each θj.
Generative model. Going forward, we model θ as being drawn from a Gaussian scale mixture prior
to encode desirable properties such as sparsity and strong hierarchy (cf. Grifﬁn & Brown, 2017;
3

Table 1: Per-iteration MCMC runtime and memory scaling of methods for sampling two-way
interactions. NAIVE refers to explicitly factorizing ΣN,τ to compute p(D | τ, σ2), WOODBURY
refers to using the Woodbury identity and matrix determinant lemma to compute p(D | τ, σ2), and
FULL refers to jointly sampling θ and τ. The third column provides the number of parameters
sampled.
METHOD
TIME
MEMORY
#
OUR METHOD
O(pN 2 + N 3)
O(pN + N 2)
O(p)
NAIVE
O(p6 + p4N)
O(p4 + p2N)
O(p)
WOODBURY
O(p2N 2 + N 3)
O(p2N + N 2)
O(p)
FULL
O(p2N)
O(p2N)
Θ(p2)
Chipman, 1996; George & McCulloch, 1993; Carvalho et al., 2009; Piironen & Vehtari, 2017). These
priors have also been used beyond sparse Bayesian regression (cf. Hamdan et al., 2005; Wainwright
& Simoncelli, 1999; Choy & Chan, 2003). A Gaussian scale mixture is equivalent to assuming that
there exists an auxiliary random variable τ ∼p(τ) such that θ is conditionally Gaussian given τ.
Let Στ denote the covariance matrix for p(θ | τ). Also, let σ2 be the latent noise variance in the
likelihood; since it is typically unknown, we treat it as random and put a prior on it as well. Hence,
the full generative model can be written
τ ∼p(τ)
σ2 ∼p(σ2)
θ | τ ∼N(0, Στ)
y(n) | x(n), θ, σ2 ∼N(θT Φ2(x(n)), σ2).
(3)
Computational challenges of inference. Again, our main goal is to tractably compute expectations
of functions under the posterior π(θ | D) ∝L(Y | θ, X)π0(θ). Since there are Θ(p2) parameter
components, direct numerical integration over each of these components is feasible only when p is at
most 3 or 4. As a result we turn to Monte Carlo integration. Two natural Monte Carlo estimators one
might use to approximate Eπ(θ|D)[f(θ)] are
1.
1
T
PT
t=1 f(θ(t)) with θ(t) iid∼π(θ | D) or
2.
1
T
PT
t=1 Eπ(θ|D,τ (t))[f(θ)] with τ (t) iid∼p(τ | D).
For the ﬁrst estimator, we can use Markov chain Monte Carlo (MCMC) techniques to sample each
θ(t) approximately independently from π(θ | D) since the posterior is available up to a multiplicative
normalizing constant. Computing the prior p(θ), however, may be analytically intractable because it
requires marginalizing out τ. We could instead additionally sample τ. To use gradient-based MCMC
samplers, sampling τ would require computing the pdfs (and gradients) of the likelihood terms
L(y(n) | x(n), θ, σ2) and the prior terms p(θ | τ) and p(τ). So the cost would be O(p2N + dim(τ))
time per iteration. Even for p moderately large, the Θ(p2 + dim(τ)) number of parameters might
require many MCMC iterations to properly explore such a large space (MacKay, 1998; Pillai et al.,
2012; Beskos et al., 2013); see also Fig. 2 for an empirical demonstration.
To explore a smaller space, and hence potentially reduce the number of MCMC iterations required
for the chains to mix, we might take the second approach: sampling from p(τ | D) by marginalizing
out the high-dimensional parameter θ. Sampling each τ requires computing
p(D | τ, σ2) =
Z
p(D | θ, σ2)dp(θ | τ).
(4)
Since p(θ | τ) is a multivariate Gaussian density function, p(D | τ, σ2) equals
(1/
√
2πσ2)N det(2πΣN,τ)
1
2 exp
 −1
2σ2 Y T Y

det(2πΣτ)
1
2 exp
 −1
2σ4 Y T Φ2(X)ΣN,τΦ2(X)T Y

(5)
4

where Σ−1
N,τ := Σ−1
τ + 1
σ2 Φ2(X)T Φ2(X). Unfortunately, computing Eq. (5) naively takes prohibitive
O(p6 + p4N) time – or O(p2N 2 + N 3) time when using linear algebra identities; see Table 1 and
Appendix F for details.
4
The Kernel Interaction Sampler
Our kernel interaction sampler (KIS) provides a recipe for efﬁciently sampling from p(τ, σ2 | D)
using MCMC. Recall from the last section that the computational bottleneck for sampling τ was
computing p(D | τ, σ2), so we focus on that problem here. We achieve large computational gains
(Table 1) by using the special model structure and a kernel trick to avoid the factorization of ΣN,τ in
Eq. (5). To that end, KIS has three main parts: (1) we re-parameterize the generative model given in
Eq. (3) using a Gaussian process (GP); (2) we show how to cheaply compute the GP kernel; and (3)
we show how these steps translate into computation of p(D | τ, σ2) in time linear in p. In Appendix A
we extend to the case of higher-order interactions.
For the moment, suppose that we could construct a covariance function kτ such that the generative
model in Eq. (3) could be re-parameterized as:
τ ∼p(τ)
g | τ ∼GP(0, kτ)
σ2 ∼p(σ2)
y(n) | g, x(n), σ2 ∼N(g(x(n)), σ2),
(6)
where GP(0, kτ) denotes a Gaussian process (GP) with mean function zero. Deﬁning the kernel
matrix (Kτ)ij := kτ(x(i), x(j)), we can conclude that (see Rasmussen & Williams, 2006, Eq. 2.30)
log p(D | τ, σ2) = −1
2Y T L−1Y −1
2 log |L| −N
2 log 2π,
(7)
where L equals Kτ + σ2IN. Let Tk denote the time it takes to evaluate kτ on a pair of points. The
computational bottleneck of Eq. (7) is computing and factorizing Kτ, which take O(N 2Tk) and
O(N 3) time, respectively. Hence, as long as Tk is O(p), we can compute p(D | τ, σ2) in time linear
in p. To achieve this scaling, we ﬁrst show (in the next result) that any generative model in the form
of Eq. (3) can be rewritten in the form of Eq. (6). We then show how kτ can be evaluated in O(p)
time for the models of interest.
Proposition 4.1. (Gaussian process representation) Let Y and ˜Y be response vectors generated
according to the models in Eq. (3) and Eq. (6) respectively for design matrix X ∈RN×p. Let
kτ(x(i), x(j)) = Φ2(x(i))⊤ΣτΦ2(x(j)). Then, Y | X
d= ˜Y | X, where
d= denotes equality in
distribution. Moreover, for every draw g | τ ∼N(0, kτ), there exists some θ ∈Rdim(Φ2) such that
g(·) = θT Φ2(·).
The proof follows directly by considering the weight-space view of a GP (Rasmussen & Williams,
2006, Chapter 2); see Appendix B for details.
Next, we need to show that kτ can be evaluated in O(p) time for models of interest. This fact
is not obvious; computing kτ on a pair of points naively still requires explicitly computing the
high-dimensional feature maps Φ2 and prior covariance matrix Στ. To compute kτ efﬁciently, we
rewrite it as a weighted sum of polynomial kernels of the form
kc
poly,d(x, ˜x) :=

xT ˜x + c
d
,
which each take O(p) time to compute. Below we deﬁne two-way interaction kernels as particular lin-
ear combinations of these polynomial kernels. Then we provide a result motivating this class; namely,
we show that any diagonal Στ prior can be written as a two-way interaction kernel. Fortunately, to
the best of our knowledge, all previous high-dimensional Bayesian regression models assume Στ is
diagonal (cf. Grifﬁn & Brown, 2017; Chipman, 1996; George & McCulloch, 1993; Carvalho et al.,
2009; Piironen & Vehtari, 2017). Hence, this restriction on Στ is mild.
5

Deﬁnition 4.2. (Two-way interaction kernel) We call the kernel k a two-way interaction kernel if for
some choice of M1, M2 ∈N, α, ψ, λ(m) ∈Rp
+ (m = 1, . . . , M1), ν(m) ∈R+ (m = 1, . . . , M2),
1 ≤im < jm ≤p (m = 1, . . . , M2), and A ∈R, the kernel k(x, ˜x) is equal to
M1
X
m=1
k1
poly,2(λ(m) ⊙x, λ(m) ⊙˜x) +
M2
X
m=1
ν(m)ximxjm ˜xim ˜xjm
+ kA
poly,1(α ⊙x, α ⊙˜x) + k0
poly,1(ψ ⊙x ⊙x, ψ ⊙˜x ⊙˜x),
where ⊙is the entrywise product.
Theorem 4.3 (1-to-1 correspondence with diagonal Στ). Suppose k is a two-way interaction kernel.
Then
k(x, ˜x) = Φ2(x)⊤SΦ2(˜x),
(8)
where the induced prior covariance matrix S is diagonal. The entries of S are given by
diag(S)(i) = α2
i + 2
M1
X
m=1
h
λ(m)
i
i2
diag(S)(ij) = 2
M1
X
m=1
h
λ(m)
i
λ(m)
j
i2
+
M2
X
k:ik=i,jk=j
ν(m)
diag(S)(ii) = ψ2
i +
M1
X
m=1
h
λ(m)
i
i4
diag(S)(0) = M1 + A,
where diag(S)(i), diag(S)(ij), diag(S)(ii), and diag(S)(0) denote the prior variances of the main
effect θxi, interaction effect θxixj, quadratic effect θx2
i , and intercept θ0, respectively. Furthermore,
for any diagonal covariance matrix S ∈Rdim(Φ2)×dim(Φ2), there exists a two-way interaction kernel
that induces S as a prior covariance matrix.
Theorem 4.3 (proof in Appendix B.2) and Proposition 4.1 imply that two-way interaction kernels
induce a space of models in 1-to-1 correspondence with models in the form of Eq. (3) when Στ is
constrained to be diagonal. Since most models of practical interest have Στ diagonal, we can readily
construct the two-way interaction kernel corresponding to Στ by solving the system of equations
diag(S)(i) = diag(Στ)(i)
diag(S)(ij) = diag(Στ)(ij)
diag(S)(ii) = diag(Στ)(ii) diag(S)(0) = diag(Στ)(0)
(9)
Each of the M1 + 2 polynomial kernels takes O(p) time to compute, and each of the M2 product
terms takes O(1) time. Therefore, we want to select M1 and M2 small so that kτ can be computed
quickly. Since there are more degrees of freedom (i.e., free variables) available to solve Eq. (9) as M1
and M2 increase, eventually a solution will exist as we show in Appendix B.2. But Theorem 4.3 does
not tell us how large M1 and M2 have to be for an arbitrary model. In Appendix C, we solve Eq. (9)
for a variety of models of practical interest and show that in these cases, M1 and M2 can be set very
small (between one and three). Thus kτ can be computed in O(p) time, and so the kernel matrix
Kτ can be computed in O(N 2p) time. Finally, then, we may compute the likelihood p(D | τ, σ2) in
O(N 2p + N 3) time.
5
The Kernel Interaction Trick: Recovering Posterior Marginals
Even if we are able to sample τ much faster using KIS, the problem of computing Ep(θ|D,τ)[f(θ)]
remains unresolved. In this section, we show that, given Kτ, any such expectation can be recovered
in O(1) time by evaluating the GP posterior at certain test points.
To provide the main intuition for our solution, suppose we would like to compute the posterior mean
of the main effect θxi. Let ei ∈Rp denote the ith unit vector. Since g = θT Φ2 by Proposition 4.1,
6

we have
g(ei) = θxi + θx2
i
g(−ei) = −θxi + θx2
i
g(ei) −g(−ei)
2
= θxi.
(10)
Since g is a Gaussian process, the distribution of Zg := (g(ei), g(−ei)) | D, τ is multivariate
Gaussian and can be computed in closed form by appropriate matrix multiplications of the kernel
matrix Kτ; see Theorem 5.1 below for details. Then, by consulting Eq. (10), one can recover
θxi | D, τ as the linear combination [1/2, −1/2]T Zg | D, τ, which is univariate Gaussian. While we
have focused on a particular instance here, this example provides the main insight for the general
formula to compute Ep(θ|D,τ)[f(θ)] from Kτ.
Theorem 5.1. (The kernel interaction trick) Let Hτ := (Kτ + σ2IN)−1 and
Aij := [ei, −ei, ej, ei + ej]T ∈R4×p.
Let Kτ(Aij, X) = Kτ(X, Aij)T be the 4 × N matrix formed by taking the kernel between each row
of Aij with each row of X. For a row-vector a ∈R4, deﬁne the scalars µa := aKτ(Aij, X)HτY
and
σ2
a := a

Kτ(Aij, Aij) −Kτ(Aij, X)HτKτ(X, Aij)

aT .
Then the distributions of θxi | τ, D, θxixj | τ, D, and θx2
i | τ, D are given by N(µa, σ2
a) with,
respectively, a = (1/2, −1/2, 0, 0), a = (−1/2, 1/2, −1, 1), and a = (1/2, 1/2, 0, 0).
Corollary 5.2. Given Kτ, the distributions of θxi, θxixj, and θx2
i take O(1) additional time and
memory to compute.
We prove Theorem 5.1 and Corollary 5.2 in Appendix B. In Appendix B.5, we generalize Theorem 5.1
by showing how to obtain the joint posterior distribution of any subset of parameters contained in θ.
Hence, we can compute Ep(θ|D,τ)[f(θ)] for an arbitrary f using the kernel interaction trick.
Note that if we would like to obtain the posterior mean of all Θ(p2) parameters, then clearly a linear
time algorithm in p is impossible. Instead, we can adopt a lazy evaluation strategy where we compute
the posterior of one of the Θ(p2) parameters only when it is needed. This approach is effective in
the many applications where we do not need to look at all the interactions. In particular, we might
ﬁrst ﬁnd the top k main effects. After selecting these variables, we could examine their interactions.
The number of interactions among the main effects (which is Θ(k2)) is much smaller than the total
number of possible interactions (which is Θ(p2)) if k ≪p. Such a strategy is natural if we believe
that θ satisﬁes the (commonly assumed) strong hierarchy restriction.
6
SKIM: Sparse Kernel Interaction Model
To demonstrate the utility and efﬁcacy of the kernel interaction sampler and kernel interaction trick,
we choose a particular model that we call the sparse kernel interaction model (SKIM). In what
follows, we ﬁrst detail SKIM, which we will see promotes sparsity and strong hierarchy. Then, by
observing that SKIM is a special case of the general model in Eq. (3), we can show that SKIM
induces a two-way interaction kernel via Theorem 4.3 and Eq. (9). We will see that this kernel has
only 3 components and thus takes only O(p) time to evaluate. By Corollary B.3, we can compute the
distribution of interaction terms from SKIM in O(1) time once we have computed the kernel matrix.
Hence, the ﬁnal computation time for discovering main effects and interaction effects with SKIM
will be O(N 2p + N 3) by the discussion at the end of Section 5.
SKIM2 is given in full detail, together with discussion of hyperparameter selection and intepretation,
in Appendix D.1. It is a particular instance of a general class of hierarchical sparsity priors (cf. Grifﬁn
2See https://github.com/agrawalraj/skim for the code.
7

(a) Runtime complexity
(b) Memory complexity
Figure 1:
Empirical evaluation of (a) time and (b) memory scaling with dimension of marginal
likelihood computation. Woodbury and Naive refer to the baselines in Section 3.
& Brown, 2017; Chipman, 1996; George & McCulloch, 1993) that have the following form:
κ ∼p(κ)
η ∼p(η)
c2 ∼p(c2)
θxi | κ, η ∼N(0, η2
1κ2
i )
θxixj | κ, η ∼N(0, η2
2κ2
i κ2
j)
θx2
i | κ, η ∼N(0, η2
3κ4
i )
θ0 | c2 ∼N(0, c2),
(11)
where θ0 is the intercept term and every ηi or κj is a scalar.
We next show that any prior in the form of Eq. (11) induces a O(p) two-way interaction kernel. The
proof is in Appendix B.6.
Proposition 6.1. Taking τ := (η, κ, c2), the generative model in Eq. (11) is equivalent to using the
following kernel in Eq. (6):
kτ(x, ˜x) = η2
2
2 k1
poly,2(κ ⊙x, κ ⊙˜x)
(η2
3 −η2
2
2 )k0
poly,1(κ ⊙x ⊙x, κ ⊙˜x ⊙˜x)
+

η2
1 −η2
2

k0
poly,1(κ ⊙x, κ ⊙˜x) + c2 −η2
2
2 .
7
Experiments
Time and memory cost versus Bayesian baselines. We ﬁrst assess the computational advantages
of our kernel interaction sampler (KIS) by comparing it with each baseline Bayesian method in
Table 1. We start by proﬁling the time and memory cost of computing p(D | τ, σ2), which we
have seen is a computational bottleneck for sampler option 2 in Section 3. In Fig. 1, we depict the
time and memory cost of p(D | τ, σ2) computation for conjugate linear regression with an isotropic
Gaussian prior on synthetic datasets with N = 50. We vary p but not N because we are interested
primarily in the high-dimensional case when p is large relative to N. Fig. 1 shows that KIS yields
orders-of-magnitude speed and memory improvements over the baseline methods for computing
p(D | τ, σ2).
We next compare inference for SKIM using KIS, which marginalizes out θ and samples τ, to jointly
sampling (θ, τ) (denoted FULL).3 We implemented KIS and FULL in Stan Carpenter et al. (2019)
and used the NUTS algorithm Hoffman & Gelman (2014) for sampling (4 chains with 1,000 iterations
3See the discussion of sampler option 1 in Section 3.
8

(a) NUTS runtimes
(b) LASSO runtime comparisons
Figure 2: The left-hand ﬁgure indicates the time to complete four parallel chains of 1000 iterations of
NUTS for the SKIM model proposed in Section 6 using KIS (denoted as SKIM-KIS) and FULL. For
each point, KIS had ˆR < 1.05 while FULL always had ˆR > 1.05. The right-hand ﬁgure compares
the runtime of inference for SKIM-KIS versus ﬁtting LASSO-based methods.
per chain). As shown in Fig. 2(a), KIS is orders of magnitude faster even for lower values of p. In
Section 3 we remarked that since FULL explores a much higher-dimensional space, there might be
issues with mixing. To explore this possibility empirically, we check the Gelman–Rubin statistic ( ˆR)
values of the output from both KIS and FULL. We found that, for FULL, the ˆR values were greater
than 1.05, with some reaching as high as 1.5 (indicating poor mixing), while for KIS all ˆR values
were less than 1.05 (suggesting good mixing).
Comparison to LASSO: synthetic data. Having demonstrated the considerable computational
savings over baseline Bayesian approaches, we next demonstrate the advantage of our method over
frequentist approaches such as the LASSO. In particular, we consider the common case when the
true high-dimensional parameter θ is assumed to be sparse and satisﬁes the requirement of strong
hierarchy. To the best of our knowledge, there has not been an extensive empirical comparison
between sparse Bayesian interaction models and sparse frequentist interaction models. The likely
reason is that each MCMC iteration for sampling τ takes O(N 2p2 + N 3) time using the Woodbury
matrix method. The per-iteration cost of the iterative optimization solver for the LASSO and the
hierarchical LASSO, on the other hand, is O(Np2), which is much faster when N is even moderately
large. Fortunately, SKIM admits a cheap-to-compute kernel function such that each MCMC iteration
takes O(N 2p + N 3) time, which is faster than the LASSO-style approaches in cases when p is large
relative to N.
We benchmark SKIM against generating all pairwise interactions and running the LASSO (denoted
pairs LASSO) and the hierarchical LASSO Lim & Hastie (2015), which constrains the ﬁtted parame-
ters to satisfy strong hierarchy. We generate 36 different synthetic datasets, which differ in the number
of observations, dimension, and signal-to-noise ratio. The covariates X are drawn from N(0, λ2Ip)
for different choices of λ. Here, λ controls the signal-to-noise ratio; when λ is larger, the signal
is stronger. We consider N ∈{50, 100, 200} observations, p ∈{50, 100, 200, 500} dimensions
(which translates into between roughly 1.25 × 103 and 1.25 × 105 total interaction parameters), and
λ ∈{1, 2, 5}. In each dataset, we select ﬁve variables (and their pairwise interactions) to affect y,
and we allow the rest of the variables to lead to spurious correlations with the response y. We set
the magnitudes of all non-zero effects to 1. Finally, y | x, θ∗∼N(0, σ2), where the noise variance
σ2 equals the largest λ2 value, namely 25, to mimic the realistic case when the noise variance is
large relative to the signal. We compare each method in terms of variable selection quality and
mean-squared error (MSE) between the ﬁtted and true parameter. For variable selection, we select
a parameter only if the posterior mean of that parameter is farther than 2.59 times its (average)
posterior standard deviation from zero; see Appendix E for details. For the hierarchical LASSO
and pairs LASSO, the variables selected are those with non-zero coefﬁcients, and we use 5-fold
cross-validation to ﬁnd the strength of the L1 penalties. We ﬁt the hierarchical LASSO using the
9

(a) Main effects
(b) Main effects differences
(c) Pairwise effects
(d) Pairwise effects differences
Figure 3: Variable selection performance of each method for the 36 synthetic datasets. Each point in
each plot indicates one of these datasets for a particular method. The green regions in the second and
last plot indicate where our method in strictly better than the other two in terms of variable selection,
while the red region indicates the datasets for which our method is strictly worse. In the ﬁrst and last
ﬁgures, better performance occurs when moving right and/or down.
glinternet package in R and pairs LASSO using sklearn in python. We implemented KIS is
Stan (4 chains with 1,000 iterations each). The ˆR values for each dataset were less than 1.05.
First, we examine how well each method selects main effects and pairwise effects. Each point in
Fig. 3(a) shows the number of main effects selected and number of incorrect main effects selected for
a given synthetic dataset. In this plot, it is clear that our method has better false discovery rate (FDR)
control over the other two methods on average. Fig. 3(c) shows the FDR performance for pairwise
effects. To compare the methods at the dataset level, in Fig. 3(b,d) we consider the difference in the
number of correct and incorrect main effects selected by our method and the LASSO methods for
each dataset. The green shaded regions indicate the datasets for which our method simultaneously
selects more correct main effects and has fewer incorrect main effects, i.e., is strictly better than the
other two methods for any variable selection metric. Finally, in Fig. 4 we look at the difference in
MSE to θ∗, broken down in terms of the error for estimating main and pairwise effects. Again, we
see for the great majority of the datasets, KIS outperforms the LASSO based approaches. In Fig. 2(b)
we see that SKIM-KIS has competitive runtimes relative to pairs LASSO and hierarchical LASSO.
Comparison to LASSO: synthetic data, real covariates. To understand the impact of the geometry
of the covariates on performance, we took the Residential Building Data Set from the UCI Machine
10

(a) MSE difference (main)
(b) MSE difference (pairwise)
Figure 4: Each red cross denotes the difference in MSE of the hierarchical LASSO and KIS from
the true main effects (left) and pairwise effects (right) for a given synthetic dataset. When the MSE
difference is larger than 0 (i.e., the green shaded region), then our method is closer to the true effect
sizes in terms of Euclidean distance. Similarly, each blue x equals the difference in MSE of all-pairs
LASSO and our method.
Table 2: Building dataset results. MAIN (PAIR) MSE refers to total error in estimating main
(pairwise) effects. The main and pairwise MSE added together yield the total MSE. The second and
fourth columns show (# of effects correctly selected) : (# of incorrect effects selected) for main and
pairwise effects, respectively. Larger green values are better while larger purple values are worse.
METHOD
MAIN MSE
# MAIN
PAIR MSE
# PAIR
SKIM
0.1
3 : 0
7.0
3 : 0
PLASSO
5.0
2 : 5
9.3
3 : 21
HLASSO
1.5
3 : 19
7.8
3 : 18
Learning Repository and simulated responses as in our previous synthetic experimental setup. In
particular, we randomly chose 5 variables and their 10 pairwise interactions to have non-zero effects.
In this case, the covariates are highly correlated (the ﬁrst 20 out of 105 principal components capture
over 99% of the variance in the data). In Table 2, we see that SKIM signiﬁcantly outperforms the
LASSO-based methods for recovering main and pairwise effects.
Comparison to LASSO: cars miles per gallon dataset. We conclude by comparing the methods
on the Auto MPG dataset, from the UCI Machine Learning Repository, which contains N = 398
samples and p = 8 variables. We consider only the 6 numerical variables (cylinders, displacement,
horsepower, weight, acceleration, model year) and standardize the data by subtracting the mean and
dividing by the standard deviation. To compare the methods, we ﬁrst ﬁt SKIM and the LASSO-based
methods (via 5-fold cross-validation) on these 6 features. Our method selects three main effects
(weight, horsepower, acceleration) and one interaction (weight × horsepower). The hierarchical
LASSO selects all six main effects and 8 out of the 15 possible pairwise interactions. Pairs LASSO
selects 5 main effects and 8 interactions.
Since there is no ground truth, and all of the main and pairwise interactions could a priori affect
miles per gallon, it is difﬁcult to compare the methods. To better assess the methods, we instead
append random noise covariates and reﬁt each model. In particular, we draw additional covariates
from a N(0, Im), for m = 100, 200 and add these noise variables to the original 6 features. The total
number of main and pairwise regression coefﬁcients grows to 5,671 and 21,321 for m = 100, 200
respectively, making the regression task very high-dimensional. The results are summarized in
Table 3. All methods are able to pick up some main effects and pairwise effects from the original
dataset. Beyond that observation, we cannot compare which main and interaction effects from the
original data are real. However, we do know that all noise effects are fake. We see that even with
11

Table 3: Auto MPG dataset results. Each column represents the (# of original effects selected) : (# of
fake effects selected). A selected main (pairwise) effect is an “original” effect if it corresponds to
one of the original 6 features (15 interactions). Main100 (Pairwise100) and Main200 (Pairwise200)
denote when 100 and 200 random noise covariates are added to the original 6 features, respectively.
Larger purple values are worse. Higher green values are not necessarily better since there are no
ground truth interactions.
METHOD
MAIN100
MAIN200
PAIR100
PAIR200
SKIM
3 : 0
3 : 0
1 : 0
1 : 0
PLASSO
4 : 1
4 : 0
4 : 99
2 : 78
HLASSO
5 : 4
6 : 46
5 : 2
4 : 38
more noise directions, our method selects the same main effects and pairwise effects as the noiseless
covariate case; that is, it does not pick up any fake effects. The two LASSO-based methods, on the
other hand, incorrectly select many noise variables as interactions.
Conclusion. Through our kernel interaction sampler we have demonstrated that Bayesian interaction
models can offer both competitive computational scaling relative to LASSO-based methods and
improved Type I and II error rates. While our method runs in time linear in p per iteration, the cubic
dependence on N still makes inference computationally challenging. Fortunately, there is a wide GP
literature that deals precisely with reducing this cubic timing dependence through inducing points
(Titsias, 2009; Qui˜nonero Candela & Rasmussen, 2005) or novel conjugate-gradient techniques
(Gardner et al., 2018). An interesting future direction will be to empirically and theoretically
understand the statistical penalty of using these inducing point methods to scale SKIM to the setting
of both large N and large p.
12

Acknowledgements
This research is supported in part by an NSF CAREER Award, an ARO YIP Award, DARPA, a Sloan
Research Fellowship, and ONR. BLT is supported by NSF GRFP.
References
Aschard, H. A perspective on interaction effects in genetic association studies. Genetic Epidemiology,
2016.
Barber, R. F. and Cand`es, E. J. Controlling the false discovery rate via knockoffs. The Annals of
Statistics, 43(5):2055–2085, 10 2015.
Benjamini, Y. and Speed, T. P. Summarizing and correcting the GC content bias in high-throughput
sequencing. Nucleic Acids Research, 40(10), 2012.
Beskos, A., Pillai, N., Roberts, G., Sanz-Serna, J., and Stuart, A. Optimal tuning of the hybrid Monte
Carlo algorithm. Bernoulli, 19(5A):1501–1534, 11 2013.
Bien, J., Taylor, J., and Tibshirani, R. A Lasso for hierarchical interactions. The Annals of Statistics,
41(3):1111–1141, 2013.
Candes, E. and Tao, T. The Dantzig selector: Statistical estimation when p is much larger than n. The
Annals of Statistics, pp. 2313–2351, 2007.
Carpenter, B., Lee, D., Brubaker, M. A., Riddell, A., Gelman, A., Goodrich, B., Guo, J., Hoffman,
M., Betancourt, M., and Li, P. Stan: A probabilistic programming language, 2019.
Carvalho, C., Polson, N., and Scott, J. Handling sparsity via the horseshoe. In International
Conference on Artiﬁcial Intelligence and Statistics, 2009.
Chen, S., Donoho, D., and Saunders, M. Atomic decomposition by basis pursuit. SIAM Journal on
Scientiﬁc Computing, pp. 33–61, 1998.
Chipman, H. Bayesian variable selection with related predictors. The Canadian Journal of Statistics,
24(1):17–36, 1996.
Choy, S. and Chan, C. Scale mixtures distributions in insurance applications. Journal of the IAA, 33:
93–104, 2003.
Gardner, J., Pleiss, G., Weinberger, K. Q., Bindel, D., and Wilson, A. G. GPyTorch: Blackbox matrix-
matrix Gaussian process inference with GPU acceleration. In Advances in Neural Information
Processing Systems. 2018.
Gelman, A., Hill, J., and Yajimam, M. Why we (usually) don’t have to worry about multiple
comparisons. Journal of Research on Educational Effectiveness, pp. 189–211, 2012.
George, E. and McCulloch, R. Variable selection via Gibbs sampling. Journal of the American
Statistical Association, 88(423):881–889, 1993.
Grifﬁn, J. and Brown, P. Hierarchical shrinkage priors for regression models. Bayesian Analysis, 12:
135–159, 2017.
Hamdan, H., Nolan, J., Wilson, M., and Dardia, K. Using scale mixtures of normals to model
continuously compounded returns. Journal of Modern Applied Statistical Methods, 2005.
Hoffman, M. and Gelman, A. The No-U-turn sampler: Adaptively setting path lengths in Hamiltonian
Monte Carlo. Journal of Machine Learning Research, 15(1):1593–1623, 2014.
Lim, M. and Hastie, T. Learning interactions via hierarchical group-lasso regularization. Journal of
Computational and Graphical Statistics, 24(3):627–654, 2015.
Lipkovich, I., Dmitrienko, A., and D’Agostino, R. Tutorial in biostatistics: data-driven subgroup
identiﬁcation and analysis in clinical trials. Statistics in Medicine, 36:136–196, 2017.
13

MacKay, D. J. C. Introduction to Monte Carlo Methods, pp. 175–204. Springer Netherlands, 1998.
Morota, G. and Gianola, D. Kernel-based whole-genome prediction of complex traits: a review.
Frontiers in Genetics, 5:363, 2014.
Nakagawa, K., Suzumura, S., Karasuyama, M., Tsuda, K., and Takeuchi, I. Safe pattern pruning:
An efﬁcient approach for predictive pattern mining. In International Conference on Knowledge
Discovery and Data Mining, 2016.
Piironen, J. and Vehtari, A. Sparsity information and regularization in the horseshoe and other
shrinkage priors. Electronic Journal of Statistics, 11:5018–5051, 2017.
Pillai, N. S., Stuart, A. M., and Thi´ery, A. H. Optimal scaling and diffusion limits for the Langevin
algorithm in high dimensions. The Annals of Applied Probability, 22(6):2320–2356, 12 2012.
Qui˜nonero Candela, J. and Rasmussen, C. E. A unifying view of sparse approximate Gaussian
process regression. Journal of Machine Learning Research, 6:1939–1959, 2005.
Rasmussen, C. E. and Williams, C. K. I. Gaussian Processes for Machine Learning. The MIT Press,
2006.
Risso, D., Schwartz, K., Sherlock, G., and Dudoit, S. GC-content normalization for RNA-seq data.
BMC Bioinformatics, 12(1):480, Dec 2011.
Shah, R. Modelling interactions in high-dimensional data with backtracking. Journal of Machine
Learning Research, 17(207):1–31, 2016.
Slim, L., Chatelain, C., Azencott, C., and Vert, J. Novel methods for epistasis detection in genome-
wide association studies. bioRxiv:325993, 2018.
Su, G., Christensen, O. F., Ostersen, T., Henryon, M., and Lund, M. S. Estimating additive and
non-additive genetic variances and predicting genetic merits using genome-wide dense single
nucleotide polymorphism markers. PLOS ONE, 7(9):1–7, 09 2012.
Tibshirani, R. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical
Society, Series B, pp. 267–288, 1994.
Titsias, M. K. Variational learning of inducing variables in sparse Gaussian processes. In International
Conference on Artiﬁcial Intelligence and Statistics, pp. 567–574, 2009.
Wainwright, M. and Simoncelli, E. Scale mixtures of Gaussians and the statistics of natural images.
In International Conference on Neural Information Processing Systems, 1999.
Weissbrod, O., Geiger, D., and Rosset, S. Multikernel linear mixed models for complex phenotype
prediction. Genome Research, 2016.
Wu, T., Chen, Y., Hastie, T., Sobel, E., and Lange, K. Genome-wide association analysis by Lasso
penalized logistic regression. Bioinformatics, 25(6):714–721, 2009.
14

A
Modeling Multi-Way Interactions
In certain applications, we might expect that there are interactions of order greater than two. For
example, suppose we are trying to predict college admissions. Then, we might expect a three-way
interaction between a candidate’s SAT score, GPA, and extracurricular involvement. Individually,
these variables might only exhibit moderate association but together they could have a multiplicative
effect. For example, we might expect that candidates who have high SAT scores, high GPAs, and
excellent extracurricular activities will be accepted with near certainty, while candidates who only
possess one/two of these qualities are borderline applicants.
We now show how to extend our results to handle such three-way, or more generally, r-way interac-
tions.
Deﬁnition A.1. (r-way interactions) The r-way interactions of a covariate vector x ∈Rp are
generated from the feature map
Φr(x) :=
r
M
d=1
M
k:k1+···+kp=d
p
Y
j=1
xkj
j ,
k ∈Np,
where Lm
j=1 aj := (a11, · · · , a1k1, · · · , am1, · · · , amkm) denotes the concatenation of vectors aj ∈
Rkj.
To model r-way interactions, we must use degree r polynomial kernels to generate all the necessary
interactions. Hence, we recommend using the following generalized two-way interaction kernel,
which we call the r-way interaction kernel.
Deﬁnition A.2. (r-way interaction kernel) A kernel k is called an r-way interaction kernel if for some
choice of M1, M2, M3 ∈N, α, ψ, λ(m) ∈Rp
+ (m = 1, . . . , M1), ν(m) ∈R+ (m = 1, . . . , M2),
and ∇(m) ∈Rp
+ (k = 1, . . . , M3) it can be re-expressed as
M1
X
m=1
k1
poly,r(λ(m) ⊙x, λ(m) ⊙y) +
M2
X
m=1
ν(m)


r
Y
s=1
xism
r
Y
s=1
yism

+
M3
X
m=1
kr−1(∇(m) ⊙x, ∇(m) ⊙y),
where ⊙is the Hadamard product and kr−1 is an r −1 degree interaction kernel. The base case
kernel (i.e., when r = 2) is provided in Deﬁnition 4.2.
To select the weights for an r-way interaction kernel, we must solve a system of equations similar to
Eq. (9), except for a target prior covariance matrix Στ ∈Rdim(Φr)×dim(Φr).
B
Proofs
B.1
Proof of Proposition 4.1
Let g(·) = θT Φ2(·) and θ | τ ∼N(0, Στ). Then, y(n) = g(x(n)) + ϵ(n). The ﬁrst claim follows by
taking φ = Φ2 and f = g in Rasmussen & Williams (2006, Equation 2.12).
The second claim follows directly from the duality between the weight-space and function-space
view of a GP (Rasmussen & Williams, 2006, Chapter 2).
B.2
Proof of Theorem 4.3
The proof of Theorem 4.3 depends critically on Lemma B.1 below, which characterizes the relation
between adding two kernels and the resulting induced prior covariance matrix.
Lemma B.1. Let k1 and k2 be two kernels such that there exists vectors a(1), a(2) ∈Rdim(Φ2) for
which ki(x, y) = ⟨a(i) ⊙Φ2(x), a(i) ⊙Φ2(y)⟩. Let k3(x, y) = k1(x, y) + k2(x, y). Then,
k3(x, y) = ⟨Σ
1
2
3 Φ2(x), Σ
1
2
3 Φ2(y)⟩
s.t.
Σ3 = diag(a(1) ⊙a(1) + a(2) ⊙a(2)).
(12)
15

Proof. By the sum property of kernels,
k1(x, y) + k2(x, y) = ⟨[a1 a2] ⊙[Φ2(x) Φ2(x)], [a1 a2] ⊙[Φ2(y) Φ2(y)]⟩
= ⟨a(1) ⊙Φ2(x), a(1) ⊙Φ2(y)⟩+ ⟨a(2) ⊙Φ2(x), a(2) ⊙Φ2(y)⟩
= ⟨a(1) ⊙a(1) ⊙Φ2(x), Φ2(y)⟩+ ⟨a(2) ⊙a(2) ⊙Φ2(x), Φ2(y)⟩
= ⟨a(1) ⊙a(1) ⊙Φ2(x) + a(2) ⊙a(2) ⊙Φ2(x), Φ2(y)⟩
= ⟨(a(1) ⊙a(1) + a(2) ⊙a(2)) ⊙Φ2(x), Φ2(y)⟩
= ΦT
2 (x) diag((a(1) ⊙a(1) + a(2) ⊙a(2)) Φ2(y)
= k3(x, y).
(13)
By Lemma B.1, it sufﬁces to write out the feature map of each kernel in Deﬁnition 4.2. The induced
feature maps of each respective kernel term in Deﬁnition 4.2 are given by ai ⊙Φ2(x), 1 ≤i ≤4 for
a1 := ((λ(m)
1
)2, · · · , (λ(m)
p
)2,
√
2λ(m)
1
λ(m)
2
, · · · ,
√
2λ(m)
p−1λ(m)
p
,
√
2λ(m)
1
, · · · ,
√
2λ(m)
p
, 1)
a2 := (0, · · · , 0, 0, · · · , 0, α1, · · · , αp,
√
A)
a3 := (ψ1, · · · , ψp, 0, · · · , 0, 0, · · · , 0, 0)
a4 := (0, · · · , 0, 0, · · · , 0,
p
ν(m), 0, · · · , 0, 0, · · · , 0, 0)
(14)
The ﬁrst claim follows from Eq. (14) and Lemma B.1.
To prove the second claim, take an arbitrary diagonal prior covariance matrix S ∈Rdim(Φ2)×dim(Φ2).
It sufﬁces to show that there exists a solution of,
diag(S)(i) = α2
i + 2
M1
X
m=1
h
λ(m)
i
i2
diag(S)(ij) = 2
M1
X
m=1
h
λ(m)
i
λ(m)
j
i2
+
K2
X
m:im=i,jm=j
ν(m)
diag(S)(ii) = ψ2
i +
M1
X
m=1
h
λ(m)
i
i4
diag(S)(0) = M2 + A.
for some choice of M1, M2 ∈N, α, ψ, λ(m) ∈Rp
+ (m = 1, . . . , M1), ν(m) ∈R+ (m = 1, . . . , M2),
and A ∈R. Take α2
i = diag(S)(i) and ψ2
i = diag(S)(ii), for i = 1, · · · , p. Take λ(m) = 0.
Let M2 = p(p−1)
2
and ν(1) = diag(S)(12), · · · , ν(M2) = diag(S)((p−1)p). Finally, letting A =
diag(S)(0) −M2 solves the system.
Remark. While we have shown one of the many ways to solve the above system for an arbitrary S,
the strategy taken above is not practically useful; computing the kernel in this fashion will take Θ(p2)
time because M2 = Θ(p2). In practice, we must leverage the polynomial kernels (i.e., those in the
M1 sum) to avoid making M2 large. We show how such a strategy works in Appendix C.
B.3
Proof of Theorem 5.1
Deﬁne g(Aij) := (g(ei), g(−ei), g(ej), g(eij)). Then,
g(Aij) | D, τ ∼N(µgij, Σij)
s.t.
µgij := Kτ(Aij, X)HτY,
Σij :=
h
Kτ(Aij, Aij) −Kτ(Aij, X)HτKτ(X, Aij)
i
,
(15)
which follows directly from Rasmussen & Williams (2006, Equation 2.21). Notice that,
θxi = g(e1)
2
−g(−e1)
2
= aT
i g(Aij)
and
θxixj = g(e1)
2
−g(−e1)
2
−g(ej)+g(eij) = aT
ijg(Aij),
(16)
16

where ai = (1/2, −1/2, 0, 0) and aij = (−1/2, 1/2, −1, 1). Furthermore, θx2
i = aT
iig(Aij) for aii =
(1/2, 1/2, 0, 0). The proof follows from Eq. (15), Eq. (16), and recalling that an afﬁne transformation h :
x 7→Ax of a multivariate Gaussian distribution Z ∼N(µ, Σ) is given by h(Z) ∼N(Aµ, AΣAT ).
B.4
Proof of Corollary 5.2
Corollary 5.2 follows immediately once we can show that Kτ(Aij, X) takes O(1) time. It sufﬁces to
show kτ(x(n), ei) and kτ(x(n), ei + ej) take O(1) time. Since kτ is a sum of polynomial kernels,
kτ(x, y) only depends on x, y ∈Rp through the inner product xT y. Hence, for vectors ˜x, ˜y ∈
RM, kτ(˜x, ˜y) is well-deﬁned and just depends on ˜xT ˜y. Now, kτ(x(n), ei) = kτ(x(n)
i
, 1) and
kτ(x(n), ei + ej) = kτ((x(n)
i
, x(n)
j
), (1, 1)). Since kτ(x(n)
i
, 1) and kτ((x(n)
i
, x(n)
j
), (1, 1)) do not
depend on p, these terms each take O(1) time to compute.
B.5
The General Kernel Interaction Trick
In this section, we generalize the kernel interaction trick, namely show how to access the distribution
of arbitrary components of θ. First, we require some new notation. For E ⊆{1, · · · , p}, |E| = M,
deﬁne
θE := (θxi1, · · · , θxiM , θxi1xi2, · · · , θxiM−1xiM ),
ij ∈E.
(17)
We show how to compute θE | τ, D from the GP posterior predictive distribution. Without any lost
of generality, we may assume E = {1, · · · , M} by relabeling the covariates.
Theorem B.2. (General kernel interaction trick) Let Hτ := (Kτ + σ2IN)−1 and
AM := [e1, −e1, · · · eM, −eM, e1 + e2, · · · , eM−1 + eM]T .
Let Kτ(AM, X) = Kτ(X, AM)T be the matrix formed by taking the kernel between each row of
AM with each row of X. Let
ai := (0, 0, · · · , 1/2, −1/2, · · · , 0, 0, · · · , 0) ∈R2M+ M(M−1)
2
aij := (0, 0, · · · , 1/2, −1/2, · · · , −1, · · · , 0, 0, · · · , 1, · · · , 0) ∈R2M+ M(M−1)
2
(18)
for i < j. That is, ai has non-zero entries at ei and −ei and aij has non-zero entries at ei, −ei, −ej,
and ei + ej. Let
RM := [a1 · · · aM a12 · · · a(M−1)M]T .
(19)
Then, θE | τ, D is a multivariate Gaussian distribution with mean RMKτ(AM, X)HτY and
covariance matrix
RM

Kτ(Aij, Aij) −Kτ(Aij, X)HτKτ(X, Aij)

RT
M.
Proof. Following the proof of Theorem 5.1,
g(AM) | D, τ ∼N(µgM , ΣM)
s.t.
µgM := Kτ(AM, X)HτY,
ΣM :=
h
Kτ(AM, AM) −Kτ(AM, X)HτKτ(X, AM)
i
.
(20)
Similar to Eq. (16),
θxi = g(e1)
2
−g(−e1)
2
= aT
i g(AM)
and
θxixj = g(e1)
2
−g(−e1)
2
−g(ej)+g(eij) = aT
ijg(AM).
(21)
The proof follows from Eq. (20), Eq. (21), and recalling that an afﬁne transformation h : x 7→RT
Mx
of a multivariate Gaussian distribution Z ∼N(µ, Σ) is given by h(Z) ∼N(RMµ, RMΣRT
M).
Corollary B.3. Given Kτ, the distribution θE | τ, D takes O(M 2) time and memory to compute.
Proof. The proof is identical to the one provided in Appendix B.4.
17

B.6
Proof of Proposition 6.1
See Appendix C.2.
C
Example Bayesian Interaction Models
In the following subsections, we show how to solve Eq. (9) for several classes of models.
C.1
Block-Degree Priors
Suppose we would like to set the prior variance of all terms with the same degree equal. That is, we
would like to use a prior of the form
η ∈R3 ∼p(η)
θxi | η ∼N(0, η2
1)
θxixj | η ∼N(0, η2
2)
θx2
i | η ∼N(0, η2
3)
θ0 | c2 ∼N(0, c2).
(22)
To ﬁnd the corresponding kernel, let λ = ( 1
4√
2
√η2, · · · ,
1
4√
2
√η2), M1 = 1 and M2 = 0. Then,
diag(S)(ij) = η2
2. Setting ψ2
i = η2
3 −1
2η2
2, implies that diag(S)(ii) = η2
3. Finally, letting α2
i =
τ 2
1 −2η2
√
2 and A = c2 −1 implies that diag(S)(i) = η2
1 and diag(S)(0) = c2 as desired. We may
equivalently re-write the induced kernel as
kblock,η(x, y) = η2
2
2 k1
poly,2(x, y)+(η2
3 −η2
2
2 )k0
poly,1(x⊙x, y⊙y)+

η2
1 −η2
2

k0
poly,1(x, y)+c2−η2
2
2 .
(23)
Hence, Eq. (22) admits a kernel that only takes O(p) time to compute.
C.2
Sparsity Priors
By Lemma B.1, the sparsity prior model provided in Eq. (11) equals kblock,η(κ ⊙x, κ ⊙y).
D
SKIM Model Details
We provide the full hierarchical form of SKIM next. SKIM is based closely on the regularized
horseshoe prior Piironen & Vehtari (2017) and the model proposed in Grifﬁn & Brown (2017):
m2 ∼InvGamma(α1, β1)
ξ2 ∼InvGamma(α2, β2)
ψ2 ∼InvGamma(α2, β2)
φ :=
s
p −s
σ
√
N
σ ∼N +(0, α3)
κi =
mλi
p
m2 + η2
1λ2
i
λi ∼C+(0, 1)
η1 ∼C+(0, φ)
η2 = η2
1
m2 ξ
η3 = η2
1
m2 ψ
θxi | η, κ ∼N(0, η2
1κ2
i )
θxj | η, κ ∼N(0, η2
1κ2
j)
θxixj | η, κ ∼N(0, η2
2κ2
i κ2
j)
θx2
i | η, κ ∼N(0, η2
3κ4
i )
θ0 | c2 ∼N(0, c2),
where s, αi, and βi are user-speciﬁed hyperparameters, C+(0, 1) is a half-Cauchy distribution, and
N + is a half-normal distribution. In Section 7, we set s = 5, α1 = 12.5, α2 = 12.5, α3 = 2,
β1 = 112.5, and β2 = 12.5. More details, such as selecting the hyperparameters, desirable properties,
and interpretations of SKIM, are provided below.
18

D.1
SKIM Details
Recall that we are primarily interested in the case when θ is sparse and satisﬁes strong-hierarchy. In
order to promote sparsity in the main effects, we require two ingredients: (1) a prior on the global
shrinkage parameter η1 and (2) a prior on the local shrinkage parameters contained in κ ∈Rp
Carvalho et al. (2009); Piironen & Vehtari (2017). Conditional on η1 and κ,
θxi | κ, η1 ∼N(0, η2
1κ2
i ),
i = 1, · · · , p.
(24)
η1 controls the overall sparsity level of the model; in particular, the model becomes sparser as η1
decreases. If we expect s non-zero main effects, then setting η1 =
s
p−s
σ
√
N will yield an expected
prior sparsity level of s by Piironen & Vehtari (2017, Equation 3.12). However, we often do not know
exactly how to select s. Hence, Piironen & Vehtari (2017) instead draw,
φ :=
s
p −s
σ
√
N
η1 ∼C+(0, φ),
(25)
to express the uncertainty of not knowing the true main effect sparsity level.
The prior variance of θxi is non-negligible only when κi is large enough to escape the global shrinkage
of η1. Hence, we draw κi from a heavy-tailed distribution so that certain main effects can escape
global shrinkage. Carvalho et al. (2009) suggest drawing κi from a half-Cauchy distribution since
this distribution has fat tails and desirable shrinkage properties. However, such a prior often leads to
undesirable numerical stability issues when using gradient-based MCMC methods such as NUTS
(Piironen & Vehtari, 2017). As a result, Piironen & Vehtari (2017) instead propose the regularized
horseshoe prior, which truncates the half-Cauchy distribution to have support only on [0, m) instead
of [0, ∞). This truncation (empirically) leads to better mixing properties, and is achieved by setting
κi =
mλi
p
m2 + η2
1λ2
i
,
λi ∼C+(0, 1).
(26)
As λi →∞, κi →m
η1 . Hence, as λi →∞, the prior variance of θxi equals m. Since we might not
know the scale m of the non-zero main effects, we place a prior on m, namely,
m2 ∼InvGamma(α1, β1)
(27)
for hyperparameters α1 and α2.
Next, we model the interactions. If strong-hierarchy holds, sparsity comes for free; if there are only
s ≪p non-zero main effects, then there are at most s(s−1)
2
≪p2 possible pairwise interactions. We
must be careful, however, because strong-hierarchy trivially holds; our main effect estimates will,
with probability one, never equal zero because the prior variances of the main effects are greater than
0 with probability one by Eq. (24) and our choice of priors. Instead, we aim for a relaxed version
of strong-hierarchy. Namely, that the prior variance of an interaction θxixj is large only if θxi and
θxj are both large. Notice that the prior variances on θxi and θxj are large only when κi and κj are
sufﬁciently far from zero. Hence, it sufﬁces to make the prior variance of θxixj large only when κi
and κj are both large.
Let ˜κ2
i = η2
1
m2 κ2
i . Then, 0 ≤˜κ2
i ≤1 and ˜κi approaches 1 as λi →∞. Since, ˜κ2
i and ˜κ2
j are bounded
by 1, ˜κ2
i ˜κ2
j will only be close to 1 when each term is close to one. That is, when both λi and λj are
large, or equivalently when κi and κj are both large. Hence, it sufﬁces to let
θxixj | η1, κ ∼N(0, ξ2˜κ2
i ˜κ2
j)
= N(0, η2
2κ2
i κ2
j)
for
η2 := η2
1
m2 ξ,
(28)
to promote strong-hierarchy, where ξ has the interpretation of the scale of the non-zero interaction
effects; as λi and λj tend to inﬁnity, the prior variance of θxixj approaches ξ2. Since we might not
know this scale, we draw
ξ2 ∼InvGamma(α2, β2),
(29)
for some choice of hyperparameters α2 and β2. Our choice of prior for θx2
i is analogous to the above
reasoning for the choice of prior on θxixj.
19

Main difference between SKIM and the model proposed in Grifﬁn & Brown (2017): Unlike
in the model proposed in Grifﬁn & Brown (2017), SKIM does not assume sparsity between the
interactions once the main effects are known. In particular, suppose, without any loss of generality,
that the ﬁrst s components of λ are large, while the remaining p −s components are very close to
zero. Then, the only interactions with non-negligible prior variance are the interactions between the
ﬁrst s variables. The number of such interactions is O(s2).
Unlike in Grifﬁn & Brown (2017), SKIM does not assume sparsity among these O(s2) interactions.
We do not assume such sparsity because the true sparsity level s is often very small (e.g., as in
genome-wide association studies), which means that s2 is small. Hence, once we have identiﬁed
which of the s variables have non-zero main effects, estimating O(s2) interactions from N datapoints
is not statistically difﬁcult relative to actually identifying the s non-zero main effects. In particular,
the mean-squared error of estimating O(s2) parameters from N datapoints is O
q
s2
N

by standard
Bernstein-von Mises results and a union bound. Thus, if s = o(
√
N), we can accurately estimate
O(s2) parameters.
E
Variable Selection Procedure
Suppose we sample τ (t) ∼p(τ | D) via our kernel interaction sampler. Then, we use these τ (t)
samples to perform variable selection in the following way. Without any loss of generality, suppose
we are deciding whether or not to include the main effect θxi. Below we will show how to construct
an interval (clower, cupper) for θxi. If this interval does not contain zero, we select θxi. This interval is
constructed by averaging the posterior means and standard deviations of θxi associated with each
sampled τ (t):
µT := 1
T
T
X
t=1
Ep(θxi|D,τ (t))[θxi]
σT := 1
T
T
X
t=1
SDp(θxi|D,τ (t))[θxi]
clower := µT −zσT
cupper := µT + zσT .
(30)
Here, SDq(θ)[θ] denotes the standard deviation of θ with respect to q(θ). In our experiments, we set
z = 2.59 which corresponds to the 99.5th percentile of a standard normal distribution.
Heuristic justiﬁcation of our variable selection procedure.
We might expect that the posterior p(θxi | D) has two modes: one mode near zero when the prior
variance of θxi is small and another mode when the prior variance is large. Thus, the posterior mean
µT will “shrink” the estimate of θxi towards zero, where the amount of shrinkage depends on the
posterior mass of each mode. To understand the variability of the posterior mean, we effectively
average the variability within each mode in Eq. (30). This averaging of variability within modes has
the advantage of not artiﬁcially increasing the variance (due to the modes being separated by regions of
low-probability) but has the disadvantage of potentially underestimating our uncertainty. For example,
suppose θxi | D
d= .5N(0, .1) + .5N(2, .1). Then, Ep(θxi|D)[θxi] = 1 and SDp(θxi|D)[θxi] = 1. In
this case, we would not select θxi if we required that the posterior mean is further than twice the
posterior standard deviation. If we instead averaged the variance within the modes (which would
equal .1), we would select θxi as we do in Eq. (30).
While we found good empirical performance of our variable selection procedure in Section 7 (e.g.,
based on FDR) we nevertheless think that variable selection in multimodal posteriors is challenging,
and an area of active research. An interesting future research direction would be to develop even better
variable selection strategies for sparse interaction models or to rigorously understand the tradeoffs
between different variable selection procedures.
F
Woodbury Identity and the Matrix Determinant Lemma
To compute the determinant in Eq. (5), one can perform a Cholesky decomposition of ΣN,τ ∈
Rdim(Φ2)×dim(Φ2). Computing ΣN,τ takes O(p4N) time and O(p2N + p4) memory. Computing the
20

Cholesky decomposition of ΣN,τ takes O(p6) time and requires O(p4) memory. This factorization
can be avoided through the Woodbury matrix lemma and matrix determinant lemma.
The Woodbury matrix identity implies that
(A−1 + UU T )−1 = A −AU(IK + U T AU)−1U T A,
(31)
where A ∈RM×M, U ∈RM×K, and IK is the K × K identity matrix. The matrix determinant
lemma implies that
det(A−1 + UU T ) = det(I + U T AU) det(A−1).
(32)
Then, by the Woodbury identity,
Στ,N = (Σ−1
τ + 1
σ2 Φ2(X)T Φ2(X))−1 = Στ −ΣτΦ2(X)T (IN +Φ2(X)ΣτΦ2(X)T )−1Φ2(X)Στ.
(33)
Computing p(D | τ) requires computing det(Στ,N). By the matrix determinant lemma,
det(Στ,N) = (det(IN + Φ2(X)ΣτΦ2(X)T ) det(Σ−1
τ ))−1.
(34)
When Στ is diagonal, the determinant equals the product of the diagonal, and its inverse equals
one over the diagonal. Both of these quantities can be computed in O(p2) time. Hence, the time
complexity for computing det(Στ,N) is dominated by computing det(IN + Φ2(X)ΣτΦ2(X)T ),
which takes O(N 2p2 + N 3) time and O(Np2) memory to store Φ2(X).
G
Standard Polynomial Kernel
The feature map induced by the standard degree two polynomial kernel is given by
Φc
poly,2(x) := (x2
1, · · · , x2
p,
√
2x1x2, · · · ,
√
2xp−1xp,
√
2cx1, · · · ,
√
2cxp, c)
= apoly,2 ⊙Φ2(x), apoly,2 := (1, · · · , 1,
√
2, · · · ,
√
2,
√
2c, · · · ,
√
2c, c).
(35)
Hence, Eq. (35) implies that
diag(Σpoly,2) = apoly,2 ⊙apoly,2.
(36)
Eq. (36) shows that the prior covariance of the interaction terms are given higher prior variance than
the main effects when c ≤1, which is often undesirable. Furthermore, this prior does not promote
sparsity, which is typically expected in high-dimensional problems.
21

