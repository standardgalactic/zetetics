Neural Networks 145 (2022) 90–106
Contents lists available at ScienceDirect
Neural Networks
journal homepage: www.elsevier.com/locate/neunet
Interpolation consistency training for semi-supervised learning
Vikas Verma a,b,∗, Kenji Kawaguchi c, Alex Lamb a, Juho Kannala b, Arno Solin b,
Yoshua Bengio a, David Lopez-Paz d
a Montreal Institute for Learning Algorithms (MILA), Canada
b Aalto University, Finland
c Harvard University, USA
d Facebook AI Research, France
a r t i c l e
i n f o
Article history:
Received 27 April 2021
Received in revised form 23 August 2021
Accepted 11 October 2021
Available online 21 October 2021
Keywords:
Semi-supervised learning
Deep Neural Networks
Mixup
Consistency regularization
a b s t r a c t
We introduce Interpolation Consistency Training (ICT), a simple and computation efficient algorithm
for training Deep Neural Networks in the semi-supervised learning paradigm. ICT encourages the
prediction at an interpolation of unlabeled points to be consistent with the interpolation of the
predictions at those points. In classification problems, ICT moves the decision boundary to low-
density regions of the data distribution. Our experiments show that ICT achieves state-of-the-art
performance when applied to standard neural network architectures on the CIFAR-10 and SVHN
benchmark datasets. Our theoretical analysis shows that ICT corresponds to a certain type of data-
adaptive regularization with unlabeled points which reduces overfitting to labeled points under high
confidence values.
© 2021 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license
(http://creativecommons.org/licenses/by/4.0/).
1. Introduction
Deep learning achieves excellent performance in supervised
learning tasks where labeled data is abundant (LeCun et al., 2015).
However, labeling large amounts of data is often prohibitive due
to time, financial, and expertise constraints. As machine learn-
ing permeates an increasing variety of domains, the number of
applications where unlabeled data is voluminous and labels are
scarce increases. For instance, recognizing documents in extinct
languages, where a machine learning system has access to a few
labels, produced by highly-skilled scholars (Clanuwat et al., 2018).
The goal of Semi-Supervised Learning (SSL) (Chapelle et al.,
2010) is to leverage large amounts of unlabeled data to im-
prove the performance of supervised learning over small datasets.
Often, SSL algorithms use unlabeled data to learn additional struc-
ture about the input distribution. For instance, the existence of
cluster structures in the input distribution could hint the sep-
aration of samples into different labels. This is often called the
cluster assumption: if two samples belong to the same cluster
in the input distribution, then they are likely to belong to the
same class. The cluster assumption is equivalent to the low-
density separation assumption: the decision boundary should lie
∗Corresponding author at: Aalto University, Finland.
E-mail addresses:
vikasverma.iitm@gmail.com (V. Verma),
kkawaguchi@fas.harvard.edu (K. Kawaguchi), lambalex@iro.umontreal.ca
(A. Lamb), juho.kaanala@aalto.fi (J. Kannala), arno.solin@aalto.fi (A. Solin),
yoshua.bengio@mila.quebec (Y. Bengio), dlp@fb.com (D. Lopez-Paz).
in the low-density regions. The equivalence is easy to infer: A
decision boundary which lies in a high-density region, will cut
a cluster into two different classes, requiring that samples from
different classes lie in the same cluster; which is the violation of
the cluster assumption. The low-density separation assumption has
inspired many recent consistency-regularization semi-supervised
learning techniques, including the Π-model (Laine & Aila, 2017;
Sajjadi et al., 2016), temporal ensembling (Laine & Aila, 2017),
VAT (Miyato et al., 2018), and the Mean-Teacher (Tarvainen &
Valpola, 2017).
Consistency regularization methods for semi-supervised learn-
ing enforce the low-density separation assumption by encour-
aging invariant prediction f (u) =
f (u + δ) for perturbations
u+δ of unlabeled points u. Such consistency and small prediction
error can be satisfied simultaneously if and only if the decision
boundary traverses a low-density path.
Different consistency regularization techniques vary in how
they choose the unlabeled data perturbations δ. One simple al-
ternative is to use random perturbations δ. However, random
perturbations are inefficient in high dimensions, as only a tiny
proportion of input perturbations are capable of pushing the
decision boundary into low-density regions. To alleviate this is-
sue, Virtual Adversarial Training or VAT (Miyato et al., 2018),
searches for small perturbations δ that maximize the change in
the prediction of the model. This involves computing the gradient
of the predictor with respect to its input, which can be expensive
for large neural network models.
https://doi.org/10.1016/j.neunet.2021.10.008
0893-6080/© 2021 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).

V. Verma, K. Kawaguchi, A. Lamb et al.
Neural Networks 145 (2022) 90–106
Fig. 1. Interpolation Consistency Training (ICT) applied to the ‘‘two moons’’ dataset, when three labels per class (large dots) and a large amount of unlabeled data
(small dots) are available. When compared to supervised learning (red), ICT encourages a decision boundary traversing a low-density region that would better reflect
the structure of the unlabeled data. Both methods employ a multilayer perceptron with three hidden ReLU layers of twenty neurons. Best viewed in colors in the
printed version.
This additional computation makes VAT (Miyato et al., 2018)
and other related methods such as Park et al. (2018) less ap-
pealing in situations where unlabeled data is available in large
quantities. Furthermore, recent research has shown that training
with adversarial perturbations can hurt generalization perfor-
mance (Nakkiran, 2019; Tsipras et al., 2018).
To overcome the above limitations, we propose the Interpola-
tion Consistency Training (ICT), an efficient consistency regular-
ization technique for state-of-the-art semi-supervised learning. In
a nutshell, ICT regularizes semi-supervised learning by encourag-
ing consistent predictions f (αu1 + (1 −α)u2) = αf (u1) + (1 −
α)f (u2) at interpolations αu1 + (1 −α)u2 of unlabeled points u1
and u2.
Our experimental results on the benchmark datasets CIFAR10
and SVHN and neural network architectures CNN-13 (Laine & Aila,
2017; Luo et al., 2018; Miyato et al., 2018; Park et al., 2018;
Tarvainen & Valpola, 2017) and WRN28-2 (Oliver et al., 2018) out-
perform (or are competitive with) the state-of-the-art methods.
ICT is simpler and more computation efficient than several of the
recent SSL algorithms, making it an appealing approach to SSL.
Fig. 1 illustrates how ICT learns a decision boundary traversing a
low density region in the ‘‘two moons’’ problem.
After the publication of an earlier version of this paper (Verma
et al., 2019), similar ideas have been explored in Berthelot, Carlini
et al. (2019) and Berthelot et al. (2020), with some additional
techniques in comparison to that proposed in Verma et al. (2019).
Although all of these methods work well in practice, the theo-
retical underpinning of these methods is not well understood.
In this work, we additionally provide a novel theory of ICT to
understand how and when ICT can succeed or fail to effectively
utilize unlabeled points.
2. Interpolation consistency training
Given a mixup (Zhang et al., 2018) operation:
Mixλ(a, b) = λ · a + (1 −λ) · b,
Interpolation Consistency Training(ICT) trains a prediction model
fθ to provide consistent predictions at interpolations of unlabeled
points:
fθ(Mixλ(uj, uk)) ≈Mixλ(fθ′(uj), fθ′(uk)),
where θ′ is a moving average of θ (Fig. 2). But, why do interpo-
lations between unlabeled samples provide a good consistency
perturbation for semi-supervised training?
To begin with, observe that the most useful samples on which
the consistency regularization should be applied are the samples
near the decision boundary. Adding a small perturbation δ to such
low-margin unlabeled samples uj is likely to push uj + δ over
the other side of the decision boundary. This would violate the
low-density separation assumption, making uj + δ a good place to
apply consistency regularization. These violations do not occur at
high-margin unlabeled points that lie far away from the decision
boundary.
Back to low-margin unlabeled points uj, how can we find a
perturbation δ such that uj and uj + δ lie on opposite sides of the
decision boundary? Although tempting, using random perturba-
tions is an inefficient strategy, since the subset of directions ap-
proaching the decision boundary is a tiny fraction of the ambient
space.
Instead, consider interpolations uj + δ = Mixλ(uj, uk) towards
a second randomly selected unlabeled examples uk. Then, the two
unlabeled samples uj and uk can either:
1. lie in the same cluster,
2. lie in different clusters but belong to the same class,
3. lie on different clusters and belong to the different classes.
Assuming the cluster assumption, the probability of (1) de-
creases as the number of classes increases. The probability of (2)
is low if we assume that the number of clusters for each class
is balanced. Finally, the probability of (3) is the highest. Then,
assuming that one of (uj, uk) lies near the decision boundary (it is
a good candidate for enforcing consistency), it is likely (because
of the high probability of (3)) that the interpolation towards uk
points towards a region of low density, followed by the cluster of
the other class. Since this is a good direction to move the decision,
the interpolation is a good perturbation for consistency-based
regularization.
Our exposition has argued so far that interpolations between
random unlabeled samples are likely to fall in low-density re-
gions.
Thus,
such
interpolations
are
good
locations
where
consistency-based regularization could be applied. But how
should we label those interpolations? Unlike random or adver-
sarial perturbations of single unlabeled examples uj, our scheme
involves two unlabeled examples (uj, uk). Intuitively, we would
like to push the decision boundary as far as possible from the
class boundaries, as it is well known that decision boundaries
with large margin generalize better (Shawe-Taylor et al., 1996).
In the supervised learning setting, one method to achieve large-
margin decision boundaries is mixup (Zhang et al., 2018). In
mixup, the decision boundary is pushed far away from the class
boundaries by enforcing the prediction model to change linearly
in between samples. This is done by training the model fθ to pre-
dict Mixλ(y, y′) at location Mixλ(x, x′), for random pairs of labeled
samples ((x, y), (x′, y′)). Here we extend mixup to the semi-
supervised learning setting by training the model fθ to predict the
‘‘fake label’’ Mixλ(fθ(uj), fθ(uk)) at location Mixλ(uj, uk). In order
to follow a more conservative consistent regularization, we en-
courage the model fθ to predict the fake label Mixλ(fθ′(uj), fθ′(uk))
91

V. Verma, K. Kawaguchi, A. Lamb et al.
Neural Networks 145 (2022) 90–106
Fig. 2. Interpolation Consistency Training (ICT) learns a student network fθ in a semi-supervised manner. To this end, ICT uses a mean-teacher fθ′ , where the teacher
parameters θ′ are an exponential moving average of the student parameters θ. During training, the student parameters θ are updated to encourage consistent
predictions fθ (Mixλ(uj, uk)) ≈Mixλ(fθ′ (uj), fθ′ (uk)), and correct predictions for labeled examples xi.
at location Mixλ(uj, uk), where θ′ is a moving average of θ, also
known as a mean-teacher (Tarvainen & Valpola, 2017).
We are now ready to describe in detail the proposed Inter-
polation Consistency Training (ICT). Consider access to labeled
samples (xi, yi) ∼DL, drawn from the joint distribution PXY (X, Y).
Also, consider access to unlabeled samples uj, uk ∼DUL, drawn
from the marginal distribution PX(X) =
PXY (X,Y)
PY|X (Y|X). Our learning goal
is to train a model fθ, able to predict Y from X. By using stochastic
gradient descent, at each iteration t, update the parameters θ to
minimize
L = LS + w(t) · LUS
where LS is the usual cross-entropy supervised learning loss over
labeled samples DL, and LUS is our new interpolation consistency
regularization term. These two losses are computed on top of
(labeled and unlabeled) minibatches, and the ramp function w(t)
increases the importance of the consistency regularization term
LUS after each iteration. To compute LUS, sample two minibatches
of unlabeled points uj and uk, and compute their fake labels ˆyj =
fθ′(uj) and ˆyk = fθ′(uk), where θ′ is an moving average of θ (Tar-
vainen & Valpola, 2017). Second, compute the interpolation um =
Mixλ(uj, uk), as well as the model prediction at that location, ˆym =
fθ(um). Third, update the parameters θ as to bring the prediction
ˆym closer to the interpolation of the fake labels Mixλ(ˆyj, ˆyk). The
discrepancy between the prediction ˆym and Mixλ(ˆyj, ˆyk) can be
measured using any loss; in our experiments, we use the mean
squared error. Following Zhang et al. (2018), on each update we
sample a random λ from Beta(α, α).
In sum, the population version of our ICT term can be written
as:
LUS =
E
uj,uk∼PX
E
λ∼Beta(α,α)ℓ(fθ(Mixλ(uj, uk)), Mixλ(fθ′(uj), fθ′(uk)))
(1)
ICT is summarized in Fig. 2 and Algorithm 1.
3. Experiments
3.1. Datasets
We follow the common practice in semi-supervised learning
literature (Laine & Aila, 2017; Luo et al., 2018; Miyato et al.,
2018; Park et al., 2018; Tarvainen & Valpola, 2017) and conduct
experiments using the CIFAR-10, SVHN, and CIFAR-100 datasets,
where only a fraction of the training data is labeled, and the
remaining data is used as unlabeled data. We followed the stan-
dardized procedures laid out by Oliver et al. (2018) to ensure a
fair comparison.
The CIFAR-10 dataset consists of 60 000 color images each of
size 32 × 32, split between 50K training and 10K test images.
This dataset has ten classes, which include images of natural
objects such as cars, horses, airplanes and deer. The CIFAR-100
is similar to the CIFAR-10 dataset, except it has 100 classes with
600 images in each class. The SVHN dataset consists of 73 257
training samples and 26 032 test samples each of size 32 × 32.
Each example is a close-up image of a house number (the ten
classes are the digits from 0–9).
We adopt the standard data-augmentation and pre-processing
scheme
which
has
become
standard
practice
in
the
semi-supervised learning literature (Athiwaratkun et al., 2019;
Laine & Aila, 2017; Luo et al., 2018; Miyato et al., 2018; Sajjadi
et al., 2016; Tarvainen & Valpola, 2017). More specifically, for
CIFAR-10, we first zero-pad each image with 2 pixels on each
side. Then, the resulting image is randomly cropped to produce
a new 32 × 32 image. Next, the image is horizontally flipped
with probability 0.5, followed by per-channel standardization and
ZCA preprocessing. For SVHN, we zero-pad each image with 2
pixels on each side and then randomly crop the resulting image
to produce a new 32 × 32 image, followed by zero-mean and
unit-variance image whitening.
3.2. Models
We conduct our experiments using CNN-13 and Wide-Resnet-
28-2 architectures. The CNN-13 architecture has been adopted
as the standard benchmark architecture in recent state-of-the-
art SSL methods (Laine & Aila, 2017; Luo et al., 2018; Miyato
et al., 2018; Park et al., 2018; Tarvainen & Valpola, 2017). We
use its variant (i.e., without additive Gaussian noise in the input
layer) as implemented in Athiwaratkun et al. (2019). We also
removed the Dropout noise to isolate the improvement achieved
through our method. Other SSL methods in Tables 1 and 2 use the
Dropout noise, which gives them more regularizing capabilities.
Despite this, our method outperforms other methods in several
experimental settings.
Oliver et al. (2018) performed a systematic study using Wide-
Resnet-28-2 (Zagoruyko & Komodakis, 2016), a specific residual
network architecture, with extensive hyperparameter search to
compare the performance of various consistency-based semi-
supervised algorithms. We evaluate ICT using this same setup as
a mean towards a fair comparison to these algorithms.
92

V. Verma, K. Kawaguchi, A. Lamb et al.
Neural Networks 145 (2022) 90–106
Algorithm 1 The Interpolation Consistency Training (ICT) Algorithm
Require: fθ(x): neural network with trainable parameters θ
Require: fθ′(x) mean teacher with θ′ equal to moving average of θ
Require: DL(x, y): collection of the labeled samples
Require: DUL(x): collection of the unlabeled samples
Require: α: rate of moving average
Require: w(t): ramp function for increasing the importance of consistency regularization
Require: T: total number of iterations
Require: Q : random distribution on [0,1]
Require: Mixλ(a, b) = λa + (1 −λ)b.
for t = 1, . . . , T do
Sample {(xi, yi)}B
i=1 ∼DL(x, y)
▷Sample labeled minibatch
LS = CrossEntropy({(fθ(xi), yi)}B
i=1)
▷Supervised loss (cross-entropy)
Sample {uj}U
j=1, {uk}U
k=1 ∼DUL(x)
▷Sample two unlabeled minibatchs
{ˆyj}U
j=1 = {fθ′(uj)}U
j=1, {ˆyk}U
k=1 = {fθ′(uk)}U
k=1
▷Compute pseudo labels
Sample λ ∼Q
▷sample an interpolation coefficient
(um = Mixλ(uj, uk), ˆym = Mixλ(ˆyj, ˆyk))
▷Compute interpolation
LUS = ConsistencyLoss({(fθ(um), ˆym)}U
m=1)
▷e.g., mean squared error
L = LS + w(t) · LUS
▷Total Loss
gθ ←∇θL
▷Compute Gradients
θ′ = αθ′ + (1 −α)θ
▷Update moving average of parameters
θ ←Step(θ, gθ)
▷e.g. SGD, Adam
end for
return θ
Table 1
Error rates (%) on CIFAR-10 using CNN-13 architecture. We ran three trials for ICT.
Model
1000 labeled
50 000 unlabeled
2000 labeled
50 000 unlabeled
4000 labeled
50 000 unlabeled
Supervised
39.95 ± 0.75
31.16 ± 0.66
21.75 ± 0.46
Supervised (Mixup)
36.48 ± 0.15
26.24 ± 0.46
19.67 ± 0.16
Supervised (Manifold Mixup)
34.58 ± 0.37
25.12 ± 0.52
18.59 ± 0.18
Π model (Laine & Aila, 2017)
31.65 ± 1.20
17.57 ± 0.44
12.36 ± 0.31
TempEns (Laine & Aila, 2017)
23.31 ± 1.01
15.64 ± 0.39
12.16 ± 0.24
MT (Tarvainen & Valpola, 2017)
21.55 ± 1.48
15.73 ± 0.31
12.31 ± 0.28
VAT (Miyato et al., 2018)
–
–
11.36 ± NA
VAT+Ent (Miyato et al., 2018)
–
–
10.55 ± NA
VAdD (Park et al., 2018)
–
–
11.32 ± 0.11
SNTG (Luo et al., 2018)
18.41 ± 0.52
13.64 ± 0.32
10.93 ± 0.14
MT+ Fast SWA
(Athiwaratkun et al., 2019)
15.58 ± NA
11.02 ± NA
9.05 ± NA
ICT
15.48 ± 0.78
9.26 ± 0.09
7.29 ± 0.02
3.3. Implementation details
We used the SGD with nesterov momentum optimizer for all
of our experiments. For the experiments in Tables 1 and 2, we run
the experiments for 400 epochs. For the experiments in Table 3,
we run experiments for 600 epochs. The initial learning rate was
set to 0.1 for CIFAR-10 and SVHN and 0.25 for CIFAR-100, which is
then annealed using the cosine annealing technique proposed in
Loshchilov and Hutter (2016) and used by Tarvainen and Valpola
(2017). The momentum parameter was set to 0.9. We used an L2
regularization coefficient 0.0001 and a batch-size of 100 in our
experiments.
In each experiment, we report mean and standard deviation
across three independently run trials.
The consistency coefficient w(t) is ramped up from its initial
value 0.0 to its maximum value at one-fourth of the total number
of epochs using the same sigmoid schedule of Tarvainen and
Valpola (2017). We used MSE loss for computing the consistency
loss following Laine and Aila (2017) and Tarvainen and Valpola
(2017). We set the decay coefficient for the mean-teacher to 0.999
following Tarvainen and Valpola (2017).
We conduct hyperparameter search over the two hyperpa-
rameters introduced by our method: the maximum value of the
consistency coefficient w(t) (we searched over the values in {1.0,
10.0, 20.0, 50.0, 100.0}) and the parameter α of distribution
93

V. Verma, K. Kawaguchi, A. Lamb et al.
Neural Networks 145 (2022) 90–106
Table 2
Error rates (%) on SVHN using CNN-13 architecture. We ran three trials for ICT.
Model
250 labeled
73 257 unlabeled
500 labeled
73 257 unlabeled
1000 labeled
73 257 unlabeled
Supervised
40.62 ± 0.95
22.93 ± 0.67
15.54 ± 0.61
Supervised (Mixup)
33.73 ± 1.79
21.08 ± 0.61
13.70 ± 0.47
Supervised ( Manifold Mixup)
31.75 ± 1.39
20.57 ± 0.63
13.07 ± 0.53
Π model (Laine & Aila, 2017)
9.93 ± 1.15
6.65 ± 0.53
4.82 ± 0.17
TempEns (Laine & Aila, 2017)
12.62 ± 2.91
5.12 ± 0.13
4.42 ± 0.16
MT (Tarvainen & Valpola, 2017)
4.35 ± 0.50
4.18 ± 0.27
3.95 ± 0.19
VAT (Miyato et al., 2018)
–
–
5.42 ± NA
VAT+Ent (Miyato et al., 2018)
–
–
3.86 ± NA
VAdD (Park et al., 2018)
–
–
4.16 ± 0.08
SNTG (Luo et al., 2018)
4.29 ± 0.23
3.99 ± 0.24
3.86 ± 0.27
ICT
4.78 ± 0.68
4.23 ± 0.15
3.89 ± 0.04
Table 3
Results on CIFAR-10 (4000 labels) and SVHN (1000 labels) (in test error %).
All results use the same standardized architecture (WideResNet-28-2). Each
experiment was run for three trials. We did not conduct any hyperparameter
search and used the best hyperparameters found in the experiments of Tables 1
and 2 for CIFAR-10 (4000 labels) and SVHN (1000 labels).
SSL Approach
CIFAR10
4000 labeled
50 000 unlabeled
SVHN
1000 labeled
73 257 unlabeled
Superviseda
20.26 ± 0.38
12.83 ± 0.47
Mean-Teachera
15.87 ± 0.28
5.65 ± 0.47
VATa
13.86 ± 0.27
5.63 ± 0.20
VAT-EMa
13.13 ± 0.39
5.35 ± 0.19
ICT
7.66 ± 0.17
3.53 ± 0.07
aRefers to the results reported in Oliver et al. (2018).
Beta(α, α) (we searched over the values in {0.1, 0.2, 0.5, 1.0}). We
select the best hyperparameter using a validation set of 5000 and
1000 labeled samples for CIFAR-10 and SVHN respectively. This
size of the validation set is the same as that used in the other
methods compared in this work.
We note the in all our experiments with ICT, to get the su-
pervised loss, we perform the interpolation of labeled sample
pair and their corresponding labels (as in mixup Zhang et al.,
2018). To make sure, that the improvements from ICT are not
only because of the supervised mixup loss, we provide the direct
comparison of ICT against supervised mixup and Manifold Mixup
training in Tables 1 and 2.
3.4. Results
We provide the results for CIFAR10 and SVHN datasets using
CNN-13 architecture in Tables 1 and 2, respectively.
To justify the use of a SSL algorithm, one must compare
its performance against the state-of-the-art supervised learn-
ing algorithm (Oliver et al., 2018). To this end, we compare
our method against two state-of-the-art supervised learning al-
gorithms (Verma et al., 2018; Zhang et al., 2018), denoted as
Supervised(Mixup) and Supervised(Manifold Mixup), respectively
in Tables 1 and 2. ICT method passes this test with a wide margin,
often resulting in a two-fold reduction in the test error in the case
of CIFAR10 (Table 1) and a four-fold reduction in the case of SVHN
(Table 2)
Furthermore, in Table 1, we see that ICT improves the test
error of other strong SSL methods. For example, in the case of
4000 labeled samples, it improves the test error of best-reported
method by ∼25%. The best values of the hyperparameter max-
consistency coefficient for 1000, 2000 and 4000 labels experi-
ments were found to be 10.0, 100.0 and 100.0 respectively and
the best values of the hyperparameter α for 1000, 2000 and 4000
labels experiments were found to be 0.2, 1.0 and 1.0 respectively.
In general, we observed that for less number of labeled data,
lower values of max-consistency coefficient and α obtained better
validation errors.
For SVHN, the test errors obtained by ICT are competitive
with other state-of-the-art SSL methods (Table 2). The best values
of the hyperparameters max-consistency coefficient and α were
found to be 100 and 0.1 respectively, for all the ICT results
reported in Table 2.
Oliver et al. (2018) performed extensive hyperparameter
search for various consistency regularization SSL algorithms using
the WRN-28-2 and they report the best test errors found for each
of these algorithms. For a fair comparison of ICT against these SSL
algorithms, we conduct experiments on WRN-28-2 architecture.
The results are shown in Table 3. ICT achieves improvement over
other methods both for the CIFAR10 and SVHN datasets.
We note that unlike other SSL methods of Tables 1–3, we do
not use Dropout regularizer in our implementation of CNN-13 and
WRN-28-2. Using Dropout along with the ICT may further reduce
the test error.
3.5. Ablation study
• Effect of not using the mean-teacher in ICT: We note that Π-
model, VAT and VAdD methods in Tables 1 and 2 do not use
a mean-teacher to make predictions on the unlabeled data.
Although the mean-teacher (Tarvainen & Valpola, 2017)
used in ICT does not incur any significant computation
cost, one might argue that a more direct comparison with
Π-model, VAT and VAdD methods requires not using a
mean-teacher. To this end, we conduct an experiment on the
CIFAR10 dataset, without the mean-teacher in ICT, i.e. the
prediction on the unlabeled data comes from the network
fθ(x) instead of the mean-teacher network fθ′(x) in Eq. (1).
We obtain test errors of 19.56 ± 0.56%, 14.35 ± 0.15% and
11.19 ± 0.14% for 1000, 2000, 4000 labeled samples re-
spectively (we did not conduct any hyperparameter search
for these experiments and used the best hyperparameters
found in the ICT experiments of Table 1). This shows that
even without a mean-teacher, ICT has major an advan-
tage over methods such as VAT (Miyato et al., 2018) and
VAdD (Park et al., 2018) that it does not require an addi-
tional gradient computation yet performs on the same level
of the test error.
• Effect of not having the mixup supervised loss: In Section 3.3,
we noted that to get the supervised loss, we perform the
interpolation of labeled sample pair and their corresponding
labels (mixup supervised loss as in Zhang et al. (2018)).
Will the performance of ICT be significantly reduced by not
having the mixup supervised loss? We conducted experi-
ments with ICT on both CIFAR10 and SVHN with the vanilla
94

V. Verma, K. Kawaguchi, A. Lamb et al.
Neural Networks 145 (2022) 90–106
Table 4
Results on CIFAR-100 with 100 labels per class using the CNN-13 architecture.
We ran three trials for ICT. ICT was implemented with the ReLU activation and
the softplus activation function.
Model
Test error (%)
Supervised-onlya
44.56 ± 0.30
Π modela
39.19 ± 0.36
TempEnsa
38.65 ± 0.51
ICT with ReLU activation
35.54 ± 0.26
ICT with softplus activation
34.99 ± 0.32
aRefers to the results reported in Laine and Aila (2017).
supervised loss. For CIFAR10, we obtained test errors of
14.86 ± 0.39, 9.02 ± 0.12 and 8.23 ± 0.22 for 1000, 2000
and 4000 labeled samples respectively. We did not conduct
any hyperparameter search and used the best values of
hyperparameters (max-consistency coefficient and α) found
in the experiments of Table 1. We observe that in the case
of 1000 and 2000 labeled samples, there is no increase
in the test error (w.r.t having the mixup supervised loss),
whereas in the case of 4000 labels, the test error increases
by approximately 1% . This suggests that, in the low labeled
data regimes, not having the mixup supervised loss in the
ICT does not incur any significant increase in the test error.
3.6. Results with a larger number of classes
The main experiments above are conducted on CIFAR-10 and
SVHN. Each of these datasets has 10 classes for the classification
task. Accordingly, we conducted additional experiments using
CIFAR-100, and reported the results with the CNN-13 architecture
in Table 4. Following the previous work on semi-supervised learn-
ing with CIFAR-100 and CNN-13 (Laine & Aila, 2017), we used 100
labeled points per class in this experiment. We did not conduct
any hyperparameter search and used the best hyperparameters
found in the experiments of CIFAR-10 as reported in Section 3.4.
As can be seen in Table 4, ICT with the ReLU activation func-
tion outperformed the previous methods as expected from the
above study with the CIFAR-10 and SVHN datasets. Moreover,
ICT worked the best after replacing the ReLU activation func-
tion with the softplus activation function (Dugas et al., 2001):
φ(z) = ln(1 + exp(κz))/κ with κ = 100. Here, the value of κ
was set without any hyperparameter search and simply taken
from the previous study for supervised learning with a different
architecture (pre-activation ResNet with 18 layers) (Kawaguchi &
Sun, 2021).
4. Theoretical analysis
In this section, we establish mathematical properties of ICT for
binary classification with fθ(u) ∈R. We begin in Section 4.1 with
additional notation, an introduction of real analytic functions, and
a property of the Euclidian norm of the Kronecker product. Using
the real analyticity and the property of the Kronecker product, we
show in Section 4.2 that ICT regularizes higher-order derivatives.
We conclude in Section 4.3 by showing how ICT can reduce
overfitting and lead to better generalization behaviors than those
without ICT.
4.1. Preliminaries
In our experiments, we did not use mean-teacher models f ′
θ
in the ablation study for CIFAR-10 and the two moon dataset in
all of Figs. 1 and 3–7. Those experiments consistently show the
advantages of ICT even without the mean-teacher models. Indeed,
the mean teacher is not a necessary mechanism of consistency-
regularization in general (e.g., see equation (1) of Han et al., 2021).
Thus, to understand and disentangle the essential mechanisms of
ICT, we consider the same setting as those experiments: i.e., this
section focuses on the mean square loss for the unlabeled data
without the mean teacher as
LUS :=
E
u,u′∼PX
E
λ∼Beta(α,α)ℓ(fθ(Mixλ(u, u′)), Mixλ(fθ(u), fθ(u′))),
(2)
where u ∈X ⊆Rd and ℓ(a, b) = (a −b)2. For example, if P(X) is
an empirical measure on the finite unlabeled data points, we can
write LUS by
LUS = 1
n2
n

i=1
n

j=1
E
λ∼Beta(α,α)ℓ(fθ(Mixλ(ui, uj)) −Mixλ(fθ(ui), fθ(uj))), (3)
because LUS := Eu,u′∼PX g(u, u′) =

g(u, u′)dPX(u)dPX(u′) =
1
n2
n
i=1
n
j=1 g(ui, uj) for any measurable function g, where the
last equality used the empirical measure PX =
1
n
n
i=1 δui with
the Dirac measures δui. We consider the function fθ in the form
of
fθ(u) = σ(hθ(u)),
(4)
where σ(a) =
1
1+e−a is the sigmoid function and hθ(u) repre-
sents the pre-activation output of the last layer of a deep neural
network.
The theory developed in this section requires the function fθ
to be real analytic, which is satisfied for a large class of deep
neural networks used in practice. Since a composition of real
analytic functions is real analytic, we only need to require that
each operation in each layer satisfies the real analyticity. The
convolution, affine map, skip connection, batch normalization
and average pooling are all real analytic functions. Therefore,
the composition of these operations preserve real analyticity.
Furthermore, many common activation functions are real ana-
lytic. For example, sigmoid σ, hyperbolic tangents and softplus
activations φ(z) = ln(1+exp(κz))/κ are all real analytic functions,
with any hyperparameter κ > 0. Here, the softplus activation can
approximate the ReLU activation for any desired accuracy as
φ(x) →relu(x) as κ →∞,
where relu represents the ReLU activation. Indeed, the ReLU
activation and the softplus activation behave similarly to each
other in numerical experiments with a large value of κ such
as κ
=
100, which is demonstrated in the previous studies
(e.g., see Kawaguchi & Sun, 2021). As a further demonstration
of this fact, we used the softplus activation in Fig. 3 (and Fig. 4)
and the ReLU activation in Fig. 7 (and Fig. 5): we can see that
with both activation functions, ICT works well and the numerical
results are consistent with our theoretical predictions. This is
expected because the softplus activation function can approx-
imate the ReLU function arbitrarily well by varying the value
of κ ∈(0, ∞) and our theoretical analyses hold for any κ ∈
(0, ∞). Moreover, Table 4 shows that ICT with the softplus acti-
vation works similarly to and can outperform ICT with the ReLU
activation. Overall, the requirement on the real analyticity of
the function fθ is easily satisfied in practice without degrading
practical performances.
We close this subsection by introducing additional notation
and proving a theoretical property of the Kronecker product.
Let ˆS =
(ui)n
i=1 be a dataset with unlabeled data points and
S = ((xi, yi))m
i=1 be a dataset with labeled points. Here, ˆS contains
unlabeled data points only and is not necessarily the whole
dataset used for the unsupervised loss; e.g., one can use the
concatenation of ˆS and (xi)n
i=1 for the unsupervised loss. Accord-
ingly, ˆS and S are independent of each other. Define ˆRm(F) to be
95

V. Verma, K. Kawaguchi, A. Lamb et al.
Neural Networks 145 (2022) 90–106
Fig. 3. Numerical validation of the theoretical prediction that ICT performs well when the confidence value 1
n
n
i=1 | 1
2 −fθ (ui)| is high, because that is when ICT acts
as a regularizer on directional derivatives of all orders. Each line in each subplot shows the decision boundary of the predictor fθ (i.e., {u : fθ (u)} =
1
2 ) after each
update of 1, 10, 100, and 1000. Best viewed in colors in the printed version.
the empirical Rademacher complexity of the set of functions F,
whereas Rm(F) is the Rademacher complexity of the set F. We
adopt the standard convention that mina∈∅Ψ (a) = ∞for any
function Ψ with the empty set ∅. We define the kth order tensor
∂kfθ(u) ∈Rd×d×···×d by
∂kfθ(u)t1t2···tk =
∂k
∂ut1∂ut2 · · · ∂utk
fθ(u).
(5)
For example, ∂1fθ(u) is the gradient of fθ evaluated at u, and
∂2fθ(u) is the Hessian of fθ evaluated at u. For any kth order
tensor ∂kfθ(b) ∈Rd×d×···×d, we define the vectorization of the
tensor by vec[∂kfθ(b)] ∈Rdk. For an vector a ∈Rd, we define
a⊗k = a ⊗a ⊗· · · ⊗a ∈Rdk where ⊗represents the Kronecker
product. The following lemma proves that the Euclidean norm of
the Kronecker products a⊗k is the Euclidean norm of the vector a
to the kth power.
Lemma 1.
Let d ∈N+ and a ∈Rd. Then, for any k ∈N+,
∥a⊗k∥2 = ∥a∥k
2
Proof. We prove this statement by induction over k ∈N+. For
the base case with k = 1,
∥a⊗1∥2 = ∥a∥2 = ∥a∥1
2,
as desired. For the inductive step, we show the statement to hold
for k + 1:
∥a⊗k+1∥2
2 = ∥a⊗k ⊗a∥2
2 =

⎡
⎢⎣
(a⊗k)1a
...
(a⊗k)dka
⎤
⎥⎦

2
2
=
dk

i=1
((a⊗k)i)2∥a∥2
2
= ∥a∥2
2
dk

i=1
((a⊗k)i)2
= ∥a∥2
2∥a⊗k∥2
2.
Here, the inductive hypothesis of ∥a⊗k∥2 = ∥a∥k
2 implies that
∥a⊗k∥2
2 = ∥a∥2k
2 , and thus
∥a⊗k+1∥2
2 = ∥a∥2
2∥a∥2k
2 = ∥a∥
2(k+1)
2
.
This implies that
∥a⊗k+1∥2 = ∥a∥k+1
2
,
which completes the inductive step.
□
4.2. Understanding ICT as regularizer on higher-order derivatives
Using the real analyticity and the Kronecker product, we show
how ICT can act as a regularizer on higher-order derivatives. More
concretely, Theorem 1 states that for any K ∈N+, the ICT loss can
be written as
ℓ(fθ(Mixλ(u, u′)), Mixλ(fθ(u), fθ(u′)))
(6)
=
 K

k=2
(ˆλ −ˆλk)
k!
vec[∂kfθ(u)]⊤(u′ −u)⊗k + O(
u′ −u
K
2 )
2
,
where O(
u′ −u
K
2 ) →0 as K →∞if we normalize the input
so that ∥u′ −u∥2 < 1. Here, Theorem 1 holds for any u, u′ ∈Rd:
for example, we can replace u by any ui or xi, where xi represents
the input part of the labeled dataset ((xi, yi))n
i=1.
Theorem 1.
Let u, u′ ∈Rd and fθ be real analytic, and define
Δ = u′ −u. Then, for any K ∈N+, there exists a pair (ζ, ζ ′) ∈
[0, ˆλ] × [0, 1] such that
ℓ(fθ(Mixλ(u, u′)), Mixλ(fθ(u), fθ(u′)))
=
 K

k=2
(ˆλ −ˆλk)
k!
vec[∂kfθ(u)]⊤Δ⊗k + Eθ(u, u′, λ)
2
,
(7)
where ˆλ = 1 −λ, Eθ(u, u′, λ) = O(∥Δ∥K
2 ) and
Eθ(u, u′, λ) = 1
K!
((1 −ζ ′)K vec[∂K+1fθ(u + ζ ′Δ)]
−ˆλ(ˆλ −ζ)K vec[∂K+1fθ(u + ζΔ)])⊤Δ⊗K.
Proof.
Let (u, u′) be an arbitrary pair of unlabeled data points.
We define the function ϕ by
ϕ(a) = fθ(u + a(u′ −u)).
Since fθ is real analytic and a →u+a(u′−u) is real analytic, their
composition ϕ is also real analytic. We first observe that
Mixλ(a, b) = λa+(1−λ)b = a+(1−λ)(b−a) = a+ ˆλ(b−a). (8)
Using Eq. (8),
fθ(Mixλ(u, u′)) = fθ(u + ˆλ(u′ −u)) = ϕ(ˆλ).
96

V. Verma, K. Kawaguchi, A. Lamb et al.
Neural Networks 145 (2022) 90–106
Using Taylor’s theorem with the Cauchy remainder, we have the
following: for any K ∈N+, there exists ζ ∈[0, ˆλ] such that
fθ(Mixλ(u, u′)) = ϕ(ˆλ) = ϕ(0) +
K

k=1
ˆλk
k! ϕ(k)(0)
+
ˆλ(ˆλ −ζ)K
K!
ϕ(K+1)(ζ),
where ϕ(k) is the kth order derivative of ϕ. Since ϕ(0) = fθ(u), this
implies that
fθ(Mixλ(u, u′)) = fθ(u) +
K

k=1
ˆλk
k! ϕ(k)(0) +
ˆλ(ˆλ −ζ)K
K!
ϕ(K+1)(ζ). (9)
On the other hand, using Eq. (8),
Mixλ(fθ(u), fθ(u′)) = fθ(u) + ˆλ(fθ(u′) −fθ(u)).
(10)
Using Taylor’s theorem with the Cauchy remainder, we have the
following: for any K ∈N+, there exists ζ ′ ∈[0, 1] such that
fθ(u′) = ϕ(1) = ϕ(0) +
K

k=1
1
k!ϕ(k)(0) + (1 −ζ ′)K
K!
ϕ(K+1)(ζ ′).
Since ϕ(0) = fθ(u), plugging this formula of fθ(u′) into Eq. (10)
yields
Mixλ(fθ(u), fθ(u′)) = fθ(u) + ˆλ
 K

k=1
1
k!ϕ(k)(0) + (1 −ζ ′)K
K!
ϕ(K+1)(ζ ′)

.
(11)
Using Eqs. (9) and (11),
ℓ(fθ(Mixλ(u, u′)), Mixλ(fθ(u), fθ(u′)))
=
 K

k=1
ˆλk
k! ϕ(k)(0) +
ˆλ(ˆλ −ζ)K
K!
ϕ(K+1)(ζ)
−ˆλ
 K

k=1
1
k!ϕ(k)(0) + (1 −ζ ′)K
K!
ϕ(K+1)(ζ ′)
2
=
 K

k=1
(ˆλk −ˆλ)
k!
ϕ(k)(0) + 1
K!
(ˆλ(ˆλ −ζ)Kϕ(K+1)(ζ)
−(1 −ζ ′)Kϕ(K+1)(ζ ′))
2
Since ˆλk −ˆλ = 0 when k = 1, this implies that
ℓ(fθ(Mixλ(u, u′)), Mixλ(fθ(u), fθ(u′)))
(12)
=
 K

k=2
(ˆλk −ˆλ)
k!
ϕ(k)(0) + 1
K!
(ˆλ(ˆλ −ζ)Kϕ(K+1)(ζ)
−(1 −ζ ′)Kϕ(K+1)(ζ ′))
2
.
We now derive the formula of ϕ(k)(a). By the chain rule, with
Δ = u′ −u and b = u + aΔ, we have that
ϕ(1)(a) = ∂fθ(u + aΔ)
∂a
=
d

t1=1
∂fθ(b)
∂bt1
∂bt1
∂a =
d

t1=1
∂fθ(b)
∂bt1
Δt1
ϕ(2)(a) = ∂
∂a
∂fθ(u + aΔ)
∂a
=
d

t1=1
∂
∂a
∂fθ(b)
∂bt1
Δt1
=
d

t1=1
d

t2=1
∂2fθ(b)
∂bt1∂bt2
Δt2Δt1
Based on this process, we consider the following formula of ϕ(k)(a)
as a candidate to be proven by induction over k ∈N+:
d

t1=1
d

t2=1
· · ·
d

tk=1
∂kfθ(b)
∂bt1∂bt2 · · · ∂btk
Δt1Δt2 · · · Δtk.
For the base case, we have already shown that ϕ(1)(a) = d
t1=1
∂fθ (b)
∂bt1 Δt1 as desired. For the inductive step, by using the inductive
hypothesis,
ϕ(k+1)(a) = ∂
∂a
d

t1=1
d

t2=1
· · ·
d

tk=1
∂kfθ(b)
∂bt1∂bt2 · · · ∂btk
Δt1Δt2 · · · Δtk
=
d

t1=1
d

t2=1
· · ·
d

tk=1
d

tk+1=1
∂k+1fθ(b)
∂bt1∂bt2 · · · ∂btk∂btk+1
Δt1Δt2 · · · ΔtkΔtk+1
as desired. Therefore, we have proven that for any k ∈N+,
ϕ(k)(a) =
d

t1=1
d

t2=1
· · ·
d

tk=1
∂kfθ(b)
∂bt1∂bt2 · · · ∂btk
Δt1Δt2 · · · Δtk.
(13)
Then, by using the vectorization of the tensor vec[∂kfθ(b)] ∈Rdk,
we can rewrite Eq. (13) as
ϕ(k)(a) = vec[∂kfθ(u + aΔ)]⊤Δ⊗k,
(14)
where Δ⊗k = Δ⊗Δ⊗· · ·⊗Δ ∈Rdk and Δ = u′−u. By combining
Eqs. (12) and (14),
ℓ(fθ(Mixλ(u, u′)), Mixλ(fθ(u), fθ(u′)))
=
 K

k=2
(ˆλk −ˆλ)
k!
ϕ(k)(0) + 1
K!
(ˆλ(ˆλ −ζ)Kϕ(K+1)(ζ)
−(1 −ζ ′)Kϕ(K+1)(ζ ′))
2
=
 K

k=2
(ˆλ −ˆλk)
k!
vec[∂kfθ(u)]⊤Δ⊗k + Eθ(u, u′, λ)
2
By using the Cauchy–Schwarz inequality,
|Eθ(u, u′, λ)|
=

1
K!
((1 −ζ ′)K vec[∂K+1fθ(u + ζ ′Δ)]
−ˆλ(ˆλ −ζ)K vec[∂K+1fθ(u + ζΔ)])⊤Δ⊗K

≤
Δ⊗K
2
K!

((1 −ζ ′)K vec[∂K+1fθ(u + ζ ′Δ)]
−ˆλ(ˆλ −ζ)K vec[∂K+1fθ(u + ζΔ)])


2
= ||Δ||K
2
K!

((1 −ζ ′)K vec[∂K+1fθ(u + ζ ′Δ)]
−ˆλ(ˆλ −ζ)K vec[∂K+1fθ(u + ζΔ)])


2
where the last line follows from Lemma 1.
□
Theorem 1 shows that ICT acts as a regularizer on derivatives
of all orders when the confidence value 1
n
n
i=1 | 1
2 −fθ(ui)| of the
prediction on unlabeled points (ui)n
i=1 is high.1 This is because
1 Here, the confidence value at an unlabeled point ui is defined by | 1
2 −fθ (ui)|,
because fθ (ui) ∈(0, 1) is the output of the sigmoid function at the last layer of
97

V. Verma, K. Kawaguchi, A. Lamb et al.
Neural Networks 145 (2022) 90–106
increasing confidence in ICT tends to decrease the norm of the
first-order derivatives due to the following observation. By the
chain rule, the first-order derivatives can be written as
∂fθ(u) = ∂σ(hθ(u))∂hθ(u).
(15)
Moreover, since ∂σ(a) =
e−a
(1+e−a)2 =
ea
(1+ea)2 and ∂σ(hθ(u)) ≥0,
we have
∥∂fθ(u)∥= |∂σ(hθ(u))|∥∂hθ(u)∥= ∂σ(hθ(u))∥∂hθ(u)∥
= ∂σ(|hθ(u)|)∥∂hθ(u)∥.
(16)
Here, ∂σ(|hθ(u)|) is maximized when |hθ(u)|=
0 and expo-
nentially decreases towards 0 as |hθ(u)| increases or equiva-
lently as the confidence of the prediction increases. In other
words, ∂σ(|hθ(u)|) is maximized when we have lowest confi-
dence (i.e., fθ(u) = 1/2), and ∂σ(|hθ(u)|) exponentially decreases
towards 0 as we increase the confidence (i.e., as fθ(u) moves to-
wards 0 or 1 from 1/2). Therefore, as long as ∥∂hθ(u)∥is bounded
(or increase slower than the exponential rate), increasing the
confidence of the prediction can implicitly minimize the norm
of the first-order derivative ∥∂fθ(u)∥. Here, the weight decay
also tends to bound ∥∂hθ(u)∥since ∂hθ(u) contains the products
of the weight matrices for the deep networks by the chain
rule.
Theorem 1 along this observation suggests that ICT works
well when the confidence on unlabeled data is high (which can be
enforced by using pseudo-labels), because that is when ICT acts as a
regularizer on derivatives of all orders. To confirm this theoretical
prediction, we conducted numerical simulations with the ‘‘two
moons’’ dataset by intentionally decreasing confidence values
on the unlabeled data points. Fig. 3 shows the results of this
experiment with the high confidence case and the low confidence
case. Here, the confidence value is defined by 1
n
n
i=1 | 1
2 −fθ(ui)|
for the unlabeled dataset (ui)n
i=1, which is intentionally reduced in
Fig. 3(b). We used the same settings as those in Fig. 1, except that
we use the softplus activation function φ(z) = ln(1 + exp(κz))/κ
with κ = 100 and we did not use mixup for the supervised loss,
in order to understand the essential mechanism of ICT (whereas
we used mixup for the supervised loss in Fig. 1). As can be seen
in Fig. 3, the numerical results are consistent with our theoretical
prediction. The qualitatively same behaviors were also observed
with the ReLU activation as shown in Appendix B.3.
4.3. On overfitting
In the previous subsection, we have shown that ICT acts as
a regularizer on the derivatives of all orders at unlabeled data
points. In this subsection, we provide a theoretical explanation re-
garding how regularizing the derivatives of all orders at unlabeled
points help reducing overfitting at labeled points.
We first recall the important lemma, Lemma 2, that bounds a
possible
degree
of
overfitting
at
labeled
points
with
the
Rademacher complexity of a hypothesis space (Bartlett & Mendel-
son, 2002; Mohri et al., 2012). Since our notion of a hypothesis
space differs from standard ones, we include a proof for the sake
of completeness. The proof is based on an argument in Bartlett
and Mendelson (2002). A key observation in Lemma 2 is that we
can define a hypothesis space based on the unlabeled dataset
ˆS = (ui)n
i=1, which is to be used later to relate the regulariza-
tion at unlabeled points to the degree of overfitting at labeled
points.
a deep neural network. With fθ (ui) =
1
2 , we have the minimum confidence,
where the confidence value | 1
2 −fθ (ui)| is zero as desired. The confidence value
is maximized towards 0.5 when we let fθ (ui) →0 or 1.
Lemma 2.
Let FˆS be a set of maps x →f (x) that depends on
an unlabeled dataset ˆS. Let q →ℓ(q, y) be a C-uniformly bounded
function for any q ∈{f (x) : f ∈FˆS, x ∈X} and y ∈Y. Then, for
any δ > 0, with probability at least 1 −δ over an i.i.d. draw of m
samples S = ((xi, yi))m
i=1, the following holds: for all maps f ∈FˆS,
Ex,y[ℓ(f (x), y)] ≤1
m
m

i=1
ℓ(f (xi), yi) + 2Rm(ℓ◦FˆS) + C

ln(1/δ)
2m
.
Proof. Define
ϕ(S) = sup
f ∈FˆS
Ex,y[ℓ(f (x), y)] −1
m
m

i=1
ℓ(f (xi), yi).
To apply McDiarmid’s inequality to ϕ(S), we compute an up-
per bound on |ϕ(S) −ϕ(S′)| where S = ((xi, yi))m
i=1 and S′ =
((x′
i, y′
i))m
i=1 are two labeled datasets differing by exactly one point
of an arbitrary index i0; i.e., Si = S′
i for all i ̸= i0 and Si0 ̸= S′
i0.
Then,
ϕ(S′) −ϕ(S) ≤sup
f ∈FˆS
ℓ(f (xi0), yi0) −ℓ(f(x′
i0), y′
i0)
m
≤C
m,
where we used that fact that ˆS and S are independent. Similarly,
ϕ(S) −ϕ(S′) ≤C
m. Notice that these steps fail if FˆS depends on S.
Thus, by McDiarmid’s inequality, for any δ > 0, with probabil-
ity at least 1 −δ,
ϕ(S) ≤ES[ϕ(S)] + C

ln(1/δ)
2m
.
Moreover,
ES[ϕ(S)] = ES

sup
f ∈FˆS
ES′

1
m
m

i=1
ℓ(f(x′
i), y′
i)

−1
m
m

i=1
ℓ(f (xi), yi)

≤ES,S′

sup
f ∈FˆS
1
m
m

i=1
(ℓ(f(x′
i), y′
i) −ℓ(f (xi), yi)

≤Eξ,S,S′

sup
f ∈FˆS
1
m
m

i=1
ξi(ℓ(f(x′
i), y′
i) −ℓ(f (xi), yi))

≤2Eξ,S

sup
f ∈FˆS
1
m
m

i=1
ξiℓ(f (xi), yi))

= 2Rm(ℓ◦FˆS)
where the first line follows the definitions of each term, the
second line uses the Jensen’s inequality and the convexity of the
supremum function, and the third line follows that for each ξi ∈
{−1, +1}, the distribution of each term ξi(ℓ(f(x′
i), y′
i)−ℓ(f (xi), yi))
is the distribution of (ℓ(f(x′
i), y′
i) −ℓ(f (xi), yi)) since ¯S and ¯S′ are
drawn i.i.d. with the same distribution. The fourth line uses the
subadditivity of the supremum function.
□
Whereas Lemma 2 is applicable for a general class of loss
functions, we now recall a concrete version of Lemma 2 for binary
classification. We can write the standard 0–1 loss (i.e., classifica-
tion error) for binary classification by
ℓ01(f (x), y) = 1{y = 1}1{f (x) ≤1/2} + 1{y = −1}1{f (x) ≥1/2}
= 1{y(2f (x) −1) ≤0}.
(17)
For any y ∈{−1, +1}, we define the margin loss ℓρ(f (x), y) =
¯ℓρ(y(2f (x) −1)) as follows:
¯ℓρ(a) =
⎧
⎨
⎩
0
if a ≥ρ
1 −a/ρ
if 0 ≤a ≤ρ
1
if a ≤0.
98

V. Verma, K. Kawaguchi, A. Lamb et al.
Neural Networks 145 (2022) 90–106
Note that for any ρ > 0, the margin loss ℓρ(yf (x)) is an upper
bound on the 0–1 loss: i.e., ℓρ(f (x), y) ≥ℓ01(f (x), y). We can
instantiate Lemma 2 for this concrete choice of the loss functions
by using the arguments in Mohri et al. (2012):
Lemma 3.
Let FˆS be a set of maps x →f (x) that depends on an
unlabeled dataset ˆS. Fix ρ > 0. Then, for any δ > 0, with probability
at least 1 −δ over an i.i.d. draw of m samples ((xi, yi))m
i=1, each of
the following holds: for all maps f ∈FˆS,
Ex,y[ℓ01(f (x), y)] ≤1
m
m

i=1
ℓρ(f (xi), yi) + 2ρ−1Rm(FˆS) +

ln(1/δ)
2m
, and
Ex,y[ℓ01(f (x), y)] ≤1
m
m

i=1
ℓρ(f (xi), yi) + 2ρ−1 ˆRm(FˆS) + 3

ln(2/δ)
2m
.
Proof.
By combining Lemma 2 and the fact that ℓρ(f (x), y) ≥
ℓ01(f (x), y) and ℓρ(f (x), y) ≤1, we have that for any δ > 0, with
probability at least 1 −δ,
Ex,y[ℓ01(f (x), y)] ≤Ex,y[ℓρ(f (x), y)] ≤1
m
m

i=1
ℓρ(f (xi), yi)
+ 2Rm(ℓρ ◦FˆS) +

ln(1/δ)
2m
.
Using Lemma 5 in Appendix A and the fact that ℓρ(f (x), y) is
1/ρ-Lipschitz,
Ex,y[ℓ01(f (x), y)] ≤1
m
m

i=1
ℓρ(f (xi), yi)+ 2ρ−1Rm(FˆS)+

ln(1/δ)
2m
.
This proves the first statement. For the second statement, we
replace δ by δ/2, use Lemma 4 in Appendix A, and take a union
bound, yielding that with probability at least 1−δ/2−δ/2 = 1−δ,
Ex,y[ℓ01(f (x), y)] ≤1
m
m

i=1
ℓρ(f (xi), yi) + 2ρ−1Rm(FˆS) +

ln(2/δ)
2m
≤1
m
m

i=1
ℓρ(f (xi), yi) + 2ρ−1 ˆRm(FˆS) + 3

ln(2/δ)
2m
This proves the second statement.
□
Given these lemmas, we are ready to investigate how the over-
fitting at labeled points can be mitigated by using the hypothesis
space FˆS,τ with the regularization on the derivatives of all orders
at unlabeled points:
FˆS,τ = {x →f (x) : ∀¯S ⊆ˆS, ∀(u, k) ∈¯S × N+, ∥vec[∂kf (u)]∥2 ≤τ¯S},
where τ = {τ¯S ∈R : ¯S ⊆ˆS} and τ¯S measures the norm of the
derivatives of all orders at unlabeled points. For each ¯S ⊆ˆS, we
write ¯S = (u¯S
1, . . . , u¯S
|¯S|). We define RS,¯S = maxx∈S minu∈¯S ∥x−u∥2,
SS,ˆS = {¯S ⊆ˆS : RS,¯S < 1}, and
IS,¯S
t
=

i ∈[m] : t = argmin
t∈{1,...,|¯S|}
∥xi −u
¯S
t ∥2

.
The following theorem shows that regularizing τ¯S – the norm
of the derivatives of all orders at unlabeled points – can help
reducing overfitting at labeled points:
Theorem 2.
Fix FˆS,τ and ρ > 0. Then, for any δ > 0, with
probability at least 1−δ over an i.i.d. draw of m samples ((xi, yi))m
i=1,
each of the following holds: for all maps f ∈FˆS,τ ,
Ex,y[ℓ01(f (x), y)]
≤1
m
m

i=1
ℓρ(f (xi), yi) + 2ρ−1ES
×
⎡
⎣min
¯S∈SS,ˆS

1 +
τ¯SRS,¯S
1 −RS,¯S
 |¯S|
t=1

|IS,¯S
t
|
m
⎤
⎦+

ln(1/δ)
2m
,
and
Ex,y[ℓ01(f (x), y)]
≤1
m
m

i=1
ℓρ(f (xi), yi) + 2ρ−1 min
¯S∈SS,ˆS

1 +
τ¯SRS,¯S
1 −RS,¯S
 |¯S|
t=1

|IS,¯S
t
|
m
+ 3

ln(2/δ)
2m
.
Proof. Let ¯S ⊆ˆS be arbitrary such that RS,¯S < 1. Using Lemma 1
and the Cauchy–Schwarz inequality,
K

k=1
vec[∂kf (u)]⊤(x −u)⊗k ≤
K

k=1
∥vec[∂kf (u)]∥2∥x −u∥k
2.
Thus, for any (f , u) ∈FˆS,τ × ¯S,
lim
K→∞
K

k=1
∥vec[∂kf (u)]∥2∥x −u∥k
2 ≤τ¯S lim
K→∞
K

k=1
∥x −u∥k
2.
Since limK→∞
K
k=1 ∥x −u∥k
2 is a geometric series, if ∥x −u∥2 ≤
R < 1, then
lim
K→∞
K

k=1
∥x −u∥k
2 ≤
1
1 −R −1.
Therefore, combining above inequalities, for any (f , u) ∈FˆS,τ × ¯S,
if ∥x −u∥2 ≤R < 1,
∞

k=1
vec[∂kf (u)]⊤(x −u)⊗k ≤
τ¯S
1 −R −1.
(18)
Let ˆt(x) = argmint∈{1,...|¯S|} ∥x −u¯S
t ∥2. Then,
ˆRm(FˆS,τ ) = Eξ sup
f ∈FˆS,τ
1
m
m

i=1
ξif (xi) = Eξ sup
f ∈FˆS,τ
1
m
m

i=1
ξif (u
¯S
ˆt(xi)
+ (xi −u
¯S
ˆt(xi))).
Thus, Eq. (18) implies that
ˆRm(FˆS,τ ) = Eξ sup
f ∈FˆS,τ
1
m
m

i=1
ξif (u
¯S
ˆt(xi) + (x −u
¯S
ˆt(xi)))
= Eξ sup
f ∈FˆS,τ
1
m
m

i=1
ξi

f (u
¯S
ˆt(xi)) +
∞

k=1
vec[∂kf (u
¯S
ˆt(xi))]⊤(xi −u
¯S
ˆt(xi))⊗k

where the series converges based on Eq. (18) since minu∈¯S
∥x −u∥2 < 1 for all x ∈Sx. Since IS,¯S
t
= {i ∈[m] : t =
argmint∈{1,...,|¯S|} ∥xi −u¯S
t ∥2} = {i ∈[m] : t = ˆt(xi)}, we can further
rewrite
ˆRm(FˆS,τ )
= Eξ sup
f ∈FˆS,τ
1
m
m

i=1
ξi

f (u
¯S
ˆt(xi)) +
∞

k=1
vec[∂kf (u
¯S
ˆt(xi))]⊤(xi −u
¯S
ˆt(xi))⊗k

= Eξ sup
f ∈FˆS,τ
1
m
|¯S|

t=1

i∈IS,¯S
t
ξi

f (u
¯S
ˆt(xi)) +
∞

k=1
vec[∂kf (u
¯S
ˆt(xi))]⊤(xi −u
¯S
ˆt(xi))⊗k

99

V. Verma, K. Kawaguchi, A. Lamb et al.
Neural Networks 145 (2022) 90–106
= Eξ sup
f ∈FˆS,τ
1
m
|¯S|

t=1

i∈IS,¯S
t
ξi

f (u
¯S
t ) +
∞

k=1
vec[∂kf (u
¯S
t )]⊤(xi −u
¯S
t )⊗k

= 1
m Eξ sup
f ∈FˆS,τ
⎡
⎢⎣
|¯S|

t=1

i∈IS,¯S
t
ξif (u
¯S
t ) +
|¯S|

t=1

i∈IS,¯S
t
ξi
∞

k=1
vec[∂kf (u
¯S
t )]⊤(xi −u
¯S
t )⊗k
⎤
⎥⎦
≤1
m Eξ
⎡
⎢⎣
|¯S|

t=1
sup
f ∈FˆS,τ
|f (u
¯S
t )|


i∈IS,¯S
t
ξi

+
|¯S|

t=1
∞

k=1
sup
f ∈FˆS,τ
∥vec[∂kf (u
¯S
t )]∥2


i∈IS,¯S
t
ξi(xi −ut)⊗k

2
⎤
⎥⎦
Using the definition of the function class FˆS,τ and the fact that
|f (u¯S
t )| ≤1, we have
ˆRm(FˆS,τ )
≤1
mEξ
⎡
⎢⎣
|¯S|

t=1
sup
f ∈FˆS,τ
|f (u
¯S
t )|


i∈IS,¯S
t
ξi

+
|¯S|

t=1
∞

k=1
sup
f ∈FˆS,τ
∥vec[∂kf (u
¯S
t )]∥2


i∈IS,¯S
t
ξi(xi −u
¯S
t )⊗k

2
⎤
⎥⎦
≤1
mEξ
⎡
⎢⎣
|¯S|

t=1


i∈IS,¯S
t
ξi

+ τ¯S
|¯S|

t=1
∞

k=1


i∈IS,¯S
t
ξi(xi −u
¯S
t )⊗k

2
⎤
⎥⎦
= 1
m
|¯S|

t=1
Eξ






⎛
⎜⎝

i∈IS,¯S
t
ξi
⎞
⎟⎠
2
+ τ¯S
m
|¯S|

t=1
∞

k=1
Eξ


i∈IS,¯S
t
ξi(xi −u
¯S
t )⊗k

2
(19)
where the last line follows the linearity of the expectation. By
using Jensen’s inequality for the concave function,
Eξ






⎛
⎜⎝

i∈IS,¯S
t
ξi
⎞
⎟⎠
2
≤





Eξ
⎛
⎜⎝

i∈IS,¯S
t
ξi
⎞
⎟⎠
2
=



Eξ

i∈IS,¯S
t

j∈IS,¯S
t
ξiξj
=





i∈IS,¯S
t

j∈IS,¯S
t
Eξ[ξiξj]
=





i∈IS,¯S
t
Eξ[ξ 2
i ]
where the last line follows the fact that the Rademacher variables
ξ1, . . . , ξm are independent. Since Eξ[ξ 2
i ] = 1,
Eξ






⎛
⎜⎝

i∈IS,¯S
t
ξi
⎞
⎟⎠
2
≤

|IS,¯S
t
|
(20)
Similarly, by using Jensen’s inequality for the concave function,
Eξ


i∈IS,¯S
t
ξi(xi −u¯S
t )⊗k

2
= Eξ





i∈IS,¯S
t

j∈IS,¯S
t
ξiξj((xi −u¯S
t )⊗k)⊤(xj −u¯S
t )⊗k
≤





i∈IS,¯S
t

j∈IS,¯S
t
Eξ[ξiξj]((xi −u¯S
t )⊗k)⊤(xj −u¯S
t )⊗k
≤





i∈IS,¯S
t
Eξ[ξ 2
i ]∥(xi −u¯S
t )⊗k∥2
2
where the last line follows the fact that Rademacher variables
ξ1, . . . , ξm are independent. Since Eξ[ξ 2
i ] = 1, using Lemma 1,
Eξ


i∈IS,¯S
t
ξi(xi −u
¯S
t )⊗k

2
≤





i∈IS,¯S
t
∥(xi −u¯S
t )⊗k∥2
2
=





i∈IS,¯S
t
(∥xi −u¯S
t ∥2)2k
≤Rk
S,¯S

|IS,¯S
t
|
(21)
Combining inequalities (19)–(21) yields
m ˆRm(FˆS,τ ) ≤
|¯S|

t=1

|IS,¯S
t
| + τ¯S
|¯S|

t=1

|IS,¯S
t
|
∞

k=1
Rk
S,¯S.
Since RS,¯S < 1, the geometric series converges as
m ˆRm(FˆS,τ ) ≤
|¯S|

t=1

|IS,¯S
t
| +
τ¯SRS,¯S
1 −RS,¯S
|¯S|

t=1

|IS,¯S
t
|
=

1 +
τ¯SRS,¯S
1 −RS,¯S
 |¯S|

t=1

|IS,¯S
t
|.
Therefore,
ˆRm(FˆS,τ ) ≤

1 +
τ¯SRS,¯S
1 −RS,¯S
 |¯S|
t=1

|IS,¯S
t
|
m
.
Since ¯S ⊆ˆS was arbitrary such that RS,¯S < 1, this inequality
holds for any ¯S such that RS,¯S < 1, yielding that for any S =
((x1, y1), . . . , (xm, ym)),
ˆRm(FˆS,τ ) = Eξ sup
f ∈FˆS,τ
1
m
m

i=1
ξif (xi) ≤min
¯S∈SS,ˆS

1 +
τ¯SRS,¯S
1 −RS,¯S

×
|¯S|
t=1

|IS,¯S
t
|
m
.
(22)
By combining Lemma 3 and inequality (22) and taking expecta-
tion over S, we obtain the statement of this theorem.
□
Since min¯S∈SS,ˆS g(¯S) ≤g(¯S′) for any ¯S′ ∈SS,ˆS and any function
g, Theorem 2 implies that with probability at least 1 −δ, for any
¯S ∈SS,ˆS,
Ex,y[ℓ01(f (x), y)]
≤1
m
m

i=1
ℓρ(f (xi), yi) + 2ρ−1

1 +
τ¯SRS,¯S
1 −RS,¯S
 |¯S|
t=1

|IS,¯S
t
|
m
+ 3

ln(2/δ)
2m
.
To further understand Theorem 2, let us consider a simple case
where the input space is normalized such that the maximum
distance between some unlabeled point uc ∈ˆS and the labeled
points S is bounded as maxx∈S ∥x −uc∥2 < 1/2. Then, by setting
100

V. Verma, K. Kawaguchi, A. Lamb et al.
Neural Networks 145 (2022) 90–106
¯S = {uc}, Theorem 2 implies that with probability at least 1 −δ,
Ex,y[ℓ01(f (x), y)] ≤1
m
m

i=1
ℓρ(f (xi), yi) + 2ρ−1(1 + τ¯S)
√
m
+ 3

ln(2/δ)
2m
,
(23)
since RS,¯S = 1
2, |¯S| = 1, and |IS,¯S
t
| = m for this particular choice of
¯S = {uc}. In Eq. (23), we can clearly see that the degree of possible
overfitting at labeled points is in the order of
1+τ¯S
√
m , which can be
reduced by regularizing τ¯S — the norm of the derivatives of all
orders at unlabeled points.
In Eq. (23) (and Theorem 2), there is a tradeoff between reduc-
ing τ¯S and minimizing the classification loss
1
m
m
i=1 ℓρ(f (xi), yi).
In an extreme case, if we minimize τ¯S to zero, then we can
minimize the order of the overfitting from
1+τ¯S
√
m
to
1
√
m but the
model f becomes a constant function which cannot minimize the
classification loss
1
m
m
i=1 ℓρ(f (xi), yi) in typical practical applica-
tions.
Whereas this tradeoff is natural in the regime of large τ¯S, it is
not desirable to require f to be a constant function in order to
obtain a global minimum value of the degree of the overfitting in
the regime of small τ¯S. Accordingly, we now prove an additional
theorem to avoid this tradeoff in the regime of small τ¯S. More
concretely, Theorem 3 shows that we can reduce the order of
the overfitting from
1+τ¯S
√
m
to
1
√
m without requiring τ¯S = 0 in
the regime where τ¯S is smaller than some confidence value at
unlabeled points as τ¯S < C¯S
(1−RS,¯S)
RS,¯S
, where C¯S measures the con-
fidence values at unlabeled points. Thus, Theorem 3 also shows
the benefit of increasing confidence values at unlabeled points,
which is consistent with our observation in Fig. 3.
Theorem 3. Fix FˆS,τ . Define C¯S = inff ∈FˆS,τ minu∈¯S |f (u) −1/2| and
S∗
S,ˆS = {¯S ⊆ˆS : RS,¯S < 1, τ¯S <
C¯S(1−RS,¯S)
RS,¯S
}. Then, for any δ > 0, with
probability at least 1−δ over an i.i.d. draw of m samples ((xi, yi))m
i=1,
each of the following holds: for all maps f ∈FˆS,τ ,
Ex,y[ℓ01(f (x), y)] ≤1
m
m

i=1
ℓ01(f (x), y)
+
ES
#
min¯S∈S∗
S,ˆS
|¯S|
t=1

|IS,¯S
t
|
$
m
+

ln(2/δ)
2m
,
Ex,y[ℓ01(f (x), y)] ≤1
m
m

i=1
ℓ01(f (x), y)
+
min¯S∈S∗
S,ˆS
|¯S|
t=1

|IS,¯S
t
|
m
+ 3

ln(1/δ)
2m
.
Proof. Without the loss of generality, let us write y ∈{−1, +1}
(if the original label y is in {0, 1}, then we can define a bijection
to map {0, 1} to {−1, +1}). Define
ς(f (x)) =
%+1
if f (x) −1
2 > 0
−1
if f (x) −1
2 ≤0
Then, we can write the 0–1 loss of the classification as
ℓ01(f (x), y) = 1{ς(f (x)) ̸= y} = 1 −yς(f (x))
2
(24)
By using Lemma 2 with the 0–1 loss, we have that for any δ > 0,
with probability at least 1 −δ, for all f ∈FˆS,τ ,
Ex,y[ℓ01(f (x), y)] ≤1
m
m

i=1
ℓ01(f (x), y)+2Rm(ℓ01◦FˆS,τ )+

ln(2/δ)
2m
.
(25)
By using Lemma 4 and taking a union bound, we have that
with probability at least 1 −δ/2 −δ/2
=
1 −δ, for all
f ∈FˆS,τ ,
Ex,y[ℓ01(f (x), y)] ≤1
m
m

i=1
ℓ01(f (x), y) + 2 ˆRm(ℓ01 ◦FˆS,τ ) + 3

ln(1/δ)
2m
.
(26)
Using Eq. (24),
ˆRm(ℓ01 ◦F¯S) = Eξ sup
f ∈FˆS,τ
1
m
m

i=1
ξiℓ01(f (xi), yi)
= Eξ sup
f ∈FˆS,τ
1
m
m

i=1
ξi
1 −yiς(f (xi))
2
= Eξ sup
f ∈FˆS,τ
1
m
m

i=1
−ξiyiς(f (xi))
2
=
1
2mEξ sup
f ∈FˆS,τ
m

i=1
ξiς(f (xi)).
Let ¯S ⊆ˆS be arbitrary such that RS,¯S < 1 and τ¯S <
C¯S(1−RS,¯S)
RS,¯S
. Let
ˆt(x) = argmint∈{1,...|¯S|} ∥x −u¯S
t ∥2. Then, for any f ∈FˆS,τ , the proof
of Theorem 2 shows that we can write
f (xi) = f (u
¯S
ˆt(xi)) +
∞

k=1
vec[∂kf (u
¯S
ˆt(xi))]⊤(xi −u
¯S
ˆt(xi))⊗k.
Since IS,¯S
t
= {i ∈[m] : t = argmint∈{1,...,|¯S|} ∥xi −u¯S
t ∥2} = {i ∈
[m] : t = ˆt(xi)}, we can rewrite
ˆRm(ℓ01 ◦F¯S) =
1
2mEξ sup
f ∈FˆS,τ
m

i=1
ξiς
×

f (u
¯S
ˆt(xi)) +
∞

k=1
vec[∂kf (u
¯S
ˆt(xi))]⊤(xi −u
¯S
ˆt(xi))⊗k

=
1
2mEξ sup
f ∈FˆS,τ
|¯S|

t=1

i∈IS,¯S
t
ξiς
×

f (u
¯S
t ) +
∞

k=1
vec[∂kf (u
¯S
t )]⊤(xi −u
¯S
t )⊗k

Using Lemma 1,

∞

k=1
vec[∂kf (u
¯S
t )]⊤(xi −u
¯S
t )⊗k
 ≤
∞

k=1
∥vec[∂kf (u
¯S
t )]∥2∥(xi −u
¯S
t )⊗k∥2
≤τ¯S
RS,¯S
1 −RS,¯S
< C¯S
where the last line follows the condition on ¯S that τ¯S <
C¯S(1−RS,¯S)
RS,¯S
(since τ¯S is defined independently of S, this condition does not
101

V. Verma, K. Kawaguchi, A. Lamb et al.
Neural Networks 145 (2022) 90–106
necessarily hold for some ¯S ⊆ˆS. If this does not hold for all ¯S ⊆ˆS,
then the statement holds vacuously with the convention that
mina∈∅Ψ (a) = ∞for any function Ψ ). Since |f (u¯S
t ) −1/2| ≥C¯S
and |∞
k=1 vec[∂kf (u¯S
t )]⊤(xi −u¯S
t )⊗k| < C¯S, we have that
ˆRm(ℓ01 ◦F¯S) =
1
2mEξ sup
f ∈FˆS,τ
|¯S|

t=1

i∈IS,¯S
t
ξiς
×

f (u
¯S
t ) +
∞

k=1
vec[∂kf (u
¯S
t )]⊤(xi −u
¯S
t )⊗k

=
1
2mEξ sup
f ∈FˆS,τ
|¯S|

t=1
ς
&
f (u
¯S
t )
' 
i∈IS,¯S
t
ξi
≤
1
2mEξ
|¯S|

t=1
sup
f ∈FˆS,τ
ς
&
f (u
¯S
t )
'


i∈IS,¯S
t
ξi

≤
1
2m
|¯S|

t=1
Eξ






⎛
⎜⎝

i∈IS,¯S
t
ξi
⎞
⎟⎠
2
By using Jensen’s inequality for the concave function,
Eξ






⎛
⎜⎝

i∈IS,¯S
t
ξi
⎞
⎟⎠
2
≤





Eξ
⎛
⎜⎝

i∈IS,¯S
t
ξi
⎞
⎟⎠
2
=





i∈IS,¯S
t

j∈IS,¯S
t
Eξ[ξiξj]
=





i∈IS,¯S
t
Eξ[ξ 2
i ]
=

|IS,¯S
t
|
Therefore,
ˆRm(ℓ01 ◦F¯S) ≤
1
2m
|¯S|

t=1

|IS,¯S
t
|.
Since ¯S ⊆ˆS was arbitrary such that RS,¯S < 1 and τ¯S <
C¯S(1−RS,¯S)
RS,¯S
,
this inequality holds for any ¯S such that RS,¯S < 1 and τ¯S <
C¯S(1−RS,¯S)
RS,¯S
, yielding
ˆRm(ℓ01 ◦F¯S) ≤
1
2m
min
¯S∈S∗
S,ˆS
|¯S|

t=1

|IS,¯S
t
|.
Taking expectation and combining with Eqs. (25)–(26) yields that
for any δ > 0, with probability at least 1−δ, each of the following
holds for all f ∈FˆS,τ :
Ex,y[ℓ01(f (x), y)] ≤1
m
m

i=1
ℓ01(f (x), y)
+
ES
#
min¯S∈S∗
S,ˆS
|¯S|
t=1

|IS,¯S
t
|
$
m
+

ln(2/δ)
2m
.
Ex,y[ℓ01(f (x), y)] ≤1
m
m

i=1
ℓ01(f (x), y)
+
min¯S∈S∗
S,ˆS
|¯S|
t=1

|IS,¯S
t
|
m
+ 3

ln(1/δ)
2m
.
□
Since min¯S∈S∗
S,ˆS g(¯S) ≤g(¯S′) for any ¯S′ ∈S∗
S,ˆS and any function
g, Theorem 3 implies that with probability at least 1 −δ, for any
¯S = {uc} ∈S∗
S,ˆS,
Ex,y[ℓ01(f (x), y)] ≤1
m
m

i=1
ℓ01(f (x), y) +
1
√
m + 3

ln(1/δ)
2m
,
(27)
as |¯S| = 1 and |IS,¯S
t
| = m for the singleton set ¯S = {uc}.
Therefore, if we increase the confidence at unlabeled points and
regularize the norm of the derivatives of all orders at unlabeled
points, we can reduce overfitting at labeled points: i.e., the clas-
sification error
1
m
m
i=1 ℓ01(f (x), y) at labeled points approaches
Ex,y[ℓ01(f (x), y)] in the rate of O(√
ln(1/δ)/m).
Finally, we remark that our theories are reflecting the fact that
the prediction at unlabeled points could be wrong, although the
confidence can be high. Indeed, in all of our experiments and
theoretical analyses, we do not assume that the pseudo labels at
unlabeled points are correct or of good quality. Instead, our proofs
analyze the possible cases of the bad quality of pseudo-labels and
show the following: a prediction of ICT at an unlabeled point is
likely correct if it has a high confidence at the unlabeled point and
if the unlabeled point is near a correctly classified labeled point.
Intuitively, this is because when the derivatives of all orders are
regularized, the prediction cannot change a lot with respect to the
input. This is one of ways how labeled and unlabeled points are
interacted in our new proof techniques in this paper. We expect
our new proof techniques to be proven useful in future studies of
semi-supervised learning.
In Theorems 2 and 3, we can see that if all unlabeled points
are too far way from the labeled points, then the bounds degrade
linearly in the distance between the labeled and unlabeled points.
This is because the prediction could be more likely inaccurate
as this distance increases. Here, by normalizing the input space,
we can guarantee for this distance to be sufficiently small. As an
example, the discussions after Theorem 2 provide the case where
the input space is normalized such that the maximum distance
between a center unlabeled point uc ∈ˆS and labeled points S is
bounded as maxx∈S ∥x −uc∥2 < 1/2.
More importantly, the distance between the labeled and unla-
beled points that is used in Theorems 2 and 3 tends to decrease as
we increase only the number of unlabeled points. This is because
the distance is defined by RS,ˆS = maxx∈S minu∈ˆS ∥x −u∥2 with
the minimum over all the unlabeled points u ∈ˆS, for example,
with ¯S = ˆS. Therefore, the quality of pseudo labels at unlabeled
points can improve only by increasing the number of unlabeled
points. This theoretical prediction is consistent with our main
experimental results in Section 3 and is further validated via the
additional experiments in Appendix B.2.
5. Related work
This work builds on two threads of research: consistency-
regularization for semi-supervised learning and interpolation-
based regularizers.
Consistency-regularization semi-supervised learning methods
(Athiwaratkun et al., 2019; Laine & Aila, 2017; Luo et al., 2018;
Miyato et al., 2018; Sajjadi et al., 2016; Tarvainen & Valpola, 2017)
encourage that realistic perturbations u + δ of unlabeled samples
u should not change the model predictions fθ(u). Motivated by
the low-density separation assumption (Chapelle et al., 2010), these
methods push the decision boundary towards the low-density
regions of the input space, achieving larger classification mar-
gins. ICT differs from these approaches in two aspects. First, ICT
chooses perturbations in the direction of another randomly cho-
sen unlabeled sample, avoiding expensive gradient computations.
102

V. Verma, K. Kawaguchi, A. Lamb et al.
Neural Networks 145 (2022) 90–106
When interpolating between distant points, the regularization
effect of ICT applies to larger regions of the input space.
Interpolation-based regularizers (Tokozume et al., 2018; Verma
et al., 2018; Zhang et al., 2018) have been recently proposed
for supervised learning, achieving state-of-the-art performances
across a variety of tasks and network architectures. While
Tokozume et al. (2018) and Zhang et al. (2018) proposed to per-
form interpolations in the input space, Verma et al. (2018) pro-
posed
to
perform
interpolation
also
in
the
hidden
space
representations. Furthermore, in the unsupervised learning set-
ting, Berthelot, Raffel et al. (2019) proposes to measure the
realism of latent space interpolations from an autoencoder to
improve its training.
We note that after the publication of an earlier version of
this paper (Verma et al., 2019), the methods in Berthelot et al.
(2020), Berthelot, Carlini et al. (2019) and Sohn et al. (2020)
have achieved state-of-the-art experimental results on bench-
mark datasets and architectures. Similar to Verma et al. (2019),
Berthelot et al. (2020), Berthelot, Carlini et al. (2019) use inter-
polation in the samples and their predicted targets for designing
semi-supervised objective functions. Sohn et al. (2020) combines
consistency regularization with pseudo-labeling (Lee, 2013).
Other works have approached semi-supervised learning from
the perspective of generative models. Some have approached
this from a consistency point of view, such as Lecouat et al.
(2018), who proposed to encourage smooth changes to the pre-
dictions along the data manifold estimated by the generative
model (trained on both labeled and unlabeled samples). Others
have used the discriminator from a trained generative adversarial
network (Goodfellow et al., 2014) as a way of extracting features
for a purely supervised model (Radford et al., 2015). Still, oth-
ers have used trained inference models as a way of extracting
features (Dumoulin et al., 2016).
Our main objective in this paper is to improve generalization,
instead of the causality invariance. While we mathematically an-
alyzed the generalization properties of consistency-regularization
with mixup, Han et al. (2021) recently discussed that intuitively,
consistency-regularization in general might be able to promote
the causality invariance too. To understand this, consider a causal-
ity invariance such that a causality from an input to a class label is
not destroyed under some input perturbation or argumentation.
Then, intuitively, consistency-regularization with this particular
input perturbation or argumentation might be able to approxi-
mately promote the causality invariance. It would be interesting
to mathematically formalize this intuition for ICT to quantify the
degree of causality invariance that can be promoted by ICT in
future work.
6. Conclusion
Machine learning is having a transformative impact on di-
verse areas, yet its application is often limited by the amount
of available labeled data. Progress in semi-supervised learning
techniques holds promise for those applications where labels are
expensive to obtain. In this paper, we have proposed a simple but
efficient semi-supervised learning algorithm, Interpolation Con-
sistency Training (ICT), which has two advantages over previous
approaches to semi-supervised learning. First, it uses almost no
additional computation, as opposed to computing adversarial per-
turbations or training generative models. Second, it outperforms
strong baselines on two benchmark datasets, even without an
extensive hyperparameter tuning. Finally, we have shown how
ICT can act as a regularizer on the derivatives of all orders and
reduce overfitting when confidence values on unlabeled data are
high, which can be achieved by additionally using pseudo-labels
on the unlabeled data. Our theoretical results predict a failure
mode of ICT with low confidence values (without using pseudo-
labels), which was confirmed in the experiments, providing a
practical guidance to use ICT with high confidence values. As for
the future work, extending ICT to interpolations not only at the
input but at hidden representations (Verma et al., 2018) could
improve the performance even further.
Declaration of competing interest
The authors declare that they have no known competing finan-
cial interests or personal relationships that could have appeared
to influence the work reported in this paper.
Acknowledgments
This work was supported by the Academy of Finland Flagship
programme: Finnish Center for Artificial Intelligence (FCAI). We
would also like to acknowledge Compute Canada for providing
computing resources used in this work.
Appendix A. Additional lemmas
The following lemmas are used in the proof of our theo-
rems. Their proofs directly follow from those from previous
works (Bartlett & Mendelson, 2002; Mohri & Medina, 2014; Mohri
et al., 2012).
Lemma 4.
Let FˆS be a set of maps x →f (x) that depends on
an unlabeled dataset ˆS. Let q →ℓ(q, y) be a C-uniformly bounded
function for any q ∈{f (x) : f ∈FˆS, x ∈X} and y ∈Y. Then, for
any δ > 0, with probability at least 1 −δ over an i.i.d. draw of m
samples ((xi, yi))m
i=1, the following holds:
Rm(FˆS) ≤ˆRm(FˆS) + C

log(1/δ)
m
.
Proof.
Since changing one point in S changes ˆRm(FˆS) by at
most C/m, McDiarmid’s inequality implies the statement of this
lemma.
□
Lemma 5 (Talagrand’s Contraction Lemma by Ledoux & Talagrand,
2013; Mohri & Medina, 2014). Let F be a set of functions mapping
X to R. Let Ψ1, . . . , Ψm be μ-Lipschitz functions for some μ > 0.
Then, for any samples S of m points x1, . . . , xm ∈X, the following
inequality holds:
1
mEσ

sup
f ∈F
m

i=1
ξi(Ψi ◦f )(xi)

≤μ
mEσ

sup
f ∈F
m

i=1
ξif (xi)

.
Proof. See the proof of Mohri and Medina (2014, Lemma 8).
□
Appendix B. Additional experiments
B.1. Kullback–Leibler (KL) divergence
Throughout the paper, we use the MSE loss for computing the
consistency loss by following the previous studies (Laine & Aila,
2017; Tarvainen & Valpola, 2017). While we found the MSE loss
to be sufficient for our problems, one may explore the use of the
KL divergence for computing the consistency loss. Figs. 4–5 show
the results of replacing the MSE loss with the KL divergence for
the experiments reported in Fig. 3 (in the main text). Here, the
confidence value is defined by 1
n
n
i=1 | 1
2 −fθ(ui)| with unlabeled
data points (ui)n
i=1. Each line in each subplot shows the decision
103

V. Verma, K. Kawaguchi, A. Lamb et al.
Neural Networks 145 (2022) 90–106
Fig. 4. Decision boundaries for ICT with the KL divergence and the softplus activation.
Fig. 5. Decision boundaries for ICT with the KL divergence and the ReLU activation.
Fig. 6. The effect of few data points on ICT: (a) 1 labeled point per class and 100 unlabeled points, (b) 5 labeled points per class and 10 unlabeled points, and (c)
5 labeled points per class and 100 unlabeled points.
boundary of the predictor fθ (i.e., {u : fθ(u)} =
1
2) after each
update of 1, 10, 100, and 1000. As we can see in the figures, with
the KL divergence (instead of the MSE loss), ICT still works as
expected and the numerical results are still consistent with our
theoretical prediction regarding the importance of the confidence
value. Given these preliminary results, it might be interesting to
study the use of the KL divergence in ICT more comprehensively
in future work.
B.2. The effect of few data points
As ICT is designed for semi-supervised learning instead of one-
shot learning, we now examine how ICT could fail in the case
of too few data points in Fig. 6. Each line in each subplot shows
the decision boundary of the predictor fθ (i.e., {u : fθ(u)} =
1
2)
after each update of 1, 10, 100, 1000, and 2000. The experiments
were conducted with the softplus activation function φ(z) =
ln(1 + exp(κz))/κ with κ = 100.
As can be seen in the figure, if we have only one labeled point
per class (Fig. 6a) or if unlabeled points are too few (Fig. 6b), then
ICT would not learn the correct decision boundary. However, by
increasing only the number of unlabeled points from 10 (Fig. 6b)
to 100 (Fig. 6c), ICT starts learning a good decision boundary
(Fig. 6c). The decision boundary is further refined by increasing
only the unlabeled data points in Figs. 1 and 3 in the main text.
Along with our result in Table 2 where ICT performed well only
with 250 labeled points for SVHN, these observations further
demonstrate the fact that ICT works well with few labeled data
points (but not too few such as one per class) as long as we
104

V. Verma, K. Kawaguchi, A. Lamb et al.
Neural Networks 145 (2022) 90–106
Fig. 7. Decision boundaries for ICT with the ReLU activation (and the MSE loss).
can increase the number of the unlabeled data points. This is
consistent with our theoretical prediction.
B.3. Using the ReLU activation in the experiments of Fig. 3
In Fig. 3, we used the softplus activation function φ(z) =
ln(1 + exp(κz))/κ with κ = 100. As an additional experiment,
we replaced the softplus activation by the ReLU activation and
reported the results in Fig. 7. As can be seen in the figure, the
numerical results with the ReLU activation are also consistent
with our theoretical prediction. This is expected because the
ReLU function is approximated arbitrarily well with the softplus
activation function by varying the value of κ ∈(0, ∞), and our
theoretical results hold for any κ ∈(0, ∞).
References
Athiwaratkun, B., Finzi, M., Izmailov, P., & Wilson, A. G. (2019). There are
many consistent explanations of unlabeled data: Why you should average. In
International conference on learning representations. URL: https://openreview.
net/forum?id=rkgKBhA5Y7.
Bartlett, P. L., & Mendelson, S. (2002). Rademacher and Gaussian complexities:
Risk bounds and structural results. Journal of Machine Learning Research,
3(Nov), 463–482.
Berthelot, D., Carlini, N., Cubuk, E. D., Kurakin, A., Sohn, K., Zhang, H., &
Raffel, C. (2020). ReMixMatch: Semi-supervised learning with distribution
matching and augmentation anchoring. In International conference on learning
representations. URL: https://openreview.net/forum?id=HklkeR4KPB.
Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., Oliver, A., & Raffel, C. A.
(2019). MixMatch: A holistic approach to semi-supervised learning. Advances
in Neural Information Processing Systems, 32.
Berthelot, D., Raffel, C., Roy, A., & Goodfellow, I. (2019). Understanding and
improving interpolation in autoencoders via an adversarial regularizer. In
International conference on learning representations. URL: https://openreview.
net/forum?id=S1fQSiCcYm.
Chapelle, O., Schlkopf, B., & Zien, A. (2010). Semi-supervised learning (1st ed.).
The MIT Press.
Clanuwat, T., Bober-Irizar, M., Kitamoto, A., Lamb, A., Yamamoto, K., & Ha, D.
(2018). Deep learning for classical Japanese literature. arXiv preprint arXiv:
1812.01718.
Dugas, C., Bengio, Y., Bélisle, F., Nadeau, C., & Garcia, R. (2001). Incorporating
second-order functional knowledge for better option pricing. Advances in
Neural Information Processing Systems, 472–478.
Dumoulin, V., Belghazi, I., Poole, B., Mastropietro, O., Lamb, A., Arjovsky, M., &
Courville, A. (2016). Adversarially learned inference. arXiv preprint arXiv:
1606.00704.
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,
Courville, A., & Bengio, Y. (2014). Generative adversarial nets. In Advances in
neural information processing systems (pp. 2672–2680).
Han, T., Tu, W.-W., & Li, Y.-F. (2021). Explanation consistency training: Fa-
cilitating consistency-based semi-supervised learning with interpretability.
In Proceedings of the AAAI conference on artificial intelligence, Vol. 35 (pp.
7639–7646).
Kawaguchi, K., & Sun, Q. (2021). A recipe for global convergence guarantee
in deep neural networks. In Proceedings of the AAAI conference on artificial
intelligence.
Laine, S., & Aila, T. (2017). Temporal ensembling for semi-supervised learning.
In International conference on learning representations.
Lecouat, B., Foo, C.-S., Zenati, H., & Chandrasekhar, V. (2018). Manifold reg-
ularization with GANs for semi-supervised learning. arXiv preprint arXiv:
1807.04307.
LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436.
Ledoux, M., & Talagrand, M. (2013). Probability in banach spaces: Isoperimetry and
processes. Springer Science & Business Media.
Lee, D.-H. (2013). Pseudo-label: The simple and efficient semi-supervised
learning method for deep neural networks. In Workshop on challenges in
representation learning, ICML, Vol. 3 (p. 896).
Loshchilov, I., & Hutter, F. (2016). SGDR: Stochastic gradient descent with
restarts. CoRR, abs/1608.03983. URL: http://arxiv.org/abs/1608.03983. arXiv:
1608.03983.
Luo, Y., Zhu, J., Li, M., Ren, Y., & Zhang, B. (2018). Smooth neighbors on teacher
graphs for semi-supervised learning. In 2018 IEEE conference on computer
vision and pattern recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22,
2018 (pp. 8896–8905).
Miyato, T., ichi Maeda, S., Koyama, M., & Ishii, S. (2018). Virtual adversarial train-
ing: a regularization method for supervised and semi-supervised learning.
IEEE Transactions on Pattern Analysis and Machine Intelligence.
Mohri, M., & Medina, A. M. (2014). Learning theory and algorithms for rev-
enue optimization in second price auctions with reserve. In International
conference on machine learning (pp. 262–270). PMLR.
Mohri, M., Rostamizadeh, A., & Talwalkar, A. (2012). Foundations of machine
learning. MIT Press.
Nakkiran, P. (2019). Adversarial robustness may be at odds with simplicity. arXiv
preprint arXiv:1901.00532.
Oliver, A., Odena, A., Raffel, C., Cubuk, E. D., & Goodfellow, I. J. (2018).
Realistic evaluation of deep semi-supervised learning algorithms. In Neural
information processing systems (NIPS).
Park, S., Park, J., Shin, S.-J., & Moon, I.-C. (2018). Adversarial dropout for
supervised and semi-supervised learning. In AAAI.
Radford, A., Metz, L., & Chintala, S. (2015). Unsupervised representation learning
with deep convolutional generative adversarial networks. arXiv preprint
arXiv:1511.06434.
Sajjadi, M., Javanmardi, M., & Tasdizen, T. (2016). Regularization with stochastic
transformations and perturbations for deep semi-supervised learning. In
NIPS’16, Proceedings of the 30th international conference on neural information
processing systems (pp. 1171–1179). USA: Curran Associates Inc., URL: http:
//dl.acm.org/citation.cfm?id=3157096.3157227.
Shawe-Taylor, J., Bartlett, P. L., Williamson, R. C., & Anthony, M. (1996). A
framework for structural risk minimisation. In Proceedings of the ninth annual
conference on computational learning theory (pp. 68–76).
105

V. Verma, K. Kawaguchi, A. Lamb et al.
Neural Networks 145 (2022) 90–106
Sohn, K., Berthelot, D., Carlini, N., Zhang, Z., Zhang, H., Raffel, C. A., Cubuk, E. D.,
Kurakin, A., & Li, C.-L. (2020). FixMatch: Simplifying semi-supervised learning
with consistency and confidence. In H. Larochelle, M. Ranzato, R. Hadsell, M.
F. Balcan, & H. Lin (Eds.), Advances in neural information processing systems,
Vol. 33 (pp. 596–608). Curran Associates, Inc., URL: https://proceedings.
neurips.cc/paper/2020/file/06964dce9addb1c5cb5d6e3d9838f733-Paper.pdf.
Tarvainen, A., & Valpola, H. (2017). Mean teachers are better role models:
Weight-averaged consistency targets improve semi-supervised deep learn-
ing results. In Advances in neural information processing systems 30 (pp.
1195–1204).
Tokozume, Y., Ushiku, Y., & Harada, T. (2018). Between-class learning for
image classification. In The IEEE conference on computer vision and pattern
recognition (CVPR).
Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., & Madry, A. (2018). Robustness
may be at odds with accuracy. Stat, 1050, 11.
Verma, V., Lamb, A., Beckham, C., Najafi, A., Mitliagkas, I., Courville, A., Lopez-
Paz, D., & Bengio, Y. (2018). Manifold mixup: Better representations by
interpolating hidden states. arXiv e-prints, (p. arXiv:1806.05236). arXiv:
1806.05236.
Verma, V., Lamb, A., Kannala, J., Bengio, Y., & Lopez-Paz, D. (2019). Interpolation
consistency training for semi-supervised learning. In Proceedings of the
twenty-eighth international joint conference on artificial intelligence, IJCAI-19
(pp. 3635–3641). International Joint Conferences on Artificial Intelligence
Organization, http://dx.doi.org/10.24963/ijcai.2019/504.
Zagoruyko, S., & Komodakis, N. (2016). Wide residual networks. In R. C. Wilson,
E. R. Hancock, & W. A. P. Smith (Eds.), Proceedings of the British machine
vision conference (BMVC) (pp. 87.1–87.12). BMVA Press, http://dx.doi.org/10.
5244/C.30.87.
Zhang, H., Cisse, M., Dauphin, Y. N., & Lopez-Paz, D. (2018). Mixup: Be-
yond empirical risk minimization. In International conference on learning
representations. URL https://openreview.net/forum?id=r1Ddp1-Rb.
106

