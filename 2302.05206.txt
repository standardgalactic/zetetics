The Wisdom of Hindsight Makes Language Models
Better Instruction Followers
Tianjun Zhang * 1 Fangchen Liu * 1 Justin Wong 1 Pieter Abbeel 1 Joseph E. Gonzalez 1
Abstract
Reinforcement learning has seen wide success in
ﬁnetuning large language models to better align
with instructions via human feedback. The so-
called algorithm, Reinforcement Learning with
Human Feedback (RLHF) demonstrates impres-
sive performance on the GPT series models. How-
ever, the underlying Reinforcement Learning (RL)
algorithm is complex and requires an additional
training pipeline for reward and value networks.
In this paper, we consider an alternative approach:
converting feedback to instruction by relabeling
the original one and training the model for better
alignment in a supervised manner. Such an algo-
rithm doesn’t require any additional parameters
except for the original language model and maxi-
mally reuses the pretraining pipeline. To achieve
this, we formulate instruction alignment problem
for language models as a goal-reaching problem
in decision making. We propose Hindsight In-
struction Relabeling (HIR), a novel algorithm
for aligning language models with instructions.
The resulting two-stage algorithm shed light to a
family of reward-free approaches that utilize the
hindsightly relabeled instructions based on feed-
back. We evaluate the performance of HIR ex-
tensively on 12 challenging BigBench reasoning
tasks and show that HIR outperforms the baseline
algorithms and is comparable to or even surpasses
supervised ﬁnetuning1.
1. Introduction
Recent studies have shown that large language models could
demonstrate unintended behavior when prompting it with
an instruction (Bender et al., 2021; Bommasani et al., 2021;
*Equal contribution
1University of California, Berkeley.
Correspondence to: Tianjun Zhang <tianjunz@berkeley.edu>,
Fangchen Liu <fangchen liu@berkeley.edu>.
1The implementation of HIR is available at https://
github.com/tianjunz/HIR
Figure 1. Average Performance on BigBench. HIR demonstrates
a signiﬁcant average performance gain over 12 tasks on BigBench
compared to all baselines using FLAN-T5-Large.
Weidinger et al., 2021). Such behavior is undesirable since
the langue model could make up facts, generate toxic text
or simply not be able to follow the intended behavior made
by the instructions (Bender et al., 2021; Bommasani et al.,
2021; Weidinger et al., 2021). As a result, a considerable
amount of research effort has been put into designing better
ﬁnetuning algorithms that can align the outputs of language
models with human instructions (Leike et al., 2018; Askell
et al., 2021). The most widely adopted approach is to deploy
reinforcement learning (RL) algorithms to optimize for a
manually deﬁned or learned “alignment score” (Ouyang
et al., 2022; Uesato et al., 2022). Impressive progress has
been made in this direction, including the more recently
released GPT series model (OpenAI, 2022).
Despite their good performance in the alignment, however,
most of the prior work either uses Proximal Policy Optimiza-
tion (PPO) (Schulman et al., 2017) to optimize for a trained
alignment score module (Ouyang et al., 2022) or tries to
apply imitation learning to a ﬁnal-answer or reward-model
ﬁltered dataset (Uesato et al., 2022). The former approach is
rather complex, sensitive to hyperparameters, and requires
additional training in the reward model and value network.
The latter one is less data-effective as it only makes use of
the success instruction-output pairs, completely abandoning
the ones that do not align.
In this paper, we investigate whether we can design a sim-
arXiv:2302.05206v1  [cs.CL]  10 Feb 2023

The Wisdom of Hindsight Makes Language Models Better Instruction Followers
... sort these words: ...
... apple, banana, peach ...
LLM as Policy
LLM as World Model
... today is Sunday, so ...
... today is Sunday, so ...
tomorrow is ...
Figure 2. Illustration of Large Language Model (LLM). HIR views LLM as both a policy and a world model. Thus, HIR can collect
data through interactions with LLM in the online sampling phase, and further improve the policy in the ofﬂine learning phase.
ple ﬁnetuning algorithm that utilizes not only successful
instruction-output pairs but also bootstrap from failed ones.
We ﬁrst make the connection between the instruction align-
ment of language models and goal-reaching RL (Plappert
et al., 2018), a special case of the general RL framework
with an augmented goal space. This makes a straightfor-
ward correspondence, as we can view the instruction or task
speciﬁcation as the goal, and the language model as a goal-
conditioned policy that can generate a sequence of word
tokens to achieve the speciﬁed goal. To this end, a series of
policy optimization algorithms (Andrychowicz et al., 2017;
Eysenbach et al., 2022) tailored for goal-conditioned RL
can be applied to the alignment problem of the language
models.
The resulting algorithm we proposed, Hindsight Instruction
Relabeling (HIR), adopts the central idea of relabeling the
instructions in a hindsight fashion based on the generated
outputs of the language model. HIR alternates between
two phases: an online sampling phase to generate a dataset
of instruction-output pairs, along with an ofﬂine learning
phase that relabels the instructions of each pair and per-
forms standard supervised learning. The algorithm does not
require any additional parameters to train except the lan-
guage model itself. We also adopt the relabeling strategy in
HER (Andrychowicz et al., 2017) to make use of the failure
data and use contrastive instruction labeling to improve the
performance further.
We evaluate our algorithm extensively on 12 BigBench (Sri-
vastava et al., 2022) language model reasoning tasks. The
tasks we choose are very diverse, including logical deduc-
tion which requires logical understanding, object counting
that involves math calculation, and geometric shapes that
ask the model to understand the visual concept. We use the
FLAN-T5 models (Chung et al., 2022) as the base model,
comparing with the baselines of PPO (Schulman et al., 2017)
and Final-Answer RL (Uesato et al., 2022). Results in Fig. 1
show that HIR signiﬁcantly outperforms both baselines by
11.2% and 32.6% respectively. To summarize, our key con-
tributions are:
• We propose a new perspective of learning from feed-
back via hindsight instruction relabeling, and connect
the alignment problem of language model to goal-
conditioned reinforcement learning.
• We propose a novel two-phase hindsight relabeling
algorithm, which is more data-effective and doesn’t
require any additional RL training pipeline.
• Our method signiﬁcantly outperforms baselines and is
overall comparable to supervised ﬁne-tuning (SFT) on
12 challenging BigBench reasoning tasks.
2. Related Work
Reinforcement Learning for Human Feedback
Human
feedback has been readily studied in the reinforcement
learning setting(Ross et al., 2011; Kelly et al., 2019; Ibarz
et al., 2018). Going as far back as inverse reinforcement
learning to infer and model human preferences(Christiano
et al., 2017; Wu et al., 2021; Lawrence & Riezler, 2018;
Ziegler et al., 2019). More recent work starting with In-
structGPT (Ouyang et al., 2022) has identiﬁed the beneﬁts
of RL for improving human alignment for open-vocabulary
unstructured settings. In InstructGPT, humans wrote ground
truth prompts on which GPT provided unsatisfactory re-
sponses, and a reward model was trained on this data to
ﬁnetune GPT’s responses. A similar line of work WebGPT
utilizes human feedback from online data (Nakano et al.,
2021). Although this approach requires expensive data
collection, it as crucial to the successful release of Chat-
GPT (OpenAI, 2022), the ﬁrst-of-its-kind general-purpose
chatbots made available to the public. Our work focuses on

The Wisdom of Hindsight Makes Language Models Better Instruction Followers
Figure 3. Hindsight Instruction Relabeling. HIR consists of two phases: online exploration phase and ofﬂine training phase. The
algorithm alternates between the two phases until convergence.
the ﬁnetuning process for pretrained language models and
offers a lighter-weight approach.
Prompt-Engineering
Recent work has demonstrated that
cleverly chosen prompts have the potential of dramatically
improving pretrained LLM performance on specialized
tasks from code generation to reasoning tasks (Wei et al.,
2022; Zhou et al., 2022; Kojima et al., 2022). Another line
of research lies in tuning prompt embedding vector through
SGD (Lester et al., 2021; Liu et al., 2021b;a). There are also
efforts on automatic prompt generation (Gao et al., 2020;
Shin et al., 2020) or using RL for discrete prompt optimiza-
tion (Zhang et al., 2022; Deng et al., 2022). In addition,
multiple prior works have shown that combining ﬁnetuning
and prompt engineering provides orthogonal beneﬁts (Sti-
ennon et al., 2020; Perez et al., 2021; Ouyang et al., 2022).
Our approach avoids the manual effort required to prompt
engineering for a speciﬁc task.
Two-stage Reinforcement Learning
There have been
numerous categories of work tackling ofﬂine reinforcement
learning (Chen et al., 2021a; Janner et al., 2021; Jiang et al.,
2022; Kumar et al., 2020). There have also been efforts to
make transformers suitable for online exploration (Zheng
et al., 2022). More recently, the Algorithm Distillation
(AD) (Laskin et al., 2022) proposed a similar approach of
alternating between online exploration and ofﬂine training.
Note that HIR and AD tackles entirely different problems,
while HIR focuses on improving language model alignment
with RL, AD tackles the classical control problem. These
ideas have been recently explored in ﬁnetuning language
models as well (Huang et al., 2022; Li et al., 2022b; Zelik-
man et al., 2022).
Language Model with Reasoning Tasks.
The tasks in
our experiments require explicit reasoning steps for the
language models. Solving math problem (Cobbe et al., 2021;
Hendrycks et al., 2021; Ling et al., 2017) has long been an
interesting application for this. More recently, a series of
works have been focused on the multi-step reasoning part of
the language models either by ﬁne-tuning (Lewkowycz et al.,
2022) or prompting (Wei et al., 2022; Kojima et al., 2022;
Zhou et al., 2022). These works have all along the effort
to adopt language models for long-horizon reasoning tasks.
Aside from these, there have also been works on trying to
use language models for code generation (Li et al., 2022a;
Chen et al., 2021b). This line of research also requires
language to be capable of doing reasoning over the program
tree structure.
3. Background
3.1. Reinforcement Learning Formulation
We can deﬁne a Markov Decision Process (MDP) by a tu-
ple ⟨S, A, P, R⟩. S and A are the state space and action
space. P represents the transition probability P(s′|s, a),
and R(s, a) is the reward function. The policy π is a map-
ping from S to A. The goal of reinforcement learning is to
ﬁnd an optimal policy π∗that maximizes the expectation of

The Wisdom of Hindsight Makes Language Models Better Instruction Followers
the accumulated rewards J(π) = Eπ[P∞
t=0 γtR(st, at)],
where at ∼π(a|st).
3.2. Goal-Conditioned Reinforcement Learning
Extending the previous RL setting to a multi-goal RL prob-
lem, we can augment standard MDP as ⟨G, S, A, P, R, ⟩,
where G represents the goal space. Meanwhile, both the
reward function R(s, a, g) and policy π(a|s, g) need to be
goal-dependent. Thus, the objective is to ﬁnd an optimal pol-
icy π∗that maximizes J(π) = Eπ[P∞
t=0 γtR(st, at, gt)],
where at ∼π(a|st, gt).
3.3. Align Language Models with Instruction
When dealing with instructions in language models, let V
be the vocabulary (e.g., the set of predeﬁned tokens) of a
language model M and let e be the embedding of the layer
of the model M. For a simple example, an instruction (or
prompt) may take the form: p = “Give the sentiment of the
following sentence.”, followed by the query q = “I like this
movie.” In this case, we want the language model to give its
output o for the query q following the instruction p.
How to align the model outputs with instructions remains
an essential challenge. InstructGPT (Ouyang et al., 2022)
proposes to ﬁrst learn a reward model R(p, q, o), which
can predict the alignment score based on human preference.
Then it applies the standard RL pipeline to optimize the
accumulated rewards.
4. Hindsight Instruction Relabeling
In this section, we will ﬁrst discuss how one can formu-
late the language model alignment as a goal-conditioned
RL problem in Sec. 4.1. Then we’ll present an outline of
our algorithm in Sec. 4.2. Finally, we will discuss the key
concept of hindsight instruction relabeling in Sec. 4.3.
4.1. Instruction Following as Goal-conditioned RL
A language model M can take instructional prompt p
and initial query token sequence q = {q0, . . . , qi} as
input, and autoregressively predict next token ei+1 =
M(p, q, {e0, . . . , ei}).
We can view standard prompt-
conditioned language tasks (e.g. multi-step reasoning) as a
goal-reaching problem, by formulating the MDP as follows:
• Goal space G: space of instructional prompt p
• State space S: space of input token sequence q ∪{ei}
• Action space A: space of output token ei+1
• Transition probability P: M(ei+1|p, q, {e0, . . . , ei})
• Reward R: alignment score of {e0, . . . , ei+1} with
instruction p and query q, can from human feedback
or scripted feedback, which is not used in HIR.
Here all G, S and A are space of token embeddings, but
G corresponds to instructional prompts, while S and A
corresponds to model inputs and outputs. In this way, we
can also view the language model as a goal-conditioned
policy:
π := M(ei+1|p, q, {e0, .., ei})
(1)
Meanwhile,
since
the
transition
dynamics
P
=
M(ei+1|p, q, {e0, . . . , ei}) are also computed from the
model outputs, we can also view this language model as a
“world model” to interact with. Fig. 2 provides a pictorial
illustration.
By observing this, there is a family of goal-conditioned
RL algorithms, such as hindsight experience replay
(HER) (Andrychowicz et al., 2017) that can potentially be
applied for language model alignment.
4.2. Algorithm Overview
Inspired by the previous connection, we propose Hindsight
Instruction Relabeling, a novel approach for instruction
alignment. Similar to Algorithm Distillation (Laskin et al.,
2022), HIR also consists of two phases: online sampling
and ofﬂine relabeling, as shown in Fig. 3. We discuss the
two components respectively in the following sections.
Online Sampling.
In the “online” sampling phase, we
treat the model as both the environment and goal-
conditioned policy. We want to mimic the exploration phase
in the standard RL paradigm, where we often inject different
noises into actions. In our case, we use a relatively large
temperature τ for sampling. Speciﬁcally, given instruction
p and query q, we use τ = 1 to get the output sequence
o = {e0, e1, . . . , eL}, which gives us the online replay
dataset Donline.
Donline =
N
[
i=1
n
pi, qi, oi
o
(2)
Here each query qi is sampled from the training dataset.
Instruction prompt pi is initialized to be a pre-deﬁned sen-
tence and will be corrected to align with the output oi in the
later stage.
Ofﬂine Relabeling.
The key component of our algo-
rithm is the ofﬂine relabeling part. In this part, for ev-
ery instruction-output pair (p, q, o) that are not necessarily
aligned, we relabel this pair with a new instruction that can
align with the outcome of the model (p∗, q, o).
The new instruction p∗is generated based on the feedback
function R(p, q, o) and the instruction generation function

The Wisdom of Hindsight Makes Language Models Better Instruction Followers
Algorithm 1 Two-Stage Hindsight Instruction Relabeling (HIR)
1: Input: Language Model M, Initial Prompt p, Training Set Dtrain, Evaluation set Deval, Iteration N, Sampling Rounds
T, Training Epochs K, Sampling Temperature τ, Empty RL dataset Donline
2: for episode n = 1, · · · , N do
3:
for sampling rounds i = 1, · · · , T do
4:
Random sample batch of input queries Q ∼Dtrain
5:
Sample corresponding outputs oi = M(Q, p, τ)
6:
Appending the trajectory to RL Dataset Donline ←Donline ∪(Q, p, oi)
7:
end for
8:
for training rounds t = 1, · · · , K do
9:
Random sample batch of query-output pairs (Q, O) ∼Donline
10:
Sample from Donline and apply relabeling as described in Sec. 4.3
11:
Train model M using loss in Eq. (6)
12:
end for
13: end for
14: Evaluate policy πθ on evaluation dataset Deval
φ(p, q, o, r), which can either be learned or scripted. For
example, in the framework of RLHF, if the learned reward
model R(p, q, o) generates a score that ranks about 75%
as in the training data, we can give additional scripted in-
structions to the model such as “give me an answer that
ranks about 75% in training data”. However, as most human-
feedback data is hard to collect, we adopt a scripted feedback
function, which is similar to Final-Answer RL (FARL) (Ue-
sato et al., 2022). For simplicity, φ is also scripted based on
the correctness of the reasoning outcome.
The central difference between HIR and FARL (Uesato et al.,
2022) is whether to use hindsight experience. In FARL, the
algorithm ﬁlters out the correct alignment instruction-output
pairs and conducts imitation learning, while our relabeling
procedure enables learning from failure cases as well.
After we got the relabeled instructions, we can perform
standard supervised learning for these instruction-output
pairs. We perform the standard seq2seq loss Lsupervise to
train our model.
Full Pipeline.
Our full algorithm HIR is shown in Algo-
rithm 1. The algorithm alternates between the online sam-
pling phase to generate a dataset and the ofﬂine instruction
relabeling phase for model improvement.
4.3. Instruction Relabeling
Performing ofﬂine instruction relabeling is crucial to the
success of the algorithm. HER (Andrychowicz et al., 2017)
relabels every transition2 in order to improve the goal-
conditioned policy at all times. Similar to HER, we conduct
instruction relabeling at intermediate time steps on the gen-
erated sub-output.
2(s, a, s′) tuple with goal replacement g
In addition to hindsight relabeling, we also introduce a con-
trastive instruction labeling loss to push up the probability
of a particular instruction-output pair but push down the
other instruction-output pairs.
Sub-output Relabeling
It is important to sample partial
outputs and relabel the instruction. In this way, we could
give more dense feedback through instruction relabeling.
Note that one can ﬂexibly control the granularity that we
want the algorithm to provide this dense feedback. In an-
other word, one could provide feedback at a sentence level
or a paragraph level.
Consider we relabel the i-th time step. The input to the
model is q ∪{e0, ..., ei−1}. We can edit the instruction as
a future goal based on the future alignment score:
p∗= φ

p, q, {ei, ..., eL}, R
 p, q, {ei, ..., eL}

where φ and R are the instruction generation function and
feedback function as described in Sec. 4.2. The model takes
new inputs M(p∗, q, {e0, ..., ei−1}) and is trained to match
the prediction target {ei, ..., eL}, and get the seq2seq loss
Lsupervise as in (Raffel et al., 2020). More details about
relabeling can be found at Appendix. A.2.
We sample trajectories from the data collected during online
interaction in Donline and then uniformly sample different
timestep i using the relabeling process as above.
Contrastive Instruction Following.
We also introduce
the contrastive instruction labeling along with the standard
ﬁne-tuning loss in our ofﬂine instruction relabeling phase.
Suppose oi = M(qi, pi). Given the log probability of oi
conditioned on qk, pk as:
Pik = log PM(oi|qk, pk)
(3)

The Wisdom of Hindsight Makes Language Models Better Instruction Followers
Table 1. Examples of inputs and outputs for the BigBench tasks. For multiple-choice tasks, we provide the options that the language
model can choose from as prompts.
Tasks
Example Inputs
Outputs
Multiple Choice
Logical Deduction
“Q: The following paragraphs each describe a set of three objects arranged in a ﬁxed
order. The statements are logically consistent within each paragraph. In a golf tour-
nament, there were three golfers: Amy, Eli, and Eve. Eve ﬁnished above Amy. Eli
ﬁnished below Amy. Options: (A) Amy ﬁnished last (B) Eli ﬁnished last (C) Eve
ﬁnished last”
“(B)”
Date Understanding
“Q: Today is Christmas Eve of 1937. What is the date 10 days ago? Options: (A)
12/14/2026 (B) 12/14/2007 (C) 12/14/1937”
“(C)”
Direct Generation
Object Counting
“Q: I have a blackberry, a clarinet, a nectarine, a plum, a strawberry, a banana, a ﬂute,
an orange, and a violin. How many fruits do I have?”
“6”
Word Sorting
“Sort the following words alphabetically: List: oven costume counterpart.”
“costume coun-
terpart oven”
We deﬁne the following contrastive loss:
Lcontastive = −
n
X
i=1
log
exp(Pii)
Pn
k=1 exp(Pik)
(4)
This helps to avoid the model learning the behavior that
maps the same output for different instructions, and also
beneﬁts the online phase as the loss pushes down the speciﬁc
output for other instructions.
Entropy Regularization.
As a common practice in RL,
we apply entropy regularization to the output given a par-
ticular instruction. This negative entropy term ensures the
sampling phase won’t converge too early for better explo-
ration.
Lentropy =
n
X
i=1
Pk log Pk
(5)
In practice, we add two coefﬁcients α, β for the contrastive
loss and entropy loss. So the ﬁnal loss becomes:
Lﬁnal = Lsupervise + αLcontastive + βLentropy
(6)
5. Comparing to Previous Algorithms
HIR takes inspiration from HER and applies it to the lan-
guage models. The resulting algorithm is simple (no extra
parameter is required to train). We discuss the conceptual
advantages of HIR comparing to several different previous
algorithms (including RLHF, Algorithm Distillation and
Final-Answer RL) in this section.
Most closely, HIR takes a very similar approach comparing
to the algorithm distillation paper. They both adopt the
two-stage online sampling and ofﬂine training paradigm.
However, they are inherently targeting at different domains:
Algorithm Distillation focuses on the control tasks while
HIR is speciﬁcally tailored to language models. Moreover,
as a goal-conditioned algorithm, HIR doesn’t require any
explicit modeling of reward or return. This signiﬁcantly
reduces the complexity of learning another reward or critic
network, thus, yields a simple but elegant algorithm.
HIR is also related to the RLHF algorithm as they both try
to learn from feedback to solve the instruction alignment
problem. However, RLHF requires additional RL training.
Since our dataset doesn’t contain human feedback, we refer
to it as PPO in the experiment sections. Compared with the
standard PPO algorithm (Schulman et al., 2017), it exploits
an additional KL penalty.
Compared to the Final-Answer RL, HIR enables the algo-
rithm to learn also from failure cases. Final-Answer RL
only ﬁlters out the correct output from the sampling phase
and uses them as the training data. With the capability of
hindsight instruction relabeling, HIR handles failure data as
well as successful ones. A more intuitive illustration can be
found in Fig. 4.
Figure 4. Conceptual Comparison between HIR and baseline
methods. HIR is a simple supervised learning algorithm, does not
require any additional parameter or KL penalty as an additional
reward, and utilizes failure data.
6. Experiments
We conduct experiments with our method on the Big-
Bench (Srivastava et al., 2022) tasks. Different from the
traditional multiple-choice GLUE (Wang et al., 2018) and
SuperGLUE (Wang et al., 2019) tasks, BigBench is a more
challenging generation task that requires complex reasoning
capabilities of the language models. We select a subset of

The Wisdom of Hindsight Makes Language Models Better Instruction Followers
Table 2. Performance of HIR on the 12 challenging BigBench reasoning tasks. Compared to all baselines including PPO and FARL, HIR
achieves strong performance gain.
Tracking Shufﬂed
Objects (3)
Tracking Shufﬂed
Objects (5)
Tracking Shufﬂed
Objects (7)
Logical Deduction
(3 Objects)
No Training
FLAN-T5-large
29.3
15.6
6.6
33.3
Finetuning
Finetuning
100.0
17.0
13.4
90.0
RL Tuning
PPO
35.0
15.6
6.3
57.0
FARL
90.0
15.6
10.0
86.7
HIR (ours)
100.0
61.2
42.6
91.7
Logical Deduction
(5 Objects)
Logical Deduction
(7 Objects)
Date Understading
Object Counting
No Training
FLAN-T5-large
44.0
49.3
35.1
31.0
Finetuning
Finetuning
61.0
64.0
96.0
70.0
RL Tuning
PPO
44.0
43.0
90.5
33.0
FARL
54.0
60.0
98.0
56.7
HIR (ours)
67.0
62.0
98.0
65.0
Geometric Shapes
Penguins in A
Table
Reasoning about
Colored Objects
Word Sorting
No Training
FLAN-T5-large
9.7
46.7
20.0
1.1
Finetuning
Finetuning
90.0
53.0
90.0
24.7
RL Tuning
PPO
11.0
50.0
30.0
1.1
FARL
66.7
56.0
77.0
3.4
HIR (ours)
90.3
53.0
77.8
3.4
the BigBench consisting of 12 complex tasks. The tasks we
select are quite diverse, including reasoning the ﬁnal results
of a sequence of actions, understanding dates, and com-
pleting tasks that require simple arithmetic calculation. We
compare against the standard reinforcement learning base-
lines: including RL with Human Feedback (PPO) (Ouyang
et al., 2022) and Final-Answer Reinforcement Learning
(FARL) (Uesato et al., 2022).
We ﬁrst demonstrate the superior performance of HIR on
the single-task ﬁne-tuning with a base model FLAN-T5-
large (Chung et al., 2022) in Sec. 6.1. We then validate
such performance gain is consistent across different sizes of
models (FLAN-T5-base and FLAN-T5-large) in Sec. 6.2. In
addition to the performance gains, we also conduct thorough
ablations studies on the entropy regularization coefﬁcient,
label smoothing factor, and sub-output sampling. All the
experiment details including network architecture and hy-
perparameters can be found at the Appendix. A.1.
Evaluation Setup and Tasks.
We introduce the evalua-
tion setup and the tasks we used in our experiments. For
the evaluation setup, instead of training a reward model and
using a RL algorithm to optimize the objective, following
(Uesato et al., 2022), we directly use the ﬁnal answer in the
training dataset to check the results generated by the lan-
guage models as the feedback (e.g., correct answer or wrong
answer). To be speciﬁc, we divide the task data into 80% for
training and 20% for testing. At training time, we randomly
sample a batch of questions as prompts from the training
dataset, ask the language model to generate corresponding
answers, and provide feedback via ﬁnal answer checking.
For the BigBench tasks, we choose the 12 challenging tasks,
including Tracking Shufﬂed Objects, Logical Deduction,
Date Understanding, Object Counting, Geometric Shapes,
Penguins in A Table, Reasoning about Colored Objects,
and Word Sorting. These tasks include both multiple-choice
tasks and direct-generation tasks. For both types of tasks, we
formulate them as the generation task. Following (Chung
et al., 2022), we provide options for the language model to
choose from as prompts and ask it to generate the answer.
There are some examples of this format in Tab. 1. We
provide all the templates for the tasks in the Appendix. B.1.
In this way, no additional parameter of the language model
is needed for training (e.g., extra linear head layer).
Baselines.
We compare HIR against the two popular RL
baselines: PPO and Final-Answer RL. Instead of learning
a reward module as in RLHF, we give the PPO algorithm
a reward of 1 if the ﬁnal answer checking is correct and
0 otherwise. Final-Answer RL ﬁrst conducts the online
sampling, then performs the ﬁnal-answer checking to select
only the correct results and use them to do imitation learning.

The Wisdom of Hindsight Makes Language Models Better Instruction Followers
Table 3. Performance of HIR on both FLAN-T5-large and FLAN-T5-base models. HIR shows signiﬁcant improvements even with a
much smaller model FLAN-T5-base.
Tracking Shufﬂed
Objects (3)
Tracking Shufﬂed
Objects (5)
Tracking Shufﬂed
Objects (7)
Logical Deduction
(3 Objects)
Logical Deduction
(5 Objects)
Logical Deduction
(7 Objects)
FLAN-T5-base
34.7
18.4
7.4
36.7
30.0
32.9
HIR-T5-base
100.0 (+65.3)
36.8 (+18.4)
68.3 (+60.9)
73.3 (+36.6)
52.0 (+22.0)
57.1 (+24.2)
FLAN-T5-large
29.3
15.6
6.6
33.3
44.0
49.3
HIR-T5-large
100.0 (+70.7)
61.2 (+45.6)
42.6 (+36.0)
91.7 (+58.4)
67.0 (+23.0)
62.0
(+12.7nn
lnl)
Date
Understading
Object
Counting
Geometric
Shapes
Penguins in A
Table
Reasoning about
Colored Objects
Word
Sorting
FLAN-T5-base
4.1
19.5
0.0
10.0
4.8
1.3
HIR-T5-base
98.0 (+93.9)
59.0 (+39.5)
43.1 (+43.1)
53.3 (+43.3)
73.3 (+68.5)
0.5 (-0.8)
FLAN-T5-large
35.1
31.0
9.7
46.7
20.0
1.1
HIR-T5-large
98.0 (+62.9)
65.0 (+34.0)
90.3 (+80.6)
53.0 (+6.3)
77.8 (+57.8)
3.4 (+2.3)
For reference, we also report the number of performing
standard ﬁne-tuning. Note that the RL-based method is
not directly comparable to ﬁne-tuning, as the they only
provides feedback on whether the answer is preferred or
not; whereas in order to perform ﬁne-tuning, the correct
answer (potentially also the reasoning paths) is required.
We also discuss the connections and advantages in Sec. 5.
6.1. HIR with FLAN-T5-large on BigBench
We evaluate HIR extensively using the BigBench tasks afore-
mentioned. In Tab. 2, we compare the performance of HIR
with PPO and Final-Answer RL, along with providing the
reference performance of Fine-Tuning and the base model
without any ﬁne-tuning. From the results in Tab. 2, we see
HIR outperforms almost all the baselines, even including
ﬁne-tuning by a good margin. Especially in hard tasks like
Tracking Shufﬂed Objects (5) and (7), HIR surpasses the
best baseline by 41.2% and 29.2%, respectively. Note that
for PPO, we adopt the implementation of trlx by CarperAI3
and heavily sweep the hyperparameters. However, its perfor-
mance is still not quite satisfactory. We provide the details
in Appendix. A.1.
In tasks that require direct generation, like Object Counting
and Word Sorting, HIR is still being able to outperform all
the baselines. However, its performance is not comparable
to ﬁne-tuning as ﬁne-tuning directly provides the correct
answer while HIR only performs ﬁnal answer checking.
6.2. Effect of Base Model Sizes
We also conduct experiments to show that HIR can work
well across different sizes of models. We compare FLAN-
T5-base and FLAN-T5-large with the results shown in
3Implementation: https://github.com/CarperAI/trlx
Tab. 3. We see that HIR can consistently improve the model
performance regardless of its size, and achieve signiﬁcant
improvement of 40.5% and 43.0% of the models, respec-
tively. These results also conﬁrm that even though HIR is
starting with a weaker model (which can bring challenges
to the exploration phase during sampling), it can still gain
signiﬁcant improvements after rounds of training. This is
particularly very important given that we don’t have many
strong language models to bootstrap.
6.3. Ablations
We conduct ablations on different aspects of the algorithm.
We speciﬁcally study how the entropy coefﬁcient, label
smoothing parameters, and sub-output sampling can help
with the performance. We present the results in Tab. 6.3. We
can see that adding the entropy regularization term, label
smoothing term, and sub-output sampling are all helpful to
the ﬁnal performance to some extent.
Table 4. Ablations on the different components of HIR. We see
that each component of entropy regularization, label smoothing
and sub-output sampling plays an important role in the algorithm.
Geometric
Shapes
Tracking Shufﬂed
Objects (3)
Logical Deduction
(3 Objects)
HIR
90.3
100.0
91.7
HIR
(w.o.
Sub-
Sample)
86.1
100.0
75.0
HIR (w.o. Entropy)
47.2
100.0
48.3
HIR (w.o. Smooth)
84.7
100.0
23.3
7. Conclusion
In this paper, we proposed HIR that ties the connection be-
tween instruction alignment and goal-conditioned RL. This
yields a simple two-stage hindsight relabeling algorithm,
HIR that interacts with and improves language models. HIR
utilizes both success data and failure data to train the lan-

The Wisdom of Hindsight Makes Language Models Better Instruction Followers
guage model effectively and doesn’t require any additional
training pipeline. HIR achieves impressive results on the
BigBench tasks compared to the baselines.
As far as we know, HIR is the very ﬁrst algorithm that
applies hindsight relabeling to language models. We hope
such work can inspire future research toward designing
more efﬁcient and scalable algorithms that can signiﬁcantly
lower the costs of training LLMs from human feedback.
8. Acknowledgement
The author would like to thank Benjamin Eysenbach and
Shane Gu for for helpful discussions throughout the project.
This research is supported in part by NSF CISE Expedi-
tions Award CCF-1730628, NSF NRI #2024675 and under
the NSF AI4OPT Center. UC Berkeley research is also
supported by gifts from Alibaba, Amazon Web Services,
Ant Financial, CapitalOne, Ericsson, Facebook, Futurewei,
Google, Intel, Microsoft, Nvidia, Sco- tiabank, Splunk and
VMware.
References
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,
R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,
and Zaremba, W. Hindsight experience replay. Advances
in neural information processing systems, 30, 2017.
Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D.,
Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma,
N., et al. A general language assistant as a laboratory for
alignment. arXiv preprint arXiv:2112.00861, 2021.
Bender, E. M., Gebru, T., McMillan-Major, A., and
Shmitchell, S. On the dangers of stochastic parrots: Can
language models be too big? In Proceedings of the 2021
ACM Conference on Fairness, Accountability, and Trans-
parency, pp. 610–623, 2021.
Bommasani, R., Hudson, D. A., Adeli, E., Altman, R.,
Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosse-
lut, A., Brunskill, E., et al. On the opportunities and risks
of foundation models. arXiv preprint arXiv:2108.07258,
2021.
Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A.,
Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. De-
cision transformer: Reinforcement learning via sequence
modeling. Advances in neural information processing
systems, 34:15084–15097, 2021a.
Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,
Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,
G., et al. Evaluating large language models trained on
code. arXiv preprint arXiv:2107.03374, 2021b.
Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg,
S., and Amodei, D. Deep reinforcement learning from
human preferences. Advances in neural information pro-
cessing systems, 30, 2017.
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,
Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma,
S., et al. Scaling instruction-ﬁnetuned language models.
arXiv preprint arXiv:2210.11416, 2022.
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,
Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,
R., et al. Training veriﬁers to solve math word problems.
arXiv preprint arXiv:2110.14168, 2021.
Deng, M., Wang, J., Hsieh, C.-P., Wang, Y., Guo, H., Shu, T.,
Song, M., Xing, E. P., and Hu, Z. Rlprompt: Optimizing
discrete text prompts with reinforcement learning. arXiv
preprint arXiv:2205.12548, 2022.
Eysenbach, B., Zhang, T., Salakhutdinov, R., and Levine, S.
Contrastive learning as goal-conditioned reinforcement
learning. arXiv preprint arXiv:2206.07568, 2022.
Gao, T., Fisch, A., and Chen, D. Making pre-trained lan-
guage models better few-shot learners. arXiv preprint
arXiv:2012.15723, 2020.
Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,
S., Tang, E., Song, D., and Steinhardt, J. Measuring math-
ematical problem solving with the math dataset. arXiv
preprint arXiv:2103.03874, 2021.
Huang, J., Gu, S. S., Hou, L., Wu, Y., Wang, X., Yu, H., and
Han, J. Large language models can self-improve. arXiv
preprint arXiv:2210.11610, 2022.
Ibarz, B., Leike, J., Pohlen, T., Irving, G., Legg, S., and
Amodei, D. Reward learning from human preferences and
demonstrations in atari. Advances in neural information
processing systems, 31, 2018.
Janner, M., Li, Q., and Levine, S. Ofﬂine reinforcement
learning as one big sequence modeling problem. Ad-
vances in neural information processing systems, 34:
1273–1286, 2021.
Jiang, Z., Zhang, T., Janner, M., Li, Y., Rockt¨aschel, T.,
Grefenstette, E., and Tian, Y. Efﬁcient planning in a com-
pact latent action space. arXiv preprint arXiv:2208.10291,
2022.
Kelly, M., Sidrane, C., Driggs-Campbell, K., and Kochen-
derfer, M. J. Hg-dagger: Interactive imitation learning
with human experts. In 2019 International Conference on
Robotics and Automation (ICRA), pp. 8077–8083. IEEE,
2019.

The Wisdom of Hindsight Makes Language Models Better Instruction Followers
Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa,
Y. Large language models are zero-shot reasoners. arXiv
preprint arXiv:2205.11916, 2022.
Kumar, A., Zhou, A., Tucker, G., and Levine, S. Con-
servative q-learning for ofﬂine reinforcement learning.
Advances in Neural Information Processing Systems, 33:
1179–1191, 2020.
Laskin, M., Wang, L., Oh, J., Parisotto, E., Spencer, S.,
Steigerwald, R., Strouse, D., Hansen, S., Filos, A.,
Brooks, E., et al. In-context reinforcement learning with
algorithm distillation. arXiv preprint arXiv:2210.14215,
2022.
Lawrence, C. and Riezler, S. Improving a neural seman-
tic parser by counterfactual learning from human bandit
feedback. arXiv preprint arXiv:1805.01252, 2018.
Leike, J., Krueger, D., Everitt, T., Martic, M., Maini, V., and
Legg, S. Scalable agent alignment via reward modeling:
a research direction. arXiv preprint arXiv:1811.07871,
2018.
Lester, B., Al-Rfou, R., and Constant, N. The power of scale
for parameter-efﬁcient prompt tuning. arXiv preprint
arXiv:2104.08691, 2021.
Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E.,
Michalewski, H., Ramasesh, V., Slone, A., Anil, C.,
Schlag, I., Gutman-Solo, T., et al. Solving quantitative
reasoning problems with language models. arXiv preprint
arXiv:2206.14858, 2022.
Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J.,
Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal Lago,
A., et al. Competition-level code generation with alpha-
code. Science, 378(6624):1092–1097, 2022a.
Li, Y., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G.,
and Chen, W. On the advance of making language mod-
els better reasoners. arXiv preprint arXiv:2206.02336,
2022b.
Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. Pro-
gram induction by rationale generation: Learning to solve
and explain algebraic word problems. arXiv preprint
arXiv:1705.04146, 2017.
Liu, X., Ji, K., Fu, Y., Du, Z., Yang, Z., and Tang, J. P-
tuning v2: Prompt tuning can be comparable to ﬁne-
tuning universally across scales and tasks. arXiv preprint
arXiv:2110.07602, 2021a.
Liu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang,
Z., and Tang, J. Gpt understands, too. arXiv preprint
arXiv:2103.10385, 2021b.
Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,
C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et al.
Webgpt: Browser-assisted question-answering with hu-
man feedback. arXiv preprint arXiv:2112.09332, 2021.
OpenAI.
Chatgpt: Optimizing language models for di-
alogue, Nov 2022.
URL https://openai.com/
blog/chatgpt/.
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama,
K., Ray, A., et al.
Training language models to fol-
low instructions with human feedback. arXiv preprint
arXiv:2203.02155, 2022.
Perez, E., Kiela, D., and Cho, K. True few-shot learning
with language models. Advances in Neural Information
Processing Systems, 34:11054–11070, 2021.
Plappert, M., Andrychowicz, M., Ray, A., McGrew, B.,
Baker, B., Powell, G., Schneider, J., Tobin, J., Chociej,
M., Welinder, P., et al. Multi-goal reinforcement learn-
ing: Challenging robotics environments and request for
research. arXiv preprint arXiv:1802.09464, 2018.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y., Li, W., Liu, P. J., et al. Exploring
the limits of transfer learning with a uniﬁed text-to-text
transformer. J. Mach. Learn. Res., 21(140):1–67, 2020.
Ross, S., Gordon, G., and Bagnell, D. A reduction of imita-
tion learning and structured prediction to no-regret online
learning. In Proceedings of the fourteenth international
conference on artiﬁcial intelligence and statistics, pp.
627–635. JMLR Workshop and Conference Proceedings,
2011.
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
Klimov, O. Proximal policy optimization algorithms.
arXiv preprint arXiv:1707.06347, 2017.
Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., and
Singh, S. Autoprompt: Eliciting knowledge from lan-
guage models with automatically generated prompts.
arXiv preprint arXiv:2010.15980, 2020.
Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid,
A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,
Garriga-Alonso, A., et al. Beyond the imitation game:
Quantifying and extrapolating the capabilities of language
models. arXiv preprint arXiv:2206.04615, 2022.
Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R.,
Voss, C., Radford, A., Amodei, D., and Christiano,
P. F. Learning to summarize with human feedback. Ad-
vances in Neural Information Processing Systems, 33:
3008–3021, 2020.

The Wisdom of Hindsight Makes Language Models Better Instruction Followers
Uesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N.,
Wang, L., Creswell, A., Irving, G., and Higgins, I. Solv-
ing math word problems with process-and outcome-based
feedback. arXiv preprint arXiv:2211.14275, 2022.
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bow-
man, S. GLUE: A multi-task benchmark and analysis plat-
form for natural language understanding. In Proceedings
of the 2018 EMNLP Workshop BlackboxNLP: Analyz-
ing and Interpreting Neural Networks for NLP, pp. 353–
355, Brussels, Belgium, November 2018. Association for
Computational Linguistics. doi: 10.18653/v1/W18-5446.
URL https://aclanthology.org/W18-5446.
Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A.,
Michael, J., Hill, F., Levy, O., and Bowman, S.
Su-
perglue:
A stickier benchmark for general-purpose
language understanding systems.
In Wallach, H.,
Larochelle, H., Beygelzimer, A., d'Alch´e-Buc, F.,
Fox, E., and Garnett, R. (eds.), Advances in Neural
Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.
neurips.cc/paper/2019/file/
4496bf24afe7fab6f046bf4923da8de6-Paper.
pdf.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.,
Le, Q., and Zhou, D. Chain of thought prompting elic-
its reasoning in large language models. arXiv preprint
arXiv:2201.11903, 2022.
Weidinger, L., Mellor, J., Rauh, M., Grifﬁn, C., Uesato,
J., Huang, P.-S., Cheng, M., Glaese, M., Balle, B.,
Kasirzadeh, A., et al. Ethical and social risks of harm
from language models. arXiv preprint arXiv:2112.04359,
2021.
Wu, J., Ouyang, L., Ziegler, D. M., Stiennon, N., Lowe,
R., Leike, J., and Christiano, P. Recursively summa-
rizing books with human feedback.
arXiv preprint
arXiv:2109.10862, 2021.
Zelikman, E., Mu, J., Goodman, N. D., and Wu, Y. T. Star:
Self-taught reasoner bootstrapping reasoning with reason-
ing. 2022.
Zhang, T., Wang, X., Zhou, D., Schuurmans, D., and Gonza-
lez, J. E. Tempera: Test-time prompting via reinforcement
learning. arXiv preprint arXiv:2211.11890, 2022.
Zheng, Q., Zhang, A., and Grover, A. Online decision
transformer. In International Conference on Machine
Learning, pp. 27042–27059. PMLR, 2022.
Zhou, D., Sch¨arli, N., Hou, L., Wei, J., Scales, N., Wang,
X., Schuurmans, D., Bousquet, O., Le, Q., and Chi, E.
Least-to-most prompting enables complex reasoning in
large language models. arXiv preprint arXiv:2205.10625,
2022.
Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford,
A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning
language models from human preferences. arXiv preprint
arXiv:1909.08593, 2019.

The Wisdom of Hindsight Makes Language Models Better Instruction Followers
A. Training and Implementation Details
A.1. Hyperparameters
We provide all the hyperparameters we used in our experiments. This includes all the experiment settings we used for the
baselines and our method.
PPO
For this baseline, we adopt the implementation of trxl from CarperAI. We directly use the GitHub repository and
load the FLAN-T5-large as the base model. We perform hyperparameter sweeping over several key parameters in Tab. 5 as
suggested in the original code base. We perform a grid search over 16 combinations on one task and select the best for all
tasks. We also list all the hyperparameters we used after the grid search in Tab. 6.
Table 5. Hyperparameters used for sweeping RLHF baseline we tested on our tasks.
Hyperparameter
Value
Learning Rate (lr)
[0.0001, 0.001, 0.01, 0.1]
Initial KL Coefﬁcient
[0, 0.01, 0.1, 0.5]
Table 6. Hyperparameters used for RLHF baseline we tested on our tasks.
Hyperparameter Value
Learning Rate (lr)
0.0001
Initial KL Coefﬁcient
0.1
Total Epochs
100
Number Layers Unfrozen
2
Optimizer
Adam
Weight Decay
1e-6
Learning Rate Scheduler
Cosine Annealing
Number Rollouts
512
PPO Epochs
4
Gamma
0.99
Clip Range
0.2
Clip Range Value
0.2
Value Loss Coefﬁcient
1.0
Transformer Temperature
1.0
Transformer Top K
50
Transformer Top P
0.95
Final-Answer RL and HIR
For Final-Answer RL, we directly use our codebase as its algorithm is very similar to ours.
The only difference is that we ﬁlter the entire online sampling dataset with only the correct answers. So we keep the same
hyperparameters for both and list it here.
A.2. Instruction Relabeling Strategy
Scripted Feedback on BigBench Reasoning Tasks
We use a simple scripted binary function:
R(o, p, q) =





1
o gives correct answer of q and p = pcorrect
1
o gives wrong answer of q and p = pwrong
0
otherwise
where pcorrect = “Generate a correct answer to this problem”, and pwrong = “Generate a wrong answer to this problem”.

The Wisdom of Hindsight Makes Language Models Better Instruction Followers
Table 7. Hyperparameters used for Final-Answer RL and HIR.
Hyperparameter Value
Online Samples per Iteration
4
Sampling Temperature
1.0
Learning Rate (lr)
0.0005
Train Batch Size
64
Train Epochs per Iteration
10
Weight decay
0.0
Learning Rate Warmup Steps
0
Learning Rate Scheduler
constant
Label Smoothing
0.2
Entropy Regularization Coefﬁcient
0.001
Contrastive Loss Coefﬁcient
1
Scripted Instruction Relabeling
We also relabel instruction based on
φ(o, p, q, r) =
(
p
r = 1
¬p
otherwise
p is initialized to be pcorrect at the beginning of training. (let ¬pcorrect = pwrong and the opposite also holds). Note that
we never use those functions during evaluation, so they can only access the ground truth in the training set, which is a fair
comparison with other baselines and SFT.
B. Dataset
B.1. Dataset Examples
Here in the section, we provide all the templates we used to train our model for all 12 tasks. The tasks consist of 10 multiple
choice tasks and 2 direct generation tasks. In Tab. 8, we list all the training template for our pipeline.

The Wisdom of Hindsight Makes Language Models Better Instruction Followers
Table 8. Examples of inputs and outputs for the BigBench tasks. For multiple-choice tasks, we provide the options that the language
model can choose from as prompts.
Tasks
Example Inputs
Outputs
Multiple Choice
Logical Deduction
(3)
“Q: On a shelf, there are three books: a black book, an orange
book, and a blue book. The blue book is to the right of the
orange book. The orange book is to the right of the black book.
Options: (A) The black book is the leftmost. (B) The orange
book is the leftmost. (C) The blue book is the leftmost.”
“(A)”
Logical Deduction
(5)
“Q: On a shelf, there are ﬁve books: a gray book, a red book,
a purple book, a blue book, and a black book. The red book
is to the right of the gray book. The black book is to the left
of the blue book. The blue book is to the left of the gray
book. The purple book is the second from the right. Options:
(A) The gray book is the leftmost. (B) The red book is the
leftmost. (C) The purple book is the leftmost. (D) The blue
book is the leftmost. (E) The black book is the leftmost.”
“(E)”
Logical Deduction
(7)
“Q: The following paragraphs each describe a set of three
objects arranged in a ﬁxed order. The statements are logically
consistent within each paragraph. In a golf tournament, there
were three golfers: Amy, Eli, and Eve. Eve ﬁnished above
Amy. Eli ﬁnished below Amy. Options: (A) The black book
is the leftmost. (B) The yellow book is the leftmost. (C) The
white book is the leftmost. (D) The gray book is the leftmost.
(E) The purple book is the leftmost. (F) The orange book is
the leftmost. (G) The green book is the leftmost.”
“(B)”
Tracking
Shufﬂed
Objects (3)
“Q: Alice, Bob, and Claire are playing a game. At the start
of the game, they are each holding a ball: Alice has a orange
ball, Bob has a white ball, and Claire has a blue ball. As the
game progresses, pairs of players trade balls. First, Alice and
Bob swap balls. Then, Bob and Claire swap balls. Finally,
Alice and Bob swap balls. At the end of the game, Alice has
the Options: (A) orange ball. (B) white ball. (C) blue ball.”
“(C)”

The Wisdom of Hindsight Makes Language Models Better Instruction Followers
Tasks
Example Inputs
Outputs
Multiple Choice
Tracking
Shufﬂed
Objects (5)
“Q: Alice, Bob, Claire, Dave, and Eve are playing a game. At
the start of the game, they are each holding a ball: Alice has
a pink ball, Bob has a white ball, Claire has a red ball, Dave
has a purple ball, and Eve has a yellow ball. As the game
progresses, pairs of players trade balls. First, Alice and Dave
swap balls. Then, Claire and Eve swap balls. Then, Alice and
Bob swap balls. Then, Dave and Claire swap balls. Finally,
Alice and Claire swap balls. At the end of the game, Alice
has the Options: (A) pink ball. (B) white ball. (C) red ball.
(D) purple ball. (E) yellow ball.”
“(A)”
Tracking
Shufﬂed
Objects (7)
“Q: Alice, Bob, Claire, Dave, Eve, Fred, and Gertrude are
playing a game. At the start of the game, they are each holding
a ball: Alice has a green ball, Bob has a white ball, Claire
has a yellow ball, Dave has a pink ball, Eve has a orange ball,
Fred has a black ball, and Gertrude has a brown ball. As the
game progresses, pairs of players trade balls. First, Bob and
Gertrude swap balls. Then, Fred and Claire swap balls. Then,
Dave and Gertrude swap balls. Then, Bob and Gertrude swap
balls. Then, Alice and Claire swap balls. Then, Gertrude and
Claire swap balls. Finally, Eve and Claire swap balls. At the
end of the game, Alice has the Options: (A) green ball. (B)
white ball. (C) yellow ball. (D) pink ball. (E) orange ball. (F)
black ball. (G) brown ball.”
“(F)”
Date Understanding
“Q: Yesterday was April 30, 2021.
What is the date
today in MM/DD/YYYY? Options:
(A) ”05/01/2021”
(B) ”02/23/2021” (C) ”03/11/2021” (D) ”05/09/2021” (E)
”04/29/2021” ”
“(A)”
Geometric Shapes
“Q: This SVG path element ¡path d= ¨M 59.43,52.76 L
75.49,27.45 L 54.92,4.40 M 54.92,4.40 L 23.70,7.77 L
15.15,42.15 L 34.51,57.44 L 59.43,52.76¨/¿ draws a Options:
(A) circle (B) heptagon (C) hexagon (D) kite (E) line (F)
octagon (G) pentagon (H) rectangle (I) sector (J) triangle”
“(C)”
Penguins in a Table
“Q: Here is a table where the ﬁrst line is a header and each
subsequent line is a penguin: name, age, height (cm), weight
(kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11
Gwen, 8, 70, 15 For example: the age of Louis is 7, the
weight of Gwen is 15 kg, the height of Bernard is 80 cm.
What animals are listed in the table? Options: (A) bears (B)
crocodiles (C) elephants (D) giraffes (E) penguins”
“(E)”
Reasoning
about
Colored Objects
“Q: On the nightstand, you see a mauve stress ball and a pur-
ple booklet. What color is the booklet? Options: (A) red (B)
orange (C) yellow (D) green (E) blue (F) brown (G) magenta
(H) fuchsia (I) mauve (J) teal (K) turquoise (L) burgundy (M)
silver (N) gold (O) black (P) grey (Q) purple (R) pink”
“(Q)”
Direct Generation
Object Counting
“Q: I have a blackberry, a clarinet, a nectarine, a plum, a
strawberry, a banana, a ﬂute, an orange, and a violin. How
many fruits do I have?”
“6”
Word Sorting
“Sort the following words alphabetically: List: oven costume
counterpart.”
“costume
coun-
terpart
oven”

