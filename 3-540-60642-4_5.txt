Chapter 5 
Orientation and Time to Contact from 
Image Divergence and Deformation 
5.1 
Introduction 
Relative motion between an observer and a scene induces deformation in im- 
age detail and shape. If these changes are smooth they can be economically 
described locally by the first order differential invariants of the image velocity 
field [123] -the curl (vorticity), divergence (dilatation), and shear (deforma- 
tion) components. The virtue of these invariants is that they have geometrical 
meaning which does not depend on the particular choice of co-ordinate system. 
Moreover they are related to the three dimensional structure of the scene and 
the viewer's motion - in particular the surface orientation and the time to con- 
tact 1 _ in a simple geometrically intuitive way. Better still, the divergence and 
deformation components of the image velocity field are unaffected by arbitrary 
viewer rotations about the viewer centre. They therefore provide an efficient, 
reliable way of recovering these parameters. 
Although the analysis of the differential invariants of the image velocity field 
has attracted considerable attention [123, 116] their application to real tasks 
requiring visual inferences has been disappointingly limited [163, 81]. This is 
because existing methods have failed to deliver reliable estimates of the differen- 
tial invariants when applied to real images. They have attempted the recovery 
of dense image velocity fields [47] or the accurate extraction of points or corner 
features [116]. Both methods have attendant problems concerning accuracy and 
numerical stability. An additional problem concerns the domain of applications 
to which estimates of differential invariants can be usefully applied. First order 
invariants of the image velocity field at a single point in the image cannot be 
used to provide a complete description of shape and motion as attempted in 
numerous structure from motion algorithms [201]. This in fact requires second 
order spatial derivatives of the image velocity field [138, 210]. Their power lies 
in their ability to efficiently recover reliable but incomplete (partial) solutions to 
1The time duration before the observer and object collide if they continue with the same 
relative translational motion [86, 133] 

118 
Chap. 5 Orientation and Time to Contact from etc. 
the structure from motion problem. They are especially suited to the domain 
of active vision, where the viewer makes deliberate (although sometimes impre- 
cise) motions, or in stereo vision, where the relative positions of the two cameras 
(eyes) are constrained while the cameras (eyes) are free to make arbitrary rota- 
tions (eye movements). This study shows that in many cases the extraction of 
the differential invariants of the image velocity field when augmented with other 
information or constraints is sufficient to accomplish useful visual tasks. 
This chapter begins with a criticism of existing structure from motion algo- 
rithms. This motivates the use of partial, incomplete but more reliable solutions 
to the structure from motion problem. The extraction of the differential in- 
variants of the image velocity field by an active observer is proposed under this 
framework. Invariants and their relationship to viewer motion and surface shape 
are then reviewed in detail in sections 5.3.1 and 5.3.2. 
The original contribution of this chapter is then introduced in section 5.4 
where a novel method to measure the differential invariants of the image velocity 
field robustly by computing average values from the integral of simple functions 
of the normal image velocities around image contours is described. This avoids 
having to recover a dense image velocity field and taking partial derivatives. 
It also does not require point or line correspondences. Moreover integration 
provides some immunity to image measurement noise. 
In section 5.5 it is shown how an active observer making small, deliberate 
motions can use the estimates of the divergence and deformation of the image 
velocity field to determine the object surface orientation and time to impact. 
The results of preliminary real-time experiments in which arbitrary image shapes 
are tracked using B-spline snakes (introduced in Chapter 3) are presented. The 
invariants are computed efficiently as closed-form functions of the B-spline snake 
control points. This information is used to guide a robot manipulator in obstacle 
collision avoidance, object manipulation and navigation. 
5.2 
Structure from motion 
5.2.1 
Background 
The way appearances change in the image due to relative motion between the 
viewer and the scene is a well known cue for the perception of 3D shape and 
motion. Psychophysical investigations in the study of the human visual system 
have shown that visual motion can give vivid 3D impressions. It is called the 
kinetic depth effect or kineopsis [86, 206]. 
The computational nature of the problem has attracted considerable atten- 
tion [201]. Attempts to quantify the perception of 3D shape have determined 
the number of points and the number of views nccdcd to recover the spatial con- 

5.2. Structure from motion 
119 
figuration of the points and the motion compatible with the views. Ullman, in 
his well-known structure from motion theorem [201], showed that a minimum of 
three distinct orthographic views of four non-planar points in a rigid configura- 
tion allow the structure and motion to be completely determined. If perspective 
projection is assumed two views are, in principle, sufficient. In fact two views of 
eight points allow the problem to be solved with linear methods [135] while five 
points from two views give a finite number of solutions [73]. 2 
5.2.2 
Problems 
with this approach 
The emphasis of these algorithms and the numerous similar approaches that 
these spawned was to look at point image velocities (or disparities in the dis- 
crete motion case) at a number of points in the image, assume rigidity, and write 
out a set of equations relating image velocities to viewer motion. The problem 
is then mathematically tractable, having been reduced in this way to the solu- 
tion of a set of equations. Problems of uniqueness and minimum numbers of 
views and configurations have consequently received a lot of attention in the 
literature [136, 73]. This structure from motion approach is however deceiv- 
ingly simple. Although it has been successfully applied in photogrammetry and 
some robotics systems [93] when a wide field of view, a large range in depths 
and a large number of accurately measured image data points are assured, these 
algorithms have been of little or no practical use in analysing imagery in which 
the object of interest occupies a small part of the field of view or is distant. 
This is because tile effects due to perspective are often small in practice. As a 
consequence, the solutions to the perspective structure from motion algorithms 
are extremely ill-conditioned, often failing in a graceless fashion [197, 214, 60] in 
the presence of image measurement noise when the conditions listed above are 
violated. In such cases the effects in the image of viewer translations parallel to 
the image plane are very difficult to discern from rotations about axes parallel 
to the image plane. 
Another related problem is the bas-relief ambiguity [95] in interpreting im- 
age velocities when perspective effects are small. In addition to the speed-scale 
ambiguity 3, more subtle effects such as the bas-relief problem are not imme- 
2 Although these results were publieised in the computer vision literature by Ullman (1979), 
Longuet-Higgins (1981) and Faugeras and Maybank (1989) they were in fact well known to 
projective geometers and photogrammetrists in the last century. In particular, solutions were 
proposed by Chasle (1855); Hesse (1863) (who derived a similar algorithm to Longuet-Higgins's 
8-point algorithm); Sturm (1869) (who analysed the case of 5 to 7 points in 2 views); Finster- 
walder (1897) and Kruppa (1913) (who applied the techniques to photographs for surveying 
purposes, showed how to recover the geometry of a scene with 5 points and investigated the 
finite number of solutions) See [43, 151] for references. 
3This is obvious from the formulations described above since translational velocities and 
depths appear together in all terms in the structure from motion equations. 

120 
Chap. 5 Orientation and Time to Contact from etc. 
diately evident in these formulations. The bas-relief ambiguity concerns the 
difficulty of distinguishing between a "shallow" structure close to the viewer and 
"deep" structures further away. Note that this concerns surface orientation and 
its effect - unlike the speed-scale ambiguity - is to distort the shape. People 
experience the same difficulty. We are rather poor at distinguishing a relief 
copy from the same sculpture in the round unless allowed to take a sideways 
look [121]. 
Finally these approaches place a lot of emphasis on global rigidity. Despite 
this it is well known that two (even orthographic) views give vivid 3D impressions 
even in the presence of a degree of non-rigidity such as the class of smooth 
transformations e.g. bending transformations which are locally rigid [131]. 
5.2.3 
The advantages of partial solutions 
The complete solution to the structure from motion problem aims to make ex- 
plicit quantitative values of the viewer motion (translation and rotation) and 
then to reconstruct a Euclidean copy of the scene. If these algorithms were 
made to work successfully, this information could of course be used in a variety 
of tasks that demand visual information including shape description, obstacle 
and collision avoidance, object manipulation, navigation and image stabilisation. 
Complete solutions to the structure from motion problem are often, in prac- 
tice, extremely difficult, cumbersome and numerically ill-conditioned. The latter 
arises because many configurations lead to families of solutions,e.g, the bas-relief 
problem when perspective effects are small. Also it is not evident that making 
explicit viewer motion (in particular viewer rotations which give no shape in- 
formation) and exact quantitative depths leads to useful representations when 
we consider the purpose of the computation (examples listed above). Not all 
visual knowledge needs to be of such a precise, quantitative nature. It is possi- 
ble to accomplish many visual tasks with only partial solutions to the structure 
from motion problem, expressing shape in terms of more qualitative descriptions 
of shape such as spatial order (relative depths) and aJ]ine structure (Euclidean 
shape up to an arbitrary affine transformation or "shear" [130, 131]). The latter 
are sometimes sufficient, especially if they can be obtained quickly, cheaply and 
reliably or if they can be augmented with other partial solutions. 
In structure from motion two major contributions to this approach have been 
made in the literature. These include the pioneering work of Koenderink and van 
Doom [123, 130], who showed that by looking at the local variation of velocities 
- rather than point image velocities - useful shape information can be inferred. 
Although a complete solution can be obtained from second-order derivatives, 
a more reliable, partial solution can be obtained from certain combinations of 
first-order derivatives - the divergence and deformation. 

5.3. Differential invariants of the image velocity field 
121 
More recently, alternative approaches to structure from motion algorithms 
have been proposed by Koenderink and Van Doorn [131] and Sparr and Nielsen [187]. 
In the Koenderink and Van Doom approach, a weak perspective projection model 
and the image motion of three points are used to completely define the affine 
transformation between the images of the plane defined by the three points. 
The deviation of a fourth point from this affine transformation specifies shape. 
Again this is different to the 3D Euclidean shape output by conventional meth- 
ods. 
Koenderink shows that it is, however, related to the latter by a relief 
transformation. They show how additional information from extra views can 
augment this partial solution into a complete solution. This is related to an ear- 
lier result by Longuet-Higgins [137], which showed how the velocity of a fourth 
point relative to the triangle formed by another three provides a useful constraint 
on translational motion and hence shape. This is also part of a recurrent theme 
in this thesis that relative local velocity or disparity measurements are reliable 
geometric cues to shape and motion. 
In summary, the emphasis of these methods is to present partial, incom- 
plete but geometrically intuitive solutions to shape recovery from structure from 
motion. 
5.3 
Differential invariants of the image velocity 
field 
Differential invariants of the image velocity field have been treated by a number 
of authors. Sections 5.3.1 and 5.3.2 review the main results which were pre- 
sented originally by Koenderink and Van Doom [123, 124, 121] in the context of 
computational vision and the analysis of visual motion. This serves to introduce 
the notation required for later sections and to clarify some of the ideas presented 
in the literature. 
5.3.1 
Review 
The image velocity of a point in space due to relative motion between the ob- 
server and the scene is given by 
(UAq) Aq 
aAq. 
(5.1) 
qt-- 
)~ 
where U is the translational velocity, ~ is the rotational velocity around the 
viewer centre and ,k is the distance to the point. The image velocity consists 
of two components. The first component is determined by relative translational 
velocity and encodes the structure of the scene, ,~. The second component de- 
pends only on rotational motion about the viewer centre (eye movements). It 

122 
Chap. 5 Orientation and Time to Contact from etc. 
a) 2D rotation (curl) 
c) Shear (deformation) 
b) Isotropic expansion (divergence) 
d) Shear (deformation) 
.t.. 
i 
Figure 5.1: Differential invariants of the image velocity field. 
To first order the image velocity field can be decomposed into curl (vorticity), 
divergence (dilation) and pure shear (deformation) components. The curl, di- 
vergence and the magnitude of the deformation are differential invariants and 
do not depend on the choice of image co-ordinate system. Their effect on appar- 
ent image shape can be described by four independent components of an affine 
transformation. These are: (a) a 21) rotation; (b) an isotropic expansion (scal- 
ing); (c) and (d) two deformation components. 
The latter two are both pure 
shears about different axes. Any deformation can be conveniently decomposed 
into these two components. Each component is dependent on an arbitrary choice 
of co-ordinate system and is not a differential invariants. 

5.3. I)ifferential invariants of the image velocity field 
123 
gives no useful information about the depth of the point or the shape of the visi- 
ble surface. It is this rotational component which complicates the interpretation 
of visual motion. The effects of rotation are hard to extricate however, although 
numerous solutions have been proposed [150]. As a consequence, point image 
velocities and disparities do not encode shape in a simple efficient way since the 
rotational component is often arbitrarily chosen to shift attention and gaze by 
('amera rotations or eye movements. 
We now look at the local variation of image velocities in the vicinity of the ray 
q. Consider an arbitrary co-ordinate system (the final results will be invariant 
to this choice) with the x - y plane spanning the image plane (tangent plane of 
projection sphere at q) and the z-axis aligned with the ray. In this co-ordinate 
system the translational velocity has components {U1, Us, U3} and the angular 
velocity has components {~l,l't2, ~3}. Let the image velocity field at a point 
(x, y) in the vicinity of q tie represented as a 21) vector field, "7(x, y) with x and 
y components (u, v). 
For a suiliciently small field of view or for a small neighbourhood (defined 
more precisely below), the image velocity field can be described by a transla- 
tion in the image (u0, v0) and by the first-order partial derivatives of the image 
velocity (u~, Uy, v~:, Vy), where [210, 150]: 
U1 
UO 
-
-
 
A 
U2 
YO 
-- 
A 
Ux 
Uy 
V x 
Vy 
(5.2) 
+ 
(5.3) 
U3 
U1 A~ 
+ -- 
(5.4) 
A 
A 2 
U1Ay 
(5.5) 
+~3 + A----- V- 
(5.6) 
-~3 + 
A--5--- 
U3 
U2Ay 
+ -- 
(5.7) 
A 
A 2 
and where the x and y subscripts represent differentiation with respect to these 
spatial parameters. 
Note that there are six equations in terms of the eight 
unknowns of viewer motion and surface orientation. The system of equations is 
thus under-constrained. 
An image feature or shape will experience a transformation as a result of the 
image velocity field. The transformation from a shape at time t to the deformed 
shape at a small instant of time later, at t + St, can also be approximated by 
a linear transformation - an affine transformation. In fact, any arbitrary small 
smooth transformation is linear in the limit and well approximated by the first 
derivative in a sufficiently small region. 

124 
Chap. 5 Orientation and Time to Contact from etc. 
To first order the image velocity field at a point (x, y) in the neighbourhood 
of q can be approximated by: 
u 
~o 
+ 
+ O(x ~, xy, ~) 
(5.s) 
V 
V 0 
V x 
Vy 
y 
where O(x 2, xy, y2) represents non-linear terms which are neglected in this anal- 
ysis. The first term is a vector [u0, v0] representing a pure translation while the 
second term is a 2 • 2 tensor -the velocity gradient tensor - and represents the 
distortion of the image shape. 
We can dccompose the velocity gradient tensor into three components, where 
each term has a simple geometric significance invariant under the transforma- 
tion of the image co-ordinate system. 4 These components are the first-order 
differential invariants of the image velocity field - the vorticity (curl) , dilatation 
(divergence) and pure shear (deformation) components. 
v~, 
vy 
- 
--77- 
1 
0 
+ T 
0 
1 
+ 
2 
sin it 
cos It 
0 
- 1 
- sin It 
cos It 
curl'7 [ 0 
-1 ] 
div-7[ 1 
0] 
def~7[cos2it 
sin2#] 
- 
2 
1 
0 
+ T  
0 
1 
+7- 
sin2it 
-cos2# 
where curl'S, div~ and def~7 represent the curl, divergence and deformation com- 
ponents and where It specifies the orientation of the axis of expansion (maximum 
extension). 5 These quantities are defined by: 
dive7 
= 
(u~ + vy) 
(5.10) 
curl'7 
= 
-(uv - v~) 
(5.11) 
(def~) cos 2it 
= 
(u~ - vy) 
(5.12) 
(deh7)sin2it 
= 
(u v + v~). 
(5.13) 
These can be derived in terms of differential invariants [116] or can be simply 
considered as combinations of the partial derivatives of the image velocity field 
with simple geometric meanings. The curl, divergence and the magnitude of the 
deformation are scalar invariants and do not depend on the particular choice 
of co-ordinate system. The axes of maximum extension and contraction rotate 
with rotations of the image plane axes. 
4Th@ decomposition is known in applied mechanics as the Cauchy-Stokes decomposition 
thcorem [5]. 
5(cos tt, sin #) is the eigcnvector of the traccless and symmetric component of the velocity 
tensor. It corresponds the positive eigenvMue with magnitude defV. The other eigenvector 
specifies the axis of contraction and is orthogonM. It corresponds to the negative eigenvalue 
with magnitude-defV. 

5.3. Differential invariants of the image velocity field 
125 
Consider the effect of these components on the transformation of apparent 
image shapes (figure 5.1). 
The curl component that measures the 2D rigid 
rotation or change in orientation of patches in the image. The divergence term 
specifies scale or size changes. The deformation term specifies the distortion of 
the image shape ms a shear (expansion in a specified direction with contraction in 
a perpendicular direction in such a way that area is unchanged). It is specified by 
an axis of expansion and a magnitude (the size of the change in this direction). 
It will be seen below that the main advantage of considering the differential 
invariants of the image velocity field is that the deformation component effi- 
ciently encodes the orientation of the surface while the divergence component 
can be used to provide an estimate of the time to contact or collision. 
Before looking at the 3[) interpretation of these invariants, it is important 
to make explicit under which conditions it is reasonable to consider the image 
velocity field to be well approximated by its first order terms. This requires 
that the transformation is locally equivalent to an affine transformation. For 
example, parMlel lines must remain parallel or equivalently the transformation 
from a plane in the world to the image plane must also be described by an anne 
mapping. This is known as weak perspective. By inspecting the quadratic terms 
in the equation of the image velocity in the vicinity of a point in the image (5.1) 
it is easy to show that we require in the field of interest: 
AA 
-~- 
<< 
1 
(5.14) 
t2.5 
<< 1 
(5.15) 
~l.q 
where 8 is a difference between two ray directions and defines the field of view in 
radians and A)~ is the depth of the relief in the field of view. A useful empirical 
result is that if the distance to the object is greater than the depth of the relief 
by an order of magnitude [193] then the assumption of weak perspective is a 
good approximation to perspective projection. 
At close distances "looming" or "fanning" effects will become noticeable and 
the affine transformation is insufficient to describe the changes in the image. In 
many practical cases, however, it is possible to restrict attention to small fields 
of view in which the weak perspective model is valid. 
5.3.2 
Relation 
to 3D shape 
and viewer 
ego-motion 
The relationships between the observed differential invariants and the three- 
dimensional configuration and the viewer motion are given. In particular the dif- 
ferential invariants are expressed in terms of the viewer translation (U1/,~, U2/,~, U3/A) 

126 
Chap. 5 Orientation and Time to Contact from etc. 
and the surface orientation (A~/A, Ay/A). From (5.2) to (5.13) we have: 
U1 
U0 
-- 
~-~2 
A 
U2 
v0 
- 
+ f~1 
A 
curl~ 
= 
-2~3 + (-VlAy + U2A~) 
(5.16) 
A2 
U3 
U1A~ + U~Ay 
diw7 
= 
2-~- + 
A2 
(5.17) 
(UtA~ - U2Ay) 
(5.18) 
(def~7) cos 2# 
= 
%2 
(def~7) sin2# 
= 
(Ul,~y + U2A~) 
A2 
(5.19) 
Note that the average image translation (uo, vo) can always be cancelled out by 
appropriate camera rotations (eye movements) (~1, ~2). Also note that diver- 
gencc and deformation are unaffected by viewer rotations such as panning or 
tilting of the camera or eye movements whereas these could lead to considerable 
changes in point image velocities or disparities. 
The differential invariants depend on the viewer motion, depth and surface 
orientation. We can express them in a co-ordinate free manner by introducing 
two 2D vector quantities: tile component of translational velocity parallel to the 
image plane scaled by depth, s A where: 
_ 
U- 
(V.q)q 
(5.20) 
A 
and the depth gradienl scaled by depth 6, F, to represent the surface orientation 
and which we define in terms of the 2D vector gradient: 7 
grad% 
- 
A 
(5.22) 
6Koenderlnk [121] defines F as a "nearness gradient" - grad(log(1/A)). In this section F 
is defined as a scaled depth gradient. These two quantities differ by a sign. 
7There are three simple ways to represent surface orientation: components of a unit vector, 
n; gradient space representation (p, q) and the spherical co-ordlnates (a, r). Changing from 
one representation to another is trivial and is listed here for completeness. 
tan (7 
= 
~p2 + q2 
q 
tan T 
= 
- 
P 
n 
= 
(sin c~ cos ~-, sin a sin r, cos q). 

5.3. Differential invariants of the image velocity field 
127 
The magnitude of the depth gradient determines the tangent of the slant of the 
surface (angle between the surface normal and the visual direction). It vanishes 
for a frontal view and is infinite when the viewer is in the tangent plane of the 
surface. Its direction specifies the direction in the image of increasing distance. 
This is equal to the tilt of the surface tangent plane, r. The exact relationship 
between the magnitude and direction of F and the slant and tilt of the surface 
(o-, r) is given by: 
IF I = 
tanc~ 
(5.23) 
ZF 
= 
T 
(5.24) 
With this new notation equations (5.16, 5.17, 5.]8 and 5.19) can be re-written 
to show the relation between the differential invariants, the motion parameters 
and the sin'face position and orientation: 
eurlq 
= 
--2t2.q+FA A 
(5.25) 
diw7 
- 
2U.q + F.A 
(5.26) 
debt 
= 
IFIIAI 
(5.27) 
where # (which specifies the axis of maximum extension) bisects A and F: 
LA +/F 
# 
- 
2 
(5.2s) 
The geometric significance of these equations is easily seen with a few examples 
(see below). Note that this formulation clearly exposes both the speed-scale 
ambiguity - translational velocities appear scaled by depth making it impossible 
to determine whether the effects are due to a nearby object moving slowly or 
a far-away object moving quickly - and the bas-relief ambiguity. The latter 
manifests itself ill the appearance of surface orientation, F, with A. Increasing 
the slant of the surface F while scaling the movement by the same amount will 
leave the local image velocity field unchanged. Thus, from two weak perspective 
views and with no knowledge of the viewer translation, it is impossible to deter- 
mine whether the deformation in the image is due to a large IAI (large "turn" 
of the object or "vergence angle" ) and a small slant or a large slant and a small 
rotation around the object. Equivalently a nearby "shallow" object will produce 
the same effect as a far away "deep" structure. We can only recover the depth 
gradient F up to an unknown scale. These ambiguities are clearly exposed with 
this analysis whereas this insight is sometimes lost in the purely algorithmic 
approaches to solving the equations of motion from the observed point image 
velocities. 
It is interesting to note the similarity between the equations of motion paral- 
lax (introduced in Chapter 2 and listed below for the convenience of comparison) 

128 
Chap. 5 Orientation and Time to Contact from etc. 
which relate the relative image velocity between two nearby points, q(2) _ q(1), 
to their relative inverse depths: 
,(2) __ q}l) : [(U/~ q) A q] 
)~C2 ) 
)~ )" 
(5.29) 
"It 
and tile equation relating image deformation to surface orientation: 
deh7=I(UAq) Aq, [grad(I)] I. 
(5.30) 
The results are essentially the same, relating local measurements of relative 
image velocities to scene structure in a simple way which is uncorrupted by the 
rotational image velocity component. In the first case (5.29), the depths are 
discontinuous and differences of discrete velocities are related to the diKerence 
of inverse depths. In the latter case, (5.30), the surface is assumed smooth and 
continuous and derivatives of image velocities are related to derivatives of inverse 
depth. 
Some examples on real image sequences are considered. These highlight the 
effect of viewer motion and surface orientation on the observed image deforma- 
tions. 
1. Panning and tilting (~1,~2) of the camera has no effect locally on the 
differential invariants (5.2). They just shift the image. At any moment 
eye movements can locally cancel the effect of the mean translation. This 
is the purpose of fixation. 
2. A rotation about the line of sight leads to an opposite rotation in the image 
(curl, (5.25)). This is simply a 2D rigid rotation. 
3. A translation towards the surface patch (figure 5.2a and b) leads to a 
uniform expausion in the image, i.e. a positive divergence. This encodes 
distance in temporal units, i.e. as a time to contact or collision. Both 
rotations about the ray and translations along the ray produce no defor- 
mation in image detail and hence contain no information about the surface 
orientation. 
4. Deformation arises for translational motion perpendicular to the visual 
direction. The magnitude and axes of the deformation depend on the ori- 
entation of the surface and the direction of translation. Figure 5.2 shows 
a surface slanted away from the viewer but with zero tilt, i.e. the depth 
increases as we move horizontally from left to right. Figure 5.2c shows 
the image after a sideways movement to the left with a camera rotation to 
keep the target in the centre of the field of view. The divergence and defor- 
mation components are immediately evident. The contour shape extends 

5.3. Differential invariants of the image velocity field 
129 
Figure 5.2: Distortions in apparent shape due to viewer motion. 
(a) The image of a planar contour (zero tilt and positive slant, i.e. the direction 
of increasing depth, F, is horizontal and from left to right). The image contour 
is localised automatically by a B-spline snake initialised in the centre of the field 
of view. (b) The effect on apparent shape of a viewer translation towards the 
target. 
The shape undergoes an isotropic expansion (positive divergence). (c) 
The effect on apparent shape when the viewer translates to the left while fixating 
on the target (i.e. A is horizontal, right to left). The apparent shape undergoes 
an isotropie contraction (negative divergence which reduces the area) and a de- 
formation in which the axis of expansion is vertical. These effects are predicted 
by equations (5.25, 5.26, 5.27 and 5.28) since the bisector of the direction of 
translation and the depth gradient is the vertical. (d) The opposite effect when 
the viewer translates to the right. The axes of contraction and expansion are 
reversed. The divergence is positive. Again the curl component vanishes. 

130 
Chap. 5 Orientation and Time to Contact from etc. 
Figure 5.3: Image deformations and rotations due to viewer motion. 
(a) The image of a planar contour (90 ~ tilt and positive slant - i.e. the direction 
of increasing depth, F, is vertical, bottom to top). (b) The effect on apparent 
shape of a viewer translation to the left. The contour undergoes a deformation 
with the axis of expansion at 135 ~ to the horizontal. The area of the contour is 
conserved (vanishing divergence). The net rotation is however non-zero. This 
is difficult to see from the contour alone. It is obvious, however, by inspection 
of the sides of the box, that there has been a net clockwise rotation. (c) These 
effects are reversed when the viewer translates to the right. 

5.3. Differential invariants of the image velocity field 
131 
. 
along the vertical axis and contracts along the horizontal as predicted by 
equations (5.28). This is followed by a reduction in apparent size due to 
the foreshortening effect as predicted by (5.26). This result is intuitively 
obvious since a movement to the left makes the object appear in a less 
frontal view. From (5.25) we sec that the curl component vanishes. There 
is no rotation of the image shape. Movement to the right (figure 5.2d) 
reverses these effects. 
For sideways motion with a surface with non-zero tilt relative to direction 
of translation, the axis of contraction and expansion are no longer aligned 
with the image axes. Figure 5.3 shows a surface whose tilt is 90 ~ (depth 
increases as we move vertically in the image). A movement to the left with 
fixation causes a deformation. The vertical velocity gradient is immediately 
apparent. The axis of expansion of the deformation is at 135 ~ to the left- 
right horizontal axis, again bisecting F and A. There is no change in the 
area of the shape (zero divergence) but a clockwise rotation. Tile evidence 
for the latter is that the horizontal edges have remained horizontal. A 
pure deformation alone would have changed these orientations. The curl 
component has the effect of hulling the net rotation. If the direction of 
motion is reversed the axis of expansion moves to 45 ~ as predicted. Again 
the basic equations of (5.25, 5.26, 5.27 and 5.28) adequately describe these 
effects. 
5.3.3 
Applications 
Applications of estimates of the differential invariants of the image velocity field 
are summarised below. 
It has already been noted that measurement of the 
differential invariants in a single neighbourhood is insufficient to completely solve 
for the structure and motion since we have six equations in the eight unknowns 
of scene structure and motion. In a single neighbourhood a complete solution 
would require the computation of second order derivatives [138, 210] to generate 
sufficient equations to solve for the unknowns. Even then solution of the resulting 
set of non-linear equations is non-trivial. 
In the following, the information available from the first-order differential 
invariants alone is investigated. It will be seen that the differential invariants are 
usually sufficient to perform useful visual tasks with the added benefit of being 
geometrically intuitive. Useful applications include providing information which 
is used by pilots when landing aircraft [86], estimating time to contact in braking 
reactions [133] and in the recovery of 3D shape up to a relief transformation [130, 
131]. 

132 
Chap. 50ricntation and Time to Contact from etc. 
1. With knowledge of translation but arbitrary rotation 
An estimate of the direction of translation is usually available when the 
viewer is making deliberate movements (in the case of active vision) or 
in the ease of binocular vision (where the camera or eye positions are 
constrained). It can also be estimated from image measurements by motion 
parallax [138, 182]. 
If the viewer translation is known, equations (5.27), (5.28) and (5.26) are 
sufficient to unambiguously recover the surface orientation and the distance 
to the object in temporal units. Due to the speed-.scale ambiguity the 
latter is expressed as a time to contact. A solution can be obtained in tim 
following way. 
9 The axis of expansion (#) of the deformation component and the 
projection in the image of the direction of translation (/A) allow the 
recovery of the tilt of the surface (5.28). 
9 We can then subtract the contribution due to the surface orientation 
and viewer translation parallel to the image axis from the image di- 
vergence (5.26). This is equal to ]def~7[ cos(r - ZA). The remaining 
component of divergence is due to movement towards or away from 
tile object. This can be used to recover the time to contact, t~': 
t o = 
. 
(5.31) 
U.q 
This has been recovered despite the fact that the viewer translation 
may not be parallel to the visual direction. 
9 The time to contact fixes the viewer translation in temporal units. It 
allows the specification of the magnitude of the translation parallel 
to the image plane (up to the same speed-scale ambiguity), A. The 
magnitude of the deformation can then be used to recover the slant, 
z, of the surface from (5.27). 
The advantage of this formulation is that camera rotations do not affect 
the estimation of shape and distance. The effects of errors in the direc- 
tion of translation are clearly evident as scMings in depth or by a relief 
transformation [121]. 
2. With fixation 
If the cameras or eyes rotate to keep the object of interest in the middle 
of the image (null the effect of image translation) the eight unknowns are 
reduccd to six. The magnitude of the rotations needed to bring the object 
back to the centre of the image determines A and hence allows us to solve 
for these unknowns, as above. Again the major effect of any error in the 
estimate of rotation is to scale depth and orientations. 

5.3. Differential invariants of the image velocity field 
133 
3. With no additional information - constraints on motion 
Even without any additional assumptions it is still possible to obtain useful 
information from the first-order differential invariants. The information 
obtained is best expressed as bounds. For example inspection of equation 
(5.26) and (5.27) shows that the time to contact must lie in an interval 
given by: 
1 
dive7 
deh7 
tc - 
+ --{- 
(5.32) 
The upper bound on time to contact occurs when the component 
of viewer 
translation parallel to the image plane is in the opposite direction to the 
depth gradient. The lower bound occurs when the translation is parallel to 
the depth gradient. The upper and lower estimates of time to contact are 
equal when there is no deformation component. 
This is the case in which 
the viewer translation is along the ray or when viewing a fronto-parallel 
surface (zero depth gradient locally). The estimate of time to contact 
is then exact. 
A similar equation was recently described by Subbarao 
[189]. He describes the other obvious result that knowledge of the curl and 
deformation components can be used to estimate bounds on the rotational 
component 
about the ray, 
eurl~7 
deh7 
a.q _ 
+ --{- 
(5.33) 
4. With no additional information - the constraints on 3D shape 
Koenderink and Van Doorn [130] showed that surface shape information 
can be obtained by considering the variation of the deformation component 
alone in small field of view when weak perspective is a valid approximation. 
This allows the recovery of 3D shape up to a scale and relief transformation. 
That is they effectively recover the axis of rotation of the object but not 
the magnitude of the turn. This yields a family of solution depending on 
the magnitude of the turn. Fixing the latter determines the slants and 
tilts of the surface. This has recently been extended in the affine structure 
from motion theorem [131, 187]. 
The invariants of the image velocity field encode the relations between shape 
and motion in a concise, geometrically appealing way. Their measurement and 
application to real examples requiring action on visual inferences will now be 
discussed. 
5.3.4 
Extraction 
of differential 
invariants 
The analysis above treated the differential invariants as observables of the image. 
There are a number of ways of extracting the differential invariants from the 

134 
Chap. 5 Orientation and Time to Contact from etc. 
image. These are summarised below and a novel method based on the moments 
of areas enclosed by closed curves is presented. 
. Partial derivative of image velocity field 
This is the most commonly stressed approach. It is based on recovering a 
dense field of image velocities and computing the partial derivatives using 
discrete approximation to derivatives [126] or a least squares estimation of 
the affine transformation parameters from the image velocities estimated 
by spatio-tcmporal methods [163, 47]. The recovery of the image velocity 
field is usually computationally expensive and ill-conditioned. 
. Point velocities in a small neighbourhood 
The image velocities of a minimum of three points in a small neighbour- 
hood are sufficient, in principle, to estimate the components of the affine 
transformation and hence the differential invariants [116, 130]. In fact it 
is only necessary to measure the change in area of the triangle formed 
by the three points and the orientations of its sides. However this is the 
minimum information. There is no redundancy in the data and hence 
this requires very accurate image positions and velocities. In [53] this is 
attempted by tracking large numbers of "corner" features [97, 208] and us- 
ing Delaunay triangulation [33] in the image to approximate the physical 
world by planar facets. Preliminary results showed that the localisation of 
"corner" features was insufficient for reliable estimation of the differential 
invariants. 
. Relative orientation of line segments 
Koenderink [121] showed how tcmporal texture density changes can yield 
estimates of the divergence. He also presented a method for recovering 
the curl and shear components that employs the orientations of texture 
elements. 
From (5.10) it is easy to show that the change in orientation (clockwise), 
Ar of an element with orientation r is given to first order by [124] 
Ar 
curlq 
1 
T 
+ 
def~Tsin2(r 
(5.34) 
Orientations arc not affected by the divergence term. They are only af- 
fected by the curl and deformation components. In particular the curl 
component changes all the orientations by the same amount. It does not 
affect the angles between the image edges. These are only affected by the 
deformation component. The relative changes in orientation can be used to 
recover deformation in a simple way since thc effects of the curl component 

5.3. Differential invariants of the image velocity field 
135 
are cancelled out. By taking the difference of (5.34) for two orientations, 
r 
and r 
it is easy to show (using simple trigonometric relations) that 
the relative change in orientation specifies both the magnitude, def~7, and 
axis of expansion of the shear, it, as shown below. 
= dof  [sio,   
01, os  
/] 
2 
P 
" 
(5.30) 
Measurement at three oriented line segments is sufficient to completely 
specify the deformation components. Note that the recovery of deforma- 
tion can be done without any explicit co-ordinate system and even without 
a reference orientation. The main advantage is that point velocities or par- 
tial derivatives are not required. Koenderink proposes this method as being 
well suited for implementation in a physiological setting [121]. 
4. Curves and closed contours 
We have seen how to estimate the differential invariants from point and line 
correspondences. Sometimes these are not available or are poorly localised. 
Often we can only reliably extract portions of curves (although we can not 
always rely on the end points) or closed contours. 
Image shapes or contours only "sample" the image velocity field. At con- 
tour edges it is only possible to measure the normal component of image 
velocity. This information can in certain cases be used to recover the im- 
age velocity field. Waxman and Wohn [211] showed how to recover the 
full velocity field from the normal components of image contours. In prin- 
ciple, measurement of eight normal velocities around a contour allow the 
characterisation of the full velocity field for a planar surface. Kanatani 
[115] also relates line integrals of image velocities around closed contours 
to the motion and orientation parameters of a planar contour. We will 
not attempt to solve for these parameters directly but only to recover the 
divergence and deformation. 
In the next section, we analyse the changing shape of a closed contour 
(not just samples of normal velocities) to recover the differential invariants. 
Integral theorems exist which express the average value of the differential 
invariants in terms of integrals of velocity around boundaries of regions. 
They deal with averages and not point properties and will potentially have 
better immunity to noise. Another advantage of closed curves is that point 
or line correspondences are not required. Only the correspondence of image 
shapes. 

136 
Chap. 5 Orientation and Time to Contact from etc. 
5.4 
Recovery of differential invariants from closed 
contours 
It has been shown that the differential invariants of the image velocity field 
conveniently characterise the changes in apparent shape due to relative motion 
between the viewer and scene. Contours in the image sample this image velocity 
field. It is usually only possible, however, to recover the normal image velocity 
component from local measurements at a curve [202, 100]. It is now shown that 
this information is often suffmient to estimate the differential invariants within 
closed curves. Moreover, since we are using the integration of normal image 
velocities around closed contours to compute average values of the differential 
invariants, this method has a noise-defeating effect leading to reliable estimates. 
The approach is based on relating the temporal derivative of the area of a 
closed contour and its moments to the invariants of the image velocity field. This 
is a generalisation of the result derived by Maybank [148], in which the rate of 
chang(; of area scaled by area is used to estimate the divergence of the image 
velocity field. 
The advantage is that it is not necessary to track point features in the image. 
Only the correspondence between shapes is required. The computationally diffi- 
cult, ill-conditioned and poorly defined process of making explicit the full image 
velocity field [100] is avoided. Moreover , areas can be estimated accurately, even 
when the full set of first order derivatives can not be obtained. 
The moments of area of a contour are defined in terms of an area integral 
with boundaries defined by the contour in the image plane (figure 5.4); 
f~ fdxdy 
(5.36) 
I] = 
(0 
where a(t) is the area of a contour of interest at time t and f is a scalar function 
of image position (x, y) that defines the moment of interest. For instance setting 
f = 1 gives the zero order moment of area (which we label I0). This is simply 
the area of tile contour. Setting f = x or f = y gives the first-order moments 
about the image x and y axes respectively. 
The moments of area can be measured directly from the image (see below 
for a novel method involving the control points of the B-spline snake). Better 
still, their temporal derivatives can also be measured. Differentiating (5.36) with 

5.4. Recovery of differential invariants from closed contours 
137 
Figure 5.4: The temporal evolution of image contours. 
For small fields of view the distortion in image shape can be described locally 
by an afJine transformation. The components of the aJfine transformation can 
be expressed in terms of contour integrals of normal image velocities. 
More 
conveniently the temporal derivatives of the area and its moments can be used to 
characterise the distortion in apparent shape and the affine transformation. 

138 
Chap. 5 Orientation and Time to Contact from etc. 
respect to time and using a result from calculus [61] it can be shown that s 
d 
d [f~ 
fdxdy] 
(5.37) 
dU(b) 
- 
dt 
(t) 
= 
~ 
[fq.nP]ds 
(5.38) 
dc (t) 
where q.n p is the normal component of the image velocity "7 at a point on the 
contour. Note that the temporal derivatives of moments of area are simply 
equivalent to integrating the normal image velocities at the contour weighted by 
a scalar f(x, y). 
By Green's theorem, an integral over the contour c(t), can be re-expressed 
as an integral over the area enclose(] by the contour, a(t). The right-hand side 
of (5.38) can be re-expressed as: 
d-td (Iy) 
= 
J~(t)[div(fxT)]dxdy 
(5.39) 
= 
[ 
[fdiv,7+(q.gradf)]dxdy 
(5.40) 
Ja (t) 
= 
[ 
[fdiv~7 + fxU + fyv]dxdy 
(5.41) 
Ja (t) 
Assuming that the image velocities can be represented by (5.8) in the area of 
interest, i.e. by constant partial derivatives: 
d-t (If) 
= 
uo 
[f~ldxdy + u~ 
[xf, + f]dxdy + uy 
[yf~ldxdy (5.42) 
(t) 
(t) 
(t) 
(0 
(t) 
(t) 
The left hand side is the temporal derivative of the moment of area described by 
f. The integrals on the right-hand side are simply moments of area (which are 
directly measurable). The coefficients of each term are the required parameters 
of the affine transformation. Tile equations are geometrically intuitive. The 
image velocity field deforms the shape of contours in the image. Shape can be 
described by moments of area. Hence measuring the change in the moments of 
area is an alternative to describing the transformation. In this way the change 
in the moments of area have been expressed in terms of the parameters of the 
affine transformation. 
8This equation can be derived by considering the flux linking the area of the contour. This 
changes with tim(; since the contour is carried by the velocity field. The flux field in our 
example does not change with time. Similar integrals appear in fluid mechanics, e.g. the flux 
transport theorem [61]. 

59 
Implementation and experimental results 
139 
If we initially set up the x - y co-ordinate system at the centroid of the 
image contour of interest so that the first moments are zero, equation (5.42) 
with f = x and f -- y shows that the centroid of the deformed shape specifics 
the mean translation [u0, v0]. Setting f = 1 leads to the extremely simple and 
powerflfl result that the divergence of the image velocity field can be estimated 
as the derivative of area scaled by area. 
dIo 
dt = Io(u~ + vy) 
(5.43) 
da(t) _ a(t)div~7. 
(5.44) 
dt 
Increasing the order of the moments, i.e. different values of f(x, y), generates 
new equations and additional constraints. In principle, if it is possible to find 
six linearly independent equations, we can solve for the affine transformation 
parameters and combine the co-efficients to recover the differential invariants. 
The validity of the affine approximation can be checked by looking at the error 
between the transformed and observed image contours. The choice of which 
moments to use is a sul)jcct for further work. Listed below are some of the 
simplest equations which have been useful in the experiments presented here. 
lo 
0 
/~ 
Io 
d 
• 
= 
0 
dt 
I~ 
2I~ 
Iv2 
0 
0 
Io 
0 
0 
Io 
0 
2I~ 
Iy 
0 
I~ 
Io 
i~ 
o 
i~ 
2i~ 
0 
3I~ 
2I~ v 
0 
t~ 
I~ 
4I~u 
31~,~v: I~ 
2[~u 
?.t o 
V0 
Ux 
~y 
Vx 
Vy 
9 (5.45) 
(Note that in this equation subscripts are used to label the moments of area. 
The left-hand side represents the temporal derivative of the moments in the 
column vector.) In practice certain contours may lead to equations which are 
not independent or ill-conditioned. The interpretation of this is that the normal 
components of image velocity are insutlicicnt to recover the true image velocity 
field globally, e.g. a fronto-parallel circle rotating about the optical axis. This 
was termed the "aperture problem in the large" by Waxman and Wohn [211] 
and investigated by Berghom and Carlsson [19]. Note however, that it is always 
possible to recover the divergence from a closed contour. 
5.5 
Implementation 
and experimental results 
5.5.1 
Tracking closed loop contours 
The implementation and results follow9 Multi-span closed loop B-spline snakes 
(introduced in Chapter 3) are used to localise and track closed image contours. 

140 
Chap. 5 Orientation and Time to Contact from etc. 
The B-spline is a curve in the image plane 
x(s) = ~ 
f~(s)q, 
(5.46) 
i 
where fi are the spline basis functions with coefficients Qi (control points of the 
curve) and s is a curve parameter (not necessarily arc length). The snakes are 
initialised as points in the centre of the image and are forced to expand radially 
outwards until they were in the vicinity of an edge where image "forces" make 
the snake stabilise close to a high contrast closed contour. Subsequent image 
motion is automatically tracked by the snake. 
B-spline snakes have useful properties such as local control and continuity. 
They also compactly represent image curves. In our applications they have the 
additional advantage that the area enclosed is a simple function of the control 
points. This also applies to the other area moments. 
From Green's theorem in the plane it is easy to show that the area enclosed 
by a curve with parameterisation x(s) and y(s) is given by: 
f.N 
a = 
x(s)y'(8)es 
(5.47) 
0 
where x(s) and y(s) are the image plane components of the B-spline and y'(s) 
is the derivative with respect to the curve parameter s. 
For a B-spline, substituting (5/t6) and its derivative: 
f sN 
a(t) = 
E E(Qx'QyJ)fifJ ds 
(5.48) 
o 
i 
j 
= ~(Qx~QyJ) ~sN fif~ds. 
(5.49) 
i 
j 
so 
Note that for each span of the B-splinc and at each time instant the basis func- 
tions remain unchanged. The integrals can thus be computed off-line in closed 
form. (At most 16 coefficients need be stored. In fact due to symmetry there are 
only 10 possible values for a cubic B-splinc). At each time instant multiplication 
with the control point positions gives the area enclosed by the contour. This 
is extremely efficient, giving the exact area enclosed by the contour. The same 
method can be used for higher moments of area as well. 
5.5.2 
Recovery 
of time to contact 
and surface 
orientation 
Here we present the results of a preliminary implementation of the theory. The 
examples are based on a camera mounted on a robot arm whose translations 
are deliberate while the rotations around the camera centre are performed to 

5.5. Implementation and experimental results 
141 
keep the target of interest in the centre of its field of view. The camera intrinsic 
parameters (image centre, scaling factors and focal length) and orientation are 
unknown. The direction of translation is assumed known and expressed with 
bounds due to uncertainty. 
Figures 5.5 to 5.10 show the results of these techniques applied to real image 
sequences from the Adept robot workspace, as well as other laboratory and 
outdoor scenes. 
Collision avoidance 
It is well known that image divergence can be used in obstacle collision avoidance. 
Nelson and Aloimonos [163] demonstrated a robotics system which computed di- 
vergence by spatio-temporal techniques applied to the images of highly textured 
visible surfaces. We describe a real-time implementation based on image con- 
tours and "act" on the visually derived information. 
Figure 5.5a shows a camera mounted on an Adept robot manipulator and 
pointing in the direction of a target contour - the lens of a pair of glasses on 
a mannequin. (We hope to extend this so that the robot initially searches by 
rotation for a contour of interest. In the present implementation, however, the 
target object is placed in the centre of the field of view.) 
The closed contour is then localised automatically by initialising a closed 
loop B-spline snake in the centre of the image. The snake "explodes" outwards 
and deforms under the influence of image forces which cause it to be attracted 
to high contrast edges (figure 5.5b). 
The robot manipulator then makes a deliberate motion towards the target. 
Tracking the area of the contour (figure 5.5c) and computing its rate of change 
allows us to estimate the divergence. For motion along the visual ray this is 
sufficient information to estimate the time to contact or impact. The estimate 
of time to contact - decreased by the uncertainty in the measurement and any 
image deformation (5.32) - is used to guide the manipulator so that it stops 
just before collision (figure 5.5d). The manipulator in fact, travels "blindly" 
after its sensing actions (above) and at a uniform speed for the time remaining 
until contact. In repeated trials image divergences measured at distances of 
0.5m to 1.0m were estimated accurately to the nearest half of a time unit. This 
corresponds to a positional accuracy of 20mm for a manipulator translational 
velocity of 40mm/s. 
The affine transformation approximation breaks down at close proximity to 
the target. This may lead to a degradation in the estimate of time to contact 
when very close to the target. 

142 
Chap. 5 Orientation and Time to Contact from etc. 
Figure 5.5: Using image divergence for collision avoidance. 
A CCD camera mounted on a robot manipulator (a) fixates on the lens of a 
pair of glasses worn by a mannequin (b). The contour is localised by a B-spline 
snake which "expands" out from a point in the centre of the image and deforms 
to the shape of a high contrast, closed contour (the rim of the lens). The robot 
then executes a deliberate motion towards the target. The image undergoes an 
isotropic expansion (divergence)(c) which can be estimated by tracking the closed 
loop snake and monitoring the rate of change of the area of the image contour. 
This determines the time to contact - a mcasure of the distance to the target in 
units of time. This is used to guide the manipulator safely to the target so that 
it stops before collision (d). 

5.5. Implementation and experimental results 
143 
Figure 5.6: Using image divergence to estimate time to contact. 
Four samples of a video sequence taken from a moving observer approaching a 
stationary car at a uniform velocity (approximately lm per time unit). A B- 
spline snake automatically tracks the area of the rear windscreen (figure 5.7). 
The image divergence is used to estimate the time to contact (figure 5.8). The 
next image in the sequence corresponds to collision! 

144 
Chap. 5 Orientation and Time to Contact from etc. 
Relative area a(t)/a(O) 
3o~ 
i 
20 
15 
52 
Time (frame number) 
Figure 5.7: Apparent area of windscreen for approaching observer. 
Time to contact (frames) 
71 
6: 
3~ 
2~ 
0 
Time (frame number) 
Figure 5.8: Estimated time to contact for approaching observer. 

5.5. Implementation and experimental results 
145 
Braking 
Figure 5.6 shows a sequence of images taken by a moving observer approaching 
the rear windscreen of a stationary car in front. In the first frame (time t = 0) 
the relative distance between the two cars is approximately 7m. The velocity of 
approach is uniform and approximately lm/time unit. 
A B-spline snake is initialised in the centre of the windscreen, and expands 
out until it localises the closed contour of the edge of the windscreen. The snake 
can then automatically track the windscreen over the sequence. Figure 5.7 plots 
the apparent area, a(t) (relative to the initial area, a(0)) as a function of time, t. 
For uniform translation along the optical axis the relationship between area and 
time is given (from (5.26) and (5.44)) by solving the first-order partial differential 
equation: 
d(a(t))= (~-~)a(t). 
(5.50) 
Its solution is given by: 
a(t) - 
a(O) 
(5.51) 
where to(0) is the initial estimate of the time to contact: 
to(0 / = A(0) 
(5.52/ 
U.q 
This is in close agreement with the data. This is more easily seen if we look at 
the variation of the time to contact with time. For uniform motion this should 
decrease linearly. The experimental results are plotted in Figure 5.8. These are 
obtained by dividing the area of the contour at a given time by its temporal 
derivative (estimated by finite differences), 
tc(t)- 2a(t) 
(5.53) 
at(t)" 
Their variation is linear, as predicted. 
These results are of useful accuracy, 
predicting the collision time to the nearest half time unit (corresponding to 
50cm in this example). 
For non-uniform motion the profile of the time to contact as a function of time 
is a very important cue for braking and landing reactions. Lee [133] describes 
experiments in which he shows that humans and animals can use this information 
in number of useful visual tasks. He showed that a driver must brake so that 
the rate of decrease of the time to contact does not exceed 0.5. 
d (tc(t)) > -0.5. 
(5.54) 

146 
Chap. 5 Orientation and Time to Contact from etc. 
The derivation of this result is straightforward. This will ensure that the vehicle 
can decelerate uniformly and safely to avoid a collision. As before, neither 
distance nor velocity appear explicitly in this expression. More surprisingly the 
driver needs no knowledge of the magnitude of his deceleration. Monitoring the 
divergence of the image velocity field affords sufficient information to control 
braking reactions. In the example of tigure 5.6 we have shown that this can be 
done extremely accurately and reliably by montitoring apparent areas. 
Landing reactions and object manipulation 
If the translational motion has a component parallel to the image plane, the 
image divergence is composed of two components. The first is the component 
which determines immediacy or time to contact. The other term is due to image 
foreshortening when the surface has a non-zero slant. The two effects can be 
separately computed by measuring the deformation. The deformation also allows 
us to recover the surface orientation. 
Note that unlike stereo vision, the magnitude of the translation is not needed. 
Nor are the camera parameters (focal length; aspect ratio is not needed for 
divergence) known or calibrated. 
Nor are the magnitudes and directions of 
the camera rotations needed to keep the target in the field of view. Simple 
measurements of area and its moments - obtained in closed form as a function 
of the B-spline snake control points - were used to estimate divergence and 
deformation. The only assumption was of uniform motion and known direction 
of translation. 
Figures 5.9 show two examples in which a robot manipulator uses these esti- 
mates of time to contact and surface orientation in a number of tasks including 
landing (approaching perpendicular to object surface) and manipulation. The 
tracked image contours are shown in figure 5.2. These show the effect of di- 
vergence (figure 5.2a and b) when the viewer moves towards the target, and 
deformation (figures 5.2c and d) due to the sideways component of translation. 
Qualitative visual navigation 
Existing techniques for visual navigation have typically used stereo or the anal- 
ysis of image sequences to determine the camera ego-motion and then the 3D 
positions of feature points. The 3D data are then analysed to determine, for 
example, navigable regions, obstacles or doors. An example of an alternative 
approach is presented. This computes qualitative information about the orien- 
tation of surfaces and times to contact from estimates of image divergence and 
deformation. The only requirement is that the viewer can make deliberate move- 
ments or has stereoscopic vision. Figure 5.10a shows the image of a door and 

5.5. Implementation and experimental results 
147 
Figure 5.9: Visually guided landing and object manipulation. 
Figures 5.9 shows two examples in which a robot manipulator uses the estimates 
of time to contact and surface orientation in a number of tasks including landing 
(approaching perpendicular to object surface) and manipulation. 
The tracked 
image contours used to estimate image divergence and deformation are shown 
in figure 5.2. 
In (a) and (b) the estimate of the time to contact and surface orientation is used 
to guide the manipulator so that it comes to rest perpendicular to the surface 
with a pre-determined clearance. Estimates of divergence and deformation made 
approximately lm away were sufficient to estimate the target object position and 
orientation to the nearest 2cm in position and 1 ~ in orientation. 
In the second example, figures (c) and (d), this information is used to position a 
suction gripper in the vicinity of the surface. A contact sensor and small probing 
motions can then be used to refine the estimate of position and guide the suction 
gripper before manipulation. An accurate estimate of the surface orientation is 
essential. The successful execution is shown in (c) and (d). 

148 
Chap. 5 Orientation and Time to Contact from etc. 
Figure 5.10: Qualitative visual navigation using image divergence and deforma- 
tion. 
(a) The image of a door and an object of interest, a pallet. (b) Movement towards 
the door and pallet produces a deformation in the image seen as an expansion 
in the apparent area of the door and pallet. This can be used to determine the 
distancc to these objects, expressed as a time to contact - the time needed for the 
viewer to reach the object if it continued with the same speed. (c) A movement 
to the left produces combinations of image deformation, divergence and rota- 
tion. This is immediately evident from both the door (positive deformation and 
a shear with a horizontal axis of expansion) and the pallet (clockwise rotation 
with shear with diagonal axis of expansion). These effects, combined with the 
knowledge that the movement between the images, are consistent with the door 
having zero tilt, i.e. horizontal direction of increasing depth, while the pallet has 
a tilt of approximately 90 ~ i.e. vertical dircction of increasing depth. They are 
sufficient to determine the orientation of thc surface qualitatively (d). This has 
been done with no knowledge of the intrinsic properties of the camera (camera 
calibration), its orientations or the translational velocities. Estimation of diver- 
gence and deformation can also be recovered by comparison of apparent areas 
and the orientation of edge segments. 

5.5. Implementation and experimental results 
149 
an object of interest, a pallet. Movement towards the door and pallet produce a 
deformation in the image. This is seen as an expansion in the apparent area of 
the door and pallet in figure 5.10b. This can be used to determine the distance 
to these objects, expressed as a time to contact - the time needed for the viewer 
to reach the object if the viewer continued with the same speed. The image 
deformation is not significant. Any component of deformation can, anyhow, be 
absorbed by (5.32) as a bound on the time to contact. A movement to the left 
(figure 5.10c) produces image deformation, divergence and rotation. This is im- 
mediately evident from both the door (positive deformation and a shear with 
a horizontal axis of expansion) and the pallet (clockwise rotation with shear 
with diagonal axis of expansion). These effects with the knowledge of the di- 
rection of translation between the images taken at figure 5.10a and 5.10c are 
consistent with the door having zero tilt, i.e. horizontal direction of increasing 
depth, while the pallet has a tilt of approximately 90 ~ i.e. vertical direction 
of increasing depth. These are the effects predicted by (5.25, 5.26, 5.27 and 
5.28) even though there are also strong perspective effects in the images. They 
are sufficient to determine the orientation of the surface qualitatively (Figure 
5.10d). This has been done without knowledge of the intrinsic properties of the 
cameras (camera calibration), the orientations of the cameras, their rotations or 
translational velocities. No knowledge of epipolar geometry is used to determine 
exact image velocities or disparities. The solution is incomplete. It can, however, 
be easily augmented into a complete solution by adding additional information. 
Knowing the magnitude of the sideways translational velocity, for example, can 
determine the exact quantitative orientations of the visible surfaces. 

