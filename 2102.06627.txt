A fast and scalable computational framework for goal-oriented
linear Bayesian optimal experimental design: Application to
optimal sensor placement
Keyi Wu1, Peng Chen2, and Omar Ghattas2
1Department of Mathematics, University of Texas at Austin, Austin, USA
2Oden Institute for Computational Engineering and Sciences, University of Texas at
Austin, Austin, USA
Abstract
Optimal experimental design (OED) is a principled framework for maximizing information gained
from limited data in inverse problems. Unfortunately, conventional methods for OED are prohibitive
when applied to expensive models with high-dimensional parameters, as we target here. We develop
a fast and scalable computational framework for goal-oriented OED of large-scale Bayesian linear
inverse problems that ﬁnds sensor locations to maximize the expected information gain (EIG) for a
predicted quantity of interest. By employing low-rank approximations of appropriate operators,
an online-oﬄine decomposition, and a new swapping greedy algorithm, we are able to maximize
EIG at a cost measured in model solutions that is independent of the problem dimensions. We
demonstrate the eﬃciency, accuracy, and both data- and parameter-dimension independence of
the proposed algorithm for a contaminant transport inverse problem with inﬁnite-dimensional
parameter ﬁeld.
1
Introduction
Optimizing the acquisition of data—e.g., what, where, and when to measure, what experiments to
run—in order to maximize information gained from the data is a fundamental and ubiquitous problem
across all of the natural and social sciences, engineering, medicine, and technology. Just three important
examples include optimal observing system design for ocean climate data [37], optimal sensor placement
for early warning of tsunami waves [25], and optimal experimental design to accelerate MRI imaging [9].
Bayesian optimal experimental design (BOED)—including formulations as active learning, Bayesian
optimization, and sensor placement—provides a probabilistic framework to maximize the expected
information gain (EIG) or mutual information (MI) for uncertain parameters or related quantities of
interest [15]. However, evaluating the EIG remains prohibitive for large-scale, complex models, due to
the need to compute double integrals with respect to both parameter and data distributions. Recently,
This research was partially funded by the National Science Foundation, Division of Mathematical Sciences under
award DMS-2012453; the Department of Energy, Oﬃce of Science, Oﬃce of Advanced Scientiﬁc Computing Research,
Mathematical Multifaceted Integrated Capability Centers (MMICCS) program under award DE-SC0019303; and the
Simons Foundation under award 560651.
1
arXiv:2102.06627v1  [math.OC]  12 Feb 2021

advances in eﬃciently evaluating the EIG and optimizing the design have been achieved using methods
based on posterior Laplace approximation-based EIG estimation [36], myopic posterior sampling for
adaptive goal-oriented BOED [34], EIG estimation by variational inference for BOED [27], BOED for
implicit models by neural EIG estimation [35], and sequential BOED with variable cost structure [45].
Interest has intensiﬁed in extending BOED to the case of experiments on, or observations of,
complex physical systems, since these can be very expensive (e.g., satellite trajectories, subsurface wells,
ocean-bottom acoustic sensors). Such physical systems are typically modeled by partial diﬀerential
equations (PDEs), which are expensive to solve and often contain inﬁnite-dimensional parameter
ﬁelds and large numbers of design variable. This presents fundamental challenges to conventional
BOED methods, which require prohibitively large numbers of (PDE) model solves. Several diﬀerent
classes of methods have been developed to tackle these computational challenges by exploiting (1)
sparsity by polynomial chaos approximation of parameter-to-observation maps [29, 30, 31], (2) intrinsic
low dimensionality by low-rank approximation of (prior-preconditioned and data-informed) operators
[2, 1, 3, 41, 23, 8], and (3) decomposibility by oﬄine (for model-constrained EIG approximation)–online
(for design optimization) decomposition [43].
Here, we focus on goal-oriented optimal experimental design (GOOED) for large-scale Bayesian
inverse problems, in the context of optimal sensor placement. That is, we seek optimal sensor locations
that maximize the information gained from the sensors, not about the model parameters, but (of greater
practical interest) for a posterior model-predictive goal. In particular, we consider linear parameter-to-
observable (PtO) maps governed by expensive models (e.g., PDEs) with high-dimensional uncertain
parameters (e.g., inﬁnite-dimensional before discretization). In [8], a gradient-based optimization
method is developed to solve the linear GOOED problem to ﬁnd the optimal sensor locations. However,
in each of the possibly very large number of iterations, many model evaluations have to be performed,
which makes the algorithm prohibitive if each model evaluation (e.g., solving PDEs) is very expensive.
Contributions. We propose a fast and scalable computational framework for high-dimensional
and Bayesian GOOED problems governed by large-scale, expensive-to-solve models. To overcome the
curse-of-dimensionality with respect to both parameter and data dimensions, we exploit the intrinsic
low-dimensionality of the PtO map and propose and analyze low-rank approximation of appropriate
data- and parameter-informed operators and the EIG. These properties are revealed by Jacobians
and Hessians of the PtO map, as has been done for model reduction for sampling and deep learning
[10, 17, 6, 38], Bayesian inference [13, 14, 16, 21, 22, 18], optimization under uncertainty [4, 20, 19],
and BOED [2, 3, 23, 41, 8, 43]. We use a randomized algorithm for the low-rank approximation, which
requires only a small and dimension-independent number of large-scale model evaluations. Moreover,
we develop an eﬃcient oﬄine-online decomposition scheme to evaluate the goal-oriented EIG, where
in the oﬄine stage the model-constrained low-rank approximations are performed just once, while
in the online stage the design optimization is performed free of model evaluations. Furthermore, for
the design optimization we introduce a new swapping greedy algorithm that ﬁrst constructs an initial
set of sensors using leverage scores, and then swaps the chosen sensors with other candidates until
certain convergence criteria are met. Finally, we demonstrate the eﬃciency, accuracy, and dimension
independence (with respect to both data and parameters) of the proposed algorithm for a contaminant
transport inverse problem with inﬁnite-dimensional parameter ﬁeld.
We present background on BOED in Section 2, propose our computational framework for GOOED
in Section 3, and report results on experiments in Section 4.
2

2
Background
2.1
Linear Bayesian inverse problem
We consider a general linear model
y = Fm + ϵ,
(1)
where y ∈Rdy is a dy-dimensional observational data vector corrupted by additive Gaussian noise
ϵ ∈N(0, Γn) with zero mean and covariance Γn ∈Rdy×dy, m ∈Rdm is a dm-dimensional uncertain
parameter vector, and F : Rdm 7→Rdy is a linear PtO map. As a speciﬁc case, m is a discretization
(e.g., by ﬁnite element method) of an inﬁnite-dimensional parameter ﬁeld in a model described by
PDEs, while F is implicitly given by solving the PDE model. In this case, the parameter dimension is
typically very high, O(106 −109) for practical applications.
We assume a Gaussian prior m ∼N(mpr, Γpr) with mean mpr and covariance Γpr for the parameter
m with density
πpr(m) ∝exp

−1
2||m −mpr||2
Γ−1
pr

,
(2)
where ||m −mpr||2
Γ−1
pr := (m −mpr)T Γ−1
pr (m −mpr). Then by Bayes’ rule the posterior density of m
satisﬁes
πpost(m|y) ∝πlike(y|m)πpr(m).
(3)
Here πlike(y|m) is the likelihood function that satisﬁes
πlike(y|m) ∝exp (−Φ(m, y))
(4)
under Gaussian noise ϵ ∈N(0, Γn) , where the potential
Φ(m, y) := 1
2||Fm −y||2
Γ−1
n .
(5)
Under the assumption of Gaussian prior and Gaussian noise, the posterior of m is also Gaussian
N(mmap, Γpost) with mean mpost = Γpost(F∗Γ−1
n y + Γ−1
pr mpr) and covariance Γpost = (Hm + Γ−1
pr )−1,
where
Hm = F∗Γ−1
n F
(6)
is the (data-misﬁt) Hessian of the potential Φ(m, y), and F∗is the adjoint of F, e.g., by solving the
adjoint PDE model.
2.2
Bayesian optimal experimental design
2.2.1
Expected information gain
The expected information gain (EIG) is deﬁned as the expected (with respect to data) Kullback-
Leibler (KL) divergence between the posterior and the prior distributions,
Ψ := Ey[DKL(πpost(·|y)∥πpr)],
(7)
where the KL divergence is deﬁned as
DKL(πpost∥πpr) :=
Z
ln
dπpost
dπpr

dπpost.
(8)
3

For a Bayesian linear inverse problem as formulated in Section 2.1, the EIG Ψ admits the closed form
[1]
Ψ = 1
2 logdet

Im + eHm

,
(9)
where Im is an identity matrix of size dm × dm, and eHm := Γ
1
2prHmΓ
1
2pr is the prior-preconditioned
Hessian that includes both data and prior information.
2.2.2
BOED for sensor placement
We consider an optimal sensor placement problem. Assume we have a collection of d candidate
sensors {si}d
i=1. We need to choose a much smaller number r < d of sensors (due to a limited budget
or physical constraints) at which data are collected. The OED problem seeks to ﬁnd the best sensor
combination from the candidates. We use a Boolean design matrix W ∈W ⊂Rr×d to represent sensor
placement such that Wij = 1 if the i-th sensor is placed at the j-th candidate location, i.e.,
Wij ∈{0, 1},
d
X
j=1
Wij = 1,
r
X
i=1
Wij ∈{0, 1}.
(10)
We assume that the observational noise for the d candidate sensors is uncorrelated, with covariance
Γd
n = diag(σ2
1, . . . , σ2
d).
(11)
As a result, for any design W with the covariance for the observation noise ϵ as Γn(W) = WΓd
nW T , we
have
Γ−1
n (W) = W(Γd
n)−1W T .
(12)
Denoting by Fd the PtO map using all d candidate sensors, we have the design-speciﬁc PtO map
F(W) = WFd,
(13)
with its adjoint F∗= F∗
dW T . We can now state the OED problem as: ﬁnd an optimal design W ∈W
such that
W = arg max
W ∈W
Ψ(W).
(14)
3
Goal-oriented optimal experimental design
The classical OED problem seeks a design that maximizes the information gain for the parameter
vector m. In this work, we consider a goal-oriented optimal experimental design (GOOED) problem
that maximizes the information gain of a predicted quantity of interest (QoI) ρ ∈Rp, which is assumed
to be a linear function of the parameter m,
ρ = Pm,
(15)
where P : Rdm 7→Rdρ is a linear map that typically involves model evaluation (e.g., solving PDEs). Due
to linearity, the prior distribution of ρ is Gaussian N(ρpr, Σpr) with mean ρpr = Pmpr and covariance
Σpr = PΓprP∗, where P∗is the adjoint of P. Moreover, the posterior distribution of ρ is also Gaussian
N(ρpost, Σpost) with mean ρpost = Pmpost and covariance Σpost = PΓpostP∗.
4

3.1
Expected information gain for GOOED
To construct an expression for EIG for GOOED, we ﬁrst introduce Proposition 3.1 [42], which
relates the observational data y and the QoI ρ.
Proposition 3.1. Model (1) and QoI (15) lead to
y = FP†ρ + η,
(16)
where P† := ΓprP∗Σ−1
pr , and η ∼N(0, Γη) with
Γη := Γn + F(Γpr −ΓprP∗Σ−1
pr PΓpr)F∗,
(17)
or equivalently Γη = Cov[ϵ] + Cov[F(Im −P†P)m], with Cov as covariance. Moreover, ρ and η are
independent.
Thus, the EIG for ρ can be obtained analogously to (9),
Ψρ(W) = 1
2 logdet

Iρ + eHρ
m(W)

,
(18)
where Iρ is an identity matrix of size dρ × dρ, and eHρ
m(W) = Σ
1
2prHρ
m(W)Σ
1
2pr, with Hρ
m(W) given by
Hρ
m(W) = (F(W)P†)∗Γ−1
η (W)F(W)P†.
(19)
3.2
Oﬄine-online decomposition for EIG Ψρ
The EIG Ψρ(W) depends on W through F(W) given in (13), which involves expensive model
evaluations (e.g., PDE solutions).
Since Ψρ(W) must be evaluated repeatedly in the course of
maximizing EIG, these repeated model evaluations would be prohibitive. To circumvent this problem,
we propose an oﬄine-online decomposition scheme, where model-constrained computation of quantities
that are independent of W is performed oﬄine a single time, and the online design optimization is
free of any model evaluations. The key result permitting this decomposition is given in the following
theorem, whose proof is given in Appendix A.
Theorem 3.2. For each design W ∈W, the goal-oriented EIG Ψρ(W) given in (18) can be computed
as
Ψρ(W) = 1
2 logdet
 Ir + LT WHρ
dW T L

,
(20)
where Ir is an identity matrix of size r × r, Hρ
d is given by
Hρ
d := FdΓprP∗Σ−1
pr PΓprF∗
d,
(21)
and L is given by the Cholesky factorization Γ−1
η
= LLT .
Note that Γη deﬁned in (17) can be equivalently written as
Γη(W) = W
 Γd
n + Hd −Hρ
d

W T ,
(22)
where Hd := FdΓprF∗
d. Hence evaluation of Ψρ(W) can be decomposed as follows: (1) construct the
model-constrained matrices Hd and Hρ
d oﬄine just once; and (2) for each W in the online optimization
process, assemble a small (r × r) matrix Γη(W) by (22), compute a Cholesky factorization Γ−1
η
= LLT ,
and assemble Ψρ(W) by (20), which are all free of the expensive model evaluations.
5

Note that Hd ∈Rd×d and Hρ
d ∈Rd×d are large matrices when we have a large number of candidate
sensors d ≫1. Moreover, their construction involves expensive model evaluations when the parameters
are high-dimensional, dm ≫1, e.g., by solving PDEs. Therefore, it is computationally not practical to
directly compute and store these matrices. Fortunately, the intrinsic ill-posedness of high-dimensional
Bayesian inverse problems—data inform only a low-dimensional subspace of parameter space, e.g.,
[32, 13, 39, 26, 7]—suggests that these matrices are likely low rank or exhibit rapid spectral decay. We
exploit this property and construct low-rank approximations of Hρ
d and Hd −Hρ
d in the next section.
3.3
Low-rank approximation
Let ∆Hd := Hd −Hρ
d, where Hρ
d and Hd are given in (21) and (22) and integrate data, parameter,
and QoI information. Noting that Hρ
d and ∆Hd are both symmetric, we compute their low-rank
approximation for given tolerances ϵζ, ϵλ > 0 as
ˆHρ
d = UkZkU T
k
and
∆ˆHd = VlΛlV T
l ,
(23)
where (Uk, Zk) represent the k dominant eigenpairs of Hρ
d with Zk = diag(ζ1, . . . , ζk) such that
ζ1 ≥ζ2 ≥· · · ≥ζk ≥ϵζ ≥ζk+1 · · · ≥ζd;
(24)
and (Vl, Λl) represent the l dominant eigenpairs of ∆Hd with Λl = diag(λ1, . . . , λl) such that
λ1 ≥λ2 ≥· · · ≥λl ≥ϵλ ≥λl+1 ≥· · · ≥λd.
(25)
For the low-rank approximation, we employ a randomized SVD algorithm [28], which requires only
O(k) and O(l) model evaluations, respectively. In practice, k, l ≪d. More details on the algorithm
applied to the example problem in Section 4 can be found in Appendix C.
With ˆΓη(W) := W

Γd
n + ∆ˆHd

W T as an approximation of Γη(W) in (17), we compute the
Cholesky factorization ˆΓ−1
η
= ˆLˆLT . Then we can deﬁne an approximate EIG as
ˆΨρ(W) := 1
2 logdet

Ir + ˆLT W ˆHρ
dW T ˆL

.
(26)
The following theorem quantiﬁes the approximation error, whose proof can be found in Appendix B.
Theorem 3.3. For any design W ∈W, the error for the goal-oriented EIG Ψρ(W) in (20) by its
approximation ˆΨρ(W) in (26) can be bounded by
|Ψρ(W) −ˆΨρ(W)| ≤1
2
d
X
i=k+1
log(1 + ζi/σ2
min)
+ 1
2
k
X
i=l+1
log(1 + λiζ1/σ4
min),
(27)
where σ2
min := min(σ2
1, . . . , σ2
d) as deﬁned in (11).
We remark that with rapid decay of the eigenvalues (ζk)k≥1 of ˆHρ
d and (λl)l≥1 of ∆ˆHd, the error
bound in (27) becomes very small. Moreover, the decay rates are often independent of the (candidate)
data dimension d and the parameter dimension dm, as demonstrated in Section 4.3. This means that
an arbitrarily-accurate EIG approximation can be constructed with a small number, O(k + l), of model
solves.
6

3.4
Swapping greedy optimization
Once the low-rank approximations of Hρ
d and ∆Hd are constructed per (23), we obtain a fast method
for evaluating the approximate EIG in (26), with certiﬁed approximation error given by Theorem 3.3.
We emphasize that this fast computation does not involve expensive model evaluations (e.g., large-scale
PDE solves) for any given design W. We now turn to the (combinatorial) optimization problem of
ﬁnding the optimal design matrix W,
W = arg max
W ∈W
ˆΨρ(W).
(28)
We next introduce a swapping greedy algorithm to solve this problem requiring only evaluation of
ˆΨρ(W).
In contrast to classical greedy algorithms that sequentially ﬁnd the optimal sensors one by one
(or batch by batch) [11, 33], we extend a swapping greedy algorithm developed for BOED in [43] to
solve the GOOED problem. Given a current sensor set, it swaps sensors with the remaining sensors
to maximize the approximate EIG ˆΨρ(W) until convergence. To initialize the chosen sensor set, we
take advantage of the low-rank approximation ˆHρ
d in (23), which contains information from the data
(through Fd), parameter (through Γpr), and QoI (through P), as can be seen from (21). In particular,
the most informative sensors can be revealed by the rows of Uk with the largest norms, or the leverage
scores of Hρ
d [12]. More speciﬁcally, given a budget of selecting r sensors from d candidate locations, we
initialize the candidate set S0 = {s1, . . . , sr} such that si, i = 1, . . . , r, is the row index corresponding
to the i-th largest row norm of Uk, i.e.,
si = arg max
s∈S\Si−1
||Uk(s, :)||2,
i = 1, . . . , r,
(29)
where Uk(s, :) is the s-th row of Uk, || · ||2 is the Euclidean norm, and the set Si−1 = {s1, . . . , si−1} for
i = 2, 3, . . . , and S0 = ∅. Then at each step of a loop for t = 1, . . . , r, we swap a sensor st from the
current chosen sensor set St−1 with one from the candidate set such that the approximate EIG ˆΨρ(W)
evaluated as in (26) can be maximized, i.e., we choose s∗such that
s∗=
arg max
s∈{st}∪(S\St−1)
ˆΨρ(Ws),
(30)
where Ws is the design matrix corresponding to the sensor choice St−1 \ {st} ∪{s}. We repeat the
loop until a convergence criterion is met, e.g., the chosen S does not change or the diﬀerence of the
approximate EIG is smaller than a given tolerance. We summarize the swapping algorithm algorithm
7

in Algorithm 1.
Algorithm 1: A swapping greedy algorithm for GOOED
1: Input: low-rank approximations (23), a set S = {1, . . . , d} of d candidate sensors, a budget of r sensors to
be placed.
2: Output: the optimal sensor set S∗with r sensors.
3: Initialize S∗= {s1, . . . , sr} ⊂S according to (29).
4: Set S0 = {∅}.
5: while S∗̸= S0 do
6:
S0 ←S∗.
7:
for t = 1, . . . , r do
8:
Choose s∗according to (30).
9:
Update St ←(St−1 \ {st}) ∪{s∗}.
10:
end for
11:
Update S∗←Sr.
12: end while
13: Output: optimal sensor choice S∗.
4
Experiments
In this section, we present results of numerical experiments for GOOED governed by a linear
dynamical PDE model with inﬁnite-dimensional parameter ﬁeld and varying numbers of candidate
sensors. This problem features the key challenges of (1) expensive model evaluation and (2) high-
dimensional parameters and data. The code is available at https://github.com/cpempire/GOOED.
4.1
Model settings
We consider sensor placement for Bayesian inversion of a contaminant source with the goal of
maximizing information gain for contaminant concentration on some building surfaces. The transport of
the contaminant can be modeled by the time-dependent advection-diﬀusion equation with homogeneous
Neumann boundary condition,
ut −k∆u + v · ∇u = 0 in D × (0, T),
u(·, 0) = m in D,
k∇u · n = 0 on ∂D × (0, T),
(31)
where k = 0.001 is the diﬀusion coeﬃcient and T > 0 is the ﬁnal time. The domain D ⊂R2 is open and
bounded with boundary ∂D depicted in Figure 1. The initial condition m is an inﬁnite-dimensional
random parameter ﬁeld in D, which is to be inferred. The velocity ﬁeld v ∈R2 is obtained as the
solution of the steady-state Navier–Stokes equations with Dirichlet boundary condition,
−1
Re∆v + ∇q + v · ∇v = 0 in D,
∇· v = 0 in D,
v = g on ∂D,
(32)
where q represents the pressure ﬁeld and the Reynolds number Re = 50. The Dirichlet boundary data
g ∈R2 are prescribed as g = (0, 1) on the left wall of the domain, g = (0, −1) on the right wall, and
g = (0, 0) elsewhere.
We consider a Gaussian prior for the parameter m ∼N(mpr, Cpr) with mean mpr and covariance
8

0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.1
0.2
0.3
0.4
0.5
Figure 1: The domain D is [0, 1]2 with two buildings (rectangles occupying [0.25, 0.5] × [0.15, 0.4], [0.6, 0.75] ×
[0.6, 0.85]) removed. Left: velocity ﬁeld v. Right: true parameter ﬁeld mtrue.
operator Cpr = A−2, where the elliptic operator A = −γ∆+ δI (with Laplacian ∆and identity I) is
equipped with Robin boundary condition γ∇m · n + βm on ∂D. Here γ, δ > 0 control the correlation
length and variance of m [24]. In our numerical test, we set mpr = 0.25, γ = 1, δ = 8. We synthesize
a “true” initial condition mtrue = min(0.5, exp(−100 ∥x −[0.35, 0.7]∥2) as the contaminant source
(Figure 1). To solve the PDE model, we use an implicit Euler method for temporal discretization with
Nt time steps, and a ﬁnite element method for spatial discretization, resulting in a dm-dimensional
discrete parameter m ∼N(mpr, Γpr), with mpr, Γpr denoting ﬁnite element discretizations of mpr, Cpr,
respectively.
Figure 2: Data of contaminant concentration at time T = 0.8, obtained as the solution of (31) at the initial
condition shown in Figure 1. The QoI maps (P1, P2, P3) are the averaged solution within the lines along the
left, right, and both buildings. Candidate sensor locations are shown in circles (9 left and 75 right).
The solution of the PDE for dm = 2023 and Nt = 40 is shown in Figure 2 at the observation
time T = 0.8. The d candidate sensor locations are also shown in Figure 2, at which we observe the
contaminant concentration u. The linear map F is deﬁned by the predicted data, i.e., the concentrations
9

at the selected sensors. Finally, we take the QoI as an averaged contaminant concentration at time
tpred within a distance δ = 0.02 from the boundaries of either the left, the right, or both buildings, with
corresponding QoI maps denoted as P1, P2, P3 (see Figure 2).
4.2
Numerical results
We ﬁrst consider the case of a small number of candidate sensors, for which we can use exhaustive
search to ﬁnd the optimal sensor combination and compare it with the sensors chosen by the standard
and swapping greedy algorithms. Speciﬁcally, we use a grid of d = 9 candidate locations {si}9
i=0 (xi ∈
{0.2, 0.55, 0.8}×{0.25, 0.5, 0.75}) as shown in Figure 2 (left) with the goal of choosing r = 2, 3, 4, 5, 6, 7, 8
sensors for the QoI prediction time tpred = 1.0. We compute the matrices Hρ
d and ∆Hd (of size 9 × 9)
without low-rank approximation since they are small.
2
3
4
5
6
7
8
number of sensors r
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
approximated EIG 
swapping greedy choice
standard greedy choice
all possible choices
(a) P1
2
3
4
5
6
7
8
number of sensors r
0.25
0.50
0.75
1.00
1.25
1.50
1.75
approximated EIG 
swapping greedy choice
standard greedy choice
all possible choices
(b) P2
2
3
4
5
6
7
8
number of sensors r
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
approximated EIG 
swapping greedy choice
standard greedy choice
all possible choices
(c) P3
Figure 3: Approximate EIG ˆΨρ at r sensors chosen by the standard and swapping greedy algorithms, and the
distribution of ˆΨρ at all possible combinations of 9 candidate sensors. The three plots are for the QoI maps
P1, P2, and P3.
We can see from Figure 3 that for QoI maps P1 and P2, both greedy algorithms ﬁnd the optimal
design, while for P3 with r = 2, 4, only swapping greedy ﬁnds the optimal design. Moreover, an increase
in r leads to diminishing returns, as the gain in information about the QoI from additional sensors
saturates. We see that ∼3 sensors is suﬃcient for either building, whereas 5 is suﬃcient for both.
Next we consider the case of the 75 candidate sensors depicted in Figure 2 (right). Exhaustive
search across all sensor combinations is not feasible in this case; instead, we compare the best EIG
10

from 200 random designs with those obtained by the greedy algorithms. We seek the r optimal sensors,
r = 5, 10, 15, 20, 25, 30, 40, 50, 60, from among the 75 candidates. Results are shown in Figure 4.
10
20
30
40
50
60
number of sensors r
0.5
1.0
1.5
2.0
2.5
3.0
3.5
approximated EIG 
swapping greedy choice
standard greedy choice
random choices
(a) P1
10
20
30
40
50
60
number of sensors r
0.5
1.0
1.5
2.0
2.5
3.0
3.5
approximated EIG 
swapping greedy choice
standard greedy choice
random choices
(b) P2
10
20
30
40
50
60
number of sensors r
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
approximated EIG 
swapping greedy choice
standard greedy choice
random choices
(c) P3
Figure 4: Approximate EIG ˆΨρ for r out of 75 sensors, found by the standard and swapping greedy algorithms,
compared with the distribution of ˆΨρ for 200 randomly-chosen sets from the 75. The three plots are for the
QoI maps P1, P2 and P3 .
We see that both greedy algorithms ﬁnd designs with larger EIG than all random choices. Moreover,
for small r, the swapping greedy algorithm ﬁnds better designs than the standard greedy. For large r,
both greedy algorithms can ﬁnd designs with similar EIG. In fact, multiple designs with similar EIG
become more likely with larger r.
To demonstrate the reduction of computational cost achieved by the oﬄine-online decomposition,
we report the total number of EIG evaluations, the number of swapping loops, and the number of swaps
of the swapping greedy algorithm (Algorithm 1) in Table 1 for 75 candidate sensors with diﬀerent target
number of sensors. We see that the number of loops at convergence is mostly 3. We observe in the
experiments that most of the swaps take place in the ﬁrst loop, followed by a smaller number of swaps
in the second loop resulting in slight sensor adjustments. There are no swaps in the last loop, which we
require as a convergence criterion. As a result of the oﬄine-online decomposition (Theorem 3.2), which
relieves the (thousands of) EIG evaluations of expensive PDE solves once the low-rank approximation
(23) is built, we achieve over 1000X speedup. This is because the PDE solves overwhelmingly dominate
the overall cost, and because the oﬄine decomposition is computed at a cost comparable to one direct
EIG evaluation by (18).
11

Table 1: Number of swapping loops (#LOOPS), swaps (# SWAPS), and EIG evaluations (# EIG EVAL) for
diﬀerent numbers of r selected sensors out of 75 candidates. Results are reported for Algorithm 1 for the goal
P1.
r
5
10
15
20
25
#loops
3
3
3
3
3
#swaps
41
73
124
164
190
#EIG eval
1050
1950
2700
3300
3750
r
30
40
50
60
#loops
2
3
3
3
#swaps
194
235
199
119
#EIG eval
2700
4200
3750
2700
Figure 5 illustrates the eﬀect of the goal of maximizing information gain for the QoIs from optimally
placed sensors. Speciﬁcally, for the parameter-to-QoI maps P1, P2, P3 that quantify the average
contaminant concentration at time tpred = 1 around left, right, and both blocks, the goal-oriented OED
ﬁnds the sensors depicted in the ﬁrst row. For P1 at longer prediction times tpred = 1, 2, 4, 8, we see in
the bottom row of Figure 5 that the optimal sensors are no longer placed in the immediate vicinity of
the building, but instead are increasingly dispersed to better detect the now more diﬀused ﬁeld. Finally,
the ability of GOOED to reduce the posterior variance in the initial condition ﬁeld is depicted in Figure
6 for diﬀerent goals P1, P2, P3. Compared to a random design (lower right), the three optimal designs
lead to lower variance surrounding regions of interest.
4.3
Scalability w.r.t. parameter and data dimensions
Here we demonstrate the fast decay of the eigenvalues of Hρ
d and ∆Hd with respect to the parameter
and data dimensions, as exploited by the algorithms of Section 3.3. For Hρ
d deﬁned in (21), we have
rank(Hρ
d) ≤min(p, d) with QoI dimension p and data dimension d. In practice, the QoI is often an
averaged quantity with small p, so the rank of Hρ
d is also small. In our tests we have rank(Hρ
d) = p = 1.
For ∆Hd = Hd −Hρ
d with Hd = FdΓprF∗, the spectrum of ∆Hd depends on that of Hd, which typically
exhibits fast decay due to ill-posedness of inverse problems. As can be observed in the left plot of Figure
7, the eigenvalues of ∆Hd decay very rapidly and independently of the parameter dimension, which
implies that the required number of PDE solves is small and independent of the parameter dimension
while achieving the same absolute accuracy of the approximate EIG by Theorem 3.3. The right plot
in Figure 7 also illustrates rapid decay of eigenvalues, as well as diminishing returns, with increasing
number of candidate sensors, suggesting that the number of PDE solves is asymptotically independent
of the data dimension for the same relative accuracy of the approximate EIG. These plots suggest that
O(100) PDE solves are required to accurately capture the information gained about the parameter
ﬁeld and QoI from the data, regardless of the parameter or sensor dimensions, when using randomized
SVD (Algorithm 2).
5
Conclusions
We have developed a fast and scalable computational framework for goal-oriented linear Bayesian
optimal experimental design governed by expensive models. Repeated fast evaluation of an (arbitrarily
accurate) approximate EIG while avoiding model evaluations is made possible by an oﬄine-online
12

(a) P1 at tpred = 1.
(b) P2 at tpred = 1.
(c) P3 at tpred = 1.
(d) P1 at tpred = 2.
(e) P1 at tpred = 4.
(f) P1 at tpred = 8.
Figure 5: Sensor locations chosen by the swapping greedy algorithm for 10 out of 75 candidates for the
parameter-to-QoI maps P1, P2, P3 at time tpred = 1 and also P1 at time tpred = 2, 4, 8.
decomposition and low-rank approximation of certain operators informed by the parameter, data, and
predictive goals of interest. Scalability, as measured by parameter- and data-dimension independence
of the number of model evaluations, is achieved by carefully exploiting the GOOED problem’s intrinsic
low dimensionality as manifested by the rapid spectral decay of several critical operators. To justify
the low-rank approximation of these operators in computing the EIG, we proved an upper bound for
the approximation error in terms of the operators’ truncated eigenvalues. Moreover, we proposed a
new swapping greedy algorithm that is demonstrated to be more eﬀective than the standard greedy
algorithm in our experiments. Numerical experiments with optimal sensor placement for Bayesian
inference of the initial condition of an advection–diﬀusion PDE demonstrated over 1000X speedups
(measured in PDE solves). Future work includes extension to nonlinear Bayesian GOOED problems
with nonlinear parameter-to-observable maps and nonlinear parameter-to-QoI maps.
References
[1] Alen Alexanderian, Philip J. Gloor, and Omar Ghattas. On Bayesian A-and D-optimal experimental
designs in inﬁnite dimensions. Bayesian Analysis, 11(3):671–695, 2016.
[2] Alen Alexanderian, Noemi Petra, Georg Stadler, and Omar Ghattas. A-optimal design of experi-
ments for inﬁnite-dimensional Bayesian linear inverse problems with regularized ℓ0-sparsiﬁcation.
13

(a) optimal design for P1 (b) optimal design for P2 (c) optimal design for P3
(d) random design
Figure 6: Pointwise posterior variance of the parameter at optimal designs for goals P1,P2, P3, compared to a
random design, for 10 sensors. The darker regions represent lower variance.
100
101
number of eigenvalue
10
6
10
5
10
4
10
3
10
2
10
1
100
101
eigenvalue of Hd
dm =  248
dm =  398
dm =  897
dm =  1525
dm =  2685
dm =  5895
100
101
102
103
number of eigenvalue
10
6
10
5
10
4
10
3
10
2
10
1
100
101
eigenvalue of Hd
d = 7
d = 13
d = 32
d = 58
d = 124
d = 232
d = 506
d = 907
d = 1566
Figure 7: Decay of the eigenvalues of ∆Hd with increasing parameter dimension (left) and data (candidate
sensor locations) dimension (right).
SIAM Journal on Scientiﬁc Computing, 36(5):A2122–A2148, 2014.
[3] Alen Alexanderian, Noemi Petra, Georg Stadler, and Omar Ghattas. A fast and scalable method
for A-optimal design of experiments for inﬁnite-dimensional Bayesian nonlinear inverse problems.
SIAM Journal on Scientiﬁc Computing, 38(1):A243–A272, 2016.
[4] Alen Alexanderian, Noemi Petra, Georg Stadler, and Omar Ghattas. Mean-variance risk-averse
optimal control of systems governed by PDEs with random parameter ﬁelds using quadratic
approximations. SIAM/ASA Journal on Uncertainty Quantiﬁcation, 5(1):1166–1192, 2017.
[5] Alen Alexanderian and Arvind K. Saibaba. Eﬃcient D-optimal design of experiments for inﬁnite-
dimensional Bayesian linear inverse problems. SIAM Journal on Scientiﬁc Computing, 40(5):A2956–
A2985, 2018.
[6] Nick Alger, Peng Chen, and Omar Ghattas. Tensor train construction from tensor actions, with
application to compression of large high order derivative tensors. SIAM Journal on Scientiﬁc
Computing, 42(5):A3516–A3539, 2020.
[7] Ilona Ambartsumyan, Wajih Boukaram, Tan Bui-Thanh, Omar Ghattas, David Keyes, Georg
Stadler, George Turkiyyah, and Stefano Zampini. Hierarchical matrix approximations of Hessians
14

arising in inverse problems governed by PDEs. SIAM Journal on Scientiﬁc Computing, 42(5):A3397–
A3426, 2020.
[8] Ahmed Attia, Alen Alexanderian, and Arvind K Saibaba.
Goal-oriented optimal design of
experiments for large-scale bayesian linear inverse problems. Inverse Problems, 34(9):095009, 2018.
[9] Tim Bakker, Herke van Hoof, and Max Welling. Experimental design for MRI by greedy policy
search. Advances in Neural Information Processing Systems, 33, 2020.
[10] O. Bashir, K. Willcox, O. Ghattas, B. van Bloemen Waanders, and J. Hill. Hessian-based model
reduction for large-scale systems with initial condition inputs. International Journal for Numerical
Methods in Engineering, 73:844–868, 2008.
[11] Andrew An Bian, Joachim M Buhmann, Andreas Krause, and Sebastian Tschiatschek. Guarantees
for greedy maximization of non-submodular functions with applications. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70, pages 498–507. JMLR. org, 2017.
[12] Christos Boutsidis, Michael W. Mahoney, and Petros Drineas.
An improved approximation
algorithm for the column subset selection problem. CoRR, abs/0812.4293, 2008.
[13] Tan Bui-Thanh, Carsten Burstedde, Omar Ghattas, James Martin, Georg Stadler, and Lucas C.
Wilcox. Extreme-scale UQ for Bayesian inverse problems governed by PDEs. In SC12: Proceedings
of the International Conference for High Performance Computing, Networking, Storage and
Analysis, 2012.
[14] Tan Bui-Thanh, Omar Ghattas, James Martin, and Georg Stadler. A computational framework
for inﬁnite-dimensional Bayesian inverse problems Part I: The linearized case, with application to
global seismic inversion. SIAM Journal on Scientiﬁc Computing, 35(6):A2494–A2523, 2013.
[15] Kathryn Chaloner and Isabella Verdinelli. Bayesian experimental design: A review. Statistical
Science, 10(3):273–304, 1995.
[16] P. Chen, U. Villa, and O. Ghattas. Hessian-based adaptive sparse quadrature for inﬁnite-dimensional
Bayesian inverse problems. Computer Methods in Applied Mechanics and Engineering, 327:147–172,
2017.
[17] Peng Chen and Omar Ghattas. Hessian-based sampling for high-dimensional model reduction.
International Journal for Uncertainty Quantiﬁcation, 9(2), 2019.
[18] Peng Chen and Omar Ghattas. Projected Stein variational gradient descent. In Advances in
Neural Information Processing Systems, 2020.
[19] Peng Chen, Michael R Haberman, and Omar Ghattas. Optimal design of acoustic metamaterial
cloaks under uncertainty. Journal of Computational Physics, page 110114, 2021.
[20] Peng Chen, Umberto Villa, and Omar Ghattas. Taylor approximation and variance reduction for
PDE-constrained optimal control under uncertainty. Journal of Computational Physics, 385:163–
186, 2019.
[21] Peng Chen, Keyi Wu, Joshua Chen, Thomas O’Leary-Roseberry, and Omar Ghattas. Projected
Stein variational Newton: A fast and scalable Bayesian inference method in high dimensions.
Advances in Neural Information Processing Systems, 2019.
15

[22] Peng Chen, Keyi Wu, and Omar Ghattas. Bayesian inference of heterogeneous epidemic mod-
els: Application to COVID-19 spread accounting for long-term care facilities. arXiv preprint
arXiv:2011.01058, 2020.
[23] Benjamin Crestel, Alen Alexanderian, Georg Stadler, and Omar Ghattas. A-optimal encoding
weights for nonlinear inverse problems, with application to the Helmholtz inverse problem. Inverse
Problems, 33(7):074008, 2017.
[24] Yair Daon and Georg Stadler. Mitigating the inﬂuence of boundary conditions on covariance
operators derived from elliptic PDEs. Inverse Problems and Imaging, 12(5):1083–1102, 2018.
[25] Angelie R Ferrolino, Jose Ernie C Lope, and Renier G Mendoza. Optimal location of sensors for
early detection of tsunami waves. In International Conference on Computational Science, pages
562–575. Springer, 2020.
[26] Pearl H. Flath, Lucas C. Wilcox, Volkan Ak¸celik, Judy Hill, Bart van Bloemen Waanders, and
Omar Ghattas.
Fast algorithms for Bayesian uncertainty quantiﬁcation in large-scale linear
inverse problems based on low-rank partial Hessian approximations. SIAM Journal on Scientiﬁc
Computing, 33(1):407–432, 2011.
[27] Adam Foster, Martin Jankowiak, Elias Bingham, Paul Horsfall, Yee Whye Teh, Thomas Rain-
forth, and Noah Goodman. Variational Bayesian optimal experimental design. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett, editors, Advances in
Neural Information Processing Systems, volume 32, pages 14036–14047. Curran Associates, Inc.,
2019.
[28] Nathan Halko, Per Gunnar Martinsson, and Joel A. Tropp. Finding structure with randomness:
Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Review,
53(2):217–288, 2011.
[29] Xun Huan and Youssef M. Marzouk. Simulation-based optimal Bayesian experimental design for
nonlinear systems. Journal of Computational Physics, 232(1):288–317, 2013.
[30] Xun Huan and Youssef M. Marzouk. Gradient-based stochastic optimization methods in Bayesian
experimental design. International Journal for Uncertainty Quantiﬁcation, 4(6):479–510, 2014.
[31] Xun Huan and Youssef M Marzouk. Sequential bayesian optimal experimental design via approxi-
mate dynamic programming. arXiv preprint arXiv:1604.08320, 2016.
[32] Tobin Isaac, Noemi Petra, Georg Stadler, and Omar Ghattas. Scalable and eﬃcient algorithms for
the propagation of uncertainty from data through inference to prediction for large-scale problems,
with application to ﬂow of the Antarctic ice sheet. Journal of Computational Physics, 296:348–368,
September 2015.
[33] Jayanth Jagalur-Mohan and Youssef Marzouk. Batch greedy maximization of non-submodular
functions: Guarantees and applications to experimental design. arXiv preprint arXiv:2006.04554,
2020.
[34] Kirthevasan Kandasamy, Willie Neiswanger, Reed Zhang, Akshay Krishnamurthy, JeﬀSchneider,
and Barnabas Poczos. Myopic posterior sampling for adaptive goal oriented design of experiments.
In International Conference on Machine Learning, pages 3222–3232. PMLR, 2019.
16

[35] Steven Kleinegesse and Michael U Gutmann. Bayesian experimental design for implicit models by
mutual information neural estimation. In International Conference on Machine Learning, pages
5316–5326. PMLR, 2020.
[36] Quan Long, Marco Scavino, Ra´ul Tempone, and Suojin Wang. Fast estimation of expected
information gains for Bayesian experimental designs based on Laplace approximations. Computer
Methods in Applied Mechanics and Engineering, 259:24–39, 2013.
[37] Nora Loose and Patrick Heimbach. Leveraging uncertainty quantiﬁcation to design ocean climate
observing systems. Journal of Advances in Modeling Earth Systems, pages 1–29, January 2021.
[38] T. O’Leary-Roseberry, U. Villa, P. Chen, and O. Ghattas. Derivative-informed projected neural net-
works for high-dimensional parametric maps governed by PDEs. https://arxiv.org/abs/2011.15110,
2020.
[39] Noemi Petra, James Martin, Georg Stadler, and Omar Ghattas. A computational framework for
inﬁnite-dimensional Bayesian inverse problems: Part II. Stochastic Newton MCMC with application
to ice sheet ﬂow inverse problems. SIAM Journal on Scientiﬁc Computing, 36(4):A1525–A1555,
2014.
[40] Constantine Pozrikidis. An introduction to grids, graphs, and networks. Oxford University Press,
2014.
[41] Arvind K Saibaba, Alen Alexanderian, and Ilse CF Ipsen. Randomized matrix-free trace and
log-determinant estimators. Numerische Mathematik, 137(2):353–395, 2017.
[42] A. Spantini, T. Cui, K. Willcox, L. Tenorio, and Y. M. Marzouk. Goal-oriented optimal approxima-
tions of Bayesian linear inverse problems. SIAM Journal on Scientiﬁc Computing, 39(5):S167–S196,
2017.
[43] Keyi Wu, Peng Chen, and Omar Ghattas. A fast and scalable computational framework for large-
scale and high-dimensional Bayesian optimal experimental design. arXiv preprint arXiv:2010.15196,
2020.
[44] Man-Chung Yue. A matrix generalization of the hardy-littlewood-p´olya rearrangement inequality
and its applications. arXiv preprint arXiv:2006.08144, 2020.
[45] Sue Zheng, David Hayden, Jason Pacheco, and John W Fisher III. Sequential Bayesian experimental
design with variable cost structure. Advances in Neural Information Processing Systems, 33, 2020.
17

Appendix A: Proof of Theorem 3.2
To start with, we introduce the Weinstein-Aronszajn identity in Proposition .1 which is proven in
[40].
Proposition .1. Let A and B be matrices of size m × n and n × m respectively, then
det(In×n + BA) = det(Im×m + AB).
(33)
Proof. Considering the design problem deﬁned in Section 2.2.2, for each design with design matrix W,
we have
F(W) = WFd, and Γn(W) = WΓd
nW T .
(34)
We can then reformulate Ψρ with the deﬁnition in (18) as
Ψρ(W) = 1
2 log det(I + eHρ
m)
= 1
2 log det(I + Σ
1
2pr(WFdP†)∗Γ−1
η (W)(WFdP†)Σ
1
2pr).
(35)
where
Γη(W) = Γn(W) + F(W)(Γpr −ΓprP∗Σ−1
pr PΓpr)F∗(W)
= WΓd
nW T + WFd(Γpr −ΓprP∗Σ−1
pr PΓpr)F∗
dW T
= W(Γd
n + Fd(Γpr −ΓprP∗Σ−1
pr PΓpr)F∗
d)W T
= W(Γd
n + FdΓprF∗
d
|
{z
}
:=Hd∈Rd×d
−FdΓprP∗Σ−1
pr PΓprF∗
d
|
{z
}
:=Hρ
d∈Rd×d
)W T
= W(Γd
n + Hd −Hρ
d
|
{z
}
:=∆Hd
)W T
= W(Γd
n + ∆Hd)W T .
(36)
To this end, we have
Ψ(W) = 1
2 logdet

I + Σ
1
2pr(WFdP†)∗Γ−1
η (W)(WFdP†)Σ
1
2pr

= 1
2 logdet


I + Σ
1
2pr(WFdP†)∗L
|
{z
}
A
LT (WFdP†)Σ
1
2pr
|
{z
}
B



= 1
2 logdet


I + LT (WFdP†)Σ
1
2pr
|
{z
}
B
Σ
1
2pr(WFdP†)∗L
|
{z
}
A



= 1
2 logdet
 I + LT (WFdP†)Σpr(WFdP†)∗L

= 1
2 logdet
 I + LT WFdΓprP∗Σ−1
pr ΣprΣ−1
pr PΓprF∗
dW T L

= 1
2 logdet
 I + LT WFdΓprP∗Σ−1
pr PΓprF∗
dW T L

= 1
2 logdet
 I + LT WHρ
dW T L

,
(37)
where we use the Cholesky decomposition Γ−1
η
= LLT in the second equality, Proposition .1 in the
third, deﬁnition of P† from (16) in the ﬁfth, and deﬁnition of Hρ
d from (21) in the last.
18

Appendix B: Proof of Theorem 3.3
To start with, we introduce necessary properties that are proven in [43] for Proposition .2, [5] for
Proposition .3 and [44] for Proposition .4.
Proposition .2. Let A and B be matrices of size m × n and n × m respectively, then AB and BA
have the same non-zero eigenvalues.
Proposition .3. Let A, B ∈Cn×n be Hermitian positive semideﬁnite with A ≥B (i.e., A −B is
Hermitian positive semideﬁnite), then
0 ≤log det(I + A) −log det(I + B) ≤log det(I + A −B).
(38)
Proposition .4. Let f : R+ →R be a continuous function that is diﬀerentiable on R+ (with x ≥0
for x ∈R+). If the function x 7→xf ′(x) is monotonically increasing on R+. Then for any matrices
A, B ∈Rn×m, it holds that
n
X
i=1
f(υi(ABT )) ≤
n
X
i=1
f(υi(A)υi(B))
(39)
where υi(·) denotes the singular values of matrices sorted in non-increasing order.
Lemma .5. Let A ∈Rn×m, B ∈Rm×m, AT A and B are Hermitian positive semideﬁnite, then
log det(I + ABAT ) ≤
m
X
i=1
log(1 + υi(AT A)υi(B)).
(40)
Proof. Since log det(I + ABAT ) = Pn
i=1 log(1 + υi(ABAT )) = Pn
i=1 log(1 + υ2
i (AB1/2)), let f(x) =
log(1 + x2), which satisﬁes Proposition .4, we have
n
X
i=1
log(1 + υ2
i (AB1/2)) ≤
n
X
i=1
log(1 + υ2
i (A)υ2
i (B1/2)) =
m
X
i=1
log(1 + υi(AT A)υi(B)).
(41)
To this end, we are at the place to prove Theorem 3.3.
Proof. Denote the eigenvalue decompositions of Hρ
d and ∆Hd as
Hρ
d = UkZkU T
k + U⊥Z⊥U T
⊥, and ∆Hd = VlΛlV T
l
+ V⊥Λ⊥V T
⊥,
(42)
where (Zk, Uk), (Vl, Λl) represent the dominant eigenpairs, and (Z⊥, U⊥), (V⊥, Λ⊥) represent the re-
maining eigenpairs. By triangle inequality, we have
|Ψρ(W) −ˆΨρ(W)| = |1
2 logdet
 Ir×r + LT WHρ
dW T L

−1
2 logdet

Ir×r + ˆLT W ˆHρ
dW T ˆL

|
≤|1
2 logdet
 Ir×r + LT WHρ
dW T L

−1
2 logdet

Ir×r + LT W ˆHρ
dW T L

|
|
{z
}
(a)
+ |1
2 logdet

Ir×r + LT W ˆHρ
dW T L

−1
2 logdet

Ir×r + ˆLT W ˆHρ
dW T ˆL

|
|
{z
}
(b)
.
(43)
19

We ﬁrst look at (a). By Proposition .3 and note that (Hρ
d −ˆHρ
d) = U⊥Z⊥U T
⊥is Hermitian positive
semideﬁnite, we have
(a) ≤1
2 logdet

Ir×r + LT WHρ
dW T L −LT W ˆHρ
dW T L

= 1
2 logdet

Ir×r + LT W(Hρ
d −ˆHρ
d)W T L

= 1
2 logdet
 Ir×r + LT WU⊥Z⊥U T
⊥W T L

.
(44)
Then applying Proposition .1, we have
(a) = 1
2 logdet

Ir×r + LT WU⊥Z1/2
⊥Z1/2
⊥U T
⊥W T L

= 1
2 logdet

I(d−k)×(d−k) + Z1/2
⊥U T
⊥W T LLT WU⊥Z1/2
⊥

= 1
2 logdet

I(d−k)×(d−k) + Z1/2
⊥U T
⊥W T (W(Γd
n + ∆Hd)W T )−1WU⊥Z1/2
⊥

.
(45)
Applying Lemme .5, let A = Z1/2
⊥U T
⊥W T , B = (W(Γd
n + ∆Hd)W T )−1, we have
(a) ≤1
2
X
i
log(1 + υi(WU⊥Z1/2
⊥Z1/2
⊥U T
⊥W T )υi((W(Γd
n + ∆Hd)W T )−1))
= 1
2
X
i
log(1 + υi(WU⊥Z⊥U T
⊥W T )υi((W(Γd
n + ∆Hd)W T )−1)).
(46)
By Proposition 3.1, ∆Hd = Cov[Fd(I −P†P)m], is a covariance matrix, thus is positive semideﬁnite.
The smallest eigenvalue of Γd
n + ∆Hd is greater than the smallest eigenvalue of Γd
n. Hence υi(W(Γd
n +
∆Hd)W T )) ≥σ2
min, i.e., υi((W(Γd
n + ∆Hd)W T )−1) ≤1/σ2
min.
Note that υi(WU⊥Z⊥U T
⊥W T ) ≤
υi(U⊥Z⊥U T
⊥) = ζi. Thus we have
(a) ≤1
2
d
X
i=k+1
log(1 + ζi/σ2
min).
(47)
Then we turn to second part (b), with Proposition .1 and Proposition .3, we have
(b) = | −1
2 logdet
 Ir×r + LT WUkZkU T
k W T L

+ 1
2 logdet

Ir×r + ˆLT WUkZkU T
k W T ˆL

|
= | −1
2 logdet

Ik×k + Z1/2
k
U T
k W T LLT WUkZ1/2
k

+ 1
2 logdet

Ik×k + Z1/2
k
U T
k W T ˆLˆLT WUkZ1/2
k

|
≤1
2 logdet

Ik×k + Z1/2
k
U T
k W T ˆLˆLT WUkZ1/2
k
−Z1/2
k
U T
k W T LLT WUkZ1/2
k

= 1
2 logdet

Ik×k + Z1/2
k
U T
k W T (ˆLˆLT −LLT )WUkZ1/2
k

= 1
2 logdet


Ik×k + Z1/2
k
U T
k W T ((W(Γd
n + ∆ˆHd)W T )−1 −(W(Γd
n + ∆Hd)W T )−1)
|
{z
}
(c)
WUkZ1/2
k


.
(48)
Note that (A+B)−1 = A−1 −A−1B(A+B)−1, let A = W(Γd
n +∆ˆHd)W T , B = W(∆Hd −∆ˆHd)W T =
WV⊥Λ⊥V T
⊥W T , we have
(A + B)−1 = (W(Γd
n + ∆Hd)W T )−1
= (W(Γd
n + ∆ˆHd)W T )−1 −(W(Γd
n + ∆ˆHd)W T )−1WV⊥Λ⊥V T
⊥W T (W(Γd
n + ∆Hd)W T )−1
⇒(c) = (W(Γd
n + ∆ˆHd)W T )−1WV⊥Λ⊥V T
⊥W T (W(Γd
n + ∆Hd)W T )−1
(49)
20

Then we can see that
(b) ≤1
2 logdet

Ik×k + Z1/2
k
U T
k W T (W(Γd
n + ∆ˆHd)W T )−1WV⊥Λ⊥V T
⊥W T (W(Γd
n + ∆Hd)W T )−1WUkZ1/2
k

= 1
2 logdet

I(d−l)×(d−l) + Λ1/2
⊥V T
⊥W T (W(Γd
n + ∆ˆHd)W T )−1WUkZ1/2
k
Z1/2
k
U T
k W T (W(Γd
n + ∆Hd)W T )−1WV⊥Λ1/2
⊥

= 1
2 logdet

I(d−l)×(d−l) + Λ1/2
⊥V T
⊥W T (W(Γd
n + ∆ˆHd)W T )−1WUkZkU T
k W T (W(Γd
n + ∆Hd)W T )−1WV⊥Λ1/2
⊥

.
(50)
Applying Lemma .5, we have
(b) ≤1
2
X
i
log(1 + υi(WV⊥Λ⊥V T
⊥W T )υi((W(Γd
n + ∆ˆHd)W T )−1WUkZkU T
k W T (W(Γd
n + ∆Hd)W T )−1))
≤1
2
k
X
i=l+1
log(1 + λiζ1/σ4
min),
(51)
where we have used
υi((W(Γd
n + ∆ˆHd)W T )−1WUkZkU T
k W T (W(Γd
n + ∆Hd)W T )−1)
≤v1((W(Γd
n + ∆ˆHd)W T )−1)v1(WUkZkU T
k W T )v1((W(Γd
n + ∆Hd)W T )−1) ≤ζ1/σ4
min
(52)
for i ≤k in the last inequality. Note that it vanishes for i > k as Zk has rank not larger than k.
Combining (47) and (51),
|Ψρ(W) −ˆΨρ(W)| ≤(a) + (b) ≤1
2
d
X
i=k+1
log(1 + ζi/σ2
min) + 1
2
k
X
i=l+1
log(1 + λiζ1/σ4
min).
(53)
Appendix C: Low-rank approximation
To compute the low-rank approximations of ∆Hd and Hρ
d as described in Section 3.3, we present
the randomized SVD algorithm for these two quantities. Recall the explicit forms of ∆Hd and Hρ
d as
Hρ
d = FdΓprP∗Σ−1
pr PΓprF∗
d, ∆Hd = FdΓprF∗
d −FdΓprP∗Σ−1
pr PΓprF∗
d.
(54)
Algorithm 2: Randomized SVD to compute H with low rank k
1: Generate i.i.d. Gaussian matrix Ω∈Rd×(k+p) with an oversampling parameter p very small (e.g., p = 10).
2: Compute Y = HΩ.
3: Compute the QR factorization Y = QR satisfying QT Q = I.
4: Compute B = QT HQ.
5: Solve an eigenvalue problem for B such that B = ZΣZT .
6: Form Uk = QZ[1 : k] and Σk = Σ[1 : k, 1 : k].
We see that this is a matrix-free eigensolver. Steps 2 and 4 represent ∆Hd action on O(2(l + p))
vectors and Hρ
d action on O(2(k + p)) vectors. In terms of the total actions, it requires 2(2l + k + p)
forward operator F and 2(l + k + p) of its adjoint F∗, 2(k + l + p) prediction operator P and its adjoint
P∗.
21

For the contaminant problem given in Section 4.1, the concentration ﬁeld u(x, t) is given by
ut −k∆u + v · ∇u = 0 in D × (0, T),
u(·, 0) = m in D,
k∇u · n = 0 on ∂D × (0, T),
(55)
we can form the parameter-to-observable map Fm as the discretized value of Bu(m) where B is the
pointwise observation operator. The adjoint problem is a terminal value problem which can be solved
backwards in time by the equation:
−pt −∇· (pv) −k∆p = B∗y in D × (0, T),
p(·, T) = 0 in D,
(pv + k∇p) · n = 0 on ∂D × (0, T).
(56)
Then we can deﬁne the adjoint of the parameter-to-observable map F∗y as the discretized value of
p(x, 0) for any y.
22

