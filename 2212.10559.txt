Why Can GPT Learn In-Context?
Language Models Secretly Perform Gradient Descent as Meta-Optimizers
Damai Dai†∗, Yutao Sun∥∗, Li Dong‡, Yaru Hao‡, Zhifang Sui†, Furu Wei‡
† Peking University
∥Tsinghua University
‡ Microsoft Research
https://github.com/microsoft/LMOps
Abstract
Large pretrained language models have shown
surprising In-Context Learning (ICL) ability.
With a few demonstration input-label pairs,
they can predict the label for an unseen input
without additional parameter updates. Despite
the great success in performance, the work-
ing mechanism of ICL still remains an open
problem. In order to better understand how
ICL works, this paper explains language mod-
els as meta-optimizers and understands ICL
as a kind of implicit ﬁnetuning.
Theoreti-
cally, we ﬁgure out that the Transformer at-
tention has a dual form of gradient descent
based optimization. On top of it, we under-
stand ICL as follows: GPT ﬁrst produces meta-
gradients according to the demonstration ex-
amples, and then these meta-gradients are ap-
plied to the original GPT to build an ICL
model. Experimentally, we comprehensively
compare the behavior of ICL and explicit ﬁne-
tuning based on real tasks to provide empiri-
cal evidence that supports our understanding.
The results prove that ICL behaves similarly
to explicit ﬁnetuning at the prediction level,
the representation level, and the attention be-
havior level. Further, inspired by our under-
standing of meta-optimization, we design a
momentum-based attention by analogy with
the momentum-based gradient descent algo-
rithm. Its consistently better performance over
vanilla attention supports our understanding
again from another aspect, and more impor-
tantly, it shows the potential to utilize our un-
derstanding for future model designing.
1
Introduction
In recent years, large pretrained language models,
especially in Transformer-based architectures (e.g.,
GPT; Brown et al. 2020), have shown strong emer-
gent In-Context Learning (ICL) ability. Different
from ﬁnetuning which needs additional parame-
ter updates, ICL just needs several demonstration
∗Contribution during internship at Microsoft Research.
(Sentence, ?)
… 
Demonstration Examples
Query Example
Feed-Forward Network
Self-Attention
(Sentence1, Answer1) (Sentence2, Answer2)
Meta-Gradients
Answer
… 
GPT
In-Context Learning
Finetuning
GPT
(Sentence1, Answer1) 
GPT
(Sentence2, Answer2) 
Back-Propagation
Forward 
Computation
𝚫𝚫𝑾𝑾𝐅𝐅𝐅𝐅
𝚫𝚫𝑾𝑾𝐈𝐈𝐈𝐈𝐈𝐈
Gradients
Dual 
View
Figure 1: According to the demonstration examples,
GPT produces meta-gradients for In-Context Learn-
ing (ICL) through forward computation. ICL works by
applying these meta-gradients to the model through at-
tention. The meta-optimization process of ICL shares
a dual view with ﬁnetuning that explicitly updates the
model parameters with back-propagated gradients.
examples prepended before the original input, and
then the model can predict the label for even unseen
inputs. On numerous downstream tasks, a large
GPT model can achieve a quite great performance,
which even exceeds some smaller models with su-
pervised ﬁnetuning. However, although ICL has
achieved great success in performance, the work-
ing mechanism of it is still an open problem to be
investigated.
In this paper, we explain ICL as a process of
meta-optimization and attempt to build connections
between GPT-based ICL and ﬁnetuning. Concen-
trating on the attention modules, we ﬁgure out that
the Transformer attention has a dual form of gra-
dient descent based optimization. On top of it, we
arXiv:2212.10559v2  [cs.CL]  21 Dec 2022

propose a novel perspective to explain ICL: (1) a
pretrained GPT serves as a meta-optimizer; (2) it
produces meta-gradients according to the demon-
stration examples through forward computation;
(3) the meta-gradients are applied to the original
language model through attention to build an ICL
model. As illustrated in Figure 1, ICL and explicit
ﬁnetuning share a dual view of gradient descent
based optimization. The only difference is that
ICL produces meta-gradients through forward com-
putation, while ﬁnetuning computes gradients by
back-propagation. Therefore, it is reasonable to un-
derstand ICL as some kind of implicit ﬁnetuning.
In order to provide empirical evidence to sup-
port our understanding, we conduct comprehensive
experiments based on real tasks. On six classiﬁ-
cation tasks, we compare the model predictions,
attention outputs, and attention scores of pretrained
GPT models in the ICL and ﬁnetuning settings.
As expected, the behavior of ICL is highly sim-
ilar to explicit ﬁnetuning at all of the prediction
level, the representation level, and the attention be-
havior level. These results are strong evidence to
prove the reasonability of our understanding that
ICL performs implicit ﬁnetuning.
Further, we attempt to take advantage of our
understanding of meta-optimization for model de-
signing. To be speciﬁc, we design a momentum-
based attention, which regards the attention val-
ues as meta-gradients and applies the momentum
mechanism to them. Experiments on both lan-
guage modeling and in-context learning show that
our momentum-based attention consistently outper-
forms vanilla attention, which supports our under-
standing of meta-optimization again from another
aspect. We note that beyond this preliminary ap-
plication, our understanding of meta-optimization
may have more potential to be used to aid in model
designing, which is worth investigating in the fu-
ture.
Our contributions are summarized as follows:
• We ﬁgure out a dual form between Trans-
former attention and gradient descent based
optimization, and explain language models as
meta-optimizers.
• We build connections between ICL and ex-
plicit ﬁnetuning and propose to understand
ICL as a kind of implicit ﬁnetuning.
• We provide several lines of empirical evidence
to prove that ICL and explicit ﬁnetuning be-
have similarly at multiple levels.
• We design a momentum-based attention that
achieves consistent performance improve-
ments, which shows the potential of our under-
standing of meta-optimization to aid in future
model designing.
2
Background
2.1
In-Context Learning with GPT
In this paper, we focus on ICL for classiﬁca-
tion tasks using GPT (Brown et al., 2020).
A
GPT model is stacked with L identical Trans-
former (Vaswani et al., 2017) decoder layers where
each layer consists of an attention module and a
feed-forward network. For a classiﬁcation task,
given a query input text x and a candidate an-
swer set Y = {y1, y2, . . . , ym}, we need to pre-
dict a label ˆy conditional on n demonstration
examples C = {(x′
1, y′
1), (x′
2, y′
2), . . . , (x′
n, y′
n)},
where (x′
i, y′
i) is an input-label pair different from
the query one. Formally, given a GPT model M,
we ﬁrst compute the probability of each answer yj:
PM(yj | C, x).
(1)
Since the label space is restricted for classiﬁca-
tion, we predict the ﬁnal answer ˆy by selecting
the answer with the highest probability from the
candidate answer set Y :
ˆy = yarg maxj PM(yj|C,x).
(2)
In practice, we usually use a pre-deﬁned template
to format the demonstrations and prepend them
before the query input. Let T (·) be the function
that formats an example, e.g.:
T (x, y) = Sentence: x. Sentiment: y.
(3)
The contextual model input I is organized like
T (x′
1, y′
1) T (x′
2, y′
2) ... T (x′
n, y′
n) T (x, _). (4)
Feeding this contextual input into M, the probabil-
ity of an answer yj is computed as
lj = M(I) · eyj,
(5)
PM(yj | C, x) = softmax(lj),
(6)
where M(I) denotes the output hidden state at the
last token position; eyj denotes the word embed-
ding of yj; and lj is the logit corresponding to the
j-th answer.

2.2
Dual Form Between Gradient Descent
Based Optimization and Attention
The idea in this paper to explain language models
as meta-optimizers is inspired by Aizerman et al.
(1964); Irie et al. (2022). They present that linear
layers optimized by gradient descent have a dual
form of linear attention. Let W0, ∆W ∈Rdout×din
be the initialized parameter matrix and the update
matrix, respectively, and x ∈Rdin be the input rep-
resentation. A linear layer optimized by gradient
descent can be formulated as
F(x) = (W0 + ∆W) x.
(7)
In the back-propagation algorithm, ∆W is com-
puted by accumulating the outer products of the
historic input representations x′T
i
∈Rdin and the
gradients ei ∈Rdout of their corresponding outputs:
∆W =
X
i
ei ⊗x′T
i .
(8)
Combing Equation (7) and Equation (8), we can de-
rive the dual form between gradient descent based
optimization and linear attention:
F(x) = (W0 + ∆W) x
=W0x + ∆Wx
=W0x +
X
i

ei ⊗x′T
i

x
=W0x +
X
i
ei

x′T
i x

=W0x + LinearAttn
 E, X′, x

,
(9)
where LinearAttn(V, K, q) denotes the linear at-
tention operation, where we regard the historic out-
put gradients E as values, the historic inputs X′ as
keys, and the current input x as the query.
3
In-Context Learning (ICL) Performs
Implicit Finetuning
We ﬁrst qualitatively analyze the Transformer atten-
tion under a relaxed linear attention form to ﬁgure
out a dual form between it and gradient descent
based optimization. Then, we compare ICL with
explicit ﬁnetuning and build connections between
these two optimization forms. Based on these theo-
retical ﬁndings, we propose to understand ICL as a
kind of implicit ﬁnetuning.
3.1
Transformer Attention as
Meta-Optimization
Let x ∈Rd be the input representation of a query
token t, and q = WQx ∈Rd′ be the attention
query vector. In the ICL setting, the attention result
of a head is formulated as
FICL(q) = Attn(V, K, q)
=WV [X′; X] softmax
 
(WK[X′; X])T q
√
d
!
, (10)
where WQ, WK, WV ∈Rd′×d are the projection
matrices for computing the attention queries, keys,
and values, respectively;
√
d denotes the scaling
factor; X denotes the input representations of query
tokens before t; X′ denotes the input represen-
tations of the demonstration tokens; and [X′; X]
denotes the matrix concatenation. For ease of qual-
itative analysis, we approximate the standard atten-
tion to a relaxed linear attention by removing the
softmax operation and the scaling factor:
FICL(q) = Attn(V, K, q)
≈WV [X′; X]
 WK[X′; X]
T q
= WV X (WKX)T q + WV X′  WKX′T q
= e
FICL(q).
(11)
We deﬁne WZSL = WV X (WKX)T as the ini-
tialized parameters to be updated since WZSLq is
the attention result in the Zero-Shot Learning (ZSL)
setting, where no demonstrations are given. Fol-
lowing the reverse direction of Equation (9), we
derive a dual form of the Transformer attention:
e
FICL(q) = WZSLq + WV X′  WKX′T q
=WZSLq + LinearAttn
 WV X′, WKX′, q

=WZSLq +
X
i
WV x′
i
 WKx′
i
T q

=WZSLq +
X
i

WV x′
i ⊗
 WKx′
i
T 
q
=WZSLq + ∆WICLq
= (WZSL + ∆WICL) q.
(12)
As shown in the above equations, the attention to
the demonstration tokens is equivalent to parameter
updates ∆WICL that take effect on WZSL. In addi-
tion, By analogy with Equation (9), we can regard
WV X′ as some meta-gradients, which are used to
compute the update matrix ∆WICL.
In summary, we explain ICL as a process of
meta-optimization: (1) a Transformer-based pre-
trained language model serves as a meta-optimizer;
(2) it produces meta-gradients according to the
demonstration examples through forward computa-
tion; (3) through attention, the meta-gradients are
applied to the original language model to build an
ICL model.

3.2
Comparing ICL with Finetuning
In order to compare the meta-optimization of ICL
with explicit optimization, we design a speciﬁc ﬁne-
tuning setting as a baseline for comparison. Con-
sidering that ICL directly takes effect on only the
attention keys and values, our ﬁnetuning setting
also updates only the parameters for the key and
value projection. Also in the relaxed linear atten-
tion form, the attention result of a ﬁnetuned head
is formulated as
e
FFT(q) = (WV + ∆WV )XXT (WK + ∆WK)T q
= (WZSL + ∆WFT) q,
(13)
where ∆WK and ∆WV denote the parameter up-
dates to WK and WV , respectively, which are ac-
quired by back-propagation from some training
objectives; and ∆WFT is the updates to WZSL in-
troduced by ﬁnetuning.
For a more fair comparison with ICL, we further
restrict the ﬁnetuning setting as follows: (1) we
specify the training examples as the demonstration
examples for ICL; (2) we train each example for
only one step in the same order as demonstrated for
ICL; (3) we format each training example with the
same template used for ICL T (x′
i, y′
i) and use the
causal language modeling objective for ﬁnetuning.
Comparing ICL and this ﬁnetuning setting, we
ﬁnd that ICL has many properties in common with
ﬁnetuning. We organize these common properties
from the following four aspects.
Both Perform Gradient Descent
Comparing
Equation (12) and Equation (13), we ﬁnd that both
ICL and ﬁnetuning introduce updates (∆WICL v.s.
∆WFT) to WZSL, which can both be regarded as
gradient descent. The only difference is that ICL
produces meta-gradients by forward computation
while ﬁnetuning acquires real gradients by back-
propagation.
Same
Training
Information
The
meta-
gradients of ICL are produced according to
the demonstration examples.
The gradients of
ﬁnetuning are also derived from the same training
examples. That is to say, ICL and ﬁnetuning share
the same source of training information.
Same Causal Order of Training Examples
ICL and our ﬁnetuning setting share the same
causal order of training examples.
ICL uses
decoder-only Transformers so the subsequent to-
kens in the demonstrations will not affect the pre-
ceding ones. For our ﬁnetuning setting, we use the
Dataset
# Valid. Examples
# Labels
CB
56
3
SST2
872
2
SST5
1101
5
Subj
2000
2
MR
1066
2
AGNews
7600
4
Table 1: Statistics of six classiﬁcation datasets.
same order of training examples and train only one
epoch, so we can also guarantee that the subsequent
examples have no effect on the preceding ones.
Both Aim at Attention
Compared with zero-
shot learning, the direct effect of ICL and our ﬁne-
tuning are both restricted to the computation of
attention keys and values. For ICL, the model pa-
rameters are unchanged and it encodes demonstra-
tion information into additional keys and values
to change the attention behavior. For ﬁnetuning,
due to our restriction, the training information can
be introduced to only the projection matrices for
attention keys and values as well.
Considering all of these common properties be-
tween ICL and ﬁnetuning, we think it is reasonable
to understand ICL as a kind of implicit ﬁnetuning.
In the rest of this paper, we compare ICL and ﬁne-
tuning empirically from multiple aspects to provide
quantitative results to support this understanding.
4
Experiments
4.1
Tasks and Datasets
We compare ICL and ﬁnetuning based on six
datasets spanning three classiﬁcation tasks. SST-
2 (Socher et al., 2013), SST-5 (Socher et al., 2013),
MR (Pang and Lee, 2005) and Subj (Pang and Lee,
2004) are four datasets for sentiment classiﬁcation;
AGNews (Zhang et al., 2015) is a topic classiﬁ-
cation dataset; and CB (de Marneffe et al., 2019)
is used for natural language inference. The statis-
tics of the validation examples and label types are
summarized in Table 1.
4.2
Experimental Settings
In our experiments, we use two GPT-like pretrained
language models with 1.3B and 2.7B model param-
eters, respectively, which are released by fairseq1.
In the rest of this paper, we call them GPT 1.3B and
1https://github.com/facebookresearch/fairseq

GPT 2.7B for short. All experiments are conducted
on NVIDIA V100 GPUs with 32 GB memory.
For each task, we use the same template to for-
mat examples for Zero-Shot Learning (ZSL), ICL,
and ﬁnetuning. Details of the templates used for
each task are provided in Appendix A. The answer
prediction processes for ZSL and ﬁnetuning are the
same with ICL as described in Section 2.1, except
that they do not have demonstration examples.
For ICL, we ﬁx the number of demonstration
examples to 32 and tune the random seed for each
task to ﬁnd a set of demonstration examples that
achieves the best validation performance. For ﬁne-
tuning, we use the same demonstration examples
for ICL as the training examples and use SGD as
the optimizer. For a fair comparison, we ﬁne-tune
the model for only one epoch and the training exam-
ples are provided in the same order as demonstrated
for ICL. We tune the learning rate for ﬁnetuning
and select the one that achieves the best validation
performance. Details of the search range and se-
lected value for the random seeds and learning rates
are shown in Appendix B.
4.3
Metrics
We design three metrics to measure the similarity
between ICL and ﬁnetuning at three different levels:
the prediction level, the representation level, and
the attention behavior level.
Recall to Finetuning Predictions (Rec2FTP)
At the prediction level, this metric measures ICL
can cover how much behavior of ﬁnetuning. We
ﬁrst count NFT, the number of query examples that
ﬁnetuning can predict correctly but ZSL cannot.
Then, among these examples, we count Nboth, the
number that ICL can also predict correctly. Finally,
we compute the Rec2FTP score as Nboth
NFT . A higher
Rec2FTP score suggests that ICL covers more be-
havior of ﬁnetuning at the prediction level.
Similarity
of
Attention
Output
Updates
(SimAOU)
This metric measures the similarity
between the effects that ICL and ﬁnetuning have
on ZSL at the representation level. For a query
example, let h(l)
X denote the output representation
of the last token at the l-th attention layer in the X
setting. The updates of ICL and ﬁnetuning com-
pared with ZSL are h(l)
ICL −h(l)
ZSL and h(l)
FT −h(l)
ZSL,
respectively.
We compute the cosine between
these two updates to get the SimAOU score at the
l-th layer. A higher SimAOU score means ICL
Model
Task
ZSL
FT
ICL
GPT 1.3B
CB
37.5
57.1
57.1
SST2
70.5
73.5
92.7
SST5
39.3
42.0
45.0
Subj
72.6
78.8
90.0
MR
65.9
68.2
89.0
AGNews
46.3
53.7
79.2
GPT 2.7B
CB
42.9
60.7
55.4
SST2
71.4
91.2
95.0
SST5
35.9
46.6
46.5
Subj
75.2
85.6
90.3
MR
60.9
78.8
91.3
AGNews
39.8
66.6
80.3
Table 2: Validation accuracy in the ZSL, ﬁnetuning,
and ICL settings on six classiﬁcation datasets.
is more inclined to update the attention output
representation in the same direction as ﬁnetuning.
Similarity of Attention Map (SimAM)
This
metric measures the similarity of the attention be-
havior of ICL and ﬁnetuning. For a query example,
let m(l,h)
X
denote the attention weights before soft-
max of the last token at the h-th attention head in
the l-th attention layer in the X setting. For ICL,
we omit the attention to the demonstration tokens
and only monitor the attention weights to the query
input. We compute the cosine between m(l,h)
ICL and
m(l,h)
FT
and then average the similarity across the
attention heads to get the SimAM score at each
layer. A higher SimAM score indicates that the
attention weights that ICL and ﬁnetuning pay to
the query tokens are more similar.
4.4
Results
Accuracy
We ﬁrst show the validation accuracy
in the ZSL, ICL, and ﬁnetuning settings on six
classiﬁcation datasets in Table 2. Compared with
ZSL, ICL and ﬁnetuning both achieve considerable
improvements, which means the optimizations they
make are both helpful to these downstream tasks.
In addition, we ﬁnd that ICL is better at few-shot
scenarios than ﬁnetuning.
Rec2FTP
We show the Rec2FTP scores for two
GPT models on six datasets in Table 3. As shown
in the table, on average, ICL can correctly predict
87.64% of the examples that ﬁnetuning can cor-
rect from ZSL. These results indicate that at the
prediction level, ICL can cover most of the correct

Model
Task
Rec2FTP
SimAOU
Random SimAOU
SimAM
ZSL SimAM
GPT 1.3B
CB
91.67
0.189
0.004
0.386
0.152
SST2
86.32
0.128
0.003
0.608
0.555
SST5
70.16
0.173
0.004
0.430
0.391
Subj
84.39
0.070
0.004
0.504
0.378
MR
92.14
0.188
0.003
0.513
0.398
AGNews
85.41
0.155
0.003
0.536
0.152
GPT 2.7B
CB
100.00
0.184
-0.001
0.362
0.228
SST2
93.87
0.113
0.003
0.687
0.687
SST5
74.32
0.142
0.001
0.411
0.380
Subj
90.46
0.100
0.004
0.375
0.346
MR
95.44
0.120
0.001
0.346
0.314
AGNews
87.48
0.210
-0.003
0.305
0.172
Table 3: Rec2FTP, SimAOU, and SimAM scores on six classiﬁcation datasets. The demonstrated SimAOU and
SimAM scores are averaged across examples and layers. For comparison, we also show two baseline metrics for
SimAOU and SimAM, respectively. On all of these datasets, ICL tends to perform similar behavior to ﬁnetuning
at the prediction, representation, and attention behavior levels.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
Layer
0.5
0.0
0.5
SimAOU
GPT 1.3B
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
Layer
0.5
0.0
0.5
SimAOU
GPT 2.7B
Figure 2: Statistics of the SimAOU scores at different layers. The yellow lines denote medians.
behavior of ﬁnetuning.
SimAOU
We present the SimAOU scores aver-
aged across examples and layers for two GPT mod-
els on six datasets in Table 3. For comparison, we
also provide a baseline metric (Random SimAOU)
that computes the similarity between ICL updates
and randomly generated updates. From the table,
we ﬁnd that ICL updates are much more similar to
ﬁnetuning updates than to random updates, which
means at the representation level, ICL tends to
change the attention results in the same direction
as ﬁnetuning changes.
SimAM
Table 3 also demonstrates the SimAM
scores averaged across examples and layers for two
GPT models on six datasets. As a baseline metric
for SimAM, ZSL SimAM computes the similarity
between ICL attention weights and ZSL attention
weights. Comparing these two metrics, we also
observe that compared with ZSL, ICL is more in-
clined to generate attention weights similar to those
of ﬁnetuning. Again, at the attention behavior level,
we prove that ICL behaves similarly to ﬁnetuning.
5
Discussions
5.1
Similarity at Different Layers
In order to investigate the similarity between ICL
and ﬁnetuning more thoroughly, we look into the
SimAOU and SimAM scores at different layers.
We randomly sample 50 validation examples from
each dataset and draw box plots for SimAOU and

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
Layer
0.5
0.0
0.5
1.0
SimAM
GPT 1.3B
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
Layer
0.5
0.0
0.5
1.0
SimAM
GPT 2.7B
Figure 3: Statistics of the SimAM scores at different layers. The yellow lines denote medians.
SimAM in Figure 2 and Figure 3, respectively.
From the ﬁgures, we ﬁnd that both SimAOU and
SimAM are ﬂuctuated at lower layers, and tend
to be steadily larger at higher layers. This phe-
nomenon suggests that the meta-optimization made
by ICL has forward accumulated effects, so with
more accumulation, ICL will behave more simi-
larly to ﬁnetuning at higher layers.
5.2
Mapping Between Optimization
Algorithm and Transformer Architecture
Gradient Descent
(GD)
GD with
Momentum
Attention
Momentum
Attention
We have ﬁgured out the dual form between
Transformer attention and gradient descent based
optimization. Inspired by this dual view, we inves-
tigate whether we can utilize momentum (Polyak,
1964; Sutskever et al., 2013), a widely used tech-
nique for improving optimization algorithms, to
improve Transformer attention.
Method
As stated in Section 3.1, the attention
values serve as some kind of meta-gradients. By
analogy with momentum SGD that averages gradi-
ents among timestamps, we try to apply Exponen-
tial Moving Average (EMA) (Hunter, 1986) to the
attention values to build momentum-based atten-
tion:
MoAttn(V, K, qt) = Attn(V, K, qt) + EMA(V )
= V softmax(KT qt
√
d
) +
t−1
X
i=1
ηt−ivi,
where η is a hyper-parameter, and vi is the i-th
attention value vector. We assume that introducing
momentum into attention will help capture long
dependency and thus lead to faster convergence
and better performance.
Experiments on Language Modeling
First, we
evaluate the effect of momentum-based attention
on language modeling. We train two GPT models
with 350M parameters from scratch, where one is
the vanilla Transformer, and another applies mo-
mentum to attention. We evaluate the perplexity
of these two models on the training set and three
validation sets with input lengths of 256, 512, and
1024, respectively. The results are shown in Ta-
ble 4. We ﬁnd that on all of the validation sets,
applying momentum to attention introduces a con-
sistent perplexity improvement compared with the
vanilla Transformer.
Experiments on In-Context Learning
We also
evaluate the in-context learning ability of the above
language models to verify the effectiveness of the
momentum-based attention on downstream tasks.
We consider six datasets for sentiment analysis
(SST5 (Socher et al., 2013), IMDB (Maas et al.,
2011), MR (Pang and Lee, 2005)), natural language
inference (CB (de Marneffe et al., 2019)), and
multi-choice selection (ARC-E (Clark et al., 2018),
PIQA (Bisk et al., 2020)). For all these datasets,
we use 32 examples as demonstrations. As shown
in Table 5, compared with the vanilla Transformer,
using momentum-based attention achieves consis-
tently higher accuracy in all the tasks.
The performance improvements on both lan-
guage modeling and in-context learning prove that

Model
Train1024
Valid256
Valid512
Valid1024
Transformer
17.61
19.50
16.87
15.14
TransformerMoAttn
17.55
19.37
16.73
15.02
Table 4: Perplexity on the training set and validation sets with different input lengths for language modeling.
Applying momentum to attention introduces a consistent perplexity improvement compared with the vanilla Trans-
former.
Model
SST5
IMDB
MR
CB
ARC-E
PIQA
Average
Transformer
25.3
64.0
61.2
43.9
48.2
68.7
51.9
TransformerMoAttn
27.4
70.3
64.8
46.8
50.0
69.0
54.7
Table 5: Accuracy on six in-context learning tasks. Introducing momentum into attention improves the accuracy
of the vanilla Transformer by 2.8 on average.
introducing momentum into attention is an effec-
tive strategy, which supports our understanding of
meta-optimization from another aspect.
6
Conclusion
In this paper, we aim to explain the working mech-
anism of GPT-based ICL. Theoretically, we ﬁgure
out a dual form of ICL and propose to understand
ICL as a process of meta-optimization. Further,
we build connections between ICL and a speciﬁc
ﬁnetuning setting and ﬁnd that it is reasonable to
regard ICL as a kind of implicit ﬁnetuning. Empir-
ically, in order to support our understanding that
ICL performs implicit ﬁnetuning, we comprehen-
sively compare the behavior of ICL and ﬁnetuning
based on real tasks. The results prove that ICL
behaves similarly to explicit ﬁnetuning at the pre-
diction level, the representation level, and the at-
tention behavior level. Further, inspired by our
understanding of meta-optimization, we design a
momentum-based attention that achieves consistent
performance improvements. We believe that our
understanding will have more potential to aid in
ICL application and model designing in the future.
References
Mark A Aizerman, Emmanuil M Braverman, and Lev I
Rozonoer. 1964. Theoretical foundation of potential
functions method in pattern recognition. Avtomatika
i Telemekhanika, 25(6):917–936.
Ekin Akyürek, Dale Schuurmans, Jacob Andreas,
Tengyu Ma, and Denny Zhou. 2022. What learning
algorithm is in-context learning? investigations with
linear models. CoRR, abs/2211.15661.
Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jian-
feng Gao, and Yejin Choi. 2020. Piqa: Reasoning
about physical commonsense in natural language. In
Thirty-Fourth AAAI Conference on Artiﬁcial Intelli-
gence.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah,
Jared
D
Kaplan,
Prafulla
Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, Sandhini Agarwal, Ariel Herbert-
Voss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,
Clemens Winter, Chris Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In
Advances in Neural Information Processing Systems,
volume 33, pages 1877–1901. Curran Associates,
Inc.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018.
Think you have solved question
answering?
try arc, the ai2 reasoning challenge.
arXiv:1803.05457v1.
Marie-Catherine de Marneffe, Mandy Simons, and
Judith Tonhauser. 2019.
The CommitmentBank:
Investigating projection in naturally occurring dis-
course.
Proceedings of Sinn und Bedeutung,
23(2):107–124.
Shivam Garg, Dimitris Tsipras, Percy Liang, and Gre-
gory Valiant. 2022. What can transformers learn in-
context? A case study of simple function classes.
CoRR, abs/2208.01066.
J Stuart Hunter. 1986.
The exponentially weighted
moving average.
Journal of quality technology,
18(4):203–210.
Kazuki Irie, Róbert Csordás, and Jürgen Schmidhuber.
2022. The dual form of neural networks revisited:
Connecting test time predictions to training patterns

via spotlights of attention.
In International Con-
ference on Machine Learning, ICML 2022, volume
162 of Proceedings of Machine Learning Research,
pages 9639–9659. PMLR.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 142–150, Port-
land, Oregon, USA. Association for Computational
Linguistics.
Bo Pang and Lillian Lee. 2004.
A sentimental edu-
cation: Sentiment analysis using subjectivity sum-
marization based on minimum cuts.
In Proceed-
ings of the 42nd Annual Meeting of the Association
for Computational Linguistics (ACL-04), pages 271–
278, Barcelona, Spain.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the
ACL.
Boris T Polyak. 1964. Some methods of speeding up
the convergence of iteration methods.
Ussr com-
putational mathematics and mathematical physics,
4(5):1–17.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013.
Recursive deep models
for semantic compositionality over a sentiment tree-
bank.
In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing,
pages 1631–1642, Seattle, Washington, USA. Asso-
ciation for Computational Linguistics.
Ilya Sutskever, James Martens, George Dahl, and Ge-
offrey Hinton. 2013. On the importance of initial-
ization and momentum in deep learning. In Interna-
tional conference on machine learning, pages 1139–
1147. PMLR.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017.
Attention is
all you need.
In Advances in Neural Information
Processing Systems, pages 5998–6008. Curran As-
sociates, Inc.
Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
siﬁcation. In Advances in neural information pro-
cessing systems.
A
Templates for In-Context Learning
We demonstrate the templates used to format exam-
ples and the candidate answer sets for six classiﬁ-
cation datasets used in our experiments in Table 6.
B
Hyper-parameters
We perform grid search to ﬁnd the best random seed
for ICL and the best learning rate for ﬁnetuning.
The search range for all the datasets is the same.
For random seeds, we search in {1, 2, 3, 4, 5, 6, 7}.
For learning rates, the search base values are
{1, 2, 3, 4, 5, 6, 7, 8, 9} and we scale them to 0.1,
0.01, and 0.001 times, i.e., we have 9 × 3 = 27
values to search.
In Table 7, we present the details of the selected
random seeds and learning rates for two GPT mod-
els on six classiﬁcation datasets.

Dataset
Template
Candidate Answer Set
CB
{Premise}
{ True, False, Neither }
Question: {Hypothesis} True, False, or Nei-
ther?
Answer: {Label}
SST-2
Sentence: {Sentence}
{ Negative, Positive }
Label: {Label}
SST-5
Sentence: {Sentence}
{ terrible, bad, neutral, good, great }
Label: {Label}
Subj
Input: {Sentence}
{ objective, subjective }
Type: {Label}
MR
Review: {Sentence}
{ Negative, Positive }
Sentiment: {Label}
AGNews
Classify the news articles into the categories
of World, Sports, Business, and Technology.
{ World, Sports, Business, Technology }
News: {Sentence}
Type: {Label}
Table 6: Formatting templates and candidate answer sets for six classiﬁcation datasets.
Hyper-parameter
Dataset
GPT 1.3B
GPT 2.7B
Random Seed
CB
3
3
SST2
2
7
SST5
5
5
Subj
4
4
MR
5
1
AGNews
3
3
Learning Rate
CB
0.100
0.090
SST2
0.020
0.007
SST5
0.006
0.003
Subj
0.003
0.002
MR
0.010
0.001
AGNews
0.200
0.060
Table 7: Selected random seeds and learning rates for two GPT models on six classiﬁcation datasets.

