STB-VMM: Swin Transformer Based Video Motion Magniﬁcation
Ricard Lado-Roig´ea, Marco A. P´ereza,∗
aIQS School of Engineering, Universitat Ramon Llull, Via Augusta 390, 08017 Barcelona, Spain
Abstract
The goal of video motion magniﬁcation techniques is to magnify small motions in a video to reveal previously invis-
ible or unseen movement. Its uses extend from bio-medical applications and deepfake detection to structural modal
analysis and predictive maintenance. However, discerning small motion from noise is a complex task, especially when
attempting to magnify very subtle, often sub-pixel movement. As a result, motion magniﬁcation techniques generally
suﬀer from noisy and blurry outputs. This work presents a new state-of-the-art model based on the Swin Transformer,
which oﬀers better tolerance to noisy inputs as well as higher-quality outputs that exhibit less noise, blurriness, and
artifacts than prior-art. Improvements in output image quality will enable more precise measurements for any ap-
plication reliant on magniﬁed video sequences, and may enable further development of video motion magniﬁcation
techniques in new technical ﬁelds.
Keywords: Computer vision, Deep Learning, Swin Transformer, Motion Magniﬁcation, Image Quality Assessment
1. Introduction
Video Motion Magniﬁcation (VMM) is a computer vision task consistent in magnifying small motions in a video
sequence, having several uses in many ﬁelds from bio-medical applications [1–3] and deepfake detection [4] to struc-
tural modal analysis [5] and condition monitoring. These techniques act like a microscope for motion, to reveal
previously invisible or unseen movements. Despite this simple premise, discerning small motions from noise is a
complex task, especially when attempting to magnify very subtle, often sub-pixel movement. As a result, motion
magniﬁcation techniques generally suﬀer from noisy and blurry outputs. Therefore, multiple authors have explored
techniques to remediate these shortcomings and improve magniﬁcation quality and performance.
Early motion magniﬁcation algorithms, such as [6], used a Lagrangian approach, reliant on motion tracking or
optical ﬂow, to isolate motion prior to magniﬁcation. However, this approach is very computationally expensive and
diﬃcult to execute artifact-free, especially in regions aﬀected by occlusion boundaries and complex motion. On the
∗Corresponding author.
Email address: marcoantonio.perez@iqs.url.edu (Marco A. P´erez)
Preprint submitted to Knowledge-Based Systems
December 22, 2022
arXiv:2302.10001v2  [cs.CV]  27 Mar 2023

other hand, more modern techniques [7–11] have relied on Eulerian approaches, which observe the changes in a ﬁxed
region of pixels instead of tracking features in time and space. These Eulerian approaches are less computationally
expensive, perform better with small motions, and generally yield better magniﬁcation results. Nevertheless, these
approaches still display noticeable blurring and artifacting due to the complex challenge of designing ﬁlters for noise
removal, which at the same time, do not interfere with motion magniﬁcation. For this reason, Oh et al. [12] proposed a
novel learning-based approach to VMM. Learning-based motion magniﬁcation departs from the use of hand-designed
ﬁlters in favor of learning those ﬁlters using Convolutional Neural Networks (CNN) instead. This method achieved
higher-quality magniﬁcation yielding fewer ringing artifacts and showing better noise characteristics than previously
published methods. However, its reliance on additional temporal ﬁltering to improve image quality sometimes pro-
duces errors in magniﬁcation. While it is possible to obtain fairly clear results with no temporal ﬁltering, the image
quality generally improves when ﬁltering is applied as it removes unwanted motion and noise before learning-based
magniﬁcation.
The method presented in this work improves on the learnable ﬁlters and abandons temporal ﬁltering to ensure
correct magniﬁcation outputs. Resulting in a novel architecture capable of producing state-of-the-art results in terms
of magniﬁed image quality. The main contributions of this work are:
a) A novel motion magniﬁcation architecture based on the SWIN transformer.
b) A discussion, comparison, and validation of learning-based VMM techniques, both in a quantitative and qualitative
sense.
c) The proposed novel architecture outperforms relevant VMM techniques in both quantitative evaluation and ob-
served output quality, oﬀering higher-quality magniﬁcation, less blurry frame reconstruction, better noise toler-
ance, and fewer artifacts than prior-art.
The following section summarizes previous inﬂuential works and their relation to the development of the presented
model. Section three describes in detail the model’s architecture and its training process. The fourth section presents
results and comparisons of the model’s performance, focusing on magniﬁcation and image quality with respect to
prior work. Finally, the conclusions of this paper are summarized in section ﬁve.
2. Related work
2.1. Learning-based video motion magniﬁcation
Eulerian approaches to video motion magniﬁcation function by decomposing video sequences into motion repre-
sentations that can later be manipulated mathematically and then reconstructed into magniﬁed frames. On the other
2

hand, Lagrangian approaches explicitly track a pixel or feature’s movement throughout a video sequence. This dis-
tinction between Lagrangian and Eulerian approaches is not dissimilar to the same terms used in ﬂuid dynamics,
where Lagrangian methods [6] track a volume of ﬂuid through the ﬂow, while Eulerian approaches [8, 7, 9] study the
evolution of ﬂow in a ﬁxed volume in space. Eulerian-based methods generally have the upper hand when processing
small motion but produce blurry results when encountering large motion. The technique presented in this paper be-
longs to the Eulerian approach and is inspired by the work of Oh et al.’s learning-based video motion magniﬁcation
[12].
Eulerian techniques generally consist of three stages: spatial decomposition, motion isolation and manipulation,
and representation denoising. From this blueprint, diﬀerent authors have proposed increasingly sophisticated tech-
niques to improve magniﬁcation quality and performance as reﬂected in table 1. In technical terms, the motion
magniﬁcation problem can be summarized as follows. Given a signal I(x, t) representing image intensity at position x
and time t, and δ(t) representing translational motion in time such that
I(x, t) = f(x + δ(t)); I(x, 0) = f(x)
(1)
The goal of motion magniﬁcation is to synthesize the signal
ˆI(x, t) = f(x + (1 + α) · δ(t))
(2)
for some ampliﬁcation factor α. In practice, only certain frequencies of motion δ(t) are useful to motion magniﬁcation,
so a selector T(·) is applied to δ(t), which is typically a temporal bandpass ﬁlter.
Method
Liu et al. [6]
Wu et al. [7]
Wadhwa et al.
[8]
Wadhwa et al.
[9]
Zhang et al. [11]
LB-VMM [12]
STB-VMM
Spatial
de-
composition
Tracking, optical
ﬂow
Laplacian
pyra-
mid
Steerable ﬁlters
Riesz pyramid
Steerable ﬁlters
Deep convolution
layers
Swin
Trans-
former
Motion
isola-
tion
-
Temporal
band-
pass ﬁlter
Temporal
band-
pass ﬁlter
Temporal
band-
pass
Temporal
band-
pass
ﬁlter
(2nd
order derivative)
Subtraction
or
bandpass ﬁlter
Subtraction
Representation
denoising
Expectation-
Maximization
-
Amplitude
weighted
Gaus-
sian ﬁltering
Amplitude
weighted
Gaus-
sian ﬁltering
Amplitude
weighted
Gaus-
sian ﬁltering
Trainable convo-
lution
Swin
Trans-
former
Table 1: Motion magniﬁcation techniques summary table. Adapted from [12].
Prior to learning-based VMM (LB-VMM), magniﬁcation techniques relied on multi-frame temporal ﬁltering to
isolate motions of interest from random noise [7–9, 13, 11]. By contrast, the learning-based approach [12] directly
employs CNNs to both ﬁlter noise and extract features, achieving comparable or better quality than prior-art without
3

using temporal ﬁltering. The LB-VMM model is composed of three stages: encoder, manipulator, and decoder. Said
model is designed to accept two frames and return a single motion-magniﬁed frame. The goal of the encoder is to
extract relevant features from each of the two input frames and yield a visual and a motion representation. The mo-
tion representation of both input frames is then passed to the manipulator, which will subtract both representations
and magnify the result by an arbitrary parameter α deﬁned by the user. Finally, the results of the manipulator and
the previously-obtained visual representation enter the decoder, where the motion and visual components are recon-
structed into a motion-magniﬁed frame. These three CNN-based components allow for ﬂexible learnable ﬁlters that
are better suited to the task of motion magniﬁcation and thus yield better quality magniﬁcation results.
To train the model and given the impossibility of obtaining motion magniﬁed video pairs, Oh et al. generated and
used a fully-synthetic dataset for training their model, built by moving segmented objects from the PASCAL VOC [14]
dataset over background images taken from MS COCO [15]. Careful consideration to the generation of the dataset
was paid to ensure accurate pixel and sub-pixel motion as well as learnability. The dataset learning examples are
parametrized to make sure they are within a deﬁned range. Speciﬁcally, the dataset’s magniﬁcation is upper-limited
to an α magniﬁcation factor of 100, and input motion is sampled so that magniﬁed motion does not exceed 30 pixels.
2.2. Transformers as a Computer Vision tool
CNNs have been a staple of the Computer Vision (CV) ﬁeld in the last few years, with many of the top-performing
models having made extensive use of them [16–18]. This period roughly started after Krizhevsky et al. [17] won
the ImageNet Large Scale Visual Recognition Challenge [19, 20] (ILSVRC) on September 30th 2012, and spurred
many publications employing CNNs and GPUs to accelerate deep learning. Through the use of ﬁlters, these networks
generate feature maps that summarize an image’s most relevant parts. These ﬁlters capture relevant local information
by the very nature of the convolution operation, which, combined with multi-scale architectures [21, 22] result in rich
feature maps that can eﬃciently obtain a representation of an image’s content, both in a local and global context.
Recently, the CV ﬁeld has been revolutionized yet again by the Vision Transformer (ViT) [23], which, employing the
attention mechanism has demonstrated state-of-the-art performance in many CV tasks. The attention mechanism was
ﬁrst popularized in the ﬁeld of Natural Language Processing (NLP) by Vaswani et al. [24], where the transformer
architecture has become the de-facto standard.
The attention mechanism can be described as mapping from a query and a set of key-value pairs into an output.
The output, represented in vector format, is computed as a weighted sum of the values, where the weight assigned
to each value is computed by a compatibility function taking into account the query and the corresponding key [24].
The transformer was the ﬁrst model which exclusively relied on self-attention to compute representations of its input
and output without using sequence-aligned recursive neural networks or convolution operations. Unlike CNNs, trans-
4

formers lack translation invariance and a locally-restricted receptive ﬁeld, in its place transformers oﬀer permutation
invariance. Said feature enabled NLP models to infer relations between words and ideas much further into a text than
previous recurrent models could. However, CV applications require the processing of grid-structured data which can
not trivially be processed by a transformer. The ViT [23] overcame this burden by mapping grid-structured data into
sequential data by splitting the image into patches. Patches are then ﬂattened into vectors and embedded into a lower
dimension. These ﬂattened patches are then summed with positional embeddings and fed as a sequence to a standard
transformer encoder. Image patches essentially become sequence tokens just like words are when working in NLP, in
fact, ViT uses the exact same encoder described in [24].
Later, Microsoft researchers improved on the ViT publishing the SWIN transformer, a hierarchical vision trans-
former using shifted windows [25]. This work further reﬁned the solution to adapt the original transformer from
language to vision. The SWIN transformer solved issues caused by large discrepancies in the scale of visual entities
at the same time that limited self-attention computation to non-overlapping local windows, yet still allowing for cross-
window interaction. The introduced limitation on the scope of self-attention signiﬁcantly reduced the computational
complexity, which scales quadratically with respect to image size, allowing for the processing of higher-resolution
images that were previously unmanageable. Further developments in the CV ﬁeld have implemented the SWIN trans-
former for various tasks achieving state-of-the-art performance [26–28].
2.3. SwinIR image restoration
Inspired by the recent prominence of the transformer and its success in many CV problems such as image classiﬁ-
cation [29, 23, 30, 25, 31–33], object detection [34–36], segmentation [30, 37, 38], crowd counting [39, 40] and image
restoration [41–43], Liang et al. [27] proposed a new state-of-the-art image restoration model based on the Swin trans-
former [25]. The SwinIR model consists yet again of three modules: a shallow feature extractor, a transformer-based
deep feature extractor and a high-quality image reconstruction module. This structure oﬀers excellent performance in
various image restoration tasks such as image super-resolution, JPEG compression artifact reduction, and image de-
noising. These applications are very interesting when working with VMM, as current state-of-the-art methods can be
negatively aﬀected by noisy input images, causing much noisier and blurrier results, especially at large magniﬁcation
rates. This occurs as a result of noise not being properly ﬁltered beforehand, therefore as the motion gets magniﬁed,
the noise gets magniﬁed as well.
5

3. Methodology
3.1. Residual Swin Transformer Block
The Residual Swin Transformer Block (RSTB) [27] is used as one of the fundamental building blocks of the
proposed architecture, appearing in parts of both the feature extractor and the reconstructor. The RSTB is a residual
block combining multiple Swin Transformer Layers (STL) [25] and convolutional layers, compounding the beneﬁts of
the spatially invariant ﬁlters of the convolutional layers with the residual connections that allow for multilevel feature
processing.
The Swin transformer layer shown in ﬁgure 2 partitions an H × W × C image into non-overlapping HW
M2 local
windows using an MxM sliding window and then computing its local attention, eﬀectively reshaping the input image
into HW
M2 × M2 × C. The main diﬀerence with respect to the original transformer layer [24] lies in the local attention
and the shifted window mechanism. For a local window feature F ∈RM2×C, the query, key, and value matrices Q, K,
and V ∈RM2×d are computed as
Q = FWQ;
K = FWk;
V = FWV
(3)
where WQ, WK, and WV are the learnable parameters shared across diﬀerent windows, and d is the dimension of Q,
K, and V. Therefore, the attention matrix is computed for each window as
Attention(Q, K, V) = softmax(QKT
√
d
+ P)V
(4)
where P is the learnable relative positional encoding. Computing the attention mechanism multiple times yields
the results of the Multi-head Self Attention (MSA), which are then passed on to a Multi-Layer Perceptron (MLP).
Therefore, the whole STL process can be summed up like so
F = MS A(LayerNorm(F)) + F
(5)
then
F = MLP(LayerNorm(F)) + F
(6)
where the MLP is formed by two fully-connected layers with a GELU activation layer in between.
6

3.2. Network architecture
The proposed model architecture, shown in ﬁgure 1, consists of three main functional blocks: the feature extractor,
the manipulator, and the reconstructor. The feature extractor is further subdivided into the shallow and deep feature
extractors, and their job is to extract a high-quality representation of an input frame. Next, the manipulator, using the
features from two frames, magniﬁes the motion by multiplying the diﬀerence between the two feature spaces by a
user-selected magniﬁcation factor α. Finally, the reconstructor converts the resulting manipulated feature space back
into a magniﬁed frame.
Figure 1: Architecture overview of the proposed model.
Given two frames of a target sequence [IA, IB] ∈RH×W×Cin (where H is the height of the image, W is the width of
the image and Cin represents the number of input channels) the convolutional shallow feature extractor (GS F) maps
high-level features into a higher dimensional feature space, thus providing early local feature extraction (FAS , FBS )
and leading to a more stable optimization and better results [44].
[FAS , FBS ] = GS F([IA, IB])
(7)
7

Then, the features extracted in the previous step are further processed in the deep feature extraction module (GDF),
which consists of N Residual Swin Transformer Blocks (RSTB).
[FAD, FBD] = GDF([FAS , FBS ])
(8)
After feature extraction, both frames’ feature spaces are then sent to the manipulator [12] (GM), which works by
taking the diﬀerence of both frames’ feature spaces and directly multiplying by a magniﬁcation factor α.
GM(FAS + FAD, FBS + FBD) = (FAS + FAD) + h(α · t(((FBS + FBD) −(FAS + FAD))))
(9)
Where t(·) is a 3×3 convolution followed by a ReLU activation, and h(·) is a 3×3 convolution followed by a 3×3
residual block.
FM = GM(FAS + FAD, FBS + FBD)
(10)
The conjoined manipulated feature space of both frames is then processed by the Mixed Magniﬁed Transformer
Block (MMTB) (GMMT B) formed by N RSTB blocks. This stage enables the attention mechanism to aﬀect the
combined magniﬁed features of both frames, resulting in a more coherent result after reconstruction.
FMMT B = GMMT B(FM)
(11)
Finally, reconstruction is dealt with a convolutional block (GR) that inverts the initial feature mapping, done in the
shallow feature extractor, back onto a frame (I ˆY).
I ˆY = GR(FM + FMMT B)
(12)
Further detail on the architecture can be found in ﬁgure 2 along with a graphical representation of the Swin
Transformer Layer (STL) and the Residual Swin Transformer Block (RSTB).
3.3. Training
The whole network is trained end-to-end using the dataset provided by [12], which allows the results comparison
to depend exclusively on network architecture. Nevertheless, in addition to enabling fair comparison, the dataset has
proven [12] to produce good-quality models capable of generalizing trained scenarios and returning excellent-quality
8

(a) Swin Transformer Layer (STL)
(b) Residual Swin Transformer
Block (RSTB)
(c) Deep Feature Extractor / Mix
Magniﬁed Transformer Block
(d) Manipulator
Figure 2: Architectural details.
magniﬁed videos on scenes totally unrelated to the dataset. These reasons led to the adoption of the dataset as the
only source of training data.
The L1-Loss cost function was chosen for end-to-end training and placed between the network’s output I ˆY and
the ground truth frame IY. Additionally, in order to improve the feature extraction and make a more robust system,
the perturbed c frames provided by the dataset were compared against their non-perturbed counterparts after feature
extraction, using yet again L1-Loss. The resulting regularization loss was then added to the end-to-end loss of the
whole network with a λ weight coeﬃcient set to 0.1.
Finally, the optimizer of choice for training the model was ADAM [45] with β1 = 0.9, β2 = 0.999, batch size set
to 5 and a learning rate of 10−5 with no weight decay.
3.4. Modes of operation
The proposed approach, STB-VMM, can be applied to any input video sequence containing two frames or more,
regardless of the time scale between the two frames. Sequences can be treated in one of two modes, static or dynamic,
borrowed from [12]. No changes to the network are made for these modes. Instead, the modes refer to the order in
which the input frames are fed to the model. The static mode, which follows more closely the classical deﬁnition
of motion magniﬁcation, uses the ﬁrst frame of the sequence as reference. In terms of computation, the static mode
can be expressed like so: model(I0, It), where the t is the frame number increasing sequentially with time. On the
9

other hand, the dynamic mode magniﬁes the diﬀerence between two consecutive frames [model(It, It+1)], therefore
magnifying velocity between each frame. Note that in each of the modes, the magniﬁcation factor α has diﬀerent
meanings.
Oh et al. [12] proposed one additional operation mode with temporal ﬁltering to mitigate the eﬀects of undesired
motion and noise. The ﬁltering was applied in the manipulator to produce temporarily-ﬁltered motion-magniﬁed
frames similar to those of classical techniques. On the downside, the temporal mode appears to cause blindness to
small motions, resulting in patchy magniﬁcation. This phenomenon occurs because motion amplitude crosses the
threshold to be large enough to be detected and causes some regions to be suddenly magniﬁed mid-sequence. This
performance degradation gets worst when the magniﬁcation factor is high and motion is small. While theoretically
possible to incorporate a temporal mode into the proposed model, the magniﬁcation results do not suﬀer from exces-
sive noise or blurring, therefore, temporal ﬁltering is unnecessary and the full spectrum of frequencies is magniﬁed
all at once producing good results.
4. Results and discussion
In the following section, the results yielded by the STB-VMM model are compared to the current state-of-the-art
learning-based video motion magniﬁcation model [12]. Performance is measured quantitatively and qualitatively,
showing that our model improves on the previous state-of-the-art in magniﬁcation quality and clarity. The video
versions of all the comparisons are available in the supplementary materials.
Quantitative comparison of image quality or Image Quality Assessment (IQA) is a complex topic involving many
variables and methods. Said methods are divided into three main categories: full-reference, reduced-reference, and
no-reference. A referenced algorithm [46–48] requires a pristine sample to assess the quality of a degraded image,
while no-reference methods [49–52] produce an image score without the need of any reference. When evaluating
VMM it is impossible to obtain a pristine motion-magniﬁed frame. Therefore, to evaluate the results presented in the
following section the MUSIQ [52, 53] algorithm was chosen to compare the models’ performance.
The following results comparative analyzes the performance of Oh et al.’s Learning-Based Video Motion Mag-
niﬁcation (LB-VMM) model and STB-VMM on ten diﬀerent video benchmarks that showcase interesting motion
magniﬁcation examples. In addition, a comparison against the baby [7] sequence is added to provide a fair point of
comparison. The sequences were captured at 1080p 60fps on a mid-range smartphone to demonstrate the potential of
STB-VMM with accessible video equipment.
10

4.1. Quantitative comparison
Table 2 shows the average, 1st, and 99th percentile average MUSIQ scores for the tested benchmark sequences
ran on the Learning-based Video Motion Magniﬁcation model and the STB-VMM model. The values presented in
the table are calculated for each individual frame of the full sequences and then summarized on an average score.
The original sequences are also added as control, and scores are expected to be higher than both of the magniﬁcation
methods.
Original
LB-VMM
STB-VMM
Avg.
η1
η99
Avg.
η1
η99
Avg.
η1
η99
AC00
72.11
69.65
72.75
55.73
49.61
58.69
62.45
61.05
63.29
AC01
69.15
68.30
70.05
48.35
34.07
51.22
59.27
57.72
60.96
Baby
74.39
69.71
74.87
55.51
53.26
59.95
57.12
54.41
62.90
Building00
66.84
66.01
75.45
52.46
49.51
62.75
52.30
50.07
56.43
Car00
52.55
50.65
54.41
31.40
18.27
35.50
43.37
23.28
48.06
Car01
55.81
54.77
57.01
33.51
30.67
64.99
50.28
48.08
52.07
Crane00
75.26
74.86
75.57
56.92
52.70
65.02
59.13
56.19
62.89
Crane01
75.09
74.63
75.44
51.05
45.25
57.37
54.93
51.11
64.70
Truss00
66.94
65.92
67.49
55.90
52.65
57.98
56.27
54.93
57.61
Wheel00
72.84
71.87
73.38
51.04
28.82
54.40
57.04
36.41
61.19
Wheel01
52.15
50.23
53.55
34.84
31.12
59.03
46.21
43.68
48.48
Total avg.
66.13
51.25
75.45
48.05
32.32
60.09
54.42
45.68
63.29
% dev. to avg.
13.58%
22.50%
14.09%
20.34%
32.75%
25.04%
10.70%
16.07%
16.28%
Table 2: Comparative MUSIQ scores of the original sequence, the sequence magniﬁed using Learning-Based Video Motion Magniﬁcation
(o3f hmhm2 bg qnoise mix4 nl n t ds3 checkpoint), and the proposed method. (x20)
The results on table 2 demonstrate that STB-VMM produces better results than LB-VMM. On average, the scores
obtained by STB-VMM are 9.63% higher, and boast a much higher 1% lows, implying that the quality of magniﬁcation
is noticeably more consistent throughout the sequence. This trend can be observed in table 3 and ﬁgure 3, where
STB-VMM shows remarkable stability on its output quality. LB-VMM only manages a single higher score than STB-
VMM in the building benchmark by a diﬀerence of 0.23% (Building00). However, in the authors’ opinion, STB-VMM
produces better quality magniﬁcation with more stable edges and less blurry patches.
On the other hand, none of the magniﬁed scores fall above the original’s, as expected. Nevertheless, magniﬁed
and original scores follow the same trend, implying that low-quality source videos produce worse outputs. However,
STB-VMM is much more capable of dealing with low-quality input images, even closing the quality gap with respect
to the original when input quality declines. The sharp quality declines seen in both car sequences can be, in part,
attributed to the poor low light performance of the camera employed.
4.2. Qualitative comparison
To reinforce the previous section’s scores and claims, this section presents a few qualitative comparisons that
demonstrate the eﬀectiveness of our proposed network against the current state-of-the-art in terms of resulting image
11

 ✁
✂
✄✂☎✆✝
✞✟✠
✡☛☛
☞
✌
✟✠
✡☛☛
✍
✎
✏
✑
✒
✓
✔
✕
✖
✗
✘
✙
✚
✛
✜✢
✣
✢
✤
✢
✥
✢
✦✢
✧✢
✟
★
☎
✩
✪
✫
✆
✁
✬
✭
✮
✯✯
✭
✮
✯
✰
✟
✆✱
✲
✟
✳✂
✝
✴
✂
☎
✄
✯✯
✮
✆✁
✯✯
✮
✆
✁
✯
✰
✮
✁
✆☎
★
✯✯
✮
✁✆☎
★
✯
✰
✌
✁
✳
✵✵
✯✯
✶
✪★★
✝
✯✯
✶
✪★★
✝
✯
✰
Figure 3: Graphic representation of the average MUSIQ scores per test sequence magniﬁed x20.
Avg. (%)
η1 (%)
η99 (%)
AC00
9.32
16.42
6.32
AC01
15.79
34.64
13.91
Baby
2.15
1.65
3.95
Building00
-0.23
0.86
-8.38
Car00
22.79
9.89
23.08
Car01
30.06
31.78
-22.65
Crane00
2.93
4.67
-2.81
Crane01
5.17
7.85
9.71
Truss00
0.55
3.46
-0.55
Wheel00
8.24
10.56
9.26
Wheel01
21.81
25.01
-19.72
Total
9.63
26.07
4.24
Table 3: MUSIQ score diﬀerence between STB-VMM and LB-VMM.
quality.
Figure 4 shows the same frame chosen at random from the Car00 sequence using both models. STB-VMM, shown
on the right, yields a much superior result in terms of image clarity that can be appreciated in both edges and texture.
The car sequence recording was ﬁlmed in a rather low light environment, thus yielding noisier/grainier video than
otherwise could have been archived. This highlights one of the main beneﬁts of the proposed architecture, which is a
much better tolerance to noisy input. Regardless of clarity, both models perform well on motion magniﬁcation with
very few artifacts, if any.
The next example, shown in ﬁgure 5, was ﬁlmed in better lighting conditions, yet the quality score of the un-
magniﬁed video is no better. This might have been caused, in part, due to the framing of the sequence, which keeps
only parts of the image in focus. Regardless of the base score set by the original, STB-VMM clearly outperforms
LB-VMM, with better-deﬁned letters and a much more clear background. In terms of motion magniﬁcation, both
methods display good quality magniﬁcation.
12

(a) Split frame showing LB-VMM on the left and STB-VMM on the right
(b) LB-VMM
(c) STB-VMM
(d) LB-VMM
(e) STB-VMM
Figure 4: Qualitative comparison of the car sequence. Highlighted in the bottom row of the ﬁgure the car’s coolant reservoir, engine cover, and
ventilation slits demonstrate that STB-VMM results are noticeably sharper and less distorted.
On the other hand, the building sequence (Building00) is the only benchmark where LB-VMM outperforms on
average STB-VMM. Nevertheless, the better edge stability oﬀered by STB-VMM enables the authors to obtain better
frequency readings from the magniﬁed video. Such application is interesting in technical ﬁelds where vibration needs
to be monitored, such as in structural health monitoring [5, 54–56]. Figure 6 shows the cropped upper right corner
of the building [57] and the slice used for frequency measuring. Below, in ﬁgure 6d, the FFTs obtained from the
movement of the sequences are plotted. While both sequences detect a peak at 14.25 Hz, STB-VMM produces a
much cleaner signal. During the experiment, the building was intentionally excited with an electrodynamic shaker
reproducing a 14.25 Hz sine wave.
The authors acknowledge that image quality can be a somewhat subjective metric and recommend watching the
comparison videos attached in the supplementary materials.
13

(a) Split frame showing LB-VMM on the left and STB-VMM on the right
(b) LB-VMM
(c) STB-VMM
(d) LB-VMM
(e) STB-VMM
Figure 5: Qualitative comparison of the wheel sequence. STB-VMM displays sharper letters and a better-deﬁned background with respect to
LB-VMM.
4.3. Limitations
In spite of the favorable comparisons, LB-VMM still has a signiﬁcant advantage in computing time over STB-
VMM. With our hardware setup1, LB-VMM magniﬁes the baby [7] sequence, consisting of 300 960x576 frames, in
approximately 76 seconds. Meanwhile, STB-VMM almost doubles the compute time, clocking in at 130 seconds for
the exact same sequence. Software optimizations combined with upcoming improvements in hardware might help
mitigate STB-VMM’s compute time shortcomings.
1AMD Ryzen 9 5950X; Nvidia RTX 3090
14

(a) Slice location
(b) LB-VMM
(c) STB-VMM
(d) FFT graph comparison
Figure 6: Vibration readings on the Building00 sequence. While the noise ﬂoor remains the same on both readings, the FFT obtained using
STB-VMM displays a much more prominent peak at 14.25 Hz.
5. Conclusions
This work presents a new state-of-the-art model for video motion magniﬁcation based on the Swin Transformer
that has been shown to outperform previous state-of-the-art learning-based models. The new model displays better
noise tolerance characteristics, a less blurry output image, and better edge stability, resulting in clearer and less noisy
magniﬁcation with very few, if any, artifacts.
On the downside, the new model requires more computing resources than previous models and cannot be run in
real-time like phase-based methods [8]. Nevertheless, applications that require precise magniﬁcation for vibration
monitoring [5] could greatly beneﬁt from improvements in the technology. Further work will address the integration
of this model in speciﬁc applications that require precise vibration monitoring and could beneﬁt from a full-ﬁeld
solution like a camera instead of installing and wiring multiple contact sensors such as accelerometers.
15

Acknowledgements
The authors would like to gratefully acknowledge the support and funding of the Catalan Agency for Business
Competitiveness (ACCI ´O) through the project INNOTEC ISAPREF 2021. Furthermore, the ﬁrst author would like to
acknowledge a Doctoral Scholarship from IQS. Finally, the authors would like to thank Dr. Eduardo Blanco from the
University of Arizona and Dr. Ariadna Chueca de Bruijn for their help.
Declaration of Competing Interest
The authors declare that they have no known competing ﬁnancial interests or personal relationships that could
appear to inﬂuence the work reported in this paper.
References
[1] O. Shabi, S. Natan, A. Kolel, A. Mukherjee, O. Tchaicheeyan, H. Wolfenson, N. Kiryati, A. Lesman, Motion magniﬁcation analysis of
microscopy videos of biological cells, PLOS ONE 15 (11) (2020) 1–18. doi:10.1371/journal.pone.0240127.
[2] A. J. McLeod, J. S. Baxter, S. de Ribaupierre, T. M. Peters, Motion magniﬁcation for endoscopic surgery, in: Medical Imaging 2014:
Image-Guided Procedures, Robotic Interventions, and Modeling, Vol. 9036, SPIE, 2014, pp. 81–88. doi:/10.1117/12.2043997.
[3] H. Lauridsen, S. Gonzales, D. Hedwig, K. L. Perrin, C. J. Williams, P. H. Wrege, M. F. Bertelsen, M. Pedersen, J. T. Butcher, Ex-
tracting physiological information in experimental biology via eulerian video magniﬁcation, BMC Biol. 17 (1) (2019) 1–26.
doi:
10.1186/s12915-019-0716-7.
[4] J. Fei, Z. Xia, P. Yu, F. Xiao, Exposing ai-generated videos with motion magniﬁcation, Multimed Tools Appl 80 (20) (2021) 30789–30802.
doi:10.1007/s11042-020-09147-3.
[5] R. Lado-Roig´e, J. Font-Mor´e, M. A. P´erez, Learning-based video motion magniﬁcation approach for vibration-based damage detection,
Meas. 206 (2023) 112218. doi:10.1016/j.measurement.2022.112218.
[6] C. Liu, A. Torralba, W. T. Freeman, F. Durand, E. H. Adelson, Motion magniﬁcation, in: ACM SIGGRAPH 2005 Papers, 2005, p. 519–526.
doi:10.1145/1186822.1073223.
[7] H.-Y. Wu, M. Rubinstein, E. Shih, J. Guttag, F. Durand, W. Freeman, Eulerian video magniﬁcation for revealing subtle changes in the world,
ACM Trans. Graph. (2012). doi:10.1145/2185520.2185561.
[8] N. Wadhwa, M. Rubinstein, F. Durand, W. T. Freeman, Phase-based video motion processing, ACM Trans. Graph. 32 (4) (2013) 1–10.
doi:10.1145/2461912.2461966.
[9] N. Wadhwa, M. Rubinstein, F. Durand, W. Freeman, Riesz pyramids for fast phase-based video magniﬁcation, in: 2014 IEEE ICCP, 2014,
pp. 1–10. doi:10.1109/ICCPHOT.2014.6831820.
[10] N. Wadhwa, H.-Y. Wu, A. Davis, M. Rubinstein, E. Shih, G. J. Mysore, J. G. Chen, O. Buyukozturk, J. V. Guttag, W. T. Freeman, F. Durand,
Eulerian video magniﬁcation and analysis, Commun. ACM 60 (1) (2016) 87–95. doi:10.1145/3015573.
[11] Y. Zhang, S. L. Pintea, J. C. van Gemert, Video acceleration magniﬁcation, CVPR (2017). doi:10.48550/arXiv.1704.04186.
[12] T.-H. Oh, R. Jaroensri, C. Kim, M. Elgharib, F. Durand, W. T. Freeman, W. Matusik, Learning-based video motion magniﬁcation (2018).
doi:10.48550/arXiv.1804.02684.
16

[13] M. A. Elgharib, M. Hefeeda, F. Durand, W. T. Freeman, Video magniﬁcation in presence of large motions, 2015 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) (2015) 4119–4127doi:10.1109/CVPR.2015.7299039.
[14] M. Everingham, L. Gool, C. K. Williams, J. Winn, A. Zisserman, The pascal visual object classes (voc) challenge, Int. J. Comput. Vis. 88 (2)
(2010) 303–338. doi:10.1007/s11263-009-0275-4.
[15] T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, C. L. Zitnick, P. Doll´ar, Microsoft coco: Common
objects in context, arXiv:1405.0312 (2015). doi:10.48550/arXiv.1405.0312.
[16] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, L. D. Jackel, Backpropagation applied to handwritten zip code
recognition, Neural Computation 1 (4) (1989) 541–551. doi:10.1162/neco.1989.1.4.541.
[17] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classiﬁcation with deep convolutional neural networks, Vol. 60, New York, NY, USA,
2017, p. 84–90. doi:10.1145/3065386.
[18] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition (2015). doi:10.48550/ARXIV.1512.03385.
[19] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Imagenet: A large-scale hierarchical image database, in: CVPR, IEEE, 2009, pp.
248–255. doi:10.1109/CVPR.2009.5206848.
[20] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, L. Fei-Fei,
Imagenet large scale visual recognition challenge (2014). doi:10.48550/ARXIV.1409.0575.
[21] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for biomedical image segmentation (2015). doi:10.48550/ARXIV.
1505.04597.
[22] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y. Mu, M. Tan, X. Wang, W. Liu, B. Xiao, Deep high-resolution representation
learning for visual recognition (2019). doi:10.48550/ARXIV.1908.07919.
[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszko-
reit, N. Houlsby, An image is worth 16x16 words: Transformers for image recognition at scale (2020). doi:10.48550/ARXIV.2010.11929.
[24] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin, Attention is all you need (2017). doi:
10.48550/ARXIV.1706.03762.
[25] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, B. Guo, Swin transformer: Hierarchical vision transformer using shifted windows
(2021). doi:10.48550/ARXIV.2103.14030.
[26] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, M. Shah, Transformers in vision: A survey, ACM Comput. Surv. (2021). doi:
10.1145/3505244.
[27] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, R. Timofte, Swinir: Image restoration using swin transformer (2021). doi:10.48550/
ARXIV.2108.10257.
[28] J. Liang, J. Cao, Y. Fan, K. Zhang, R. Ranjan, Y. Li, R. Timofte, L. Van Gool, Vrt: A video restoration transformer (2022). doi:10.48550/
ARXIV.2201.12288.
[29] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya, J. Shlens, Stand-alone self-attention in vision models (2019).
doi:
10.48550/ARXIV.1906.05909.
[30] B. Wu, C. Xu, X. Dai, A. Wan, P. Zhang, Z. Yan, M. Tomizuka, J. Gonzalez, K. Keutzer, P. Vajda, Visual transformers: Token-based image
representation and processing for computer vision (2020). doi:10.48550/ARXIV.2006.03677.
[31] Y. Li, K. Zhang, J. Cao, R. Timofte, L. Van Gool, Localvit: Bringing locality to vision transformers (2021). doi:10.48550/ARXIV.2104.
05707.
[32] Y. Liu, Y.-H. Wu, G. Sun, L. Zhang, A. Chhatkuli, L. Van Gool, Vision transformers with hierarchical attention (2021). doi:10.48550/
ARXIV.2106.03180.
17

[33] A. Vaswani, P. Ramachandran, A. Srinivas, N. Parmar, B. Hechtman, J. Shlens, Scaling local self-attention for parameter eﬃcient visual
backbones (2021). doi:10.48550/ARXIV.2103.12731.
[34] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, S. Zagoruyko, End-to-end object detection with transformers (2020). doi:
10.48550/ARXIV.2005.12872.
[35] L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, M. Pietik¨ainen, Deep Learning for Generic Object Detection: A Survey, Int J
Comput Vis 128 (2) (2020) 261–318. doi:10.1007/s11263-019-01247-4.
[36] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, H. J´egou, Training data-eﬃcient image transformers and distillation through
attention (2020). doi:10.48550/ARXIV.2012.12877.
[37] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu, J. Feng, T. Xiang, P. H. S. Torr, L. Zhang, Rethinking semantic segmentation from
a sequence-to-sequence perspective with transformers (2020). doi:10.48550/ARXIV.2012.15840.
[38] H. Cao, Y. Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, M. Wang, Swin-unet: Unet-like pure transformer for medical image segmentation
(2021). doi:10.48550/ARXIV.2105.05537.
[39] D. Liang, X. Chen, W. Xu, Y. Zhou, X. Bai, TransCrowd: weakly-supervised crowd counting with transformers, Sci. China Inf. Sci. 65 (6)
(2022) 160104. doi:10.1007/s11432-021-3445-y.
[40] G. Sun, Y. Liu, T. Probst, D. P. Paudel, N. Popovic, L. Van Gool, Boosting crowd counting with transformers (2021). doi:10.48550/
ARXIV.2105.10926.
[41] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu, C. Xu, W. Gao, Pre-trained image processing transformer (2020).
doi:10.48550/ARXIV.2012.00364.
[42] J. Cao, Y. Li, K. Zhang, L. Van Gool, Video super-resolution transformer (2021). doi:10.48550/ARXIV.2106.06847.
[43] Z. Wang, X. Cun, J. Bao, W. Zhou, J. Liu, H. Li, Uformer: A general u-shaped transformer for image restoration (2021). doi:10.48550/
ARXIV.2106.03106.
[44] T. Xiao, M. Singh, E. Mintun, T. Darrell, P. Doll´ar, R. Girshick, Early convolutions help transformers see better (2021). doi:10.48550/
ARXIV.2106.14881.
[45] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, ICLR (2015). doi:10.48550/arXiv.1412.6980.
[46] O. Keles¸, M. A. Yılmaz, A. M. Tekalp, C. Korkmaz, Z. Dogan, On the computation of psnr for a set of images or video (2021). doi:
10.48550/ARXIV.2104.14868.
[47] Z. Wang, A. Bovik, H. Sheikh, E. Simoncelli, Image quality assessment: from error visibility to structural similarity, IEEE Trans. Image
Process. 13 (4) (2004) 600–612. doi:10.1109/TIP.2003.819861.
[48] L. Wang, A survey on iqa (2021). doi:10.48550/ARXIV.2109.00347.
[49] A. Mittal, A. K. Moorthy, A. C. Bovik, No-reference image quality assessment in the spatial domain, IEEE Trans. Image Process. 21 (2012)
4695–4708. doi:10.1109/TIP.2012.2214050.
[50] A. Mittal, R. Soundararajan, A. C. Bovik, Making a “completely blind” image quality analyzer, IEEE Signal Process Lett. 20 (3) (2013)
209–212. doi:10.1109/LSP.2012.2227726.
[51] N. Venkatanath, D. Praneeth, M. C. Bh, S. S. Channappayya, S. S. Medasani, Blind image quality evaluation using perception based features,
in: IEEE NCC, IEEE, 2015, pp. 1–6. doi:10.1109/NCC.2015.7084843.
[52] J. Ke, Q. Wang, Y. Wang, P. Milanfar, F. Yang, Musiq: Multi-scale image quality transformer (2021). doi:10.48550/ARXIV.2108.05997.
[53] Pytorch toolbox for image quality assessment (2022).
URL https://pypi.org/project/pyiqa/
[54] J. Font-Mor´e, G. Reyes-Carmenaty, R. Lado-Roig´e, , M. A. P´erez, Performance analysis of vibration-based damage indicators under low-
18

modal information structures, Mechanical Systems and Signal Processing(Under review) (2023).
[55] M. A. P´erez, J. Font-Mor´e, J. Fern´andez-Esmerats, Structural damage assessment in lattice towers based on a novel frequency domain-based
correlation approach, Eng. Struct. 226 (2021). doi:10.1016/j.engstruct.2020.111329.
[56] M. A. P´erez, R. Serra-L´opez, A frequency domain-based correlation approach for structural assessment and damage identiﬁcation, Mechani-
cal Systems and Signal Processing 119 (2019) 432–456. doi:/10.1016/j.ymssp.2018.09.042.
[57] E. Figueiredo, G. Park, J. Figueiras, C. Farrar, K. Worden, Structural health monitoring algorithm comparisons using standard data sets, Tech.
Rep. LA-14393, 961604 (2009). doi:10.2172/961604.
19

