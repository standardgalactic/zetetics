How does over-squashing affect the power of GNNs?
Francesco Di Giovanni∗
University of Cambridge
fd405@cam.ac.uk
T. Konstantin Rusch∗
ETH Zürich
konstantin.rusch@sam.math.ethz.ch
Michael M. Bronstein
University of Oxford
Andreea Deac
Mila, Université de Montréal
Marc Lackenby
University of Oxford
Siddhartha Mishra
ETH Zürich
Petar Veličković
Google DeepMind
Abstract
Graph Neural Networks (GNNs) are the state-of-the-art model for machine learning on graph-structured
data. The most popular class of GNNs operate by exchanging information between adjacent nodes, and
are known as Message Passing Neural Networks (MPNNs). Given their widespread use, understanding the
expressive power of MPNNs is a key question. However, existing results typically consider settings with
uninformative node features. In this paper, we provide a rigorous analysis to determine which function
classes of node features can be learned by an MPNN of a given capacity. We do so by measuring the level
of pairwise interactions between nodes that MPNNs allow for. This measure provides a novel quantitative
characterization of the so-called over-squashing effect, which is observed to occur when a large volume of
messages is aggregated into fixed-size vectors. Using our measure, we prove that, to guarantee sufficient
communication between pairs of nodes, the capacity of the MPNN must be large enough, depending on
properties of the input graph structure, such as commute times. For many relevant scenarios, our analysis
results in impossibility statements in practice, showing that over-squashing hinders the expressive power of
MPNNs. We validate our theoretical findings through extensive controlled experiments and ablation studies.
1
Introduction
Graphs describe the relational structure for a large variety of natural and artificial systems, making learning on
graphs imperative in many contexts [59, 23, 62]. To this end, Graph Neural Networks (GNNs) [31, 55] have
emerged as a widely popular framework for graph machine learning, with plentiful success stories in science
[5, 19, 9, 57, 40] and technology [46, 45, 22].
Given an underlying graph and features, defined on its nodes (and edges), as inputs, a GNN learns parametric
functions from data. Due to the ubiquity of GNNs, characterizing their expressive power, i.e., which class
of functions a GNN is able to learn, is a problem of great interest. In this context, most available results
in literature on the universality of GNNs pertain to impractical higher-order tensors [43, 37] or unique node
identifiers that may break the symmetries of the problem [41]. In particular, these results do not necessarily
apply to Message Passing Neural Networks (MPNNs) [30], which have emerged as the most popular class of GNN
models in recent years. Concerning expressivity results for MPNNs, the most general available characterization
is due to [64] and [47], who proved that MPNNs are, at most, as powerful as the Weisfeiler-Leman graph
isomorphism test [61] in distinguishing graphs without any features. This brings us to an important question:
∗Equal contribution.
1
arXiv:2306.03589v2  [cs.LG]  16 Aug 2023

u
v
w
q
. . .
z
Figure 1: We study the power of MPNNs in terms of the mixing they induce among features and show that
this is affected by the model (via norm of the weights and depth) and the graph topology (via commute times).
For the given graph, the MPNN learns stronger mixing (tight springs) for nodes v, u and u, w since their
commute time is small, while nodes u, q and u, z, with high commute-time, have weak mixing (loose springs).
We characterize over-squashing as the inverse of the mixing induced by an MPNN and hence relate it to its
power. In fact, the MPNN might require an impractical depth to solve tasks on the given graph that depend
on high-mixing of features assigned to u, z.
Which classes of functions can MPNNs of a given capacity learn, if node features are specified?
Razin et al. [50] address this question by characterizing the separation rank of MPNNs; however, their analysis
only covers unconventional architectures that do not correspond to MPNN models used in practice. In contrast,
Alon and Yahav [2] investigate this question empirically, by observing that MPNNs fail to solve tasks which
involve long-range interactions among nodes. This limitation was ascribed to a phenomenon termed as over-
squashing, which loosely entails messages being ‘squashed’ into fixed-size vectors when the receptive field of
a node grows too fast. This effect was formalized in [58, 25, 8], who showed that the Jacobian of the nodes
features is affected by topological properties of the graph, such as curvature and effective resistance. However,
all the aforementioned papers ignore the specifics of the task at hand, i.e., the underlying function that the
MPNN seeks to learn, leading us to the following question:
How does over-squashing affect the expressive power of MPNNs? Can we measure it?
Contributions. Our main goal is to address the above question and show how over-squashing can be
understood as the misalignment between the task and the graph-topology, ultimately limiting the
classes of functions that MPNNs of practical size can learn (see Figure 1). We start by measuring
the extent to which an MPNN allows any pair of nodes in the graph to interact (via mixing their features).
With this measure as a tool, we characterize which functions of node features can be learned by an MPNN and
how the model architecture and parameters, as well as the topology of the graph, affect the expressive power.
More concretely,
• We introduce a novel metric of expressive power that is based on the Hessian of the graph function learned
by an MPNN and measures the ability of a model to mix features associated with different nodes. This
approach complements the current repertoire of tools at disposal to assess the power of GNNs, and is of
interest independent of the over-squashing phenomenon.
• We then prove upper bounds on the Hessian of functions learned by MPNNs, hence deriving limitations
on the power of MPNNs to mix features (i.e., model interactions) according to the novel metric mentioned
above. As far as we know, this is the first theoretical result stating limitations of MPNNs to learn
functions and their derivatives.
• We characterize over-squashing as the reciprocal of the maximal mixing that MPNNs can induce among
nodes: the higher this measure, the smaller the class of functions MPNNs can learn. Differently from the
2

WL-test, which looks at the ability to separate node representations based on the computational trees,
here we are interested in studying the ability of an MPNN to mix information associated with different
nodes and the cost required to do so in terms of size of the weights and depth. In fact, our results also
apply even when powerful structural (positional) features are provided.
• We prove that the weights and depth must be sufficiently large – depending on the topology – to ensure
mixing. For some tasks, the depth must exceed the highest commute time on the graph, resulting in
impossibility statements. Our results show that MPNNs of practical size, fail to learn functions with
strong mixing among nodes at high commute time.
• We illustrate our theoretical results with controlled experiments that verify our analysis, by highlighting
the impact of the architecture (depth), of the topology (commute time), and of the underlying task (the
level of mixing required).
Thus, we present a novel paradigm for deriving rigorous results in relating two key notions in graph machine
learning, that of GNN expressive power and over-squashing.
2
The Message-Passing paradigm
Definitions on graphs. We denote a graph by G = (V, E), where V is the set of n nodes while E are the
edges. We assume that G is undirected, connected and non-bipartite and define the n × n adjacency matrix
A as Avu = 1 if (v, u) ∈E and zero otherwise. We let D be the diagonal degree matrix with Dvv := dv and
use dmax and dmin to denote the maximal and minimal degrees. Since we are interested in the over-squashing
phenomenon, which affects the propagation of information, we need to quantify distances on G. We let dG(v, u)
be the length of the shortest path connecting nodes v and u (geodesic distance). While dG describes how far
two nodes u, v are in G, it does not account for how many different routes they can use to communicate. In fact,
we will see below that the over-squashing of nodes v, u and, more generally, the mixing induced by MPNNs
among the features associated with v, u, can be better quantified by their commute time τ(v, u), equal to the
expected number of steps for a random walk to start at v, reach u, and then come back to v.
The MPNN-class. For most problems, graphs are equipped with features {xv}v∈V ⊂Rd, whose matrix
representation is X ∈Rn×d. To study the interactions induced by a GNN among pairs of features, we focus on
graph-level tasks – in Section E of the Appendix, we extend the discussion and our main theoretical results to
node-level tasks. The goal then is to predict a function X 7→yG(X), where we assume that the graph G is fixed
and thus yG : Rn×d →R is a function of the node features. MPNNs define a family of parametric functions
through iterative local updates of the node features: the feature of node v at layer t is derived as
h(t)
v
= f (t) 
h(t−1)
v
, g(t) 
{{h(t−1)
u
, (v, u) ∈E}}

,
h(0)
v
= xv
(1)
where f (t), g(t) are learnable functions and the aggregation function g(t) is invariant to permutations. Specifically,
we study a class of MPNNs of the following form,
h(t)
v
= σ

Ω(t)h(t−1)
v
+ W(t) X
u
Avuψ(t)(h(t−1)
v
, h(t−1)
u
)

,
h(0)
v
= xv,
(2)
where σ acts pointwise, Ω(t), W(t) ∈Rd×d are weight matrices, A ∈Rn×n is any matrix satisfying Avu > 0 if
(v, u) ∈E and zero otherwise – A is typically some (normalized) version of the adjacency matrix A – and ψ(t)
is a learnable message function. The layer-update in (2) includes common MPNN-models such as GCN [38],
SAGE [33], GIN [64], and GatedGCN [11]. As commented extensively in Section 6, this is the most general
class of MPNN equations studied thus far in theoretical works on over-squashing; unless otherwise stated, all
our considerations and analysis apply to MPNNs as in (2).
3

For graph-level tasks, an additional permutation-invariant readout READ is required – usually MAX, MEAN, or
SUM. We define the graph-level function computed by the MPNN after m layers to be
y(m)
G
(X) = ⟨θ, READ({{h(m)
v
}})⟩,
(3)
for some learnable θ ∈Rd. While a more complex non-linear readout can be considered, we restrict to a single
linear-layer since we are interested in the mixing among variables induced by the MPNN itself through the
topology (and not in readout, independently of the graph-structure).
3
On the mixing induced by Message Passing Neural Networks
As one of the main contributions of this paper, we propose a new framework for characterizing the expressive
power of MPNNs by estimating the amount of mixing they induce among pairs of features xv and xu, corre-
sponding to the nodes v and u ∈V. To motivate our definition, let us fix the underlying graph G, let yG be the
ground-truth function to be learned, and suppose, for simplicity, that the node features {xi} are all scalar. If yG
is a smooth function, then we can take the Taylor expansion of yG at any point ¯x = (¯x1, . . . , ¯xn) and obtain a
polynomial in the variables (x1, . . . , xn), up to higher-order corrections. The mixing induced by yG around the
feature value ¯x can then be expressed in terms of mixed product monomials of the form xvxu, and the powers
thereof, with v and u different nodes in G. The lowest-degree mixed monomials of this form are multiplied by
the Hessian (i.e. the second-order derivatives) of yG. Accordingly, we can take the entries v, u of the Hessian of
yG as the simplest measure of pairwise mixing induced by the given function over the nodes v, u. In fact, to
make things more concrete, note that if yG(x) = ϕ0(xv) + ϕ1(xu), then the mixing (Hessian) would always be
zero since the variables are fully separable; conversely, if yG(x) = ϕ(⟨xv, xu⟩), then the amplitude of the mixing
(Hessian) depends on how nonlinear ϕ is. The same reasoning applies to higher-dimensional node features too:
Definition 3.1. For a twice differentiable graph-function yG of node features {xi}, the maximal mixing
induced by yG among the features xv and xu associated with nodes v, u is
mixyG(v, u) = max
xi
max
1≤α,β≤d

∂2yG(X)
∂xαv ∂xβ
u
 .
(4)
We note that the first maximum is taken over all input features – considering that the Hessian of yG is itself a
function of the node features over the graph – while the second maximum is taken over all entries α, β of the
d-dimensional node features xv and xu.
Problem statement. Our goal is to study the expressive power of MPNNs in terms of the (maximal) mixing
they can generate among nodes v, u. A low value of mixing implies that the MPNN cannot learn functions
yG that require high mixing of the features associated with v, u and hence they cannot model ‘product’-type
interactions as per our explanation based on the Taylor expansion. We investigate how weights and depth on
the one side, and the graph topology on the other, affect the mixing of an MPNN.
The requirement of smoothness. In many applications, especially when deploying neural network models
to solve partial differential equations, the predictions need to be sufficiently regular (smooth), which motivates
the adoption of smooth non-linear activations [34, 10, 27] beyond the piece-wise linear function ReLU. Our
analysis below follows this paradigm and holds for all activations σ that are (at least) twice differentiable.
3.1
Pairwise mixing induced by MPNNs
In this Section, our goal is to derive an upper bound on the maximal mixing induced by MPNNs, as defined
above, over the features associated with pairs of nodes v, u. To motivate the structure of this bound, we
consider the simple yet illustrative setting of an MPNN as in (2) with scalar features, weights ω, w > 0 and a
4

linear message function of the form ψ(x, y) = c1x + c2y, for some learnable constants c1, c2. In this case, the
layer-update (2) takes the very simple form,
h(t)
v
= σ(w(Sh(t−1))v),
S := ω
wI + c1diag(A1) + c2A ∈Rn×n,
(5)
where 1 ∈Rn is the vector of ones. Hence, the operator wS governs the flow of information from layer t −1 to
layer t – once we factor out the derivatives of σ. Then, it is straightforward to observe that the k-power of this
matrix (wS)k determines the propagation of information on the graph over k layers, i.e. over walks of length k.
Suppose that nodes v, u are at distance r on the graph. If this MPNN has depth m, to have any mixing among
the features of v and u, there has to be a node i that receives information from both v and u, which only
occurs when m ≥r/2. If m = r/2, then we can estimate the mixing at node i as ((wS)m)iv((wS)m)iu. When
m > r/2, information from v and u can arrive at i after being processed at an intermediate node j; in fact, this
node j first collects information from both v and u over a walk of length m −k, and then shares it with node i
over a walk of length k. We then obtain terms of the form P
k
P
j((wS)m−k)jv((wS)k)ij((wS)m−k)ju, since we
are summing over all possible ways of aggregating information from v and u across intermediate nodes j, before
mixing it at i.
A similar argument to upper bound the maximal mixing also works in the general case of (2), once we account
for bounds on the non-linear activation function σ by cσ = max{|σ′|, |σ′′|}, and we choose ω, w, c1, c2 satisfying
∥Ω(t)∥≤ω, ∥W(t)∥≤w, ∥∇iψ(t)∥≤ci,
for i = 1, 2, where ∇iψ is the Jacobian of ψ with respect to the i-th variable, and ∥· ∥is the operator (or
spectral) norm of a matrix. For models with linear ψ, such as GCN, SAGE or GIN, these constants will suffice
in deriving the upper bound on the mixing. However, in the more general case of non-linear message functions
ψ, which for example includes GatedGCN, we also need to account for the term Qk, defined below, which arises
due to the Hessian of ψ when taking second-order derivatives of the MPNN (2): given S in (5), we set
Pk := (Sm−k−1)⊤diag(1⊤Sk)(ASm−k−1)
Qk := Pk + P⊤
k + (Sm−k−1)⊤diag(1⊤Sk(diag(A1) + A))Sm−k−1.
(6)
We assume that the Hessian of ψ is bounded as ∥∇2ψ(t)∥≤c(2). Recall that y(m)
G
is the MPNN-prediction (3)
and that mixy(m)
G
(v, u) is its maximal mixing of nodes v, u as per Definition 3.1.
Theorem 3.2. Consider an MPNN of depth m as in (2), where σ and ψ(t) are C2 functions and we denote the
bounds on their derivatives and on the norm of the weights as above. Let S and Qk be defined as in (5) and (6),
respectively. If the readout is MAX, MEAN or SUM and θ in (3) has unit norm, then the mixing mixy(m)
G
(v, u)
induced by the MPNN over the features of nodes v, u satisfies
mixy(m)
G
(v, u) ≤
m−1
X
k=0
(cσw)2m−k−1 
w(Sm−k)⊤diag(1⊤Sk)Sm−k + c(2)Qk

vu .
(7)
Theorem 3.2 shows how the mixing induced by an MPNN depends on the model (via regularity of σ, norm of
the weights w, and depth m) and on the graph-topology (via the powers of A, which enters the definition of S in
(5)). Despite its generality, from (7) it is as yet unclear which specific properties of the underlying graph affect
the mixing the most, and how weights and depth of the MPNN can compensate for it. Therefore, our goal now
is to expand (7) and relate it to known quantities on the graph and show how this can be used to characterize
the phenomenon of over-squashing. First, we introduce a notion of capacity of an MPNN in the spirit of [41].
The capacity of an MPNN. For simplicity, we assume that cσ = 1, since this is satisfied by most commonly
used non-linear activations – it is straightforward to extend the analysis to arbitrary cσ.
5

Definition 3.3. Given an MPNN with m layers and w the maximal spectral norm of the weights, we say that
the pair (m, w) represents the capacity of the MPNN.
A larger capacity, by increasing either m or w, or both, heuristically implies that the MPNN has more expressive
power and can hence induce larger mixing among the features associated with nodes v, u. Accordingly, given v, u,
we formulate the problem of expressivity as: what is the capacity required to induce enough mixing mixyG(v, u)?
Studying expressivity through derivatives. In applications to physics and PDEs, we may often need
the neural-network prediction to also match the derivatives of the ground-truth function [34]. Theorem 3.2
provides an upper bound on the ability of an MPNN to learn functions with non-trivial second-order derivatives
among nodes. In particular, (7) shows that the second-order derivatives of MPNN predictions as in (2), cannot
approximate second-order derivatives of graph-functions yG whose associated mixing is larger than the right
hand side of (7). Our analysis is more general than the over-squashing problem, and represents, to the best of
our knowledge, the first theoretical analysis on the limitations of MPNNs to approximate classes of functions
and the derivatives thereof.
4
Over-squashing limits the expressive power of MPNNs
Over-squashing was originally described in [2] as the failure of MPNNs to propagate information across distant
nodes. In fact, [8, 25] showed that over-squashing – interpreted as the sensitivity of node v to the input feature
at node u via their Jacobian – is affected by the effective resistance (or equivalently, commute time) of v and
u – however no connection to how over-squashing affects the expressive power of MPNNs, nor proposal of
measures for over-squashing have been derived. Over-squashing is then related to the inability of MPNNs to
model interactions among certain nodes, depending on the underlying graph topology. Since one can rely on the
Taylor expansion of a graph function to model such interactions through the second-order derivatives, i.e. the
maximal mixing, we leverage Definition 3.1 to propose a novel, broader, but more accurate, characterization of
over-squashing:
Definition 4.1. Given the prediction y(m)
G
of an MPNN with capacity (m, w), we define the pairwise over-
squashing of v, u as
OSQv,u(m, w) =

mixy(m)
G
(v, u)
−1
.
Our notion of over-squashing is a pairwise measure over the graph that naturally depends on the graph-topology,
as well as the capacity of the model. In particular, it captures how over-squashing pertains to the ability of
the model to mix (induce interactions) between features associated with different nodes. If such maximal
mixing is large, then there is no obstruction to exchanging information between the given nodes and hence
the over-squashing measure would be small; conversely, the over-squashing is large precisely when the model
struggles to mix features associated with nodes v and u.
In general though, computing the actual mixing induced by an MPNN may be difficult, especially if we wish
to derive an analytic expression thereof. Accordingly, we can rely on Theorem 3.2 to derive a proxy for the
over-squashing measure that will be used to obtain necessary conditions on the capacity of an MPNN to induce
a required level of mixing:
Definition 4.2. Given an MPNN with capacity (m, w), we approximate OSQv,u(m, w) by
g
OSQv,u(m, w) :=
 m−1
X
k=0
w2m−k−1
w(Sm−k)⊤diag(1⊤Sk)Sm−k + c(2)Qk

vu
−1
≤OSQv,u(m, w).
First, we note that the upper bound is a simple application of Theorem 3.2. To justify our characterization,
we begin by considering simple settings. If the network has no bandwidth through the weights (w = 0), then
6

]
OSQv,u(m, 0) = ∞. Besides, the proposed measure captures the special case of under-reaching [6], since ]
OSQv,u
is infinite (i.e., zero mixing) whenever 2m < dG(v, u), where dG is the shortest-walk distance on G. We also
recall that for simplicity we have taken cσ = 1, but the measure extends to arbitrary non-linear activations σ.
Finally, we generalize the characterization of OSQ to node-level tasks in Section E of the Appendix.
We can rephrase our novel approach to studying expressivity through pairwise mixing, in terms of the over-
squashing measure and its proxy. By Theorem 3.2 we derive that a necessary condition for a smooth
MPNN to learn a function yG with mixing mixyG(v, u) is
]
OSQv,u(m, w) < (mixyG(v, u))−1 .
(8)
An MPNN of given capacity might suffer or not from over-squashing, depending on the level of mixing required
by the underlying task. Over-squashing can then be understood as the misalignment between the task and the
underlying topology, as measured by the gap between the maximal mixing induced by an MPNN over nodes
v, u and the mixing required by the task.
Strategy: For a given graph G, to reduce the value of OSQ and hence satisfy (8), the capacity (m, w) must
satisfy constraints posed by G and the choice of v, u. Since we can increase the capacity by taking either larger
weights or more layers, we consider these two regimes separately. Below, we expand (8) in order to derive
minimal requirements on the quantities w and m to induce a certain level of mixing. Equivalently, we derive
lower bounds on the capacity of an MPNN to learn functions with given second-order derivatives (mixing).
Choice of A. For simplicity, we restrict our analysis to the case A = Asym := D−1/2AD−1/2 and extend the
results to D−1A and A in Section D of the Appendix. For the unnormalized adjacency, the bounds look more
favourable, simply because we are no longer normalizing the messages, which makes the model more sensitive
for all nodes. In Section D.4 we comment on how one could account for the lack of normalization by taking
relative measurement for OSQ in the spirit of [64].
4.1
The case of fixed depth m and variable weights norm w
To assess the ability of the norm of the weights w to increase the capacity of an MPNN and hence reduce ]
OSQ,
we consider the limit case where the depth m is the minimal required for an MPNN to have a non-zero mixing
among v, u (half the shortest-walk distance dG between the nodes).
Theorem 4.3. Let A = Asym, r := dG(v, u), m = ⌈r/2⌉, and q be the number of paths of length r between v and
u. For an MPNN satisfying Theorem 3.2 with capacity (m = ⌈r/2⌉, w), we find g
OSQv,u(m, w)·(c2w)r(Ar)vu ≥1.
In particular, if the MPNN generates mixing mixyG(v, u), then
w ≥dmin
c2
mixyG(v, u)
q
 1
r
.
Theorem 4.3 highlights that if the depth is set as the minimum required for any non-zero mixing, then the
norm of the weights w has to be large enough depending on the connectivity of G – recall that for models as
GCN, we have c2 = 1. However, increasing w is not optimal, since it makes the whole model more sensitive for
all nodes, which may lead to poorer generalization capabilities [7, 28].
Some examples. We illustrate the bounds in Theorem 4.3 and for simplicity, we set c2 = 1. Consider a tree Td
of arity d, with v the root and u a leaf at distance r and depth m = r/2; then ]
OSQv,u(m, w) ≥w−r(d + 1)r−1
and the spectral norm required to generate mixing y(v, u) is
w ≥(d + 1)
y(v, u)
d + 1
 1
r
.
7

We note that by taking d = 1 we recover the case of the path-graph (1D grid). Since the spectral norm of the
weights grows with the branching factor, we see that, in general, the capacity required by MPNNs to solve
long-range tasks could be higher on graphs than on sequences [2]. We also consider the case of a 1-layer MPNN
on a complete graph Kn with v ̸= u. We find that ]
OSQv,u(m, w) ≥(n −1)/w and hence the spectral norm
required to generate mixing y(v, u) is w ≥(n −1)y(v, u). We note how the measure of over-squashing also
captures the problem of redundancy of messages [16]. In fact, even if v, u are at distance 1, the more nodes
are there in the complete graph and hence the more messages are exchanged, the more difficult for a shallow
MPNN to induce enough mixing among those specific nodes.
4.2
The case of fixed weights norm w and variable depth m
We now study the (desirable) setting where w is bounded, and derive the depth necessary to induce mixing
of nodes v, u.
Below, we let ∆= I −Asym be the normalized graph Laplacian, whose eigenvalues are
0 = λ0 < λ1 ≤. . . ≤λn−1; we note that λ1 is the spectral gap and λn−1 < 2 if G is not bipartite [18]. We also
recall that dG is the shortest-walk distance and τ is the commute time (defined in Section 2). Finally, if dmax
and dmin denote the maximal and minimal degrees, respectively, we set γ :=
p
dmax/dmin.
Theorem 4.4. Consider an MPNN satisfying Theorem 3.2, with max{w, ω/w + c1γ + c2} ≤1, and A = Asym.
If g
OSQv,u(m, w) · (mixyG(v, u)) ≤1, and hence the MPNN generates mixing mixyG(v, u) among the features
associated with nodes v, u, then the number of layers m satisfies
m ≥τ(v, u)
4c2
+
|E|
√dvdu
mixyG(v, u)
γµ
−1
c2
γ + |1 −c2λ∗|r−1
λ1
+ 2c(2)
µ

,
where r = dG(v, u), µ = 1 + 2c(2)(1 + γ) and |1 −c2λ∗| = max0<ℓ≤n−1 |1 −c2λℓ| < 1.
Theorem 4.4 provides a necessary condition on the depth of an MPNN to induce enough mixing among nodes
v, u. We see that the MPNN must be sufficiently deep if the task depends on interactions between nodes at
high commute time τ and in fact, if the required mixing is large enough, then m = Ω(τ(v, u)). Note that the
lower bound on the depth can translate into a practical impossibility statement, since the commute time τ can
be as large as O(n3) [15]. We also recall that for MPNN models such as GCN, the bounds above simplify since
c(2) = 0 and hence µ = 1.
Therefore, if (i) the graph is such that the commute time between v, u is large, and (ii) the task depends
on high-mixing of features associated with v, u, then over-squashing limits the expressive power of
MPNNs since the depth has to scale impractically with the graph size n – which also brings forth other issues
such as over-smoothing [52, 14, 53, 24]. In contrast to existing analysis on the limitations of MPNNs via
graph-isomorphism test, our results characterize the expressivity of MPNNs even when meaningful features
are provided. In fact, Theorem 4.4 implies that:
Corollary 4.5. On a graph with features, MPNNs as in Theorem 4.4 with depth m ≤n, cannot learn functions
that induce high mixing among features of nodes with large commute time.
We also note that for two adjacent nodes v, u, the commute time equals 2|E| if (v, u) is a cut-edge [1]. Our result
shows that MPNNs may require m = Ω(|E|) to generate enough mixing of features associated with a cut-edge,
drawing a connection with [66], where it was shown that most GNNs fail to identify cut-edges on unattributed
graphs. Finally, since the commute time is proportional to the effective resistance R by 2R(v, u)|E| = τ(v, u),
Theorem 4.4 connects recent works [25, 8] relating the Jacobian of node features and R, and the expressive
power of MPNNs in terms of their mixing.
We conclude this Section by emphasizing how the theoretical results we have derived are far more general than
assessing the inability of an MPNN to solve tasks with long-range interactions. Our precise characterization
8

of mixing has allowed to formally deduce limitations on the ability of an MPNN to mix features, based
on the capacity of the model and properties of the the graph-topology. Our analysis shows, precisely, how
over-squashing can be understood as a fundamental problem associated with how hard is for an MPNN to
exchange information between nodes that are ‘badly connected’, as per their commute time metric distance.
5
Experimental validation of the theoretical results
Next, we aim to empirically verify the theoretical findings of this paper, i.e., the characterization of the impact
of the graph topology (via commute time τ), the GNN architecture (depth, norm of weights), and the underlying
task (node mixing) on over-squashing. This, however, requires detailed information about the underlying
function to be learned, which is not readily available in practice. Hence, we perform our empirical test in
a controlled environment, but at the same time, we base our experiments on the real world ZINC chemical
dataset [35] and follow the experimental setup in [26], constraining the number of molecular graphs to 12K.
Moreover, we exclude the edge features from this experiment and fix the MPNN size to ∼100K parameters.
However, instead of regressing the constrained solubility based on the molecular input graphs, we define our
own synthetic node features as well as our own target values as follows.
Let {Gi} be the set of the 12K ZINC molecular graphs. We set all node features to zero, except for two, which
are set to uniform random numbers xi
ui, xi
vi between 0 and 1 (i.e., xi
ui, xi
vi ∼U(0, 1)) for all i. The target
output is set to yi = tanh(xi
ui + xi
vi) for all i. Hence, the task entails a non-linear mixing with non-vanishing
second derivatives. The two non-zero node features xi
ui, xi
vi are positioned on Gi according to the commute
time τ, i.e., for a given α ∈[0, 1], we choose the nodes ui, vi as the α-quantile of the τ-distribution over Gi. This
enables us to have the desired control on the level of commute time of the underlying mixing (i.e., α1 < α2, if
α1 results in a lower τ than α2). See Fig. 2 for an illustration of a ZINC graph together with the histogram of
the corresponding τ between all pairs of different nodes. We call this graph dataset the synthetic ZINC dataset.
τ(•,•) ≈30
τ(•,•) ≈36
τ(•,•) ≈252
0
50
100
150
200
250
Commute time τ
0
20
40
60
Frequencies
Figure 2: (Left) Exemplary molecular graph of the ZINC (12K) dataset with colored nodes corresponding to
different values of commute time τ. We note that τ is a more refined measure than the distance, and in fact
beyond long-range nodes (red case), τ also captures other topological properties (yellow nodes are adjacent but
belong to a cut-edge, so their commute-time is 2|E|). (Right) Histogram of commute time τ between all pairs
of the graph nodes.
We consider four different MPNN models namely GCN [38], GIN [64], GraphSAGE [33], and GatedGCN [11].
Moreover, we choose the MAX-pooling as the GNN readout, which is supported by Theorem 3.2 and forces the
GNNs to make use of the message-passing in order to learn the mixing.
9

5.1
The role of commute time
In this task, we empirically analyse the effect of the commute time τ of the underlying mixing on the performance
of the MPNNs. To this end, we fix the architecture for all considered MPNNs. In particular, we set the depth
to m = maxi⌈diam(Gi)/2⌉, which happens to be m = 11 for the considered ZINC 12K graphs, such that the
MPNNs are guaranteed not to underreach. We further vary the value of the α-quantile of the τ-distributions
over the graphs Gi between 0 and 1, thus controlling the level of commute times. According to our theoretical
findings in Section 4, the measure ]
OSQv,u (Definition 4.2) heavily depends on the commute time τ of the
underlying mixing as derived in Theorem 4.4. In fact, this can be empirically verified in Appendix Fig. 7,
where ]
OSQv,u increases for increasing values of α. Thus, we would expect the MPNNs to perform significantly
worse for increasing levels of the commute time. This is indeed confirmed in Fig. 3 depicting the resulting test
mean absolute error (MAE), and showing that the test MAE increases for larger values of α for all considered
MPNNs. The same qualitative behavior can be observed for the training MAE (Appendix F.3). We further
note that the models exhibit low standard deviation (over several trained MPNNs), as can be seen in Appendix
Table 2.
0.0
0.2
0.4
0.6
0.8
1.0
Commute time α-quantile
10−4
10−3
10−2
Test MAE
GCN
GIN
GraphSAGE
GatedGCN
Figure 3: Test MAE of GCN, GIN, GraphSAGE, and
GatedGCN on synthetic ZINC, where the commute
time of the underlying mixing is varied, while the
MPNN architecture is fixed (e.g., depth, number of
parameters), i.e., mixing according to increasing val-
ues of the α-quantile of the τ-distribution over the
ZINC graphs.
5
8
16
32
Number of layers m
0.050
0.010
0.005
Test MAE
GCN
GIN
GraphSAGE
GatedGCN
Figure 4: Test MAE of GCN, GIN, GraphSAGE,
and GatedGCN on synthetic ZINC, where the com-
mute time is fixed to be high (i.e., at the level of the
0.8-quantile), while only the depth of the underlying
MPNN is varied between 5 and 32 (all other archi-
tectural components are fixed).
5.2
The role of depth
In this task, we study the effect of the depth on the performance of the MPNNs. To this end, we consider
a high commute time-regime by setting α = 0.8. Note that in this case the maximum (over all graphs Gi)
shortest path between two nodes ui, vi is 14. Therefore, a depth of m = 7 is sufficient to avoid under-reaching
on all graphs. However, according to the over-squashing measure we provide and the conclusions of Theorem
4.4, we expect the MPNNs to be able to induce more mixing among nodes v, u, and hence reduce the error,
as we increase the number of layers. This expectation is further evidenced in Appendix Fig. 8, where the
computed ]
OSQ decreases for increasing number of layers. In Fig. 4, we plot the test MAE of all considered
MPNNs for increasing number of layers. We can indeed see that all considered GNNs benefit from depth, and
thus higher capacity (Definition 3.3), as GatedGCN obtains the lowest test MAE with 17 layers, GraphSAGE
with 30 layers, GIN with 24 layers, and GCN with 32 layers. The same qualitative behavior can be observed
for the training MAE (Appendix F.3). Our theoretical results provide a strong explanation as to why a task
only depending on the mixing of nodes within 14 hops – so that 7 layers would suffice – actually benefits from
10

many more layers. Naturally, we cannot increase the depth arbitrarily, as at some point other issues emerge,
such as vanishing gradients, which impact the trainability of the MPNNs [51]. Moreover, as can be seen in
Table 3, all models exhibit low standard deviation (over several trained MPNNs). We finally point out that,
while over-squashing occurs – as can be seen when the number of layers ‘only’ equals the minimal one to avoid
under-reaching – it can be mitigated by taking much deeper architectures. However, this improvement here is
also made possible because the task only depends on the interaction between nodes at large commute time; in
real-world scenarios, typically one has interactions between nodes at small and large commute time, and in
this case increasing the depth may have a positive or negative impact for the mixing of nodes that have small
commute time.
5.3
The role of mixing
We further test the considered MPNN architectures on their performance with respect to different mixings.
To this end, we consider again the tanh-based mixing as in our previous tasks (i.e., regressing targets
yi = tanh(xi
ui + xi
vi) for each graph Gi in the dataset), as well as another mixing based on the exponential
function (i.e., with targets yi = exp(xi
ui + xi
vi)). We note that these two tasks differ significantly in terms of
their maximal mixing values (4) (shown in Table 1). Thus, according to (8) and Theorem 4.4, we would expect
that MPNNs perform significantly worse in the case of higher maximal mixing, i.e., for the exponential-based
mixing compared to the tanh-mixing. To confirm this empirically, we train the MPNNs on both types of mixing
and provide the resulting relative MAEs (i.e., MAE divided by the L1-norm of the targets) in Table 1. We can
see that all four MPNNs perform significantly better on the tanh-mixing than on the exponential-based mixing.
Moreover, increasing the range for the exponential-based mixing from 1 to 1.5 further impairs the performance
of all considered MPNNs. In order to check if this difference in performance can simply be explained by a
higher capacity required by a neural network to accurately approximate the mapping exp(x + y) compared to
tanh(x + y) for some inputs x, y ∈R, we train a simple two-layer feed-forward neural network (with 2 inputs,
i.e., x and y) on both mappings. The trained networks reach a similarly low relative MAE of 4.6 × 10−4 for
the tanh(x + y) mapping as well as 4.1 × 10−4 for the exp(x + y) mapping using an input range of (0, 1) and
4.0 × 10−4 for an input range of (0, 1.5). Thus, we can conclude that the significant differences in the obtained
results in Table 1 are not caused by a higher capacity required by a neural network to learn the underlying
mappings of the different mixings.
Table 1: Relative MAE of GCN, GIN, GraphSAGE and GatedGCN on different choices of mixing on synthetic
ZINC for a fixed 0.8-quantile of the commute time distributions over graphs Gi.
Mixing
input interval
maximal mixing
GCN
GIN
GraphSAGE
GatedGCN
tanh(xi
ui + xi
vi)
(0, 1)
≈0.77
0.024
0.014
0.006
0.004
exp(xi
ui + xi
vi)
(0, 1)
≈7.4
0.043
0.021
0.033
0.008
exp(xi
ui + xi
vi)
(0, 1.5)
≈20.1
0.054
0.035
0.075
0.014
6
Discussion
Related Work: expressive power of MPNNs. The MPNN class in (2) is as powerful as the 1-WL
test [61] in distinguishing unattributed graphs [64, 47]. In fact, MPNNs typically struggle to compute graph
properties on feature-less graphs [21, 17, 54, 41]. The expressivity of GNNs has also been studied from the
point of view of logical and tensor languages [6, 4, 29]. Nonetheless, far less is known about which functions
of node features MPNNs can learn and the capacity required to do so. Razin et al. [50] recently studied the
separation rank of a specific MPNN class. While this approach is a strong inspiration for our work, the results
in [50] only apply to a single unconventional family of MPNNs which does not include MPNN models used
11

in practice. Our results instead hold in the full generality of (2) and provide a novel approach to investigate
expressivity of MPNNs through the mixing they are able to generate among features. Besides, to the best of our
knowledge, this is the first work formally analysing the limitations on the expressive power of MPNNs to learn
functions and their second-order derivatives.
Differences between mixing and the WL test. To further motivate how our proposed metric of expressivity
differs from the more established one based on the WL test, we emphasize that throughout our analysis we
had no assumption on the nature of the features, that can in fact be structural or positional – meaning that
the MPNNs we have considered above, may also be more powerful than the 1-WL test. Our derivations do
not rely on the ability to distinguish different node representations, but rather on the ability of the MPNN to
mix information associated with different nodes. This novel alternative paradigm may help design GNNs that
are more powerful at mixing than MPNNs, and may further shed light on how and when frameworks such as
Graph Transformers can solve the underlying task better than conventional MPNNs.
Differences between our results and existing works on over-squashing.
The problem of over-
squashing was introduced in [2] and studied through sensitivity analysis [63] in [58]. This approach was
generalized in [8, 25] who proved that the Jacobian of node features is likely to be small if the nodes have high
commute time (effective resistance). We discuss more in detail the novelty of this work when compared to
[58, 25]. (i) In works of [58, 25] there is no analysis on which functions MPNNs cannot learn as a consequence
of over-squashing, nor a formal measure for assessing over-squashing is provided. In fact, they only derive
some conditions under which the Jacobian of node features can be small, but do not connect it to the ability
of MPNNs to learn desired function classes. Besides, the Jacobian of node features may not be suited for
studying over-squashing for graph-level tasks. Note that our theory also holds for node-level tasks – as argued
in Section E of the Appendix, looking at the Jacobian corresponds to a first-order (Taylor) expansion of the
prediction, while our emphasis extends this to second-order terms. (ii) While [58] shows that the Jacobian
of nodes nodes at distance 2 is harmfully small when they are separated by an edge with strong negative
curvature, their analysis fails to study over-squashing among nodes at distance larger than 2 and does not
provide insights on the capacity required to learn certain tasks. (iii) Finally, the analysis in [25] does not
account for MPNNs such as GatedGCN (while ours does), and is only carried out in a simplified setting where
the activation function is chosen to be a ReLU and its derivatives are taken to be the same, in expectation, for
every path in the computational graph, which is an unrealistic assumption (our results instead work for smooth
activations with no extra assumption). We have extended these ideas to connect over-squashing and expressive
power by studying higher-order derivatives of the MPNN and relating them to the capacity of the model and the
underlying graph-topology.
6.1
Limitations and ways forward
The measures OSQ and g
OSQ. Definition 4.1 considers pairs of nodes and second-order derivatives; this could
be generalized to build a hierarchy of measures with increasingly higher-order interactions of nodes. The proxy
(lower bound) ]
OSQ in Definition 4.2 we have based our analysis on, allows to derive necessary conditions for
the MPNN to learn classes of functions based on their mixing. If, depending on the problem, one has access to
better estimates of the mixing induced by an MPNN than (7), then one can extend our approach and get a
finer approximation of OSQ. In particular, extending the analysis to lower bounds on the mixing, in order to
provide sufficient conditions for learning tasks, is a topic for future work. In the paper we focused our analysis
on the case of graph-level tasks; we provide an extension to node-level functions in Section E.
Beyond sum-aggregations. Our results apply to MPNNs as in (2), where A is constant, and do not include
attention-based MPNNs [60, 12] or Graph-Transformers [39, 44, 65, 49] which further depend on features
via normalization. Extending the analysis to these models is conceptually similar and only more technically
involved, and we note again how the class of MPNNs considered in (2) is, to date, the most general one studied
in relation to over-squashing. More generally, one could replace the aggregation P
u Avu in (2) with a smooth,
permutation invariant operator L [13, 48] and follow our approach to study the mixing abilities of L MPNNs.
12

Our formalism will then prove useful to assess if different aggregations are more expressive in terms of the
mixing (interactions) they are able to generate.
Graph rewiring. Another way of going beyond (2) to find MPNNs with lower OSQ is to replace A with a
different matrix A′, (partly) independent of the connectivity of the input graph. In other words, one could
rewire the graph, by replacing G with G′. Indeed, Theorem 4.4 further validates why recent graph-rewiring
methods such as [3, 36, 20, 8] manage to alleviate over-squashing: by adding edges that decrease the overall
effective resistance (commute time) of the graph, these methods reduce the measure OSQ. More generally,
Definition 4.2 allows one to measure whether a given rewiring is beneficial in terms of over-squashing (and
hence of the mixing generated) and to what extent. In fact, it follows from Theorem 4.4 that methods like
[20, 56] are optimal from the point of view of friendliness of the computational graph to message-passing and
sparsity, since they both propose to propagate information over expander graphs, which are both sparse and
have commute time scaling (approximately) linearly with the number of edges.
Acknowledgements
The authors would like to thank Dr. Martín Arjovsky, Dr. Charles Blundell and Dr. Karl Tuyls for their
insightful feedback and constructive suggestions on an earlier version of the manuscript.
References
[1] R. Aleliunas, R. M. Karp, R. J. Lipton, L. Lovász, and C. Rackoff. Random walks, universal traversal
sequences, and the complexity of maze problems. In 20th Annual Symposium on Foundations of Computer
Science (sfcs 1979), pages 218–223. IEEE Computer Society, 1979.
[2] U. Alon and E. Yahav. On the bottleneck of graph neural networks and its practical implications. In
International Conference on Learning Representations, 2021.
[3] A. Arnaiz-Rodríguez, A. Begga, F. Escolano, and N. Oliver. DiffWire: Inductive Graph Rewiring via the
Lovász Bound. In The First Learning on Graphs Conference, 2022. URL https://openreview.net/pdf?
id=IXvfIex0mX6f.
[4] W. Azizian and M. Lelarge. Expressive power of invariant and equivariant graph neural networks. In
International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=
lxHgXYN4bwl.
[5] V. Bapst, T. Keck, A. Grabska-Barwińska, C. Donner, E. D. Cubuk, S. S. Schoenholz, A. Obika, A. W.
Nelson, T. Back, D. Hassabis, et al. Unveiling the predictive power of static structure in glassy systems.
Nature Physics, 16(4):448–454, 2020.
[6] P. Barceló, E. V. Kostylev, M. Monet, J. Pérez, J. Reutter, and J. P. Silva. The logical expressiveness of
graph neural networks. In International Conference on Learning Representations, 2019.
[7] P. L. Bartlett, D. J. Foster, and M. J. Telgarsky. Spectrally-normalized margin bounds for neural networks.
Advances in neural information processing systems, 30, 2017.
[8] M. Black, A. Nayyeri, Z. Wan, and Y. Wang. Understanding oversquashing in gnns through the lens of
effective resistance. arXiv preprint arXiv:2302.06835, 2023.
[9] C. Blundell, L. Buesing, A. Davies, P. Veličković, and G. Williamson. Towards combinatorial invariance
for kazhdan-lusztig polynomials. arXiv preprint arXiv:2111.15161, 2021.
13

[10] J. Brandstetter, D. E. Worrall, and M. Welling. Message passing neural pde solvers. In International
Conference on Learning Representations, 2021.
[11] X. Bresson and T. Laurent. Residual gated graph convnets. arXiv preprint arXiv:1711.07553, 2017.
[12] S. Brody, U. Alon, and E. Yahav. How attentive are graph attention networks? In International Conference
on Learning Representations, 2022.
[13] M. M. Bronstein, J. Bruna, T. Cohen, and P. Veličković. Geometric deep learning: Grids, groups, graphs,
geodesics, and gauges. arXiv preprint arXiv:2104.13478, 2021.
[14] C. Cai and Y. Wang. A note on over-smoothing for graph neural networks. arXiv preprint arXiv:2006.13318,
2020.
[15] A. K. Chandra, P. Raghavan, W. L. Ruzzo, R. Smolensky, and P. Tiwari. The electrical resistance of a
graph captures its commute and cover times. computational complexity, 6(4):312–340, 1996.
[16] R. Chen, S. Zhang, Y. Li, et al. Redundancy-free message passing for graph neural networks. Advances in
Neural Information Processing Systems, 35:4316–4327, 2022.
[17] Z. Chen, L. Chen, S. Villar, and J. Bruna. Can graph neural networks count substructures? Advances in
neural information processing systems, 33:10383–10395, 2020.
[18] F. R. Chung and F. C. Graham. Spectral graph theory. American Mathematical Soc., 1997.
[19] A. Davies, P. Veličković, L. Buesing, S. Blackwell, D. Zheng, N. Tomašev, R. Tanburn, P. Battaglia,
C. Blundell, A. Juhász, et al. Advancing mathematics by guiding human intuition with ai. Nature, 600
(7887):70–74, 2021.
[20] A. Deac, M. Lackenby, and P. Veličković. Expander graph propagation. In The First Learning on Graphs
Conference, 2022.
[21] N. Dehmamy, A.-L. Barabási, and R. Yu. Understanding the representation power of graph neural networks
in learning graph topology. Advances in Neural Information Processing Systems, 32, 2019.
[22] A. Derrow-Pinion, J. She, D. Wong, O. Lange, T. Hester, L. Perez, M. Nunkesser, S. Lee, X. Guo,
B. Wiltshire, et al. Eta prediction with graph neural networks in google maps. In Proceedings of the 30th
ACM International Conference on Information & Knowledge Management, pages 3767–3776, 2021.
[23] G. DeZoort, P. W. Battaglia, C. Biscarat, and J.-R. Vlimant. Graph neural networks at the large hadron
collider. Nature Reviews Physics, pages 1–23, 2023.
[24] F. Di Giovanni, J. Rowbottom, B. P. Chamberlain, T. Markovich, and M. M. Bronstein. Graph neural
networks as gradient flows. arXiv preprint arXiv:2206.10991, 2022.
[25] F. Di Giovanni, L. Giusti, F. Barbero, G. Luise, P. Lio, and M. Bronstein. On over-squashing in message
passing neural networks: The impact of width, depth, and topology. In International Conference on
Machine Learning, 2023.
[26] V. P. Dwivedi, C. K. Joshi, T. Laurent, Y. Bengio, and X. Bresson. Benchmarking graph neural networks.
arXiv:2003.00982, 2020.
[27] L. Equer, T. K. Rusch, and S. Mishra. Multi-scale message passing neural pde solvers. arXiv preprint
arXiv:2302.03580, 2023.
[28] V. Garg, S. Jegelka, and T. Jaakkola. Generalization and representational limits of graph neural networks.
In International Conference on Machine Learning, pages 3419–3430. PMLR, 2020.
14

[29] F. Geerts and J. L. Reutter. Expressiveness and approximation properties of graph neural networks. In
International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=
wIzUeM3TAU.
[30] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum
chemistry. In International Conference on Machine Learning, pages 1263–1272. PMLR, 2017.
[31] M. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. In Proceedings. 2005
IEEE International Joint Conference on Neural Networks, 2005., volume 2, pages 729–734. IEEE, 2005.
[32] B. Gutteridge, X. Dong, M. M. Bronstein, and F. Di Giovanni. Drew: Dynamically rewired message
passing with delay. In International Conference on Machine Learning, pages 12252–12267. PMLR, 2023.
[33] W. L. Hamilton, R. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances
in Neural Information Processing Systems, pages 1025–1035, 2017.
[34] K. Hornik, M. Stinchcombe, and H. White. Universal approximation of an unknown mapping and its
derivatives using multilayer feedforward networks. Neural networks, 3(5):551–560, 1990.
[35] J. J. Irwin, T. Sterling, M. M. Mysinger, E. S. Bolstad, and R. G. Coleman. Zinc: a free tool to discover
chemistry for biology. Journal of chemical information and modeling, 52(7):1757–1768, 2012.
[36] K. Karhadkar, P. K. Banerjee, and G. Montufar. FoSR: First-order spectral rewiring for addressing
oversquashing in GNNs. In The Eleventh International Conference on Learning Representations, 2023.
URL https://openreview.net/forum?id=3YjQfCLdrzz.
[37] N. Keriven and G. Peyré. Universal invariant and equivariant graph neural networks. Advances in Neural
Information Processing Systems, 32, 2019.
[38] T. N. Kipf and M. Welling. Semi-Supervised Classification with Graph Convolutional Networks. In
International Conference on Learning Representations, 2017.
[39] D. Kreuzer, D. Beaini, W. Hamilton, V. Létourneau, and P. Tossou. Rethinking graph transformers with
spectral attention. In Advances in Neural Information Processing Systems, volume 34, pages 21618–21629,
2021.
[40] G. Liu, D. B. Catacutan, K. Rathod, K. Swanson, W. Jin, J. C. Mohammed, A. Chiappino-Pepe, S. A.
Syed, M. Fragis, K. Rachwalski, J. Magolan, M. G. Surette, B. K. Coombes, T. Jaakkola, R. Barzilay,
J. J. Collins, and J. M. Stokes. Deep learning-guided discovery of an antibiotic targeting acinetobacter
baumannii. Nature Chemical Biology, May 2023. ISSN 1552-4469. doi: 10.1038/s41589-023-01349-8. URL
https://doi.org/10.1038/s41589-023-01349-8.
[41] A. Loukas. What graph neural networks cannot learn: depth vs width. In International Conference on
Learning Representations, 2020. URL https://openreview.net/forum?id=B1l2bp4YwS.
[42] L. Lovász. Random walks on graphs. Combinatorics, Paul erdos is eighty, 2(1-46):4, 1993.
[43] H. Maron, E. Fetaya, N. Segol, and Y. Lipman. On the universality of invariant networks. In International
conference on machine learning, pages 4363–4371. PMLR, 2019.
[44] G. Mialon, D. Chen, M. Selosse, and J. Mairal. Graphit: Encoding graph structure in transformers. CoRR,
abs/2106.05667, 2021.
[45] A. Mirhoseini, A. Goldie, M. Yazgan, J. W. Jiang, E. Songhori, S. Wang, Y.-J. Lee, E. Johnson, O. Pathak,
A. Nazi, et al. A graph placement methodology for fast chip design. Nature, 594(7862):207–212, 2021.
[46] F. Monti, F. Frasca, D. Eynard, D. Mannion, and M. M. Bronstein. Fake news detection on social media
using geometric deep learning. arXiv preprint arXiv:1902.06673, 2019.
15

[47] C. Morris, M. Ritzert, M. Fey, W. L. Hamilton, J. E. Lenssen, G. Rattan, and M. Grohe. Weisfeiler and
leman go neural: Higher-order graph neural networks. In AAAI Conference on Artificial Intelligence,
pages 4602–4609. AAAI Press, 2019.
[48] E. Ong and P. Veličković. Learnable commutative monoids for graph neural networks. arXiv preprint
arXiv:2212.08541, 2022.
[49] L. Rampasek, M. Galkin, V. P. Dwivedi, A. T. Luu, G. Wolf, and D. Beaini. Recipe for a general, powerful,
scalable graph transformer. In Advances in Neural Information Processing Systems, 2022.
[50] N. Razin, T. Verbin, and N. Cohen. On the ability of graph neural networks to model interactions between
vertices. arXiv preprint arXiv:2211.16494, 2022.
[51] T. K. Rusch, B. Chamberlain, J. Rowbottom, S. Mishra, and M. Bronstein. Graph-coupled oscillator
networks. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of
Proceedings of Machine Learning Research, pages 18888–18909. PMLR, 2022.
[52] T. K. Rusch, M. M. Bronstein, and S. Mishra. A survey on oversmoothing in graph neural networks. arXiv
preprint arXiv:2303.10993, 2023.
[53] T. K. Rusch, B. P. Chamberlain, M. W. Mahoney, M. M. Bronstein, and S. Mishra. Gradient gating for
deep multi-rate learning on graphs. In International Conference on Learning Representations, 2023.
[54] R. Sato, M. Yamada, and H. Kashima. Approximation ratios of graph neural networks for combinatorial
problems. Advances in Neural Information Processing Systems, 32, 2019.
[55] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model.
IEEE transactions on neural networks, 20(1):61–80, 2008.
[56] H. Shirzad, A. Velingker, B. Venkatachalam, D. J. Sutherland, and A. K. Sinop. Exphormer: Sparse
transformers for graphs. arXiv preprint arXiv:2303.06147, 2023.
[57] J. M. Stokes, K. Yang, K. Swanson, W. Jin, A. Cubillos-Ruiz, N. M. Donghia, C. R. MacNair, S. French,
L. A. Carfrae, Z. Bloom-Ackermann, et al. A deep learning approach to antibiotic discovery. Cell, 180(4):
688–702, 2020.
[58] J. Topping, F. Di Giovanni, B. P. Chamberlain, X. Dong, and M. M. Bronstein. Understanding over-
squashing and bottlenecks on graphs via curvature. In International Conference on Learning Representa-
tions, 2022.
[59] P. Veličković. Everything is connected: Graph neural networks. Current Opinion in Structural Biology, 79:
102538, 2023.
[60] P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y. Bengio. Graph attention networks. In
International Conference on Learning Representations, 2018.
[61] B. Weisfeiler and A. Leman. The reduction of a graph to canonical form and the algebra which appears
therein. nti, Series, 2(9):12–16, 1968.
[62] G. Williamson. Is deep learning a useful tool for the pure mathematician? arXiv preprint arXiv:2304.12602,
2023.
[63] K. Xu, C. Li, Y. Tian, T. Sonobe, K.-i. Kawarabayashi, and S. Jegelka. Representation learning on graphs
with jumping knowledge networks. In International Conference on Machine Learning, pages 5453–5462.
PMLR, 2018.
[64] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In International
Conference on Learning Representations, 2019.
16

[65] C. Ying, T. Cai, S. Luo, S. Zheng, G. Ke, D. He, Y. Shen, and T.-Y. Liu. Do transformers really perform
badly for graph representation? In Advances in Neural Information Processing Systems, volume 34, pages
28877–28888, 2021.
[66] B. Zhang, S. Luo, L. Wang, and D. He. Rethinking the expressive power of gnns via graph biconnectivity.
In The Eleventh International Conference on Learning Representations, 2023.
17

A
Outline of the appendix
We provide an overview of the appendix. Since in the appendix we report additional theoretical results and
considerations, we first point out to the most relevant content: the proofs of the main results, the extension of
our discussion and analysis to node-level tasks, and the additional ablation studies.
Where to find proofs of the main results. We prove Theorem 3.2 in Section C.1, we prove Theorem 4.3 in
Section D.1, and finally we prove Theorem 4.4 in Section D.3.
Where to find the extension to node-level tasks.Concerning the case of node-level tasks, we present a
thorough discussion on the matter in Section E, where we extend the definition of the over-squashing measure
and generalize Theorem 3.2 and Theorem 4.4 to node-level predictions of the MPNN class in (2).
Where to find additional ablation studies. In Section F we have conducted further experiments on the
profile of the over-squashing measure ]
OSQ across different MPNN models as well as on the training mean
average error, to further validate our claims on over-squashing hindering the expressive power of MPNNs.
Next, we summarize the contents of the Appendix more in detail below.
• In order to be self-consistent, in Section B we review important notions pertaining to the spectrum of
the graph-Laplacian and known properties of random walks on graphs, that will be then be used in our
proofs.
• In Section C we prove the main theorem on the maximal mixing induced by MPNNs (Theorem 3.2). In
particular, we also derive additional results on the mixing generated at a specific node, which will turn
out useful when extending the characterization of the over-squashing measure ]
OSQ for node-level tasks.
• In Section D we prove the main results of Section 4, mainly Theorem 4.3 and Theorem 4.4. Further, we
also derive an explicit (sharper) characterization of the depth required to induce enough mixing among
nodes, in terms of the pseudo-inverse of the graph-Laplacian. Finally, in Section D.4 we extend the results
to the case of the unnormalized adjacency matrix and discuss relative over-squashing measures.
• In Section E we generalize the over-squashing measure for node-level tasks, commenting on the differences
between our approach and existing works (mainly [58, 8, 25]). In particular, we show that the same
conclusions of Theorem 4.4 hold for node-level predictions too.
• Finally, in Section F we report additional details on our experimental setup and further ablation studies
concerning the over-squashing measure ]
OSQ.
B
Summary of spectral properties on graphs
Basic notions of spectral theory on graphs.
Throughout the appendix, we let ∆be the normalized graph Laplacian defined by ∆= I −D−1/2AD−1/2.
It is known [18] that the graph Laplacian is a symmetrically, positive semi-definite matrix whose spectral
decomposition takes the form
∆=
n−1
X
ℓ=0
λℓϕℓϕ⊤
ℓ,
(9)
where {ϕℓ} is an orthonormal basis in Rn and 0 = λ0 < λ1 < . . . < λn−1 – recall that since we assume G to be
connected, the zero eigenvalue has multiplicity one, i.e. λ1 > 0. We also note that we typically write ϕℓ(v)
for the value of ϕℓat v ∈V, and that the kernel of ∆is spanned by ϕ0 with ϕ0(v) =
p
dv/2|E|. the results
would extend to the bipartite cas As usual when doing spectral analysis one too if graphs, we exclude the edge
18

case of the bipartite graph to make sure that the largest eigenvalue of the graph Laplacian satisfies λn−1 < 2
– yet all results hold for the bipartite case too provided we take ∥∇2ψ∥< 1. Finally, we let ∆† denote the
pseudo-inverse of the graph Laplacian, which can be written as
∆† =
n−1
X
ℓ=1
1
λℓ
ϕℓϕ⊤
ℓ,
(10)
and we emphasize that the sum starts from ℓ= 1 since we need to ignore the kernel of ∆spanned by the
orthonormal vector ϕ0.
Basic properties of Random Walks on graphs. A simple Random Walk (RW) on G is a Markov chain
supported on the nodes V with transition matrix defined by P(v, u) = d−1
v . While a RW can be studied through
different properties, the one we are interested in is the commute time τ, which represents the expected number
of steps for a RW starting at v, to visit u and then come back to v. The commute time is a distance on the
graph and captures the diffusion properties associated with the underlying topology. In fact, while nodes that
are distant often have larger commute time, the latter is more expressive than the shortest-walk graph-distance,
since it also accounts (for example) for the number of paths connecting two given nodes. Thanks to [42], we can
write down the commute time among two nodes using the spectral representation of the graph Laplacian in (9):
τ(v, u) = 2|E|
n−1
X
ℓ=1
1
λℓ
ϕℓ(v)
√dv
−ϕℓ(u)
√du
2
.
(11)
C
Proofs and additional details of Section 3
The goal of this section amounts to proving Theorem 3.2. To work towards this result, we first derive bounds
on the Jacobian and Hessian of a single node feature after m layers before the readout READ operation. We
emphasize that our analysis below is novel, when compared to previous works of [8, 25], on many accounts.
First, [8, 25] do not consider higher (second) order derivatives, limiting their discussion to the case of first order
derivatives, which are not suited to capture notions of mixing among features – we will expand on this topic in
Section E. Second, even for the case of first-order derivatives, our result below is more general since it holds
for all MPNNs as in (2), which includes (i) message-functions ψ that also depend on the input features (as
for GatedGCN), and (ii) choices of message-passing matrices A that could be weighted and (or) asymmetric.
Third, the analysis in [8, 25] does not account for the role of the readout map and hence fails to study the
expressive power of graph-level prediction of MPNNs as measured by the mixing they generate among nodes.
Conventions and notations for the proofs. First, we recall that h(0)
v
= xv ∈Rd is the input feature at
node v. Below, we write h(t),α
v
for the α-th entry of the feature h(t)
v . To simplify the notations, we rewrite the
layer-update in (2) using coordinates as
h(t),α
v
= σ(˜h(t−1),α
v
),
1 ≤α ≤d,
(12)
where ˜h(t−1),α
v
is the entry α of the pre-activated feature of node v at layer t. We also let ∂1,pψ(t),r and ∂2,pψ(t),r
be the p-th derivative of (ψ(t)(·, x))r and of (ψ(t)(x, ·))r, respectively. To avoid cumbersome notations, we
usually omit to write the arguments of the derivatives of the message-functions ψ. Similarly, we let ∇1ψ (∇2ψ)
be the d × d Jacobian matrix of ψ with respect to the first (second) variable. Finally, given nodes i, v, u ∈V we
introduce the following terms:
∇uh(m)
v
:= ∂h(m)
v
∂xu
∈Rd×d,
∇2
uvh(m)
i
:= ∂2h(m)
i
∂xu∂xv
∈Rd×(d×d).
First, we derive an upper bound on the first-order derivatives of the node-features. This will provide useful to
derive the more general second-order estimate of the MPNN-prediction. We highlight that the result below
19

extends the analysis in [25] to MPNNs with arbitrary (i.e. non-linear) message functions ψ, such as GatedGCN
[11].
Theorem C.1. Given MPNNs as in (2), let σ and ψ(t) be C1 functions and assume |σ′| ≤cσ, ∥Ω(t)∥≤ω,
∥W(t)∥≤w, ∥∇1ψ(t)∥≤c1, and ∥∇2ψ(t)∥≤c2. Let S ∈Rn×n be
S := ω
wI + c1diag(A1) + c2A.
Given nodes v, u ∈V and m the number of layers, the following holds:
∥∇uh(m)
v
∥≤(cσw)m(Sm)vu.
(13)
Proof. Recall that the dimension of the features is taken to be d for any layer 1 ≤t ≤m. We proceed by
induction. If m = 1 and we fix entries 1 ≤α, β ≤d, then using the shorthand in (12), we obtain
(∇uh(1)
v )αβ = σ′(˜h(0),α
v
)

Ω(1)
αβδvu +
X
r
W (1)
αr
X
j
Avj

∂1,βψ(1),rδvu + ∂2,βψ(1),rδju

=

diag(σ′(˜h(0)
v ))

Ω(1)δvu + W(1) X
j
Avjδvu∇1ψ(1) + Avu∇2ψ(1)
αβ.
Therefore, we can bound the (spectral) norm of the Jacobian on the left hand side by
∥∇uh(1)
v ∥≤∥diag(σ′(˜h(0)
v ))∥

∥Ω(1)∥δvu + ∥W(1)∥(c1
X
j
Avjδvu + c2Avu)

≤cσ(ωδvu + w(c1
X
j
Avjδvu + c2Avu)) = cσwSvu,
which proves the estimate on the Jacobian for the case of m = 1. We now take the induction step, and follow
the same argument above to write the node Jacobian after m layers as
(∇uh(m)
v
)αβ = σ′(˜h(m−1),α
v
)
 X
r
Ω(m)
αr (∇uh(m−1)
v
)rβ

+ σ′(˜h(m−1),α
v
)

W (m)
αr
X
j
Avj
X
p

∂1,pψ(m),r(∇uh(m−1)
v
)pβ + ∂2,pψ(m),r(∇uh(m−1)
j
)pβ

=

diag(σ′(˜h(m−1)
v
))Ω(m)∇uh(m−1)
v

αβ
+

diag(σ′(˜h(m−1)
v
))W(m) X
j
Avj∇1ψ(m)∇uh(m−1)
v
+ Avj∇2ψ(m)∇uh(m−1)
j
)

αβ.
Therefore, we can use the induction step to bound the Jacobian as
∥∇uh(m)
v
∥≤cσω(cσw)m−1(Sm−1)vu + (cσw)m X
j
Avjc1(Sm−1)vu + Avjc2(Sm−1)ju

= (cσw)mω
wI + c1diag(A1) + c2A

(Sm−1)

vu = (cσw)m(Sm)vu,
which completes the proof for the first-order bounds.
Before we move to the second-order estimates, we introduce some additional preliminary notations. Given
nodes i, v, u, a matrix S ∈Rn×n – which will always be chosen as per (5) – and an integer ℓ, we write
P(ℓ)
i(vu) := (Sℓ)iv(ASℓ)iu + (Sℓ)iu(ASℓ)iv +
X
j
(Sℓ)jv (diag(A1) + A)ij (Sℓ)ju.
(14)
In particular, we denote by P(ℓ)
(vu) ∈Rn the vector with entries (P(ℓ)
(vu))i = P(ℓ)
i(vu), for 1 ≤i ≤n.
20

Theorem C.2. Given MPNNs as in (2), let σ and ψ(t) be C2 functions and assume |σ′|, |σ′′| ≤cσ, ∥Ω(t)∥≤ω,
∥W(t)∥≤w, ∥∇1ψ(t)∥≤c1, ∥∇2ψ(t)∥≤c2, ∥∇2ψ(t)∥≤c(2). Let S ∈Rn×n be
S := ω
wI + c1diag(A1) + c2A.
Given nodes i, v, u ∈V, if P(ℓ)
(vu) ∈Rn is as in (14) and m is the number of layers, then we derive
∥∇2
uvh(m)
i
∥≤
m−1
X
k=0
X
j∈V
(cσw)2m−k−1 w(Sm−k)jv(Sk)ij(Sm−k)ju
+ c(2)
m−1
X
ℓ=0
(cσw)m+ℓ(Sm−1−ℓP(ℓ)
(vu))i.
(15)
Proof. First, we note that ∇2
uvh(m)
i
is a matrix of dimension Rd×(d×d). We then use the following ordering for
indexing the columns – which is consistent with a typical way of labelling columns of the Kronecker product of
matrices, as detailed below (note that indices here start from 1):
∂2h(m),α
i
∂xβ
v∂xγ
u
:=

∇2
uvh(m)
i

α,d(β−1)+γ.
(16)
As above, we proceed by induction and start from the case m = 1:

∇2
uvh(1)
i

α,d(β−1)+γ = σ′′(˜h(0),α
i
)

Ω(1)
αγδiv +
X
r
W (1)
αr
X
j
Aij(δiv∂1,γψ(1),r + δjv∂2,γψ(1),r)

×

Ω(1)
αβδiu +
X
r
W (1)
αr
X
j
Aij(δiu∂1,βψ(1),r + δju∂2,βψ(1),r)

+ σ′(˜h(0),α
i
)
X
r
W (1)
αr
 X
j
Aijδiu(∂1,γ∂1,βψ(1),rδiv + ∂2,γ∂1,βψ(1),rδjv)

+ σ′(˜h(0),α
i
)
X
r
W (1)
αr

Aiu(∂1,γ∂2,βψ(1),rδiv + ∂2,γ∂2,βψ(1),rδuv)

:= (Q1)α,β,γ + (Q2)α,β,γ + (Q3)α,β,γ,
where Q1 is the term containing second derivatives of ψ while Q2, Q3 are the remaining expressions including
second-order derivatives of the message functions ψ. Using the same strategy as for the first-order estimates,
we can rewrite the first term Q1 as
(Q1)α,β,γ =

diag(σ′′(˜h(0)
v ))

Ω(1)δiv + W(1)(
X
j
Aijδiv∇1ψ(1) + Aiv∇2ψ(1))

αγ
×

Ω(1)δiu + W(1)(
X
j
Aijδiu∇1ψ(1) + Aiu∇2ψ(1))

αβ
We now observe that given two matrices B, C ∈Rd×d and 1 ≤α, α′, β, γ ≤d, the entries of the Kronecker
product B ⊗C can be indexed as
(B ⊗C)d(α−1)+α′,d(β−1)+γ = BαβCα′γ.
We now introduce the d × (d × d) sub-matrix of B ⊗C defined by
(B ⊗C)′
α,d(β−1)+γ = BαβCαγ.
(17)
Therefore, we can rewrite (Q1)α,β,γ as the entry (α, d(β −1) + γ) of the d × (d × d) sub-matrix
(Q1)α,d(β−1)+γ = (B ⊗C)′
α,d(β−1)+γ,
(18)
21

where
B := diag(σ′′(˜h(0)
v ))

Ω(1)δiv + W(1)(
X
j
Aijδiv∇1ψ(1) + Aiv∇2ψ(1))

,
C := Ω(1)δiu + W(1)(
X
j
Aijδiu∇1ψ(1) + Aiu∇2ψ(1)).
Next, we proceed to write (Q2)α,β,γ in matricial form. Before we do that, we observe that the Hessian of the
message functions (xi, xj) 7→ψ(t)(xi, xj) takes the form
∇2ψ(t) =
∇2
11ψ(t)
∇2
12ψ(t)
∇2
21ψ(t)
∇2
22ψ(t)

,
where ∇2
abψ(t) ∈Rd×(d×d) and is indexed as follows
(∇2
abψ(t))r,d(β−1)+γ = ∂a,β∂b,γψ(t),r,
where a, b ∈{1, 2}. Using these notations, we note that
X
r
W (1)
αr ∂1,γ∂1,βψ(1),r =

W(1)∇2
11ψ(1)
α,d(β−1)+γ .
Therefore, we derive
(Q2)α,β,γ = (Q2)α,d(β−1)+γ =
X
j
Aijδiuδiv(diag(σ′(˜h(0)
i ))W(1)∇2
11ψ(1))α,d(β−1)+γ
+ Aivδiu(diag(σ′(˜h(0)
i ))W(1)∇2
12ψ(1))α,d(β−1)+γ.
(19)
A similar argument works for Q3:
(Q3)α,β,γ = (Q3)α,d(β−1)+γ = Aiuδiv(diag(σ′(˜h(0)
i ))W(1)∇2
21ψ(1))α,d(β−1)+γ
+ Aiuδuv(diag(σ′(˜h(0)
i ))W(1)∇2
22ψ(1))α,d(β−1)+γ.
(20)
Therefore, we can combine (18), (19), and (20) to write
∥∇2
uvh(1)
i ∥≤∥Q1∥+ ∥Q2∥+ ∥Q3∥
≤cσ (ωδiv + w(c1diag(A1)iδiv + c2Aiv)) (ωδiu + w(c1diag(A1)iδiu + c2Aiu)
+ cσwc(2) (diag(A1)iδivδiu + Aivδiu)
+ cσwc(2) (Aiuδiv + Aiuδuv) .
Finally, we can rely on (14) to re-arrange the equation above as
∥∇2
uvh(1)
i ∥≤(cσw)(w(S)iv(S)iu) + wc(2)cσ(δivAiu + δiuAiv +
X
j
δjv (diag(A1) + A)ij δju)
= (cσw)(w(S)iv(S)iu) + c(2)cσwP(0)
i(vu),
which proves the bound for the second-order derivatives in the case m = 1.
We now assume that the claim holds for all layers t ≤m −1, and compute the second order derivative after m
22

layers:

∇2
uvh(m)
i

α,d(β−1)+γ = σ′′(˜h(m−1),α
i
)
×
 X
r
Ω(m)
αr (∇uh(m−1)
i
)rβ
+ W (m)
αr
X
j
Aij(
X
p
∂1,pψ(m),r(∇uh(m−1)
i
)pβ + ∂2,pψ(m),r(∇uh(m−1)
j
)pβ)

×
 X
r
Ω(m)
αr (∇vh(m−1)
i
)rγ
+ W (m)
αr
X
j
Aij(
X
p
∂1,pψ(m),r(∇vh(m−1)
i
)pγ + ∂2,pψ(m),r(∇vh(m−1)
j
)pγ)

+ σ′(˜h(m−1),α
i
)
X
r
W (m)
αr
X
j
Aij
X
p,q
∂1,p∂1,qψ(m),r(∇uh(m−1)
i
)pβ(∇vh(m−1)
i
)qγ
+ σ′(˜h(m−1),α
i
)
X
r
W (m)
αr
X
j
Aij
X
p,q
∂1,p∂2,qψ(m),r(∇uh(m−1)
i
)pβ(∇vh(m−1)
j
)qγ
+ σ′(˜h(m−1),α
i
)
X
r
W (m)
αr
X
j
Aij
X
p,q
∂1,p∂2,qψ(m),r(∇uh(m−1)
j
)qβ(∇vh(m−1)
i
)pγ
+ σ′(˜h(m−1),α
i
)
X
r
W (m)
αr
X
j
Aij
X
p,q
∂2,p∂2,qψ(m),r(∇uh(m−1)
j
)pβ(∇vh(m−1)
j
)qγ
+ σ′(˜h(m−1),α
i
)
X
r
Ω(m)
αr (∇2
uvh(m−1)
i
)r,d(β−1)+γ
+ σ′(˜h(m−1),α
i
)
X
r
W (m)
αr
X
j
Aij(
X
p
∂1,pψ(m),r(∇2
uvh(m−1)
i
)p,d(β−1)+γ
+ σ′(˜h(m−1),α
i
)
X
r
W (m)
αr
X
j
Aij(
X
p
∂2,pψ(m),r(∇2
uvh(m−1)
j
)p,d(β−1)+γ
:= Rα,β,γ +
X
a,b∈{1,2}
(Qab)α,β,γ + Zα,β,γ,
where R is the term containing second derivatives of the non-linear map σ, Qab is indexed according to the
second derivatives of the message-functions ψ, and finally Z is the term containing second-order derivatives of
the features. For the term Rα,β,γ we can argue as in the m = 1 case and use the sub-matrix notation in (17) to
rewrite it as the entry (α, d(β −1) + γ) of the d × (d × d) sub-matrix
Rα,d(β−1)+γ = (B ⊗C)′
α,d(β−1)+γ,
(21)
where
B := diag(σ′′(˜h(m−1)
i
))(Ω(m)∇uh(m−1)
i
+ W(m)(
X
j
Aij∇1ψ(m)∇uh(m−1)
i
+ ∇2ψ(m)∇uh(m−1)
j
)),
C := Ω(m)∇vh(m−1)
i
+ W(m)(
X
j
Aij∇1ψ(m)∇vh(m−1)
i
+ ∇2ψ(m)∇vh(m−1)
j
)
Next we consider the terms (Qab)α,β,γ. Without loss of generality, we focus on (Q11)α,β,γ and use again the
same argument in the m = 1 case, to rewrite it as (Q11)α,β,γ = (Q11)α,d(β−1)+γ where
Q11 = diag(σ′(˜h(m−1)
i
))
X
j
Aij(W(m)∇2
11ψ(m)∇uh(m−1)
i
⊗∇vh(m−1)
i
),
(22)
where again we are indexing the matrix ∇2
11ψ(m) by
(∇2
11ψ(m))r,p(d−1)+q = ∂1,p∂1,qψ(m),r.
23

The other Q-terms can be estimated similarly. Finally, we rewrite Zα,β,γ = (Z)α,d(β−1)+γ, where
Z = diag(σ′(˜h(m−1)
i
))

Ω(m)∇2
uvh(m−1)
i
+ W(m) X
j
Aij(∇1ψ(m)∇2
uvh(m−1)
i
+ ∇2ψ(m)∇2
uvh(m−1)
j
)

(23)
Therefore, we have rewritten the second-derivatives of the features in matricial form as
∇2
uvh(m)
i
= R +
X
a,b∈{1,2}
Qab + Z.
To complete the proof, we now simply need to estimate the three terms and show they fit the recursion claimed
for m. For the case of R in (21), we find
∥R∥≤cσ(ω∥∇uh(m−1)
i
∥+ w(c1diag(A1)i∥∇uh(m−1)
i
∥+ c2
X
j
Aij∥∇uh(m−1)
j
∥))
× (ω∥∇vh(m−1)
i
∥+ w(c1diag(A1)i∥∇vh(m−1)
i
∥+ c2
X
j
Aij∥∇vh(m−1)
j
∥).
If we write Dh(m−1) ∈Rn×n as the matrix with entries (Dh(m−1))ij = ∥∇jh(m−1)
i
∥, then we obtain
∥R∥≤cσw(wSDh(m−1))iv(SDh(m−1))iu.
We can then plug the first-order estimates derived in Theorem C.1 and obtain
∥R∥≤cσw(wS(cσw)m−1Sm−1)iv(S(cσw)m−1Sm−1)iu = (cσw)2m−1(w(Sm)iv(Sm)iu).
(24)
Next, we move onto the Q-terms, and use again the first-order estimates in Theorem C.1 – and the fact that
we can bound the norm of ∇2
abψ(m) by c(2) – to derive
∥
X
a,b∈{1,2}
Qab∥≤c(2)(cσw)2m−1(diag(A1)i(Sm−1)iv(Sm−1)iu +
X
j
Aij(Sm−1)ju(Sm−1)jv)
+ c(2)(cσw)2m−1((Sm−1)iv(ASm−1)iu + (Sm−1)iu(ASm−1)iv)
= c(2)(cσw)2m−1P(m−1)
i(vu) .
(25)
Finally, if we let D2hvu ∈Rn be the vector with entries (D2hvu)i = ∥∇2
uvh(m−1)
i
∥, then
∥Z∥≤cσ

ω∥∇2
uvh(m−1)
i
∥+ w

c1diag(A1)i∥∇2
uvh(m−1)
i
∥+ c2
X
j
Aij∥∇2
uvh(m−1)
j
∥

(26)
= cσw(SD2hvu)i.
24

Therefore, we can use the induction to derive
∥Z∥≤cσw
X
s
Sis
m−2
X
k=0
X
j∈V
(cσw)2m−2−k−1 w(Sm−1−k)jv(Sk)sj(Sm−1−k)ju
+ cσw
X
s
Sis(c(2)
m−2
X
ℓ=0
(cσw)m−1+ℓ(Sm−2−ℓP(ℓ)
(vu))s)
=
m−2
X
k=0
X
j∈V
(cσw)2m−k−2 w(Sm−1−k)jv(Sk+1)ij(Sm−1−k)ju
+ c(2)(
m−2
X
ℓ=0
(cσw)m+ℓ(Sm−1−ℓP(ℓ)
(vu))i)
=
m−1
X
k=1
X
j∈V
(cσw)2m−k−1 w(Sm−k)jv(Sk)ij(Sm−k)ju + c(2)(
m−2
X
ℓ=0
(cσw)m+ℓ(Sm−1−ℓP(ℓ)
(vu))i)
By (24), we derive that the R-term corresponds to the k = 0 entry of the first sum, while (25) corresponds to
the case ℓ= m −1 of the second sum, which completes the induction and hence our proof.
C.1
Proof of Theorem 3.2
We can now use the previous characterization to derive estimates on the Hessian of the graph-level function
computed by MPNNs. We restate Theorem 3.2 here for convenience.
Theorem 3.2. Consider an MPNN of depth m as in (2), where σ and ψ(t) are C2 functions and we denote the
bounds on their derivatives and on the norm of the weights as above. Let S and Qk be defined as in (5) and (6),
respectively. If the readout is MAX, MEAN or SUM and θ in (3) has unit norm, then the mixing mixy(m)
G
(v, u)
induced by the MPNN over the features of nodes v, u satisfies
mixy(m)
G
(v, u) ≤
m−1
X
k=0
(cσw)2m−k−1 
w(Sm−k)⊤diag(1⊤Sk)Sm−k + c(2)Qk

vu .
(7)
Proof. First, we recall that according to Definition 3.1, we are interested in bounding the quantity
mixy(m)
G
(v, u) = max
X
max
1≤β,γ≤d

∂2y(m)
G
(X)
∂xβ
u∂xγ
v
 .
Let us first consider the choice READ = SUM, so that by (3) we get
mixy(m)
G
(v, u) ≤

d
X
α=1
θα
X
i∈V
∂2h(m),α
i
∂xβ
u∂xγ
v

As before, we index the columns of the Hessian of hi as ∂2h(m),α
i
∂xβ
u∂xγ
v = (∇2
uvh(m)
i
)α,d(β−1)+γ and hence obtain
mixy(m)
G
(v, u) ≤
X
i∈V
∥(∇2
uvh(m)
i
)⊤θ∥≤
X
i∈V
∥∇2
uvh(m)
i
∥,
(27)
25

since θ has unit norm. Note that the very same bound in (27) also holds if we replaced the SUM readout with
either the MAX or the MEAN readout. We can then rely on Theorem C.2 and find
mixy(m)
G
(v, u) ≤
X
i∈V
m−1
X
k=0
X
j∈V
(cσw)2m−k−1 w(Sm−k)jv(Sk)ij(Sm−k)ju
+ c(2) X
i∈V
m−1
X
ℓ=0
(cσw)m+ℓ(Sm−1−ℓP(ℓ)
(vu))i
=
m−1
X
k=0
(cσw)2m−k−1 
w(Sm−k)⊤diag(1⊤Sk)Sm−k
vu
+ c(2)
m−1
X
ℓ=0
(cσw)m+ℓ(1⊤Sm−1−ℓ)P(ℓ)
(vu)
=
m−1
X
k=0
(cσw)2m−k−1 
w(Sm−k)⊤diag(1⊤Sk)Sm−k
vu
+ c(2)
m−1
X
k=0
(cσw)2m−k−1(1⊤Sk)P(m−k−1)
(vu)
.
For the second term we can the simply use the formula in (14) to rewrite it in matricial form as claimed – recall
that Qk is defined in (6).
D
Proofs and additional details of Section 4
Throughout this Section, for simplicity, we assume cσ = max{|c′
σ|, |c′′
σ|} to be smaller or equal than one – this
is satisfied by the vast majority of commonly used non-linear activations, and extending the results below to
arbitrary cσ is straightforward.
D.1
Proof of Theorem 4.3
We begin by proving lower bounds on the spectral norm of the weights, when the depth is the minimal one
required to induce any non-zero mixing among nodes v, u. For convenience, we restate Theorem 4.3 here as
well.
Theorem 4.3. Let A = Asym, r := dG(v, u), m = ⌈r/2⌉, and q be the number of paths of length r between v and
u. For an MPNN satisfying Theorem 3.2 with capacity (m = ⌈r/2⌉, w), we find g
OSQv,u(m, w)·(c2w)r(Ar)vu ≥1.
In particular, if the MPNN generates mixing mixyG(v, u), then
w ≥dmin
c2
mixyG(v, u)
q
 1
r
.
Proof. Without loss of generality, we assume that r is even, so that by our assumptions, we can simply take
m = r/2. According to Theorem 3.2, we know that the maximal mixing induced by an MPNN of depth m over
the features associated with nodes v, u is bounded from above as
mixy(m)
G
(v, u) ≤
m−1
X
k=0
w2m−k−1
wSm−kdiag(Sk1)Sm−k + c(2)Qk

vu.
26

where we have replaced cσ with one, as per our assumption. Since m = r/2, where r is the distance among
nodes v, u, then the only non-zero contribution for the first term is obtained for k = 0 – otherwise we would find
a path of length 2(m −k) connecting v and u hence violating the assumptions – and is equal to w2m(S2m)vu,
and note that 2m = r. Concerning the terms Qk instead, the longest-walk contribution for nodes v, u is 2m −1
(when k = 0), meaning that Qk = 0 for all 0 ≤k ≤m −1 if m = 2r. Accordingly, we can reduce the bound
above to:
mixy(m)
G
(v, u) ≤w2m
S2m
vu = w2m
(c2A)2m
vu,
where in the last equality we have again used that 2m = r, so when expanding the power of S only the
highest-order term in the A-variable gives non-zero contributions. If we replace now 2m = r and use the
characterization of over-squashing in Definition 4.2, then
]
OSQv,u(m = r
2, w) ≥(c2w)−r
1
(Ar)vu
.
Therefore, if the MPNN generates mixing mixy(m)
G
(v, u) among the features of v and u, then (8) is satisfied,
meaning that the spectral norm of the weights must be larger than
w ≥1
c2
mixy(m)
G
(v, u)
(Ar)vu
 1
r .
The term Ar in general can be estimated sharply depending on the knowledge we have of the underlying graph.
To get a universal – albeit potentially looser bound – it suffices to note that along each path connecting v and
u, the product of the entries of A can be bounded from above by (dmin)r, which completes the proof.
We highlight that if A = Arw = D−1A – i.e. the aggregation over the neighbours consists of a mean-operation
as for the GraphSAGE architecture – then one can apply the very same proof above and derive
Corollary D.1. The same lower bound for w in Theorem 4.3, holds when A = D−1A.
D.2
Spectral bounds
Next, we study the case of fixed, bounded spectral norm of the weights, but variable depth, since we are
interested in showing that over-squashing hinders the expressive power of MPNNs for tasks requiring high-mixing
of features associated with nodes at high commute time. We first provide a characterization of the maximal
mixing (and hence of the over-squashing measure) in terms of the graph-Laplacian and its pseudo-inverse.
Convention. In the proofs below we usually deal with matrices with nonnegative entries. Accordingly, we
introduce the following convention: we write that A ≤B if Aij ≤Bij for all entries 1 ≤i, j ≤n.
Theorem D.2. Let γ :=
q
dmax
dmin and set A = Asym or A = Arw. Consider an MPNN as in Thm. 3.2 with
depth m, max{w, ω/w + c1γ + c2} ≤1. Define Z := I −c2∆. Then the maximal mixing of nodes v, u generated
by such MPNN after m layers is
mixy(m)
G
(v, u) ≤γk
m
√dvdu
2|E|

1 + 2c(2)(1 + γs)

+ 1
c2

Z2(I −Z2m)(I + Z)−1∆†
vu

+ 2c(2)
c2
γk
(1 + γs)I −∆

(I −Z2m)(I + Z)−1∆†
vu,
where k = s = 1 if A = Asym or k = 4, s = 2 if A = Arw.
Proof. We first focus on the symmetrically normalized case A = Asym = D−1/2AD−1/2, which we recall that
we can rewrite as Asym = I −∆, where ∆is the (normalized) graph Laplacian (9). Since w ≤1 and the
27

message-passing matrix is symmetric, by Theorem 3.2, we can bound the maximal mixing of an MPNN as in
the statement by
mixy(m)
G
(v, u) ≤
m−1
X
k=0

Sm−kdiag(Sk1)Sm−k + c(2)Qk

vu.
We focus on the first sum. Since A = D1/2(D−1A)D−1/2, where D−1A is a row-stochastic matrix, we see that
Sij ≤ω
wδij + c1γδij + c2Aij,
meaning that we can write S ≤αI + c2A, where α = ω/w + c1γ, using the convention introduced above.
Accordingly, we can estimate the row-sum of the powers of S by using (Ap1)i ≤γ as
(Sk1)i ≤
k
X
p=0
k
p

αk−pcp
2(Ap1)i ≤γ
k
X
p=0
k
p

αk−pcp
2 = γ(α + c2)k ≤γ,
where the last inequality simply follows from the assumptions. Therefore, we find
 m−1
X
k=0
Sm−kdiag(Sk1)Sm−k
vu ≤γ
m−1
X
k=0
(S2(m−k))vu = γ
m
X
k=1
(S2k)vu.
(28)
By the assumptions on the regularity of the message-functions, we can estimate S from above by S ≤αI+c2A =
(α + c2)I −c2∆≤Z, and derive
 m−1
X
k=0
Sm−kdiag(Sk1)Sm−k
vu ≤γ
m
X
k=1
(Z2k)vu.
From the spectral decomposition of the graph-Laplacian in (9) and the properties that λ0 = 0 and ϕ0(v) =
p
dv/2|E|, we find
m
X
k=1
(Z2k)vu =
m
X
k=1
n−1
X
ℓ=0
(1 −c2λℓ)2kϕℓ(v)ϕℓ(u)
= m
√dvdu
2|E|
+
n−1
X
ℓ=1
1 −(1 −c2λℓ)2(m+1)
1 −(1 −c2λℓ)2
−1

ϕℓ(v)ϕℓ(u)
= m
√dvdu
2|E|
+
n−1
X
ℓ=1
(1 −c2λℓ)2(1 −(1 −c2λℓ)2m)
(2 −c2λℓ)c2λℓ

ϕℓ(v)ϕℓ(u).
Since c2 ≤1 and G is not bipartite, we derive that (I + Z) = 2I −c2∆is invertible and hence that the following
decomposition holds:
(I + Z)−1 =
X
ℓ≥0
1
2 −c2λℓ
ϕℓϕ⊤
ℓ.
Therefore, we can rely on the spectral-decomposition of the pseudo-inverse of the graph-Laplacian in (10) to get
 m−1
X
k=0
Sm−kdiag(Sk1)Sm−k
vu ≤γ

m
√dvdu
2|E|
+ 1
c2

Z2(I −Z2m)(I + Z)−1∆†
vu

.
(29)
It now remains to bound the term c(2) Pm−1
k=0 (Qk)vu. First, we note that by the symmetry of A and the estimate
(Sk1)i ≤γ, that we derived above, we obtain
c(2)
m−1
X
k=0
(Qk)vu ≤2c(2)γ
 m−1
X
k=0
(AZ2(m−k−1))vu + γ(Z2(m−k−1))vu

.
28

Then we can use the identity A = I −∆, to find
c(2)
m−1
X
k=0
(Qk)vu ≤2c(2)γ
m−1
X
k=0

((1 + γ)I −∆)Z2(m−k−1)
vu.
(30)
By relying on the spectral decomposition as above, we finally get
c(2)
m−1
X
k=0
(Qk)vu ≤2c(2)γ

m
√dvdu
2|E| (1 + γ)

+ 2c(2)γ
 X
ℓ>0
(1 + γ −λℓ)1 −(1 −c2λℓ)2m
(2 −c2λℓ)c2λℓ
ϕℓ(v)ϕℓ(u)

.
As done previously, we can rewrite the last terms via (I + Z)−1 as
c(2)
m−1
X
k=0
(Qk)vu ≤2c(2)γ

m
√dvdu
2|E| (1 + γ)

+ 2c(2)
c2
γ

(1 + γ)I −∆)(I −Z2m)(I + Z)−1∆†
vu.
(31)
We can then combine (29) and (31) and derive the bound we claimed, when A = Asym.
For the case
A = Arw = D−1A, it suffices to notice that S ≤α′I + c2A, where α′ = ω/w + c1 and that
(1⊤Sk)i ≤
X
j
k
X
p=0
k
p

(α′)k−pcp
2((D−1A)p)ji ≤dmax
dmin
(α′ + c2)k ≤γ2,
where we have used that by assumption α′ + c2 ≤1. Similarly, we get (Sm−k)⊤S(m−k) ≤γ2Z2(m−k). Finally,
the Qk-term can be bounded by
c(2)
m−1
X
k=0
(Qk)vu ≤2c(2) m−1
X
k=0
γ4AsymZ2(m−k−1) + γ6Z2(m−k−1)
,
and we can follow the previous steps in the symmetric case to complete the proof.
Corollary D.3. Under the assumptions of Theorem D.2, if the message functions in (2) are linear – as for
GCN, SAGE, or GIN – then the maximal mixing induced by such an MPNN of m layers is
mixy(m)
G
(v, u) ≤
dmax
dmin
k
m
√dvdu
2|E|
+ 1
c2

Z2(I −Z2m)(I + Z)−1∆†
vu
Proof. This follows from Theorem D.2 simply by noticing that if the message-function ψ in (2) is linear, then
the upper bound for the norm of the Hessian can be taken to be zero, i.e. c(2) = 0.
D.3
Proof of Theorem 4.4
We now expand the previous results to derive the minimal number of layers required to induce mixing in the
case of bounded weights, showing that the depth may need to grow with the commute time of nodes. We recall
that γ is
p
dmax/dmin while 0 = λ0 < λ1 ≤. . . ≤λn−1 are the eigenvalues of the symmetrically normalized
graph Laplacian (9). We restate Theorem 4.4 below.
29

Theorem 4.4. Consider an MPNN satisfying Theorem 3.2, with max{w, ω/w + c1γ + c2} ≤1, and A = Asym.
If g
OSQv,u(m, w) · (mixyG(v, u)) ≤1, and hence the MPNN generates mixing mixyG(v, u) among the features
associated with nodes v, u, then the number of layers m satisfies
m ≥τ(v, u)
4c2
+
|E|
√dvdu
mixyG(v, u)
γµ
−1
c2
γ + |1 −c2λ∗|r−1
λ1
+ 2c(2)
µ

,
where r = dG(v, u), µ = 1 + 2c(2)(1 + γ) and |1 −c2λ∗| = max0<ℓ≤n−1 |1 −c2λℓ| < 1.
Proof. From now on we let r be the shortest-walk distance between v and u. If m < r/2, then we incur the
under-reaching issue and hence we get zero mixing among the features associated with nodes v, u. Accordingly,
we can choose m ≥r/2. We need to provide an estimate on the maximal mixing induced by an MPNN as in the
statement. We focus on the bound in Theorem 3.2, and recall that the first sum can be bounded as in (28) by
 m−1
X
k=0
Sm−kdiag(Sk1)Sm−k
vu ≤γ
m
X
k=1
(S2k)vu ≤γ
m
X
k=1
(Z2k)vu,
where Z := I −c2∆. We can then bound the geometric sum by accounting for the odd powers too. Therefore,
we get
γ
m
X
k=1
(Z2k)vu ≤γ
2m
X
k=0
(Zk)vu = γ
2m
X
k=0
X
ℓ≥0
(1 −c2λℓ)kϕℓ(v)ϕℓ(u).
As for the proof of Theorem D.2, we separate the contribution of the zero-eigenvalue and that of the positive
ones, so we find that
2m
X
k=0
(Zk)vu ≤(2m + 1)
√dvdu
2|E|
+
X
ℓ>0
1 −(1 −c2λℓ)2m+1
c2λℓ

ϕℓ(v)ϕℓ(u)
= (2m + 1)
√dvdu
2|E|
+
X
ℓ>0
1
c2λℓ
ϕℓ(v)ϕℓ(u) −
X
ℓ>0
(1 −c2λℓ)2m+1
c2λℓ
ϕℓ(v)ϕℓ(u).
(32)
Thanks to the characterization of commute-time provided in (11), we derive
X
ℓ>0
1
c2λℓ
ϕℓ(v)ϕℓ(u) = −τ(v, u)
4c2|E|
p
dvdu + 1
2c2
X
ℓ>0
1
λℓ

ϕ2
ℓ(v)
r
du
dv
+ ϕ2
ℓ(u)
r
dv
du

≤−τ(v, u)
4c2|E|
p
dvdu +
1
2c2λ1
r
dv
du
+
r
du
dv
−
√dvdu
|E|

(33)
where in the last inequality we have used that P
ℓ>0 ϕ2
ℓ(v) = 1 −ϕ2
0(v) since {ϕℓ} constitute an orthonormal
basis, with ϕ0(v) =
p
dv/2|E|, and that λℓ≥λ1, for all ℓ> 0. Next, we estimate the second sum in (32), and
we note that λ∗in the statement is either λ1 or λn−1:
−
X
ℓ>0
(1 −c2λℓ)2m+1
c2λℓ
ϕℓ(v)ϕℓ(u) ≤
X
ℓ>0
|1 −c2λ∗|2m+1
c2λℓ
|ϕℓ(v)ϕℓ(u)|
≤|1 −c2λ∗|2m+1
2c2λ1
X
ℓ>0
(ϕ2
ℓ(v) + ϕ2
ℓ(u))
≤|1 −c2λ∗|r
2c2λ1

2 −dv
2|E| −du
2|E|

,
(34)
30

where in the last inequality we have used that |1 −c2λ∗| < 1 and that m ≥r/2 (otherwise we would have
zero-mixing due to under-reaching). Therefore, by combining (33) and (34), we derive that the first sum on the
right hand side of (7) can be bounded from above by
 m−1
X
k=0
Sm−kdiag(Sk1)Sm−k
vu ≤γ

(2m + 1)
√dvdu
2|E|
−τ(v, u)
4c2|E|
p
dvdu

+
γ
2c2λ1
r
dv
du
+
r
du
dv
−
√dvdu
|E|

+ γ |1 −c2λ∗|r
2c2λ1

2 −dv
2|E| −du
2|E|

.
(35)
Next, we continue by estimating the second sum entering the right hand side of (7). We recall that by (30), we
have
c(2)
m−1
X
k=0
(Qk)vu ≤2c(2)γ
m−1
X
k=0

((1 + γ)I −∆)Z2(m−k−1)
vu = 2c(2)γ
m−1
X
k=0

((1 + γ)I −∆)Z2k
vu
≤2c(2)γ
2(m−1)
X
k=0

((1 + γ)I −∆)Zk
vu
= 2c(2)γ
2(m−1)
X
k=0
X
ℓ≥0
((1 + γ) −λℓ)(1 −c2λℓ)kϕℓ(v)ϕℓ(u).
We then proceed as above, and separate the contributions associated with the kernel of the Laplacian, to find
m−1
X
k=0
(Qk)vu ≤2γ

(1 + γ)(2m −1)
√dvdu
2|E|

+ 2γ(1 + γ)
X
ℓ>0
1
c2λℓ
(1 −(1 −c2λℓ)2m−1)ϕℓ(v)ϕℓ(u)
(36)
−2γ
c2
X
ℓ>0
(1 −(1 −c2λℓ)2m−1)ϕℓ(v)ϕℓ(u).
(37)
For the term in (36), we can apply the same estimate as for the case of (32). Similarly, we can bound (37) by
−2γ
c2
X
ℓ>0
(1 −(1 −c2λℓ)2m−1)ϕℓ(v)ϕℓ(u) ≤2γ
c2
X
ℓ>0
1
2

ϕ2
ℓ(v) + ϕ2
ℓ(u)

≤γ
c2

2 −dv
2|E| −du
2|E|

.
Therefore, we can finally bound the Qk-terms in (7) by
c(2)
m−1
X
k=0
(Qk)vu ≤2c(2)γ

(1 + γ)(2m −1)
√dvdu
2|E|
−(1 + γ)τ(v, u)
4c2|E|
p
dvdu

+ c(2)γ
1 + γ
c2λ1
r
dv
du
+
r
du
dv
−
√dvdu
|E|

c(2)γ 1 + γ
c2λ1
|1 −c2λ∗|r−1
2 −dv
2|E| −du
2|E|

+ c(2)γ
c2

2 −dv
2|E| −du
2|E|

.
(38)
31

We can the combine (35) and (38), to find that the maximal mixing induced by an MPNN of m layers as in
the statement of Theorem 4.4, is
mixy(m)
G
(v, u) ≤γ
p
dvdu
 m
|E|µ +
1
2|E| −τ(v, u)
4c2|E| µ

+ γ
µ
2c2λ1
r
dv
du
+
r
du
dv
−
√dvdu
|E|

+ γ
µ
2c2λ1
|1 −c2λ∗|r−1
2 −dv
2|E| −du
2|E|

+ γc(2)
c2

2 −dv
2|E| −du
2|E|

,
(39)
where µ := 1 + 2c(2)(1 + γ) and we have removed the term −2c(2)γ(1 + γ)√dvdu/2|E| ≤0. Moreover, since
λ1 < 1 unless G is the complete graph (and if that was the case, then we could take the distance r below to
simply be equal to 1) and c2 ≤1, we find
γ
p
dvdu
1
2|E|

1 −
µ
c2λ1

1 + |1 −c2λ∗|r−1
2
r
dv
du
+
r
du
dv

≤0.
Accordingly, we can simplify (39) as
mixy(m)
G
(v, u) ≤γ
p
dvdu
 m
|E|µ −τ(v, u)
4c2|E| µ

+
γµ
2c2λ1
r
dv
du
+
r
du
dv

+ γµ
c2λ1
|1 −c2λ∗|r−1 + 2γc(2)
c2
.
We can now rearrange the terms and obtain
m ≥τ(v, u)
4c2
+
|E|
√dvdu
mixyG(v, u)
γµ
−
1
2c2λ1
r
dv
du
+
r
du
dv

−
1
c2λ1
|1 −c2λ∗|r−1 −2c(2)
c2
1
µ

≥τ(v, u)
4c2
+
|E|
√dvdu
mixyG(v, u)
γµ
−
1
2c2λ1
(2γ) −
1
c2λ1
|1 −c2λ∗|r−1 −2c(2)
c2
1
µ

≥τ(v, u)
4c2
+
|E|
√dvdu
mixyG(v, u)
γµ
−1
c2
γ + |1 −c2λ∗|r−1
λ1
+ 2c(2)
µ

,
which completes the proof.
We note that the case of A = Arw follows easily since one can adapt the previous argument exactly as in the
proof of Theorem D.2, which lead to the same bounds once we replace γ with γ′ = dmax/dmin.
First, we note that the bounds again simplify further and become sharper if the message-functions ψ in (2) are
linear.
Corollary D.4. If the assumptions of Theorem 4.4 are satisfied, and the message-functions ψ are linear – as
for GCN, GIN, GraphSAGE – then
m ≥τ(v, u)
4c2
+
|E|
√dvdu
mixyG(v, u)
γ
−
1
c2λ1

γ + |1 −c2λ∗|r−1
.
D.4
The case of the unnormalized adjacency matrix
In this Section we extend the analysis on the depth required to induce mixing, to the case of the unnormalized
adjacency matrix A. When A = A, the aggregation in (2) is simply a sum over the neighbours, a case that
32

covers the classical GIN-architecture. In this way, the messages are no longer scaled down by the degree of
(either) the endpoints of the edge, which means that, in principle, the whole GNN architecture is more sensitive
but independent of where we are in the graph. First, we generalize Theorem 4.4 to this setting. We note that
the same conclusions hold, provided that the maximal spectral norm of the weights is smaller than the maximal
degree dmax; this is not surprising, since it accounts for the lack of the normalization of the messages.
Corollary D.5. Consider an MPNN as in (2) with A = A. If ω/(wdmax) + c1 + c2 ≤1 and wdmax ≤1, then
the minimal depth m satisfies the same lower bound as in Theorem 4.4 with γ = 1.
Proof. First, we note that in this case
Sij ≤ω
wδij + c1dmaxδij + c2Aij ≤dmax

αδij + c2(Asym)ij

,
where α = ω/(wdmax) + c1. In particular, we find that
(Sk1)i ≤
k
X
p=0
k
p

αk−pcp
2(Ap1)i ≤(dmax)k(α + c2)k.
Accordingly, we can bound the first sum in (7) as
 m−1
X
k=0
w2m−k(dmax)k(α + c2)k(dmax)2(m−k)
αI + c2Asym
2(m−k)
vu ≤
 m−1
X
k=0
Z2(m−k)
vu,
where we have used the assumptions α + c2 ≤1, and wdmax ≤1, and the definition Z := I −c2∆in Theorem
D.2. Since this term is the same one entering the argument in the proof of Theorem 4.4 (once we set γ = 1) we
can proceed in the same way to estimate it. A similar argument works for the sum of the Qk terms, which,
thanks to our assumptions, can still be bounded as in (30) with γ = 1 so that we can finally simply copy the
proof of Theorem 4.4.
A relative measurement for g
OSQ. To account for the fact that different message-passing matrices A may
lead to inherently quite distinct scales (think of the case where the aggregation is a mean vs when it is a sum),
one could modify the over-squashing characterization in Definition 4.2 as follows:
Definition D.6. Given an MPNN with capacity (m, w), we define the relative over-squashing of nodes v, u
as
g
OSQ
rel
v,u(m, w) :=



Pm−1
k=0 w2m−k−1
w(Sm−k)⊤diag(1⊤Sk)Sm−k + c(2)Qk

vu
maxi,j∈V
Pm−1
k=0 w2m−k−1

w(Sm−k)⊤diag(1⊤Sk)Sm−k + c(2)Qk

ij



−1
.
The normalization proposed here is similar to the idea of relative score introduced in [63]. This way, a larger
scale induced by a certain choice of the message-passing matrix A, is naturally accounted for by the relative
measurement. In particular, the relative over-squashing is now quantifying the maximal mixing among a certain
pair of nodes v, u compared to the maximal mixing that the same MPNN over the same graph can generate
among any pair of nodes. In our theoretical development in Section 4 we have decided to rely on the absolute
measurement since our analysis depends on the derivation of the maximal mixing induced by an MPNN (i.e.
upper bounds) which translate into necessary criteria for an MPNN to generate a given level of mixing. In
principle, to deal with relative measurements, one would also need some form of lower bound on the maximal
mixing and hence address also whether the conditions provided are indeed sufficient. We reserve a thorough
investigation of this angle to future work.
33

E
The case of node-level tasks
In this Section we discuss how one can extend our analysis to node-level tasks and further comment on the
novelty of our approach compared to existing results in [8, 25]. First, we emphasize that the analysis on the
Jacobian of node features carried over in [8, 25] cannot be extended to graph-level functions and that in fact,
our notion of mixing is needed to assess how two different node-features are communicating when the target is
a graph-level function.
From now on, let us consider the case where the function we need to learn is Y : Rn×d →Rn×d, and as usual
we assume it to be equivariant with respect to permutations of the nodes. A natural attempt to connect the
results in [8, 25] and the expressivity of the MPNNs – in the spirit of our Section 3 – could be to characterize
the first-order interactions (or mixing of order 1) of the features associated with nodes v, u with respect to
the underlying node-level task Y as
mix(1)
Y (v, u) = max
X
max
1≤α,β≤d

∂(Y(X))α
v
∂xβ
u
 ,
where (Y(X))v ∈Rd is the value of the node-level map at v. Accordingly, one can then use Theorem C.1 to
derive upper bounds on the maximal first-order interactions that MPNNs (2) can induce among nodes. As a
consequence of this approach, we would still find that MPNNs struggle to learn functions with large mix(1)
Y (v, u)
if nodes v, u have large commute time. In particular, in light of Theorem C.1, we can extend the measure of
over-squashing to the case of first-order interactions for node-level tasks. Once again, below we tacitly assume
that the non-linear activation σ satisfies |σ′| ≤1, although it is straightforward to extend the formulation to
the general case.
Definition E.1. Given an MPNN as in (2) with capacity (m, w), we define the first-order over-squashing
of v, u as
OSQ(1)
v,u(m, w) :=

mix(1)
Y (v, u)
−1
.
As for the case of graph-level tasks, we can then study a proxy (lower bound) for the node-level over-squashing
of order 1 by:
Definition E.2. Given an MPNN as in (2) with capacity (m, w) and S defined in (5), we approximate the
first-order over-squashing of v, u as
g
OSQ
(1)
v,u(m, w) :=

(cσw)m(Sm)vu
−1
.
It follows then from Theorem C.1, that a necessary condition for an MPNN to learn a node-level function Y
with first-order mixing mix(1)
Y (v, u) is
]
OSQ
(1)
v,u(m, w) <

mix(1)
Y (v, u)
−1
.
It is straightforward to argue as in Theorem 4.4 and [8] for example, to derive that nodes at higher effective
resistance will incur higher first-order over-squashing. Accordingly:
An MPNN as in (2) with bounded capacity, cannot learn node-level functions with high first-order interactions
among nodes v, u with high effective resistance.
Building a hierarchy of measures. Although first-order derivatives might be enough to capture some form
of over-squashing for node-level tasks, even in this scenario we can study the pairwise mixing induced at a
specific node, and hence consider the curvature (or Hessian) of the node-level function Y – which is more
expressive than the first-order Jacobian. Accordingly, for a node-level function Y : Rn×d →Rn×d, we say that
34

it has second-order interactions (or mixing of order 2) mix(2)
Y (i, v, u) of the features associated with nodes
v, u at a given node i when
mix(2)
Y (i, v, u) = max
X

∂2(Y(X))i
∂xu∂xv
 .
We can then restate Theorem C.2 as follows – we let Y(m) be the node-level function computed by an MPNN
after m layers.
Corollary E.3. Given MPNNs as in (2), let σ and ψ(t) be C2 functions and assume |σ′|, |σ′′| ≤cσ, ∥Ω(t)∥≤ω,
∥W(t)∥≤w, ∥∇1ψ(t)∥≤c1, ∥∇2ψ(t)∥≤c2, ∥∇2ψ(t)∥≤c(2). Let S ∈Rn×n be defined as in (5). Given nodes
i, v, u ∈V, if P(ℓ)
(vu) ∈Rn is as in (14) and m is the number of layers, then the maximal mixing of order 2 of
the MPNN at node i satisfies
mix(2)
Y(m)(i, v, u) ≤
m−1
X
k=0
X
j∈V
(cσw)2m−k−1 w(Sm−k)jv(Sk)ij(Sm−k)ju
+ c(2)
m−1
X
ℓ=0
(cσw)m+ℓ(Sm−1−ℓP(ℓ)
(vu))i.
(40)
Similarly to Definition 4.2, we can use the maximal mixing (at the node-level) to characterize the over-squashing
of order two at a specific node as follows: as usual, for simplicity we assume that cσ = 1.
Definition E.4. Given an MPNN as in (2) with capacity (m, w) and S defined in (5), we approximate the
second-order over-squashing of v, u at node i as
g
OSQ
(2)
i,v,u(m, w) :=
 m−1
X
k=0
X
j∈V
w2m−k(Sm−k)jv(Sk)ij(Sm−k)ju
+ c(2)
m−1
X
ℓ=0
wm+ℓ(Sm−1−ℓP(ℓ)
(vu))i
−1
.
It is then straightforward to extend our theoretical analysis to derive how ]
OSQ
(2)
prevents MPNNs from
learning node-level functions with high-mixing at some specific node i of features associated with nodes
v, u at large commute time.
To support our claim, consider the setting in Theorem 4.4 and hence let
A = Asym = D−1/2AD−1/2. Under the same assumptions of Theorem 4.4, we find
(Sk)ij ≤1.
We can then simply copy the proof of Theorem 4.4 once we set γ = 1 and extend its conclusions as follows:
Corollary E.5. Consider an MPNN as in (2) and let the assumptions of Theorem 4.4 hold. If the MPNN
generates second-order mixing mix(2)
Y (i, v, u) at node i, with respect to the features associated with nodes v, u,
then the number of layers m satisfy:
m ≥τ(v, u)
4c2
+
|E|
√dvdu
mixyG(v, u)
µ
−1
c2
1 + |1 −c2λ∗|r−1
λ1
+ 2c(2)
µ

,
where µ = 1 + 4c(2).
Accordingly, in this Section we have adapted our results from graph-level tasks to node-level tasks and proved
that:
The message of Section E. An MPNN of bounded capacity (m, w), cannot learn node-level functions that,
at some node i, induce high (first order or second order) mixing of features associated with nodes v, u whose
commute time is large.
35

F
Additional details of experiments and further ablations 5
F.1
Computing the commute time
The commute time τ between two nodes u, v ∈V on a graph G can be efficiently computed via the effective
resistance R, with τ(u, v) = 2|E|R(u, v). In order to compute the effective resistance R, we introduce the
(non-normalized) Laplacian matrix L = D −A, where D is the degree matrix. The effective resistance can
then be computed by
R(u, v) = Γuu + Γvv −2Γuv,
where Γ is the the Moore-Penrose inverse of
L + 1
|V |1|V |×|V |,
with 1|V |×|V | ∈R|V |×|V | being a matrix with all entries set to one.
F.2
Statistics of the results
Table 2: Mean and standard deviation (multiplied by a factor of 102) of the results reported in Fig. 3.
Model
α = 0.0
α = 0.2
α = 0.4
α = 0.6
α = 0.8
α = 1.0
GCN
0.80 ± 0.027
1.92 ± 0.039
1.89 ± 0.037
2.21 ± 0.030
3.26 ± 0.061
6.02 ± 0.012
GIN
0.34 ± 0.014
0.68 ± 0.183
1.08 ± 0.353
1.82 ± 0.291
2.78 ± 0.140
5.48 ± 0.683
GraphSAGE
0.04 ± 0.002
0.30 ± 0.020
0.58 ± 0.032
0.81 ± 0.034
1.39 ± 0.018
6.43 ± 0.017
GatedGCN
0.01 ± 0.001
0.06 ± 0.002
0.14 ± 0.007
0.27 ± 0.014
0.46 ± 0.014
6.46 ± 0.011
Table 3: Mean and standard deviation (multiplied by a factor of 102) of the results reported in Fig. 4.
Model
m = 5
m = 10
m = 15
m = 20
m = 25
m = 32
GCN
6.12 ± 0.003
3.53 ± 0.017
2.13 ± 0.003
1.55 ± 0.048
1.50 ± 0.392
1.14 ± 0.024
GIN
6.06 ± 0.010
1.74 ± 0.015
1.48 ± 0.424
1.89 ± 0.010
1.05 ± 0.123
1.02 ± 0.093
GraphSAGE
6.40 ± 0.006
1.84 ± 0.068
0.55 ± 0.041
0.61 ± 0.007
0.46 ± 0.035
0.44 ± 0.119
GatedGCN
6.40 ± 0.001
0.39 ± 0.049
0.29 ± 0.025
0.35 ± 0.032
0.43 ± 0.045
0.82 ± 0.171
F.3
On the training error
In this section, we report the training error of the MPNNs trained in section 5. Fig. 5 shows the training MAE
corresponding to the experiment in section 5.1, while Fig. 6 shows the training MAE corresponding to section
5.2. We can see that in both cases, the training MAE exhibits the same qualitative behavior as the reported
test MAE in the main paper, i.e., the training MAE increases for increasing levels of commute time τ, while it
decreases for increasing number of MPNN layers, which further validates our claim that over-squashing hinders
the expressive power of MPNNs.
36

0.0
0.2
0.4
0.6
0.8
1.0
Commute time α-quantile
10−5
10−4
10−3
10−2
10−1
Train MAE
GCN
GIN
GraphSAGE
GatedGCN
Figure 5: Train MAE of GCN, GIN, GraphSAGE,
and GatedGCN on synthetic ZINC, where the com-
mute time of the underlying mixing is varied, while
the MPNN architecture is fixed (e.g., depth, number
of parameters), i.e., mixing according to increasing
values of the α-quantile of the τ-distribution over the
ZINC graphs.
5
8
16
32
Number of layers m
0.050
0.010
0.001
Train MAE
GCN
GIN
GraphSAGE
GatedGCN
Figure 6: Train MAE of GCN, GIN, GraphSAGE,
and GatedGCN on synthetic ZINC, where the com-
mute time is fixed to be high (i.e., at the level of the
0.8-quantile), while only the depth of the underlying
MPNN is varied between 5 and 32 (all other archi-
tectural components are fixed).
F.4
On the over-squashing measure
In this section, we examine how the over-squashing measure ]
OSQ (as of Definition 4.2) depends on the commute
time τ as well as on the depth of the underlying MPNN. To this end, we follow the experimental setup of
section 5.1 and 5.2, but instead of training the models and presenting their performance in terms of the test
MAE, we compute ]
OSQ of the underlying models. We can see in Fig. 7 that ]
OSQ increases for increasing
values of the α-quantile of the τ-distribution for all MPNNs considered here. Moreover, we can see in Fig. 8
that ]
OSQ decreases for increasing number of layers for all considered models.
0.0
0.2
0.4
0.6
0.8
1.0
Commute time α-quantile
10−7
10−5
10−3
10−1
OSQ
GCN
GIN
GraphSAGE
GatedGCN
Figure 7: ]
OSQ (Definition 4.2) of GCN, GIN, Graph-
SAGE, and GatedGCN on synthetic ZINC, where
the commute time of the underlying mixing is varied,
while the MPNN architecture is fixed (e.g., depth,
number of parameters), i.e., mixing according to in-
creasing values of the α-quantile of the τ-distribution
over the ZINC graphs.
5
8
16
32
Number of layers m
10−19
10−15
10−11
10−7
10−3
OSQ
GCN
GIN
GraphSAGE
GatedGCN
Figure 8: ]
OSQ (Definition 4.2) of GCN, GIN, Graph-
SAGE, and GatedGCN on synthetic ZINC, where
the commute time is fixed to be high (i.e., at the
level of the 0.8-quantile), while only the depth of the
underlying MPNN is varied between 5 and 32 (all
other architectural components are fixed).
37

