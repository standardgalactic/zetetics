
Bayesian Speech and Language
Processing
With this comprehensive guide you will learn how to apply Bayesian machine
learning techniques systematically to solve various problems in speech and
language processing.
A range of statistical models is detailed, from hidden Markov models to
Gaussian mixture models, n-gram models, and latent topic models along with
applications including automatic speech recognition, speaker verification, and
information retrieval. Approximate Bayesian inferences based on MAP,
Evidence, Asymptotic, VB, and MCMC approximations are provided as well as
full derivations of calculations, useful notations, formulas, and rules.
The authors address the difficulties of straightforward applications and provide
detailed examples and case studies to demonstrate how you can successfully use
practical Bayesian inference methods to improve the performance of information
systems.
This is an invaluable resource for students, researchers, and industry
practitioners working in machine learning, signal processing, and speech and
language processing.
Shinji Watanabe received his Ph.D. from Waseda University in 2006. He has
been a research scientist at NTT Communication Science Laboratories, a visiting
scholar at Georgia Institute of Technology and a senior principal member at
Mitsubishi Electric Research Laboratories (MERL), as well as having been an
associate editor of the IEEE Transactions on Audio Speech and Language
Processing, and an elected member of the IEEE Speech and Language
Processing Technical Committee. He has published more than 100 papers in
journals and conferences, and received several awards including the Best Paper
Award from IEICE in 2003.

Jen-Tzung Chien is with the Department of Electrical and Computer
Engineering and the Department of Computer Science at the National Chiao
Tung University, Taiwan, where he is now the University Chair Professor. He
received the Distinguished Research Award from the Ministry of Science and
Technology, Taiwan, and the Best Paper Award of the 2011 IEEE Automatic
Speech Recognition and Understanding Workshop. He serves currently as an
elected member of the IEEE Machine Learning for Signal Processing Technical
Committee.

“This book provides an overview of a wide range of fundamental theories of
Bayesian learning, inference, and prediction for uncertainty modeling in speech
and language processing. The uncertainty modeling is crucial in increasing the
robustness of practical systems based on statistical modeling under real
environment, such as automatic speech recognition systems under noise, and
question answering systems based on limited size of training data. This is the
most advanced and comprehensive book for learning fundamental Bayesian
approaches and practical techniques.”
Sadaoki Furui, Tokyo Institute of Technology

Bayesian Speech and Language
Processing
SHINJI WATANABE
Mitsubishi Electric Research Laboratories
JEN-TZUNG CHIEN
National Chiao Tung University

University Printing House, Cambridge CB2 8BS, United Kingdom Cambridge University Press is part of
the University of Cambridge.
It furthers the University’s mission by disseminating knowledge in the pursuit of education, learning and
research at the highest international levels of excellence.
www.cambridge.org
Information on this title: www.cambridge.org/9781107055575
© Cambridge University Press 2015
This publication is in copyright. Subject to statutory exception and to the provisions of relevant collective
licensing agreements, no reproduction of any part may take place without the written permission of
Cambridge University Press.
First published 2015
Printed in the United Kingdom by Clays, St Ives plc A catalog record for this publication is available from
the British Library Library of Congress Cataloging in Publication data Watanabe, Shinji (Communications
engineer) author.
Bayesian speech and language processing / Shinji Watanabe, Mitsubishi Electric Research Laboratories;
Jen-Tzung Chien, National Chiao Tung University.
 pages  cm ISBN 978-1-10705557-5 (hardback) 1. Language and languages – Study and teaching –
Statistical methods. 2. Bayesian statistical decision theory. I. Title.
P53.815.W38 2015
410.1′51–dc23
      2014050265
ISBN 978-1-10705557-5 Hardback Cambridge University Press has no responsibility for the persistence or
accuracy of URLs for external or third-party internet websites referred to in this publication, and does not
guarantee that any content on such websites is, or will remain, accurate or appropriate.

Contents
Preface
Notation and abbreviations
Part I  General discussion
1   Introduction
1.1  Machine learning and speech and language processing
1.2  Bayesian approach
1.3  History of Bayesian speech and language processing
1.4  Applications
1.5  Organization of this book
2   Bayesian approach
2.1  Bayesian probabilities
2.1.1  Sum and product rules
2.1.2  Prior and posterior distributions
2.1.3  Exponential family distributions
2.1.4  Conjugate distributions
2.1.5  Conditional independence
2.2  Graphical model representation
2.2.1  Directed graph
2.2.2  Conditional independence in graphical model
2.2.3  Observation, latent variable, non-probabilistic variable
2.2.4  Generative process
2.2.5  Undirected graph
2.2.6  Inference on graphs
2.3  Difference between ML and Bayes
2.3.1  Use of prior knowledge
2.3.2  Model selection

2.3.3  Marginalization
2.4  Summary
3   Statistical models in speech and language processing
3.1  Bayes decision for speech recognition
3.2  Hidden Markov model
3.2.1  Lexical unit for HMM
3.2.2  Likelihood function of HMM
3.2.3  Continuous density HMM
3.2.4  Gaussian mixture model
3.2.5  Graphical models and generative process of CDHMM
3.3  Forward–backward and Viterbi algorithms
3.3.1  Forward–backward algorithm
3.3.2  Viterbi algorithm
3.4  Maximum likelihood estimation and EM algorithm
3.4.1  Jensen’s inequality
3.4.2  Expectation step
3.4.3  Maximization step
3.5  Maximum likelihood linear regression for hidden Markov model
3.5.1  Linear regression for hidden Markov models
3.6  n-gram with smoothing techniques
3.6.1  Class-based model smoothing
3.6.2  Jelinek–Mercer smoothing
3.6.3  Witten–Bell smoothing
3.6.4  Absolute discounting
3.6.5  Katz smoothing
3.6.6  Kneser–Ney smoothing
3.7  Latent semantic information
3.7.1  Latent semantic analysis
3.7.2  LSA language model
3.7.3  Probabilistic latent semantic analysis
3.7.4  PLSA language model
3.8  Revisit of automatic speech recognition with Bayesian manner
3.8.1  Training and test (unseen) data for ASR
3.8.2  Bayesian manner
3.8.3  Learning generative models
3.8.4  Sum rule for model
3.8.5  Sum rule for model parameters and latent variables

3.8.6  Factorization by product rule and conditional independence
3.8.7  Posterior distributions
3.8.8  Difficulties in speech and language applications
Part II  Approximate inference
4   Maximum a-posteriori approximation
4.1  MAP criterion for model parameters
4.2  MAP extension of EM algorithm
4.2.1  Auxiliary function
4.2.2  A recipe
4.3  Continuous density hidden Markov model
4.3.1  Likelihood function
4.3.2  Conjugate priors (full covariance case)
4.3.3  Conjugate priors (diagonal covariance case)
4.3.4  Expectation step
4.3.5  Maximization step
4.3.6  Sufficient statistics
4.3.7  Meaning of the MAP solution
4.4  Speaker adaptation
4.4.1  Speaker adaptation by a transformation of CDHMM
4.4.2  MAP based speaker adaptation
4.5  Regularization in discriminative parameter estimation
4.5.1  Extended Baum–Welch algorithm
4.5.2  MAP interpretation of i-smoothing
4.6  Speaker recognition/verification
4.6.1  Universal background model
4.6.2  Gaussian super vector
4.7  n-gram adaptation
4.7.1  MAP estimation of n-gram parameters
4.7.2  Adaptation method
4.8  Adaptive topic model
4.8.1  MAP estimation for corrective training
4.8.2  Quasi-Bayes estimation for incremental learning
4.8.3  System performance
4.9  Summary
5   Evidence approximation

5.1  Evidence framework
5.1.1  Bayesian model comparison
5.1.2  Type-2 maximum likelihood estimation
5.1.3  Regularization in regression model
5.1.4  Evidence framework for HMM and SVM
5.2  Bayesian sensing HMMs
5.2.1  Basis representation
5.2.2  Model construction
5.2.3  Automatic relevance determination
5.2.4  Model inference
5.2.5  Evidence function or marginal likelihood
5.2.6  Maximum a-posteriori sensing weights
5.2.7  Optimal parameters and hyperparameters
5.2.8  Discriminative training
5.2.9  System performance
5.3  Hierarchical Dirichlet language model
5.3.1  n-gram smoothing revisited
5.3.2  Dirichlet prior and posterior
5.3.3  Evidence function
5.3.4  Bayesian smoothed language model
5.3.5  Optimal hyperparameters
6   Asymptotic approximation
6.1  Laplace approximation
6.2  Bayesian information criterion
6.3  Bayesian predictive classification
6.3.1  Robust decision rule
6.3.2  Laplace approximation for BPC decision
6.3.3  BPC decision considering uncertainty of HMM means
6.4  Neural network acoustic modeling
6.4.1  Neural network modeling and learning
6.4.2  Bayesian neural networks and hidden Markov models
6.4.3  Laplace approximation for Bayesian neural networks
6.5  Decision tree clustering
6.5.1  Decision tree clustering using ML criterion
6.5.2  Decision tree clustering using BIC
6.6  Speaker clustering/segmentation
6.6.1  Speaker segmentation

6.6.2  Speaker clustering
6.7  Summary
7   Variational Bayes
7.1  Variational inference in general
7.1.1  Joint posterior distribution
7.1.2  Factorized posterior distribution
7.1.3  Variational method
7.2  Variational inference for classification problems
7.2.1  VB posterior distributions for model parameters
7.2.2  VB posterior distributions for latent variables
7.2.3  VB–EM algorithm
7.2.4  VB posterior distribution for model structure
7.3  Continuous density hidden Markov model
7.3.1  Generative model
7.3.2  Prior distribution
7.3.3  VB Baum–Welch algorithm
7.3.4  Variational lower bound
7.3.5  VB posterior for Bayesian predictive classification
7.3.6  Decision tree clustering
7.3.7  Determination of HMM topology
7.4  Structural Bayesian linear regression for hidden Markov model
7.4.1  Variational Bayesian linear regression
7.4.2  Generative model
7.4.3  Variational lower bound
7.4.4  Optimization of hyperparameters and model structure
7.4.5  Hyperparameter optimization
7.5  Variational Bayesian speaker verification
7.5.1  Generative model
7.5.2  Prior distributions
7.5.3  Variational posteriors
7.5.4  Variational lower bound
7.6  Latent Dirichlet allocation
7.6.1  Model construction
7.6.2  VB inference: lower bound
7.6.3  VB inference: variational parameters
7.6.4  VB inference: model parameters
7.7  Latent topic language model

7.7.1  LDA language model
7.7.2  Dirichlet class language model
7.7.3  Model construction
7.7.4  VB inference: lower bound
7.7.5  VB inference: parameter estimation
7.7.6  Cache Dirichlet class language model
7.7.7  System performance
7.8  Summary
8   Markov chain Monte Carlo
8.1  Sampling methods
8.1.1  Importance sampling
8.1.2  Markov chain
8.1.3  The Metropolis–Hastings algorithm
8.1.4  Gibbs sampling
8.1.5  Slice sampling
8.2  Bayesian nonparametrics
8.2.1  Modeling via exchangeability
8.2.2  Dirichlet process
8.2.3  DP: Stick-breaking construction
8.2.4  DP: Chinese restaurant process
8.2.5  Dirichlet process mixture model
8.2.6  Hierarchical Dirichlet process
8.2.7  HDP: Stick-breaking construction
8.2.8  HDP: Chinese restaurant franchise
8.2.9  MCMC inference by Chinese restaurant franchise
8.2.10  MCMC inference by direct assignment
8.2.11  Relation of HDP to other methods
8.3  Gibbs sampling-based speaker clustering
8.3.1  Generative model
8.3.2  GMM marginal likelihood for complete data
8.3.3  GMM Gibbs sampler
8.3.4  Generative process and graphical model of multi-scale GMM
8.3.5  Marginal likelihood for the complete data
8.3.6  Gibbs sampler
8.4  Nonparametric Bayesian HMMs to acoustic unit discovery
8.4.1  Generative model and generative process
8.4.2  Inference

8.5  Hierarchical Pitman–Yor language model
8.5.1  Pitman–Yor process
8.5.2  Language model smoothing revisited
8.5.3  Hierarchical Pitman–Yor language model
8.5.4  MCMC inference for HPYLM
8.6  Summary
Appendix A Basic formulas
Appendix B Vector and matrix formulas
Appendix C Probabilistic distribution functions
References
Index

Preface
In general, speech and language processing involves extensive knowledge of
statistical models. The acoustic model using hidden Markov models and the
language model using n-grams are mainly introduced here. Both acoustic and
language models are important parts of modern speech recognition systems
where the learned models from real-world data are full of complexity, ambiguity,
and uncertainty. The uncertainty modeling is crucial to tackle the lack of
robustness for speech and language processing.
This book addresses fundamental theories of Bayesian learning, inference,
and prediction for the uncertainty modeling. Uniquely, compared with standard
textbooks for dealing with the fundamental Bayesian approaches, this book
focuses on the practical methods of the approaches to make them applicable to
actual speech and language problems. We (the authors) have been studying these
topics for a long time with a strong belief that the Bayesian approaches could
solve “robustness” issues in speech and language processing, which are the most
difficult problem and most serious shortcoming of real systems based on speech
and language processing. In our experience, the most difficult issue in applying
Bayesian approaches is how to appropriately choose a specific technique among
the many Bayesian techniques proposed in statistics and machine learning so far.
One of our answers to this question is to provide the approximated Bayesian
inference methods rather than focusing on covering the whole Bayesian
techniques. We categorize the Bayesian approaches into five categories: the
maximum 
a-posteriori 
estimation; 
evidence 
approximation; 
asymptotic
approximation; variational Bayes; and Markov chain Monte Carlo. We also
describe the speech and language processing applications within this
categorization so that readers can appropriately choose the approximated
Bayesian techniques for their problems.

This book is part of our long-term cooperative efforts to promote the
Bayesian approaches in speech and language processing. We have been pursuing
this goal for more than ten years, and part of our efforts was to organize a
tutorial lecture with this theme at the 37th International Conference on
Acoustics, Speech, and Signal Processing (ICASSP) in Kyoto, Japan, March
2012. The success of this tutorial lecture prompted the idea of writing a textbook
with this theme. We strongly believe in the importance of the Bayesian
approaches, and we sincerely encourage the researchers who work with
Bayesian speech and language processing.
Acknowledgments
First we want to thank all of our colleagues and research friends, especially
members of NTT Communication Science Laboratories, Mitsubishi Electric
Research Laboratories (MERL), National Cheng Kung University, IBM T. J.
Watson Research Center, and National Chiao Tung University (NCTU). Some of
the studies in this book were actually conducted when the authors were working
in these institutes. We also would like to thank many people for reading a draft
and giving us valuable comments which greatly improved this book, including
Tawara Naohiro, Yotaro Kubo, Seong-Jun Hahm, Yu Tsao, and all of the
students from the Machine Learning Laboratory at NCTU. We are very grateful
for support from Anthony Vetro, John R. Hershey, and Jonathan Le Roux at
MERL, and Sin-Horng Chen, Hsueh-Ming Hang, Yu-Chee Tseng, and Li-Chun
Wang at NCTU. The great efforts of the editors of Cambridge University Press,
Phil Meyler, Sarah Marsh, and Heather Brolly, are also appreciated. Finally, we
would like to thank our families for supporting our whole research lives.
Shinji Watanabe
Jen-Tzung Chien

Notation and abbreviations
General notation
This book observes the following general mathematical notation to avoid any
confusion arising from notation: 
Set of boolean values 
Set of positive integers 
Set of real numbers
Set of positive real numbers 
Set of D dimensional real numbers 
Set of all possible strings composed of letters 
Empty set
a
Scalar variable

a
Vector variable
Elements of a vector, which can be described with the square brackets 
, 
 denotes the transpose operation A
Matrix variable
Elements of a matrix, which can be described with the square brackets 
D × D identity matrix 
Determinant of square matrix 
Trace of square matrix A, 
Set or sequential variable 
Elements in a set, which can be described with the curly brackets 
Elements in a set, where the range of index n is omitted for simplicity 
A set of sequential variables, which explicitly describes the range of

elements from n to 
 by using  in the subscript 
The number of elements in a set A. For example 
 or 
Function of x 
 or 
Probabilistic distribution function of x 
Functional of f. Note that a functional uses the square brackets 
 while a
function uses the bracket 
.
The expectation of 
 with respect to probability distribution 
 or 
Another form of the expectation of 
, where the subscript with the
probability distribution and/or the conditional variable is omitted, when it is
trivial.
Kronecker delta function for discrete variables a and 
Dirac delta function for continuous variables x and 
, 
, 
, 
, …
The variables estimated by a specific criterion (e.g., Maximum Likelihood
(ML)) are represented with the superscript of the abbreviation of the criterion.

Basic notation used for speech and language
processing
We also list the notation specific for speech and language processing. This book
tries to maintain consistency by using the same notation, while it also tries to use
commonly used notation in each application. Therefore, some of the same
characters are used to denote different variables, since this book needs to
introduce many variables.
Common notation
Θ
Set of model parameters M
Model variable including types of models, structure, hyperparameters, etc.
Ψ
Set of hyperparameters 
Auxiliary function used in the EM algorithm H
Hessian matrix
Acoustic modeling
Number of speech frames 
Speech frame index

D dimensional feature vector at time t 
Sequence of T feature vectors 
Number of unique HMM states in an HMM
HMM state at time t 
Sequence of HMM states for T speech frames 
Number of unique mixture components in a GMM
Latent mixture variable at time t 
Sequence of latent mixture variables for T speech frames 
Forward probability of the partial observations 
 until time t and
state j at time t 
Backward probability of the partial observations 
 from 
to the end given state j at time t 
The highest probability along a single path, at time t which accounts for
previous observations 
 and ends in state j at time t 
Posterior probability of staying state i at time t and state j at time 

Posterior probability of staying at state j and mixture component k at time t 
Initial state probability of state j at time 
State transition probability from state 
 to state 
Gaussian mixture weight at component k of state j 
Gaussian mean vector at component k of state j 
Gaussian covariance matrix at component k of state j. Symmetric matrix 
Gaussian precision matrix at component k of state j. Symmetric matrix, and
the inverse of covariance matrix 
Language modeling
Category (e.g., word in most cases, phoneme sometimes). The element is
represented by a string in 
 (e.g., “I” and “apple” for words and a and k for
phonemes) or a natural number in 
 when the elements of categories are
numbered.
Vocabulary (dictionary), i.e., a set of distinct words, which is a subset of 

Vocabulary size
Ordered index number of distinct words in vocabulary 
Word pointed by an ordered index v 
A set of distinct words, which is equivalent to vocabulary 
Number of categories in a chunk (e.g., number of words in a sentence or
number of phonemes or HMM states in a speech segment) 
ith position of category (e.g., word or phoneme) 
Word at ith position 
Word sequence from 1 to J
Word sequence from 
 to i 
n-gram probability, which considers 
 order Markov model 
Number of occurrences of word sequence 
 in a training corpus 
Interpolation weight for each 

Number of documents
Document index
mth document, which would be represented by a string or positive integer 
Number of co-occurrences of word 
 in document 
Number of unique latent topics 
ith latent topic variable for word 
Sequence of latent topic variables for J words
Abbreviations
AIC: Akaike Information Criterion (page 217)
AM: Acoustic Model (page 3)
ARD: Automatic Relevance Determination (page 194)

ASR: Automatic Speech Recognition (page 58)
BIC: Bayesian Information Criterion (page 8)
BNP: Bayesian Nonparametrics (pages 337, 345)
BPC: Bayesian Predictive Classification (page 218)
CDHMM: Continuous Density Hidden Markov Model (page 157)
CRP: Chinese Restaurant Process (page 350)
CSR: Continuous Speech Recognition (page 334)
DCLM: Dirichlet Class Language Model (page 326)
DHMM: Discrete Hidden Markov Model (page 62)
DNN: Deep Neural Network (page 224)
DP: Dirichlet Process (page 348)
EM: Expectation Maximization (page 9)
fMLLR: feature-space MLLR (page 204)
GMM: Gaussian Mixture Model (page 63)
HDP: Hierarchical Dirichlet Process (page 337)
HMM: Hidden Markov Model (page 59)

HPY: Hierarchical Pitman–Yor Process (page 383)
HPYLM: Hierarchical Pitman–Yor Language Model (page 384)
iid: Independently, identically distributed (page 216)
KL: Kullback–Leibler (page 79)
KN: Kneser–Ney (page 102)
LDA: Latent Dirichlet Allocation (page 318)
LM: Language Model (page 3)
LSA: Latent Semantic Analysis (page 113)
LVCSR: Large Vocabulary Continuous Speech Recognition (page 97)
MAP: Maximum A-Posteriori (page 7)
MAPLR: Maximum A-Posteriori Linear Regression (page 287)
MBR: Minimum Bayes Risk (page 56)
MCE: Minimum Classification Error (page 59)
MCMC: Markov Chain Monte Carlo (page 337)
MDL: Minimum Description Length (page 9)
MFCC: Mel-Frequency Cepstrum Coefficients (page 249)

MKN: Modified Kneser–Ney (page 111)
ML: Maximum Likelihood (page 77)
ML2: Type-2 Maximum Likelihood (page 188)
MLLR: Maximum Likelihood Linear Regression (page 200)
MLP: MultiLayer Perceptron (page 326)
MMI: Maximum Mutual Information (page 167)
MMSE: Minimum Mean Square Error (page 139)
MPE: Minimum Phone Error (page 167)
nCRP: nested Chinese Restaurant Process (page 360)
NDP: Nested Dirichlet Process (page 360)
NMF: Non-negative Matrix Factorization (page 124)
pdf: probability density function (page 63)
PLP: Perceptual Linear Prediction (page 54)
PLSA: Probabilistic Latent Semantic Analysis (page 113)
PY: Pitman–Yor Process (page 379)
QB: Quasi-Bayes (page 180)

RHS: Right-Hand Side (page 199)
RLS: Regularized Least-Squares (page 188)
RVM: Relevance Vector Machine (page 192)
SBL: Sparse Bayesian Learning (page 194)
SBP: Stick Breaking Process (page 348)
SMAP: Structural Maximum A-Posteriori (page 288)
SMAPLR: Structural Maximum A-Posteriori Linear Regression (page 288)
SVD: Singular Value Decomposition (page 114)
SVM: Support Vector Machine (page 188)
tf–idf: term frequency – inverse document frequency (page 113)
UBM: Universal Background Model (page 172)
VB: Variational Bayes (page 7)
VC: Vapnik–Chervonenkis (page 191)
VQ: Vector Quantization (page 62)
WB: Witten–Bell (page 102)
WER: Word Error Rate (page 56)

WFST: Weighted Finite State Transducer (page 60)
WSJ: Wall Street Journal (page 108)

Part I
General discussion

1    Introduction
1.1   Machine learning and speech and language
processing
Speech and language processing is one of the most successful examples of
applying machine learning techniques to real problems. Current speech and
language 
techniques 
embody 
our 
real-world 
information 
processing,
automatically 
including 
information 
extraction, 
question 
answering,
summarization, dialog, conversational agent, and machine translation (Jurafsky
& Martin 2000). Among these, one of the most exciting applications of speech
and language processing is speech recognition based voice search technologies
(by Google, Nuance) and conversational agent technologies (by Apple)
(Schalkwyk, Beeferman, Beaufays et al. 2010). These successful applications
started to make people in general casually use speech interface rather than text
interface in mobile devices, and the applications of speech and language
processing are widely expanding.
One of the core technologies of speech and language processing is automatic
speech recognition (ASR) and related techniques. Surprisingly, these techniques
are fully based on statistical approaches by using large amounts of data. The
machine learning techniques are applied to utilize these data. For example, the
main components of ASR are acoustic and language models. The acoustic model
(AM) provides a statistical model of each phoneme/word unit, and it is
represented by a hidden Markov model (HMM). The HMM is one of the most
typical examples of dealing with sequential data based on machine learning
techniques (Bishop 2006), and machine learning techniques provide an efficient
method of computing a maximum likelihood value for the HMM and an efficient
training algorithm of the HMM parameters. The language model (LM) also
provides an n-gram based statistical model for word sequences, which is also

trained by using the large amount of data based on machine learning techniques.
These statistical models and their variants are used for the other speech and
language applications, including speaker verification and information retrieval,
and thus, machine learning is a core component of speech and language
processing.
Machine learning covers a wide range of applications in addition to speech
and language processing, including bioinformatics, data mining, and computer
vision. Machine learning also covers various theoretical fields including pattern
recognition, information theory, statistics, control theory, and applied
mathematics. Therefore, many people are studying and developing machine
learning techniques, and the progress of machine learning is rather fast. By
following the rapid progress of machine learning, researchers in speech and
language processing interact positively with the machine learning community or
communities in the machine learning application field by importing (and
sometimes exporting) advanced machine learning techniques. For example, the
recent great improvement of ASR comes from this interaction for discriminative
approaches (recent progress summaries for discriminative speech recognition
techniques are found in Gales, Watanabe & Fossler-Lussier (2012), Heigold,
Ney, Schluter et al. (2012), Saon & Chien (2012b), Hinton, Deng, Yu
et al. (2012). The discriminative training of HMM parameters has been mainly
studied in speech recognition research since the 1990s, and became a standard
technique around the 2000s. In addition, the deep neural network replaces the
emission probability of the HMM from the Gaussian mixture model (GMM) (or
is used as feature extraction (Hermansky, Ellis & Sharma 2000, Grézl, Karafiát,
Kontár et al. 2007) for the GMM) and achieves further improvement on the
discriminative training based ASR performance. Actually, current successful
applications of speech and language processing are highly supported by these
breakthroughs based on the discriminative techniques developed through the
interaction with the machine learning community. By following the successful
experience, researchers in speech and language processing try to collaborate
with the machine learning community further to find new technologies.
1.2   Bayesian approach
This book also follows the trend of tight interaction with the machine learning
community, but focuses on another active research topic in machine learning,

called the Bayesian approach. The Bayesian approach is a major probabilistic
theory that represents a causal relationship of data. By dealing with variables
introduced in a model as probabilistic variables, we can consider uncertainties
included in these variables based on the probabilistic theory.
As a simple example of uncertainty, we think of statistically modeling
several data (
, 
, …, 
) by a Gaussian distribution 
 with mean
and variance parameters μ and Σ, as shown in Figure 1.1, i.e.,
(1.1)
Now, we consider the Bayesian approach, where the mean parameter is
uncertain, and is distributed by a probabilistic function 
. Since μ is
uncertain, we can consider the several possible μs instead of one fixed μ, and the
Bayesian approach considers representing a distribution of x by several
Gaussians with possible mean parameters (
, 
, and 
 in the example of
Figure 1.1),
(1.2)
where 
, 
, … are generated from the distribution 
. The extreme case
of this uncertainty consideration is to represent the distribution of x, which is
represented by all possible Gaussian distributions, with all possible mean
parameters weighted by the probability 
, which is represented as the
following integral equation:
(1.3)
This expectation over uncertain variables is called marginalization. The grayed
band in Figure 1.1 provides an image of the expected distribution, where the
mean parameter is marginalized over all possible infinite mean values. This is a
unique aspect of the Bayesian approach that represents variables in a model by
probabilistic distributions, and holds their uncertainties.

Figure 1.1 The uncertainty of the mean parameter of a Gaussian distribution.
This uncertainty consideration often improves the generalization capability
of a model that yields to mitigate the mismatch between training and unseen data
and avoid over-fitting problems by the effect of regularization and
marginalization. For example, Figure 1.2 points out the over-fitting problem. In
real applications, we often face phenomena where observed data cannot cover all
possible unobserved data (unseen data) and their distributions are mismatched.
The Gaussian distribution with a fixed mean parameter that is estimated from
observed data can well represent the observed data, but it cannot represent
unseen data properly. This is a well-known over-fitting problem, that the model
overly fits its parameters to represent observed data. However, since the
Bayesian approach considers all possible mean parameters by the
marginalization, some of these parameters would properly model unseen data
more accurately than the fixed mean parameter case. The effect of this more
powerful representation ability for unseen data leads to improved generalization
capability, which is a famous advantage of the Bayesian approach. In addition,
the example can also be viewed as showing that the mean parameter is
regularized not to overly fit the observed data to 
. The effect of setting the
constraints for variables via their probabilistic distributions is a typical example
of the regularization, and the regularization also leads to improved
generalization capability.

Figure 1.2 The Bayesian approach that holds uncertainty of the mean
parameter has an ability to describe unseen data.
Furthermore, the Bayesian theory provides a straightforward mathematical
way to infer (predict) unobservable probabilistic variables by using the basic
rules (including the Bayes theorem) within the basic probabilistic theory. The
beauties of this mathematical treatment based on probabilistic theory and the
expected robustness by considering uncertainties attract many machine learning
researchers to study the Bayesian approach. Actually, Bayesian machine learning
has also rapidly grown similarly to discriminative techniques, and various
interesting approaches have been proposed (Bishop 2006, Barber 2012).
However, compared with the successful examples of the discriminative
techniques, the applications of the Bayesian approach in speech and language
processing are rather limited despite its many advantages. One of the most
difficult problems is that the exact Bayesian approach cannot be applied in our
speech and language processing without some approximations. For example, the
Bayesian approach is based on the conditional distribution given datum x (that is
called posterior distribution, 
). However, it is generally difficult to obtain
the posterior distribution analytically, since we cannot solve the equation
analytically to obtain the posterior distribution. The computation is often
performed numerically, which limits the practical applications of the Bayesian
approach, especially for speech and language processing that deals with large
amounts of data. Thus, how to make the Bayesian approach more practical for
speech and language processing is the most critical problem in Bayesian speech
and language processing.

This book is aimed to guide readers in machine learning or speech and
language processing to apply the Bayesian approach to speech and language
processing in a systematic way. In other words, this book aims to bridge the gap
between the machine learning community and the speech and language
community by removing their preconceived ideas of the difficulty in these
applications (Figure 1.3) (Watanabe & Chien 2012). The key idea for this
guidance is how to approximate the Bayesian approach for specific speech and
language applications (Ghahramani 2004). There are several approximations
developed mainly in the machine learning and Bayesian statistics fields to deal
with the problems. This book mainly deals with the following approximations:
Chapter 4: Maximum a-posteriori approximation (MAP);
Chapter 5: Evidence approximation;
Chapter 6: Asymptotic approximation;
Chapter 7: Variational Bayes (VB);
Chapter 8: Markov chain Monte Carlo (MCMC).
Note that there are some other interesting approximations (e.g., loopy belief
propagation (Murphy, Weiss & Jordan 1999, Yedidia, Freeman & Weiss 2003)
and expectation propagation (Minka 2001)), but our book focuses on the above
approximations. These approximations are described in the corresponding
chapters in detail. We organize the chapters in Part II categorized by these
approximation techniques, unlike application-oriented categorization (e.g.,
speech recognition and speech synthesis) as has appeared in many other speech
and language books (Jurafsky & Martin 2000, Huang, Acero & Hon 2001), to
emphasize how to approximate the Bayesian approach for practical applications.
For example, the first sections in each chapter in Part II describe the introduction
of the corresponding approximation, and provide the recipe for how to use the
approximation in machine learning problems in general. The following sections
in the chapter provide the approximated solutions of statistical models used in
specific speech and language applications by following the recipe. This book
mainly deals with popular statistical models including HMM, GMM, neural
network, factor analysis, n-gram, and latent topic models. Table 1.1 summarizes
approximated Bayesian inferences for statistical models discussed in this book.
The applications covered by this book are typical topics in speech and language
processing based on these statistical models with the approximated Bayesian
treatment, and mainly related to automatic speech recognition.

Figure 1.3 The aim of this book.
Table 1.1 Approximated Bayesian inference for statistical models discussed in
this book.
Approximation
HMM
GMM
Neural
Factor
n-gram
Latent topic
network
analysis
model
MAP
4.3, 4.5
4.6
4.7
4.8
Evidence
5.2
5.3
Asymptotic
6.3, 6.5
6.6
6.4
VB
7.3
7.5
7.6
MCMC
8.3, 8.4
8.5
1.3   History of Bayesian speech and language
processing
There have been various studies to apply the Bayesian approach in speech and
language processing. Although one of the aims of this book is to summarize
these studies in a more systematic way in terms of an approximated Bayesian
inference view, this section briefly reviews the history of these studies, according
to time, by categorizing them within four trends.
The major earliest trend of using the Bayesian approach to speech and
language processing started with the statistical modeling of automatic speech
recognition in the 1980s. Furui (2010) reviews a historical perspective of
automatic speech recognition and calls the technologies developed with
statistical modeling around the 1980s the third generation technology. In a

Bayesian perspective, this statistical modeling movement corresponded to the
first introduction of probabilistic variables for speech recognition outputs (e.g.,
word sequences). As a result, the statistical modeling of automatic speech
recognition formulates the speech recognition process as Bayes decision theory,
and the noisy channel model is provided to solve the decision problem of
determining most probable word sequences by considering the product of
acoustic and language model distributions based on the Bayes theory
(Jelinek 1976). The language model is used to provide a prior distribution of
word sequences.
The second trend in the 1990s was to expand the Bayesian perspective from
the speech recognition outputs to model parameters by regarding these as
probabilistic variables. Maximum a-posteriori (MAP) estimation of HMM
parameters is known as the most successful application of the Bayesian approach
(Lee, Lin & Juang 1991, Gauvain & Lee 1994). This approach was used for
speaker adaptation, where the prior distribution of HMM parameters is estimated
from a large amount of speaker-independent data and the MAP estimation is
used to estimate target speaker’s HMM parameters with a small amount of target
speaker data. The prior distribution regularizes the target speaker’s HMM
parameters to guarantee the performance of speaker independent HMMs instead
of overly tuning to the model. Another example of the expansion was to treat
model structure as a probabilistic variable in the late 1990s. This treatment
enables model selection by selecting a most probable model structure from the
posterior distribution of the model structure given training data (e.g., the
numbers of HMM states and Gaussians). The Bayesian information criterion
(BIC) and the minimum description length (MDL) criterion are successful
examples (Shinoda & Watanabe 1996, Chen & Gopinath 1999, Chou &
Reichl 1999, Zhou & Hansen 2000) that cover wide applications of speech and
language processing (e.g., acoustic model selection, speaker clustering, and
speaker segmentation).
This second trend made the importance of the Bayesian approach come alive
within the speech and language communities. From the late 1990s to 2000s,
many other Bayesian studies have been applied to speech and language
processing, which are classified as the third trend. The Bayesian techniques
(MAP and BIC) used in the second trend miss the important Bayesian concept,
marginalization, which makes the full treatment of the Bayesian approach
difficult. By following the progress of VB, MCMC, Evidence approximation,

and graphical model techniques in machine learning in the 1990s, people in
speech and language processing started to apply more exact Bayesian
approaches by fully incorporating marginalization. These studies covered almost
all statistical models in speech and language processing (Bilmes &
Zweig 2002, Watanabe, Minami, Nakamura & Ueda 2002, Blei, Ng &
Jordan 2003, Saon & Chien 2011). The most successful approach in the third
trend is latent Dirichlet allocation (LDA (Blei et al. 2003)), which provides a VB
solution of a latent topic model from a probabilistic latent semantic analysis
(Hofmann 1999b). LDA has been mainly developed in machine learning and
natural language processing by incorporating a Gibbs sampling solution
(Griffiths & Steyvers 2004), and structured topic models (Wallach 2006), and
LDA was extended to incorporate Bayesian nonparametrics (Teh, Jordan, Beal
& Blei 2006), which became the fourth trend in Bayesian speech and language
processing.
The fourth trend is a still ongoing trend that tries to fully incorporate
Bayesian nonparametrics with speech and language processing. Due to their
computational costs and algorithmic difference from the standard expectation
maximization (EM) type algorithms (Dempster, Laird & Rubin 1976), the
applications were limited to latent topic models and related studies. However,
recent computational progress (e.g., many core processing and GPU processing)
has enabled broadening of the fourth trend in statistical models in speech and
language 
processing 
other 
than 
extended 
latent 
topic 
models
(Teh 2006, Goldwater 2007, Fox, Sudderth, Jordan et al. 2008, Ding &
Ou 2010, Lee & Glass 2012).
This book covers all four trends and categorizes these popular techniques
with the approximated Bayesian inference techniques.
1.4   Applications
This book aims to describe the following target applications:
Automatic speech recognition (ASR, Figure 1.4)
This is a main application in this book, that converts human speech to texts.
The main techniques required in ASR are based on speech enhancement for
noise reduction, speech feature extraction, acoustic modeling, language

modeling, pronunciation lexicon modeling, and search. The book mainly
deals with acoustic models represented by HMM and neural networks, and
language models represented by n-gram and latent topic models.
Figure  1.4 Automatic speech recognition.
Speaker verification/segmentation/clustering (Figure 1.5)
Speaker recognition techniques automatically provide a speaker identity
given speech data. This often requires extracting speech segments uttered
by target speakers from the recorded utterances (speaker segmentation). In
addition, some of the real applications cannot have speaker labels in
advance, and clustering speech segments to a specific speaker cluster
(speaker clustering) is also another important direction. Gaussian or GMMs
are used as statistical models. However, state-of-the-art speaker verification
systems use GMMs as preprocessing, namely GMM parameters estimated
from a speech segment are used as speaker features. The estimated features
are further processed by a factor analysis to remove speaker-independent
feature characteristics statistically.

Figure 1.5 Speaker verification/segmentation/clustering.
(Spoken) Information retrieval (IR, Figure 1.6)
Information retrieval via document classification given a text (or spoken)
query is another application of speech and language processing. The
document can be represented by various units including newspaper articles,
web pages, dialog conversations, sentences/utterances; which is used
depends on applications. The most successful application for information
retrieval is a search engine that uses a web page as a document unit. Voice
search is an instance of a search engine based on spoken information
retrieval where spoken terms are converted to text-form terms by using
ASR and these terms are used as a query. The approach is based on the
vector space model, which represents documents and queries as vectors in a
vector space. The vector is simply represented by count or weighted count
of unique words in a vocabulary. The approach often uses n-gram or latent
topic models to provide more informative vector representation of
documents.

Figure 1.6 (Spoken) Information retrieval.
1.5   Organization of this book
This book is divided into two parts. Part I starts by describing the Bayesian
approach in general in Chapter 2, but the topics discussed in this book are
focused on practical machine learning problems regarding speech and language
processing rather than on general Bayesian statistics. Chapter 3 also provides
general discussion of speech and language processing by providing some
important statistical models. The discussion is based on the conventional style
based on the maximum likelihood approach. These models are re-formulated in
the latter sections in a Bayesian manner with an appropriated approximation.
The final section in Chapter 3 also describes the application of the Bayesian
approach to statistical models in automatic speech recognition, and discusses the
difficulty of straightforward application.
As described before, Part II provides approximated Bayesian inferences
based on MAP, Evidence, Asymptotic, VB, and MCMC approximations, and
relevant sections describe statistical models and their applications, as shown in
Table 1.1. We also provide in the appendices some useful formulas and rules
used.

For readers
Our book assumes three types of readers: 1) undergraduate and graduate
students; 2) those who have some knowledge in machine learning; and 3) those
who have some knowledge in speech and language processing. We recommend
undergraduate and graduate students to read through this book, as we have made
it self-consistent, especially for deriving equations. Readers who have
knowledge in machine learning can skip the beginning part of Chapter 2 about
the “Bayesian approach” in general. Similarly, readers who have knowledge in
speech and language processing may skip Chapter 3 about statistical models in
“speech and language processing”. However, compared with conference and
journal papers in this field, the chapter provides comprehensive understanding of
standard statistical models with full derivations of equations. Therefore, we
highly recommend readers familiar with these models to read through the whole
book.

2    Bayesian approach
This chapter describes a general concept and statistics of the Bayesian approach.
The Bayesian approach covers wide areas of statistics (Bernardo & Smith 2009,
Gelman, Carlin, Stern et al. 2013), pattern recognition (Fukunaga 1990),
machine learning (Bishop 2006, Barber 2012), and applications of these
approaches. In this chapter, we start the discussion from the basic probabilistic
theory, and mainly describe the Bayesian approach by aiming to follow a
machine learning fashion of constructing and refining statistical models from
data. The role of the Bayesian approach in machine learning is very important
since the Bayesian approach provides a systematic way to infer unobserved
variables (e.g., classification category, model parameters, latent variables, model
structure) given data. This chapter limits the discussions considering the speech
and language problems in the latter chapters, by providing simple probabilistic
rules, and prior and posterior distributions in Section 2.1. The section also
provides analytical solutions of posterior distributions of simple models. Based
on the basic introduction, Section 2.2 introduces a useful representation of the
relationship of probabilistic variables in the Bayesian approach, called the
Graphical model. The graphical model representation gives us an intuitive view
of statistical models even when they have complicated relationships between
their variables. Section 2.3 explains the difference between Bayesian and
maximum likelihood (ML) approaches. The following chapters extend the
general Bayesian approach described in this chapter to deal with statistical
models in speech and language processing.
2.1   Bayesian probabilities
This section describes the basic Bayesian framework based on probabilistic
theory. Although some of the definitions, equations, and concepts are trivial, this
section reviews the basics to assist readers to fully understand the Bayesian

approach.
In the Bayesian approach, all the variables that are introduced when models
are parameterized, such as model parameters and latent variables, are regarded as
probabilistic variables. Thus, let a be a discrete valuable, then the Bayesian
approach deals with a as a probabilistic variable, and aims to obtain 
:
(2.1)
Hereinafter, we assume that a is a discrete variable, and the expectation is
performed by the summation over a for simplicity. Since 
 is a probabilistic
distribution, 
 always satisfies the following condition:
(2.2)
These properties help us to solve some calculations appearing in the following
sections. In the continuous variable case, the summation 
 is replaced with the
integral .
2.1.1
 Sum and product rules
Since the Bayesian approach treats all variables as probabilistic variables, the
probabilistic theory gives us the two important probabilistic rules to govern the
relationship between the variables. Let a and b be arbitrary probabilistic
variables,
Sum rule
(2.3)
Product rule
(2.4)

Here, 
 is a joint probability that represents a probability of all possible
joint events of a and b. 
 or 
 is a conditional probability. These are
generalized to N probabilistic variables, for example, if we have 
 probabilistic variables, there rules are represented as:
Sum rule
(2.5)
Product rule
(2.6)
In a Bayesian manner, we formulate the probability distributions based on these
rules. For example, the famous Bayes theorem can be derived by reforming the
product rule in Eq. (2.4), as follows:
(2.7)
(2.8)
To derive Eq. (2.8), we use the sum and product rules for 
. The following
discussion provides more practical examples based on this discussion.
2.1.2
 Prior and posterior distributions
The above Bayes theorem has an interesting meaning if we consider a
conditional probability distribution of a given an observation x. The conditional
distribution 
 is called posterior distribution, and the main purpose of the
Bayesian approach is to infer the posterior distribution of various valuables.

Based on the Bayes theorem in Eq. (2.7), the posterior distribution is
decomposed in Eq. (2.9) to the following three distributions:
(2.9)
(2.10)
where 
 is a likelihood function of x and 
 is a distribution without
considering any observation, and called prior distribution. 
 is a distribution
of an observation, and can be computed by using 
 and 
 based on
Eq. (2.10). In most speech processing applications, it is difficult to estimate the
posterior distribution directly (Section 3.8 describes it in detail). Therefore, the
posterior distribution 
 is indirectly estimated via this Bayes theorem,
which is derived from the sum and product rules, which are equivalence
equations without approximation.
Since the posterior distribution provides a probability of a given data x, this
matches one of the machine learning goals of refining information of a from data
x. Therefore, the posterior distribution plays an important role in machine
learning, and obtaining an appropriate posterior distribution for our problems in
speech and language processing is a main goal of this book.
Once we obtain the posterior distribution 
, we can obtain the values of
a via:
Maximum a-posteriori (MAP) procedure:
(2.11)
Expectation with respect to the posterior distribution:
(2.12)

The MAP and expectation are typical ways to obtain meaningful information
about a given x in terms of the probabilistic theory. From Eq. (2.10), 
 is
disregarded in the MAP procedure as a constant factor that is independent of a,
MAP, and expectation, which makes the calculation simple. The MAP and
expectation are generalized to obtain meaningful information 
 given the
posterior distribution 
. More specifically, if we consider a likelihood
function of unseen data y given a, i.e., 
, these procedures are rewritten as:
Maximum a-posteriori (MAP) procedure:
(2.13)
Expectation with respect to the posterior distribution:
(2.14)
Thus, we can predict y by using these procedures. Note that the MAP procedure
decides a deterministic value of a, while the expectation procedure keeps
possible a for the expectation. Therefore, the MAP procedure is called hard
decision and the expectation procedure is called soft decision.
The expectation is a more general operation than MAP in terms of
considering the distribution shape. For example, if we approximate 
 with a
specific Kronecker delta function 
 where 
, Eq.
(2.14) is represented as
(2.15)

where
(2.16)
Thus, the MAP value is obtained from the specific case of the expectation value
without considering the distribution shape of 
. However, in many cases,
the MAP value is also often used since the expectation needs a complex
computation due to the summation over a. Note that the above derivation via a
Kronecker delta function (or Dirac delta function when we consider continuous
variables) is often used to provide the relationship of the MAP and expectation
values.
2.1.3
 Exponential family distributions
The previous section introduces the posterior distribution. This section focuses
on a specific problem of posterior distributions that consider the model
parameter Θ given a set of D dimensional observation vectors, i.e., 
. The problem here is to obtain the posterior
distribution 
, i.e., it is a general estimation problem of obtaining the
distribution of Θ from data X. Once we obtain 
, for example, we can
estimate 
 or compute some expectation values, as we discussed in Section
2.1.2.
Then, the Bayes theorem, which provides the relationship between prior and
posterior distributions in Eq. (2.9) or (2.10), can be represented as follows:
(2.17)
(2.18)

Here, we use the integral  rather than 
 in Eq. (2.18), since model parameters
are often represented by continuous variables (e.g., mean and variance
parameters in a Gaussian distribution). In this particular case, the Bayes theorem
has the more practical meaning that the posterior distribution 
 is
represented by the likelihood function 
, and the prior distribution of the
model parameters 
. Thus
(2.19)
which is also called evidence function or marginal likelihood . The evidence
plays an important role in Bayesian inference, which is described in Chapter 5 in
detail.
Basically, we can set any distributions (e.g., Gaussian, gamma, Dirichlet,
Laplace, Rayleigh distributions, etc.) to prior and posterior distributions.
However, a particular family of distributions called conjugate distribution makes
analytical derivation simpler. Before we describe the conjugate distributions, the
following section explains exponential family distributions, which are required to
explain conjugate distributions.
Exponential family
The exponential family is a general distribution family, which contains standard
distributions including Gaussian distribution, gamma distribution, and
multinomial distribution. Let 
 be a vector form of model parameters. A
distribution of a set of observation vectors 
 given 
(likelihood function), which belongs to the exponential family, is represented by
the following exponential form:
(2.20)
where 
 is a sufficient statistics vector obtained from observation vector X, 
 is a logarithmic normalization factor.  is a transformed vector of , and is
called a natural parameter vector. If 
, it is called the canonical form,

that simplifies Eq. (2.20) as follows:
(2.21)
The canonical form makes the calculation of posterior distributions simple.
If we have J multiple parameter vectors, we can represent the exponential
form as the factorized form:
(2.22)
When the transformed model parameters are composed of a matrix or a vector,
we can also define the exponential family distribution. For example, a
multivariate Gaussian distribution 
 is parameterized by a mean vector 
 and a covariance matrix 
, and the corresponding transformed parameters are
also represented by vector 
 and matrix 
. Then, the exponential family
distribution for 
 is defined as follows:
(2.23)
Here, 
 and 
 are vector and matrix representations of sufficient statistics,
respectively. The rest of this section provides examples of 
, and 
for standard distributions (Gaussian, multivariate Gaussian, and multinomial
distributions).
Example 2.1 Gaussian (unknown mean):
We focus on the exponential family form of the Gaussian distribution for scalar
observation 
. As a simple example, we only focus
on the Gaussian mean as a model parameter, and regard the precision parameter
r as a constant value, i.e., 
 where the variables located right after the
semicolon; means that these are not treated as probabilistic variables, but

specific values. That is 
 in Eq. (2.20). We use the precision parameter r
instead of the variance parameter Σ,1 which makes the solution simple. Based on
the definition in Appendix C.5, the standard form of the Gaussian distribution is
represented as
(2.24)
We assume that 
 are independent and identically distributed
random variables from the Gaussian. The standard form of the Gaussian
distribution is rewritten as the following exponential form:
(2.25)
Thus, the Gaussian distribution is represented by the following exponential form
in Eq. (2.20):
(2.26)

Since 
 in Eq. (2.26), it is regarded as a canonical form, as discussed in
Eq. (2.21). Note that the parameterization of 
 and 
 is not unique. For
example, we can obtain the following parameterization from:
(2.27)
This is also another exponential form of the Gaussian distribution with unknown
mean.
Example 2.2 Gaussian (unknown mean and precision):
Similarly to Example 2.1, we focus on the exponential family form of the
Gaussian distribution for scalar observation X, but regard r as also unknown.
Therefore, 
. Thus, unlike the scalar forms of the natural parameter γ
and sufficient statistics t in Eq. (2.24), the Gaussian distribution is represented by
the vector form of these as
(2.28)
Therefore,

(2.29)
Again, the parameterization of 
 and 
 is not unique and the
parameterization of 
 and 
 is also possible.
Example 2.3 Multivariate Gaussian (unknown mean and precision):
The next example is to derive an exponential form of the multivariate Gaussian
distribution with D dimensional mean vector 
 and 
 precision matrix
R(we use precision matrix R instead of covariance matrix 
 to make the
solution simple). A set of the parameters is 
. This is the most
important example in this book, since statistical models in speech and language
processing are often represented by multivariate Gaussian distributions, as
discussed in Chapter 3. Let 
 be independent and
identically distributed random variables from the multivariate Gaussian
distribution. Again, based on the definition in Appendix C.6, the standard form
of the Gaussian distribution is represented as
(2.30)
Now we focus on the exponential part in Eq. (2.30), which is rewritten as

follows:
(2.31)
To make the observation vector and parameter the inner product form, we first
use the trace representation of the quadratic term of 
 as
(2.32)
where we use the fact that the trace of the scalar value is equal to the original
scalar value, the cyclic property, and the distributive property of the trace as in
Appendix B:
(2.33)
(2.34)
(2.35)
In addition, we can also use the following equation:

(2.36)
Here, since these values are scalar values, we use the following equation to
derive Eq. (2.36).
(2.37)
since the transpose of the scalar value is the same as the original scalar value (
) and R is a symmetric matrix (
). Thus, by substituting Eqs.
(2.32) and (2.36) into Eq. (2.31), Eq. (2.31) is rewritten as
(2.38)
Note that Eq. (2.38) is a useful form, and it is used in the following sections to
calculate the various equations for the multivariate Gaussian distribution.
Therefore, by substituting Eq. (2.38) into Eq. (2.30), we can obtain the
exponential form of the multivariate Gaussian distribution as follows:

(2.39)
Thus, by comparing with Eq. (2.23), we obtain the following parameterization
for the multivariate Gaussian distribution:
(2.40)
Note that if 
, we have 
, 
, 
, and Eq.
(2.40) is equivalent to Eq. (2.29).
Example 2.4 Multinomial distribution:
The standard form of the multinomial distribution (Eq. (C.2)) is represented as
follows:

(2.41)
where 
 is a non-negative integer, and
(2.42)
The parameter 
 has the following constraint:
(2.43)
Therefore, the number of the free parameters is 
. To deal with the
constraint, we first consider the 
 as the target vector parameters,
i.e., 
. 
 is represented by
(2.44)
Similarly to the previous Gaussian-based distributions, the multinomial
distribution is also represented as the exponential form as follows:
(2.45)
By using Eqs. (2.42) and (2.44) for 
 and 
, respectively, the exponential

part of Eq. (2.45) is rewritten as
(2.46)
where 
 denotes the proportional relation between left-and right-hand-side
equations. Since the probabilistic function has the normalization factor, which
can be neglected for most of the calculations, 
 is often used to omit the
normalization constant from the equations. Thus, we can derive the linear
relationship between 
 and 
, which is defined with 
 as follows:
(2.47)
Note that 
 is represented by 
 by using the following equation:
(2.48)
This is confirmed by substituting Eq. (2.47) into Eq. (2.48) as

(2.49)
Therefore, the canonical form of the multinomial distribution with the parameter 
 for 
 is represented as
(2.50)
Note that since the multinomial distribution has constraints for the observation 
 in Eq. (2.42) and the parameter 
 in Eq. (2.44), the obtained canonical form
of the multinomial distribution involves these constraints with 
 variables
for sufficient statistics t and the transformed vector .
The obtained exponential family forms for Gaussian, multivariate Gaussian, and
multinomial distributions are often used in the Bayesian treatment of statistical
models in speech and language processing.

2.1.4
 Conjugate distributions
The previous section introduces the exponential family distributions and
provides some examples of these distributions. Based on the exponential family
distributions, this section explains how to obtain the posterior distributions when
we use the exponential family distributions as the likelihood functions. For such
a distribution, we can find a nice property to obtain the posterior distribution
analytically if we set a particular type of distribution.
Let 
 be a likelihood function for a set of observation vectors 
. We first start the discussion from the simple case that the
parameters are represented as a vector form, i.e., 
. An exponential family
distribution of 
 is defined in Eq. (2.20) as
(2.51)
Here use 
 for simplicity. Then, we use the following Bayes theorem
for  based on Eq. (2.18) to calculate the posterior distribution 
:
(2.52)
where we disregard the normalization factor p(X). For this calculation, we need
to prepare a prior distribution 
. Instead of considering the prior distribution
of 
, we consider the prior distribution of 
. We set a prior distribution for
, which is parameterized with additional variables 
 and ϕ, where the
parameters of prior and posterior distributions are called hyperparameters. The
hyperparameter appearing in this book is often used as the parameter of prior or
posterior distributions. Then, the prior distribution is proportional to the
following function form:
(2.53)
Here, 
 is introduced in Eqs. (2.20) and (2.51) as a logarithmic normalization
factor of the likelihood function. This form of prior distribution is called
conjugate prior distribution.

We can calculate the posterior distribution of 
 via 
 by substituting
Eqs. (2.51) and (2.53) into Eq. (2.52):
(2.54)
where we use the definition used in the conjugate prior distribution (Eq. (2.53)).
This solution means that the conjugate posterior distribution is analytically
obtained with the same distribution function as the conjugate prior distribution
by just using the simple rule of changing hyperparameters from (
, ϕ) to (
, 
).
Note that the setting of ϕ is not unique. We consider the case that 
 is
decomposed into M functions, i.e.,
(2.55)
Then, similarly to Eq. (2.53), we can provide M hyperparameters for a prior
distribution as follows:
(2.56)
The corresponding posterior distribution is similarly derived by substituting
Eqs. (2.51), (2.55), and (2.56) into Eq. (2.52) as:

(2.57)
Thus, we can derive the posterior distribution with M hyperparameters. The
setting of 
 is an additional flexibility of the prior distribution. If we use
many 
, we could precisely represent a prior distribution. However, by using
a few 
, we can easily control the shape of a prior distribution with a few free
parameters.
If the transformed model parameters are composed of a vector 
 and matrix
, as discussed in Eq. (2.23), we also have similar result. A likelihood function
of this exponential family distribution is represented by the following general
form:
(2.58)
Here, similarly to Eq. (2.55), we use the following equation for the 
 function:
(2.59)
Therefore, by providing the following prior distribution form as a conjugate
prior with hyperparameters 
 and 
:

(2.60)
We can calculate the posterior distribution by substituting Eqs. (2.60), (2.58),
and (2.59) into Eq. (2.52):
(2.61)
Here we use the distributive property of the trace in Appendix B that:
(2.62)
Now, we summarize the conjugate prior and posterior distributions. The
exponential family distributions with the vector form parameters 
 have the
following relationship:
(2.63)
When the distribution has vector and matrix parameters, we have the following
relationship:
(2.64)
Therefore, the posterior distribution of the natural parameters (
) is
analytically obtained by using Eqs. (2.63) and (2.64) as a rule. The posterior
distribution of the original parameters 
 is obtained by transforming the
posterior distribution of the natural parameters.

The rest of this section provides examples of the conjugate prior and
posterior distributions for some exponential family distributions.
Example 2.5 Conjugate distributions for Gaussian (unknown mean):
We first describe the case that we only consider a Gaussian mean parameter μ,
and the precision parameter 
 is regarded as a constant value. Based on
the discussion in Example 2.1, the canonical form of the Gaussian distribution is
represented as follows:
(2.65)

where
(2.66)
Therefore, by substituting γ and 
 in Eq. (2.66) into the general form of the
conjugate distribution in Eq. (2.53), we can derive the function of mean μ as
follows:
(2.67)
Thus, the prior distribution of μ is represented by a Gaussian distribution with 
and 
 as the mean and precision parameters, respectively:
(2.68)
Based on the conjugate distribution rule (Eq. (2.63)), the posterior distribution is
easily solved by just replacing 
 and 
 in Eq. (2.68)
without complex calculations:

(2.69)
Therefore, similarly to the prior distribution, the posterior distribution of μ is
represented by a Gaussian distribution with 
 and 
 as the mean
and variance parameters, respectively:
(2.70)
Thus, both prior and posterior distributions are represented in the same form as a
Gaussian distribution with different parameters.
Now we consider the meaning of the solution of Eqs. (2.68) and (2.70). We
parameterize the φ and ν by newly introducing the following parameters:
(2.71)
Then, the prior and posterior distributions of μ in Eqs. (2.68) and (2.70) are
rewritten as:
(2.72)

where
(2.73)
These are famous Bayesian solutions of the posterior distribution of the
Gaussian mean. We can consider the two extreme cases that the amount of data
is zero or very large. Then, the posterior distribution is represented as:
N → 0
(2.74)
This solution means that we only use the prior information when we
don’t have data.
N 
 0
(2.75)
where 
 is the ML estimate of μ, and the posterior distribution is
close to the ML value with small standard deviation, which is similar
to the delta function that has a peak value at the ML estimate.
Thus, the solution of Eq. (2.72) approaches the delta function with the ML
estimate when the amount of data is very large and approaches the prior
distribution when the amount of data is very small. The mean parameter of the
posterior distribution,
(2.76)

is interpolated by the prior mean parameter 
 and the ML estimate, and 
 can
control an interpolation ratio.
Example 2.6 Conjugate distributions for Gaussian (unknown mean and
precision):
Similarly to Example 2.5, we first rewrite a Gaussian distribution. In this
situation, the set of the parameters is 
. From Eq. (2.29), the Gaussian
distribution with precision r has the following exponential form:
(2.77)
where we introduce 
 and 
 that are discussed in Eq. (2.55). The variables in
the above equations are represented as follows:
(2.78)
Therefore, by substituting 
, 
, and 
 in Eq. (2.78) into the general
form of the conjugate distribution in Eq. (2.56), we can derive the function of
mean μ and precision r as follows:

(2.79)
where we omit the factor that does not depend on r and μ. By making a complete
square form of μ, we can obtain a Gaussian distribution of μ with mean 
 and
precision 
:
(2.80)
Now we consider the rest of the exponential factor 
. By focusing on r and
using the definition of a gamma distribution (Appendix C.11), the factor is
rewritten as follows:
(2.81)
where the definition of a gamma distribution is as follows:

(2.82)
where 
 is a Gamma function (Appendix A.4). Thus, precision 
 is
represented by a gamma distribution with 
 and 
 as parameters.
This representation can be simplified by using the following definition for
the other definition of the gamma distribution 
 described in Eq.
(C.81) instead of the original gamma distribution defined in Eq. (C.74):
(2.83)
Equation (2.81) is rewritten as
(2.84)
Thus, the conjugate prior distribution is represented as the product form of
the following Gaussian and gamma distributions:
(2.85)
This can be also represented as a Gaussian-gamma distribution (or so-called
normal-gamma) defined in Appendix C.13, as follows:
(2.86)

The Gaussian-gamma distribution is a conjugate prior distribution of the joint
variable μ and r.
Similarly to the previous example, we introduce the following new
parameters:
(2.87)
By using Eq. (2.87), the conjugate prior distribution of Eq. (2.85) is rewritten by
using these new parameters as follows:
(2.88)
Note that we can also use Gaussian-gamma distribution as:
(2.89)
Thus, we can derive the prior distribution of joint variable μ and r as the product
of the Gaussian and gamma distributions in Eq. (2.88), or the single Gaussian-
gamma distribution in Eq. (2.89).
Now, we focus on the posterior distribution of μ and r. Based on the
conjugate distribution theory, the posterior distribution is represented as the same
form of the Gaussian-gamma distribution as the prior distribution (2.89) with
hyperparameters 
, and  as follows:

(2.90)
Based on the conjugate distribution rule (Eq. (2.63)), the hyperparameters of the
posterior distribution are easily solved by just replacing 
 and 
 in Eq. (2.87) without complex calculations, as follows:
(2.91)
Thus, we summarize the result of the hyperparameters of the conjugate posterior
distribution as
(2.92)
Note that in this representation, the posterior distribution parameters of 
 and 

are obtained by simply adding the number of observations N to the prior
distribution parameters of 
 and 
, respectively.
Finally, we summarize the result. The prior and posterior distributions of μ
and r in Eqs. (2.89) and (2.90) are also summarized as:
(2.93)

or
(2.94)
Similarly to the discussion about the mean parameter μ in Example 2.5, we can
consider the two extreme cases, that the amount of data is zero or very large, for
the behavior of the precision parameter solution r. The posterior distribution of r
is represented as:
 
(2.95)
This solution means that we only use the prior information when we don’t
have data.
 
(2.96)
Since the mean of the gamma distribution 
 (with  factor) is defined
in Eq. (C.83), the mean of r in this limit is represented as:
(2.97)

This is equivalent to the maximum likelihood estimation of 
represented as follows:
(2.98)
Thus, the mean of the posterior distribution approaches the ML estimate
of r when the amount of data is large.
Similarly, based on the definition of the variance of the gamma
distribution (with  factor) in Eq. (C.84), the variance is also represented
as
(2.99)
Note that the order of 
 in Eq. (2.98) is a constant for N, and the
variance of the precision parameter 
 approaches 0 , i.e., the
posterior distribution of r has a strong peak at 
 with a very small
variance. Therefore, the posterior distribution of precision parameter 
 in the case of a large amount of data can be approximated as the
following Dirac delta function with the ML estimate:
(2.100)
This conclusion is similar to the case of the large amount limitation of the

posterior distribution of mean parameter 
 in Eq. (2.75).
This Gaussian-gamma distribution is used to model the prior and posterior
distributions of Gaussian parameters (μ and r) for scalar continuous
observations, or can be used for vector continuous observations when we use a
diagonal covariance matrix.
Example 2.7 Conjugate distributions for multivariate Gaussian (unknown
mean vector and precision matrix):
Based on the discussion of Eq. (2.39) in Example 2.3, the canonical form of the
multivariate Gaussian distribution is represented as follows:
(2.101)

where
(2.102)
Therefore, by substituting 
, 
, and 
 in Eq. (2.102) into the
general form of the conjugate distribution in Eq. (2.60), we can derive the
function of mean 
 and 
 as follows:
(2.103)
where we omit the factor that does not depend on Rand 
. Similarly to Example
2.6, we first use a complete square form of 
 to derive a Gaussian distribution
from Eq. (2.103).
In Appendix B.4, we have the following formula for the complete square
form of vectors:

(2.104)

where
(2.105)
Therefore, by 
, 
, and 
 in Eqs. (2.104) and
(2.105), Eq. (2.103) is rewritten as follows:
(2.106)
Now we focus on the 
 term in Eq. (2.106). By using the matrix formula in
Appendix B, 
 is rewritten as
(2.107)
Thus, the conjugate prior distribution is rewritten as:
(2.108)
Therefore, Eq. (2.108) is represented as the following Gaussian–Wishart
distribution in Appendix C.15:

(2.109)

where
(2.110)
Thus, we can derive the prior distribution as the Gaussian–Wishart distribution.
Now, we focus on the posterior distribution of 
 and R. Similarly, the
posterior distribution is represented as the same form of the Gaussian–Wishart
distribution as the prior distribution (2.109), with hyperparameters 
, and 
 as follows:
(2.111)
Based on the conjugate distribution rule, Eq. (2.64), the hyperparameters of the
posterior distribution are easily solved by just replacing 
, 
, and 
 in Eq. (2.110) without complex
calculations, as follows:

(2.112)
Thus, we derive the posterior distribution that is also represented as a Gaussian–
Wishart distribution. Finally, the prior and posterior distributions of 
 and R in
Eqs. (2.109) and (2.111) are also summarized as:
(2.113)

or
(2.114)
Example 2.8 Conjugate distributions for multinomial distribution:
Based on the discussion of Eq.  (2.50) in Example 2.4, the canonical form of the
multivariate Gaussian distribution is represented as follows:
(2.115)

where
(2.116)
Note that we have the following constraints:
(2.117)
Therefore, by substituting 
, 
, and 
 in Eq. (2.78) into the general
form of the conjugate distribution in Eq. (2.56), we can derive the function of 
as follows:
(2.118)

where hyperparameters 
 are defined as follows:
(2.119)
Thus, the conjugate prior distribution is represented as a Dirichlet distribution
defined in Appendix C.4 as
(2.120)
Based on the conjugate distribution rule, Eq. (2.63), the hyperparameters of the
posterior distribution are easily solved by just replacing 
 and 
 in Eq. (2.118) without complex calculations, as follows:
(2.121)
where hyperparameters 
 are obtained as follows:
(2.122)
Thus, we derive the posterior distribution that is represented as a Dirichlet
distribution. Finally, the prior and posterior distributions of ω are given as:

(2.123)
Table 2.1 shows a recipe of the kind of distributions we use as a conjugate
prior.
Table 2.1 Conjugate priors.
Likelihood function
Unknown variable
Conjugate prior
Gaussian
Gaussian C.5
Gaussian
Gamma C.11
Gaussian
Gaussian–gamma C.13
Multivariate Gaussian
Multivariate Gaussian C.6
Multivariate Gaussian
Wishart C.14
Multivariate Gaussian
Gaussian–Wishart C.15
Multinomial
Dirichlet C.4
This section provides a solution of the posterior distribution for rather simple
statistical models. However, in practical applications, we still face the problems
of solving the equations, and often require the approximation to solve them
efficiently. The next section explains a powerful approximation method,
conditional independence, in Bayesian probabilities.
2.1.5
 Conditional independence
Another important mathematical operation of the Bayesian approach, as well as
the product and sum rules (Section 2.1.1), is called conditional independence.
Let a, b, and c be probabilistic variables, the conditional independence of a and
b on c is represented as follows:
(2.124)

(2.125)
This is a useful assumption for the Bayesian approach when factorizing the joint
probability distribution. For example, Eq. (2.124) based on the product rule
needs to consider 
 or 
. Suppose a, b, and c are discrete elements
of sets, i.e., 
, 
, and 
, 
 or 
 considers the
probability of all combinations of a, b, and c, which correspond to 
. The number of combinations is increased exponentially, if the
number of valuables is increased. Therefore, it is computationally very
expensive to obtain the conditional distribution, and almost impossible to
consider large amounts of data as probabilistic valuables. Thus, the conditional
independence approximation in Eq. (2.125) greatly reduces the computational
complexity, and makes the Bayesian treatment of speech and language
processing tractable.
By using the product rule, the conditional independence equation is rewritten
as follows:
(2.126)
Thus,
(2.127)
is also equivalently used as the conditional independence assumption of
Eq. (2.125).
The conditional independence is often used in the following sections to make
the complicated relationship between probabilistic variables simple. For
example, speech recognition has many probabilistic variables which come from
acoustic and language models. It is very natural and effective to assume
conditional independence between acoustic model and language model variables
because these do not depend on each other explicitly.

Example 2.9 Naive Bayes classifier:
One of the simplest classifiers in the machine learning approach is the naive
Bayes classifier. The approach is used for many applications including document
classification (Lewis 1998, McCallum & Nigam 1998). For example, if we have
N data (
), and want to classify the data to a specific
category , this can be performed by using the posterior distribution of category
c as follows:
(2.128)
The naive Bayes classifier approximates this posterior distribution with the
product rule and conditional independence assumption as follows:
(2.129)
This approach approximates the posterior distribution 
 with the
product of likelihood 
 for all samples and prior distribution 
. Since
the naive Bayes classifier is very simple and easy to implement, it is often used
as an initial attempt of the machine learning approach if we have training data
with labels to obtain 
 for all c. For example, in document classification, a
multinomial distribution is used to represent the likelihood function 
.
2.2   Graphical model representation
The previous sections (especially Sections 2.1.1 and 2.1.5) discuss how to
provide the mathematical relationship between probabilistic variables in a
Bayesian manner. This section briefly introduces a graphical model
representation that visualizes the relationship between these probabilistic
valuables to provide a more intuitive way of understanding the model. A

graphical model framework is also widely used in Bayesian machine learning
studies, and this book introduces basic graphical model descriptions, which are
used in the following sections.
2.2.1
 Directed graph
First, we simply consider the following joint distribution of a and b, which can
be rewritten as the following two factorization forms based on the product rule:
(2.130)
(2.131)
Therefore, to obtain the joint distribution, we compute either Eq. (2.130) or
(2.131) depending on the problem. The graphical model can separately represent
these factorization forms intuitively.
Figure 2.1 Graphical models of 
 and 
.
Figure 2.1 represents the graphical models of 
 and 
,
respectively. The node represents a probabilistic variable, and the directed link
represents the conditional dependency of two probabilistic variables. For
example, Eq. (2.130) is composed of the conditional distribution 
 and then
the corresponding graphical representation provides the directed link from node
a to node b in Figure 2.1(a). Conversely, the conditional distribution 
 in
Eq. (2.131) is represented by the directed link from node b to node a in Figure
2.1(b).
Thus, the graphical model specifies a unique factorization form of a joint
distribution, intuitively. The graph composed of the directed link, which
represents the conditional distribution, is called a directed graph. The graphical

model can also deal with an undirected graph, which is a graphical
representation of a Markov random field, but this book focuses on the directed
graph representation, which is often used in the later applications.
2.2.2
 Conditional independence in graphical model
As we discussed in Section 2.1.5, practical applications often need some
approximations in the dependency of probabilistic variables to avoid a
complicated dependency of the factorized distribution. We can represent this
approximation in the graphical model representation. If we consider the joint
distribution of a, b, and c, the joint distribution is, for example, represented as
the following factorization form based on the product rule:
(2.132)
The graphical model of Eq. (2.132) is represented in Figure 2.2(a). Note that all
nodes are connected to each other by directed links. This graph is called a full
connected graph.
On the other hand, the joint distribution with the following conditional
independence can also be represented as a graphical model in Figure 2.2(b):
(2.133)
Note that the link between a and b has disappeared from Figure 2.2(b). Thus, the
conditional independence in the graphical model is represented by pruning links
in the graphs, which corresponds to reducing the dependencies in probabilistic
variables, and leads to reduced computational cost.

Figure 2.2 Graphical models of 
 and 
.
In real applications, we need to consider large numbers of variables. For
example, the naive Bayes classifier introduced in Example 2.9 has to consider 
 probabilistic variables ( 
 and c):
(2.134)
The graphical model of this case can be simplified from Figure 2.3(a) to 2.3(b)
by using the plate. Based on the plate, we can represent a complicated
relationship of probabilistic variables intuitively. In Section 8.2, we also consider
the case when the number of probabilistic variables is dealt with as infinite in
Bayesian nonparametrics. Then, the number of variables can be represented by
using 
 in a graphical model, as shown in Figure 2.4.
Figure 2.3 Graphical model of 
.

Figure 2.4 Graphical model of 
.
Thus, the graphical model can represent the dependencies of variables based
on the product rule and conditional independence graphically. This dependency-
network-based Bayesian method is also called a Bayesian network. In particular
the Bayesian treatment that considers the dynamical relationship between
probabilistic 
variables 
is also 
called 
a 
dynamic 
Bayesian 
network
(Ghahramani 1998, Murphy 2002). A dynamic Bayesian network provides
efficient solutions to the time-series statistical models similarly to HMM and
Kalman filters, which are also used in speech recognition (Zweig &
Russell 1998, Nefian, Liang, Pi et al. 2002, Livescu, Glass & Bilmes 2003). It is
helpful to understand probabilistic models, even when they are very complicated
in the equation form.
2.2.3
 Observation, latent variable, non-probabilistic variable
Previous sections deal with the graphical model of all probabilistic variables.
However, our machine learning problems for speech and language processing
have three types of variables: observation, latent variables, and non-probabilistic
variables. For example, let x be an observation, z is a latent variable, and θ is a
model parameter, which we don’t deal with as a probabilistic variable in this
section, unlike the full Bayesian approach. The probability distribution of x is
represented as follows:
(2.135)

The corresponding graphical model is represented in Figure 2.5(a). Note that
three variables x, z, θ have different roles in this equation. For example, x is a
final output of this equation as an observation, which is not marginalized, while z
is a latent variable and should be marginalized. To distinguish the observation
and latent variables, the node representing an observation is tinted. θ is not a
probabilistic variable in this explanation, and so it is put in the graph without a
circle.
Figure 2.5 Graphical models that have observation x, latent variable z, model
parameter θ, and hyperparameter 
. Part (a) treats θ as a non-probabilistic
variable, and (b) treats θ as a probabilistic variable to be marginalized.
Similarly, if we consider the same model, but treat θ as a probabilistic
variable, θ is marginalized by the prior distribution of θ with hyperparameter 
.
The probability distribution of x is represented as follows:
(2.136)
Here we assume θ to be a continuous variable, and use the integral instead of the
summation. We can regard θ as a latent variable in a broad sense, but the other
sections distinguish the model parameters and latent variables. The
corresponding graphical model is represented in Figure 2.5(b). Thus, by using

the representations of observation, latent variables, and non-probabilistic
variables, we can provide graphical models of various distributions other than
joint distributions. These are basic rules of providing a directed graphical model
from the corresponding probabilistic equation.
The directed graph basically describes how observation variables are
generated conditioned on the other probabilistic variables. This statistical model
of describing the generation of observation variables is called a Generative
model. HMM, GMM, Kalman filter, n-gram, latent topic model, and deep belief
network are typical examples of generative models that can generate speech
feature vectors and word sequences. The next section also introduces another
way of intuitively understanding our complicated statistical models by
describing how observation variables are generated from the distributions in our
models.
2.2.4
 Generative process
This section also explains another representation of the Bayesian approach based
on the generative process. This representation is used to generate the
probabilistic variables in an algorithmic way. The generative process is used to
express the joint distribution. The basic syntax of the generative process is as
follows:
Non-probabilistic variables: placed as “require”;
Latent variables: “drawn” from their probability distribution;
Model parameters: “drawn” from their (prior) probability distribution;
Observations: finally “drawn” from their probability distribution given
sampled latent variables and model parameters.
If we also want to represent the marginalization of a probabilistic variable, we
can use an additional syntax “Average” for the marginalization.
As an example of Eq. (2.136), Algorithm 1 represents the generative process
of the joint distribution 
, which is represented as:
(2.137)

where x, z, θ, and 
 are observations, latent variables, model parameters, and
hyperparameter (non-probabilistic variables), respectively.
Algorithm 1 Generative process of 
Require:  
  1:  Draw θ from 
 
  2:  Draw z from 
 
  3:  Draw x from 
This generative process also helps us to understand models intuitively by
understanding how probabilistic variables are generated algorithmically.
Therefore, both the generative process and graphical model are often provided in
the Bayesian approach to represent a complicated generative model. In some of
the statistical models used in this book, we provide the generative process and
graphical model to allow readers to understand the models intuitively.
2.2.5
 Undirected graph
Another example of a graphical model is called an undirected graph (Figure 2.6),
that represents the relationship of probabilistic variables but does not have
explicit parent–child relationships compared with the directed graph. The
network is called a Markov random field, and the probabilistic distribution is
usually expressed by a potential function 
 (a positive, but otherwise
arbitrary, real-valued function):
(2.138)
where Z is a normalization constant of this distribution, and is called the
partition function for this special case. This approach is often used as a context
of a log linear discriminative model, where 
 is a linear function of a
feature obtained by a, b, and c and the corresponding weight.

Figure 2.6 Undirected graph.
A factor graph is another class of graphical model representing the
conditional independence relationship between variables. Actually, the factor
graph can provide a more concrete representation of the joint distribution of
variables than that of the undirected graph. The factor graph introduces
additional square nodes to a graph, which can explicitly represent the
dependency of several variables.
For example, the partition function can be represented by several cases, as
shown in Figures 2.7 and 2.8. Both graphs are fully connected and can represent
the joint distribution of a, b, and c. However, the partition function of Figure 2.7
is computed by using the three pairs of partition functions as follows:
(2.139)
The possible partition functions are 
. On
the other hand, Figure 2.8 considers the potential function of the joint event for
a, b, and c:
(2.140)
The possible partition functions are 
. Therefore, if the number
of possible variables (
, 
, and 
) is very large, Figure 2.7 is a more
compact representation since the number of possible functions would be smaller.

Figure 2.7 Factor graph.
Figure 2.8 Factor graph.
Thus, factor graphs are more specific about the precise form of the
factorization of undirected graphs, and can be used to mainly represent some
discriminative models (logistic regression, conditional random field (Lafferty,
McCallum & Pereira 2001)). This book generally deals with generative models
(HMM, GMM, n-gram, and latent topic model), and does not deal with these
discriminative models. However, there are several important applications of
discriminative models to speech and language processing (e.g., Gunawardana,
Mahajan, Acero et al. 2005, Fosler & Morris 2008, Zweig & Nguyen 2009,
Gales et al. 2012) in addition to the recent trend of deep neural networks (Hinton
et al. 2012). We present an example of a Bayesian treatment of neural network
acoustic models in Section 6.4. The fully Bayesian treatment of the other
discriminative models in speech and language processing is an interesting future
direction.
2.2.6
 Inference on graphs
One of the powerful advantages of the graphical model representation is that
once we fix a graphical model, we can infer all variables in the graph efficiently
by using belief propagation if the graph does not have a loop.

For example, belief propagation provides a sum product algorithm that can
efficiently compute the distribution 
 of the probabilistic variable in an
arbitrary node by using message passing. In the HMM case, this sum product
algorithm corresponds to the forward–backward algorithm, as discussed in
Section 3.3.1. Similarly, belief propagation provides a max sum algorithm that
can efficiently compute the arg max value (
) in an arbitrary
node by using message passing. Similarly to the sum product algorithm, the max
sum algorithm corresponds to the Viterbi algorithm, as discussed in Section
3.3.2. A detailed discussion about the relationship between the forward–
backward/Viterbi algorithms in the HMM and these algorithms can be found in
Bishop (2006).
However, most of our applications have a loop in a graph, and we cannot use
the exact inference based on the above algorithms. The following chapters
introduce the approximations of the Bayesian inferences, and especially
variational Bayes (VB), as discussed in Chapter 7, and Markov chain Monte
Carlo (MCMC), as discussed in Chapter 8, these being promising approaches to
obtain approximate inferences in a graphical model. Actually, progress of the
graphical model approach has been linked to the progress of these Bayesian
inference techniques.
The other approximated approach to inference in a graphical model that
contains cycles or loops is to use the sum-product algorithm for the graph even
though there is no guarantee of convergence. This approach is called loopy belief
propagation, and it is empirically known that it is convergent in some
applications.
2.3   Difference between ML and Bayes
As discussed in previous sections, the Bayesian approach deals with all variables
introduced for modeling as probabilistic variables. This is the unique difference
between the Bayesian approach and the other standard statistical framework, the
Maximum Likelihood (ML) approach. Actually this difference can yield various
advantages over ML. This section overviews the advantage of the Bayesian
approach over the ML approach in general. We discuss this, along with a general
pattern recognition problem, as we consider practical speech and language

processing issues in the following chapters. For comparison, we summarize the
difference between the approaches in terms of model setting, training, and
classification.
Model setting
ML: 
Generative model distribution 
.
Bayes: 
Generative model distribution 
 
Prior distributions 
 and 
.
In addition to the generative model distribution, the Bayesian approach
needs to set prior distributions.

Training
ML: Point estimation
.
Bayes: Distribution estimation
 and 
.
ML point-estimates are given by the optimal values 
 by using the EM
algorithm generally when the model has latent variables, while the
Bayesian approach estimates posterior distributions. In addition, ML only
focuses on model parameters Θ, but the Bayesian approach focuses on both
model parameters Θ and model M.

Classification
ML:
(2.141)
Bayes:
(2.142)
Here,  is obtained in the ML training step, and 
 is usually set in advance
by an expert or optimized by evaluating the performance of model M using
a development set. Equation (2.142) is obtained by the probabilistic sum
and product rules and conditional independence, as discussed in the
previous sections. Compared with ML, the Bayes approach marginalizes Θ
and M through the expectations of the posterior distributions 
and 
, respectively.
Thus, the main differences between ML and Bayes are (i) use of prior
distributions, (ii) use of distributions of model M, (iii) expectation with respect
to probabilistic variables based on posterior distributions. These differences
yield several advantages of the Bayesian approach over ML. The following
sections describe the three main advantages.
2.3.1
 Use of prior knowledge
First, we describe the most famous Bayesian advantage over ML based on the
use of prior knowledge. Figure 2.9 depicts this advantage focusing on the
estimation of model parameters (the mean and variance of a Gaussian). The
dashed line shows the true distribution, and the solid line shows the estimated
Gaussian distributions based on ML and Bayes. If data to be used to estimate
parameters are not sufficient and biased to the small peak of the true distribution,
ML tends to estimate the wrong parameter by using the biased data. This is
because ML estimates only consider the likelihood function, which leads it to

estimate the parameters that generate the observed data:
(2.143)
Thus, ML can correctly estimate parameters only when the amount of data is
sufficient.
Figure 2.9 Use of prior knowledge.
On the other hand, the Bayesian approach also considers the prior
distribution of model parameters 
. Now, we consider point estimation
using the maximum a-posteriori (MAP) value, instead of considering the
Bayesian distribution estimation for simply comparing the prior effect with ML.
For example, based on Eq. (3.345), the MAP estimate of Θ is represented as
follows:
(2.144)
The result considers the prior distribution as a regularization term. So if we set a
constraint on a distribution form by appropriate prior knowledge, we can recover
a wrong estimation due to the sparse data problem in ML, and we can estimate
the parameter correctly. Details are discussed in Chapter 4.
2.3.2
 Model selection

The model selection is a unique function of the Bayesian approach, which
determines a model structure from data automatically. For example, Figure 2.10
shows how many Gaussians we use to estimate the parameters. It is well known
that likelihood values always increase as the number of parameters increases.
Therefore, if we enforce use of the ML criterion for the model selection, ML
tends to select too many Gaussians, which results in over-fitting. There are some
extensions of ML to deal with model selection based on information criteria
(e.g., Akaike 1974, Rissanen 1984). However, in most cases of speech and
language processing, the ML framework usually optimizes model structure by
evaluating the performance of the model using a development set. The
development set is usually obtained from a part of training/test data. Although
this optimization is straightforward, it is very difficult to use in some of the
applications when the performance evaluation is difficult (e.g., it has a large
computational cost for evaluation or there is no objective performance measure).
Figure 2.10 Model selection.
The Bayesian approach can deal with model selection within the framework.
For example, our Bayesian approach to acoustic modeling involves the posterior
distribution of model 
, which will be described in Section 3.8.7. Once
we obtain the posterior disquisition, we can select an appropriate model structure
in terms of the MAP procedure introduced in Section 2.1.2, as follows:
(2.145)
Thus, an appropriate model structure (e.g., the topology of HMMs (Stolcke &
Omohundro 1993, Watanabe, Minami, Nakamura et al. 2004)) can be selected
according to training data, without splitting them to create development data.

Instead of using the MAP procedure, we can use expectation based on 
. This is stricter in the Bayesian sense, and Eq. (2.142) actually includes
the expectation over the posterior distribution of models. This approach
corresponds to using multiple models with different model structures to classify
unseen categories. However, in terms of the computational costs (needs large
memory and computational time for the multiple model case), people usually
carry out model selection by using the MAP procedure.
We also note that M involves other model variations than model structure as
elements. For example, hyperparameters introduced in the model can be
optimized by using the same MAP procedure or marginalized out by using the
expectation. In particular, the optimization of hyperparameters through the
posterior distributions of M is also a powerful example of the Bayesian
advantage over ML.
2.3.3
 Marginalization
The final Bayesian advantage over ML is the marginalization effect, which was
discussed in the expectation effect of the Bayesian approach in the previous
section. Since the stochastic fluctuation in the expectation absorbs estimation
errors, the marginalization improves the robustness in estimation, classification,
and regression over unknown data. In Figure 2.11, the left figure shows the
maximum likelihood based distribution with , while the right figure shows the
example of marginalization over model parameters Θ in the likelihood function 
 given a model structure M. Since the right figure considers the
probabilistic fluctuations of Θ by the posterior distribution 
, the
marginalized function (expected with respect to Θ) can mitigate the error effects
in estimating Θ with the variance, and make the likelihood function robust to
unseen data. The marginalization can be performed to all probabilistic variables
in a model including latent variables Z, model structure M (and
hyperparameters), in addition to the model parameter Θ example in Figure 2.11,
if we obtain the prior/posterior distributions of these probabilistic variables.

Figure 2.11 Marginalization over a set of model parameters Θ.
The marginalization is another unique advantage of the Bayesian approach
over ML, whereby incorporating the uncertainty of variables introduced in a
model based on probabilistic theory achieves robustness for unseen data.
However, it requires an expectation with respect to variables that essentially
needs to consider the integral or summations over the variables. Again, this is
the main difficulty of the practical Bayesian approach, and it needs some
approximations especially to utilize the Bayesian advantage of this
marginalization effect.
Although marginalization is not usually performed for observation O,
observation features in speech and language processing often include noises, and
marginalization over observation features is effective for some applications. For
example, if we use a speech enhancement technique as a front-end denoising
process of automatic speech recognition, the process inevitably includes noise
estimation errors, and the errors can be propagated to speech recognition, which
degrades the performance greatly. The approach called uncertainty techniques
tries to mitigate the errors by using the following Bayesian marginalization of
the conventional continuous-density HMM (CDHMM) likelihood function over
observation features O (Droppo, Acero & Deng 2002, Delcroix, Nakatani &
Watanabe 2009, Kolossa & Haeb-Umbach 2011):
(2.146)
The main challenges of the uncertainty techniques are how to estimate feature
uncertainties 
 (the distribution of observation features 
 with
hyperparameter 
) and how to integrate the marginal likelihood function with
the decoding algorithm of the HMM. The approaches have been successfully

applied to noisy speech recognition tasks, and show improvements by mitigating
the error effects in speech enhancement techniques (Barker, Vincent, Ma
et al. 2013, Vincent, Barker, Watanabe et al. 2013).
Thus, we have explained the three main practical advantages of the Bayesian
approaches. Note that all of the advantages are based on the posterior
distributions, and obtaining the posterior distributions for our target applications
is a main issue of the Bayesian approaches. Once we obtain the posterior
distributions, Bayesian inference allows us to achieve robust performance for
our applications.
2.4   Summary
This chapter introduces the selected Bayesian approaches used for speech and
language processing by starting from the basic Bayesian probabilistic theory
with graphical models, and concludes with a summarization of the Bayesian
advantages over ML. The discussion is rather general, and to apply Bayesian
approaches to our practical problems in speech and language processing, we still
need to bridge a gap between the theoretical Bayesian approaches and these
practical problems. This is the main goal of this book. The next chapter deals
with basic statistical models used in speech and language processing based on
ML, and it will be extended in the latter chapters toward this main goal.
1  This book regards Σ as the variance parameter (not the standard deviation, which is represented as σ), as
shown in Appendix C.5, to make the notation consistent with the covariance matrix 
.

3   Statistical models in speech and language
processing
This chapter focuses on basic statistical models (Gaussian mixture models
(GMM), hidden Markov models (HMM), n-gram models and latent topic
models), which are widely used in speech and language processing. These are
well-known generative models, and these probabilistic models can generate
speech and language features based on their likelihood functions. We also
provide parameter-learning schemes based on maximum likelihood (ML)
estimation which is derived according to the expectation and maximization (EM)
algorithm (Dempster et al. 1976). Basically, the following chapters extend these
statistical models from ML schemes to Bayesian schemes. These models are
fundamental for speech and language processing. We specifically build an
automatic speech recognition (ASR) system based on these models and extend
them to deal with different problems in speaker clustering, speech verification,
speech separation and other natural language processing systems.
In this chapter, Section 3.1 first introduces the probabilistic approach to
ASR, which aims to find the most likely word sequence W corresponding to the
input speech feature vectors O. Bayes decision theory provides a theoretical
solution to build up a speech recognition system based on the posterior
distribution of the word sequence 
 given speech feature vectors O. Then
the Bayes theorem decomposes the problem based on 
 into two
problems based on two generative models of speech features 
 (acoustic
model) and language features p(W) (language model), respectively. Therefore,
the Bayes theorem changes the original problem to these two independent
generative model problems.
Next, Section 3.2 introduces the HMM with the corresponding likelihood
function as a generative model of speech features. The section first describes the

discrete HMM, which has a multinomial distribution as a state observation
distribution, and Section 3.2.4 introduces the GMM as a state observation
distribution of the continuous density HMM for acoustic modeling. The GMM
by itself is also used as a powerful statistical model for other speech processing
approaches in the later chapters. Section 3.3 provides the basic algorithms of
forward–backward and Viterbi algorithms. In Section 3.4, ML estimation of
HMM parameters is derived according to the EM algorithm to deal with latent
variables included in the HMM efficiently. Thus, we provide the conventional
ML treatment of basic statistical models for acoustic models based on the HMM.
From Section 3.6, we go on to describe statistical language models as a
generative model of language features. As a standard language model, we
introduce n-gram models. Similarly to the HMM parameters, the n-gram
parameters are also calculated by using the ML estimation. However, ML
solutions to n-gram parameters are easily overestimated due to the intrinsic
sparse data problems in natural languages. Therefore, the section also describes
conventional (rather heuristic) smoothing techniques. Some of the smoothing
techniques introduced here are revisited in later chapters to be interpreted as the
Bayesian approach, where a Bayesian principle provides these smoothing
techniques to regularize these models. In addition, it is well known that the
number of n-gram parameters is exponentially increased with large n, which
makes it impossible to model a whole document structure within the n-gram
model.
Section 3.7 provides another generative model of language features, called
the latent topic model, which deals with a statistical treatment of a document
model. The section also discusses a way of combining such document models
and n-gram models.
Finally, following the discussions of statistical acoustic and language models,
Section 3.8 provides an example of applying the Bayesian approach to ASR, as a
case study. The section provides an exact Bayesian manner of formulating the
standard statistical model (HMM) in ASR, and introduces the posterior
distributions of the variables used in acoustic models. The section points out the
problem arising mainly due to the posterior distributions, which can be solved in
the later chapters.

3.1   Bayes decision for speech recognition
This section describes a statistical speech recognition framework as an example
of speech and language processing based on the Bayes decision theory. Before
the Bayes decision discussion, we first introduce general speech recognition
briefly.
Automatic speech recognition aims to extract helpful text information from
speech signals, where both speech and text are represented by sequential
patterns. These patterns correspond to the time-series signals which are observed
in sequences of random variables. Speech recognition is processed in a temporal
domain. There are some other technical data, e.g., music signal, video signal,
text document, seismic signal, gene sequence, EEG signal, ECG signal, financial
data, which are also collected in a time domain. The sequential pattern property
is unique and different from data analysis in a spatial domain for general image
processing and spectral signal processing. In general, speech and text data are
driven under some specialized stochastic process and probabilistic model, e.g.,
HMMs (Rabiner & Juang 1986) are used to represent speech signals and n-gram
models are used to characterize word sequences. In front-end processing, speech
signals are first chunked into different time frames t with frame length 25 ms and
then transformed to a sequence of speech feature vectors by using mel-frequency
cepstral coefficients (MFCCs) (Davis & Mermelstein 1980) or perceptual linear
prediction (PLP) coefficients (Hermansky 1990). Each feature vector 
 is
regarded as a random vector consisting of entries with continuous value.
However, the nth word 
 in a word sequence is a discrete value or label
among all words in a dictionary 
 with vocabulary size 
. Thus, speech
recognition, which involves acoustic and language models, handles the modeling
of continuous data as well as discrete data.
Now we provide a mathematical notation for a speech recognition problem,
which recognizes a speech utterance and outputs the corresponding word
sequence in the utterance. Let 
 be a D dimensional feature vector at
frame t, and 
 be a speech feature sequence for T frames
of one utterance. The number of dimensions (D) is usually 39, which consists of
12 dimensional MFCCs + log power, with delta and delta delta coefficients
(Furui 1981). On the other hand, the corresponding word sequence is represented
by 
. Here, 
 is the nth word in this word

sequence with N words. The continuous-valued speech feature sequence O and
the discrete-valued word sequence W are sequential patterns in an automatic
speech recognition system. Based on the mathematical notations of the speech
feature and word sequences, the Bayes decision theory is introduced to find a
decision rule or mapping function 
 which maps an input feature sequence O
into an output word sequence W by
(3.1)
A popular decision rule is designed to find the most likely word sequence 
corresponding to input feature sequence O based on the maximum a-posteriori
(MAP) decision rule,
(3.2)
where 
 is the posterior distribution of W given O. The posterior
distribution is often rewritten as
(3.3)
The probabilistic product rule decomposes the posterior distribution into
likelihood function 
 and prior probability p(W) based on acoustic model
and language model, respectively. This is a well-known process, called the noisy
channel model, that can deal with a speech recognition problem based on
acoustic and language models. The same scheme of decomposition of this
decision rule is widely used for other speech and language processing including
machine translation (Brown, Cocke, Pietra et al. 1990, Brants, Popat, Xu
et al. 2007), spell correction (Brill & Moore 2000), and voice conversion (Saito,
Watanabe, Nakamura et al. 2012).
However, it is more general to follow a Bayesian perspective for pattern

recognition and fulfil an optimal Bayes decision to estimate the decision rule 
 of an input sentence O by minimizing the expected loss function or Bayes
risk, which is defined by Lee & Huo (2000) as a functional of the decision rule:
(3.4)
where 
 denotes the expectation function over joint distribution 
.
For the later derivations, Eq. (3.4) provides the two equivalent equations in the
third and fourth lines by using the product rule. The loss function satisfies this
property:
(3.5)
meaning that the loss due to misclassification 
 is larger than or equal
to the loss without misclassification 
. Therefore, the optimal decision
rule 
 can be obtained by minimizing the Bayes risk, which corresponds to
minimizing the expected loss function.
Bayes risk is expanded into two expressions, which are shown in the third
and fourth equations in the right-hand-side of Eq. (3.4). Following the third
equation in Eq. (3.4), we find that Bayes decision rule is equivalent to dealing
with a minimization problem:
(3.6)

Here 
 denotes a set of all possible decision functions. This optimization can
be solved by minimizing the expression in the brackets in the above equation,
since the decision rule function does not depend on O in general. This
minimization is satisfied by considering the following optimal decision rule
given any O:
(3.7)
That is, finding the optimal decision rule 
 in terms of minimizing the
expected loss (in Eq. (3.4)) is equivalent to finding the optimal decision rule
function in terms of the expected loss function given O.
In (Goel & Byrne 2000, Chien, Huang, Shinoda et al. 2006), a minimum
Bayes risk (MBR) classification was proposed to fulfil an optimal Bayes
decision in Eq. (3.7) for automatic speech recognition by using a predefined loss
function. In Goel & Byrne (2000), the word error rate (WER) loss function 
 was calculated using Levenshtein distance between word sequence
hypotheses. This function was used to build an MBR decision rule:
(3.8)
which is derived according to the fourth equation in the right-hand-side of
Eq. (3.4).
More popularly, a meaningful loss function for speech recognition is simply
specified by a so-called zero-one loss function which treats misclassification of
each observation sample O equally, namely by using

(3.9)
Substituting Eq. (3.9) into the fourth line in Eq. (3.4) leads to zero-one Bayes
risk:
(3.10)
In the third line of Eq. (3.10), the expectation operation using zero-one loss
function 
 is calculated over all observations which are wrongly
classified, i.e., 
. This loss function corresponds to unconditional error
probability, which is reasonable to act as a measure of goodness of the decision
rule for speech recognition.
In addition, Eq. (3.10) is further rewritten as
(3.11)
by using the following properties:
(3.12)

(3.13)
The resulting decision rule 
 follows the minimum classification error
criterion which leads to the MAP decision rule as addressed in Eq. (3.2), i.e.,
(3.14)
The most likely word sequence 
 is found so as to achieve the highest posterior
probability for correctly classified observation sequence 
.
Using an MAP decision rule, the probability measure 
 calculates
how likely the acoustic observation sequence O is, based on the word sequence
hypothesis W. We also name 
 as the acoustic likelihood function. There
are many kinds of acoustic models which are assumed in calculation of the
statistical model 
 based on a set of acoustic parameters Θ. In this
chapter, the hidden Markov model (HMM) is considered for acoustic modeling
and the HMM parameters Θ are plugged into the probability measure estimator 
. On the other hand, the probability measure p(W) is defined as the
prior probability of word sequence W. This measure calculates the joint
probability for a sequence of words based on a set of multinomial parameters or
n-gram parameters Θ. The plug-in language model 
 is determined as a
language model estimator. Here, acoustic parameters and linguistic parameters
are both included in model parameters Θ. For an automatic speech recognition
(ASR) system, we will estimate the acoustic parameters and the linguistic
parameters from a set of training utterances O and their word transcriptions W
according to the Maximum Likelihood (ML) estimation. We assume that these
plug-in models 
 given ML parameters Θ are true. The
prediction of new test utterance O based on the estimated MAP decision rule 
 is performed by
(3.15)

However, the point estimates of the ML-based acoustic model and language
model Θ from the given observation space 
 using the collected training data
may not generalize well for the unknown test data outside the training space 
.
The distributions 
 and p(W) may not be correctly assumed or may be
over-trained or under-trained. From a Bayesian perspective, these issues could be
tackled by treating acoustic parameters and linguistic parameters Θ as random
variables. Consideration of these uncertainties is helpful for recognition of new
test data. For this consideration, the expected loss function in Eq. (3.4) is
calculated by additionally marginalizing over continuous parameters Θ:
(3.16)
The Bayesian predictive classification (BPC) rule,
(3.17)
(Jiang, Hirose & Huo 1999, Huo & Lee 2000, Lee & Huo 2000, Chien &
Liao 2001) was proposed to establish a robust decision rule for unknown test
speech. The zero-one loss function 
 was applied in previous studies on the
BPC rule. Details of BPC-based speech recognition will be addressed in Section
6.3.
In addition, Bayes decision theory was developed to estimate the
discriminative acoustic model for speech recognition (Juang & Katagiri 1992).
The idea is to minimize the expected loss function based on the logistic sigmoid
function,
(3.18)
which is a function of misclassification measure defined as

(3.19)
In Eq. (3.18), α is a tuning parameter and the misclassification measure of
phone class 
 from training data 
 is calculated by measuring the
distance between the discriminant functions 
 of the target phone 
 and its competing phones 
. The discriminant function of
competing phones is averaged over all phones except the target phone. The
discriminative acoustic model was trained according to the minimum
classification error (MCE) criterion which is closely related to the minimum
Bayes risk for optimal Bayes decision (Juang & Katagiri 1992).
3.2   Hidden Markov model
The previous section describes the Bayes decision theory and introduces acoustic
model 
 and language models p(W). This section describes hidden
Markov models (HMMs) (Rabiner & Juang 1986) (Figure 3.1) as a standard
statistical acoustic model in detail. Before describing the HMM in detail, we first
explain what HMM represents in speech recognition.
Figure 3.1 Graphical model of HMM without model parameters.
3.2.1
 Lexical unit for HMM
The acoustic model 
 means that we provide a likelihood function of the
observations O given the word sequence W. However, since the number of all
possible word sequences is an exponential order, we cannot prepare a likelihood

function for each word sequence. Instead, we first introduce a lexical sequence 
 that is composed of phonemes (e.g., a,/k/),
context-dependent phonemes (e.g., a-/k/-/i/,a-/k/-a), or words (e.g., numbers,
commands) as 
. A discussion about the context-dependent phoneme
(allophone) unit can be found in Section 6.5. We usually use a phoneme unit
defined by linguistics, and the automatic discovery of the phoneme unit from
speech data by using Bayesian nonparametrics can be found in Section 8.4. The
definition of the lexical unit depends on application, but the standard acoustic
model for LVCSR uses (context-dependent) phonemes as a lexical unit, and
hereinafter in this chapter, we use phonemes to define a lexical unit. Then, 
 is
a set of all distinct phonemes.
By using the lexical sequence L, we can revisit the MAP decision rule in
Eq. (3.15) for ASR as follows:
(3.20)
By using the product rule, and assuming that the likelihood function only
depends on the lexical sequence L, it is rewritten as:
(3.21)
where 
 is called a lexical model. Usually, the lexical model is not a
probabilistic model, but is obtained deterministically by using a lexical
dictionary, which provides a phoneme sequence (or multiple phoneme
sequences) given a word. We further assume that the alignment of O for
phoneme 
 is already given. This means that O is segmented to 
 where 
, and 
 is given.1

By assuming that 
 is independent and identically distributed (iid) for
phoneme 
, the acoustic model is factorized by m as:
(3.22)
This 
 is an actual likelihood function that we deal with for ASR, and is
represented by an HMM. Therefore, the acoustic model is composed of 
HMMs where 
 denotes the number of distinct phonemes. The following
section explains the HMM for one phoneme, and omits phoneme 
 in the
explanation. In addition, since the alignment is already given, we omit the
segmentation information m based on 
, and use 
instead of 
 to be modeled by an HMM.
3.2.2
 Likelihood function of HMM
This section describes a likelihood function of the HMM for a phoneme, where
the HMM is a popular formalism for representation of sequential patterns. The
likelihood function defined here is used to estimate HMM parameters.
In a set of D dimensional continuous-valued speech feature vectors O, each
observation 
 is represented under a Markov state 
 as illustrated in
Figure 3.1. We assume that each speech frame 
 is independent conditionally
on its state label 
. Starting from the state at the first time frame with an initial
state probability 
, which is a model parameter of HMM,
(3.23)
where each state at time t depends on the state at its previous time 
. The
state transition probability
(3.24)

is also introduced as HMM parameters to drive the change of HMM states or
equivalently characterize the transition of acoustic events under a phone unit.
The parameters of initial state probabilities and state transition probabilities
should satisfy the following constraints:
(3.25)
respectively. A sequence of hidden states 
corresponding to a sequence of observations O is marginalized in calculation of
likelihood function. HMMs basically involve a doubly stochastic process. One is
for the observed speech frame sequence O and the other is for the corresponding
HMM state sequence S, which is not observed and is regarded as a latent
variable. The model which includes a latent variable is called the latent model.
Since S is not observed, the likelihood function of O is given by the
summation of the joint distribution of O and S over all possible state sequences
conditioned on a set of HMM parameters Θ:
(3.26)
 denotes a set of all possible state sequences, which is often omitted when it is
trivial. Note that the summation over all possible state sequences requires the
exponential order of computations, which is not feasible in a practical use. The
joint distribution 
 is also called complete data likelihood, where a set
of observed data and latent variables (
) is called complete data. The joint
distribution (complete data likelihood) is a useful representation to provide
approximate solutions for the latent model (Dempster et al. 1976, Huang, Ariki
& Jack 1990). Therefore, we focus on the joint likelihood function of the HMM.
By using the product rule, the joint distribution in Eq. (3.26) is decomposed
into the likelihood function 
 given the state sequence S, and the state
sequence probability 
 as follows:

(3.27)
Since 
 is independent and identically distributed (iid) given the state sequence
, the likelihood function 
 is represented as follows:
(3.28)
Here, 
 is a set of state-dependent HMM parameters, where j is the index of the
HMM state. Similarly, the state sequence probability 
 given the state
sequence 
 can be represented by the initial probability in
Eq. (3.23) and transition probabilities in Eq. (3.24), as follows:
(3.29)
Thus, by substituting Eqs. (3.28), (3.29), and (3.27) into Eq. (3.26), we can
obtain the following equation:
(3.30)
This is the basic equation of the likelihood function of the HMM with the
parameter 
. 
 is called the emission probability,
and can be any distribution of 
.
If we use discrete values as observations for 
, the emission probability of
the HMM is represented by a multinomial distribution. This HMM is called a
discrete HMM (DHMM). In the DHMM, the HMM parameters are formed as 
 where the emission probabilities of the discrete
observation values k given state j are represented as the multinomial distribution
parameter 
. Assuming that the vector space of observations 
 in state

j is partitioned into K subspaces by clustering or vector quantization, we express
the state observation probability 
 using the probability 
, which is
constrained by the property
(3.31)
Here 
 denotes a set of discrete feature vectors that belongs to a partitioned
subspace k. This partition is undertaken by using Vector Quantization (VQ)
techniques.
Historically, DHMM was first used for speech modeling with the VQ
techniques (Matsui & Furui 1994, Jelinek 1997). However, since the speech
feature vector is a continuous value, the Gaussian distribution or the mixture of
Gaussian distributions is often used for an acoustic model. That is, an acoustic
model, which dealt with discrete observation values obtained from VQ codes,
was shifted to so-called continuous density HMM (CDHMM), where the HMM
with Gaussian distribution based emission probabilities could model continuous
observation values more appropriately. CDHMM is described in the next section.
3.2.3
 Continuous density HMM
For the HMM framework addressed in Section 3.2, the state observation
probability 
 is calculated according to the type of observation data.
When using discrete observation symbols, each observation vector 
 is
simply represented by a codebook partition k which is determined through vector
quantization over the vector space of all training samples O. Nonetheless, the
representation of a high dimensional feature vector 
 based on a set of
discrete codebook labels is not adequate to fully reflect the randomness of the
observation vector. More generally, we calculate the probability density function
(pdf) of the observation vector 
 given an HMM state 
 to determine the
state observation probability 
. It is popular to represent the randomness
of continuous-valued 
 using the multivariate Gaussian distribution 
defined in Appendix C.6 as:

(3.32)
with state-dependent mean vector 
 and covariance matrix 
.
However, a single Gaussian distribution is insufficient to represent the state-
dependent observation space for an HMM state j because there are large amounts
of training data collected from varying speakers with different genders, ages,
accents, speaking rates, channel distortions and background noises, which are
used to train the parameters of individual HMM states. Accordingly, a Gaussian
mixture model (GMM) is adopted to represent the state-dependent observation
space. This is based on a set of Gaussian distributions which reflect the
variations of speech feature vectors within an HMM state due to various acoustic
conditions. The state observation probability density function of a feature vector 
 at time t and in state j is expressed by GMM with K mixture components:
(3.33)
In Eq. (3.33), each mixture component of state j is expressed by a Gaussian
distribution:
(3.34)

and the prior probability of a mixture component 
 is defined as a mixture
weight,
(3.35)
The state-dependent GMM parameters 
, 
, 
 consist of mixture
weights 
, mean vectors 
, and covariance matrices 
 for K Gaussian
mixture components. The resulting realization of HMM using GMM as state
observation probability is also called the continuous density HMM (CDHMM).
The CDHMM parameters are formed as
(3.36)
with an additional constraint on the prior probability of mixture components or
the mixture weights.
Typically, HMM covariance matrices are assumed to be diagonal in practical
implementation, i.e., 
. Note that this book uses
Σ to represent a diagonal component of covariance, or variance when 
.
Then, the diagonal covariance version of Eq. (3.34) is factorized to the
univariate Gaussian distribution (Appendix C.5) by the dimension index d as
follows:
(3.37)
This diagonal covariance representation can represent the distribution with all
scalar variables, which makes the calculation very simple compared with the
vector and matrix representation in the multivariate Gaussian distribution in
Eq. (3.34). In addition, the number of parameters for the covariance matrix is

reduced from 
 to D, which can reduce the computational and
memory costs. The full covariance Gaussian in Eq. (3.34) also requires
computation of the inverse and the determinant of the covariance matrix (
 and 
), which makes the computation numerically unstable in addition to
incurring the matrix computation cost. However, the speech feature vectors have
correlations over dimensions, and the diagonal covariance assumption is not
adequate. There are more sophisticated models such as the subspace mean and
variance methods, aiming to bridge the gap between full and diagonal covariance
modeling, which have been proposed (Gales 1999, Axelrod, Gopinath &
Olsen 2002). In this book, we keep the vector and matrix representation for the
multivariate Gaussian distribution for generality.
Based on the above likelihood discussion of the CDHMM for single
observation vector 
 given HMM state 
 at frame t, we consider the
marginal likelihood for a whole observation sequence O. Under CDHMM, we
have an additional latent variable 
 from the mixture component of GMM
for each speech frame 
 at time t, compared with Eq. (3.30). Thus, the
sequence of mixture components
(3.38)
corresponding 
to 
the 
sequence 
of 
observed 
speech 
frames 
, is introduced in estimation of HMM parameters in
addition to 
. Considering two sets of latent
variables S and V, we consider the likelihood function of the CDHMM by
following the formulation in Section 3.2.2. First, the marginal likelihood
function of the CDHMM is represented as the joint distribution 
:
(3.39)
where all possible state sequences S and mixture component sequences V are
considered in calculation of the marginal distribution. The joint distribution 
 is also called the complete data likelihood, as we discussed in
Eq. (3.26) with additional latent variable V. This equation is further decomposed
by using the product rule as follows:

(3.40)
We provide an actual distribution for each probabilistic function.
Since the distributions 
 are given in Eq. (3.29), we focus on 
 and 
. Since 
 only depends on 
, given the state
sequence 
, 
, it is represented by the factorized form
with frame t as follows:
(3.41)
Similarly, from Eq. (3.34), 
 is also represented by the factorized
form with frame t given S and V as:
(3.42)
Thus, by substituting Eqs. (3.29), (3.41), and (3.42) into Eq. (3.40), the complete
data likelihood function of the CDHMM is represented as
(3.43)
Thus, the marginal likelihood of the CDHMM is extended from Eq. (3.30) as

(3.44)
This is the basic equation of the likelihood function of the CDHMM with the
parameter 
. As we discussed in the previous
section, the summation over all possible state and mixture component sequences
requires an exponential order of computations, which is not tractable in practical
use. Section 3.3 describes how to efficiently compute the marginal likelihood by
using the forward and backward algorithms, and most probable state sequence
based on the Viterbi algorithm. Section 3.4 also describes how to estimate the
HMM parameters Θ efficiently based on these algorithms and the expectation
and maximization algorithm.
Before moving to these explanations, as a subset model of the CDHMM, we
introduce a simple GMM without an HMM, which is also widely used in speech
and language processing.
3.2.4
 Gaussian mixture model
The GMM is a simplified model of the CDHMM, without considering the state
sequence S in the previous section. However, this simple GMM is still very
powerful for modeling speech features. For example, the GMM is used in speech
and noise models in speech enhancement, and in speaker models in speaker
verification and clustering, which are discussed in Section 4.6. The GMM can
also be widely used to model other signals than speech, e.g., image processing,
and biosignals. Therefore, this section only introduces the marginal likelihood of
a GMM to be used in later chapters, similar to that of the CDHMM in Eq. (3.44):
(3.45)

with the parameter 
. Since 
 is independent of 
in a GMM, the sequential summation over V is independently applied to each 
. So, we can factorize 
 into the following tth frame
distribution given the kth mixture component :
(3.46)
Thus, unlike the HMM case that needs to consider the HMM state sequence S
(computation based on an exponential order for the frame length), the GMM can
compute the likelihood of all possible V by using the straightforward
computation of Eq. (3.45) with the linear order computation of the frame length.
As regards the parameter estimation, the EM algorithm is widely used, and this
is discussed in Section 3.4.
Figure 
3.2 
illustrates 
a 
univariate 
GMM 
distribution 
 
which 
is
plotted with a dashed line and is formed as a mixture of three Gaussian
distributions with different means 
 and variances 
,
which are shown with a solid line. As shown in the figure, the GMM is a multi-
modal distribution with multiple peaks, which can model multiple factors in
speech features. Therefore, the GMM can accurately represent speech variations
that cannot be represented by a single Gaussian distribution.

Figure 3.2 Distributions of three individual Gaussians (dashed line) and the
corresponding GMM (solid line).
3.2.5
 Graphical models and generative process of CDHMM
The previous sections explain the marginal likelihood functions of the GMM and
CDHMM based on their joint likelihood distributions. As we discussed in the
graphical model representation in Section 2.2, once we obtain the joint
likelihood distribution of a model, we can obtain the corresponding graphical
model and generative process of the model. This section provides the graphical
model and generative process of the GMM and CDHMM, respectively, and
these are used in the following chapters to deal with these models by using the
Bayesian approach.
Graphical models and generative process of GMM
Based on the previous explanations, we can provide a generative process and
graphical model of a K-component GMM discussed in Section 3.2.4 as an
example. Given feature vectors O, latent variables V, and model parameters Θ,
the joint distribution of a complete data set 
 is represented from
Eq. (3.45) as follows:
(3.47)
Thus, the joint distribution of the GMM is parameterized as
(3.48)
Thus, we have the following three kinds of variables:
Observation: 
;
Latent: 
;

Non-probabilistic: 
.
The dependencies of the above variables are expressed in Eq. (3.48), and we
provide the generative process of the GMM in Algorithm 2. Given the mixture
weight 
, latent variable 
 is sampled from the multinomial distribution.
In addition, by using the plate for t and k, as discussed in Section 2.2.2, we
can simply write the graphical model of the GMM, as shown in Figure 3.3.
Algorithm 2 Generative process of Gaussian mixture model
Require:  T, 
 
1:  for 
 do 
2:     Draw 
 from 
 
3:     Draw 
 from 
 
4:  end for
Figure 3.3 Graphical model of Gaussian mixture model.
Graphical models and generative process of CDHMM
Similar to the GMM case, we can also provide a generative process and
graphical model of a continuous density HMM discussed in Section 3.2 as
another example. Given feature vectors O, latent variables S and V, and model
parameters Θ, the joint likelihood function of complete data O, V, and S based
on a continuous density HMM is represented as follows:

(3.49)
The CDHMM has the following three kinds of variables:
Observation: 
;
Latent: 
 and 
;
Non-probabilistic: 
, 
, 
.
Algorithm 3 Generative process of continuous density hidden Markov model
Require:  T, Θ 
1:  Draw 
 from 
 
2:  Draw 
 from 
 
3:  Draw 
 from 
 
4:  for 
 do 
5:     Draw 
 from 
 
6:     Draw 
 from 
 
7:     Draw 
 from 
 
8:  end for
Similarly to the GMM, dependencies of the above variables are expressed in
Eq. (3.49), and we provide the generative process of the CDHMM in Algorithm
3. Figure 3.4 shows the graphical model of the CDHMM that uses the plates for
the number of HMM states J, and the number of GMM components K. The
graphical model of the CDHMM is rather complicated since the state transition
makes it difficult to use the plate representation for t.

Figure 3.4 Graphical model of continuous density hidden Markov model
given parameters Θ.
Thus, we provide the graphical model and generative process examples of
our typical target model, CDHMM. Note that these are rather simple based on a
non-Bayesian approach since we do not deal with the model parameters as
probabilistic variables (do not use circle representations for them).
3.3   Forward–backward and Viterbi algorithms
This section describes how to compute the likelihood values of HMMs (given a
latent variable) by using the famous forward–backward and Viterbi algorithms,
which are used to solve the statistical speech recognition problem in Eq. (3.3).
Although the direct computation of the likelihood values causes the
combinatorial explosion problem, these algorithms can provide feasible
computational costs. These algorithms are also used to estimate the model
parameters, which is described in Section 3.4
3.3.1
 Forward–backward algorithm
Basically, the direct computation of marginal likelihood 
in Eq. (3.30) involves on the order of 
 calculations, since at every 
, there are J possible states that can be reached (i.e., there are 
possible state sequences), and for each such state sequence about 
calculations are required for each term in the sum of Eq. (3.30). By considering
the latent variable of mixture components of the CDHMM in Eq. (3.44), the

direct computation of marginal likelihood 
 is further
increased to the order of 
. However, this large computation problem
could be tackled by applying the forward–backward algorithm (Rabiner &
Juang 1986). According to this algorithm, the forward variable 
 is defined
by
(3.50)
This forward variable is known as the probability of the partial observation
sequence 
 until time t and state j at time t given the current
HMM parameters Θ. We also define an emission probability given HMM state j
at frame t as
(3.51)
In the CDHMM, this is represented by the likelihood function of the GMM, as
described in Eq. (3.33). A forward procedure, which is a dynamic programming
method, is inductively derived to speed up computation of 
 as follows:
Forward algorithm
Initialization
(3.52)
Induction

(3.53)
Termination
(3.54)
Thus, we can compute the likelihood value 
 recursively by using the
forward variable 
. This iterative algorithm for computing the forward
variable is called the forward algorithm. In Eq. (3.52) and Eq. (3.53), we see that
the calculation of 
 requires on the order of 
calculations which is dramatically reduced from 
 as required in the direct
calculation. Figure 3.5 illustrates the sequence of operations for computation of
the forward variable 
.

Figure 3.5 Propagation of forward variable from 
 to 
. All
possible states at time 
 are considered. Adapted from Rabiner &
Juang (1993).
Similarly, we consider a backward variable which is defined as
(3.55)
This variable represents the probability of the partial observation sequence from 
 to the end, given state j at time t and model parameters Θ. Again, we can
inductively derive the following backward procedure:
Backward algorithm
Initialization
(3.56)
Induction
By using the sum and product rules, we can rewrite 
 as
(3.57)

By using the conditional independence property of the HMM,
(3.58)
Termination
(3.59)
Thus, the backward algorithm can also compute the likelihood value 
recursively by using the backward variable 
. In the initialization step of the
backward procedure, 
 is arbitrarily assigned to be 1 . In the induction step,
the backward variable 
 is calculated in a backward fashion from 
 to the beginning 
. Figure 3.6 shows the sequence of
operations required for computation of the backward variable 
. This iterative
algorithm for computing the backward variable is called the backward
algorithm.

Figure 3.6 Propagation of backward variable from 
 to 
. All
possible states at time 
 are considered. Adapted from Rabiner &
Juang (1993).
The forward variable 
 and backward variable 
 are used to calculate
the posterior probability of a specific case. For example, if we consider the
posterior probability 
 when the HMM state is j at frame t, we first
define
(3.60)
which is useful in the later explanations. By using the sum and product rules, 
 is represented by the likelihood ratio of
(3.61)
Now we focus on the joint distribution 
, which is rewritten by
forward variable 
 in Eq. (3.50) and backward variable 
 in Eq. (3.55), as
follows:

(3.62)
From the first to second lines, we use the conditional independence assumption
of the HMM. Thus, finally the posterior distribution 
 is represented as:
(3.63)
This probability will be used to find the optimal state sequence based on the
Viterbi algorithm, as mentioned in Section 3.3.2, as well as to estimate the HMM
parameters based on the maximum likelihood method, as addressed in Section
3.4.
3.3.2
 Viterbi algorithm
Finding the optimal state sequence 
 of observation sequence
O, in terms of the maximum a-posteriori sense, is seen as a fundamental problem
in the HMM framework. It is formulated as
(3.64)
where 
 find the most probable  from all possible state sequences,
and this also involves on the order of 
 calculations, similar to the likelihood
computation 
 in Section 3.3.1. In addition, since 
 does not
depend on S, we can also rewrite Eq. (3.64) as
(3.65)

That is, the optimal state sequences obtained in terms of the posterior
distribution 
 and the joint distribution 
 are equivalent.
The optimal state sequence can be used to determine the segmentation of a
speech sentence into phones or sub-phones, if we assume these as latent
variables. In this case, the speech frames staying in the same state behave
similarly, and the transition from one state to the other is treated as a
segmentation boundary.
In addition, the optimal state sequence 
 is applied to approximate the
calculation of the marginal likelihood function over S as follows:
(3.66)
That is, the marginal likelihood function is approximately represented as the
joint likelihood distribution of 
 and O, which can be represented with
Eq. (3.30) as
(3.67)
Therefore, the approximation of marginal likelihood using the optimal state
sequence provides the solution to the segmental likelihood function 
 since
likelihood calculation is based on the speech segmentation using . Considering
all possible state sequences, 
 is simplified to applying only the optimal state
sequence . The number of calculations is further reduced to 
 as referred to
Eq. (3.67).
We can individually determine the most likely state  for each time frame 
 according to the posterior probability as

(3.68)
The posterior probability 
 is calculated according to the forward-
backward algorithm. As shown in Eq. (3.63), the joint distribution of observation
sequence O and state 
 at time t is calculated as a product of the forward
variable, which accounts for partial observations 
 and state j at time
t, and the backward variable, which accounts for the remainder of observations 
 given state j at time t with the normalization factor. However,
Eq. (3.68) may not determine a valid optimal state sequence since the probability
of occurrence of the sequences of states is not considered.
To cope with this problem, we need to find the single best state sequence by
using Eq. (3.64) or an equivalent form of (3.65), directly. A dynamic
programming method, called the Viterbi algorithm (Viterbi 1967), was proposed
to efficiently find the single best state sequence. To do so, we define the highest
probability along a single path, at time t, which accounts for the first t
observations and ends in state j using a new notation:
(3.69)
By induction, a recursive formula of 
 from 
 is derived to calculate this
probability. To derive the equation, we first focus on the joint distribution
appearing in 
, which can be rewritten when 
 and 
 as:
(3.70)
Here we use the conditional independence of the HMM from the second to the
third lines. Thus, by using Eq. (3.69), 
 is computed recursively from 
 as:

(3.71)
We need to keep track of the state that maximized Eq. (3.71) so as to backtrack
to the single best state sequence in the following Viterbi algorithm:
Initialization
(3.72)
Recursion
(3.73)
Termination
(3.74)
State sequence backtracking
(3.75)
In the termination step, the segmental likelihood function  is calculated and is
equivalent to Eq. (3.67). It is noteworthy that this Viterbi algorithm is similar to
the forward procedure in the forward–backward algorithm. The key difference is
the maximization in Eq. (3.73) over previous states, which is used in place of a
summation operation in Eq. (3.53). The variable 
 in the Viterbi algorithm is
meaningfully related to the forward variable 
 in the forward–backward
algorithm.

Now, we summarize what we can compute from the HMM without taking on
the combinatorial explosion problem. These values are used in the decoding step
and the training step of estimating model parameters Θ, which is discussed in the
next section.
: 
The marginalized likelihood function from the forward or backward
algorithm.
:
The posterior probability of 
 from the forward–backward algorithm.
: 
The optimal state sequence from the Viterbi algorithm.
:
The segmental joint likelihood function from the Viterbi algorithm.
3.4   Maximum likelihood estimation and EM
algorithm
Previous sections discuss the HMM-based speech modeling given model
parameters Θ to compute the likelihood values and so on, efficiently based on
the forward, backward, and Viterbi algorithms. One of the powerful properties of
the HMM is that it also provides an efficient algorithm to obtain the model
parameters based on the ML estimation from training data. The training
algorithm is based on the EM algorithm, which can tackle the incomplete data
problem in ML estimation. In what follows, we address how the EM algorithm is
derived by applying Jensen’s inequality and show the procedure of EM steps for
estimation of HMM parameters, which involves latent variables in probabilistic
functions 
(Nakagawa 
1988, 
Huang 
et 
al. 
1990, 
Rabiner 
&
Juang 1993, Bilmes 1998).
3.4.1
 Jensen’s inequality
We start the discussion of HMM parameter estimation based on the Maximum
Likelihood (ML) criterion, so that the optimal model parameters  are computed

by the likelihood function 
:2
(3.76)
We assume that speech data O are observed and are generated by some
distribution. However, as we discussed in Section 3.2, the HMM has additional
hidden variables S (HMM state sequence) and V (GMM component sequence),
which are not observed. In this situation where the model includes unobserved
variables, O is called incomplete data (S and V). (Θ would also be included as
unobserved variables in a broad sense, but this book considers latent variables as
unobserved variables.). Conversely, the complete data Y are composed of the
observed data O as well as the hidden variables 
.
In general, the maximum likelihood estimation of the model parameter Θ for
complete data O is difficult, since we need to consider the following equation:
(3.77)
As we discussed in Section 3.3, the summation over all possible S and V has to
overcome the combinatorial explosion problem, and the direct optimization of Θ
for this equation is not feasible. This problem is called the incomplete data
problem.
According to the EM algorithm, the incomplete data problem in the ML
estimation is resolved by iteratively and alternatively performing the expectation
step (E-step) and the maximization step (M-step), which results in obtaining the
local optimum solution of the model parameters. In the E-step, we calculate an
auxiliary function 
, which is an expectation of the logarithm of the
likelihood function using new HMM parameters 
 given the current parameters
Θ, i.e.

(3.78)
In the M-step, we maximize the auxiliary function instead of Eq. (3.77) with
respect to the HMM parameters 
 and estimate new parameters by
(3.79)
The updated HMM parameters 
 are then treated as the current parameters for
the next iteration of EM steps. This iterative estimation only obtains local
optimum solutions, and not global optimum solutions. However, a careful setting
of the initial model parameters would help the solution to reach appropriate
parameter values, and moreover, the algorithm theoretically guarantees that the
likelihood value is always increased as the number of iterations increases. This
property is very useful in the implementation, since we can easily debug training
source codes based on the EM algorithm by checking likelihood values.
Now, we prove how this indirect optimization for the auxiliary function 
 increases a likelihood value. For this proof, we first define the
following logarithmic marginal function 
:
(3.80)
Note that since the logarithmic function is a monotonically increasing function,
we can check 
 instead of 
 for the proof. We introduce the notation
of complete data:
(3.81)
Then, the conditional distribution of Y given incomplete data O and parameters
Θ has the following relationship from the product rule:
(3.82)

where the likelihood function 
 is rewritten as 
, since Y
includes O. Therefore, by taking the logarithmic operation for both sides in
Eq. (3.82), we derive the equation for 
 as
(3.83)
Then, by taking the expectation operation with respect to the posterior
distribution of latent variables 
 for Eq. (3.83), Eq. (3.83) is
represented as
(3.84)
where 
 is defined as follows:
(3.85)
 is defined in Eq. (3.78). Then, we obtain this relation from Eq. (3.85):
(3.86)
Here, Jensen’s inequality is introduced to prove the inequality (Dempster
et al. 1976),
(3.87)
Jensen’s inequality 
If x is a random variable with mean value 
 and 
 is a convex
function, then
(3.88)

with equality if and only if x is a degenerate distribution at μ.
The inequality in Eq. (3.87) is derived by
(3.89)
where a convex function based on the negative logarithm 
 is
adopted. As shown in Eq. (3.89), the difference 
 is obtained
as the relative entropy or Kullback–Leibler (KL) divergence (Kullback &
Leibler 1951) between the distributions 
 and 
, that is
(3.90)
Given Eqs. (3.87) and (3.86), it is straightforward to see that if 
 satisfies
(3.91)

then we can prove that
(3.92)
Since 
 and the logarithmic function is a monotonic function,
(3.93)
Since 
 always satisfies the inequality Eq. (3.91), we prove
that the parameter estimated by the EM procedure, 
, always increases the
likelihood value as:
(3.94)
Such an EM procedure is bound to monotonically increase the auxiliary function
 as well as the original likelihood function 
. Note that since it is
not a direct optimization of the original likelihood function, the optimization of
the auxiliary function leads to a local optimum solution to the ML parameter
estimation.
3.4.2
 Expectation step
To find ML estimates of HMM parameters, we expand the auxiliary function in
Eq. (3.78) and rewrite it by substituting the joint distribution of complete data
likelihood (Eq. (3.49)) into Eq. (3.78) as

(3.95)
Note that four terms depend on the initial weight 
, state transition probability 
, mixture weight 
, and Gaussian parameters 
, respectively. We
provide the solution for each term:
We first focus on the first term depending on the initial weight 
, and
define the following auxiliary function for 
:
(3.96)
Since 
 only depends on 
, we obtain the following equation that
marginalizes 
 over 
 and V as:
(3.97)
Therefore, 
 can be rewritten as
(3.98)
where 
 is an occupation probability defined in Eq. (3.60) as:
(3.99)
This is computed from the forward–backward algorithm.
Next, we focus on the second term in Eq. (3.95), which depends on the state
transition probability 
, and define the following auxiliary function for 

:
(3.100)
Here we replace the summation from 
 to 
 for
notational convention. Similar to 
, we obtain
(3.101)
Therefore, by replacing the summation over t with the summation over 
, we obtain
(3.102)
where 
 is an expected transition probability from 
 to 
,
and is defined as:
(3.103)
Note that by using this technique, the summation over all possible
sequences (that leads to a combinatorial explosion) can be replaced with a
summation over the number of HMM states and the number of frames.
Thus, this auxiliary function can be computed feasibly. For this
computation, we need to obtain 
, which is discussed later as a variant
of the forward–backward algorithm.

The third term in Eq. (3.95) depends on the mixture weight 
, and so we
define the following auxiliary function for 
:
(3.104)
Similarly to the case of 
, we first obtain the following equation:
(3.105)
Therefore,
(3.106)
where 
 is an expected occupation probability at 
 and 
,
and is defined as:
(3.107)
The computation of 
 is also discussed later as a variant of the
forward–backward algorithm.
Finally, the fourth term in Eq. (3.95) depends on the Gaussian parameters 
 and 
, and defines the following auxiliary function for 
 and 
:

(3.108)
Similarly to 
, by using Eq. (3.105), 
 can be rewritten
with 
 as
(3.109)
By using the definition of the multivariate Gaussian distribution in
Section C.6:
(3.110)
Equation (3.109) can be rewritten as
(3.111)
where 
 denotes the proportional relationship in the logarithmic domain.
That is, the normalization factor in the linear domain is changed for the
normalization constant in the logarithmic domain, and we shall continue to
use 
 in this book. Therefore, the normalization constant term that does
not depend on 
, 
 is omitted from this expression.
Thus, we summarize the auxiliary function 
:

(3.112)
where each term is defined as follows:
(3.113)
(3.114)
(3.115)
(3.116)
As an equivalent form of Eq. (3.116), we also write the following auxiliary
function 
, which replaces the covariance matrix 
 with the
precision matrix 
 as:
(3.117)
This equivalent representation is used to make the computation simple in the
following sections. Note that Eqs. (3.113)–(3.117) are represented by the
following posterior distributions:

(3.118)
(3.119)
Similarly to 
, as discussed in Section 3.3.1, these values
are also computed efficiently by using the forward–backward algorithm.
First, 
 has the following relationships with 
 and 
:
(3.120)
The posterior probability 
 can be calculated by using Eq. (3.63) based on the
forward variables and backward variables 
, but this can also be
computed from 
 or 
.
The posterior probability 
 of occupying state j and Gaussian
component k in Eq. (3.119) can be computed by the forward–backward
algorithm. By using the sum and product rules, 
 is represented by the
posterior distribution 
 and joint likelihood function of 
 as:
(3.121)
By using Eq. (3.33) for 
 and Eq. (3.63) for 
,
Eq. (3.121) is represented as follows:

(3.122)
In a similar manner, we express the posterior probability 
 in Eq. (3.118) by
(3.123)
Now we focus on the joint likelihood function 
, which is
factorized by
(3.124)
By using the conditional independence assumption of the HMM, we can rewrite
the equation as:
(3.125)
Thus, by using the forward variable 
 in Eq. (3.50) and the backward variable
 in Eq. (3.55), Eq. (3.123) is finally written as

(3.126)
Figure 3.7 illustrates how the posterior probability 
 of visiting states i and j
in consecutive time frames is calculated by applying the forward–backward
algorithm.
Figure 3.7 Calculation of posterior probability 
 based on forward
variable 
 and backward variable 
. Adapted from Rabiner &
Juang (1993).
In Eq. (3.63), Eq. (3.122) and Eq. (3.123), the posterior probabilities 
, 
, and 
 are calculated, respectively, through a kind of soft
computation, i.e., the assignment information is represented by the probabilistic
values for the elements i, j, and k. Instead of this soft computation, a simple and
efficient approximation is to find the segmental ML estimates based on a hard
computation where only the single best state sequence 
 and mixture
component sequence 
 are considered with 0 (not assigned) or 1
(assigned) values for the elements i, j, and k. This computation complexity is
significantly reduced. For this consideration, the calculation of posterior
probabilities is simplified as
(3.127)

(3.128)
(3.129)
where 
 denotes a Kronecker delta function that returns 1 when 
and 0 otherwise. These probabilities are 1 for the cases of the best states i, j, and
Gaussians k, and 0 for all of the other cases. Note that  is computed by using
the Viterbi algorithm that maximizes the following segmental joint likelihood
function as discussed in Section 3.3.2:
(3.130)
The value of , given HMM state j, is computed by
(3.131)
The E-step in the EM algorithm is completed by calculating these posterior
probabilities.
3.4.3
 Maximization step
In the maximization step, we aim to maximize 
, 
, 
, and 
 with respect to 
, 
, 
, and 
 so as to estimate ML
parameters 
, 
, 
, and 
, respectively. However, the constraints of
probability parameters
(3.132)
have to be imposed in the constrained optimization problem. For example, when
considering the estimation of initial state probabilities 
, we construct a
Lagrange function (or Lagrangian):

(3.133)
by combining the original auxiliary function in Eq. (3.113) and the constraint in
Eq. (3.132) with an additional Lagrange multiplier η as a scaling factor. Then we
differentiate this Lagrangian with respect to individual probability parameter 
and set it to zero to obtain
(3.134)
By substituting Eq. (3.134) into the constrains in Eq. (3.132), we obtain
(3.135)
The ML estimate of new initial state probability is derived by substituting
Eq. (3.135) into Eq. (3.134):
(3.136)
In the same manner, we derive the ML estimates of new state transition
probability and new mixture component probability, which are provided by

(3.137)
(3.138)
These results show that the initial state probability, state transition probability,
and mixture weight can be computed using the ratio of the occupation statistics.
On the other hand, when estimating new HMM mean vectors 
, we
individually differentiate 
 in Eq. (3.117) with respect to 
 for
each Gaussian component k in state j and set it to zero:
(3.139)
where we use the following vector derivative rule in Eq. (B.9):
(3.140)
Therefore, the ML estimate of the HMM mean vector is derived as shown by

(3.141)
Note that the mean vector is represented as the first-order expected value of 
by using the occupation probability of 
.
At the same time, the new HMM covariance matrices 
 or their
inverse matrices 
 (also called the precision matrices) are
estimated by differentiation of 
 in Eq. (3.117) with respect to 
for each Gaussian k at each state j and setting it to zero:
(3.142)
where we use the following matrix derivative rule in Eqs. (B.8) and (B.10):
(3.143)
(3.144)
Thus, the new estimates of the HMM precision and covariance matrices are
derived by using the ML estimate 
 for 
 in Eq. (3.142) as

(3.145)
Interestingly, the calculation of the derived covariance matrices 
 in
Eq. (3.145) is interpreted as the weighted ensemble expectation and covariance
matrices, as well as the calculation of the ML mean vectors 
 in
Eq. (3.141). The occupation probability 
 of state j and mixture component
k at time t is treated as the weighting factor in calculation of the weighted
expectation function. Note that Eq. (3.145) is
(3.146)
Thus, the ML estimate of the covariance matrix is computed from the second-
order statistic and the ML estimate of the mean vector.
Now, we summarize the ML estimates of the CDHMM as follows:
(3.147)
(3.148)

(3.149)
(3.150)
(3.151)
If we consider the diagonal covariance matrix, the dth diagonal element of
Eq. (3.151) is modified as follows:
(3.152)
To compute these estimated values, we need to compute the following values:
(3.153)
These statistics are sufficient to compute the parameters, and are called sufficient
statistics of the CDHMM. In particular, 
, 
, and 
 are called the 0th,
1st, and 2nd order sufficient statistics of the Gaussian at HMM state j and
mixture component k, respectively. The sufficient statistic is first mentioned in
Section 2.1.3, where the estimation problems are rather simple, and they do not

include latent variables. In the latent models, the probabilistic assignment
information of the occupation probabilities 
, 
, 
 is important to
obtain the sufficient statistics. These statistics, composed of the occupation
probabilities 
, 
, 
, are computed from the forward–backward
algorithm, as discussed in the expectation step (Section 3.4.2).
Based on these sufficient statistics, we can rewrite the auxiliary function as
follows:
(3.154)
(3.155)
(3.156)
(3.157)
These forms are used in the analytical discussions in later chapters.
This total EM algorithm for iteratively estimating the HMM parameters is
called the Baum–Welch algorithm (Baum, Petrie, Soules et al. 1970), and is
based on the forward-backward algorithm, the accumulation of the sufficient
statistics, and the update of the HMM parameters, as shown in Algorithm 4. The
Baum–Welch algorithm can be extended based on the Bayesian approach (see
Section 4.3 and Section 7.3).

Algorithm 4 Baum–Welch algorithm
Require:  
 
1:  repeat 
2:     Compute the forward variable 
 from the forward algorithm 
3:     Compute the backward variable 
 from the backward algorithm 
4:     Compute the occupation probabilities 
, 
, and 
 
5:     Accumulate the sufficient statistics 
, 
, 
, and 
 
6:     Estimate the new HMM parameters 
 
7:     Update the HMM parameters 
 
8:  until Convergence
In the implementation of Viterbi or segmental ML estimation, we apply the
current HMM parameter estimates 
 and the Viterbi
algorithm to perform Viterbi decoding to find the state alignment. Given the best
state sequence 
 and mixture component sequence 
, new ML estimates 
 are computed as the ensemble mean vector and covariance matrix,
where 
 is seen as the count 
 of training samples 
 which
are aligned in state j and Gaussian component k (Juang & Rabiner 1990). This
method of using the Viterbi training instead of the forward–backward algorithm
to obtain (a part of) occupation probabilities in the training step is called
segmental K-means training or Viterbi training.
We should note that to make the Baum–Welch algorithm work in real speech
data, we need to consider some heuristics in the ML EM algorithm. For example,
how to provide initial values is always an important question, and it is usually
resolved by using a simple algorithm (e.g., K-means clustering or random
initialization). The other important issue comes from the data sparseness, e.g.,
some mixture components or hidden states cannot have sufficient data assigned
in the Viterbi or forward–backward algorithm, which makes the parameter
estimation (especially covariance parameter estimation) unstable. This can be
heuristically avoided by setting a threshold to update these parameters, or setting
minimum threshold values for covariance parameters. These problems can be
solved by the Bayesian approaches.
Despite the above problems, the Baum–Welch algorithm based on the EM

algorithm is widely used in current CDHMM training. This Baum–Welch
algorithm has several advantages. For example, the algorithm is unique in that
the computational cost of the E step is much more than that of the M-step, since
the E step computational cost depends on the training data size. However, the E-
step can be parallelized with many computers by distributing a chunk of data to a
computer and computing a sufficient statistic of the chunk independently of the
other computers. Therefore, the Baum–Welch algorithm has a very nice data
scalability, which enables it to use a large amount of training data. In addition,
within the algorithm, the likelihood always increases by the EM theory.
Therefore, by monitoring the likelihood values, we can easily detect errors and
bugs during a training procedure.
3.5   Maximum likelihood linear regression for hidden
Markov model
As we discussed in the previous section, CDHMM parameters can be estimated
by statistical ML methods, the effectiveness of which depends on the quality and
quantity of available data that should distribute according to the statistical
features of the intended signal space or conditions. As there is no sure way of
collecting sufficient data to cover all conditions, adaptive training of HMM
parameters from a set of previously obtained parameters to a new set that befits a
specific environment with a small amount of new data is an important research
issue.
In speech recognition, one approach is to view the adaptation of model
parameters to new data (e.g., speaker adaptation) as a transformation problem;
that is, the new set of model parameters is a transformed version of the old set: 
, where 
 denotes the new set of data available at
moment n for the existing model parameters 
 to adapt to. Most frequently and
practically, the function f is chosen to be of an affine transformation type
(Digalakis, Ritischev & Neumeyer 1995, Leggetter & Woodland 1995):
(3.158)
where various parts of the model parameters, e.g., the mean vectors or the

variances, are envisaged in a vector space. The adaptation algorithm therefore
involves deriving the affine map components, A and b, from the adaptation data 
. A number of algorithms have been proposed for this purpose (see Lee &
Huo (2000), and Shinoda (2010) for details).
The linear regression method for HMM parameters estimates the affine
transformation parameters from a set of adaptation data, usually limited in size.
The transformation with the estimated parameters is then applied to the
previously trained HMMs, resulting in the set of “adapted models.” Note that for
automatic speech recognition, the number of the Gaussian distributions or
simply Gaussians, which are used as component distributions in forming state-
dependent mixture distributions, is typically in the thousands or more. If each
mean vector in the set of Gaussians is to be modified by a unique transformation
matrix, the number of “adaptation parameters” can be quite large. The main
problem of this method is thus how to improve “generalization capability” by
avoiding the over-training problem when the amount of adaptation data is small.
To solve the problem, we introduce a model selection approach.
The model selection approach was originally proposed within the estimation
of linear transformation parameters by using the maximum likelihood EM
algorithm, as discussed in Section 3.4. The technique is called maximum
likelihood linear regression (MLLR). MLLR proposes to share one linear
transformation in a cluster of many Gaussians in the HMM set, thereby
effectively reducing the number of free parameters that can then be trained with
a small amount of adaptation data. The Gaussian clusters are usually constructed
as a tree structure, as shown in Figure 3.8, which is pre-determined and fixed
throughout adaptation. This tree (called a regression tree) is constructed based on
a centroid splitting algorithm, described in Young, Evermann, Gales
et al. (2006). This algorithm first makes two centroid vectors from a random
perturbation of the global mean vector computed from Gaussians assigned to a
target leaf node. Then it splits a set of these Gaussians according to the
Euclidean distance between Gaussian mean vectors and two centroid vectors.
The two sets of Gaussians obtained are assigned to child nodes, and this
procedure is continued to finally build a tree.

Figure 3.8 Gaussian tree representation of linear regression parameters.
The utility of the tree structure is commensurate with the amount of
adaptation data; namely, if we have a small amount of data, it uses only coarse
clusters (e.g., the root node of a tree in the top layer of Figure 3.8) where the
number of free parameters in the linear transformation matrices is small. On the
other hand, if we have a sufficiently large amount of data, it can use fine clusters
where the number of free parameters in the linear transformation matrices is
large, potentially improving the precision of the estimated parameters. This
framework needs to select appropriate Gaussian clusters according to the amount
of data, i.e., it needs an appropriate model selection function. Usually, model
selection is performed by setting a threshold value manually (e.g., the total
number of speech frames assigned to a set of Gaussians in a node).
3.5.1
 Linear regression for hidden Markov models
This section briefly explains a solution for the linear regression parameters for
HMMs within a maximum likelihood EM algorithm framework. It uses a
solution based on a variance normalized representation of Gaussian mean
vectors to simplify the solution.3 In this section, we only focus on the
transformation of Gaussian mean vectors in CDHMMs.
First, we review the basic EM algorithm of the conventional HMM
parameter estimation, as discussed in Section 3.4, to set the notational

convention and to align with the subsequent development of the MLLR
approach. Let 
 be a sequence of D dimensional
feature vectors for T speech frames. The latent variables in a continuous density
HMM are composed of HMM states and mixture components of GMMs. A
sequence of HMM states is represented by 
, where the value
of 
 denotes an HMM state index at frame t. Similarly, a sequence of mixture
components is represented by 
, where the value of 
denotes a mixture component index at frame t. As introduced in Eq. (3.78), the
EM algorithm deals with the following auxiliary function as an optimization
function instead of directly using the model likelihood:
(3.159)
where Θ is a set of HMM parameters and 
 is a complete data
likelihood given Θ. 
 is the posterior distribution of the latent
variables given the previously estimated HMM parameters Θ. Equation (3.78) is
an expected value, and is efficiently computed by using the forward–backward
algorithm as the E-step of the EM algorithm, as we discussed in Section 3.3.
The M-step of the EM algorithm estimates HMM parameters, as follows:
(3.160)
The E-step and the M-step are performed iteratively until convergence, and
finally we obtain the HMM parameters as a close approximation of the
stationary point solution.
Now we focus on the linear transformation parameters within the EM
algorithm. We prepare a transformation parameter matrix 
 for each
leaf node j in a Gaussian tree. Here, we assume that the Gaussian tree is pruned
by a model selection approach as a model structure M, and the set of leaf nodes
in the pruned tree is represented by 
. Hereinafter, we use Z to denote a joint
event of S and V (i.e., 
). This will much simplify the following

development pertaining to the adaptation of the mean and the covariance
parameters. Similarly to Eq. (3.159), the auxiliary function with respect to a set
of transformation parameters 
 can be represented as
follows:
(3.161)
Here k denotes a unique mixture component index of all Gaussians in the target
HMMs (for all phoneme HMMs in a speech recognition case), and K is the total
number of Gaussians. 
 is the posterior probability of
mixture component k at t, derived from the previously estimated transformation
parameters 
.4 Expression 
 is a transformed mean vector with 
, and
the concrete form of this vector is discussed in the next paragraph. In the Q
function, we disregard the parameters of the state transition probabilities and the
mixture weights, since they do not depend on the optimization with respect to 
. Expression 
 denotes a Gaussian distribution with mean
parameter 
 and covariance matrix parameter 
, and is defined in Appendix
C.6 as follows:
(3.162)
We use the trace based representation. Factor 
 is a normalization factor,
and is defined as follows:
(3.163)
In the following paragraphs, we derive Eq. (3.161) as a function of 
 to
optimize 
.
We consider the concrete form of the transformed mean vector 
 based on

the variance normalized representation. We first define the Cholesky
decomposition matrix 
 as follows:
(3.164)
where 
 is a 
 triangular matrix. If the Gaussian k is included in a set
of Gaussians 
 in leaf node j (i.e., 
), the affine transformation of a
Gaussian mean vector in a covariance normalized space 
 is represented
as follows:
(3.165)
where 
 is an augmented normalized vector of an initial (non-adapted) Gaussian
mean vector 
 and j is a leaf node index that holds a set of Gaussians. Thus,
transformation parameter 
 is shared among a set of Gaussians 
. The
clustered structure of the Gaussians is usually represented as a binary tree where
a set of Gaussians belongs to each node.
Now we focus on how to obtain the Q function of 
. By following the
equations in Example 2.3 with considering the occupation probability 
 in
Eq. (2.39), Eq. (3.161) is represented as follows:

(3.166)
where 
, 
, and 
 are defined as follows:
(3.167)
As introduced in Eq. (3.153), these are the 0th, 1st, and 2nd order sufficient
statistics of Gaussians in HMMs, respectively. Then, the Q function of 
 is
represented by substituting Eq. (3.165) into Eq. (3.166) as follows:

(3.168)
where 
 and 
 are 0th and 1st order sufficient statistics of linear regression
parameters defined as:
(3.169)
Here 
 is a 
 matrix and 
 is a 
 symmetric
matrix. To derive Eq. (3.168), we use the fact that the trace of the scalar value is
equal to the original scalar value, the cyclic property, and the distributive
property of the trace as in Appendix B:
(3.170)
(3.171)
(3.172)

We also use the definition of the Cholesky decomposition in Eq. (3.164).
Since Eq. (3.168) is represented as a quadratic form with respect to 
, we
can obtain the optimal 
 in the sense of ML, similar to the discussion in
Section 3.4.3. By differentiating the Q function with respect to 
, we can derive
the following equation:
(3.173)
Here, we use the following matrix formulas for the derivation in Appendix B.3:
(3.174)
Thus, we can obtain the following analytical solution:
(3.175)
Therefore, the optimized mean vector parameter is represented as:
(3.176)
Therefore, 
 is analytically obtained by using the statistics (
 and 
 in
Eq. (3.169)) and initial HMM parameters (
 and 
). This solution corresponds
to the M-step of the EM algorithm, and the E-step is performed by the forward–
backward algorithm, similar to that of HMMs, to compute these statistics. The
training procedure is shown in Algorithm 5.

Algorithm 5 Maximum likelihood linear regression
Require:  Θ and 
 
1:  repeat 
2:     Compute the occupation probability 
. 
3:     Accumulate the sufficient statistics 
, 
, 
, 
, and 
 
4:     Estimate the transformation parameters 
 
5:     Update the HMM parameters Θ 
6:  until Convergence
MLLR is one of the most popular techniques for acoustic modeling, and
there are many variants of transformation types for HMMs, e.g., Sankar &
Lee (1996), Chien, Lee & Wang (1997), Chen, Liau, Wang et al. (2000), Mak,
Kwok & Ho (2005) and Delcroix et al. (2009). In addition to speech recognition,
there are many other applications which are based on the adaptive training of
HMMs (e.g., speech synthesis (Tamura, Masuko, Tokuda et al. 2001), speaker
verification (Stolcke, Ferrer, Kajarekar et al. 2005), face recognition (Sanderson,
Bengio & Gao 2006) and activity recognition (Maekawa & Watanabe 2011)).
3.6   n-gram with smoothing techniques
As we discussed in Section 3.1, a language model (LM) is known as crucial prior
information for large vocabulary continuous speech recognition (LVCSR),
according to the Bayes decision rule:
(3.177)
Many other applications include document classification, information retrieval,
optical character recognition, machine translation, writing correction, and bio-
informatics. An overview of language modeling in LVCSR systems has been
given 
in 
Chen 
&
Goodman (1999), Kita (1999), Rosenfeld (2000), Bellegarda (2004), and Saon &
Chien (2012b).

A language model is a probability distribution p(W) over a sequence of word
strings 
 that describes how likely it is that the word
sequence W occurs as a sentence in some domain of interest.5 Recall that word w
is represented by a string, and it is an element of a set of distinct words 
,
which is also called a vocabulary or dictionary. Here, 
 is a word at position i.
The word string 
 and the continuous speech vector 
 are both
sequential patterns but in different data types and different time scales.
To get used to this notation, we provide several examples to represent the
following word sequence:
This sentence has five words and one period at the end of the sentence. By
regarding the period as one word,6 this sentence is totally composed of a six-
word sequence (i.e., 
), and can be represented by
(3.178)
Note that the vocabulary for this sentence is composed of distinct unique words
represented as:
(3.179)
and the vocabulary size in this example is
(3.180)
We can also introduce the following summation over vocabulary 
, which is
important in this section:
(3.181)

This summation is performed over each vocabulary in Eq. (3.179), and not over
a position i in Eq. (3.178).
Basically, the prior word probability is employed to characterize the
regularities in natural language. The probability of a word sequence 
 is represented based on the product rule as:
(3.182)
where Θ denotes the n-gram parameters, namely the n-gram probabilities, which
is explained later. Here, to describe the word sequence from ith word to nth
word, we use the following notation:
(3.183)
We also define the following special cases:
(3.184)
where 
 denotes an empty set. For example, the word sequences 
 are represented as
(3.185)
and so on. When 
 in Eq. (3.182), the conditional distribution is written as:

(3.186)
However, it makes the model larger, as the number of words in a sequence is
larger, and we need to model it with the fixed model size. Thus, Eq. (3.182) is
approximated with the 
th order Markov assumption by multiplying the
probabilities of a predicted word 
 conditional on its preceding 
 words 
:
(3.187)
This model is called an n-gram model. Usually n is taken to be from 2 to 5,
which depends on the size of the training data and applications. When 
 in
Eq. (3.187), the conditional distribution is written as:
(3.188)
which is called a unigram model that does not depend on any history of words.
The n-gram parameter is defined as the weight given to a conditional word
sequence. The probabilistic distribution of 
 is parameterized as with a
multinomial distribution as:
(3.189)

where
(3.190)
Note that the number of distinct n-gram parameters for 
 would be the
index to the power of the vocabulary size 
, i.e., 
. In this section, we use
the following notation to present a set of n-gram parameters:
(3.191)
The number of parameters is a very large since the vocabulary size of LVCSR
would be more than 50 000, and the main problem of language modeling is how
to compactly represent these parameters.
The straightforward way to estimate the multinomial distribution of an n-
gram 
 from a text corpus 
 is to compute the ML estimate by
(3.192)
where the multinomial likelihood function is obtained from the definition in
Appendix C.2 by
(3.193)

where 
 denotes the number of occurrences of word sequence 
 in
training corpus 
. To estimate the parameter, similarly to the state transitions
and mixture weights in the HMM in Section 3.4.3, we introduce a Lagrange
multiplier η and solve the constrained optimization problem by maximizing
(3.194)
Setting the derivative of Eq. (3.194) with respect to 
 to zero, we obtain
(3.195)
By substituting this result into constraint Eq. (3.190), we find the value of the
Lagrange multiplier
(3.196)
and the ML solution in the form of
(3.197)
Without loss of generality, we neglect the notation Θ in the following
expressions. The goal of the most popularly used language models, trigram

models, is to determine the probability of a word given the previous two words 
, which is estimated as the number of times the word sequence 
 occurs in some corpus of training data divided by the number
of times the word sequence 
 occurs.
However, again the number of n-gram parameters depends on the number of
word combinations in a word sequence 
, which is
counted as the number of different words 
 at different temporal positions
from 
 to i. This number is exponentially increased by involving large
n. Although n-gram is effective at exploiting local lexical regularities, it suffers
from the inadequacies of training data and long-distance information due to too
many word combinations and too narrow an n-gram window size, respectively.
These limitations substantially weaken the regularization of the trained n-gram
models and the prediction for unseen words. The limitation of n-gram window
size could be resolved by exploiting large-span latent semantic information
(Hofmann 1999b, Bellegarda 2000), which is addressed in Section 3.7. In what
follows, we address different smoothing solutions to the problem of insufficient
training data in the n-gram model.
3.6.1
 Class-based model smoothing
A simple and meaningful approach to tackle the data sparseness problem is to
consider the transition probabilities between classes rather than words, namely to
adopt the class-based n-gram language model (Brown, Desouza, Mercer
et al. 1992, Chen 2009):
(3.198)
where 
 is the class assignment of word 
, 
 is the probability of
word 
, generated from class 
, and 
 is the class n-gram. The class
assignments of different words are determined beforehand according to the word
clustering using the metric of mutual information. An existing linguistic class
(e.g., part of speech) is also used to provide the class assignments.
The word probability given a class 
 is usually estimated by using the
ML estimation, similarly to Eq. (3.197), as:

(3.199)
where 
 is the number of word counts labeled by both 
 in the
corpus 
. The class n-gram probability 
 is estimated by using the ML
estimation (replacing the word counts in Eq. (3.197) with class counts), or a
smoothing technique, explained in the following sections.
The model parameters are composed of
(3.200)
Both are represented by the multinomial distributions. Since the number of
distinct classes 
 is much smaller than the vocabulary size 
, the model size
is significantly reduced. Model parameters could be reliably estimated. For
example, the number of parameters of 
 is 
, and the number of
parameters of 
 is 
. Since 
, the total number of
parameters of a class n-gram model 
 is much smaller than an n-
gram model 
. The class-based n-gram is also seen as a smoothed language
model.
3.6.2
 Jelinek–Mercer smoothing
As reported in Chen & Goodman (1999), it is usual to deal with the issue of data
sparseness in an n-gram model by using a linear interpolation method where the
nth order language model 
 is estimated by interpolating with the 
th order language model in a form of
(3.201)
where 
 denotes the interpolation weight which is estimated for each 
 in

accordance with the ML method. The reason the interpolation weight 
 does
not depend on 
 comes from the constraint of the sum-to-one property of an n-
gram model. The nth order smoothed model is calculated recursively as a linear
interpolation between the nth order ML model and the 
th order smoothed
model, that is
(3.202)
To carry through this recursive process to the end, we can take the smoothed
unigram or first-order model to be the ML distribution in Eq. (3.192), i.e., when 
,
(3.203)
where 
 is obtained from Eq. (3.197) as:
(3.204)
Or we can take the smoothed zeroth-order model to be the discrete uniform
distribution (Appendix C.1),
(3.205)
where the uniform distribution is defined with the vocabulary size 
 as:
(3.206)

Now, let us consider the original interpolation equation Eq. (3.201). The
parameter 
 is estimated individually for the 
 that maximizes the
probability of some of the data. Practically, the selection could be done for
buckets of parameters. In general, this class of interpolated models is also known
as the n-gram model with Jelinek–Mercer smoothing (Jelinek & Mercer 1980),
which is a standard form of interpolation smoothing. The smoothing techniques
in the following sections (Witten–Bell (WB) smoothing in Section 3.6.3,
absolute discount in Section 3.6.4, Kneser–Ney (KN) smoothing in Eq. (3.232),
and PLSA smoothing in Eq. (3.319)) provide specific interpolation weights in
this standard form, as shown in Table 3.1.
An important constraint of the n-gram model is that the summation over 
goes to 1, as shown in Eq. (3.190). The Jelinek–Mercer smoothing (and the
following smoothing techniques) satisfies this condition, i.e.,
(3.207)
We can prove this condition recursively from the unigram case to the n-gram
case. First, it is obvious that for the unigram models for the Eqs. (3.203) and
(3.205) cases:
(3.208)
Next, the bi-gram case is also proved by:
(3.209)

where we use Eq. (3.208) and 
. That is proven in the n-gram case,
trivially. The important property of this proof of the sum-to-one condition is that
the summation over 
 does not depend on the interpolation weight 
.
3.6.3
 Witten–Bell smoothing
Witten–Bell smoothing (Witten & Bell 1991) is considered to be an instance of
interpolation smoothing as addressed in Section 3.6.2 by setting a specific value
for the interpolation parameter 
. The Witten–Bell smoothing first defines
the following number based on the number of unique words that follow the
history 
:
(3.210)
The notation 
 represents the number of distinct words that have one or more
counts, and the 
 represents any possible words at i with this condition. By
using 
, the Witten–Bell smoothing assigns the factor 
 for the
lower-order model (the second term in Eq. (3.201)) where
(3.211)
This factor is interpreted as the frequency with which we should use the lower-
order model to predict the next word. It is meaningful that more unique words
appearing after history words 
, i.e., larger 
, implies that more
reliable values of the 
-grams are estimated, and the higher weight factor 
 should be assigned for 
-grams. Similarly, the rest of the weight 
 (the first term in Eq. (3.201)) is represented from Eq. (3.211) as

(3.212)
This is the modified ML estimate with the unique word count 
.
According to the interpolation smoothing in Eq. (3.201) and Eqs. (3.211) and
(3.212), the Witten–Bell smoothing is expressed by
(3.213)
Note that the factor used in the Witten–Bell smoothing in Eq. (3.211) only
depends on 
, and has the same dependency as 
. Therefore, it is trivial
that the Witten–Bell smoothing satisfies the sum-to-one condition, as it is a
special solution of the interpolation smoothing (Eq. (3.201)) that satisfies the
sum-to-one condition.
3.6.4
 Absolute discounting
Absolute discounting is also considered to be an instance of interpolation
smoothing as addressed in Section 3.6.2. However, the equation form is not
represented as the Jelinek–Mercer form, which we set down again as follows for
comparison:
(3.214)
Recall that Eq. (3.214) has the weight 
 depending on the previous word
sequence 
, and is composed of the ML probability 
 and lower-

order probability 
. However, in absolute discounting, instead of
multiplying the higher-order ML model by a factor 
, the higher-order
distribution is created by subtracting a fixed discount d for the case of non-zero
count. The absolute discounting is defined by
(3.215)
where 
 denotes a discounting parameter.
The interpolation weight 
 is formed with the unique word count 
 defined in Eq. (3.210) as:
(3.216)
Note that the weight does not depend on 
, but 
, similarly to the Jelinek–
Mercer form in Eq. (3.214). The way to find weight parameter 
 is again
based on the sum-to-one condition. Consider the condition
(3.217)
Then, by taking the summation over 
 for both sides in Eq. (3.215),
Eq. (3.215) can be rewritten as:

(3.218)
Therefore,
(3.219)
Now we focus on the numerator in Eq. (3.219) that represents the total discount
value from d. By considering the cases when 
 and 
, we
can derive the following equation:
(3.220)
Therefore, by substituting Eq. (3.220) into Eq. (3.219), and by using the unique
word count 
, Eq. (3.219) is finally represented as:
(3.221)
where 
 means that the summation is undertaken for a subset of distinct
words in the vocabulary 
 that satisfies the condition 
. Since d does
not depend on 
, the numerator can be represented with d times the number of
distinct words 
 that satisfies the condition 
, which corresponds to 
. Thus, we prove Eq. (3.216).

This weight means that the discount d in an observed n-gram event 
 at current word 
 is distributed to compensate for those unseen
events 
 where a lower-order model 
 is adopted. The
discount d is shared for all n-grams and could be measured by using the total
number of n-grams with exactly one and two counts, i.e., 
 and 
 in the training data, respectively (Ney, Essen & Kneser 1994).
In summary, the absolute discounting is represented by
(3.222)
Note that the absolute discounting does not include the exact ML probabilities
(although they can be included by re-arranging the first term). The weight
parameter for the lower-order model is proportional to the value 
,
which has a similarity to that in Witten–Bell smoothed n-grams, as seen in
Eq. (3.211).
Thus, we have explained Witten–Bell smoothing in Section 3.6.3 and
absolute discount in Section 3.6.4 as instances of the interpolation (Jelinek–
Mercer) smoothing techniques. The next section introduces another type of well-
known smoothing technique called backoff smoothing, with Katz smoothing as
an example.
3.6.5
 Katz smoothing
Katz smoothing (Katz 1987) was developed by intuitively combining higher-
order models with lower-order models through scaling of the ML distribution.
Taking a bi-gram as an example, the Katz smoothing is performed by calculating
the probability by considering the cases where the co-occurrence count 
 of 
 and 
 is zero or positive as follows:
(3.223)

where 
 is a discount ratio that reduces the probability estimated from the
ML. The discount ratio is usually is obtained by the Good–Turing estimate
(Good 1953), and this book does not describe it in detail. The expression 
 is a scaling factor that only depends on the previous word 
,
while 
 is a unigram probability, and we usually use the ML unigram
probability for the bi-gram case, i.e.,
(3.224)
Now we focus on the scaling factor 
, which is obtained by the sum-
to-one condition of 
 and 
 that must be satisfied for any
probabilistic distributions. By summing over 
 in both sides of Eq. (3.223), the
equation can be rewritten as
(3.225)
This leads to the following equation:
(3.226)
where 
 or 
 means that the summation is undertaken for a
subset of distinct words in the vocabulary 
 that satisfies the condition 
 or 
. Thus, we obtain that
(3.227)

This smoothing technique can be generalized to the n-gram probability as
(3.228)

where
(3.229)
This smoothing scheme is known as a realization of backoff smoothing. This
smoothing is obtained by the weighted multiplication of the ML probability and
is different from interpolation (Jelinek–Mercer) smoothing, as discussed in
Section 3.6.2, which is obtained by the weighted summation of the ML
probability. Note that both smoothing techniques include some free parameters
(weight 
 in interpolation smoothing and the discount factor d in backoff
smoothing), but the other parameters are obtained by using the sum-to-one
constraint of the probability distribution. Similarly to the meaning of , backoff
smoothing relies more on the lower-order n-gram probability when d is small
(close to 0), while backoff smoothing relies more on the ML n-gram probability
when d is large (close to 1). The next section describes a famous (modified)
Kneser–Ney smoothing, which can be realized with both interpolation and
backoff smoothing methods.
3.6.6
 Kneser–Ney smoothing
Kneser–Ney (KN) smoothing (Kneser & Ney 1995) can be interpreted as an
extension of the absolute discount approach, as discussed in Section 3.6.4. When
the highest-order probability case is considered, it is same as the absolute
discount. However, the unique property of KN smoothing is that it provides a
special lower-order probability based on the numbers of distinct words.
For example, in the highest-order probability case, the original Kneser–Ney
method was developed as the following backoff smoothed model:
(3.230)

where 
 is chosen to make the distribution sum to 1 and has the form
(3.231)
Alternatively, the Kneser–Ney model could be estimated according to the
interpolation smoothing scheme in the highest-order probability case based on
(3.232)
Note that the first term of Eq. (3.232) is calculated from a modification of the
ML probability, and Eq. (3.232) is exactly the same as the absolute discounting
in Eq. (3.222), except for the lower-order probability 
. However, in
the lower-order probability, instead of using Eq. (3.232) recursively by changing
n to 
, the Kneser–Ney smoothing provides more intuitive probability than
the ML probability for the first term by considering the continuation of words.
For a simple explanation, we consider the bi-gram case (the highest-order
probability is a bi-gram probability), i.e., Eq. (3.232) is represented as:
(3.233)
The question here is whether we really use the unigram probability for 
.
This is often illustrated with an example of the bi-gram “San Francisco” (Chen
& Goodman 1999, Jurafsky 2014). In the training data of the Wall Street Journal
(WSJ0) corpus (Paul & Baker 1992), the bi-gram “San Francisco” appears 3222
times, while the word “Francisco” appeared 3329 times. The other word
“glasses” appears 185 times. That is

(3.234)
From these statistics, we can say that:
The word “Francisco” almost always (96.8%) follows “San,” and does not
follow the other words in most cases.
However, the word “Francisco” is more common (18 times more) than
“glasses.”
Then, let us consider the following sentences:
1.    I can’t see without my reading “glasses.”
2.    I can’t see without my reading “Francisco.”
We consider the case when there are no “reading glasses” and “reading
Francisco” in a training corpus, and the lower-order probability 
 (the
second term in (3.233)) is used to compute these sentence probabilities.
Intuitively, we want to make the probability of the first sentence with “glasses”
higher than the other. However, if we use the count-oriented (ML-like)
probability for 
, the sentence with “Francisco” is assigned higher
probability because the simple word count of “Francisco” is much larger than
that of “glasses.” This problem can be avoided by considering the continuity of
word sequences rather than word counts in the lower-order probability.7
The KN smoothing provides the following unigram probability that does not
use the word count (ML-like) information:
(3.235)
Here, from the definition of the unique count 
 in Eq. (3.210), 

and 
 are represented as follows:
(3.236)
(3.237)
In the previous example,
(3.238)
The unique count of 
 is larger than that of 
. Thus, the probability of the sentence with “glasses”
becomes larger when we use the number of unique words in (3.235), which is a
more intuitive result in this example.
The probability based on the unique counts can be generalized from the
unigram case in Eq. (3.235) to higher-order n-gram probabilities, except for the
highest-order n-gram that uses the absolute discounting from Eq. (3.232). For
example, 
, which is the second term in Eq. (3.232), is represented as
follows:
(3.239)

where
(3.240)
Thus, we can obtain the interpolation version of the Kneser–Ney smoothing for
all cases, which is summarized as follows:
The highest-order n-gram probability
(3.241)
Lower-order n-gram probability
(3.242)
Unigram probability
(3.243)
In general, the interpolated n-gram model yields better performance than the
backoff n-gram model (Chen & Goodman 1999).
Modified Kneser–Ney smoothing

In Kneser–Ney smoothing and discounting smoothing, a discount parameter d in
Eq. (3.241) plays an important role in striking a balance between the target-order
n-gram probability and the lower-order one. Then, it is a simple question
whether the single discount parameter d is precise enough to handle the balance.
Basically, the distribution of distinct words follows the power-law property
where only a few frequent words form a high proportion of the total, but a lot of
rare words occur only one or two times, i.e.,
(3.244)
This power-law property comes from the “rich-get-richer behavior” in natural
language, and this property also applies to n-grams, as well as words. Thus,
Kneser–Ney smoothing is modified by separately using three discount
parameters 
, 
, and 
 for those n-grams with one, two, and three or more
counts, respectively, instead of using a single discount d for all non-zero counts,
as follows:
(3.245)
These parameters are empirically determined as (Chen & Goodman 1999):
(3.246)
(3.247)
(3.248)
(3.249)

where 
 is the total number of n-grams appearing j times in a training corpus,
i.e.,
(3.250)
Based on the new discounting parameter 
 in Eq. (3.245), a modified
Kneser–Ney (MKN) smoothing is conducted to estimate the smoothed n-grams
using
(3.251)
where 
 is derived by considering the sum-to-one condition. By taking the
summation over 
 in both sides of Eq. (3.251), Eq. (3.251) is represented as
follows:
(3.252)
Now we focus on the first term of the 4th line in Eq. (3.252). By using
Eq. (3.245), this term can be represented as:

(3.253)
Thus, by substituting Eq. (3.253) into Eq. (3.252), we obtain

(3.254)
The MKN smoothed n-grams were shown to perform better than the previously
described smoothed n-grams in terms of perplexity and word error rates for
LVCSR tasks (Chen & Goodman 1999).
Table 3.1 summarizes different language model smoothing methods and their
corresponding smoothing techniques including interpolation smoothing and
backoff smoothing. (Modified) Kneser–Ney smoothing, addressed in Section
3.6.6, can be implemented in both interpolation smoothing and backoff
smoothing. Bayesian approaches extend these standard n-gram language models
to MAP estimation of an n-gram language model in Section 4.7, a hierarchical
Dirichlet language model in Section 5.3, and a hierarchical Pitman–Yor language
model in Section 8.5. Since n-gram language models always have to address the
sparse data problem, Bayesian approaches provide an elegant solution to deal
with the problem theoretically.
Table 3.1 Summary of different language model smoothing methods in terms
of interpolation smoothing and backoff smoothing.
Interpolation smoothing
Backoff smoothing
Jelinek–Mercer smoothing
Section 3.6.2
Witten–Bell smoothing
Section 3.6.3
Absolute discount
Section 3.6.4
Katz smoothing
Section 3.6.5
Kneser–Ney smoothing
Eq. (3.232)
Eq. (3.230)
PLSA smoothing
Eq. (3.319)
The next section considers another generative model of a text that can deal
with document information rather than a simple word sequence modeled by an
n-gram model.
3.7 Latent semantic information

One of the main limitations of the standard n-gram model is the inadequacy of
long-distance information caused by n-gram window size. How to extract
semantically meaningful information outside an n-gram window becomes crucial
to achieve large-span language modeling. Traditionally, the cache-based
language model (Kuhn & De Mori 1990) was proposed to exploit long-distance
information where the short-term pattern in history words is continuously
captured for word prediction. On the other hand, we may characterize long-
distance information by finding long-term semantic dependencies which are seen
as global constraints. The short-term statistics within an n-gram window serve as
local constraints. Combining local and global constraints provides complete and
structural information for word sequence probabilities.
In the literature, latent semantic analysis (LSA) (Berry, Dumais &
O’Brien 1995) has been popular for many years (Manning & Schütze 1999) to
construct latent topic space in information retrieval areas to evaluate the
similarity between a query and a document in that space. LSA was extended to
probabilistic latent semantic analysis (PLSA) (Hofmann 1999b, Hofmann 2001)
by dealing with latent semantics as latent variables, an approach which is based
on the maximum likelihood theory with the EM algorithm. Thus, PLSA provides
an additional generative model to an n-gram for a text that considers long-term
semantic information in the text. LSA and PLSA have been applied to develop
large-span 
language 
models 
or 
topic-based 
language 
models 
in
Bellegarda (2000) and in Gildea & Hofmann (1999), respectively, by combining
these with n-gram language models, and these are addressed below.
3.7.1
 Latent semantic analysis
Latent semantic analysis (LSA) introduces an additional longer-term index
document d to word w. The document usually holds several to thousands of
sentences, and the definition (e.g., a paragraph, article, journal paper, etc.)
depends on target applications. Suppose there are M documents in a corpus 
,
which include words with vocabulary size 
. LSA focuses on the 
word-document matrix W:

(3.255)
The element of a word-document matrix 
 is often represented based on a
co-occurrence based count 
, which is the number of occurrences of
word 
 in document 
. The word 
 is a word in vocabulary 
pointed by an ordered index 
, and 
 in the subscript does not
denote a position in a word sequence. The co-occurrence element is often
weighted by considering the importance of words in documents. As we discuss
later, tf–idf (term frequency–inverse document frequency) or information-
theoretic measure is used as an instance of the co-occurrence element. Note that
in this word-document matrix representation, we only consider the count
information (or related information) for each distinct word 
 and do not
consider the sequential information of words (e.g., 
) in a document. This
feature 
representation 
of 
words 
is 
also 
called 
bag 
of 
words
(Lewis 1998, Joachims 2002, Blei et al. 2003). The column vector 
can represent information about document m with a vector. This representation is
called a vector space model of a text.
Document similarity
The problem of dealing with this matrix is that the column vector 
 is a
sparse representation from natural language, and it is difficult to obtain the
semantic information from this representation. For example, if we consider the
similarity between documents 
 and 
, from the word-document matrix W
with well-known cosine similarity, the cosine similarity is defined as
(3.256)
The cosine similarity has the following property, and it is often used to measure
the similarity in natural language processing:
(3.257)
The similarity between documents 
 and 
 based on the 
 dimensional

space can be calculated as follows:
(3.258)
Since 
 and 
 are very sparse, most of the products are zero, and the
cosine similarity cannot obtain meaningful similarity scores. In addition, the
number of dimensions (vocabulary size 
, the number of documents M, or
both) is too large to use. Therefore, LSA conducts a singular value
decomposition (SVD) over the 
 word-document matrix W and obtains
(3.259)
as shown in Figure 3.9. In Eq. (3.259), S is a 
 diagonal matrix with
reduced dimension, 
, U is a 
 matrix whose columns
are the first K eigenvectors derived from word-by-word correlation matrix 
, and V is an 
 matrix whose columns are the first K
eigenvectors derived from the document-by-document correlation matrix 
.
Figure 3.9 Singular value decomposition for latent semantic analysis.
Document similarity in LSA
After the projection, instead of focusing on the original 
 word-
document matrix W, LSA focuses on the factored 
 matrix 
. Each

column of 
 characterizes the location of a particular document in the
reduced K-dimensional semantic space. Therefore, we define the following
document vector 
 as an mth column vector of 
, which is a lower
dimensional vector than 
:
(3.260)
This can be weighted by the diagonal matrix S. Therefore, the similarity between
documents 
 and 
 based on this lower K-dimensional representation is
calculated as
(3.261)
Compared with Eq. (3.258), 
 is a denser feature, and can provide more
(semantically) meaningful information (Manning & Schütze 1999).
Now, we consider the application based on information retrieval. We have a
query q, which is composed of several words and can be regarded as an instance
of a document. Then the problem of information retrieval is to search similar
documents to query q, and computing the similarity between q and existing
document 
 is very important. To evaluate the cosine similarity between the
existing document 
 and this query q in the K-dimensional space, we first
transform the corresponding occurrence vector 
 to the K-dimensional vector 
 as follows:
(3.262)
Thus, the cosine similarity between q and 
 can be computed based on
Eq. (3.261) as follows:

(3.263)
Thus, we can also obtain the similarity between the query q and document 
with more (semantically) meaningful space, which can be used for information
retrieval.
Word-document matrix
The discussion so far does not explicitly introduce a suitable value for the co-
occurrence element 
 of the word-document matrix W. Actually it is
empirically determined, and the straightforward representation is based on the
co-occurrence based count 
:
(3.264)
This is the most simple representation of W, and this representation is actually
extended as a probabilistic version of LSA (PLSA) in the next section. Another
popular representation is based on tf–idf (Salton & Buckley 1988, Manning &
Schütze 1999), which represents the element as follows:
(3.265)
where 
 is called term frequency, defined as
(3.266)
This is computed from the co-occurrence based count 
 in Eq. (3.264)
with a normalization factor. The quantity 
 is called the inverse document
frequency, 
and 
it 
is 
computed 
from 
the 
number 
of 
documents 
 that include word 
:

(3.267)
 would score a lower weight for the co-occurrence element when the
word 
 has appeared in many documents, since such a word would be less
important.
Bellegarda (2000) also proposes another weight based on an information-
theoretic measure as follows:
(3.268)
Compared with Eq. (3.265), the inverse document frequency in Eq. (3.265) is
replaced with 
, where 
 denotes the normalized entropy of word 
in a corpus, defined as
(3.269)
The entropy of 
 would be increased when 
 is distributed among many
documents, which decreases the weight 
. Therefore, similarly to tf–idf,
the word distributed in many documents would be less important, and have
lower weight for the co-occurrence element.
3.7.2
 LSA language model
The LSA language model was proposed to capture long-range word
dependencies through discovery of latent topics from a text corpus. When
incorporating LSA into an n-gram model, the prediction of word 
 making use
of n-gram probability 
 is calculated from two information sources in
history:

1.    The n-gram history words
(3.270)
2.    The long-term topic information of all history words 
, which is
represented by co-occurrence count (Eq. (3.264)), tf–idf (Eq. (3.265)), or
information-theoretic measure (Eq. (3.268)), as discussed in the previous
section:
(3.271)
The K-dimensional projected vector 
, from the history vector in the
original space 
 onto the LSA space, is obtained based on Eq. (3.262)
as:
(3.272)
By using these two types of history information, the LSA language model is
represented by using the product and sum rules as:
(3.273)
From the definition of 
 in Eq. (3.270), 
 is represented as the following
n-gram probability:
(3.274)

This can be calculated based on Section 3.6. Next, we focus on the distribution
of the long-term topic information 
 in Eq. (3.273). By using the
definitions of 
 and 
 in Eqs. (3.270) and (3.271), the distribution can be
rewritten as:
(3.275)
The above approximation assumes that the document vector 
 only
depends on 
 and does not depend on the word history 
, in accordance
with the discussion in Bellegarda (2000). This approximation would be effective
when 
 is a content word. In addition, by using the Bayes theorem, 
can be further rewritten as follows:
(3.276)
where 
 is easily calculated from a unigram probability. The 
 is a
unigram probability given the document vector. Thus, the LSA language model
is finally represented as follows:
(3.277)
Here, how to compute 
 is an important problem. This is performed in
the projected LSA space, rather than the original high dimensional space.
The 
 is determined by the cosine measure between word 
 and
pseudo document 
 in the LSA space, as we discussed in Eq. (3.261). First, 
is converted with the corresponding co-occurrence vector in the 
 dimensional
space as

(3.278)
Vector 
 is a one shot vector in the 
 dimensional space, where the element
is 1 when 
 and 0 otherwise. The document vector in the LSA space
for 
 is obtained from Eq. (3.262) as
(3.279)
Therefore, based on this equation and Eq. (3.272), the cosine similarity in the
LSA K-dimensional space is obtained as:
(3.280)
However, since a cosine value goes to negative, it cannot be used as a probability
of 
 that must satisfy the non-negativity. Coccaro & Jurafsky (1998)
propose the following value as a probabilistic distribution:
(3.281)
where Z is a normalization constant. The minimum cosine similarity prevents a
negative value, and it is also scaled by a tuning parameter γ.
In Eq. (3.280), the pseudo document vector 
 is recursively updated
from 
 to 
 by using
(3.282)
where 
 denotes the number of words appearing in word sequence :

(3.283)
and 
 is a normalized entropy, as we defined in Eq. (3.269). Eq. (3.282) is
obtained as a pseudo document vector in matrix W, which is defined according
to the entry value of matrix W as given in Eq. (3.268). This updating is
equivalent to computing
(3.284)
This equation is obtained in accordance with Eqs. (3.272) and (3.279). Having
the updating formula for 
 or 
 for pseudo document vector or history
words 
 and the projection vector 
 of current word 
 in common topic
space, we calculate the n-gram based on topic information in Eq. (3.280) and
substitute it into an LSA language model according to Eq. (3.273).
Thus, we can consider a long-term effect in a language model by using LSA.
However, to integrate it with a language model, we need to provide a
probabilistic treatment for LSA, e.g., Eq. (3.281), which is a rather heuristic
approach. The next section introduces a fully probabilistic formulation of LSA,
PLSA with an elegant inference and statistical learning methods based on the
EM algorithm.
3.7.3
 Probabilistic latent semantic analysis
LSA was extended to the probabilistic latent semantic analysis (PLSA) and
applied for document representation in Hofmann (1999b) and Hofmann (2001).
We first introduce a 
-length word sequence, given a document 
, as
(3.285)
We also introduce a set of M documents as

(3.286)
The first step of the ML procedure within the statistical model framework is to
provide a joint likelihood function of 
 of the word sequence and document,
which can be represented as:
(3.287)
where 
 is a set of the word sequences and documents in a corpus, which is
defined as:
(3.288)
Equation (3.287) can be factorized by using the product rule and the conditional
independence assumption of a word generative model given document 
 as:
(3.289)
This can be modeled by the word-document representation (e.g., word-document
matrix W in Eq. (3.255)), as we discussed in Section 3.7.1. However, similarly
to the LSA case, this representation has too many sparse variables, and it is
difficult to deal with this representation directly. Instead, we introduce a latent
topic variable 
, where K is the number of topics and each topic
corresponds to a higher concept of words (e.g., politics and sports, if we deal
with news articles). This model is called a latent topic model. Then we also
introduce a latent topic sequence 
 for a corresponding word sequence 
 for a
document 
 as

(3.290)
Then, we model that the words 
 are conditionally independent given the
corresponding topic 
, and the probability of 
 given 
 is represented by the
following mixture model:
(3.291)
This model factorizes the word probability into the topic-dependent unigram
probability 
 and the topic proportion probability 
. This
factorization corresponds to representing the huge size of a 
 word-
document matrix with the reduced subspace K used in an LSA, as discussed in
Section 3.7.1.
Thus, we can construct a likelihood function of 
 as
(3.292)
This model for representing a generative process of the word sequences and
documents is called probabilistic latent semantic analysis (PLSA). Here, the
PLSA model parameters 
 consist of two sets of
multinomial parameters. The complete data likelihood function of 
 and Z can
be represented as:
(3.293)
Figure 3.10 provides a graphical model of PLSA and Algorithm 6 provides a
generative process of PLSA.

Algorithm 6 Generative process of probabilistic latent semantic analysis.
Require:  M, 
 
1:  for 
 do 
2:     Draw 
 from 
 
3:     for 
 do 
4:        Draw 
 from 
 
5:        Draw 
 from 
 
6:     end for 
7:  end for
Figure 3.10 Graphical model of probabilistic latent semantic analysis.
Parameter estimation based on EM algorithm
Once we obtain the likelihood function, maximum likelihood (ML) theory is
applied to estimate PLSA parameters by
(3.294)
Since PLSA has a latent variable z, it can be efficiently solved by using the EM
algorithm, as we discussed in Section 3.4. The EM algorithm is introduced to
find ML estimates 
. In the E-step of the EM algorithm, we calculate an
auxiliary function of new parameters 
 given the current parameters Θ. The
function is obtained by replacing HMM latent variables S and V, and speech
feature observation O with PLSA latent variable Z and document observation 
, giving

(3.295)
By substituting Eq. (3.293) into Eq. (3.295), the auxiliary function can be
rewritten as
(3.296)
where 
 means a multinomial distribution with parameter 
. By using a
similar trick to that used in Eq. (3.102), we can change the order of the
summations, and by executing the summation over 
, Eq. (3.296) can be
further rewritten as
(3.297)
PLSA assumes that the above probabilistic distributions 
 and 
are represented by using the multinomial distribution, which is defined in
Appendix C.2 as:
(3.298)

Therefore, the topic-dependent unigram distribution 
 and topic
proportion distribution 
 are parameterized as:
(3.299)
Thus, the parameter Θ is represented as the following set of these multinomial
distributions:
(3.300)
Now, we focus on the posterior distribution of latent variable 
. From
the graphical model, 
 has the following conditional independence property:
(3.301)
The second probability means that the probability of topic k is given by the word
 with 
 and Θ, and it is iid for the position i in 
. Then, we can rewrite
Eq. (3.297) as the following equation:
(3.302)
where 
 is a co-occurrence count of word 
 in document 
. Note
that the summation over words in a document 
 is replaced with the
summation over distinct words within a dictionary 
. Equation (3.302) is
factorized into the topic-dependent unigram parameter 
 and the topic
proportion parameter 
 as follows:

(3.303)
Two multinomial parameters 
 and 
 should meet the constraints for
probability parameters,
(3.304)
Thus, Eq. (3.302) provides the auxiliary function of a PLSA with the parameter
constraint Eq. (3.304). Therefore, similarly to Section 3.4, we can iteratively
estimate parameters based on the EM algorithm.
M-step
In the M-step, we maximize the extended auxiliary function with respect to new
parameters 
 under constraints of Eq. (3.304). Again, as we discussed in
Section 3.4.3, this constrained optimization problem is solved through
introducing a Lagrange multiplier η and establishing the extended auxiliary

function of 
 in Eq. (3.303) as
(3.305)
By differentiating Eq. (3.305) with respect to 
 and setting it to zero, we obtain
(3.306)
Again, substituting this equation into constraint Eq. (3.304), we obtain the value
of Lagrange multiplier η,
(3.307)
Consequently, we find the following ML estimate of 
 (i.e., the topic-
dependent unigram proportion probability) for a PLSA topic model:
(3.308)
Similarly, the parameter of topic proportion probability 
 is estimated based on
the M-step as

(3.309)
Thus, we obtain the ML solutions of the PLSA parameters based on the EM
algorithm.
E-step
The E-step needs to consider the following posterior distribution of the latent
topic k with the previously estimated PLSA parameter Θ:
(3.310)
By using the sum and product rules and the conditional independence
(Eq. (3.291)), the posterior distribution is rewritten as
(3.311)
By using the multinomial parameters 
 and 
, this can be represented by
(3.312)
Thus, the posterior distribution can be computed by using the previously
estimated PLSA parameter Θ. We summarize the parameter estimation algorithm
of the PLSA in Algorithm 7. In a practical implementation, storing the above

posterior distribution 
 explicitly needs a huge size of memory (that
corresponds to store 
 data), and it is impossible to do that for
large-scale text data. Instead, the PLSA parameter update is performed by using
a matrix multiplication based operation. Actually, this PLSA parameter update
corresponds to the multiplicative matrix update of non-negative matrix
factorization (NMF) (Lee & Seung 1999) based on the KL divergence cost
function (Gaussier & Goutte 2005).
PLSA is another well-known generative model in addition to the n-gram
model that generates words with exponential family distributions (multinomial
distribution). Since the generative model is represented with exponential family
distributions, PLSA can be extended by using Bayesian approaches, as we
discussed in Section 2.1.3. This extension is called the latent Dirichlet allocation
(Blei et al. 2003), where we use a Dirichlet distribution as a conjugate prior,
which is discussed in Section 7.6. We also show that PLSA parameters can be
estimated efficiently by using the EM algorithm. However, PLSA does not
consider the short-span language property, unlike an n-gram model, and the next
section discusses how to involve the PLSA and n-gram for the use of a language
model.
Algorithm 7 EM algorithm for PLSA
Require:  
, 
 
1:  repeat 
2:     for 
 do 
3:        for 
 do 
4:           for 
 do 
5:              Compute the posterior probability 
 
6:           end for 
7:        end for 
8:     end for 
9:     for 
 do 
10:        for 
 do 
11:           Compute the topic-dependent unigram probability 
12:           
 
13:        end for 

14:     end for 
15:     for 
 do 
16:        for 
 do 
17:           Compute the topic proportion probability 
18:           
 
19:        end for 
20:     end for 
21:  
 
22:  until Convergence
3.7.4
 PLSA language model
In Gildea & Hofmann (1999), Akita & Kawahara (2004) and Mrva &
Woodland (2004), a PLSA framework was applied to estimate the n-gram model,
which is seen as a combination of large-span and short-span language models.
That is, we consider the short-span language model (n-gram)
(3.313)
and long-span language model
(3.314)
to obtain the probability of 
. As an alternative, a long-span language model 
, a cache-based language model (Kuhn & De Mori 1990), or a trigger-
based language model (Lau, Rosenfeld & Roukos 1993) are used as
conventional approaches. As an example of the document 
, we consider the
history of all words from the beginning 
. Given word sequence 
, latent
topic probabilities of the history 
 are inferred by using a PLSA model and
incorporated into an n-gram language model.
To obtain the latent topic probabilities 
, maximum likelihood PLSA
parameters 
 

 are estimated in advance from a large amount of training
documents in the training step, as we discussed in the previous section. Then,
given Θ as an initial parameter set of Algorithm 7, we estimate 
 in the test
step. However, to avoid over-training as we only use 
 and to avoid a high
computational cost in the test step, we only update 
 with fewer
iterations than the full estimation of the PLSA parameter Θ in the training step.
In addition, we use the following online EM algorithm (Neal & Hinton 1998) to
update topic posterior probabilities to make the estimation in an on-line manner
for every word position i:
(3.315)
Here, 
 in the first and second terms in the right-hand-side is obtained by
the previous estimation, and the first term is computed on-the-fly based on a pre-
computation of 
, which is obtained from 
 in Eq. (3.308). The
factors 
 and 
 denote a linear interpolation ratio, and the contribution of the
second term becomes larger when the length of the word sequence i is larger.
These factors are derived as a specific solution of the on-line EM algorithm, and
Mrva & Woodland (2004) suggest modifying these factors for practical use.
In an initialization stage (i.e., 
 in Eq. (3.184)), the initial topic
posterior probability could be obtained from a prior topic probability in the
training stage as follows:

(3.316)
where 
 is estimated from the maximum likelihood equation based on the
word co-occurrence counts 
, i.e.,
(3.317)
Once we obtain the topic proportion probability 
, the PLSA based n-gram
probability is finally calculated by
(3.318)
Instead of hard-clustering computation in class-based n-gram in Eq. (3.198),
PLSA n-gram in Eq. (3.318) performs a so-called soft-clustering computation by
marginalizing the topic index k in a Bayesian sense. This model is known as a
large-span language model because long-distance topics 
 and
history words 
 within the n-gram window as well as outside the n-gram
window are all taken into account for word prediction. However, since the
computation can be performed by considering the history words 
, it is not
pre-computed, unlike n-gram models, which makes the implementation of the

PLSA language model harder than n-gram models in ASR.
Smoothing with n-grams
PLSA n-grams could be further improved by combining with ML n-grams based
on additional linear interpolation with factor :
(3.319)
The interpolation parameter  could be found by maximizing likelihood or tuned
by maximizing the ASR performance of some validation data.
We can also use a unigram rescaling technique, which is approximated based
on the conditional independence assumption of 
 and 
:
(3.320)
Note that this representation does not hold the sum-to-one condition, and it must
be normalized.
A more precise unigram rescaling technique can be performed by using the
dynamic unigram marginal (also known as the minimum discrimination
information (MDI) adaptation), which can consider backoff probabilities in an n-
gram probability (Kneser, Peters & Klakow 1997, Niesler & Willett 2002, Tam
& Schultz 2006). First, we define the following unigram rescaling factor π for
word 
:
(3.321)

Then the new n-gram probability with a history of n-gram word sequence 
is represented as follows:
(3.322)
where 
 is an n-gram probability obtained in advance, and 
 and 
are normalization constants, which can be computed by:
(3.323)

and
(3.324)
Then, 
 is also iteratively calculated by 
, 
-gram
probability 
, and 
. Thus, the unigram rescaled language model
is obtained by modifying the backoff coefficients.
In this chapter, we address several language model methods tackling the
issues of small sample size and long-distance information. Different language
model smoothing methods including interpolation smoothing, backoff smoothing
and unigram smoothing are summarized. The Bayesian language modeling based
on MAP estimation and VB learning is addressed in Section 4.7 and Section 7.7,
respectively.
3.8 Revisit of automatic speech recognition with
Bayesian manner
In Chapter 2, we introduce the basic mathematical tools in the Bayesian
approach, including the sum and product rules, and the conditional
independence, and provide simple ways to obtain the posterior distributions. In
addition, throughout the discussions from Section 3.1 to Section 3.7, we also
introduce statistical models of speech and language, which are mainly used for
ASR. Based on these mathematical tools and statistical models, this section
revisits methods to formulate the whole ASR framework in a Bayesian manner,
as consistently as possible, by only using the sum rule, product rule, and
conditional independence, as discussed in Section 2.1. In particular, we focus on
how to train our ASR model based on the Bayesian manner. The aim of this
section is to show the limitation of simply using basic Bayesian tools for ASR,
which leads to the requirement of approximated Bayesian inference techniques
in the following chapters.

3.8.1
 Training and test (unseen) data for ASR
To deal with the ASR problem within a machine learning framework, we need to
consider two sets of data: training data and test data. The training data usually
consist of a sequence of speech feature vectors (O) and the corresponding label
sequence (W) for all utterances. The test data consist of only a sequence of
speech feature vectors (
) for all utterances, and there is a candidate label
sequence (
) among all possible word sequences. The ASR problem is to
correctly estimate the corresponding label sequence (
). Therefore, we
summarize these four variables as:
O: speech feature sequence for training (observed);
W: word sequence for training (observed);
: speech feature sequence for test (observed);
: a candidate of word sequences for test (not observed).
Since we have various possible candidates for 
, it is natural to consider the
following conditional probabilistic distribution of 
, given the other observed
variables, as:
(3.325)
Therefore, this section starts with the formulation based on this conditional
probability.
Note that if we want to estimate the correct word sequence, it is performed
by using the well-known MAP decision rule, as we discussed in Eq. (3.2):
(3.326)
Therefore, the following section focuses on 
 in more detail.
3.8.2
 Bayesian manner

Recalling the discussion in Section 2.1, the Bayesian manner deals with these
variables as the arguments of probability distributions as follows:
(3.327)
We should also recall that these probabilistic variables (x: continuous variable
and n: discrete variable) have the following mathematical properties:
(3.328)
(3.329)
(3.330)
(3.331)
These properties yield various benefits for probabilistic pattern recognition
problems. Based on this probabilistic variable treatment, we can apply the sum,
product rules for all the variables, and we can also use the Bayes theorem for
Eq. (3.325).
MAP decision rule
By replacing a with word sequence 
 and b with speech feature vector 
 in
Bayes theorem Eq. (2.7) in Section 2.1.1, we can derive the following noisy
channel model of speech recognition based on the well-known MAP decision
rule (Eq. (3.2)):

(3.332)
(3.333)
(3.334)
Here, the denominator of Eq. (3.333) is disregarded with the arg max operation,
since 
 does not depend on 
.
The product rule (Bayes theorem) based on the MAP decision theory
decomposes the posterior distribution 
 into the two generative models
of 
 and 
, i.e., acoustic model 
 and language model 
. Solving
Eq. (3.332) by directly obtaining 
 is called the discriminative approach.
The approach tries to infer what a speaker wants to say in his/her brain from the
observation 
, as shown in Figure 3.11.
Figure 3.11 Probabilistic speech recognition: discriminative approach.
On the other hand, the generative approach obtains 
 indirectly via
the generative models 
 and 
 based on Eq. (3.333). Therefore, the
approach tries to imitate the generation process of 
 given 
 in the acoustic
model and W itself in the language model, as shown in Figure 3.12. Since the
generation process comes from a physical phenomenon or linguistic phenomena,
we can involve various knowledge of the phenomena (e.g., articulatory models
in speech production and grammatical or semantic models in linguistics) in
principle. The rest of this section further discusses the Bayesian formulation
along with the generative approach.

Figure 3.12 Probabilistic speech recognition: generative approach.
3.8.3
 Learning generative models
Bayesian formulation reasonably extends the generative models to teach them
from training data, by adding training data (O and W) to conditional variables in
the target distribution, namely,
(3.335)
Usually, we also use the reasonable conditional independence assumption for the
language model expressed by 
. Now, we focus on the
acoustic model 
, and make this abstract distribution more
concrete.
3.8.4
 Sum rule for model
To make the distribution more concrete, we usually provide a model with the
distribution. For example, when we have some data to analyze, we first start to
consider what kind of models we use to make the problems concrete by
considering HMMs or GMMs for speech feature sequences, and n-gram for
word sequences. However, we do not know whether the model provided is
correct or not, and it should be tested by different model settings. Thus, the
model settings can be regarded as probabilistic variables in the Bayesian
formulation. In addition, the variation of setting model topologies, distribution
forms of prior and posterior distributions, and hyperparameters of these
distributions can also be treated as probabilistic variables, as shown in Table 3.2.
We call this probabilistic variable to denote the model setting model variable M.
Table 3.2 Examples of model variables.

Variation
Examples
Model types
HMM, GMM, SVM, neural network, etc.
Model topologies
# HMM states, # Gaussians, # hidden layers, etc.
Priors/posteriors
Gaussian, Laplace, Gamma, Bernoulli, etc.
Hyperparameters
parameters of priors/posteriors, kernel parameters, etc.
For example, once we decide to use an HMM for speech feature modeling,
we have many arbitrary properties for the model topology: (i) whether we use a
word unit, context-independent phoneme unit, or context-dependent phoneme
unit; (ii) left to right HMM, including skip transitions, or fully connected ergodic
HMM; (iii) how many shared-triphone HMM states and how many Gaussians in
these states. Some of the model variables dealing with this model structure are
called model structure variables, and so-called structure learning in the machine
learning field tries to optimize the model structure using a more general
framework than the Bayesian framework.
The model variable M can be involved in our abstract distribution
(Eq. (3.335)) of the acoustic model by using the sum rule (Eq. (2.3)) as follows:
(3.336)
As M would include continuous values (e.g., hyperparameters), the
marginalization over M should involve both summation and integration.
However, for simplicity, we use the summation in this formulation.
3.8.5
 Sum rule for model parameters and latent variables
Once we set a model variable to a specific value, we can also provide the
corresponding model parameters Θ and latent variables Z for training data and 
 for test data. For example, once we decide a model for the specific setting
based on a standard HMM–GMM with a fixed model topology, these variables
can also be involved in the distribution with M (Eq. (3.336)) by using the sum
rule (Eq. (2.3)) as follows:

(3.337)
However, it is really difficult to deal with this joint distribution, and we need to
factorize the distribution.
3.8.6
 
Factorization 
by 
product 
rule 
and 
conditional
independence
First, we factorize Eq. (3.337) by using the product rule (Eq. (2.4)), as follows:
(3.338)
In this formulation, we keep the joint distribution of 
 and 
 and do not
factorize them. This is because the distribution corresponds to the complete data
likelihood function, which is useful to handle latent models based on the EM
algorithm, as discussed in Section 3.4.
Since the dependencies of the distributions in Eq. (3.338) are very
complicated, we use the reasonable conditional independence assumptions for
these distributions as follows:
(3.339)
(3.340)
(3.341)

(3.342)
Equation (3.339) means that the test data are generated by the Θ and M and do
not depend on the training data (O and W) and their latent variable Z, explicitly.
The other assumptions just use the conditional independence of 
. By using the
assumptions, we can approximate the original distribution of 
 as
follows:
(3.343)
where we use the fact that 
 since Z does not depend on
the other distributions. We also introduce lexical category c, and further factorize
the equation as:
(3.344)
Thus, the acoustic model can be represented by the joint distribution of 
 and 
 (complete data likelihood) and the posterior distributions of model
parameters Θ and model M. Since 
 does not depend on training
data (O and W), which can be obtained by setting the model and its parameters,
we only focus on the two posterior distributions in Eq. (3.344).
3.8.7
 Posterior distributions
The posterior distributions of models and model parameters can be rewritten by
the following equations by using the sum and product rules (Eqs. (2.3) and
(2.4)).

Model parameter Θ:
(3.345)
(3.346)
Model M:
(3.347)
(3.348)
Note that the posterior distributions are represented by the following two types
of distributions:
Joint distribution of training data O and Z: 
 
;
Prior distributions of Θ and M : 
 and 
.
Again, the joint distribution can be obtained by setting the model and its
parameters. Once we set the prior distributions 
 and 
, we can obtain
the posterior distributions 
 and 
 by solving (3.346) and
(3.348). Then, the acoustic model likelihood can be computed by using
Eq. (3.344).
3.8.8
 Difficulties in speech and language applications
Although the Bayesian approach is powerful, it is very difficult to realize. One of
the most critical aspects is that we cannot solve the above equations. The
practical Bayesian approach rests on how to find appropriate approximations:
Chapter 4: Maximum a-posteriori approximation;
Chapter 5: Evidence approximation;
Chapter 6: Asymptotic approximation;
Chapter 7: Variational Bayes;

Chapter 8: Markov chain Monte Carlo.
In the machine learning field, other approximations are also studied actively
(e.g., loopy belief propagation (Murphy et al. 1999), expectation propagation
(Minka 2001)). This book introduces the above approximations in speech and
language applications.
1  In the actual search (
) process in Eq. (3.21), this alignment information is not fixed, and is
searched at the same time. Many techniques have been developed to efficiently search this huge space
considering the lexicon and word sequences (e.g., Ney, Haeb-Umbach, Tran et al. (1992) and weighted
finite state transducer (WFST) based techniques (Mohri, Pereira & Riley 2002, Hori &
Nakamura 2013)).
2  The optimization with respect to the posterior distribution 
 is more reasonable in the Bayesian
sense, and it will be discussed in the following chapters.
3  This is first described in Gales & Woodland (1996) as normalized domain MLLR. The structural Bayes
approach Shinoda & Lee (2001) for bias vector estimation in HMM adaptation also uses this normalized
representation.
4  k denotes a combination of all possible HMM states and mixture components. In the common HMM
representation (e.g., in this chapter), k can be represented by these two indexes in Eq. (3.109).
5  Some languages do not have word boundaries explicitly marked by white-space in a text (e.g., Japanese
and Chinese). Therefore, to process a text for language molding, these languages need an additional
word segmentation step (Sproat, Gale, Shih & Chang 1996, Matsumoto, Kitauchi, Yamashita
et al. 1999, Kudo 2005).
6  In the implementation, we additionally define the start of a sentence with an auxiliary symbol for
practical use, which makes the number of words seven in this example.
7  Actually “reading glasses” has appeared twice in the WSJ0 corpus, and the lower-order probability
cannot be used so much in the WSJ0 language model when we set d very small. This is another solution
to resolve this discontinuity problem by correcting a very large size of corpus.

Part II
Approximate inference

4    Maximum a-posteriori approximation
Maximum a-posteriori (MAP) approximation is a well-known and widely used
approximation for Bayesian inference. The approximation covers all variables
including model parameters Θ, latent variables Z, and classification categories C
(word sequence W in the automatic speech recognition case). For example, the
Viterbi algorithm (
) in the continuous density hidden Markov
model (CDHMM), as discussed in Section 3.3.2, corresponds to the MAP
approximation of latent variables, while the forward–backward algorithm, as
discussed in Section 3.3.1, corresponds to an exact inference of these variables.
As another example, the MAP decision rule (
) in Eq. (3.2) also
corresponds to the MAP approximation of inferring the posterior distribution of
classification categories. Since the final goal of automatic speech recognition is
to output the word sequence, the MAP approximation of the word sequence
matches the final goal.1 Thus, the MAP approximation can be applied to all
probabilistic variables in speech and language processing as an essential
technique.
This chapter starts to discuss the MAP approximation of Bayesian inference
in detail, but limits the discussion only to model parameters Θ in Section 4.1. In
the MAP approximation for model parameters, the prior distributions work as a
regularization of these parameters, which makes the estimation of the parameters
more robust than that of the maximum likelihood (ML) approach. Another
interesting property of the MAP approximation for model parameters is that we
can easily involve the inference of latent variables by extending the EM
algorithm from ML to MAP estimation. Section 4.2 describes the general EM
algorithm with the MAP approximation by following the ML-based EM
algorithm, as discussed in Section 3.4. Based on the general MAP–EM
algorithm, Section 4.3 provides MAP–EM solutions for CDHMM parameters,
and introduces the well-known applications based on speaker adaptation. Section

4.5 describes the parameter smoothing method in discriminative training of the
CDHMM, which actually corresponds to the MAP solution for discriminative
parameter estimation. Section 4.6 focuses on the MAP estimation of GMM
parameters, which is a subset of the MAP estimation of CDHMM parameters. It
is used to construct speaker GMMs that are used for speaker verification.
Section 4.7 provides an MAP solution of n-gram parameters that leads to one
instance of interpolation smoothing, as discussed in Section 3.6.2. Finally,
Section 4.8 deals with the adaptive MAP estimation of latent topic model
parameters.
4.1   MAP criterion for model parameters
This section begins with a general discussion of the MAP approximation for
model parameters Θ. For simplicity, we first review the posterior distribution of
model parameters given observations O without latent variables Z. Instead of
estimating posterior distributions, the MAP estimation focuses on the following
parameter estimation:
(4.1)
This corresponds to estimating the model parameter 
 given training data
O. By using the product and sum rules, as discussed in Section 2.1.1, we can
rewrite the above equation as follows:
(4.2)
Since 
 does not depend on Θ, we can avoid computing
this integral directly.2 Furthermore, if we use an exponential family distribution
for a likelihood function and a conjugate distribution for a prior distribution, as

discussed in Section 2.1.3, the MAP estimate is represented as the mode of the
corresponding conjugate posterior distribution, analytically. This is an advantage
of using conjugate distributions.3
Equation (4.2) also tells us that the posterior distribution is composed of the
likelihood function and the prior distribution, thus the estimation is based on the
maximum likelihood function with the additional contribution of the prior
distribution. That is, the prior distribution acts to regularize model parameters in
the ML estimation, as we discussed in Section 2.3.1 as the best-known Bayesian
advantage over ML. For example, let us consider the likelihood function 
 as a one-dimensional Gaussian distribution with mean μ
and precision as 1, and the prior distribution 
 as a one-dimensional
Gaussian distribution of the mean vector 
 and the scale parameter r. Then, the
MAP estimation can be represented as follows:
(4.3)
where from the second to the third lines, we use the definition of a Gaussian
distribution (Appendix C.5) as follows:
(4.4)
Thus, the optimization problem of the MAP solution corresponds to solving the
minimum mean square error (MMSE) estimation with the 
 regularization term
around 
. The scale parameter r behaves as a tuning parameter to balance the
MMSE estimation and the regularization term. These parameters are called
regularization parameters, which can be hyperparameters of the prior
distribution. Equation (4.3) can be analytically solved by using the conjugate
distribution rule, as discussed in Section 2.1.4, or by using the following

derivative method:
(4.5)
We obtain the MAP estimate of μ analytically as:
(4.6)
Thus, the regularization term sets a constraint for the ML estimate 
 with
a regularization constant r.
Similarly, if we use a Laplace distribution as a prior distribution, the prior
distribution works as an 
 regularization term. The Laplace distribution is
defined as follows (Appendix C.10):
(4.7)
Therefore, using 
 instead of 
, Eq. (4.3) is rewritten as
follows:

(4.8)
Thus, the prior distribution effect in the MAP parameter estimation is often
regarded as a regularization of parameters. Consequently, Eq. (4.3) can
incorporate the prior knowledge of parameters via hyperparameters 
, and the
MAP approximation retains the Bayesian advantage of use of prior knowledge,
as discussed in Section 2.3.1. Note that Eq. (4.8) is not differentiable with
respect to μ, and it does not have a well-defined conjugate distribution.
Therefore, the MAP estimation with the Laplace prior (  regularization) is often
undertaken by a numerical method.
Now, we introduce a useful mathematical operation for the MAP
approximation of model parameters. To compute the expected values of the
posterior distribution with respect to model parameters Θ, the MAP
approximation can use the following posterior distribution represented by a
Dirac delta function:
(4.9)
where the Dirac delta function has the following property:
(4.10)
This posterior distribution intuitively corresponds to having a location parameter
with the MAP estimate 
 and very small (0) variance. If the model
parameters are represented by discrete variables, we can use the Kronecker delta
function. Therefore, once we obtain the MAP estimation of model parameters 
, we can compute the expected value of function 
 based on Eq. (4.9)

as follows:
(4.11)
Here we use Eq. (4.10) to calculate the integral. Since this is equivalent to just
plugging in the MAP estimates to the 
, this procedure is called plug-in
MAP (Lee & Huo 2000). For example, if we use the likelihood function of
unseen data 
 for 
, Eq. (4.11) is rewritten as follows:
(4.12)
That is, the likelihood function of unseen data can be obtained by simply
replacing Θ with the MAP estimate 
. This can be used as a likelihood
function to compute likelihood values in prediction and classification steps. The
Dirac delta-function-based posterior representation is very useful, since the
representation connects the analytical relationship between the MAP-based point
estimation and the Bayesian distribution estimation.
Thus, the MAP approximation does not need to solve the marginalization
explicitly in the training and prediction/classification steps. Although the
approximation lacks the Bayesian advantages of the model selection and
marginalization, as discussed in Section 2.3, it still has the most effective
Bayesian advantage over ML, namely use of prior knowledge. Equation (4.3)
also shows that the effect of the prior distribution in the MAP estimation works
as a regularization. Therefore, the MAP approximation is widely used in
practical Bayesian applications. In addition, the MAP approximation is simply
extended to deal with latent variables based on the EM algorithm, which is a key
technique in training statistical models in speech and language processing, as we
discussed in Chapter 3. The next section discusses the MAP version of the EM
algorithm.
4.2   MAP extension of EM algorithm

As we discussed in Section 3.4, most statistical models used in speech and
language processing have to deal with latent variables Z, e.g., HMM states and
mixture components of the CDHMM in acoustic modeling, and latent topics in
language modeling. The maximum likelihood approach has an efficient solution
based on the EM algorithm, which optimizes the auxiliary function 
instead of a (log) likelihood function. This section describes the EM extension of
MAP parameter estimation in general.
4.2.1
 Auxiliary function
Following the discussion in Section 3.4, we prove that the EM steps ultimately
lead to the local optimum value 
 in terms of the MAP criterion. First, since
the logarithmic function is a monotonic function, the MAP criterion in Eq. (4.2)
is represented as follows:
(4.13)
By introducing latent variable Z, the above equation can be written as
(4.14)
As discussed in the ML–EM algorithm, the summation over latent variable 
is computationally very difficult since the latent variable in speech and language
processing is represented as a possible sequence, and the number of these
variables is exponential. Therefore, we need to avoid having to compute 
directly.
Similarly to the ML–EM algorithm in Section 3.4, in M-step, we maximize
the MAP version of the auxiliary function with respect to the parameters Θ, and
estimate new parameters by

(4.15)
The updated parameters 
 are then treated as the current parameters for the
next iteration of EM steps. The 
 is defined as the expectation of the
joint distribution 
 with respect to 
 as follows:
(4.16)
where 
 does not depend on Z, and can be separated from the expectation
operation. Compared with the ML auxiliary function 
 (Eq. (3.78)), we
have an additional term 
, which comes from a prior distribution of
model parameters.
Now we explain how optimization of the auxiliary function 
 leads to the
local optimization of 
 or 
. For the explanation, we define the
logarithmic function of the joint distribution 
 as
follows:
(4.17)
This is similar to 
 in Eq. (3.83), but 
 has an additional factor from 
. Now, we represent 
 in the above equation with the distributions of
latent variable Z based on the product rule of probabilistic variables, as follows:
(4.18)
Therefore, by substituting Eq. (4.18) into Eq. (4.17), we obtain

(4.19)
Now we perform the expectation operation with respect to 
 for both
sides of the equation, and obtain the following relationship:
(4.20)
where 
 and 
 are not changed since these do not depend on Z.
Note that the third term of Eq. (4.20) is represented as 
, which is exactly
the same definition as Eq. (3.85). Thus, we derive a similar equation to that of
the ML auxiliary function Eq. (3.86):
(4.21)
Since 
 is the same as in Eq. (3.86) and has a bound based on the
Jensen’s inequality, we can apply the same discussion to 
 to show that
 is the auxiliary function of the MAP criterion.
Thus, we prove that the E-step performing the expectation and M-step
maximizing the auxiliary function with respect to the model parameters Θ,
always increase the joint likelihood value as:
(4.22)
This leads to a local optimization of the joint likelihood function 
, which
corresponds to the MAP criterion in Eq. (4.13). We call this procedure the MAP–
EM algorithm.
4.2.2
 A recipe

Based on the previous discussions, we summarize in the text box a general
procedure to obtain the MAP estimation of model parameters. The following
section describes the concrete form of MAP–EM solutions for CDHMMs
similarly to Section 3.4.
1.    Set a likelihood function for a statistical model (generative model) with
model parameters.
2.    Set appropriate prior distributions for model parameters (possibly
conjugate distributions to obtain analytical results based on the conjugate
distribution discussion in Section 2.1.4).
3.    Solve the parameter estimation by maximizing the MAP objective
function:
i.    Solve posterior distributions for model parameters when we can use
conjugate priors;
ii.    Solve the parameter estimation by getting the modes of posterior
distributions.
4.3   Continuous density hidden Markov model
This section describes the MAP estimation of HMM parameters based on the
MAP–EM algorithm (Lee et al. 1991, Gauvain & Lee 1994). Following the
general procedure for MAP estimation (as set out in Section 4.2.2), we first
review a likelihood function of the CDHMM, as discussed in Section 3.2.3. Then
we provide a concrete form of the prior distribution 
 for full and diagonal
covariance cases. Then, according to the derivation of the ML–EM algorithm in
Section 3.4, we also derive the concrete form solutions of the MAP–EM
algorithm of the CDHMM.
4.3.1
 Likelihood function

We first provide the complete data likelihood function with speech feature
sequence 
, 
HMM 
state 
sequence 
, 
and 
GMM 
component 
sequence 
, which is introduced in Eq. (3.43) as
follows:
(4.23)
Recall that a set of HMM parameters Θ holds:
Initial state probability 
;
State transition probability 
;
Mixture weight 
;
Gaussian mean vector 
;
Gaussian covariance matrix 
.
The next section provides prior distribution 
.
4.3.2
 Conjugate priors (full covariance case)
The prior distribution is considered to be the following joint distribution form:
(4.24)
However, since it is difficult to handle this joint distribution, we usually factorize
it by assuming conditional independence for each HMM state and mixture
component. Then, the prior distribution is rewritten as follows:
(4.25)

where we also assume that 
, 
, 
, and 
 are independent of each
other, although we keep the dependency of 
 and 
.
Now we provide the concrete forms of the above prior distributions for the
HMM parameters based on the conjugate distribution discussion in Section
2.1.4. We first focus on the prior distributions of the initial state probability 
,
state transition probability 
, and Gaussian mixture weight 
. Note that these
probabilistic variables have the same constraint that 
, 
, and 
. In addition, these are represented by a
multinomial distribution in the complete data likelihood function. Therefore,
according to Table 2.1, these are represented by a Dirichlet distribution with
hyperparameters as follows:
(4.26)
where 
, 
, and 
.
Next we consider the prior distribution of Gaussian mean vector 
 and
Gaussian precision matrix 
. For simplicity, we focus on the precision matrix
R, which is the inverse matrix of the covariance matrix 
, i.e.,
(4.27)
According to Table 2.1, the joint prior distribution of Gaussian mean vector 
and Gaussian precision matrix 
 can be written as follows:
(4.28)
where 
 is a Wishart distribution, which is defined in Appendix C.14. Note

that the prior distribution 
 of the mean vector depends on covariance
matrix 
, and cannot be factorized independently. Instead, these parameters are
represented by the joint prior distribution 
 with the Gaussian–Wishart
distribution (Appendix C.15) or the product form in Eq. (4.28), as we discussed
in Section 2.1.4. We can also provide the prior distribution for the original
covariance matrix 
 by using the inverse-Wishart distribution instead of the
Wishart distribution in Eq. (4.28). Both representations yield the same result in
the MAP estimation of HMM parameters.
Consequently, the conjugate prior distribution of a CDHMM is represented
by the following factorization form with each parameter:
(4.29)
Note that the prior distribution of a CDHMM is represented by three types of
distributions, i.e., Dirichlet, Gaussian, and Wishart distributions. The prior
distribution has five scalar hyperparameters 
, one vector
hyperparameter 
, and one matrix hyperparameter 
. A set of these
hyperparameters is written as Ψ in this chapter, i.e.,
(4.30)
In the following sections, we sometimes represent the prior distribution as 
 instead of 
 to deal with the hyperparameter dependency on the
prior distributions explicitly.
4.3.3
 Conjugate priors (diagonal covariance case)

In practical use, the Gaussians in a CDHMM are often represented by a diagonal
covariance matrix, as we discussed in Section 3.2.3. To deal with a conjugate
distribution of a diagonal covariance matrix, we need to provide a specific
distribution rather than the Wishart distribution since the off-diagonal elements
are always zero, and it is not suitable to represent these random variables as the
Wishart distribution. Instead, we use the gamma distribution for each diagonal
component. We first define the 
 element of the precision matrix as 
:
(4.31)
Then the joint prior distribution of Eq. (4.28) is factorized by a feature
dimension, and it is replaced with the gamma distribution as follows:
(4.32)
where a set of hyperparameters Ψ is represented as
(4.33)

where
(4.34)
Similarly to the full covariance case, Eq. (4.32) can also be represented by a
Gaussian-gamma distribution (Appendix C.13).
The dependency of the hyperparameter ϕ is not unique and can be arranged
by considering applications due to the flexible parameterization of an
exponential family distribution, as discussed in Section 2.1.3. For example, 
and 
 can be changed depending on a dimension d (i.e., 
, 
). These make
the model more precise, but need more effort to set 
, 
 for all dimensions
manually or automatically. Actually, these values are often shared among all js
and ks (i.e., 
 etc.).
4.3.4
 Expectation step
Once we set the prior distributions and likelihood function, by following the
recipe in Section 4.2.2, we can perform the MAP–EM algorithm to estimate the
model parameter Θ. This section considers the concrete form of the MAP
expectation step. This procedure is very similar to Section 4.2.1 except for the
additional consideration of the prior distribution 
. The auxiliary function
used in the MAP–EM algorithm is represented as
(4.35)
According to Section 3.4, the auxiliary function 
 is factorized by a sum
of four individual auxiliary functions as:

(4.36)
Similarly, from Eq. (4.29), the prior distribution of all model parameters 
can be decomposed into the four individual prior distributions as
(4.37)
Therefore, by using the factorization forms of Eqs. (4.36) and (4.37), similarly to
the ML case, Eq. (4.36) is also represented as a sum of four individual auxiliary
functions defined as follows:
(4.38)

where
(4.39)
(4.40)
(4.41)
(4.42)
where 
, 
, and 
 are the posterior probabilities, which are
introduced in Eqs. (3.99), (3.118), (3.119) as follows:
(4.43)

Note that Θ is estimated by using the MAP estimation 
 for Θ instead of the
ML estimation, which is discussed in Section 4.3.6.
We can also obtain the auxiliary function of diagonal covariance Gaussians
instead of Eq. (4.42) by using a Gaussian-gamma distribution as a prior
distribution, as discussed in Eq. (4.32) as follows:
(4.44)
Here we use the gamma distribution 
 described in Eq. (C.81) instead
of the original gamma distribution defined in Eq. (C.74), which provides a good
relationship with the Wishart distribution, i.e., if R is a scalar value (the number
of dimension 
), the hyperparameters of the Wishart distribution become
the same as 
 and ,4 as we discussed in Example 2.6. Note that the vector and
matrix operations in Eq. (4.42) are represented as scalar operations with the
summation over the dimension. This is a very good property for which to obtain
the analytical solutions due to the simplicity of the scalar calculations. In
addition, this representation avoids vector and matrix computations, which also
makes implementation simple. This section provides both full and diagonal
covariance solutions, but the diagonal covariance solution is used for most of our
applications.
4.3.5
 Maximization step
The maximization step in the ML–EM algorithm obtains the maximum values of
parameters by using derivative techniques, as discussed in Section 3.4.3. In this
section, we provide other solutions for this problem which:
1.    Calculate the posterior distributions;

2.    Obtain the mode values of the posterior distributions, which are used as
the MAP estimates.
In general, it is difficult to analytically obtain the posterior distributions.
However, since we use the conjugate prior distributions of a CDHMM, as
discussed in Section 2.1.3, we can easily obtain the posterior distributions for
these problems.
Initial weight
We first focus on 
 in Eq. (4.39):
(4.45)
Recall that the Dirichlet distribution (Appendix C.4) is represented as follows:
(4.46)
Then, by substituting Eq. (4.46) into Eq. (4.45), Eq. (4.45) is rewritten as
follows:
(4.47)

Comparing the result with Eq. (4.46), it is the same function form with different
hyperparameters. Therefore, the auxiliary function 
 is represented by
the following Dirichlet distribution:
(4.48)

where
(4.49)
We finally omit the ratio of the normalization factor of the prior and posterior
distributions, which do not depend on 
. Actually, this Dirichlet distribution
corresponds to the posterior distribution of π with new hyperparameter . This
result is similar to that of the conjugate prior and posterior distributions for
multinomial likelihood function, as we discussed in Example 2.8.
Once we obtain the analytical form of the posterior distribution, the MAP
estimate can be obtained as the mode of the Dirichlet distribution (Appendix
C.4):
(4.50)
Thus, we obtain the MAP estimate of the initial weight 
, which is
proportional to the hyperparameter 
. We discuss the meaning of this solution in
Section 4.3.7.
State transition
Similarly, the auxiliary function of state transition parameters A is obtained as
follows:
(4.51)

where
(4.52)
Therefore, the mode of the Dirichlet distribution is obtained as:
(4.53)
The solution is similar to the initial weight in Eq. (4.50), and it is computed from
the statistics of the accumulated posterior values of the state transition 
and prior parameter 
.
Again, the result indicates that the auxiliary function of the state transition is
represented by the same Dirichlet distribution as that used in the prior
distribution with different hyperparameters. This result corresponds to the
conjugate distribution analysis for multinomial distribution, as we discussed in
Section 2.8. Therefore, although we need to handle the latent variables within a
MAP–EM framework based on the iterative calculation, the conjugate prior
provides an analytic solution in the M-step, and it makes the estimation process
efficient.
Mixture weight
Similar to the state transition, the auxiliary function of the mixture weight
parameters 
 is as follows:

(4.54)

where
(4.55)
Therefore, the mode of the Dirichlet distribution is obtained as:
(4.56)
Again, it is computed from the statistics of the accumulated posterior values of
the state occupancy 
 and prior parameter 
.
Mean vector and covariance matrix
Finally, we focus on the auxiliary function of the mean vector 
 and precision
matrix R. Recall that the multivariate Gaussian distribution (Appendix C.6) is
represented as follows:
(4.57)
and the Wishart distribution (Appendix C.14) is represented as follows:
(4.58)
Then, 
 is represented by using Eqs. (4.42) with the
normalization constant, (4.57), and (4.58), as follows:

(4.59)
where the final line includes the terms that do not depend on 
 and R. Then we
rearrange Eq. (4.59) so that we can write it in a probabilistic form. First, we omit
j, k, and  in Eq. (4.59) for simplicity, and consider the following function:

(4.60)
Then, we focus on 
 that has the terms in 
 that depend on 
. 
 is rewritten as follows:
(4.61)
Since this is a quadratic form of 
, it can be represented by a Gaussian
distribution by arranging Eq. (4.61) into the complete square form. Although it is
complicated to deal with the complete square form for vectors, we can use the
following complete square rules found in Eqs. (B.16) and (B.17):
(4.62)

where
(4.63)
Therefore, 
by 
using 
this 
rule 
(e.g., 
, 
, 
, and 
), Eq. (4.61) is rewritten
with the complete square form as follows:
(4.64)
where 
, , and v(R) are defined as follows:
(4.65)
Note that 
 and  correspond to the mean and covariance hyperparameters of
the conjugate Gaussian distribution of 
. Thus, 
 is rewritten with the

definition of the Gaussian distribution in Eq. (4.57) as follows:
(4.66)
where v(R) is used to obtain the analytic form of R with the rest of the R-
dependent terms in Eq. (4.60). That is, the auxiliary function (Eq. (4.60)) with
Eq. (4.66) is represented as follows:
(4.67)
Now we focus on h(R) that has the terms in the second line in Eq. (4.67). By
using the definition of v(R) in Eq. (4.65), we can rewrite h(R) as follows:
(4.68)
Since Eq. (4.68) includes the trace operation, it is difficult to rearrange this
equation. Therefore, by using the trace rule of 
 (Eq. (B.1)), we
represent all terms except for the 
 term in Eq. (4.68) as follows:

(4.69)
In addition, by using the trace rules of 
 and 
 (Eqs. (B.2) and (B.3)), Eq. (4.69) is finally
represented by comparing h(R) with the definition of the Wishart distribution
(Eq. (4.58)), as follows:
(4.70)
Thus, Eq. (4.70) can be represented as the Wishart distribution with the
following hyperparameters:
(4.71)
Note that these hyperparameters are almost equivalent to those of the conjugate
distribution analysis discussed in Eq. (2.112). The only difference is that the
statistics in Eq. (4.71) are computed from the expectation value of the posterior

distribution of a latent variable 
.
Here, 
 in the original Q function, Eq. (4.59), is represented by
Eqs. (4.67) and (4.70) as follows:
(4.72)
Thus, we have found that 
 can be represented with the same
distribution form as the prior distributions, which is represented by Gaussian and
Wishart distributions as follows:
(4.73)
where we recover the indexes j, k, and ′. By using the definition of the Gaussian–
Wishart distribution in Appendix C.15, 
 can also be represented
as:
(4.74)
By summarizing the result of Eqs. (4.65) and (4.71), the hyperparameters of
these distributions are defined as:

(4.75)
The MAP estimates of these values are obtained by considering the modes of the
Gaussian–Wishart distribution in Appendix C.15. The modes of 
 and 
 are
represented as:
(4.76)
Note that 
 cannot be obtained when 
. Thus, we can analytically
obtain the M-step solutions of CDHMM parameters (i.e., initial weight, state
transition, mixture weight, mean vector and covariance matrix) in the MAP
sense, thanks to the conjugate prior distributions. We summarize the solutions
below. The hyperparameters of the posterior distributions are represented with
Gaussian sufficient statistics and the prior hyperparameters as:

(4.77)
A set of hyperparameters  is defined as follows:
(4.78)
Then, the MAP solutions of HMM parameters are represented with the posterior
hyperparameters as follows:

(4.79)
Compared with the ML M-step of CDHMM parameters in Eq. (3.151), these
solutions are more complicated, and actually incur more computational cost than
that of the ML solution. However, the computational cost of the M-step is much
smaller than that of the E-step, and the additional computational cost of the MAP
estimate can be disregarded in practical use.
Mean vector and diagonal covariance matrix
In practise, we often use the diagonal covariance matrix for a multivariate
Gaussian distribution for HMMs, as we discussed in Section 3.2.3. This section
provides the MAP solution for the diagonal covariance case (Gauvain &
Lee 1991). Since the one-dimensional solution of the full-covariance Gaussian
posterior distribution in the previous discussion corresponds to that of the
diagonal covariance Gaussian posterior distribution of a diagonal element, we
can obtain the hyperparameters of the posterior distribution of the CDHMM
parameters by using 
 for each diagonal component. We also summarize
the solution of the MAP estimates of HMM parameters of the diagonal
covariance Gaussian case below. The hyperparameters of the posterior
distributions are represented with Gaussian sufficient statistics and the prior
hyperparameters as:

(4.80)
In this case, a set of hyperparameters  is defined as follows:
(4.81)

where
(4.82)
Then, the MAP solutions of HMM parameters can be represented with the
posterior hyperparameters as follows:
(4.83)
Thus, we obtain the MAP estimates of HMM parameters in both diagonal and
full covariance matrix cases.
4.3.6
 Sufficient statistics
As we discussed in Eq. (4.43), the posterior probabilities of the state transition 
, and mixture occupation 
 can be computed by plugging the MAP
estimates 
 obtained by using Eqs. (4.79) or (4.83) into the variables in
Sections 3.3 and 3.4.2. Note that the analytical results here are exactly the same
as those in the ML–EM algorithm (except for the MAP estimates 
): we list
these for convenience.
First, the MAP forward variable 
 is computed by using the following

equation:
Initialization
4.84
Induction
(4.85)
Termination
4.86
Here, 
 is a GMM emission probability distribution with the MAP estimate
parameters defined as:
(4.87)
The MAP backward variable 
 is computed by using the following equations:
Initialization
4.88
Induction

(4.89)
Termination
4.90
Therefore, based on the MAP forward and backward variables, we can compute
the posterior probabilities as follows:
(4.91)
(4.92)
Once we have computed the posterior probabilities, we can compute the
following sufficient statistics:
(4.93)

The superscripts 
 and 
 denote the 1st and 2nd order statistics.
This total EM algorithm for iteratively estimating the HMM parameters
based on the MAP estimate is set out in Algorithm 8. Compared with the ML
Baum–Welch algorithm, Algorithm 4, it requires the hyperparameters Ψ of the
CDHMMs. Section 7.3 also introduces a variant of the Baum–Welch algorithm
based on variational Bayes. Note again that the MAP E-step (computing forward
variables, occupation probabilities, and accumulation) is exactly the same as that
of the ML E-step, and retains the nice property of the parallelization and data
scalability. In addition, since the E-step computation is dominant in the
algorithm, the computational costs of the ML and MAP Baum–Welch algorithms
are almost same.
Algorithm 8 MAP Baum–Welch algorithm
Require:  Ψ and 
 
1:  repeat 
2:           Compute the forward variable 
 from the forward algorithm 
3:           Compute the backward variable 
 from the backward algorithm 
4:           Compute the occupation probabilities 
, 
, and 
 
5:           Accumulate the sufficient statistics 
, 
, 
, and 
 (or 
)
6:           Estimate the new hyperparameters  
7:           Estimate the new HMM parameters 
 
8:           Update the HMM parameters 
 
9:  until Convergence
4.3.7
 Meaning of the MAP solution
This section discusses the meaning of the MAP solution obtained by Eqs. (4.79)
and (4.83). We consider the two extreme case of the MAP solution, where the
amount of data is small and large. That is, we consider the small data limit as 
. On the other hand, the large data limit corresponds to 
.
Mixture weight 
We first focus on the MAP estimate of the mixture weight ω, but the

discussion can be applied to the state transition a. 
Large sample: 
The MAP estimate of the mixture weight is represented as follows:
4.94
Since 
 in the large sample case, the MAP estimate 
 approaches the
ML estimate 
 when 
 is sufficiently larger than 
:
(4.95)
Small sample: 
Similarly, the MAP estimate 
 approaches the following value when 
is sufficiently smaller than 
:
(4.96)
The weight is only computed from the prior hyperparameter 
. Thus, the
mixture weight approaches the ML estimate when the amount of data is
large, while it approaches the weight obtained only from the prior
hyperparameters when the amount is small. Hyperparameter 
 can be
regarded as a scale.
Mean 
By using Eqs. (4.93) and (4.79), the MAP estimate of the mean vector can
be rewritten as follows:

4.97
This equation means that the MAP estimate 
 is linearly interpolated by
the ML estimate 
 and the hyperparameter 
, as shown in Figure 4.1.  is
an interpolation ratio, and it has a specific meaning when the amount of
data is sufficiently large 
 or small 
. 
Large sample:
4.98
Similarly to the discussion of the mixture weight, the MAP estimate of the
mean vector theoretically converges to the ML estimate. 
Small sample:
4.99
This is a good property of the MAP estimate. Although the ML estimate
with a small amount of data incorrectly estimates the mean vector, which
degrades the performance drastically, the MAP estimate can smooth the
incorrect estimation based on the hyperparameter 
. In the practical
situation, we also often have a zero count problem (i.e., 
), which
makes the ML estimate singular due to the zero divide. However, the MAP
solution of the mean vector avoids this problem and provides a reasonable
estimate obtained from the hyperparameter 
.
Covariance matrix 
By using Eqs. (4.93) and (4.79), the MAP estimate of the covariance matrix
can be rewritten as follows:

(4.100)
Large sample:
4.101
The result is the same when we use the diagonal covariance. 
Small sample (full covariance):
4.102
Unlike the mean vector case, the covariance matrix of the small sample
limit is represented by the two hyperparameters 
 and 
. To make the
solution meaningful, we need to set 
 to avoid a negative or zero
value of the variance.
Small sample (diagonal covariance):
By using Eq. (4.83), we can also obtain the variance parameter for
dimension d as follows:
4.103
Similarly to the diagonal case, we need to set 
 to avoid a negative or
zero value of the variance.
In summary, the MAP solutions can smooth the estimation values with the
hyperparameters when the amount of data is small, and the solutions approach
the ML estimates when the amount of data is large. A similar discussion has
already been presented in Section 2.1.4, as a general property of the Bayesian
approach.

Figure 4.1 Geometric meaning of the MAP estimate of the Gaussian mean
vector. It is represented as the linear interpolation of the prior mean vector 
and the ML estimate of the mean vector 
. The interpolation ratio depends
on the hyperparameter 
 and the amount of data 
 assigned to the Gaussian.
The MAP estimation of the CDHMM parameters can be applied to general
CDHMM training. However, since CDHMM is usually trained with a sufficient
amount of training data, we do not have to use MAP estimation, and ML
estimation is enough in most cases, which corresponds to the case of the large
sample limitation in the above discussion. However, we often face the case when
the amount of data is small at an adaptation scenario. The following section
introduces one of the most successful applications of the MAP estimation for
speaker adaptation.
4.4   Speaker adaptation
Speaker adaptation is one of the most important techniques in speech
recognition, mainly to deal with speaker variations in speech (Lee &
Huo 2000, Shinoda 2010). The speech features of a speaker are different from
those of another speaker, which degrades the performance of speech recognition.
A straightforward solution for this problem is to build a speaker-dependent
acoustic model for a specific person. However, it is difficult to collect sufficient
training data with labels.
Speaker adaptation aims to solve the problem by first building a speaker-

independent (SI) acoustic model 
 by using many speakers’ data, and updates
the model as a speaker-dependent (SD) acoustic model 
 with a small amount
of data of the target speaker, as shown in Figure 4.2. The speaker-independent
acoustic model is usually made by the conventional maximum likelihood
procedure, as we discussed in Chapter 3, or discriminative training. It is also
obtained by using so-called speaker adaptive training 
(Anastasakos,
McDonough, Schwartz et al. 1996) or cluster adaptive training (Gales, Center &
Heights 2000), which normalizes the speaker characteristics (or some other
characteristics (e.g., noises, speaking styles) obtained from clustering of speech
utterances) by using a variant of the maximum likelihood linear regression
(MLLR) technique, which is discussed in Section 3.5.1.
Figure 4.2 Speaker adaptation of HMM parameters. The initial HMMs are
trained with many speakers, and then the HMMs are adapted to the target
speaker’s model with a small amount of adaptation data.
4.4.1
 Speaker adaptation by a transformation of CDHMM
Once we have SI model parameters 
, the problem is how to estimate the SD
model parameters 
 without over-training. Basically, the number of SI model
parameters is very large. For example, in the famous speech recognition task
using read speech of Wall Street Journal (WSJ) sentences, the number of
CDHMM parameters amounts to several millions or more. On the other hand,
the amount of speech data for the target speaker with text labels would be a few

minutes at most, and the number of frames corresponds to the order of tens of
thousands, and is even smaller than the number of standard CDHMM
parameters. The following ML estimate with the EM algorithm, as we discussed
in Section 3.4, causes serious over-training:
(4.104)
There are several approaches to overcoming the problem by using the maximum
likelihood linear regression (MLLR) (Digalakis et al. 1995, Leggetter &
Woodland 1995, Gales & Woodland 1996), as discussed in Section 3.5,
eigenvoice approaches (Kuhn, Junqua, Ngyuen et al. 2000), and so on. These
approaches set a parametric constraint of fewer CDHMM parameters, and
estimate these parameters (Λ) instead of CDHMM parameters with ML
indirectly, that is:
(4.105)
Detailed discussions of these adaptation techniques can be found in Lee &
Huo (2000) and Shinoda (2010). We can also consider the Bayesian treatment of
this indirect estimation of transformation parameters Λ, which is discussed in
Section 7.4.
4.4.2
 MAP-based speaker adaptation
In speaker adaptation using the MAP estimation (MAP adaptation), we directly
estimate the SD CDHMM parameters 
, unlike the MLLR and eigenvoice
techniques.5 The MAP estimation can avoid the over-training problem. Then, we
use SI CDHMM parameters 
 as hyperparameters of the prior distributions,
i.e.,
(4.106)

(4.107)
where 
 is a prior distribution with hyperparameters of the prior
distribution, and it is set as a conjugate distribution of CDHMM, as discussed in
Section 4.3.3. We discuss below how to set 
 to hyperparameters Ψ in detail.
Let 
, 
, 
, 
, and 
 be the SI CDHMM parameters with diagonal
covariance. Although there are several ways to determine hyperparameters from
the speaker-independent HMM parameters, we can set the following relationship
between hyperparameters and SI parameters by using Eqs. (4.96), (4.99), and
(4.103):
(4.108)
This equation is obtained based on the constraint that we can obtain the SI
performance when the amount of adaptation data for the target speaker is zero.
To satisfy the above equations, we can use the following hyperparameter setting:
(4.109)

Note that the above hyperparameter setting has two additional parameters ϕ and 
. These are often set with fixed values (e.g., 
, 
). Thus, by
substituting Eq. (4.109) into Eq. (4.80) and (4.83), the MAP estimates of SD
HMM parameters are obtained as:
(4.110)
Gauvain & Lee (1994) compare speaker adaptation performance by employing
ML and MAP estimations of acoustic model parameters using the DARPA Naval
Resources Management (RM) task (Price, Fisher, Bernstein et al. 1988). With 2
minutes of adaptation data, the ML word error rate was 31.5 % and was worse
than the speaker independent word error rate (13.9 %) due to the over-training
effect. However, the MAP word error rate was 8.7 %, clearly showing the
effectiveness of the MAP approach. MAP estimation has also been used in
speaker verification based on universal background models (Reynolds, Quatieri
& Dunn 2000), which is described in Section 4.6, and in the discriminative

training of acoustic models in speech recognition as a parameter smoothing
technique (Povey 2003), which is described in the next section.
4.5   Regularization in discriminative parameter
estimation
This section describes another well-known application of MAP estimation in
discriminative training of CDHMM parameters. Discriminative training is based
on discriminative criteria, which minimizes the ASR errors directly rather than
maximizing likelihood values (Juang & Katagiri 1992), and improves the
performance further from the ML-based CDHMM. However, discriminative
training of CDHMM parameters always has a problem of over-estimation, and
the regularization effect of the MAP estimation helps to avoid this problem.
Discriminative training has been studied by many researchers, and there are
many approaches to realize it for ASR based on different discriminative criteria
and optimization techniques (e.g., maximum mutual information (MMI)
criterion (Bahl, Brown, de Souza et al. 1986), MMI with extended Baum–Welch
algorithm (Normandin 1992), minimum classification error (MCE) criterion
(Juang & Katagiri 1992), MCE with various gradient methods (McDermott,
Hazen, Le Roux et al. 2007), minimum phone error (MPE) criterion (Povey &
Woodland 2002), and the unified interpretation of these techniques (Schlüter,
Macherey, Müller et al. 2001, Nakamura, McDermott, Watanabe et al. 2009)).
This section explains the regularization effect of the MMI estimation of
HMM parameters with the extended Baum–Welch algorithm (Povey &
Woodland 2002). In this section, we limit the discussion of discriminative
training to focus on introducing the application of MAP estimation.
4.5.1
 Extended Baum–Welch algorithm
The MMI estimation of HMM parameters can be performed by the extended
Baum–Welch algorithm or variants of gradient based methods. The MMI
estimation starts from the following objective function based on the posterior
distribution of the word sequence:

(4.111)
where 
 is the rth utterance’s acoustic feature sequence
whose length is 
. The total number of the utterances is R. 
 is a correct word
sequence of the utterance r, and 
 is a set of all possible state sequences given 
.6 Similarly, W is a word sequence hypothesis, and the summation over W is
performed among all possible word sequences. κ is the acoustic score scale, and 
 is an acoustic likelihood, and 
 is the language model probability.
Θ is a set of all acoustic model (CDHMM) parameters for all context-dependent
phonemes, unlike the definition of the CDHMM parameters for single context-
dependent phonemes in Section 4.2. The MMI estimate of Θ can be obtained by
optimizing this objective function as follows:
(4.113)
When we only consider the numerator of Eq. (4.111) in this optimization, that
corresponds to the maximum likelihood estimation of the CDHMM parameters.
By using the extended Baum–Welch algorithm, a new mean vector and
variance at dimension d are iteratively updated from the previously estimated 
 and 
7 at the τ iteration step, as follows:
(4.114)

The derivation of the extended Baum–Welch algorithm can also be found in
Section 5.2.8. Here, 
, 
, and 
 are the Gaussian sufficient statistics
defined in Eq. (4.93), but these are obtained from the numerator of the lattice.
Similarly, 
, 
, and 
 are obtained from the denominator of the lattice. D
is a smoothing parameter used with the previous estimated parameters.
By comparison with the ML estimates of 
 and Σ in Eq. (3.151), which is
only computed from the Gaussian sufficient statistics, the MMI estimates are
computed from 
 and 
, and the numerator and denominator statistics.
This is the main difference between ML and MMI estimation methods. However,
by setting D, 
, 
, and 
 to 0, Eq. (4.114) is close to the ML estimates if
we consider that the numerator statistics can be regarded as the statistics used in
the ML–EM, i.e.,
(4.115)
Therefore, the MMI estimate can also involve the ML-like solution in a specific
limitation.
We can further provide an interesting interpretation of the MMI estimate.
First we focus on the following difference statistics between the numerator and
denominator statistics:
(4.116)
Then, we can rewrite Eq. (4.114) with these difference statistics as follows:

(4.117)
Therefore, Eq. (4.117) means that the MMI estimates are represented by the
linear interpolation between the difference-based Gaussian statistics and the
previously estimated parameters. D plays a role of tuning the linear interpolation
ratio.
Note that the MMI estimates are based on the difference statistics, and if the
denominator statistics are large, the difference statistics become small, and the
MMI estimates would meet an over-training problem. The smoothing based on
the D with the previous estimated parameters could mitigate the over-training
problem, and the combination of the MMI estimation with the MAP estimation
can further mitigate it.
4.5.2
 MAP interpretation of i-smoothing
In MMI and MPE training (Povey & Woodland 2002), the following smoothing
terms are introduced for the numerator statistics in Eq. (4.114):
(4.118)
where η is called the i-smoothing factor. This section reviews this statistics
update, which can be interpreted as the MAP estimation where η behaves as a
hyperparameter in the MAP estimation. 
 and 
 are obtained from the
maximum likelihood estimation (i.e., 
 and 
), or estimation
based on discriminative training.

To derive the above smoothing factor, we first consider the conjugate
distribution of the diagonal-covariance Gaussian distribution, which is based on
the Gaussian–gamma distribution, as shown in Table 2.1. The Gaussian–gamma
distribution is defined in Appendix C.13 as follows:
(4.119)
where we omit state index j, mixture component k, and dimension index d for
simplicity. By setting hyperparameters 
, and 
 with the following
variables:
(4.120)
the prior distribution is represented as follows:
(4.121)
By using this prior distribution similarly to the MAP auxiliary function in
Section 4.2, the MMI objective function with the prior distribution can be
obtained as follows:
(4.122)
Based on the extended Baum–Welch calculation with the additional prior
distribution, we can obtain the following update equation (Povey &
Woodland 2002):

(4.123)
This equation is based on Eq. (4.117), with the effect of prior distribution 
 through Eq. (4.118).
Below, we discuss this update equation with the MAP solution by following
the similar discussion in the previous section based on the ML–EM conversion.
By setting D, 
, 
, and 
 to 0, Eq. (4.123) is represented as follows:
(4.124)
(4.125)
By comparing the MAP solutions for μ and Σ in Eq. (4.83), we find that
Eqs. (4.124) and (4.125) correspond to the MAP solutions. Thus, we have found
that the i-smoothing in the MMI and MPE solutions can be interpreted as MAP.
From Eq. (4.123), the smoothing terms that come from D can also be similarly
interpreted as MAP, when we consider the following Gaussian–gamma prior
distribution:
(4.126)
Therefore, we could also provide the MAP interpretation of the D-related

terms in the extended Baum–Welch algorithm. However, how to provide this
prior distribution in the objective function is not trivial, and the theoretical
analysis of this interpretation in the discriminative training framework is an
interesting open question.
Povey & Woodland (2002) use this i-smoothing technique with MMI and
MPE estimation methods, and report 1% absolute WER improvement from the
MMI estimation method without the i-smoothing technique. In addition, using
the speaker independent HMM parameters as prior hyperparameters (Povey,
Gales, Kim et al. 2003, Povey, Woodland & Gales 2003) also realizes
discriminative acoustic model adaptation based on the MAP estimation. There
are several studies of using Bayesian approaches to discriminative training of
acoustic models (e.g., based on minimum relative entropy discrimination (Kubo,
Watanabe, Nakamura et al. 2010)), and Section 5.2 also introduces the Bayesian
sensing HMM with discriminative training based on the evidence framework.
4.6   Speaker recognition/verification
In this section we focus on text-independent speaker recognition or speaker
verification systems, and show how MAP estimation is used. Speaker
recognition is a similar problem to automatic speech recognition. Let 
be a sequence of D dimensional speech feature vectors. Usually O is one
utterance by a specific speaker. Similarly to speech recognition, MFCC is
usually used as a feature. The speaker recognition task is to estimate speaker
label  among a speaker set  by using the maximum a-posteriori estimation for
the posterior distribution:
(4.127)
where 
 is obtained from a statistical speaker model, and is discussed later.
This is similar to a speech recognition problem, as shown in Eq. (3.2) by
replacing the estimation target from the word sequence W with the speaker index
c. Since the output is not structured compared with W, speaker recognition can
be realized by relatively simple models compared with ASR.

Speaker verification is to determine whether O is spoken by a target speaker
s, which is regarded as single-speaker detection. This is reformulated as a basic
test between two hypotheses:
 : O is from the hypothesized speaker s;
 : O is not from the hypothesized speaker s.
And the verification is performed by comparing the posterior distributions of 
and 
 as follows:
(4.128)
where ε is a decision threshold. By using the Bayes rule, we can rewrite
Eq. (4.128) with a likelihood ratio as follows:
(4.129)
where we disregard the contribution of the prior distributions of each hypothesis.
Thus, by using the generative model of the 
 and 
, we can compare the two
hypotheses. We use GMM as these generative models. Hence, 
 and 
 are represented from 
 and 
, where 
 and 
 are
sets of GMM parameters.
4.6.1
 Universal background model
The generative model of 
 must consider the characteristics of many speakers.
That can be achieved by training the GMM parameters 
 from many speakers.
The GMM of 
 is called the universal background model (UBM), and 
denotes its GMM parameters. Therefore, the likelihood for test data O can be
computed by:

(4.130)
where 
 are computed from many training data 
 in advance, uttered by
various speakers to train 
 with the ML training as follows:
(4.131)
which can be performed efficiently by using the EM algorithm, as we discussed
in Section 3.4.
While we use many data 
 to train 
, the hypothesis speaker model 
 with model parameters 
 can be trained by using only a small amount of
data O (e.g., one utterance). Although ML has an over-training problem in this
setting, MAP estimation can avoid the problem, and estimate 
 as follows:
(4.132)
where 
 is a prior distribution and 
 are hyperparameters of
GMM parameters. Note that some of the hyperparameters are obtained from the
UBM parameters 
. This is a very similar technique to MAP adaptation of
CDHMM parameters, as we discussed in Section 4.4.2, where the target model is
based on a speaker-dependent CDHMM while the prior model is based on a
speaker-independent CDHMM. Equation (4.132) is a subset solution of
CDHMM, as we discussed in Section 4.3, and the 
 is obtained as follows:
(4.133)

where
(4.134)
Reynolds et al. (2000) suggest using specific hyperparameter settings for 
, 
, 
, 
 to obtain the following forms:
(4.135)
where 
, 
, and 
 are hyperparameters, and can be controlled by a tuning
parameter β, as follows:
(4.136)
Note that this solution also has the MAP property of avoiding sparse data
problems.
The hypothesis test in Eq. (4.128) can be performed by considering the
likelihood ratio test of UBM and HYP GMMs as

(4.137)
Thus, we have shown that MAP estimation plays an important role in speaker
verification based on UBM, especially in estimating the hypothesis speaker
model.
4.6.2
 Gaussian super vector
The MAP estimation of speaker models is further developed by using the
Gaussian super vector technique (Campbell, Sturim & Reynolds 2006). The idea
of this approach is to consider the MAP estimated GMM parameters as a feature
of speaker verification or speaker recognition. The verification/recognition is
performed by using a multi-class Support Vector Machine (SVM)
(Vapnik 1995), cosine similarity scoring, or other simple classifier. Suppose we
have 
 features, the GMM–UBM process can create the following super vector
by concatenating the Gaussian mean vector 
, estimated from 
:
(4.138)
The super vector is also obtained by using the vectorized form of the
transformation matrix estimated by using the MLLR algorithm (Stolcke
et al. 2005). The technique is widely used for speaker and language recognition
tasks (Kinnunen & Li 2010), and it can usually be used with factor analysis
techniques (Kenny 2010, Dehak, Kenny, Dehak et al. 2011) by representing the
super vector with the speaker-specific and other (channel) factors:
(4.139)

where the speaker and channel specific factors are also represented by the linear
model with the transformation matrices 
 and 
. 
 is a bias vector, and 
 is
a noise vector. The approach is also applied to video processing (Shinoda &
Inoue 2013). Thus, MAP estimation is still used as an important component of
speaker verification tasks, but the techniques have been developed further based
on the above factor analysis. Section 7.5 describes a VB solution of this factor
analysis.
4.7   n-gram adaptation
MAP estimation is also used to obtain the n-gram language model
(Federico 1996, Masataki, Sagisaka, Hisaki et al. 1997). In the n-gram language
model, the generative probability of the word sequence 
with vocabulary 
 can be basically represented as a product of multinomial
distributions, as discussed in Section 3.6:
(4.140)
where 
 denotes the n-gram parameters. As discussed in Eq. (3.197),
the ML estimate of Θ is obtained by using the number of occurrences 
 of
word sequence 
 in training corpus 
:
(4.141)
Note that we do not consider the smoothing techniques in this section to make
the discussion simple.

4.7.1
 MAP estimation of n-gram parameters
The MAP extension from the above ML framework can be performed by
considering the posterior distribution and introducing the prior distribution as
follows:
(4.142)
Since 
 is a multinomial distribution, as discussed in Section 2.1.4, we
use the following Dirichlet distribution for 
 in Appendix C.4:
(4.143)
Thus, we can analytically solve Eq. (4.142) as follows:
(4.144)
This is a similar result to the MAP solutions of mixture weights or transition
probabilities in Section 4.3.5. Since the n-gram parameter estimation in this
setting does not include the latent variables, we can obtain the solution without
using the EM algorithm. Therefore, the difference between Eq. (4.144) and those
in Section 4.3.5 is between using discrete counts 
 or EM-based expected
counts γ and ξ, which are continuous values. Note that the parameters
represented by a Dirichlet distribution always satisfy the sum-to-one condition
required for n-gram language modeling.
4.7.2
 Adaptation method

Similarly to MAP estimation based speaker adaptation for HMM parameters,
MAP estimation of n-gram parameters can be used for speaker/task adaptations
(Federico 1996, Masataki et al. 1997). Let 
 be the speaker (or task)
independent corpus, and 
 be the speaker (or task) dependent corpus. The
following hyperparameter setting is often used:
(4.145)
Here 
 is obtained from the ML estimation in Eq. (4.141) by using 
.
Similarly, 
 is a word count obtained from 
. Then, the numerator of
the MAP estimation in Eq. (4.144) is rewritten as:
(4.146)

Note that
(4.147)
Therefore, Eq. (4.146) can be regarded as a well-known linear interpolation of
two n-gram language model parameters 
 and 
, i.e.,
(4.148)
where 
 is a linear interpolation ratio defined as:
(4.149)
The linear interpolation ratio depends on the count of each corpus and
hyperparameter α.
This linear interpolation based MAP solution can be regarded as an instance
of well-known interpolation smoothing techniques in n-gram language modeling
(Chen & Goodman 1999, Rosenfeld 2000), as discussed in Section 3.6.2. The
analytical result shows that the linear interpolation technique can be viewed as
the MAP estimation of n-gram parameters in a Bayesian sense.
4.8   Adaptive topic model

We are facing the era of big data. The volume of data collections grows vastly.
Statistical document modeling becomes increasingly important in language
processing areas. As addressed in Section 3.7.3, probabilistic latent semantic
analysis (PLSA) has been developed to represent a set of documents according
to the maximum likelihood (ML) principle. The semantics and statistics can be
effectively captured for document representation. However, PLSA is highly
sensitive to the target domain, which is continuously changing in real-world
applications. Similarly to the adaptation of hidden Markov models to a new
speaker in Section 4.3 and the adaptation of n-gram models to a new recognition
task in Section 4.7, we are interested in adapting the topic-based document
model using PLSA to a new application domain from a set of application-
specific documents.
A Bayesian PLSA framework (Chien & Wu 2008) is presented to establish
an adaptive topic model to improve document representation by incrementally
extracting the up-to-date latent semantic information to match the changing
domains at run time. The Dirichlet distribution is introduced to serve as the
conjugate priors for PLSA parameters, which are multinomial distributed. The
reproducible prior/posterior distributions facilitate two kinds of adaptation
applications. One is corrective training while the other is incremental learning.
An incremental PLSA is constructed to accomplish the parameter estimation as
well as the hyperparameter updating. Differently from standard PLSA using an
ML estimate, the Bayesian PLDA is capable of performing dynamic document
indexing and modeling. The mechanism of adapting a topic model based on
Bayesian PLSA is similar to the mechanisms of folding-in (Berry et al. 1995)
and SVD updating (Bellegarda 2002) based on latent semantic analysis (LSA)
(Berry et al. 1995, Bellegarda 2000), which is known as a nonparametric
approach. The updating and downdating in an SVD-based LSA framework could
not be directly applied for an ML-based PLSA framework. To add up-to-date or
remove out-of-date knowledge, the adaptive PLSA is developed for document
modeling. The goal of adaptive PLSA aims to use the newly collected
documents, called adaptation documents, to adapt an existing PLSA model to
match the domains of new queries or documents in information retrieval
systems. In Chien & Wu (2008), adaptive PLSA is shown to be superior to
adaptive LSA in information retrieval tasks. In what follows, we address the
methods of maximum a-posteriori estimation and quasi-Bayes estimation
designed for corrective training and incremental learning, respectively.

4.8.1
 MAP estimation for corrective training
Corrective training is intended to use batch collection data to correct the ML-
based PLSA parameters 
 to fit new domain knowledge via the MAP
estimation. In a topic model based on PLSA, two sets of multinomial parameters
 have been estimated in the training phase subject to the
constraints of multinomial distributions as given in Eq. (3.304). The first one is
the topic-dependent unigram probability 
 of a vocabulary word 
,
and the second one is the posterior probability 
 of topic k given an
observed document 
. According to MAP estimation, we adapt PLSA
parameters 
 by maximizing the a-posteriori probability or
the sum of logarithms of likelihood function 
 of adaptation words and
documents 
 and prior distribution 
:
(4.150)
Here, prior distribution 
 represents the randomness of multinomial
parameters 
 and 
. Again, it is mathematically attractive to
select the conjugate prior as the candidate for Bayesian inference. The Dirichlet
distribution is known as the conjugate prior for multinomial parameters. Owing
to the selection of conjugate prior, two properties of Bayesian learning could be
obtained: 1) a closed-form solution for rapid adaptation; and 2) a reproducible
prior/posterior distribution pair for incremental learning. Assuming parameters 
 and 
 are independent, the prior distribution of the entire
parameter set based on Dirichlet density is expressed by
(4.151)
where 
 denote the hyperparameters of Dirichlet densities.
Following the EM algorithm, we implement Eq. (4.151) by calculating the

posterior auxiliary function
(4.152)
By imposing the constraints of multinomial parameters in Eq. (3.304) into the
constrained optimization, we form the extended auxiliary function as
(4.153)
which is manipulated and extended from the ML auxiliary function 
given in Eq. (3.295). In Eq. (4.153), 
 and 
 denote the Lagrange
multipliers for two constraints of multinomial parameters. Then we differentiate
Eq. (4.153) with respect to individual multinomial parameters 
 and set it
to zero:
(4.154)

and obtain
(4.155)
By substituting this result into the constraint 
, we find the
Lagrange parameter
(4.156)
Accordingly, we derive the MAP estimates of two PLSA parameters in closed
form:
(4.157)
(4.158)
where the posterior probability 
 is calculated according to
Eq. (3.311) by using adaptation documents 
 based on the current estimates 
. MAP estimates in Eq. (4.158) are seen as an extension of
ML estimates of Eq. (3.308) and Eq. (3.309) by interpolating with the prior
statistics 
 and 
, respectively. If prior density is non-informative or
adaptation data 
 are abundant, MAP estimates are reduced to ML estimates.
The MAP PLSA algorithm is developed for corrective training or batch

adaptation, and adapts the existing parameters to 
 in a single epoch. In
MAP PLSA, the Dirichlet priors and their hyperparameters 
 are
adopted to characterize the variations of topic-dependent document and word
probabilities. These priors are used to express the environmental variations.
MAP PLSA involves both word-level 
 and document-level 
parameters. In general, MAP parameters 
 perform better than ML
parameters 
 when classifying future documents with new terms, topics, and
domains.
4.8.2
 Quasi-Bayes estimation for incremental learning
Using MAP estimation, only a single learning epoch is performed to correct
PLSA parameters. Batch learning is performed. However, batch learning cannot
catch the continuously changing domain knowledge or deal with the non-
stationary documents collected from real-world applications. An adaptive
information system should continuously update system parameters with new
words and topics. Out-of-date words or documents should fade away from the
system as time moves on. Accordingly, we tackle the updating and downdating
problems simultaneously for latent semantic indexing. MAP PLSA could not
incrementally accumulate statistics for adaptive topic modeling. It is more
interesting to develop an incremental learning algorithm to track the changing
topics and domains in test documents. A learning procedure is executed
repeatedly in different epochs. Incremental learning is also known as sequential
learning or online learning, which is important for speaker adaptation in
automatic speech recognition systems where speaker characteristics gradually
change with time (Huo & Lee 1997, Chien 1999).
To implement incremental learning for adaptive topic modeling, we
continuously estimate PLSA parameters in different learning epochs using the
incrementally observed adaptation documents. At the n learning epoch, we
estimate PLSA parameters by maximizing the posterior distribution using a
sequence of adaptation documents 
:

(4.159)
where the posterior distribution 
 is approximated by the closest
tractable 
prior 
distribution 
 
with 
sufficient 
statistics 
or
hyperparameters 
 which are evolved from history documents 
. This
estimation method is also called the quasi-Bayes (QB) estimation (Huo &
Lee 1997, Chien 1999). QB estimation provides recursive learning of PLSA
parameters,
(4.160)
from incrementally observed documents,
(4.161)
At 
each 
epoch, 
we 
only 
use 
the 
current 
block 
of 
documents 
 and the accumulated statistics 
 to update
PLSA parameters from 
 to 
. Current block data 
 are released after
accumulating statistics from 
 to 
. Memory and computation
requirements are reduced at each epoch. The key technique in QB PLSA comes
from the introduction of incremental hyperparameters. If we substitute the
hyperparameters 
 into Eq. (4.157) and Eq. (4.158), the QB
estimates 
 are obtained. This QB PLSA method is geared
with the updating mechanism of hyperparameters, which is derived by the E-step
for QB estimation in Eq. (4.159). By referring to the auxiliary function of MAP
PLSA in Eq. (4.152) and Eq. (4.153), the QB auxiliary function of new estimates
 given current estimates 
 is defined by

(4.162)
It is important that the exponential of the QB auxiliary function in Eq. (4.162)
can be arranged as a new Dirichlet distribution:
(4.163)
with the updated hyperparameters 
 derived thus:
(4.164)
(4.165)

where the posterior probability
(4.166)
is obtained by using the current block of adaptation data 
 based on
current QB estimates 
. Importantly, a reproducible distribution
pair of a prior in Eq. (4.151) and a posterior in Eq. (4.163) is established. This
property is crucial to activate the updating mechanism of hyperparameters for
incremental learning. New hyperparameters 
 are estimated by
combining the previous hyperparameters 
 with the accumulated
statistics from adaptation documents 
 at learning epoch n.
Basically, QB estimation finds the point estimate for adaptive topic
modeling. This estimation is seen as an extended realization of MAP estimation
by activating the mechanism of hyperparameter updating so that incremental
learning is established to compensate the non-stationary variations in
observation data which may be speech signals, word sequences, or text
documents. Incremental learning based on QB estimation is helpful for speech
and language applications including speech recognition, information retrieval,
and others.
4.8.3
 System performance
The performance of the adaptive topic model was evaluated through the tasks of
corrective training and incremental learning. The evaluation was performed for
the application of document categorization. Table 4.1 shows the set-up of
experimental data of the Reuters-21578 dataset. We collected training,
adaptation, and test documents from the five most populous categories in the
Reuters-21578 dataset for system evaluation. Preprocessing stages of stemming
and stop word removal were done. In the task of incremental learning, we used
one-third of the adaptation documents at each epoch and investigated the effect
of incremental learning in three learning epochs. The performance of corrective
training and incremental learning was compared. Training samples of each

category were roughly partitioned into half for training and the other half for
adaptation. A fivefold cross validation over training and adaptation sets was
performed. In the implementation, we determined PLSA probability for each test
document. The cosine similarity of feature vectors between a test document and
a given class model was calculated for pattern classification. The class feature
vector consisted of PLSA probabilities averaging over all documents
corresponding to a class. The classification error rate was computed over all test
documents in five populous classes. We obtained the classification error rates for
PLSA (Hofmann 1999b, 2001) (3.47%), SVD updating (Bellegarda 2002)
(3.39%), MAP PLSA (Chien & Wu 2008) (3.04%), QB PLSA (Chien &
Wu 2008) at 1st learning epoch (3.13%), QB PLSA at 2nd learning epoch
(3.04%), and QB PLSA at 3rd learning epoch (3%). Corrective training using
SVD updating and MAP PLSA and incremental learning using QB PLSA at
different epochs decrease the classification error rates. SVD updating is worse
than MAP PLSA and QB PLSA. Incremental learning using QB PLSA performs
slightly better than batch learning using MAP PLSA.
Table 4.1 Numbers of training, adaptation, and test documents for five
populous classes in the Reuters-21578 dataset.
Acquisitions
Crude
Earn
Money-fx
Trade
Number of
training documents
825
196
1447
284
189
Number of added
documents per epoch
275
65
475
85
60
Number of
test documents
719
189
1087
180
117
4.9   Summary
This chapter introduced various applications of MAP approximation for model
parameter posteriors. Since the approach can be easily realized from the existing
ML based approaches by simply considering the regularization term based on
model parameter priors, it is widely used for speech and language processing
including acoustic and language modeling in ASR, speaker verification, and
document processing. Although the MAP approximation can utilize the most

famous Bayesian advantage of “use of prior knowledge,” it does not deal with
probabilistic variables explicitly with the marginalization, and it does not fully
utilize the Bayesian concept. The following chapters consider more strict
Bayesian approaches for speech and language processing.
However, if we consider some other spoken language processing applications given automatic speech
recognition inputs (e.g., dialog, machine translation, and information retrieval), we need to consider how to
provide 
 rather than 
 to avoid propagating any speech recognition errors to the
post-processing applications.
This term is called the evidence, which is neglected in the MAP approximation. However, the importance of
the evidence is discussed in Chapter 5.
In other words, it is not simple to obtain the mode of the posterior distribution, if we do not use the
conjugate distribution, since we cannot obtain the posterior distribution analytically. For example, if we use
the Laplace distribution for the prior distribution, the mode of posterior distributions cannot be obtained
analytically, and we need some numerical computation.
We can set a hyperparameter ϕ which depends on each element d, i.e., 
. Considering the
compatibility with the Wishart distribution, this book uses 
, which is independent of d.
There are several approaches combining indirect adaptation via the estimation of transformation parameters
and MAP-based direct estimation of CDHMM parameters (Digalakis & Neumeyer 1996, Takahashi &
Sagayama 1997).
The summation over state sequences 
 in the numerator in Eq. (4.111) is often approximated by the
Viterbi sequence without the summation obtained by the Viterbi algorithm in Section 3.3.2, i.e.,
(4.112)
Similarly, the exact summation over W in the denominator is almost impossible in the large-scale ASR, and
it is also approximated by the summation over pruned word sequences in a lattice, which is obtained after
the ASR decoding process.
Note again that Σ means the diagonal component of the covariance matrix, and does not mean the standard
deviation σ.

5    Evidence approximation
In a maximum a-posteriori (MAP) approximation as addressed in Chapter 4, we
treat model parameters Θ of a target model M as unknown but deterministic
variables which are estimated by maximizing the posterior probability 
using some observation data O. Prior distribution 
 with heuristically
determined hyperparameters Ψ is introduced. This is known as the point
estimation of a target model. However, from a full Bayesian perspective, we
treat model parameters as random variables where the randomness is represented
by a prior distribution 
. In contrast with point estimation in a MAP
approximation based on heuristic hyperparameters, the distribution estimation is
implemented for full Bayesian learning. According to the distribution estimation,
we find the whole prior distribution 
, or equivalently estimate the
corresponding hyperparameters Ψ from observations O in an empirical fashion.
In this implementation, the marginalization of likelihood function 
 with
respect to model parameters Θ should be calculated for model construction, as
follows:1
(5.1)
Rather than trusting the point estimate in MAP approximation, the resulting
evidence function 
 in evidence framework considers all possible values
of model parameters when making a prediction of O as new observation data or
training data depending on the task. In cases of complicated model structure and
coupled latent variables, this evidence function is prone to be intractable and
should be approximated to estimate the optimal hyperparameters Ψ. For the
applications of speech and language processing, we focus on acoustic modeling
and language modeling in accordance with the distribution estimation based on
evidence approximation.

In what follows, Section 5.1 first presents the evidence framework and
addresses the type-2 maximum likelihood estimation for general pattern
recognition. The optimal hyperparameters are estimated based on this
framework. The optimization of evidence function or marginal likelihood is then
extended to sparse Bayesian learning for acoustic modeling based on Bayesian
sensing hidden Markov models (Saon & Chien 2012a) in Section 5.2. In this
section, the scheme of automatic relevance determination is introduced and
illustrated. In addition, evidence approximation to a hierarchical Dirichlet
language model (MacKay & Peto 1995) is detailed in Section 5.3. The optimal
hyperparameters are obtained for the acoustic model as well as the language
model. These Bayesian models are beneficial for noise-robust speech recognition
and large vocabulary continuous speech recognition.
5.1   Evidence framework
This section begins with a general discussion of the evidence framework. We
first review the well-known Bayes theorem, as discussed in Section 2.1. The
evidence function 
, given prior parameters Ψ, is introduced in the Bayes
theorem, which relates posterior distribution 
 to the likelihood
function 
, prior distribution 
, and the evidence function 
by
(5.2)
In words:
(5.3)
The evidence function is a marginal likelihood function which takes all values of
model parameters Θ into account. It is precisely the normalization term that

appears in the denominator in Bayes’ theorem as shown in Eq. (5.2).
Although the evidence function has appeared in the previous sections (e.g.,
the MAP approximation in Eq. (4.2)), it has not been explicitly considered so far.
However, the evidence function 
 can directly link the hyperparameters Ψ
and observations O by marginalizing the model parameters Θ, and can be used
to infer the hyperparameters Ψ in the Bayesian framework.
5.1.1
 Bayesian model comparison
This section also considers the model structure M in the evidence framework. In
MacKay (1992a), the evidence framework was proposed to conduct a Bayesian
model comparison, where the best model or model structure M is selected
according to the posterior distribution of M as
(5.4)
Here 
 is a prior distribution of the model structure M. In the case that each
model is equally probable or has uniform probability, i.e., 
,
different models M are ranked according to the evidence function 
,
which is obtained by marginalizing the model parameters Θ based on the product
and sum rules as
(5.5)
The marginalization of likelihood function over model parameters Θ or model
structure is calculated to come out with a meaningful objective for model
selection. However, we may obtain some insight into the model evidence by
making a simple approximation to the integral over parameters Θ
(MacKay 1992a, Bishop 2006). Consider the case of a model with a posterior
distribution which is sharply peaked around the most probable value 
,
with the width 
. Then the posterior distribution is represented as 

. Similarly, we also assume that the prior is flat with width 
,
and we have 
. Therefore, by using the Bayes theorem, the evidence
function in Eq. (5.5) can be approximated without solving the integral as:
(5.6)
where we omit the model structure index M for simplicity. The approximation to
model evidence is illustrated by Figure 5.1. Without loss of generality, the
notation Θ in Eq. (5.6) is treated as a single parameter. Taking the logarithm, we
obtain
(5.7)
In this approximation, the first term represents the goodness-of-fit to the data O
given the most probable parameter value 
. The second term is known as the
Occam factor (MacKay 1992a) which penalizes the model according to its
complexity. Theoretically, we have the property 
. The
Occam factor is negative and it increases in magnitude as the ratio 
 gets
smaller. Thus, if parameters are finely tuned to the data in posterior distribution,
then the penalty term is large. In practice, the model complexity or the Occam
factor is multiplied by the number of adaptive parameters N in Θ. The optimal
model complexity, as determined by the maximum evidence, is given by a trade-
off between these two competing terms. A refined version of this evidence
approximation could be further derived as:
(5.8)

by using the Laplace approximation, which is detailed in Section 6.1. In
Eq. (5.8), H is the Hessian matrix:
(5.9)
which is the second derivative of the negative log posterior. The determinant of
this matrix plays an important role in the Occam factor, which penalizes the
model complexity.
Figure 5.1 Evidence approximation. Adapted from Bishop (2006).
We can attain further insight into Bayesian model comparison and
understand how the marginal likelihood is favorable to the models with
intermediate complexity by considering Figure 5.2 (Bishop 2006). Here, the
horizontal axis is a one-dimensional representation of the data space O. We
consider three models 
, 
, and 
 in which 
 is the simplest and 
 is
the most complex. When generating a particular data set from a specific model,
we first choose the values of the parameters from their prior distribution 
.
Then, given these parameter values, we sample the data from 
. A simple
or shallow model has little variability and so will generate data sets using p(O),
which is confined to a relatively small region in space O. In contrast, a complex
or deep model can generate a variety of data sets, and so its distribution p(O) is
spread over a large region of the data space O. In this example, for the particular
observed data set 
, the model 
 with intermediate complexity has the
largest evidence.

Figure 5.2 Model comparison based on the maximum evidence. Adapted
from Bishop (2006).
5.1.2
 Type-2 maximum likelihood estimation
The complexity of a model M is generally defined by the scope of data set O that
model M could predict. This scope is not only determined by the model size
(e.g., model structure, model order, or number of parameters N), but is also
affected by the hyperparameters Ψ of the parameters Θ which are used to
generate the observations O. Instead of point estimation of model parameters Θ
in MAP approximation based on heuristically determined hyperparameters, the
evidence approximation conducts distribution estimation which determines the
optimal prior distribution 
 as a whole, or equivalently infers the optimal
hyperparameters  corresponding to the prior distribution according to
(5.10)
Differently from conventional ML estimation for model parameters Θ, the type-2
maximum likelihood (ML2) estimation aims to search the optimal
hyperparameters 
 from the observation data O. In the literature, a point
estimation based on ML or MAP conducts the so-called level-1 inference, while
the distribution estimation based on ML2 undertakes the level-2 inference
(MacKay 1995, Kwok 2000). The level-3 inference is performed to rank
different models M according to the posterior probability 
 or the
evidence function 
 with equally probable model M. Three levels of
inference can be iterated.

The evidence framework or the ML2 estimation has been developed for
different learning machines including linear regression/classification networks
(MacKay 
1992b, 
Bishop 
2006), 
feed-forward 
neural 
network 
(NN)
(MacKay 1992c), support vector machine (SVM) (Kwok 2000), and hidden
Markov model (Zhang, Liu, Chien et al. 2009). For the cases of linear regression
and neural network regression models, the optimal hyperparameters of weight
parameters and modeling errors are estimated by maximizing the evidence
function of training data which is marginalized with respect to the weight
parameters. In Zhang et al. (2009), the optimal hyperparameters were derived for
the mixture model of exponential family distributions and then realized to build
the 
robust 
HMMs 
for 
noisy 
speech 
recognition. 
Practically, 
these
hyperparameters Ψ are interpreted as the regularization parameter 
, which
plays a crucial role in the regularized regression models. The regularized models
are developed to prevent the over-fitting problem in conventional models based
on ML estimation or least-squares regression. In what follows, the evidence
framework is illustrated to be closely connected to the regularization theory,
which has been developed to regularize model structure and deal with the over-
fitting problem when building generative models for speech recognition and
other information systems.
5.1.3
 Regularization in regression model
Model regularization is a fundamental issue in pattern recognition. It aims to
estimate the smoothed parameters or construct a generalized model which has
good prediction capability for future unseen data. The gap or mismatch between
training data and test data can be compensated by tackling this issue. The over-
fitting problem or the ill-posed problem is also resolved by following the
regularization theory. In the regularized least-squares (RLS) model 
, the
over-fitting problem is avoided by incorporating a regularization term 
into the training objective, and this penalizes too complex a model. Here, model
parameters Θ are rewritten by an N-dimensional weight vector w. The
regularization term is determined by the weight parameters w and the
corresponding model structure M. The simplest form of this regularizer is given
by a sum-of-squares of the parameter elements:

(5.11)
The sum-of-squares error function is calculated by introducing training samples 
 given by model parameters w, i.e.
(5.12)
where 
 is the target value of the observation 
 at time t. Accordingly, the
training objective for RLS parameters 
 is yielded as a regularized least-
squares function which is formed by
(5.13)
Notably, a regularization parameter  is introduced in Eq. (5.13) to balance the
trade-off between the sum-of-squares error function 
 and the model
complexity penalty function 
. Minimization of the training objective in
Eq. (5.13) eventually obtains a set of parameters w which works simultaneously
towards the goals of fitting the data and reducing the norm of the solution.
Regularization theory is beneficial for model selection. Regularization parameter
 is generally selected by applying the cross validation method using a small set
of validation data which is outside the training data O.
Nevertheless, it is more attractive to pursue Bayesian interpretation of model
regularization. Considering the same regression problem, but now under a
probabilistic framework, we assume that the modeling error 
 has a
Gaussian distribution:
(5.14)
and the parameter vector w comes from a Gaussian distribution

(5.15)
where I is the 
 identity matrix and β and α are the precision
parameters for modeling error and parameter vector, respectively. Here, the
hyperparameters 
 consist of α and β. The MAP estimate of model
parameters 
 is obtained by maximizing the posterior distribution
(5.16)
or equivalently minimizing the negative log posterior distribution,
(5.17)
by using training samples 
. It is interesting to see that
maximizing the posterior distribution in Eq. (5.17) is equivalent to minimizing
the regularized least-squares error function in Eq. (5.13) with a regularization
parameter 
. Readers may refer to (MacKay 1992c, Bishop 2006) for
detailed solution to optimal hyperparameters 
 of a linear
regression model with regression function
(5.18)
A linear combination of fixed non-linear functions of the input variable 
 is
considered. Here, 
 denotes the basis functions and 
 is assigned. For the case of a neural network regression model, ML2
estimation of hyperparameters is addressed in Section 6.4.
The comparison among RLS estimation, MAP estimation, and ML2
estimation is further investigated below. According to RLS estimation, level-1
inference is performed to find model parameters 
 by using training data O
while the hyperparameter 
 is estimated in level-2 inference via a cross

validation scheme by using additional validation data. However, Bayesian
inference is implemented to calculate model parameters 
 in level-1
inference based on MAP estimation. In level-2 inference, the hyperparameters 
 are inferred using ML2 estimation. By applying MAP and
ML2 methods, the same training data O are used to estimate parameters 
and hyperparameters 
, respectively. It is desirable that no validation data
are required by using the Bayesian approach. Besides, RLS and MAP methods
fulfil the point estimation and assume that the estimates 
 and 
 are true values for prediction of new data O in a test phase according
to likelihood function 
. Instead of relying on single parameter values 
in the RLS or MAP method, the ML2 method implements the distribution
estimation and directly infers the optimal hyperparameters 
 by
maximizing the predictive distribution or the marginal likelihood of training data
, where all possible values of parameters w are
considered. In a test phase, the same marginal distribution 
, given the
estimated hyperparameters , is applied for prediction of new data O.
5.1.4
 Evidence framework for HMM and SVM
Although the evidence framework is only addressed for a linear regression
model 
, extensions to the classification models including HMM and SVM
have been proposed in (Zhang et al. 2009) and (Kwok 2000), respectively. When
ML2 estimation is applied for the HMM framework, we estimate the
hyperparameters of continuous-density HMM parameters including initial state
probabilities 
, state transition probabilities 
, mixture weights 
, mean
vectors 
, and covariance matrices 
. For the probability parameters 
, 
 and 
 in multinomial distributions, ML2 estimation is performed to find
the corresponding hyperparameters Ψ which are the parameters of Dirichlet
priors for multinomial or discrete variables of states j, state pairs 
, and
mixture components k, respectively. For the remaining Gaussian parameters 
and 
 of continuous feature vectors 
, ML2 estimation aims to calculate
the corresponding hyperparameters Ψ which are the parameters of Gaussian–
Wishart priors for Gaussian mean vectors 
 and precision (or inverse
covariance) matrices 
. In general, Dirichlet distribution is known as the
conjugate prior for multinomial variables while Gaussian–Wishart distribution is
seen as the conjugate prior for Gaussian variables. By following this guideline,
the closed-form solution to the integral in marginal likelihood does exist, so that
the optimization of marginal distribution with respect to individual

hyperparameters has an analytical solution. These hyperparameters characterize
the uncertainties of HMM parameters which could be applied for robust speech
recognition according to the Bayesian predictive classification as addressed in
Eq. (3.16) and Section 6.3. This approach is different from conventional BPC
based on the hyperparameters which are heuristically determined or calculated in
an ensemble way (Huo & Lee 2000, Chien & Liao 2001).
The support vector machine (SVM) approach is based on the idea of
structural risk minimization, which shows that the generalization error is
bounded by the sum of the training set error and the Vapnik-Chervonenkis (VC)
dimension of the learning machine (Abu-Mostafa 1989, Vapnik 1995). By
minimizing this upper bound, generalization to future data is improved.
Generalization error is related not to the number of inputs, but to the margin with
which it separates the data. SVM has been successfully applied in many
classification problems including speech recognition (Ganapathiraju, Hamaker &
Picone 2004). Although SVM is a nonparametric method, the probabilistic
framework and Bayesian perspective have been introduced to deal with the
selection of two tuning parameters or hyperparameters, including:
a regularization parameter 
, which determines the trade-off between
minimizing the training errors and minimizing the model complexity;
a kernel parameter, which implicitly defines the high dimensional feature
space to be used.
Conventionally, these hyperparameters are empirically selected by hand or via
cross validation. The evidence framework has been applied to find the optimal
regularization parameter for SVM (Kwok 2000). Next, we address the detailed
estimation of hyperparameters for two practical solutions to speech recognition.
One is developed for sparse Bayesian acoustic modeling while the other is
proposed for hierarchical Dirichlet language modeling.
5.2   Bayesian sensing HMMs
Speech recognition systems are usually constructed by collecting large amounts
of training data and estimating a large number of model parameters to achieve
the desired recognition accuracy on test data. A large set of context-dependent
Gaussian components (several hundred thousand components is usually the

norm) is trained to build context-dependent phone models. GMMs with
Gaussian mean vectors and diagonal covariance matrices may not be an accurate
representation of high dimensional acoustic features. The Gaussian components
may be overdetermined. The mismatch between training data and test conditions
may not be carefully compensated. The uncertainty of estimated HMM
parameters may not be properly characterized. A Bayesian learning approach is
introduced to tackle these issues based on the basis representation. ML2
estimation is conducted to estimate the automatic relevance determination
(ARD) parameter (MacKay 1995, Tipping 2001) which is the state-dependent
hyperparameter of weight parameter in basis representation. Sparse Bayesian
learning is performed by using the ARD parameter.
5.2.1
 Basis representation
An acoustic feature vector ocan be viewed as lying in a vector space spanned by
a set of basis vectors. Such a basis representation has been popular for regression
problems in machine learning and for signal recovery in the signal processing
literature. This approach is now increasingly important for acoustic feature
representation. Compressive sensing and sparse representation are popular topics
in the signal processing community. The basic idea of compressive sensing is to
encode a feature vector 
 based on a set of overdetermined dictionary or
basis vectors 
 via
(5.19)
where the sensing weights 
 are sparse and the basis vectors 
 are formed by training samples. A relatively small set of relevant basis
vectors is used for sparse representation based on this exemplar-based method.
The sparse solution to w can be derived by optimizing the 
-regularized
objective function (Sainath, Ramabhadran, Picheny et al. 2011). However, the
exemplar-based method is a memory-based method, which is time-consuming
with high memory cost. It is also important to integrate HMMs into sparse
representation of continuous speech frames 
.
5.2.2
 Model construction
Bayesian sensing HMMs (BS-HMMs) (Saon & Chien 2012a) are developed by

incorporating Markov chains into the basis representation of continuous speech.
A Bayesian sensing framework is presented for speech recognition. The
underlying aspect of BS-HMMs is to measure an observed feature vector 
 of a
speech sentence O based on a compact set of state-dependent dictionary 
 at state j. For each frame, the reconstruction error between
measurement 
 and its representation 
, where 
, is
assumed to be Gaussian distributed with zero mean and a state-dependent
covariance matrix or inverse precision matrix 
. The state likelihood
function with time-dependent sensing weights 
 is defined by
(5.20)
The Bayesian perspective in BS-HMMs has its origin from the relevance vector
machine (RVM) (Tipping 2001). Figure 5.3 illustrates the graphical model based
on BS-HMMs. The RVM is known as a sparse Bayesian learning approach for
regression and classification problems. We would like to apply RVM to conduct
sparse basis representation and combine it with HMMs to characterize the
dynamics in the time domain. Therefore, BS-HMM parameters are obtained by 
. Obviously, similarly to conventional GMM-based HMMs,
we can extend BS-HMM to deal with a mixture model for basis representation
where the mixture weight 
, the basis vectors 
, and the precision matrix 
of individual mixture component k are incorporated. In what follows, we neglect
the extension to a mixture model and exclude the time-dependent weight
parameters 
 from the parameter set Θ.
Figure 5.3 Graphical model of BS-HMM.
5.2.3
 Automatic relevance determination

However, the sensing weights 
 play a crucial role in basis representation, and
so we introduce Bayesian compressive sensing (Ji, Xue & Carin 2008) for
acoustic modeling. The idea of Bayesian learning in BS-HMMs is to yield the
distribution estimates of the speech feature vectors 
 due to the variations of
sensing weights 
 in basis representation. A Gaussian prior with zero mean
and state-dependent diagonal precision matrix 
 is introduced to
characterize the weight vector, i.e.
(5.21)
Considering a hierarchical prior model where precision parameter 
 is
represented by a gamma prior with parameters a and b in Appendix C.11:
(5.22)
The marginal prior distribution is derived as a Student’s t-distribution as defined
in Appendix C.16:
(5.23)
which is known as a sparse prior distribution, since this distribution has a heavy
tail and steep peak. Student’s t-distribution, illustrated in Figure 5.5, is a heavy
tailed distribution and is more robust to outliers than a Gaussian distribution
(Figure 5.4) (Bishop 2006).

Figure 5.4 An example of two-dimensional Gaussian distribution.
Figure 5.5 An example of two-dimensional Student’s t-distribution.
Correspondingly, if the precision parameter 
 in 
 is large, the
weight parameter 
 is likely to be zero, 
, which implies that the
associated basis vector 
 is irrelevant to Bayesian basis representation of a
target observation 
. The physical meaning of automatic relevance
determination (ARD) is then reflected by the precision parameter 
 in state-
dependent hyperparameters, 
. We simply call 
 the ARD
parameter. According to the ARD scheme, only relevant basis vectors are
selected to represent sequence data O. Sparse Bayesian learning (SBL) can be
realized by using an ARD scheme (Tipping 2001). In the implementation, the
values of state-dependent ARD parameters 
 can be used to rank or
select salient basis vectors 
 which are relevant to a target HMM state
j. The larger the estimated value 
, the more likely it is that the basis vector 
should be pruned from the parameter set. The compressed model can be
achieved by applying this property (Saon & Chien 2011). The ARD parameter
serves as a compression factor for model complexity control. One can initially

train a large model and then prune it to a smaller size by removing basis
elements which correspond to the larger ARD values. Although we appear to be
utilizing a non-sparse Gaussian prior over the weights, in truth the hierarchical
formulation implies that the real weight prior is clearly recognized as
encouraging sparsity. Considering this sparse Bayesian basis representation, BS-
HMM 
parameters 
and 
hyperparameters 
are 
formed 
by 
 consisting of initial state probability 
,
state transition probability 
, basis vectors 
, and precision matrices of
reconstruction errors 
 and sensing weights 
. Level-1 inference and level-2
inference are done simultaneously in BS-HMMs.
5.2.4
 Model inference
We estimate BS-HMM parameters and hyperparameters from the observed
speech data O according to the type-2 ML (ML2) estimation:
(5.24)
Without loss of generality, we view Eq. (5.24) as the ML2 estimation because
the marginal likelihood with respect to sensing weights 
 is calculated at each
frame 
t 
in 
likelihood 
function 
. 
However, 
the
marginalization over 
, 
, 
, and 
 is not considered. Since the optimization
procedure is affected by an incomplete data problem, the EM algorithm
(Dempster et al. 1976) is applied to find the optimal solution to 
. In E-step, an auxiliary function is calculated by averaging
the log likelihood function of the new estimates 
, given the old estimates
 over all latent variables 
:
(5.25)
In the M-step, we maximize the auxiliary function with respect to new
parameters and hyperparameters 
,

(5.26)
to find optimal parameters and hyperparameters. The auxiliary function is
expanded by
(5.27)
where 
 is the posterior probability of being in state j at
time t given the current parameters and hyperparameters 
 generating
measurements O. We tacitly use the convention 
. Since the
estimation of initial state probability 
 and state transition probability 
 is the
same as that in HMMs, we neglect the estimation of 
 hereafter.
5.2.5
 Evidence function or marginal likelihood
The key issue in the E-step is to calculate the frame-based evidence function or
marginal likelihood 
, which is marginalized over sensing
weights 
 at state 
 and is proportional to

(5.28)
In Eq. (5.28), the notations
(5.29)
(5.30)
are introduced, and the integral of a Gaussian distribution
(5.31)
is manipulated. By applying the Woodbury matrix inversion (Eq. (B.20))
(5.32)

given the dimensionally compatible matrices A, U, C, and V, the marginal
likelihood 
 is derived as
(5.33)
which is a Gaussian likelihood function with zero mean. The equality of the
determinant
(5.34)
is held. This implies that frame discrimination among different states is done
solely on the basis of the covariance matrix. Apparently, the covariance matrix 
 is positive definite, so that 
 is a valid probability
density function. For diagonal 
, the marginal likelihood is seen as a new
Gaussian distribution with a factor analyzed covariance matrix 
, where the factor loading matrix 
 is seen as a rank-N correction to 
(Saon & Chien 2011).
5.2.6
 Maximum a-posteriori sensing weights
In BS-HMMs, we can determine the maximum a-posteriori (MAP) estimate of
Bayesian sensing weights 
 for each observation 
 from
(5.35)
which is seen as a weighted product in vector space of observation 
 and
transposed basis vectors 
. The notations 
 (or equivalently 
) and 
 are
the mean vector and the covariance matrix of the posterior distribution 

, respectively. The precision matrix for 
 is modified from 
of a-priori density 
 to 
 of the a-posteriori distribution 
.
The difference term 
 comes from the likelihood function 
, and is
caused by the measurement 
 for observation 
 at frame t represented by
new basis vectors 
 of state j. This is meaningful because Bayesian learning
performs subjective inference, naturally increasing the model precision.
5.2.7
 Optimal parameters and hyperparameters
By substituting Eq. (5.28) into Eq. (5.27), the optimal BS-HMM parameters and
hyperparameters are estimated by maximizing the expanded auxiliary function
with respect to individual parameters and hyperparameters 
. The
auxiliary function in a BS-HMM state is simplified to
(5.36)
Let us first consider the maximization of Eq. (5.28) with respect to 
hyperparameter matrix 
. We take the gradient of Eq. (5.28) with respect to 
and set it to zero to obtain
(5.37)
The value of 
 that maximizes the auxiliary function satisfies

(5.38)
Notably, Eq. (5.38) is an implicit solution to 
 because 
 is a function of 
.
The hyperparameter 
 of sensing weights is obtained by adding the
covariance matrix 
 of posterior 
 and the weighted
autocorrelation of MAP sensing weights 
.
To find an ML2 estimate of basis vectors 
, we maximize Eq. (5.36) by
taking the gradient of the terms related to  and setting it to zero, which leads to
(5.39)
where the gradients of the two terms are derived as the 
 matrices given
by
(5.40)
(5.41)

The optimal solution 
, which is a 
 matrix, satisfies
(5.42)
Similarly to the solution to 
, the ML2 estimate can be expressed in an implicit
form written by a function 
, namely
(5.43)
This solution is viewed as a weighted operation in the outer product space of
observations 
 and MAP sensing weights 
. The posterior
probabilities 
 and the ML2 hyperparameters 
 serve as the weights and
the rotation operator of the weighted average, respectively.
To find the ML2 estimate of precision matrix 
, we maximize Eq. (5.36)
with respect to  and obtain
(5.44)

where
(5.45)
We derive the ML2 solution as
(5.46)
which is also an implicit solution to 
 since the right-hand-side (RHS) of
Eq. (5.46) depends on 
. Note that the RHS of Eq. (5.46) is symmetric positive
definite. The first term is a scaled covariance matrix 
 of the posterior
distribution 
, which is doubly transformed by . The second term
is interpreted as a covariance matrix weighted by posterior probabilities 
,
and is calculated using observations 
 and “mean” vectors 
. This
corresponds to performing Bayesian sensing, again using the MAP estimates 
.
We can see that type-2 ML (ML2) estimates of BS-HMM parameters and
hyperparameters are consistently formulated as the implicit solutions, which are
beneficial for efficient implementation and good convergence in parameter
estimation. Differently from conventional basis representation, where basis
vectors and sensing weights are found separately, BS-HMMs provide a
multivariate Bayesian approach to hybrid estimation of the compact basis

vectors and the precision matrices of sensing weights under a consistent
objective function. No training examples are stored for memory-based
implementation.
To improve LVCSR performance based on BS-HMMs, there have been
several extensions developed for acoustic modeling. As mentioned in Section
5.2.2, the mixture model of BS-HMMs can be extended by considering multiple
sets of basis vectors per state. A mixture model of basis vectors is included for
acoustic modeling. Using this mixture model, each observation 
 at state 
 is expressed by
(5.47)
where 
 is the mixture weight of jth component with the constraint
(5.48)
Here, the reconstruction error of an observation vector 
 due to the jth
component with basis vectors 
 is assumed to be Gaussian
distributed with zero mean and precision matrix 
. In addition, BS-HMMs can
be constructed by incorporating a non-zero mean vector 
 in the prior density
of sensing weights, i.e.,
(5.49)
Similarly to the Maximum Likelihood Linear Regression (MLLR) adaptation for
HMMs, BS-HMMs are developed for speaker adaptation where the nth BS-
HMM basis vector is transformed by
(5.50)

where 
 is a 
 regression matrix and 
 is the extended
basis vector. The type-2 ML estimation can be applied to calculate the optimal
solutions to non-zero mean vector 
 and regression matrix 
.
5.2.8
 Discriminative training
Finally, BS-HMMs are sophisticated, incorporating both model-space and
feature-space discriminative training, which is crucial to improve classification
of confusing patterns in pattern recognition systems. Developing discriminative
training for BS-HMMs is important for LVCSR. Instead of the goodness-of-fit
criterion using marginal likelihood function, the objective function for
discriminative training is established according to the mutual information
between observation data O and the sequence of reference words 
 (Bahl
et al. 1986, Povey & Woodland 2002, Povey, Kanevsky, Kingsbury et al. 2008):
(5.51)
which consists of a numerator term 
 and a denominator term 
.
The Maximum Mutual Information (MMI) estimation of BS-HMMs 
 is
performed for discriminative training. To solve the optimization problem, we
calculate 
the 
weak-sense 
auxiliary 
function 
(Povey 
&
Woodland 2002, Povey 2003), where the HMM state sequence S is incorporated
as follows:
(5.52)

The property of weak-sense auxiliary function turns out to meet the condition
(5.53)
where the mode of MMI auxiliary function and its weak-sense auxiliary function
have the same value. The smoothing function 
 in Eq. (5.52) is added
to ensure that the objective function 
 is improved by this extended EM
algorithm. For this, the smoothing function should satisfy
(5.54)
In what follows, we address the discriminative training of basis vectors 
 of
state j. The same procedure can be applied to estimate the discriminative
precision matrix of reconstruction errors 
.
One possible choice of smoothing function meeting Eq. (5.54) is formed by
the Kullback–Leibler divergence 
 between marginal likelihoods of the
current estimate 
 and the new estimate 
 given by

(5.55)
Here, 
 is a state-dependent smoothing constant. Typically, the smoothing
function is mathematically intractable when applying marginal likelihoods.
Noting this, we can approximate the marginal likelihood by using an average
plug-in MAP estimate:
(5.56)
obtained by taking an ensemble average of the MAP estimates in Eq. (5.35)
using all observation frames 
. Ignoring the terms independent of 
, the smoothing function is obtained by substituting the approximate MAP
estimate of Eq. (5.56) into

(5.57)
As a result, the weak-sense auxiliary function is expressed in terms of state
occupation posteriors as follows:
(5.58)
where the state occupation posteriors of staying in state 
 at time t in the
numerator and denominator terms are calculated by
(5.59)
(5.60)
given the reference word sequence 
 and all possible word sequences 
,
respectively. The current estimates 
 are used in this calculation. To find an
MMI estimate for basis parameters, we differentiate Eq. (5.58) with respect to 
and set it to zero:
(5.61)

which is derived by considering the definition of variables 
 and 
 in
Eq. (5.29) and Eq. (5.30), respectively. Again, the implicit solution to an MMI
estimate of  should satisfy
(5.62)
This solution is expressed as a recursive function 
 of new basis parameter 
. Strictly speaking, an MMI estimation based on marginal likelihood is seen as
a type 2 MMI (MMI2) estimation, which is different from conventional MMI
training (Bahl et al. 1986, Povey & Woodland 2002, Povey et al. 2008) based on
likelihood function without marginalization. In Eq. (5.62), the second terms in
the numerator and the denominator come from smoothing function 
 and
serve to prevent instability in the MMI2 optimization procedure. The solution is
highly affected by the difference between statistics for the reference hypothesis
and statistics for the competing hypotheses 
.
It is clear that discriminative training and Bayesian learning are
simultaneously performed in a type 2 MMI estimation. By doing this, the
performance of LVCSR can be significantly improved (Saon & Chien 2012b).
The robustness to uncertainty of sensing weights in a basis representation can be
assured. Some experimental results are described below.
5.2.9
 System performance
Evaluation of BS-HMMs was performed by using the LVCSR task in the domain
of Arabic broadcast news transcription which was part of the DARPA GALE
program. In total, 1800 hours of manually transcribed Arabic broadcast news
and conversations were used in this evaluation (Saon & Chien 2011). The results

on several test sets were reported: DEV’07 (2.5 hours), DEV’08 (3 hours) and
DEV’09 (3 hours). The front-end processing was performed as mentioned in
Saon & Chien (2012b). The vocal-tract-length-warped PLP (Hermansky 1990)
cepstrum features were extracted with a context window of nine frames. The
features were mean and variance normalized on a per speaker basis. Linear
discriminant analysis was used to reduce the feature dimension to 40. The
maximum likelihood training of the acoustic model was interleaved with the
estimation of a global semi-tied covariance transform (Gales 1998). All models
in this evaluation were estimated based on pentaphones and speaker adaptively
trained with feature-space MLLR (fMLLR). Each pentaphone was modeled by a
3-state left-to-right HMM without state skipping. At test time, speaker
adaptation was performed with vocal-tract-length normalization (Wegmann,
McAllaster, Orloff et al. 1996), fMLLR and multiple regression MLLR
transforms. The vocabulary contained 795K words. The decoding was done with
4-gram language models which were estimated with modified Kneser–Ney
smoothing. The acoustic models were discriminatively trained in both feature
space and model space according to the boosted MMI criterion (Povey,
Kingsbury, Mangu et al. 2005). The baseline acoustic models had 5000 context-
dependent HMM states and 800K 40-dimensional diagonal covariance
Gaussians.
In the implementation, BS-HMM parameters were initialized by training a
large HMM model with 2.8M diagonal covariance Gaussians by maximum
likelihood method. The means of GMM were clustered and then treated as the
initial basis 
 for state j and mixture component k. The resulting number of
mixture components in BS-HMMs after the clustering step was 417K. The
precision matrices 
 and 
 were assumed to be diagonal and were initialized
to the identity matrix. Table 5.1 compares the performance of the baseline 800K
Gaussians model and the 2.8M Gaussians model used to train baseline acoustic
models after ML training and to seed BS-HMMs after ML2 training. The
number of free parameters is included in this comparison. As we can see, BS-
HMMs outperform both baseline systems in terms of word error rates (%). The
possible reasons are twofold. The first one is that the covariance modeling in
BS-HMMs is more accurate than that in HMMs. The second one is due to the
Bayesian parameter updates which provide an effective smoothing.
Table 5.1 Comparison of the number of free parameters and word error rates
for baseline acoustic models after ML training and BS-HMMs after ML2

training.
System
Nb. parameters
WER
DEV’07
DEV’08
DEV’09
Baseline 800K
64.8M
13.8%
16.4%
19.6%
Baseline 2.8M
226.8M
14.1%
16.2%
19.3%
BS-HMM 417K
148.5M
13.6%
16.0%
18.9%
In contrast, we conducted a model comparison for BS-HMMs according to
the estimated hyperparameters of sensing weights. Model compression was
performed by discarding 50% of the basis vectors 
 corresponding to the
largest hyperparameters 
. This results in a compressed model with
approximately 91M free parameters (about 30% larger than the 800K baseline
HMM). For both original and compressed models, we performed model-space
discriminative training using the MMI2 criterion. Table 5.2 reports the
recognition performance before and after model-space discriminative training.
We find that the compressed models outperform the originals after
discriminative training even though they start from a higher word error rate after
ML2 estimation. However, discriminative training is significantly more
expensive than ML estimation which makes it difficult to find the optimal model
size.
Table 5.2 Comparison of word error rates for original and compressed BS-
HMMs before and after model-space discriminative training with MMI2
training.
Model
Training
DEV07
DEV08
DEV09
Original
ML2
12.0%
13.9%
17.4%
Compressed
ML2
12.4%
14.2%
17.6%
Original
MMI2
10.7%
11.9%
15.0%
Compressed
MMI2
10.4%
11.7%
14.8%

5.3   Hierarchical Dirichlet language model
In what follows, we revisit the interpolation smoothing methods presented in
Section 3.6 and used to deal with the small sample size problem in ML
estimation of n-gram parameters 
. Different from the heuristic
solutions to language model smoothing in Section 3.6, a full Bayesian language
model is proposed to realize interpolation smoothing for n-grams. The theory of
evidence approximation is developed and applied to construct the hierarchical
Dirichlet language model (MacKay & Peto 1995, Kawabata & Tamoto 1996).
5.3.1
 n-gram smoothing revisited
In general, the frequency estimator in a higher-order language model has large
variance, because there are so many possible word combinations in an n-gram
event 
 that only a small fraction of them have been observed in the data.
A simple linear interpolation scheme for an n-gram language model is performed
by interpolating an ML model of an n-gram 
 with that of an 
-
gram 
 using
(5.63)
where  denotes the interpolation weight, which can be determined empirically
from validation data. Basically, it is not possible to make language models
without making a-priori assumptions. The smoothed n-gram 
 in
Eq. (5.63) can be seen as an integrated n-gram model which is determined from
a hierarchical model based on the smoothing parameters a-posteriori from the
data. In what follows, we would like to reverse-engineer the underlying model,
which gives a probabilistic meaning to language model smoothing. We will
explain the construction of a hierarchical prior model and illustrate how this
model is used to justify the smoothed language model from a Bayesian
perspective. A type-2 ML estimation is conducted to find optimal
hyperparameters from training data. No cross validation procedure is required in
a Bayesian language model.
5.3.2
 Dirichlet prior and posterior

n-gram model parameters 
 are known as multinomial parameters
which are used to predict n-gram events of word 
 appearing after observing
history words 
. The ML estimation of an n-gram 
 from a text
corpus 
 has been shown in Eq. (3.192). Here, we are interested in a Bayesian
language model where the prior density of multinomial parameters is introduced.
A parameter vector consists of N multinomial parameters:
(5.64)
The conjugate prior over multinomial parameters  is specified by a Dirichlet
prior with hyperparameters 
, which is a multivariate
distribution in the form of
(5.65)
We express the normalization constant of the Dirichlet distribution by
(5.66)
The mean vector of Dirichlet distribution is given by
(5.67)
When we observe the training samples 
, the posterior distribution is derived as
another Dirichlet distribution:

(5.68)
with the updated hyperparameters 
. In Eq. (5.68), each entry 
 of
(5.69)
denotes the number of occurrences of the ith n-gram event in  in training data 
. This shows the property of a conjugate prior by using the Dirichlet
distribution.
5.3.3
 Evidence function
To obtain the predictive probability of a word 
 given history words 
 and
training data 
, we apply the sum rule to calculate the evidence function or the
marginal likelihood:
(5.70)
which is marginalized over all values of parameter . This predictive distribution
is equivalent to calculating the mean of an n-gram parameter:

(5.71)
based on the posterior distribution 
, which is a Dirichlet distribution
with hyperparameters 
. Here, the n-gram probability 
 is simply
expressed by 
 where j denotes the backoff smoothing information from the
lower-order model 
, which is addressed in Section 5.3.4.
We may further conduct the next level of inference by inferring the
hyperparameters given the data. The posterior distribution of 
 is expressed by
(5.72)
The hierarchical prior/posterior model of parameters  and hyperparameters 
is constructed accordingly. The marginal likelihood over hyperparameters 
 is
yielded by
(5.73)
We may find the most probable MAP estimate 
 by
(5.74)
Then the marginal distribution is approximated as
(5.75)
In addition, we would like to calculate the optimal hyperparameters via ML2
estimation from training data 
 based on the evidence framework. To do so, we
need to determine the evidence function given hyperparameters 
. By
referring to Eq. (5.68), this function is derived as

(5.76)
which is viewed as a ratio of normalization constants of posterior probability 
 over prior probability 
.
5.3.4
 Bayesian smoothed language model
It is important to illustrate the physical meaning of predictive distribution in
Eq. (5.70). The hyperparameter 
 appears as an effective initial count for an n-
gram event 
. This marginal likelihood is integrated by the information
sources from prior statistics 
 as well as training data 
 or their counts of
occurrences c. On the other hand, the predictive distribution in Eq. (5.70) can be
rewritten as
(5.77)
where 
 denotes the ML model introduced in Eq. (3.192) and 
implies the interpolation weight for prior statistics and is herein obtained as
(5.78)
It is interesting to see that the predictive distribution in Eq. (5.77) is interpreted
as the smoothed n-gram based on the interpolation smoothing. We build the tight
connection between the Bayesian language model and a linearly smoothed
language model as addressed in Section 3.6. The prior statistics or
hyperparameters 
 should sufficiently reflect the backoff information

from the low-order model 
 when calculating the predictive n-gram
probability 
. Comparing Eq. (5.78) and Eq. (3.211), the Bayesian
language model is shown to be equivalent to the Witten–Bell smoothed language
model in the case that the hyperparameters 
 are selected to meet the
condition
(5.79)
(As before, the 
 represents any possible words at i that are summed over.)
Nevertheless, the advantage of the Bayesian smoothed language model is to
automatically determine the optimal hyperparameters 
 from training data
.
5.3.5
 Optimal hyperparameters
According to the evidence framework, a type-2 ML estimation is carried out to
find optimal hyperparameters 
 by maximizing the evidence
function
(5.80)
More specifically, we find individual parameter 
 through calculating the
differentiation
(5.81)
where the di-gamma function

(5.82)
is incorporated. We may use the conjugate gradient algorithm to find 
 or
apply some approximation to derive an explicit optimization algorithm.
In general, it is reasonable that 
 and 
. We can use the
recursive formula of the di-gamma function (MacKay & Peto 1995),
(5.83)
to combine the first and fourth terms in the brackets of Eq. (5.81) to obtain
(5.84)
The number of terms in the right-hand-side of Eq. (5.84) is 
. Assuming 
is smaller than 1, we can approximate Eq. (5.84), for 
, by
(5.85)
Here, the function inside the brackets,

(5.86)
is approximated by a Taylor series at the point 
. Further, we apply the
approximation of a di-gamma function,
(5.87)
to approximate the second and third terms in Eq. (5.81) (MacKay & Peto 1995):
(5.88)
For each count c and word i, let 
 be the number of backoff contexts j such
that 
, and let 
 denote the largest c such that 
. Denote the
number of entries in row i of 
 that are non-zero 
 by 
. We compute the
quantities:
(5.89)
(5.90)

Finally, the solution to optimal hyperparameters 
 is obtained. The
hyperparameter 
 corresponding to each word i should satisfy
(5.91)
Again, this solution to ML2 estimation is expressed as an implicit solution
because the right-hand-side of Eq. (5.91) is a function of hyperparameters 
.
Starting from the initial hyperparameters 
 and the resulting function 
, the
individual hyperparameter is then updated to 
. The function 
 is updated
as well. The estimation procedure 
 converges very rapidly.
1  This chapter regards model structure M and parameters of prior distribution Ψ as the hyperparameters in
a broad sense. Equation (5.1) formulates the likelihood function given Ψ, but the discussion can be
applied to the case of using M.

6    Asymptotic approximation
Asymptotic approximation is also well known in practical Bayesian approaches
(De Bruijn 1970) for approximately obtaining the posterior distributions. For
example, as we discussed in Chapter 2, the posterior distributions of a model
parameter 
 
and 
a 
model 
 
given 
an 
observation 
) are usually difficult to solve. The approach assumes
that we have enough data (i.e., T is sufficiently large), which also makes
Bayesian inference mathematically tractable. As a particular example of
asymptotic approximations, we introduce the Laplace approximation and
Bayesian information criterion, which are widely used for speech and language
processing.
The Laplace approximation is used to approximate a complex distribution as
a Gaussian distribution (Kass & Raftery 1995, Bernardo & Smith 2009). It
assumes that the posterior distribution is highly peaked at about its maximum
value, which corresponds to the mode of the posterior distribution. Then the
posterior distribution is modeled as a Gaussian distribution with the mode as a
mean parameter. By using the approximation, we can obtain the posterior
distributions analytically to some extent. Section 6.1 first explains the Laplace
approximation in general. In Sections 6.3 and 6.4 we also discuss use of the
Laplace approximation for analytically obtaining Bayesian predictive
distributions for acoustic modeling and Bayesian extension of successful neural-
network-based acoustic modeling, respectively.
Another example of this asymptotic approximation is the Bayesian
information criterion (or Schwarz criterion (Schwarz 1978)). The Bayesian
information criterion also assumes the large sample case, and approximates the
posterior distribution of a model 
 with a simple equation. Since the
Bayesian information criterion assumes the large sample case, it is also

described as an instance of asymptotic approximations. Section 6.2 explains the
Bayesian information criterion in general; it is used for model selection
problems in speech processing. For example, Section 6.5 discusses the
optimization of an appropriate model structure of hidden Markov models, and
Section 6.6 discusses estimation of the number of speakers and detecting speech
segments in conversations by regarding these approaches as model selection
problems.
6.1   Laplace approximation
This section first describes the basic theory of the Laplace approximation. We
first consider a simple case where a model does not have a latent variable. We
focus on the posterior distributions of model parameters, but this approximation
can be applied to the other continuous probabilistic variables. Let 
 be a J
dimensional 
vector 
form 
of 
continuous 
model 
parameters 
and 
 be a T-frame sequence of D dimensional feature
vectors. We consider the posterior distribution of 
, which has the following
distribution 
:
(6.1)
where 
 is a continuous function over 
. Note that most parametric
distributions can be represented by this equation. The Laplace approximation
approximates 
 as a Gaussian distribution with the mode of 
 as a
mean parameter, which can be obtained numerically or analytically for some
specific distributions. Let 
 be the mode (MAP value, as discussed in
Chapter 4) of 
, that is:
(6.2)
Here we use Eq. (6.1). Since the mode 
 is the minimum value in 
, 
 also satisfies the following equation:

(6.3)
where 
 is the gradient operator with respect to . Based on the property, a
Taylor expansion around 
 approximates 
 as follows:
(6.4)
where the second term in the first line is canceled by using Eq. (6.3). Equation
(6.4) is a basic equation in this chapter. H is a 
 Hessian matrix defined
as
(6.5)
By using Eq. (6.1), it can also be represented as
(6.6)
The Hessian matrix also appeared in Eq. (5.9), as the second derivative of the
negative log posterior. The determinant of this matrix plays an important role in
the Occam factor, which penalizes the model complexity. Note that the Hessian
matrix is a symmetric matrix based on the following property:
(6.7)
Thus, by substituting the Taylor expansion of 
 (Eq. (6.4)) into Eq. (6.1),
we obtain the approximated form of 
, as follows:

(6.8)
Here, 
 does not depend on θ, and we can neglect it. Thus, 
 is
approximated by the following Gaussian distribution with mean vector 
and covariance matrix 
:
(6.9)
Note that since the dimensionality of the Hessian matrix H is the number of
parameters J, and it is often large in our practical problems, we would have a
numerical issue as to how to obtain 
. To summarize the Laplace
approximation, if we have an arbitrary distribution 
 with given
continuous distribution 
, the Laplace approximation provides an analytical
Gaussian distribution by using the mode 
 and the Hessian matrix H. 
is usually obtained by some numerical computation or the MAP estimation.
Now, let us consider the relationship between MAP (Chapter 4) and Laplace
approximations. As we discussed in Eq. (4.9), the MAP approximation of the
posterior distribution is represented by a Dirac delta function as follows:
(6.10)
Therefore, Eq. (6.9) approaches the MAP solution, when the variance in the
Gaussian distribution in Eq. (6.9) becomes very small. This often arises when
the amount of training data is very large. In this sense, the Laplace
approximation is a more precise approximation for the true posterior distribution
than MAP, in terms of the asymptotic approximation. This precision of the
Laplace approximation comes from consideration of the covariance matrix
(Hessian matrix H) effect in Eq. (6.9).

The Laplace approximation is widely used as an approximated Bayesian
inference method for various topics (e.g., Bayesian logistic regression
(Spiegelhalter & Lauritzen 1990, Genkin, Lewis & Madigan 2007), Gaussian
processes (Rasmussen & Williams 2006), and Bayesian neural networks
(MacKay 1992c)). Sections 6.3 and 6.4 discuss the applications of the Laplace
approximation to Bayesian predictive classification and neural networks in
acoustic modeling, respectively.
Note that the Laplace approximation is usually used to obtain the posterior
distribution for non-exponential family distributions. Although the Laplace
approximation can be used to deal with latent variable problems in the posterior
distribution (e.g., Gaussian mixture model), the assumption of approximating a
multiple peak distribution with a single peak Gaussian is not adequate in many
cases. Therefore, the Laplace approximation is often used with the other
approximations based on the EM algorithm (MAP–EM, VB–EM) or sampling
algorithm (MCMC) that handle latent variable problems.
6.2   Bayesian information criterion
This section describes the Bayesian information criterion in general. We focus
on the posterior distribution of model 
, which is introduced in
Section 3.8.7. We first think about a simple case when there is no latent variable.
Then, 
 is represented by using the Bayes rule and the sum rule as
follows:
(6.11)
where 
 denotes a prior distribution for a model M. Similarly to the
formalization for Laplace approximation, we use a vector form of model
parameters, that is 
, and J is the number of parameters. Then, 
denotes a prior distribution for a model parameter  given M, and 
denotes a likelihood function given  and M.

Suppose we want to compare two types of models (
 and 
) in terms of
the posterior values of these models. Figure 6.1 compares fitting the same data
with a single Gaussian (
) or two Gaussians (
). For this comparison, we
can compute the following ratio of 
 and 
 and check whether
the ratio is larger/less than 1:
(6.12)
This factor is called the Bayes factor (Kass & Raftery 1993, 1995). To obtain the
Bayes factor, we need to compute the marginal likelihood term:
(6.13)
Since it is difficult to solve this integral analytically, the following section
approximates this calculation based on the asymptotic approximation, similarly
to the Laplace approximation, as discussed in Section 6.1.
Figure 6.1 An example of model comparison, fitting the same data with a
single Gaussian (
) or two Gaussians (
). By comparing the posterior
probabilities of 
 and 
, we can select a more probable model.
We first define the following continuous function g over :
(6.14)

This is slightly different from the definition of function f in Eq. (6.1), as
Eq. (6.1) focuses on the posterior distribution of the parameters. Then, similarly
to the previous section, we define the following MAP value:
(6.15)
By using the Taylor expansion result in Eq. (6.4) with 
, Eq. (6.13) can be
represented as follows:
(6.16)
where J is the number of dimensions of , that is the number of parameters. The
Hessian matrix H is defined with 
 as
(6.17)
Or it is defined with 
 and 
 as
(6.18)
The integral in Eq. (6.16) can be solved by using the following normalization
property of a Gaussian distribution:

(6.19)
Thus, from Eq. (6.16), the marginal likelihood is computed by using the
determinant of the Hessian matrix 
.
Here, we focus on the determinant of the Hessian matrix 
. From the
definition in Eq. (6.18), the Hessian matrix can be represented as:
(6.20)
Thus, the Hessian matrix is decomposed to the logarithmic likelihood and prior
term, and we neglect the prior term that does not depend on the amount of data.
If we assume that the observed data are independently and identically distributed
(iid), the likelihood term of T frame feature vectors is approximated when T is
very large as1
(6.21)
where I is a Fisher information matrix obtained from a single observation, which
does not depend on the amount of data. Therefore, we can approximate the
determinant of the Hessian matrix as:
(6.22)
This approximation implies that the Hessian matrix approaches zero when the
amount of data is large. That is

(6.23)
Therefore, the Laplace approximation can be equivalent to the MAP
approximation when the amount of data is large, which is a reasonable outcome.
In the BIC approximation, by substituting Eq. (6.22) into Eq. (6.16), we
obtain
(6.24)
Further, by neglecting the terms that do not depend on T, we finally obtain the
following simple equation:
(6.25)
Here, 
 is often approximated by the ML estimate 
 instead of 
, since 
 approaches 
 when 
, as discussed in Section 4.3.7. Model
selection based on this equation is said to be based on the Bayesian information
criterion (BIC), or Schwartz information criterion, which approximates the
logarithmic function of the posterior distribution of a model 
. The first
term on the right-hand-side is a log-likelihood term with the MAP estimate (it is
often approximated by the ML estimate) of model parameter . It is well known
that this log-likelihood value is always increased when the number of parameters
increases. Therefore, the log-likelihood value from the maximum likelihood
criterion cannot be used for model selection since it will always prefer the model
that has the largest number of parameters. On the other hand, Eq. (6.25), which
is based on the Bayesian criterion, has an additional term, which is proportional
to the number of model parameters, denoted by J, and the logarithmic number of

observation frames, denoted by 
. This term provides the penalty so that the
total value in Eq. (6.25) is penalized not to select a model with too many
parameters.
Comparing Eq. (6.25) with the regularized form of the objective function in
Eq. (4.3) in the MAP estimation, Eq. (6.25) can be viewed as having an 
regularization term:
(6.26)
Therefore, the BIC objective function can also be discussed within a
regularization perspective. Another well-known model selection criterion, the
Akaike information criterion (AIC) (Akaike 1974), is similarly represented by
(6.27)
It also has the 
 regularization term, but it does not depend on the amount of
data T.
Using the BIC, we can simplify the model comparison based on the Bayes
factor. That is, by substituting Eq. (6.25) into the logarithm of the Bayes factor
(Eq. (6.12)) we can also obtain the following equation:
(6.28)
Therefore, we can perform the model comparison by checking the sign of
Eq. (6.28). If the sign is positive, 
 is a more appropriate model in the BIC

sense.
However, since we usually cannot assume that a single Gaussian distribution
applies to a complicated model and/or that there are enough data to satisfy the
large sample approximation, the BIC assumption is not valid for our practical
problems. Therefore, in practical use, we introduce a tuning parameter  which
heuristically controls the balance of the log-likelihood and penalty terms, as
follows:
(6.29)
This actually works effectively for the model selection problems of HMMs and
GMMs, including speech processing (Chou & Reichl 1999, Shinoda &
Watanabe 2000) and image processing (Stenger, Ramesh, Paragios et al. 2001).
We discuss the applications of BIC to speech processing in Sections 6.5 and 6.6.
6.3   Bayesian predictive classification
This section addresses how the Laplace approximation is developed to build the
Bayesian predictive classification (BPC) approach which has been successfully
applied for robust speech recognition in noisy environments (Jiang et al. 1999,
Huo & Lee 2000, Lee & Huo 2000, Chien & Liao 2001). As mentioned in
Section 3.1, the Bayes decision theory for speech recognition can be realized as
the BPC rule where the expected loss function in Eq. (3.16) is calculated by
marginalizing 
over 
the 
continuous 
density 
HMM 
parameters 
, as discussed in Section 3.2.3, as well
as the n-gram parameters 
, as discussed in Section 3.6. Here, we
only focus on the acoustic models based on HMMs. The marginalization over all
possible parameter values aims to construct a decision rule which considers the
prior uncertainty of HMM parameters. The resulting speech recognition is
robust to mismatch conditions between training and test data. In general, such
marginalization could compensate some other ill-posed conditions due to the
uncertainties and variations from the observed data and the assumed models. In
this section, we first describe how a Bayesian decision rule is established for
robust speech recognition and how the uncertainties of model parameters are

characterized. Then, the approximate solution to the HMM-based decision rule is
formulated by using the Laplace approximation. We also present some other
extensions of the BPC decision rule.
6.3.1
 Robust decision rule
In automatic speech recognition, we usually apply the plug-in maximum a-
posteriori (MAP) decision rule, as given in Eq. (3.2), and combine the
approximate acoustic model 
 and language model 
 for finding
the most likely word sequence W corresponding to an input speech sentence 
. The maximum likelihood (ML) parameters 
, as described in
Section 3.4, and the MAP parameters 
, as formulated in Section 4.3, are
treated as the point estimates and plugged into the MAP decision for speech
recognition. However, in practical situations, speech recognition systems are
occasionally applied in noisy environments. The training data may be collected
in ill-posed conditions, where the HMM parameters may be over-trained with
limited training data conditions, and can lose the generalization capability for
future data conditions. Thus, the assumed model may not properly represent the
real-world speech data. For this situation, the plug-in MAP decision, relying only
on the single best HMM parameter values 
 or 
, is risky for speech
recognition. Because of these considerations, we are motivated to establish a
robust decision rule which accommodates the uncertainties of the assumed
model and the collected data so that the error risks in speech recognition can be
reduced.
In the HMM framework (Section 3.2), the prior uncertainty of GMM
parameters 
 in an HMM state j should be
characterized from training data and then applied for a BPC decision of future
data. In this section, we use the following notation to clearly distinguish training
and future data:
(6.30)
Let the distribution 
 of a future observation frame 
 under an HMM
state j be distorted in an admissible set 
 with a distortion level 
 which

is denoted by
(6.31)
where 
 denotes the admission region of the GMM parameter space. In the
special case of no distortion (
), 
 is a singleton set which
consists of the ideal and non-distorted distribution of an HMM state j with the
parameters 
 estimated from a training set 
. However, in real-world
applications, there exist many kinds of distortions (
) between the trained
models and the test observations. These distortions and variations should be
modeled and incorporated in the Bayes decision to achieve robustness of speech
recognition in adverse environments.
The Bayesian inference approach provides a good way to formalize this
parameter uncertainty problem and formulate the solution to the robust decision
rule. To do so, we intend to consider the uncertainty of the HMM parameters Θ
to be random. An a-priori distribution 
 could serve as the prior
knowledge about Θ, where 
 are located in a region of interest of the
HMM parameter space Ω, and Ψ denotes the corresponding hyperparameters.
Such prior information may come from subject considerations and/or from
previous experience. Therefore, the BPC decision rule is established as
(6.32)

where
(6.33)
Here, 
 denotes the posterior distribution of HMM parameters Θ given
the training utterances 
, which is written as
(6.34)
In Eq. (6.33), the marginalization is performed over all possible HMM parameter
values Θ. This marginalized distribution is also called the predictive distribution.
Model uncertainties are considered in the BPC decision rule. The optimum BPC
decision rule was illustrated in Nadas (1985).
The crucial difference between the plug-in MAP decision and the BPC
decision is that the former acts as if the estimated HMM parameters were the
true ones, whereas the latter averages over the uncertainty of parameters. In an
extreme case, if 
 with 
 denoting the Dirac delta
function with the ML or MAP estimate, the BPC decision rule coincides with the
plug-in MAP decision rule based on ML HMM parameters 
, i.e., the
predictive distribution can be approximated as:
(6.35)
Therefore, the plug-in MAP decision rule with the ML/MAP estimate
corresponds to an approximation of the BPC decision rule.

Using the continuous density HMMs, the missing data problem happens so
that the predictive distribution is calculated by considering all sequences of
HMM states 
 and mixture components 
:
(6.36)
where the posterior distribution 
 given training data 
 is replaced by
the prior distribution 
 given hyperparameters Ψ in accordance with
an empirical Bayes theory. This predictive distribution is seen as an integration
of likelihood function 
 over all possible values of 
.
However, the computation of predictive distribution over all state sequences S
and mixture component sequences V is complicated and expensive. A popular
way to deal with this computation is to employ the Viterbi approximation, as we
discussed in Section 3.3.2, to compute the approximate predictive distribution
(Jiang et al. 1999):
(6.37)
Therefore, 
the 
approximated 
predictive 
distribution 
only 
considers
marginalization over the model parameter Θ. The following sections describe
how to deal with model parameter marginalization.
6.3.2
 Laplace approximation for BPC decision
In this section, Laplace approximation is adopted to approximate the integral in

calculation of the predictive distribution 
 in Eq. (6.36) or in
Eq. (6.37), given state and mixture component sequences S and V. Let us define
the continuous function 
, similarly to Eqs. (6.1) and (6.14):
(6.38)
The value of Θ that minimizes 
 is obtained as a mode or an MAP estimate
of HMM parameters,
(6.39)
Note that we have a solution to the above optimization based on the MAP
estimation, as we discussed in Section 4.3. Then since we have the equation 
 from Eq. (6.39), a second-order Taylor expansion around 
 can be used to approximate 
 as follows:
(6.40)
where H denotes a Hessian matrix defined as
(6.41)
We should note that we use a vector representation of Θ (i.e., 
 and M is
the number of all CDHMM parameters) by arranging all CDHMM parameters (
) to form a huge vector, to make the above gradient
well defined.2
Therefore, we obtain the approximate predictive distribution, which is
derived from

(6.42)
Similarly to Eq. (6.18), the predictive distribution in Eq. (6.42) is approximately
obtained through arranging a multivariate Gaussian distribution in the integrand
of Eq. (6.42) as follows:
(6.43)
As a result, the BPC decision based on Laplace approximation is implemented
by Eq. (6.43). The logarithm of predictive distribution, 
, is seen
as follows:
(6.44)
The likelihood of the first term can be obtained by using the MAP–EM (or
Viterbi approximation), as we discussed in Section 4.3. The second term is based
on the logarithm of the determinant of the Hessian matrix. From Eq. (6.9), the
Hessian matrix can be interpreted as the precision (inverse covariance) matrix of
the model parameters (
. Therefore, if the model
parameters are uncertain, the determinant of the covariance matrix of Θ becomes
large (i.e., the determinant of the Hessian matrix H becomes small), the log
likelihood of the predictive distribution is decreased. Thus, considering model
uncertainty in the BPC decision is meaningfully reflected in Eq. (6.43).
6.3.3
 BPC decision considering uncertainty of HMM means
To simplify the discussion, we only consider the uncertainty of the mean vectors
in HMM parameters for BPC decoding, as the mean vectors are the most

dominant parameters for the ASR performance. In addition, by only focusing on
the mean vectors, we can avoid the difficulty of applying the Laplace
approximation to the covariance and weight parameters, which have constraints,
assuming the HMM means of state j3 and mixture component k in dimension d
are independently generated from Gaussian prior distributions with the Hessian
matrix as the precision matrix.
The joint prior distribution 
 is expressed by
(6.45)
with a collection of hyperparameters including Gaussian mean vectors 
and diagonal covariance matrices 
 for different states and
mixture components. The GMM mixture weights 
 and diagonal covariance
matrices 
 of the original CDHMM are assumed
to be deterministic without uncertainty. Note that the above prior distribution is
conditional on the word sequence W, as it is required in the BPC decision rule in
Eq. (6.36). This condition means that we only deal with the prior distributions of
HMM parameters appearing in the state sequences obtained by hypothesized
word sequence W. But this condition can be disregarded since this condition for
the prior distribution does not matter in the following analysis.
Given an unknown test utterance 
 and the
unobserved state sequence 
 and mixture component
sequence 
 obtained by hypothesized word sequence W,
we combine the Gaussian likelihood function 
 and the Gaussian
prior distribution 
 based on the MAP–EM approach
(Section 4.2) to give:
(6.46)

Equation (6.46) is further rewritten to come up with an approximate posterior
distribution which is arranged as a Gaussian distribution:
(6.47)

with the hyperparameters derived as
(6.48)
(6.49)

where
(6.50)
(6.51)
(6.52)
The posterior distribution in Eq. (6.46) is obtained because we adopt the
conjugate prior to model the uncertainty of HMM means. Recall the MAP
estimation result of the mean vector in Eq. (6.53):
(6.53)
Comparing Eqs. (6.53) and (6.48), it is apparent that Eq. (6.48) based on the
BPC solution considers the effect of 
.
An EM algorithm is iteratively applied to approximate the log posterior
distribution of new estimate 
, 
, by using the posterior auxiliary
function of new estimate 
 given the current estimate 
, 
.
The calculation converges after several EM iterations. Given the updated
occupation 
probability, 
the 
Gaussian 
posterior 
distribution 
of 
, which concatenates all Gaussian mean vectors, is
calculated and used to find the approximation in Eq. (6.9):
(6.54)

Therefore, we substitute
(6.55)
(6.56)
into Eq. (6.43). This approximation avoids a need to directly compute the
Hessian matrix, where the number of dimensions of the Hessian matrix is very
large 
(
). 
The 
BPC-based 
recognition 
is 
accordingly
implemented for robust speech recognition.
In Chien & Liao (2001), the BPC decision was extended to a transformation-
based BPC decision where the uncertainties of transformation parameters were
taken into account. The transformation parameters of HMM means and
variances were characterized by the Gaussian–Wishart distribution. Using this
decision, HMM parameters were treated as deterministic values. In
Chien (2003), the transformation-based BPC was further combined with the
maximum likelihood linear regression adaptation (Leggetter & Woodland 1995).
The linear regression based BPC was proposed for noisy speech recognition. The
transformation regression parameters were represented by the multivariate
Gaussian distribution. The predictive distributions in these two methods were
approximated by calculating the predictive distributions for individual frames 
without involving the Laplace approximation.
6.4   Neural network acoustic modeling
Acoustic modeling based on the deep neural network (DNN) is now a new trend
towards achieving high performance in automatic speech recognition (Dahl, Yu,
Deng et al. 2012, Hinton et al. 2012). The context-dependent DNNs were
combined with HMMs and have recently shown significant improvements over
discriminatively trained HMMs with state-dependent GMMs (Seide, Li, Chen
et al. 2011). In particular, the DNN was proposed to conduct a greedy and layer-
wise pretraining of the weight parameters with either a supervised or
unsupervised criterion (Hinton, Osindero & Teh 2006, Salakhutdinov 2009).
This pretraining step prevents the supervised training of the network from being

trapped in a poor local optimum. In general, there are five to seven hidden layers
with thousands of sigmoid non-linear neurons in a DNN model. The output layer
consists of softmax non-linear neurons. In addition to the pretraining scheme, the
success of the DNN acoustic model also comes from the tandem processing,
frame randomization (Seide et al. 2011) and (Hessian-free) sequence training
(Kingsbury, Sainath & Soltau 2012, Veselỳ, Ghoshal, Burget et al. 2013).
Without loss of generality, we address the artificial neural network (NN) acoustic
model for ASR, based on the feed-forward multilayer perceptron with a single
hidden layer, as illustrated in Figure 6.2. We bring in the issue of model
regularization in construction of NNs and present the Bayesian neural networks
(MacKay 1992c, Bishop 2006) for robust speech recognition. This framework
can obviously be extended to DNN-based speech recognition with many hidden
layers. Importantly, we introduce a prior distribution to express the uncertainty
of synaptic weights in Bayesian NNs. This uncertainty serves as a penalty factor
to avoid too-complicated or over-trained models. The variations of acoustic
models due to the heterogeneous training data could be compensated through
Bayesian treatment. The Laplace approximation is applied to derive the
predictive distribution for robust classification of a speech signal. In what
follows, we first describe the combination of Bayesian NNs and HMMs for the
application of speech recognition. MAP estimation is applied to find a solution
to adaptive NN training. A Laplace approximation is developed to construct
Bayesian NNs for HMM-based speech recognition.
Figure 6.2 A neural network based on the multilayer perceptron with a single
hidden layer. 
, 
, and 
 denote the input vector, hidden
unit vector, and output vector at frame t, respectively. 
 and 
 are

introduced to consider the bias terms in the transformation, and these are
usually set with some fixed values (e.g., 
). 
 and 
 denote the weight vectors (to use a Laplace approximation, we
represent these values as the vector representation rather than the matrix
representation) from the input to hidden layers and from the hidden to output
layers, respectively.
6.4.1
 Neural network modeling and learning
A standard neural network for a K-class classification problem is depicted in
Figure 6.2, which is established as a feed-forward network function with a layer
structure. This function maps the D input neurons 
 to K output
neurons 
 where
(6.57)
is given by a softmax non-linear function, or the normalized exponential
function which satisfies 
 and 
. In Eq. (6.57), 
 is
calculated as the output unit activation given by
(6.58)

where
(6.59)
denotes the logistic sigmoid function and
(6.60)
denotes the synaptic weights of the first and the second layers, respectively. For
model training, the target values of output neurons are assigned as Bernoulli
variables where value 1 in the kth neuron means that 
 belongs to class k and
the other neurons have target value 0 . In this figure, 
 and the
corresponding weights 
 denote the bias parameters in neurons. This model
is also known as the multilayer perceptron. Such a neural network model
performs well for pattern recognition for two reasons. One is the nested non-
linear function, which can learn the complicated deep structure from real-world
data through feed-forwarding the input signals to the output classes layer by
layer. The second reason is that it adopts the softmax function or the logistic
sigmoid function 
 as the neuron processing unit, which is beneficial
for building high-performance discriminative models. In general, deep neural
networks are affordable for learning the invariance information and extracting
the hierarchy of concepts or features from data. Higher-level concepts are
defined from lower-level ones. The logistic sigmoid function and softmax
function play an important role for binary classification and multi-class
classification, respectively. These functions are essential for calculating posterior
probability and establishing the discriminative model. Basically, we collect a set
of training samples and their target values 
. The synaptic weights are
estimated by calculating an error function 
 and minimizing it through the
gradient descent algorithm,
(6.61)

where η denotes the learning rate and τ denotes the iteration index. The sum-of-
squares error function is calculated for the regression problem while the cross
entropy error function is applied for the classification problem. The error back-
propagation algorithm has been proposed to find gradient vector 
 and
widely applied for training of individual synaptic weights 
 in different
layers.
6.4.2
 Bayesian neural networks and hidden Markov models
For the application of speech recognition, the NNs should be combined with the
HMMs (denoted by NN–HMMs) to deal with sequential training of synaptic
weights 
w 
from 
a 
sequence 
of 
speech 
feature 
vectors 
. As discussed before, this feature can be obtained
by stacking the neighboring MFCC or filter bank features. For example, if the
original features are represented by 
, the stacked
feature for the NN input is represented by
(6.62)
Then, the number of feature dimensions becomes 
. L is set
between five and seven depending on the task. This expanded feature can model
the short-range dynamics of speech features, as we model in the conventional
GMM framework by using the delta cepstrum or linear discriminant analysis
techniques (Furui 1986, Haeb-Umbach & Ney 1992).
However, the stacking technique cannot fully model the long-range speech
dynamics. The Markov states are used to characterize the temporal correlation in
sequential patterns. We estimate the NN–HMM parameters 
,
consisting of initial state probabilities 
, state transition probabilities A and the
N-dimensional synaptic weights w, by maximizing the likelihood function 
. Each output neuron calculates the posterior probability of a context
dependent HMM state k given a feature vector 
 using synaptic weights w, i.e.,
(6.63)

ML estimation of Θ has an incomplete data problem and should be solved
according to the EM algorithm. The EM algorithm is an iterative procedure
where the E-step is to calculate an auxiliary function of training utterances O
using new NN–HMM parameters 
 given parameters Θ at the current iteration:
(6.64)
where 
 denotes the state occupation probability. NN–
HMM parameters 
 at a new iteration are then estimated by M-step:
(6.65)
Here, we only consider the estimation of the synaptic weights w by minimizing
the corresponding negative auxiliary function:
(6.66)
To implement the ML supervised training of NN–HMM parameters w, we
first perform Viterbi decoding of training speech 
 by
using true transcription and NN–HMM parameters Θ or w at the current
iteration. Each frame 
 is assigned an associated HMM state or output neuron 
. This Viterbi alignment is equivalent to assigning a label or target value 
 of a speech frame 
 at an output neuron k of either 0 or 1. That is, 
 is treated as a Bernoulli target variable. The negative auxiliary

function in ML estimation is accordingly viewed as the cross entropy error
function between the Bernoulli target values based on current estimate w and the
posterior distributions from the NN outputs using new estimate 
. By
minimizing the cross entropy between the desired outputs 
 and the actual
outputs 
 over all time frames 
 and all context-dependent
HMM states 
, we establish the discriminative acoustic models (NN–
HMMs) for speech recognition. The ML NN–HMM parameters 
are estimated. The error back-propagation algorithm can be implemented to train
the synaptic weights 
 by minimizing 
, as given in Eq. (6.66).
More importantly, we address the maximum a-posteriori (MAP) estimation
of NN–HMM parameters where the uncertainty of synaptic weights is
compensated by using a prior distribution. For simplicity, we assume that the
continuous values of weights in different layers come from a multivariate
Gaussian distribution,
(6.67)
where 
 denotes the mean vector and 
 denotes the covariance matrix. This
prior information could be empirically inferred from training data. There are two
Bayesian inference stages in the so-called Bayesian NN–HMMs. The first stage
is to find the point estimate of synaptic weights according to the MAP
estimation. This idea is similar to MAP estimation of HMM parameters
(Gauvain & Lee 1994). MAP estimates of NN–HMM parameters are calculated
for adaptive training or speaker adaptation. The second stage is to conduct the
distribution estimation, and this stage tries to fulfil a full Bayesian analysis by
calculating the marginal likelihood with respect to all values of synaptic weights.
Robustness of speech recognition to variations of synaptic weights due to
heterogeneous data is assured.
In the first inference stage, MAP estimates of model parameters 
 are calculated by maximizing a-posteriori probability or the
product of likelihood function and prior distribution. An EM algorithm is applied
to find MAP estimates by maximizing the posterior auxiliary function 
. A MAP estimate of synaptic weights 
 is derived by
minimizing the corresponding auxiliary function:

(6.68)
which is viewed as a kind of penalized cross entropy error function. The prior
distribution provides subjective information for Bayesian learning, or
equivalently, serves as a penalty function for model training. 
 plays the role
of a metric matrix of the 
 distance. Again, the error back-propagation
algorithm is applied to estimate the synaptic weights 
 at a new iteration. Using
this algorithm, the derivative of penalty function with respect to 
 contributes
the estimation of 
. The estimated 
 is treated as a point estimate or
deterministic value for prediction or classification of future data.
6.4.3
 Laplace approximation for Bayesian neural networks
In the second Bayesian inference stage, the synaptic weights w are treated as a
latent random variable. The hyperparameters 
 are used to
represent the uncertainty of random parameters w. Such uncertainty information
plays an important role in subjective inference, adaptive learning, and robust
classification. Similarly to the BPC based on the standard HMMs as addressed in
Section 6.3, we would like to present the BPC decision rule based on the
combined NN–HMMs. To do so, we need to calculate the predictive distribution
of test utterance O which is marginalized by considering all values of weight
parameters w in the likelihood function 
:
(6.69)
The integral is performed over the parameter space of w. However, due to the
non-linear feed-forward network function 
, the
closed-form solution to the integral calculation does not exist and should be

carried out by using the Laplace approximation. By applying the Laplace
approximation for integral operation in BIC and BPC decision, as given in Eqs.
(6.16) and (6.43), we develop the BPC decision based on the NN–HMMs
according to the approximate predictive distribution:
(6.70)
where the mode 
 of posterior distribution 
 is obtained
through the EM iteration procedure,
(6.71)
and the Hessian matrix H is also calculated according to the EM algorithm
through
(6.72)
which is the second-order differentiation of the log posterior distribution with
respect to NN–HMM parameters at the mode 
. Here, the auxiliary
function 
 is introduced because the NN–HMM framework involves
the missing data problem. As explained in Section 6.3.3, the EM algorithm is
applied to iteratively approximate the posterior distribution at its mode by
(6.73)
Accordingly, we construct the BPC decision based on NN–HMMs, which can
achieve robustness in automatic speech recognition.

6.5   Decision tree clustering
This section introduces the application of BIC-based model selection to decision
tree clustering of context-dependent HMMs, as performed in Shinoda &
Watanabe (1997) and Chou & Reichl (1999), without having to set a heuristic
stopping criterion, as is done in (Young, Odell & Woodland 1994).4 This section
first describes decision tree clustering based on the ML criterion, which
determines the tying structure of context-dependent HMMs efficiently. Then, the
approach is extended by using BIC.
6.5.1
 Decision tree clustering using ML criterion
The decision tree method has been widely used to construct a tied state HMM
effectively by utilizing the phonetic knowledge-based constraint and the binary
tree search (Odell 1995) approaches. Here, we introduce a conventional decision
tree method using the ML criterion.
Let 
 denote a set of states that tree node n holds. We start with only a
root node 
 which holds a set of all context-dependent phone (e.g.,
triphone hereinafter, as shown in Figure 6.5) HMM states 
 for an identical
center phoneme. The set of triphone states is then split into two sets depending
on question Q, 
 and 
, which are held by two new nodes, 
 and 
,
respectively, as shown in Figure 6.3. The partition is determined by an answer to
a phonemic question Q, such as “is the preceding phoneme a vowel” and “is the
following phoneme a nasal.” A particular question is chosen so that the partition
is the optimal of all the possibilities, based on the likelihood value. We continue
this splitting successively for every new set of states to obtain a binary tree, as
shown in Figure 6.4, where each leaf node holds a shared set of triphone states.
The states belonging to the same cluster are merged into a single HMM state. A
set of triphones is thus represented by a set of tied-state HMMs. An HMM in an
acoustic model usually has a left-to-right topology with three or four temporal
states. A decision tree is usually produced for each state in the sequence, and the
trees are independent of each other, or a single decision tree is produced for all
states.

Figure 6.3 Splitting a set of HMM states 
 in node n into two sets of states
 and 
 according to question Q. The objective function is changed
from 
 to 
 after the split.
Figure 6.4 Binary decision tree. A context-dependent HMM state is
represented by a single Gaussian distribution, and a set of states is assigned to
each node. Two child nodes are obtained based on a yes/no answer to a
(phonemic) question.

Figure 6.5 Examples of context-dependent (triphone) HMM and sharing
structure. We prepare a three-state left-to-right HMM for each triphone, where
a-k -a denotes a jth state in a triphone HMM with preceding phoneme a,
central phoneme k, and following phoneme a. In this example, triphone HMM
states a-k -a, a-k -i, and a-/s/ -i are shared with the same Gaussian
distribution.
The phonetic question concerns the preceding and following phoneme
context, and is obtained through knowledge of the phonetics (Odell 1995) or it is
also decided in a data-driven manner (Povey, Ghoshal, Boulianne et al. 2011).
Table 6.1 shows examples of the question. In a conventional ML-based
approach, an appropriate question is obtained by maximizing a likelihood value
as follows:

(6.74)
where 
 denotes the gain of log-likelihood when a state set in a node is split
by a question Q. To calculate 
, we assume the following constraints:
Data alignments 
 and 
 for each state are fixed while splitting.
Emission probability distribution in a state is represented by a single
Gaussian distribution (i.e., 
).
Covariance matrices have only diagonal elements.
A contribution of state transitions 
 and initial weights 
 for likelihood is
disregarded.
These constraints simplify the likelihood calculation without using an iterative
calculation, which greatly reduces the computational time.
Table 6.1 Examples of phonetic questions for a 
th HMM state of a
phoneme a.
Question
Yes
No
Preceding phoneme
{a, i, u, e, o} -a
 - { all }
otherwise
is vowel?
Following phoneme
{ all } -a
 - {a, i, u, e, o}
otherwise
is vowel?
Following phoneme
{ all } -a
 - {/b/, d, g}
otherwise
is media ?
Preceding phoneme
{u, o} -a
 - { all }
otherwise
is back vowel?
Equation (6.74) corresponds to an approximation of the Bayes factor, as
discussed in Eq. (6.12) to compare two models: 
 models a set of triphones in
the node n with a single Gaussian; and 
 models two sets of triphones in the
nodes  and 
 separated by a question Q:

(6.75)
We obtain the gain of log-likelihood 
 in Eq. (6.74) under the above
constraints. Let 
 be a set of feature vectors that
are assigned to HMM state i by the Viterbi algorithm. 
 denotes the frame
number of training data assigned to state i, and D denotes the number of feature
vector dimensions. From the constraints, log-likelihood 
 for a training data
set, assigned to state set Ω, is expressed by the following D dimensional
Gaussian distribution:
(6.76)
where 
 and 
 denote a D dimensional mean vector and a 
diagonal covariance matrix for a data set in Ω, respectively. ML estimates 
and 
 are obtained by using the derivative technique. First, Eq. (6.76) is
rewritten as
(6.77)
where 
 is a d-d element of the diagonal covariance matrix 
. Then, by
using the following derivative with respect to 
,

(6.78)
we can obtain the ML estimate of the mean vector 
 as:
(6.79)
Similarly, by using the following derivative with respect to 
,
(6.80)
we can obtain the ML estimate of the diagonal component of the covariance
matrix 
 as:
(6.81)
We summarize the above ML estimates as:
(6.82)
 denotes the frame number of training data assigned to HMM
states belonging to Ω. 
 denotes a d-d component for matrix 
. By
substituting Eq. (6.82) into Eq. (6.76), we can derive the following log-
likelihood 
 with the ML estimates of 
:

(6.83)
Note that the likelihood value is represented by the determinant of the ML
estimate of the covariance matrix, which can easily be calculated as we use the
diagonal covariance.
Therefore, a gain of log-likelihood 
 can be solved as follows
(Odell 1995):
(6.84)
Here l in Eq. (6.84) is defined as:
where we use the following relation:
(6.85)
Equations (6.84) and (6.5.1) show that 
 can be calculated using the ML
estimate 
 and frame number 
. Finally, the appropriate question in the
sense of the ML criterion can be computed by substituting Eq. (6.84) into

Eq. (6.74).
However, we cannot use this likelihood criterion for stopping the split of
nodes in a tree. That is, the positivity of 
 for any split causes the ML
criterion to always select the model structure in which the number of states is the
largest. That is, no states are shared at all. To avoid this, the ML criterion
requires the following threshold to be set to stop splitting manually:
(6.86)
There exist other approaches to stopping splitting manually by setting the
number of total states, or the maximum depth of the tree, as well as a hybrid
approach combining those approaches. However, the effectiveness of the
thresholds in all of these manual approaches has to be judged by the
performance of the development set.
6.5.2
 Decision tree clustering using BIC
We consider automatic model selection based on the BIC criterion, which is
widely used in model selection for various aspects of statistical modeling. Recall
the 
following 
definition 
of 
a 
BIC-based 
objective 
function 
for 
 in Eq. (6.29):
(6.87)
where 
 is the ML or MAP estimate of model parameters for model M, J is the
number of model parameters, and 
 is a tuning parameter. Similarly to
Section 6.5.1, we use a single Gaussian for each node. Then, Eq. (6.87) for node
Ω can be rewritten as follows:
(6.88)

where 
 when we use the diagonal covariance matrix. Therefore, the
gain of objective function 
 using the BIC criterion (the logarithmic Bayes
factor) is obtained while splitting a state set by question Q, as follows:
(6.89)
where  is a tuning parameter in the BIC criterion, and 
 denotes the frame
number of data assigned to a root node.5 Equation (6.89) suggests that the BIC
objective function penalizes the gain in log-likelihood on the basis of the balance
between the number of free parameters and the amount of training data, and the
penalty can be controlled by varying 
. Model structure selection is achieved
according to the amount of training data by using 
 instead of using 
 in
Eq. (6.74), that is
(6.91)
We can also use 
 for stopping splitting when this value is negative, without
using a threshold in Eq. (6.86), that is
(6.92)
Thus, we can obtain the tied-state HMM by using the BIC model selection
criterion.6
Note that the BIC criterion is an asymptotic criterion that is theoretically
effective only when the amount of training data is sufficiently large. Therefore,
in the case of a small amount of training data, model selection does not perform
well because of the uncertain ML estimates 
 and 
. Shinoda &
Watanabe (2000) show the effectiveness of the BIC criterion for decision tree
clustering in a 5000 Japanese word recognition task by comparing the
performance of acoustic models based on BIC with models based on heuristic
stopping criteria (namely, the state occupancy count and the likelihood
threshold). BIC selected 2069 triphone HMM states automatically with an

80.4 % recognition rate, while heuristic stopping criteria selected 1248 and 591
states with recognition rates of 77.9 % and 66.6 % in the best and worst cases,
respectively. This result clearly shows the effectiveness of model selection using
BIC. An extension of the BIC objective function by considering a tree structure
is also discussed in (Hu & Zhao 2007), and an extension based on variational
Bayes is discussed in Section 7.3.6. In addition, BIC is used for Gaussian
pruning in acoustic models (Shinoda & Iso 2001), and speaker segmentation
(Chen & Gopinath 1999), which is discussed in the next section.
6.6   Speaker clustering/segmentation
BIC-based speaker segmentation is a particularly important technique for
speaker diarization, which has been widely studied recently (Chen &
Gopinath 1999, Anguera Miro, Bozonnet, Evans et al. 2012). The approach is
usually performed in two steps; the first step segments a long sequence of speech
to several chunks by using a model selection approach. Then the second step
clusters these chunks to form speaker segments, which can also be performed by
a model selection approach. Both steps use BIC as model selection. This section
considers the first step of speaker segmentation based on the BIC criterion.
6.6.1
 Speaker segmentation
First, we consider the simple segmentation problems of segmenting a speech
sequence to two chunks where these chunks are uttered by two speakers,
respectively, as shown in Figure 6.6. That is:
(6.93)
where t is a change point from one speaker to the other speakers or noises.
Assuming that each speaker utterance is modeled by a single Gaussian, which is
a very simple assumption but works well in a practical use, this problem can be
regarded as a model selection problem where we have a set of candidates
represented by:
;
, 
 

for 
.
The model 
 means that the sequence can be represented by one Gaussian
with mean vector 
 and the diagonal covariance matrix 
, that is, the sequence
only has one speaker utterance. Similarly, the model 
 for 
means that the sequence has two speakers with a change point t with two
Gaussians that have mean vectors 
 and 
, and diagonal covariance
matrices 
 and 
, depending on the change point t.
Figure 6.6 Speaker segmentation for audio data, which find a speaker
boundary t given speech features 
. The BIC segmentation
method assumes that each segmentation is represented by a Gaussian
distribution.
Based on these hypothesized models, we can also consider the Bayes factor
in Eq. (6.12) to compare two models of 
 and 
:
(6.94)
Similarly to the decision tree clustering, we can select an appropriate model
from 
 and 
 by using BIC. Recall the following definition of a BIC-
based objective function of model M for 
 in Eq. (6.29):

(6.95)
where 
 is the ML or MAP estimate of model parameters for model M, J is the
number of model parameters, and 
 is a tuning parameter. Since we use a
Gaussian (with diagonal covariance matrix) for the likelihood function in
Eq. (6.95), the BIC function for 
 is written as follows:
(6.96)
where 
 is the ML estimate of the diagonal covariance matrix:
(6.97)
This result is obtained by assuming that we only have one state in Eq. (6.83).
Similarly, 
 is written as follows:
(6.98)

where 
 and 
 are the ML estimates of the diagonal covariance matrices, and
are obtained as follows:
(6.99)
Again, this result is obtained by assuming that we only have two separated states
(
 and 
) in Eq. (6.83).
Therefore, the difference of BIC values between 
 and 
 is represented
as follows:
(6.100)
This is the objective function of finding the segmentation boundary based on the
BIC criterion. If the 
 is positive, t is a segmentation boundary, and the
optimal boundary  is obtained by maximizing 
 as follows:
(6.101)
Thus, we can find the optimal boundary of two speech segments by using BIC. If
we consider multiple changing points based on BIC, we can use Algorithm 9.
The approach has also been extended to deal with prior distributions (Watanabe
& Nakamura 2009).

Algorithm 9 Detecting multiple changing points
Require:  interval 
 
1:  while 
 do 
2:     Detect if there is one changing point in 
 via BIC 
3:     if no change in 
 then 
4:        
 
5:     else 
6:        
 
7:     end if 
8:  end while
6.6.2
 Speaker clustering
Once we have the speech segments obtained by the BIC-based speech
segmentation, we can again use the BIC criterion to cluster the speech segments,
where the cluster obtained can be interpreted as a specific speaker. This can be
obtained by similar techniques to the decision tree clustering of context-
dependent HMMs in Section 6.5.2. The main difference between them is that the
decision tree clustering is performed by splitting the nodes from the root node by
using the (phonetic) question, while the speaker clustering starts to merge the
leaf nodes, which are represented by a speech segment, successively to build a
tree.
Let i be a speech segment index obtained by speech segmentation
techniques, and we have in total J segments. Then, similarly to the previous
sections, we can compute the approximated Bayes factor based on the following
difference of the BIC value when merging the speech segments i and  as
(6.102)
where the BIC values of 
 and 
 are represented as:

(6.103)
where 
 and 
 are the number of frames assigned to the speech segment i and
the ML estimate of the covariance matrix of the speech segment i, respectively.
Similarly, the BIC value of 
, which merges two nodes i and , is represented
as
(6.104)
where 
, and 
 is the ML estimate of the covariance matrix when
we represent the speech segments i and  as a single Gaussian.
The most appropriate combination of merging two nodes can be obtained by
considering all possible i and  combinations and selecting the combination
(6.105)
And the selected  and  can be merged to a new node. This process is iteratively
performed while it satisfies 
. The final leaf nodes can represent a speaker
cluster. This approach is also called agglomerative clustering, and BIC based
agglomerative clustering is actually used to build state-of-the-art speaker
diarization systems (Wooters & Huijbregts 2008), which provide the “who spoke
when” information in speech data.
6.7   Summary
This chapter describes the asymptotic approximation of the Bayesian approach,
and introduces the Laplace approximation and the BIC criterion. Both
approaches are very powerful for approximating the Bayesian inference of
acoustic model parameters in speech recognition and Bayesian inference of
model selection problems in speech clustering and segmentation. However, the
asymptotic approximation makes the single Gaussian assumption for the prior
and posterior distributions and large sample assumptions for target data, which

limits the application for speech and language processing. The next chapter
provides another approximation technique, variational Bayes, which can handle
the problem in the asymptotic approximation.
1  This proof is not obvious. See Ghosh, Delampady & Samanta (2007) for more detailed derivations.
2  We should also note that most CDHMM parameters have some constraint (e.g., state transitions and
mixture weights have positivity and sum-to-one constraint) and the gradient of these parameters should
consider these constraints. However, the following example only considers mean vector parameters, and
we do not have this constraint problem.
3  Note that index j denotes all HMM states for all phonemes, unlike index j for a phoneme in Section 3.2.
Therefore, the number of HMM states J appearing later in this section also means the number of all
HMM states used in the acoustic models, which could be several thousand when we use a standard
LVCSR setup.
4  Acoustic model selections based on BIC and minimum description length (MDL) criteria have been
independently proposed, but they are practically the same if we deal with Gaussian distributions.
Therefore, they are identified in this book and referred to as BIC.
5  The following objective function
(6.90)
is derived in Shinoda & Watanabe (2000). There are several ways to set the penalty term from
Eq. (6.89).
6  The BIC criterion also has a tuning parameter in Eq. (6.87). However, the setting is more robust than
the threshold setting in Eq. (6.86) (Shinoda & Watanabe 2000).

7    Variational Bayes
Variational Bayes (VB) was developed in the machine learning community in
the 1990s (Attias 1999, Jordan, Ghahramani, Jaakkola et al. 1999) and has now
become a standard technique to approximated Bayesian inference for latent
models, based on the EM-like algorithm. In Chapter 4, we have also dealt with
latent models based on the maximum a-posteriori (MAP) EM algorithm.
However, the MAP approximation uses the point estimation of model parameters
instead of the distribution estimation, which is far from a true Bayesian manner
of regarding all the variables introduced in our problem as probabilistic random
variables. Another approximation based on the asymptotic approximation in
Chapter 6 assumes a complex posterior distribution as a single Gaussian
distribution without latent variables, which is not a true assumption for many of
our applications. The evidence approximation in Chapter 5 also does not
explicitly deal with latent models (can be obtained by combining MAP, VB, or
MCMC). Instead of considering the MAP, evidence, and asymptotic
approximations, VB can efficiently approximate complicated integrals and
expectations over model parameters, based on variational method within a
specific family of distribution types (exponential family, as discussed in
Section 2.1.3). The key idea of the variational technique is to find the lower
bound of the marginal log likelihood, similar to the EM algorithm in Section 3.4,
and obtain the posterior distributions directly based on the variational method.
This chapter starts to explain the general framework of VB in Section 7.1,
and more specific pattern recognition problems in Section 7.2. Then this chapter
goes on to provide a VB version of the EM algorithm for statistical models and
model selection in speech and language processing, including speech recognition
in Sections 7.3 and 7.4 and speaker verification in Section 7.5. Sections 7.6 and
7.7 also deal with latent topic models and their extensions; these try to capture
long-range topic information from (spoken) documents, based on VB solutions.

7.1   Variational inference in general
This section starts by describing a general latent model with observation data 
, and the set of all variables introduced in our model
including latent variables, parameters, hyperparameters, and model structure Z.
The latter sections specify Z with more specific variables. The goal of Bayesian
inference is to obtain posterior distributions of any variables introduced in the
problem, that is:
(7.1)
As discussed in Section 2.1.2, once we obtain 
, we can estimate various
information by the MAP or expectation procedure. In VB, we consider an
arbitrary posterior distribution 
. We use the approximated posterior
distribution denoted by 
 to distinguish it from the true posterior distribution 
. Then the problem is how to obtain a 
 that is close to 
, so as to
obtain a well-approximated posterior distribution.
7.1.1
 Joint posterior distribution
As a measure of evaluating the difference between two distributions, we use the
Kullback–Leibler divergence (Kullback & Leibler 1951), as introduced in the
ML–EM algorithm (Section 3.4). The Kullback–Leibler divergence between 
 and 
 is defined as follows:
(7.2)
Z can be a set of discrete variables or a set of both continuous and discrete
variables, and, strictly speaking, we should use the summation sum
 for
discrete variables and 
 for continuous variables in such a case. However, for
simplicity we use 
 instead of mixing integrals and summations in the
following formulation.
The KL divergence (Eq. (7.2)) is represented as

(7.3)

where
(7.4)
is called the variational lower bound. The reason we call it the lower bound is
that 
 is a lower bound of the evidence (marginal log likelihood) 
 because of the non-negativity of the KL divergence, i.e.,
(7.5)
Equation (7.3) means that we can obtain the optimal 
 by maximizing the
variational lower bound 
 that corresponds to minimizing the KL
divergence since the log evidence 
 does not depend on Z. That is
(7.6)
To obtain the optimal 
, we use a variational method for a functional 
, which achieves mapping a function to a real (or complex) value, i.e.,
. Thus, the approach is called variational Bayes. The variational
method is discussed in Section 7.1.3.
7.1.2
 Factorized posterior distribution
In practical applications, we need to consider the factorized form of the joint
distribution 
 to make the calculation simple. To do this, we consider the
jth element of Z and make the following conditional independence assumption:
(7.7)

J is the total number of elements. This is an essential approximation requirement
of VB to make the problem practical. 
 would be a subset of model parameters
(e.g., Gaussian mean vector 
 and covariance matrix 
 in a particular
component of GMM/CDHMM), or an HMM state indicator 
 at frame t, for
instance. Note that we do not assume the factorization form for the true posterior
. Instead, the true posterior 
 can be represented as the following
marginalized distribution of 
 over all 
 except 
:
(7.8)
where 
 denotes the complementary set of 
.
By using Eq. (7.8), the KL divergence between 
 and 
 (not the
KL divergence between the joint distributions in Eq. (7.2)) is represented as
follows:
(7.9)
Since Eq. (7.9) has integrals in the logarithmic function, it is difficult to deal
with. Therefore, we use the following Jensen’s inequality for a concave function
f, distribution function 
 (
), and an arbitrary function 
introduced in Section 3.4.1:
(7.10)

In the special case of 
, 
, 
, and 
,
Eq. (7.10) can be rewritten as follows:
(7.11)
where we use Eq. (7.7) to cancel 
 in the fraction of the first line. We also
use the following relationship, which is true when 
:
(7.12)

In the special case of
(7.13)
Eq. (7.12) is represented as
(7.14)
where we use Eq. (7.7) to obtain 
, and use the definition of the variational
lower bound in Eq. (7.4). Therefore, by substituting Eq. (7.14) into the KL
divergence Eq. (7.9), Eq. (7.9) is finally represented as follows:
(7.15)
Compared with Eq. (7.3) that is the equality relationship, Eq. (7.15) is the
inequality relationship. Equation (7.15) still has the nice property that the
maximization of the variational lower bound corresponds to reducing the KL
divergence that then causes the approximated posterior 
 to approach the
true posterior 
. That is, if we obtain the following posterior distribution:

(7.16)
 could be a well-approximated posterior distribution in terms of reducing
the KL divergence between the true posterior 
 and approximated
posterior 
. In this section, a tilde 
 is added to indicate
variationally optimized values or functions. However, the maximization of the
posterior distribution in terms of the lower bound 
 does not directly
correspond to minimization of the KL divergence, and we cannot globally
optimize 
 in terms of the KL divergence when we use the lower bound as
the objective functional. This is a shortcoming of the factorization
approximation in Eq. (7.7), but it enables us to obtain the posterior distribution
of each variable 
, unlike the joint distribution 
, which is more
practical. The next section discusses how to optimize the approximated posterior
by using the variational method.
7.1.3
 Variational method
The variational method is based on functional differentiation, which is a
technique for obtaining an optimal function based on a variational calculation,
and is defined as follows:
Continuous function case
(7.17)
where 
 is a continuous function to be optimized, 
 is a functional of 
 and 
 is a Dirac delta function.
Discrete function case
(7.18)
Similarly, 
 is a discrete function to be optimized, and 
 is a Kronecker
delta function. This section aims to obtain the following optimized posterior

distribution based on the above variational method:
(7.19)
For simplicity of the calculation, we simplify 
 to 
 in this section.
If we consider 
 constraint, the functional differentiation is
represented by substituting 
 and 
 into 
 and 
 in Eq. (7.17),
respectively, as follows:
(7.20)
where K is a Lagrange multiplier, as introduced in Section 3.4.3 for the function
derivative. We focus on the first term in the brackets in the second line of
Eq. (7.20). The first term is rewritten as follows:
(7.21)

By expanding the logarithmic term in Eq. (7.21) with respect to ε, Eq. (7.21) can
be represented thus:
Equation (7.21)
(7.22)
where 
 denotes a set of terms of no less than the second power of ε. 
, but in the following equations, we simply use Z instead of 
, as we
do not have to distinguish them. Therefore, by substituting Eq. (7.22) into
Eq. (7.20), it can be represented as:
Equation (7.20)
(7.23)

We use Eq. (7.7) to factorize 
 and 
. Therefore, the optimal posterior
(VB posterior) 
 satisfies the relation whereby Eq. (7.23) 
, and is
obtained as:
(7.24)
Since only the second term in the right-hand-side depends on 
, the optimal VB
posterior is finally derived as:
(7.25)
or by considering the normalization constant, it is derived as
(7.26)
where the omitted notations are recovered (
). Thus, we obtain
the general form of the VB posterior distribution 
 by using the
variational method. Equation (7.25) tells us that if we want to infer some
probabilistic variables, we first need to prepare the joint distribution of the
observation X and target variables. Note that 
 and the other posterior
distributions 
 depend on each other due to the expectation in
Eq. (7.25). Therefore, this optimization can be performed iteratively from the
initial posterior distributions for all 
. The following sections provide
more practical forms of VB posteriors.
7.2   Variational inference for classification problems
This section provides more specific formulations for our speech and language
processing issues which focus more on pattern classification problems. Let O be
a training data set of feature vectors, and Z be a set of discrete latent variables.
Then, with a fixed model structure M, posterior distributions for model
parameters 
 and 
 given category c are expressed as

follows:1
(7.27)

and
(7.28)
where 
 is a prior distribution for Θ. Here, 
 represents the set of all
categories without c. In this section, we can also regard the prior hyperparameter
setting as the model structure setting, and include its variations in index M. The
posterior distributions for the model structure 
 are expressed as follows:
(7.29)
where 
 denotes a prior distribution for model structure M.
These equations cannot be solved analytically, because the acoustic model
for speech recognition includes latent variables in HMMs and GMMs, as
discussed in Section 3.2, and the total number of model parameters amounts to
more than one million. In addition, these parameters depend on each other
hierarchically. Solving all integrals and expectations numerically requires huge
amounts of computation time. Therefore, when applying the Bayesian approach
to acoustic modeling for speech recognition, an effective approximation
technique is necessary. Therefore, this section focuses on the VB approach and
derives general solutions for VB posterior distributions 
, 
,
and 
 to approximate the corresponding true posteriors. To begin with, by
following the general VB formulation in Section 7.1.2, we assume that
(7.30)
This assumption means that probabilistic variables associated with each category

are statistically independent from other categories. In addition, these posterior
distributions depend on the model variable M, which is not marginalized. The
speech data used are assumed to be well transcribed and the label information is
assumed to be reliable. In addition, the frequently used feature extraction (e.g.,
MFCC) from the speech is good enough for the statistical independence
assumption of the observation data to be guaranteed. Therefore, the assumption
of class independence is reasonable.
7.2.1
 VB posterior distributions for model parameters
This subsection discusses VB posterior distributions for model parameters with
fixed model structure M. Initially, arbitrary posterior distribution 
 is
introduced, and the Kullback–Leibler (KL) divergence (Kullback &
Leibler 1951) between 
 and true posterior distribution 
 is
considered:
(7.31)
Substituting Eq. (7.27) into Eq. (7.31), Eq. (7.31) is rewritten as follows:
(7.32)
Then applying the continuous Jensen’s inequality Eq. (7.10) to Eq. (7.32), the
following inequality is obtained:

(7.33)
From the third to the fourth line, we use the definition 
 and the
relation 
, which is derived from Eq. (7.30).
Thus, we finally obtain the following inequality:
(7.34)

where
(7.35)
This corresponds to the variational lower bound, as discussed in Eq. (7.3). The
inequality 
(7.34) 
is 
strict 
unless 
 
and 
 (i.e., the arbitrary posterior distribution q is equivalent
to the true posterior distribution p). From the assumption Eq. (7.30), 
 is
decomposed into each category as follows:
(7.36)
This indicates that the total objective function is calculated by summing all
objective functions for each category.
From inequality Eq. (7.34), 
 approaches 
 as the right-
hand-side decreases. Therefore, the optimal posterior distribution can be
obtained by a variational method. Since the term 
 can be disregarded,
minimization is changed to maximization of 
 with respect to 
,
and is given by the following variational equation:

(7.37)
From this equation, the optimal VB posterior distribution 
 is obtained
by using the variational method as follows:
(7.38)
This result can also be obtained by using the general formula of the variational
posterior in Eq. (7.25). By using the replacement 
, Eq. (7.25) can be
rewritten as follows:
(7.39)
Here, we use the factorization property of the posterior distributions in
Eq. (7.30). This result means that the optimal posterior distribution of model
parameters 
 is obtained by its prior distribution 
 and the
expected complete data likelihood 
.
7.2.2
 VB posterior distributions for latent variables
A similar method is used for the optimal VB posterior distribution 
.
An inequality similar to Eq. (7.37) is obtained by considering the KL divergence

between the arbitrary posterior distribution 
 and the true posterior
distribution 
 as follows:
(7.40)
The optimal VB posterior distribution 
 is also obtained by
maximizing 
 with respect to 
 with the variational method as
follows:
(7.41)
This result is also obtained by using the general formula of the variational
posterior in Eq. (7.25). Compared with the result for 
 in Eq. (7.38),
Eq. (7.41) does not need to prepare the prior distribution for Z.
7.2.3
 VB–EM algorithm
Equations (7.38) and (7.41) are closed-form expressions, and these optimizations
can be effectively performed by iterative calculations analogous to the
expectation and maximization (EM) algorithm (Dempster et al. 1976), as
discussed in Sections 3.4 and 4.2, which increases 
 at every iteration up to a
converged value. Then, Eqs. (7.38) and (7.41), respectively, correspond to the
maximization step (M-step) and the expectation step (E-step) in the VB
approach. We call this algorithm the variational Bayes expectation and
maximization (VB–EM) algorithm. Therefore, by substituting q into 
, these
equations can be represented as follows:
(7.42)
Note that optimal posterior distributions for a particular category can be obtained
simply by using the category’s variables, i.e., we are not concerned with the

other categories in the calculation, since Eq. (7.42) only depends on category c,
which is based on the assumption given by Eq. (7.30).
Finally, to compare the VB approach with the conventional ML approach for
training latent variable models, the training specifications for ML and VB are
summarized in Table 7.1.
Table 7.1 Training specifications for ML and VB.
Training
Min-max optimization
Objective function
ML
ML–EM
differential method
Q function
VB
VB–EM
variational method
 functional
7.2.4
 VB posterior distribution for model structure
The VB posterior distributions for a model structure are derived in the same way
as in Section 7.2.1, and model selection is carried out employing the posterior
distribution. Arbitrary posterior distribution 
 is introduced and the KL
divergence between 
 and the true posterior distribution 
 is
considered:
(7.43)
Substituting Eq. (7.29) into Eq. (7.43) and using Jensen’s inequality, the
inequality of Eq. (7.43) can be obtained as follows:
(7.44)
Similarly to the discussion in Section 7.2.1, from the inequality Eq. (7.44), 
 approaches 
 as the right-hand-side decreases.

Compared with the posterior distributions of model parameters and latent
variables, we cannot use the formula Eq. (7.25), since it is not practical to
marginalize all possible model structures M. Therefore, the optimal posterior
distribution for a model structure can again be obtained by a variational method,
as explained in Section 7.1.3. If we consider the constraint 
, the
functional differentiation is represented by substituting respectively 
 and 
 into 
 and 
 in Eq. (7.18) as follows:
(7.45)
where K is a Lagrange multiplier. We focus on the first term in the brackets in
the 2nd line of Eq. (7.45). This term can be rewritten as follows:
(7.46)
By expanding the logarithmic term in Eq. (7.46) with respect to ε, Eq. (7.46) is
represented as:
Equation (7.46)

(7.47)
Therefore, by substituting Eq. (7.47) into Eq. (7.45), Eq. (7.45) is represented as:
(7.48)
Therefore, the optimal posterior (VB posterior) 
 satisfies the relation
whereby Eq. (7.48) 
, and is obtained as:
(7.49)
By disregarding the normalization constant, the optimal VB posterior is finally
derived as:
(7.50)
Compared with Eqs. (7.38) and (7.41), the posterior obtained is represented by
the total variational lower bound.
Assuming that 
 is a uniform distribution,2 the proportional relation
between 
 and 
 is obtained as follows, based on the convexity of the
logarithmic function:
(7.51)
Therefore, an optimal model structure in the sense of maximum posterior
probability estimation can be selected as follows:

(7.52)
This indicates that by maximizing total 
 with respect to both 
, 
, and M, we can obtain the optimal parameter distributions and select
the optimal model structure simultaneously (Attias 1999, Ueda 
&
Ghahramani 2002).
Thus, we analytically derive the variational posterior distributions of general
latent models. The next section applies these solutions to the continuous density
hidden Markov model (CDHMM), as we apply ML–EM and MAP–EM to
CDHMMs in Sections 3.4 and 4.3, respectively.
7.3   Continuous density hidden Markov model
This section reformulates the CDHMM training for speech processing based on
the VB framework (Valente & Wellekens 2003, Somervuo 2004, Watanabe
et al. 2004). The four formulations are obtained by using the VB framework to
perform acoustic model construction (model setting, training, and selection) and
speech 
classification 
consistently, 
based 
on 
the 
Bayesian 
approach.
Consequently, the conventional formulations based on the ML and MAP
approaches in Sections 3.4 and 4.3 are replaced by formulations based on the
Bayesian approach as follows:
Set generative model distributions
 Set generative model distributions and prior distributions
(Section 7.3.1 and 7.3.2);
ML/MAP Baum–Welch algorithm
 VB Baum–Welch algorithm (Section 7.3.3);
Log likelihood 
 VB objective function (Section 7.3.4);
ML/MAP-based classification
 VB–BPC (Section 7.3.5).

These four formulations are explained in the following four subsections, by
applying the acoustic model for speech recognition to the general solution in
Section 7.2.
7.3.1
 Generative model
Similarly to the MAP–EM algorithm in Section 4.3, setting of the emission and
prior distributions is required when calculating the VB posterior distributions.
This section provides these distributions for CDHMM again, to provide the VB-
based analytical solutions.
Let 
 be a sequential speech data set for a speech
segment of a phoneme category. Since the formulations for the posterior
distributions are common to all phoneme categories, the phoneme category
index c is omitted from this section to simplify the equation forms. D is used to
denote the dimension number of the feature vector and T to denote the frame
number. The complete data likelihood function with speech, HMM state, and
GMM component sequences (O, S, and V), which is introduced in Eqs. (3.44)
and (4.23), is expressed by
(7.53)
where 
.3 Although we have many segments for each phoneme
category and the generative model distribution must consider the product of each
segment in Eq. (7.53), this is also omitted in this book. Here, S and V are sets of
discrete latent variables, which are the concrete forms of Z in Section 7.2. The
parameter 
 denotes the state transition probability from state i to state j, and 
 is the kth weight factor of the Gaussian mixture for state j. In addition, 
 denotes the Gaussian with mean vector 
 and
covariance matrix 
 defined as:
(7.54)

 is a set of model parameters. Here, J denotes
the number of states in an HMM sequence and K denotes the number of
Gaussian components in a state. This section only considers the diagonal
covariance matrix case.
7.3.2
 Prior distribution
Conjugate distributions, which are based on the exponential function, are as easy
to use as prior distributions since the function forms of prior and posterior
distributions become the same (Berger 1985, Gauvain & Lee 1994, Bernardo &
Smith 2009), as discussed in Sections 2.1.3 and 4.3.3. Then a distribution is
selected where the probabilistic variable constraint is the same as that of the
model parameter. The state transition probability 
 and the mixture weight
factor 
 have the constraint that 
 and 
. Therefore, the
Dirichlet distributions for 
, 
, and 
 are used, where the variables of the
Dirichlet distribution satisfy the above constraint. Similarly, the diagonal
elements of the inverse covariance matrix 
 are always positive, and the
gamma distribution is used. The range of the mean vector 
 is from 
to 
, and the multivariate Gaussian distribution is used. Thus, as introduced in
Eqs. (4.29) and (4.32), the prior distribution for a CDHMM with a diagonal
covariance matrix is expressed as follows:
(7.55)
Here, 
 is a set of prior
parameters. In Eq. (7.55), 
 denotes a Dirichlet distribution and 
denotes a gamma distribution. (It is different from the conventional definition of
the gamma distribution, see Appendix C.11.) If the covariance matrix elements
are off the diagonal, a Gaussian–Wishart distribution is used as the prior

distribution of 
 and 
. The explicit forms of the distributions are defined as
follows (Appendix C.4, C.5, and C.11):
(7.56)
where the normalization constants are defined as follows:
(7.57)
In the Bayesian approach, an important problem is how to set the prior
parameters. Here, two kinds of prior parameters of 
 and 
 are set using
sufficient amounts of data from:
Statistics of higher hierarchy acoustic models for the acoustic model
construction task;
Statistics of speaker independent models for the speaker adaptation task.
The other parameters (
, and 
) have a meaning as regarding tuning
the balance between the values obtained from training data and the above
statistics. These parameters are set appropriately based on experiments, as
discussed in speaker adaptation (Section 4.4).

Finally, Algorithm 10 provides a generative process for a CDHMM with
prior distribution. For simplicity, the initial weight, the hyperparameters, and the
model structure are given in this generative process, but it can also be sampled
from some distributions. Compared with Algorithm 3, CDHMM parameters are
also sampled from the prior distributions.
Algorithm 10 Generative process for continuous density hidden Markov
model with prior distributions
Require:  Ψ, M, and 
 
1:  for 
 do 
2:     Draw 
 from 
 
3:  end for 
4:  for 
 do 
5:     for 
 do 
6:        Draw 
 from 
 
7:        for 
 do 
8:           Draw 
 from 
 
9:           Draw 
 from 
 
10:        end for 
11:     end for 
12:  end for 
13:  Draw 
 from 
 
14:  Draw 
 from 
 
15:  Draw 
 from 
 
16:  for 
 do 
17:     Draw 
 from 
 
18:     Draw 
 from 
 
19:     Draw 
 from 
 
20:  end for
7.3.3
 VB Baum–Welch algorithm
This subsection introduces concrete forms of the VB posterior distributions for
model parameters 
 and for latent variables 
 in acoustic

modeling, which are efficiently computed by VB iterative calculations within the
VB framework. This calculation is effectively carried out by the VB Baum–
Welch algorithm (MacKay 1997).
VB M-step
First, the VB M-step for acoustic model training is explained. This is solved by
substituting the acoustic model setting in Section 7.3.1 into the general solution
for the VB M-step in Section 7.2. From Eq. (7.42), the VB posterior distributions
for the model parameters are represented as follows:
(7.58)
Taking the logarithmic operation, Eq. (7.58) is represented as:
(7.59)
Here, 
 is a VB auxiliary function defined as follows:
(7.60)
On the other hand, the MAP auxiliary function in Eq. (4.16) is defined as
follows:
(7.61)
Comparing the VB and MAP auxiliary functions, these are almost equivalent
except the posterior distributions of latent variables, i.e., 
 vs. 

. Since 
 is obtained by marginalizing Θ, 
 is
more appropriate in terms of the Bayesian treatment.
Therefore, this VB-M step is solved by using the result of the MAP-M step
solution, except that 
 is obtained by using the VB-E step. The
calculated results for the optimal VB posterior distributions for the model
parameters are summarized as follows:
(7.62)
The concrete forms of the distributions are defined as follows:
(7.63)
where the normalization constants are:

(7.64)
Note that Eqs. (7.55) and (7.62) are members of the same function family, and
the only difference is that the set of prior parameters 
 in Eq. (7.55) is replaced
with a set of posterior distribution parameters 
 in Eq. (7.62), where 
 is
defined as:
(7.65)
The conjugate prior distribution is adopted because the posterior distribution is
theoretically a member of the same function family as the prior distribution, and
is obtained analytically, which is a characteristic of the exponential distribution
family, as discussed in Section 2.1.4. Here,  values are calculated from:
(7.66)

 denotes the sufficient statistics of the transition matrix, and 
, 
, and 
denote zeroth-, first-, and second-order sufficient statistics of the GMM,
respectively, and are defined as follows:
(7.67)
These 
sufficient 
statistics 
 
are
computed by using 
 and 
, defined as follows:
(7.68)
Here, 
 is a VB transition posterior distribution, which denotes the transition
probability from a state i to a state j at a frame t, and 
 is a VB occupation
posterior distribution, which denotes the occupation probability of a mixture
component k in a state j at a frame t, in the VB approach. These are similar to
those defined in Eq. (3.119) by using the ML–EM algorithm of a CDHMM.
Therefore,  can be calculated from 
, 
, and 
, enabling 
to be obtained.
VB E-step
Before we focus on the calculation of 
 and 
, we first focus on the
posterior distribution of the joint distribution of the HMM state and mixture
component sequences 
. From Eq. (7.25), 
 is
represented as follows:
(7.69)

Since 
 does not depend on S and V, we find that the complete data
likelihood marginalized by the model parameter Θ can also be obtained by
considering the same expectation with Eq. (7.69):
(7.70)
Once we obtain the complete data likelihood 
, we can compute the
posterior probabilities 
 and 
 by using the forward–backward
algorithm, similarly to Section 3.3. Therefore, we focus on how to compute the
following expectation of the complete data log likelihood:
(7.71)
By substituting Eq. (7.53) into Eq. (7.71), Eq. (7.71) can be represented as
follows:
(7.72)
Now we focus on the case when 
, 
, and 
, where we need
to compute the following equations:
(7.73)

We can also define the following function based on Eq. (7.73):
(7.74)
This equation behaves similarly to the likelihood function of 
 in
Eq. (7.53). Note that 
 is not properly normalized, and cannot be
dealt with as a probabilistic distribution. However, from Eq. (7.70), 
is proportional to 
, and this function has the following relationship
from Eq. (7.69):
(7.75)
where 
 and 
 are normalization constants of 
 and 
 respectively
for each frame, and satisfy the following condition:
(7.76)
Note that it is not easy to obtain the normalization factors 
 and 
 explicitly,
since it requires 
 and 
. However, it will be shown later
that the calculation of the occupation probabilities does not require computation
of the normalization factors explicitly, but only requires 
, 
, and 
.
Therefore, we can compute various values from Eq. (7.74) (e.g., the forward and
backward variables and the occupation probabilities), as discussed in
Section 3.3. Thus, the following paragraphs provide the analytical solutions of 
, 
, and 
 in detail.

State transition 
First, the integral over 
 is solved from Eq. (7.63) by using a partial integral
technique and a normalization constant:
(7.77)
Then we use the following derivative formula:
(7.78)
By substituting Eq. (7.78) into Eq. (7.77), Eq. (7.77) can be rewritten as:
(7.79)
where we replace the derivative and integral, and the integral can be performed
to derive the inverse of the normalization constant of the Dirichlet distribution.
From Eq. (7.64), this derivative can be calculated as follows:

(7.80)
where 
 is a di-gamma function, which first appeared in Eq. (5.82), and is
defined as
(7.81)
Thus, 
 is finally obtained as follows:
(7.82)
Mixture weight 

In a way similar to that used for 
, the integral over 
 is solved from
Eq. (7.63), and 
 is obtained as follows:
(7.83)
Gaussian distribution 
First, 
 can be factorized for each dimension:
(7.84)
Therefore, we focus on calculation of the d element. Since the calculation is
more complicated than the two previous calculations, the indexes j, k, t, and d
are removed to simplify the derivation. By using (7.63), 
 can be rewritten
as follows:
(7.85)
Now we focus on the integral over mean parameter μ. To calculate the integral,
we first rewrite the part that is related to μ as follows:

(7.86)
The integral of the above terms can be analytically solved. We first consider the
following partial derivative:
(7.87)
Therefore, the first integral can be represented as:
(7.88)
By replacing the integral with the partial derivative, we can solve the integral as:
(7.89)

The other two integrals are trivially solved as follows:
(7.90)
Therefore, Eq. (7.86) is solved as:
(7.91)
Now, we focus on the integral over r, because the integral without 
 can be
easily computed by the result of the mean value of the gamma distribution in
Appendix C.11 as:
(7.92)
Therefore, we focus on the final term. From Eqs. (7.63) and (7.64), the concrete
form of the gamma distribution, 
, is defined as follows:
(7.93)

where
(7.94)
Similarly to the Dirichlet and Gaussian distributions, we consider the following
derivative:
(7.95)
Therefore, the integral is solved by using this relationship as:
(7.96)
The derivative with respect to  is calculated as follows:

(7.97)
where 
 is a di-gamma function defined in Eq. (7.81). Therefore,
(7.98)
Thus, finally 
 is obtained analytically as follows:
(7.99)
Reverting to the indexes k, j, t, and d, 
 is represented as
(7.100)
Thus, we obtain 
 and 
, which are summarized as follows:
(7.101)

These variables are used to compute the VB transition probability 
 and VB
occupation probability 
.
VB transition probability 
 and occupation probability 
(VB E-step)
From Eq. (7.25), VB transition probability 
 is represented as:
(7.102)
Section 3.4.2 shows an efficient computation of the transition probability based
on the complete data likelihood 
. Here we also consider how to
obtain it based on the VB version of the complete data likelihood 
,
as introduced in Eq. (7.69). However, from Eq. (7.74), 
, and 
 can only
compute the unnormalized likelihood function 
, that is
(7.103)
Therefore, as we discussed in Section 3.4.2, from the dependency of the HMM,
we can represent 
 as follows:
(7.104)
Here, 
 is a forward variable at frame t in state i, as introduced in Eq. (3.50).
Similarly, 
 is a backward variable at frame 
 in state j, as introduced in
Eq. (3.55). The forward and backward variables based on the VB formulation
are represented as described below.

First, the VB forward variable 
 is computed by using the following
equation:
Initialization
(7.105)
Then, unnormalized forward variable 
 is defined as:
(7.106)
Induction
(7.107)
where the unnormalized forward variable 
 is represented as

(7.108)
Termination
(7.109)
The VB forward variable 
 is obtained with the unnormalized forward
variable 
 and normalization constants 
 and 
. From this algorithm, we
can compute the unnormalized forward variable 
 similarly to the original
forward algorithm, but we should be careful that the unnormalized forward
variable is not a probability, and probabilistic calculation (sum and product rules
etc.) must be performed via the normalized VB forward variable 
.
Similarly, the VB backward variable 
 is computed by using the following
equations:
Initialization
(7.110)
Induction

(7.111)
where the unnormalized backward variable 
 is represented as
(7.112)
Termination
(7.113)
Therefore, based on the VB forward and backward variables, we can compute
the posterior probabilities as follows:
(7.114)
Thus, we can compute the transition probability with unnormalized VB variables
, and 
, and unnormalized forward and backward variables 
 and 
, where the normalization constants 
 and 
 are canceled out. This is
based on a well-known scaling property of the HMM forward–backward
algorithm (Rabiner & Juang 1993). Similarly the occupation probability is also

calculated as:
(7.115)
These probabilities are obtained similarly to the ML cases in Eqs. (3.126) and
(3.122), and the MAP case in Eqs. (4.91) and (4.92) with the VB-based variables
obtained by Eq. (7.101). Thus, 
 and 
 are calculated efficiently by
using a probabilistic assignment via the familiar forward–backward algorithm.
This algorithm is called the VB forward–backward algorithm.
Similarly to the VB forward–backward algorithm, the Viterbi algorithm is
also derived within the VB approach by exchanging the summation over i for the
maximization over i in the calculation of the unnormalized forward probability 
. This algorithm is called the VB Viterbi algorithm.
Thus, VB posteriors can be calculated iteratively in the same way as the
Baum–Welch algorithm, even for a complicated sequential model that includes
latent variables such as HMM and GMM for acoustic models. These calculations
are referred to as a VB Baum–Welch algorithm, as proposed in
MacKay (1997), Watanabe et al. (2002), Beal (2003), Watanabe et al. (2004).
7.3.4
 Variational lower bound
This section discusses the VB objective function 
 for a whole acoustic
model topology, i.e., the variational lower bound, and provides general
calculation results. The variational lower bound is a criterion for both posterior
distribution estimation, and model topology optimization in acoustic model
construction. This section begins by focusing on one phoneme category. By
substituting the VB posterior distribution obtained in Section 7.3.3, we obtain
analytical results for 
, and therefore, this calculation also requires a VB
iterative calculation based on the VB Baum–Welch algorithm used in the VB
posterior calculation. We can separate 
 into two components: one is
composed solely of 
, whereas the other is mainly composed of 
. Therefore, we define 
 and 
, and represent 
 as follows:

(7.116)

where
(7.117)
First, we focus on 
. Based on the variational solution of 
 in
Eq. (7.42), 
 is rewritten as follows:
(7.118)
where Z is a normalization constant. This equation means that 
 is represented
by the logarithmic function of the normalization constant Z. By using the
definition of the VB auxiliary function 
 in Eq. (7.60), Z can be rewritten as
(7.119)
Here, from the similarity of the VB and MAP auxiliary functions, as discussed in
Section 7.3.3, 
 can be obtained by using the analytical results of the MAP
auxiliary function. From Eq. (4.38), 
 is decomposed into the following
auxiliary functions:
(7.120)

where 
, 
, and 
 are obtained from the analytical solutions of the
corresponding MAP auxiliary functions in Eqs. (4.51), (4.54), and (4.73), as
follows:
(7.121)
(7.122)
(7.123)
If we consider the diagonal covariance matrix, Eq. (7.123) is modified by using
the gamma distribution as follows:
(7.124)
Here 
 in Eq. (7.124) and 
 in Eqs. (7.121) and (7.122) are
normalization constants of Gamma and Dirichlet distributions, respectively, and
are defined as follows:

(7.125)
Therefore, by substituting Eqs. (7.121), (7.122), and (7.124) into Eq. (7.119),
and by using the definition of the normalization constant of the Dirichlet and
gamma distributions in Eq. (7.125), the integral in Z is performed with the
normalization of Θ, and Z is simply obtained as follows:
(7.126)
From Eq. (7.126), 
 can be calculated by using the statistics of the posterior
distribution parameters 
 given in Eq. (7.66). This part is equivalent to the
objective function for model selection based on Akaike’s Bayesian information
criterion (Akaike 1980). The whole 
 for all categories is obtained by simply
summing up the 
 results obtained in this section for all categories as in
Eq. (7.36).

Now we focus on 
. From the definition in Eq. (7.117), 
 can be
represented as follows:
(7.127)
Therefore, 
 denotes the entropy of the posterior distribution 
. As
we discussed in Section 7.69, it is difficult to obtain the analytical form of 
 due to the normalization constant, and direct calculation of the
above entropy is also difficult. Instead, we focus on the variational complete data
likelihood form 
, which is obtained by the Bayes theorem as
follows:
(7.128)
Based on the discussion in Eq. (7.73), 
 is represented with the
unnormalized function 
 as follows:
(7.129)
where C is a normalization constant, defined as follows:
(7.130)
, 
, 
 are analytically calculated in the VB-E step (Eq. (7.101)).
Therefore, by substituting (7.129) into (7.128), the normalization constant is

canceled out, and we can obtain the following equation:
(7.131)
Therefore, 
 can be rewritten as follows:
(7.132)
Note that the second term corresponds to the summation of all possible S and V
for unnormalized function 
, which can be computed in the VB
forward algorithm in Eq. (7.109) as follows:
(7.133)
Now we focus on the first term in Eq. (7.132). From the discussions in
Section 3.4.2, we can convert summation over sequences S, V to a summation
over HMM states i and j, and mixture component k in this term. Therefore, this
term is represented as follows:

(7.134)
Thus, we obtain the term without computing the summation over S and V.
Finally, we summarize calculation of 
 by using the definitions of 
, 
, 
 in Eq. (7.101), as follows:
(7.135)
Thus, we also obtain the analytical result for 
, which corresponds to the latent
variable effect for the variational lower bound.
The analytical result for the variational lower bound 
 for the CDHMM is
determined using 
 in Eq. (7.126) and 
 in Eq. (7.135). Although the
analytical result looks complicated, all variables are already computed in the VB
expectation and maximization steps. We also want to emphasize that the
computation is quite feasible since it is carried out without a summation over all
possible latent variable sequences S and V. The variational lower bound is
derived analytically so that it retains the effects of the dependence between
model parameters and of the latent variables, defined in the generative model
distribution in Eq. (7.53), unlike the conventional Bayesian information criterion

and minimum description length (BIC/MDL) approaches, as discussed in
Section 6.5. Therefore, the variational lower bound can compare any acoustic
models with respect to all topological aspects and their combinations, e.g.,
contextual and temporal topologies in HMMs, the number of components per
GMM in an HMM state, and the dimensional size of feature vectors, based on
the following equation:
(7.136)
Here 
, 
, 
, and 
 denote search spaces of HMM-temporal, HMM-
contextual, GMM and feature vector topologies, respectively.
Based on the discussion in Section 7.3, the seven steps in Algorithm 11
provide a VB training algorithm for acoustic modeling. Here, τ denotes an
iteration count, and ε denotes a threshold that checks whether 
 converges.
Thus, the posterior distribution estimation in the VB framework can be
effectively constructed based on the VB Baum–Welch algorithm, which is
analogous to the ML Baum–Welch algorithm (Algorithm 4). In addition, VB can
realize the model selection using the VB objective function as shown in Step 9.
Thus, VB can construct an acoustic model consistently based on the Bayesian
approach.
Algorithm 11 Variational Bayesian Baum–Welch algorithm for CDHMMs
with model selection
Require:  Set posterior parameter 
 from initialized transition
probability 
, occupation probability 
, and model structure
M (prior parameter 
 is included) for each category 
1:  repeat 
2:     Compute 
, 
, and 
 using 
. (By
Eq. (7.101)) 
3:     Update 
 and 
 via the Viterbi algorithm or forward–
backward algorithm. (By Eqs. (7.114) and (7.115)) 
4:     Accumulate the sufficient statistics 
, 

(by Eq. (7.67) 
5:     Compute 
 using 
 and 
. (By Eq. (7.66)) 
6:     Calculate total 
 for all categories. (By using Eqs. (7.126) and
(7.135) and summing up all categories’ 
) 
7:     Calculate 
, 
 
8:  until 
Calculate 
 for all possible M and find 
)
Note that if we change 
 (a value with 
 attached indicates an
ML estimate), 
 and 
 (where 
 means the log-likelihood
for a model M), this algorithm becomes an ML-based framework, except for the
model selection. Therefore, in the implementation phase, the VB framework can
be realized in the conventional systems of acoustic model construction by adding
the prior distribution setting and by changing the estimation procedure and
objective function calculation.
7.3.5
 VB posterior for Bayesian predictive classification
This subsection deals with the Bayes decision rule based on the VB approach. It
is related to the Bayesian predictive classification, as discussed in Section 6.3.1
with the Laplace approximation, but this section deals with the same issue with
VB. In this section, we use the following notation to clearly distinguish the
training and recognition data, similar to Section 6.3.1:
(7.137)
In recognition, 
 denotes the feature vector
sequence of input speech, and 
 denotes the
corresponding HMM state sequence. Although our target application is ASR,
which outputs the word sequence W, this section simplifies the decoding rule for
the explanation. That is, the decoding needs to handle the word sequence W in
addition to the state sequence S, but it can be combined with the LVCSR decoder
if we can build the Viterbi algorithm. Therefore, this section focuses on
formulating the Viterbi algorithm within the VB framework, similarly to that
within the ML framework, as discussed in Section 3.3.2.

The Viterbi algorithm can achieve the optimal state sequence  by using a
conditional probability function 
 given input data O and training data 
, as follows:
(7.138)
 is a variant of predictive distribution (Berger 1985, Bernardo &
Smith 2009), because this distribution predicts the probability of unknown data
O conditioned by training data 
. Note that Eq. (7.138) does not depend on
parameters Θ and model M, and these can be explicitly involved by considering
the following sum rule:
(7.139)
This predictive distribution based approach involves considering the integrals
and true posterior distributions, an approach which is also applied to speech
recognition (Huo & Lee 2000, Jiang et al. 1999, Lee & Huo 2000, Chien &
Liao 2001), as discussed in Section 6.3.1, with the Laplace approximation.
After VB-based acoustic modeling in Section 7.3.4, an appropriate model
structure 
 is selected based on the VB model selection Eq. (7.136), and the
optimal VB posterior distributions are obtained 
. Therefore, the true
posterior distributions can be approximated by the VB posteriors, and
Eq. (7.139) is approximated as:
(7.140)

Thus, we can build the Viterbi algorithm for the expectation over the model
parameter Θ by using the VB posterior. In the following section we omit the
models structure index 
 for simplicity.
Similarly to Section 3.3.2, we first define the following expected highest
probability along a single path, at time t, which accounts for the first t
observations and ends in state j:
(7.141)
By using 
 recursively, we can obtain the most probable state sequence as
follows:
Initialization
(7.142)
Recursion
(7.143)
Termination
(7.144)
State sequence backtracking

(7.145)
Thus, we can perform the Viterbi algorithm for the predictive distribution based
on the VB posteriors. To realize the Viterbi algorithm, we need to consider the
following expectation:
(7.146)
We provide the solution for each of the expected variables. Note that these are
different from the expected variables of the CDHMM parameters in the VB E-
step, as discussed in Eq. (7.73),4 i.e.,
(7.147)
This is because the VB E-step is a training step, and we need to optimize these
values based on the variational method, which necessitates considering the
expectation of the parameters in the logarithmic domain. In the prediction case,
since we already have the posterior distributions in the training step, Eq. (7.146)
simply performs the expectation determination for the CDHMM parameters
directly.
Expected state transition 
We first focus on calculation of the expected state transition 
. Although this
can be obtained as the mean result of the Dirichlet distribution in Appendix C.4,
we provide the derivation for its educational value. Based on the definition of the
Dirichlet distribution in Eq. (7.62), we can obtain the following equation:

(7.148)
Now we define the following variable:
(7.149)
By using , the integral in Eq. (7.148) is solved as:
(7.150)
The normalization constant of the Dirichlet distribution is defined (Appendix
C.4) as
(7.151)
Therefore, by substituting the concrete form of the normalization constant into
Eq. (7.150) and by using the definition of 
, Eq. (7.150) can be represented as
follows:

(7.152)
Finally, we use the following formula for the gamma function:
(7.153)
Then, Eq. (7.152) is analytically obtained as the following simple equation:
(7.154)
Note that the state transition probability is obtained from the normalized weight,
which is proportional to the posterior hyperparameter 
, and the result is very
intuitive.
Expected mixture weight 
Since the mixture weight 
 is represented by a multinomial distribution, it is
similar to the state transition 
. Similarly to 
, the expected state
transition 
 is calculated as follows:

(7.155)
Again, the mixture weight probability is obtained using the normalized weight of
the posterior hyperparameter .
Expected Gaussian distribution 
Finally, we calculate the expected value of the Gaussian distribution with VB
posteriors for Gaussian parameters. First the expectation is factorized for each
dimension when we use the diagonal covariance as follows:
(7.156)
The indexes of state ij, mixture component k, frame t, and dimension d are
removed to simplify the derivation.
Based on the definition of the Gaussian and gamma distributions in
Eq. (7.62), we can obtain the following equation:
(7.157)
First, we focus on the integration with respect to μ, and completing the square
with respect to μ. Then, by integrating with respect to μ, and arranging the
equation, the following equation is obtained:

(7.158)
Here we discuss the case when the VB posterior for r is the Dirac delta function
around the MAP value of r, and the argument of its Dirac delta function is the
maximum value of the VB posterior. Then, the result of the integration with
respect to r is obtained by changing r to the MAP value 
 in Eq. (7.158)
in Appendix C.11. Therefore, the following equation is obtained:
(7.159)
Thus, by recovering the omitted indexes, we can obtain
(7.160)
This is the analytical result of the expected function of a Gaussian distribution
with expectation only over the mean parameter μ.
By substituting Eq. (7.158) into Eq. (7.157), we can obtain the following
integral:

(7.161)
First we use the following notation to simplify the integral:
(7.162)
Note that β depends on the observation o. Then, the integral with the explicit
range of r is rewritten as follows:
(7.163)
Now, we convert r with the following variable x:
(7.164)
Now 
 is a hyperparameter of the Dirichlet distribution, and 
, therefore 
 from Eq. (7.162), and the range of x becomes 
. Then, the integral
can be rewritten as:
(7.165)

Here, from the formula of the gamma function, we can further rewrite the above
integral as:
(7.166)
Since 
 does not depend on the observation o, we can disregard it as a
constant value. Finally, by recovering the variables of α and β from Eq. (7.162),
Eq. (7.161) is obtained as the following equation:
(7.167)
Here we refer to the concrete form of the Student’s t-distribution given in
Appendix C.16:
(7.168)
The parameters μ, κ, and  of the Student’s t-distribution correspond to those of
the above equation as follows:

(7.169)
Thus, the result of the integral with respect to μ and r (Eq. (7.157)) is represented
as the Student’s t-distribution:
(7.170)
The third parameter in the t-distribution is called the degree of freedom, and if
this value is large, the distribution approaches the Gaussian distribution
theoretically.
Thus, by recovering the omitted indexes, we can obtain
(7.171)
This is the analytical result of the expected Gaussian distribution with
marginalization of both mean and precision parameters μ and r. Compared with
Eq. (7.160), the marginalization of both parameters changes the distribution from
the Gaussian distribution to the Student’s t-distribution. The latter is called a
long tail distribution since it is a power law function, and it provides a robust
classification in general, when the amount of training data is small.
Since the degree of freedom in this solution is the posterior hyperparameter
of the precision parameter 
, and it is proportional to the amount of data, as
shown in Eq. (7.67), this solution approaches the Gaussian distribution. Then,
the variance parameters in Eq. (7.171) also approximately approach the
following value:
(7.172)

Thus, Eq. (7.171) is approximated by the following Gaussian distribution when
the amount of data O is large:
(7.173)
This solution corresponds to the MAP estimation result of the Gaussian
distribution in CDHMM, as discussed in Section 4.3.5.
In Watanabe & Nakamura (2006), experimental results were reported to
show the effectiveness of the Bayesian predictive classification without
marginalization (it corresponds to the MAP estimation in Section 4.3), with
marginalization of only mean parameters (corresponds to Eq. (7.160)), and with
marginalization of both mean and covariance parameters (corresponds to
Eq. (7.171)). Speaker adaptation experiments for LVCSR (30 000 vocabulary
size) show that the Student’s t-distribution-based Bayesian predictive
classification improved the performance from the MAP estimation and the
marginalization results, reducing the WERs by 2.3 % and 1.2 %, respectively,
when we only used one utterance (3.3 seconds on average) for the adaptation
data. Since all the results use the same prior hyperparameter values, the
improvement purely comes from the marginalization effect. In addition, if the
amount of adaptation data increased, the performance of these three methods
converged to the same value, which is also expected, based on the discussion of
analytical results of the Student’s t-distribution in Eq. (7.173).
The use of VB-based Bayesian predictive classification makes acoustic
modeling in speech recognition a totally Bayesian framework that follows a
consistent concept, whereby all acoustic procedures (model parameter
estimation, model selection, and speech classification) are carried out based on
posterior distributions. For example, compare the variational Bayesian speech
recognition framework with a conventional ML-BIC approach: the model
parameter estimation, model selection and speech classification are based on ML
(Chapter 3) and BIC (Chapter 6). BIC is an asymptotic criterion that is
theoretically effective only when the amount of training data is sufficiently large.
Therefore, for a small amount of training data, model selection does not perform
well because of the uncertainty of the ML estimates. The next section aims at

solving the problem caused by a small amount of training data by using VB.
7.3.6
 Decision tree clustering
This section revisits decision tree clustering of the context-dependent HMM
states, as we discussed in Section 6.5, based on the VB framework (Watanabe
et al. 2004, Hashimoto, Zen, Nankaku et al. 2008, Shiota, Hashimoto, Nankaku
et al. 2009). Similarly to Eq. (6.74), we approximate the Bayes factor in
Section 6.2 by selecting an appropriate question at each split, chosen to increase
the variational lower bound/VB objective function 
 in the VB framework, as
discussed in Section 7.3.4. When node n is split into a Yes node ( ) and No
node (
) by question Q (we use 
 with this hypothesized model, obtained
from question Q, and 
 with the original model), the appropriate question 
is chosen from a set of questions as follows:
(7.174)
where 
 is the gain in the VB objective function when node n is split by
Q, which is defined as:
(7.175)
 denotes a set of (non-shared) context-dependent HMM states at node n in a
decision tree. The question is chosen to maximize the gain in 
 by splitting.
The VB objective function for decision tree construction is also simply
calculated under the following same constraints as the ML approach:
Data alignments 
 and 
 for each state are fixed while splitting.
Emission probability distribution in a state is represented by a single
Gaussian distribution (i.e., 
).
Covariance matrices have only diagonal elements.
A contribution of state transitions 
 and initial weights 
 for likelihood is
disregarded.

By using these conditions, the objective function is obtained without iterative
calculations, which reduces the calculation time. Under conditions of fixed data
assignment and single Gaussian assumptions, the latent variable part of 
 can
be disregarded, i.e., Eq. (7.116) is approximated as
(7.176)
In the VB objective function of model parameter 
 (Eq. (7.126)), the factors of
posterior parameters of state transition 
 and mixture component 
 can also be
disregarded under the above conditions. Therefore, the objective function 
 in
node n (we omit the index n for simplicity) for assigned data set 
, where 
 can be obtained
from the modification of 
 in Eq. (7.126) as follows:
(7.177)
where 
 is a subset of the posterior parameters in
Eq. (7.66), and is represented by:
(7.178)
 and 
 are the sufficient statistics of a set of states in node n, as defined as
follows.

(7.179)
Note that since we use the hard aligned data 
 (based on the Viterbi
algorithm), the assignment information 
 is included in this representation.
Here, 
 is a set of prior parameters. One choice of
setting the prior hyperparameters 
 and 
 would be to set them by using
monophone (root node) HMM state statistics (
, 
 and 
) as follows:
(7.180)
The other parameters 
 and 
 are set manually. By substituting Eq. (7.178)
into Eq. (7.177), the gain 
 can be obtained when n is split into 
, 
 by
question Q:
(7.181)
Here, 
 is defined by:
(7.182)
The terms that do not contribute to 
 are disregarded. The final term in
Eq. (7.181) is only computed from the prior hyperparameter Ψ. Similarly to the
BIC criterion in Eq. (6.92), node splitting stops when the condition

(7.183)
is satisfied. A model structure based on the VB framework can be obtained by
executing this construction for all trees, resulting in the maximization of total 
. This implementation based on the decision tree method does not require
iterative calculations, and can construct clustered-state HMMs efficiently. There
is another major method for the construction of clustered-state HMMs that uses a
successive state splitting algorithm, and which does not remove latent variables
in HMMs (Takami & Sagayama 1992, Ostendorf & Singer 1997). Therefore, this
requires the VB Baum–Welch algorithm and calculation of the latent variable
part of the lower bound/VB objective function for each splitting. This is realized
as the VB SSS algorithm by Jitsuhiro & Nakamura (2004).
The relationship between VB model selection and the conventional BIC
model selection, based on Eqs. (7.181) and (6.89), respectively, is discussed
below. Based on the condition of a sufficiently large amount of data, the
posterior hyperparameters in Eq. (7.178) are approximated as follows:
(7.184)
In addition, from Stirling’s approximation, the logarithmic gamma function has
the following relationship:
(7.185)
when 
. By substituting Eq. (7.184) into Eq. (7.182) and using
Eq. (7.185), 
 is approximated as

(7.186)
Then, an asymptotic form of Eq. (7.182) is composed of a log-likelihood gain
term and a penalty term depending on the number of free parameters (2D in this
diagonal covariance Gaussian case) and the amount of training data, i.e., the
asymptotic form becomes the BIC-type objective function form, as shown in
Eq. (6.88). Therefore, VB theoretically involves the BIC objective function, and
so BIC model selection is asymptotically equivalent to VB model selection,
which demonstrates the advantages of VB, especially for small amounts of
training data.
7.3.7
 Determination of HMM topology
Once a clustered-state model structure is obtained, acoustic model selection is
completed by determining the number of mixture components per state. GMMs
include latent variables, and their determination requires the VB Baum–Welch
algorithm and computation of the latent variable part of the variational lower
bound, unlike the clustering triphone HMM states in Section 7.4.5. Therefore,
this section deals with determination of the number of GMM components per
state by considering the latent variable effects. Then, the effectiveness of VB
model selection in latent variable models is confirmed (Jitsuhiro &
Nakamura 2004) for the successive state splitting algorithm, and the
effectiveness of VB model selection for GMMs is re-confirmed (Valente &
Wellekens 2003). In general, there are two methods for determining the number
of mixture components. With the first method, the number of mixture
components per state is the same for all states. The objective function 
 is
calculated for each number of mixture components, and the number of mixture

components that maximizes the total 
 is determined as being the appropriate
one (fixed-number GMM method). With the second method, the number of
mixture components per state can vary by state; here, Gaussians are split and
merged to increase 
 and determine the number of mixture components in
each state (varying-number GMM method). A model obtained by the varying-
number GMM method is expected to be more accurate than one obtained by the
fixed-number GMM method, although the varying-number GMM method
requires more computation time.
We require the variational lower bound for each state to determine the
number of mixture components. In this case, the state alignments vary and states
are expressed as GMMs. Therefore, the model includes latent variables and the
component 
 cannot be disregarded, unlike the case of triphone HMM state
clustering. However, since the number of mixture components is determined for
each state and the state alignments do not change greatly, the contribution of the
state transitions to the objective function is expected to be small, and can be
ignored. Therefore, the objective function 
 for a particular state j is
represented from Eqs. (7.126) and (7.135) as follows:
(7.187)
where 
 is represented by removing the HMM terms in Eq. (7.126) as
follows:
(7.188)
Similarly, 
 is also represented as follows:

(7.189)
Therefore, with the fixed-number GMM method, the total 
 is obtained by
summing up all states’ 
, which determines the number of mixture
components per state. With the varying-number GMM method, the change of 
 per state is compared after merging or splitting the Gaussians, which also
determines the number of mixture components. The number of mixture
components is also automatically determined by using the BIC/MDL objective
function (Chen & Gopinath 1999, Shinoda & Iso 2001). However, the BIC/MDL
objective function is based on the asymptotic condition and cannot be applied to
latent models in principle. On the other hand, the variational lower bound
derived by VB does not need the asymptotic condition and can determine an
appropriate model structure with latent variables.
Table 7.2 shows experimental results for automatic determination of the
acoustic model topology by using VB and the conventional heuristic approach
that determines the model topology by evaluating ASR performance on
development sets. Note that VB was only used for the model topology
determination, and the other procedures (e.g., training and decoding) were
performed by using the conventional (ML) approaches. Therefore, Table 7.2
simply shows the effectiveness of the model selection. We used two tasks based
on read speech recognition of news articles, JNAS (Shikano, Kawahara,
Kobayashi et al. 1999) and WSJ (Paul & Baker 1992), an isolated word speech
recognition task (JEIDA 100 city name recognition), and a lecture speech
recognition task, CSJ (Furui, Maekawa & Isahara 2000). Table 7.2 provides the
ASR performance of the determined model topology with the number of total
HMM states and a mixture component in an HMM state, where we used the
same number of mixture component for all states. In the various ASR tasks, VB
achieved comparable performance to the conventional method by selecting
appropriate model topologies without using a development set. Thus, these

experiments proved that the VB model selection method can automatically
determine an appropriate acoustic model topology with a comparable
performance to that obtained by using a development set.
Table 7.2 Automatic determination of acoustic model topology.
Read speech
Read speech
Isolated word
Lecture
(JNAS)
(WSJ)
(JEIDA)
(CSJ)
VB
91.7 %
91.3 %
97.9 %
74.5 %
# states
912
2504
254
1986
# components
40
32
35
32
ML + Dev. Set
91.4 %
91.3 %
98.1 %
74.2 %
# states
1000
7500
1000
3000
# components
30
32
15
32
7.4   Structural Bayesian linear regression for hidden
Markov model
As discussed in Section 3.5, a Bayesian treatment of the affine transformation
parameters of CDHMM is an important issue to improve the generalization
capability of the model adaptation. While the regression tree used in the
conventional maximum likelihood linear regression (MLLR) can be considered
one form of prior knowledge, i.e., how various Gaussian distributions are
related, another approach is to explicitly construct and use prior knowledge of
regression parameters in an approximated Bayesian paradigm.
For example, maximum a-posteriori linear regression (MAPLR) (Chesta,
Siohan & Lee 1999) replaces the ML criterion with the MAP criterion
introduced in Chapter 4 in the estimation of regression parameters. Quasi-Bayes
linear regression (Chien 2002) also replaces the ML/MAP criterion with a quasi-
Bayes criterion. With the explicit prior knowledge acting as a regularization
term, MAPLR appears to be less susceptible to the overfitting problem. The
MAPLR is extended to the structural MAP (SMAP) (Shinoda & Lee 2001) and
the structural MAPLR (SMAPLR) (Siohan, Myrvoll & Lee 2002), both of which
fully utilize the Gaussian tree structure used in the model selection approach to
efficiently set the hyperparameters in prior distributions. In SMAP and

SMAPLR, the hyperparameters in the prior distribution in a target node are
obtained from the statistics in its parent node. Since the total number of speech
frames assigned to a set of Gaussians in the parent node is always larger than
that in the target node, the statistics obtained in the parent node are more reliable
than those in the target node, and these can be good prior knowledge for
transformation parameter estimation in the target node.
Another extension of MAPLR is to replace MAP approximation by a fully
Bayesian treatment of latent models, using VB. This section employs VB for the
linear 
regression 
problem 
(Watanabe 
& 
Nakamura 
2004, 
Yu 
&
Gales 2006, Watanabe, Nakamura & Juang 2013), but we focus on model
selection and efficient prior utilization at the same time, in addition to estimation
of the linear transformation parameters of HMMs proposed in previous work
(Watanabe & Nakamura 2004, Yu & Gales 2006). In particular, we consistently
use the variational lower bound as the optimization criterion for the model
structure and hyperparameters, in addition to the posterior distributions of the
transformation parameters and the latent variables. As we discussed in
Section 7.2, since this optimization leads the approximated variational posterior
distributions to the true posterior distributions theoretically in the sense of
minimizing Kullback–Leibler divergence between them, the above consistent
approach 
leads 
to 
improved 
generalization 
capability 
(Neal 
&
Hinton 1998, Attias 1999, Ueda & Ghahramani 2002).
This section provides an analytical solution to the variational lower bound by
marginalizing all possible transformation parameters and latent variables
introduced in the linear regression problem. The solution is based on a variance-
normalized representation of Gaussian mean vectors to simplify the solution as
normalized domain MLLR. As a result of variational calculation, we can
marginalize the transformation parameters in all nodes used in the structural
prior setting. This is a part of the solution of the variational message passing
algorithm (Winn & Bishop 2006), which is a general framework of variational
inference in a graphical model. Furthermore, the optimization of the model
topology and hyperparameters in the proposed approach yields an additional
benefit in the improvement of the generalization capability. For example, this
approach infers the linear regression without controlling the Gaussian cluster
topology and hyperparameters as the tuning parameters. Thus linear regression
for HMM parameters is accomplished without excessive parameterization in a
Bayesian sense.

7.4.1
 Variational Bayesian linear regression
This section provides an analytical solution for Bayesian linear regression by
using a variational lower bound. The previous section only considers a
regression matrix in leaf node 
, but we also consider a regression matrix
in leaf or non-leaf node 
 in the Gaussian tree given model structure M.
Then 
we 
focus 
on 
a 
set 
of 
regression 
matrices 
in 
all 
nodes 
, instead of 
, and marginalize 
 in a
Bayesian manner. This extension involves the structural prior setting as proposed
in SMAP and SMAPLR (Shinoda & Lee 2001, Siohan et al. 2002, Yamagishi,
Kobayashi, Nakano et al. 2009).
In this section, we mainly deal with:
prior distribution of model parameters 
;
true posterior distribution of model parameters and latent variables 
;
variational posterior distribution of model parameters and latent variables 
;
generative model distribution 
.
Note that the prior and generative model distributions are given, as shown in the
generative process of Algorithm 12, and we obtain the variational posterior
distribution, which is an approximation of the true posterior distribution.
7.4.2
 Generative model
As discussed in Section 3.5, the generative model distribution with the
expectation with respect to the posterior distributions of latent variables is
represented as follows:
(7.190)
where 
 is the generative model distribution of the transformed

HMM parameters with transformed mean vectors 
. We use 
 based on the
following variance normalized representation:
(7.191)
 is the Cholesky decomposition matrix of 
, and 
 is
obtained based on the initial mean vector 
. This representation makes the
calculation simple.5
7.4.3
 Variational lower bound
With regard to variational Bayesian approaches, we first focus on the following
marginal log-likelihood 
 with a set of HMM parameters Θ, a set
of hyperparameters Ψ, and a model structure:6, 7
(7.192)
 is a prior distribution of transformation matrices 
. In the
following explanation, we omit Θ, M, and Ψ in the prior distribution and
generative model distribution for simplicity, i.e., 
 
 
,
and 
 
 
.
Similarly to Eq. (7.15), since the variational Bayesian approach focuses on
the variational lower bound of the marginal log likelihood 
 with a set of
hyperparameters Ψ and a model structure M, Eq. (7.192) is represented as
follows:

(7.193)
The inequality in Eq. (7.193) is supported by the Jensen’s inequality in
Eq. (7.10). 
 is an arbitrary distribution, and is optimized by using a
variational method to be discussed later. For simplicity, we omit M, Ψ, and O
from the distributions. As discussed in Section 7.1, the variational lower bound
is a better approximation of the marginal log likelihood than the auxiliary
functions of maximum likelihood EM and maximum a-posteriori EM algorithms
that point-estimate model parameters, especially for small amount of training
data. Therefore, the variational Bayes can mitigate the sparse data problem that
the conventional approaches must resolve.
The variational Bayes regards the variational lower bound 
 as an
objective function for the model structure and hyperparameter, and an objective
functional for the joint posterior distribution of the transformation parameters
and latent variables (Attias 1999, Ueda & Ghahramani 2002). In particular, if we
consider the true posterior distribution 
 (we omit conditional
variables M and Ψ for simplicity), we obtain the following relationship:
(7.194)
This equation means that maximizing the variational lower bound 
 with
respect to 
 corresponds to minimizing the KL divergence between 
 and 
 indirectly. Therefore, this optimization leads to
finding 
, which approaches the true posterior distribution.8
Thus, in principle, we can straightforwardly obtain the (sub) optimal model
structure, hyperparameters, and posterior distribution, as follows:

(7.196)
These optimization steps are performed alternately, and finally lead to local
optimum solutions, similar to the EM algorithm. However, it is difficult to deal
with the joint distribution 
 directly, and we propose factorizing them by
utilizing a Gaussian tree structure. In addition, we also set a conjugate form of
the prior distribution 
. This procedure is a typical recipe of VB to make a
solution mathematically tractable similarly to that of the classical Bayesian
adaptation approach.
Structural prior distribution setting in a binary tree
We utilize a Gaussian tree structure to factorize the prior distribution 
.
We consider a binary tree structure, but the formulation is applicable to a general
non-binary tree. We define the parent node of i as 
, the left child node of i as 
, and the right child node of i as 
, as shown in Figure 7.1, where a
transformation matrix is prepared for each corresponding node i. If we define 
 as the transformation matrix in the root node, we assume the following
factorization for the hierarchical prior distribution 
:
(7.197)
To make the prior distribution a product form in the last line of Eq. (7.197), we
define 
. As seen, the effect of the transformation matrix in a
target node propagates to its child nodes.

Figure 7.1 Binary tree structure with transformation matrices. If we focus on
node i, the transformation matrices in the parent node, left child node, and
right child node are represented as 
, 
, and 
, respectively.
This hierarchical prior setting is based on an intuitive assumption that the
statistics in a target node are highly correlated with the statistics in its parent
node. In addition, since the total number of speech frames assigned to a set of
Gaussians in the parent node is always larger than that in the target node, the
statistics obtained in the parent node are more reliable than in the target node,
and these can be good prior knowledge for the transformation parameter
estimation in the target node.
With a Bayesian approach, we need to set a practical form of the above prior
distributions. A conjugate distribution in Section 2.1.4 is preferable as far as
obtaining an analytical solution is concerned, and we set a matrix variate
Gaussian distribution similar to maximum a-posteriori linear regression
(MAPLR (Chesta et al. 1999)). A matrix variate Gaussian distribution is defined
in Appendix C.9 as follows:
(7.198)
where 
 is a normalization constant defined as:
(7.199)

 is a 
 location matrix, 
 is a 
 symmetric
scale matrix, and 
 is a 
 symmetric scale matrix. The term 
represents correlation of column vectors, and 
 represents correlation of raw
vectors. These are hyperparameters of the matrix variate Gaussian distribution.
There are many hyperparameters to be set, and this makes the implementation
complicated. In this section, we try to find another conjugate distribution with
fewer hyperparameters than Eq. (7.198). To obtain a simple solution for the final
analytical results, we use a spherical Gaussian distribution that has the following
constraints on 
 and 
:
(7.200)
where 
 is the 
 identity matrix and 
 indicates a precision
parameter. Then Eq. (7.198) can be rewritten as follows:
(7.201)
where 
 is a normalization factor, and is defined as
(7.202)
This approximation means that matrix elements do not have any correlation
with each other. This can produce simple solutions for Bayesian linear
regression.9
Based on the spherical matrix variate Gaussian distribution, the conditional
prior distribution 
 in Eq. (7.197) is obtaining by setting the location
matrix as the transformation matrix 
 in the parent node with the precision
parameter 
 as follows:

(7.204)
Note that in the following sections 
 and 
 are marginalized. In addition,
we set the location matrix in the root node as the deterministic value of 
. 
Since 
 
from 
Eq. 
(7.191), 
this
hyperparameter setting means that the initial mean vectors are not changed if we
only use the prior knowledge. This makes sense in the case of a small amount of
data by fixing the HMM parameters as their initial values; this in a sense also
inherits the philosophical background of Bayesian adaptation, although the
objective function has been changed from a-posteriori probability to a lower
bound of the marginal likelihood. Therefore, we just have 
as a set of hyperparameters Ψ, which will also be optimized in our framework.
Algorithm 12 Generative process of structural Bayesian transformation of
CDHMM
Require:  Ψ and Θ 
1:  Draw 
 from 
 
2:  Update 
 from transformation matrices in leaf nodes 
 
3:  Draw O from CDHMM with 
Variational calculus
In VB, we also assume the following factorization form for the posterior
distribution 
:
(7.205)
Then, from the general variational calculation for 
 with respect to 
 based on Eq. (7.25), we obtain the following (sub) optimal solution for 
:

(7.206)
 means a set of transformation matrices at a set of nodes for 
 that does
not include 
, i.e.,
(7.207)
Then, by using Eqs. (7.197) for 
 and (7.205) for 
, we can rewrite
the equation, as follows:
(7.208)
Note that the second term depends on two nodes  and 
, and the expectation
over  is not trivial. In this expectation, we can consider the following two cases
of variational posterior distributions:
1) Leaf node
We first focus on the initial term of Eq. (7.208). If i is a leaf node, we can
disregard the expectation with respect to 
 in the nodes other than
the parent node 
 of the target leaf node. Thus, we obtain the following simple
solution:
(7.209)

2) Non-leaf node (with child nodes)
Similarly, if i is a non-leaf node, in addition to the parent node 
 of the target
node, we also have to consider the child nodes 
 and 
 of the target node for
the expectation, as follows:
(7.210)
(7.211)
(7.212)
(7.213)
In both cases, the posterior distribution of the transformation matrix in the target
node depends on those in the parent and child nodes. Therefore, the posterior
distributions are iteratively calculated. This inference is known as a variational
message passing algorithm (Winn & Bishop 2006), and Eqs. (7.209)–(7.213) are
specific solutions of the variational message passing algorithm to a binary tree
structure. The next section provides a concrete form of the posterior distribution
of the transformation matrix.
Posterior distribution of transformation matrix
We first focus on Eq. (7.213), which is a general form of Eq. (7.209) that has
additional terms based on child nodes to Eq. (7.209). Equation (7.213) is based
on the expectation with respect to 
 and 
. The term with 
 is
represented as the following expression similar to Eqs. (3.168) and (3.161):

(7.214)
This equation is calculated from the sufficient statistics (
, 
, 
, and 
 in
Eqs. (3.169) and (3.167)), that is
(7.215)
(7.216)
These are computed by the VB-E step (e.g., 
), which is
described in the next section. This equation form means that the term can be
factorized by node i. This factorization property is important for the following
analytic solutions and algorithm. Actually, by considering the expectation with
respect to 
, we can integrate out the terms that do not depend on 
, as follows:
(7.217)

Thus, we can obtain the simple quadratic form for this expectation.
Next, we consider Eq. (7.210). Since we use a conjugate prior distribution, 
 is also represented by the following matrix variate Gaussian distribution
as the same distribution family with the prior distribution:
(7.218)
Note that the posterior distribution has a unique form in that the first covariance
matrix is an identity matrix while the second is a symmetric matrix. We discuss
this form with the analytical solution, later.
By substituting Eqs. (7.197) and (7.218) into Eq. (7.210), Eq. (7.210) is
represented as follows:
(7.219)
To solve the integral, we use the following matrix distribution formula:
(7.220)
Then, by using the concrete form of the prior distribution in Eq. (7.201) and by
disregarding the terms that do not depend on 
, Eq. (7.219) can be solved as
the logarithmic function of the matrix variate Gaussian distribution that has the
posterior distribution parameter 
 as a hyperparameter:

(7.221)
Similarly, Eqs. (7.211) and (7.212) are solved as follows:
(7.222)
Thus, the expected value terms of the three prior distributions in Eq. (7.210) are
represented as the following matrix variate Gaussian distribution:
(7.223)
It is an intuitive solution, since the location parameter 
 is represented as a
linear interpolation of the location values of the posterior distributions in the

parent and child nodes. The precision parameters control the linear interpolation
ratio.
Similarly, we can also obtain the expected value term of the prior term in
Eq. (7.209), and we summarize the prior terms of the non-leaf and leaf node
cases as follows:
(7.224)

where
(7.225)
Thus, the effect of prior distributions becomes different depending on whether
the target node is a non-leaf node or leaf node. The solution is different from our
previous solution (Watanabe, Nakamura & Juang 2011), since the previous
solution does not marginalize the transformation parameters in non-leaf nodes.
In the Bayesian sense, this solution is stricter than the previous solution.
Based on Eqs. (7.214) and (7.224), we can finally derive the quadratic form
of 
 as follows:
(7.226)
where we disregard the terms that do not depend on 
. Thus, by defining the
following matrix variables:

(7.227)
we can derive the posterior distribution of 
 analytically. The analytical
solution is expressed as
(7.228)

where
(7.229)
The posterior distribution also becomes a matrix variate Gaussian distribution,
since we use a conjugate prior distribution for 
. From Eq. (7.227), 
 are
linearly interpolated by hyperparameter 
 and the first-order statistics of the
linear regression matrix 
. 
 controls the balance between the effects of the
prior distribution and adaptation data. This solution is the M-step of the VB–EM
algorithm, and corresponds to that of the ML–EM algorithm in Section 3.5.
Compared with Eq. (7.201), Eq. (7.228) keeps the first covariance matrix as
a diagonal matrix, while the second covariance matrix 
 has off-diagonal
elements. This means that the posterior distribution only considers the
correlation between column vectors in W. This unique property comes from the
variance normalized representation introduced in Section 3.5, which makes
multivariate Gaussian distributions in HMMs uncorrelated, and this relationship
is taken over to the VB solutions.
Although the solution for a non-leaf node would make the prior distribution
robust by taking account of the child node hyperparameters, this structure makes
the dependency of the target node on the other linked nodes complex. Therefore,
in the implementation step, we approximate the hyperparameters of the posterior
distribution for a non-leaf node to those for a leaf node by 
 and 
in the Eq. (7.225), and this makes an algorithm simple.
The next section explains the E-step of the VB–EM algorithm, which
computes sufficient statistics 
, 
, 
, and 
 in Eqs. (3.169) and (3.167).
These are obtained by using 
, of which mode 
 is used for the Gaussian
mean vector transformation.
Posterior distribution of latent variables
From the variational calculation of 
 with respect to 
 based on

Eq. (7.25), we also obtain the following posterior distribution:
(7.230)
By using the factorization form of the variational posterior (Eq. (7.205)), we can
disregard the expectation with respect to the variational posteriors other than that
of the target node i. Therefore, to obtain the above VB posteriors of latent
variables, we have to consider the following integral:
(7.231)
Since the Gaussian mean vectors are only updated in the leaf nodes, node i in
this section is regarded as a leaf node. By substituting Eqs. (7.228) and (3.162)
into Eq. (7.231), the equation can be represented as:
(7.232)

where
(7.233)
The analytical result is almost equivalent to the E-step of conventional MLLR,
which means that the computation time is almost the same as that of the
conventional MLLR E-step.
We derive the posterior distribution of latent variables 
, introduced in
Section 7.4.3, based on the VB framework. In this derivation, we omit indexes i,
k, and t for simplicity. By substituting the concrete form (Eq. (3.162)) of the
multivariate Gaussian distribution into Eq. (7.231), the equation can be
represented as:
(7.234)
where we use the following equation for the normalization term:
(7.235)
Let us now focus on the quadratic form 
 of Eq. (7.234). By considering 
 in Eq. (3.164), 
 can be rewritten as follows:
(7.236)

where we use the fact that the trace of the scalar value is equal to the original
scalar value and the cyclic property of the trace in Appendix B:
(7.237)
(7.238)
We also define 
 matrix R, 
 matrix Y, and 
 matrix U in Eq. (7.236) as follows:
(7.239)
The integral of Eq. (7.236) over W can be decomposed into the following
three terms:
(7.240)
where we use the distributive property of the trace in Appendix B:
(7.241)
and use Eq. (7.235) in the third term of the second line in Eq. (7.240).
We focus on the integrals 
 and 
. Since 
 is a scalar value, 
can be rewritten as follows:

(7.242)
Here, we use the following matrix properties:
(7.243)
(7.244)
Thus, the integral is finally solved as
(7.245)

where we use
(7.246)
Similarly, we also rewrite 
 in Eq. (7.240) based on Eqs. (7.243) and
(7.244) as follows:
(7.247)
Thus, the integral is finally solved as
(7.248)

where we use
(7.249)
Thus, we have solved all the integrals in Eq. (7.240).
Finally, we substitute the integral results of 
 and 
 (i.e., Eqs. (7.248)
and (7.245)) into Eq. (7.240), and rewrite Eq. (7.240) based on the concrete
forms of R, Y, and U defined in Eq. (7.239) as follows:
(7.250)
Then, by using the cyclic property in Eq. (7.238) and 
 in Eq. (3.164),
we can further rewrite Eq. (7.240) as follows:
(7.251)
Thus, we obtain the quadratic form with respect to o, which becomes a
multivariate Gaussian distribution form. By recovering the omitted indexes i, k,
and t, and substituting the integral result in Eq. (7.251) into Eq. (7.234), we
finally solve Eq. (7.231) as:

(7.252)
Here, we use the concrete form of the multivariate Gaussian distribution in
Eq. (3.162).
Note that the Gaussian mean vectors are updated in the leaf nodes in this
result, while the posterior distributions of the transformation parameters are
updated for all nodes.
Variational lower bound
By using the factorization form (Eq. (7.205)) of the variational posterior
distribution, the variational lower bound defined in Eq. (7.193) is decomposed as
follows:
(7.253)
The second term, which contains 
, is an entropy value and is calculated at
the E-step in the VB–EM algorithm. The first term (
) is a logarithmic
evidence term for M and 
, and we can obtain an
analytical solution of 
. Because of the factorization forms in
Eqs. (7.205), (7.197), and (7.214), 
 can be represented as the summation
over i, as follows:

(7.254)

where
(7.255)
Note that this factorization form has some dependencies from parent and child
node parameters through Eqs. (7.225) and (7.227). To derive an analytical
solution, we first consider the expectation with respect to 
 only for cluster i.
By substituting Eqs. (3.168), (7.201), and (7.228) into 
, the
expectation can be rewritten, as follows:
(7.256)
If we consider only the leaf node case, by using Eq. (7.227), the expectation of 
 part can be rewritten as:
(7.257)
The result obtained does not depend on 
. Therefore, the expectation with

respect to 
 can be disregarded in 
. Consequently, we can
obtain the following analytical result for the lower bound:
(7.258)
The first line of the result obtained corresponds to the likelihood value given the
amount of data and the covariance matrices of the Gaussians. The other terms
consider the effect of the prior and posterior distributions of the model
parameters. This result is used as an optimization criterion with respect to the
model structure M and the hyperparameters Ψ.
Note that the objective function can be represented as a summation over i
because of the factorization form of the prior and posterior distributions. This
representation property is used for our model structure optimization in Section
7.4.5 for a binary tree structure representing a set of Gaussians used in the
conventional MLLR.
7.4.4
 Optimization of hyperparameters and model structure
In this section, we describe how to optimize hyperparameters Ψ and model
structure M by using the variational lower bound as an objective function. Once
we obtain the variational lower bound, we can obtain an appropriate model
structure and hyperparameters that maximize the lower bound at the same time
as follows:
(7.259)

We use two approximations for the variational lower bound to make the
inference algorithm practical. First, we fix latent variables Z during the above
optimization. Then, 
 in Eq. (7.253) is also fixed for M and Ψ, and can
be disregarded in the objective function. Thus, we can only focus on 
 in
the optimization step, which reduces computational cost greatly, as follows:

(7.260)
This approximation is widely used in acoustic model selection (likelihood
criterion (Odell 1995) and Bayesian criterion (Watanabe et al. 2004)). Second, as
we discussed in Section 7.4.3, the solution for a non-leaf node (Eq. (7.224))
makes the dependency of the target node on the other linked nodes complex.
Therefore, we approximate 
 
 
 by 
 and so on,
where 
 is defined in the next section. Therefore, in the implementation
step, we approximate the posterior distribution for a non-leaf node to that for a
leaf node to make the algorithm simple.
7.4.5
 Hyperparameter optimization
Even though we marginalize all of transformation matrix (
), we still have to
set the precision hyperparameters 
 for all nodes. Since we can derive the
variational lower bound, we can optimize the precision hyperparameter, and can
remove the manual tuning of the hyperparameters with the proposed approach.
This is an advantage of the proposed approach with regard to SMAPLR (Siohan
et al. 2002), since SMAPLR has to hand-tune its hyperparameters corresponding
to 
.
Based on the leaf node approximation for variational posterior distributions,
in addition to the fixed latent variable approximation (
), in
this section the method we implement approximately optimizes the precision
hyperparameter as follows:
(7.261)

where
(7.262)
This approximation makes the algorithm simple because we can optimize the
precision hyperparameter within the target and parent nodes, and do not have to
consider the child nodes. Since we only have one scalar parameter for this
optimization step, we simply used a line search algorithm to obtain the optimal
precision hyperparameter. If we consider a more complex precision structure
(e.g., a precision matrix instead of a scalar precision parameter in the prior
distribution setting Eq. (7.200)), the line search algorithm may not be adequate.
In that case, we need to update hyperparameters by using some other
optimization technique (e.g., gradient ascent).
Model selection
The remaining tuning parameter in the proposed approach is how many clusters
we prepare. This is a model selection problem, and we can also automatically
obtain the number of clusters by optimizing the variational lower bound. In the
binary tree structure, we focus on a subtree composed of a target non-leaf node i
and its child nodes 
 and 
. We compute the following difference based on
Eq. (7.262) of the parent and that of the child nodes:10
(7.263)
This difference function is used for a stopping criterion in a top-down clustering
strategy. This difference function similarly appeared in the model selection of
the context-dependent CDHMM topologies in Sections 6.5 and 7.3.6. Then if the
sign of 
 is negative, the target non-leaf node is regarded as a new leaf node
determined by the model selection in terms of optimizing the lower bound. Next
we prune the child nodes 
 and 
. By checking the signs of 
 for all
possible nodes, and pruning the child nodes when 
 have negative signs, we

can obtain the pruned tree structure, which corresponds to maximizing the
variational lower bound locally. This optimization is efficiently accomplished by
using a depth-first search.
Thus, by optimizing the hyperparameters and model structure, we can avoid
setting any tuning parameters. We summarize this optimization in Algorithms
13, 14, and 15. Algorithm 13 prepares a large Gaussian tree with a set of nodes 
, prunes a tree based on the model selection (Algorithm 14), and transforms
HMMs (Algorithm 15). Algorithm 14 first optimizes the precision
hyperparameters Ψ, and then the model structure M. Algorithm 15 transforms
Gaussian mean vectors in HMMs at the new root nodes in the pruned tree 
obtained by Algorithm 14.
Algorithm 13 Structural Bayesian linear regression.
1:  Prepare an initial Gaussian tree with a set of nodes 
 
2:  Initialize 
 
3:  repeat 
4:     VB E-step 
5:     
 Prune_tree(root node) // prune a tree by model selection 
6:     # of leaf nodes = Transform_HMM(root node) // Transform HMMs in
the pruned tree 
7:  until Total lower bound is converged or a specified number of iterations
has been reached.
Algorithm 14 Prune_tree(node i)
1:  if First iteration then 
2:     
 // These are used as 
3:     Update 
   // hyperparameters of parent nodes 
4:  end if 
5:  if Node i has child nodes then 
6:     
 
7:     Update 
 

8:     
 Prune_tree(node 
) + Prune_tree(node 
) 
 
9:     if 
 then 
10:        Prune child nodes // this node becomes a leaf node 
11:     end if 
12:     return  
 
13:  else 
14:     
 
15:     Update 
 
16:     return  
 
17:  end if
Algorithm 15 Transform_HMM(node i)
1:  if Node i has child nodes then 
2:     return  Transform_HMM(node 
) + Transform_HMM(node 
) 
3:  else 
4:     Update 
 
5:     return  1 
6:  end if
Watanabe et al. (2013) compare the VB linear regression method (VBLR)
with MLLR and SMAPLR, as regards the WSJ, for various amounts of
adaptation data by using LVCSR experiments for the Corpus of Spontaneous
Japanese (CSJ). With a small amount of adaptation data, VBLR outperforms the
conventional approaches by about 1.0% absolute accuracy improvement, while
with a large amount of adaptation data, the accuracies of all approaches are
comparable. This property is theoretically reasonable since the variational lower
bound would be tighter than the EM-based objective function for a small amount
of data, while the lower bound would approach it for a large amount of data
asymptotically. Therefore, it can be concluded that this improvement comes
from the optimization of the hyperparameters and the model structure in VBLR,
in addition to mitigation of the sparse data problem arising in the Bayesian
approach.

7.5   Variational Bayesian speaker verification
This section describes an application of VB to speaker verification (Zhao, Dong,
Zhao et al. 2009, Kenny 2010, Villalba & Brümmer 2011). The main goal of this
approach is to obtain the feature representation that only holds speaker specific
characteristics. As discussed in Section 4.6.2, state-of-the-art speaker
verification systems use the super vector obtained by the GMM–UBM or MLLR
techniques, as a feature. However, the super vector still includes various factors
other than the speaker characteristics with very high dimensional representation.
Use of factor analysis is critical in speaker verification to remove these irrelevant
factors of the super vector and find the lower dimensional representation
(Kenny 2010, Kinnunen & Li 2010, Dehak et al. 2011). In addition, a Bayesian
treatment of the factor analysis yields robust modeling of the speaker
verification. This section discusses a VB treatment of the factor analysis model
by providing a generative model of the super vector (Section 7.5.1), prior
distributions (Section 7.5.2), variational posteriors (Section 7.5.3), and
variational lower bound (Section 7.5.4).
7.5.1
 Generative model
Let 
 be a D dimensional feature vector of n
recordings. Note that 
 is a super vector, and it can be the Gaussian super
vector, vectorized form of the MLLR matrix, or the factor vector obtained after
the initial factor analysis process.11 If we use the Gaussian super vector, the
number of dimensions D would be the product of multiplying the number of
mixture components in GMM (usually 
) and the number of speech
feature dimensions (usually 
 when we use MFCC and delta
features) when we use GMM–UBM, that is
(7.264)
Therefore, D would be 
 40 thousands, and it is much larger than
the number of speech feature dimensions that we are dealing with at CDHMMs.
Note also that the feature 
 is obtained for each recording (utterance), and the
frame level process is performed when super vectors are extracted by GMM–
UBM.

The generative model is represented as follows (Kenny, Boulianne, Ouellet
et al. 2007, Kenny 2010):
(7.265)
where 
 is a global mean vector for the feature vectors and can be
regarded as a bias vector of the feature vectors. Vector 
 is a vector
having a 
 dimensional standard Gaussian distribution, which does not depend
on the recording n, and it can represent stationary speaker characteristics across
recordings. On the other hand, 
 is a vector having a 
 dimensional
standard Gaussian distribution depending on the recording n, and it denotes
channel characteristics changing over a recording. We also define 
. 
 as a D dimensional vector having a Gaussian
distribution with 0 mean vector, and 
 precision matrix.
In this book, m, 
, 
, and R(
)
are regarded as non-probabilistic model parameters and assumed to be estimated
without the Bayesian framework based on ML/MAP. However, Villalba &
Brümmer (2011) deal with these parameters as probabilistic variables, and
provide a fully Bayesian solution of the factor analysis based speaker modeling
by VB. The other probabilistic variables 
, 
, and 
 are generated from
the following Gaussian distributions:
(7.266)
where we assume a zero mean spherical Gaussian distribution for 
 and 
.
The model parameters 
, 
, and 
 are positive, and
the probabilistic treatment of these parameters is discussed later.
In summary, the conditional distribution of O is represented as follows:

(7.267)
The conditional joint distribution of O, 
, and 
 is represented as follows:
(7.268)
where
(7.269)
The following section regards 
, 
, and 
 as probabilistic variables.
7.5.2
 Prior distributions
We provide the conjugate prior distributions for 
, 
, and 
 that are
represented by a gamma distribution, as we discussed in Section 2.1.3.
Kenny (2010) provides a simple hyperparameter setting for each gamma
distribution in Appendix C.11 by using only one hyperparameter for each
distribution, i.e., the model parameters 
, 
, and 
 are generated from the
following prior distributions:
(7.270)

where 
, 
, and 
 (
) are hyperparameters in this model. Since the mean
and variance of the gamma distribution 
 are  and , respectively,
this parameterization means that 
, 
, and 
 have the same mean value
with 1, but the variance values are changed with , , and , respectively. Thus,
we can provide the following concrete forms of the prior distributions for 
, 
, and 
:
(7.271)
In this model, 
 is a set of hidden variables, and the
posterior distribution of each variable can be obtained by using variational
Bayes. Thus, the joint prior distribution given hyperparameters Φ is represented
as follows:
(7.272)
Note that 
 depends on 
, and we cannot fully factorize them. A similar
discussion applies to 
 and 
.
Now, we can provide the complete data likelihood function given
hyperparameters Ψ and Φ based on Eqs. (7.267) and (7.272), and this can be
used to obtain the variational posteriors:

(7.273)
These probabilistic distributions are represented by Gaussian and gamma
distributions. In the following sections we simplify the complete data likelihood
function 
 to 
 to avoid complicated equations. Algorithm 16
provides a generative process for the joint factor analysis speaker model with
Eq. (7.273).
Algorithm 16 Generative process for joint factor analysis speaker model
Require:  
 and 
 1:  Draw 
 from 
 
2:  Draw 
 from 
 
3:  for 
 do 
4:     Draw 
 from 
 
5:     Draw 
 from 
 
6:     Draw 
 from 
 
7:     Draw 
 from 
 
8:  end for
7.5.3
 Variational posteriors
To deal with the variational posteriors, we assume the following factorization
based on the VB recipe:

(7.274)
Section 7.1.3 discusses how we can obtain the following general solution for
approximated variational posteriors:
(7.275)
We focus on the actual solutions for 
, 
, 
, 
, and 
.
: this is calculated by substituting the factors depending on 
 in
Eq. (7.273) into Eq. (7.275) as follows:
(7.276)
Now let us consider the two expectations in the above equation. From the
definition of the multivariate Gaussian distribution in Appendix C.6, we can
obtain the following equation by disregarding the terms that do not depend
on 
:

(7.277)
where we omit the subscript 
 in the expectation, as it is trivial.
Similarly, the rest of the expectations in Eq. (7.276) are also represented as
follows:
(7.278)
Thus, by substituting Eqs. (7.277) and (7.278) into Eq. (7.276), we find that
(7.279)
where we use the trace form definition of the multivariate Gaussian
distribution in Appendix C.6. Thus, 
 is represented as a Gaussian
distribution, and 
 and 
 are posterior hyperparameters obtained as
follows:

(7.280)
Thus, 
the 
hyperparameters 
obtained 
are 
represented 
with prior
hyperparameters Ψ and the expected values of 
 and 
.
: this is similarly calculated by substituting the factors depending
on 
 in Eq. (7.273) into Eq. (7.275) as follows:
(7.281)
The expectations are rewritten as follows:

(7.282)
Thus, 
 is also represented as a Gaussian distribution, and 
 and 
 are posterior hyperparameters obtained as follows:
(7.283)
Note that the hyperparameters obtained are represented with prior
hyperparameters Ψ and the expected values of 
, 
, and 
.
Compared with Eq. (7.280), Eq. (7.283) has a similar functional form, but it
is computed recording by recording, while Eq. (7.280) is computed with
accumulation over every recording n.
: this is also similarly calculated by substituting the factors
depending on 
 in Eq. (7.273) into Eq. (7.275). However, compared with
the previous two cases, the gamma and Gaussian distributions appear in the
formulation as follows:
(7.284)
By using the definition of the gamma distribution in Appendix C.11, this

equation can be rewritten as follows:
(7.285)
Thus, 
 is represented as a gamma distribution, and 
 and 
 are
posterior hyperparameters obtained as follows:
(7.286)
These posterior hyperparameters are obtained with their original prior
hyperparameter 
 and the second-order expectation value of 
.
: this is similarly calculated by substituting the factors depending
on 
 in Eq. (7.273) into Eq. (7.275):
(7.287)
 is represented as a gamma distribution, and 
 and 
 are
posterior hyperparameters, obtained as follows:

(7.288)
These posterior hyperparameters are also obtained with their original prior
hyperparameter 
 and the second-order expectation value of 
, and
these are very similar to the posterior hyperparameters of 
 in
Eq. (7.286).
: finally, this is also calculated by substituting the factors depending
on 
 in Eq. (7.273) into Eq. (7.275) as follows:
(7.289)
 is represented as a gamma distribution, and 
 and 
 are
posterior hyperparameters obtained as follows:
(7.290)
where 
 is a residual vector appearing in the basic equation of the joint
factor analysis in Eq. (7.265), and is represented as:
(7.291)
 is an expectation over both 
 and 
, and defined as follows:

(7.292)
This value is computed by the first-and second-order expectation of 
 and
.
Thus, we can provide the VB posterior distributions of all hidden variables
analytically. Note that these equations are iteratively performed to obtain the
sub-optimal posterior distributions. That is, all the posterior distribution
calculations need the expectation values of hidden variables Z, which can be
computed using the posterior distributions obtained with the previous iteration.
We provide the expectation values of 
 and 
, which are easily obtained by
reference to the expectation formulas of a multivariate Gaussian distribution in
Appendix C.6 and posterior hyperparameters in Eqs. (7.280) and (7.283):
(7.293)
Similarly, we provide the expectation values of 
, 
, and 
, which are
also easily obtained by reference to the expectation formulas of gamma
distribution in Appendix C.11 and posterior hyperparameters in Eqs. (7.286),
(7.288), and (7.290):

(7.294)
Finally, we summarize the analytical results of the posterior distributions 
, as follows:
(7.295)
where posterior hyperparameters are represented as:
(7.296)

(7.297)
Once we obtain the VB posterior distributions, we can also calculate the
variational lower bound, which is discussed in the next section.
7.5.4
 Variational lower bound
As discussed in Section 4.6, speaker verification can be performed by using the
likelihood ratio (Eq. (4.129)):
(7.298)
where 
 means that O is from the hypothesized speaker, while 
 means that
O is not from the hypothesized speaker. Instead of using the likelihood of p(O)
where we neglect the hypothesis index H, using the lower bound, we can treat
speaker verification in a Bayesian sense. That is, we use the variational lower
bound equation (in Eq. (7.5)):
(7.299)
Then we use 
 instead of p(O). From the definition of the variational
lower bound in Eq. (7.4), Eq. (7.299) can be decomposed into the following
terms as follows:

(7.300)
where the second term is the Kullback–Leibler divergence between the
variational posterior and prior distributions.
Let us focus on the first term of the variational lower bound. The conditional
likelihood 
 is represented as the following Gaussian by using Eq. (7.267):
(7.301)
By using Eq. (7.301), 
 is represented as follows:
(7.302)
This expectation is calculated by using the definition of multivariate Gaussian
distribution in Appendix C.6 and the residual vector 
 in Eq. (7.291), as
follows:
(7.303)

Therefore, Eq. (7.302) is calculated as:
(7.304)
Now, let us focus on the KL divergence in Eq. (7.301), which is decomposed
into the following terms based on the factorization forms of Eqs. (7.272) and
(7.274):
(7.305)
The KL divergence is decomposed into the three KL divergence terms. Now,
consider 
, which can be further factorized as follows:
(7.306)
Thus, we need to compute the KL divergences of Gaussian and gamma
distributions, respectively. We use the following formulas, which are the
analytical result of the KL divergence between Gaussian and gamma
distributions:
(7.307)
and

(7.308)
Therefore, by using Eq. (7.307), 
 can be rewritten as
follows:
(7.309)
Similarly, other terms can be obtained by using VB analytically.
Thus, we can obtain the variational lower bound for the joint factor analysis.
This can be used as an objective function of the likelihood test, as discussed
before, and is also used to optimize the hyperparameters Ψ and Φ based on the
evidence approximation, as discussed in Chapter 5.
7.6   Latent Dirichlet allocation
Latent Dirichlet allocation (LDA) (Blei et al. 2003) is known as the popular
machine learning approach which was proposed to build the latent topic model
for information retrieval, document modeling, text categorization, and
collaborative filtering. LDA has been successfully applied for image modeling,
music retrieval, speech recognition, and many others. In general, LDA is an
extended study from a topic model based on probabilistic latent semantic
analysis (PLSA) (Hofmann 1999b, Hofmann 2001), which was addressed in
Section 3.7.3. PLSA extracts the latent semantic information and estimates the
topic parameters according to maximum likelihood (ML) estimation, which
suffers from the over-trained problem. In addition, PLSA could not represent the

unseen words and documents. The number of PLSA parameters increases
remarkably with the number of collected documents. To compensate these
weaknesses, LDA incorporates the Dirichlet priors to characterize the topic
mixture probabilities. The marginal likelihood over all possible values of topic
mixture probabilities is calculated and maximized so as to construct the LDA-
based topic model for document representation. Unseen documents are
generalized by using the LDA parameters, which are estimated according to the
variational Bayes inference procedure. The model complexity is controlled as
the training documents become larger. PLSA and LDA extract the topic
information at document level, and this could be combined in a language model
for speech recognition. In what follows, we first address the construction of an
LDA model from a set of training documents. The optimization objective is
formulated for model training. Then the variational Bayes (VB) inference is
detailed. The variational distribution over multiple latent variables is introduced
to find a VB solution to the LDA model.
7.6.1
 Model construction
LDA provides a powerful mechanism to discover latent topic structure from a
bag of M documents with a bag of words 
, where 
.
The text corpus is denoted by 
. Figure 7.2 is a graphical
representation of LDA. Using LDA, each of the n words 
 is represented by a
multinomial distribution conditioned on the topic 
:
(7.310)
where 
 denotes the multinomial parameters. The topic 
 of word 
 is also generated by a multinomial distribution given parameters 
:
(7.311)
where 
, 
 and K topics are assumed. Importantly, the latent
topics of each document are treated as random variables. We assume that the
multinomial parameters  of K topics are drawn from a Dirichlet distribution,

(7.312)
where 
 is the gamma function and 
 is a K-vector parameter with
component 
. This K-dimensional Dirichlet random variable  lies in the 
-simplex. Thus, LDA parameters 
 contain the K-dimensional
Dirichlet parameters 
 for K topic mixtures  and the topic-
dependent unigram parameters 
. The parameters 
and  are estimated by maximizing the marginal likelihood of a text corpus 
or an N-word document w over the topic mixtures 
 and the topic labels 
:
(7.313)
where
(7.314)
The marginal likelihood is calculated by integrating over continuous variable 
and discrete variable z. Note that LDA parameters 
 are estimated
according to the type-2 maximum likelihood method, as addressed in
Section 5.1.2, because the likelihood function considers all possible values of
topic mixtures in different topics. Latent variables in LDA include topic mixtures
and topic labels 
. Strictly speaking, the LDA model parameters Θ contain 
 while the hyperparameters Ψ should contain 
. But, using LDA, only the
parameters  are integrated out. Without loss of generality, we treat both 
 and 
 as LDA parameters to be optimized from training data w.

Figure 7.2 Representation of latent Dirichlet allocation.
However, in model inference, we should apply the EM algorithm which
involves a calculation of posterior distribution of latent variables 
 given a
document w:
(7.315)
Unfortunately, this distribution is intractable because the normalization term
(7.316)
is an intractable function due to the coupling between  and  in the summation
over latent topics. Here, the superscript v in 
 denotes the component index,
i.e., 
 and 
 for 
. Next, we address the variational Bayes (VB)
inference procedure, which is divided into three steps, namely finding the lower
bound, finding the variational parameters, and finding the model parameters.
7.6.2
 VB inference: lower bound
Although the posterior distribution is intractable for exact inference, a variety of
approximate inference algorithms can be used for LDA including a Laplace
approximation, variational approximation, and Markov chain Monte Carlo. In
this section, a simple convexity-based variational inference is introduced to
implement an LDA model by using Jensen’s inequality. A variational
distribution,

(7.317)
is used as a surrogate for the posterior distribution 
, where the
variational parameters  and  are estimated via an optimization. According to
Jensen’s inequality using the convex function 
, we have
(7.318)
where 
 denotes the expectation operation and variational parameters  and 
are omitted for simplicity. Jensen’s inequality provides us with a lower bound 
 on the logarithm of marginal likelihood, given an arbitrary
variational distribution 
. It can be easily verified that the difference
between the left-hand-side and the right-hand-side of Eq. (7.318) is the KL
divergence between the variational posterior distribution and the true posterior
distribution. We have
(7.319)
Therefore, maximizing the lower bound 
 with respect to  and  is
equivalent to finding the optimal variational distribution 
 with
variational parameters  and , which is closest to the true posterior distribution 
. To do so, the lower bound is expanded by

(7.320)
Using the fact that the expectation of the sufficient statistics is equivalent to the
derivative of the log normalization factor with respect to the natural parameter,
we obtain (Blei et al. 2003)
(7.321)
where Ψ is the first derivative of the log gamma function, also called the di-
gamma function, as used in Eqs. (5.82) and (7.81). The lower bound is further
expanded in terms of variational parameters 
 and model parameters 
by
(7.322)
Typically, finding the lower bound of marginal likelihood in VB inference is

equivalent to performing the VB E-step. However, we need to estimate the
optimal variational parameters  and  to finalize the VB E-step.
7.6.3
 VB inference: variational parameters
The variational Dirichlet parameters  and variational multinomial parameters 
are estimated by maximizing the lower bound in Eq. (7.322). The terms related
to  are collected and arranged thus:
(7.323)
Differentiating with respect to the individual parameter 
, we have
(7.324)
Setting this equation to zero yields the optimal variational parameters
(7.325)
Note that the variational Dirichlet parameters 
 are seen as the surrogate
of  the Dirichlet model parameters 
, which sufficiently reflect the topic
mixture probabilities .
On the other hand, when optimizing the lower bound 
 with

respect to variational multinomial parameters 
, a constrained maximization
problem should be tackled under the constraint
(7.326)
Therefore, we collect the terms in the lower bound related to the individual
variational parameter 
 and form the Lagrangian with the Lagrange multiplier 
:
(7.327)
where the unique word v is selected for 
 such that 
 and 
 for 
. Differentiating this Lagrangian with respect to 
 yields
(7.328)
We set this differentiation to zero and derive the variational parameter 
 which
is written as a function of Lagrange multiplier 
. By substituting the derived
variational parameter 
 into the constraint given in Eq. (7.326), we can
estimate the multiplier and then obtain the optimal variational parameters:
(7.329)

7.6.4
 VB inference: model parameters
After finding the optimal variational parameters 
, we fix these parameters
and substitute the optimal variational distribution 
 to calculate the
updated lower bound 
. The VB M-step treats the variational lower
bound 
 as a surrogate of the intractable marginal log likelihood 
, and estimates the LDA parameters by
(7.330)
First, we deal with the optimization over the conditional multinomial
distributions 
. The lower bound is hereafter
calculated from a text corpus 
. The terms containing model
parameters  are collected and the constraints
(7.331)
are imposed, so as to form the Lagrangian with Lagrange multipliers 
:
(7.332)
We differentiate this Lagrangian with respect to individual 
 and set it to zero
to find the optimal conditional multinomial distributions:

(7.333)
To deal with the optimization over the Dirichlet parameters 
, we
collect the terms in the lower bound which contain 
 and give
(7.334)
Differentiating with respect to individual 
 gives
(7.335)
There is no closed-form solution to the optimal Dirichlet parameter 
, since
the right-hand-side of Eq. (7.335) depends on 
 where 
. We should use
an iterative algorithm to find the 
 optimal parameter vector 
. Here, the
Newton–Raphson optimization is applied to find the optimal parameters by
iteratively performing
(7.336)
where τ is the iteration index, 
 is the 
 gradient vector and H is the 
 Hessian matrix consisting of the second-order differentiations in
different entries:

(7.337)
where 
 denotes a Kronecker delta function. The inverse of Hessian matrix 
 can be obtained by applying the Woodbury matrix inversion. As a
result, LDA model parameters 
 are estimated in a VB M-step. The VB
inference based on an EM algorithm is accordingly completed by maximizing
the variational lower bound of marginal likelihood 
 through
performing a VB E-step for updating variational parameters 
 and a VB M-
step for estimating model parameters 
. The increase of the lower bound is
assured by VB–EM iterations.
7.7   Latent topic language model
LDA is established as a latent topic model which is designed for document
representation by using a bag of words in the form of document wor text corpus 
. LDA has been successfully extended for language modeling and applied for
continuous speech recognition in Tam & Schultz (2005), Chien & Chueh (2011).
However, before the LDA language model, the topic model based on PLSA was
constructed and merged in the n-gram language model (Gildea &
Hofmann 1999), as addressed in Section 3.7.3. But, the PLSA-based topic model
suffers from the weaknesses of poor generalization and redundant model
complexity. The performance of the resulting PLSA language model is limited.
Compared to the PLSA language model, we are more interested in the LDA
language model and its application in speech recognition. In the literature, the
LDA-based topic model is incorporated into the n-gram language model based
on an indirect method (Tam & Schultz 2005) and a direct method (Chien &
Chueh 2011), which are introduced in Section 7.7.1 and Section 7.7.2,
respectively.
7.7.1
 LDA language model
LDA is generally indirect for characterizing the n-gram regularities of a current
word 
 given its history words 
. The word index n in the document model
differs from i in the language model. A document whas N words while a
sentence has T words. The hierarchical Dirichlet language model in Section 5.3
was presented as an alternative to language model smoothing (MacKay &

Peto 1995). In Yaman, Chien & Lee (2007), the hierarchical Dirichlet priors
were estimated by a maximum a-posteriori method for language model
adaptation. These two methods have adopted the Dirichlet priors to explore the
structure of a language model from lower-order n-gram to higher-order n-gram.
There was no topic-based language model involved. In Wallach (2006), the LDA
bi-gram was proposed by considering a bag of bi-gram events from the collected
documents in construction of an LDA. This LDA bi-gram was neither derived
nor specifically employed for speech recognition. The basic LDA model ignores
the word order and is not in accordance with sentence generation in speech
recognition. In Tam & Schultz (2005, 2006), the LDA model parameters 
were calculated from training documents 
 and then used to estimate the online
topic probabilities or the variational Dirichlet parameters 
. The online
parameters  were estimated by treating all history words in a sentence 
 (Tam
& Schultz 2005), or even the transcription of a whole sentence 
 (Tam &
Schultz 2006) as a single document w.
Similarly to the PLSA n-gram as illustrated in Section 3.7.3, the LDA n-gram
is formed as a soft-clustering model or a topic mixture model, which is
calculated by combining the topic probabilities 
 driven by history
words 
 and the topic-dependent unigrams 
 of
current word 
. The combination is marginalized over different topics:
(7.338)
Here, the topic multinomial distributions can be driven either by the history
words 
 or by the whole sentence words 
. These
multinomial distributions are approximated and proportional to the variational
Dirichlet parameters  as calculated in Eq. (7.325). In this implementation, the
pre-trained model parameters 
 and the online estimated variational
parameters 
 should be calculated to determine the LDA n-gram 
in an online fashion. In (Tam & Schultz 2005), the LDA language model was
improved by further adapting the standard ML-based n-gram model using the
LDA n-gram according to a linear interpolation scheme with a parameter 

:
(7.339)
In Tam & Schultz (2006), the language model adaptation based on the unigram
rescaling was implemented by
(7.340)
Typically, the model parameters 
 in the LDA language model are inferred at
the document level, catching the long-distance topic information but only
indirectly representing the n-gram events.
7.7.2
 Dirichlet class language model
LDA (Blei et al. 2003) builds a hierarchical Bayesian topic model and extracts
the latent topics or clusters from a collection of M documents 
. The
bag-of-words scheme is adopted without considering the word sequence, and so
it is not directly designed for speech recognition where the performance is
seriously affected by the prior probability of a word string 
or the language model p(W). In Bengio, Ducharme, Vincent et al. (2003), the
neural network language model was proposed to deal with the data sparseness
problem in language modeling by projecting the ordered history vector into a
continuous space and then calculating the n-gram language model based on the
multilayer perceptron. MLP involves the error back-propagation training
algorithm based on the least-squares estimation, which is vulnerable to the
overfitting problem (Bishop 2006). Considering the topic modeling in LDA and
the continuous representation of history word sequence 
 in a neural network
language model, the Dirichlet class language model (DCLM) (Chien &
Chueh 2011) is presented to build a direct LDA language model for speech
recognition. For a vocabulary with 
 words, the 
 history words 
 are
first represented by a 
 vector 
 consisting of 
 block
subvectors. Each block is represented by the 1 -to-
 coding scheme with a 
dimensional vector where the vth word of vocabulary is encoded by setting the

vth entry of the vector to be one and all the other entries to be zero. The order of
history words is considered in 
. Figure 7.3 shows the system architecture of
calculating a DCLM 
. A global projection is involved to project
the ordered history vector 
 into a latent topic or class space where the
projection 
 could be either a linear function or a non-linear function. In
the case of a linear function, a projection matrix 
 consisting
of C basis vectors 
 is involved. The class structure of all n-gram events from
training corpus 
 is represented by Dirichlet distributions. The class
uncertainty is compensated by marginalizing the likelihood function over the
Dirichlet priors. The latent structure in the DCLM reflects the class of an n-gram
event rather than the topic in an LDA model. A DCLM is regarded as a kind of
class-based language model, which is inferred by the variational Bayes (VB)
procedure by maximizing the variational lower bound of a marginal likelihood of
training data.
Figure 7.3 System architecture for Dirichlet class language model.
7.7.3
 Model construction
A DCLM acts as a Bayesian class-based language model, which involves the
prior distribution of the class variable. Figure 7.4 is a graphical representation of
a DCLM from a training set of current words and history words 
. The training corpus has H history events in 
. Each
history event 
 has 
 possible predicted words 
. Note that the 
-dimensional discrete history vector is projected to a C-dimensional

continuous class space using the class-dependent linear discriminant function
(7.341)
This function is used as the hyperparameter of a Dirichlet prior for the class
mixture probability 
 or equivalently the class posterior probability of latent
variable 
 given history vector 
:
(7.342)
Thus, the uncertainty of class posterior probabilities 
 is
characterized by a Dirichlet prior distribution:
(7.343)
subject to the constraint 
 or 
. Each word 
 is
generated by the conditional probability 
 given a latent
class 
. The n-gram probability obtained using DCLM is calculated by
marginalizing the joint likelihood over the latent variables including class
mixtures  and C latent classes:

(7.344)
In Eq. (7.344), the integral is calculated over a multinomial variable 
 with Dirichlet prior distribution 
 and is equivalent to
the distribution mean. This integral is seen as a marginalization over different
classes c, and is also obtained as a class mixture model with the class-dependent
unigram probabilities 
 weighted by the class mixture
probabilities 
 driven by the ordered history vector 
.
However, we should estimate DCLM parameters 
 and substitute them into
Eq. (7.344) to calculate the Dirichlet class (DC) n-gram probabilities.
Figure 7.4 Representation of a Dirichlet class language model.
7.7.4
 VB inference: lower bound

Similarly to LDA, DCLM parameters 
 are estimated according to the
type-2 maximum likelihood method where the marginal likelihood over latent
variables is maximized. As seen in Figure 7.4, the latent variables in DCLM
include the continuous values of class mixture probabilities 
 and the
discrete values of class labels 
. The optimization problem is formulated
as
(7.345)
where
(7.346)
The log marginal likelihood is calculated by integrating over the continuous 
and summing the discrete 
. However, directly optimizing
Eq. (7.346) is intractable because of the coupling between 
 and 
 in the
summation over latent classes. The posterior distribution
(7.347)

becomes intractable for model inference. This posterior distribution is calculated
from the n-gram events with the 
 predicted words occurring after the fixed
history words 
,
(7.348)
where 
 denotes the count of an n-gram event. The variational Bayes (VB)
inference procedure is involved to construct the variational DCLM where the
lower bound of marginal likelihood in Eq. (7.346) serves as the surrogate to be
maximized to find the optimal 
.
To do so, a decomposed variational distribution,
(7.349)
is introduced to approximate the true posterior distribution 
. A
graphical representation of the variational DCLM is illustrated in Figure 7.5.
Here, 
 and 
 denote the history-dependent variational
Dirichlet and multinomial parameters, respectively. By referring to Section 7.6.2,
the lower bound on the log marginal likelihood is derived by applying the
Jensen’s inequality and is expanded by

(7.350)
We have applied Eq. (7.321) to find the variational lower bound for DCLM.
Figure 7.5 Representation of a variational Dirichlet class language model.
7.7.5
 VB inference: parameter estimation
A VB–EM algorithm has been developed for inference of DCLM parameters. In
this VB–EM procedure, we first conduct the VB E-step to find the optimal

expectation function or lower bound 
 in Eq. (7.350), or equivalently
to estimate the optimal variational Dirichlet parameters 
 and variational
multinomial parameters 
. Similarly to Section 7.6.3, we collect the terms in
Eq. (7.350) that are related to 
 and maximize these terms, expressed by 
,
with respect to 
 to find the optimal variational Dirichlet parameters:
(7.351)
In addition, when maximizing the lower bound 
 with respect to
the variational multinomial parameters 
, we need to collect all terms related to 
 and solve a constrained optimization problem subject to a constraint for
multinomial distributions:
(7.352)
The Lagrangian 
 given the history-dependent Lagrange multipliers 
 is
arranged and maximized so as to find the optimal variational multinomial
distributions:
(7.353)
where the unique word v is selected for 
 such that 
 and 
 for 
.
The variational lower bound is updated with the optimal variational parameters 
.
On the other hand, in a VB M-step, we fix the variational parameters 
and optimize the updated lower bound to estimate the DCLM model parameters:

(7.354)
In 
estimating 
the 
conditional 
multinomial 
distributions 
, the terms containing model parameters 
 are
collected and the constraints
(7.355)
are imposed to arrange the Lagrangian 
 with C Lagrange multipliers 
.
By solving
(7.356)
and considering the constraints, we estimate the optimal conditional multinomial
distributions
(7.357)
by substituting the updated variational multinomial distributions 
.
However, there is no closed-form solution to optimal DCLM parameter 
. This parameter is used to project the ordered history vector 
 into latent class space. The projected parameter is treated as the Dirichlet
class parameter. We may apply the Newton–Raphson algorithm or simply adopt
the gradient descent algorithm
(7.358)

to derive the optimal Dirichlet class parameter  by using the gradient
(7.359)
Given 
the 
estimated 
model 
parameters 
, 
the 
DCLM 
n-gram 
 in Eq. (7.344) is implemented. The DCLM n-gram could be
improved by interpolating with the modified Kneser–Ney (MKN) language
model, as discussed in Eq. (3.251). However, the performance of DCLM is
limited due to the modeling of Dirichlet classes only inside the n-gram window.
7.7.6
 Cache Dirichlet class language model
In general, the long-distance information outside the n-gram window is not
captured. This weakness can be compensated in the cache DCLM (Chien &
Chueh 2011). The cache DCLM treats all history words 
 as cache memory
and incorporates their class information 
 into language modeling
as
(7.360)
where the marginalization over latent classes c and class mixtures 
 is

performed. We have the posterior distribution
(7.361)
where
(7.362)
The denominator term of this posterior distribution is independent of 
 and
could be ignored in calculating the cache DCLM in Eq. (7.360). For practical
purposes, the summation over 
 is simplified by adopting a single best class
sequence 
, where the best class 
 of word 
 is detected from the
previous 
 words 
 and the best classes 
 corresponding to previous 
 words, namely
(7.363)
A detailed derivation is given in Chien & Chueh (2011).
As a result, the recursive detection from 
 to 
 is done and applied to
approximate the cache DCLM as

(7.364)
Here, the product of the Dirichlet distribution 
 and the multinomial
distribution 
 is a new Dirichlet distribution. Taking the integral in
Eq. (7.364) is equivalent to finding the mean of the new Dirichlet distribution. In
this cache DCLM, we introduce a weighting factor 
 to balance two
terms in the numerator and the denominator and a forgetting factor 
to discount the distant class information. A graphical representation of the cache
DCLM is given in Figure 7.6. The best classes 
 corresponding to
all history words 
 are recursively detected and then merged in
prediction of the next word 
.
Figure 7.6 Representation of a cache Dirichlet class language model.
7.7.7
 System performance
The Wall Street Journal (WSJ) corpus was utilized to evaluate different language

models for continuous speech recognition (CSR). The SI-84 training set was
adopted to estimate the HMM parameters. The feature vector consisted of 12
MFCCs and one log energy and their first and second derivatives. Triphone
models were built for 39 phones and one background silence. Each triphone
model had three states with eight Gaussian components. The 1987-1989 WSJ
corpus with 38M words was used to train the baseline backoff trigrams. A total
of 86K documents and 3M trigram histories were used. The 20K non-verbalized
punctuation, closed vocabularies were adopted. A total of 333 test utterances
were sampled from the November 1992 ARPA CSR benchmark test data. In the
implementation, the baseline trigram was used to generate 100-best lists. Various
language models were interpolated with a baseline trigram using an interpolation
weight, and were employed for N-best rescoring. The neural network LM
(NNLM) (Bengio et al. 2003), class-based LM (Brown et al. 1992), PLSA LM
(Gildea & Hofmann 1999), LDA LM (Tam & Schultz 2005, Tam &
Schultz 2006), DCLM, and cache DCLM (Chien & Chueh 2011) were evaluated
in the comparison.
Table 7.3 lists some examples of latent topics and classes, and the
corresponding frequently used words. The three topics and classes were selected
from LDA LM with 
 and DCLM with 
, respectively. The
frequently used words were identified according to the likelihood of the words
given target topics or classes. We can see that the frequently used words within a
topic or class are semantically close to each other. For some cases, the topically
related words from LDA LM appear independently. These topics are not suitable
for generating natural language. In contrast, DCLM extracts the class
information from n-gram events and performs the history clustering based on
sentence generation. For example, the latent class “In+” denotes the category of
words that follow the preposition “in.” The words “fact,” “addition,” and ”June”
usually follow the word “in,” and appear as frequent words of the same class.
The word order is reflected in the clustering procedure. Table 7.4 reports the
word error rates for baseline LM, NNLM, class-based LM, PLSA LM, LDA LM,
DCLM and cache DCLM with 
 and 
 under different sizes
of training data. The issue of small sample size is examined. It is consistent that
the word error rate is reduced when the amount of training data is increased. In
the case of sparse training data (6M), the topic-based and class-based methods
work well. In particular, the cache DCLM with 
 achieved an error
rate reduction of 12.9% which outperforms the other related methods. When the
number of classes is increased to 
, the improvement is obvious for the

case of large training data (38M).
Table 7.3 Comparison of frequently used words of the latent variables
extracted by LDA LM and DCLM.
Topic/class
Frequently used words in latent topics or classes
LDA LM
Family
toy, kids, theater, event, season, shoe, teen, children’s, plays, films,
sports, magazines, Christmas, bowling, husband, anniversary, girls,
festival, couple, parents,wife, friends
 
 
 
Election
candidates, race, voters, challenger, democrat, state’s, selection,
county, front, delegates, elections, reverend, republicans, polls,
conventions, label, politician, ballots
 
 
 
War
troops, killed, Iraqi, attack, ship, violence, fighting, soldiers, mines,
Iranian, independence, marines, revolution, died, nation, protect,
armed, democracy, violent, commander
 
 
 
DCLM
Quantity
five, seven, two, eight, cents, six, one, nine, four, three, zero, million,
point, percent, years, megabyte, minutes, milligrams, bushels, miles,
marks, pounds, yen, dollars
 
 
 
Business
exchange, prices, futures, index, market, sales, revenue, earnings,
trading, plans, development, business, funds, organization, traders,
ownership, holdings, investment
 
 
 
In+
addition, the, fact, American, October, recent, contrast, Europe, June,
Tokyo, July, March, turn, other, my, Washington, order, Chicago, case,
China, general, which
Table 7.4 Comparison of word error rates for different methods with different
sizes of training data and numbers of classes.
Size of training data
12M
38M

6M
18M
Baseline LM
39.2%
21.3%
15.8%
12.9%
NNLM
35.5%
19.6%
15.0%
12.4%
Class-based LM
35.5%
19.7%
15.0%
12.4%
PLSA LM
36.0%
19.8%
15.0%
12.3%
LDA LM
35.9%
19.7%
14.7%
12.2%
DCLM (C=200)
35.9%
19.6%
14.6%
12.0%
Cache DCLM (C=200)
34.2%
19.3%
14.5%
11.9%
DCLM (C=500)
35.2%
19.2%
14.3%
11.7%
Cache DCLM (C=500)
33.9%
19.0%
14.2%
11.6%
7.8   Summary
This chapter has introduced various applications of VB to speech and language
processing, including CDHMM-based acoustic modeling, acoustic model
adaptation, latent topic models, and latent topic language models. Compared
with the previous inference approximations based on MAP, evidence, and
asymptotic approximations, VB deals with Bayesian inference based on a
distribution estimation without considering the asymptotic property, which often
provides better solutions in terms of consistently using the Bayesian manner. In
addition, the inference algorithm obtained can be regarded as an extension of the
conventional EM type iterative algorithm. This makes implementation easier as
we can utilize existing source codes based on the ML and MAP–EM algorithms.
One of the difficulties in VB is that obtaining the analytical solutions of VB
posterior distributions and variational lower bound is hard due to the
complicated expectation and integral calculations. However, the solutions
provided in this chapter (general formulas and specific solutions for acoustic and
language modeling issues) would cover most of the mathematical analysis in the
other VB applications in speech and language processing. Actually, there have
been various other aspects of VB including speech feature extraction (Kwon,
Lee & Chan 2002, Valente & Wellekens 2004a), voice activity detection
(Cournapeau, Watanabe, Nakamura et al. 2010), speech/speaker GMM
(Valente 2006, Pettersen 2008), speaker diarization (Valente, Motlicek &
Vijayasenan 2010, Ishiguro, Yamada, Araki et al. 2012), and statistical speech
synthesis (Hashimoto, Nankaku & Tokuda 2009, Hashimoto, Zen, Nankaku
et al. 2009). Readers who are interested in these topics could follow these

studies and develop new techniques with the solutions provided in this chapter.
Although VB provides a fully Bayesian treatment, VB still cannot overcome
the problem of local optimum solutions based on the EM style algorithm, and
VB also cannot provide analytical solutions for non-exponential family
distributions. The next chapter deals with MCMC-based Bayesian approaches,
which potentially overcome these problems with a fully Bayesian treatment.
1  It is reasonable to deal with the prior distribution 
 of model parameters given model M instead
of 
, since the actual functional form of model parameters is determined by model M. Conversely, it
is very difficult to consider the prior distribution of model parameters 
 without the model setting.
2  We can set an informative prior distribution for 
 instead of the uniform distribution. Several prior
distributions for model structure are considered in Chapter 8.
3  This section does not explicitly provide the posterior solution of the initial weight, as it is trivial.
4  Again we omit the initial transition parameters for simplicity.
5  Hahm, Ogawa, Fujimoto et al. (2012) discuss the use of conventional MLLR estimation without the
variance normalization in the VB framework and its application to feature-space MLLR (fVBLR).
6  Ψ and M can also be marginalized by setting their distributions. This section point-estimates Ψ and M by
a MAP approach, similar to the evidence approximation in Chapter 5.
7  We can also marginalize the HMM parameters Θ. This corresponds to jointly optimizing HMM and
linear regression parameters.
8  The following sections assume factorization forms of 
 to make solutions mathematically
tractable. However, this factorization assumption weakens the relationship between the KL divergence
and the variational lower bound. For example, if we assume 
, and focus on the
KL divergence between 
 and 
, we obtain the following inequality:
(7.195)
Compared with Eq. (7.194), the relationship between the KL divergence and the variational lower bound
is less direct due to the inequality relationship. In general, the factorization assumption distances optimal

variational posteriors from the true posterior within the VB framework.
9  A matrix variate Gaussian distribution in Eq. (7.198) is also represented by the following multivariate
Gaussian distribution (Dawid 1981):
(7.203)
where 
 is a vector formed by the concatenation of the columns of 
,
and 
 denotes the Kronecker product. Based on this form, a VB solution in this section could be
extended without considering the variance normalized representation used (Chien 2002).
10  Since we approximate the posterior distribution for a non-leaf node to that for a leaf node, the
contribution of the variational lower bounds from the non-leaf nodes to the total lower bounds can be
disregarded, and Eq. (7.263) is used as a pruning criterion. If we do not use this approximation, we just
compare the difference between the values 
 of the leaf and non-leaf node cases in
Eq. (7.258).
11  A Bayesian treatment of the factor analysis in a state-of-the-art speaker verification system can be
performed after the first step of the factor analysis, called i vector analysis (Dehak et al. 2011). That is,
Bayesian factor analysis is often performed for the first-step factor vector (i vector) for each utterance,
instead of the Gaussian super vector. This book discusses Bayesian factor analysis for the Gaussian
super vector for simplicity.

8   Markov chain Monte Carlo
For most probabilistic models of practical interest, exact inference is intractable,
and so we have to resort to some form of approximation. Markov chain Monte
Carlo (MCMC) is another realization of full Bayesian treatment of practical
Bayesian 
solutions 
(Neal 
1993, 
Gilks, 
Richardson 
&
Spiegelhalter 1996, Bishop 2006). MCMC is known as a stochastic
approximation which acts differently from the deterministic approximation
based on variational Bayes (VB) as addressed in Chapter 7. Variational inference
using VB approximates the posterior distribution through factorization of the
distribution over multiple latent variables and scales well to large applications.
MCMC uses the numerical sampling computation rather than solving integrals
and expectation analytically. Since MCMC can use any distributions in principle,
it is capable of wide applications, and can be used for Bayesian nonparametric
(BNP) learning, which provides highly flexible models whose complexity grows
appropriately with the amount of data. Although, due to the computational cost,
the application of MCMC to speech and language processing is limited to small-
scale problems currently, this chapter describes promising new directions of
Bayesian nonparametrics for speech and language processing by automatically
growing models to deal with speaker diarization, acoustic modeling, language
acquisition, and hierarchical language modeling. The strengths and weaknesses
using VB and MCMC are complementary. In what follows, we first introduce
the general background of sampling methods including MCMC and Gibbs
sampling algorithms. Next, the Bayesian nonparametrics are calculated to build a
flexible topic model based on the hierarchical Dirichlet process (HDP) (Teh
et al. 2006). Several applications in speech and language processing areas are
surveyed. GMM-based speaker clustering, CDHMM-based acoustic unit
discovery by using MCMC, and the language model based on the hierarchical
Pitman–Yor process (Teh et al. 2006) are described.

The fundamental problem in MCMC involves finding the expectation of
some function 
 with respect to a probability distribution 
 where the
components of 
 might comprise discrete or continuous variables, which are
some factors or parameters to be inferred under a probabilistic model. In the case
of continuous variables, we would like to evaluate the expectation
(8.1)
where the integral is replaced by summation in the case of discrete variables. We
assume that such expectations are too complicated to be evaluated analytically.
The general idea behind sampling methods is to obtain a set of samples 
 drawn independently from the distribution 
. We may
approximate the integral by a sample mean of function 
 over these samples 
:
(8.2)
Since the samples 
 are drawn from the distribution 
, the estimator  has
the correct mean, i.e., 
. In general, ten to twenty independent samples
may suffice to estimate an expectation. However, the samples 
 may not be
drawn independently. The effective sample size might be much smaller than the
apparent sample size L. This implies that a relatively large sample size is
required to achieve sufficient accuracy.
8.1   Sampling methods
A simple strategy can be designed to generate random samples from a given
distribution. We first consider how to generate random numbers from non-
uniform distributions, assuming that we already have a source of uniformly
distributed random numbers. Let θ be uniformly distributed by 
 over the
interval 
. We transform the values of θ using some function 
 by 
. The distributions of variables θ and y are related by

(8.3)
Taking integrals for both sides of Eq. (8.3), we have
(8.4)
which is the indefinite integral of 
. Thus, 
, meaning that we have to
transform the uniformly distributed random numbers θ using a function 
,
which is the inverse of the indefinite integral of the desired distribution of y.
Figure 8.1 depicts the geometrical interpretation of the transformation method
for generating non-uniformly distributed random numbers. In addition, the
generalization to multiple variables is straightforward and involves the Jacobian
of the transform of variables:
(8.5)
A similar scheme can be applied to draw a multivariate distribution with M
variables.
Figure 8.1 Transformation method for generating non-uniformly distributed
random numbers from probability distribution 
. Function 
 represents
the indefinite integral of 
. Adapted from (Bishop 2006).

8.1.1
 Importance sampling
The technique of importance sampling provides a framework for approximating
the expectation in Eq. (8.1) directly but does not provide the mechanism for
drawing samples from distribution 
. The finite sum approximation to
expectation in Eq. (8.2) depends on being able to draw samples from the
distribution 
. One simple strategy for evaluating expectation function would
be to discretize -space into a uniform grid, and to evaluate the integrand as a
sum of the form
(8.6)
However, the problem with this approach is that the number of terms in the
summation grows exponentially with the dimensionality of . In many cases, the
probability distributions of interest often have much of their mass confined to
relatively small regions of 
 space. Accordingly, uniform sampling is very
inefficient because in high-dimensional space, only a very small proportion of
the samples make a significant contribution to the sum. We would like to sample
the points falling in regions where 
 is large, or where the product 
 is
large.
Suppose we wish to sample from a distribution 
 that is not simple or a
standard distribution. Sampling directly from 
 is difficult. Importance
sampling is based on the use of a proposal distribution 
 from which it is easy
to draw samples. Figure 8.2 illustrates the proposal distribution for importance
sampling. The expectation in Eq. (8.1) is expressed in the form of a finite sum
over samples 
 drawn from 
:

(8.7)
The quantities 
 are known as the importance weights, and they
correct the bias introduced by sampling from the wrong distribution.
Figure 8.2 Proposal distribution 
 for importance sampling in estimation
of expectation of function 
 with probability distribution 
. Adapted
from (Bishop 2006).
Usually, the distribution 
 can only be evaluated up to a normalization
constant, so that 
, where 
 can be evaluated easily and 
 is
unknown. We may use the importance sampling distribution 
 to
determine the expectation function:
(8.8)
where 
. We have
(8.9)

We then obtain
(8.10)

where we define
(8.11)
Clearly, the performance of the importance sampling method highly depends on
how well the sampling distribution 
 matches the desired distribution 
.
8.1.2
 Markov chain
One major weakness in evaluation of expectation function based on the
importance sampling strategy is the severe limitation in spaces of high
dimensionality. We accordingly turn to a very general and powerful framework
called Markov chain Monte Carlo (MCMC), which allows sampling from a large
class of distributions and which scales well with the dimensionality of the
sample space. Before discussing MCMC methods in more detail, it is useful to
study some general properties of Markov chains and investigate under what
conditions a Markov chain can converge to the desired distribution. A first-order
Markov chain is defined for a series of variables 
 such that the
following conditional independence holds for 
:
(8.12)
This Markov chain starts from the probability distribution for initial variable 
 and operates with the transition probability 
. A Markov chain
is called homogeneous if the transition probabilities are unchanged for all m. The
marginal probability for a variable 
 is expressed in terms of the marginal
probabilities over the previous variable 
 in the chain,
(8.13)
A distribution is said to be invariant or stationary with respect to a Markov chain

if each step in the chain keeps the distribution invariant. For a homogeneous
Markov chain with transition probability 
, the distribution 
 is
invariant if it has the following property:
(8.14)
Our goal is to use Markov chains to sample from a given distribution. We can
achieve this goal if we set up a Markov chain such that the desired distribution is
invariant. It is required that for 
, the distribution 
 converges
to the required invariant distribution 
, which is obtained irrespective of the
choice of initial distribution 
. This invariant distribution is also called the
equilibrium distribution. A sufficient condition for an invariant distribution 
is to choose the transition probabilities to satisfy the detailed balance, i.e.
(8.15)
for a particular distribution 
.
8.1.3
 The Metropolis–Hastings algorithm
As discussed in importance sampling, we keep sampling from a proposal
distribution and maintain a record of the current state 
. The proposal
distribution 
 depends on this current state. The sequence of samples 
 forms a Markov chain. The proposal distribution is chosen to be
sufficiently simple to draw samples directly. At each sampling cycle, we
generate a candidate sample 
 from the proposal distribution and then accept
the sample according to an appropriate criterion. In a basic Metropolis algorithm
(Metropolis, Rosenbluth, Rosenbluth et al. 1953), the proposal distribution is
assumed to be symmetric, namely 
 for all values of 
 and 
. The candidate sample is accepted with the probability
(8.16)

where 
 with a readily evaluated distribution 
 and an unknown
normalization value 
. To fulfil this algorithm, we can choose a random number
u with uniform distribution over the unit interval 
 and accept the sample if 
. Definitely, if the step from 
 to 
 causes an increase in the
value of 
, then the candidate point is accepted. Once the candidate sample is
accepted, then 
, otherwise the candidate sample 
 is discarded, 
 is set to 
. The next candidate sample is drawn from the distribution 
. This leads to multiple copies of samples in the final list of samples.
As long as 
 is positive for any values of 
 and 
, the distribution of 
 approaches 
 as 
.
The basic Metropolis algorithm is further generalized to the Metropolis–
Hastings algorithm (Hastings 1970) which is widely adopted in MCMC
inference. This generalization is developed by relaxing the assumption in the
Metropolis algorithm that the proposal distribution is no longer a symmetric
function of its arguments. Using this algorithm, at step τ with current state 
,
we draw a sample 
 from the proposal distribution 
 and then accept it
with the probability
(8.17)
where k denotes the members of the set of possible transitions. For the case of a
symmetric proposal distribution, the Metropolis–Hastings criterion in Eq. (8.17)
is reduced to the Metropolis criterion in Eq. (8.16). We can show that 
 is an
invariant distribution of the Markov chain generated by the Metropolis–Hastings
algorithm by investigating the property of the detailed balance in Eq. (8.15). We
find that
(8.18)
The choice of proposal distribution is important in an MCMC algorithm. For
continuous state spaces, a common choice is a Gaussian centered on the current

state, leading to an important trade-off in determining the variance parameter of
this distribution. If the variance is small, the proportion of accepted transitions is
high, but a slow random walk is taken through the state space. On the other
hand, if the variance parameter is large, the rejection rate is high because the
updated state has low probability 
. Figure 8.3 shows a schematic for
selecting an isotropic Gaussian proposal distribution to sample random numbers
from a correlated multivariate Gaussian distribution. In order to keep the
rejection rate low, the scale ρ of the proposal distribution 
 should be
comparable to the smallest standard deviation 
, which leads to a random
walk so that the number of steps for separating states is of order 
where 
 is the largest standard deviation.
Figure 8.3 Using an isotropic Gaussian proposal distribution (circle) to
sample random numbers for a correlated bivariate Gaussian distribution
(ellipse). Adapted from (Bishop 2006).
8.1.4
 Gibbs sampling
Gibbs sampling (Geman & Geman 1984, Liu 2008) is a simple and widely
applicable realization of an MCMC algorithm which is a special case of the
Metropolis–Hastings algorithm. Consider the distribution of M random variables
 and suppose that we have some initial state for the
Markov chain. Each step of the Gibbs sampling procedure replaces the value of
one of the variables by a value drawn from the distribution of that variable
conditioned on the values of the remaining states. That is to say, we replace the
ith component 
 by a value drawn from the distribution 
, where 
denotes 
 but with 
 omitted. The sampling procedure is repeated by
cycling through the variables in a particular order or in a random order from
some distribution. This procedure samples the required distribution 
, which
should be invariant at each of the Gibbs sampling steps or in the whole Markov
chain. This is because the marginal distribution 
 is invariant and the

conditional distribution 
 is correct at each sampling step. Gibbs
sampling of M variables for T steps follows this procedure:
Initialize 
.
For 
:
Sample 
.
Sample 
. 
   
Sample 
. 
   
Sample 
.
Gibbs sampling can be shown to be a special case of the Metropolis–
Hastings algorithm. Consider the Metropolis–Hastings sampling step involving
the variable 
 in which the remaining variables 
 are fixed. The transition
probability from 
 to 
 is then given by 
, because the
remaining variables are unchanged by the sampling step, and 
. By
using 
, the acceptance probability in the Metropolis–
Hastings algorithm is obtained by
(8.19)
where 
 is applied. This result indicates that the sampling steps in the
Metropolis–Hastings algorithm are always accepted.
Collapsed Gibbs sampling
Collapsed Gibbs sampling (Liu 1994, Griffiths & Steyvers 2004) is a method for
using a marginal conditional distribution where some of variables are integrated
out, instead of sampling. For example, suppose we have a set of variables Λ, the
proposal distribution in the Gibbs sampling is represented as follows:

(8.20)
 is a prior distribution. This integral can be analytically solved when we use
a conjugate prior, and the following sections sometimes use a marginal
conditional distribution.
8.1.5
 Slice sampling
One weakness in the Metropolis–Hastings algorithm is the sensitivity to step
size. If this is too small, the result has slow decorrelation due to random walk
behavior, while if it is too large, the sampling procedure is not efficient due to a
high rejection rate. The slice sampling approach (Neal 2003) provides an
adaptive step size which is automatically adjusted to fit the characteristics of the
distribution. Again, it is required that the unnormalized distribution 
 is
available to be evaluated. Consider the univariate case. Slice sampling is
performed by augmenting θ with an additional variable u and drawing samples
from the joint space of 
. The goal is to sample uniformly from the area
under the distribution given by
(8.21)
where 
. The marginal distribution of θ is given by
(8.22)
To carry out this scheme, we first sample from 
 by sampling from 
 and then neglecting the u values. Alternatively sampling θ and u can be
achieved. Given the value of θ, we evaluate 
 and then sample u uniformly in
the range 
. We then fix u and sample θ uniformly from the “slice”
through the distribution defined by 
. As illustrated in Figure 8.4(a),
for a given value 
, a value of u is chosen uniformly in the region 

, which defines a slice through the distribution, shown by solid
horizontal lines. Because it is infeasible to sample directly from a slice, a new
sample of θ is drawn from a region 
, which contains the
previous value 
, as illustrated in Figure 8.4(b). We want the region to
encompass as much of the slice as possible so as to allow large moves in θ space
while having as little as possible of this region lying outside the slice, because
this makes the sampling less efficient.
Figure 8.4 Slice sampling over a distribution 
 through: (a) finding the
slice containing current sample 
 with 
; and (b) detecting the
region of interest with two end points 
 and 
 so that we can uniformly
draw a new sample 
 within the slice. Adapted from (Bishop 2006).
We start with a region of width w which contains current sample 
 and
then judge if both end points are within the slice. If either end point is within the
slice, the region is extended in this direction by increments of value w until the
end point falls outside the region. A candidate value 
 is then chosen uniformly
from this region. If it lies inside the slice, it forms new sample 
. If it lies
outside the slice, the region is shrunk such that 
 forms an end point. A new
candidate point is drawn uniformly from this reduced region until a value of θ is
found that lies within the slice. When applying slice sampling to multivariate
distributions, we repeatedly sample each variable in turn in the manner of Gibbs
sampling based on a conditional distribution 
.
8.2   Bayesian nonparametrics

The sampling methods mentioned in Section 8.1 are widely employed to infer
the Bayesian nonparametrics (BNP) which are now seen as a new trend in
speech and language processing areas where the data representation is a
particular concern and is extensively studied. The BNP learning aims to deal
with the issue that probabilistic methods are often not viewed as sufficiently
expressive because of a long list of limitations and assumptions on probability
distribution and fixed model complexity. It is attractive to pursue an expressive
probabilistic representation with a less assumption-laden approach to inference.
We would like to move beyond the simple fixed-dimensional random variables,
e.g., multinomial distributions, Gaussians, and other exponential family
distributions, and to mimic the flexibility in probabilistic representations. BNP
methods avoid the restrictive assumptions of parametric models by defining
distributions on function spaces.
Therefore, the stochastic process, containing an indexed collection of
random variables, provides the flexibility to define probability distributions on
spaces of probability distributions. We relax the limitation of finite
parameterizations and consider the models over infinite-dimensional objects
such as trees, lists, and collections of sets. The expressive data structures could
be explored by computationally efficient reasoning and learning through the so-
called combinatorial stochastic processes (Pitman 2006). BNP learning is
accordingly developed from a Bayesian perspective by upgrading the priors in
classic Bayesian analysis from parametric distributions to stochastic processes.
Prior distributions are then replaced by the prior process in BNP inference. The
flexible Bayesian learning and representation is conducted from the prior
stochastic process to the posterior stochastic process. BNP involves the
combinatorics of sums and products over prior and posterior stochastic processes
and automatically learned model structure from the observed data. The model
selection problem could be tackled by BNP learning.
8.2.1
 Modeling via exchangeability
Some of the foundation of Bayesian inference is addressed in this section. The
concept of exchangeability is critical in motivating BNP learning. Consider a
probabilistic model with an infinite sequence of random factors or parameters 
 to be inferred from observation data 
, which
could be either continuous such as the speech features O or discrete such as the

word labels W. We say that such a sequence is infinitely exchangeable if the joint
probability distribution of any finite subset of those random variables is invariant
to permutation. For any N, we have
(8.23)
where π denotes a permutation. The assumption of exchangeability is weaker
than that of independence among random variables. This assumption often better
describes the data we encounter in realization of stochastic processes for BNP
learning. De Finetti’s theorem states that the infinite sequence is exchangeable if
and only if for any N random variables the sequence has the following property
(Bernardo & Smith 2009):
(8.24)
There exists an underlying random measure G and a distribution function P such
that random variables 
 are conditionally independent given G, which is not
restricted to be a finite-dimensional object.
The P lya urn model is a probability model for sequentially labeling the
balls in an urn. Consider an empty urn and a countably infinite collection of
colors. Randomly pick a color according to some fixed distribution 
 and place
a ball having the same color in the urn. For all subsequent balls, either choose a
ball from the urn uniformly and return that ball to the urn with another ball of the
same color, or choose a new color from 
 and place a ball of that color k in the
urn. We express this process mathematically by
(8.25)
where 
 is a parameter of the process and 
 denotes the number of balls
of color k. Even though we define the model by considering a particular ordering
of the balls, the resulting distribution is independent of the order. It can be

proved that the joint distribution 
 is written as a product of
conditionals given in Eq. (8.25) and the resulting expression is independent of
the order of N random variables. The exchangeability in the P lya urn model is
confirmed. Because of this property, by De Finetti’s theorem, the existence of an
underlying probability measure G renders the ball colors conditionally
independent. This random measure corresponds to a stochastic process known as
the Dirichlet process, which is introduced in Section 8.2.2.
The property of exchangeability is essential for an MCMC inference
procedure. Let us consider the joint distribution of  and x given by
(8.26)
which is viewed as the product of a prior in the first term and a likelihood
function in the second term. The first term 
 is modeled by the P
lya urn marginal distributions. In particular, our goal is to sample 
 from
observation data x based on the Gibbs sampling. The problem is to sample a
particular component  while all of the other components are fixed. Because the
joint distribution of 
 is invariant to permutation, we can freely
permute the vector to move 
 to the end of the list. The prior probability of the
last component given all of the preceding components is given by the urn model
as given in Eq. (8.25). We multiply each of the distributions by the likelihood
function 
 and integrate with respect to 
. We assume that the prior
measure 
 and the likelihood function are conjugate so that the integral can be
done in closed form. For each component, the derived result is the conditional
distribution of 
 given the other components and given 
. This is done for
different components 
 and the process iterates. This link between
exchangeability and an efficient inference algorithm is important for BNP
learning.
In addition, it is crucial to realize the P lya urn model for Bayesian speech
and language processing over a speech and text corpus. The ball means a word 
 or a feature vector 
 in the corpus and the ball color indicates the cluster
label of this word or feature vector. This P lya urn model defines a distribution
of acoustic features or word labels which is not fixed in dimensionality and can

be used to induce a distribution on partitions or clusterings. The distribution on
partitions is known as the Chinese restaurant process (Aldous 1985), which is
addressed in Section 8.2.4. The Chinese restaurant process and the P lya urn
model are viewed as the essentials of the BNP model for clustering where the
random partition provides a prior on clusterings and the color associated with a
cell can be represented by a distribution associated with a given cluster.
8.2.2
 Dirichlet process
The Dirichlet process (DP) (Ferguson 1973) plays a crucial role in BNP
learning. It is a stochastic process for a random probability measure G over a
measurable space Ω,
(8.27)
such that, for any finite measurable partition 
, the random vector
 is distributed as a finite-dimensional Dirichlet distribution
with parameters 
, i.e.
(8.28)
with two parameters, a positive scaling parameter or concentration parameter, 
, and a base probability measure, 
. This process is a measure of
measures over space Ω where 
. To realize the Dirichlet process in
Eq. (8.27), an infinite sequence of points 
 is drawn independently from the
base probability measure 
 so that the probability measure of the process is
established by
(8.29)
with probability 1 (Sethuraman 1994). In Eq. (8.29), 
 is an atom or a unit
mass at the point 
 and 
 are the random weights which depend on the
concentration parameter 
. Note that the Dirichlet process G is random in two

ways. One random process is for weights 
 while the other is for locations 
.
We can say that Dirichlet process G in Eq. (8.29) has the Dirichlet marginals as
given in Eq. (8.28). According to De Finetti’s theorem, the DP is seen as the De
Finetti mixture distribution underlying the P lya urn model as addressed in
Section 8.2.1. DP can be presented from the perspectives of the stick-breaking
construction, the P lya urn scheme, and a limit of finite mixture models which is
detailed in Sections 8.2.3, 8.2.4, and 8.2.5 respectively.
8.2.3
 DP: Stick-breaking construction
The stick-breaking construction for DP was presented by Sethuraman (1994). In
general, the DP and the stick-breaking process (SBP) are essential tools in BNP.
Consider the stick-breaking weights 
 on a countably infinite set. We want
to find a distribution of the non-negative mixture weights 
 having the
property
(8.30)
One solution to this problem is provided by a procedure known as “stick-
breaking.” Considering a unit-length stick, we independently draw a proportion 
 in the kth break from a Beta distribution with a concentration parameter 
:
(8.31)
Each break decides a proportion 
 while the proportion of the remaining stick is
. The mixture weight of the first component is 
 and that of the kth
component is determined by
(8.32)
Under this SBP, it is straightforward to show the property of mixture weights
that sum up to one with probability one. Figure 8.5 illustrates an infinite

sequence of segments 
 from SBP. Let us define an infinite sequence of
independent random variables 
 and 
 where
(8.33)
Then, the probability measure G defined in Eq. (8.29) can be shown to be the
measure distributed according to DP
 (Sethuraman 1994). We may
interpret the sequence 
 as a random probability measure on the
positive integers. This measure is formally denoted by the GEM distribution
(Pitman 2002)
(8.34)
Figure 8.5 The stick-breaking process.
8.2.4
 DP: Chinese restaurant process
The second perspective on the DP is provided by the P lya urn scheme
(Blackwell & MacQueen 1973) showing that the draws from DP are both
discrete and exhibit a clustering property. Let 
 be a sequence of
independently and identically distributed (iid) random factors or parameters
drawn from G for individual observations 
 under some
distribution function. The probability model over the infinite sequence is written
as
(8.35)
The factors 
 are conditionally independent given G, and hence are
exchangeable. Consider the successive conditional distributions of 
 of 

given the previous factors 
 of observations 
,
where G has been integrated out. It was shown that these conditional
distributions have the following form (Blackwell & MacQueen 1973):
(8.36)
We can interpret the conditional distributions as a simple urn model where a ball
of a distinct color is associated with each atom 
. The balls are drawn
equiprobably or uniformly. As seen in the second term of Eq. (8.36), a new atom
is created by drawing from 
 with probability proportional to 
. A ball of
new color is added to the urn. Equation (8.36) means that 
 has a positive
probability of being equal to one of the previous draws 
. This
process results in a reinforcement effect: the more often a point is drawn, the
more probable it is to be drawn in the future. To make the clustering more
explicitly, we introduce a new set of variables that represent distinct values of
atoms. Let 
 denote the distinct values taken from previous factors 
 and 
 be the number of customers or values 
 that are sitting at
or are associated with 
 of table or cluster k. Equation (8.36) is re-expressed by
Figure 8.6 The Chinese restaurant process.
(8.37)
From this re-expression, we find that the P lya urn scheme produces a
distribution on partitions which is closely related to the one using a different
metaphor called the Chinese restaurant process (CRP) (Aldous 1985). The
metaphor of CRP is addressed as follows. Consider a Chinese restaurant with an

unbounded number of tables. Each 
 corresponds to a customer 
 who enters
the restaurant. The distinct values 
 correspond to the tables at which the
customers sit. The ith customer sits at the table indexed by 
 with probability
proportional to the number of customers 
 who are already seated there (in this
case we set 
), or sits at a new table with probability proportional to 
(in this case, we increment K, draw 
, and set 
). Figure 8.6
shows the scenario of the CRP where the current customer 
 either chooses an
occupied table from 
 or a new table 
 according to
(8.38)
The CRP probability model in Eq. (8.38) is closely related to the P lya urn
model in Eq. (8.25).
8.2.5
 Dirichlet process mixture model
One of the most important applications of the DP is to explore the nonparametric
prior over the parameters of a mixture model. The resulting model is referred to
as the DP mixture model (Antoniak 1974). Consider the probability model of an
infinite sequence of observations 
 in Eq. (8.35), with graphical
representation as in Figure 8.7(a). The probability measure G can be represented
by using a stick-breaking construction. The factors 
 take on values 
 with
probability 
. Here, an indicator value 
 is introduced to reveal the positive
integer value or cluster index for factor , and it is distributed according to . A
DP mixture model can be represented by the following conditional distributions:
(8.39)
Therefore, we have the mixture model 
 and 
.

Figure 8.7 (a) Dirichlet process mixture model; (b) hierarchical Dirichlet
process mixture model.
Alternatively, the DP mixture model can be derived as the limit of a sequence
of finite mixture models where the number of mixture components is taken to
infinity (Neal 1992). This limiting process provides the third perspective on DP.
Suppose that we have K mixture components. Let 
 denote the
mixture weights. In the limit 
, the vectors  are closely related and
are equivalent up to a random size-biased permutation. We use a Dirichlet prior
on  with symmetric parameters 
. We thus have the following
model:
(8.40)
The corresponding mixture model 
 was shown to have the
property
(8.41)

as 
 for every measurable function 
. The marginal distribution of
the observations 
 approaches that using a DP mixture model.
8.2.6
 Hierarchical Dirichlet process
The spirit of the graphical model based on directed graphs, as mentioned in
Section 2.2, is mainly from that of hierarchical Bayesian modeling, while the
graphical model literature has focused almost exclusively on parametric
hierarchies where each of the conditionals is a finite-dimensional distribution.
Nevertheless, it is possible to build hierarchies in which the components are
stochastic processes. We illustrate how to do this based on the Dirichlet process.
In particular, we are interested in finding solutions to the problems in which
the observations are organized into groups, where the observations are assumed
to be exchangeable both within each group and across groups. For example, in
document representation, the words in each document in a text corpus are
associated with the data in each group from a set of grouped data. We want to
discover the structures of words from a set of training documents. It is important
to find the clusterings of word labels for data representation. Let j index the
groups or documents and i index the observations or words within each group.
We assume that 
 are exchangeable within each group j and also
between groups. The group data 
 in an infinite set of groups 
 are exchangeable.
Assuming that each observation is drawn independently from a mixture
model, each observation 
 is associated with a mixture component k. Let 
denote a parameter or factor specifying the mixture component 
 associated
with the observation 
. The factors 
 are not generally distinct. Let 
denote the distribution of observation 
 given the factor 
. Let 
 denote a
distribution for the factors 
 associated with group j. Due to
exchangeability, the factors are conditionally independent given 
. The
probability model for this stochastic process is expressed by
(8.42)

The hierarchical Dirichlet process (HDP) (Teh et al. 2006) was proposed to
conduct BNP learning of grouped data, in which each group is associated with a
mixture model and we wish to link these mixture models. An HDP is a
nonparametric prior distribution over a set of random probability measures. The
process defines a set of probability measures 
, one for each group j, and a
global probability measure 
 shared for different groups. The nonparametric
priors in HDP are given by the following hierarchy:
(8.43)
The prior random measure of jth group 
 is a DP with a shared base measure 
 for grouped data which is itself drawn from the DP with a base measure H.
Here, the hyperparameters γ and α are the concentration parameters and H
provides the prior distribution for the factors 
. Basically, the distribution 
varies around the prior H with variations governed by γ while the distribution 
over the factors in the jth group deviates from 
 with variations controlled by 
. Analogous to the DP mixture model mentioned in Section 8.2.5, HDP is
feasible to construct the HDP mixture model as shown graphically in Figure
8.7(b). The HDP mixture model is expressed by:
(8.44)
We can see that this model achieves the goal of sharing clusters across
groups by assigning the same parameters or factors to those observations. That
is, if 
, the observations 
 and 
 belong to the same cluster. The
equality of factors is possible because both 
 and 
 are possibly drawn from 
, which is a discrete measure over a measurable partition 
. Since 
from different groups shares the atoms from 
, the observations in different
groups j can be assigned to the same cluster. In what follows, we present the
realization of the HDP from the perspectives of a stick-breaking construction and
a Chinese restaurant process.

8.2.7
 HDP: Stick-breaking construction
Using a stick-breaking construction, the global measure 
 and the individual
measure 
 in HDP are expressed by the mixture models with the shared atoms 
 but different weights 
 and 
, respectively, as given
by:
(8.45)
where atom 
 is drawn from base measure H and weights  are drawn from the
GEM distribution 
. Note that the weights 
 are independent
given  because the 
s are independent given 
.
Let 
 denote a measurable partition and 
 for 
. Here, 
 is a finite partition of the positive integers. For
each group j, we have
(8.46)
Hence, each 
 is independently distributed according to 
. The HDP
mixture model is then represented by:

(8.47)
We may further show the explicit relationship between the elements of  and 
.
The stick-breaking construction for 
 defines the variables 
 as
(8.48)
Also, the stick-breaking construction for a probability measure 
 of
group j is performed by
(8.49)
This completes the stick-breaking construction for the HDP.
8.2.8
 HDP: Chinese restaurant franchise
The Chinese restaurant process for a DP is further extended to the Chinese

restaurant franchise for an HDP, which allows for multiple restaurants sharing a
set of dishes, as depicted in Figure 8.8. The metaphor is as follows. There is a
restaurant franchise which shares the menu across restaurants. At each table of
each restaurant, one dish is ordered from the menu by the first customer who sits
there. This dish is shared among all customers sitting at that table. Different
tables in different restaurants can serve the same dish. In this scenario, the
restaurants correspond to the groups while the customers correspond to the
factors 
 of observations 
. Let 
 denote the dishes in a global
menu which are drawn from H. We introduce the variable, 
, that indicates the
dish served at table t in restaurant j. In this restaurant franchise, we first consider
the conditional distribution for a customer 
, given the previous customers 
 in restaurant j, and 
, where the DP 
 is integrated out. From
Eq. (8.37), we obtain
Figure 8.8 A Chinese restaurant franchise containing three restaurants
(rectangles) with an infinite number of tables (circles) and dishes 
. The
term 
 denotes the ith customer in restaurant j, and 
 is the indicator of a
dish on the tth table of restaurant j. Different dishes 
, 
, and 
 are
marked by different shading patterns.
(8.50)

The notation 
 denotes the number of customers in restaurant j at table t
eating dish k. Marginal count is represented by dot. Thus, 
 denotes the
number of customers in restaurant j at table t. The notation 
 means the
number of tables in restaurant j serving dish k. Thus, 
 represents the number
of tables in restaurant j. This conditional is a mixture or a draw, which can be
obtained by drawing from the terms on the right-hand-side of Eq. (8.50) with the
probabilities given by the corresponding mixture weights for the occupied tables
and an unoccupied table. If a term in the first summation is chosen, we
increment 
 and set 
. If the second term is chosen, we increment 
,
draw 
 and set 
.
Next, we proceed to integrate out 
, which is a DP, and apply Eq. (8.37)
again to find the conditional distribution of a factor 
 given the previous factors
in the different restaurants:
(8.51)
If we draw 
 by choosing the term in the summation on the right-hand-side of
Eq. (8.51), we set 
. If we choose the second term, we increment K, draw
a new 
 and set 
. It is meaningful that the mixture probability
of the first term is proportional to 
, which represents the number of tables
serving dish k, while the probability of the second term is proportional to the
concentration parameter γ of DP 
. This completes the HDP implementation
based on the Chinese restaurant franchise. To obtain samples 
 for each j and i,
we first draw 
 from Eq. (8.50). If a new sample from 
 is needed, we use
Eq. (8.51) to obtain a new sample 
 and set 
.
Moreover, the HDP can be derived from the perspective of infinite limit over
the finite mixture models. This is done by considering the collection of finite
mixture models where L-dimensional  and 
 are used as the global and group-
dependent mixing probabilities, respectively. The HDP finite mixture model is
accordingly expressed by

(8.52)
It can be shown that the limit of this model as 
 approaches the HDP
mixture model. The parametric hierarchical prior for  and 
 in Eq. (8.52) is
also seen as the hierarchical Dirichlet model, which has been applied for
language modeling (MacKay & Peto 1995), as addressed in Section 5.3.
8.2.9
 MCMC inference by Chinese restaurant franchise
The MCMC sampling schemes have been developed for inference of the HDP
mixture model. A straightforward Gibbs sampler based on the Chinese restaurant
franchise can be implemented. To do so, the posterior probabilities for drawing
tables 
 and dishes 
 should be determined. Let 
 be the index of
the factor 
 associated with 
, and let 
 be the index of 
 associated with 
. In the Chinese restaurant franchise, the customer i in restaurant j sits at table 
,
whereas 
table 
t 
in 
restaurant 
j 
serves 
dish 
. 
Let 
, 
, 
, 
 and 
. Notations 
 and 
 denote the vectors k and t with the exception of
components 
 and 
, respectively, and 
 denotes the number of customers in
restaurant j who enjoy the dish 
, but leaving out customer 
. Similarly, we
have 
 and 
. The concentration parameters γ and 
 of DPs
are assumed to be fixed. When implementing HDP using MCMC sampling, we
need to calculate the conditional distribution of 
 under mixture component k
given all data samples except 
,
(8.53)
Here, 
 is a prior distribution from H and is conjugate to the likelihood 
. We can integrate out the mixture component parameter 
 in closed

form. Similarly to Eq. (8.53), the conditional distribution 
 can be
calculated by using 
, the customers at restaurant j sitting at table t. In what
follows, we derive the posterior probabilities for sampling tables 
 and dishes 
which are then used to reconstruct 
s and 
s based on the 
s. The property of
exchangeability is employed in Eqs. (8.50) and (8.51) for 
s and 
s, or
equivalently for s and 
s, respectively.
Sampling t: We want to calculate the conditional distribution of 
 given the
rest of variables 
. This conditional posterior for 
 is obtained by combining
the conditional prior for 
 with the likelihood of generating 
. Basically, the
likelihood function due to 
 given previously occupied table t is expressed by 
, as shown in Eq. (8.53). The likelihood function for a new table 
 can be calculated by integrating out all possible values of 
 using
Eq. (8.51):
(8.54)
where 
 is a prior density of 
 given by
(8.55)
According to Eq. (8.50), the conditional prior probability of taking a previously
occupied table 
 is proportional to 
, while the probability of taking a new
table 
 is proportional to 
. Thus, the conditional posterior
probability for sampling table  is derived from
(8.56)

Sampling k: When the sampled table is a new one, 
, we need to
further order a new dish 
 from the global menu. This is done according to
the conditional posterior probability, which is combined from a conditional prior
for 
 and a likelihood function due to 
. The conditional posterior probability
is then derived from
(8.57)
Because changing 
 actually changes the component membership of all
customers 
 at table t, we can calculate the conditional posterior probability of
ordering dish 
 from
(8.58)
where the likelihood function 
 due to the customers 
 at table t of
restaurant j is used.
8.2.10
 MCMC inference by direct assignment
In the first MCMC procedure based on the Chinese restaurant franchise
representation, the observations are first assigned to some table 
, and the tables
are then assigned to some mixture component 
. This indirect association with
mixture components could be realized by directly assigning mixture components
through a new variable 
 which is the same as 
. The variable 
 indicates
the index of a mixture component corresponding to the customer 
. Using this
direct assignment, the tables are represented only in terms of the number of
tables 
. A bookkeeping scheme is involved. We would like to instantiate and
sample from 
 by using the factorized posterior on 
 across groups. To do
so, an explicit construction for 
 is expressed in the form of

(8.59)
which can be also expressed as
(8.60)
From the expressions in Eq. (8.59), we can see that the values 
 and γ in the
first MCMC inference based on the Chinese restaurant franchise are replaced by
 and 
 in the second MCMC inference based on the respective direct
assignments. In the MCMC inference based on direct assignment, we need to
sample the indicators of mixture components 
 and the numbers of tables 
, according to the corresponding posterior probabilities which are
provided in what follows.
Sampling z: The idea of sampling z is to group together the terms associated
with each k in Eqs. (8.54) and (8.56) so as to calculate the conditional posterior
probability:
(8.61)
Note that we have combined Eqs. (8.54) and (8.56) based on a new variable 
and have replaced 
 with 
 and γ with 
. To fulfil the sampling of z in
Eq. (8.61), we have to further sample m and .

Sampling m: According to the direct assignment of data items to mixture
components z, it is sufficient to sample m and  in place of t and k. To find the
conditional distribution of 
, we consider the conditional distribution of 
under the condition 
. From Eq. (8.50), the prior probability that data item
 is assigned to some table t such that 
 is
(8.62)
while the probability that is assigned to a new table under mixture component k
is
(8.63)
These probabilities in a Gibbs sampler have the equilibrium distribution, which
is the prior probability over the assignment of 
 observations to components
in a DP with concentration parameter 
. The corresponding distribution over
the number of mixture components is the desired conditional distribution of 
which is written as (Antoniak 1974, Teh et al. 2006):
(8.64)
Here, 
 denotes the unsigned Stirling numbers of the first kind, which are
calculated from
(8.65)
with initial conditions 
, 
 for 
 and 
 for 
.
Sampling : Having the samples m and the fixed hyperparameter γ, the 
parameters are sampled from a Dirichlet distribution,

(8.66)
This completes the MCMC inference for the HDP, based on the scheme of direct
assignment of mixture components. It was shown that MCMC inference with the
scheme of direct assignment is better than the scheme using the Chinese
restaurant franchise in terms of convergence speed.
8.2.11
 Relation of HDP to other methods
In general, HDP can be seen as a building block for a variety of speech and
language processing applications. An instance of application is the latent
Dirichlet allocation (LDA) model (Blei et al. 2003), where each entity is
associated not with a single cluster but with a set of clusters. In LDA
terminology, each document is associated with a set of topics. As described in
Section 7.6, an LDA model is constructed as a Bayesian parametric model with a
fixed number of topics. HDP relaxes the limitation of a finite dimension in latent
topic space in LDA and builds the flexible topic model with infinite clusters or
topics. Multiple DPs are used to capture the uncertainty regarding the number of
mixture components. HDP is viewed as a BNP version of an LDA model. The
topics or clusters 
 for the jth document are drawn from the nonparametric
prior 
, while the measures 
 are drawn from a DP with a base measure 
.
This allows the same topics to appear in multiple documents.
There are some other ways to connect multiple DPs. One idea is based on the
nested Dirichlet process (NDP) (Rodriguez, Dunson & Gelfand 2008), which
was proposed to model a collection of dependent distributions by using random
variables as atoms at the higher level and random distributions as atoms at the
lower level. This combinatorial process borrows information across DPs while
also allowing DPs to be clustered. The simultaneous multilevel clustering can be
achieved by such a nested setting. NDP is characterized by
(8.67)
Using HDP, the distributions 
 of different j share the same atoms but assign

them with different weights. However, using NDP, these distributions may have
completely different atoms and weights. By marginalizing over the DPs, the
resulting urn model is closely related to the nested Chinese restaurant process
(nCRP) (Blei, Griffiths & Jordan 2010), which is known as a tree model of
Chinese restaurants. The scenario of nCRP is addressed as follows. A customer
enters the tree at a root Chinese restaurant and sits at a table. This table points to
another Chinese restaurant, where the customer goes to dine the next evening.
The construction is done recursively. Thus, a given customer follows a path
through the tree of restaurants. Through BNP inference of topic hierarchies, a
hierarchical topic model is established (Blei, Griffiths, Jordan et al. 2004). Each
document or customer chooses a tree path of topics while each word in the
document is represented by a mixture model of hierarchical topics along this tree
path. In the following sections, we address some BNP methods which have been
successfully applied for building the speaker diarization system and for
developing the solutions of acoustic and language models to speech recognition
systems.
8.3   Gibbs sampling-based speaker clustering
This section describes an application of MCMC techniques for speech features.
We focus on speaker clustering, as discussed in Section 6.6.2 based on a
Bayesian information criterion (BIC). Section 6.6.2 models a speaker cluster
with a single Gaussian where we assume a stationary property for speech
features within a segment. However, this is actually not correct since it has
various temporal patterns based on linguistic variations, speaking styles, and
noises. Therefore, we model these short-term variations with a GMM, while
speaker cluster characteristics are modeled by a mixture of GMMs to consider
the multi-scale properties of speech dynamics (Moraru, Meignier, Besacier
et al. 2003, Valente 
& 
Wellekens 
2004b, 
Wooters, 
Fung, 
Peskin
et al. 2004, Meignier, Moraru, Fredouille et al. 2006).
An important aspect of this speech modeling technique is the consideration
of the multi-scale property in dynamics within a probabilistic framework. For
example, PLSA (in Section 3.7.3) and LDA (in Section 7.6) are successful
approaches in terms of the multi-scale property. They deal accurately with two
types of scales, namely, word-level and document-level scales (i and m in the
complete data likelihood function of the latent topic model in Eq. (3.293)), based

on a latent topic model (Hofmann 1999a). The approach discussed in this section
is inspired by these successful approaches, and aims to apply a fully Bayesian
treatment to the multi-scale properties of speech dynamics.
There have been several studies on Bayesian speech modeling, e.g., by using
maximum a-posteriori (MAP) in Chapter 4 or variational Bayesian (VB)
approaches in Chapter 7 for speech recognition (Gauvain & Lee 1994, Watanabe
et al. 2004), speaker verification (Reynolds et al. 2000), and speaker clustering
(Valente & Wellekens 2004b). While all of these approaches are based on the
EM-type deterministic algorithm, this section focuses on another method of
realizing fully Bayesian treatment, namely sampling approaches based on
MCMC. The main advantage of the sampling approaches is that they can avoid
local optimum problems in addition to providing other Bayesian advantages
(mitigation of data sparseness problems and capability of model structure
optimization). While their heavy computational cost could be a problem in
realization, recent improvements in computational power and the development
of theoretical and practical aspects related to the sampling approaches allow us
to apply them to practical problems (e.g., (Griffiths & Steyvers 2004, Goldwater
& Griffiths 2007, Porteous, Newman, Ihler et al. 2008) in natural language
processing). Therefore, the aim of this work is to apply a sampling approach to
speech modeling considering the multi-scale properties of speech dynamics.
The following sections formulate the multi-scale GMM by utilizing a Gibbs
sampling approach. In this section, for its educational value, we first describe
Gibbs sampling for a standard GMM. Section 8.3.2 derives the marginal
likelihood of the GMM, which is used for deriving GMM Gibbs samplers in
Section 8.3.3. Section 8.3.4 provides the generative process and a graphical
model of multi-scale GMM for speaker clustering. Based on the analytical
results of GMM Gibbs sampling, Section 8.3.5 derives the marginal likelihood
of the multi-scale GMM, which is used for deriving Gibbs samplers in
Section 8.3.6.
8.3.1
 Generative model
This section considers the two types of observation vector sequences. One is an
utterance- (or segment-) level sequence and the other is a frame-level sequence.
Then, a D dimensional observation vector (e.g., MFCC) at frame t in utterance u

is represented as 
. A set of observation vectors in utterance u is
represented as
(8.68)
 denotes the number of frames at an utterance u.
Next, we assume that the frame-level sequence is modeled by a GMM as
usual, and the utterance-level sequence is modeled by a mixture of these GMMs.
Two kinds of latent variables are involved in multi-scale GMM for each
sequence: utterance-level latent variable 
 and frame-level latent variable 
. Utterance-level latent variables may represent emotion, topic, and
speaking style as well as speakers, depending on the speech variation. The joint
likelihood function of U observation vectors (
) with the
latent 
variable 
sequences 
(
 
and 
) can be expressed as follows:
(8.69)
where 
 denotes the utterance-level mixture weight, and 
denotes the frame-level mixture weight. The terms 
 and 
denote the mean vector and precision matrix parameters of the Gaussian
distribution, s and k denote utterance-level and frame-level mixture indexes,
respectively, and S and K denote the number of speakers and the number of
mixture components, respectively. Therefore, a set of model parameters Θ is
defined as
(8.70)
Note that this is almost equivalent to the following pdf of the GMM in
Section 3.2.4:

(8.71)
The next section first describes Gibbs sampling for the standard GMM.
8.3.2
 GMM marginal likelihood for complete data
We assume a diagonal covariance matrix for the Gaussian distributions as usual,
where the d-d diagonal element of the precision/covariance matrix is expressed
as 
. The following conjugate distributions are used as the prior
distributions of the model parameters:
(8.72)
where 
 are the hyperparameters. 
 and 
 denote
Dirichlet and gamma distributions in Appendices C.4 and C.11, respectively.
In a Bayesian inference framework, we focus on the marginal likelihood for
the complete data. In the complete data case, all of the latent variables are treated
as observations, i.e., the assignments of all the latent variables are hypothesized
to be given in advance. Then, 
 return 0 or 1 based on the
assignment information, and the sufficient statistics of the GMM given all latent
variables V can be represented as follows:
(8.73)
The quantity 
 is a count of frames assigned to k, and 
 and 
 are first-
order and second-order sufficient statistics, respectively. We define a set of these

sufficient statistics as
(8.74)
The subscript V would be omitted when it is obvious. Note that the statistics 
is a positive discrete number while those appearing in the EM algorithm (ML in
Eq. (3.153), MAP in Eq. (4.93), and VB in Eq. (7.67)) are positive continuous
values since the occurrences of latent variables in the EM algorithm are
expectation values based on the posterior probabilities of latent variables.
Based on the sufficient statistics representation in Eq. (8.73), the complete
data likelihood of Eq. (8.71) can be represented by using the product formula of
the Kronecker delta function in Eq. (A.4) (
) as follows:
(8.75)
Since the likelihood function is represented by the exponential distributions
(multinomial and Gaussian distributions), these parameters integrate out, and we
can use collapsed Gibbs sampling, as discussed in Section 8.1.4. The marginal
likelihood for the complete data, 
, is represented by the following
expectations by substituting Eqs. (8.72) and (8.75) into the following integration:
(8.76)
Note that this calculation is similar to those of the MAP auxiliary function in

Section 4.3.5 and variational lower bound in Section 7.3.4. For example, 
 can be rewritten as
(8.77)
This is the same as the MAP auxiliary function of Eq. (4.54), and by analogy,
Eq. (8.77) can be rewritten with the posterior distribution based equation in
Eq. (4.54), as follows:
(8.78)
where 
 is a posterior hyperparameter, which is defined as:
(8.79)
Therefore, the integral is finally solved as follows:
(8.80)

Thus, the integration is calculated analytically, similarly to the calculation of the
variational lower bound. The integral with respect to 
 and 
 in Eq. (8.76) is
also analytically calculated by using Eq. (7.124). First, the expectation is
rewritten as follows:
(8.81)
where posterior hyperparameters 
, 
, 
, and 
 are defined as follows:
(8.82)
Thus, we finally solve all integrals in Eq. (8.76) with use of concrete forms of
the normalization constants of the Dirichlet and gamma distributions in
Appendices C.4 and C.11, as follows:

(8.83)
Below we summarize the posterior hyperparameters, which are obtained from
the hyperparameters of the prior distributions (
) and sufficient statistics
(Eq. (8.73)) as follows:
(8.84)
The marginal likelihood obtained is quite similar to the model parameter part of
the variational lower bound in Eq. (7.126), since both functions are obtained by
integrating out the Gaussian parameters for complete data likelihood. Based on
the marginal likelihood for these complete data, we can calculate the marginal
conditional distribution of 
, as shown below.
8.3.3
 GMM Gibbs sampler
As discussed in Section 8.1.4, a collapsed Gibbs sampler can assign latent
variables by using the marginal conditional distribution. First, from the sum and
product rules, the marginal conditional distribution 
 is represented
as follows:
(8.85)
Here, 
 indicates a set that does not include the tth frame element. Therefore,
by considering the normalization constant, the posterior probability can be
obtained by using Eq. (8.83) as follows:

(8.86)
where 
 is a set of posterior hyperparameters computed given latent
variables of 
. Note that the denominators of 
 have the same
latent variable for all frames except t. To compute 
, we first need to
compute 
, which is a set of sufficient statistics for all frames except t as
follows:
(8.87)
From Eq. (8.73), 
 can be computed by simply subtracting the zeroth-, first-,
and second-order values of 
 as:
(8.88)
Thus, 
 can be obtained by simply adding the zeroth-, first-, and second-
order values of 
 for all 
 as:
(8.89)
 in Eq. (8.86) is defined as follows:

(8.90)
This equation is obtained by canceling out the factors in the numerator and
denominator of Eq. (8.83). Thus, we obtain the Gibbs sampler, which assigns
mixture component k at frame t.
Note that if we use multinomial distributions in LDA instead of Gaussian
distributions, the numerator and denominator of the Gibbs sampler are further
canceled out (see Griffiths & Steyvers (2004)) based on the formula of the
gamma function in Appendix A.4. Actually, 
 in Eq. (8.90) can be similarly
canceled out, while the other factors cannot be canceled out. Therefore, the
computational cost of the Gaussian-based Gibbs sampler is large compared with
LDA, since we need to compute Eq. (8.90) for every k and every frame t.
8.3.4
 Generative process and graphical model of multi-scale
GMM
Based on solution of GMM Gibbs sampling in the previous section, we consider
the Bayesian treatment of this multi-scale GMM for speaker clustering, which is
an extension of GMM. For the conditional likelihood equation in Eq. (8.69), we
again assume a diagonal covariance matrix for the Gaussian distributions. We
also assume that the prior hyperparameters of the GMM parameters 
, 
, and 
 for each s are shared with the parameters of one GMM
(universal background model assumption (Reynolds et al. 2000)), which is used
for speaker and speech recognition (subspace GMM (Povey, Burget, Agarwal
et al. 2010)). Then the following conjugate distributions are used as the prior
distributions of the model parameters:
(8.91)

where 
 are the hyperparameters.
Based on the likelihood function and prior distributions, the generative
process of multi-scale GMM can be expressed in Algorithm 17. The
corresponding graphical model is shown in Figure 8.9. Now we have introduced
multi-scale GMM, the following sections derive a solution for multi-scale GMM
based on Gibbs sampling.
Figure 8.9 Model of multi-scale Gaussian mixture model. The model deals
with two time scales based on frame t and utterance u.
Algorithm 17 Generative process of multi-scale GMM
Require:  
 
1:  Draw h from 
 
2:  for each utterance-level mixture component 
 do 
3:     Draw 
 from 
 
4:     for each frame-level mixture component 
 do 
5:        for each dimension 
 do 
6:           Draw 
 from 
 
7:        end for 

8:        Draw 
 from 
 
9:     end for 
10:  end for 
11:  for each utterance 
 do 
12:     Draw 
 from Mult (h) 
13:     for each frame 
 do 
14:        Draw 
 from Mult (
) 
15:        Draw 
 from 
 
16:     end for 
17:  end for
8.3.5
 Marginal likelihood for the complete data
Similarly to the GMM Gibbs sampler, we prepare 
 given
utterance u, which returns 0 or 1 based on the assignment information. In
addition, we also prepare the assignment information of an utterance-level
mixture with 
. The sufficient statistics of multi-scale GMM
can be represented as follows:
(8.92)
Here 
 is a count of utterances assigned to speaker cluster s, and 
is a count of frames assigned to mixture component k in s. 
 and 
 are first-
order and second-order sufficient statistics, respectively.
Based on the sufficient statistics representation in Eq. (8.92), the complete
data likelihood of Eq. (8.69) can be represented by using the product formula of
the Kronecker delta function in Eq. (A.4) (
) as follows:

(8.93)
The marginal likelihood for the complete data, 
, is represented by
the following expectations by substituting Eqs. (8.91) and (8.93) into the
following integration:
(8.94)
By following the derivations in Section 8.3.2, the expectations are calculated
analytically. This section simply provides the analytical results of the
expectations. By using Eq. (4.45), 
 is solved as:
(8.95)
where  is a posterior hyperparameter, which is defined as:
(8.96)

The other integral with respect to 
 in Eq. (8.94) is also calculated as follows:
(8.97)
where 
 is a posterior hyperparameter, which is defined as:
(8.98)
The integral with respect to 
 and 
 in Eq. (8.94) is also analytically
calculated as follows:
(8.99)
where posterior hyperparameters 
, 
, 
, and 
 are defined as follows:
(8.100)
Thus, we finally solve all the integrals in Eq. (8.94) as follows:

(8.101)
We summarize below the posterior hyperparameters, which are obtained from
the hyperparameters of the prior distributions (
) and sufficient statistics
(Eq. (8.92)) as follows:
(8.102)
Based on the marginal likelihood for these complete data, we can calculate the
marginal conditional distribution of 
 and 
, as shown below.
8.3.6
 Gibbs sampler
We provide a collapsed Gibbs sampler 
 for a frame-
level mixture component k, which has similarities with the GMM Gibbs sampler
in Section 8.3.3. In addition, we also provide a collapsed Gibbs sampler 
 for utterance-level mixture component s, which is a
result of speaker clustering.
Frame-level mixture component

The Gibbs sampler assigns frame-level mixture component k at frame t by using
the following equation:
(8.103)
where 
 is defined as follows:
(8.104)
Therefore, by considering the normalization constant, the posterior probability
can be obtained as follows:
(8.105)
This equation is analytically derived by using the marginal likelihood for
complete data (Eq. (8.101)).
Utterance-level mixture component
As with the frame-level mixture component case, the Gibbs sampler assigns
utterance-level mixture s at utterance u by using the following equation:
(8.106)
The value of 
 is computed by the sufficient statistics using 
 and 
.

Therefore, the posterior probability can be obtained as follows:
(8.107)
Thus, we can derive a solution for the multi-scale mixture model based on Gibbs
sampling, which jointly infers the latent variables by interleaving frame-level
and utterance-level samples. Algorithm 18 provides a sample code for the
proposed approach.
Algorithm 18 Gibbs sampling-based multi-scale mixture model.
1:  Initialize 
 
2:  repeat 
3:     for 
 shuffle 
 do 
4:        for 
 shuffle 
 do 
5:        Sample 
 by using Eq. (8.105) 
6:        end for 
7:     end for 
8:     for 
 shuffle 
 do 
9:        Sample 
 by using Eq. (8.107) 
10:     end for 
11:  until some condition is met
MCMC-based acoustic modeling for speaker clustering was investigated
with respect to the difference in the MCMC and VB estimation methods by
Tawara, Ogawa, Watanabe et al. (2012). Table 8.1 shows speaker clustering
results in terms of the average cluster purity (ACP), average speaker purity
(ASP), and geometric mean of those values (K value) with respect to the
evaluation criteria in speaker clustering. We used the Corpus of Spontaneous
Japanese (CSJ) dataset (Furui et al. 2000) and investigated the speaker clustering
performance for MCMC and VB for various amounts of data. Table 8.1 shows

that the MCMC-based method outperformed the VB method by avoiding local
optimum solutions, especially when only a few utterances could be used. These
results also supported the importance of the Gibbs-based Bayesian properties.
Table 8.1 Comparison of MCMC and VB for speaker clustering. ACP:
average cluster purity, ASP: average speaker purity, and K value: geometric
mean of ACP and ASP.
Evaluation data
Method
ACP
ASP
K value
CSJ-1
MCMC
0.808
0.898
0.851
(# spkr10, # utt 50)
VB
0.704
0.860
0.777
CSJ-2
MCMC
0.852
0.892
0.871
(# spkr10, # utt 100)
VB
0.695
0.846
0.782
CSJ-3
MCMC
0.866
0.892
0.879
(# spkr10, # utt 200)
VB
0.780
0.870
0.823
CSJ-4
MCMC
0.784
0.694
0.738
(# spkr10, # utt 2,491)
VB
0.773
0.673
0.721
CSJ-5
MCMC
0.740
0.627
0.681
(# spkr10, # utt 2,321)
VB
0.693
0.676
0.684
Since the mixture of GMM is trained by MCMC, it is a straightforward
extension to deal with a Dirichlet process mixture model, as discussed in
Section 8.2.5, for speaker clustering, where the number of speaker clusters is
jointly optimized based on this model. There are several studies of applying the
Dirichlet process mixture model to speaker clustering (Fox et al. 2008, Tawara,
Ogawa, Watanabe et al. 2012b). The next section introduces the application of
an MCMC-based Dirichlet process mixture model to cluster HMMs (mixture of
HMMs).
8.4   Nonparametric Bayesian HMMs to acoustic unit
discovery
This section describes an application of Bayesian nonparametrics in Section 8.2
to acoustic unit discovery based on HMMs (Lee & Glass 2012, Lee, Zhang &
Glass 2013, Torbati, Picone & Sobel 2013, Lee 2014). Acoustic unit discovery
aims to automatically find an acoustic unit (e.g., phoneme) from speech data

without transcriptions, and this is used to build ASR or spoken term detection
systems with limited language resources (Schultz & Waibel 2001, Lamel,
Gauvain & Adda 2002, Jansen, Dupoux, Goldwater et al. 2013). One of the
powerful advantages of Bayesian nonparametrics is to find the model structure
appropriately, and it is successfully applied to word unit discovery in natural
language processing (Goldwater, Griffiths & Johnson 2009, Mochihashi,
Yamada & Ueda 2009) and in spoken language processing (Neubig, Mimura,
Mori & Kawahara 2010). This section regards sub-word units as latent variables
in one nonparametric Bayesian model. More specifically, it formulates a
Dirichlet process mixture model where each mixture is an HMM used to model a
sub-word unit and to generate observed segments of that unit. This model seeks
the set of sub-word units, segmentation, clustering, and HMMs that best
represents the observed data through an iterative inference process. This
inference process can be performed by using Gibbs sampling.
To realize this acoustic unit discovery, the approach deals with the following
variables:
D dimensional speech feature 
 at frame t in utterance n.
Binary boundary variable 
 that has value 1 when the speech frame
t is at the end point of a segment, and 0 otherwise.
Boundary index 
 returns the frame index of the qth
boundary in utterance n. The initial boundary 
.
Segment of features 
.
Unit label 
 to specify the unit label of 
. U is the
number of the cluster, and u is a unit index. In addition, 
 indicates a unit label at frame t and utterance n.
HMM 
 that represents one cluster unit u with a standard continuous
density HMM (Section 3.2.3) that has state transition 
 from
HMM state i to j, mixture weight 
 at mixture component k in
state j, and the mean vector 
 and (diagonal) precision matrix 
.
HMM 
state 
and 
GMM 
component 
 
and 
.
The difference between this HMM and the conventional CDHMM in
Section 3.2.3 given a phoneme unit is that this approach regards unit label 
and the number of units U as a latent variable, which is obtained by a Dirichlet

process. Therefore, the notation is similar to that in Section 3.2.3 except that it
includes the cluster index u explicitly. A similar model is used in Gish, Siu, Chan
et al. (2009) and Siu, Gish, Chan et al. (2014) based on an ML-style iterative
procedure instead of Bayesian nonparametrics. In this section, the numbers of
HMM states J and mixture components K are assumed to be the same fixed
values for all units, but they can also be optimized by using Bayesian
nonparametrics 
(Rasmussen 
2000, 
Beal, 
Ghahramani 
&
Rasmussen 2002, Griffiths & Ghahramani 2005).
8.4.1
 Generative model and generative process
Since we use a fully Bayesian approach, the variables introduced in this model
are regarded as probabilistic variables. For simplicity we consider that the
boundary variable 
 is given in this formulation. The Bayesian approach first
provides a generative process for complete data. We define latent variable Z as
(8.108)
Here, we assume the number of units is infinite, i.e., 
, and latent
variables will be generated based on the Dirichlet process. The other variables
are defined as
(8.109)
 is the number of units appearing in utterance n, and 
 is a weight
parameter of unit u. Then, the conditional distribution is represented as follows:

(8.110)
where each likelihood function can be represented as follows:
(8.111)

and
(8.112)
 is an initial weight in an HMM. For 
, we use the Dirichlet
process mixture model, described in Section 8.2.5.
The model parameters are sampled from a base distribution with
hyperparameter 
, and we use a conjugate prior distribution of CDHMM 
 with diagonal covariance matrices, as we discussed in Section 4.3.2,
which is represented as
(8.113)
, 
, 
, 
, 
, and 
 are the prior hyperparameter (
). 
 and 
 can
be obtained using the Gaussian mean and precision parameters of all data.
Algorithm 19 provides a generative process of a nonparametric Bayesian
HMM. A Dirichlet process mixture model (DPM) can sample the acoustic unit u
from existing clusters or a new cluster for every speech segment. Thus, the
model finally generates a sequence of speech features without fixing an acoustic
unit explicitly.

Algorithm 19 Generative process of a nonparametric Bayesian HMM
Require:  Concentration parameter γ, Base distribution of DP 
,
Boundary 
. 
1:  for every utterance 
 do 
2:     for every segment 
 do 
3:        Draw u and 
 from 
 (from existing clusters or
a new one) 
4:        Draw 
 from 
 
5:        Draw 
 from 
 
6:        Draw 
 from 
 
7:        for every feature dimension 
 do 
8:           Draw 
 from 
 
9:           Draw 
 from 
 
10:        end for 
11:        Draw j from 
 
12:        Draw k from 
 
13:        Draw o from 
 
14:         for every frame 
 do 
15:           Draw j from 
 
16:           Draw k from 
 
17:           Draw o from 
 
18:        end for 
19:     end for 
20:  end for
8.4.2
 Inference
The approach infers all latent variables by using Gibbs sampling, as discussed in
Section 8.1.4, which samples a target latent variable z from the following
conditional posterior distribution:
(8.114)
where 
 denotes a set of all hidden variables in 
 except for z. In this section

we provide conditional distributions for cluster label 
, HMM state sequence 
, GMM component sequence 
, and HMM parameters 
 so that we can
perform the Gibbs sampling of these latent variables.
Cluster label 
:
Let 
 be the set of distinctive cluster units. The conditional posterior
distribution of 
 is represented as follows:
(8.115)
where γ is a hyperparameter of the DP prior, 
 represents the number of
cluster labels in 
 taking the value u, and 
 is the number of speech
segments. In this formulation, we do not marginalize 
 unlike Section 8.3,
but use the sampled values, as discussed below. 
If 
 belongs to a new cluster that has not existed before, the conditional
posterior distribution for this new cluster is represented as
(8.116)
where the integral is approximated by a Monte Carlo estimation
(Rasmussen 1999, Neal 2000, Tawara, Ogawa, Watanabe et al. 2012b).
Note that Eqs. (8.115) and (8.116) are a typical solution of the Dirichlet
process in Eq. (8.38). The Gibbs sampler for existing clusters 
depends on the number of occurrences 
 and their likelihood, while that
for a new cluster 
 depends on the concentration
parameter γ and the marginalized likelihood. The likelihood values of 
 and 
 can be computed by considering all possible
HMM states 
 and mixture components 
 based on the forward
algorithm in Section 3.3.1. However, since the following Gibbs samplers
can sample 
 and 
, we can use the following conditional likelihood
values:
(8.117)
This is easily computed by accumulating all Gaussian likelihood values
given 
 and 
.

HMM state :
The conditional posterior distribution of HMM state 
 is obtained from the
following distribution, given the previous state 
 and the succeeding state
:
(8.118)
Note that this algorithm does not require the forward–backward algorithm
in Section 3.3.1 to obtain the occupation probability, compared with the
conventional HMM. There is an alternative algorithm to sample a state
sequence similar to the forward-backward algorithm, called the forward
filtering backward sampling algorithm of an HMM (Scott 2002,
Mochihashi et al. 2009) in the MCMC framework. 
Given 
, which is also obtained by the Gibbs sampler, we can also
approximate Eq. (8.118) as
(8.119)
This avoids computing the Gaussian likelihoods for all mixture
components.
GMM component 
:
The conditional posterior distribution of GMM component 
 at cluster
u and state 
 is obtained from the following distribution:
(8.120)
Thus, the Gibbs samplers of 
, 
, and 
 can provide the
latent variable sequences of C, S, and V. Once we have C, S, and V, we can
compute the sufficient statistics for the CDHMM, as follows:

(8.121)
Thus, we can obtain the posterior distribution analytically based on the conjugate
analysis, as we have shown in Sections 2.1.4 and 4.3. This section only provides
the analytical solutions for the posterior distributions of CDHMM parameters,
which are used to sample CDHMM parameters.
HMM parameters 
:
Initial weight
(8.122)
where
(8.123)
State transition
(8.124)
where
(8.125)
Mixture weight
(8.126)
where
(8.127)

Mean vector and covariance matrix at dimension d
(8.128)
where
(8.129)
Note that compared with the collapsed Gibbs sampling solutions in
Section 8.3, this samples the Gaussian parameters, as well as the other
variables. Therefore, the Gibbs samplers for latent variables become rather
simple equations.
Thus, we can obtain the nonparametric Bayesian HMMs for acoustic unit
discovery given cluster boundaries. The MCMC is performed by sampling latent
variables C, S, and V, and model parameters 
, iteratively. Note that the
number of clusters U is changing according to the Dirichlet process, and finally
becomes a fixed number. This clustering corresponds to automatically obtaining
the acoustic unit in a Bayesian nonparametric manner.
Lee & Glass (2012) also considered cluster boundaries as latent variables
which can be obtained by Gibbs sampling. In addition, to provide an appropriate
initialization of the boundaries, the approach uses a pre-segmentation technique
based on Glass (2003). The model obtained was compared with the other
acoustic unit discovery method based on the dynamic time warping technique
using the distance between GMM posteriors (Zhang & Glass 2009). The
nonparametric Bayesian HMM achieved comparable results to the dynamic time
warping technique in terms of the spoken term detection measures, which shows
the effectiveness of the nonparametric Bayesian acoustic unit discovery. The
approach carries a huge computational cost compared with the other EM type
approaches (based on ML, MAP, and VB), and its scalability for the amount of

data and the difficulty of parallelization remain serious problems. However, this
is one of a few successful approaches using Bayesian nonparametrics for speech
data, and it potentially has various advantages over the conventional EM type
approaches (e.g., it could mitigate the local optimum problem and use any other
distributions than conjugate distributions).
8.5   Hierarchical Pitman–Yor language model
In an LVCSR system, in addition to an acoustic model based on HMMs, the
other key component is a language model based on n-grams. In recent years,
BNP learning has been substantially developed for language modeling (Teh
et al. 2006) and successfully applied for several LVCSR tasks in Huang &
Renals (2008). In this section, we introduce a hierarchical Bayesian
interpretation for language modeling, based on a nonparametric prior called the
Pitman–Yor (PY) process. The motivation of conducting Bayesian learning of n-
gram language models is to tackle the limitations such as overfitting of
maximum likelihood estimation and the lack of rich contextual knowledge
sources. The PY process offers a principled approach to language model
smoothing which produces a power-law distribution for natural language. The
Bayesian language model based on Bayesian nonparametrics is a realization of a
full Bayesian solution according to the MCMC inference procedure. Such a
model is a distribution estimate, which is different from the Bayesian language
model based on maximum a-posteriori (MAP) estimation as addressed in
Section 4.7. A MAP-based language model is known as a point estimate of
language model. A BNP-based language model integrates the values of
parameters into a marginalized language model. It is interesting to note that the
resulting hierarchical PY language model is a direct generalization of the
hierarchical Dirichlet language model.
In what follows, we first survey the PY process and explain the importance
of the power-law property in language modeling. Then we revisit language
model smoothing based on Kneser–Ney smoothing and find the clue for
connection to BNP learning. Next the hierarchical PY process is constructed to
estimate a language model which provides the hierarchical Bayesian
interpretation for a Kneser–Ney smoothed language model. The relation to the
hierarchical Dirichlet language model will be illustrated. Lastly, the MCMC
inference for the hierarchical PY language model is addressed.

8.5.1
 Pitman–Yor process
The PY process (Pitman & Yor 1997) is known as the two-parameter Poisson–
Dirichlet process 
, which is expressed as a three-parameter
distribution over distributions where 
 is a discount parameter, 
 is a
strength parameter and 
 is a base distribution. Base distribution 
 can be
understood as the mean of draws from the PY process. This process can be used
to draw the unigram language model. Let 
 denote the unigram probability
of a word 
 and 
 denote the vector of unigram
probabilities, which is drawn from a PY process:
(8.130)
Here, 
 is a mean vector where 
 is the a-priori probability
of word w. In practice, this base measure is usually set to be uniform 
 for all 
. The parameters d and 
 both control the degree
of variability around 
 in different ways. When 
, the PY process
reverts to the DP, which is denoted by 
. The PY process is seen as a
generalization of the DP.
Basically, there is no analytic form for distribution of the PY process. We
would like to work out the nonparametric distribution over sequences of words
from the PY process. Let 
 be a sequence of words drawn
independently and identically from G given the mean distribution 
. The
procedure of generating draws from a PY process that can be described
according to the metaphor of the “Chinese restaurant process” (Pitman 2006). As
introduced in Section 8.2.4, imagine a Chinese restaurant containing an infinite
number of tables, each with infinite seating capacity. Customers enter the
restaurant and seat themselves. The first customer sits at the first table while
each subsequent customer sits at an occupied table with probability proportional
to the number of customers who are already sitting there 
, or at a new
unoccupied table with probability proportional to 
 where 
 is the
current number of occupied tables. That is, if 
 is the index of the table chosen
by the ith customer, this customer sits at the table k given the seating
arrangement of the previous 
 customers, 
, with
probability

(8.131)
where 
 denotes the number of customers sitting at table k and 
 is
the total number of customers. The above generative procedure produces a
sequence of words drawn independently from G with G marginalized out.
It is important to investigate the behaviors of drawing the sequence of words
from the PY process. Firstly, the rich-gets-richer clustering property can be
observed. That is, the more words have been assigned to a draw from 
, the
more likely subsequent words will be assigned to the draw. Secondly, the more
we draw from 
, the more likely a new word will be assigned to a new draw
from 
. Combining these two behaviors produces the so-called power-law
distribution where many unique or distinct words are observed, most of them
rarely. This distribution resembles the distribution of words which is seen in
natural language. The power-law distribution has been found to be one of the
most striking statistical properties of word frequencies in natural language.
Figure 8.10 demonstrates the power-law behavior of the PY process which is
controlled by parameters d and 
, showing the average number of unique
words among 25 sequences of words drawn from G, as a function of the number
of words, for various values of 
 and d. We find that 
 controls the total
number of unique words while d adjusts the asymptotic growth of the number of
unique words. Figure 8.11 displays the proportion of words appearing only once
among the unique words. These figures indicate the proportion of words that
occur rarely. We can see that larger d and 
 produce more rare words. This
phenomenon is reflected by the probability of producing a new unoccupied table
(8.132)

Figure 8.10 Power-law distribution: (a) the number of unique words as a
function of the number of words, drawn on a log–log scale with 
and 
 (dashdot line), 20 (solid line), and 1 (dashed line); (b) the same
as (a) with 
 and 
 (dashdot line), 0.6 (solid line), and 0
(dashed line).

Figure 8.11 Power-law distribution: (a) the proportion of words appearing
only once, as a function of the number of words drawn with 
 and 
 (dashdot line), 20 (solid line), and 1 (dashed line); (b) the same as
(a) with 
 and 
 (dashdot line), 0.6 (solid line), and 0 (dashed
line).
8.5.2
 Language model smoothing revisited
A key issue in a language model is to handle the sparseness of training data for
training n-gram parameters under the conditions of a large n-gram window size n
and large dictionary size 
. As addressed in Section 3.6, a series of language
model smoothing methods has been developed to tackle the data sparseness issue
in a statistical n-gram model. One important trick in these methods is to
incorporate an absolute discount parameter d in the count of an observed n-gram

event 
 of a word 
 and its preceding history
words 
. Owing to this discount, we modify the counts for lower order n-
gram probabilities so as to construct the interpolated Kneser–Ney smoothing
(Kneser & Ney 1995):
(8.133)
which is rewritten from Eq. (3.232) with an order-dependent discount parameter 
 and a new notation 
, expressing the
number of unique words that follow the history words u. The term 
 denotes
the backoff context of u. If 
, then 
.
We would like to investigate the relation between language model smoothing
and Bayesian learning. Such a relation is crucial to developing a BNP-based
language model. In a standard Bayesian framework for language modeling, a
prior distribution is placed over the predictive distribution for the language
model. The predictive distribution is estimated by marginalizing out the latent
variables from the posterior distribution. The problem of zero-probability can be
avoided by taking advantage of knowledge expressed by the priors. A smoothed
language model can be obtained. In Yaman et al. (2007), the n-gram smoothing
was conducted under a framework called the structural maximum a-posteriori
(MAP) adaptation where the interpolation of n-gram statistics with an 
-
gram model was performed in a hierarchical and recursive fashion. As
mentioned in Chapter 4, such a MAP approximation only finds a point estimate
of a language model without considering the predictive distribution. To pursue a
full Bayesian language model, the hierarchical Dirichlet language model
(MacKay & Peto 1995) in Section 5.3 calculates the hierarchical predictive
distribution of an n-gram by marginalizing the Dirichlet posterior over the
Dirichlet priors.
However, the PY process was shown to be more fitted as a prior distribution
than a Dirichlet distribution to the applications in natural language processing
(Goldwater, Griffiths & Johnson 2006). It is because the power-law distributions
of word frequencies produced by the PY process are more likely to be close to
the heavy-tailed distributions observed in natural language. But, the PY process,
addressed in Section 8.5.1, is only designed for a unigram language model. In

what follows, we extend the hierarchical Dirichlet language model, which adopts
the Dirichlet prior densities, to the hierarchical PY language model, which
utilizes the PY process as nonparametric priors and integrates out these prior
measures.
8.5.3
 Hierarchical Pitman–Yor language model
Similarly to the extension from the Dirichlet process to the hierarchical Dirichlet
process for representation of multiple documents, in this section, we address the
extension from the PY process to the hierarchical PY (HPY) process which is
developed to realize the probability measure for the smoothed n-gram language
model. Let 
 represent the vector of word probability estimates
for unigrams of all words w in a vocabulary 
. A PY process prior for unigram
probabilities is expressed by
(8.134)
where 
 is a global mean vector in the form of a noninformative base
distribution or uniform distribution:
(8.135)
This PY process can be used to calculate the predictive unigram of a word w
according to the metaphor of the Chinese restaurant process. The customers or
word tokens enter the restaurant and seat themselves at either an occupied table
or a new table with the probabilities in Eq. (8.131). Each table is labeled by a
word 
 initialized by the first customer sitting on it. The next customer
can only sit on those tables with the same label w. That is, those 
 customers
corresponding to the same word label w can sit at different tables, with 
being the number of tables with label w. In our notation, 
 means the
total number of tables. The number of customers at table k is denoted by 
, and
the total number of customers is expressed by 
. Given the seating
arrangement of customers S, the discount parameter 
, and the strength
parameter 
, the predictive unigram probability of a new word w is given by

(8.136)
where 
 if table k has the label w, 
 otherwise. This equation
is derived similarly to the metaphor of the Chinese restaurant process designed
for the Dirichlet process, as mentioned in Section 8.2.4. The key difference of
the PY process compared to the Dirichlet process is the discount parameter 
which is used to adjust the power-law distribution of unigrams based on the PY
process. By averaging over seating arrangements and hyperparameters 
, we obtain the probability 
 for a unigram language model.
Interestingly, if we set 
, the PY process is reduced to a DP which
produces the Dirichlet distribution 
. In this case, the predictive
unigram probability based on the PY process prior in Eq. (8.136) is accordingly
reduced to
(8.137)
where 
 is used for all w. This equation is equivalent to the
hierarchical Dirichlet unigram model as given in Eq. (5.70). The only difference
here is to adopt a single shared hyperparameter 
 for all word labels 
.
Next, we extend the unigram language model to the n-gram language model
based on the HPY process. An n-gram language model is defined as a
probability measure over the current word 
 given a history context 
. Let 
 be the vector of the target probability
distributions of all vocabulary words 
 given the history context u. We
use a PY process as the prior for 
 in the form of

(8.138)
with the hyperparameters 
 and 
 specific to the length of context 
.
However, 
 is still an unknown base measure. A PY process is recursively
placed over it:
(8.139)
with parameters which are functions of 
. This is repeated until we reach
the PY process prior 
 for a unigram model. Figure 8.12 is a schematic
diagram showing the hierarchical priors for the smoothed language model based
on the HPY process. This process enables us to generalize from the unigram
language model to the n-gram language model. The resulting probability
distribution is called the hierarchical Pitman–Yor language model (HPYLM).
Figure 8.12 Hierarchical Pitman–Yor process for n-gram language model.
A hierarchical Chinese restaurant process can be used to develop a
generative procedure for drawing words from the HPYLM with all 
marginalized out. The context u corresponds to a restaurant. This procedure
gives us a representation of HPYLM for efficient inference using an MCMC
algorithm, and easy computation of the predictive probability distribution from
new test words. Through this representation, the correspondence between the
Kneser–Ney language model and HPYLM can be illustrated. In the metaphor of
hierarchical CRP, there are multiple hierarchical restaurants or PY processes,
with each corresponding to one context. Different orders of n-gram models share
information with each other through the interpolation of higher-order n-grams
with lower-order n-grams. We draw words from 
 of the PY process by using
the CRP as discussed in Section 8.5.1. Further, we draw words from another

CRP to sample the parent distribution 
, which is itself sampled according
to a PY process. This is recursively applied until we need draws from the global
mean distributions 
. By referring to the predictive unigram probability from
the PY process as shown in Eq. (8.136), the predictive n-gram probability 
 under a particular combination of seating arrangement S and
the hyperparameters 
 and 
 can be obtained from
(8.140)
where 
 is the number of customers sitting at table k with label w, 
 and 
 is the number of occupied tables with label w. It is
interesting to see that the Kneser–Ney language model (KN–LM) in Eq. (8.133)
is closely related to the HPYLM in Eq. (8.140). HPYLM is a generalized
realization of KN–LM with an additional concentration parameter 
. We can
interpret the interpolated KN–LM as an approximate inference scheme for the
HPYLM.
8.5.4
 MCMC inference for HPYLM
An MCMC algorithm can be used to infer the posterior probability of seating
arrangement S. Given some training data 
, we count the number of
occurrences 
 of each word w appearing after each context u of length 
. This means that there are 
 samples drawn from the PY process 
. We are interested in the posterior probability over the latent variables 
 and the parameters 
 of all lower-
order models with 
. Because the hierarchical Chinese restaurant
process marginalizes out each 
, we can replace  by the seating arrangement
in the corresponding restaurant using 
. The posterior
probability is then obtained from
(8.141)

Given this posterior probability, we can calculate the predictive n-gram
probability of a test word w after observing a context u:
(8.142)
which is an expectation of predictive probability under a particular set of seating
arrangements S and PY parameters Θ. The overall predictive probability in the
integral of Eq. (8.142) is approximated by using the L samples 
 drawn
from posterior probability 
.
In the implementation of Teh et al. (2006), Huang & Renals (2008), the
discount parameter and concentration parameter were drawn by a beta
distribution and a gamma distribution, respectively. Here, we address the
approach to sampling the seating arrangement 
 corresponding to 
. We
employ Gibbs sampling to keep track of the current state of each variable of
interest in the model, and iteratively re-sample the state of each variable given
the current states of all other variables. After a sufficient number of iterations,
the states of variables in the seating arrangement S converge to the required
samples from the posterior probability. The variables consist of, for each context
(restaurant) u and each word (customer) 
 drawn from 
, the index 
 of
the draw from 
 assigned 
. In the Chinese restaurant metaphor, this is
the table index of where the lth customer sat at the restaurant corresponding to 
. If 
 has value w, it can only be assigned to draws from 
 that have
the same value w. The posterior probability of drawing a table 
 for the last
word 
 from 
 is given below:
(8.143)

(8.144)
where the superscript 
 means the corresponding set of variables or counts
with 
 excluded.
One other key difference between the KN–LM in Eq. (8.133) and the
HPYLM in Eq. (8.140) is that the KN–LM adopts a fixed discount 
 while
HPYLM produces different discounts 
 for different words w due to the
values of 
, which counts the number of occupied tables labeled by w. In
general, 
 is on average larger if 
 is larger. The actual amount of
discount grows gradually as the count 
 grows. The physical meaning of
discounting in the smoothed n-grams based on HPYLM is consistent with the
discount scheme in the modified Kneser–Ney language model (MKN–LM),
which specifies three discounts 
, 
, and 
 for those n-grams with one (
), two (
), and three or more (
) counts, as addressed in
Section 3.6.6. The discounts 
 in MKN–LM are empirically
determined while the discounts 
 in HPYLM are automatically associated
with each word w. In particular, if we restrict 
 to be at most 1,
(8.145)
we get the same discount value so long as 
, or equivalently we conduct
absolute discounting. We also have the relationships among the 
s and 
:
(8.146)
If we further assume 
 for all model orders 
, the
predictive probability of HPYLM in Eq. (8.140) is directly reduced to that given
by the interpolated Kneser–Ney model.
8.6   Summary

Due to the high computational cost and lack of scalability for data and model
sizes, MCMC approaches are still in a development stage for speech and
language processing applications that essentially need to deal with large-scale
data. However, the recent advance in computational powers (CPU speed,
memory size, many cores, and GPU) and algorithm development make it
possible to realize middle-scale data processing of MCMC, especially for
language processing based on multinomial and Dirichlet distributions, as
discussed in this chapter. One of the most attractive features of MCMC is that
we can handle any types of expectation calculation by the Monte Carlo
approximations, which can realize all Bayesian approaches in principle.
Therefore, we expect further development of computational powers and
algorithms to widen the applications of MCMC to large-scale problems, and
enable fully Bayesian speech and language processing based on MCMC in the
near future.

Appendix A    Basic formulas
A.1   Expectation
(A.1)
(A.2)
A.2   Delta function
A.2.1
 Kronecker delta function
(A.3)
A.2.2
 Product formula of Kronecker delta function
(A.4)
(Proof)
(A.5)

A.2.3
 Dirac delta function
(A.6)
A.3   Jensen’s inequality
For a concave function f, where X is a probabilistic random variable sampled
from a distribution function, and an arbitrary function 
, we have the
following inequality:
(A.7)
In the special case of 
, (A.7) is rewritten as follows:
(A.8)
A.4   Gamma function
(A.9)
(A.10)
A.4.1
 Stirling’s approximation
(A.11)
A.4.2
 Di-gamma function

(A.12)

Appendix B    Vector and matrix formulas
This appendix lists some useful vector and matrix formulas. Note that these
formulas are selected for the purpose of deriving equations in this book in
Bayesian speech and language processing, and do not cover the whole field of
vector and matrix formulas.
B.1   Trace
(B.1)
(B.2)
(B.3)
(B.4)
(B.5)
B.2   Transpose
(B.6)
(B.7)
B.3   Derivative

(B.8)
(B.9)
(B.10)
(B.11)
(B.12)
(B.13)
(B.14)
(B.15)
B.4   Complete square
When A is a symmetric matrix,
(B.16)
where
(B.17)

By using the above complete square formula, we can also derive the
following formula when matrices 
 and 
 are symmetric:
(B.18)
where
(B.19)
B.5   Woodbury matrix inversion
(B.20)

Appendix C    Probabilistic distribution functions
This appendix lists the probabilistic distribution functions (pdfs) used in the
main discussions in this book. We basically use the definitions of these functions
followed by Bernardo & Smith (2009). Each pdf section also provides values of
mean, mode, variance, etc., which are used in the book, although it does not
include a complete set of distribution values to avoid complicated descriptions in
this appendix. The sections also provide some derivations of these values if these
derivations are not complicated.
C.1   Discrete uniform distribution
When we have a discrete variable 
, the discrete
uniform distribution is defined as:
pdf:
(C.1)
where N is the number of distinct elements.
C.2   Multinomial distribution
pdf:
(C.2)

where 
 is a nonnegative integer, and has the following constraint:
(C.3)
The parameter 
 has the following constraint:
(C.4)
pdf (
 and 
 
):
(C.5)
C.3   Beta distribution
Beta distribution is a special case of the Dirichlet distribution with two
probabilistic variables x and 
.
pdf:
(C.6)
where
(C.7)
and
(C.8)
Normalization constant:
(C.9)

where 
 is a gamma function.
Mean:
(C.10)
This is derived as follows:
(C.11)
where we use the following property of the gamma function:
(C.12)
Mode:
(C.13)
It is derived as follows:
(C.14)
Therefore,

(C.15)
C.4   Dirichlet distribution
A Dirichlet distribution is a generalized beta distribution with J probabilistic
variables.
pdf:
(C.16)
where
(C.17)
Normalization constant:
(C.18)
Mean:
(C.19)

(C.20)
(C.21)
Mode:
(C.22)
C.5   Gaussian distribution
Note that this book uses Σ as a variance. The standard deviation σ is represented
as 
. We also list the pdf with precision scale parameter 
, which
can also be used in this book.
pdf (with variance Σ):
(C.23)
where

(C.24)
and
(C.25)
Normalization constant (with variance Σ):
(C.26)
pdf (with precision r):
(C.27)
where
(C.28)
Normalization constant (with precision r):
(C.29)
Mean:
(C.30)
Variance:
(C.31)
Mode:
(C.32)
C.6   Multivariate Gaussian distribution

pdf (with covariance matrix 
):
(C.33)
where
(C.34)
and
(C.35)
 is positive definite.
Normalization constant (with covariance matrix 
):
(C.36)
pdf (with precision matrix R):
(C.37)
Normalization constant (with precision matrix R):
(C.38)
Mean:
(C.39)
Variance:
(C.40)

Mode:
(C.41)
C.7   Multivariate Gaussian distribution (diagonal
covariance matrix)
This is a special case of the multivariate Gaussian distribution. The diagonal
covariance matrix reduces the number of parameters and makes the analytical
treatment simple (it is simply represented as a product of scalar Gaussian
distributions).
pdf (with covariance matrix 
, where 
):
(C.42)
where
(C.43)
and
(C.44)
Normalization constant (with variance 
):
(C.45)
pdf (with precision matrix 
, where 
):

(C.46)
Normalization constant (with precision 
):
(C.47)
Mean:
(C.48)
Variance:
(C.49)
Mode:
(C.50)
C.8   Spherical Gaussian distribution
This is another special case of the multivariate Gaussian distribution where the
diagonal covariance matrix is represented with the identity matrix with a single
scaling parameter.
pdf (with covariance matrix 
):
(C.51)

where
(C.52)
and
(C.53)
Normalization constant (with variance Σ):
(C.54)
pdf (with precision matrix 
):
(C.55)
Normalization constant (with precision r):
(C.56)
Mean:
(C.57)
Variance:
(C.58)
Mode:
(C.59)
C.9   Matrix variate Gaussian distribution

pdf:
(C.60)
where
(C.61)
and
(C.62)
 and 
 are correlation matrices and are positive definite. When 
,
it becomes the multivariate Gaussian distribution.
Normalization constant:
(C.63)
Mean:
(C.64)
Variance:
(C.65)
(C.66)
Mode:
(C.67)
C.10   Laplace distribution
pdf:

(C.68)
where
(C.69)
and
(C.70)
Normalization constant:
(C.71)
Mean:
(C.72)
Mode:
(C.73)
Note that the probabilistic distribution function is not continuous at μ, and it is
not differentiable.
C.11   Gamma distribution
A gamma distribution is used as a prior/posterior distribution of precision
parameter r of a Gaussian distribution.
pdf:

(C.74)
where
(C.75)
and
(C.76)
Normalization constant:
(C.77)
Mean:
(C.78)
Variance:
(C.79)
Mode:
(C.80)
The shape of the gamma distribution is not symmetric, and the mode and mean
values are different.
To make the notation consistent with the Wishart distribution in Appendix
C.14, we also use the following definition for the gamma distribution, with 

and 
 in the original gamma distribution defined in Eq. (C.74):
pdf (with  factor):
(C.81)
Normalization constant (with  factor):
(C.82)
Mean (with  factor):
(C.83)
Variance (with  factor):
(C.84)
Mode (with  factor):
(C.85)
It is shown in Appendix C.14 that this gamma distribution (
) with 
factor is equivalent to the Wishart distribution with the number of dimensions 1 (
).

C.12   Inverse gamma distribution
An inverse gamma distribution is used as a prior/posterior distribution of
variance parameter Σ. It can be obtained by simply replacing y in a gamma
distribution by  with 
.
pdf:
(C.86)
where
(C.87)
and
(C.88)
Normalization constant:
(C.89)
Mean:
(C.90)
Mode:
(C.91)

C.13   Gaussian–gamma distribution
We provide a Gaussian–gamma distribution, which is used as a conjugate prior
distribution of a Gaussian distribution with mean μ and precision r. This is also
known as the normal-gamma distribution. Note that we use the gamma
distribution defined in Eq. (C.81) instead of the original definition of the gamma
distribution in Eq. (C.74).
pdf:
(C.92)
Note that the power of r is 
, which is different from the original gamma
distribution definition in Section (C.11), because it also considers an 
factor in the Gaussian distribution.
Normalization constant:
(C.93)
Mean:
(C.94)
Mode:
(C.95)
The means and modes of mean and precision parameters are the same as those of

the Gaussian and gamma distributions, respectively.
C.14   Wishart distribution
pdf:
(C.96)
where
(C.97)
and
(C.98)
Y and 
 are positive definite.
Normalization constant:
(C.99)
where 
 is a multivariate Gamma function.
Mean:
(C.100)
Mode:
(C.101)
When 
, the Wishart distribution is equivalent to the gamma distribution
defined in Eq. (C.81).

C.15   Gaussian–Wishart distribution
pdf:
(C.102)
Note 
that 
when 
, 
Gaussian–Wishart 
distribution 
 
approaches 
Gaussian–gamma 
distribution 
 in Appendix C.13.
Normalization constant:
(C.103)
Mean:
(C.104)
Mode:
(C.105)
Again, when 
, these modes are equivalent to the modes of the
Gaussian–gamma distribution in Section C.13.
C.16   Student’s t-distribution
pdf:

(C.106)
where
(C.107)
and
(C.108)
Normalization constant:
(C.109)
Mean:
(C.110)
Mode:
(C.111)
Parameter κ is called the degrees of freedom, and if κ is large, the distribution
approaches the Gaussian distribution. Note that Student’s t-distribution is not
included in the exponential family (Section 2.1.3).

References
Abu-Mostafa, Y. S. (1989), “The Vapnik–Chervonenkis dimension: information versus complexity in
learning,” Neural Computation 1, 312–317.
Akaike, H. (1974), “A new look at the statistical model identification,” IEEE Transactions on
Automatic Control 19(6), 716–723.
Akaike, H. (1980), “Likelihood and the Bayes procedure,” in J. M. Bernardo, M. H. DeGroot, D. V.
Lindley & A. F. M. Smith, eds, Bayesian Statistics, University Press, Valencia, Spain,
pp. 143–166.
Akita, Y., & Kawahara, T. (2004), “Language model adaptation based on PLSA of topics and
speakers,” Proceedings of Annual Conference of International Speech Communication
Association (INTERSPEECH), pp. 1045–1048.
Aldous, D. (1985), “Exchangeability and related topics,” École d’Été de Probabilités de Saint-Flour
XIII1983, pp. 1–198.
Anastasakos, T., McDonough, J., Schwartz, R., & Makhoul, J. (1996), “A compact model for
speaker-adaptive training,” Proceedings of International Conference on Spoken Language
Processing (ICSLP), pp. 1137–1140.
Anguera Miro, X., Bozonnet, S., Evans, N., et al. (2012), “Speaker diarization: A review of recent
research,” IEEE Transactions on Audio, Speech, and Language Processing 20(2), 356–370.
Antoniak, C. E. (1974), “Mixtures of Dirichlet processes with applications to Bayesian
nonparametric problems,” Annals of Statistics 2(6), 1152–1174.
Attias, H. (1999), “Inferring parameters and structure of latent variable models by variational
Bayes,” Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI), pp. 21–
30.
Axelrod, S., Gopinath, R., & Olsen, P. (2002), “Modeling with a subspace constraint on inverse

covariance matrices,” Proceedings of International Conference on Spoken Language
Processing (ICSLP), pp. 2177–2180.
Bahl, L. R., Brown, P. F., de Souza, P. V., & Mercer, R. L. (1986), “Maximum mutual information
estimation of hidden Markov model parameters for speech recognition,” Proceedings of
International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 49–52.
Barber, D. (2012), Bayesian Reasoning and Machine Learning, Cambridge University Press.
Barker, J., Vincent, E., Ma, N., Christensen, H., & Green, P. (2013), “The PASCAL CHiME speech
separation and recognition challenge,” Computer Speech and Language 27, 621–633.
Baum, L. E., Petrie, T., Soules, G., & Weiss, N. (1970), “A maximization technique occurring in the
statistical analysis of probabilistic functions of Markov chains,” The Annals of Mathematical
Statistics, pp. 164–171.
Beal, M. J. (2003), Variational algorithms for approximate Bayesian inference, PhD thesis,
University of London.
Beal, M. J., Ghahramani, Z., & Rasmussen, C. E. (2002), “The infinite hidden Markov model,”
Advances in Neural Information Processing Systems 14, 577–584.
Bellegarda, J. (2004), “Statistical language model adaptation: review and perspectives,” Speech
Communication 42(1), 93–108.
Bellegarda, J. R. (2000), “Exploiting latent semantic information in statistical language modeling,”
Proceedings of the IEEE 88(8), 1279–1296.
Bellegarda, J. R. (2002), “Fast update of latent semantic spaces using a linear transform framework,”
Proceedings of International Conference on Acoustics, Speech, and Signal Processing
(ICASSP), pp. 769–772.
Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003), “A neural probabilistic language model,”
Journal of Machine Learning Research 3, 1137–1155.
Berger, J. O. (1985), Statistical Decision Theory and Bayesian Analysis, Second Edition, Springer-
Verlag.
Bernardo, J. M., & Smith, A. F. M. (2009), Bayesian Theory, Wiley.
Berry, M. W., Dumais, S. T., & O’Brien, G. W. (1995), “Using linear algebra for intelligent
information retrieval,” SIAM Review 37(4), 573–595.

Bilmes, J. A. (1998), A gentle tutorial of the EM algorithm and its application to parameter
estimation for Gaussian mixture and hidden Markov models, Technical Report TR-97-021,
International Computer Science Institute.
Bilmes, J., & Zweig, G. (2002), “The graphical models toolkit: An open source software system for
speech and time-series processing,” Proceedings of International Conference on Acoustics,
Speech, and Signal Processing (ICASSP), pp. 3916–3919.
Bishop, C. M. (2006), Pattern Recognition and Machine Learning, Springer.
Blackwell, D., & MacQueen, J. B. (1973), “Ferguson distribution via P lya urn schemes,” The
Annals of Statistics 1, 353–355.
Blei, D., Griffiths, T., & Jordan, M. (2010), “The nested Chinese restaurant process and Bayesian
nonparametric inference of topic hierarchies,” Journal of the ACM 57(2), article 7.
Blei, D., Griffiths, T., Jordan, M., & Tenenbaum, J. (2004), “Hierarchical topic models and the
nested Chinese restaurant process,” Advances in Neural Information Processing Systems
16, 17–24.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003), “Latent Dirichlet allocation,” Journal of Machine
Learning Research 3, 993–1022.
Brants, T., Popat, A. C., Xu, P., Och, F. J., & Dean, J. (2007), “Large language models in machine
translation,” Proceedings of the 2007 Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),
Association for Computational Linguistics, pp. 858–867.
Brill, E., & Moore, R. C. (2000), “An improved error model for noisy channel spelling correction,”
Proceedings of the 38th Annual Meeting of Association for Computational Linguistics,
Association for Computational Linguistics, pp. 286–293.
Brown, P., Desouza, P., Mercer, R., Pietra, V., & Lai, J. (1992), “Class-based n-gram models of
natural language,” Computational Linguistics 18(4), 467–479.
Brown, P. F., Cocke, J., Pietra, S. A. D., et al. (1990), “A statistical approach to machine translation,”
Computational Linguistics 16(2), 79–85.
Campbell, W. M., Sturim, D. E., & Reynolds, D. A. (2006), “Support vector machines using GMM
supervectors for speaker verification,” Signal Processing Letters, IEEE 13(5), 308–311.
Chen, K.-T., Liau, W.-W., Wang, H.-M., & Lee, L.-S. (2000), “Fast speaker adaptation using

eigenspace-based maximum likelihood linear regression,” Proceedings of International
Conference on Spoken Language Processing (ICSLP), pp. 742–745.
Chen, S. F. (2009), “Shrinking exponential language models,” in Proceedings of Human Language
Technologies: The 2009 Annual Conference of the North American Chapter of the Association
for Computational Linguistics, Association for Computational Linguistics, pp. 468–476.
Chen, S. F., & Goodman, J. (1999), “An empirical study of smoothing techniques for language
modeling,” Computer Speech & Language 13(4), 359–393.
Chen, S., & Gopinath, R. (1999), “Model selection in acoustic modeling,” Proceedings of European
Conference on Speech Communication and Technology (EUROSPEECH), pp. 1087–1090.
Chesta, C., Siohan, O., & Lee, C.-H. (1999), “Maximum a posteriori linear regression for hidden
Markov model adaptation,” Proceedings of European Conference on Speech Communication
and Technology (EUROSPEECH), pp. 211–214.
Chien, J.-T. (1999), “Online hierarchical transformation of hidden Markov models for speech
recognition,” IEEE Transactions on Speech and Audio Processing 7(6), 656–667.
Chien, J.-T. (2002), “Quasi-Bayes linear regression for sequential learning of hidden Markov
models,” IEEE Transactions on Speech and Audio Processing 10(5), 268–278.
Chien, J.-T. (2003), “Linear regression based Bayesian predictive classification for speech
recognition,” IEEE Transactions on Speech and Audio Processing 11(1), 70–79.
Chien, J.-T., & Chueh, C.-H. (2011), “Dirichlet class language models for speech recognition,” IEEE
Transactions on Audio, Speech, and Language Processing 19(3), 482–495.
Chien, J.-T., Huang, C.-H., Shinoda, K., & Furui, S. (2006), “Towards optimal Bayes decision for
speech recognition,” Proceedings of International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), pp. 45–48.
Chien, J.-T., Lee, C.-H., & Wang, H.-C. (1997), “Improved Bayesian learning of hidden Markov
models for speaker adaptation,” Proceedings of International Conference on Acoustics,
Speech, and Signal Processing (ICASSP), pp. 1027–1030.
Chien, J. T., & Liao, G.-H. (2001), “Transformation-based Bayesian predictive classification using
online prior evolution,” IEEE Transactions on Speech and Audio Processing 9(4), 399–410.
Chien, J.-T., & Wu, M.-S. (2008), “Adaptive Bayesian latent semantic analysis,” IEEE Transactions
on Audio, Speech, and Language Processing 16(1), 198–207.

Chou, W., & Reichl, W., (1999), “Decision tree state tying based on penalized Bayesian information
criterion,” Proceedings of International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), pp. 345–348.
Coccaro, N., & Jurafsky, D. (1998), “Towards better integration of semantic predictors in statistical
language modeling,” Proceedings of International Conference on Spoken Language
Processing (ICSLP), pp. 2403–2406.
Cournapeau, D., Watanabe, S., Nakamura, A., & Kawahara, T. (2010), “Online unsupervised
classification with model comparison in the variational Bayes framework for voice activity
detection,” IEEE Journal of Selected Topics in Signal Processing 4(6), 1071–1083.
Dahl, G. E., Yu, D., Deng, L., & Acero, A. (2012), “Context-dependent pre-trained deep neural
networks for large-vocabulary speech recognition,” IEEE Transactions on Audio, Speech and
Language Processing 20(1), 30–42.
Davis, S. B., & Mermelstein, P. (1980), “Comparison of parametric representations for monosyllabic
word recognition in continuously spoken sentences,” IEEE Transactions on Acoustics, Speech,
and Signal Processing 28(4), 357–366.
Dawid, A. P. (1981), “Some matrix-variate distribution theory: notational considerations and a
Bayesian application,” Biometrika 68(1), 265–274.
De Bruijn, N. G. (1970), Asymptotic Methods in Analysis, Dover Publications.
Dehak, N., Kenny, P., Dehak, R., Dumouchel, P., & Ouellet, P. (2011), “Front-end factor analysis for
speaker verification,” IEEE Transactions on Audio, Speech, and Language Processing
19(4), 788–798.
Delcroix, M., Nakatani, T., & Watanabe, S. (2009), “Static and dynamic variance compensation for
recognition of reverberant speech with dereverberation preprocessing,” IEEE Transactions on
Audio, Speech, and Language Processing 17(2), 324–334.
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1976), “Maximum likelihood from incomplete data
via the EM algorithm,” Journal of Royal Statistical Society B 39, 1–38.
Digalakis, V., & Neumeyer, L. (1996), “Speaker adaptation using combined transformation and
Bayesian methods,” IEEE Transactions on Speech and Audio Processing 4, 294–300.
Digalakis, V., Ritischev, D., & Neumeyer, L. (1995), “Speaker adaptation using constrained
reestimation of Gaussian mixtures,” IEEE Transactions on Speech and Audio Processing
3, 357–366.

Ding, N., & Ou, Z. (2010), “Variational nonparametric Bayesian hidden Markov model,”
Proceedings of International Conference on Acoustics, Speech, and Signal Processing
(ICASSP), pp. 2098–2101.
Droppo, J., Acero, A., & Deng, L. (2002), “Uncertainty decoding with SPLICE for noise robust
speech recognition,” Proceedings of International Conference on Acoustics, Speech, and
Signal Processing (ICASSP)’, Vol. 1, pp. I–57.
Federico, M. (1996), “Bayesian estimation methods of n-gram language model adaptation,”
Proceedings of International Conference on Spoken Language Processing (ICSLP), pp. 240–
243.
Ferguson, T. (1973), “A Bayesian analysis of some nonparametric problems,” The Annals of
Statistics 1, 209–230.
Fosler, E., & Morris, J. (2008), “Crandem systems: Conditional random field acoustic models for
hidden Markov models,” Proceedings of International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), pp. 4049–4052.
Fox, E. B., Sudderth, E. B., Jordan, M. I., & Willsky, A. S. (2008), “An HDP-HMM for systems with
state persistence,” Proceedings of International Conference on Machine Learning (ICML),
pp. 312–319.
Fukunaga, K. (1990), Introduction to Statistical Pattern Recognition, Academic Press.
Furui, S. (1981), “Cepstral analysis technique for automatic speaker verification,” IEEE Transactions
on Acoustics, Speech and Signal Processing 29(2), 254–272.
Furui, S. (1986), “Speaker independent isolated word recognition using dynamic features of speech
spectrum,” IEEE Transactions on Acoustics, Speech and Signal Processing 34, 52–59.
Furui, S. (2010), “History and development of speech recognition,” in Speech Technology, F. Chen
and K. Jokinen, eds., Springer, pp. 1–18.
Furui, S., Maekawa, K., & H. Isahara, M. (2000), “A Japanese national project on spontaneous
speech corpus and processing technology,” Proceedings of ASR’00, pp. 244–248.
Gales, M. (1998), “Maximum likelihood linear transformations for HMM-based speech recognition,”
Computer Speech and Language 12, 75–98.
Gales, M., Center, I., & Heights, Y. (2000), “Cluster adaptive training of hidden Markov models,”
IEEE Transactions on Speech and Audio Processing 8(4), 417–428.

Gales, M. J. F. (1999), “Semi-tied covariance matrices for hidden Markov models,” IEEE
Transactions on Speech and Audio Processing 7(3), 272–281.
Gales, M. J. F., & Woodland, P. C. (1996), Variance compensation within the MLLR framework,
Technical Report 242, Cambridge University Engineering Department.
Gales, M., Watanabe, S., & Fossler-Lussier, E. (2012), “Structured discriminative models for speech
recognition,” IEEE Signal Processing Magazine 29(6), 70–81.
Ganapathiraju, A., Hamaker, J., & Picone, J. (2004), “Applications of support vector machines to
speech recognition,” IEEE Transactions on Signal Processing 52(8), 2348–2355.
Gaussier, E., & Goutte, C. (2005), “Relation between PLSA and NMF and implications,”
Proceedings of the 28th Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval, ACM, pp. 601–602.
Gauvain, J.-L., & Lee, C.-H. (1991), “Bayesian learning of Gaussian mixture densities for
hidden Markov models,” Proceedings of DARPA Speech and Natural Language Workshop,
pp. 272–277.
Gauvain, J.-L., & Lee, C.-H. (1994), “Maximum a posteriori estimation for multivariate Gaussian
mixture observations of Markov chains,” IEEE Transactions on Speech and Audio Processing
2, 291–298.
Gelman, A., Carlin, J. B., Stern, H. S., et al. (2013), Bayesian Data Analysis, CRC Press.
Geman, S., & Geman, D. (1984), “Stochastic relaxation, Gibbs distributions, and the Bayesian
restoration of images,” IEEE Transactions on Pattern Analysis and Machine Intelligence
6(1), 721–741.
Genkin, A., Lewis, D. D., & Madigan, D. (2007), “Large-scale Bayesian logistic regression for text
categorization,” Technometrics 49(3), 291–304.
Ghahramani, Z. (1998), “Learning dynamic Bayesian networks,” in Adaptive Processing of
Sequences and Data Structures, Springer, pp. 168–197.
Ghahramani, Z. (2004), “Unsupervised learning,” Advanced Lectures on Machine Learning, pp. 72–
112.
Ghosh, J. K., Delampady, M., & Samanta, T. (2007), An Introduction to Bayesian Analysis: Theory
and Methods, Springer.
Gildea, D., & Hofmann, T. (1999), “Topic-based language models using EM,” Proceedings of
European Conference on Speech Communication and Technology (EUROSPEECH),

pp. 2167–2170.
Gilks, W. R., Richardson, S., & Spiegelhalter, D. J. (1996), Markov Chain Monte Carlo in Practice,
Chapman & Hall/CRC Interdisciplinary Statistics.
Gish, H., Siu, M.-h., Chan, A., & Belfield, W. (2009), “Unsupervised training of an HMM-based
speech recognizer for topic classification,” Proceedings of Annual Conference of International
Speech Communication Association (INTERSPEECH), pp. 1935–1938.
Glass, J. (2003), “A probabilistic framework for segment-based speech recognition,” Computer
Speech & Language 17(2-3), 137–152.
Goel, V., & Byrne, W. (2000), “Minimum Bayes-risk automatic speech recognition,” Computer
Speech and Language 14, 115–135.
Goldwater, S. (2007), Nonparametric Bayesian models of lexical acquisition, PhD thesis, Brown
University.
Goldwater, S., & Griffiths, T. (2007), “A fully Bayesian approach to unsupervised part-of-speech
tagging,” Proceedings of Annual Meeting of the Association of Computational Linguistics,
pp. 744–751.
Goldwater, S., Griffiths, T., & Johnson, M. (2009), “A Bayesian framework for word segmentation:
Exploring the effects of context,” Cognition 112(1), 21–54.
Goldwater, S., Griffiths, T. L., & Johnson, M. (2006), “Interpolating between types and tokens by
estimating power-law generators,” Advances in Neural Information Processing Systems 18.
Good, I. J. (1953), “The population frequencies of species and the estimation of populations,”
Biometrika 40, 237–264.
Grézl, F., Karafiát, M., Kontár, S., & Cernocky, J. (2007), “Probabilistic and bottle-neck features for
LVCSR of meetings,” Proceedings of International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), pp. 757–760.
Griffiths, T., & Ghahramani, Z. (2005), Infinite latent feature models and the Indian buffet process,
Technical Report, Gatsby Unit.
Griffiths, T., & Steyvers, M. (2004), “Finding scientific topics,” in Proceedings of the National
Academy of Sciences, 101 Suppl. 1, 5228–5235.
Gunawardana, A., Mahajan, M., Acero, A., & Platt, J. C. (2005), “Hidden conditional random fields

for phone classification,” Proceedings of Annual Conference of International Speech
Communication Association (INTERSPEECH), pp. 1117–1120.
Haeb-Umbach, R., & Ney, H. (1992), “Linear discriminant analysis for improved large vocabulary
continuous speech recognition,” International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), Vol. 1, pp. 13–16.
Hahm, S. J., Ogawa, A., Fujimoto, M., Hori, T., & Nakamura, A. (2012), “Speaker adaptation using
variational Bayesian linear regression in normalized feature space,” in Proceedings of Annual
Conference of International Speech Communication Association (INTERSPEECH), pp. 803–
806.
Hashimoto, K., Nankaku, Y., & Tokuda, K. (2009), “A Bayesian approach to hidden semi-Markov
model based speech synthesis,” in Proceedings of Annual Conference of International Speech
Communication Association (INTERSPEECH), pp. 1751–1754.
Hashimoto, K., Zen, H., Nankaku, Y., Lee, A., & Tokuda, K. (2008), “Bayesian context clustering
using cross valid prior distribution for HMM-based speech recognition,” Proceedings of
Annual Conference of International Speech Communication Association (INTERSPEECH),
pp. 936–939.
Hashimoto, K., Zen, H., Nankaku, Y., Masuko, T., & Tokuda, K. (2009), “A Bayesian approach to
HMM-based speech synthesis,” Proceedings of International Conference on Acoustics,
Speech, and Signal Processing (ICASSP) 2009, pp. 4029–4032.
Hastings, W. K. (1970), “Monte Carlo sampling methods using Markov chains and their
applications,” Biometrika 57, 97–109.
Heigold, G., Ney, H., Schluter, R., & Wiesler, S. (2012), “Discriminative training for automatic
speech recognition: Modeling, criteria, optimization, implementation, and performance,” IEEE
Signal Processing Magazine 29(6), 58–69.
Hermansky, H. (1990), “Perceptual linear predictive (PLP) analysis of speech,” Journal of the
Acoustic Society of America 87(4), 1738–1752.
Hermansky, H., Ellis, D., & Sharma, S. (2000), “Tandem connectionist feature extraction for
conventional HMM systems,” Proceedings of International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), pp. 1635–1638.
Hinton, G., Deng, L., Yu, D., et al. (2012), “Deep neural networks for acoustic modeling in speech
recognition,” IEEE Signal Processing Magazine 29(6), 82–97.
Hinton, G., Osindero, S., & Teh, Y. (2006), “A fast learning algorithm for deep belief nets,” Neural

Computation 18, 1527–1554.
Hofmann, T. (1999a), “Probabilistic latent semantic analysis,” Proceedings of Uncertainty in
Artificial Intelligence, pp. 289–296.
Hofmann, T. (1999b), “Probabilistic latent semantic indexing,” Proceedings of the Annual
International ACM SIGIR Conference on Research and Development in Information
Retrieval, pp. 50–57.
Hofmann, T. (2001), “Unsupervised learning by probabilistic latent semantic analysis,” Machine
Learning 42(1-2), 177–196.
Hori, T., & Nakamura, A. (2013), “Speech recognition algorithms using weighted finite-state
transducers,” Synthesis Lectures on Speech and Audio Processing 9(1), 1–162.
Hu, R., & Zhao, Y. (2007), “Knowledge-based adaptive decision tree state tying for conversational
speech recognition,” IEEE Transactions on Audio, Speech, and Language Processing
15(7), 2160–2168.
Huang, S., & Renals, S. (2008), “Unsupervised language model adaptation based on topic and role
information in multiparty meeting,” Proceedings of Annual Conference of International
Speech Communication Association (INTERSPEECH), pp. 833–836.
Huang, X. D., Acero, A., & Hon, H. W. (2001), Spoken Language Processing: A Guide to Theory,
Algorithm, and System Development, Prentice Hall.
Huang, X. D., Ariki, Y., & Jack, M. A. (1990), Hidden Markov Models for Speech Recognition,
Edinburgh University Press.
Huo, Q., & Lee, C.-H. (1997), “On-line adaptive learning of the continuous density hidden Markov
model based on approximate recursive Bayes estimate,” IEEE Transactions on Speech and
Audio Processing 5(2), 161–172.
Huo, Q., & Lee, C.-H. (2000), “A Bayesian predictive classification approach to robust speech
recognition,” IEEE Transactions on Speech and Audio Processing 8, 200–204.
Ishiguro, K., Yamada, T., Araki, S., Nakatani, T., & Sawada, H. (2012), “Probabilistic speaker
diarization with bag-of-words representations of speaker angle information,” IEEE
Transactions on Audio, Speech, and Language Processing 20(2), 447–460.
Jansen, A., Dupoux, E., Goldwater, S., et al. (2013), “A summary of the 2012 JHU CLSP workshop
on zero resource speech technologies and models of early language acquisition,” Proceedings
of International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 8111–

8115.
Jelinek, F. (1976), “Continuous speech recognition by statistical methods,” Proceedings of the IEEE
64(4), 532–556.
Jelinek, F. (1997), Statistical Methods for Speech Recognition, MIT Press.
Jelinek, F., & Mercer, R. L. (1980), “Interpolated estimation of Markov source parameters from
sparse data,” Proceedings of the Workshop on Pattern Recognition in Practice, pp. 381–397.
Ji, S., Xue, Y., & Carin, L. (2008), “Bayesian compressive sensing,” IEEE Transactions on Signal
Processing 56(6), 2346–2356.
Jiang, H., Hirose, K., & Huo, Q. (1999), “Robust speech recognition based on a Bayesian prediction
approach,” IEEE Transactions on Speech and Audio Processing 7, 426–440.
Jitsuhiro, T., & Nakamura, S. (2004), “Automatic generation of non-uniform HMM structures based
on variational Bayesian approach,” Proceedings of International Conference on Acoustics,
Speech, and Signal Processing (ICASSP), pp. 805–808.
Joachims, T. (2002), “Learning to classify text using support vector machines: Methods, theory, and
algorithms,” Computational Linguistics 29(4), 656–664.
Jordan, M., Ghahramani, Z., Jaakkola, T., & Saul, L. (1999), “An introduction to variational methods
for graphical models,” Machine Learning 37(2), 183–233.
Juang, B.-H., & Rabiner, L. (1990), “The segmental K-means algorithm for estimating parameters of
hidden Markov models,” IEEE Transactions on Acoustics, Speech and Signal Processing
38(9), 1639–1641.
Juang, B., & Katagiri, S. (1992), “Discriminative learning for minimum error classification,” IEEE
Transactions on Signal Processing 40(12), 3043–3054.
Jurafsky, 
D. 
(2014), 
“From 
languages 
to 
information,”
http://www.stanford.edu/class/cs124/lec/languagemodeling.pdf .
Jurafsky, D., & Martin, J. H. (2000), Speech and Language Processing: An Introduction to Natural
Language Processing, Computational Linguistics, and Speech Recognition, Prentice Hall.
Kass, R. E., & Raftery, A. E. (1993), Bayes factors and model uncertainty, Technical Report 254,
Department of Statistics, University of Washington.

Kass, R. E., & Raftery, A. E. (1995), “Bayes factors,” Journal of the American Statistical
Association 90(430), 773–795.
Katz, S. (1987), “Estimation of probabilities from sparse data for the language model component of a
speech recognizer,” IEEE Transactions on Acoustics, Speech, and Signal Processing
35(3), 400–401.
Kawabata, T., & Tamoto, M. (1996), “Back-off method for n-gram smoothing based on binomial
posteriori distribution,” Proceedings of International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), Vol. 1, pp. 192–195.
Kenny, P. (2010), “Bayesian speaker verification with heavy tailed priors,” Keynote Speech,
Odyssey Speaker and Language Recognition Workshop.
Kenny, P., Boulianne, G., Ouellet, P., & Dumouchel, P. (2007), “Joint factor analysis versus
eigenchannels in speaker recognition,” IEEE Transactions on Audio, Speech, and Language
Processing 15(4), 1435–1447.
Kingsbury, B., Sainath, T. N., & Soltau, H. (2012), “Scalable minimum Bayes risk training of deep
neural network acoustic models using distributed Hessian-free optimization,” Proceedings of
Annual Conference of International Speech Communication Association (INTERSPEECH),
pp. 10–13.
Kinnunen, T., & Li, H. (2010), “An overview of text-independent speaker recognition: from features
to supervectors,” Speech Communication 52(1), 12–40.
Kita, K. (1999), Probabilistic Language Models, University of Tokyo Press (in Japanese).
Kneser, R., & Ney, H. (1995), “Improved backing-off for m-gram language modeling,” Proceedings
of International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 181–
184.
Kneser, R., Peters, J., & Klakow, D. (1997), “Language model adaptation using dynamic marginals,”
Proceedings of European Conference on Speech Communication and Technology
(EUROSPEECH), pp. 1971–1974.
Kolossa, D., & Haeb-Umbach, R. (2011), Robust Speech Recognition of Uncertain or Missing Data:
Theory and Applications, Springer.
Kubo, Y., Watanabe, S., Nakamura, A., & Kobayashi, T. (2010), “A regularized discriminative
training method of acoustic models derived by minimum relative entropy discrimination,”
Proceedings of Annual Conference of International Speech Communication Association
(INTERSPEECH), pp. 2954–2957.

Kudo, 
T. 
(2005), 
“Mecab: 
Yet 
another 
part-of-speech 
and 
morphological 
analyzer,”
http://mecab.sourceforge.net/.
Kuhn, R., & De Mori, R. (1990), “A cache-based natural language model for speech recognition,”
IEEE Transactions on Pattern Analysis and Machine Intelligence 12(6), 570–583.
Kuhn, R., Junqua, J., Ngyuen, P., & Niedzielski, N. (2000), “Rapid speaker adaptation in eigenvoice
space,” IEEE Transactions on Speech and Audio Processing 8(6), 695–707.
Kullback, S., & Leibler, R. A. (1951), “On information and sufficiency,” Annals of Mathematical
Statistics 22(1), 79–86.
Kwok, J. T.-Y. (2000), “The evidence framework applied to support vector machines,” IEEE
Transactions on Neural Networks 11(5), 1162–1173.
Kwon, O., Lee, T.-W., & Chan, K. (2002), “Application of variational Bayesian PCA for speech
feature extraction,” Proceedings of International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), Vol. 1, pp. 825–828.
Lafferty, J., McCallum, A., & Pereira, F. (2001), “Conditional random fields: Probabilistic models
for segmenting and labeling sequence data,” Proceedings of International Conference on
Machine Learning, pp. 282–289.
Lamel, L., Gauvain, J.-L., & Adda, G. (2002), “Lightly supervised and unsupervised acoustic model
training,” Computer Speech & Language 16(1), 115–129.
Lau, R., Rosenfeld, R., & Roukos, S. (1993), “Trigger-based language models: A maximum entropy
approach,” Proceedings of International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), Vol. 2, IEEE, pp. 45–48.
Lee, C.-H., & Huo, Q. (2000), “On adaptive decision rules and decision parameter adaptation for
automatic speech recognition,” Proceedings of the IEEE 88, 1241–1269.
Lee, C.-H., Lin, C.-H., & Juang, B.-H. (1991), “A study on speaker adaptation of the parameters of
continuous density hidden Markov models,” IEEE Transactions on Acoustics, Speech, and
Signal Processing 39, 806–814.
Lee, C.-Y. (2014), Discovering linguistic structures in speech: models and applications, PhD thesis,
Massachusetts Institute of Technology.
Lee, C.-Y.,& Glass., J. (2012), “A nonparametric Bayesian approach to acoustic model discovery,”

Proceedings of Annual Meeting of the Association for Computational Linguistics, pp. 40–49.
Lee, C.-Y., Zhang, Y., & Glass, J. (2013), “Joint learning of phonetic units and word pronunciations
for ASR,” Proceedings of the 2013 Conference on Empirical Methods on Natural Language
Processing (EMNLP), pp. 182–192.
Lee, D. D., & Seung, H. S. (1999), “Learning the parts of objects by non-negative matrix
factorization,” Nature 401(6755), 788–791.
Leggetter, C. J., & Woodland, P. C. (1995), “Maximum likelihood linear regression for speaker
adaptation of continuous density hidden Markov models,” Computer Speech and Language
9, 171–185.
Lewis, D. D. (1998), “Naive (Bayes) at forty: The independence assumption in information
retrieval,” Proceedings of the 10th European Conference on Machine Learning, Springer-
Verlag, pp. 4–15.
Liu, J. (1994), “The collapsed Gibbs sampler in Bayesian computations with applications to a gene
regulation problem,” Journal of the American Statistical Association 89(427).
Liu, J. S. (2008), Monte Carlo Strategies in Scientific Computing, Springer.
Livescu, K., Glass, J. R., & Bilmes, J. (2003), “Hidden feature models for speech recognition using
dynamic Bayesian networks,” Proceedings of Annual Conference of International Speech
Communication Association (INTERSPEECH), pp. 2529–2532.
MacKay, D. J. C. (1992a), “Bayesian interpolation,” Neural Computation 4(3), 415–447.
MacKay, D. J. C. (1992b), “The evidence framework applied to classification networks,” Neural
Computation 4(5), 720–736.
MacKay, D. J. C. (1992c), “A practical Bayesian framework for back-propagation networks,” Neural
Computation 4(3), 448–472.
MacKay, D. J. C. (1995), “Probable networks and plausible predictions – a review of practical
Bayesian methods for supervised neural networks,” Network: Computation in Neural Systems
6(3), 469–505.
MacKay, D. J. C. (1997), Ensemble learning for hidden Markov models, Technical Report,
Cavendish Laboratory, University of Cambridge.
MacKay, D. J. C., & Peto, L. C. B. (1995), “A hierarchical Dirichlet language model,” Natural
Language Engineering 1(3), 289–308.

Maekawa, T., & Watanabe, S. (2011), “Unsupervised activity recognition with user’s
physical characteristics data,” Proceedings of International Symposium on Wearable
Computers, pp. 89–96.
Mak, B., Kwok, J., & Ho, S. (2005), “Kernel eigenvoice speaker adaptation,” IEEE Transactions on
Speech and Audio Processing 13(5), 984–992.
Manning, C. D., & Schütze, H. (1999), Foundations of Statistical Natural Language Processing,
MIT Press.
Masataki, H., Sagisaka, Y., Hisaki, K., & Kawahara, T. (1997), “Task adaptation using MAP
estimation in n-gram language modeling,” Proceedings of International Conference on
Acoustics, Speech, and Signal Processing (ICASSP), pp. 783–786.
Matsui, T., & Furui, S. (1994), “Comparison of text-independent speaker recognition methods using
VQ-distortion and discrete/continuous HMMs,” IEEE Transactions on Speech and Audio
Processing 2(3), 456–459.
Matsumoto, Y., Kitauchi, A., Yamashita, T., et al. (1999), “Japanese morphological analysis system
ChaSen version 2.0 manual,” NAIST Technical Report.
McCallum, A., & Nigam, K. (1998), “A comparison of event models for naive Bayes text
classification,” in Proceedings of the Association for the Advancement of Artificial
Intelligence (AAAI) Workshop on Learning for Text Categorization, Vol. 752, pp. 41–48.
McDermott, E., Hazen, T., Le Roux, J., Nakamura, A., & Katagiri, S. (2007), “Discriminative
training for large-vocabulary speech recognition using minimum classification error,” IEEE
Transactions on Audio, Speech, and Language Processing 15(1), 203–223.
Meignier, S., Moraru, D., Fredouille, C., Bonastre, J.-F., & Besacier, L. (2006), “Step-by-step and
integrated approaches in broadcast news speaker diarization,” Computer Speech & Language
20(2), 303–330.
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., & Teller, E. (1953), “Equation
of state calculations by fast computing machines,” Journal of Chemical Physics 21(6), 1087–
1092.
Minka, T. P. (2001), “Expectation propagation for approximate Bayesian inference,” Proceedings of
Conference on Uncertainty in Artificial Intelligence (UAI), pp. 362–369.
Mochihashi, D., Yamada, T., & Ueda, N. (2009), “Bayesian unsupervised word segmentation with
nested Pitman–Yor language modeling,” Proceedings of Joint Conference of Annual Meeting

of the ACL and International Joint Conference on Natural Language Processing of the
AFNLP, pp. 100–108.
Mohri, M., Pereira, F., & Riley, M. (2002), “Weighted finite-state transducers in speech recognition,”
Computer Speech and Language 16, 69–88.
Moraru, D., Meignier, S., Besacier, L., Bonastre, J.-F., & Magrin-Chagnolleau, I. (2003), “The
ELISA consortium approaches in speaker segmentation during the NIST 2002 speaker
recognition evaluation,” Proceedings of International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), Vol. 2, pp. 89–92.
Mrva, D., & Woodland, P. C. (2004), “A PLSA-based language model for conversational telephone
speech,” in Proceedings of Annual Conference of International Speech Communication
Association (INTERSPEECH), pp. 2257–2260.
Murphy, K. P. (2002), Dynamic Bayesian networks: representation, inference and learning, PhD
thesis, University of California, Berkeley.
Murphy, K. P., Weiss, Y., & Jordan, M. I. (1999), “Loopy belief propagation for approximate
inference: An empirical study,” Proceedings of Conference on Uncertainty in Artificial
Intelligence (UAI), pp. 467–475.
Nadas, A. (1985), “Optimal solution of a training problem in speech recognition,” IEEE
Transactions on Acoustics, Speech and Signal Processing 33(1), 326–329.
Nakagawa, S. (1988), Speech Recognition by Probabilistic Model, Institute of Electronics,
Information and Communication Engineers (IEICE) (in Japanese).
Nakamura, A., McDermott, E., Watanabe, S., & Katagiri, S. (2009), “A unified view for
discriminative objective functions based on negative exponential of difference measure
between strings,” Proceedings of International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), pp. 1633–1636.
Neal, R., & Hinton, G. (1998), “A view of the EM algorithm that justifies incremental, sparse, and
other variants,” Learning in Graphical Models, pp. 355–368.
Neal, R. M. (1992), “Bayesian mixture modeling,” Proceedings of the Workshop on Maximum
Entropy and Bayesian Methods of Statistical Analysis 11, 197–211.
Neal, R. M. (1993), “Probabilistic inference using Markov chain Monte Carlo methods,” Technical
Report CRG-TR-93-1, Dept. of Computer Science, University of Toronto.
Neal, R. M. (2000), “Markov chain sampling methods for Dirichlet process mixture models,”
Journal of Computational and Graphical Statistics 9(2), 249–265.

Neal, R. M. (2003), “Slice sampling,” Annals of Statistics 31, 705–767.
Nefian, A. V., Liang, L., Pi, X., Liu, X., & Murphy, K. (2002), “Dynamic Bayesian networks
for audio-visual speech recognition,” EURASIP Journal on Applied Signal Processing
11, 1274–1288.
Neubig, G., Mimura, M., Mori, S., & Kawahara, T. (2010), “Learning a language model from
continuous speech,” Proceedings of Annual Conference of International Speech
Communication Association (INTERSPEECH), pp. 1053–1056.
Ney, H., Essen, U., & Kneser, R. (1994), “On structuring probabilistic dependences in stochastic
language modeling,” Computer Speech and Language 8, 1–38.
Ney, H., Haeb-Umbach, R., Tran, B.-H., & Oerder, M. (1992), “Improvements in beam search for
10000-word continuous speech recognition,” Proceedings of International Conference on
Acoustics, Speech, and Signal Processing (ICASSP), Vol. 1, IEEE, pp. 9–12.
Niesler, T., & Willett, D. (2002), “Unsupervised language model adaptation for lecture speech
transcription,” Proceedings of Annual Conference of International Speech Communication
Association (INTERSPEECH), pp. 1413–1416.
Normandin, Y. (1992), “Hidden Markov models, maximum mutual information estimation, and the
speech recognition problem,” PhD thesis, McGill University, Montreal, Canada.
Odell, J. J. (1995), The use of context in large vocabulary speech recognition, PhD thesis, Cambridge
University.
Ostendorf, M., & Singer, H. (1997), “HMM topology design using maximum likelihood successive
state splitting,” Computer Speech and Language 11, 17–41.
Paul, D. B., & Baker, J. M. (1992), “The design for the Wall Street Journal-based CSR corpus,”
Proceedings of the Workshop on Speech and Natural Language, Association for
Computational Linguistics, pp. 357–362.
Pettersen, S. (2008), Robust speech recognition in the presence of additive noise, PhD thesis,
Norwegian University of Science and Technology.
Pitman, J. (2002), “Poisson-Dirichlet and GEM invariant distributions for split-and-merge
transformation of an interval partition,” Combinatorics, Probability and Computing 11, 501–
514.
Pitman, J. (2006), Combinatorial Stochastic Processes, Springer-Verlag.

Pitman, J., & Yor, M. (1997), “The two-parameter Poisson-Dirichlet distribution derived from a
stable subordinator,” Annals of Probability 25(2), 855–900.
Porteous, I., Newman, D., Ihler, A., et al. (2008), “Fast collapsed Gibbs sampling for latent Dirichlet
allocation,” Proceedings of the 14th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pp. 569–577.
Povey, D. (2003), Discriminative training for large vocabulary speech recognition, PhD thesis,
Cambridge University.
Povey, D., Burget, L., Agarwal, M., et al. (2010), “Subspace Gaussian mixture models for speech
recognition,” Proceedings of International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), pp. 4330–4333.
Povey, D., Gales, M. J. F., Kim, D., & Woodland, P. C. (2003), “MMI-MAP and MPE-MAP for
acoustic model adaptation,” Proceedings of European Conference on Speech Communication
and Technology (EUROSPEECH) 8, 1981–1984.
Povey, D., Ghoshal, A., Boulianne, G., et al. (2011), “The Kaldi speech recognition toolkit,”
Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop (ASRU).
Povey, D., Kanevsky, D., Kingsbury, B., et al. (2008), “Boosted MMI for model and feature-space
discriminative training,” Proceedings of International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), pp. 4057–4060.
Povey, D., Kingsbury, B., Mangu, L., et al. (2005), “fMPE: Discriminatively trained features for
speech recognition,” Proceedings of International Conference on Acoustics, Speech, and
Signal Processing (ICASSP) 1, 961–964.
Povey, D., & Woodland, P. C. (2002), “Minimum phone error and I-smoothing for improved
discriminative training,” Proceedings of International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), pp. 13–17.
Povey, D., Woodland, P., & Gales, M. (2003), “Discriminative MAP for acoustic model adaptation,”
Proceedings of International Conference on Acoustics, Speech, and Signal Processing
(ICASSP) 1, I–312.
Price, P., Fisher, W., Bernstein, J., & Pallett, D. (1988), “The DARPA 1000-word resource
management database for continuous speech recognition,” Proceedings of International
Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 651–654.
Rabiner, L. R., & Juang, B.-H. (1986), “An introduction to hidden Markov models,” IEEE ASSP
Magazine 3(1), 4–16.

Rabiner, L. R., & Juang, B.-H. (1993), Fundamentals of Speech Recognition, Vol. 14, PTR Prentice
Hall.
Rasmussen, C. E. (1999), “The infinite Gaussian mixture model,” Advances in Neural Information
Processing Systems 12, 554–560.
Rasmussen, C. E., & Williams, C. K. I. (2006), Gaussian Processes for Machine Learning, Adaptive
Computation and Machine Learning, MIT Press.
Reynolds, D., Quatieri, T., & Dunn, R. (2000), “Speaker verification using adapted Gaussian mixture
models,” Digital Signal Processing 10(1-3), 19–41.
Rissanen, J. (1984), “Universal coding, information, prediction and estimation,” IEEE Transactions
on Information Theory 30, 629–636.
Rodriguez, A., Dunson, D. B., & Gelfand, A. E. (2008), “The nested Dirichlet process,” Journal of
the American Statistical Association 103(483), 1131–1154.
Rosenfeld, R. (2000), “Two decades of statistical language modeling: Where do we go from here?,”
Proceedings of the IEEE 88(8), 1270–1278.
Sainath, T. N., Ramabhadran, B., Picheny, M., Nahamoo, D., & Kanevsky, D. (2011), “Exemplar-
based sparse representation features: from TIMIT to LVCSR,” IEEE Transactions on Audio,
Speech and Language Processing 19(8), 2598–2613.
Saito, D., Watanabe, S., Nakamura, A., & Minematsu, N. (2012), “Statistical voice conversion based
on noisy channel model,” IEEE Transactions on Audio, Speech, and Language Processing
20(6), 1784–1794.
Salakhutdinov, R. (2009), Learning deep generative models, PhD thesis, University of Toronto.
Salton, G., & Buckley, C. (1988), “Term-weighting approaches in automatic text retrieval,”
Information Processing & Management 24(5), 513–523.
Sanderson, C., Bengio, S., & Gao, Y. (2006), “On transforming statistical models for non-frontal face
verification,” Pattern Recognition 39(2), 288–302.
Sankar, A., & Lee, C.-H. (1996), “A maximum-likelihood approach to stochastic matching for robust
speech recognition,” IEEE Transactions on Speech and Audio Processing 4(3), 190–202.
Saon, G., & Chien, J.-T. (2011), “Some properties of Bayesian sensing hidden Markov models,”

Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),
pp. 65–70.
Saon, G., & Chien, J.-T. (2012a), “Bayesian sensing hidden Markov models,” IEEE Transactions on
Audio, Speech, and Language Processing 20(1), 43–54.
Saon, G., & Chien, J.-T. (2012b), “Large-vocabulary continuous speech recognition systems: A look
at some recent advances,” IEEE Signal Processing Magazine 29(6), 18–33.
Schalkwyk, J., Beeferman, D., Beaufays, F., et al. (2010), “ ‘Your word is my command’: Google
search by voice: A case study,” in Advances in Speech Recognition, Springer, pp. 61–90.
Schlüter, R., Macherey, W., Müller, B., & Ney, H. (2001), “Comparison of discriminative training
criteria and optimization methods for speech recognition,” Speech Communication
34(3), 287–310.
Schultz, T., & Waibel, A. (2001), “Language-independent and language-adaptive acoustic modeling
for speech recognition,” Speech Communication 35(1), 31–51.
Schwarz, G. (1978), “Estimating the dimension of a model,” The Annals of Statistics 6(2), 461–464.
Scott, S. (2002), “Bayesian methods for hidden Markov models,” Journal of the American Statistical
Association 97(457), 337–351.
Seide, F., Li, G., Chen, X., & Yu, D. (2011), “Conversational speech transcription using context-
dependent deep neural networks,” Proceedings of Annual Conference of International Speech
Communication Association (INTERSPEECH), pp. 437–440.
Sethuraman, J. (1994), “A constructive definition of Dirichlet priors,” Statistica Sinica 4, 639–650.
Shikano, K., Kawahara, T., Kobayashi, T., et al. (1999), Japanese Dictation Toolkit – Free Software
Repository for Automatic Speech Recognition, http://www.ar.media.kyoto-u.ac.jp/dictation/.
Shinoda, K. (2010), “Acoustic model adaptation for speech recognition,” IEICE Transactions on
Information and Systems 93(9), 2348–2362.
Shinoda, K., & Inoue, N. (2013), “Reusing speech techniques for video semantic indexing,” IEEE
Signal Processing Magazine 30(2), 118–122.
Shinoda, K., & Iso, K. (2001), “Efficient reduction of Gaussian components using MDL criterion for
HMM-based speech recognition,” Proceedings of International Conference on Acoustics,

Speech, and Signal Processing (ICASSP), pp. 869–872.
Shinoda, K., & Lee, C.-H. (2001), “A structural Bayes approach to speaker adaptation,” IEEE
Transactions on Speech and Audio Processing 9, 276–287.
Shinoda, K., & Watanabe, T. (1996), “Speaker adaptation with autonomous model complexity
control by MDL principle,” Proceedings of International Conference on Acoustic, Speech, and
Signal Processing (ICASSP), pp. 717–720.
Shinoda, K., & Watanabe, T. (1997), “Acoustic modeling based on the MDL criterion for speech
recognition,” Proceedings of European Conference on Speech Communication and
Technology (EUROSPEECH), Vol. 1, pp. 99–102.
Shinoda, K., & Watanabe, T. (2000), “MDL-based context-dependent subword modeling for speech
recognition,” Journal of the Acoustical Society of Japan (E) 21, 79–86.
Shiota, S., Hashimoto, K., Nankaku, Y., & Tokuda, K. (2009), “Deterministic annealing based
training algorithm for Bayesian speech recognition,” Proceedings of Annual Conference of
International Speech Communication Association (INTERSPEECH), pp. 680–683.
Siohan, O., Myrvoll, T. A., & Lee, C. H. (2002), “Structural maximum a posteriori linear regression
for fast HMM adaptation,” Computer Speech and Language 16(1), 5–24.
Siu, M.-h., Gish, H., Chan, A., Belfield, W., & Lowe, S. (2014), “Unsupervised training of an HMM-
based self-organizing unit recognizer with applications to topic classification and keyword
discovery,” Computer Speech & Language 28(1), 210–223.
Somervuo, P. (2004), “Comparison of ML, MAP, and VB based acoustic models in large vocabulary
speech recognition,” Proceedings of International Conference on Spoken Language
Processing (ICSLP), pp. 830–833.
Spiegelhalter, D. J., & Lauritzen, S. L. (1990), “Sequential updating of conditional probabilities on
directed graphical structures,” Networks 20(5), 579–605.
Sproat, R., Gale, W., Shih, C., & Chang, N. (1996), “A stochastic finite-state word-segmentation
algorithm for Chinese,” Computational Linguistics 22(3), 377–404.
Stenger, B., Ramesh, V., Paragios, N., Coetzee, F., & Buhmann, J. M. (2001), “Topology free hidden
Markov models: Application to background modeling,” Proceedings of International
Conference on Computer Vision (ICCV)’, Vol. 1, pp. 294–301.
Stolcke, A., Ferrer, L., Kajarekar, S., Shriberg, E., & Venkataraman, A. (2005), “MLLR transforms
as features in speaker recognition,” Proceedings of Annual Conference of International Speech

Communication Association (INTERSPEECH), pp. 2425–2428.
Stolcke, A., & Omohundro, S. (1993), “Hidden Markov model induction by Bayesian model
merging,” Advances in Neural Information Processing Systems, pp. 11–18, Morgan
Kaufmann.
Takahashi, J., & Sagayama, S. (1997), “Vector-field-smoothed Bayesian learning for fast
and incremental speaker/telephone-channel adaptation,” Computer Speech and Language
11, 127–146.
Takami, J., & Sagayama, S. (1992), “A successive state splitting algorithm for efficient allophone
modeling,” Proceedings of International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), pp. 573–576.
Tam, Y.-C., & Schultz, T. (2005), “Dynamic language model adaptation using variational Bayes
inference,” Proceedings of Annual Conference of International Speech Communication
Association (INTERSPEECH), pp. 5–8.
Tam, Y.-C., & Schultz, T. (2006), “Unsupervised language model adaptation using latent semantic
marginals,” Proceedings of Annual Conference of International Speech Communication
Association (INTERSPEECH), pp. 2206–2209.
Tamura, M., Masuko, T., Tokuda, K., & Kobayashi, T. (2001), “Adaptation of pitch and spectrum for
HMM-based speech synthesis using MLLR,” Proceedings of International Conference on
Acoustics, Speech, and Signal Processing (ICASSP), pp. 805–808.
Tawara, N., Ogawa, T., Watanabe, S., & Kobayashi, T. (2012a), “Fully Bayesian inference of multi-
mixture Gaussian model and its evaluation using speaker clustering,” Proceedings of
International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 5253–
5256.
Tawara, N., Ogawa, T., Watanabe, S., Nakamura, A., & Kobayashi, T. (2012b), “Fully Bayesian
speaker clustering based on hierarchically structured utterance-oriented Dirichlet process
mixture model,” Proceedings of Annual Conference of International Speech Communication
Association (INTERSPEECH), pp. 2166–2169.
Teh, Y. W. (2006), “A hierarchical Bayesian language model based on Pitman–Yor processes,”
Proceedings of International Conference on Computational Linguistics and Annual Meeting of
the Association for Computational Linguistics, pp. 985–992.
Teh, Y. W., Jordan, M. I., Beal, M. J., & Blei, D. M. (2006), “Hierarchical Dirichlet processes,”
Journal of the American Statistical Association 101(476), 1566–1581.

Tipping, M. E. (2001), “Sparse Bayesian learning and the relevance vector machine,” Journal of
Machine Learning Research 1, 211–244.
Torbati, A. H. H. N., Picone, J., & Sobel, M. (2013), “Speech acoustic unit segmentation using
hierarchical Dirichlet processes,” Proceedings of Annual Conference of International Speech
Communication Association (INTERSPEECH), pp. 637–641.
Ueda, N., & Ghahramani, Z. (2002), “Bayesian model search for mixture models based on
optimizing variational bounds,” Neural Networks 15, 1223–1241.
Valente, F. (2006), “Infinite models for speaker clustering,” Proceedings of Annual Conference of
International Speech Communication Association (INTERSPEECH), pp. 1329–1332.
Valente, F., Motlicek, P., & Vijayasenan, D. (2010), “Variational Bayesian speaker diarization of
meeting recordings,” Proceedings of International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), pp. 4954–4957.
Valente, F., & Wellekens, C. (2003), “Variational Bayesian GMM for speech recognition,”
Proceedings of European Conference on Speech Communication and Technology
(EUROSPEECH), pp. 441–444.
Valente, F. & Wellekens, C. (2004a), “Variational Bayesian feature selection for Gaussian mixture
models,” Proceedings of International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), Vol. 1, pp. 513–516.
Valente, F., & Wellekens, C. (2004b), “Variational Bayesian speaker clustering,” Proceedings of
ODYSSEY The Speaker and Language Recognition Workshop, pp. 207–214.
Vapnik, V. (1995), The Nature of Statistical Learning Theory, Springer-Verlag.
Veselỳ, K., Ghoshal, A., Burget, L., & Povey, D. (2013), “Sequence-discriminative training of deep
neural networks,” Proceedings of Annual Conference of International Speech Communication
Association (INTERSPEECH), pp. 2345–2349.
Villalba, J., & Brümmer, N. (2011), “Towards fully Bayesian speaker recognition: Integrating out the
between-speaker covariance,” Proceedings of Annual Conference of International Speech
Communication Association (INTERSPEECH), pp. 505–508.
Vincent, E., Barker, J., Watanabe, S., et al. (2013), “The second ‘CHiME’ speech separation and
recognition challenge: An overview of challenge systems and outcomes,” Proceedings of
IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pp. 162–167.

Viterbi, A. J. (1967), “Error bounds for convolutional codes and an asymptotically optimal decoding
algorithm,” IEEE Transactions on Information Theory IT-13, 260–269.
Wallach, H. M. (2006), “Topic modeling: beyond bag-of-words,” Proceedings of International
Conference on Machine Learning, pp. 977–984.
Watanabe, S., & Chien, J. T. (2012), “Tutorial: Bayesian learning for speech and language
processing,” International Conference on Acoustics, Speech, and Signal Processing (ICASSP).
Watanabe, S., Minami, Y., Nakamura, A., & Ueda, N. (2002), “Application of variational Bayesian
approach to speech recognition,” Advances in Neural Information Processing Systems.
Watanabe, S., Minami, Y., Nakamura, A., & Ueda, N. (2004), “Variational Bayesian estimation and
clustering for speech recognition,” IEEE Transactions on Speech and Audio Processing
12, 365–381.
Watanabe, S., & Nakamura, A. (2004), “Acoustic model adaptation based on coarse-fine training of
transfer vectors and its application to a speaker adaptation task,” Proceedings of International
Conference on Spoken Language Processing (ICSLP), pp. 2933–2936.
Watanabe, S., & Nakamura, A. (2006), “Speech recognition based on Student’s t-distribution derived
from total Bayesian framework,” IEICE Transactions on Information and Systems E89-
D, 970–980.
Watanabe, S., & Nakamura, A. (2009), “On-line adaptation and Bayesian detection of environmental
changes based on a macroscopic time evolution system,” Proceedings of IEEE International
Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 4373–4376.
Watanabe, S., Nakamura, A., & Juang, B. (2011), “Bayesian linear regression for hidden Markov
model based on optimizing variational bounds,” Proceedings of IEEE Workshop on Machine
Learning for Signal Processing, pp. 1–6.
Watanabe, S., Nakamura, A., & Juang, B.-H. (2013), “Structural Bayesian linear regression for
hidden Markov models,” Journal of Signal Processing Systems, 1–18.
Wegmann, S., McAllaster, D., Orloff, J., & Peskin, B. (1996), “Speaker normalization on
conversational telephone speech,” Proceedings of International Conference on Acoustics,
Speech, and Signal Processing (ICASSP), pp. 339–341.
Winn, J., & Bishop, C. (2006), “Variational message passing,” Journal of Machine Learning
Research 6(1), 661.

Witten, I. H., & Bell, T. C. (1991), “The zero-frequency problem: estimating the probabilities of
novel events in adaptive text compression,” IEEE Transactions on Information Theory
37, 1085–1094.
Wooters, C., Fung, J., Peskin, B., & Anguera, X. (2004), “Towards robust speaker segmentation: The
ICSI-SRI fall 2004 diarization system,” in RT-04F Workshop, Vol. 23.
Wooters, C., & Huijbregts, M. (2008), “The ICSI RT07s speaker diarization system,” in Multimodal
Technologies for Perception of Humans, Springer, pp. 509–519.
Yamagishi, J., Kobayashi, T., Nakano, Y., Ogata, K., & Isogai, J. (2009), “Analysis of speaker
adaptation algorithms for HMM-based speech synthesis and a constrained SMAPLR
adaptation algorithm,” IEEE Transactions on Audio, Speech, and Language Processing
17(1), 66–83.
Yaman, S., Chien, J.-T., & Lee, C.-H. (2007), “Structural Bayesian language modeling and
adaptation,” Proceedings of Annual Conference of International Speech Communication
Association (INTERSPEECH), pp. 2365–2368.
Yedidia, J. S., Freeman, W. T., & Weiss, Y. (2003), “Understanding belief propagation and its
generalizations,” Exploring Artificial Intelligence in the New Millennium 8, 236–239.
Young, S., Evermann, G., Gales, M., et al. (2006), “The HTK book (for HTK version 3.4),”
Cambridge University Engineering Department.
Young, S. J., Odell, J. J., & Woodland, P. C. (1994), “Tree-based state tying for high accuracy
acoustic modelling,” Proceedings of the Workshop on Human Language Technology, pp. 307–
312.
Yu, K., & Gales, M. J. F. (2006), “Incremental adaptation using Bayesian inference,” Proceedings of
International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 217–
220.
Zhang, Y., & Glass, J. R. (2009), “Unsupervised spoken keyword spotting via segmental DTW on
Gaussian posteriorgrams,” Proceedings of IEEE Automatic Speech Recognition &
Understanding Workshop (ASRU), pp. 398–403.
Zhang, Y., Liu, P., Chien, J.-T. & Soong, F. (2009), “An evidence framework for Bayesian learning
of continuous-density hidden Markov models,” Proceedings of International Conference on
Acoustics, Speech, and Signal Processing (ICASSP), pp. 3857–3860.
Zhao, X., Dong, Y., Zhao, J., et al. (2009), “Variational Bayesian joint factor analysis for speaker

verification,” Proceedings of IEEE International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), pp. 4049–4052.
Zhou, B., & Hansen, J. H. (2000), “Unsupervised audio stream segmentation and clustering via the
Bayesian information criterion,” Proceedings of Annual Conference of International Speech
Communication Association (INTERSPEECH), pp. 714–717.
Zweig, G., & Nguyen, P. (2009), “A segmental CRF approach to large vocabulary continuous speech
recognition,” Proceedings of IEEE Automatic Speech Recognition and Understanding
Workshop (ASRU), pp. 152–157.
Zweig, G., & Russell, S. (1998), “Speech recognition with dynamic Bayesian networks,”
Proceedings of the National Conference on Artificial Intelligence, pp. 173–180.

Index
Page numbers in bold type indicate a main explanatory entry.
Acoustic model, 3, 55, 59
Acoustic unit discovery, 372
Agglomerative clustering, 240
Akaike information criterion, 217
Approximate inference, 47, 137, 320, 385
Automatic relevance determination, 192, 193
Backoff smoothing, 107, 108 208, 380
Backward algorithm, 66, 73
Backward variable, 72, 85, 90, 159, 266
Bag of words, 114, 318 , 324
Basis representation, 192
Baum–Welch algorithm, 90, 160, 254, 257, 284, 285
Bayes decision, 8, 54, 97, 218, 274
Bayes factor, 214, 233, 238, 240, 282
Bayes risk, 55, 56
Bayesian compressive sensing, 193
Bayesian information criterion, 211, 214, 216, 273, 361
Bayesian interpretation, 189, 379
Bayesian model comparison, 185
Bayesian network, 41
Bayesian neural network, 225
Bayesian nonparametrics, 337, 345
Bayesian predictive classification, 58, 218, 274, 282
Belief propagation, 46
Bernoulli distribution, 131, 226
Beta distribution, 348, 386, 393, 394

Canonical form, 17, 24, 27, 34
Chinese restaurant franchise, 355
Chinese restaurant process, 349, 380, 383
Cholesky decomposition, 94, 289
Co-occurrence, 113
Collapsed Gibbs sampling, 344, 363
Complete data, 61, 77
Complete data likelihood, 61, 65
Complete square, 30, 34, 152, 391
Concave function, 244, 389
Concentration parameter, 348, 385
Conditional independence, 38, 40, 119, 132, 244, 341
Conditional probability, 14
Conjugate distribution, 17, 24, 138, 292
Conjugate prior, 25, 177, 178, 191, 206, 223, 344
Context-dependent phoneme, 59
Continuous density hidden Markov model, 63, 143, 254, 373
Convex function, 78, 320
Corrective training, 177
Correlation matrix, 114
Cosine similarity, 114, 118, 173
Cross entropy error function, 228
Cross validation, 189
De Finetti’s theorem, 346, 348
Decision tree clustering, 230, 282
Decoding algorithm, 51, 204
Deep neural network, 4, 224
Di-gamma function, 209, 262, 265, 321, 389
Dirac delta function, 16, 33, 140, 213, 246, 388
Dirichlet class language model, 326
Dirichlet distribution, 38, 144, 190, 256, 394
Dirichlet process, 348
Dirichlet process mixture model, 351, 373
Discrete uniform distribution, 102, 392
Discriminative approach, 130
Discriminative training, 166, 200
Distribution estimation, 47, 184, 187, 193, 228, 269, 379
Document model, 176, 318

Dynamic Bayesian network, 41
Dynamic programming, 70, 75
Eigenvoice, 164
EM algorithm, 66, 76, 113, 120, 123, 141, 178, 227, 251
Emission probability, 62
Entropy, 116
Evidence framework, 185, 190, 208
Evidence function, 17, 184, 185, 188, 196, 207, 208
Exchangeability, 346, 357
Expectation propagation, 7
Exponential family, 17, 26, 188
Extended Baum–Welch algorithm, 167
Factor analysis, 7, 174, 197, 307, 317
Factor graph, 45
Factor loading, 197
Feature extraction, 4, 9, 336
Feature-space MLLR, 204, 289
Fisher information, 216
Forward algorithm, 66, 71
Forward variable, 70, 75, 85, 90, 158, 266
Forward–backward algorithm, 46, 70
Gamma distribution, 30, 38, 256, 308, 400
Gamma function, 30, 278, 389
Gaussian distribution, 18–20, 38, 395
Gaussian mixture model, 4, 66, 67, 172, 361
Gaussian supervector, 173 , 306
Gaussian–gamma distribution, 31, 38, 402
Gaussian–Wishart distribution, 35, 38, 190, 256, 403
GEM distribution, 349
Generalization error, 191
Generative approach, 130
Generative model, 43, 53
Gibbs sampling, 343, 347, 373, 375, 386
Graphical model, 13, 40, 67, 367
Heavy-tailed distribution, 383

Hessian matrix, 187, 212, 215, 221, 229
Hidden Markov model, 3, 59, 68, 188
Hidden variable, 77, 309, 375
Hierarchical Dirichlet language model, 205, 379, 383
Hierarchical Dirichlet process, 337, 352
Hierarchical Pitman–Yor language model, 378, 384
Hierarchical Pitman–Yor process, 383
Hierarchical prior, 193, 206, 207, 291, 356, 384
Hyperparameter, 25, 43, 180, 184, 187, 208, 219, 229
i-smoothing factor, 169
Ill-posed problem, 188, 218
Implicit solution, 198, 200, 210
Importance sampling, 338
Importance weights, 339
Incomplete data, 77, 195, 227
Incremental hyperparameter, 180
Incremental learning, 177, 179
Independently, identically distributed (iid), 216, 349
Infinitely exchangeable, 346
Information retrieval, 3, 10, 115
Interpolation smoothing, 102, 103, 104, 176, 208
Inverse gamma distribution, 401
Jacobian, 338
Jelinek–Mercer smoothing, 102
Jensen’s inequality, 76, 244, 290, 320, 389
Joint probability, 14
Katz smoothing, 106
Kernel parameter, 191
Kronecker delta function, 16, 85, 246, 324, 388
Kullback–Leibler divergence, 79, 201, 243, 288, 290, 316, 320
 regularization, 217
 regularization, 139
 regularization, 139
Lagrange multiplier, 86, 100, 123, 178, 246, 253
Language model, 3, 55, 97, 97, 116, 125, 205, 208, 324, 378, 383

Laplace approximation, 187, 211, 220, 229
Laplace distribution, 139, 399
Latent Dirichlet allocation, 360
Latent model, 61
Latent semantic analysis, 113, 177
Latent topic, 119, 319
Latent topic model, 7, 43, 53, 119, 123, 138, 176, 242, 318, 337, 360
Least-squares regression, 188
Level-1 inference, 188
Level-2 inference, 188
Levenshtein distance, 56
Lexical model, 60
Linear regression, 188
Logistic regression, 46
Logistic sigmoid, 58, 226
Loopy belief propagation, 47
Loss function, 55
Machine learning, 3
Machine translation, 3, 55, 97
MAP adaptation, 165, 172
MAP decision rule, 55, 58, 129, 130
MAP–EM algorithm, 143
Marginal likelihood, 17, 51, 65, 70, 184, 196, 208, 228, 293, 319, 361, 362, 368
Marginalization, 50
Markov chain, 340
Markov chain Monte Carlo, 337, 347, 356, 358, 385
Markov random field, 40, 44
Matrix variate Gaussian distribution, 292, 399
Max sum algorithm, 46
Maximum a-posteriori, 49, 137, 177, 228
Maximum evidence, 186
Maximum likelihood, 76, 113, 120, 227
Maximum likelihood linear regression, 91, 163, 164, 174, 200, 287
Maximum mutual information, 167, 201
Message passing, 46
Metropolis–Hastings algorithm, 341
Minimum Bayes risk, 56
Minimum classification error, 167

Minimum description length, 230, 273
Minimum discrimination information, 127
Minimum mean square error, 139
Minimum phone error, 167
Model selection, 8, 49, 186
Model variable, 131
Modified Kneser–Ney smoothing, 110, 204, 387
Multilayer perceptron, 224, 226, 326
Multinomial distribution, 22, 62, 99, 392
Multivariate Gaussian distribution, 38, 396
-gram, 3, 97, 99, 174
Naive Bayes classifier, 39, 41
Natural parameter, 17
Nested Chinese restaurant process, 360
Nested Dirichlet process, 360
Neural network, 188, 224, 326
Noisy channel model, 8, 55
Noisy speech recognition, 188, 224
Non-negative matrix factorization, 124
Occam factor, 186
Over-fitting problem, 5, 188
Pólya urn model, 346, 348
Partition function, 45
Phoneme, 59
Pitman–Yor process, 379
Plug-in MAP, 140
Point estimation, 47, 184, 187, 218
Positive definite, 197, 396
Posterior, 6, 15, 133, 185
Power-law, 110, 379, 380
Precision matrix, 34
Predictive distribution, 219, 275
Prior, 15, 48, 138, 185
Probabilistic latent semantic analysis, 119, 120, 176, 318
Product rule, 14, 132
Proposal distribution, 339

Quasi-Bayes, 179
Regression tree, 92, 287
Regularization, 6, 49, 138, 141, 167, 188
Regularization parameter, 139, 188, 191
Regularization theory, 188
Regularized least-squares, 188
Relevance vector machine, 192
Reproducible distribution pair, 181
Robust decision rule, 58, 218
Robustness, 50, 203
Segmental K-means, 90
Singular value decomposition, 114, 177
Slice sampling, 344
Sparse Bayesian learning, 184, 194
Sparse prior distribution, 194
Speaker adaptation, 91, 163, 200
Speaker adaptive training, 91, 97, 163
Speaker clustering, 10, 239, 360
Speaker dependent, 163
Speaker diarization, 240
Speaker independent, 163
Speaker segmentation, 10
Speaker verification, 10, 171, 306
Speech recognition, 3, 7, 8, 9, 51, 53, 54, 59, 91, 97, 128, 180, 185, 191, 225,
254
Spherical Gaussian distribution, 292, 398
Stick-breaking construction, 348, 353
Stirling’s approximation, 285, 389
Student’s t-distribution, 193, 281, 404
Sufficient statistics, 17, 89, 95, 159, 180, 259, 363, 368, 377
Sum product algorithm, 46
Sum rule, 14, 132
Sum-of-squares error function, 189
Support vector machine, 173, 188
tf-idf, 113
Tied-state HMM, 230

Triphone, 230
Type-2 maximum likelihood, 187, 319, 328
Uncertainty, 5
Uncertainty techniques, 51
Undirected graph, 44
Unigram model, 99
Unigram rescaling, 127, 325
Universal background model, 172, 306, 367
Variational Bayes, 242, 243, 318, 329
Variational Bayes (VB) auxiliary function, 258
Variational Bayes (VB) Viterbi algorithm, 269
Variational Bayes Baum–Welch algorithm, 269
Variational Bayes expectation and maximization (VB–EM) algorithm, 252
Variational distribution, 320
Variational lower bound, 242, 243, 269, 285, 288–290, 316
Variational method, 242, 246, 252
Variational parameter, 320
Vector quantization, 62
Vector space model, 10, 114
Viterbi algorithm, 46, 66, 74, 275
Viterbi training, 90
Weak-sense auxiliary function, 201
Weighted finite state transducer, 60
Wishart distribution, 38, 145, 403
Woodbury matrix inversion, 196, 324, 391
Word-document matrix, 113

