Cross-Domain Transfer via Semantic Skill Imitation
Karl Pertsch1∗,
Ruta Desai2,
Vikash Kumar2,
Franziska Meier2,
Joseph J. Lim3,
Dhruv Batra2,4,
Akshara Rai2
1University of Southern California, 2Meta AI, 3KAIST, 4Georgia Tech
https://kpertsch.github.io/star
Abstract: We propose an approach for semantic imitation, which uses demon-
strations from a source domain, e.g., human videos, to accelerate reinforcement
learning (RL) in a different target domain, e.g., a robotic manipulator in a simulated
kitchen. Instead of imitating low-level actions like joint velocities, our approach
imitates the sequence of demonstrated semantic skills like “opening the microwave”
or “turning on the stove”. This allows us to transfer demonstrations across en-
vironments (e.g., real-world to simulated kitchen) and agent embodiments (e.g.,
bimanual human demonstration to robotic arm). We evaluate on three challenging
cross-domain learning problems and match the performance of demonstration-
accelerated RL approaches that require in-domain demonstrations. In a simulated
kitchen environment, our approach learns long-horizon robot manipulation tasks,
using less than 3 minutes of human video demonstrations from a real-world kitchen.
This enables scaling robot learning via the reuse of demonstrations, e.g., collected
as human videos, for learning in any number of target domains.
Keywords: Reinforcement Learning, Imitation, Transfer Learning
1
Introduction
Source Domain  
Demonstration
Target Domain 1
1
2
3
Semantic 
Imitation
Open Microwave
Turn on Stove
Open Cabinet
Target Domain 2
Semantic 
Imitation
1
2
3
Open Microwave
Turn on Stove
Open Cabinet
1
Open Microwave
2
Turn on Stove
3
Open Cabinet
Figure 1: We address semantic imitation, which
aims to leverage demonstrations from a source do-
main, e.g., human video demonstrations, to accel-
erate the learning of the same tasks in a different
target domain, e.g., controlling a robotic manipula-
tor in a simulated kitchen environment.
Consider a person imitating an expert in two sce-
narios: a beginner learning to play tennis, and
a chef following a recipe for a new dish. In the
former case, when mastering the basic skills of
tennis, humans tend to imitate the precise arm
movements demonstrated by the expert. In con-
trast, when operating in a familiar domain, such
as a chef learning to cook a new dish, imitation
happens on a higher scale. Instead of imitating
individual movements, they follow high-level,
semantically meaningful skills like “stir the mix-
ture” or “turn on the oven”. Such semantic skills
generalize across environment layouts, and al-
low humans to follow demonstrations across
substantially different environments.
Most works that leverage demonstrations in
robotics imitate low-level actions. Demonstra-
tions are typically provided by manually moving
the robot [1] or via teleoperation [2]. A critical
challenge of this approach is scaling: demonstra-
tions need to be collected in every new environ-
ment. On the other hand, imitation of high-level
(semantic) skills has the promise of generaliza-
tion: demonstrations can be collected in one
kitchen and applied to any number of kitchens,
∗Work done during an internship at Meta AI. Correspondence to pertsch@usc.edu.
6th Conference on Robot Learning (CoRL 2022), Auckland, New Zealand.
arXiv:2212.07407v1  [cs.LG]  14 Dec 2022

eliminating the need to re-demonstrate in every new environment. Learning via imitation of high-level
skills can lead to scalable and generalizable robot learning.
In this work, we present Semantic Transfer Accelerated RL (STAR), which accelerates RL using
cross-domain demonstrations by leveraging semantic skills, instead of low-level actions. We consider
a setting with signiﬁcantly different source and target environments. Figure 1 shows an example: a
robot arm learns to do a kitchen manipulation task by following a visual human demonstration from a
different (real-world) kitchen. An approach that follows the precise arm movements of the human will
fail due to embodiment and environment differences. Yet, by following the demonstrated semantic
skills like “open the microwave" and “turn on the stove", our approach can leverage demonstrations
despite the domain differences. Like the chef in the above example, we use prior experience for
enabling this semantic transfer. We assume access to datasets of prior experience collected across
many tasks, in both the source and target domains. From this data, we learn semantic skills like “open
the microwave” or “turn on the stove”. Next, we collect demonstrations of the task in the source
domain and ﬁnd “semantically similar” states in the target domain. Using this mapping, we learn a
policy to follow the demonstrated semantic skills in semantically similar states in the target domain.
We present results on two semantic imitation problems in simulation and on real-to-sim transfer
from human videos. In simulation, we test STAR in: (1) a maze navigation task across mazes of
different layouts and (2) a sequence of kitchen tasks between two variations of the FrankaKitchen
environment [3]. In both tasks our approach matches the learning efﬁciency of methods with in-
domain demonstrations, despite only using cross-domain demonstrations. Additionally, we show that
a human demonstration video recorded within 3 minutes in a real-world kitchen can accelerate the
learning of long-horizon manipulation tasks in the FrankaKitchen by hundreds of thousands of robot
environment interactions.
In summary, our contributions are twofold: (1) we introduce STAR, an approach for cross-domain
transfer via learned semantic skills, (2) we show that STAR can leverage demonstrations across
substantially differing domains to accelerate the learning of long-horizon tasks.
2
Related Work
Learning from demonstrations. Learning from Demonstrations (LfD, Argall et al. [4]) is a popular
method for learning robot behaviors using demonstrations of the target task, often collected by human
operators. Common approaches include behavioral cloning (BC, Pomerleau [5]) and adversarial
imitation approaches [6]. A number of works have proposed approaches for combining these
imitation objectives with reinforcement learning [7, 8, 9, 10]. However, all of these approaches
require demonstrations in the target domain, limiting their applicability to new domains. In contrast,
our approach imitates the demonstrations’ semantic skills and thus enables transfer across domains.
Skill-based Imitation. Using temporal abstraction via skills has a long tradition in hierarchical
RL [11, 12, 13]. Skills have also been used for the imitation of long-horizon tasks. Pertsch et al.
[14], Hakhamaneshi et al. [15] learn skills from task-agnostic ofﬂine experience [16, 17] and imitate
demonstrated skills instead of primitive actions. But, since the learned skills do not capture semantic
information, they require demonstrations in the target domain. Xu et al. [18], Huang et al. [19] divide
long-horizon tasks into subroutines, but struggle if the two domains requires a different sequence of
subroutines, e.g., if skill pre-conditions are not met in the target environment. Our approach is robust
to such mismatches without requiring demonstrations in the target domain.
Cross-Domain Imitation. Peng et al. [20] assume a pre-speciﬁed mapping between source and
target domain. [21, 22] leverage ofﬂine experience to learn mappings while [23, 24, 25] rely on paired
demonstrations. A popular goal is to leverage human videos for robot learning since they are easy
to collect at scale. [26, 27] learn reward functions from human demonstrations and Schmeckpeper
et al. [28] add human experience to an RL agent’s replay buffer, but they only consider short-horizon
tasks and rely on environments being similar. Yu et al. [29] meta-learn cross-domain subroutines, but
cannot handle different subroutines between source and target. Our approach imitates long-horizon
tasks across domains, without a pre-deﬁned mapping and is robust to different semantic subroutines.
2

s0
s1
s2
s3
s4
a0
a1
a2
a3
q(z|s, a, k)
πl(a|s, k, z)
s2
s0
s1
s3
s4
k
s
a
π(a|s)
= In-Support Regularization
= Out-of-Support Regularization
= Pre-Trained & Frozen
pdemo(k|s)
<latexit sha1_base64="y3NCKlQn/uVLDFB4qRqfCdKRMdM=">ACAXicbVC7SgNBFJ2NrxhfqzaCzWAQYhN2RdAyaGMZwTwgWcPs5CYZMvtg5q4Yltj4KzYWitj6F3b+jZNkC08MHA451zu3OPHUmh0nG8rt7S8srqWXy9sbG5t79i7e3UdJYpDjUcyUk2faZAihBoKlNCMFbDAl9Dwh1cTv3EPSosovMVRDF7A+qHoCc7QSB37IL5rIzxg2oUgGpeGtG3SPVJxy46ZWcKukjcjBRJhmrH/mp3I54ECKXTOuW68TopUyh4BLGhXaiIWZ8yPrQMjRkAWgvnV4wpsdG6dJepMwLkU7V3xMpC7QeBb5JBgwHet6biP95rQR7F14qwjhBCPlsUS+RFCM6qYN2hQKOcmQI40qYv1I+YIpxNKUVTAnu/MmLpH5adp2ye3NWrFxmdeTJITkiJeKSc1Ih16RKaoSTR/JMXsmb9WS9WO/Wxyas7KZfIH1ucPNAWsw=</latexit><latexit sha1_base64="y3NCKlQn/uVLDFB4qRqfCdKRMdM=">ACAXicbVC7SgNBFJ2NrxhfqzaCzWAQYhN2RdAyaGMZwTwgWcPs5CYZMvtg5q4Yltj4KzYWitj6F3b+jZNkC08MHA451zu3OPHUmh0nG8rt7S8srqWXy9sbG5t79i7e3UdJYpDjUcyUk2faZAihBoKlNCMFbDAl9Dwh1cTv3EPSosovMVRDF7A+qHoCc7QSB37IL5rIzxg2oUgGpeGtG3SPVJxy46ZWcKukjcjBRJhmrH/mp3I54ECKXTOuW68TopUyh4BLGhXaiIWZ8yPrQMjRkAWgvnV4wpsdG6dJepMwLkU7V3xMpC7QeBb5JBgwHet6biP95rQR7F14qwjhBCPlsUS+RFCM6qYN2hQKOcmQI40qYv1I+YIpxNKUVTAnu/MmLpH5adp2ye3NWrFxmdeTJITkiJeKSc1Ih16RKaoSTR/JMXsmb9WS9WO/Wxyas7KZfIH1ucPNAWsw=</latexit><latexit sha1_base64="y3NCKlQn/uVLDFB4qRqfCdKRMdM=">ACAXicbVC7SgNBFJ2NrxhfqzaCzWAQYhN2RdAyaGMZwTwgWcPs5CYZMvtg5q4Yltj4KzYWitj6F3b+jZNkC08MHA451zu3OPHUmh0nG8rt7S8srqWXy9sbG5t79i7e3UdJYpDjUcyUk2faZAihBoKlNCMFbDAl9Dwh1cTv3EPSosovMVRDF7A+qHoCc7QSB37IL5rIzxg2oUgGpeGtG3SPVJxy46ZWcKukjcjBRJhmrH/mp3I54ECKXTOuW68TopUyh4BLGhXaiIWZ8yPrQMjRkAWgvnV4wpsdG6dJepMwLkU7V3xMpC7QeBb5JBgwHet6biP95rQR7F14qwjhBCPlsUS+RFCM6qYN2hQKOcmQI40qYv1I+YIpxNKUVTAnu/MmLpH5adp2ye3NWrFxmdeTJITkiJeKSc1Ih16RKaoSTR/JMXsmb9WS9WO/Wxyas7KZfIH1ucPNAWsw=</latexit><latexit sha1_base64="y3NCKlQn/uVLDFB4qRqfCdKRMdM=">ACAXicbVC7SgNBFJ2NrxhfqzaCzWAQYhN2RdAyaGMZwTwgWcPs5CYZMvtg5q4Yltj4KzYWitj6F3b+jZNkC08MHA451zu3OPHUmh0nG8rt7S8srqWXy9sbG5t79i7e3UdJYpDjUcyUk2faZAihBoKlNCMFbDAl9Dwh1cTv3EPSosovMVRDF7A+qHoCc7QSB37IL5rIzxg2oUgGpeGtG3SPVJxy46ZWcKukjcjBRJhmrH/mp3I54ECKXTOuW68TopUyh4BLGhXaiIWZ8yPrQMjRkAWgvnV4wpsdG6dJepMwLkU7V3xMpC7QeBb5JBgwHet6biP95rQR7F14qwjhBCPlsUS+RFCM6qYN2hQKOcmQI40qYv1I+YIpxNKUVTAnu/MmLpH5adp2ye3NWrFxmdeTJITkiJeKSc1Ih16RKaoSTR/JMXsmb9WS9WO/Wxyas7KZfIH1ucPNAWsw=</latexit>
pTA(k|s)
<latexit sha1_base64="vpQAmA6WDz/Qp9Ril18cg5Y6Mfw=">AB/3icbVDLSgMxFM3UV62vUcGNm2AR6qbMiKDLqhuXFfqCdiyZNOGZjJDckcsYxf+ihsXirj1N9z5N6btLT1QOBwzrncm+PHgmtwnG8rt7S8srqWXy9sbG5t79i7ew0dJYqyOo1EpFo+0UxwyerAQbBWrBgJfcGa/vB64jfvmdI8kjUYxcwLSV/ygFMCRuraB/FdB9gDpLXLcWmIOyYLWJ907aJTdqbAi8TNSBFlqHbtr04voknIJFBtG67TgxeShRwKti40Ek0iwkdkj5rGypJyLSXTu8f42Oj9HAQKfMk4Kn6eyIlodaj0DfJkMBAz3sT8T+vnUBw4aVcxgkwSWeLgkRgiPCkDNzjilEQI0MIVdzciumAKELBVFYwJbjzX14kjdOy65Td27Ni5SqrI48O0REqIRedowq6QVURxQ9omf0it6sJ+vFerc+ZtGclc3soz+wPn8AN6iVjw=</latexit><latexit sha1_base64="vpQAmA6WDz/Qp9Ril18cg5Y6Mfw=">AB/3icbVDLSgMxFM3UV62vUcGNm2AR6qbMiKDLqhuXFfqCdiyZNOGZjJDckcsYxf+ihsXirj1N9z5N6btLT1QOBwzrncm+PHgmtwnG8rt7S8srqWXy9sbG5t79i7ew0dJYqyOo1EpFo+0UxwyerAQbBWrBgJfcGa/vB64jfvmdI8kjUYxcwLSV/ygFMCRuraB/FdB9gDpLXLcWmIOyYLWJ907aJTdqbAi8TNSBFlqHbtr04voknIJFBtG67TgxeShRwKti40Ek0iwkdkj5rGypJyLSXTu8f42Oj9HAQKfMk4Kn6eyIlodaj0DfJkMBAz3sT8T+vnUBw4aVcxgkwSWeLgkRgiPCkDNzjilEQI0MIVdzciumAKELBVFYwJbjzX14kjdOy65Td27Ni5SqrI48O0REqIRedowq6QVURxQ9omf0it6sJ+vFerc+ZtGclc3soz+wPn8AN6iVjw=</latexit><latexit sha1_base64="vpQAmA6WDz/Qp9Ril18cg5Y6Mfw=">AB/3icbVDLSgMxFM3UV62vUcGNm2AR6qbMiKDLqhuXFfqCdiyZNOGZjJDckcsYxf+ihsXirj1N9z5N6btLT1QOBwzrncm+PHgmtwnG8rt7S8srqWXy9sbG5t79i7ew0dJYqyOo1EpFo+0UxwyerAQbBWrBgJfcGa/vB64jfvmdI8kjUYxcwLSV/ygFMCRuraB/FdB9gDpLXLcWmIOyYLWJ907aJTdqbAi8TNSBFlqHbtr04voknIJFBtG67TgxeShRwKti40Ek0iwkdkj5rGypJyLSXTu8f42Oj9HAQKfMk4Kn6eyIlodaj0DfJkMBAz3sT8T+vnUBw4aVcxgkwSWeLgkRgiPCkDNzjilEQI0MIVdzciumAKELBVFYwJbjzX14kjdOy65Td27Ni5SqrI48O0REqIRedowq6QVURxQ9omf0it6sJ+vFerc+ZtGclc3soz+wPn8AN6iVjw=</latexit><latexit sha1_base64="vpQAmA6WDz/Qp9Ril18cg5Y6Mfw=">AB/3icbVDLSgMxFM3UV62vUcGNm2AR6qbMiKDLqhuXFfqCdiyZNOGZjJDckcsYxf+ihsXirj1N9z5N6btLT1QOBwzrncm+PHgmtwnG8rt7S8srqWXy9sbG5t79i7ew0dJYqyOo1EpFo+0UxwyerAQbBWrBgJfcGa/vB64jfvmdI8kjUYxcwLSV/ygFMCRuraB/FdB9gDpLXLcWmIOyYLWJ907aJTdqbAi8TNSBFlqHbtr04voknIJFBtG67TgxeShRwKti40Ek0iwkdkj5rGypJyLSXTu8f42Oj9HAQKfMk4Kn6eyIlodaj0DfJkMBAz3sT8T+vnUBw4aVcxgkwSWeLgkRgiPCkDNzjilEQI0MIVdzciumAKELBVFYwJbjzX14kjdOy65Td27Ni5SqrI48O0REqIRedowq6QVURxQ9omf0it6sJ+vFerc+ZtGclc3soz+wPn8AN6iVjw=</latexit>
πsem(k|s)
<latexit sha1_base64="cNuFyzr3mzwB0m2tA/vnk2QGE=">AB/XicbVDLSgMxFM34rPU1PnZugkWomzIjgi6LblxWsA9ox5J7ShSWZIMkIdir/ixoUibv0Pd/6NaTsLbT0QOJxzLvfmhAln2njet7O0vLK6tl7YKG5ube/sunv7DR2nikKdxjxWrZBo4ExC3TDoZUoICLk0AyH1xO/+QBKs1jemVECgSB9ySJGibFS1z3sJOw+0yDG5SHu2KTB+rTrlryKNwVeJH5OSihHret+dXoxTQVIQznRu17iQkyogyjHMbFTqohIXRI+tC2VBIBOsim14/xiV6OIqVfdLgqfp7IiNC65EIbVIQM9Dz3kT8z2unJroMiaT1ICks0VRyrGJ8aQK3GMKqOEjSwhVzN6K6YAoQo0trGhL8Oe/vEgaZxXfq/i356XqV5HAR2hY1RGPrpAVXSDaqiOKHpEz+gVvTlPzovz7nzMoktOPnOA/sD5/AHZQJTP</latexit><latexit sha1_base64="cNuFyzr3mzwB0m2tA/vnk2QGE=">AB/XicbVDLSgMxFM34rPU1PnZugkWomzIjgi6LblxWsA9ox5J7ShSWZIMkIdir/ixoUibv0Pd/6NaTsLbT0QOJxzLvfmhAln2njet7O0vLK6tl7YKG5ube/sunv7DR2nikKdxjxWrZBo4ExC3TDoZUoICLk0AyH1xO/+QBKs1jemVECgSB9ySJGibFS1z3sJOw+0yDG5SHu2KTB+rTrlryKNwVeJH5OSihHret+dXoxTQVIQznRu17iQkyogyjHMbFTqohIXRI+tC2VBIBOsim14/xiV6OIqVfdLgqfp7IiNC65EIbVIQM9Dz3kT8z2unJroMiaT1ICks0VRyrGJ8aQK3GMKqOEjSwhVzN6K6YAoQo0trGhL8Oe/vEgaZxXfq/i356XqV5HAR2hY1RGPrpAVXSDaqiOKHpEz+gVvTlPzovz7nzMoktOPnOA/sD5/AHZQJTP</latexit><latexit sha1_base64="cNuFyzr3mzwB0m2tA/vnk2QGE=">AB/XicbVDLSgMxFM34rPU1PnZugkWomzIjgi6LblxWsA9ox5J7ShSWZIMkIdir/ixoUibv0Pd/6NaTsLbT0QOJxzLvfmhAln2njet7O0vLK6tl7YKG5ube/sunv7DR2nikKdxjxWrZBo4ExC3TDoZUoICLk0AyH1xO/+QBKs1jemVECgSB9ySJGibFS1z3sJOw+0yDG5SHu2KTB+rTrlryKNwVeJH5OSihHret+dXoxTQVIQznRu17iQkyogyjHMbFTqohIXRI+tC2VBIBOsim14/xiV6OIqVfdLgqfp7IiNC65EIbVIQM9Dz3kT8z2unJroMiaT1ICks0VRyrGJ8aQK3GMKqOEjSwhVzN6K6YAoQo0trGhL8Oe/vEgaZxXfq/i356XqV5HAR2hY1RGPrpAVXSDaqiOKHpEz+gVvTlPzovz7nzMoktOPnOA/sD5/AHZQJTP</latexit><latexit sha1_base64="cNuFyzr3mzwB0m2tA/vnk2QGE=">AB/XicbVDLSgMxFM34rPU1PnZugkWomzIjgi6LblxWsA9ox5J7ShSWZIMkIdir/ixoUibv0Pd/6NaTsLbT0QOJxzLvfmhAln2njet7O0vLK6tl7YKG5ube/sunv7DR2nikKdxjxWrZBo4ExC3TDoZUoICLk0AyH1xO/+QBKs1jemVECgSB9ySJGibFS1z3sJOw+0yDG5SHu2KTB+rTrlryKNwVeJH5OSihHret+dXoxTQVIQznRu17iQkyogyjHMbFTqohIXRI+tC2VBIBOsim14/xiV6OIqVfdLgqfp7IiNC65EIbVIQM9Dz3kT8z2unJroMiaT1ICks0VRyrGJ8aQK3GMKqOEjSwhVzN6K6YAoQo0trGhL8Oe/vEgaZxXfq/i356XqV5HAR2hY1RGPrpAVXSDaqiOKHpEz+gVvTlPzovz7nzMoktOPnOA/sD5/AHZQJTP</latexit>
πlat(z|s, k)
<latexit sha1_base64="c/Ra8OoU+P8GWDNB5OTzhF6v8xI=">ACAHicbVDLSsNAFJ3UV62vqAsXbgaLUEFKIoIui25cVrAPaGKZTCft0MkzNwINWTjr7hxoYhbP8Odf+P0sdDWAwOHc87lzj1BIrgGx/m2CkvLK6trxfXSxubW9o69u9fUcaoa9BYxKodEM0El6wBHARrJ4qRKBCsFQyvx37rgSnNY3kHo4T5EelLHnJKwEhd+8BL+H0mCOSVR+yZJGB9iocnXbvsVJ0J8CJxZ6SMZqh37S+vF9M0YhKoIFp3XCcBPyMKOBUsL3mpZgmhQ9JnHUMliZj2s8kBOT42Sg+HsTJPAp6ovycyEmk9igKTjAgM9Lw3Fv/zOimEl37GZICk3S6KEwFhiP28A9rhgFMTKEUMXNXzEdEUomM5KpgR3/uRF0jyruk7VvT0v165mdRTRITpCFeSiC1RDN6iOGoiHD2jV/RmPVkv1rv1MY0WrNnMPvoD6/MHhWVrw=</latexit><latexit sha1_base64="c/Ra8OoU+P8GWDNB5OTzhF6v8xI=">ACAHicbVDLSsNAFJ3UV62vqAsXbgaLUEFKIoIui25cVrAPaGKZTCft0MkzNwINWTjr7hxoYhbP8Odf+P0sdDWAwOHc87lzj1BIrgGx/m2CkvLK6trxfXSxubW9o69u9fUcaoa9BYxKodEM0El6wBHARrJ4qRKBCsFQyvx37rgSnNY3kHo4T5EelLHnJKwEhd+8BL+H0mCOSVR+yZJGB9iocnXbvsVJ0J8CJxZ6SMZqh37S+vF9M0YhKoIFp3XCcBPyMKOBUsL3mpZgmhQ9JnHUMliZj2s8kBOT42Sg+HsTJPAp6ovycyEmk9igKTjAgM9Lw3Fv/zOimEl37GZICk3S6KEwFhiP28A9rhgFMTKEUMXNXzEdEUomM5KpgR3/uRF0jyruk7VvT0v165mdRTRITpCFeSiC1RDN6iOGoiHD2jV/RmPVkv1rv1MY0WrNnMPvoD6/MHhWVrw=</latexit><latexit sha1_base64="c/Ra8OoU+P8GWDNB5OTzhF6v8xI=">ACAHicbVDLSsNAFJ3UV62vqAsXbgaLUEFKIoIui25cVrAPaGKZTCft0MkzNwINWTjr7hxoYhbP8Odf+P0sdDWAwOHc87lzj1BIrgGx/m2CkvLK6trxfXSxubW9o69u9fUcaoa9BYxKodEM0El6wBHARrJ4qRKBCsFQyvx37rgSnNY3kHo4T5EelLHnJKwEhd+8BL+H0mCOSVR+yZJGB9iocnXbvsVJ0J8CJxZ6SMZqh37S+vF9M0YhKoIFp3XCcBPyMKOBUsL3mpZgmhQ9JnHUMliZj2s8kBOT42Sg+HsTJPAp6ovycyEmk9igKTjAgM9Lw3Fv/zOimEl37GZICk3S6KEwFhiP28A9rhgFMTKEUMXNXzEdEUomM5KpgR3/uRF0jyruk7VvT0v165mdRTRITpCFeSiC1RDN6iOGoiHD2jV/RmPVkv1rv1MY0WrNnMPvoD6/MHhWVrw=</latexit><latexit sha1_base64="c/Ra8OoU+P8GWDNB5OTzhF6v8xI=">ACAHicbVDLSsNAFJ3UV62vqAsXbgaLUEFKIoIui25cVrAPaGKZTCft0MkzNwINWTjr7hxoYhbP8Odf+P0sdDWAwOHc87lzj1BIrgGx/m2CkvLK6trxfXSxubW9o69u9fUcaoa9BYxKodEM0El6wBHARrJ4qRKBCsFQyvx37rgSnNY3kHo4T5EelLHnJKwEhd+8BL+H0mCOSVR+yZJGB9iocnXbvsVJ0J8CJxZ6SMZqh37S+vF9M0YhKoIFp3XCcBPyMKOBUsL3mpZgmhQ9JnHUMliZj2s8kBOT42Sg+HsTJPAp6ovycyEmk9igKTjAgM9Lw3Fv/zOimEl37GZICk3S6KEwFhiP28A9rhgFMTKEUMXNXzEdEUomM5KpgR3/uRF0jyruk7VvT0v165mdRTRITpCFeSiC1RDN6iOGoiHD2jV/RmPVkv1rv1MY0WrNnMPvoD6/MHhWVrw=</latexit>
πl(a|s, k, z)
<latexit sha1_base64="9oyZxgtvpK0SDTEixVXDmE96DWI=">ACAXicbVDLSgMxFM3UV62vUTeCm2ARKpQyI4Iui25cVrAPaMeSTNtaCYzJHeEOtSNv+LGhSJu/Qt3/o1pOwtPRA4nHMuN/f4seAaHOfbyi0tr6yu5dcLG5tb2zv27l5DR4mirE4jEamWTzQTXLI6cBCsFStGQl+wpj+8mvjNe6Y0j+QtjGLmhaQvecApASN17YNOzO9SMS4R3DE5wLqMh2X8cNK1i07FmQIvEjcjRZSh1rW/Or2IJiGTQAXRu06MXgpUcCpYONCJ9EsJnRI+qxtqCQh0146vWCMj43Sw0GkzJOAp+rviZSEWo9C3yRDAgM9703E/7x2AsGFl3IZJ8AknS0KEoEhwpM6cI8rRkGMDCFUcfNXTAdEQqmtIpwZ0/eZE0TiuU3FvzorVy6yOPDpER6iEXHSOquga1VAdUfSIntErerOerBfr3fqYRXNWNrOP/sD6/AFx+5WR</latexit><latexit sha1_base64="9oyZxgtvpK0SDTEixVXDmE96DWI=">ACAXicbVDLSgMxFM3UV62vUTeCm2ARKpQyI4Iui25cVrAPaMeSTNtaCYzJHeEOtSNv+LGhSJu/Qt3/o1pOwtPRA4nHMuN/f4seAaHOfbyi0tr6yu5dcLG5tb2zv27l5DR4mirE4jEamWTzQTXLI6cBCsFStGQl+wpj+8mvjNe6Y0j+QtjGLmhaQvecApASN17YNOzO9SMS4R3DE5wLqMh2X8cNK1i07FmQIvEjcjRZSh1rW/Or2IJiGTQAXRu06MXgpUcCpYONCJ9EsJnRI+qxtqCQh0146vWCMj43Sw0GkzJOAp+rviZSEWo9C3yRDAgM9703E/7x2AsGFl3IZJ8AknS0KEoEhwpM6cI8rRkGMDCFUcfNXTAdEQqmtIpwZ0/eZE0TiuU3FvzorVy6yOPDpER6iEXHSOquga1VAdUfSIntErerOerBfr3fqYRXNWNrOP/sD6/AFx+5WR</latexit><latexit sha1_base64="9oyZxgtvpK0SDTEixVXDmE96DWI=">ACAXicbVDLSgMxFM3UV62vUTeCm2ARKpQyI4Iui25cVrAPaMeSTNtaCYzJHeEOtSNv+LGhSJu/Qt3/o1pOwtPRA4nHMuN/f4seAaHOfbyi0tr6yu5dcLG5tb2zv27l5DR4mirE4jEamWTzQTXLI6cBCsFStGQl+wpj+8mvjNe6Y0j+QtjGLmhaQvecApASN17YNOzO9SMS4R3DE5wLqMh2X8cNK1i07FmQIvEjcjRZSh1rW/Or2IJiGTQAXRu06MXgpUcCpYONCJ9EsJnRI+qxtqCQh0146vWCMj43Sw0GkzJOAp+rviZSEWo9C3yRDAgM9703E/7x2AsGFl3IZJ8AknS0KEoEhwpM6cI8rRkGMDCFUcfNXTAdEQqmtIpwZ0/eZE0TiuU3FvzorVy6yOPDpER6iEXHSOquga1VAdUfSIntErerOerBfr3fqYRXNWNrOP/sD6/AFx+5WR</latexit><latexit sha1_base64="9oyZxgtvpK0SDTEixVXDmE96DWI=">ACAXicbVDLSgMxFM3UV62vUTeCm2ARKpQyI4Iui25cVrAPaMeSTNtaCYzJHeEOtSNv+LGhSJu/Qt3/o1pOwtPRA4nHMuN/f4seAaHOfbyi0tr6yu5dcLG5tb2zv27l5DR4mirE4jEamWTzQTXLI6cBCsFStGQl+wpj+8mvjNe6Y0j+QtjGLmhaQvecApASN17YNOzO9SMS4R3DE5wLqMh2X8cNK1i07FmQIvEjcjRZSh1rW/Or2IJiGTQAXRu06MXgpUcCpYONCJ9EsJnRI+qxtqCQh0146vWCMj43Sw0GkzJOAp+rviZSEWo9C3yRDAgM9703E/7x2AsGFl3IZJ8AknS0KEoEhwpM6cI8rRkGMDCFUcfNXTAdEQqmtIpwZ0/eZE0TiuU3FvzorVy6yOPDpER6iEXHSOquga1VAdUfSIntErerOerBfr3fqYRXNWNrOP/sD6/AFx+5WR</latexit>
pTA(z|s, k)
<latexit sha1_base64="c2fNTx68cHPMust/FGx9JuE3es=">ACBHicbVC7SgNBFJ2Nrxhfq5ZpBoMQcKuCFpGbSwj5AXZNcxOZpMhsw9m7opx2cLGX7GxUMTWj7Dzb5w8Ck08MHA451zu3OPFgiuwrG8jt7S8srqWXy9sbG5t75i7e0VJZKyBo1EJNseUzwkDWAg2DtWDISeIK1vOHV2G/dMal4FNZhFDM3IP2Q+5wS0FLXLMa3qQPsHtL6RZaVH7Cj04DVMR4edc2SVbEmwIvEnpESmqHWNb+cXkSTgIVABVGqY1sxuCmRwKlgWcFJFIsJHZI+62gakoApN50ckeFDrfSwH0n9QsAT9fdESgKlRoGnkwGBgZr3xuJ/XicB/9xNeRgnwEI6XeQnAkOEx43gHpeMghpQqjk+q+YDogkFHRvBV2CPX/yImeVGyrYt+clqXszryqIgOUBnZ6AxV0TWqoQai6BE9o1f0ZjwZL8a78TGN5ozZzD76A+PzB8bql38=</latexit><latexit sha1_base64="c2fNTx68cHPMust/FGx9JuE3es=">ACBHicbVC7SgNBFJ2Nrxhfq5ZpBoMQcKuCFpGbSwj5AXZNcxOZpMhsw9m7opx2cLGX7GxUMTWj7Dzb5w8Ck08MHA451zu3OPFgiuwrG8jt7S8srqWXy9sbG5t75i7e0VJZKyBo1EJNseUzwkDWAg2DtWDISeIK1vOHV2G/dMal4FNZhFDM3IP2Q+5wS0FLXLMa3qQPsHtL6RZaVH7Cj04DVMR4edc2SVbEmwIvEnpESmqHWNb+cXkSTgIVABVGqY1sxuCmRwKlgWcFJFIsJHZI+62gakoApN50ckeFDrfSwH0n9QsAT9fdESgKlRoGnkwGBgZr3xuJ/XicB/9xNeRgnwEI6XeQnAkOEx43gHpeMghpQqjk+q+YDogkFHRvBV2CPX/yImeVGyrYt+clqXszryqIgOUBnZ6AxV0TWqoQai6BE9o1f0ZjwZL8a78TGN5ozZzD76A+PzB8bql38=</latexit><latexit sha1_base64="c2fNTx68cHPMust/FGx9JuE3es=">ACBHicbVC7SgNBFJ2Nrxhfq5ZpBoMQcKuCFpGbSwj5AXZNcxOZpMhsw9m7opx2cLGX7GxUMTWj7Dzb5w8Ck08MHA451zu3OPFgiuwrG8jt7S8srqWXy9sbG5t75i7e0VJZKyBo1EJNseUzwkDWAg2DtWDISeIK1vOHV2G/dMal4FNZhFDM3IP2Q+5wS0FLXLMa3qQPsHtL6RZaVH7Cj04DVMR4edc2SVbEmwIvEnpESmqHWNb+cXkSTgIVABVGqY1sxuCmRwKlgWcFJFIsJHZI+62gakoApN50ckeFDrfSwH0n9QsAT9fdESgKlRoGnkwGBgZr3xuJ/XicB/9xNeRgnwEI6XeQnAkOEx43gHpeMghpQqjk+q+YDogkFHRvBV2CPX/yImeVGyrYt+clqXszryqIgOUBnZ6AxV0TWqoQai6BE9o1f0ZjwZL8a78TGN5ozZzD76A+PzB8bql38=</latexit><latexit sha1_base64="c2fNTx68cHPMust/FGx9JuE3es=">ACBHicbVC7SgNBFJ2Nrxhfq5ZpBoMQcKuCFpGbSwj5AXZNcxOZpMhsw9m7opx2cLGX7GxUMTWj7Dzb5w8Ck08MHA451zu3OPFgiuwrG8jt7S8srqWXy9sbG5t75i7e0VJZKyBo1EJNseUzwkDWAg2DtWDISeIK1vOHV2G/dMal4FNZhFDM3IP2Q+5wS0FLXLMa3qQPsHtL6RZaVH7Cj04DVMR4edc2SVbEmwIvEnpESmqHWNb+cXkSTgIVABVGqY1sxuCmRwKlgWcFJFIsJHZI+62gakoApN50ckeFDrfSwH0n9QsAT9fdESgKlRoGnkwGBgZr3xuJ/XicB/9xNeRgnwEI6XeQnAkOEx43gHpeMghpQqjk+q+YDogkFHRvBV2CPX/yImeVGyrYt+clqXszryqIgOUBnZ6AxV0TWqoQai6BE9o1f0ZjwZL8a78TGN5ozZzD76A+PzB8bql38=</latexit>
Figure 2: Model overview for pre-training (left) and target task learning (right). We pre-train a
semantic skill policy πl (grey) and use it to decode actions from the learned high-level policies πsem
and πlat (blue and yellow) during target task learning. See training details in the main text.
3
Problem Formulation
We deﬁne a source environment S and a target environment T. In the source environment, we
have N demonstrations τ S
1:N with τ S
i = {sS
0 , aS
0 , sS
1 , aS
1 , . . . } sequences of states sS and actions aS.
Our goal is to leverage these demonstrations to accelerate training of a policy π(sT ) in the target
environment, acting on target states sT and predicting actions aT . π(sT ) maximizes the discounted
target task reward JT = Eπ
 PL−1
l=0 γlR(sT
l , aT
l )

for an episode of length L. We account for
different state-action spaces (sS, aS) vs. (sT , aT ) between source and target, but drop the superscript
in the following sections, assuming that the context makes it clear whether we are addressing source
or target states. In Section 4.3 we describe how we bridge this environment gap. Without loss of
generality we assume that the source and target environments are substantially different; sequences
of low-level actions that solve a task in the source environment do not lead to high reward in the
target environment. In the following we will also use the term domain to refer to two environments
with this property. Yet, we assume that the demonstrations show a set of semantic skills, which when
followed in the target environment can lead to task success. Here the term semantic skill refers to a
high-level notion of skill, like “open the microwave” or “turn on the oven”, which is independent of
the environment-speciﬁc low-level actions required to perform it. We further assume that both source
and target environment allow for the execution of the same set of semantic skills.
Semantic imitation requires an agent to understand the semantic skills performed in the demon-
strations. We use task-agnostic datasets DS and DT in the source and target domains to extract
such semantic skills. Each Di consists of state-action trajectories collected across a diverse range
of prior tasks, e.g., from previously trained policies or teleoperation, as is commonly assumed in
prior work [16, 17, 14, 15]. We also assume discrete semantic skill annotations kt ∈K, denoting the
skill being executed at time step t. These can be collected manually, but we demonstrate how to use
pre-trained action recognition models as a more scalable alternative (Sec. 5.2).
4
Approach
Algorithm 1 STAR (Semantic Transfer Accelerated RL)
Pre-Train low-level policy πl(a|s, k, z)
▷cf. Sec. 4.1
Match source demos to target states
▷cf. Sec. 4.3
Pre-train pdemo(k|s), pTA(k|s), pTA(z|s, k), D(s)
▷cf. Tab. 1
for each target train iteration do
Collect online experience (s, k, z, R, s′)
Update high-level policies with eq. 3
▷cf. Alg. 2
return trained high-level policies πsem(k|s), πlat(z|s, k)
Our approach STAR imitates demon-
strations’ semantic skills, instead of
low-level actions, to enable cross-
domain, semantic imitation. We use
a two-layer hierarchical policy with
a high-level that outputs the semantic
skill and a low-level that executes the skill. We ﬁrst describe our semantic skill representation,
followed by the low-level and high-level policy learning. Algorithm 1 summarizes our approach.
4.1
Semantic Skill Representation
A skill is characterized by both its semantics, i.e., whether to open the microwave or turn on the stove,
as well as the details of its low-level execution, e.g., at what angle to approach the microwave or
3

where to grasp its door handle. Thus, we represent skills via a low-level policy πl(a|s, k, z) which is
conditioned on the current environment state s, the semantic skill ID k and a latent variable z which
captures the execution details. For example, when “turning on the stove", a are the joint velocities, s
is the robot and environment state, k is the semantic skill ID of this skill, and z captures the robot
hand orientation as it interacts with the stove. A single tuple (k, z) represents a sequence of H steps,
since such temporal abstraction facilitates long-horizon imitation [14]. We train our model as a
conditional variational autoencoder (VAE) [30] over a sequence of actions given a state and semantic
skill ID. Thus, the latent variable z represents all information required to reconstruct a0:H−1 that is
not contained in the skill ID, i.e., information about how to execute the semantic skill.
Figure 2, left depicts the training setup for πl. We randomly sample an H-step state-action sub-
sequence (s0:H, a0:H−1) from DT . An inference network q(z|s, a, k) encodes the sequence into a
latent representation z conditioned on the semantic skill ID k at the ﬁrst time step. k and z are passed
to πl, which reconstructs the sampled actions. Our training objective is a standard conditional VAE
objective that combines a reconstruction and a prior regularization term:
Lπl = Eq
 H−1
Y
t=0
log πl(at|st, k, z)

|
{z
}
reconstruction
−βDKL
 q(z|s0:H, a0:H−1, k), p(z)

|
{z
}
prior regularization
.
(1)
Here DKL denotes the Kullback-Leibler divergence. We use a simple uniform Gaussian prior p(z)
and a weighting factor β for the regularization objective [31]. The semantic skill ID k is pre-deﬁned,
discrete and labelled, while the latent z is learned and continuous. In this way, our formulation
captures discrete aspects of manipulation skills (open a microwave vs. turn on a stove) while being
able to continuously modulate each semantic skill (e.g., different ways of approaching the microwave).
4.2
Semantic Transfer Accelerated RL
After pre-training the low-level policy πl(a|s, k, z), we learn the high-level policy using the source
domain demonstrations. Concretely, we train a policy πh(k, z|s) that predicts tuples (k, z) which get
executed via πl. Note that unlike prior work [14], our high-level policy outputs both, the semantic
skill k and the low-level execution latent z. It is thus able to choose which semantic skill to execute
and tailor its execution to the target domain. Cross-domain demonstrations solely guide the semantic
skill choice, since the low-level execution might vary between source and target domains. Thus, we
factorize πh into a semantic sub-policy πsem(k|s) and a latent, non-semantic sub-policy πlat(z|s, k):
π(a|s) = πl(a|s, k, z)
|
{z
}
skill policy
· πlat(z|s, k) πsem(k|s)
|
{z
}
high-level policy πh(k, z|s)
.
(2)
Intuitively, this can be thought of as ﬁrst deciding what skill to execute (e.g., open the microwave),
followed by how to execute it. We pre-train multiple models via supervised learning for training πh:
(1) two semantic skill priors pdemo(k|s) and pTA(k|s), trained to infer the semantic skill annotations
from demonstrations and task-agnostic dataset DT respectively, (2) a task-agnostic prior pTA(z|s, k)
over the latent skill variable z, trained to match the output of the inference network on DT and (3) a
discriminator D(s), trained to classify whether a state is part of the demonstration trajectories. We
summarize all pre-trained components and their supervised training objectives in Appendix, Table 1.
We provide an overview of our semantic imitation architecture and the used regularization terms in
Figure 2, right. We build on the idea of weighted policy regularization with a learned demonstration
support estimator from Pertsch et al. [14] (for a brief summary, see appendix B). We regularize
the high-level semantic policy πsem (blue) towards the demonstration skill distribution pdemo(k|s)
when D(s) classiﬁes the current state as part of the demonstrations (green). For states which D(s)
classiﬁes as outside the demonstration support, we regularize πsem towards the task-agnostic prior
pTA(k|s) (red). We always regularize the non-semantic sub-policy πlat(z|s, k) (yellow) towards the
task-agnostic prior pTA(z|s, k), since execution-speciﬁc information cannot be transferred across
4

domains. The overall optimization objective for πh is:
Eπh

˜r(s, a) −αqDKL
 πsem(k|s), pdemo(k|s)

· D(s)
|
{z
}
demonstration regularization
−αpDKL
 πsem(k|s), pTA(k|s)

· (1 −D(s))
|
{z
}
task-agnostic semantic prior regularization
,
−αlDKL
 πlat(z|s, k), pTA(z|s, k)

|
{z
}
task-agnostic execution prior regularization

.
(3)
αq, αp and αl are either ﬁxed or automatically tuned via dual gradient descent. We augment the
target task reward using the discriminator D(s) to encourage the policy to reach states within the
demonstration support: ˜r(s, a) = (1 −κ) · R(s, a) + κ ·

log D(s) −log
 1 −D(s)

. In the setting
with no target environment rewards (pure imitation learning), we rely solely on this discriminator
reward for policy training (Section D). For a summary of the full procedure, see Algorithm 2.
The ﬁnal challenge is that the discriminator D(s) and the prior pdemo(k|s) are trained on states from
the source domain, but need to be applied to the target domain. Since the domains differ substantially,
we cannot expect the pre-trained networks to generalize. Instead, we need to explicitly bridge the
state domain gap, as described next.
4.3
Cross-Domain State Matching
Source Domain State
Target Domain States
Open 
Microwave
Turn On 
Stove
Open 
Cabinet
Figure 3: State matching between source and tar-
get domain. For every source domain state from
the demonstrations, we compute the task-agnostic
semantic skill distribution pTA(k|s) and ﬁnd the
target domain state with the most similar seman-
tic skill distribution from the task-agnostic dataset
DT . We then relabel the demonstrations with these
matched states from the target domain.
Our goal is to ﬁnd semantically similar states
between the source and the target environment.
These are states with similar distributions over
likely semantic skills. E.g. if the agent’s hand is
reaching for the handle of a closed microwave,
the probability for the skill “open microwave” is
high, while the probability for other skills, e.g.
“turn on stove” is low. Crucially, this is true
independent of the domain and independent of
whether e.g. a human or robot is executing the
action. Thus, we can use the skill prior distribu-
tions to ﬁnd semantically similar states.
Following this intuition, we ﬁnd correspond-
ing states based on the similarity between the
task-agnostic semantic skill prior distributions
pTA(k|s). We illustrate an example in Figure 3:
for a given source demonstration state sS with
high likelihood of opening the microwave, we
ﬁnd a target domain state sT that has high likeli-
hood of opening the microwave, by minimizing
the symmetric KL divergence between the task-
agnostic skill distributions (we omit (·)TA for brevity):
min
sT ∈DT
DKL
 pT (k|sT ), pS(k|sS)

+ DKL
 pS(k|sS), pT (k|sT )

(4)
In practice, states can be matched incorrectly when the task agnostic dataset chooses one skill with
much higher probability than others. In such states, the divergence in equation 4 is dominated by one
skill, and others are ignored, causing matching errors. Using a state’s temporal context can result in
more robust correspondences by reducing the inﬂuence of high likelihood skills in any single state.
We compute an aggregated skill distribution φ(k|s) using a temporal window around the current state:
φ(k|st) =
1
Z(s)

T
X
i=t
γi
+p(k|si) +
t−1
X
j=1
γt−j
−p(k|st−j)

(5)
Here, γ+, γ−∈[0, 1] determine the forward and backward horizon of the aggregate skill distribution.
Z(s) ensures that the aggregate probability distribution sums to one. Instead of pTA in equation 4, we
use φ(k|s). By matching all source-domain demonstrations states to states in the target domain via
φ(k|s), we create a proxy dataset of target state demonstrations, which we use to pre-train the models
pdemo(k|s) and D(s). Once trained, we use them for training the high-level policy via equation 3.
5

SkiLD (Oracle)
SPiRL
STAR (Ours)
SkillSeq
BC+RL
Figure 5: Left: Performance on the simulated semantic imitation tasks. STAR, matches the perfor-
mance of the oracle, SkiLD, which has access to target domain demonstrations and outperforms both
SPiRL, which does not use demonstrations, and SkillSeq, which follows the demonstrated semantic
skills sequentially. Right: Ablations in the kitchen environment, see main text for details.
5
Experiments
Source Environment
Target Environment
Maze Navigation
Simulated Kitchen
Real-World Kitchen
1
2
3
4
1
1
1
2
2
2
3
3
3
4
4
4
Figure 4: We evaluate on three pairs of source (top) and
target (bottom) environments. Left: maze navigation. The
agent needs to follow a sequence of colored rooms (red
path) but the maze layout changes substantially between
source and target domains. Middle: kitchen manipulation.
A robotic arm executes a sequence of skills, but the layout of
the kitchens differs. Right: Same as before, but with human
demonstrations from a real-world kitchen.
Our experiments are designed to an-
swer the following questions: (1) Can
we leverage demonstrations across do-
mains to accelerate learning via se-
mantic imitation? (2) Can we use se-
mantic imitation to teach a robot a
new task from real-world videos of
humans performing the task? (3) Is
our approach robust to missing skills
in the demonstrations? We test se-
mantic imitation across two simulated
maze and kitchen environments, as
well as from real-world videos of hu-
mans to a simulated robot. Our results
show that our approach can accelerate
learning from cross-domain demon-
strations, even with real-to-sim gap.
5.1
Cross-Domain
Imitation in Simulation
We ﬁrst test our approach STAR in two simulated settings: a maze navigation and a robot kitchen
manipulation task (see Figure 4, left & middle). In the maze navigation task, both domains have
corresponding rooms, indicated by their color in Figure 4. The agent needs to follow a sequence of
semantic skills like “go to red room”, “go to green room” etc. In the kitchen manipulation task, a
Franka arm tackles long-horizon manipulation tasks in a simulated kitchen [3]. We deﬁne 7 semantic
skills, like “open the microwave” or “turn on the stove” in the source and target environments. In
both environments we collect demonstrations in the source domain, and task-agnostic datasets in
both the source and target domains using motion planners and human teleoperation respectively. For
further details on action and observation spaces, rewards and data collection, see Sec C.4.
We compare our approach to multile prior skill-based RL approaches with and without demonstration
guidance: SPiRL [16] learns skills from DT and then trains a high-level policy over skills; BC+RL [7,
8] pre-trains with behavioral cloning and ﬁnetunes with SAC [32]; SkillSeq, similar to Xu et al. [18],
sequentially executes the ground truth sequence of semantic skills as demonstrated; SkiLD [14]
is an oracle with access to demonstrations in the target domain and follows them using learned
skills. For more details on the implementation of our approach and all comparisons, see appendix,
Sections C.1 - C.3.
Figure 5, left, compares the performance of all approaches in both tasks. BC+RL is unable to leverage
the cross-domain demonstrations and makes no progress on the task. SPiRL is able to learn the kitchen
6

Source Environment
Target Environment
Figure 6: Semantic imitation from human demonstrations. Left: Qualitative state matching results.
The top row displays frames subsampled from a task demonstration in the human kitchen source
domain. The bottom row visualizes the states matched to the source frames via the procedure
described in Section 4.3. The matched states represent corresponding semantic scenes in which the
agent e.g., opens the microwave, turns on the stove or opens the cabinet. Right: Quantitative results
on the kitchen manipulation task from human video demonstrations.
manipulation task, but requires many more environment interactions to reach the same performance
as our approach. SkillSeq succeeds in approximately 20% of the maze episodes and solves on average
3 out of 4 subtasks in the kitchen manipulation environment after ﬁne-tuning. The mixed success is
due to inaccuracies in execution of the skill policies since SkillSeq follows the ground truth sequence
of high-level skills. Our approach, STAR, can use cross-domain demonstrations to match the learning
efﬁciency of SkiLD (oracle) that has access to target domain demonstrations. This shows that our
approach is effective at extracting useful information from cross-domain demonstrations. During
downstream task training of the high-level semantic and execution policies our approach can ﬁx
both, errors in the high-level skill plan and the low-level skill execution. The ability to jointly adapt
high-level and low-level policies and e.g. react to failures in the low-level policy rather than following
a ﬁxed high-level plan is crucial for good performance on long-horizon tasks. We ﬁnd that this trend
holds even in the “pure” imitation learning (IL) setting without environment rewards, where we solely
rely on the learned discriminator reward to guide learning (see appendix, Section D for detailed
results). Thus, STAR can be used both, as a demonstration-guided RL algorithm and for cross-domain
imitation learning. Qualitative results can be viewed at https://tinyurl.com/star-rl and in
Figure 8.
To study the different components of our approach, we run ablations in the FrankaKitchen environment
(Fig. 5, right). Removing the discriminator-based weighting for the demonstration regularization
(-D-weight) (Eq. 4) or removing the demonstration regularization altogether (-DemoReg), leads
to poor performance. In contrast, removing the discriminator-based dense reward (-D-reward)
or temporal aggregation during matching (-TempAgg) affects learning speed but has the same
asymptotic performance. Finally, a model without the latent variable z (-z) cannot model the diversity
of skill executions in the data; the resulting skills are too imprecise to learn long-horizon tasks. We
show qualitative examples of the effect of varying matching window sizes [γ−, γ+] on the project
website: https://tinyurl.com/star-rl.
5.2
Imitation from Human Demonstrations
In this section we ask: can our approach be used to leverage human video demonstrations for teaching
new tasks to robots? Imitating human demonstrations presents a larger challenge since it requires
bridging domain differences that span observation spaces (from images in the real-world to low-
dimensional states in simulation), agent morphologies (from a bimanual human to a 7DOF robot
arm), and environments (from the real-world to a simulated robotic environment). To investigate this
question, we collect 20 human video demonstrations in a real-world kitchen, which demonstrate a
task the robotic agent needs to learn in the target simulated domain. Instead of collecting a large,
task-agnostic dataset in the human source domain and manually annotating semantic skill labels, we
demonstrate a more scalable alternative: we use an action recognition model, pre-trained on the EPIC
Kitchens dataset [33], zero-shot to predict semantic skill distributions on the human demonstration
videos. We deﬁne a mapping from the 97 verb and 300 noun classes in EPIC Kitchens to the skills
present in the target domain and then use our approach as described in Section 4.2, using the EPIC
skill distributions as the task-agnostic skill prior pTA(k|s). For data collection details, see Section C.4.
7

We visualize qualitative matching results between the domains in Figure 6, left. We successfully
match frames to the corresponding semantic states in the target domain. In Figure 6, right, we
show that this leads to successful semantic imitation of the human demonstrations. Our approach
STAR with EPIC Kitchens auto-generated skill distributions is able to reach the same asymptotic
performance as the oracle approach that has access to target domain demonstrations, with only slightly
reduced learning speed. It also outperforms the SkillSeq and SPiRL baselines (for qualitative results
see https://tinyurl.com/star-rl).
To recap: for this experiment we did not collect a large, task-agnostic human dataset and we did not
manually annotate any human videos. Collecting a few human demonstrations in an unseen kitchen
was sufﬁcient to substantially accelerate learning of the target task on the robot in simulation. This
demonstrates one avenue for scaling robot learning by (1) learning from easy-to-collect human video
demonstrations and (2) using pre-trained skill prediction models to bridge the domain gap.
5.3
Robustness to Noisy Demonstrations and Labels
STAR (all Tasks)
SPiRL
STAR (w/o Task 1)
STAR (w/o Task 2)
STAR (w/o Task 3)
SkillSeq (w/o Task 1)
SkillSeq (w/o Task 2)
SkillSeq (w/o Task 3)
Figure 7: Semantic imitation with missing skills in the
demonstrations. Our approach STAR still learns the full
task faster than learning without demonstrations (SPiRL),
while SkillSeq get stuck at the missing skill.
In realistic scenarios agents often need
to cope with noisy demonstration data,
e.g., with partial demonstrations or
faulty labels. Thus, we test STAR’s
ability to handle such noise. First, we
test imitation from partial demonstra-
tions with missing subskills. These
commonly occur when there are large
differences between source and tar-
get domain, e.g., the demonstration
domain might already have a pot on
the stove, and starts with “turn on the
stove”, but in the target domain we
need to ﬁrst place the pot on the stove.
We test this in the simulated kitchen
tasks by dropping individual subskills
from the demonstrations (“w/o Task i’ in Figure 7). Figure 7 shows that the SkillSeq approach strug-
gles with such noise: it gets stuck whenever the corresponding skill is missing in the demonstration.
In contrast, STAR can leverage demonstrations that are lacking complete subskills and still learn
faster than the no-demonstration baseline SPiRL. When a skill is missing, the STAR agent ﬁnds
itself off the demonstration support. Then the objective in equation 3 regularizes the policy towards
the task-agnostic skill prior, encouraging the agent to explore until it ﬁnds its way (back) to the
demonstration support. This allows our method to bridge “holes” in the demonstrations. We also
test STAR’s robustness to noisy semantic skill labels, in Section E. We ﬁnd that STAR is robust to
errors in the annotated skill lengths and to uncertain skill detections. Only frequent, high-conﬁdence
mis-detections of skills can lead to erroneous matches and decreased performance. Both experiments
show that STAR’s guidance with semantic demonstrations is robust to noise in the training and
demonstration data.
6
Conclusion and Limitations
In this work, we presented STAR, an approach for imitation based on semantic skills that can use
cross-domain demonstrations for accelerating RL. STAR is effective on multiple semantic imitation
problems, including using real-world human demonstration videos for learning a robotic kitchen
manipulation task. Our results present a promising way to use large-scale human video datasets like
EPIC Kitchens [33] for behavior learning in robotics. However, our approach assumes a pre-deﬁned
set of semantic skills and semantic skill labels on the training data. We demonstrated how such
assumptions can be reduced via the use of pre-trained skill prediction models. Yet, obtaining such
semantic information from cheaper-to-collect natural language descriptions of the training trajectories
without a pre-deﬁned skill set is an exciting direction for future work. Additionally, strengthening
the robustness to skill mis-labelings, e.g., via a more robust state matching mechanism, can further
improve performance on noisy, real-world datasets.
8

Acknowledgments
This work was supported by Institute of Information & Communications Technology Planning
& Evaluation (IITP) grants (No.2019-0-00075, Artiﬁcial Intelligence Graduate School Program,
KAIST; No.2022-0-00077, AI Technology Development for Commonsense Extraction, Reasoning,
and Inference from Heterogeneous Data) and National Research Foundation of Korea (NRF) grant
(NRF-2021H1D3A2A03103683), funded by the Korean government (MSIT).
References
[1] P. Sharma, L. Mohan, L. Pinto, and A. Gupta. Multiple interactions made easy (mime): Large
scale demonstrations data for imitation. In Conference on robot learning, pages 906–915.
PMLR, 2018.
[2] A. Mandlekar, Y. Zhu, A. Garg, J. Booher, M. Spero, A. Tung, J. Gao, J. Emmons, A. Gupta,
E. Orbay, S. Savarese, and L. Fei-Fei. Roboturk: A crowdsourcing platform for robotic skill
learning through imitation. In CoRL, 2018.
[3] A. Gupta, V. Kumar, C. Lynch, S. Levine, and K. Hausman. Relay policy learning: Solving
long-horizon tasks via imitation and reinforcement learning. CoRL, 2019.
[4] B. D. Argall, S. Chernova, M. Veloso, and B. Browning. A survey of robot learning from
demonstration. Robotics and autonomous systems, 57(5):469–483, 2009.
[5] D. A. Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In Proceedings of
Neural Information Processing Systems (NeurIPS), pages 305–313, 1989.
[6] J. Ho and S. Ermon. Generative adversarial imitation learning. NeurIPS, 2016.
[7] A. Rajeswaran, V. Kumar, A. Gupta, G. Vezzani, J. Schulman, E. Todorov, and S. Levine.
Learning complex dexterous manipulation with deep reinforcement learning and demonstrations.
In Robotics: Science and Systems, 2018.
[8] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel. Overcoming exploration
in reinforcement learning with demonstrations. In 2018 IEEE International Conference on
Robotics and Automation (ICRA), pages 6292–6299. IEEE, 2018.
[9] Y. Zhu, Z. Wang, J. Merel, A. Rusu, T. Erez, S. Cabi, S. Tunyasuvunakool, J. Kramár, R. Hadsell,
N. de Freitas, and N. Heess. Reinforcement and imitation learning for diverse visuomotor skills.
In Robotics: Science and Systems, 2018.
[10] X. B. Peng, P. Abbeel, S. Levine, and M. van de Panne. Deepmimic: Example-guided deep
reinforcement learning of physics-based character skills. ACM Transactions on Graphics (TOG),
37(4):1–14, 2018.
[11] R. S. Sutton, D. Precup, and S. Singh. Between MDPs and semi-MDPs: A framework for
temporal abstraction in reinforcement learning. Artiﬁcial Intelligence, 112:181–211, 1999.
[12] P.-L. Bacon, J. Harb, and D. Precup. The option-critic architecture. In AAAI, 2017.
[13] O. Nachum, S. S. Gu, H. Lee, and S. Levine. Data-efﬁcient hierarchical reinforcement learning.
NeurIPS, 2018.
[14] K. Pertsch, Y. Lee, Y. Wu, and J. J. Lim. Demonstration-guided reinforcement learning with
learned skills. In Conference on Robot Learning (CoRL), 2021.
[15] K. Hakhamaneshi, R. Zhao, A. Zhan, P. Abbeel, and M. Laskin. Hierarchical few-shot imitation
with skill transition models. arXiv preprint arXiv:2107.08981, 2021.
[16] K. Pertsch, Y. Lee, and J. J. Lim. Accelerating reinforcement learning with learned skill priors.
In Conference on Robot Learning (CoRL), 2020.
[17] A. Ajay, A. Kumar, P. Agrawal, S. Levine, and O. Nachum. Opal: Ofﬂine primitive discovery
for accelerating ofﬂine reinforcement learning. arXiv preprint arXiv:2010.13611, 2020.
9

[18] D. Xu, S. Nair, Y. Zhu, J. Gao, A. Garg, L. Fei-Fei, and S. Savarese. Neural task programming:
Learning to generalize across hierarchical tasks. In 2018 IEEE International Conference on
Robotics and Automation (ICRA). IEEE, 2018.
[19] D.-A. Huang, S. Nair, D. Xu, Y. Zhu, A. Garg, L. Fei-Fei, S. Savarese, and J. C. Niebles. Neural
task graphs: Generalizing to unseen tasks from a single video demonstration. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019.
[20] X. B. Peng, E. Coumans, T. Zhang, T.-W. Lee, J. Tan, and S. Levine. Learning agile robotic
locomotion skills by imitating animals. RSS, 2020.
[21] L. Smith, N. Dhawan, M. Zhang, P. Abbeel, and S. Levine. Avid: Learning multi-stage tasks
via pixel-level translation of human videos. arXiv preprint arXiv:1912.04443, 2019.
[22] N. Das, S. Bechtle, T. Davchev, D. Jayaraman, A. Rai, and F. Meier. Model-based inverse
reinforcement learning from visual demonstrations. CoRL, 2020.
[23] Y. Duan, M. Andrychowicz, B. C. Stadie, J. Ho, J. Schneider, I. Sutskever, P. Abbeel, and
W. Zaremba. One-shot imitation learning. NeurIPS, 2017.
[24] P. Sharma, D. Pathak, and A. Gupta. Third-person visual imitation learning via decoupled
hierarchical controller. NeurIPS, 2019.
[25] T. Yu, C. Finn, A. Xie, S. Dasari, T. Zhang, P. Abbeel, and S. Levine. One-shot imitation from
observing humans via domain-adaptive meta-learning. arXiv preprint arXiv:1802.01557, 2018.
[26] P. Sermanet, C. Lynch, Y. Chebotar, J. Hsu, E. Jang, S. Schaal, S. Levine, and G. Brain.
Time-contrastive networks: Self-supervised learning from video. In 2018 IEEE international
conference on robotics and automation (ICRA), 2018.
[27] A. S. Chen, S. Nair, and C. Finn. Learning generalizable robotic reward functions from"
in-the-wild" human videos. RSS, 2021.
[28] K. Schmeckpeper, O. Rybkin, K. Daniilidis, S. Levine, and C. Finn. Reinforcement learning
with videos: Combining ofﬂine observations with interaction. CoRL, 2020.
[29] T. Yu, P. Abbeel, S. Levine, and C. Finn. One-shot hierarchical imitation learning of compound
visuomotor tasks. RSS, 2018.
[30] K. Sohn, H. Lee, and X. Yan. Learning structured output representation using deep conditional
generative models. In Proceedings of Neural Information Processing Systems (NeurIPS), 2015.
[31] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerch-
ner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In
ICLR, 2017.
[32] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy
deep reinforcement learning with a stochastic actor. ICML, 2018.
[33] D. Damen, H. Doughty, G. M. Farinella, , A. Furnari, J. Ma, E. Kazakos, D. Moltisanti,
J. Munro, T. Perrett, W. Price, and M. Wray. Rescaling egocentric vision: Collection, pipeline
and challenges for epic-kitchens-100. International Journal of Computer Vision (IJCV), 2021.
[34] T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Kumar, H. Zhu, A. Gupta,
P. Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905,
2018.
[35] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han. On the variance of the adaptive
learning rate and beyond. In ICLR, 2020.
[36] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
[37] Y. Lee, A. Szot, S.-H. Sun, and J. J. Lim. Generalizable imitation learning from observation via
inferring goal proximity. Advances in Neural Information Processing Systems, 34, 2021.
10

[38] K. Lowrey, A. Rajeswaran, S. Kakade, E. Todorov, and I. Mordatch. Plan Online, Learn Ofﬂine:
Efﬁcient Learning and Exploration via Model-Based Control. In ICLR, 2019.
[39] H. Fan, Y. Li, B. Xiong, W.-Y. Lo, and C. Feichtenhofer. Pyslowfast, 2020.
11

Algorithm 2 STAR (Semantic Transfer Accelerated RL) – High-level Policy Optimization
1: Inputs: H-step reward function ˜r(st, kt, zt), reward weight κ, discount η, target divergences
δq, δp, δl, learning rates λπ, λQ, λα, target update rate τ.
2: Load
pre-trained
&
freeze:
low-level
policy
πl(at|st, kt, zt),
skill
priors
pdemo(kt|st), pTA(kt|st), pTA(zt|st, kt), discriminator D(s), progress predictor P(s)
3: Initialize: replay buffer D, high-level policies πsem
σ (kt|st), πlat
θ (zt|st, kt), critic Qφ(st, kt, zt),
target network Q ¯φ(st, kt, zt)
4:
5: for each iteration do
6:
for every H environment steps do
7:
kt ∼πsem(kt|st)
▷sample semantic skill from policy
8:
zt ∼πlat(zt|st, zt)
▷sample latent skill from policy
9:
st′ ∼p(st+H|st, πl(at|st, kt, zt))
▷execute skill in environment
10:
D ←D ∪{st, kt, zt, RΣ, st′}
▷store transition in replay buffer, RΣ = H-step summed reward
11:
for each gradient step do
12:
˜r = (1 −κ) · RΣ + κ ·

log D(st) −log
 1 −D(st)

▷compute combined reward
13:
˜r = 1(D(s) < 0.5) · ˜r + 1(D(s) ≥0.5) · ˜r · P(s) ▷optionally shape reward with progress predictor
14:
¯Q = ˜r + η

Q ¯φ(st′, πlat
θ (zt′|st′, πsem
σ (kt′|st′)))
15:
−

αqDKL
 πsem
σ (kt′|st′), pdemo(kt′|st′)

· D(st′)
16:
+ αpDKL
 πsem
σ (kt′|st′), pTA(kt′|st′)

·
 1 −D(st′)

17:
+ αlDKL
 πlat
θ (zt′|st′, kt′), pTA(zt′|st′, kt′)

▷compute
Q-target
18:
(σ, θ) ←(σ, θ) −λπ∇(σ,θ)

Qφ(st, πlat
θ (zt|st, πsem
σ (kt|st)))
19:
−

αqDKL
 πsem
σ (kt|st), pdemo(kt|st)

· D(st)
20:
+ αpDKL
 πsem
σ (kt|st), pTA(kt|st)

·
 1 −D(st)

21:
+ αlDKL
 πlat
σ (zt|st, kt), pTA(zt|st, kt)

▷update
high-level policy weights
22:
φ ←φ −λQ∇φ
 1
2
 Qφ(st, kt, zt) −¯Q
2
▷update critic weights
23:
αq ←αq −λα∇αq

αq · (DKL(πsem
σ (kt|st), pdemo(kt|st)) −δq)

▷update alpha values
24:
αp ←αp −λα∇αp

αp · (DKL(πsem
σ (kt|st), pTA(kt|st)) −δp)

25:
αl ←αl −λα∇αl

αl · (DKL(πlat
θ (zt|st, kt), pTA(zt|st, kt)) −δl)

26:
¯φ ←τφ + (1 −τ)¯φ
▷update target network weights
27: return trained high-level policies πsem
σ (kt|st), πlat
θ (zt|st, kt)
A
Full Algorithm
We present a detailed description of the downstream RL algorithm for our STAR approach in
Algorithm 2. It builds on soft actor-critic [32, 34]. In contrast to the original SAC we operate in a
hybrid action space with mixed discrete and continuous actions: πsem(k|s) outputs discrete semantic
skill IDs and πlat(z|s, k) outputs continuous latent variables.
For all input hyperparameters we use the default values from Pertsch et al. [16, 14] and only adapt
the regularization weights αq, αp and αl for each task. They can either be set to a ﬁxed value or
automatically tuned via dual gradient descent in lines 24-26 by setting target parameters δq, δp and
δl [34].
B
Overview of Pertsch et al. [14]
While the goal of our work is to imitate semantic skills across domains, we build on ideas from
Pertsch et al. [14], which use in-domain demonstrations. Pertsch et al. [14] study demonstration-
guided RL using a two-layer hierarchical policy architecture: a high-level policy πh(z|s) outputs
temporally extended actions, or skills, as learned latent representation z. The skill z gets decoded
into actions a by a learned low-level policy πl(a|s, z). Here, z captures a skill’s behavior in terms
of its low-level actions instead of its semantics. Pertsch et al. [14] assume access to two datasets:
demonstration trajectories Ddemo which solve the task at hand and a task-agnostic dataset DTA of
state-action trajectories from a range of prior tasks. First, they pre-train the latent skill representation
12

z and the low-level policy πl(a|s, z) using DTA. Next, they use the demonstration dataset Ddemo
and the task-agnostic dataset DTA to learn a demonstration prior pdemo(z|s) and a task-agnostic prior
pTA(z|s) over z. The former captures the distribution over skills in the demonstrations, while the
latter represents the skills in the task-agnostic dataset. Additionally, they use both datasets to train a
discriminator D(s) to distinguish states sampled from the task-agnostic and demonstration data. Both
pre-trained prior distributions are used to regularize the high-level policy πh(z|s) during RL: when
D(s) classiﬁes a state as part of the demonstrations, the policy is regularized towards pdemo(z|s),
encouraging it to imitate the demonstrated skills. In states which D(s) classiﬁes as outside the
demonstration support, the policy is regularized towards pTA(z|s), encouraging it to explore the
environment to reach back onto the demonstration support. The optimization objective for πh(z|s) is:
Eπh

R(s, a) −αqDKL
 πh(z|s), pdemo(z|s)

· D(s)
|
{z
}
demonstration prior regularization
−αpDKL
 πh(z|s), pTA(z|s)

· (1 −D(s))
|
{z
}
task-agnostic prior regularization

.
(6)
Crucially, the learned skills z do not represent semantic skills, but instead reﬂect the underlying
sequences of low-level actions. Thus, Pertsch et al. [14]’s approach is unsuitable for cross-domain
imitation, since a policy would imitate the demonstration’s low-level actions instead of its semantics.
C
Implementation Details
C.1
Skill Learning
We summarize the pre-training objectives of all model components in Table 1. We instantiate all
components with deep neural networks. We use a single-layer LSTM with 128 hidden units for
the inference network and 3-layer MLPs with 128 hidden units for the low-level policy. The skill-
representation z is a 10-dimensional continuous latent variable. All skill priors are implemented
as 5-layer MLPs with 128 hidden units. The semantic skill priors output logits of a categorical
distribution over k, the non-semantic prior outputs mean and log-variance of a diagonal Gaussian
distribution over z. We use batch normalization after every layer and leaky ReLU activations. We
auto-tune the regularization weight β for training the low-level skill policy using dual gradient descent
and set the target to 2e−2 for the maze and 1e−2 for all kitchen experiments.
When training on image-based human data we add a 6-layer CNN-encoder to the semantic skill prior
pTA(k|s) trained on the source domain dataset DS. The encoder reduces image resolution by half and
doubles the number of channels in each layer, starting with a resolution of 64x64 and 8 channels in
the ﬁrst layer. We use batch normalization and leaky ReLU activations for this encoder too.
The demonstration discriminator D(s) is implemented as a 2-layer MLP with 32 hidden units and no
batch normalization to avoid overﬁtting. We use a sigmoid activation in it’s ﬁnal layer to constrain its
output in range (0, 1).
For cross-domain state matching we use a symmetric temporal window with γ−, γ+ = 0.99. Only in
the experiments with missing skills (see Section 5.3) we set γ−, γ+ = 0.
All networks are optimized using the RAdam optimizer [35] with parameters β1 = 0.9 and β2 =
0.999, batch size 128 and learning rate 1e−3. The computational complexity of our approach is
comparable to that of prior skill-based RL approaches like Pertsch et al. [16]. On a single NVIDIA
V100 GPU we can train the low-level policy and all skill priors in approximately 10 hours and the
demonstration discriminator in approximately 3 hours.
C.2
Semantic Imitation
The high-level policies πsem(k|s) and πlat(z|s, k) are implemented as 5-layer MLPs with batch
normalization and ReLU activations. The former outputs the logits of a categorical distribution over
k, the latter the mean and log-variance of a diagonal Gaussian distribution over z. We initialize
the semantic high-level policy with the pre-trained demonstration skill prior pdemo(k|s) and the
non-semantic high-level policy with the pre-trained task-agnostic latent skill prior pTA(z|s, k). We
implement the critic as a 5-layer MLP with 256 hidden units per layer that outputs a |K|-dimensional
vector of Q-values. The scalar Q-value is then computed as the expectation under the output
distribution of πsem.
13

Table 1: List of all pre-trained model components, their respective functionality and their pre-training
objectives. We use ⌊·⌋to indicate stopped gradients and τ T to denote demonstration trajectories
relabeled with matched target domain states from Section 4.3.
MODEL
SYMBOL
DESCRIPTION
TRAINING OBJECTIVE
Skill Policy
πl(a|s, k, z)
Executes a given skill, deﬁned by se-
mantic skill ID and low-level execution
latent.
Equation (1)
Demonstration
Semantic Skill
Distribution
pdemo(k|s)
Captures semantic skill distribution of
demonstration sequences.
Es,a,k∼τT

−PK
i=1 ki · log pdemo(ki|s)

Task-Agnostic
Semantic Skill
Prior
pTA(k|s)
Captures semantic skill distribution of
task-agnostic prior experience.
Es,a,k∼DT

−PK
i=1 ki · log pTA(ki|s)

Task-Agnostic
Low-level
Execution Prior
pTA(z|s, k)
Captures distribution over low-level ex-
ecution latents from task-agnostic prior
experience.
Es,a,k∼DT

DKL
 ⌊q(z|s, a, k)⌋, pTA(z|s, k)

Demonstration
Support
Dis-
criminator
D(s)
Determines whether a state is within the
support of the demonstrations.
−1
2·

Es∼τT

log D(s)

|
{z
}
demonstrations
+ Es∼DT

log
 1 −D(s)

|
{z
}
task-agnostic data

We use batch size 256, replay buffer capacity of 1e6 and discount factor γ = 0.99. We warm-start
training by initializing the replay buffer with 2000 steps. We use the Adam optimizer [36] with
β1 = 0.9, β2 = 0.999 and learning rate 3e−4 for updating policy and critic. Analogous to SAC,
we train two separate critic networks and compute the Q-value as the minimum over both estimates
to stabilize training. The target networks get updated at a rate of τ = 5e−3. The latent high-level
policy’s actions are limited in the range [−2, 2] by a tanh "squashing function" (see Haarnoja et al.
[32], appendix C). We set all α parameters to ﬁxed values of 10 in the maze navigation task and 5e−2
in all kitchen tasks.
For reward computation we set the factor κ = 0.9, i.e., we blend environment and discriminator-based
rewards. In practice, we ﬁnd that we can improve convergence speed by using a shaped discriminator
reward that increases towards the end of the demonstration. This is comparable to goal-proximity
based rewards used in in-domain imitation, e.g., Lee et al. [37]. To compute the shaped reward, we
pre-train a progress predictor P(s) along with the discriminator D(s). P(s) estimates the time step
of a state within a demonstration relative to the total length of the demonstration, thus its outputs
are bound in the range [0, 1]. We implement the progress predictor as a simple 3-layer MLP with a
sigmoid output activation. During RL training we can then compute the shaped reward as:
r(s, a) = κ · R(s, a) + (1 −κ) ·
P(s) · RD
if P(s) ≥0.5
RD
otherwise
with RD = log D(st) −log
 1 −D(st)

(7)
For all RL results we average the results of three independently seeded runs and display mean and
standard deviation across seeds. The computation time for these experiments varies by environment
and is mainly determined by the simulation time of the used environments. Across all environments
we can typically ﬁnish downstream task training within <12h on a single NVIDIA V100 GPU.
C.3
Comparisons
SPiRL.
We follow the approach of Pertsch et al. [16] which ﬁrst trains a latent skill representation
from task-agnostic data and then uses a pre-trained task-agnostic prior to regularize the policy
during downstream learning. To allow for fair comparison, we adapt the SPiRL approach to work
with our semantic skill model. In this way both SPiRL and STAR use the same set of learned
semantic skills. During downstream task learning we regularize both high-level policies πsem(k|s)
14

SkiLD (oracle)
SPiRL
SkillSeq
STAR (ours)
Figure 8: Qualitative maze results. We visualize the trajectories of the different policies during
training. The SkiLD approach leverages in-domain demonstrations to quickly learn how to reach the
goal. SPiRL leverages skills from the task-agnostic dataset to widely explore the maze, but fails to
reach the goal. SkillSeq makes progress towards the goal, but can get stuck in intermediate rooms,
leading to a substantially lower success rate than the oracle method. Our approach, STAR, is able to
match the performance of the oracle baseline and quickly learn to reach the goal while following the
sequence of demonstrated rooms.
and πlat(z|s, k) using the corresponding task-agnostic skill priors pTA(k|s) and pTA(z|s, k), analogous
to the task-agnostic skill prior regularization in the original SPiRL work.
SkiLD.
We similarly adapt SkiLD [14] to work with our learned semantic skill model. In contrast to
the SPiRL comparison, we now regularize both high-level policies with skill distributions trained on
the target domain demonstrations whenever D(s) classiﬁes a state as being part of the demonstration
support (see Section B).
SkillSeq.
We pre-train a skill-ID conditioned policy on the task-agnostic target domain dataset DT
using behavioral cloning. We split this policy into a 3-layer MLP encoder and a 3-layer MLP policy
head that produces the output action. The policy has an additional 3-layer MLP output head that is
trained to estimate whether the current skill terminates in the input state. We use the semantic skill
labels k in the task-agnostic dataset to determine states in which a skill ends and train the termination
predictor as a binary classiﬁer. During downstream learning, we use a programmatic high-level policy
that has access to the true sequence of semantic skills required to solve the downstream task and
conditions the low-level policy on these skill IDs one-by-one. The skill ID is switched to the next
skill when the pre-trained termination predictor infers the current state as a terminal state for the
current skill. For fair comparison we use online RL for ﬁnetuning the skill-conditioned policy via
soft actor-critic (SAC, Haarnoja et al. [32]).
BC+RL.
We train a policy directly on the source domain demonstrations via behavioral cloning. We
then use this pre-trained policy to initialize the policy during target task training in the target domain.
We ﬁne-tune this initialization using SAC with the rewards provided by the target environment.
Similar to Rajeswaran et al. [7], Nair et al. [8] we regularize the policy towards the pre-trained BC
policy during downstream learning.
C.4
Environments and Data Collection
Maze navigation.
We generate two maze layouts with the same number of rooms. We indicate a
room’s semantic ID via its color in Figure 10. We ensure the same “room connectivity” between both
layouts, i.e., corresponding semantic rooms have the same openings between each other. For example
the yellow room connects to the blue room but not to the green room in both layouts. This ensures
that we can follow the same sequence of semantic rooms in both environments. While we ensure
that the semantic layout of the mazes is equivalent, their physical layout is substantially different:
the mazes are rotated by 180 degrees, for example the red room is in the bottom right corner for the
ﬁrst maze but in the top left corner for the second. Additionally, the layout of individual rooms and
the positions of obstacle walls change between the domains. As a result, simple imitation of the
low-level planar velocity actions from one domain will not lead to successfully following the same
sequence of semantic rooms in the other domain. We deﬁne a total of 48 semantic skills: one for
15

FrankaKitchen [Gupta et al.’19]
1
2
3
4
1
2
3
4
Rearranged FrankaKitchen
Real-World Kitchen
1
2
3
4
Figure 9: Three semantically equivalent kitchen environments. Left: FrankaKitchen environment [3],
middle: rearranged FrankaKitchen environment, right: real-world kitchen. In all three environments
we deﬁne the same set of seven semantic object manipulation skills like “open the microwave”, “turn
on the stove” etc. The two simulated kitchen environments require different robot joint actuations to
perform the same semantic skills. The real-world kitchen has a different agent embodiment (robot vs.
human), layout and observation domain (low-dimensional state vs image observations).
each room-to-room traversal, e.g., “go from the red room to the green room”, and one for reaching a
goal within each room, e.g., “reach a goal in the green room”. Thus, the semantic description of a
demonstrated trajectory could for example be: “Go from the red room to the green room, then from
the green to the beige room, . . . , then from the blue to the orange room and then reach a goal in the
orange room.”
Source Maze
Target Maze
Figure 10: Source and target seman-
tic maze navigation environments. A
room’s color indicates its semantic ID.
The red trajectory shows the traversal
of semantic rooms demonstrated in the
source domain and the corresponding tra-
jectory in the target domain. The low-
level planar velocity commands required
to follow the demonstration in the target
domain is substantially different.
Simulated Kitchen.
We use the FrankaKitchen environ-
ment of Gupta et al. [3] (see Figure 9, left) and deﬁne
a set of seven semantic manipulation skills: opening the
microwave, opening the slide and hinge cabinet, turning
on bottom and top stove and ﬂipping the light switch. We
also create a rearranged version of the kitchen environment
(Figure 9, middle) with different layout and visual appear-
ance but the same set of semantic interaction options. In
both environments we use the state-action deﬁnition of
Gupta et al. [3]: (1) a 60-dimensional state representation
consisting of the agent’s joint state as well as object states
like opening angles or object pose, (2) a 9-dimensional
action space consisting of 7 robot joint velocities and two
gripper ﬁnger positions.
For the FrankaKitchen environment we can use the data
provided by Gupta et al. [3]: 600 human teleoperated
sequences each solving four different semantic tasks in
sequence. In the newly created rearranged kitchen envi-
ronment we collect a comparable dataset by controlling
the robot via trajectory optimization on a dense reward
function. We use the CEM implementation of Lowrey
et al. [38]. For both datasets we label the semantic skills
by programmatically detecting the end state of an object interaction using the low-dimensional state
representation.
Real-World Kitchen.
Data collection is performed by ﬁxating a GoPro camera to the head of a
human data collector which then performs a sequence of semantic skills. The camera is angled to
widely capture the area in front of the human. During data collection and within each trajectory we
vary the viewpoint, skill execution speed and hand used for skill execution. We collect 20 human
demonstration sequences for the task sequence: open microwave, move kettle, turn on stove, open
cabinet. We then automatically generate semantic skill predictions via zero-shot inference with a
pre-trained action recognition model. Speciﬁcally, we use the publicly available SlowFast model
16

trained on the EPIC Kitchens 100 dataset [33, 39]. The model takes in a window of 32 consecutive
video images at a 256 × 256 px resolution and outputs a distribution over 97 verb and 300 object
classes. Since our simulated FrankaKitchen target environment does not support the same set of
skills, we deﬁne a mapping from the output of the EPIC Kitchens model to the applicable skills
in the Franka Kitchen environment, e.g., we will map outputs for the verb “open” and the noun
“microwave” to the “open microwave” skill in FrankaKitchen. Note that some skill distinctions in the
FrankaKitchen environment are not supported by EPIC Kitchens, like “turn on top burner” vs “turn
on bottom burner”. In such cases we map the outputs of the EPIC Kitchens model to a single skill in
the target environment. With this skill mapping we ﬁnetune the EPIC Kitchens model for outputting
the relevant classes. Note that this model ﬁnetuning is performed with the original EPIC Kitchens
data, i.e., no additional, domain speciﬁc data is used in this step and no additional annotations need to
be collected. This ﬁnetuning is performed such that the resulting model directly outputs a distribution
over the relevant skills. Alternatively, the relevant skills could be extracted from the output of the
original model and the distribution could be renormalized.
To generate the skill predictions for the human video demonstrations, we move a sliding window of 32
frames over the demonstrations and generate a prediction in each step using the EPIC Kitchens model.
We pad the resulting skill distribution sequence with the ﬁrst and last predicted skill distribution to
obtain the same number of skill predictions as there are frames in the demonstration video. Then we
use the sequence of skill distributions to perform cross-domain matching and semantic imitation as
detailed in Section 4.2, without any changes to the algorithm.
D
Imitation Learning Results
STAR w/   Env Reward
STAR w/o Env Reward
BC
GAIL
Figure 11: Imitation learning on the
simulated FrankaKitchen task. Our ap-
proach STAR is able to learn the target
task even without access to any environ-
ment rewards, while common imitation
learning approaches fail to learn the task
due to the large domain gap between
source demonstrations and target task en-
vironment.
We evaluate our approach in the “pure” imitation learning
setting in the kitchen environment. Here, we assume no
access to environment rewards. Instead, we rely solely
on the discriminator-based reward learned from the cross-
domain demonstrations to guide learning (see Section 4.2).
We present evaluations in the FrankaKitchen environment
in Figure 11. Our approach STAR is able to learn the
target task from demonstrations without any environment
rewards, although learning is somewhat slower than in the
demonstration-guided RL setting with environment reward
access. In contrast, standard imitation learning approaches
are unable to learn the task since they struggle with the
large domain gap between source domain demonstrations
and target domain execution. These results show that our
approach STAR is applicable both, in the demonstration-
guided RL setting with environment rewards, and in the
imitation learning setting without environment rewards.
E
Label Noise Robustness Analysis
An important aspect for the scalability of an approach is its ability to cope with noise in the training
data. While prior work on skill-based RL has investigated the robustness of such approaches to
suboptimal behavior in the training data [16], we will focus on an aspect of the training data that
is speciﬁcally important for our cross-domain imitation approach: the semantic skill labels. In this
section, we investigate the robustness of our approach to different forms of noise on the semantic skill
labels. Such noise can either be introduced through inaccuracies in the manual labeling process or via
an automated form of skill labeling, as performed with the EPIC kitchens models in Section 5.2. To
cleanly investigate the robustness to different forms of skill label noise, we start from a noise-free set
of labels, which we can easily obtain programmatically in the simulated FrankaKitchen environment.
We then artiﬁcially perturb the labels to introduce artifacts that mimic realistic labeling errors. This
allows us to (1) investigate different forms of noise independently and (2) vary the magnitude of the
introduced noise in a controlled way.
Speciﬁcally, we introduce noise along three axis:
17

No Noise
Noise (strong)
Noise (weak)
Noise (middle)
Figure 12: Robustness of our approach, STAR, to different forms of noise in the semantic skill labels.
Our approach is robust to noise in the length of annotated skills and uncertainty between different
skills. While STAR is also shows some robustness to completely incorrect skill labels, frequent and
conﬁdent mis-detections / mis-labelings can lead to errors during the cross-domain matching and
thus impair learning performance on the target task.
Table 2: Parametrization of the noise levels for the skill label robustness experiment.
SKILL LENGTH NOISE
SKILL UNCERTAINTY NOISE
SKILL MISDETECTION NOISE
Varied
Parameter
ln
(Percentual length
noise window)
Nn
(Number of
uncertain segments)
Nm
(number of
misdetected segments)
Weak Noise
10 %
1
1
Middle Noise
20 %
2
2
Strong Noise
30 %
3
3
• skill length noise: artiﬁcially perturbs the length of a labeled skill within a range [1 −
ln . . . 1 + ln] of the true length of the skill, mimicking a labeler’s uncertainty on when
exactly a skill ends
• skill uncertainty noise: perturbs the distribution over detected skills around Nn transition
between skills by adding probability weight to erroneous skills produced via a random walk,
mimicking the uncertainty e.g., produced by a pre-trained action recognition model
• skill misdetection noise: adds Nm incorrectly detected skill segments at randomly sampled
points throughout the sequence of randomly sampled lengths, mimicking mis-labelings
which can (rarely) occur in human data or (more frequently) in auto-labeled data
We show evaluations of our approach with different levels of noise along all three axis in Figure 12.
We perform these evaluations in the simulated FrankaKitchen environment and average performance
across 10 seeds to reduce the noise-induced variance in the results. The parameters of the different
tested noise levels are detailed in Table 2.
The results in Figure 12 show that STAR is robust to a wider range of noise levels in the annotated
skill length and uncertainty between the skills: the performance does not signiﬁcantly change even
with increased noise levels. However, we ﬁnd that conﬁdent mis-predictions / mis-labelings of skills
can have a negative impact on the performance. Particularly if mis-predictions happen frequently
(“Noise (strong)”), states between the source and target domain can be mismatched, leading to worse
target task performance. But we ﬁnd that even in the case of mis-detections STAR is able to handle a
moderate amount of such noise robustly, which is important for STAR’s scalability to large and noisy
real-world datasets.
18

s
a
π(a|s)
pdemo(k|s)
pTA(k|s)
⇡sem(k|s)
⇡lat(z|s, k)
⇡l(a|s, k, z)
pTA(z|s, k)
= Pre-Trained w/ Supervised 
Learning & Frozen
= Trained Online with RL
Support 
Discriminator
Demonstration 
Regularization
Task-Agnostic 
Regularization
Execution 
Regularization
Semantic Policy
Low-level Policy
Execution 
Latent Policy
D(s)
Figure 13: Visualization of all model components. The colors indicate the objective type used for
training. Only the high-level policy is trained with online RL on the downstream task, all other
components are pre-trained fully ofﬂine via supervised learning and frozen during downstream
training.
s
a
π(a|s)
pTA(k|s)
<latexit sha1_base64="vpQAmA6WDz/Qp9Ril18cg5Y6Mfw=">AB/3icbVDLSgMxFM3UV62vUcGNm2AR6qbMiKDLqhuXFfqCdiyZNOGZjJDckcsYxf+ihsXirj1N9z5N6btLT1QOBwzrncm+PHgmtwnG8rt7S8srqWXy9sbG5t79i7ew0dJYqyOo1EpFo+0UxwyerAQbBWrBgJfcGa/vB64jfvmdI8kjUYxcwLSV/ygFMCRuraB/FdB9gDpLXLcWmIOyYLWJ907aJTdqbAi8TNSBFlqHbtr04voknIJFBtG67TgxeShRwKti40Ek0iwkdkj5rGypJyLSXTu8f42Oj9HAQKfMk4Kn6eyIlodaj0DfJkMBAz3sT8T+vnUBw4aVcxgkwSWeLgkRgiPCkDNzjilEQI0MIVdzciumAKELBVFYwJbjzX14kjdOy65Td27Ni5SqrI48O0REqIRedowq6QVURxQ9omf0it6sJ+vFerc+ZtGclc3soz+wPn8AN6iVjw=</latexit><latexit sha1_base64="vpQAmA6WDz/Qp9Ril18cg5Y6Mfw=">AB/3icbVDLSgMxFM3UV62vUcGNm2AR6qbMiKDLqhuXFfqCdiyZNOGZjJDckcsYxf+ihsXirj1N9z5N6btLT1QOBwzrncm+PHgmtwnG8rt7S8srqWXy9sbG5t79i7ew0dJYqyOo1EpFo+0UxwyerAQbBWrBgJfcGa/vB64jfvmdI8kjUYxcwLSV/ygFMCRuraB/FdB9gDpLXLcWmIOyYLWJ907aJTdqbAi8TNSBFlqHbtr04voknIJFBtG67TgxeShRwKti40Ek0iwkdkj5rGypJyLSXTu8f42Oj9HAQKfMk4Kn6eyIlodaj0DfJkMBAz3sT8T+vnUBw4aVcxgkwSWeLgkRgiPCkDNzjilEQI0MIVdzciumAKELBVFYwJbjzX14kjdOy65Td27Ni5SqrI48O0REqIRedowq6QVURxQ9omf0it6sJ+vFerc+ZtGclc3soz+wPn8AN6iVjw=</latexit><latexit sha1_base64="vpQAmA6WDz/Qp9Ril18cg5Y6Mfw=">AB/3icbVDLSgMxFM3UV62vUcGNm2AR6qbMiKDLqhuXFfqCdiyZNOGZjJDckcsYxf+ihsXirj1N9z5N6btLT1QOBwzrncm+PHgmtwnG8rt7S8srqWXy9sbG5t79i7ew0dJYqyOo1EpFo+0UxwyerAQbBWrBgJfcGa/vB64jfvmdI8kjUYxcwLSV/ygFMCRuraB/FdB9gDpLXLcWmIOyYLWJ907aJTdqbAi8TNSBFlqHbtr04voknIJFBtG67TgxeShRwKti40Ek0iwkdkj5rGypJyLSXTu8f42Oj9HAQKfMk4Kn6eyIlodaj0DfJkMBAz3sT8T+vnUBw4aVcxgkwSWeLgkRgiPCkDNzjilEQI0MIVdzciumAKELBVFYwJbjzX14kjdOy65Td27Ni5SqrI48O0REqIRedowq6QVURxQ9omf0it6sJ+vFerc+ZtGclc3soz+wPn8AN6iVjw=</latexit><latexit sha1_base64="vpQAmA6WDz/Qp9Ril18cg5Y6Mfw=">AB/3icbVDLSgMxFM3UV62vUcGNm2AR6qbMiKDLqhuXFfqCdiyZNOGZjJDckcsYxf+ihsXirj1N9z5N6btLT1QOBwzrncm+PHgmtwnG8rt7S8srqWXy9sbG5t79i7ew0dJYqyOo1EpFo+0UxwyerAQbBWrBgJfcGa/vB64jfvmdI8kjUYxcwLSV/ygFMCRuraB/FdB9gDpLXLcWmIOyYLWJ907aJTdqbAi8TNSBFlqHbtr04voknIJFBtG67TgxeShRwKti40Ek0iwkdkj5rGypJyLSXTu8f42Oj9HAQKfMk4Kn6eyIlodaj0DfJkMBAz3sT8T+vnUBw4aVcxgkwSWeLgkRgiPCkDNzjilEQI0MIVdzciumAKELBVFYwJbjzX14kjdOy65Td27Ni5SqrI48O0REqIRedowq6QVURxQ9omf0it6sJ+vFerc+ZtGclc3soz+wPn8AN6iVjw=</latexit>
πsem(k|s)
<latexit sha1_base64="cNuFyzr3mzwB0m2tA/vnk2QGE=">AB/XicbVDLSgMxFM34rPU1PnZugkWomzIjgi6LblxWsA9ox5J7ShSWZIMkIdir/ixoUibv0Pd/6NaTsLbT0QOJxzLvfmhAln2njet7O0vLK6tl7YKG5ube/sunv7DR2nikKdxjxWrZBo4ExC3TDoZUoICLk0AyH1xO/+QBKs1jemVECgSB9ySJGibFS1z3sJOw+0yDG5SHu2KTB+rTrlryKNwVeJH5OSihHret+dXoxTQVIQznRu17iQkyogyjHMbFTqohIXRI+tC2VBIBOsim14/xiV6OIqVfdLgqfp7IiNC65EIbVIQM9Dz3kT8z2unJroMiaT1ICks0VRyrGJ8aQK3GMKqOEjSwhVzN6K6YAoQo0trGhL8Oe/vEgaZxXfq/i356XqV5HAR2hY1RGPrpAVXSDaqiOKHpEz+gVvTlPzovz7nzMoktOPnOA/sD5/AHZQJTP</latexit><latexit sha1_base64="cNuFyzr3mzwB0m2tA/vnk2QGE=">AB/XicbVDLSgMxFM34rPU1PnZugkWomzIjgi6LblxWsA9ox5J7ShSWZIMkIdir/ixoUibv0Pd/6NaTsLbT0QOJxzLvfmhAln2njet7O0vLK6tl7YKG5ube/sunv7DR2nikKdxjxWrZBo4ExC3TDoZUoICLk0AyH1xO/+QBKs1jemVECgSB9ySJGibFS1z3sJOw+0yDG5SHu2KTB+rTrlryKNwVeJH5OSihHret+dXoxTQVIQznRu17iQkyogyjHMbFTqohIXRI+tC2VBIBOsim14/xiV6OIqVfdLgqfp7IiNC65EIbVIQM9Dz3kT8z2unJroMiaT1ICks0VRyrGJ8aQK3GMKqOEjSwhVzN6K6YAoQo0trGhL8Oe/vEgaZxXfq/i356XqV5HAR2hY1RGPrpAVXSDaqiOKHpEz+gVvTlPzovz7nzMoktOPnOA/sD5/AHZQJTP</latexit><latexit sha1_base64="cNuFyzr3mzwB0m2tA/vnk2QGE=">AB/XicbVDLSgMxFM34rPU1PnZugkWomzIjgi6LblxWsA9ox5J7ShSWZIMkIdir/ixoUibv0Pd/6NaTsLbT0QOJxzLvfmhAln2njet7O0vLK6tl7YKG5ube/sunv7DR2nikKdxjxWrZBo4ExC3TDoZUoICLk0AyH1xO/+QBKs1jemVECgSB9ySJGibFS1z3sJOw+0yDG5SHu2KTB+rTrlryKNwVeJH5OSihHret+dXoxTQVIQznRu17iQkyogyjHMbFTqohIXRI+tC2VBIBOsim14/xiV6OIqVfdLgqfp7IiNC65EIbVIQM9Dz3kT8z2unJroMiaT1ICks0VRyrGJ8aQK3GMKqOEjSwhVzN6K6YAoQo0trGhL8Oe/vEgaZxXfq/i356XqV5HAR2hY1RGPrpAVXSDaqiOKHpEz+gVvTlPzovz7nzMoktOPnOA/sD5/AHZQJTP</latexit><latexit sha1_base64="cNuFyzr3mzwB0m2tA/vnk2QGE=">AB/XicbVDLSgMxFM34rPU1PnZugkWomzIjgi6LblxWsA9ox5J7ShSWZIMkIdir/ixoUibv0Pd/6NaTsLbT0QOJxzLvfmhAln2njet7O0vLK6tl7YKG5ube/sunv7DR2nikKdxjxWrZBo4ExC3TDoZUoICLk0AyH1xO/+QBKs1jemVECgSB9ySJGibFS1z3sJOw+0yDG5SHu2KTB+rTrlryKNwVeJH5OSihHret+dXoxTQVIQznRu17iQkyogyjHMbFTqohIXRI+tC2VBIBOsim14/xiV6OIqVfdLgqfp7IiNC65EIbVIQM9Dz3kT8z2unJroMiaT1ICks0VRyrGJ8aQK3GMKqOEjSwhVzN6K6YAoQo0trGhL8Oe/vEgaZxXfq/i356XqV5HAR2hY1RGPrpAVXSDaqiOKHpEz+gVvTlPzovz7nzMoktOPnOA/sD5/AHZQJTP</latexit>
πlat(z|s, k)
<latexit sha1_base64="c/Ra8OoU+P8GWDNB5OTzhF6v8xI=">ACAHicbVDLSsNAFJ3UV62vqAsXbgaLUEFKIoIui25cVrAPaGKZTCft0MkzNwINWTjr7hxoYhbP8Odf+P0sdDWAwOHc87lzj1BIrgGx/m2CkvLK6trxfXSxubW9o69u9fUcaoa9BYxKodEM0El6wBHARrJ4qRKBCsFQyvx37rgSnNY3kHo4T5EelLHnJKwEhd+8BL+H0mCOSVR+yZJGB9iocnXbvsVJ0J8CJxZ6SMZqh37S+vF9M0YhKoIFp3XCcBPyMKOBUsL3mpZgmhQ9JnHUMliZj2s8kBOT42Sg+HsTJPAp6ovycyEmk9igKTjAgM9Lw3Fv/zOimEl37GZICk3S6KEwFhiP28A9rhgFMTKEUMXNXzEdEUomM5KpgR3/uRF0jyruk7VvT0v165mdRTRITpCFeSiC1RDN6iOGoiHD2jV/RmPVkv1rv1MY0WrNnMPvoD6/MHhWVrw=</latexit><latexit sha1_base64="c/Ra8OoU+P8GWDNB5OTzhF6v8xI=">ACAHicbVDLSsNAFJ3UV62vqAsXbgaLUEFKIoIui25cVrAPaGKZTCft0MkzNwINWTjr7hxoYhbP8Odf+P0sdDWAwOHc87lzj1BIrgGx/m2CkvLK6trxfXSxubW9o69u9fUcaoa9BYxKodEM0El6wBHARrJ4qRKBCsFQyvx37rgSnNY3kHo4T5EelLHnJKwEhd+8BL+H0mCOSVR+yZJGB9iocnXbvsVJ0J8CJxZ6SMZqh37S+vF9M0YhKoIFp3XCcBPyMKOBUsL3mpZgmhQ9JnHUMliZj2s8kBOT42Sg+HsTJPAp6ovycyEmk9igKTjAgM9Lw3Fv/zOimEl37GZICk3S6KEwFhiP28A9rhgFMTKEUMXNXzEdEUomM5KpgR3/uRF0jyruk7VvT0v165mdRTRITpCFeSiC1RDN6iOGoiHD2jV/RmPVkv1rv1MY0WrNnMPvoD6/MHhWVrw=</latexit><latexit sha1_base64="c/Ra8OoU+P8GWDNB5OTzhF6v8xI=">ACAHicbVDLSsNAFJ3UV62vqAsXbgaLUEFKIoIui25cVrAPaGKZTCft0MkzNwINWTjr7hxoYhbP8Odf+P0sdDWAwOHc87lzj1BIrgGx/m2CkvLK6trxfXSxubW9o69u9fUcaoa9BYxKodEM0El6wBHARrJ4qRKBCsFQyvx37rgSnNY3kHo4T5EelLHnJKwEhd+8BL+H0mCOSVR+yZJGB9iocnXbvsVJ0J8CJxZ6SMZqh37S+vF9M0YhKoIFp3XCcBPyMKOBUsL3mpZgmhQ9JnHUMliZj2s8kBOT42Sg+HsTJPAp6ovycyEmk9igKTjAgM9Lw3Fv/zOimEl37GZICk3S6KEwFhiP28A9rhgFMTKEUMXNXzEdEUomM5KpgR3/uRF0jyruk7VvT0v165mdRTRITpCFeSiC1RDN6iOGoiHD2jV/RmPVkv1rv1MY0WrNnMPvoD6/MHhWVrw=</latexit><latexit sha1_base64="c/Ra8OoU+P8GWDNB5OTzhF6v8xI=">ACAHicbVDLSsNAFJ3UV62vqAsXbgaLUEFKIoIui25cVrAPaGKZTCft0MkzNwINWTjr7hxoYhbP8Odf+P0sdDWAwOHc87lzj1BIrgGx/m2CkvLK6trxfXSxubW9o69u9fUcaoa9BYxKodEM0El6wBHARrJ4qRKBCsFQyvx37rgSnNY3kHo4T5EelLHnJKwEhd+8BL+H0mCOSVR+yZJGB9iocnXbvsVJ0J8CJxZ6SMZqh37S+vF9M0YhKoIFp3XCcBPyMKOBUsL3mpZgmhQ9JnHUMliZj2s8kBOT42Sg+HsTJPAp6ovycyEmk9igKTjAgM9Lw3Fv/zOimEl37GZICk3S6KEwFhiP28A9rhgFMTKEUMXNXzEdEUomM5KpgR3/uRF0jyruk7VvT0v165mdRTRITpCFeSiC1RDN6iOGoiHD2jV/RmPVkv1rv1MY0WrNnMPvoD6/MHhWVrw=</latexit>
πl(a|s, k, z)
<latexit sha1_base64="9oyZxgtvpK0SDTEixVXDmE96DWI=">ACAXicbVDLSgMxFM3UV62vUTeCm2ARKpQyI4Iui25cVrAPaMeSTNtaCYzJHeEOtSNv+LGhSJu/Qt3/o1pOwtPRA4nHMuN/f4seAaHOfbyi0tr6yu5dcLG5tb2zv27l5DR4mirE4jEamWTzQTXLI6cBCsFStGQl+wpj+8mvjNe6Y0j+QtjGLmhaQvecApASN17YNOzO9SMS4R3DE5wLqMh2X8cNK1i07FmQIvEjcjRZSh1rW/Or2IJiGTQAXRu06MXgpUcCpYONCJ9EsJnRI+qxtqCQh0146vWCMj43Sw0GkzJOAp+rviZSEWo9C3yRDAgM9703E/7x2AsGFl3IZJ8AknS0KEoEhwpM6cI8rRkGMDCFUcfNXTAdEQqmtIpwZ0/eZE0TiuU3FvzorVy6yOPDpER6iEXHSOquga1VAdUfSIntErerOerBfr3fqYRXNWNrOP/sD6/AFx+5WR</latexit><latexit sha1_base64="9oyZxgtvpK0SDTEixVXDmE96DWI=">ACAXicbVDLSgMxFM3UV62vUTeCm2ARKpQyI4Iui25cVrAPaMeSTNtaCYzJHeEOtSNv+LGhSJu/Qt3/o1pOwtPRA4nHMuN/f4seAaHOfbyi0tr6yu5dcLG5tb2zv27l5DR4mirE4jEamWTzQTXLI6cBCsFStGQl+wpj+8mvjNe6Y0j+QtjGLmhaQvecApASN17YNOzO9SMS4R3DE5wLqMh2X8cNK1i07FmQIvEjcjRZSh1rW/Or2IJiGTQAXRu06MXgpUcCpYONCJ9EsJnRI+qxtqCQh0146vWCMj43Sw0GkzJOAp+rviZSEWo9C3yRDAgM9703E/7x2AsGFl3IZJ8AknS0KEoEhwpM6cI8rRkGMDCFUcfNXTAdEQqmtIpwZ0/eZE0TiuU3FvzorVy6yOPDpER6iEXHSOquga1VAdUfSIntErerOerBfr3fqYRXNWNrOP/sD6/AFx+5WR</latexit><latexit sha1_base64="9oyZxgtvpK0SDTEixVXDmE96DWI=">ACAXicbVDLSgMxFM3UV62vUTeCm2ARKpQyI4Iui25cVrAPaMeSTNtaCYzJHeEOtSNv+LGhSJu/Qt3/o1pOwtPRA4nHMuN/f4seAaHOfbyi0tr6yu5dcLG5tb2zv27l5DR4mirE4jEamWTzQTXLI6cBCsFStGQl+wpj+8mvjNe6Y0j+QtjGLmhaQvecApASN17YNOzO9SMS4R3DE5wLqMh2X8cNK1i07FmQIvEjcjRZSh1rW/Or2IJiGTQAXRu06MXgpUcCpYONCJ9EsJnRI+qxtqCQh0146vWCMj43Sw0GkzJOAp+rviZSEWo9C3yRDAgM9703E/7x2AsGFl3IZJ8AknS0KEoEhwpM6cI8rRkGMDCFUcfNXTAdEQqmtIpwZ0/eZE0TiuU3FvzorVy6yOPDpER6iEXHSOquga1VAdUfSIntErerOerBfr3fqYRXNWNrOP/sD6/AFx+5WR</latexit><latexit sha1_base64="9oyZxgtvpK0SDTEixVXDmE96DWI=">ACAXicbVDLSgMxFM3UV62vUTeCm2ARKpQyI4Iui25cVrAPaMeSTNtaCYzJHeEOtSNv+LGhSJu/Qt3/o1pOwtPRA4nHMuN/f4seAaHOfbyi0tr6yu5dcLG5tb2zv27l5DR4mirE4jEamWTzQTXLI6cBCsFStGQl+wpj+8mvjNe6Y0j+QtjGLmhaQvecApASN17YNOzO9SMS4R3DE5wLqMh2X8cNK1i07FmQIvEjcjRZSh1rW/Or2IJiGTQAXRu06MXgpUcCpYONCJ9EsJnRI+qxtqCQh0146vWCMj43Sw0GkzJOAp+rviZSEWo9C3yRDAgM9703E/7x2AsGFl3IZJ8AknS0KEoEhwpM6cI8rRkGMDCFUcfNXTAdEQqmtIpwZ0/eZE0TiuU3FvzorVy6yOPDpER6iEXHSOquga1VAdUfSIntErerOerBfr3fqYRXNWNrOP/sD6/AFx+5WR</latexit>
pTA(z|s, k)
<latexit sha1_base64="c2fNTx68cHPMust/FGx9JuE3es=">ACBHicbVC7SgNBFJ2Nrxhfq5ZpBoMQcKuCFpGbSwj5AXZNcxOZpMhsw9m7opx2cLGX7GxUMTWj7Dzb5w8Ck08MHA451zu3OPFgiuwrG8jt7S8srqWXy9sbG5t75i7e0VJZKyBo1EJNseUzwkDWAg2DtWDISeIK1vOHV2G/dMal4FNZhFDM3IP2Q+5wS0FLXLMa3qQPsHtL6RZaVH7Cj04DVMR4edc2SVbEmwIvEnpESmqHWNb+cXkSTgIVABVGqY1sxuCmRwKlgWcFJFIsJHZI+62gakoApN50ckeFDrfSwH0n9QsAT9fdESgKlRoGnkwGBgZr3xuJ/XicB/9xNeRgnwEI6XeQnAkOEx43gHpeMghpQqjk+q+YDogkFHRvBV2CPX/yImeVGyrYt+clqXszryqIgOUBnZ6AxV0TWqoQai6BE9o1f0ZjwZL8a78TGN5ozZzD76A+PzB8bql38=</latexit><latexit sha1_base64="c2fNTx68cHPMust/FGx9JuE3es=">ACBHicbVC7SgNBFJ2Nrxhfq5ZpBoMQcKuCFpGbSwj5AXZNcxOZpMhsw9m7opx2cLGX7GxUMTWj7Dzb5w8Ck08MHA451zu3OPFgiuwrG8jt7S8srqWXy9sbG5t75i7e0VJZKyBo1EJNseUzwkDWAg2DtWDISeIK1vOHV2G/dMal4FNZhFDM3IP2Q+5wS0FLXLMa3qQPsHtL6RZaVH7Cj04DVMR4edc2SVbEmwIvEnpESmqHWNb+cXkSTgIVABVGqY1sxuCmRwKlgWcFJFIsJHZI+62gakoApN50ckeFDrfSwH0n9QsAT9fdESgKlRoGnkwGBgZr3xuJ/XicB/9xNeRgnwEI6XeQnAkOEx43gHpeMghpQqjk+q+YDogkFHRvBV2CPX/yImeVGyrYt+clqXszryqIgOUBnZ6AxV0TWqoQai6BE9o1f0ZjwZL8a78TGN5ozZzD76A+PzB8bql38=</latexit><latexit sha1_base64="c2fNTx68cHPMust/FGx9JuE3es=">ACBHicbVC7SgNBFJ2Nrxhfq5ZpBoMQcKuCFpGbSwj5AXZNcxOZpMhsw9m7opx2cLGX7GxUMTWj7Dzb5w8Ck08MHA451zu3OPFgiuwrG8jt7S8srqWXy9sbG5t75i7e0VJZKyBo1EJNseUzwkDWAg2DtWDISeIK1vOHV2G/dMal4FNZhFDM3IP2Q+5wS0FLXLMa3qQPsHtL6RZaVH7Cj04DVMR4edc2SVbEmwIvEnpESmqHWNb+cXkSTgIVABVGqY1sxuCmRwKlgWcFJFIsJHZI+62gakoApN50ckeFDrfSwH0n9QsAT9fdESgKlRoGnkwGBgZr3xuJ/XicB/9xNeRgnwEI6XeQnAkOEx43gHpeMghpQqjk+q+YDogkFHRvBV2CPX/yImeVGyrYt+clqXszryqIgOUBnZ6AxV0TWqoQai6BE9o1f0ZjwZL8a78TGN5ozZzD76A+PzB8bql38=</latexit><latexit sha1_base64="c2fNTx68cHPMust/FGx9JuE3es=">ACBHicbVC7SgNBFJ2Nrxhfq5ZpBoMQcKuCFpGbSwj5AXZNcxOZpMhsw9m7opx2cLGX7GxUMTWj7Dzb5w8Ck08MHA451zu3OPFgiuwrG8jt7S8srqWXy9sbG5t75i7e0VJZKyBo1EJNseUzwkDWAg2DtWDISeIK1vOHV2G/dMal4FNZhFDM3IP2Q+5wS0FLXLMa3qQPsHtL6RZaVH7Cj04DVMR4edc2SVbEmwIvEnpESmqHWNb+cXkSTgIVABVGqY1sxuCmRwKlgWcFJFIsJHZI+62gakoApN50ckeFDrfSwH0n9QsAT9fdESgKlRoGnkwGBgZr3xuJ/XicB/9xNeRgnwEI6XeQnAkOEx43gHpeMghpQqjk+q+YDogkFHRvBV2CPX/yImeVGyrYt+clqXszryqIgOUBnZ6AxV0TWqoQai6BE9o1f0ZjwZL8a78TGN5ozZzD76A+PzB8bql38=</latexit>
Task-Agnostic 
Regularization
Execution 
Regularization
Semantic Policy
Low-level Policy
Execution 
Latent Policy
s
a
π(a|s)
pdemo(k|s)
<latexit sha1_base64="y3NCKlQn/uVLDFB4qRqfCdKRMdM=">ACAXicbVC7SgNBFJ2NrxhfqzaCzWAQYhN2RdAyaGMZwTwgWcPs5CYZMvtg5q4Yltj4KzYWitj6F3b+jZNkC08MHA451zu3OPHUmh0nG8rt7S8srqWXy9sbG5t79i7e3UdJYpDjUcyUk2faZAihBoKlNCMFbDAl9Dwh1cTv3EPSosovMVRDF7A+qHoCc7QSB37IL5rIzxg2oUgGpeGtG3SPVJxy46ZWcKukjcjBRJhmrH/mp3I54ECKXTOuW68TopUyh4BLGhXaiIWZ8yPrQMjRkAWgvnV4wpsdG6dJepMwLkU7V3xMpC7QeBb5JBgwHet6biP95rQR7F14qwjhBCPlsUS+RFCM6qYN2hQKOcmQI40qYv1I+YIpxNKUVTAnu/MmLpH5adp2ye3NWrFxmdeTJITkiJeKSc1Ih16RKaoSTR/JMXsmb9WS9WO/Wxyas7KZfIH1ucPNAWsw=</latexit><latexit sha1_base64="y3NCKlQn/uVLDFB4qRqfCdKRMdM=">ACAXicbVC7SgNBFJ2NrxhfqzaCzWAQYhN2RdAyaGMZwTwgWcPs5CYZMvtg5q4Yltj4KzYWitj6F3b+jZNkC08MHA451zu3OPHUmh0nG8rt7S8srqWXy9sbG5t79i7e3UdJYpDjUcyUk2faZAihBoKlNCMFbDAl9Dwh1cTv3EPSosovMVRDF7A+qHoCc7QSB37IL5rIzxg2oUgGpeGtG3SPVJxy46ZWcKukjcjBRJhmrH/mp3I54ECKXTOuW68TopUyh4BLGhXaiIWZ8yPrQMjRkAWgvnV4wpsdG6dJepMwLkU7V3xMpC7QeBb5JBgwHet6biP95rQR7F14qwjhBCPlsUS+RFCM6qYN2hQKOcmQI40qYv1I+YIpxNKUVTAnu/MmLpH5adp2ye3NWrFxmdeTJITkiJeKSc1Ih16RKaoSTR/JMXsmb9WS9WO/Wxyas7KZfIH1ucPNAWsw=</latexit><latexit sha1_base64="y3NCKlQn/uVLDFB4qRqfCdKRMdM=">ACAXicbVC7SgNBFJ2NrxhfqzaCzWAQYhN2RdAyaGMZwTwgWcPs5CYZMvtg5q4Yltj4KzYWitj6F3b+jZNkC08MHA451zu3OPHUmh0nG8rt7S8srqWXy9sbG5t79i7e3UdJYpDjUcyUk2faZAihBoKlNCMFbDAl9Dwh1cTv3EPSosovMVRDF7A+qHoCc7QSB37IL5rIzxg2oUgGpeGtG3SPVJxy46ZWcKukjcjBRJhmrH/mp3I54ECKXTOuW68TopUyh4BLGhXaiIWZ8yPrQMjRkAWgvnV4wpsdG6dJepMwLkU7V3xMpC7QeBb5JBgwHet6biP95rQR7F14qwjhBCPlsUS+RFCM6qYN2hQKOcmQI40qYv1I+YIpxNKUVTAnu/MmLpH5adp2ye3NWrFxmdeTJITkiJeKSc1Ih16RKaoSTR/JMXsmb9WS9WO/Wxyas7KZfIH1ucPNAWsw=</latexit><latexit sha1_base64="y3NCKlQn/uVLDFB4qRqfCdKRMdM=">ACAXicbVC7SgNBFJ2NrxhfqzaCzWAQYhN2RdAyaGMZwTwgWcPs5CYZMvtg5q4Yltj4KzYWitj6F3b+jZNkC08MHA451zu3OPHUmh0nG8rt7S8srqWXy9sbG5t79i7e3UdJYpDjUcyUk2faZAihBoKlNCMFbDAl9Dwh1cTv3EPSosovMVRDF7A+qHoCc7QSB37IL5rIzxg2oUgGpeGtG3SPVJxy46ZWcKukjcjBRJhmrH/mp3I54ECKXTOuW68TopUyh4BLGhXaiIWZ8yPrQMjRkAWgvnV4wpsdG6dJepMwLkU7V3xMpC7QeBb5JBgwHet6biP95rQR7F14qwjhBCPlsUS+RFCM6qYN2hQKOcmQI40qYv1I+YIpxNKUVTAnu/MmLpH5adp2ye3NWrFxmdeTJITkiJeKSc1Ih16RKaoSTR/JMXsmb9WS9WO/Wxyas7KZfIH1ucPNAWsw=</latexit>
πsem(k|s)
<latexit sha1_base64="cNuFyzr3mzwB0m2tA/vnk2QGE=">AB/XicbVDLSgMxFM34rPU1PnZugkWomzIjgi6LblxWsA9ox5J7ShSWZIMkIdir/ixoUibv0Pd/6NaTsLbT0QOJxzLvfmhAln2njet7O0vLK6tl7YKG5ube/sunv7DR2nikKdxjxWrZBo4ExC3TDoZUoICLk0AyH1xO/+QBKs1jemVECgSB9ySJGibFS1z3sJOw+0yDG5SHu2KTB+rTrlryKNwVeJH5OSihHret+dXoxTQVIQznRu17iQkyogyjHMbFTqohIXRI+tC2VBIBOsim14/xiV6OIqVfdLgqfp7IiNC65EIbVIQM9Dz3kT8z2unJroMiaT1ICks0VRyrGJ8aQK3GMKqOEjSwhVzN6K6YAoQo0trGhL8Oe/vEgaZxXfq/i356XqV5HAR2hY1RGPrpAVXSDaqiOKHpEz+gVvTlPzovz7nzMoktOPnOA/sD5/AHZQJTP</latexit><latexit sha1_base64="cNuFyzr3mzwB0m2tA/vnk2QGE=">AB/XicbVDLSgMxFM34rPU1PnZugkWomzIjgi6LblxWsA9ox5J7ShSWZIMkIdir/ixoUibv0Pd/6NaTsLbT0QOJxzLvfmhAln2njet7O0vLK6tl7YKG5ube/sunv7DR2nikKdxjxWrZBo4ExC3TDoZUoICLk0AyH1xO/+QBKs1jemVECgSB9ySJGibFS1z3sJOw+0yDG5SHu2KTB+rTrlryKNwVeJH5OSihHret+dXoxTQVIQznRu17iQkyogyjHMbFTqohIXRI+tC2VBIBOsim14/xiV6OIqVfdLgqfp7IiNC65EIbVIQM9Dz3kT8z2unJroMiaT1ICks0VRyrGJ8aQK3GMKqOEjSwhVzN6K6YAoQo0trGhL8Oe/vEgaZxXfq/i356XqV5HAR2hY1RGPrpAVXSDaqiOKHpEz+gVvTlPzovz7nzMoktOPnOA/sD5/AHZQJTP</latexit><latexit sha1_base64="cNuFyzr3mzwB0m2tA/vnk2QGE=">AB/XicbVDLSgMxFM34rPU1PnZugkWomzIjgi6LblxWsA9ox5J7ShSWZIMkIdir/ixoUibv0Pd/6NaTsLbT0QOJxzLvfmhAln2njet7O0vLK6tl7YKG5ube/sunv7DR2nikKdxjxWrZBo4ExC3TDoZUoICLk0AyH1xO/+QBKs1jemVECgSB9ySJGibFS1z3sJOw+0yDG5SHu2KTB+rTrlryKNwVeJH5OSihHret+dXoxTQVIQznRu17iQkyogyjHMbFTqohIXRI+tC2VBIBOsim14/xiV6OIqVfdLgqfp7IiNC65EIbVIQM9Dz3kT8z2unJroMiaT1ICks0VRyrGJ8aQK3GMKqOEjSwhVzN6K6YAoQo0trGhL8Oe/vEgaZxXfq/i356XqV5HAR2hY1RGPrpAVXSDaqiOKHpEz+gVvTlPzovz7nzMoktOPnOA/sD5/AHZQJTP</latexit><latexit sha1_base64="cNuFyzr3mzwB0m2tA/vnk2QGE=">AB/XicbVDLSgMxFM34rPU1PnZugkWomzIjgi6LblxWsA9ox5J7ShSWZIMkIdir/ixoUibv0Pd/6NaTsLbT0QOJxzLvfmhAln2njet7O0vLK6tl7YKG5ube/sunv7DR2nikKdxjxWrZBo4ExC3TDoZUoICLk0AyH1xO/+QBKs1jemVECgSB9ySJGibFS1z3sJOw+0yDG5SHu2KTB+rTrlryKNwVeJH5OSihHret+dXoxTQVIQznRu17iQkyogyjHMbFTqohIXRI+tC2VBIBOsim14/xiV6OIqVfdLgqfp7IiNC65EIbVIQM9Dz3kT8z2unJroMiaT1ICks0VRyrGJ8aQK3GMKqOEjSwhVzN6K6YAoQo0trGhL8Oe/vEgaZxXfq/i356XqV5HAR2hY1RGPrpAVXSDaqiOKHpEz+gVvTlPzovz7nzMoktOPnOA/sD5/AHZQJTP</latexit>
πlat(z|s, k)
<latexit sha1_base64="c/Ra8OoU+P8GWDNB5OTzhF6v8xI=">ACAHicbVDLSsNAFJ3UV62vqAsXbgaLUEFKIoIui25cVrAPaGKZTCft0MkzNwINWTjr7hxoYhbP8Odf+P0sdDWAwOHc87lzj1BIrgGx/m2CkvLK6trxfXSxubW9o69u9fUcaoa9BYxKodEM0El6wBHARrJ4qRKBCsFQyvx37rgSnNY3kHo4T5EelLHnJKwEhd+8BL+H0mCOSVR+yZJGB9iocnXbvsVJ0J8CJxZ6SMZqh37S+vF9M0YhKoIFp3XCcBPyMKOBUsL3mpZgmhQ9JnHUMliZj2s8kBOT42Sg+HsTJPAp6ovycyEmk9igKTjAgM9Lw3Fv/zOimEl37GZICk3S6KEwFhiP28A9rhgFMTKEUMXNXzEdEUomM5KpgR3/uRF0jyruk7VvT0v165mdRTRITpCFeSiC1RDN6iOGoiHD2jV/RmPVkv1rv1MY0WrNnMPvoD6/MHhWVrw=</latexit><latexit sha1_base64="c/Ra8OoU+P8GWDNB5OTzhF6v8xI=">ACAHicbVDLSsNAFJ3UV62vqAsXbgaLUEFKIoIui25cVrAPaGKZTCft0MkzNwINWTjr7hxoYhbP8Odf+P0sdDWAwOHc87lzj1BIrgGx/m2CkvLK6trxfXSxubW9o69u9fUcaoa9BYxKodEM0El6wBHARrJ4qRKBCsFQyvx37rgSnNY3kHo4T5EelLHnJKwEhd+8BL+H0mCOSVR+yZJGB9iocnXbvsVJ0J8CJxZ6SMZqh37S+vF9M0YhKoIFp3XCcBPyMKOBUsL3mpZgmhQ9JnHUMliZj2s8kBOT42Sg+HsTJPAp6ovycyEmk9igKTjAgM9Lw3Fv/zOimEl37GZICk3S6KEwFhiP28A9rhgFMTKEUMXNXzEdEUomM5KpgR3/uRF0jyruk7VvT0v165mdRTRITpCFeSiC1RDN6iOGoiHD2jV/RmPVkv1rv1MY0WrNnMPvoD6/MHhWVrw=</latexit><latexit sha1_base64="c/Ra8OoU+P8GWDNB5OTzhF6v8xI=">ACAHicbVDLSsNAFJ3UV62vqAsXbgaLUEFKIoIui25cVrAPaGKZTCft0MkzNwINWTjr7hxoYhbP8Odf+P0sdDWAwOHc87lzj1BIrgGx/m2CkvLK6trxfXSxubW9o69u9fUcaoa9BYxKodEM0El6wBHARrJ4qRKBCsFQyvx37rgSnNY3kHo4T5EelLHnJKwEhd+8BL+H0mCOSVR+yZJGB9iocnXbvsVJ0J8CJxZ6SMZqh37S+vF9M0YhKoIFp3XCcBPyMKOBUsL3mpZgmhQ9JnHUMliZj2s8kBOT42Sg+HsTJPAp6ovycyEmk9igKTjAgM9Lw3Fv/zOimEl37GZICk3S6KEwFhiP28A9rhgFMTKEUMXNXzEdEUomM5KpgR3/uRF0jyruk7VvT0v165mdRTRITpCFeSiC1RDN6iOGoiHD2jV/RmPVkv1rv1MY0WrNnMPvoD6/MHhWVrw=</latexit><latexit sha1_base64="c/Ra8OoU+P8GWDNB5OTzhF6v8xI=">ACAHicbVDLSsNAFJ3UV62vqAsXbgaLUEFKIoIui25cVrAPaGKZTCft0MkzNwINWTjr7hxoYhbP8Odf+P0sdDWAwOHc87lzj1BIrgGx/m2CkvLK6trxfXSxubW9o69u9fUcaoa9BYxKodEM0El6wBHARrJ4qRKBCsFQyvx37rgSnNY3kHo4T5EelLHnJKwEhd+8BL+H0mCOSVR+yZJGB9iocnXbvsVJ0J8CJxZ6SMZqh37S+vF9M0YhKoIFp3XCcBPyMKOBUsL3mpZgmhQ9JnHUMliZj2s8kBOT42Sg+HsTJPAp6ovycyEmk9igKTjAgM9Lw3Fv/zOimEl37GZICk3S6KEwFhiP28A9rhgFMTKEUMXNXzEdEUomM5KpgR3/uRF0jyruk7VvT0v165mdRTRITpCFeSiC1RDN6iOGoiHD2jV/RmPVkv1rv1MY0WrNnMPvoD6/MHhWVrw=</latexit>
πl(a|s, k, z)
<latexit sha1_base64="9oyZxgtvpK0SDTEixVXDmE96DWI=">ACAXicbVDLSgMxFM3UV62vUTeCm2ARKpQyI4Iui25cVrAPaMeSTNtaCYzJHeEOtSNv+LGhSJu/Qt3/o1pOwtPRA4nHMuN/f4seAaHOfbyi0tr6yu5dcLG5tb2zv27l5DR4mirE4jEamWTzQTXLI6cBCsFStGQl+wpj+8mvjNe6Y0j+QtjGLmhaQvecApASN17YNOzO9SMS4R3DE5wLqMh2X8cNK1i07FmQIvEjcjRZSh1rW/Or2IJiGTQAXRu06MXgpUcCpYONCJ9EsJnRI+qxtqCQh0146vWCMj43Sw0GkzJOAp+rviZSEWo9C3yRDAgM9703E/7x2AsGFl3IZJ8AknS0KEoEhwpM6cI8rRkGMDCFUcfNXTAdEQqmtIpwZ0/eZE0TiuU3FvzorVy6yOPDpER6iEXHSOquga1VAdUfSIntErerOerBfr3fqYRXNWNrOP/sD6/AFx+5WR</latexit><latexit sha1_base64="9oyZxgtvpK0SDTEixVXDmE96DWI=">ACAXicbVDLSgMxFM3UV62vUTeCm2ARKpQyI4Iui25cVrAPaMeSTNtaCYzJHeEOtSNv+LGhSJu/Qt3/o1pOwtPRA4nHMuN/f4seAaHOfbyi0tr6yu5dcLG5tb2zv27l5DR4mirE4jEamWTzQTXLI6cBCsFStGQl+wpj+8mvjNe6Y0j+QtjGLmhaQvecApASN17YNOzO9SMS4R3DE5wLqMh2X8cNK1i07FmQIvEjcjRZSh1rW/Or2IJiGTQAXRu06MXgpUcCpYONCJ9EsJnRI+qxtqCQh0146vWCMj43Sw0GkzJOAp+rviZSEWo9C3yRDAgM9703E/7x2AsGFl3IZJ8AknS0KEoEhwpM6cI8rRkGMDCFUcfNXTAdEQqmtIpwZ0/eZE0TiuU3FvzorVy6yOPDpER6iEXHSOquga1VAdUfSIntErerOerBfr3fqYRXNWNrOP/sD6/AFx+5WR</latexit><latexit sha1_base64="9oyZxgtvpK0SDTEixVXDmE96DWI=">ACAXicbVDLSgMxFM3UV62vUTeCm2ARKpQyI4Iui25cVrAPaMeSTNtaCYzJHeEOtSNv+LGhSJu/Qt3/o1pOwtPRA4nHMuN/f4seAaHOfbyi0tr6yu5dcLG5tb2zv27l5DR4mirE4jEamWTzQTXLI6cBCsFStGQl+wpj+8mvjNe6Y0j+QtjGLmhaQvecApASN17YNOzO9SMS4R3DE5wLqMh2X8cNK1i07FmQIvEjcjRZSh1rW/Or2IJiGTQAXRu06MXgpUcCpYONCJ9EsJnRI+qxtqCQh0146vWCMj43Sw0GkzJOAp+rviZSEWo9C3yRDAgM9703E/7x2AsGFl3IZJ8AknS0KEoEhwpM6cI8rRkGMDCFUcfNXTAdEQqmtIpwZ0/eZE0TiuU3FvzorVy6yOPDpER6iEXHSOquga1VAdUfSIntErerOerBfr3fqYRXNWNrOP/sD6/AFx+5WR</latexit><latexit sha1_base64="9oyZxgtvpK0SDTEixVXDmE96DWI=">ACAXicbVDLSgMxFM3UV62vUTeCm2ARKpQyI4Iui25cVrAPaMeSTNtaCYzJHeEOtSNv+LGhSJu/Qt3/o1pOwtPRA4nHMuN/f4seAaHOfbyi0tr6yu5dcLG5tb2zv27l5DR4mirE4jEamWTzQTXLI6cBCsFStGQl+wpj+8mvjNe6Y0j+QtjGLmhaQvecApASN17YNOzO9SMS4R3DE5wLqMh2X8cNK1i07FmQIvEjcjRZSh1rW/Or2IJiGTQAXRu06MXgpUcCpYONCJ9EsJnRI+qxtqCQh0146vWCMj43Sw0GkzJOAp+rviZSEWo9C3yRDAgM9703E/7x2AsGFl3IZJ8AknS0KEoEhwpM6cI8rRkGMDCFUcfNXTAdEQqmtIpwZ0/eZE0TiuU3FvzorVy6yOPDpER6iEXHSOquga1VAdUfSIntErerOerBfr3fqYRXNWNrOP/sD6/AFx+5WR</latexit>
pTA(z|s, k)
<latexit sha1_base64="c2fNTx68cHPMust/FGx9JuE3es=">ACBHicbVC7SgNBFJ2Nrxhfq5ZpBoMQcKuCFpGbSwj5AXZNcxOZpMhsw9m7opx2cLGX7GxUMTWj7Dzb5w8Ck08MHA451zu3OPFgiuwrG8jt7S8srqWXy9sbG5t75i7e0VJZKyBo1EJNseUzwkDWAg2DtWDISeIK1vOHV2G/dMal4FNZhFDM3IP2Q+5wS0FLXLMa3qQPsHtL6RZaVH7Cj04DVMR4edc2SVbEmwIvEnpESmqHWNb+cXkSTgIVABVGqY1sxuCmRwKlgWcFJFIsJHZI+62gakoApN50ckeFDrfSwH0n9QsAT9fdESgKlRoGnkwGBgZr3xuJ/XicB/9xNeRgnwEI6XeQnAkOEx43gHpeMghpQqjk+q+YDogkFHRvBV2CPX/yImeVGyrYt+clqXszryqIgOUBnZ6AxV0TWqoQai6BE9o1f0ZjwZL8a78TGN5ozZzD76A+PzB8bql38=</latexit><latexit sha1_base64="c2fNTx68cHPMust/FGx9JuE3es=">ACBHicbVC7SgNBFJ2Nrxhfq5ZpBoMQcKuCFpGbSwj5AXZNcxOZpMhsw9m7opx2cLGX7GxUMTWj7Dzb5w8Ck08MHA451zu3OPFgiuwrG8jt7S8srqWXy9sbG5t75i7e0VJZKyBo1EJNseUzwkDWAg2DtWDISeIK1vOHV2G/dMal4FNZhFDM3IP2Q+5wS0FLXLMa3qQPsHtL6RZaVH7Cj04DVMR4edc2SVbEmwIvEnpESmqHWNb+cXkSTgIVABVGqY1sxuCmRwKlgWcFJFIsJHZI+62gakoApN50ckeFDrfSwH0n9QsAT9fdESgKlRoGnkwGBgZr3xuJ/XicB/9xNeRgnwEI6XeQnAkOEx43gHpeMghpQqjk+q+YDogkFHRvBV2CPX/yImeVGyrYt+clqXszryqIgOUBnZ6AxV0TWqoQai6BE9o1f0ZjwZL8a78TGN5ozZzD76A+PzB8bql38=</latexit><latexit sha1_base64="c2fNTx68cHPMust/FGx9JuE3es=">ACBHicbVC7SgNBFJ2Nrxhfq5ZpBoMQcKuCFpGbSwj5AXZNcxOZpMhsw9m7opx2cLGX7GxUMTWj7Dzb5w8Ck08MHA451zu3OPFgiuwrG8jt7S8srqWXy9sbG5t75i7e0VJZKyBo1EJNseUzwkDWAg2DtWDISeIK1vOHV2G/dMal4FNZhFDM3IP2Q+5wS0FLXLMa3qQPsHtL6RZaVH7Cj04DVMR4edc2SVbEmwIvEnpESmqHWNb+cXkSTgIVABVGqY1sxuCmRwKlgWcFJFIsJHZI+62gakoApN50ckeFDrfSwH0n9QsAT9fdESgKlRoGnkwGBgZr3xuJ/XicB/9xNeRgnwEI6XeQnAkOEx43gHpeMghpQqjk+q+YDogkFHRvBV2CPX/yImeVGyrYt+clqXszryqIgOUBnZ6AxV0TWqoQai6BE9o1f0ZjwZL8a78TGN5ozZzD76A+PzB8bql38=</latexit><latexit sha1_base64="c2fNTx68cHPMust/FGx9JuE3es=">ACBHicbVC7SgNBFJ2Nrxhfq5ZpBoMQcKuCFpGbSwj5AXZNcxOZpMhsw9m7opx2cLGX7GxUMTWj7Dzb5w8Ck08MHA451zu3OPFgiuwrG8jt7S8srqWXy9sbG5t75i7e0VJZKyBo1EJNseUzwkDWAg2DtWDISeIK1vOHV2G/dMal4FNZhFDM3IP2Q+5wS0FLXLMa3qQPsHtL6RZaVH7Cj04DVMR4edc2SVbEmwIvEnpESmqHWNb+cXkSTgIVABVGqY1sxuCmRwKlgWcFJFIsJHZI+62gakoApN50ckeFDrfSwH0n9QsAT9fdESgKlRoGnkwGBgZr3xuJ/XicB/9xNeRgnwEI6XeQnAkOEx43gHpeMghpQqjk+q+YDogkFHRvBV2CPX/yImeVGyrYt+clqXszryqIgOUBnZ6AxV0TWqoQai6BE9o1f0ZjwZL8a78TGN5ozZzD76A+PzB8bql38=</latexit>
Demonstration 
Regularization
Execution 
Regularization
Semantic Policy
Low-level Policy
Execution 
Latent Policy
s
a
π(a|s)
pdemo(k|s)
<latexit sha1_base64="y3NCKlQn/uVLDFB4qRqfCdKRMdM=">ACAXicbVC7SgNBFJ2NrxhfqzaCzWAQYhN2RdAyaGMZwTwgWcPs5CYZMvtg5q4Yltj4KzYWitj6F3b+jZNkC08MHA451zu3OPHUmh0nG8rt7S8srqWXy9sbG5t79i7e3UdJYpDjUcyUk2faZAihBoKlNCMFbDAl9Dwh1cTv3EPSosovMVRDF7A+qHoCc7QSB37IL5rIzxg2oUgGpeGtG3SPVJxy46ZWcKukjcjBRJhmrH/mp3I54ECKXTOuW68TopUyh4BLGhXaiIWZ8yPrQMjRkAWgvnV4wpsdG6dJepMwLkU7V3xMpC7QeBb5JBgwHet6biP95rQR7F14qwjhBCPlsUS+RFCM6qYN2hQKOcmQI40qYv1I+YIpxNKUVTAnu/MmLpH5adp2ye3NWrFxmdeTJITkiJeKSc1Ih16RKaoSTR/JMXsmb9WS9WO/Wxyas7KZfIH1ucPNAWsw=</latexit><latexit sha1_base64="y3NCKlQn/uVLDFB4qRqfCdKRMdM=">ACAXicbVC7SgNBFJ2NrxhfqzaCzWAQYhN2RdAyaGMZwTwgWcPs5CYZMvtg5q4Yltj4KzYWitj6F3b+jZNkC08MHA451zu3OPHUmh0nG8rt7S8srqWXy9sbG5t79i7e3UdJYpDjUcyUk2faZAihBoKlNCMFbDAl9Dwh1cTv3EPSosovMVRDF7A+qHoCc7QSB37IL5rIzxg2oUgGpeGtG3SPVJxy46ZWcKukjcjBRJhmrH/mp3I54ECKXTOuW68TopUyh4BLGhXaiIWZ8yPrQMjRkAWgvnV4wpsdG6dJepMwLkU7V3xMpC7QeBb5JBgwHet6biP95rQR7F14qwjhBCPlsUS+RFCM6qYN2hQKOcmQI40qYv1I+YIpxNKUVTAnu/MmLpH5adp2ye3NWrFxmdeTJITkiJeKSc1Ih16RKaoSTR/JMXsmb9WS9WO/Wxyas7KZfIH1ucPNAWsw=</latexit><latexit sha1_base64="y3NCKlQn/uVLDFB4qRqfCdKRMdM=">ACAXicbVC7SgNBFJ2NrxhfqzaCzWAQYhN2RdAyaGMZwTwgWcPs5CYZMvtg5q4Yltj4KzYWitj6F3b+jZNkC08MHA451zu3OPHUmh0nG8rt7S8srqWXy9sbG5t79i7e3UdJYpDjUcyUk2faZAihBoKlNCMFbDAl9Dwh1cTv3EPSosovMVRDF7A+qHoCc7QSB37IL5rIzxg2oUgGpeGtG3SPVJxy46ZWcKukjcjBRJhmrH/mp3I54ECKXTOuW68TopUyh4BLGhXaiIWZ8yPrQMjRkAWgvnV4wpsdG6dJepMwLkU7V3xMpC7QeBb5JBgwHet6biP95rQR7F14qwjhBCPlsUS+RFCM6qYN2hQKOcmQI40qYv1I+YIpxNKUVTAnu/MmLpH5adp2ye3NWrFxmdeTJITkiJeKSc1Ih16RKaoSTR/JMXsmb9WS9WO/Wxyas7KZfIH1ucPNAWsw=</latexit><latexit sha1_base64="y3NCKlQn/uVLDFB4qRqfCdKRMdM=">ACAXicbVC7SgNBFJ2NrxhfqzaCzWAQYhN2RdAyaGMZwTwgWcPs5CYZMvtg5q4Yltj4KzYWitj6F3b+jZNkC08MHA451zu3OPHUmh0nG8rt7S8srqWXy9sbG5t79i7e3UdJYpDjUcyUk2faZAihBoKlNCMFbDAl9Dwh1cTv3EPSosovMVRDF7A+qHoCc7QSB37IL5rIzxg2oUgGpeGtG3SPVJxy46ZWcKukjcjBRJhmrH/mp3I54ECKXTOuW68TopUyh4BLGhXaiIWZ8yPrQMjRkAWgvnV4wpsdG6dJepMwLkU7V3xMpC7QeBb5JBgwHet6biP95rQR7F14qwjhBCPlsUS+RFCM6qYN2hQKOcmQI40qYv1I+YIpxNKUVTAnu/MmLpH5adp2ye3NWrFxmdeTJITkiJeKSc1Ih16RKaoSTR/JMXsmb9WS9WO/Wxyas7KZfIH1ucPNAWsw=</latexit>
pTA(k|s)
<latexit sha1_base64="vpQAmA6WDz/Qp9Ril18cg5Y6Mfw=">AB/3icbVDLSgMxFM3UV62vUcGNm2AR6qbMiKDLqhuXFfqCdiyZNOGZjJDckcsYxf+ihsXirj1N9z5N6btLT1QOBwzrncm+PHgmtwnG8rt7S8srqWXy9sbG5t79i7ew0dJYqyOo1EpFo+0UxwyerAQbBWrBgJfcGa/vB64jfvmdI8kjUYxcwLSV/ygFMCRuraB/FdB9gDpLXLcWmIOyYLWJ907aJTdqbAi8TNSBFlqHbtr04voknIJFBtG67TgxeShRwKti40Ek0iwkdkj5rGypJyLSXTu8f42Oj9HAQKfMk4Kn6eyIlodaj0DfJkMBAz3sT8T+vnUBw4aVcxgkwSWeLgkRgiPCkDNzjilEQI0MIVdzciumAKELBVFYwJbjzX14kjdOy65Td27Ni5SqrI48O0REqIRedowq6QVURxQ9omf0it6sJ+vFerc+ZtGclc3soz+wPn8AN6iVjw=</latexit><latexit sha1_base64="vpQAmA6WDz/Qp9Ril18cg5Y6Mfw=">AB/3icbVDLSgMxFM3UV62vUcGNm2AR6qbMiKDLqhuXFfqCdiyZNOGZjJDckcsYxf+ihsXirj1N9z5N6btLT1QOBwzrncm+PHgmtwnG8rt7S8srqWXy9sbG5t79i7ew0dJYqyOo1EpFo+0UxwyerAQbBWrBgJfcGa/vB64jfvmdI8kjUYxcwLSV/ygFMCRuraB/FdB9gDpLXLcWmIOyYLWJ907aJTdqbAi8TNSBFlqHbtr04voknIJFBtG67TgxeShRwKti40Ek0iwkdkj5rGypJyLSXTu8f42Oj9HAQKfMk4Kn6eyIlodaj0DfJkMBAz3sT8T+vnUBw4aVcxgkwSWeLgkRgiPCkDNzjilEQI0MIVdzciumAKELBVFYwJbjzX14kjdOy65Td27Ni5SqrI48O0REqIRedowq6QVURxQ9omf0it6sJ+vFerc+ZtGclc3soz+wPn8AN6iVjw=</latexit><latexit sha1_base64="vpQAmA6WDz/Qp9Ril18cg5Y6Mfw=">AB/3icbVDLSgMxFM3UV62vUcGNm2AR6qbMiKDLqhuXFfqCdiyZNOGZjJDckcsYxf+ihsXirj1N9z5N6btLT1QOBwzrncm+PHgmtwnG8rt7S8srqWXy9sbG5t79i7ew0dJYqyOo1EpFo+0UxwyerAQbBWrBgJfcGa/vB64jfvmdI8kjUYxcwLSV/ygFMCRuraB/FdB9gDpLXLcWmIOyYLWJ907aJTdqbAi8TNSBFlqHbtr04voknIJFBtG67TgxeShRwKti40Ek0iwkdkj5rGypJyLSXTu8f42Oj9HAQKfMk4Kn6eyIlodaj0DfJkMBAz3sT8T+vnUBw4aVcxgkwSWeLgkRgiPCkDNzjilEQI0MIVdzciumAKELBVFYwJbjzX14kjdOy65Td27Ni5SqrI48O0REqIRedowq6QVURxQ9omf0it6sJ+vFerc+ZtGclc3soz+wPn8AN6iVjw=</latexit><latexit sha1_base64="vpQAmA6WDz/Qp9Ril18cg5Y6Mfw=">AB/3icbVDLSgMxFM3UV62vUcGNm2AR6qbMiKDLqhuXFfqCdiyZNOGZjJDckcsYxf+ihsXirj1N9z5N6btLT1QOBwzrncm+PHgmtwnG8rt7S8srqWXy9sbG5t79i7ew0dJYqyOo1EpFo+0UxwyerAQbBWrBgJfcGa/vB64jfvmdI8kjUYxcwLSV/ygFMCRuraB/FdB9gDpLXLcWmIOyYLWJ907aJTdqbAi8TNSBFlqHbtr04voknIJFBtG67TgxeShRwKti40Ek0iwkdkj5rGypJyLSXTu8f42Oj9HAQKfMk4Kn6eyIlodaj0DfJkMBAz3sT8T+vnUBw4aVcxgkwSWeLgkRgiPCkDNzjilEQI0MIVdzciumAKELBVFYwJbjzX14kjdOy65Td27Ni5SqrI48O0REqIRedowq6QVURxQ9omf0it6sJ+vFerc+ZtGclc3soz+wPn8AN6iVjw=</latexit>
πsem(k|s)
<latexit sha1_base64="cNuFyzr3mzwB0m2tA/vnk2QGE=">AB/XicbVDLSgMxFM34rPU1PnZugkWomzIjgi6LblxWsA9ox5J7ShSWZIMkIdir/ixoUibv0Pd/6NaTsLbT0QOJxzLvfmhAln2njet7O0vLK6tl7YKG5ube/sunv7DR2nikKdxjxWrZBo4ExC3TDoZUoICLk0AyH1xO/+QBKs1jemVECgSB9ySJGibFS1z3sJOw+0yDG5SHu2KTB+rTrlryKNwVeJH5OSihHret+dXoxTQVIQznRu17iQkyogyjHMbFTqohIXRI+tC2VBIBOsim14/xiV6OIqVfdLgqfp7IiNC65EIbVIQM9Dz3kT8z2unJroMiaT1ICks0VRyrGJ8aQK3GMKqOEjSwhVzN6K6YAoQo0trGhL8Oe/vEgaZxXfq/i356XqV5HAR2hY1RGPrpAVXSDaqiOKHpEz+gVvTlPzovz7nzMoktOPnOA/sD5/AHZQJTP</latexit><latexit sha1_base64="cNuFyzr3mzwB0m2tA/vnk2QGE=">AB/XicbVDLSgMxFM34rPU1PnZugkWomzIjgi6LblxWsA9ox5J7ShSWZIMkIdir/ixoUibv0Pd/6NaTsLbT0QOJxzLvfmhAln2njet7O0vLK6tl7YKG5ube/sunv7DR2nikKdxjxWrZBo4ExC3TDoZUoICLk0AyH1xO/+QBKs1jemVECgSB9ySJGibFS1z3sJOw+0yDG5SHu2KTB+rTrlryKNwVeJH5OSihHret+dXoxTQVIQznRu17iQkyogyjHMbFTqohIXRI+tC2VBIBOsim14/xiV6OIqVfdLgqfp7IiNC65EIbVIQM9Dz3kT8z2unJroMiaT1ICks0VRyrGJ8aQK3GMKqOEjSwhVzN6K6YAoQo0trGhL8Oe/vEgaZxXfq/i356XqV5HAR2hY1RGPrpAVXSDaqiOKHpEz+gVvTlPzovz7nzMoktOPnOA/sD5/AHZQJTP</latexit><latexit sha1_base64="cNuFyzr3mzwB0m2tA/vnk2QGE=">AB/XicbVDLSgMxFM34rPU1PnZugkWomzIjgi6LblxWsA9ox5J7ShSWZIMkIdir/ixoUibv0Pd/6NaTsLbT0QOJxzLvfmhAln2njet7O0vLK6tl7YKG5ube/sunv7DR2nikKdxjxWrZBo4ExC3TDoZUoICLk0AyH1xO/+QBKs1jemVECgSB9ySJGibFS1z3sJOw+0yDG5SHu2KTB+rTrlryKNwVeJH5OSihHret+dXoxTQVIQznRu17iQkyogyjHMbFTqohIXRI+tC2VBIBOsim14/xiV6OIqVfdLgqfp7IiNC65EIbVIQM9Dz3kT8z2unJroMiaT1ICks0VRyrGJ8aQK3GMKqOEjSwhVzN6K6YAoQo0trGhL8Oe/vEgaZxXfq/i356XqV5HAR2hY1RGPrpAVXSDaqiOKHpEz+gVvTlPzovz7nzMoktOPnOA/sD5/AHZQJTP</latexit><latexit sha1_base64="cNuFyzr3mzwB0m2tA/vnk2QGE=">AB/XicbVDLSgMxFM34rPU1PnZugkWomzIjgi6LblxWsA9ox5J7ShSWZIMkIdir/ixoUibv0Pd/6NaTsLbT0QOJxzLvfmhAln2njet7O0vLK6tl7YKG5ube/sunv7DR2nikKdxjxWrZBo4ExC3TDoZUoICLk0AyH1xO/+QBKs1jemVECgSB9ySJGibFS1z3sJOw+0yDG5SHu2KTB+rTrlryKNwVeJH5OSihHret+dXoxTQVIQznRu17iQkyogyjHMbFTqohIXRI+tC2VBIBOsim14/xiV6OIqVfdLgqfp7IiNC65EIbVIQM9Dz3kT8z2unJroMiaT1ICks0VRyrGJ8aQK3GMKqOEjSwhVzN6K6YAoQo0trGhL8Oe/vEgaZxXfq/i356XqV5HAR2hY1RGPrpAVXSDaqiOKHpEz+gVvTlPzovz7nzMoktOPnOA/sD5/AHZQJTP</latexit>
Support 
Discriminator
Demonstration 
Regularization
Task-Agnostic 
Regularization
Semantic Policy
Low-level Policy
πl(a|s, k)
<latexit sha1_base64="RJ5xYKyj8Ju9kMwk9Syq3sY5Xho=">AB/nicbVDJSgNBFHzjGuM2Kp68NAYhgoQZEfQY9OIxglkgM4aeTk/SpGeh+40QhoC/4sWDIl79Dm/+jZ3loIkFDUVPd7rClIpNDrOt7W0vLK6tl7YKG5ube/s2nv7DZ1kivE6S2SiWgHVXIqY1Gg5K1UcRoFkjeDwc3Ybz5ypUS3+Mw5X5Ee7EIBaNopI596KXiIZejMiWeySHRZ2Rw2rFLTsWZgCwSd0ZKMEOtY3953YRlEY+RSap123VS9HOqUDJR0Uv0zylbEB7vG1oTCOu/Xxy/oicGKVLwkSZFyOZqL8nchpPYwCk4wo9vW8Nxb/89oZhld+LuI0Qx6z6aIwkwQTMu6CdIXiDOXQEMqUMLcS1qeKMjSNFU0J7vyXF0njvOI6FfuolS9ntVRgCM4hjK4cAlVuIUa1IFBDs/wCm/Wk/VivVsf0+iSNZs5gD+wPn8Av+uUrQ=</latexit><latexit sha1_base64="RJ5xYKyj8Ju9kMwk9Syq3sY5Xho=">AB/nicbVDJSgNBFHzjGuM2Kp68NAYhgoQZEfQY9OIxglkgM4aeTk/SpGeh+40QhoC/4sWDIl79Dm/+jZ3loIkFDUVPd7rClIpNDrOt7W0vLK6tl7YKG5ube/s2nv7DZ1kivE6S2SiWgHVXIqY1Gg5K1UcRoFkjeDwc3Ybz5ypUS3+Mw5X5Ee7EIBaNopI596KXiIZejMiWeySHRZ2Rw2rFLTsWZgCwSd0ZKMEOtY3953YRlEY+RSap123VS9HOqUDJR0Uv0zylbEB7vG1oTCOu/Xxy/oicGKVLwkSZFyOZqL8nchpPYwCk4wo9vW8Nxb/89oZhld+LuI0Qx6z6aIwkwQTMu6CdIXiDOXQEMqUMLcS1qeKMjSNFU0J7vyXF0njvOI6FfuolS9ntVRgCM4hjK4cAlVuIUa1IFBDs/wCm/Wk/VivVsf0+iSNZs5gD+wPn8Av+uUrQ=</latexit><latexit sha1_base64="RJ5xYKyj8Ju9kMwk9Syq3sY5Xho=">AB/nicbVDJSgNBFHzjGuM2Kp68NAYhgoQZEfQY9OIxglkgM4aeTk/SpGeh+40QhoC/4sWDIl79Dm/+jZ3loIkFDUVPd7rClIpNDrOt7W0vLK6tl7YKG5ube/s2nv7DZ1kivE6S2SiWgHVXIqY1Gg5K1UcRoFkjeDwc3Ybz5ypUS3+Mw5X5Ee7EIBaNopI596KXiIZejMiWeySHRZ2Rw2rFLTsWZgCwSd0ZKMEOtY3953YRlEY+RSap123VS9HOqUDJR0Uv0zylbEB7vG1oTCOu/Xxy/oicGKVLwkSZFyOZqL8nchpPYwCk4wo9vW8Nxb/89oZhld+LuI0Qx6z6aIwkwQTMu6CdIXiDOXQEMqUMLcS1qeKMjSNFU0J7vyXF0njvOI6FfuolS9ntVRgCM4hjK4cAlVuIUa1IFBDs/wCm/Wk/VivVsf0+iSNZs5gD+wPn8Av+uUrQ=</latexit><latexit sha1_base64="RJ5xYKyj8Ju9kMwk9Syq3sY5Xho=">AB/nicbVDJSgNBFHzjGuM2Kp68NAYhgoQZEfQY9OIxglkgM4aeTk/SpGeh+40QhoC/4sWDIl79Dm/+jZ3loIkFDUVPd7rClIpNDrOt7W0vLK6tl7YKG5ube/s2nv7DZ1kivE6S2SiWgHVXIqY1Gg5K1UcRoFkjeDwc3Ybz5ypUS3+Mw5X5Ee7EIBaNopI596KXiIZejMiWeySHRZ2Rw2rFLTsWZgCwSd0ZKMEOtY3953YRlEY+RSap123VS9HOqUDJR0Uv0zylbEB7vG1oTCOu/Xxy/oicGKVLwkSZFyOZqL8nchpPYwCk4wo9vW8Nxb/89oZhld+LuI0Qx6z6aIwkwQTMu6CdIXiDOXQEMqUMLcS1qeKMjSNFU0J7vyXF0njvOI6FfuolS9ntVRgCM4hjK4cAlVuIUa1IFBDs/wCm/Wk/VivVsf0+iSNZs5gD+wPn8Av+uUrQ=</latexit>
D(s)
<latexit sha1_base64="hD01c9KasdDmQVoRS6pc5pM73B4=">AB63icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LevBYwX5AG8pmu2mX7m7C7kYoX/BiwdFvPqHvPlv3KQ5aOuDgcd7M8zMC2LOtHdb6e0tr6xuVXeruzs7u0fVA+POjpKFKFtEvFI9QKsKWeStg0znPZiRbEIO0G09vM7z5RpVkH80spr7AY8lCRrDJpLu6Ph9Wa27DzYFWiVeQGhRoDatfg1FEkGlIRxr3fc2PgpVoYRTueVQaJpjMkUj2nfUokF1X6a3zpHZ1YZoTBStqRBufp7IsVC65kIbKfAZqKXvUz8z+snJrz2UybjxFBJFovChCMToexNGKEsNnlmCimL0VkQlWmBgbT8WG4C2/vEo6Fw3PbXgPl7XmTRFHGU7gFOrgwRU04R5a0AYCE3iGV3hzhPivDsfi9aSU8wcwx84nz8w2I2q</latexit><latexit sha1_base64="hD01c9KasdDmQVoRS6pc5pM73B4=">AB63icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LevBYwX5AG8pmu2mX7m7C7kYoX/BiwdFvPqHvPlv3KQ5aOuDgcd7M8zMC2LOtHdb6e0tr6xuVXeruzs7u0fVA+POjpKFKFtEvFI9QKsKWeStg0znPZiRbEIO0G09vM7z5RpVkH80spr7AY8lCRrDJpLu6Ph9Wa27DzYFWiVeQGhRoDatfg1FEkGlIRxr3fc2PgpVoYRTueVQaJpjMkUj2nfUokF1X6a3zpHZ1YZoTBStqRBufp7IsVC65kIbKfAZqKXvUz8z+snJrz2UybjxFBJFovChCMToexNGKEsNnlmCimL0VkQlWmBgbT8WG4C2/vEo6Fw3PbXgPl7XmTRFHGU7gFOrgwRU04R5a0AYCE3iGV3hzhPivDsfi9aSU8wcwx84nz8w2I2q</latexit><latexit sha1_base64="hD01c9KasdDmQVoRS6pc5pM73B4=">AB63icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LevBYwX5AG8pmu2mX7m7C7kYoX/BiwdFvPqHvPlv3KQ5aOuDgcd7M8zMC2LOtHdb6e0tr6xuVXeruzs7u0fVA+POjpKFKFtEvFI9QKsKWeStg0znPZiRbEIO0G09vM7z5RpVkH80spr7AY8lCRrDJpLu6Ph9Wa27DzYFWiVeQGhRoDatfg1FEkGlIRxr3fc2PgpVoYRTueVQaJpjMkUj2nfUokF1X6a3zpHZ1YZoTBStqRBufp7IsVC65kIbKfAZqKXvUz8z+snJrz2UybjxFBJFovChCMToexNGKEsNnlmCimL0VkQlWmBgbT8WG4C2/vEo6Fw3PbXgPl7XmTRFHGU7gFOrgwRU04R5a0AYCE3iGV3hzhPivDsfi9aSU8wcwx84nz8w2I2q</latexit><latexit sha1_base64="hD01c9KasdDmQVoRS6pc5pM73B4=">AB63icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LevBYwX5AG8pmu2mX7m7C7kYoX/BiwdFvPqHvPlv3KQ5aOuDgcd7M8zMC2LOtHdb6e0tr6xuVXeruzs7u0fVA+POjpKFKFtEvFI9QKsKWeStg0znPZiRbEIO0G09vM7z5RpVkH80spr7AY8lCRrDJpLu6Ph9Wa27DzYFWiVeQGhRoDatfg1FEkGlIRxr3fc2PgpVoYRTueVQaJpjMkUj2nfUokF1X6a3zpHZ1YZoTBStqRBufp7IsVC65kIbKfAZqKXvUz8z+snJrz2UybjxFBJFovChCMToexNGKEsNnlmCimL0VkQlWmBgbT8WG4C2/vEo6Fw3PbXgPl7XmTRFHGU7gFOrgwRU04R5a0AYCE3iGV3hzhPivDsfi9aSU8wcwx84nz8w2I2q</latexit>
STAR “- DemoReg”
STAR “- D-weight”
STAR “- z”
Figure 14: Overview of some of the performed ablations. -DemoReg: removes the demonstration reg-
ularization for the high-level semantic policy and uses only the task-agnostic prior for regularization,
-D-weight: removes the discriminator-based weighting between demonstration and task-agnostic reg-
ularization and uses only the former for guiding the policy during downstream training, -z: removes
the latent variable z from the low-level policy and instead uses a deterministic low-level policy and
no execution latent policy.
F
Detailed Ablation Description
We provide an overview of the components of our approach in Figure 13. The ﬁgure highlights
that most components are trained ofﬂine with simple supervised objectives and then frozen during
downstream task learning, making their training straightforward and reproducible. Only the high-level
semantic and execution policy are trained via online RL on the downstream task.
We also provide a more detailed description of the performed ablation studies from Figure 5, right,
below. These ablation studies demonstrate the importance of the different components of our model.
Finally, we visualize the resulting models for multiple of our ablation studies in Figure 14.
STAR - D-reward.
Ablates the discriminator-based dense reward (see Section 4.2). Instead trains
the high-level policy only based on the environment-provided reward on the downstream task.
STAR - TempAgg.
Ablates the temporal aggregation during cross-domain matching (see Sec-
tion 4.3). Instead uses single state semantic skill distributions to ﬁnd matching states.
19

STAR - DemoReg.
Ablates the policy regularization with cross-domain skill distributions. Instead
simply regularizes with task-agnostic skill priors derived from the target domain play data (see
Figure 14, left).
STAR - D-weight.
Ablates the discriminator-based weighting between demonstration and task-
agnostic skill distributions. Instead always regularizes the high-level semantic policy towards the
demonstration skill distribution (see Figure 14, middle).
STAR - z.
Ablates the use of the latent execution variable z in the skill policy. Instead trains
a simpler low-level policy without latent variable z and removes the execution latent policy (see
Figure 14, right).
G
Additional Ablation Experiments
We perform an additional ablation experiment to test whether replacing the high-level policy’s
weighted KL-regularization scheme from equation 3 with a simpler behavioral cloning regularization
objective can lead to comparable performance. Concretely, we replace the policy’s objective from
equation 3 with:
max
πh

Q(s, a) −αEk∼πsem(k|s)pdemo(k|s)
|
{z
}
BC regularization
−αlDKL
 πlat(z|s, k), pTA(z|s, k)

|
{z
}
task-agnostic execution prior regularization

.
(8)
We also experimented with removing the execution prior regularization term, i.e., setting αl = 0, but
found it to be crucial for training since the initial policy rapidly degrades without it.
STAR (ours)
BC Regularization
BC Planner
We report quantitative results on the human video demon-
stration to simulated kitchen manipulation task in the ﬁg-
ure on the right. The BC-Reg objective in equation 8 ob-
tains 75% lower performance than our full objective from
equation 3. This is because the behavioral cloning regular-
ization is also computed on states outside the demonstra-
tions’ support, leading to incorrect regularization. Instead,
our approach uses the discriminator to only apply regular-
ization within the support of the demonstrations.
We also add comparison to an even simpler baseline that
clones the transferred semantic skill embeddings from
the demonstrations, equivalent to a semantic-level BC
planner. This approach does not perform well due to
accumulating errors of the high-level planner (see ﬁgure on the right). Without online training, this
approach cannot correct the shortcomings of the planner.
20

