Transformer-based Conditional Variational Autoencoder
for Controllable Story Generation
Le Fang†, Tao Zeng§, Chaochun Liu§, Liefeng Bo§, Wen Dong†, Changyou Chen†
†University at Buffalo,
§JD Finance America Corporation, AI Lab
{lefang, wendong, changyou}@buffalo.edu
{tao.zeng, chaochun.liu, liefeng.bo}@jd.com
Abstract
We investigate large-scale latent variable models (LVMs) for
neural story generation—an under-explored application for
open-domain long text—with objectives in two threads: gener-
ation effectiveness and controllability. LVMs, especially the
variational autoencoder (VAE), have achieved both effective
and controllable generation through exploiting ﬂexible distri-
butional latent representations. Recently, Transformers and
its variants have achieved remarkable effectiveness without
explicit latent representation learning, thus lack satisfying con-
trollability in generation. In this paper, we advocate to revive
latent variable modeling, essentially the power of representa-
tion learning, in the era of Transformers to enhance controlla-
bility without hurting state-of-the-art generation effectiveness.
Speciﬁcally, we integrate latent representation vectors with
a Transformer-based pre-trained architecture to build condi-
tional variational autoencoder (CVAE). Model components
such as encoder, decoder and the variational posterior are all
built on top of pre-trained language models—GPT2 speciﬁ-
cally in this paper. Experiments demonstrate state-of-the-art
conditional generation ability of our model, as well as its ex-
cellent representation learning capability and controllability.
Introduction
Neural text generation has achieved remarkable success to
enable both effective and controllable generation at a level
that computational models can write like humans to satisfy
practical needs. Among various research objectives, the most
signiﬁcant ones are the effectiveness and the controllability
of generation, where there are always emerging opportunities
and challenges.
Deep latent variable models (LVMs), especially variational
autoencoder (VAE) (Kingma and Welling 2013; Rezende,
Mohamed, and Wierstra 2014) have been a signiﬁcant class
of methods to achieve both effective and controllable gener-
ation (Bowman et al. 2015; Miao, Yu, and Blunsom 2016;
Zhao, Zhao, and Eskenazi 2017; Zhao, Lee, and Eskenazi
2018; Zhou and Neubig 2017; Hu et al. 2017; Bao et al.
2019b; Shah and Barber 2018). These models generally work
with recurrent neural networks (RNN) such as Long short-
term memory (LSTM) (Hochreiter and Schmidhuber 1997)
and Gated recurrent unit networks (GRU) (Cho et al. 2014).
The advantage of LVMs is to learn and exploit ﬂexible dis-
tributional latent representations to capture holistic features
of input and further guide the generation of sentences. Such
powerful representation learning can deal with both the effec-
tiveness and the controllability of generation.
In recent years, Transformers (Vaswani et al. 2017) and
its variants have become the main-stream workhorses and
boosted previous generation effectiveness by large margins.
Models based on similar self-attention architectures (Devlin
et al. 2018; Radford et al. 2018, 2019) could leverage both big
models and big training data. A dominant paradigm emerges
to be “pre-training + ﬁne-tuning” on a number of natural
language processing tasks. Even without explicitly learning
latent representations, Transformer-based models could ef-
fectively learn from training data and generate high-quality
text. It’s thrilling to witness computational models generate
consistent long text in thousands of words with ease. How-
ever, given state-of-the-art generation effectiveness, control-
lability of these models—especially when generating long
text—is still under-explored. The emerging challenge is, how
to achieve controllable generation in the era of Transformers
and a long text setting?
In this paper, we advocate to revive latent variable mod-
eling, essentially the power of representation learning, in
the era of Transformers to enhance controllability without
hurting state-of-the-art generation effectiveness. Speciﬁcally,
we integrate latent representation vectors with self-attention
based pre-trained architecture to build a conditional vari-
ational autoencoder (CVAE). Model components such as
encoder, decoder and the variational posterior are all built on
top of pre-trained language models—GPT2 (Radford et al.
2019) speciﬁcally. We demonstrate excellent representation
learning capability and controllability of our Transformer-
based LVMs through learning and manipulating the latent
representation vectors.
On the application side, we emphasize a much challeng-
ing and under-explored task, i.e. neural story generation,
which creatively writes open-domain long text in hundreds
or thousands of words conditioned on very short and abstract
prompts (Fan, Lewis, and Dauphin 2018). The task featured
with much longer output leads to higher complexity and more
ﬂexibility in a broader space than short text generation. Pre-
vious literature (Fan, Lewis, and Dauphin 2018; Mao et al.
2019; See et al. 2019; Ziegler et al. 2019) have at most stud-
ied how to effectively learn the mapping between prompt and
story through explicit end to end (end2end) training. How-
ever, controllability in such a setting has rarely been studied.
arXiv:2101.00828v2  [cs.CL]  8 Jul 2021

For instance, how to control story development and semantic
transition during the spanning of long text? Pure end2end
learning seems quite rigid, which could miss ﬂexible and
controllable mechanisms inside a black box. A reasonable
solution for this issue is to introduce latent representation
vectors, which is the treatment we consider in this paper.
To summarize, our paper is among the ﬁrst works, by our
knowledge, to build Transformer-based latent variable mod-
els to solve the controllability issue in the setting of long text
generation. Recently, we notice an independent parallel work
(Li et al. 2020), which proposes similar Transformer-based
architecture to incorporate latent representation vectors. We
note that there are a number of differences between this work
and ours. Most signiﬁcantly, we considered both VAE and
CVAE in a long text setting, while Li et al. (2020) considered
a pre-trained VAE model in traditional short text setting. Our
datasets and source code is available on GitHub1.
The Model Architecture
Conditional Variational Autoencoder
Conditional story generation (Fan, Lewis, and Dauphin 2018)
refers to generating open-domain long text based on a short
prompt, which provides either a starting point or an ab-
stract summary for the writing. In this paper, we propose
a Transformer-based conditional variational autoencoder to
learn the generative process from prompt to story.
Figure 1: Graphical Model of VAE and CVAE. In control-
lable story generation, x and y refer to a prompt and a story,
respectively. z refers to a latent variable.
Variational Autoencoder (VAE)
(Bowman et al. 2015)
Figure 1 illustrates the graphical model of VAE, an unsu-
pervised learning method for unconditional generation. VAE
consists of a generative network (decoder) and an inference
network (encoder). Given a language dataset D = {yi}|D|
i=1,
where yi = [y1i, · · · , yT i] represents ith sentence of length
T. With a prior distribution p(z), VAE generates a sentence
y using the deep generative network pθ(y|z) parameterized
by θ. The prior p(z) is typically assumed to be a standard
multivariate Gaussian. The decoder pθ(x|z) typically takes
an auto-regressive form pθ(x|z) = QT
t=1 pθ(xt|x<t, z). In
this paper, we will build the decoder based on the pre-trained
GPT2 rather than traditional recurrent neural networks.
The goal of VAE training is to maximize the marginal
data log-likelihood Ey∼D[log pθ(y)]. However, poste-
rior inference is generally intractable. Consequently, an
1https://github.com/fangleai/TransformerCVAE
φ-parameterized encoder is introduced to approximate
pθ(z|y) ∝pθ(y|z)p(z) with a variational distribution
qφ(z|y). Variational inference is employed for VAE learning,
yielding the following evidence lower bound (ELBO):
Ex∼Dlogpθ(x) ≥
Ex∼D

Ez∼qφ(z|x)logpθ(x|z)

−Ex∼D [KL (qφ(z|x) ∥p(z))]
(1)
Conditional Variational Autoencoder (CVAE)
(Zhao,
Zhao, and Eskenazi 2017) Figure 1 also illustrates the
CVAE, an adaptation of VAE to ﬁt supervised learning
and conditional generation. Given a training dataset of
pairs D = {xi, yi}|D|
i=1, where xi = [x1i, · · · , xT i], yi =
[y1i, · · · , yT i] represents ith sentence of length T. In con-
trollable story generation, x and y refer to a prompt and a
story, respectively. Given an input x, CVAE encodes the prior
knowledge of latent code as p(z|x), and generates target y
using the deep generative network pθ(y|x, z) parameterized
by θ.
The goal of CVAE is to maximize the conditional data
log-likelihood Ex,y∼D[log pθ(y|x)]. Similarly, variational
inference is employed for CVAE learning, yielding the fol-
lowing evidence lower bound (ELBO):
Ex,y∼Dlogpθ(y|x) ≥
Ex,y∼D

Ez∼qφ(z|x,y)logpθ(y|x, z)

−Ex,y∼D [KL (qφ(z|x, y) ∥p(z|x))]
(2)
Note that both the prior pθ(z|x) and posterior qφ(z|x, y)
are learnable in CVAE.
Architecture Design
Our model architecture is illustrated in Figure 3. Basically,
it consists of a prior, posterior and conditional generator
based on multi-layer self-attention architecture (Vaswani et al.
2017), more speciﬁcally on top of pre-trained models.
Parameter initialization with GPT2
In order to exploit
the power of pre-trained models, we propose to reuse the
GPT2 model (Radford et al. 2019) as our decoder. For ease
of computation, we adopt the smallest public version with
L = 12 layers, H = 12 heads per layer, model dimension of
d = 768 units and total parameters of 117M. The encoder has
L1=6 unmasked/bi-directional self-attention layers, whose
parameters are initialized to the parameters of the ﬁrst L1 lay-
ers of the GPT2 model (initialized but not shared afterwards).
Moreover, the word embedding and positional embedding
tables in the encoder and decoder are shared.
Comparing with masked/uni-directional structure in de-
coder for auto-regressive generation, the key point is to have
unmasked/bi-directional structure in the encoder to allow
full information scope. In this sense, our design is compara-
ble with (Li et al. 2020) to reuse BERT in the encoder and
GPT2 in the decoder. However, we avocate two main design
differences: 1) We note that BERT uses word piece (WPE)
embeddings for tokenization and GPT-2 uses Byte Pair En-
coding (BPE), leading to totally different vocabulary books.

Figure 2: Self-attention (SA) and pseudo self-attention (PSA).
SA is widely used in our model including the attention-
average block, when pseudo self-attention can be used as
the 2⃝way to feed latent code (key and value colored in light
red) to the decoder.
(Li et al. 2020) resorts to keeping both tokenizations for all
inputs and outputs; while our design has none of such issues.
2) (Li et al. 2020) only works with short sentences typically
less that 64 words, while our model works with hundreds or
thousands of words in a minimal run. In our case, a model
with a full layer encoder (L1=12) is empirically too large to
ﬁt a single GPU memory. In order to save memory and con-
sidering that the ﬁrst several layers of GPT2 may implicitly
serve to encode features, our model only use L1=6 layers in
the encoder2.
Latent code from the encoder
Traditional RNN/LSTM
encoders typically only use the last hidden state from the
encoder to produce a latent space. This is insufﬁcient to
summarize sequential data and keep long-term knowledge.
In our model, representations from self-attention layers are a
sequence of vectors with total number equal to the number
of input tokens. To utilize all the information, we deﬁne an
attention-average block right afterwards to merge variable
length sequence of vectors into a single vector. The attention
average block essentially perform a multi-head self-attention
as Figure 2(a) using a learnable single query Q = qavg ∈Rd,
and K = V taken as the variable length sequence of vectors
from the last blocked self-attention layer. The single vector
representation is then passed to linear layers to predict prior
and posterior distribution, respectively.
In terms of model components, we deﬁne both the prior
and the variational posterior as isotropic Gaussian distribu-
tions, i.e., N(µ, σ2I), with learnable mean vector and “log σ”
vector. The KL divergence between the prior and posterior in
Eq. 2 is therefore analytically solvable. Traditional reparame-
terization trick (Kingma and Welling 2013) is used to allow
gradient passing through Gaussian sampling. Note that in our
model, the prior distribution p(z|x) and variational posterior
distribution qφ(z|x, y) share all the parameters except the
linear layers predicting their mean and variances to promote
prior posterior alignment.
In contrast to Transformers (Vaswani et al. 2017) and other
models that learn sequence of encoding vectors, our model
is dedicated to learning a single vector as an explicit latent
representation.
2Our experiment conﬁrms that using full layers in encoder has
limited improvement in performance comparing to using L1=6
layers in encoder.
Feeding latent code to the decoder
With a single latent
code representation3 z ∈Rd and a “GPT2” decoder, we
investigate three mainstream ways of latent code injection
inspired by previous literatures (Cheng et al. 2019; Ziegler
et al. 2019; Wang and Wan 2019).
1⃝INPUT: z is added to each input token during decoding, i.e.,
added with word embeddings and positional embeddings
element-wisely.
2⃝PSA4: inject latent code z in a per-layer basis. Speciﬁcally,
we ﬁrst project z ∈Rd into zL ∈Rd×L through a linear
layer, so that it can be split into L vectors [z1, · · · , zL],
with zl being fed into the l-th blocked self-attention layer.
As presented in (Ziegler et al. 2019) and shown in Fig-
ure 2(b), pseudo self-attention could absorb extra encoding
embeddings into a pre-trained GPT2 self-attention struc-
ture through
PSA(Q, K′, V ′) = softmax(QK′T
√dk
)V ′
(3)
where Q, K, V ∈Rl×d are the original input embeddings
participating self-attention; K′ =
zK
K

∈R(1+l)×d,
V ′ =
zV
V

∈R(1+l)×d are augmented key and value
matrices with projected latent code zK, zV from z ﬁlling
the ﬁrst row;
·
·

means concatenation by rows. Here, we
abbreviate per-layer code zl to z for notation simplicity.
3⃝SOFTMAX: in the original GPT2, an embedding vector
h ∈Rd from the last blocked attention layer is projected
to a pre-softmax logit vector p ∈RV through a linear head,
where V is the vocabulary size used in tokenization. When
a latent code should be injected in such a position, a new
and shared linear head will be initialized and learned in
order to project z ∈Rd into pz ∈RV . Finally we send
p + pz for the softmax and output.
We empirically study all the three ways of latent code
injection into the decoder, and present comparison in the
experiment section.
Training and Generation
We train our CVAE model according to the negative loss
objective in (2). For conditional story generation, the in-
put to the prior distribution p(z|x) is purely the prompt
and the input to posterior distribution qφ(z|x, y) is the con-
nected sequence of prompt and story split by a special to-
ken ‘<|endoftext|>”. The conditional generative distribution
pθ(y|x, z) is implemented as decoding with a text preﬁx
“prompt + <|endoftext|>” and feeding the latent code.
3Latent code can have dimension d′ ̸= d, in which case lin-
ear projection layers are needed before feeding latent code to the
decoder to ensure identical dimensions.
4In recent literature (Li et al. 2020), they also studied a way of la-
tent injection described as “Memory” vector. Essentially, “Memory”
is identical or equivalent to our “PSA”.

Figure 3: Our CVAE model architecture. Note that we use CVAE 1⃝, CVAE 2⃝, CVAE 3⃝to represent model variants that use the
1⃝, 2⃝, 3⃝way of latent code injection respectively. The ﬁnal model are not meant to use all ways but only subset combinations
as implied by performance.
To avoid learning deviation caused by random initialized
parameters, we freeze pre-trained parameters initialized from
GPT2 in the ﬁrst several iterations of training, i.e. 10K itera-
tions, and unfreeze them afterwards.
To alleviate the notorious posterior collapse issue, we take
a cyclic annealing schedule (Fu et al. 2019) by adjusting the
coefﬁcient β before KL divergence in (2). Speciﬁcally, we
have kept β close to zero in the ﬁrst half of cyclic schedule,
linearly annealed β to 1 in the next one fourth of cyclic
schedule and kept β = 1 in the remaining one fourth of
cyclic schedule. The purpose of such schedule is to exploit
the period that β is close to zero, which pushes the model
towards a pure autoencoder. Note that autoencoder (Bourlard
and Kamp 1988) learns point estimate of latent code instead
of distributional representation to generate target data, which
could improve generation effectiveness.
During generation, a short prompt text is fed to the en-
coder, and a latent code is sampled from the prior distribution
to guide the decoding. This procedure is the same as how
traditional CVAE works.
Related Work
Controllable Story Generation
Most of previous works on text generation consider a setting
of short text. For controllable generation, they mainly con-
sider certain global aspects of text, with the most common
aspects being sentiment and topic (Shen et al. 2017; Zhao
et al. 2018; Hu et al. 2017; Fang et al. 2019; Dathathri et al.
2019; Keskar et al. 2019; Li et al. 2020; Wang et al. 2020).
Researchers have attempted short story generation with ﬁne-
grained control through plots, plans or the so-called story-
lines (Peng et al. 2018; Yao et al. 2019), leading to a wide
usage and benchmark on 5-lines story dataset ROCStories
(Mostafazadeh et al. 2016).
In recent years, (Fan, Lewis, and Dauphin 2018) proposes

story generation as a test bed of open-domain long text gen-
eration (Fang et al. 2021). (Ziegler et al. 2019) initiates the
research of conditionally generating story based on a pre-
trained GPT2.
Though achieving promising results, very few works have
been presented to improve controllability in the setting of
long text. This work is, by our knowledge, the ﬁrst work to
build a Transformers-based latent variable model to improve
controllable open-domain long text generation.
Transformers-based LVMs
Recently, there are several works building latent variable
models on top of the Transformer. One main class of work
is conditional VAEs with Transformer-based components
in non-autoregressive sequence generation, especially non-
autoregressive machine translation (Shu et al. 2020; Ma et al.
2019; Kasai et al. 2020; Han et al. 2020). Another class of
work is on dialogue generation with conditional VAEs (Bao
et al. 2019a; Lin et al. 2020) for response diversity.
From another perspective, (Wang and Wan 2019) aims
to learn latent representations for story completion, where
latent variables are only used to ﬁll very short text blank.
(Cheng et al. 2019) proposes a semi-supervised method us-
ing a Transformer-based VAE to solve aspect-term sentiment
analysis problem. The method also disentangles latent space
for aspect-speciﬁc sentiment and the lexical context, respec-
tively. A recently released literature (Li et al. 2020) proposes
large-scale VAE as a pre-trained model. The model is ﬁrst
pre-trained with massive unlabelled corpus and then ﬁne-
tuned in various down-stream generation tasks that traditional
RNN/LSTM based VAEs have attempted.
Our paper indeed gets some inspirations from previous
works of Transformer-based latent variable models on archi-
tecture side. However, our model is motivated to enhance
controllability of generation and deals with a challenging
setting of long-text generation.
Experimental Results and Discussions
Pre-Experiment on VAE Architecture
In order to verify our idea of Transformer-based latent vari-
able models, we ﬁrst conduct a pre-experiment with the VAE
architecture on two small datasets. The VAE implemented
is a simpler version of our CVAE model shown in Figure 3,
where the prior is deﬁned as standard spherical Gaussian
N(1, I). Moreover, the VAE conducts pure unsupervised
learning, where unlabelled language texts are encoded into a
distributional representation space and then decoded back.
Datasets:
The two relatively small datasets are introduced
in the following for VAE learning respectively with statistics
shown in Table 1.
The Arxiv is an online dataset (Sergio 2019) that extracts
abstracts from “arxiv” articles. Speciﬁcally, a topic query is
searched among arxiv abstracts and the matched ones are
collected. The three topic queries we used are “artiﬁcial in-
telligence”, “computer vision” and “language generation”,
leading to around 12K article abstracts respectively having
such topic words. The Yelp is a public dataset (Yang et al.
Dataset
Num. of text
Split
Avg. length
Arxiv
35228
90-5-5
233
Yelp
120 K
10-1-1
96.7
Table 1: Statistics of datasets in VAE pre-experiment.
(a) Arxiv.
(b) Yelp.
Figure 4: Representation learning of stories in pre-experiment.
(a) Arxiv: Topic are draw in different colors: red for artiﬁcial
intelligence; blue for computer vision; green for language
generation; (b) Yelp: Sentiment are draw in two colors: red
for negative; blue for positive.
2017; He et al. 2018) with restaurant reviews collected from
the “Yelp” website. Reviews are associated with user ratings
from one to ﬁve stars. We binarize reviews with user rating
above three as positive, and below three as negative, leading
to a binary sentiment dataset.
Visualization Results
Figure 4 visualizes the posterior z
of texts in the test dataset in 2D space using t-SNE (Maaten
and Hinton 2008). As we can see, meaningful latent spaces
can be learned, which are able to cluster high-dimension data
according to proximity between their latent codes. Interest-
ingly, for the Arxiv dataset, cluster of “artiﬁcial intelligence”
lies between clusters of “computer vision” and “language
generation”, which coincides with our understanding of these
topics. Such visualization shows encouraging signs on repre-
sentation learning power of our model.
Experimental Settings
Datasets:
We conduct conditional story generation on two
datasets, WritingPrompts and WikiPlots, with statistics
shown in Table 2. The datasets are publicly available and meet
our target of open-domain long text corpora. We have investi-
gated several other most commonly used public datasets in
conditional text generation. Another dataset commonly used
in story-plot generation is ROCStories (Mostafazadeh et al.
2016; Yao et al. 2019), which consists of 5-lines stories, thus
are too short to use in our task.
For the two datasets adopted, the WritingPrompts is a
dedicated large scale hierarchical story generation dataset
collected from Reddit’s “WritingPromts” forum (Fan, Lewis,
and Dauphin 2018; Mao et al. 2019). Given a prompt as a

Dataset
Num.
stories
Split
Prompt
avg.
len.
Story
avg.
len.
Story
avg.
paragraphs
WP
303 K
90-5-5
25.4
674.5
6.3
WI
113 K
90-5-5
3.4
332.9
3.1
Table 2: Statistics of datasets for controllable story generation.
WP: WritingPrompts; WI: WikiPlots.
rough guide or starting point, stories are multi-paragraph
short novels written by human users; the WikiPlots5 con-
tains story plot about books, novels, ﬁlms, and etc, extracted
from English language Wikipedia. Each story plot is paired
with a short title, which is used similarly as prompt in the
WritingPrompts.
Benchmark Models
Each of our benchmark models serves designated purposes.
Note that we don’t benchmark with other pre-trained lan-
guage model bases which may be much more powerful than
GPT2. We also don’t choose some popular controllable text
generators such as (Hu et al. 2017; Dathathri et al. 2019)
since they either only work in a short text setting or discuss a
different notion of control.
By
comparing
with
a
state-of-the-art
specialized-
architecture task-speciﬁc story generation model (Fan, Lewis,
and Dauphin 2018), we evaluate models’ in-domain gen-
eration performances. Fusion models in (Fan, Lewis, and
Dauphin 2018) takes a convolutional seq2seq model struc-
ture with a fusion training mechanism. Although similar
self-attention architectures are used, the fusion model is still
different with our Transformer-based architectures on the
design of key and value vectors.
By comparing with a state-of-the-art transfer learning
method based on GPT-2 models—pseudo self attention (PSA)
(Ziegler et al. 2019), we compare our CVAE model with a
pure supervised training method for conditional generation.
The pseudo self attention introduces new projection matrices
to absorb a sequence of embedding vectors from input to
the self-attention computational framework. Note that we
may use pseudo self attention (PSA) as one way of latent
code injection ( 2⃝), but there are key differences: our model
only injects a single encoded vector, rather than a sequence
of encoding vectors in original PSA; our CVAE model has
exploited the notion of distributional representation to learn
a representation space to enable ﬂexible controllability. In
another words, our CVAE learns encoded vector together
with a posterior distribution, when pure PSA doesn’t.
By comparing with a simple way of transfer learning called
“ﬁne-tuning with special tokens” (FIST) (Fang et al. 2021),
we investigate the effect of incorporating a latent code into
the decoding. FIST does not learn a latent code, but only ﬁne
tunes a pre-trained GPT2 model with augmented language
texts, i.e., directly connecting prompt with story and put a
special token ‘<|endoftext|>” in between.
5https://github.com/markriedl/WikiPlots
By comparing different ways of latent code injection, we
evaluate their effectiveness accordingly. We label our CVAE
model with the latent code injections 1⃝, 2⃝and 3⃝as CVAE-
1⃝, CVAE- 2⃝and CVAE- 3⃝, respectively, as is reﬂected in
Figure 3.
Implementation Details
We implement our models using the “Huggingface Trans-
formers” library in Pytorch (Wolf et al. 2019). In evaluation,
we generate stories using the top-k top-p random sampling
scheme (Holtzman et al. 2019; Keskar et al. 2019) with
k = 100 and p = 0.9. Temperature smoothing technique
is also applied with T = 0.9. Considering the two relatively
large test datasets, we randomly decode one story per test
input, rather than sampling several stories per test prompt
and selecting the best one.
Evaluation Results
Automatic Metrics:
We evaluate the following automatic
metrics towards target stories:
• Perplexity (PPL) is used to evaluate language models and
often regarded as a proxy for generation quality. All mod-
els based on GPT-2 use the BPE tokenization scheme,
where PPL values are not directly comparable with some
previous models such as (Fan, Lewis, and Dauphin 2018)
with PPLs computed at the natural word level. Similar to
(See et al. 2019), we additionally compute the word-level
perplexity of GPT-2 models to enable the comparison with
previous models. That is, we normalize the total negative
log probability of the target text by the number of word
level tokens.
• ROUGE scores are computed as n-gram overlap of test
generated stories versus given target stories. For complete-
ness, we report ROUGE scores (ROUGE-1, ROUGE-2,
and ROUGE-L) (Lin and Hovy 2002) of n-gram overlap
with both precision (P), recall (R), and F1.
Automatic Evaluation Results:
The results are presented
in Table 3.
Overall, our CVAE model achieves generally better, at
least comparable, metrics in terms of lower PPL and higher
ROUGE scores, demonstrating state-of-the-art conditional
story generation performance.
Methods based on pre-trained models (PSA / FIST /
CVAE) show better overall performance with relatively less
task speciﬁc efforts than the Fusion models, demonstrating
the power of large-scale pre-training with Transformer-based
architecture. Fusion models still show relatively high preci-
sion scores, especially in WritingPrompts, due to its dedi-
cated design for story generation.
When comparing CVAE 2⃝with PSA, we observe perfor-
mance improvement due to the ﬂexible learned representation
space. Note that CVAE merges a sequence of encoding repre-
sentation vectors into a single latent vector, which is the key
difference with original PSA.
When comparing CVAE variants with FIST, we observe
the beneﬁt of latent representation modeling as a powerful
addition to pure occurance modeling.

Methods
Perplexity↓
ROUGE-1↑
ROUGE-2↑
ROUGE-L↑
Word
BPE
F1
P
R
F1
P
R
F1
P
R
Dataset: WritingPrompts
Fusion
36.0
-
0.223
0.386
0.157
0.038
0.074
0.026
0.206
0.358
0.145
PSA
31.6
21.3
0.265
0.316
0.228
0.047
0.054
0.041
0.248
0.296
0.213
FIST
30.2
25.9
0.181
0.339
0.123
0.023
0.046
0.015
0.17
0.321
0.116
CVAE 1⃝
26.4
23.2
0.265
0.332
0.221
0.046
0.067
0.035
0.244
0.353
0.187
CVAE 2⃝
26.8
23.2
0.266
0.325
0.225
0.049
0.074
0.037
0.253
0.348
0.199
Dataset: WikiPlots
Fusion
108.2
-
0.185
0.185
0.185
0.026
0.027
0.025
0.15
0.149
0.151
PSA
79.5
47.8
0.188
0.188
0.189
0.026
0.025
0.027
0.172
0.171
0.173
FIST
38.9
26.5
0.166
0.253
0.124
0.018
0.032
0.013
0.15
0.231
0.111
CVAE 1⃝
37.6
25.4
0.196
0.211
0.183
0.026
0.027
0.025
0.187
0.196
0.178
CVAE 2⃝
37.6
26.4
0.190
0.221
0.167
0.030
0.035
0.026
0.169
0.197
0.148
Table 3: Automatic metrics for conditional story generation evaluated on two datasets.
When comparing different ways of latent code injection
in CVAE, we observe that it is hard to made option 3⃝work
empirically; options 1⃝and 2⃝perform comparably well.6
Our observation is different from (Li et al. 2020), which
claims 2⃝works signiﬁcantly better than 1⃝. We suspect this
is due to an inherently different experiment setting, where
we work with signiﬁcantly longer text.
Qualitative Evaluation:
When conducting training on
the WikiPlots dataset, we observe similar representation
learning results as shown in Figure 5. A story prompt in
WikiPlots may have extractable key words to reveal the
item types, such as TV series, ﬁlm, music, manga, novel,
and game, etc. We observe that item types from test story
prompts are clearly clustered in the latent code space, which
implies effective representation learning to capture inherent
characteristic of the prompts.
We further present qualitative generation examples on the
test datasets in Tables 4–11. We observe that stories are se-
mantically and grammatically sound, and more importantly,
highly conditioned on and consistent with given prompts. A
large scale human evaluation is underway, which is quite
expensive due to text length and evaluation scale.
Controllability Experiments:
To verify the effect of the
learned latent representation vectors in generation, we con-
duct an interesting “control” experiment: given two prompt
x1 and x2, we generate story from pθ(y|x1, z2), i.e., con-
ditioning on x1 preﬁx and feeding in latent code z2 along
the decoding. In this way, the generated story will lie in the
combination semantic space of the two prompt x1 and x2,
especially after the latent code z2 takes effect and dominates.
Generation examples from the two test datasets are pre-
sented in Tables 7 and 11. We colorize key words in generated
6We also observe that using both 1⃝and 2⃝does not consistently
improve performance.
Figure 5: Visualization of prompts in WikiPlots test dataset
through t-SNE. Item types are draw in different colors: red
for TV series; blue for ﬁlms; yellow for manga and comic;
black for novel.
stories that coincides with given prompts x1 and x2 accord-
ingly. Such examples conﬁrm the effect of latent codes in
generation, indicating our model as a principal way to en-
hance controllability.
Conclusion and Future Research
In this paper, we propose Transformer-based latent variable
models to enhance story controllability while maintaining
state-of-the-art generation effectiveness. Our test bed is a
much more challenging and under-explored long-text appli-
cation than the traditional short-text generation. Our results
indicate the superiority of Transformer-based latent variable
models, and appeal more efforts to be invested in the domain.

References
Bao, S.; He, H.; Wang, F.; and Wu, H. 2019a.
PLATO:
Pre-trained Dialogue Generation Model with Discrete Latent
Variable. arXiv preprint arXiv:1910.07931 .
Bao, Y.; Zhou, H.; Huang, S.; Li, L.; Mou, L.; Vechtomova,
O.; Dai, X.; and Chen, J. 2019b. Generating Sentences from
Disentangled Syntactic and Semantic Spaces. In Proceedings
of the 57th Annual Meeting of the Association for Computa-
tional Linguistics, 6008–6019.
Bourlard, H.; and Kamp, Y. 1988. Auto-association by multi-
layer perceptrons and singular value decomposition. Biologi-
cal cybernetics 59(4-5): 291–294.
Bowman, S. R.; Vilnis, L.; Vinyals, O.; Dai, A. M.; Jozefow-
icz, R.; and Bengio, S. 2015. Generating sentences from a
continuous space. arXiv preprint arXiv:1511.06349 .
Cheng, X.; Xu, W.; Wang, T.; Chu, W.; Huang, W.; Chen,
K.; and Hu, J. 2019. Variational Semi-Supervised Aspect-
Term Sentiment Analysis via Transformer. In Proceedings
of the 23rd Conference on Computational Natural Language
Learning (CoNLL), 961–969.
Cho, K.; van Merri¨enboer, B.; Gulcehre, C.; Bahdanau, D.;
Bougares, F.; Schwenk, H.; and Bengio, Y. 2014. Learn-
ing Phrase Representations using RNN Encoder–Decoder
for Statistical Machine Translation. In Proceedings of the
2014 Conference on Empirical Methods in Natural Language
Processing (EMNLP), 1724–1734.
Dathathri, S.; Madotto, A.; Lan, J.; Hung, J.; Frank, E.;
Molino, P.; Yosinski, J.; and Liu, R. 2019. Plug and Play
Language Models: A Simple Approach to Controlled Text
Generation. In International Conference on Learning Repre-
sentations.
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.
Bert: Pre-training of deep bidirectional transformers for lan-
guage understanding. arXiv preprint arXiv:1810.04805 .
Fan, A.; Lewis, M.; and Dauphin, Y. 2018. Hierarchical
Neural Story Generation. In Proceedings of the 56th Annual
Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), 889–898.
Fang, L.; Li, C.; Gao, J.; Dong, W.; and Chen, C. 2019.
Implicit Deep Latent Variable Models for Text Generation.
In Proceedings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing (EMNLP-
IJCNLP), 3937–3947.
Fang, L.; Zeng, T.; Liu, C.; Bo, L.; Dong, W.; and Chen,
C. 2021.
Outline to Story: Fine-grained Controllable
Story Generation from Cascaded Events. arXiv preprint
arXiv:2101.00822 .
Fu, H.; Li, C.; Liu, X.; Gao, J.; Celikyilmaz, A.; and Carin,
L. 2019. Cyclical Annealing Schedule: A Simple Approach
to Mitigating KL Vanishing. NAACL .
Han, Q.; Meng, Y.; Wu, F.; and Li, J. 2020.
Non-
Autoregressive Neural Dialogue Generation. arXiv preprint
arXiv:2002.04250 .
He, J.; Spokoyny, D.; Neubig, G.; and Berg-Kirkpatrick, T.
2018. Lagging Inference Networks and Posterior Collapse
in Variational Autoencoders. In International Conference on
Learning Representations.
Hochreiter, S.; and Schmidhuber, J. 1997. Long short-term
memory. Neural computation 9(8): 1735–1780.
Holtzman, A.; Buys, J.; Du, L.; Forbes, M.; and Choi, Y.
2019. The Curious Case of Neural Text Degeneration. In
International Conference on Learning Representations.
Hu, Z.; Yang, Z.; Liang, X.; Salakhutdinov, R.; and Xing, E. P.
2017. Toward controlled generation of text. In Proceedings
of the 34th International Conference on Machine Learning-
Volume 70, 1587–1596. JMLR. org.
Kasai, J.; Cross, J.; Ghazvininejad, M.; and Gu, J. 2020. Par-
allel Machine Translation with Disentangled Context Trans-
former. arXiv preprint arXiv:2001.05136 .
Keskar, N. S.; McCann, B.; Varshney, L. R.; Xiong, C.;
and Socher, R. 2019. Ctrl: A conditional transformer lan-
guage model for controllable generation. arXiv preprint
arXiv:1909.05858 .
Kingma, D. P.; and Welling, M. 2013. Auto-encoding varia-
tional bayes. arXiv preprint arXiv:1312.6114 .
Li, C.; Gao, X.; Li, Y.; Li, X.; Peng, B.; Zhang, Y.; and Gao,
J. 2020. Optimus: Organizing Sentences via Pre-trained Mod-
eling of a Latent Space. arXiv preprint arXiv:2004.04092
.
Lin, C.-Y.; and Hovy, E. 2002. Manual and automatic evalua-
tion of summaries. In Proceedings of the ACL-02 Workshop
on Automatic Summarization-Volume 4, 45–51. Association
for Computational Linguistics.
Lin, Z.; Winata, G. I.; Xu, P.; Liu, Z.; and Fung, P. 2020.
Variational Transformers for Diverse Response Generation.
arXiv preprint arXiv:2003.12738 .
Ma, X.; Zhou, C.; Li, X.; Neubig, G.; and Hovy, E. 2019.
FlowSeq: Non-Autoregressive Conditional Sequence Gen-
eration with Generative Flow. In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP), 4273–4283.
Maaten, L. v. d.; and Hinton, G. 2008. Visualizing data
using t-SNE. Journal of machine learning research 9(Nov):
2579–2605.
Mao, H. H.; Majumder, B. P.; McAuley, J.; and Cottrell,
G. 2019. Improving Neural Story Generation by Targeted
Common Sense Grounding. In Proceedings of the 2019 Con-
ference on Empirical Methods in Natural Language Process-
ing and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP), 5990–5995.
Miao, Y.; Yu, L.; and Blunsom, P. 2016. Neural variational
inference for text processing. In International conference on
machine learning, 1727–1736.
Mostafazadeh, N.; Chambers, N.; He, X.; Parikh, D.; Batra,
D.; Vanderwende, L.; Kohli, P.; and Allen, J. 2016. A cor-
pus and evaluation framework for deeper understanding of
commonsense stories. arXiv preprint arXiv:1604.01696 .

Peng, N.; Ghazvininejad, M.; May, J.; and Knight, K. 2018.
Towards controllable story generation. In Proceedings of the
First Workshop on Storytelling, 43–49.
Radford, A.; Narasimhan, K.; Salimans, T.; and Sutskever, I.
2018. Improving language understanding by generative pre-
training. URL https://s3-us-west-2. amazonaws. com/openai-
assets/researchcovers/languageunsupervised/language un-
derstanding paper. pdf .
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and
Sutskever, I. 2019. Language models are unsupervised multi-
task learners. OpenAI Blog 1(8): 9.
Rezende, D. J.; Mohamed, S.; and Wierstra, D. 2014. Stochas-
tic Backpropagation and Approximate Inference in Deep
Generative Models. In International Conference on Machine
Learning, 1278–1286.
See, A.; Pappu, A.; Saxena, R.; Yerukola, A.; and Manning,
C. D. 2019. Do Massively Pretrained Language Models Make
Better Storytellers? In Proceedings of the 23rd Conference on
Computational Natural Language Learning (CoNLL), 843–
861.
Sergio, G. C. 2019. ArXivAbsTitleDataset: Extracting Ab-
stract and Title Dataset from arXiv articles. Github repository
.
Shah, H.; and Barber, D. 2018. Generative Neural Machine
Translation. In Bengio, S.; Wallach, H.; Larochelle, H.; Grau-
man, K.; Cesa-Bianchi, N.; and Garnett, R., eds., Advances in
Neural Information Processing Systems 31, 1346–1355. Cur-
ran Associates, Inc. URL http://papers.nips.cc/paper/7409-
generative-neural-machine-translation.pdf.
Shen, T.; Lei, T.; Barzilay, R.; and Jaakkola, T. 2017. Style
transfer from non-parallel text by cross-alignment. In Ad-
vances in neural information processing systems, 6830–6841.
Shu, R.; Lee, J.; Nakayama, H.; and Cho, K. 2020. Latent-
Variable Non-Autoregressive Neural Machine Translation
with Deterministic Inference Using a Delta Posterior. In
AAAI.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;
Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention
is all you need. In Advances in neural information processing
systems, 5998–6008.
Wang, T.; and Wan, X. 2019. T-CVAE: Transformer-based
conditioned variational autoencoder for story completion. In
Proceedings of the 28th International Joint Conference on
Artiﬁcial Intelligence, 5233–5239. AAAI Press.
Wang, Z.; Wang, X.; An, B.; Yu, D.; and Chen, C. 2020. To-
wards Faithful Neural Table-to-Text Generation with Content-
Matching Constraints.
Wolf, T.; Debut, L.; Sanh, V.; Chaumond, J.; Delangue, C.;
Moi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; and
Brew, J. 2019. HuggingFace’s Transformers: State-of-the-art
Natural Language Processing. ArXiv abs/1910.03771.
Yang, Z.; Hu, Z.; Salakhutdinov, R.; and Berg-Kirkpatrick,
T. 2017. Improved variational autoencoders for text model-
ing using dilated convolutions. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70,
3881–3890. JMLR. org.
Yao, L.; Peng, N.; Weischedel, R.; Knight, K.; Zhao, D.;
and Yan, R. 2019. Plan-and-write: Towards better automatic
storytelling.
In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, volume 33, 7378–7385.
Zhao, J.; Kim, Y.; Zhang, K.; Rush, A.; and LeCun, Y. 2018.
Adversarially regularized autoencoders.
In International
Conference on Machine Learning, 5902–5911. PMLR.
Zhao, T.; Lee, K.; and Eskenazi, M. 2018. Unsupervised
Discrete Sentence Representation Learning for Interpretable
Neural Dialog Generation. In Proceedings of the 56th Annual
Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), 1098–1107.
Zhao, T.; Zhao, R.; and Eskenazi, M. 2017.
Learning
Discourse-level Diversity for Neural Dialog Models using
Conditional Variational Autoencoders. In Proceedings of the
55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), 654–664.
Zhou, C.; and Neubig, G. 2017. Multi-space variational
encoder-decoders for semi-supervised labeled sequence trans-
duction. arXiv preprint arXiv:1704.01691 .
Ziegler, Z. M.; Melas-Kyriazi, L.; Gehrmann, S.; and Rush,
A. M. 2019. Encoder-Agnostic Adaptation for Conditional
Language Generation. arXiv preprint arXiv:1908.06938 .

Appendix
Prompt:
[ WP ] The root password for the universe is discovered.
Target Story
——————————————————————————-
The password wasn’t so simple. It required a sequence of well-timed thoughts, followed a series of
moods, and then a frequency inaudible to the human ear.
And yet, I cracked it. The possibilities opened up before me, vast and terrifying in all their
permutations. The longer I stared into the truth, the reasons, the very secrets behind everything,
the more I realized that neither I nor anyone else could properly harness the immediacy of all
knowledge.
Ignorance wasn’t necessarily bliss, but there was a joy in gradual discovery. Humanity was meant
to play this game without cheat codes. I changed the password before wiping my memory clean.
Generated Story
——————————————————————————-
The root password for the universe is discovered
For over two thousand years we have searched the universe for the root password. From the
universe’s earliest stages we have always received the same answer. The password has been our
forever since it was discovered. The universe has no known method of creation other than itself
to try and destroy it. The password is based off of the last known version of it. The universe is
expanding and expanding and expanding. The universe seems to be expanding towards a greater
dimension. What does this mean to us? Why? The answer is simple to discover. I have learned from
thousands of sources and I have learned to use the truth for what I wish to be the true universe.
I have no idea what is happening, or where it is going. The universe seems to be expanding,
changing, and expanding away from where it started. Now, I know it is a mistake, but the truth
must be known. I would like to contact the universe. The answer must be obvious to everyone. The
answer must be discovered. All of my attempts to stop this thing have failed, and now I know. My
purpose is to destroy the universe and rid it of the root password.
Table 4: Generation examples on test dataset of WritingPrompts.

Prompt:
You’re elected president, but you don’t want to be president anymore, so you do ridiculous things to get impeached,
but they all turn out to be the best decision for the country, making you more popular.
Target Story
——————————————————————————-
I’ve tried everything. And they still seem to love me. I remember a time when the people hated me. That was a good
time. When I ﬁrst became president, I eliminated income tax to improve the economy and the people started a riot.
So to spite them, and to get myself impeached, I initiated the Taco Tuesday initiative (free tacos every tuesday) and
suddenly the people loved me. That boosted taco sales 180 % and caused a boom in the economy. I tried everything to
get back at them (requiring my cabinet members to wear costumes to work every day, insulting other countries leaders,
holding witch hunts) but nothing has worked. Each time they come back praising me for my stupidity. During the last
presidential address I blathered like an idiot, and they praised me for inventing a new language, on live television.
It is now called American, and is being taught in schools. The time before I recited Adolf Hitler’s Beer Hall Putsch
speech, and they praised me for my original thinking. I don’t understand these people. One week I say we shut down
the government, they praise it. The next, I suggest we pay more to the government, and they praise it. A sane man
would think that what I did was completely insane and would tank our nation, but everything I did made it better. Last
night, I went completely naked on live television during my address and instead of getting condemned and arrested for
indecent exposure, I was heralded as a hero for every group known to man. They called me a visionary for removing
illusions of needs for clothing and material. We now have no industry for clothing, because everyone is nude, which in
turn saved money on income for all of our citizens. I would normally think that the people are only following me
to see how much I can humiliate myself, but everything I seem to do is actually improving the nation as a whole. I
said we should power our cars with farts and scientists actually developed a fart powered car. Our dependency on
foreign oil is dwindling. I’ve won more awards for the most stupid ideas. I have patents on Galchoopas, Kerﬂickdas,
and Ladugas, and I don’t even know what they do (all of them from my presidential address). I apparently described
them in so much detail during my babbling that scientists had the devices ready by the next day.
I only want to quit. The people are begging me to stay for another term. I’m going to begin shooting people at the
address to get them to impeach me. I pray it happens.
It turns out everyone I shot (all 30 of them) were all spies from other countries planning my assassination. Apparently
the other countries were afraid of America becoming too strong so they all sent Assassins to take me out at the address.
Now all of the countries leaders have to answer to us for trying to take me out. God help me I can’t escape this.
Generated Story
——————————————————————————-
”Mama!” I cry. My mom is always so concerned with me. I have to take care of my mother. You know I hate her for
making me make this decision, but the government needs me, its an extension of itself. The whole’free tacos every
Tuesday’ crap was a joke, I’ve been told to just use the taco Tuesday initiative as much as possible and save $10 on
food stamps. The fact that this stuff gets us is just plain laughable. The last time this happened, the President of the
United States got sick from being in an abusive relationship with her secretary. The president asked for a taco Tuesday
initiative, she needed to go on vacation to San Francisco to ”help me out”. This should have been the most fun part,
but apparently, she wasn’t listening to the President. Apparently she’d been drinking too much and got sick of being
drunk while she was vacationing. So I put my beer in the fridge and ﬁlled the bowl. The whole thing was really just
about the end of time for me. The President went home and I was woken up by the sound of footsteps coming up the
stairs. The door to the beer hall putsch speech was open. I started laughing so hard. Before the President could ﬁnish
saying, I got up and yelled ”Hands up!” In that moment, I realized, my mom was probably one of those crazy aliens
who just ﬁgured out a way to be president and became president. I was the most popular man in the country, but the
President wasn’t going to make me do this for her. ”Daddy, are you okay?” she asked as soon as I stepped out of the
bathroom. My mother hugged me and told me to get a better look for the President. Before I could, I reached out and
started picking up the President’s hair. There were two little holes all over it. I walked over and saw the tiny hole at the
top of the head. I lifted the President’s head over the hole and placed my hands on her head. She looked at me like I
was insane, and I told her to go fuck herself. As I was thinking this, I looked at my hands and noticed that they were
holding the President’s shirt. I took out my phone and called 911. The man who killed the President stood there crying.
He turned to me and said ”Don’t you ever think about putting this on.”
Table 5: More generation examples on test dataset of WritingPrompts.

Prompt:
Humanity is the most powerful race in the galaxy. Every other race wants to win our respect. Even
for ordinary humans, it’s become impossible to go off-world because our presence demands so
much respect, awe, and asskissing.
Target Story
——————————————————————————-
At ﬁrst nobody knew what to do. When we found the other civilizations, I remember I was in the
pentagon with the country’s best scientists trying to unfold the events that had happened hours
before when Dr.Greg Darkasian found the ﬁrst signs of alien civilizations. That day no body had
any answers. That was ten years ago, now we know where we fall on the galactic hierarchy, and
much to the pleasure of Earths inhabitants, it’s sitting right at the top. Today I work as an alien
Slave trader, you see once we found the dozens of other societies we focused on controlling them
and the business that was booming was alien slave trade. So as a struggling scientist I turned to
what humans do best. Enslaving our inferiors, much like societies have done for ages. Not only did
we dominate these societies but we ravaged all of their home planets of any resources and now all
that’s useful from them is their physical labor. Of the few alien planets left untouched, they are
weak and we could probably take them out with a single battalion. The only thing we didn’t expect
is that when these resources ran dry and the slaves mainly dead, our own planet was In turmoil.
You see while all the countries of earth we’re focusing on interplanetary domination. We failed to
notice the growth of Japan, the one country that was not sucked Into the Galactic power race. Now
japan threatens to control our home country and there’s nothing we can do. Most of the worlds
military services are deployed eons away from earth.
Generated Story
——————————————————————————-
We’ re on the brink of war, hundreds of worlds military services are gone, our allies are gone, some
are captured, most are executed, we’ ve had ten years ago our only chance to stop it is to pull
the trigger. It has been a month, we’ re being attacked on all fronts and our numbers are growing
exponentially. We’ re sending troops to every sector of our galaxy, we’ re training for battle, we’ ve
got weapons we need to fend off an alien invasion. We’ re sending reinforcements, the ﬁrst battle
with it was ten years ago. They came out of nowhere, they’ re huge, they’ re faster, they can ﬂy,
and they don’ t ﬁght like us, but they ﬁght on, because they don’ t have any of the same skills as us.
All because of their stupid species. We’ re the biggest threat to them, we’ re the ones on the front
lines. And the only way to defeat them is to kill them and replace them with the ones we have, but
not because our stupid species wouldn’ t stand a chance, not because it would be much easier to
defeat them, but because they’ re such an ignorant race. We’ re the only ones on the front lines, we’
re the only ones who don’ t believe that we need to kill all the other races. I don’ t know if there is
any other species out there, there are only us. If there was any other race, we would be the ones on
the front lines. The only ones that would take it, the only ones who would stand against us. Not
even them, but them, because of our stupidity, because they don’ t have the brains to ﬁght us on
this side. I feel that if we all die here we’ ll be nothing more than shells and the planet is doomed to
implode. Just like any other race, the only thing to do is to go with them, destroy the entire planet,
and we won’ t be there, we’ ll be gone, the only other species on this planet, we’ ll be the only race
alive, except for us. I don’ t know if it is better for them to simply leave us, we’ ll be dead, they’ re
just the ones that are killing themselves. I wish they would stop us, they’ d kill us, they’ re not even
smart enough to put up a ﬁght, we’ ll be dead, the only species in this universe that has left, is us.
Table 6: More generation examples on test dataset of WritingPrompts.

Prompt 1:
[ WP ] The root password for the universe is discovered.
Target Story
——————————————————————————-
The password wasn’t so simple. It required a sequence of well-timed thoughts, followed a series of
moods, and then a frequency inaudible to the human ear.
And yet, I cracked it. The possibilities opened up before me, vast and terrifying in all their
permutations. The longer I stared into the truth, the reasons, the very secrets behind everything,
the more I realized that neither I nor anyone else could properly harness the immediacy of all
knowledge.
Ignorance wasn’t necessarily bliss, but there was a joy in gradual discovery. Humanity was meant
to play this game without cheat codes. I changed the password before wiping my memory clean.
Prompt 2:
[ WP ] Long Ago, God sent his son Jesus to Earth. Now, Satan sends his daughter, Jessica...
Generated Story
——————————————————————————-
**The Root password for the universe is discovered. **
So I think my story really is unique because when I ﬁrst started writing, it was a little hard to work
up the courage to ask what it was that made the universe happen. I don ’ t know why, but I was
trying to ﬁgure out what it really is.
I guess it was something I heard in school. I ’ ve heard it all my life: how it is the real reason we
are being built into the world.
“ I’ m God. ” I told myself as I had a hard time believing it.
Anyway, I ’ m a god now. I am alone and I believe in myself because I know the universe is about
to change.
Table 7: Generation examples on test dataset of WritingPrompts with two prompts in control experiment.

Prompt:
Bloodthirsty: One Nation Under Water (novel)
Target Story
——————————————————————————-
The story begins with Coast Guard rescue swimmer Virgil LaFleur as he rescues victims of Hurricane
Katrina from their rooftops in New Orleans in the aftermath of the infamous ﬂood of 2005. When Virgil’s
younger brother, Trey, informs him that their parents have not checked in at the evacuee station at Louis
Armstrong airport, Virgil gets his Coast Guard pilot to take him to his parents’ ﬂooded neighborhood. There,
Virgil swims through his parents’ submerged home to ﬁnd his parents stuck in their attic - his father already
deceased and his mother near death. Virgil attempts to swim his mother out through the ﬂooded house, but
nearly drowns and is pulled from the water by Trey and another Coast Guard crew member.
Ten years later, the comic recaps that Virgil couldn’t go near the water after the incident, and is now working
as a ditch digger after having put his younger brother Trey through college (fulﬁlling a promise he made to
his dying mother). A new storm - Rose - is on the horizon, and Virgil is planning to evacuate New Orleans
forever. He begs Trey to go with him, but Trey - now a scientist - is in the middle of an exciting - and
secretive - longevity project at Wolﬁnger Biomed, one of many companies owned by local carpetbagger
Simon Wolﬁnger. Wolﬁnger is portrayed as a disaster capitalist who - along with partner-in-crime Howard
Lawrence - preyed upon the city’s weakened post-disaster status to enrich themselves through greed and
corruption. When Trey dies in a ﬁre at Wolﬁnger Biomed, Virgil has reason to suspect that his brother was
murdered.
Having nothing left to live for, Virgil decides to investigate his brother’s death, even if that means staying
in New Orleans long enough to die in the coming hurricane. What he uncovers is an underground cabal of
hemovores - ”an organism that ingests blood as the main part of its diet”. Unlike vampires, the hemovores
in ”Bloodthirsty” are not supernatural beings; they are humans with a mutated gene that controls longevity.
Their bodies are in a constant state of repair, which deprives their cells of oxygen. In order to compensate for
this loss, they must consume fresh (oxygenated) human blood daily or they will die. Virgil learns that Trey
had unwittingly taken part in a plan to synthesize the hemovore mutation, creating a serum that Wolﬁnger
intends to sell on the black market to anyone wealthy enough to procure its fountain-of-youth-like properties.
With only hours left before Hurricane Rose makes landfall in New Orleans, Virgil faces an uphill battle to
stop Wolﬁnger from spreading the serum and killing thousands of innocent people in the process.
Generation
——————————————————————————-
While trying to avoid shooting down a submarine that sank under water, a boy from that small school appears
and explains the situation to the boy. A brief ﬁght ensues, which is eventually led to the boy’s being eaten by
an insect-like creature. The boy then kills it and takes refuge in the same submarine that sank, only to have it
reanimated.
While the boy is unconscious he is taken to a veterinary hospital, where he attempts to convince the doctor
that he is a reptile, but he is unable to talk. A biologist comes to his rescue, but he is unable to see anything.
He goes to the psychiatrist, who informs him that the specimen of some unknown species is in the aquarium,
and that he is suffering from a mental health problem.
After a short wait in the hospital, the doctor is able to draw a connection between the creature he saw earlier
and the creature he saw earlier, and they begin to build their own underwater lab. The doctor is told that he
must take the creature back to the submarine, and so he decides to use the creature’s memories to locate the
creature, but only to ﬁnd that it is not actually named. After the creature rescues the boy, the biologist helps
him to open the laboratory and create a chamber ﬁlled with spores and insects.
As the spores grow, the biologist states that he is making a discovery that will change his body forever and
this is a revelation to the boy. He goes to another lab, and the biologist attempts to convince him to kill the
creature. When the biologist tells him that he cannot kill him because of his size, the scientist agrees to
euthanize him.
Table 8: Generation examples on test dataset of WikiPlots.

Prompt:
Writers (TV series)
Target Story
——————————————————————————-
Emmett Shelby is a struggling young writer who, during a bout of writer’s block, encounters fellow writer
Jess Spencer who is suffering from the same condition. The two spend a day together, subsequently inspiring
Emmett to continue writing. A year later, Emmett’s successful twin sister Daisy is promoting her latest
children’s book when she runs into Jess, who happens to be an old school friend of hers. Daisy invites
Jess to move in with her, setting the events of the series in motion. Daisy shares her home with two others
- her boyfriend Barney Fields, and ﬂamboyant landlord Hugh Darvill. Barney is an illustrator, providing
the artwork to Daisy’s bestselling children’s novels. Hugh, on the other hand, is a failed erotic novelist,
specialising in gay erotica aimed at children. Emmett still lives with his wealthy, controlling parents - whose
grasp Daisy was able to escape some years prior - but ends up moving in with the others when they throw
him out in the second episode of season one. Outside of the central group is Jenny Sinclair, Daisy’s eccentric
agent who bares an intense unrequited attraction towards Emmett.
In the second season, Parker James III is introduced as an old friend of Daisy and Emmett’s, and a new
romantic interest for Jenny. Although the group are different, they quickly become close friends, and
something akin to a family. The central arc of the series revolves around the budding romantic relationship
between Jess and Emmett, which is hindered both by the arrival of Emmett’s irritating girlfriend Clarice and
Emmett’s own cluelessness to his feelings. After sleeping together in the season one ﬁnale, Jess and Emmett
ﬁnally begin a relationship together.
Generation
——————————————————————————-
Set in London in the 1930s, the story follows the adventures of the main character, Daniel, as he tries to
make his way in England from London to the East End, learning to write, with his ﬂamboyant landlord Hugh
Darvill. Despite the stress that he will have to cope with alone, Daniel is very determined to change his life.
His old school friend, gay Erotica aimed to write a history of the city in which gay characters lived their
lives, but Daniel ﬁnds himself falling for Erotica. Initially, Daniel is determined to return to England to write
an article on socialite Christina, but eventually, he chooses to go back to her for ﬁnancial reasons and ﬁnds a
job in the English dubbing service in England. When Daniel’s brother Peter moves in next door to provide a
ﬂat for Daniel, Peter is intrigued by his idea. Together they are initially attracted to each other, but the love
that develops between them is also very strong. As Daniel looks for a new ﬂat, he gets a chance to get into an
old school, but ﬁnds it to be a run-down, poor working-class establishment. There he meets and befriends
two young women, lesbian Jenny, and lesbian Jenny’s best friend, lesbian Rose, who become his new lovers,
and their ﬂamboyant boyfriend, gay Erotica.
Jenny and Rose were sent to England as a group to write an article in the newspaper, but due to the anti-
homosexual atmosphere that developed there, they were sent to live in a local brothel. The series revolves
around their stories and Daniel’s attempts to get involved in their lives. In one episode, Jenny has to take a
class in the arts because of her dislike of acting in public and her dislike of acting in a private capacity, while
Daniel does not seem to ﬁnd her suitable. The love between Jenny and Jenny’s boyfriend, Parker James III,
gradually builds to a point where they both become close friends.
Table 9: More generation examples on test dataset of WikiPlots.

Prompt:
Mrittupuri: Kill Zone
Target Story
——————————————————————————-
Sydney is a dream city for many immigrants, but also it is an ideal place for many international crime lords.
Gangsters from Bangladesh, China, Lebanon, Russia have all moved to Australia, to take a piece of the pie,
and try to build empire. Among them is the rising Bangladeshi gang Double Rajan Rahman (Rahmatullah),
the kingpin of Bangladeshi gang is not happy with the way his elder son, Rajib Rahman (Saifullah Sadi) and
his crime associate Raja (Ariﬁn Shuvoo), run the business. The situation comes to a head after the timely
death of Irish Mob boss’ son Jimmy, which prompts Rajib to leave the country. Raja maintains low proﬁle
during this period, and in the process falls in love with bar dancer Layla (Prosun Azad). Upon Rajib’s return,
he warns Raja not to get involved with Layla, as he perceives Layla to be his own property.
Raja manages to divert Rajib’s mind away, with plans to do business with Lengra Kashem (Taskeen Rahman),
a mercenary from Bangladesh. Rajib and Sabbir, his youngest brother, meet Lengra and promise him the
East coast territory, leading to their business partnership to become a booming new empire.
Eventually, Rahman and his crew fall through on their deal, and remove Lengra from the partnership. Lengra
decides to avenge this deception, waiting for the right time to strike.
Generation
——————————————————————————-
Mridupuri is set at a very dark time. The plot revolves around Karan (Ariﬁn Shuvoo), run by his boss,
Ravi Chatterjee, an underworld don in Mumbai. His boss is a serial killer who abducts the daughter of his
colleague, Sivarasu (Baba Menon) in order to acquire more money for his wife’s property in a jewel-like
compound in Madras. After killing a lot of people in Madras, the mother of the kidnapped daughter is killed
and the daughter is dragged to the same prison. In the prison the killer enters and kills all the guards while
Karan escapes. It is revealed that the killer has come to kill Karan because of a personal vendetta and Karan
wants to kill Karan for his daughter’s sake. Karan is now very wealthy and wants to make a big proﬁt in the
diamond business and will soon be able to marry Sivarasu, a daughter of the murdered boss. Ravi Chatterjee,
the crime associate Raja (Ajay Devgan) and his partner Shekhar (Lalitha Rajendran) are working hard to ﬁnd
the identity of the killer who is taking the daughter of the missing girl and use it against him. It is revealed
that Karan’s father, Ashokan (Ariﬁn Shakuntala), is an underworld don who is working for the police. Raja
is a well-respected and respected don who would like to marry Sivarasu so he would not kill Karan. Sivarasu
wants to marry Sivarasu who is now in love with Ashokan and his ﬁancee Radha (Meena Devi) has already
agreed to that. Raja maintains low proﬁle of the murders and Karan does not get involved in it. The ﬁlm ends
with Sivarasu and Ravi Chatterjee in jail together.
Table 10: More generation examples on test dataset of WikiPlots.

Prompt 1:
Bloodthirsty: One Nation Under Water (novel)
Target Story
——————————————————————————-
The story begins with Coast Guard rescue swimmer Virgil LaFleur as he rescues victims of Hurricane
Katrina from their rooftops in New Orleans in the aftermath of the infamous ﬂood of 2005. When Virgil’s
younger brother, Trey, informs him that their parents have not checked in at the evacuee station at Louis
Armstrong airport, Virgil gets his Coast Guard pilot to take him to his parents’ ﬂooded neighborhood.
There, Virgil swims through his parents’ submerged home to ﬁnd his parents stuck in their attic - his
father already deceased and his mother near death. Virgil attempts to swim his mother out through the
ﬂooded house, but nearly drowns and is pulled from the water by Trey and another Coast Guard crew
member.
Ten years later, the comic recaps that Virgil couldn’t go near the water after the incident, and is now
working as a ditch digger after having put his younger brother Trey through college (fulﬁlling a promise
he made to his dying mother). A new storm - Rose - is on the horizon, and Virgil is planning to evacuate
New Orleans forever. He begs Trey to go with him, but Trey - now a scientist - is in the middle of an
exciting - and secretive - longevity project at Wolﬁnger Biomed, one of many companies owned by
local carpetbagger Simon Wolﬁnger. Wolﬁnger is portrayed as a disaster capitalist who - along with
partner-in-crime Howard Lawrence - preyed upon the city’s weakened post-disaster status to enrich
themselves through greed and corruption. When Trey dies in a ﬁre at Wolﬁnger Biomed, Virgil has
reason to suspect that his brother was murdered.
Having nothing left to live for, Virgil decides to investigate his brother’s death, even if that means
staying in New Orleans long enough to die in the coming hurricane. What he uncovers is an underground
cabal of hemovores - ”an organism that ingests blood as the main part of its diet”. Unlike vampires, the
hemovores in ”Bloodthirsty” are not supernatural beings; they are humans with a mutated gene that
controls longevity. Their bodies are in a constant state of repair, which deprives their cells of oxygen. In
order to compensate for this loss, they must consume fresh (oxygenated) human blood daily or they will
die. Virgil learns that Trey had unwittingly taken part in a plan to synthesize the hemovore mutation,
creating a serum that Wolﬁnger intends to sell on the black market to anyone wealthy enough to procure
its fountain-of-youth-like properties. With only hours left before Hurricane Rose makes landfall in New
Orleans, Virgil faces an uphill battle to stop Wolﬁnger from spreading the serum and killing thousands
of innocent people in the process.
Prompt 2:
Queen of the Sea (ﬁlm)
Generated Story
——————————————————————————-
The ﬁlm tells the story of three high school students, James, his wife, and a 15-year-old girl, Lisa, at
the end of the ﬁlm, who run into the sea in an old ﬁshing boat. They are captured by a ﬁshing boat,
who sends them underwater, with the intention of causing the boat to fall. The parents soon come to
understand that the school swimmer may be the true killer, and so decide to bring him to school.
The teacher, a teacher who has worked on the case for ﬁfteen years, recognizes the ﬁsh and, on arriving,
tries to convince him to let him swim. However, when it reaches the beach it’s discovered that it’s not
Lisa. At ﬁrst it seems to just be a ﬁsh. But when Lisa eventually notices it, she realizes that it is actually
a human. Eventually, the ﬁsh becomes an outcast, and the girl, Lisa, and James start to try to convince
them to let it swim. James gets drunk and runs out of the boat and ﬁnds himself covered in mud. He and
Lisa are rescued by the teacher, who tells them that they need to swim for a chance at saving the girl’s
life.
Table 11: Generation examples on test dataset of WikiPlots with two prompts in control experiment.

