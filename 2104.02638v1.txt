Comparing Transfer and Meta Learning Approaches on
a Uniﬁed Few-Shot Classiﬁcation Benchmark
Vincent Dumoulin * 1 Neil Houlsby * 1 Utku Evci 1 Xiaohua Zhai 1 Ross Goroshin 1 Sylvain Gelly 1
Hugo Larochelle 1
Abstract
Meta and transfer learning are two successful fam-
ilies of approaches to few-shot learning. Despite
highly related goals, state-of-the-art advances in
each family are measured largely in isolation of
each other. As a result of diverging evaluation
norms, a direct or thorough comparison of dif-
ferent approaches is challenging. To bridge this
gap, we perform a cross-family study of the best
transfer and meta learners on both a large-scale
meta-learning benchmark (Meta-Dataset, MD),
and a transfer learning benchmark (Visual Task
Adaptation Benchmark, VTAB). We ﬁnd that, on
average, large-scale transfer methods (Big Trans-
fer, BiT) outperform competing approaches on
MD, even when trained only on ImageNet. In
contrast, meta-learning approaches struggle to
compete on VTAB when trained and validated
on MD. However, BiT is not without limitations,
and pushing for scale does not improve perfor-
mance on highly out-of-distribution MD tasks. In
performing this study, we reveal a number of dis-
crepancies in evaluation norms and study some of
these in light of the performance gap. We hope
that this work facilitates sharing of insights from
each community, and accelerates progress on few-
shot learning.
1. Introduction
Few-shot learning — the ability to learn from a limited num-
ber of training examples — is a challenge that has received a
lot of attention from the machine learning research commu-
nity in the past few years (see Wang et al., 2020 for a recent
survey). We do not yet have an algorithm that can match the
human ability to acquire diverse new concepts from very
few examples, rather than from orders of magnitude more
training data (Lake et al., 2015). From a practical perspec-
*Equal contribution
1Google Research, Brain Team. Corre-
spondence to: Vincent Dumoulin <vdumoulin@google.com>.
tive, data collection and labeling is often time-consuming or
expensive, and as a result, not all learning problems afford
large quantities of training data.
Few-shot learning approaches can be grouped into two main
categories: transfer learning and meta-learning1. For trans-
fer learning, a model is ﬁrstly pre-trained on an “upstream”
dataset (e.g. ImageNet (Deng et al., 2009)), and later ﬁne-
tuned on different downstream tasks. Transfer learning
approaches (Pan & Yang, 2009) are best exempliﬁed when
less downstream data is available. Typical downstream tasks
have thousands or more training examples, but transfer may
in principle be applied to few-shot classiﬁcation.
Meta-learning may also be used to solve few-shot classiﬁ-
cation problems. Instead of relying on a hand-designed
algorithm to transfer pre-trained representations to new
tasks, meta-learning (i.e.
“learning to learn”) attempts
to discover a learning algorithm which yields good gen-
eralization (Schmidhuber, 1987; Hospedales et al., 2020).
Meta-learning seeks an “algorithmic solution” to few shot
learning, and does not place great emphasis on the data and
architectures to train them. In contrast, transfer learning
approaches tend to focus on learning representations using
simple algorithms (supervised learning and ﬁne-tuning), and
focus more on the data source, architectures, and scale.
The existence of these different subﬁelds, each with their
standardized evaluation protocols, means that practical
knowledge on how to learn from few labeled examples can
sometimes be fragmented. Recent advances in transfer learn-
ing and meta-learning are not directly comparable if they
are evaluated in different ways, which limits the adoption
of best practices.
In order to bridge this gap, we use a few-shot classiﬁcation
evaluation protocol that can be adopted by both transfer
learning and meta-learning to facilitate “apples-to-apples”
comparisons between recent advances. To offer a low bar-
rier of entry and leverage prior work, we combine the Visual
1We use this categorization for convenience and simplicity
in writing. However we highlight that an alternative considera-
tion could view meta-learning as belonging to transfer learning
approaches, as they indeed can be used to model forms of transfer.
arXiv:2104.02638v1  [cs.LG]  6 Apr 2021

Comparing Transfer and Meta Learning Approaches on a Uniﬁed Few-Shot Classiﬁcation Benchmark
Task Adaptation Benchmark (VTAB) (Zhai et al., 2019)2
and Meta-Dataset (MD) (Triantaﬁllou et al., 2020)3 — two
comprehensive few-shot classiﬁcation benchmarks recently
introduced in the transfer learning and few-shot classiﬁca-
tion literature, respectively — into an evaluation protocol
which we refer to as VTAB+MD. With this, we can verify
whether advances in one ﬁeld transfer across benchmarks,
and can test overﬁtting to a particular benchmark. Our main
contributions are:
1. We bring together two challenging transfer learning
and few-shot classiﬁcation benchmarks and perform a
large-scale study on several competitive few-shot clas-
siﬁcation approaches from both research communities.
We establish BiT-L (Kolesnikov et al., 2020) as SOTA
on this uniﬁed evaluation protocol, and show that com-
petitive approaches on the MD benchmark struggle to
outperform transfer learning on VTAB.
2. We carefully study the impact of different aspects of the
BiT model formulation (network scale, data, normal-
ization layer choice, and resolution). Beyond showing
aggregate beneﬁts on MD learning episodes, coher-
ent with observations in (Kolesnikov et al., 2020), we
demonstrate that not all effects are consistent across all
of MD’s sources of test tasks. In particular, we iden-
tify Omniglot and QuickDraw as two data sources for
which BiT-L does no better than competing approaches
despite being signiﬁcantly larger both in terms of data
and architecture size.
3. We show that despite recent advances in cross-domain
few-shot classiﬁcation, meta-learning approaches still
struggle to generalize to test tasks that are signiﬁcantly
outside of the training task distribution, as evidenced by
their poor performance on VTAB with respect to com-
parable transfer learning implementations. We identify
adaptability and scale as two promising avenues of
future research to overcome these difﬁculties.
As evidenced by our results comparing transfer learning and
meta-learning approaches on VTAB+MD, the collaboration
across these ﬁelds that the benchmark affords is beneﬁcial
to both research communities, and we hope to facilitate the
sharing of insights and accelerate progress on shared goal
of learning from a limited number of examples.
2. Background and related Work
2.1. Transfer Learning
Transfer learning has long been used to exploit knowledge
obtained on one task to improve performance on another,
2https://github.com/google-research/task_adaptation
3https://github.com/google-research/meta-dataset
typically with less data. In the context of computer vision,
the most popular form of transfer is to initialize a network
with weights obtained by pre-training on ImageNet (Huh
et al., 2016). More recently, transfer from larger datasets has
been shown effective, including 100M Flickr images (Joulin
et al., 2016; Li et al., 2017), JFT with 300M images (Sun
et al., 2017), and 3.5B Instagram images (Mahajan et al.,
2018). Most state-of-the-art methods on image classiﬁca-
tion benchmarks now use some form of transfer learning,
and the best results are obtained by combining large-scale
networks with large pre-training datasets (Kolesnikov et al.,
2020; Xie et al., 2019; Dosovitskiy et al., 2020). Transfer
learning has made a considerable impact in few-shot learn-
ing, most recently in in NLP (Brown et al., 2020) where
very large models have proven successful for learning trans-
fer with few datapoints. In computer vision, learning with
few datapoints is, perhaps, more commonly addressed with
semi-supervised learning (e.g. (Sohn et al., 2020)), how-
ever (Kolesnikov et al., 2020) show that large vision models
transfer well to popular classiﬁcation benchmarks (Ima-
geNet, CIFAR, etc.) and VTAB-1k.
Several recent papers report that well-tuned transfer learn-
ing baselines are competitive with more complex few-shot
classiﬁcation approaches (Chen et al., 2019; Dhillon et al.,
2020; Chen et al., 2020b; Tian et al., 2020). Our work adds
to these observations by applying an established few-shot
classiﬁcation evaluation protocol (Meta-Dataset) to large
scale (both in terms of data and capacity) transfer learners.
Doing so highlights some limitations of episodic approaches
in a new way, and also reveals where transfer learning falls
short.
2.2. Episodic approaches to few-shot classiﬁcation
Few-shot classiﬁcation evaluation proceeds by sampling
learning episodes from a test set of classes: ﬁrst the test
classes are subsampled into an N-way classiﬁcation prob-
lem, then examples of the N sampled test classes are sub-
sampled and partitioned into a k-shot support set (used
to ﬁt the model on k examples per class, for a total of
Nk support examples) and a query set (used to evaluate
the model’s generalization performance on the learning
episode). Meta-learning approaches to few-shot classiﬁca-
tion are usually trained in a way that mimics the evaluation
conditions (called episodic training). Episodes are formed
using a disjoint training set of classes and the meta-learner is
trained in an end-to-end fashion by learning from the support
set, evaluating on the query set, and backpropagating the
loss through the learning procedure. This is hypothesized to
be beneﬁcial to performance on test episodes (Vinyals et al.,
2016), and iconic gradient-based and metric-based meta-
learning approaches such as MAML (Finn et al., 2017) or
Prototypical Networks (Snell et al., 2017) (respectively) are
trained episodically. The recent literature is rich in few-shot

Comparing Transfer and Meta Learning Approaches on a Uniﬁed Few-Shot Classiﬁcation Benchmark
classiﬁers, and an exhaustive survey is beyond the scope of
this paper; see Wang et al. (2020) for an overview.
2.3. Benchmarks
Many visual classiﬁcation benchmarks consist of sin-
gle datasets, e.g.
ImageNet (Deng et al., 2009), CI-
FAR (Krizhevsky, 2009), COCO (Lin et al., 2014), etc.
However, benchmarks with multiple datasets are becoming
more popular. The Visual Decathlon (Rebufﬁet al., 2017)
contains ten classiﬁcation tasks, and focuses on multi-task
learning. The Facebook AI SSL challenge4 contains various
vision tasks (classiﬁcation, detection, etc.) and targets linear
transfer of self-supervised models.
Established episodic evaluation benchmarks range in scale
and domain diversity from Omniglot (Lake et al., 2015) to
mini-ImageNet (Vinyals et al., 2016), CIFAR-FS (Bertinetto
et al., 2019), FC100 (Oreshkin et al., 2018), and tiered-
ImageNet (Ren et al., 2018). Guo et al. (2020) propose
a cross-domain few-shot classiﬁcation evaluation protocol
where learners are trained on mini-ImageNet and evaluated
on episodes sampled from four distinct target domains.
We use VTAB (1k example version) and Meta-Dataset as
representative benchmarks for few-shot classiﬁcation since
they offer the largest domain variety in their respective com-
munities. Furthermore, VTAB and Meta-Dataset have been
used in the development of state-of-the-art transfer learning
and meta-learning methods, respectively.
2.4. Related problems
Domain adaptation (Wang & Deng, 2018) addresses the
problem setting where a large corpus of labeled data is avail-
able for a “source” domain, but the target application’s input
distribution is different (e.g. natural images vs sketches).
In supervised domain adaptation very few labeled samples
are available from the “target” domain. In contrast to meta-
learning, there is usually only one target domain and the
class (label) distribution is usually assumed to be the same
between the source and target domains.
Low-shot classiﬁcation (Thrun, 1996) is interested in clas-
siﬁcation problems for which lots of training examples are
available for a “base” set of classes and knowledge about
“novel” classes is integrated incrementally and with a limited
number of training examples.
While low-shot classiﬁcation and domain adaptation are
very relevant to real-world applications and are also impor-
tant components of humans’ learning ability, for the purpose
of this work we concentrate on few-shot classiﬁcation prob-
lems for which the sets of training and test tasks do not
4https://sites.google.com/corp/view/
fb-ssl-challenge-iccv19/home
overlap in terms of image classes.
2.5. Evaluated approaches
In this work we evaluate existing approaches from the trans-
fer learning and meta-learning literature. The main transfer
learning algorithm we consider is the recent Big Trans-
fer (Kolesnikov et al., 2020). This algorithm attains near
state-of-the-art performance on VTAB, as well as a number
of other benchmark image classiﬁcation datasets such as
ImageNet (Deng et al., 2009), CIFAR-10/100 (Krizhevsky,
2009), Oxford-IIIT Pets (Parkhi et al., 2012), and Flowers-
102 (Nilsback & Zisserman, 2008).
We also consider recent SOTA approaches on Meta-Dataset:
SUR (Dvornik et al., 2020), which is trained on multiple
training sources, and CrossTransformers (Doersch et al.,
2020), which is trained only on ImageNet. We also in-
clude representatives of metric-based and gradient-based
meta-learning approaches: Prototypical Networks (Snell
et al., 2017) and ProtoMAML (Triantaﬁllou et al., 2020),
respectively.
Prototypical Networks (Snell et al., 2017) learn a represen-
tation (via episodic training) for which a Gaussian classiﬁer
with an identity covariance matrix performs well. For any
given episode, the support embeddings of each class are av-
eraged into prototypes, and the classiﬁer logits are computed
as the “query-embedding to prototype” Euclidean distances.
ProtoMAML (Triantaﬁllou et al., 2020) is a variant of
MAML (Finn et al., 2017) (also trained episodically) which
initializes the output layer weights and biases in a way that
is equivalent to Prototypical Network’s Gaussian classiﬁer.
During training, the optimization loop on the support set is
unrolled, the query loss computed at the end is backpropa-
gated through the optimization loop to update the trainable
initialization parameters. Note that ProtoMAML uses the
ﬁrst-order variant of MAML, which ignores second-order
derivatives to save on computation and memory.
SUR (Dvornik et al., 2020) trains separate feature extractors
for each of MD’s training sources via supervised learning.
To make a prediction for a test episode, the model con-
structs a representation by concatenating the modulated
embeddings of each backbone and then optimizes the sig-
moidal modulation coefﬁcients (one per feature extractor)
to minimize a nearest-centroid loss (computed using the
cosine similarity) on the support set and its corresponding
class centroids. Query examples are then classiﬁed based
on their cosine similarity with these class centroids, in the
modulated and concatenated embedding space.
CrossTransformers (Doersch et al., 2020) improves on
centroid-based few-shot classiﬁcation approaches by intro-
ducing a Transformer-based (Vaswani et al., 2017) com-
ponent which replaces the feature extractor’s ﬁnal global

Comparing Transfer and Meta Learning Approaches on a Uniﬁed Few-Shot Classiﬁcation Benchmark
pooling operation and whose purpose is to build class pro-
totypes which are query-aligned and spatially aware. The
paper also introduces an auxiliary self-supervised task which
reformulates SimCLR (Chen et al., 2020a)’s contrastive in-
stance discrimination task into an episodic learning problem
(called SimCLR episodes).
Big Transfer (BiT) (Kolesnikov et al., 2020) consists of pre-
trained weights and a transfer learning protocol. BiT models
are based on ResNet-v2, except that batch normalization
layers are replaced with group normalization, and weight
standardization is applied. BiT models are pre-trained on
datasets of different sizes: The ILSVRC-2012 ImageNet
datasets (1.3M images) “BiT-S”, the full ImageNet-21k
dataset (13M images) (Deng et al., 2009) “BiT-M”, or JFT-
300M (300M images) (Sun et al., 2017) “BiT-L”.
MD-Transfer refers to the transfer learning baseline used in
(Triantaﬁllou et al., 2020). In contrast to BiT, it (1) uses the
entire episode when calculating gradients,5 (2) uses batch
normalization, (3) does validation on MD-v2 for model se-
lection, (4) ﬁne-tunes using the Adam optimizer, a constant
learning rate of 0.01, and 100 parameter updates, and (5)
uses a cosine classiﬁer head. Note: (4) and (5) were selected
based on the accuracy on MD-v2 validation episodes.
3. Unifying VTAB and Meta-Dataset
We start by describing VTAB and Meta-Dataset, both of
which evaluate on tasks with limited training data. Note
that each benchmark use slightly different terminology. The
tasks that can be used for learning prior to evaluation are
referred to as upstream tasks in VTAB and training tasks in
MD. Similarly, tasks on which evaluation performance is re-
ported are referred to as downstream and test tasks by VTAB
and MD, respectively. Since each test task itself contains
training and test examples, MD refers to these as support
and query sets. To avoid confusion, when appropriate, we
will prefer MD’s nomenclature
VTAB features 19 evaluation tasks which can be grouped
into “natural”, “structured”, and “specialized” sets of tasks.
Each task corresponds to an existing classiﬁcation prob-
lem (e.g. CIFAR100) or one converted into classiﬁcation
(e.g. DMLab). For the VTAB-1k variant (that we use in
VTAB+MD), the support set is constructed by taking the
original problem’s training set and randomly subsampling
1000 examples. The performance on the task is then mea-
sured as the average accuracy on a query set which consists
of the original problem’s entire test set. VTAB allows a
model to be trained or validated on any dataset except the
5When data augmentation is used, resulting images are not
re-sampled for different batches. In contrast BIT uses a ﬁxed batch
size of 512 images, which can include two different augmented
versions of the same image.
19 evaluation tasks, and it does not provide validation tasks.
Meta-Dataset features 10 test “sources” (i.e. existing classi-
ﬁcation problems) from which learning episodes are formed
by 1) selecting a source, 2) randomly subsampling classes,
and 3) randomly subsampling examples within the selected
classes that are assigned either to the support set or query
set. Performance is measured as the query accuracy aver-
aged over many (typically 600) test episodes and aggregated
across the 10 test sources. Training and validation sources
are also provided, some of which intersect with the 10 test
sources. For intersecting sources, the classes are partitioned
into training, validation, and test set classes so that the
validation and test classes are never seen during training.
Meta-Dataset also features several datasets whose classes
are never sampled during training or validation, in order to
measure out-of-distribution (OOD) performance.
Conceptually, VTAB and Meta-Dataset can be combined
by either treating the 19 VTAB evaluation tasks as 19 test
episodes (albeit with a larger-than-usual support and query
set), or treating every Meta-Dataset test episode as a evalua-
tion task and grouping the tasks into 10 additional sets of
tasks. This makes it easy for approaches that already evalu-
ate on Meta-Dataset or VTAB to extend their evaluation to
VTAB+MD.
In combining VTAB and Meta-Dataset into VTAB+MD,
we have to resolve certain task/source collisions. This also
provides an opportunity of improving on design choices
previously made for VTAB and Meta-Dataset. In order to
disambiguate between the original VTAB and MD formula-
tions and their VTAB+MD-adapted counterparts, we refer to
the VTAB+MD ones as VTAB-v2 and MD-v2, respectively.
We make the following changes:
• VTAB does not provide a validation set of tasks; we
therefore propose to use Meta-Dataset’s validation
episodes for that purpose.
• Meta-Dataset partitions ImageNet classes into train-
ing, validation, and test sets of classes, which makes
it awkward to leverage pre-trained ImageNet initial-
izations; we therefore choose to treat ImageNet as a
training-only source in MD-v2.
• Finally, VTAB’s Flowers102 and DTD tasks are scat-
tered into training, validation, and test classes in Meta-
Dataset, which we resolve by entirely removing Flow-
ers as a MD-v2 source and removing DTD as a VTAB-
v2 task, respectively.
We report both aggregated and per-dataset accuracies for
VTAB+MD. Aggregated reporting consists of the average
query accuracy for episodes of all MD-v2 test sources and
the average test accuracy for all VTAB-v2 tasks, which

Comparing Transfer and Meta Learning Approaches on a Uniﬁed Few-Shot Classiﬁcation Benchmark
is further decomposed into “natural”, “specialized”, and
“structured” task averages (Figure 1). Detailed reporting
breaks down the accuracies into their individual MD-v2
sources and VTAB-v2 tasks; we provide detailed reporting
ﬁgures and tables in the Appendix.
We allow the use of the following data for upstream training
or meta-training:
1. All of the ImageNet training set.
2. The training sets of classes of the Omniglot, Aircraft,
CU Birds, DTD, QuickDraw, and Fungi datasets as
deﬁned by MD-v2.
3. Any dataset whose images do not overlap with
VTAB+MD’s evaluation images.
The use of any subset of the above choices therefore ensures
no overlap with data used by test tasks. For example, the use
of choices 1 and 2 above will be referred to as all MD-v2
sources in our experiments.
4. Experiments
We begin by evaluating all approaches on VTAB+MD, fol-
lowing closely the prescriptions in their respective papers,
in an effort to answer the question: How would current
approaches fare in a direct comparison?
Practices differ between transfer learning and few-shot clas-
siﬁcation evaluation. Few-shot classiﬁcation benchmarks
tend to standardize around a restricted set of input reso-
lutions (84 × 84, 126 × 126) and network architectures
(four-layer CNN, ResNet-18, etc.). Episodic training also
imposes restrictions on input resolution and network capac-
ity, since the batch size is determined by an episode’s ways
and shots and the support set cannot be trivially sharded into
independent batches and distributed across multiple accel-
erators. This is especially true for large-scale benchmarks
such as Meta-Dataset, where support sets can contain up
to 500 examples. This makes it difﬁcult to scale up meta-
learners; one notable effort is the CrossTransformer model,
which trains a ResNet-34 architecture on 224 × 224 inputs
using a customized multi-GPU implementation. Transfer
learning benchmarks on the other hand typically train at
224 × 224 (and may evaluate at even higher resolution), and
routinely use network architectures in the ResNet-50 scale
and beyond. We summarize some of these high level details
and differences here:
• For BiT we use the ResNet-101x3 architecture trained
on JFT (“BiT-L-R101x3”).6 This model is trained and
6The BiT paper also presents an even larger ResNet-152x4,
however we limit to the ResNet-101x3 to speed up experiments
evaluated at 224 × 224. While increasing resolution
during transfer is recommended (Touvron et al., 2019),
we match the pre-training and test resolutions to match
the other methods.
• In accordance with the practice established in Meta-
Dataset, MD-Transfer, ProtoMAML, and ProtoNets
are initialized from a ResNet-18 classiﬁer trained on
ImageNet at 126 × 126. They are then further trained
(episodically for ProtoMAML and ProtoNets) on either
ImageNet or all MD-v2 training sources.
• CTX (CrossTransformers) trains a ResNet-34 architec-
ture from scratch on 224 × 224 ImageNet episodes as
well as SimCLR episodes.
• SUR reuses the 84 × 84 ResNet-18 backbones pro-
vided by the paper authors, with two key differences:
(1) we re-train the ImageNet backbone using the entire
ImageNet dataset using the recommended hyperparam-
eters, and (2) we remove the Flowers backbone, since
Flowers is an evaluation task in VTAB+MD.
Additional implementation details are provided in the Ap-
pendix. The differences in performance will undoubtedly be
inﬂuenced by design decisions informed by each approach’s
original evaluation setting, which we investigate through
ablations on BiT-L (subsection 4.2).
All non-BiT learning approaches and baselines considered
in this work perform model selection on MD-v2 validation
episodes using Triantaﬁllou et al. (2020)’s hyperparameter
search space (detailed in the Appendix, along with the best
values found).
For BiT, we follow hyperparameter selection strategies simi-
lar to previous works. For MD-v2 we use the transfer heuris-
tic suggested in Kolesnikov et al. (2020): 500 steps of SGD
with learning rate 0.003, momentum 0.9. However, instead
of the recommended task-dependent image resolutions, we
use a ﬁxed resolution of 224 × 224 since other methods all
use constant resolution. For VTAB-v2, we use the same
optimizer but with a small hyperparameter sweep suggested
in Zhai et al. (2019) over the product of {2.5k, 10k} steps
and learning rate {0.01, 0.001}. We train on the VTAB
recommended 800 training example splits, select the single
hyperparameter with the best average performance across
tasks on the 200 example validation splits, and evaluate that
setting on the test sets. Therefore, for each of VTAB and
MD, each model uses a single set of hyperparameters for all
tasks.
which run on many episodes, and it R101x3 large enough to demon-
strate the effect of scale.

Comparing Transfer and Meta Learning Approaches on a Uniﬁed Few-Shot Classiﬁcation Benchmark
VTAB-v2
(all)
VTAB-v2
(natural)
VTAB-v2
(specialized)
VTAB-v2
(structured)
MD-v2
Task / Source
40
60
80
100
Accuracy
MD-Transfer (ImageNet-only)
ProtoMAML (ImageNet-only)
ProtoNets (ImageNet-only)
CTX (ImageNet-only)
BiT-ResNet-101x3 (ImageNet-only)
BiT-ResNet-18 (ImageNet-only)
VTAB-v2
(all)
VTAB-v2
(natural)
VTAB-v2
(specialized)
VTAB-v2
(structured)
MD-v2
Task / Source
40
60
80
100
Accuracy
MD-Transfer (all MD-v2 sources)
ProtoMAML (all MD-v2 sources)
ProtoNets (all MD-v2 sources)
SUR (all MD-v2 sources)
BiT-ResNet-101x3 (JFT)
Figure 1. VTAB-v2 and MD-v2 aggregated accuracies for approaches trained only on ImageNet (left) or larger-scale datasets (right).
BiT-L (ResNet-101x3) emerges as SOTA, both in the ImageNet-only setting and when using larger-scale datasets.
Omniglot
Aircraft
Birds
Textures
QuickDraw
Fungi
Traffic Sign
MSCOCO
Average
Source
40
60
80
100
Accuracy
BiT-ResNet-18 (ImageNet-only)
MD-Transfer (ImageNet-only)
MD-Transfer (all MD-v2 sources)
Figure 2. Despite identical network architectures (ResNet-18) and
input resolutions (126×126), transfer learner implementations from
the transfer learning (BiT-ResNet-18) or few-shot classiﬁcation
(MD-Transfer) communities exhibit different performance proﬁles.
Omniglot
Aircraft
Birds
Textures
QuickDraw
Fungi
Traffic Sign
MSCOCO
Average
Source
60
80
100
Accuracy
BiT-ResNet-18 (126x126)
BiT-ResNet-18 (224x224)
BiT-ResNet-50 (126x126)
BiT-ResNet-50 (224x224)
CTX
Figure 3. Scaling up the resolution and network capacity con-
tributes to BiT’s success on MD-v2, but not across all test sources.
For Omniglot and QuickDraw a higher resolution decreases per-
formance for larger-capacity networks. All models are trained on
ImageNet. CTX accuracies are shown for reference.
4.1. Comparison of selected approaches
BiT-L achieves SOTA
BiT-L (trained on ImageNet/JFT)
emerges as the overall best-performing approach on
VTAB+MD, outperforming other approaches by at least
3.5/7.8% and 10.4/14.4% on MD-v2 and VTAB-v2, respec-
tively (Figure 1; see the Appendix for tables summariz-
ing the contents of all ﬁgures presented in the main text).
This is consistent with existing few-shot classiﬁcation work
which shows that “baseline” transfer learners beneﬁt from
scaling up the input architecture (Chen et al., 2019) and
the upstream dataset (Dhillon et al., 2020). As reported
by Kolesnikov et al. (2020) on standard transfer datasets
(CIFAR-10, Oxford Pets, etc.), increasing network capacity
even further does not appear to show clear signs of overﬁt-
ting on tasks for which there is little training data available;
our results show that the observation also holds on MD-v2,
whose learning episode sampling procedure allows for even
smaller data regimes. This highlights one of the disadvan-
tages that episodic approaches face: scaling them up is a
signiﬁcantly harder engineering challenge. This doesn’t pre-
clude the possibility that other approaches trained on JFT
using a ResNet-101x3 network architecture would perform
as well as (or even better than) BiT-L, but it is a hypothetical
setting that is out of reach for most of the existing implemen-
tations. In the Appendix we make a ﬁrst attempt to scale up
SUR’s backbones to ResNet-50 trained on 224 × 224 im-
ages. This yields an overall 5% improvement on VTAB-v2,
but a marginal improvement on MD-v2 (< 1%).
Meta-learning performance suffers on VTAB-v2
In
contrast to BiT, Figure 1 shows that meta-learning ap-
proaches struggle to compete with transfer learning on
VTAB-v2.
MD-Transfer outperforms MD-v2’s meta-
learning champions (CTX, SUR), with the exception of CTX
on VTAB-v2’s natural tasks. A scaled-down ResNet-18
variant of BiT trained on 126 × 126 inputs (yellow column)
consistently outperforms CTX and SUR. This is consistent
with Chen et al. (2019)’s observation that meta-learning ap-
proaches may be competitive on tasks derived from classes
similar to those used in training but struggle with cross-
dataset generalization. This is especially noticeable for
SUR, which underperforms CTX on VTAB-v2 despite hav-
ing been trained on more datasets. This represents an oppor-

Comparing Transfer and Meta Learning Approaches on a Uniﬁed Few-Shot Classiﬁcation Benchmark
Omniglot
Aircraft
Birds
Textures
QuickDraw
Fungi
Traffic Sign
MSCOCO
Average
Source
60
80
100
Accuracy
BiT-ResNet-101x3 (ImageNet)
BiT-ResNet-101x3 (ImageNet-21k)
BiT-ResNet-101x3 (JFT)
CTX (ImageNet)
SUR (all MD-v2 sources)
Figure 4. The scale of the upstream task contributes to BiT-L’s
success on MD-v2, but not necessarily monotonically and not across
all test sources. On Trafﬁc Sign, performance decreases with the
scale of the upstream task. All models are trained with 224 × 224
inputs. CTX and SUR accuracies are shown for reference.
Omniglot
Aircraft
Birds
Textures
QuickDraw
Fungi
Traffic Sign
MSCOCO
Average
Source
60
70
80
90
Accuracy
BiT-ResNet-50 (JFT)
BiT-ResNet-50 (JFT, deduplicated)
BiT-ResNet-50 (JFT, class-ablated)
Figure 5. The presence of test image duplicates in JFT is not a
contributing factor to BiT-L’s success on MD-v2, but the presence
of aircraft-, bird-, and fungi-related classes does play a role for their
respective test sources, as evidenced by the drop in performance
when removing those classes from JFT. All models are trained with
224 × 224 inputs.
tunity to apply existing cross-domain few-shot classiﬁcation
approaches (Tseng et al., 2020; Sun et al., 2020; Phoo &
Hariharan, 2020; Liu et al., 2020; Cai & Shen, 2020) at
scale.
ProtoMAML is competitive with transfer learning on the
specialized VTAB-v2 tasks, but less so on the other splits.
The adaptation protocol for both ProtoMAML is very sim-
ilar to ﬁne-tuning used by transfer learning. The main dif-
ferences are in the trained initial weights, and the hyperpa-
rameter selection strategy. ProtoMAML weights are ﬁrst
initialized by ImageNet weights used for the MD-Transfer
baseline. However, during meta-training ProtoMAML uses
very few adaptation steps, and it uses similarly few during
adaptation (see Appendix for details). As a result it seems
that limiting the ability for the model to adapt, even when
the episodes are small, outweighs the reﬁned initialization
weights.
Large-scale transfer is not always a silver bullet
Ex-
amining a per-source performance breakdown for MD-v2
reveals a more nuanced picture: whereas BiT-L outperforms
other approaches on Birds, Textures, and MSCOCO, it un-
derperforms competing approaches on Omniglot and Quick-
Draw despite being signiﬁcantly larger (Figure 4). On those
sources, the beneﬁts of meta-learning — and more generally
of incorporating inductive biases informed by knowledge
of the test distribution of tasks — appear clearer. SUR
performs well on Omniglot and QuickDraw, most likely be-
cause some of its backbones were trained on classes similar
to those used to form test episodes. CTX, which is only
trained on ImageNet classes, outperforms BiT-L trained on
JFT, even in the face of a signiﬁcant capacity and data disad-
vantage. This shows that while success cases of large-scale
transfer learning have been recently highlighted (Kolesnikov
et al., 2020; Dosovitskiy et al., 2020), its failure cases should
be examined and tackled as well, and that recent approaches
to few-shot classiﬁcation can offer insights in that regard.
4.2. Deconstructing BiT-L’s success on MD-v2
The BiT paper (Kolesnikov et al., 2020) established that
large-scale transfer learning performs well on few-shot clas-
siﬁcation tasks, including VTAB-1k evaluation tasks, and
beneﬁts from both larger network architectures and up-
stream datasets. As our results show, these performance
gains are not uniform across MD-v2 test sources. This
raises the following questions: To what extents do speciﬁc
ﬁndings in transfer learning carry over to MD-v2?
Implementation details matter
We scale down BiT-L
to the typical few-shot classiﬁcation regime (ResNet-18,
126 × 126 inputs) in order to control for network archi-
tecture and input resolution. Figure 1 shows that while
transfer learning remains competitive with meta-learning ap-
proaches, SOTA approaches on Meta-Dataset (SUR, CTX)
still achieve the best MD-v2 performance in that regime (al-
though as noted above, their performance degrades severely
on VTAB-v2 tasks). This observation is consistent with re-
cent work which shows that such transfer learning baselines
are competitive, but not optimal, on few-shot classiﬁca-
tion tasks, both on Meta-Dataset (Chen et al., 2020b) and
on smaller benchmarks (Chen et al., 2019; Dhillon et al.,
2020).
Interestingly, the scaled-down BiT model’s performance
proﬁle differs from that of MD-Transfer, despite sharing
the same network capacity and input resolution: it under-
performs on MD-v2’s Omniglot, Aircraft, and Trafﬁc Sign
(Figure 2) but outperforms MD-Transfer on VTAB-v2.
This highlights the fact that several design decisions inﬂu-

Comparing Transfer and Meta Learning Approaches on a Uniﬁed Few-Shot Classiﬁcation Benchmark
ence performance, some of which are seldom discussed in
the literature. For instance, Saikia et al. (2020) reports that
using cross-domain and cross-task data for hyperparame-
ter tuning yields few-shot classiﬁcation improvements in a
cross-domain setting, and Gulrajani & Lopez-Paz (2020)
advocates that the model selection strategy should be con-
sidered as part of the model speciﬁcation when evaluating
domain adaptation approaches. MD-Transfer beneﬁts from
training on multiple MD-v2 sources, however this differ-
ence pales in comparison to the differences introduced by
different hyperparameters in the baselines.
Scale helps, but less so on OOD MD tasks
Figure 3
shows a global trend where increasing the input resolution
and network capacity helps with performance on MD-v2,
but with a few exceptions. Omniglot and QuickDraw are
non-natural, highly out-of-distribution with respect to Ima-
geNet, and contain fairly low resolution images. On these
tasks, increasing capacity and resolution does not have clear
positive effect; in fact, on Omniglot larger models perform
worse. Trafﬁc Sign also contains low resolution images; it
beneﬁts from an increase in resolution, but there is not a
clear trend with respect to network size. Overall, while the
224 × 224 ResNet-50 variant of BiT trained on ImageNet is
able to surpass CTX’s average performance on MD-v2 by
1.69%, it mainly does so by increasing the performance gap
on data sources for which it already outperforms CTX.
BiT-L’s normalization strategy matters
Figure 6 shows
that replacing BiT-L’s group normalization and weight stan-
dardization (GNWS) with batch normalization (BN) de-
grades its performance on MD-v2. This result is remarkably
consistent, and appears on all tasks. Since BN is problematic
for few-shot classiﬁcation (Bronskill et al., 2020), GNWS
shows promise alongside alternatives such as Bronskill et al.
(2020)’s TaskNorm layer.
Sometimes more data is a good solution
BiT-L trained
on JFT is obviously at an advantage in terms of data, but
interestingly Figure 4 shows that the trend is very much
test source-dependent on MD-v2. For Trafﬁc Sign the trend
reverses: BiT-L is better off training on ImageNet than on
ImageNet-21k or JFT.
Overall ImageNet-21k and JFT exhibit similar performance
proﬁles, with two notable exceptions: training on JFT in-
creases performance on Aircraft, and a similar effect is
observed with ImageNet-21k on Fungi. Furthermore, for
some MD-v2 test sources such as Omniglot, QuickDraw and
Trafﬁc Sign BiT-L underperforms CTX even when trained
on a much larger upstream task. This suggests that the ex-
tent to which data scaling helps with performance is highly
dependent on the contents of the dataset itself.
We run two ablations to verify this hypothesis (Figure 5). We
Omniglot
Aircraft
Birds
Textures
QuickDraw
Fungi
Traffic Sign
MSCOCO
Average
Source
60
70
80
90
Accuracy
BiT-ResNet-50 (GNWS)
BiT-ResNet-50 (BN)
Figure 6.
Group normalization and weight standardization
(GNWS) contribute to BiT’s success on MD-v2. Replacing them
with batch normalization (BN) causes performance to degrade
across all sources. Both models are trained on ImageNet with
224 × 224 inputs. The dashed line represents the best performing
meta-learner (CTX)’s average accuracy on MD-v2.
train ResNet-50 BiT models on three variants of JFT: (green)
JFT itself, (orange) JFT deduplicated based on all MD-v2
test sources (∼0.002% of JFT’s training data), and (purple)
JFT where all aircraft-, bird-, and fungi-related classes were
removed (∼3% of JFT’s training data). While the effect of
deduplication is negligible, the removal of classes related
to some of MD-v2’s test sources has a drastic impact on
Aircraft and Birds performance, even if the corresponding
reduction in training data is relatively small. This result is
consistent with our ﬁndings that SUR performs best on tasks
which match its pre-training sources: while individual image
duplicates appear unimportant, domain coverage is, and
large-scale datasets are more likely to cover more domains.
5. Conclusion
We introduce a few-shot classiﬁcation evaluation protocol
called VTAB+MD which aims to facilitate exchanging and
comparing ideas between the transfer learning and few-shot
classiﬁcation communities. Our extensive evaluation of
recent competitive approaches show that a carefully engi-
neered training and ﬁne-tuning of large scale networks (as
exempliﬁed by BiT) is a remarkably competitive and robust
baseline for few-shot classiﬁcation, and that this approach
generalizes across large-scale, multi-dataset benchmarks.
Our investigation highlights interesting avenues for future
research. BiT’s scaling advantage diminishes when moving
to tasks that are extremely out-of-distribution, and lever-
aging information from multiple upstream training tasks
(as exempliﬁed by SUR) may prove beneﬁcial in that re-
spect. Meta-learning approaches are hindered from making
use of large backbones and input resolutions due to engi-
neering/implementation difﬁculties, but we may yet see the
true beneﬁts of meta-learning when these issues have been
overcome.

Comparing Transfer and Meta Learning Approaches on a Uniﬁed Few-Shot Classiﬁcation Benchmark
Acknowledgements
The authors would like to thank Fabian Pedregosa, Carl
Doersch, Eleni Triantaﬁllou, Pascal Lamblin, Lucas Beyer,
Joan Puigcerver, and Cristina Vasconcelos for their invalu-
able help and feedback.
References
Bertinetto, L., Henriques, J. F., Torr, P. H., and Vedaldi, A.
Meta-learning with differentiable closed-form solvers. In
ICLR, 2019.
Bronskill, J., Gordon, J., Requeima, J., Nowozin, S., and
Turner, R. Tasknorm: Rethinking batch normalization for
meta-learning. In ICML. PMLR, 2020.
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,
J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
In NeurIPS, 2020.
Cai, J. and Shen, S. M. Cross-domain few-shot learning
with meta ﬁne-tuning. arXiv preprint arXiv:2005.10544,
2020.
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A
simple framework for contrastive learning of visual rep-
resentations. In ICML. PMLR, 2020a.
Chen, W.-Y., Liu, Y.-C., Kira, Z., Wang, Y.-C. F., and Huang,
J.-B. A closer look at few-shot classiﬁcation. In ICLR,
2019.
Chen, Y., Wang, X., Liu, Z., Xu, H., and Darrell, T. A
new meta-baseline for few-shot learning. arXiv preprint
arXiv:2003.04390, 2020b.
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,
L. Imagenet: A large-scale hierarchical image database.
In CVPR, 2009.
Dhillon, G. S., Chaudhari, P., Ravichandran, A., and Soatto,
S. A baseline for few-shot image classiﬁcation. ICLR,
2020.
Doersch, C., Gupta, A., and Zisserman, A. CrossTrans-
formers: spatially-aware few-shot transfer. In NeurIPS,
2020.
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,
Heigold, G., Gelly, S., et al. An image is worth 16x16
words: Transformers for image recognition at scale. arXiv
preprint arXiv:2010.11929, 2020.
Dvornik, N., Schmid, C., and Mairal, J. Selecting relevant
features from a multi-domain representation for few-shot
classiﬁcation. In ECCV. Springer, 2020.
Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-
learning for fast adaptation of deep networks. In ICML,
2017.
Gulrajani, I. and Lopez-Paz, D. In search of lost domain
generalization. arXiv preprint arXiv:2007.01434, 2020.
Guo, Y., Codella, N. C., Karlinsky, L., Codella, J. V., Smith,
J. R., Saenko, K., Rosing, T., and Feris, R. A broader
study of cross-domain few-shot learning. In ECCV, 2020.
Hospedales, T., Antoniou, A., Micaelli, P., and Storkey,
A. Meta-learning in neural networks: A survey. arXiv
preprint arXiv:2004.05439, 2020.
Huh, M., Agrawal, P., and Efros, A. A.
What makes
imagenet good for transfer learning?
arXiv preprint
arXiv:1608.08614, 2016.
Joulin, A., Van Der Maaten, L., Jabri, A., and Vasilache, N.
Learning visual features from large weakly supervised
data. In ECCV, 2016.
Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J.,
Gelly, S., and Houlsby, N. Big transfer (BiT): General
visual representation learning. In ECCV, 2020.
Krizhevsky, A. Learning multiple layers of features from
tiny images. Technical report, University of Toronto,
2009.
Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B.
Human-level concept learning through probabilistic pro-
gram induction. Science, 2015.
Li, Y., Yang, J., Song, Y., Cao, L., Luo, J., and Li, L.-J.
Learning from noisy labels with distillation. In ICCV,
2017.
Lin, T.-Y., Maire, M., Belongie, S. J., Hays, J., Perona, P.,
Ramanan, D., Doll´ar, P., and Zitnick, C. L. Microsoft
coco: Common objects in context. In ECCV, 2014.
Liu, B., Zhao, Z., Li, Z., Jiang, J., Guo, Y., Shen, H., and
Ye, J. Feature transformation ensemble model with batch
spectral regularization for cross-domain few-shot classiﬁ-
cation. arXiv preprint arXiv:2005.08463, 2020.
Mahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri,
M., Li, Y., Bharambe, A., and Van Der Maaten, L. Ex-
ploring the limits of weakly supervised pretraining. In
ECCV, 2018.
Nilsback, M.-E. and Zisserman, A. Automated ﬂower clas-
siﬁcation over a large number of classes. In Indian Con-
ference on Computer Vision, Graphics and Image Pro-
cessing, 2008.

Comparing Transfer and Meta Learning Approaches on a Uniﬁed Few-Shot Classiﬁcation Benchmark
Oreshkin, B. N., Rodriguez, P., and Lacoste, A. Tadam:
Task dependent adaptive metric for improved few-shot
learning. In NeurIPS, 2018.
Pan, S. J. and Yang, Q. A survey on transfer learning. IEEE
Transactions on knowledge and data engineering, 2009.
Parkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar,
C. V. Cats and dogs. In CVPR, 2012.
Phoo, C. P. and Hariharan, B. Self-training for few-shot
transfer across extreme task differences. arXiv preprint
arXiv:2010.07734, 2020.
Rebufﬁ, S.-A., Bilen, H., and Vedaldi, A. Learning multiple
visual domains with residual adapters. In NeurIPS, 2017.
Ren, M., Triantaﬁllou, E., Ravi, S., Snell, J., Swersky, K.,
Tenenbaum, J. B., Larochelle, H., and Zemel, R. S. Meta-
learning for semi-supervised few-shot classiﬁcation. In
ICLR, 2018.
Saikia, T., Brox, T., and Schmid, C. Optimized generic fea-
ture learning for few-shot classiﬁcation across domains.
arXiv preprint arXiv:2001.07926, 2020.
Schmidhuber, J. Evolutionary principles in self-referential
learning, or on learning how to learn: the meta-meta-
... hook. PhD thesis, Technische Universit¨at M¨unchen,
1987.
Snell, J., Swersky, K., and Zemel, R. Prototypical networks
for few-shot learning. In NeurIPS, 2017.
Sohn, K., Berthelot, D., Li, C.-L., Zhang, Z., Carlini, N.,
Cubuk, E. D., Kurakin, A., Zhang, H., and Raffel, C. Fix-
match: Simplifying semi-supervised learning with consis-
tency and conﬁdence. arXiv preprint arXiv:2001.07685,
2020.
Sun, C., Shrivastava, A., Singh, S., and Gupta, A. Revisiting
unreasonable effectiveness of data in deep learning era.
In ICCV, 2017.
Sun, J., Lapuschkin, S., Samek, W., Zhao, Y., Cheung,
N.-M., and Binder, A. Explanation-guided training for
cross-domain few-shot classiﬁcation.
arXiv preprint
arXiv:2007.08790, 2020.
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,
Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich,
A. Going deeper with convolutions. In CVPR, 2015.
Thrun, S. Is learning the n-th thing any easier than learning
the ﬁrst? In NeurIPS, 1996.
Tian, Y., Wang, Y., Krishnan, D., Tenenbaum, J. B., and
Isola, P. Rethinking few-shot image classiﬁcation: a good
embedding is all you need? In ECCV, 2020.
Touvron, H., Vedaldi, A., Douze, M., and J´egou, H. Fixing
the train-test resolution discrepancy. In NeurIPS, 2019.
Triantaﬁllou, E., Zhu, T., Dumoulin, V., Lamblin, P., Evci,
U., Xu, K., Goroshin, R., Gelada, C., Swersky, K., Man-
zagol, P.-A., and Larochelle, H. Meta-Dataset: A dataset
of datasets for learning to learn from few examples. In
ICLR, 2020.
Tseng, H.-Y., Lee, H.-Y., Huang, J.-B., and Yang, M.-H.
Cross-domain few-shot classiﬁcation via learned feature-
wise transformation. In ICLR, 2020.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention
is all you need. In NeurIPS, 2017.
Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al.
Matching networks for one shot learning. In NeurIPS,
2016.
Wang, M. and Deng, W. Deep visual domain adaptation: A
survey. Neurocomputing, 2018.
Wang, Y., Yao, Q., Kwok, J. T., and Ni, L. M. Generalizing
from a few examples: A survey on few-shot learning.
ACM Computing Surveys (CSUR), 2020.
Xie, Q., Hovy, E., Luong, M.-T., and Le, Q. V.
Self-
training with noisy student improves imagenet classiﬁca-
tion. arXiv preprint arXiv:1911.04252, 2019.
Zhai, X., Puigcerver, J., Kolesnikov, A., Ruyssen, P.,
Riquelme, C., Lucic, M., Djolonga, J., Pinto, A. S.,
Neumann, M., Dosovitskiy, A., Beyer, L., Bachem, O.,
Tschannen, M., Michalski, M., Bousquet, O., Gelly, S.,
and Houlsby, N. A large-scale study of representation
learning with the visual task adaptation benchmark. arXiv
preprint arXiv:1910.04867, 2019.

Comparing Transfer and Meta Learning Approaches on a Uniﬁed Few-Shot Classiﬁcation Benchmark
A. Additional experiment details
Experiments presented in this work are ran in two main
computing infrastructure: TPU-v3 (all BIT experiments)
and Nvidia V100 (rest).
For Prototypical networks, ProtoMAML and MD-Transfer,
model and hyperparameter selection is based on the average
query accuracy over episodes sampled from all of MD-
v2’s validation classes. For each approach we perform a
hyperparameter search using Triantaﬁllou et al. (2020)’s
search space (Tables 1, 2, and 3, presented alongside the
best values found), for a total of 99 runs for each approach.
We re-train CrossTransformers on episodes sampled from
all ImageNet classes, with 50% of the episodes converted to
SimCLR episodes — this corresponds to the CTX+SimCLR
Eps setting in Doersch et al. (2020). We use the recom-
mended hyperparameters and perform a light sweep over
learning rates in {0.01, 0.001, 0.0006, 0.0001} and found
Doersch et al. (2020)’s recommended 0.0006 learning rate
to be optimal in our case as well. Model selection is per-
formed using MD-v2 validation episodes — this is a slight
departure from CrossTransformers’ ImageNet-only prototol
that is made necessary by the fact that all ImageNet classes
participate in training episodes in MD-v2.
Since pre-trained SUR backbones were already made avail-
able by the authors,7 we re-used all of them with two ex-
ceptions: (1) we re-trained the ImageNet backbone on all
ImageNet classes using the provided training script (be-
cause the original backbone was trained on Meta-Dataset’s
ImageNet training classes), and (2) we ignored the VGG
Flowers backbone (because the dataset is included as one
of VTAB-v2’s downstream tasks). We ran Dvornik et al.
(2020)’s inference code as-is for evaluation.
All Big Transfer models are pre-trained as described
in (Kolesnikov et al., 2020). The pre-processing at training
time is at 224 resolution, using random horizontal ﬂipping
and inception crop (Szegedy et al., 2015). In all of our exper-
iments, during transfer we only resize images to the desired
resolution (126 or 224) at both ﬁne-tuning and evaluation
time. While higher resolution and further data augmentation
further improves performance, we remove this additional
confounding factor.
B. Detailed ﬁgures and accuracy tables
We show a detailed breakdown of VTAB-V2 accuracies
(Figure 7) for investigated approaches. We also provide
detailed accuracy tables (Tables 4 through 9) for all plots
displayed in the main text. For MD-v2 we show 95% conﬁ-
dence intervals computed over 60 episodes for BiT learners
and 600 episodes for all other approaches.
7https://github.com/dvornikita/SUR
C. Bridging the Performance Gap Between
MD-Transfer Baseline and ProtoMAML
Given the stark differences between ProtoMAML and MD-
Transfer on VTAB-v2, we ran a few additional experi-
ments in order to better explain these discrepancies. We
swapped their evaluation hyperparameters, meaning that
we ﬁne-tuned MD-Transfer for 10 steps using a learn-
ing rate of 0.0054 without using a cosine classiﬁer (MD-
Transfer (ProtoMAML hypers)) and that we ran Pro-
toMAML’s inner-loop for 100 steps using a learning rate
of 1 × 10−2 with a linear classiﬁcation head (ProtoMAML
(MD-Transfer hypers)). Note that this does not completely
bridge the hyperparameter gap between the two approaches,
but it does bring them closer to each other. The remain-
ing differences are that (1) the validation procedure used
for early stopping is different, and (2) ProtoMAML ini-
tializes the output layer with class prototypes, whereas the
output layer weights in MD-Transfer are sampled from a
normal distribution. Additionally, to isolate the effect of
cosine-classiﬁcation, we run MD-Transfer with a linear clas-
siﬁcation head while keeping the learning rate and number
of training steps the same (MD-Transfer (linear head)).
Figure 8 shows that ProtoMAML gets better results on MD-
v2 with MD-Transfer hyperparameters (more ﬁne-tuning
steps with a smaller learning rate), with apparent gains on
Quickdraw and Trafﬁc Signs. ProtoMAML’s prototypical
initialization seems to yield better performance for “in-
domain” datasets (i.e. datasets participating to the training
split of classes), however we observe diminishing returns
for test-only datasets like Trafﬁc Sign.
Disabling cosine classiﬁcation (MD-Transfer (linear
head)) seems to harm ﬁne-tuning performance greatly on
all datasets except QuickDraw. Trafﬁc Signs in particuar
beneﬁts greatly from a cosine classiﬁcation head, as evi-
denced by the 10% drop in performance observed when
switching to a linear classiﬁcation head. On VTAB, again,
MD-Transfer hyperparameters help improve ProtoMAML
performance, hinting at the fact that the hyperparameter
selection procedure used for ProtoMAML is sub-optimal.
D. Larger-scale SUR experiments
In this section we investigate increasing the capacity
(ResNet-50) and input resolution (224 × 224) of SUR back-
bones. We re-train backbones for all seven of MD-v2’s
training sources of data using BiT’s upstream training hy-
perparameters and adjusting the number of training steps
as needed to ensure convergence. We trained two back-
bone variants: one with a regular linear classiﬁcation head,
and one with a temperature-adjusted cosine classiﬁer head.
Backbones were trained for:

Comparing Transfer and Meta Learning Approaches on a Uniﬁed Few-Shot Classiﬁcation Benchmark
Hyperparameter
Search space
Best
Backbone
{ResNet-18, 4-layer convnet}
ResNet-18
Resolution
{84, 126}
126
Outer-loop LR
log-uniform(1e-6, 1e-2)
0.0004
Outer-loop LR decay freq.
{100, 500, 1k, 2.5k, 5k, 10k}
1k
Outer-loop LR decay rate
uniform(0.5, 1.0)
0.6478
Inner-loop LR
log-uniform(5e-3, 5e-1)
0.0054
Inner-loop steps
{1, 6, 10}
10
Additional inner-loop steps (evaluation)
{0, 5}
0
Table 1. ProtoMAML hyperparameter search space.
Hyperparameter
Search space
Best
Backbone
{ResNet-18, 4-layer convnet}
ResNet-18
Resolution
{84, 126}
126
Training LR
log-uniform(1e-6, 1e-2)
3.4293725734843445e-06
Fine-tuning LR
{1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 2e-1}
1e-2
Fine-tuning steps
{50, 75, 100, 125, 150, 175, 200}
100
Fine-tune with Adam?
{True, False}
True
Cosine classiﬁer head?
{True, False}
True
Cosine logits multiplier
{1, 2, 10, 100}
10
Weight-normalize the classiﬁer head?
{True, False}
True
Fine-tune all layers?
{True, False}
True
Table 2. MD-Transfer hyperparameter search space.
Hyper
Search space
Best
Backbone
{ResNet-18, 4-layer convnet}
ResNet-18
Resolution
{84, 126}
126
LR
log-uniform(1e-6, 1e-2)
0.0003
LR decay freq.
{100, 500, 1k, 2.5k, 5k, 10k}
500
LR decay rate
uniform(0.5, 1.0)
0.8857
Table 3. Prototypical Networks hyperparameter search space.
• ImageNet: 90 epochs
• Quickdraw: 4 epochs
• Birds, Omniglot, Fungi: 900 epochs
• Textures: 1350 epochs
• Aircraft: 4500 epochs
The LR schedule is adjusted proportionally to the number of
epochs. For simplicity we select the ﬁnal backbone check-
points rather than selecting based on an episodic loss.
Figure 9 shows an appreciable 5% improvement on VTAB-
v2, most of which is driven by an improvement on spe-
cialized tasks. On the other hand, the aggregate perfor-
mance gain on MD-v2 is negligible. While performance on
MSCOCO, Fungi, Birds, and Textures is increased signiﬁ-
cantly, the larger input resolution and backbone capacity has
a negligible or detrimental effect on QuickDraw, Omniglot,
and Aircraft. We hypothesize that the drop in Aircraft per-
formance is due to the large batch size used by BiT and a
suboptimal model selection strategy.
Overall these results are encouraging, but a more thorough
investigation is needed before we can draw deﬁnitive con-
clusions.

Comparing Transfer and Meta Learning Approaches on a Uniﬁed Few-Shot Classiﬁcation Benchmark
Data source
MD-Transfer
ProtoMAML
ProtoNets
CTX
BiT-ResNet-101x3
BiT-ResNet-18
Omniglot
80.92 ± 1.20%
68.35 ± 1.28%
65.47 ± 1.35%
84.55 ± 0.94%
72.35 ± 4.70%
71.87 ± 4.38%
Aircraft
75.45 ± 1.20%
58.18 ± 0.96%
54.25 ± 1.03%
85.31 ± 0.83%
78.34 ± 3.57%
70.23 ± 3.78%
Birds
61.23 ± 1.30%
69.69 ± 0.98%
64.78 ± 0.98%
72.92 ± 1.07%
91.02 ± 1.49%
81.65 ± 2.26%
DTD
66.66 ± 1.01%
68.71 ± 0.83%
64.91 ± 0.76%
77.29 ± 0.71%
87.06 ± 2.61%
78.62 ± 2.86%
QuickDraw
61.12 ± 1.06%
55.52 ± 1.02%
53.26 ± 1.02%
73.29 ± 0.78%
65.08 ± 4.13%
64.81 ± 3.71%
Fungi
35.39 ± 1.08%
38.88 ± 1.05%
36.37 ± 1.08%
47.95 ± 1.19%
60.68 ± 4.43%
49.81 ± 4.28%
Trafﬁc Sign
85.31 ± 0.95%
53.83 ± 1.05%
50.27 ± 1.05%
80.12 ± 0.97%
76.23 ± 4.68%
69.53 ± 4.55%
MSCOCO
39.66 ± 1.05%
43.32 ± 1.12%
41.08 ± 0.99%
51.39 ± 1.06%
69.74 ± 2.69%
57.84 ± 3.03%
Caltech101
70.00 %
78.81 %
74.18 %
84.24 %
88.59 %
83.32 %
CIFAR100
32.57 %
36.22 %
31.13 %
37.51 %
58.35 %
49.37 %
Flowers102
66.69 %
65.39 %
61.99 %
81.75 %
81.88 %
76.38 %
Pets
49.06 %
68.33 %
58.33 %
70.88 %
89.97 %
78.95 %
Sun397
15.05 %
8.05 %
17.73 %
24.79 %
35.47 %
27.09 %
SVHN
83.54 %
45.31 %
38.06 %
67.22 %
79.23 %
80.71 %
EuroSAT
89.41 %
83.02 %
80.63 %
86.43 %
94.64 %
93.53 %
Resics45
65.46 %
57.79 %
54.11 %
67.65 %
76.71 %
71.03 %
Patch Camelyon
81.11 %
76.75 %
74.26 %
79.77 %
82.97 %
79.73 %
Retinopathy
58.07 %
73.51 %
28.82 %
35.48 %
73.85 %
67.06 %
CLEVR-count
40.09 %
30.32 %
30.33 %
27.89 %
70.73 %
50.59 %
CLEVR-dist
52.97 %
34.29 %
39.99 %
29.61 %
54.19 %
58.79 %
dSprites-loc
83.81 %
36.68 %
32.95 %
23.19 %
95.38 %
93.39 %
dSprites-ori
46.70 %
18.69 %
15.60 %
46.92 %
61.13 %
52.15 %
SmallNORB-azi
36.40 %
12.20 %
12.21 %
37.02 %
17.50 %
23.17 %
SmallNORB-elev
31.29 %
18.26 %
18.02 %
21.62 %
36.40 %
28.92 %
DMLab
43.14 %
33.28 %
32.12 %
31.92 %
45.58 %
41.86 %
KITTI-dist
64.70 %
56.96 %
55.70 %
54.34 %
82.24 %
76.15 %
MD-v2
63.22 %
57.06 %
53.80 %
71.60 %
75.06 %
68.04 %
VTAB (all)
56.11 %
46.33 %
42.01 %
50.46 %
68.04 %
62.90 %
VTAB (natural)
52.82 %
50.35 %
46.90 %
61.07 %
72.25 %
65.97 %
VTAB (specialized)
73.51 %
72.77 %
59.45 %
67.33 %
82.04 %
77.84 %
VTAB (structured)
49.89 %
30.08 %
29.62 %
34.06 %
57.89 %
53.13 %
Table 4. VTAB+MD accuracies for approaches trained only on ImageNet.

Comparing Transfer and Meta Learning Approaches on a Uniﬁed Few-Shot Classiﬁcation Benchmark
Data source
MD-Transfer
ProtoMAML
ProtoNets
SUR
BiT-ResNet-101x3 (JFT)
Omniglot
82.04 ± 1.27%
90.15 ± 0.65%
85.29 ± 0.89%
92.84 ± 0.52%
76.45 ± 4.04%
Aircraft
76.77 ± 1.16%
82.10 ± 0.60%
74.34 ± 0.81%
84.44 ± 0.58%
93.30 ± 1.44%
Birds
61.23 ± 1.29%
73.36 ± 0.92%
68.00 ± 1.01%
75.80 ± 0.96%
97.06 ± 0.53%
DTD
65.98 ± 1.07%
66.32 ± 0.76%
65.26 ± 0.69%
70.35 ± 0.72%
88.96 ± 2.14%
QuickDraw
61.29 ± 1.06%
66.37 ± 0.95%
60.57 ± 1.00%
81.71 ± 0.57%
71.27 ± 3.77%
Fungi
35.47 ± 1.05%
46.32 ± 1.11%
39.84 ± 1.10%
63.72 ± 1.08%
62.59 ± 4.29%
Trafﬁc Sign
84.71 ± 0.94%
50.28 ± 1.05%
49.79 ± 1.07%
49.99 ± 1.08%
69.13 ± 5.34%
MSCOCO
39.56 ± 1.00%
39.00 ± 1.04%
39.65 ± 1.03%
49.41 ± 1.08%
76.36 ± 2.23%
Caltech101
70.58 %
73.06 %
71.98 %
82.33 %
91.78 %
CIFAR100
31.33 %
29.72 %
27.70 %
33.69 %
76.32 %
Flowers102
66.08 %
60.22 %
57.11 %
55.72 %
99.33 %
Pets
49.09 %
56.61 %
50.99 %
76.34 %
95.45 %
Sun397
13.94 %
8.05 %
14.19 %
27.49 %
57.24 %
SVHN
83.20 %
46.78 %
41.93 %
18.66 %
66.47 %
EuroSAT
88.74 %
80.07 %
77.74 %
78.91 %
95.33 %
Resics45
63.67 %
53.48 %
50.79 %
62.40 %
85.76 %
Patch Camelyon
81.53 %
75.85 %
73.75 %
75.60 %
81.81 %
Retinopathy
57.61 %
73.18 %
28.04 %
27.91 %
72.02 %
CLEVR-count
40.30 %
32.72 %
31.96 %
29.99 %
61.54 %
CLEVR-dist
52.86 %
35.43 %
39.35 %
37.06 %
55.96 %
dSprites-loc
85.87 %
41.96 %
38.07 %
29.96 %
96.80 %
dSprites-ori
46.41 %
23.00 %
16.25 %
19.84 %
63.84 %
SmallNORB-azi
36.49 %
13.42 %
12.27 %
12.86 %
13.78 %
SmallNORB-elev
31.16 %
18.76 %
17.38 %
18.15 %
29.68 %
DMLab
43.03 %
32.49 %
31.83 %
33.31 %
48.22 %
KITTI-dist
58.65 %
54.43 %
42.05 %
52.32 %
78.62 %
MD-v2
63.38 %
64.24 %
60.34 %
71.03 %
79.39 %
VTAB (all)
55.59 %
44.96 %
40.19 %
42.92 %
70.55 %
VTAB (natural)
52.37 %
45.74 %
43.98 %
49.04 %
81.10 %
VTAB (specialized)
72.89 %
70.65 %
57.58 %
61.20 %
83.73 %
VTAB (structured)
49.35 %
31.52 %
28.65 %
29.19 %
56.05 %
Table 5. VTAB+MD accuracies for approaches trained on more data (all of MD-v2’s training sources, unless noted otherwise).

Comparing Transfer and Meta Learning Approaches on a Uniﬁed Few-Shot Classiﬁcation Benchmark
Data source
BiT-ResNet-18
(126 × 126)
BiT-ResNet-18
(224 × 224)
BiT-ResNet-50
(126 × 126)
BiT-ResNet-50
(224 × 224)
CTX
Omniglot
71.87 ± 4.38%
72.73 ± 4.64%
68.56 ± 4.68%
68.03 ± 4.86%
84.55 ± 0.94%
Aircraft
70.23 ± 3.78%
73.61 ± 3.80%
74.09 ± 3.64%
77.42 ± 3.55%
85.31 ± 0.83%
Birds
81.65 ± 2.26%
87.22 ± 1.88%
86.82 ± 1.57%
90.82 ± 1.46%
72.92 ± 1.07%
DTD
78.62 ± 2.86%
82.62 ± 2.70%
82.35 ± 2.56%
84.97 ± 2.53%
77.29 ± 0.71%
QuickDraw
64.81 ± 3.71%
66.34 ± 3.60%
66.98 ± 3.62%
66.56 ± 3.69%
73.29 ± 0.78%
Fungi
49.81 ± 4.28%
53.93 ± 4.44%
54.63 ± 4.20%
59.37 ± 4.25%
47.95 ± 1.19%
Trafﬁc Sign
69.53 ± 4.55%
75.39 ± 4.34%
71.09 ± 4.66%
73.52 ± 4.69%
80.12 ± 0.97%
MSCOCO
57.84 ± 3.03%
59.97 ± 2.89%
64.55 ± 2.93%
65.69 ± 2.71%
51.39 ± 1.06%
Caltech101
83.32 %
84.59 %
85.69 %
87.22 %
84.24 %
CIFAR100
49.37 %
47.10 %
55.85 %
54.42 %
37.51 %
Flowers102
76.38 %
82.65 %
81.87 %
83.33 %
81.75 %
Pets
78.95 %
83.91 %
86.07 %
87.91 %
70.88 %
Sun397
27.09 %
29.11 %
31.62 %
33.29 %
24.79 %
SVHN
80.71 %
83.40 %
78.47 %
70.40 %
67.22 %
EuroSAT
93.53 %
93.82 %
94.14 %
94.44 %
86.43 %
Resics45
71.03 %
74.12 %
74.92 %
76.13 %
67.65 %
Patch Camelyon
79.73 %
80.67 %
81.55 %
83.06 %
79.77 %
Retinopathy
67.06 %
74.47 %
71.15 %
70.24 %
35.48 %
CLEVR-count
50.59 %
55.25 %
53.69 %
74.03 %
27.89 %
CLEVR-dist
58.79 %
58.69 %
54.59 %
51.55 %
29.61 %
dSprites-loc
93.39 %
98.59 %
92.53 %
82.72 %
23.19 %
dSprites-ori
52.15 %
46.46 %
51.40 %
55.11 %
46.92 %
SmallNORB-azi
23.17 %
20.71 %
20.10 %
17.79 %
37.02 %
SmallNORB-elev
28.92 %
21.75 %
26.95 %
32.07 %
21.62 %
DMLab
41.86 %
43.74 %
42.54 %
43.18 %
31.92 %
KITTI-dist
76.15 %
78.78 %
77.80 %
79.93 %
54.34 %
MD-v2
68.04 %
71.48 %
71.14 %
73.30 %
71.60 %
VTAB (all)
62.90 %
64.32 %
64.50 %
65.38 %
50.46 %
VTAB (natural)
65.97 %
68.46 %
69.93 %
69.43 %
61.07 %
VTAB (specialized)
77.84 %
80.77 %
80.44 %
80.97 %
67.33 %
VTAB (structured)
53.13 %
53.00 %
52.45 %
54.55 %
34.06 %
Table 6. VTAB+MD accuracies for BiT learners trained on various input resolutions and network capacities. CrossTransformers (CTX)
accuracies are provided for context. All approaches are trained only on ImageNet.

Comparing Transfer and Meta Learning Approaches on a Uniﬁed Few-Shot Classiﬁcation Benchmark
Data source
BiT-ResNet-50 (GNWS)
BiT-ResNet-50 (BN)
Omniglot
68.03 ± 4.86%
61.66 ± 5.13%
Aircraft
77.42 ± 3.55%
76.82 ± 3.71%
Birds
90.82 ± 1.46%
87.59 ± 1.84%
DTD
84.97 ± 2.53%
83.72 ± 3.39%
QuickDraw
66.56 ± 3.69%
63.83 ± 4.03%
Fungi
59.37 ± 4.25%
53.77 ± 4.43%
Trafﬁc Sign
73.52 ± 4.69%
70.46 ± 4.70%
MSCOCO
65.69 ± 2.71%
61.50 ± 2.73%
Caltech101
87.22 %
88.72 %
CIFAR100
54.42 %
53.78 %
Flowers102
83.33 %
85.45 %
Pets
87.91 %
88.24 %
Sun397
33.29 %
31.60 %
SVHN
70.40 %
85.57 %
EuroSAT
94.44 %
95.35 %
Resics45
76.13 %
79.02 %
Patch Camelyon
83.06 %
80.13 %
Retinopathy
70.24 %
73.13 %
CLEVR-count
74.03 %
43.10 %
CLEVR-dist
51.55 %
49.65 %
dSprites-loc
82.72 %
83.19 %
dSprites-ori
55.11 %
46.49 %
SmallNORB-azi
17.79 %
18.93 %
SmallNORB-elev
32.07 %
34.32 %
DMLab
43.18 %
44.67 %
KITTI-dist
79.93 %
76.97 %
MD-v2
73.30 %
69.92 %
VTAB (all)
65.38 %
64.35 %
VTAB (natural)
69.43 %
72.22 %
VTAB (specialized)
80.97 %
81.91 %
VTAB (structured)
54.55 %
49.67 %
Table 7. VTAB+MD accuracies for BiT learners trained with either group normalization + weight standardization (GNWS) or batch
normalization (BN). All approaches are trained only on 224 × 224 ImageNet examples.

Comparing Transfer and Meta Learning Approaches on a Uniﬁed Few-Shot Classiﬁcation Benchmark
Data source
BiT-ResNet-101x3
(ImageNet)
BiT-ResNet-101x3
(ImageNet-21k)
BiT-ResNet-101x3
(JFT)
CTX
Omniglot
72.35 ± 4.70%
78.49 ± 4.00%
76.45 ± 4.04%
84.55 ± 0.94%
Aircraft
78.34 ± 3.57%
75.49 ± 4.32%
93.30 ± 1.44%
85.31 ± 0.83%
Birds
91.02 ± 1.49%
98.10 ± 0.45%
97.06 ± 0.53%
72.92 ± 1.07%
DTD
87.06 ± 2.61%
89.79 ± 2.40%
88.96 ± 2.14%
77.29 ± 0.71%
QuickDraw
65.08 ± 4.13%
69.16 ± 3.79%
71.27 ± 3.77%
73.29 ± 0.78%
Fungi
60.68 ± 4.43%
70.70 ± 3.91%
62.59 ± 4.29%
47.95 ± 1.19%
Trafﬁc Sign
76.23 ± 4.68%
72.51 ± 4.73%
69.13 ± 5.34%
80.12 ± 0.97%
MSCOCO
69.74 ± 2.69%
76.07 ± 2.26%
76.36 ± 2.23%
51.39 ± 1.06%
Caltech101
88.59 %
89.54 %
91.78 %
84.24 %
CIFAR100
58.35 %
78.08 %
76.32 %
37.51 %
Flowers102
81.88 %
99.09 %
99.33 %
81.75 %
Pets
89.97 %
92.00 %
95.45 %
70.88 %
Sun397
35.47 %
50.35 %
57.24 %
24.79 %
SVHN
79.23 %
69.08 %
66.47 %
67.22 %
EuroSAT
94.64 %
95.63 %
95.33 %
86.43 %
Resics45
76.71 %
80.77 %
85.76 %
67.65 %
Patch Camelyon
82.97 %
81.26 %
81.81 %
79.77 %
Retinopathy
73.85 %
75.27 %
72.02 %
35.48 %
CLEVR-count
70.73 %
66.75 %
61.54 %
27.89 %
CLEVR-dist
54.19 %
53.85 %
55.96 %
29.61 %
dSprites-loc
95.38 %
90.00 %
96.80 %
23.19 %
dSprites-ori
61.13 %
62.47 %
63.84 %
46.92 %
SmallNORB-azi
17.50 %
15.40 %
13.78 %
37.02 %
SmallNORB-elev
36.40 %
37.05 %
29.68 %
21.62 %
DMLab
45.58 %
45.37 %
48.22 %
31.92 %
KITTI-dist
82.24 %
78.45 %
78.62 %
54.34 %
MD-v2
75.06 %
78.79 %
79.39 %
71.60 %
VTAB (all)
68.04 %
70.02 %
70.55 %
50.46 %
VTAB (natural)
72.25 %
79.69 %
81.10 %
61.07 %
VTAB (specialized)
82.04 %
83.23 %
83.73 %
67.33 %
VTAB (structured)
57.89 %
56.17 %
56.05 %
34.06 %
Table 8. VTAB+MD accuracies for BiT-L learners trained on varying amounts of upstream data. CrossTransformers (CTX) accuracies are
provided for context. All approaches are trained on 224 × 224 inputs.

Comparing Transfer and Meta Learning Approaches on a Uniﬁed Few-Shot Classiﬁcation Benchmark
Caltech101
CIFAR-100
Flowers102
Pets
Sun397
SVHN
Camelyon
EuroSAT
Resisc45
Retinopathy
CLEVR-Count
CLEVR-Dist
DMLab
dSpr-Loc
dSpr-Ori
KITTI-Dist
sNORB-Azim
sNORB-Elev
VTAB-v2 (all)
VTAB-v2 (natural)
VTAB-v2 (specialized)
VTAB-v2 (structured)
Task
20
40
60
80
100
Accuracy
MD-Transfer (ImageNet-only)
ProtoMAML (ImageNet-only)
ProtoNets (ImageNet-only)
CTX (ImageNet-only)
ResNet-101x3 (ImageNet-only)
ResNet-18 (ImageNet-only)
Caltech101
CIFAR-100
Flowers102
Pets
Sun397
SVHN
Camelyon
EuroSAT
Resisc45
Retinopathy
CLEVR-Count
CLEVR-Dist
DMLab
dSpr-Loc
dSpr-Ori
KITTI-Dist
sNORB-Azim
sNORB-Elev
VTAB-v2 (all)
VTAB-v2 (natural)
VTAB-v2 (specialized)
VTAB-v2 (structured)
Task
20
40
60
80
100
Accuracy
MD-Transfer (all MD-v2 sources)
ProtoMAML (all MD-v2 sources)
ProtoNets (all MD-v2 sources)
SUR (all MD-v2 sources)
ResNet-101x3 (JFT)
Figure 7. VTAB-v2 accuracies, broken down by downstream task, for approaches trained only on ImageNet (top) or larger-scale datasets
(bottom).
Data source
BiT-ResNet-50 (JFT)
BiT-ResNet-50 (JFT, deduplicated)
BiT-ResNet-50 (JFT, class-ablated)
Omniglot
69.37 ± 4.42%
69.89 ± 4.71%
69.10 ± 4.72%
Aircraft
87.13 ± 2.28%
86.27 ± 2.25%
73.09 ± 3.76%
Birds
92.50 ± 1.24%
92.59 ± 1.16%
79.22 ± 2.92%
DTD
87.43 ± 2.05%
87.48 ± 2.21%
87.72 ± 2.14%
QuickDraw
63.99 ± 4.23%
63.65 ± 4.23%
64.45 ± 4.05%
Fungi
56.03 ± 4.22%
56.48 ± 4.47%
54.94 ± 4.53%
Trafﬁc Sign
66.21 ± 4.94%
66.13 ± 5.03%
63.79 ± 4.98%
MSCOCO
70.39 ± 2.44%
71.06 ± 2.40%
70.15 ± 2.55%
MD-v2
74.13 %
74.19 %
70.31 %
Table 9. VTAB+MD accuracies for BiT-L learners trained on ablated JFT variants. The deduplicated variant of JFT removes all images
that are found in MD-v2 test sources, and the class-ablated variant removes all images belonging to airplane-, birds-, and fungi-related
classes. All approaches are trained on 224 × 224 inputs.

Comparing Transfer and Meta Learning Approaches on a Uniﬁed Few-Shot Classiﬁcation Benchmark
VTAB-v2
(all)
VTAB-v2
(natural)
VTAB-v2
(specialized)
VTAB-v2
(structured)
MD-v2
Task / Source
30
40
50
60
70
80
90
Accuracy
MD-Transfer
MD-Transfer (ProtoMAML hypers)
MD-Transfer (linear head)
ProtoMAML
ProtoMAML (MD-Transfer hypers)
Omniglot
Aircraft
Birds
Textures
QuickDraw
Fungi
Traffic Sign
MSCOCO
Average
Source
40
60
80
100
120
Accuracy
MD-Transfer
MD-Transfer (ProtoMAML hypers)
MD-Transfer (linear head)
ProtoMAML
ProtoMAML (MD-Transfer hypers)
Caltech101
CIFAR-100
Flowers102
Pets
Sun397
SVHN
Camelyon
EuroSAT
Resisc45
Retinopathy
CLEVR-Count
CLEVR-Dist
DMLab
dSpr-Loc
dSpr-Ori
KITTI-Dist
sNORB-Azim
sNORB-Elev
VTAB-v2 (all)
VTAB-v2 (natural)
VTAB-v2 (specialized)
VTAB-v2 (structured)
Task
20
40
60
80
100
Accuracy
MD-Transfer
MD-Transfer (ProtoMAML hypers)
MD-Transfer (linear head)
ProtoMAML
ProtoMAML (MD-Transfer hypers)
Figure 8. Ablation study for different hyper parameters found by ProtoMAML and MD-Transfer, broken down by downstream task. All
backbones are trained the all MD-V2 training data.
VTAB-v2
(all)
VTAB-v2
(natural)
VTAB-v2
(specialized)
VTAB-v2
(structured)
MD-v2
Task / Source
30
40
50
60
70
80
Accuracy
SUR (ResNet-18)
SUR (ResNet-50)
SUR (ResNet-50, linear head)
Omniglot
Aircraft
Birds
Textures
QuickDraw
Fungi
Traffic Sign
MSCOCO
Average
Source
50
60
70
80
90
Accuracy
SUR (ResNet-18)
SUR (ResNet-50)
SUR (ResNet-50, linear)
Figure 9. Ablation study for different hyper parameters found by ProtoMAML and MD-Transfer, broken down by downstream task, for
Meta Dataset-v2 (top) and VTAB (bottom). All backbones are trained the all MD-V2 training data.

