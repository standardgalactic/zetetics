ChatGPT Outperforms Crowd-Workers for
Text-Annotation Tasks∗
Fabrizio Gilardi†
Meysam Alizadeh‡
Ma¨el Kubli§
March 28, 2023
Abstract
Many NLP applications require manual data annotations for a variety of tasks,
notably to train classiﬁers or evaluate the performance of unsupervised models.
Depending on the size and degree of complexity, the tasks may be conducted by
crowd-workers on platforms such as MTurk as well as trained annotators, such as
research assistants. Using a sample of 2,382 tweets, we demonstrate that ChatGPT
outperforms crowd-workers for several annotation tasks, including relevance, stance,
topics, and frames detection.
Speciﬁcally, the zero-shot accuracy of ChatGPT
exceeds that of crowd-workers for four out of ﬁve tasks, while ChatGPT’s intercoder
agreement exceeds that of both crowd-workers and trained annotators for all tasks.
Moreover, the per-annotation cost of ChatGPT is less than $0.003—about twenty
times cheaper than MTurk.
These results show the potential of large language
models to drastically increase the eﬃciency of text classiﬁcation.
∗This project received funding from the European Research Council (ERC) under the European
Union’s Horizon 2020 research and innovation program (grant agreement nr. 883121). We thank Fabio
Melliger and Paula Moser for excellent research assistance and Elliott Ash, Juan Bermeo, Emma Hoes,
Jonathan Kl¨user, Maria Korobeynikova, and Natalia Umansky for helpful feedback.
†Department of Political Science, University of Zurich (https://fabriziogilardi.org/)
‡Department of Political Science, University of Zurich (https://malizad.github.io/)
§Department of Political Science, University of Zurich (https://maelkubli.ch/)
1
arXiv:2303.15056v1  [cs.CL]  27 Mar 2023

1
Introduction
Many NLP applications require high-quality labeled data, notably to train classiﬁers or
evaluate the performance of unsupervised models. For example, researchers often aim to
ﬁlter noisy social media data for relevance, assign texts to diﬀerent topics or conceptual
categories, or measure their sentiment or stance. Regardless of the speciﬁc approach
used for these tasks (supervised, semi-supervised, or unsupervised), labeled data are
needed to build a training set or a gold standard against which performance can be
assessed. Such data may be available for high-level tasks such as semantic evaluation1
and hate speech (ElSherief et al., 2021), as well as, sometimes, more speciﬁc tasks such
as party ideology (Herrmann and D¨oring, 2023). More typically, however, researchers
have to conduct original annotations to ensure that the labels match their conceptual
categories (Benoit et al., 2016). Until recently, two main strategies were available. First,
researchers can recruit and train coders, such as research assistants. Second, they can
rely on crowd-workers on platforms such as Amazon Mechanical Turk (MTurk). Often,
these two strategies are used in combination: trained annotators create a relatively small
gold-standard dataset, and crowd-workers are employed to increase the volume of labeled
data. Each strategy has its own advantages and disadvantages. Trained annotators tend
to produce high-quality data, but involve signiﬁcant costs. Crowd workers are a much
cheaper and more ﬂexible option, but the quality may be insuﬃcient, particularly for
complex tasks and languages other than English. Moreover, there have been concerns
that MTurk data quality has decreased (Chmielewski and Kucker, 2020), while alternative
platforms such as CrowdFlower and FigureEight are no longer practicable options for
academic research since they were acquired by Appen, a company that is focused on a
business market.
This paper explores the potential of large language models (LLMs) for text annotation
tasks, with a focus on ChatGPT, which was released in November 2022. It demonstrates
that zero-shot ChatGPT classiﬁcations (that is, without any additional training) outper-
form MTurk annotations, at a fraction of the cost. LLMs have been shown to perform
very well for a wide range of purposes, including ideological scaling (Wu et al., 2023), the
classiﬁcation of legislative proposals (Nay, 2023), the resolution of cognitive psychology
tasks, (Binz and Schulz, 2023), and the simulation of human samples for survey research
(Argyle et al., 2023). While a few studies suggested that ChatGPT might perform text
annotation tasks of the kinds we have described (Kuzman et al., 2023; Huang et al.,
2023), to the best of our knowledge no systematic evaluation has been conducted yet.
Our analysis relies on a sample of 2,382 tweets that we collected for a previous study
1e.g., https://semeval.github.io/SemEval2023/tasks.html.
2

(Alizadeh et al., 2022). For that project, the tweets were labeled by trained annotators
(research assistants) for ﬁve diﬀerent tasks: relevance, stance, topics, and two kinds of
frame detection. Using the same codebooks that we developed to instruct our research
assistants, we submitted the tasks to ChatGPT as zero-shot classiﬁcations, as well as to
crowd-workers on MTurk. We then evaluated the performance of ChatGPT against two
benchmarks: (i) its accuracy, relative to that of crowd-workers, and (ii) its intercoder
agreement, relative to that of crowd workers as well as of our trained annotators.
We ﬁnd that for four out of ﬁve tasks, ChatGPT’s zero-shot accuracy is higher than
that of MTurk. For all tasks, ChatGPT’s intercoder agreement exceeds that of both
MTurk and trained annotators. Moreover, ChatGPT is signiﬁcantly cheaper than MTurk:
the ﬁve classiﬁcation tasks cost about $68 on ChatGPT (25,264 annotations) and $657
on MTurk (12,632 annotations) (see Section 4 for details). ChatGPT’s per-annotation
cost is therefore about $0.003, or a third of a cent—about twenty times cheaper than
MTurk, with higher quality. At this cost, it might potentially be possible to annotate
entire samples, or to create large training sets for supervised learning. Based on our tests,
100,000 annotations would cost about $300.
While further research is needed to better understand how ChatGPT and other LLMs
perform in a broader range of contexts, these results demonstrate their potential to trans-
form how researchers conduct data annotations, and to disrupt parts of the business model
of platforms such as MTurk.
2
Results
We use a dataset of 2,382 tweets that we annotated manually for a previous study on the
discourse around content moderation (Alizadeh et al., 2022). Speciﬁcally, we relied on
trained annotators (research assistants) to construct a gold standard for ﬁve conceptual
categories with diﬀerent numbers of classes: relevance of tweets for the content modera-
tion issue (relevant/irrelevant); stance regarding Section 230, a key part of US internet
legislation (keep/repeal/neutral); topic identiﬁcation (six classes); a ﬁrst set of frames
(content moderation as a problem, as a solution, or neutral); and a second set of frames
(fourteen classes) (see Section 4 for details).
We then performed these exact same classiﬁcations with ChatGPT and with crowd-
workers recruited on MTurk, using the same codebook we developed for our research
assistants (see Appendix S1). For ChatGPT, we conducted four sets of annotations. To
explore the eﬀect of ChatGPT’s temperature parameter, which controls the degree of
randomness of the output, we conducted the annotations with the default value of 1 as
well as with a value of 0.2, which implies less randomness. For each temperature value,
3

Accuracy
Intercoder Agreement
0%
25%
50%
75%
100% 0%
25%
50%
75%
100%
Frames II (14 classes)
Topics (6 classes)
Stance (3 classes)
Frames I (3 classes)
Relevance (2 classes)
Trained annotators
MTurk
ChatGPT (temp 1)
ChatGPT (temp 0.2)
Figure 1: ChatGPT zero-shot text annotation performance, compared to MTurk and
trained annotators. ChatGPT’s accuracy outperforms that of MTurk for four of the ﬁve
tasks. ChatGPT’s intercoder agreement outperforms that of both MTurk and trained an-
notators in all tasks. Accuracy means agreement with the trained annotators.
we conducted two sets of annotations to compute ChatGPT’s intercoder agreement. For
MTurk, we aimed to select the best available crowd-workers, notably by ﬁltering for
workers who are classiﬁed as “MTurk Masters” by Amazon, who have an approval rate
of over 90%, and who are located in the US. Our procedures are described in detail in
Section 4.
We report ChatGPT’s zero-shot performance for two diﬀerent metrics: accuracy and
intercoder agreement (Figure 1).
Accuracy is measured as the percentage of correct
annotations (using our trained annotators as a benchmark), while the intercoder agree-
ment is computed as the percentage of tweets that were assigned the same label by two
diﬀerent annotators (research assistant, crowd-workers, or ChatGPT runs). Regarding
accuracy, Figure 1 shows that ChatGPT outperforms MTurk for four out of ﬁve tasks.
Of these four tasks, in one case (relevance) ChatGPT has a slight edge but its perfor-
mance is very similar to that of MTurk. In the other three cases (frames I, frames II, and
stance), ChatGPT’s performance is between 2.2 and 3.4 times higher than that of MTurk.
Moreover, the level of accuracy of ChatGPT is, overall, more than adequate, considering
the diﬃculty of the tasks, the number of classes, and the fact that the annotations are
zero-shot. For relevance, with two classes (relevant/irrelevant), ChatGPT’s accuracy is
72.8%, while for stance it is 78.7% with three classes (positive/negative/neutral). As the
number of classes increases, accuracy decreases, although the intrinsic diﬃculty of the
task also plays a role. Regarding intercoder agreement, Figure 1 shows that ChatGPT’s
performance is very high, over 95% for all tasks when the temperature parameter is set
4

at 0.2. These values are higher than any of the human intercoder agreements, including
trained annotators. Even with the default temperature value of 1, which implies more
randomness, intercoder agreement consistently exceeds 84%. The relationship between
intercoder agreement and accuracy is positive, but weak (Pearson’s correlation coeﬃcient:
0.17). Although the correlation is based on just ﬁve data points, it suggests that a lower
temperature value may be preferable for annotation tasks, as it seems to increase the
consistency of the results without decreasing accuracy signiﬁcantly.
We underscore that the test to which we subjected ChatGPT is hard. Content mod-
eration is a complex topic, and the tasks are not a toy example.
Instead, they are
tasks that we conducted in the context of a previous study (Alizadeh et al., 2022), and
which required considerable resources. With the exception of stance, we developed the
conceptual categories for our particular research purposes, and the deﬁnitions required
extensive discussions in the team. Moreover, some of the tasks involve a large number of
classes, as many as fourteen for “Frames II”. The diﬃculty of this task is demonstrated
by the low degree of intercoder agreement among our trained annotators (about 50%).
Stance classiﬁcation involved only three classes, but intercoder agreement among our
trained annotators was relatively modest (78.3%), which shows that also this task was
not straightforward. However, ChatGPT still achieved an accuracy of 78.6% for stance.
We conclude that the performance of ChatGPT is impressive, particularly considering
that its annotations are zero-shot.
3
Discussion
This paper demonstrates the potential of LLMs to transform text-annotation procedures
for a variety of tasks common to many research projects. Despite the focus on a single
dataset and the relatively limited number of tests, the evidence strongly suggests that
LLMs may already be a superior approach compared to crowd-annotations on platforms
such as MTurk. At the very least, the ﬁndings demonstrate the importance of studying
the text-annotation properties and capabilities of LLMs more in depth. The following
questions and steps seem particularly promising: (i) performance of ChatGPT across
multiple languages; (ii) performance of ChatGPT across multiple types of text (social
media, news media, legislation, speeches, etc.); (iii) implementation of few-shot learn-
ing on ChatGPT, compared with ﬁne-tuned models such as BERT and RoBERTa; (iv)
construction of semi-automated data labeling systems in which a model ﬁrst learns by
observing human annotations, and is then used to recommend or even automate the label-
ing (Desmond et al., 2021); (v) using chain of thought prompting and other strategies to
increase the performance of zero-shot reasoning (Kojima et al., 2022); (vi) and of course
5

implementation of annotation tasks with GPT-4, as soon as availability permits.
4
Materials and Methods
Twitter Data
We used tweets about the issue of content moderation that we collected for a previous
study (Alizadeh et al., 2022). The original dataset covers all tweets containing relevant
keywords and hashtags, posted between January 2020 and April 2021 (2,586,466 original
tweets, 154,451 quoted retweets, and 6,991,897 retweets by 3,445,376 total unique users).
In this study, we focused on a subset of 2,382 tweets that were labeled by our trained
annotators.
Text Classiﬁcation Tasks
We implemented several text classiﬁcation tasks: (1) relevance: whether a tweet is about
content moderation; (2) topic detection: whether a tweet is about a set of six pre-deﬁned
topics (i.e. Section 230, Trump Ban, Complaint, Platform Policies, Twitter Support,
and others); (3) stance detection: whether a tweet is in favor of, against, or neutral
about repealing Section 230 (a piece of US legislation central to content moderation);
(4) general frame detection (“frames I”): whether a tweet contains a set of two opposing
frames which we call them “problem’ and “solution” frames. The solution frame describes
all tweets framing the issue of content moderation as a solution to other issues (e.g., hate
speech). The problem frame, on the other hand, describes all tweets framing content
moderation as a problem on its own as well as to other issues (e.g., free speech); (5)
policy frame detection (“frames II”): whether a tweet contains a set of fourteen policy
frames proposed in (Card et al., 2015). The full text of instruction for the ﬁve annotation
tasks is presented in Appendix S1. We used the exact same wordings for ChatGPT and
crowd-sourcing annotations.
Human Annotation
We trained a team of two political science graduate students to annotate tweets for all
ﬁve tasks explained in Section 4. For each task, the coders were given the same set of
instructions described in Section 4 and detailed in Appendix S1. Then, the coders were
asked to independently annotate the tweets task by task. To compute the accuracy of
ChatGPT and MTurk, we considered only tweets that both trained annotators agreed
upon.
6

Crowd-Workers Annotation
We employed MTurk workers to annotate the same set of tasks as for the human annota-
tors and ChatGPT. We provided the MTurk crowd workers the same set of instructions
as for the others as described in Section 4. To maximize annotation quality, we restricted
access to the tasks to workers who are classiﬁed as “MTurk Masters” by Amazon, who
have a HIT (Human Intelligence Task) approval rate greater than 90% with at least 50
approved HITs, and who are located in the US. Moreover, we ensured that no worker
could annotate more than 20% of the tweets for a given task. As with the trained human
annotators, each tweet was annotated by two diﬀerent crowd-workers.
ChatGPT Annotation
We used the ChatGPT API with the ‘gpt-3.5-turbo’ version to classify the tweets. The an-
notation was conducted between March 9 and March 20, 2023. For each annotation task,
we prompted ChatGPT with the corresponding annotation instruction text described in
Section 4 of Material and Methods and detailed in Appendix S1. We intentionally avoided
adding any ChatGPT-speciﬁc prompts such as ‘Let’s think step by step’ to ensure com-
parability between ChatGPT and MTurk crowd-workers. After testing several variations,
we decided to feed tweets one by one to ChatGPT using the following prompt: “Here’s
the tweet I picked, please label it as [Task Speciﬁc Instruction (e.g. ‘one of the topics in
the instruction’)].” We set the temperature parameter at 1 (default value) and 0.2 (which
makes the output more focused and deterministic as opposed to higher values, which
make the output more random) to explore diﬀerences across all tasks. For each tem-
perature setting, we collected two responses from ChatGPT to compute the intercoder
agreement. That is, we collected four ChatGPT responses for each tweet. We create a
new chat session for every tweet to ensure that the ChatGPT results are not inﬂuenced
by the history of annotations.
Evaluation Metrics
We consider the trained human annotations as our gold standard and compare the per-
formance of ChatGPT and MTurk crowd-workers with it. First, we compute average
accuracy (i.e. percentage of correct predictions), that is, the number of correctly clas-
siﬁed instances over the total number of cases to be classiﬁed. Second, we compared
the intercoder agreement of ChatGPT, MTurk crowd-workers, and trained human anno-
tators. To measure intercoder agreement, we computed the percentage of instances for
which both annotators in a given group are reporting the same class.
7

References
Alizadeh, M., F. Gilardi, E. Hoes, K. J. Kl¨user, M. Kubli, and N. Marchal (2022). Content
moderation as a political issue: The twitter discourse around trump’s ban. Journal of
Quantitative Description: Digital Media 2.
Argyle, L. P., E. C. Busby, N. Fulda, J. R. Gubler, C. Rytting, and D. Wingate (2023).
Out of One, Many: Using Language Models to Simulate Human Samples. Political
Analysis, 1–15.
Benoit, K., D. Conway, B. E. Lauderdale, M. Laver, and S. Mikhaylov (2016). Crowd-
sourced text analysis: Reproducible and agile production of political data. American
Political Science Review 116(2), 278–295.
Binz, M. and E. Schulz (2023). Using cognitive psychology to understand gpt-3. Proceed-
ings of the National Academy of Sciences 120(6), e2218523120.
Card, D., A. Boydstun, J. H. Gross, P. Resnik, and N. A. Smith (2015). The media
frames corpus: Annotations of frames across issues. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 438–
444.
Chmielewski, M. and S. C. Kucker (2020). An MTurk Crisis? Shifts in Data Quality
and the Impact on Study Results. Social Psychological and Personality Science 11(4),
464–473.
Desmond, M., E. Duesterwald, K. Brimijoin, M. Brachman, and Q. Pan (2021). Semi-
automated data labeling. In NeurIPS 2020 Competition and Demonstration Track, pp.
156–169. PMLR.
ElSherief, M., C. Ziems, D. Muchlinski, V. Anupindi, J. Seybolt, M. De Choudhury, and
D. Yang (2021). Latent Hatred: A Benchmark for Understanding Implicit Hate Speech.
Herrmann, M. and H. D¨oring (2023). Party Positions from Wikipedia Classiﬁcations of
Party Ideology. Political Analysis 31(1), 22–41.
Huang, F., H. Kwak, and J. An (2023).
Is chatgpt better than human annotators?
potential and limitations of chatgpt in explaining implicit hate speech. arXiv preprint
arXiv:2302.07736.
Kojima, T., S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa (2022). Large language
models are zero-shot reasoners. arXiv preprint arXiv:2205.11916.
8

Kuzman, T., I. Mozetiˇc, and N. Ljubeˇsi´c (2023). Chatgpt: Beginning of an end of manual
linguistic data annotation? use case of automatic genre identiﬁcation. arXiv e-prints,
arXiv–2303.
Nay, J. J. (2023). Large Language Models as Corporate Lobbyists.
Wu, P. Y., J. A. Tucker, J. Nagler, and S. Messing (2023). Large Language Models Can
Be Used to Estimate the Ideologies of Politicians in a Zero-Shot Learning Setting.
9

S1
Annotation Codebook
S1.1
Background on content moderation (to be used for all
tasks)
For this task, you will be asked to annotate a sample of tweets about content moderation.
Before describing the task, we explain what we mean by “content moderation”.
“Content moderation” refers to the practice of screening and monitoring content
posted by users on social media sites to determine if the content should be published
or not, based on speciﬁc rules and guidelines. Every time someone posts something on a
platform like Facebook or Twitter, that piece of content goes through a review process
(“content moderation”) to ensure that it is not illegal, hateful or inappropriate and that
it complies with the rules of the site. When that is not the case, that piece of content
can be removed, ﬂagged, labeled as or ‘disputed.’
Deciding what should be allowed on social media is not always easy. For example,
many sites ban child pornography and terrorist content as it is illegal. However, things are
less clear when it comes to content about the safety of vaccines or politics, for example.
Even when people agree that some content should be blocked, they do not always agree
about the best way to do so, how eﬀective it is, and who should do it (the government
or private companies, human moderators, or artiﬁcial intelligence).
S1.2
Task 1: Relevance
For each tweet in the sample, follow these instructions:
1. Carefully read the text of the tweet, paying close attention to details.
2. Carefully read the text of the tweet, paying close attention to details.
Tweets should be coded as RELEVANT when they directly relate to content modera-
tion, as deﬁned above. This includes tweets that discuss: social media platforms’ content
moderation rules and practices, governments’ regulation of online content moderation,
and/or mild forms of content moderation like ﬂagging.
Tweets should be coded as IRRELEVANT if they do not refer to content moderation,
as deﬁned above, or if they are themselves examples of moderated content. This would
include, for example, a Tweet by Donald Trump that Twitter has labeled as “disputed”, a
tweet claiming that something is false, or a tweet containing sensitive content. Such tweets
might be subject to content moderation, but are not discussing content moderation.
Therefore, they should be coded as irrelevant for our purposes.
10

S1.3
Task 2: Problem/Solution Frames
Content moderation can be seen from two diﬀerent perspectives:
• Content moderation can be seen as a PROBLEM; for example, as a restriction of
free speech
• Content moderation can be seen as a SOLUTION; for example, as a protection
from harmful speech
For each tweet in the sample, follow these instructions:
1. Carefully read the text of the tweet, paying close attention to details.
2. Classify the tweet as describing content moderation as a problem, as a solution, or
neither.
Tweets should be classiﬁed as describing content moderation as a PROBLEM if they
emphasize negative eﬀects of content moderation, such as restrictions to free speech, or
the biases that can emerge from decisions regarding what users are allowed to post.
Tweets should be classiﬁed as describing content moderation as a SOLUTION if they
emphasize positive eﬀects of content moderation, such as protecting users from various
kinds of harmful content, including hate speech, misinformation, illegal adult content, or
spam.
Tweets should be classiﬁed as describing content moderation as NEUTRAL if they do
not emphasize possible negative or positive eﬀects of content moderation, for example if
they simply report on the content moderation activity of social media platforms without
linking them to potential advantages or disadvantages for users or stakeholders.
S1.4
Task 3: Policy Frames
Content moderation, as described above, can be linked to various other topics, such as
health, crime, or equality.
For each tweet in the sample, follow these instructions:
1. Carefully read the text of the tweet, paying close attention to details.
2. Classify the tweet into one of the topics deﬁned below.
The topics are deﬁned as follows:
• ECONOMY: The costs, beneﬁts, or monetary/ﬁnancial implications of the issue
(to an individual, family, community, or to the economy as a whole).
11

• Capacity and resources: The lack of or availability of physical, geographical, spatial,
human, and ﬁnancial resources, or the capacity of existing systems and resources
to implement or carry out policy goals.
• MORALITY: Any perspective—or policy objective or action (including proposed
action)that is compelled by religious doctrine or interpretation, duty, honor, righ-
teousness or any other sense of ethics or social responsibility.
• FAIRNESS AND EQUALITY: Equality or inequality with which laws, punishment,
rewards, and resources are applied or distributed among individuals or groups. Also
the balance between the rights or interests of one individual or group compared to
another individual or group.
• CONSTITUTIONALITY AND JURISPRUDENCE: The constraints imposed on
or freedoms granted to individuals, government, and corporations via the Consti-
tution, Bill of Rights and other amendments, or judicial interpretation. This deals
speciﬁcally with the authority of government to regulate, and the authority of in-
dividuals/corporations to act independently of government.
• POLICY PRESCRIPTION AND EVALUATION: Particular policies proposed for
addressing an identiﬁed problem, and ﬁguring out if certain policies will work, or if
existing policies are eﬀective.
• LAW AND ORDER, CRIME AND JUSTICE: Speciﬁc policies in practice and their
enforcement, incentives, and implications. Includes stories about enforcement and
interpretation of laws by individuals and law enforcement, breaking laws, loopholes,
ﬁnes, sentencing and punishment. Increases or reductions in crime.
• SECURITY AND DEFENSE: Security, threats to security, and protection of one’s
person, family, in-group, nation, etc. Generally an action or a call to action that
can be taken to protect the welfare of a person, group, nation sometimes from a
not yet manifested threat.
• HEALTH AND SAFETY: Health care access and eﬀectiveness, illness, disease, san-
itation, obesity, mental health eﬀects, prevention of or perpetuation of gun violence,
infrastructure and building safety.
• QUALITY OF LIFE: The eﬀects of a policy on individuals’ wealth, mobility, access
to resources, happiness, social structures, ease of day-to-day routines, quality of
community life, etc.
12

• CULTURAL IDENTITY: The social norms, trends, values and customs constitut-
ing culture(s), as they relate to a speciﬁc policy issue.
• PUBLIC OPINION: References to general social attitudes, polling and demographic
information, as well as implied or actual consequences of diverging from or “getting
ahead of” public opinion or polls.
• POLITICAL: Any political considerations surrounding an issue. Issue actions or
eﬀorts or stances that are political, such as partisan ﬁlibusters, lobbyist involvement,
bipartisan eﬀorts, deal-making and vote trading, appealing to one’s base, mentions
of political maneuvering. Explicit statements that a policy issue is good or bad for
a particular political party.
• EXTERNAL REGULATION AND REPUTATION: The United States’ external
relations with another nation; the external relations of one state with another; or
relations between groups. This includes trade agreements and outcomes, compar-
isons of policy outcomes or desired policy outcomes.
• OTHER: Any topic that does not ﬁt into the above categories.
S1.5
Task 4: Stance Detection
In the context of content moderation, Section 230 is a law in the United States that
protects websites and other online platforms from being held legally responsible for the
content posted by their users. This means that if someone posts something illegal or
harmful on a website, the website itself cannot be sued for allowing it to be posted.
However, websites can still choose to moderate content and remove anything that violates
their own policies.
For each tweet in the sample, follow these instructions:
1. Carefully read the text of the tweet, paying close attention to details.
2. Classify the tweet as having a positive stance towards Section 230, a negative stance,
or a neutral stance.
S1.6
Task 5: Topic Detection
Tweets about content moderation may also discuss other related topics, such as:
1. Section 230, which is a law in the United States that protects websites and other
online platforms from being held legally responsible for the content posted by their
users (SECTION 230).
13

2. The decision by many social media platforms, such as Twitter and Facebook, to
suspend Donald Trump’s account (TRUMP BAN).
3. Requests directed to Twitter’s support account or help center (TWITTER SUP-
PORT).
4. Social media platforms’ policies and practices, such as community guidelines or
terms of service (PLATFORM POLICIES).
5. Complaints about platform’s policy and practices in deplatforming and content
moderation or suggestions to suspend particular accounts, or complaints about
accounts being suspended or reported (COMPLAINTS).
6. If a text is not about the SECTION 230, COMPLAINTS, TRUMP BAN, TWIT-
TER SUPPORT, and PLATFORM POLICIES, then it should be classiﬁed in
OTHER class (OTHER).
For each tweet in the sample, follow these instructions:
1. Carefully read the text of the tweet, paying close attention to details.
2. Please classify the following text according to topic (deﬁned by function of the
text, author’s purpose and form of the text). You can choose from the following
classes: SECTION 230, TRUMP BAN, COMPLAINTS, TWITTER SUPPORT,
PLATFORM POLICIES, and OTHER
14

