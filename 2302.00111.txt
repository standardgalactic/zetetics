Learning Universal Policies via Text-Guided Video Generation
Yilun Du * 1 2 Mengjiao Yang * 3 2 Bo Dai 2 Hanjun Dai 2 Oﬁr Nachum 2
Joshua B. Tenenbaum 1 Dale Schuurmans 2 4 Pieter Abbeel 3
Abstract
A goal of artiﬁcial intelligence is to construct
an agent that can solve a wide variety of tasks.
Recent progress in text-guided image synthesis
has yielded models with an impressive ability to
generate complex novel images, exhibiting combi-
natorial generalization across domains. Motivated
by this success, we investigate whether such tools
can be used to construct more general-purpose
agents. Speciﬁcally, we cast the sequential deci-
sion making problem as a text-conditioned video
generation problem, where, given a text-encoded
speciﬁcation of a desired goal, a planner synthe-
sizes a set of future frames depicting its planned
actions in the future, after which control actions
are extracted from the generated video. By lever-
aging text as the underlying goal speciﬁcation,
we are able to naturally and combinatorially gen-
eralize to novel goals. The proposed policy-as-
video formulation can further represent environ-
ments with different state and action spaces in a
uniﬁed space of images, which, for example, en-
ables learning and generalization across a variety
of robot manipulation tasks. Finally, by leverag-
ing pretrained language embeddings and widely
available videos from the internet, the approach
enables knowledge transfer through predicting
highly realistic video plans for real robots. 1
1. Introduction
Building models that solve a diverse set of tasks has be-
come a dominant paradigm in the domains of vision and
language. In natural language processing, large pretrained
models have demonstrated remarkable zero-shot learning of
new language tasks (Brown et al., 2020; Chowdhery et al.,
*Equal contribution. Order determined by coin ﬂip.
1MIT
2Google Research, Brain Team 3UC Berkeley 4University of
Alberta.
Correspondence to: Yilun Du <yilundu@mit.edu>,
Mengjiao Yang <sherryy@berkeley.edu>.
1See
video
visualizations
at
https://
universal-policy.github.io/unipi/.
2022; Hoffmann et al., 2022). Similarly, in computer vision,
models such as those proposed in (Radford et al., 2021;
Alayrac et al., 2022) have shown remarkable zero-shot clas-
siﬁcation and object recognition capabilities. A natural next
step is to use such tools to construct agents that can complete
different decision making tasks across many environments.
However, training such agents faces the inherent challenge
of environmental diversity, since different environments op-
erate with distinct state action spaces (e.g., the joint space
and continuous controls in MuJoCo are fundamentally dif-
ferent from the image space and discrete actions in Atari).
Such diversity hampers knowledge sharing, learning, and
generalization across tasks and environments. Although
substantial effort has been devoted to encoding different
environments with universal tokens in a sequence model-
ing framework (Reed et al., 2022), it is unclear whether
such an approach can preserve the rich knowledge embed-
ded in pretrained vision and language models and leverage
this knowledge to transfer to downstream reinforcement
learning (RL) tasks. Furthermore, it is difﬁcult to construct
reward functions which specify different tasks across envi-
ronments.
In this work, we address the challenges in environment di-
versity and reward speciﬁcation by leveraging video (i.e.,
image sequences) as a universal interface for conveying
action and observation behavior in different environments,
and text as a universal interface for expressing task descrip-
tions. In particular, we design a video generator as a planner
that sequentially conditions on a current image frame and
a text snippet describing a current goal (i.e., the next high-
level step) to generate a trajectory in the form of an image
sequence, after which an inverse dynamics model is used
to extract the underlying actions from the generated video.
Such an approach allows the universal nature of language
and video to be leveraged in generalizing to novel goals and
tasks across diverse environments. Speciﬁcally, we instanti-
ate the text-conditioned video generation model using video
diffusion. A set of underlying actions are then regressed
from the synthesized frames and used to construct a policy
to implement the planned trajectory. The proposed model,
UniPi, is visualized in Figure 1.
We have found that formulating policy generation via text-
conditioned video synthesis yields the following advantages:
arXiv:2302.00111v2  [cs.AI]  2 Feb 2023

Learning Universal Policies via Text-Guided Video Generation
Cut A Pineapple in 
A Few Steps
Move A Red Block 
to A Brown Box
Wipe Plate with  
Yellow Sponge
Wipe A Brown Box 
Before Pick up A Red 
Block. Then Put Red 
Block on Brown Box
Text-Conditioned Video Generation
Combinatorial Language
Long-Horizon Planning
Multi-Task Generalization
Internet-Scale Knowledge Transfer
Wipe the 
Counter with 
Robot Arm and 
Red Cloth
Figure 1. Text-Conditional Video Generation as Universal Policies. Text-conditional video generations enables us to train general
purpose policies on wide sources of data (simulated, real robots and YouTube) which may be applied to downstream multi-task settings
requiring combinatorical language generalization, long-horizon planning, or internet-scale knowledge.
Combinatorial Generalization.
The rich combinatorial
nature of language can be leveraged to synthesize novel com-
binatorial behaviors in the environment. This enables the
proposed approach to rearrange objects to new unseen com-
binations of geometric relations, as shown in Section 4.1.
Multi-task Learning.
Formulating action prediction as
a video prediction problem readily enables learning across
many different tasks. We illustrate in Section 4.2 how this
enables learning across language-conditioned tasks and gen-
eralizing to new ones at test time without ﬁnetuning.
Action Planning.
The video generation procedure corre-
sponds to a planning procedure where a sequence of frames
representing actions is generated to reach the target goal.
Such a planning procedure is naturally hierarchical: a tem-
porally sparse sequence of images toward a goal can ﬁrst be
generated, before being reﬁned with a more speciﬁc plan.
Moreover, the planning procedure is steerable, in the sense
that the plan generation can be biased by new constraints
introduced at test-time through test-time sampling. Finally,
plans are produced in a video space that is naturally inter-
pretable by humans, making action veriﬁcation and plan
diagnosis easy. We illustrate the efﬁcacy of hierarchical
sampling in Table 2 and steerability in Figure 5.
Internet-Scale Knowledge Transfer.
By pretraining a
video generation model on a large-scale text-video dataset
recovered from the internet, one can recover a vast reposi-
tory of “demonstrations” that aid the construction of a text-
conditioned policy in novel environments. We illustrate how
this enables the realistic synthesis of robot motion videos
from given natural language instructions in Section 4.3.
The main contribution of this work is to formulate text-
conditioned video generation as a universal planning strat-
egy from which diverse behaviors can be synthesized. While
such an approach departs from typical policy generation in
RL, where subsequent actions to execute are directly pre-
dicted from a current state, we illustrate that UniPi exhibits
notable generalization advantages over traditional policy
generation methods across a variety of domains.
2. Problem Formulation
We ﬁrst motivate a new abstraction, the Uniﬁed Predictive
Decision Process (UPDP), as an alternative to the Markov
Decision Process (MDP) commonly used in RL, and then
show an instantiation of a UPDP with diffusion models.
2.1. Markov Decision Process
The Markov Decision Process (Puterman, 1994) is a broad
abstraction used to formulate many sequential decision mak-
ing problems. Many RL algorithms have been derived
from MDPs with empirical successes (Haarnoja et al., 2018;
Hafner et al., 2020; Zhang et al., 2022b), but existing algo-
rithms are typically unable to combinatorially generalize
across different environments. Such difﬁculty can be traced
back to certain aspects of the underlying MDP abstraction:
i) The lack of a universal state interface across different
control environments. In fact, since different environ-
ments typically have separate underlying state spaces, one
would need to a construct a complex state representation
to represent all environments, making learning difﬁcult.
ii) The explicit requirement of a real-valued reward function
in an MDP. The RL problem is usually deﬁned as maxi-
mizing the accumulated reward in an MDP. However, in
many practical applications, how to design and transfer
rewards is unclear and different across environments.
iii) The dynamics model in an MDP is environment and agent
dependent. Speciﬁcally, the dynamics model T(s′|s, a)
characterizing transition between states (s.s′) under ac-
tion a, is explicitly dependent to the environment and
action space of the agent, which can be signiﬁcantly dif-
ferent between different agents and tasks.
2.2. Uniﬁed Predictive Decision Process
These difﬁculties inspire us to construct an alternative ab-
straction for uniﬁed sequential decision making across many
environments. Our abstraction, termed Uniﬁed Predictive
Decision Process (UPDP), exploits images as a universal
interface across environments, texts as task speciﬁers to

Learning Universal Policies via Text-Guided Video Generation
avoid reward design, and a task-agnostic planning module
separated from environment-dependent control to enable
knowledge sharing and generalization.
Formally, we deﬁne a UPDP to be a tuple G = ⟨X, C, H, ρ⟩,
where X denotes the observation space of images, C denotes
the space of textual task descriptions, H ∈N is a ﬁnite
horizon length, and ρ(·|x0, c) : X × C →∆(X H) is a
conditional video generator. That is, ρ(·|xo, c) ∈∆(X H)
is a conditional distribution over H-step image sequences
determined by the ﬁrst frame x0 and the task description
c. Intuitively, ρ synthesizes H-step image trajectories that
illustrate possible paths for completing a target task c. For
simplicity, we focus on ﬁnite horizon, episodic tasks.
Given a UPDP G, we deﬁne a trajectory-task conditioned
policy π(·|{xh}H
h=0, c) : X H+1 × C →∆(AH) to be a
conditional distribution over H-step action sequences AH.
Ideally, π(·|{xh}H
h=0, c) speciﬁes a conditional distribu-
tion of action sequences that achieves the given trajectory
{xh}H
h=0 in the UPDP G for the given task c. To achieve
such an alignment, we will consider an ofﬂine RL scenario
where we have access to a dataset of existing experience
D = {(xi, ai)H−1
i=0 , xH, c}n
j=1 from which both ρ(·|x0, c)
and π(·|{xh}H
h=0, c) can be estimated.
In contrast to an MDP, a UPDP directly models video-based
trajectories and bypasses the need to specify a reward func-
tion beyond the textual task description. Since the space of
video observations X H and task descriptions C are both nat-
urally shared across environments and easily interpretable
by humans, any video-based planner ρ(·|x0, c) can be con-
veniently reused, transferred and debugged. Another beneﬁt
of a UPDP over an MDP is that UPDP isolates video-based
planner using ρ(·|x0, c) from deferred action selection us-
ing π(·|{xh}H
h=0, c). This design choice isolates planning
decisions from action-speciﬁc mechanisms, allowing the
planner to be environment and agent agnostic.
UPDP can be understood as implicitly planning over an
MDP and directly outputting the optimal trajectory under
instructions. The abstraction of UPDP bypasses reward de-
sign, state extraction and explicit planning, and allows for
non-Markovian modeling of image-based state space. How-
ever, learning a planner in UPDP requires videos and task
descriptions, whereas traditional MDPs do not require such
data, so whether MDP or UPDP is more suitable for a given
task depends on what types of training data is available.
Although the non-Markovian model and the requirement of
video and text data induce additional difﬁculties in UPDP
comparing to MDP, it is possible to leverage existing large
text-video models that have been pretrained on massive,
web-scale datasets to alleviate these complexities.
2.3. Diffusion Models for UPDP
Let τ = [x1, . . . , xH] ∈X H denote a sequence of images.
We leverage the signiﬁcant recent advances in diffusion
Video Diffusion
Tiling
Temporal Super 
Resolution
Inverse Dynamics
Robot Actions
Move Joint = [Δx, Δy]
Rotate = Δw …
Rotate
Universal Policy (UniPi)
Inverse Dynamics
Text
Figure 2. Given an input observation and text instruction, we plan
a set of images representing agent behavior. Images are converted
to actions using an inverse dynamics model.
models for capturing the conditional distribution ρ(τ|x0, c),
which we will leverage as a text and initial-frame condi-
tioned video generator in a UPDP. We emphasize that the
UPDP formulation is also compatible with other probabilis-
tic models, such as a variational autoencoder (Kingma &
Welling, 2013), energy-based model (Du & Mordatch, 2019;
Dai et al., 2019), or generative adversarial network (Goodfel-
low et al., 2014). For completeness we brieﬂy cover the core
formulation at a high-level, but defer details to background
references (Sohl-Dickstein et al., 2015).
We start with an unconditional model. A continuous-time
diffusion model deﬁnes a forward process qk(τk|τ) =
N(·; αkτ, σ2
kI), where k ∈[0, 1] and αk, σ2
k are scalars
with predeﬁned schedules. A generative process p(τ) is
also deﬁned, which reverses the forward process by learn-
ing a denoising model s(τk, k). Correspondingly τ can
be generated by simulating this reverse process with an
ancestral sampler (Ho et al., 2020) or numerical integra-
tion (Song et al., 2020). In our case, the unconditional
model needs to be further adapted to condition on both
the text instruction c and the initial image x0. Denote
the conditional denoiser as s(τk, k|c, x0).
We leverage
classiﬁer-free guidance (Ho & Salimans, 2022) and use
ˆs(τk, k|c, x0) = (1 + ω)s(τk, k|c, x0) −ωs(τk, k) as the
denoiser in the reverse process for sampling, where ω con-
trols the strength of the text and ﬁrst-frame conditioning.
3. Decision Making with Videos
Next we describe our proposed approach UniPi in detail,
which is a concrete instantiation of the diffusion UPDP.
UniPi incorporates each of the two main components dis-
cussed in Section 2 and shown in Figure 2: (i) a diffu-
sion model for the universal video-based planner ρ(·|x0, c),
which synthesizes videos conditioned on the ﬁrst frame and
task descriptions; and (ii) a task-speciﬁc action generator
π(·|{xh}H
h=0, c), which infers actions sequences from gen-
erated videos through inverse dynamics modeling.

Learning Universal Policies via Text-Guided Video Generation
3.1. Universal Video-Based Planner
Encouraged by the recent success of text-to-video models
(Ho et al., 2022a), we seek to construct a video diffusion
module as the trajectory planner, which can faithfully syn-
thesize future image frames given an initial frame and tex-
tual task description. However, the desired planner departs
from the typical setting in text-to-video models (Villegas
et al., 2022; Ho et al., 2022a) which normally generate
unconstrained videos given a text description. Planning
through video generation is more challenging as it requires
models to both be able to generate constrained videos that
start at a speciﬁed image, and then complete the target task.
Moreover, to ensure valid action inference across synthe-
sized frames in a video, the video prediction module needs
to be able to track the underlying environment state across
synthesized video frames.
Conditional Video Synthesis.
To generate a valid and
executable plan, a text-to-video model must synthesize a
constrained video plan starting at the current observed im-
age. One approach to solve this problem is to modify the
underlying test-time sampling procedure of an unconditional
model, by ﬁxing the ﬁrst frame of the generated video plan
to always begin at the observed image, as done in (Janner
et al., 2022). However, we found that this performed poorly
and led to subsequent frames in the video plan to deviate
signiﬁcantly from the original observed image. Instead, we
found it more effective to explicitly train a constrained video
synthesis model by providing the ﬁrst frame of each video
as explicit conditioning context during training.
Trajectory Consistency through Tiling.
Existing text-
to-video models typically generate videos where the un-
derlying environment state changes signiﬁcantly during the
temporal duration (Ho et al., 2022a). To construct an accu-
rate trajectory planner, it is important that the environment
remain consistent across all time points. To enforce en-
vironment consistency in conditional video synthesis, we
provide, as additional context, the observed image when
denoising each frame in the synthesized video. In particular,
we re-purposed a temporal super-resolution video diffusion
architecture, and provided as context the conditioned vi-
sual observation tiled across time, as the opposed to a low
temporal-resolution video for denoising at each timestep.
In this model, we directly concatenate each intermediate
noisy frame with the conditioned observed image across
sampling steps, which serves as a strong signal to maintain
the underlying environment state across time.
Hierarchical Planning.
When constructing plans in high
dimensional environments with long time horizons, directly
generating a set of actions to reach a goal state quickly
becomes intractable due to the exponential blow-up of the
underlying search space. Planning methods often circum-
vent this issue by leveraging a natural hierarchy in planning.
Speciﬁcally, planning methods ﬁrst construct coarse plans
operating on low dimensional states and actions, which may
then be reﬁned into plans in the underlying state and action
spaces. Similar to planning, our conditional video genera-
tion procedure likewise exhibits a natural temporal hierarchy.
We ﬁrst generate videos at a coarse level by sparsely sam-
pled videos (“abstractions”) of our desired behavior along
the time axis. Then we reﬁne the videos to represent valid be-
havior in the environment by super-resolving videos across
time. Meanwhile, coarse-to-ﬁne super-resolution further
improves consistency via interpolation between frames.
Flexible Behavioral Modulation.
When planning a se-
quence of actions to a given sub-goal, one can readily incor-
porate external constraints to modulate the generated plan.
Such test-time adaptability can be implemented by compos-
ing a prior h(τ) during plan generation to specify desired
constraints across the synthesized action trajectory (Janner
et al., 2022), which is also compatible with UniPi. In partic-
ular, the prior h(τ) can be speciﬁed using a learned classiﬁer
on images to optimize a particular task, or as a Dirac delta
on a particular image to guide a plan towards a particular
set of states. To train the text-conditioned video generation
model, we utilize the video diffusion algorithm in (Ho et al.,
2022a), where pretrained language features from T5 (Raffel
et al., 2020) are encoded. Please see Appendix A for the
underlying architecture and training details.
3.2. Task Speciﬁc Action Adaptation
Given a set of synthesized videos, we may train a small
task-speciﬁc inverse-dynamics model to translate frames
into a set of actions as described below.
Inverse Dynamics.
We train a small model to estimate
actions given input images. The training of the inverse
dynamics is independent from the planner and can be done
on a separate, smaller and potentially suboptimal dataset
generated by a simulator.
Action Execution.
Finally, we generate an action se-
quence given x0 and c by synthesizing H image frames
and applying the learned inverse-dynamics model to pre-
dict the corresponding H actions. Inferred actions can then
be executed via closed-loop control, where we generate H
new actions after each step of action execution (i.e., model
predictive control), or via open-loop control, where we se-
quentially execute each action from the intially inferred
action sequence. For computational efﬁciency, we use an
open-loop controller in all our experiments in this paper.
4. Experimental Evaluation
The focus of these experiments is to evaluate UniPi in terms
of its ability to enable effective, generalizable decision mak-
ing. In particular, we evaluate
(1) the ability to combinatorially generalize across different
subgoals in Section 4.1,

Learning Universal Policies via Text-Guided Video Generation
Seen
Novel
Model
Place
Relation
Place
Relation
State + Transformer BC (Brohan et al., 2022)
19.4 ± 3.7
8.2 ± 2.0
11.9 ± 4.9
3.7 ± 2.1
Image + Transformer BC (Brohan et al., 2022)
9.4 ± 2.2
11.9 ± 1.8
9.7 ± 4.5
7.3 ± 2.6
Image + TT (Janner et al., 2021)
17.4 ± 2.9
12.8 ± 1.8
13.2 ± 4.1
9.1 ± 2.5
Diffuser (Janner et al., 2022)
9.0 ± 1.2
11.2 ± 1.0
12.5 ± 2.4
9.6 ± 1.7
UniPi (Ours)
59.1 ± 2.5
53.2 ± 2.0
60.1 ± 3.9
46.1 ± 3.0
Table 1. Task Completion Accuracy on Combinatorial Environment. UniPi generalizes well to both seen and novel combinations of
language prompts in Place (e.g., place X in Y) and Relation (e.g., place X to the left of Y) tasks.
Put A Yellow Block 
in the Brown Box
Put An Orange Block 
Left of A Red Block
Put A Red Block on 
A Purple Block
Input Frame
Synthesized Frames
Figure 3. Combinatorial Video Generation. Generated videos for unseen language goals at test time.
Synthesized Frames
Put the Right 
Cyan Block 
on An Orange 
Block
Executed Action Frames
Figure 4. Action Execution. Synthesized video plans and exe-
cuted actions in the simulated environment. The two video plans
roughly align with each other.
(2) the ability to effectively learn and generalize across many
tasks in Section 4.2,
(3) the ability to leverage existing videos on the internet to
generalize to complex tasks in Section 4.3.
See experimental details in Appendix A. Additional re-
sults are given in Appendix B and videos at https:
//universal-policy.github.io/unipi/.
4.1. Combinatorial Policy Synthesis
First, we measure the ability of UniPi to combinatorially
generalize to different language tasks.
Setup.
To measure combinatorial generalization, we use
the combinatorial robot planning tasks in (Mao et al., 2022).
In this task, a robot must manipulate blocks in an environ-
ment to satisfy language instructions, i.e., put a red block
right of a cyan block. To accomplish this task, the robot
Put the Right 
Cyan Block 
on An Orange 
Block
Synthesized Frames
Put the Left 
Cyan Block 
on An Orange 
Block
Input Frame
Intermediate 
Guidance
Figure 5. Adaptable Planning. By guiding test-time sampling
towards a an intermediate image, we can adapt our planning proce-
dure to move a particular block.
must ﬁrst pick up a white block, place it in the appropriate
bowl to paint it a particular color, and then pick up and place
the block in a plate so that it satisﬁes the speciﬁed relation.
In contrast to (Mao et al., 2022) which uses pre-programmed
pick and place primitives for action prediction, we predict
actions in the continuous robotic joint space for both the
baselines and our approach.
We split the language instructions in this environment into
two sets: one set of instructions (70%) that is seen during
training, and another set (30%) that is only seen during
testing. The precise locations of individual blocks, bowls,
and plates in the environment are fully randomized in each
environment iteration. We train the video model on 200k
example videos of generated language instructions in the
train set. Details of this environment can be found in Ap-
pendix A. We constructed demonstrations of videos in this
task by using a scripted agent.

Learning Universal Policies via Text-Guided Video Generation
Put the Red Blocks in 
A Gray Bowl
Pack All the Porcelain 
Salad Plate Objects in 
the Brown Box
Pack All the Green 
and Blue Blocks into 
the Brown Box
Input Frame
Synthesized Frames
Figure 6. Multitask Video Generation. Generated video plans on different new test tasks in the multitask setting.
Frame
Frame
Temporal
Condition
Consistency
Heirarchy
Place
Relation
No
No
No
13.2 ± 3.2
12.4 ± 2.4
Yes
No
No
52.4 ± 2.9
34.7 ± 2.6
Yes
Yes
No
53.2 ± 3.0
39.4 ± 2.8
Yes
Yes
Yes
59.1 ± 2.5
53.2 ± 2.0
Table 2. Task Completion Accuracy Ablations. Each compo-
nent of UniPi improves its performance.
Baselines.
We compare the proposed approach with three
separate representative approaches. First, we compare to ex-
isting work that uses goal-conditioned transformers to learn
across multiple environments, where goals can be speciﬁed
as episode returns (Lee et al., 2022), expert demonstrations
(Reed et al., 2022), or text and images (Brohan et al., 2022).
To represent these baselines, we construct a transformer be-
havior cloning (BC) agent to predict the subsequent action
to execute given the task description and either the visual
observation (Image + Transformer BC) or the underlying
robot joint state (State + Transformer BC). Second, given
that our approach regresses a sequence of actions to ex-
ecute, we further compare with transformer models that
regress a sequence of future actions to execute, similar to
the goal-conditioned behavioral cloning of the Trajectory
Transformer (Janner et al., 2021) (Image + TT). Finally, to
highlight the importance of the video-as-policy approach,
we compare UniPi with learning a diffusion process that,
conditioned on an image observation, directly infers future
robot actions in the joint space (as opposed to diffusing
future image frames), corresponding to (Janner et al., 2022;
Ajay et al., 2022). For both our method and each baseline,
we condition the policy on encoded language instructions
using pretrained T5 embeddings. Note that in this setting,
existing ofﬂine reinforcement learning baselines are not di-
rectly applicable as we do not have access to the reward
functions in the environment.
Metrics.
To compare UniPi with baselines, we measure
ﬁnal task completion accuracy across new instances of the
environment and associated language prompts. We subdi-
Place
Pack
Pack
Model
Bowl
Object
Pair
State + Transformer BC
9.8 ± 2.6
21.7 ± 3.5
1.3 ± 0.9
Image + Transformer BC
5.3 ± 1.9
5.7 ± 2.1
7.8 ± 2.6
Image + TT
4.9 ± 2.1
19.8 ± 0.4
2.3 ± 1.6
Diffuser
14.8 ± 2.9
15.9 ± 2.7
10.5 ± 2.4
UniPi (Ours)
51.6 ± 3.6
75.5 ± 3.1
45.7 ± 3.7
Table 3. Task Completion Accuracy on Multitask Environ-
ment. UniPi generalizes well to new environments when trained
on a set of different multi-task environments.
vide the evaluation along two axes: (1) whether the language
instruction has been seen during training and (2) whether
the language instruction speciﬁes placing a block in relation
to some other block as opposed to direct pick-and-place.
Combinatorial Generalization.
In Table 1, we ﬁnd that
UniPi generalizes well to both seen and novel combinations
of language prompts . We illustrate our action generation
pipeline in Figure 4 and different generated video plans
using our approach in Figure 3.
Ablations.
In Table 2, we ablate UniPi on seen language
instructions and in-relation-to tasks. Speciﬁcally, we study
the effect of conditioning the video generative model on
the ﬁrst observation frame (frame condition), tiling the ob-
served frame across timesteps (frame consistency) and super-
resolving video generation across time (temporal hierarchy).
All components of UniPi are crucial for good performance.
In settings where frame consistency is not enforced, we pro-
vide a zeroed out image as context to the non-start frames
in a video.
Adaptability.
We next assess the ability of UniPi to adapt
at test time to new constraints. In Figure 5, we illustrate
the ability to construct plans which color and move one
particular block to a speciﬁed geometric relation.
4.2. Multi-Environment Transfer
We next evaluate the ability of UniPi to effectively learn
across a set of different tasks and generalize, at test time, to
a new set of unseen environments.

Learning Universal Policies via Text-Guided Video Generation
Input Frame
Synthesized Frames
Flip Pot 
Upright in Sink
Turn Faucet
Left
Pick Up Sponge 
and Wipe Plate
Put Big Spoon from
Basket to Tray
Figure 7. High Fidelity Plan Generation. UniPi can generate high resolution video plans across different language prompts.
Synthesized Frames
Pick Up 
Yellow Corn
(Scratch)
Pick Up 
Yellow Corn
(Pretrained)
Put Carrot 
On Burner 
(Scratch)
Put Carrot 
On Burner 
(Pretrained)
Figure 8. Pretraining Enables Combinatorical Generalization.
Using internet pretraining enables UniPi to synthesize videos of
tasks not seen during training. In contrast, a model trained from
scratch incorrectly generates plans of different tasks.
Setup.
To measure multi-task learning and transfer, we
use the suite of language guided manipulation tasks from
(Shridhar et al., 2022). We train our method using demon-
strations across a set of 10 separate tasks from (Shridhar
et al., 2022), and evaluate the ability of our approach to
transfer to 3 different test tasks. Using a scripted oracle
agent, we generate a set of 200k videos of language execu-
tion in the environment. We report the underlying accuracy
in which each language instruction is completed. Details of
this environment can be found in Appendix A.
Baselines.
We use the same baseline methods as in Sec-
tion 4.1. While our environment setting is similar to that of
(Shridhar et al., 2022), this method is not directly compa-
rable to our approach, as CLIPort abstracts actions to the
existing primitives of pick and place as opposed to using
joint space of a robot. CLIPort is also designed to solve the
signiﬁcantly simpler problem of inferring only the poses
upon which to pick and place objects (with no easy manner
to adapt to our setting).
Input Frame
Synthesized Frame
Figure 9. Robustness to Background Change. UniPi learns to
be robust to changes of underlying background, such as black
cropping or the addition of photo-shopped objects.
Multitask Generalization.
In Table 3 we present re-
sults of our approach and baselines across new tasks. Our
approach is able to generalize and synthesize new videos
and decisions of different language tasks, and can generate
videos consisting of picking different kinds of objects and
different colored objects. We further present video visual-
izations of our approach in Figure 6.
4.3. Real World Transfer
Finally we evaluate the extent to which UniPi can generalize
to real world scenarios and construct complex behaviors by
leveraging widely available videos on the internet.
Setup.
Our training data consists of an internet-scale pre-
training dataset and a smaller real-world robotic dataset. The
pretraining dataset uses the same data as (Ho et al., 2022a),
which consists of 14 million video-text pairs, 60 million
image-text pairs, and the publicly available LAION-400M
image-text dataset. The robotic dataset is adopted from the

Learning Universal Policies via Text-Guided Video Generation
Model (24x40)
CLIP Score ↑
FID ↓
FVD ↓
No Pretrain
24.43 ± 0.04
17.75 ± 0.56
288.02 ± 10.45
Pretrain
24.54 ± 0.03
14.54 ± 0.57
264.66 ± 13.64
Table 4. Video Generation Quality of UniPi on Real Environ-
ment. The use of existing data on the internet improves video plan
predictions under all metrics considered.
Bridge dataset (Ebert et al., 2021) with 7.2k video-text pairs,
where we use the task IDs as texts. We partition the 7.2k
video-text pairs into train (80%) and test (20%) splits. We
pretrain UniPi on the pretraining dataset followed by ﬁne-
tuning on the train split of the Bridge data. Architectural
details can be found in Appendix A.
Video Synthesis.
We are particularly interested in the
effect of pretraining on internet-scale video data that is not
robotic speciﬁc. We report the CLIP scores, FIDs, and VIDs
(averaged across frames and computed on 32 samples) of
UniPi trained on Bridge data, with and without pretrain-
ing. As shown in Table 4, UniPi with pretraining achieves
signiﬁcantly higher FID and FVD and a marginally better
CLIP score than UniPi without pretraining, suggesting that
pretraining on non-robot data helps with generating plans
for robots. Interestingly, UniPi without pretraining often
synthesizes plans that fail to complete the task (Figure 7),
which is not well reﬂected in the CLIP score, suggesting the
need for better generation metrics for control-speciﬁc tasks.
Generalization.
We ﬁnd that internet-scale pretraining
enables UniPi to generalize to novel task commands and
scenes in the test split not seen during training, whereas
UniPi trained only on task-speciﬁc robot data fails to gener-
alize. Speciﬁcally, Figure 8 shows the generalization results
of novel task commands that do not exist in the Bridge
dataset. Additionally, UniPi is relatively robust to back-
ground changes such as black cropping or the addition of
photo-shopped objects as shown in Figure 9.
5. Related Work
Learning Generative Models of the World.
Models
trained to generate environment rewards and dynamics that
can serve as “world models” for model-based reinforce-
ment learning and planning have been recently scaled to
large-scale architectures developed for vision and language
(Hafner et al., 2020; Janner et al., 2021; Zhang et al., 2021;
Micheli et al., 2022). These works separate learning the
world model from planning and policy learning, and ar-
guably present a mismatch between the generative modeling
objective of the world and learning optimal policies. Addi-
tionally, learning a world model requires the training data to
be in a strict state-action-reward format, which is incompati-
ble with the largely available datasets on the internet, such as
YouTube videos. While methods such as VPT (Baker et al.,
2022) can utilize internet-scale data through learning an
inverse dynamics model to label unlabled videos, an inverse
dynamics model itself does not support model-based plan-
ning or reinforcement learning to further improve learned
policies beyond imitation learning. Our text-conditioned
video policies can be seen as jointly learning the world
model and conducting hierarchical planning simultaneously,
and is able to leverage widely available datasets that are not
speciﬁcally designed for sequential decision making.
Diffusion Models for Decision Making.
Diffusion mod-
els have recently been applied to different decision making
problems (Janner et al., 2022; Ajay et al., 2022; Wang et al.,
2022; Zhang et al., 2022a; Urain et al., 2022; Zhong et al.,
2022). Most similar to this work, Janner et al. (2022) trained
an unconditional diffusion model to generate trajectories
consisting of joint-based states and actions, and used a sepa-
rately trained reward model to select generated plans. Ajay
et al. (2022) on the other hand, trained a conditional dif-
fusion model to guide behavior synthesis from desired re-
wards, constraints or agent skills. Unlike both works, which
learn task-speciﬁc policies from scratch, our approach of
text-condition video generation as a universal policy can
leverage internet-scale knowledge to learn generalist agents
that can be deployed to a variety of novel tasks and envi-
ronments. Additionally, Kapelyukh et al. (2022) applied
web-scale text-conditioned image diffusion to generate a
goal image to condition a policy on, whereas our work uses
video diffusion to learning universal policies directly.
Learning Generalist Agents.
Inspired by the large-scale
pretraining success of vision and language domains, large-
scale sequence and image models have recently been applied
to learning generalist decision making agents (Reed et al.,
2022; Lee et al., 2022; Kumar et al., 2022). However, these
generalist agents can only operate under environments with
the same state and action spaces (e.g., Atari games) (Lee
et al., 2022; Kumar et al., 2022), or require studious tok-
enization (Reed et al., 2022) that might seem unnatural in re-
inforcement learning settings where different environments
have distinct state and actions spaces. Another downside
of using customized tokens for control is the inability to
directly utilize knowledge from pretrained vision and lan-
guage models. Our approach, on the other hand, uses text
and images as universal interfaces for policy learning so that
the knowledge from pretrained vision and language models
can be preserved. Our choice of diffusion as opposed to
autoregressive sequence modeling also enables long-term
and hierarchical planning.
Learning Text-Conditioned Policies.
There has been a
growing amount of work using text commands as a way
to learn multi-task and generalist control policies (Huang
et al., 2022; Ahn et al., 2022; Brohan et al., 2022; Lynch &
Sermanet, 2020; Mahmoudieh et al., 2022; Liu et al., 2022).
Different from our framing of video-as-policies, existing
work directly trains a language-conditioned control policy
in the action space of some speciﬁc robot, leaving cross-

Learning Universal Policies via Text-Guided Video Generation
morphology multi-environment learning of generalist agents
as an unsolved problem. We believe this paper is the ﬁrst
to propose images as a universal state and action space to
enable broad knowledge transfer across environments, tasks,
and even between humans and robots.
6. Conclusion
We have demonstrated the utility of representing policies
using text-conditioned video generation, showing that this
enables effective combinatorial generalization, multi-task
learning, and real world transfer. These positive results
point to the broader direction of using generative models
and the wealth of data on the internet as powerful tools to
generate general-purpose decision making systems.
Acknowledgements
Thanks to Chitwan Saharia and Ruiqi Gao for the assis-
tance with video diffusion setup. Thanks to Douglas Eck,
Zoubin Ghahramani, George Tucker, Wilson Yan, Young-
gyo Seo, Youngwoon Lee for reviewing draft versions of
this manuscript. Thanks to Shane Gu for initial discussion
on the idea. We gratefully acknowledges the support of a
Canada CIFAR AI Chair, NSERC and Amii, NSF GRFP,
and support from Berkeley BAIR industrial consortion.
References
Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O.,
David, B., Finn, C., Gopalakrishnan, K., Hausman, K.,
Herzog, A., et al. Do as i can, not as i say: Ground-
ing language in robotic affordances.
arXiv preprint
arXiv:2204.01691, 2022. 8
Ajay, A., Du, Y., Gupta, A., Tenenbaum, J., Jaakkola,
T., and Agrawal, P.
Is conditional generative model-
ing all you need for decision-making? arXiv preprint
arXiv:2211.15657, 2022. 6, 8
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,
Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds,
M., et al. Flamingo: A visual language model for few-
shot learning. NeurIPS, 2022. URL https://arxiv.
org/abs/2204.14198. 1
Baker, B., Akkaya, I., Zhokhov, P., Huizinga, J., Tang, J.,
Ecoffet, A., Houghton, B., Sampedro, R., and Clune, J.
Video pretraining (vpt): Learning to act by watching un-
labeled online videos. arXiv preprint arXiv:2206.11795,
2022. 8
Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J.,
Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,
Hsu, J., et al. Rt-1: Robotics transformer for real-world
control at scale. arXiv preprint arXiv:2212.06817, 2022.
5, 6, 8
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
Advances in neural information processing systems, 33:
1877–1901, 2020. 1
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
G., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,
Gehrmann, S., et al. Palm: Scaling language modeling
with pathways. arXiv preprint arXiv:2204.02311, 2022.
1
Dai, B., Liu, Z., Dai, H., He, N., Gretton, A., Song, L.,
and Schuurmans, D. Exponential family estimation via
adversarial dynamics embedding. Advances in Neural
Information Processing Systems, 32, 2019. 3
Du, Y. and Mordatch, I.
Implicit generation and gen-
eralization in energy-based models.
arXiv preprint
arXiv:1903.08689, 2019. 3
Ebert, F., Yang, Y., Schmeckpeper, K., Bucher, B., Geor-
gakis, G., Daniilidis, K., Finn, C., and Levine, S. Bridge
data: Boosting generalization of robotic skills with cross-
domain datasets. arXiv preprint arXiv:2109.13396, 2021.
8
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
Warde-Farley, D., Ozair, S., Courville, A., and Bengio,
Y. Generative adversarial nets. In Advances in Neural
Information Processing Systems. 2014. 3
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft
actor-critic: Off-policy maximum entropy deep reinforce-
ment learning with a stochastic actor. In International
conference on machine learning, pp. 1861–1870. PMLR,
2018. 2
Hafner, D., Lillicrap, T., Norouzi, M., and Ba, J. Mas-
tering atari with discrete world models. arXiv preprint
arXiv:2010.02193, 2020. 2, 8
Ho, J. and Salimans, T. Classiﬁer-free diffusion guidance.
arXiv preprint arXiv:2207.12598, 2022. 3
Ho, J., Jain, A., and Abbeel, P. Denoising diffusion prob-
abilistic models.
In Advances in Neural Information
Processing Systems, 2020. 3
Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko,
A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J.,
et al. Imagen video: High deﬁnition video generation
with diffusion models. arXiv preprint arXiv:2210.02303,
2022a. 4, 7, 12
Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M.,
and Fleet, D. J. Video diffusion models. arXiv preprint
arXiv:2204.03458, 2022b. 12

Learning Universal Policies via Text-Guided Video Generation
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,
Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,
Welbl, J., Clark, A., et al. Training compute-optimal
large language models. arXiv preprint arXiv:2203.15556,
2022. 1
Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence,
P., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y., et al.
Inner monologue: Embodied reasoning through planning
with language models. arXiv preprint arXiv:2207.05608,
2022. 8
Janner, M., Li, Q., and Levine, S. Ofﬂine reinforcement
learning as one big sequence modeling problem. Ad-
vances in neural information processing systems, 34:
1273–1286, 2021. 5, 6, 8, 12
Janner, M., Du, Y., Tenenbaum, J., and Levine, S. Planning
with diffusion for ﬂexible behavior synthesis. In Interna-
tional Conference on Machine Learning, 2022. 4, 5, 6, 8,
12
Kapelyukh, I., Vosylius, V., and Johns, E. Dall-e-bot: In-
troducing web-scale diffusion models to robotics. arXiv
preprint arXiv:2210.02438, 2022. 8
Kingma, D. P. and Welling, M. Auto-encoding variational
bayes. CoRR, abs/1312.6114, 2013. 3
Kumar, A., Agarwal, R., Geng, X., Tucker, G., and Levine,
S. Ofﬂine q-learning on diverse multi-task data both
scales and generalizes. arXiv preprint arXiv:2211.15144,
2022. 8
Lee, K.-H., Nachum, O., Yang, M., Lee, L., Freeman,
D., Xu, W., Guadarrama, S., Fischer, I., Jang, E.,
Michalewski, H., et al. Multi-game decision transformers.
arXiv preprint arXiv:2205.15241, 2022. 6, 8, 12
Liu, H., Lee, L., Lee, K., and Abbeel, P.
Instruction-
following agents with jointly pre-trained vision-language
models. arXiv preprint arXiv:2210.13431, 2022. 8
Lynch, C. and Sermanet, P. Language conditioned imi-
tation learning over unstructured data. arXiv preprint
arXiv:2005.07648, 2020. 8
Mahmoudieh, P., Pathak, D., and Darrell, T. Zero-shot
reward speciﬁcation via grounded natural language. In
ICLR 2022 Workshop on Generalizable Policy Learning
in Physical World, 2022. 8
Mao, J., Lozano-Perez, T., Tenenbaum, J. B., and Kaelbing,
L. P. PDSketch: Integrated Domain Programming, Learn-
ing, and Planning. In Advances in Neural Information
Processing Systems (NeurIPS), 2022. 5
Micheli, V., Alonso, E., and Fleuret, F.
Transform-
ers are sample efﬁcient world models. arXiv preprint
arXiv:2209.00588, 2022. 8
Puterman, M. L. Markov Decision Processes: Discrete
Stochastic Dynamic Programming. John Wiley & Sons,
Inc., 1994. 2
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,
Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,
et al. Learning transferable visual models from natural
language supervision. In International Conference on
Machine Learning, pp. 8748–8763. PMLR, 2021. 1
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y., Li, W., Liu, P. J., et al. Exploring
the limits of transfer learning with a uniﬁed text-to-text
transformer. J. Mach. Learn. Res., 21(140):1–67, 2020.
4, 12
Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G.,
Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky,
Y., Kay, J., Springenberg, J. T., et al. A generalist agent.
arXiv preprint arXiv:2205.06175, 2022. 1, 6, 8, 12
Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton,
E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S.,
Lopes, R. G., et al. Photorealistic text-to-image diffusion
models with deep language understanding. arXiv preprint
arXiv:2205.11487, 2022. 12
Shridhar, M., Manuelli, L., and Fox, D. Cliport: What and
where pathways for robotic manipulation. In Conference
on Robot Learning, pp. 894–906. PMLR, 2022. 7
Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and
Ganguli, S. Deep unsupervised learning using nonequi-
librium thermodynamics. In International Conference on
Machine Learning, 2015. 3
Song, J., Meng, C., and Ermon, S. Denoising diffusion
implicit models. arXiv preprint arXiv:2010.02502, 2020.
3
Urain, J., Funk, N., Chalvatzaki, G., and Peters, J. Se
(3)-diffusionﬁelds: Learning cost functions for joint
grasp and motion optimization through diffusion. arXiv
preprint arXiv:2209.03855, 2022. 8
Villegas, R., Babaeizadeh, M., Kindermans, P.-J., Moraldo,
H., Zhang, H., Saffar, M. T., Castro, S., Kunze, J., and
Erhan, D. Phenaki: Variable length video generation
from open domain textual description. arXiv preprint
arXiv:2210.02399, 2022. 4
Wang, Z., Hunt, J. J., and Zhou, M. Diffusion policies as an
expressive policy class for ofﬂine reinforcement learning.
arXiv preprint arXiv:2208.06193, 2022. 8

Learning Universal Policies via Text-Guided Video Generation
Zhang, E., Lu, Y., Wang, W., and Zhang, A. Lad: Language
augmented diffusion for reinforcement learning. arXiv
preprint arXiv:2210.15629, 2022a. 8
Zhang, M. R., Paine, T., Nachum, O., Paduraru, C., Tucker,
G., ziyu wang, and Norouzi, M. Autoregressive dynamics
models for ofﬂine policy evaluation and optimization. In
International Conference on Learning Representations,
2021. 8
Zhang, T., Ren, T., Yang, M., Gonzalez, J., Schuurmans, D.,
and Dai, B. Making linear mdps practical via contrastive
representation learning. In International Conference on
Machine Learning, pp. 26447–26466. PMLR, 2022b. 2
Zhong, Z., Rempe, D., Xu, D., Chen, Y., Veer, S., Che,
T., Ray, B., and Pavone, M. Guided conditional diffu-
sion for controllable trafﬁc simulation. arXiv preprint
arXiv:2210.17366, 2022. 8

Learning Universal Policies via Text-Guided Video Generation
A. Architecture, Training, and Evaluation Details
A.1. Video Diffusion Training Details
We use the same base architecture and training setup as Ho et al. (2022b) which utilizes Video U-Net architecture with
3 residual blocks of 512 base channels and channel multiplier [1, 2, 4], attention resolutions [6, 12, 24], attention head
dimension 64, and conditioning embedding dimension 1024. We use noise schedule log SNR with range [-20, 20]. We make
modiﬁcations Video U-Net to support ﬁrst-frame conditioning during training. Speciﬁcally, we replicate the ﬁrst frame to
be conditioned on at all future frame indices, and apply temporal super resolution model condition on the replicated ﬁrst
frame by concatenating the ﬁrst frame channel-wise to the noisy data similar to Saharia et al. (2022). We use temporal
convolutions as opposed to temporal attention to mix frames across time, to maintain local temporal consistency across
time, which has also been previously noted in Ho et al. (2022a). We train each of our video diffusion models for 2M steps
using batch size 2048 with learning rate 1e-4 and 10k linear warmup steps. We use 256 TPU-v4 chips for our ﬁrst-frame
conditioned generation model and temporal super resolution model.
We use T5-XXL (Raffel et al., 2020) to process input prompts which consists of 4.6 billion parameters. For combinatorial
and multi-task generalization experiments on simulated robotic manipulation, we train a ﬁrst-frame conditioned video
diffusion models on 10x48x64 videos (skipping every 8 frames) with 1.7B parameters and a temporal super resolution of
20x48x64 (skipping every 4 frames) with 1.7B parameters. The resolution of the videos are chosen so that the objects
being manipulated (e.g., blocks being moved around) are clearly visible in the video. For the real world video results,
we ﬁnetune the 16x40x24 (1.7B), 32x40x24 (1.7B), 32x80x48 (1.4B), and 32x320x192 (1.2B) temporal super resolution
models pretrained on the data used by Ho et al. (2022a).
A.2. Inverse Dynamics Training Details
UniPi’s inverse dynamics model is trained to directly predict the 7-dimensional controls of the simulated robot arm from
an image observation mean squared error. The inverse dynamics model consists of a 3x3 convolutional layer, 3 layers of
3x3 convolutions with residual connection, a mean-pooling layer across all pixel locations, and an MLP layer of (128, 7)
channels to predict the ﬁnal controls. The inverse dynamics model is trained using the Adam optimizer with gradient norm
clipped at 1 and learning rate 1e-4 for a total of 2M steps where linear warmup is applied to the ﬁrst 10k steps.
A.3. Baselines Training Details
We describe the architecture details of various baselines below. The training details (e.g., learning rate, warm up, gradient
clip) of each baseline follow those of the inverse dynamics model detailed above.
Transformer BC (Reed et al., 2022; Lee et al., 2022).
We employ the same transformer architecture as the 10M model
of Lee et al. (2022) with 4 attention layers of 8 heads each and hidden size 512. We apply 4 layers of 3x3 convolution with
residual connection to extract image features, which, together with T5 text embeddings, are used as inputs to the transformer.
We additionally experimented with vision transformer style linearization of the image patches similar to Lee et al. (2022),
but found the performance to be similar. We use a context length of 4 and skip every 4 frames similar to UniPi’s inverse
dynamics. We tried increasing the context length of the transformer to 8 but it did not help improve performance.
Transformer TT (Janner et al., 2021).
We use a similar transformer architecture as the Transformer BC baseline detailed
above. Instead of predicting the immediate next control in the sequence as in Transformer BC, we predict the next 8 controls
(skipping every 4 controls similar to other baselines) at the output layer. We have also tried autoregressively predicting the
next 8 controls, but found the errors to accumulate quickly without additional discretization.
State-Based Diffusion (Janner et al., 2022).
For the state-based diffusion baseline, we use a similar architecture as
UniPi’s ﬁrst-frame conditioned video diffusion, where instead of diffusing and generating future image frames, we replicate
future controls across different pixel locations and apply the same U-Net structure as UniPi to learn state-based diffusion
models.
A.4. Details of the Combinatorial Planning Task
In the combinatorial planning tasks, we sample random 6 DOF poses for blocks, colored bowls, the ﬁnal placement box.
Blocks start off uncolored (white) and must be placed in a bowl to obtain a color. The robot then must manipulate and
move the colored block to have the desired geometric relation in the placement box. The underlying action space of the
agent corresponds to 6 joint values of robot plus a discrete contact action. When the contact action is active, the nearest
block on the table is attached to the robot gripper (where for methods that predict continuous actions, we thresholded action
prediction > 0.5 to correspond to contact). Given individual action predictions for different models, we simulate the next

Learning Universal Policies via Text-Guided Video Generation
state of the environment by running the joint controller in Pybullet to try reach the predicted joint state (with a timeout
of 2 seconds due to certain actions being physically infeasible). As only a subset of the video dataset contained action
annotations, we trained the inverse-dynamics model on action annotations from 20k generated videos.
A.5. Details of the CLIPort Multi-Environment Task
In the CLIPort environment, we use the same action space as the combinatorial planning tasks and execute actions similarly
using the built in joint controller in Pybullet. As our training data, we use a scripted agent on put-block-in-bowl-
unseen-colors, packing-unseen-google-objects-seq, assembling-kits-seq-unseen-colors,
stack-block-pyramid-seq-seen-colors,
tower-of-hanoi-seq-seen-colors,
assembling-
kits-seq-seen-colors,
tower-of-hanoi-seq-unseen-colors,
stack-block-pyramid-seq-
unseen-colors,
packing-seen-google-objects-seq,
packing-boxes-pairs-seen-colors,
packing-seen-google-objects-group. As our test data, we used the environments put-block-in-bowl-
seen-colors, packing-unseen-google-objects-group, packing-boxes-pairs-unseen-colors.
We trained the inverse dynamics on action annotation across the 200k generated videos.

Learning Universal Policies via Text-Guided Video Generation
B. Additional Results
B.1. Additional Results on Combinatorial Generalization
Put A Brown Block 
on An Orange Block
Put A Red Block Right 
of An Orange Block
Put a Yellow block in 
the Brown box
Input Frame
Synthesized Frames
Figure 10. Combinatorial Video Generation. Additional results on UniPi’s generated videos for unseen language goals at test time.
B.2. Additional Results on Multi-Environment Transfer
Put the Gray Blocks 
in A Brown Bowl
Pack All the Purple And 
Red Blocks into the 
Brown Box
Pack All the Pepsi 
Max Box Objects in 
the Brown Box
Input Frame
Synthesized Frames
Figure 11. Multitask Video Generation. Additional results on UniPi’s generated video plans on different new tasks in the multitask
setting.

Learning Universal Policies via Text-Guided Video Generation
B.3. Additional Results on Real-World Transfer
Close small box flaps
Lift bowl
Put potato on plate
Input Frame
Synthesized Frames
Put sweet potato in 
pot which is in sink 
distractors
Turn lever vertical to 
front distractors
Figure 12. High Fidelity Plan Generation. Additional results on UniPi’s high resolution video plans across different language prompts.

