Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics
and the 11th International Joint Conference on Natural Language Processing, pages 5596–5611
August 1–6, 2021. ©2021 Association for Computational Linguistics
5596
Maria: A Visual Experience Powered Conversational Agent
Zujie Liang1∗†
Huang Hu2† Can Xu2
Chongyang Tao2
Xiubo Geng2
Yining Chen2
Fan Liang1
Daxin Jiang2‡
1School of Electronics and Information Technology,
Sun Yat-sen University, Guangzhou, China
2Microsoft STCA NLP Group, Beijing, China
1{liangzj9@mail2.sysu.edu.cn, isslf@mail.sysu.edu.cn}
2{huahu,caxu,chotao,xigeng,yinichen,djiang}@microsoft.com
Abstract
Arguably, the visual perception of conversa-
tional agents to the physical world is a key way
for them to exhibit the human-like intelligence.
Image-grounded conversation is thus proposed
to address this challenge. Existing works fo-
cus on exploring the multimodal dialog mod-
els that ground the conversation on a given
image. In this paper, we take a step further
to study image-grounded conversation under a
fully open-ended setting where no paired dia-
log and image are assumed available. Speciﬁ-
cally, we present Maria, a neural conversation
agent powered by the visual world experiences
which are retrieved from a large-scale image
index. Maria consists of three ﬂexible compo-
nents, i.e., text-to-image retriever, visual con-
cept detector and visual-knowledge-grounded
response generator. The retriever aims to re-
trieve a correlated image to the dialog from an
image index, while the visual concept detec-
tor extracts rich visual knowledge from the im-
age. Then, the response generator is grounded
on the extracted visual knowledge and dialog
context to generate the target response.
Ex-
tensive experiments demonstrate Maria outper-
forms previous state-of-the-art methods on au-
tomatic metrics and human evaluation, and can
generate informative responses that have some
visual commonsense of the physical world.
1
Introduction
Building intelligent conversational agents that can
not only converse freely with human but also have
the ability to perceive the physical world, has been
one of the longest standing goals of natural lan-
guage processing (NLP) and artiﬁcial intelligence
(AI). Although the recent large-scale conversation
models trained on text-only corpora, such as Meena
∗Work performed during the internship at Microsoft.
†Equal contribution.
‡Corresponding author.
Human-A: Hey! How was your vacation?
Human-B: Awesome! I had a good time 
with my friends in Hawaii, the beaches
are very beautiful there.
Human-A: Cool! did you play beach 
volleyball with your friends?
(Human-A: Cool, have you had a BBQ
with your friends on the beach? The 
grilled fish was great!)
Human-B: Nope, but it sounds great.
Maybe next time.
Figure 1: An example of human conversations. When
human-B talks about vacation on the beach of Hawaii,
human-A recalls his/her past experience of playing vol-
leyball or having BBQ on the beach.
(Adiwardana et al., 2020), Blender (Roller et al.,
2020) and DialoGPT (Zhang et al., 2020), have
shown the compelling performance, they are still
lack of the perception ability to our physical world.
A recent study (Bisk et al., 2020) points out the suc-
cessful linguistic communication relies on a shared
experience of the world that makes language re-
ally meaningful. The visual perception is a rich
signal for modeling a vastness of experiences in
the world that cannot be documented by text alone
(Harnad, 1990). On the other hand, human-human
conversations involve their understandings of con-
text, the background knowledge they had, and per-
haps most importantly the experiences of the world
they shared, e.g., what they have seen before.
Figure 1 shows a conversation between humans.
Human-A recalls his/her past experience of play-
ing volleyball or having BBQ on the beach when
human-B talks about vacation on the beach of
Hawaii. However, the association relationship be-
tween beach and volleyball (or BBQ) is hard to
capture in traditional knowledge bases, such as
knowledge graph. Motivated by this, we select
a common word “pizza” and collect the top 17
words that mostly co-occur with “pizza” on Google

5597
Figure 2: The word co-occurrence distribution with “pizza” on Google knowledge graph and MS-COCO images.
Knowledge Graph1 and MS-COCO images2 (Lin
et al., 2014). As shown in Figure 2, the words co-
occurring with “pizza” on knowledge graph tend to
be the abstract concepts, while the co-occurrence
relationship of object tags on images reﬂects some
commonsense of our physical world, e.g., “pizza”
is usually on the “dining table”, people usually
use “knife” when eating “pizza”. Interestingly, we
found the “pizza” also co-occurs with “cell phone”
and even “plotted plant”. This indicates when peo-
ple eat pizza, they sometimes would put their cell
phones aside on the table, or there might exist some
plotted plants in the restaurant. Thus, empowering
conversational agents to have the visual perception
ability about the physical world is a key way for
them to exhibit the human-like intelligence.
The existing works (Mostafazadeh et al., 2017;
Huber et al., 2018; Shuster et al., 2020) focus on ex-
ploring the multimodal dialog models that ground
the conversation on a given image. Recently, Yang
et al. (2020) propose to learn the dialog generation
model with both image-grounded dialogs and tex-
tual dialogs by resorting to text-to-image synthesis
techniques (Xu et al., 2018; Qiao et al., 2019) to
restore a latent image for the text-only dialog. Even
so, these works are still constrained by the assump-
tion that the dialog is conducted center around a
given (or synthesized) image.
In this paper, we take a step further to extend
the assumption of image-grounded conversation
to a fully open-ended setting where no image-
dialog pairs are assumed available. Speciﬁcally,
we present Maria, a neural conversational agent
powered by visual world experiences which are
retrieved from a pre-built image index, e.g., the
1https://developers.google.com/
knowledge-graph/
2We calculate the co-occurrence distribution of object tags
from the images in MS-COCO dataset. More examples could
be found in Appendices.
Open Images Dataset (Kuznetsova et al., 2018).
Maria consists of three components:
text-to-
image retriever, visual concept detector, and visual-
knowledge-grounded response generator. The re-
triever is responsible for retrieving a piece of vi-
sual world experiences, e.g., a correlated image
to the dialog from an image index. The visual
concept detector utilizes the object detector from
UpDown (Anderson et al., 2018) to extract the re-
gions features (i.e., bboxes) and the corresponding
visual concepts (i.e., tags) from the retrieval images.
Hence, we can construct (bboxes, tags, context, re-
sponse) 4-tuple as the training data. Finally, these
constructed 4-tuples are used to train the visual-
knowledge-grounded response generator, which is
built on the top of a multi-layer Transformer archi-
tecture (Vaswani et al., 2017). To effectively inject
the visual knowledge into the response generator,
we carry out the Masked Concept Prediction and
Visual Knowledge Bias besides the response gen-
eration objective. The former aims to align the se-
mantic representations between textual words and
image regions, while the latter tries to provide more
visual knowledge to facilitate the dialog generation.
The experimental results on Reddit Conversation
Corpus (Dziri et al., 2019a) demonstrate that Maria
signiﬁcantly outperforms previous state-of-the-art
methods, and can generate informative responses
with visual commonsense of our physical world.
Overall, the contributions of this paper are sum-
marized as follows:
• We explore the task of image-grounded dia-
log generation under a fully open-ended set-
ting where no speciﬁc image-dialog pairs are
assumed available, i.e., zero-resource image-
grounded conversation. To the best of our
knowledge, this is the ﬁrst work to connect
dialog corpus with the unpaired image data;
• We present Maria, a neural conversational

5598
agent consisting of three ﬂexible components,
which can effectively capture the visual com-
monsense from images and accordingly gen-
erate informative and vivid responses;
• Extensive experiments on the widely used
Reddit Conversation Corpus are conducted
to justify the effectiveness of Maria.
2
Related Work
Vision and Language
In the research of vision
and language, various tasks have been extensively
studied, such as image captioning (Vinyals et al.,
2015; Lu et al., 2017; Hu et al., 2020), visual ques-
tion answering (Antol et al., 2015; Anderson et al.,
2018), visual dialog (Das et al., 2017a,b). Popular
benchmark datasets in this area include MS-COCO
(Lin et al., 2014), VisDial (Das et al., 2017a) and
Visual Genome (Krishna et al., 2017). Visual di-
alog is a task to answer the questions about the
factual content of the image in a multi-turn manner.
Differently, image-grounded conversation studies
how to reply to a dialog context and a given image
with proper responses in an open-ended way.
Dialog Generation
Encouraged by the success
of the neural sequence-to-sequence architecture
(Sutskever et al., 2014) on machine translation,
end-to-end neural approaches on open-domain dia-
log generation (Vinyals and Le, 2015; Shang et al.,
2015; Serban et al., 2016; Sordoni et al., 2015; Xing
et al., 2017; Wu et al., 2018; Zhang et al., 2020;
Xu et al., 2019; Adiwardana et al., 2020) have been
widely studied in literature. Recently, there is an
emerging trend towards grounding the dialog gen-
eration models on the external knowledge, such as
knowledge graphs (Zhou et al., 2018), documents
(Ghazvininejad et al., 2018; Dinan et al., 2019; Kim
et al., 2020; Zhao et al., 2020a,b; Li et al., 2020)
and images (Mostafazadeh et al., 2017; Shuster
et al., 2020; Yang et al., 2020). Different from the
previous work on knowledge-grounded conversa-
tion that connects dialogs with unpaired document
knowledge (Li et al., 2020), our work lies in the
research of image-grounded conversation where a
response is generated with a dialog context and a
given image. Existing works (Mostafazadeh et al.,
2017; Shuster et al., 2020; Yang et al., 2020) in
this direction assume there is a given (or synthe-
sized) image for the dialog and explore the multi-
modal dialog models. In contrast to these works,
we study the image-grounded conversation under
Multi-turn Reddit Conversation
Image Index (Open Images)
Visual Concept Detector
Visual-Commonsense-Aware Response Generation Model 
Training: (𝑪, 𝑹) ; Inference: 𝑪
Text-to-Image Retrieve Module
Top-k Images (k=1 here): 
(𝑽𝟏, 𝑽𝟐, … , 𝑽𝒌)
Training Quaternion: 𝑶, 𝑸, 𝑪, 𝑹; Inference Triplet: (𝑶, 𝑸, 𝑪)
Figure 3: The ﬂowchart of our framework. O, Q, C, R
represents the image region features, extracted visual
concepts, dialog context and response.
a fully open-ended assumption where no paired
dialog and image are assumed available, i.e., zero-
resource image-grounded conversation.
3
Problem Formalization
Suppose we have a dialog set D = {(Ci, Ri)}n
i=1,
where ∀i ∈{1, . . . , n}, Ci refers to a dialog
context and Ri is a response to Ci. We assume
there is a set of images V = {Vj}m
j=1, where
∀j ∈{1, . . . , m}, Vj denotes an image. ∀C ∈D,
we assume that there is an image V that triggered
by the given dialog context C and response R. Our
goal is to estimate a generation model P(R|V, C)
from D and V. Thus, given a new dialog context C
associated with an image V , the model can gener-
ate a response R according to P(R|V, C).
4
Methodology
To learn such a generation model P(R|V, C), we
need to tackle several challenges: (1) How to bridge
the gap between unpaired dialog corpus and image
data; (2) After obtaining the correlated images, how
to extract the detailed visual features and concepts;
(3) How to effectively inject the visual knowledge
into response generator and enable it to generate re-
sponses that are visual-knowledge-grounded. Fig-
ure 3 illustrates the framework of our approach. We
ﬁrst build a large-scale image dataset and leverage
a cross-modal matching model to retrieve a corre-
lated image using the content of the dialog. Then
an off-the-shelf object detector is applied to extract-
ing the object features and visual concepts from the
retrieval image. Finally, the response generator is
trained to generate the target response conditioned

5599
on the context, extracted object features, and vi-
sual concepts. In the rest of this section, we will
elaborate these three modules.
4.1
Text-to-Image Retriever
In this section, we develop a retrieval model that
assigns each dialog with a correlated image V .
Speciﬁcally, we train a text-to-image matching
model from image captioning dataset and utilize it
to construct the (C, R, V ) triple data.
Modeling
To improve the efﬁciency of cross-
modal retrieval model on large-scale dialog corpus
and image dataset, we adopt a two-tower architec-
ture (Lu et al., 2019) to accelerate the retrieval pro-
cess where the image features can be pre-extracted
ofﬂine. The model takes a sentence T and an im-
age V as input, and predicts the relevance score
s(T, V ) between the sentence and the image. We
use a text encoder and an image encoder to pro-
duce the representations of T and V , respectively.
The text encoder is a pre-trained BERT-base model
(Devlin et al., 2019) and we use the hidden state of
special token [CLS] as the embedding of T:
et = BERT(T)
(1)
Then a Multi-Layer Perceptron (MLP) projects the
sentence embedding into the cross-modal space.
We follow Tan and Bansal (2020) to perform L2-
normalization on the last output features, by which
we can simplify the nearest neighbor search prob-
lem in the euclidean space to the Maximum Inner
Product problem (Mussmann and Ermon, 2016):
ft (T) =
Ht (et)
∥Ht (et)∥
(2)
Similarly, the image encoder is composed of a pre-
trained ResNeXt backbone (Xie et al., 2017) and a
MLP with L2 normalization:
fv (V ) =
Hv (ev)
∥Hv (ev)∥, ev = ResNeXt(V ) (3)
Thus, we deﬁne the relevance score s(T, V ) as an
inner product of the language feature representation
ft (T) and image feature representation fv (V ):
s(T, V ) = ft (T)⊤fv (V )
(4)
Training
We train the cross-modal matching
model on MS-COCO image captioning dataset (Lin
et al., 2014), where each image is paired with 5 sen-
tences describing its visual content. The model is
optimized by minimizing the hinge loss so that
the relevance score s (T, V ) of the positive image-
sentence pair can be larger than the negative pair
s (T, V −) by at least a margin M:
Lhinge
 T, V, V −
=
l
X
i=1
max{0, M −s (T, V ) +s
 T, V −	
(5)
Inference
Given the trained retrieval model, we
can now assign each dialog with a correlated im-
age V . To ensure the diversity and richness of the
retrieval results, we fetch 500,000 images from the
large-scale Open Images dataset (Kuznetsova et al.,
2018) as our image set V. The image Vi ∈V with
the maximum relevance score is paired with the
given dialog (Ci, Ri) ∈D. Note that for the dialog
in the training set, we use both the context C and re-
sponse R are concatenated as the query for retrieval
(i.e., T = (C, R)), which is beneﬁcial to retrieving
an image with the related visual knowledge. On the
other hand, for the validation/test set of the dialog
corpus, the query is only the context (i.e., T = C)
so as to keep consistent with the real-world setting
where the response is unavailable and need to be
generated at inference.
4.2
Visual Concept Detector
Given the correlated image Vi to the dialog as the
visual clue, we can now extract the visual knowl-
edge from it. One naive approach is to utilize the
CNN-based models to extract the latent image fea-
tures. However, this approach does not consider the
ﬁne-grained representation modeling for images,
which is crucial for the dialog model to understand
the local visual features in images. To address this
issue, we adopt an object detection model (Ander-
son et al., 2018) pre-trained on Visual Genome
(Krishna et al., 2017) to extract a set of salient
object features O = {ok}K
k=1, where each object
feature ok is a 2048-dimensional vector. These
features represent the images at the level of objects
and other salient regions, which has proven to be
vital in many high-level image understanding tasks.
Besides, the same detector is used to extract a set
of visual concepts Q = {qm}K
m=1, where each con-
cept qm is the high-precision textual label of the
visual region, e.g., “sunset”, “melon”, etc. In this
manner, we simultaneously obtain the ﬁne-grained
image representations and the necessary visual con-
cepts for the subsequent dialog generation.

5600
4.3
Visual-Knowledge-Grounded Response
Generator
In this section, we propose a uniﬁed architecture
to effectively inject a set of region features and
corresponding visual concepts into the response
generation model. In following parts, we describe
the model design and training objectives in detail.
4.3.1
Model Architecture
Figure 4 shows the architecture of our response gen-
eration model, which is a multi-layer transformer
network for both bidirectional vision/context
(O, Q, C) encoding, and unidirectional response
R decoding, via the ﬂexible self-attention masks
inspired by (Dong et al., 2019).
4.3.2
Input Representation
For each token, the ﬁnal input representation to
the multi-layer transformer network is the element-
wise summation of four kinds of embeddings, in-
cluding token-level, turn-level, position-level, and
segment-level. Then, we concatenate all the input
representations to one sequence for model training.
Token-Level
The token-level embeddings are
the concatenation of (Ow, Qw, Cw, Rw), which de-
note the token embedding sequence of visual ob-
jects, visual concepts, contexts and response re-
spectively. Note that Ow is the object embedding
transformed by a linear layer into the same dimen-
sion as word embedding.
Turn-Level
Since the dialog is multi-turn, we
encode this turn order with a relative turn embed-
ding (Bao et al., 2020). Speciﬁcally, the turn num-
ber is counted from the last utterance of the dia-
logue to the beginning. Note that as for the tokens
corresponding to O and Q, we simply set them the
same as the ﬁrst utterance of C.
Position-Level
Positional embedding encodes
the signal of the token order in the total input se-
quence, which is the same as positional encoding
of the original transformer (Vaswani et al., 2017).
Segment-Level
Segment embedding is em-
ployed to differentiate which segment the token
is in, i.e., O, Q, C or R.
4.3.3
Masked Concept Prediction
Due to the inherent gap between visual modality
and textual modality, directly optimizing the model
by response generation objective may result in the
insufﬁcient utilization of the visual knowledge. To
align the semantic representations of two modali-
ties, we devise Masked Concept Prediction (MCP)
objective. 15% of the visual concepts are randomly
replaced with [MASK] tokens in each training in-
stance, which need to be predicted by the model.
However, one problem still remains, i.e., the visual
concepts have no speciﬁc order when extracting
from images. In other words, we need to model
MCP as a matching problem of set, which does not
need to consider the order of predicted concepts
when there are more than two concepts masked
out simultaneously. To tackle this, inspired by Hu
et al. (2020), we adopt the Hungarian Matching
Loss (Stewart et al., 2016; Carion et al., 2020) to
estimate an optimal mapping α so that the predic-
tion for each masked position is assigned one of the
target concepts. Here we denote the set of all input
as X = (O, Q, C, R), the set of the bidirectional
self-attention part of X as B = (O, Q, C), the set
of masked concepts as ˆQ, the set of unmasked to-
kens as B\ ˆQ, and the prediction probabilities of
the corresponding representations in the ﬁnal layer
of transformer as H = {hi}m
i=1 where hi is the
probability distribution of the i-th masked position.
Hence, the MCP loss can be deﬁned as:
LMCP( ˆQ, H, α) =
−
X
qα(i)∈ˆQ
log hi

qα(i) | B\ ˆQ

(6)
where α(i) is the index of the target concept as-
signed to the i-th prediction. When predicting a
masked concept, the model will have to resort to
visual region features, dialog contexts and other un-
masked visual concepts. This would help the model
to align the cross-modal representations between
text and visual regions.
4.3.4
Masked Response Prediction
Encouraged by the success of UniLM (Dong et al.,
2019) in Seq2Seq tasks, we adopt the Masked Re-
sponse Prediction (MRP) objective to model the
response generation. During training, 70% of the
tokens in R are randomly masked with the special
token [MASK]. The model is optimized to recover
the masked tokens. The masked response tokens
and other unmasked tokens in the whole input se-
quence can be denoted as ˆR and X\ ˆR, respectively.
Suppose that pi is the conditional probability dis-
tribution of the i-th token in R, the MRP loss is
the Negative Log-Likelihood (NLL) of the masked

5601
Position-Level
Token-Level
Network
Turn-Level
Segment-Level
Multi-Layer Transformer
vis
Response (𝑹)
Dialog Context (𝑪)
Visual Concepts (𝑸)
Region Features (𝑶)
vis
vis
vis
vis
vis
tag
tag
tag
tag
usr
usr
usr
usr
usr
usr
usr
usr
usr
usr
sys
sys
sys
sys
sys
sys
sys
sys
sys
sys
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
-2
-2
-2
-2
-2
-2
-2
-2
-2
-2
-2
-2
-2
-2
-2
-2
-1
-1
-1
-1
0
0
0
0
0
0
0
0
0
0
did
you
eat
?
.
is
a
for
!
SEP
SEP
SEP
SEP
BOS
EOS
MASK
MASK
MASK
MASK
ham
what
pizza
table
cola
cola
pizza
match
pizza
perfect
𝑂
𝑄
𝐶
𝑅
𝑂
𝑄
𝐶
𝑅
𝑂, 𝑄, 𝐶: bidirectional attention
𝑅:  attend to left content
prevent from attending
Figure 4: The overview of the response generation model. There are four kinds of inputs, i.e., image region features
O, extracted visual concepts Q, dialog context C and response R. The self-attention mask in R is unidirectional,
i.e., can only attend to the left context, while the self-attention mask in other segments is bidirectional.
response tokens as follow:
LMRP(X, ˆR) = −
X
wi∈ˆR
log pi

wi | X\ ˆR

(7)
Note that the self-attention mask in R is left-to-
right, but the rest are bidirectional. In other words,
the tokens in O, Q and C can attend to each other
from both directions, while the tokens in R can
attend all tokens in O, Q, C and the leftward tokens
in R including itself. MRP implicitly encourages
the model to generate responses by learning the
relationship among all input tokens.
For decoding, we ﬁrst encode the image regions,
visual concepts, dialog contexts, and a special to-
ken [BOS] as input. Then the model starts the
generation by feeding a [MASK] token and sam-
ples a word from the predicted distribution over
vocabulary. Then, the [MASK] token is replaced
by the generated token and a new [MASK] is ap-
pended to the input sequence for next word pre-
diction. The generation process terminates when
the model predicts [EOS] token or reaches the
pre-deﬁned maximum length.
Visual Knowledge Bias
Normally, the top pro-
jection layer of generation model produces a prob-
ability distribution over the vocabulary:
p = softmax(Wer + b),
(8)
where the er ∈Rd, W ∈R|V |×d and b ∈R|V | are
the last output of the transformer network, weight
and bias parameters of the decoding head, respec-
tively. |V | denotes the vocabulary size. So far, the
visual world knowledge is introduced into the re-
sponse generation model by the shared-parameter
self-attention layers. To further inject the visual
knowledge into the generation model, we design a
simple but effective strategy, namely Visual Knowl-
edge Bias (VKB). Concretely, an additional visual
vocabulary bias bq is ﬁrst calculated as follow:
bq = Fq(eq
avg)
(9)
where Fq : Rd →R|V | is a projection layer.
eq
avg denotes the average pooling on all hidden
representations of visual concepts, i.e., eq
avg =
AvgPooling(Eq) where Eq = (eq
1, ..., eq
K). Then,
we mask non-visual-concept tokens in the vocabu-
lary and the masked vocabulary bias ˆbq ∈R|V | is
added to the top layer of generation model to get
the ﬁnal distribution over vocabulary:
ˆp = softmax(Wer + b + ˆbq)
(10)
We leverage this ﬁnal vocabulary distribution to cal-
culate the MRP loss in Eq. 7 to optimize the model.
This visual knowledge bias would encourage the
model to generate more visual knowledge related
tokens in the response.
To sum up, the ﬁnal objective of our response
generation model is to minimize the integrated loss:
L = LMRP + LMCP
(11)
5
Experimental Setup
5.1
Datasets
To evaluate the performance of Maria, we con-
duct comprehensive experiments on the Reddit
dataset released by Yang et al. (2020), which is
a large-scale and high-quality multi-turn conversa-
tions extracted from Reddit Conversation Corpus
(Dziri et al., 2019b). Each dialog has 3 to 5 ut-
terances, and the training/validation/test set has
1M/20K/20K dialogs respectively.
We train and validate the retrieval model us-
ing the Karpathy’s split3 of the MS-COCO image
captioning data, where the images are split into
3https://cs.stanford.edu/people/
karpathy/deepimagesent

5602
113.2K/5K/5K samples as training/validation/test
set, respectively.
After the retrieval model is
trained, we fetch 500K images from the Open Im-
ages dataset as the image index, and then retrieve
images from it by dialog context and response to
construct the training data for response generator.
5.2
Evaluation Metrics
Both automatic metrics and human evaluation are
employed to assess the performance of Maria and
baselines. Automatic metrics include: (1) Fluency:
perplexity (PPL) measures the conﬁdence of the
generated responses; (2) Relevance: BLEU-1 (Pa-
pineni et al., 2002), Rouge-L (Lin, 2004), and we
follow Serban et al. (2017) to utilize Embedding
Average cosine similarity, Vector Extrema cosine
similarity, and Embedding Greedy Matching score.
All this metrics are calculated by running the public
NLG evaluation script4; (3) Diversity: Distinct-1
(Dist-1) and Distinct-2 (Dist-2) (Li et al., 2016)
are deﬁned as the number of distinct uni-grams or
bi-grams divided by the total amount of words.
In human evaluation, we randomly select 100
dialogue contexts and the corresponding generated
responses for Maria and compared baselines. Three
human annotators are asked to score the response
quality on a scale of {0, 1, 2} from three aspects,
including Fluency, Relevance and Richness. The
higher score means the better. Since each response
receives 3 scores on each aspect, we report the
average scores over annotators and responses. The
inter-annotator agreement is measured by Fleiss’
Kappa(Fleiss and Cohen, 1973).
5.3
Implementation Details
For the retrieval model, ResNeXt-101-32x8d fea-
ture is used as the visual embedding, while the
concatenation of the last 4 layers of BERT’s out-
puts is used as the textual embedding. Both em-
beddings are then respectively fed into an MLP
composed of three layers of size (1024, 1024, 512).
When training the retrieval model, we set the mar-
gin M = 0.5 for the hinge loss, and only tune
the parameters of both MLPs while freezing the
parameters of ResNeXt and BERT. The total train-
ing epoch is 20. At inference, the FAISS (Johnson
et al., 2019) library is utilized to accelerate the in-
ner product search by batch processing. We use the
off-the-shelf object detector from UpDown (An-
derson et al., 2018) to extract top-k (k=36) image
4https://github.com/Maluuba/nlg-eval
region features and the corresponding visual con-
cepts. The detector is a Faster R-CNN (Ren et al.,
2015) model trained on the Visual Genome dataset
(Krishna et al., 2017).
For the response generation model, we set the
number of transformer layers L = 12 and the hid-
den embedding dimension D = 768. Besides,
the network parameters are initialized by UniLM.
The maximum sequence lengths of context and re-
sponse are set to 110 and 40, respectively. The
sequence lengths of region features and concept
tokens are both set to 36. The batch size is 64. We
use the Adam Optimizer (Kingma and Ba, 2015)
with a learning rate 3e-5 to train the response gener-
ation model. The training is conducted on 4 Nvidia
Tesla P40 24G GPU cards for 20 epochs.
5.4
Baselines
We compare the following baselines in the exper-
iments: (1) Seq2Seq: A standard Sequence to Se-
qence model with attention mechanism (Bahdanau
et al., 2015). (2) HRED: A Hierarchical Recurrent
Encoder-Decoder neural network (Serban et al.,
2016). (3) VHRED: A variation of HRED that
introduces latent variables into the generation (Ser-
ban et al., 2017). (4) ReCoSa: A hierarchical
transformer-based model (Zhang et al., 2019) that
achieves the state-of-the-art performance on bench-
marks of dialog generation. (5) ImgVAE: A di-
alog generation model (Yang et al., 2020) that is
trained on both textual dialogs and image-grounded
dialogs by recovering a latent image behind the tex-
tual dialog within a conditional variational auto-
encoding framework. (6) DialoGPT: An open-
domain dialog model (Zhang et al., 2020) that
ﬁne-tunes GPT-2 (Radford et al., 2019) on massive
Reddit data. Since DialoGPT is a dialog generation
model trained on the text-only corpus, we introduce
it as an auxiliary baseline. For a fair comparison,
we choose the same model size (L=12,D=768) of
DialoGPT (117M) as our model.
6
Experimental Results
6.1
Automatic and Human Evaluations
We summarize the experimental results of auto-
matic evaluations in Table 1. Maria achieves the
substantial performance improvements over base-
lines on all metrics except for the comparison to Di-
aloGPT. Especially, Maria signiﬁcantly surpasses
ImgVAE on Dist-1/2, which indicates introducing
richer visual knowledge, i.e., image region features

5603
Model
PPL
BLEU-1
Rouge-L
Average
Extrema
Greedy
Dist-1
Dist-2
Seq2Seq (Bahdanau et al., 2015)
77.27
12.21
10.81
78.38
40.06
62.64
0.53
1.96
HRED (Serban et al., 2016)
84.02
11.68
11.29
75.54
37.49
60.41
0.89
3.21
VHRED (Serban et al., 2017)
78.01
12.22
11.82
75.57
39.24
62.07
0.87
3.49
ReCoSa (Zhang et al., 2019)
71.75
12.75
11.75
79.84
42.29
63.02
0.66
3.83
ImgVAE (Yang et al., 2020)
72.06
12.58
12.05
79.95
42.38
63.55
1.52
6.34
DialoGPT (Zhang et al., 2020)
36.03
5.87
5.20
77.80
35.40
58.39
10.41
49.86
Maria
54.38
14.21
13.02
82.54
44.14
65.98
8.44
33.35
Maria (w/o MCP)
66.71
13.91
11.60
81.59
41.06
64.10
8.36
31.80
Maria (w/o VKB)
65.51
12.76
11.76
82.49
40.22
64.49
7.15
29.44
Maria (w/o VKB & MCP)
62.64
11.50
10.45
77.52
41.27
61.00
6.92
28.53
Maria (w/o images)
64.75
10.70
9.15
78.89
39.88
62.39
6.88
28.01
Maria (w/o concepts)
69.24
11.43
10.61
82.96
41.02
65.07
4.56
16.44
Maria (w/o images & concepts)
69.50
10.75
8.34
80.62
41.15
64.25
3.69
10.11
Table 1: Evaluation results of generated responses on the test set. Numbers in bold denote that the improvement
over the best performing baseline is statistically signiﬁcant. Numbers with underline refer to the best results except
for the comparison to DialoGPT (Zhang et al., 2020).
Model
Fulency
Relevance
Richness
Kappa
ImgVAE
1.79
0.58
0.67
0.67
DialoGPT
1.93
0.92
1.20
0.59
Maria
1.89
1.06
0.97
0.62
Table 2: Human evaluation results.
and the corresponding visual concepts, is beneﬁ-
cial to generating more diverse and informative
responses. This also reﬂects in human evaluation
of Table 2 that the richness score of Maria is higher
than that of ImgVAE. Besides, in terms of rele-
vance metrics including BLEU-1, Rouge-L, Aver-
age, Extrema and Greedy, Maria outperforms all
baselines and even performs better than DialoGPT.
This indicates introducing the extra visual knowl-
edge related to dialog context can further force the
model to produce more relevant responses.
On the other hand, the discrepancy of data dis-
tributions between the training data (i.e., Image-
Chat (Shuster et al., 2020) dataset) and test data
(i.e., Reddit conversation dataset) of the text-to-
image synthesis model in ImgVAE limits its per-
formance in practice. Besides, constrained by the
capability of the text-to-image synthesis model, the
richness and diversity of the synthesized images
are undesirable, while Maria can retrieve a vari-
ety of images from the large-scale image index.
That may be the reason why ImgVAE consistently
underperforms our Maria on relevance including
automatic evaluation and human judgement, which
also shows the superiority of the retrieval method
for the zero-resource image-grounded conversation.
Another observation is that Maria slightly under-
performs DialoGPT on PPL and Dist-1/2. Since
DialoGPT is a large-scale pre-training based dialog
generation model and introduces the extra mutual
information maximization objective to improve the
informativeness of generated responses, which is
consistent in human evaluation with respect to ﬂu-
ency and richness.
6.2
Ablation Study
We conduct extensive ablation experiments over
different model variants and input components to
better understand their relative importance to the
dialog generation task. As shown in Table 1, train-
ing the simpliﬁed versions of Maria or removing
any visual signals from input components leads to
worse performance in terms of relevance and diver-
sity. In particular, the results on the ablation study
validate that: (1) The performance improvement
of dialog generation beneﬁts from the MCP’s ef-
fectiveness in aligning the representations of text
and vision; (2) When training Maria, introducing
VKB can further improve the quality and diversity
of generated responses; (3) Rich visual knowledge,
i.e., image region features and visual concepts, play
a signiﬁcant role in improving the performance of
dialog generation. Especially, removing the visual
concepts leads to a dramatic performance drop on
diversity. The phenomenon is due to the lack of
necessary visual concepts, Maria can not well un-
derstand the visual world knowledge when only
learning from the visual features.
6.3
Case Analysis
To further investigate the quality of responses gen-
erated by Maria, we put an example of generated
responses in Figure 5. As we can see from Fig-
ure 5, when the context talks about the supermarket
“Aldi”, Maria can retrieve a “pizza” related image
and generate the informative response grounded on

5604
Dialog Context:
Maria
A: No Aldi? hahah jokes.
B: Aldi is by far the best.
(Note: Aldi is the name of a supermarket)
The
pizza
at
aldi
is
the
best
in
the
world
Figure 5: The visualization of attention weights on the retrieved image by Maria for an example.
it, i.e., “the pizza at Aldi is the best in the world”.
This implies the commonsense that the supermarket
usually has the pizza to sell. It is also observed that
Maria pays more attention to the relevant image
regions when generating the word “pizza”, which
demonstrates that Maria could capture useful visual
knowledge from the image and subsequently lever-
age it to generate commonsense-aware responses.
More cases are demonstrated in Appendices.
7
Conclusions
In this paper, we present Maria, a neural conver-
sational agent powered by the visual world expe-
riences. It is able to retrieve the visual world ex-
periences with users and generate human-like re-
sponses with some visual commonsense. Extensive
experiments demonstrate Maria achieves substan-
tial improvements over the state-of-the-art methods
in automatic and human evaluation. The future
works could include: (1) Design a more precise
and comprehensive image retriever to include mul-
tiple retrieval images; (2) Combining the retrieve
module and dialog generation into an end-to-end
model, instead of learning them individually; (3)
Explore more efﬁcient neural architectures to inject
the visual knowledge into response generation.
References
Daniel Adiwardana, Minh-Thang Luong, David R So,
Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,
Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,
et al. 2020.
Towards a human-like open-domain
chatbot. arXiv preprint arXiv:2001.09977.
Peter Anderson, Xiaodong He, Chris Buehler, Damien
Teney, Mark Johnson, Stephen Gould, and Lei
Zhang. 2018. Bottom-up and top-down attention for
image captioning and visual question answering. In
2018 IEEE Conference on Computer Vision and Pat-
tern Recognition, CVPR 2018, Salt Lake City, UT,
USA, June 18-22, 2018, pages 6077–6086. IEEE
Computer Society.
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C. Lawrence Zitnick,
and Devi Parikh. 2015. VQA: visual question an-
swering. In 2015 IEEE International Conference on
Computer Vision, ICCV 2015, Santiago, Chile, De-
cember 7-13, 2015, pages 2425–2433. IEEE Com-
puter Society.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015.
Neural machine translation by jointly
learning to align and translate.
In 3rd Inter-
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings.
Siqi Bao, Huang He, Fan Wang, Hua Wu, and Haifeng
Wang. 2020. PLATO: Pre-trained dialogue genera-
tion model with discrete latent variable. In Proceed-
ings of the 58th Annual Meeting of the Association
for Computational Linguistics, pages 85–96, Online.
Association for Computational Linguistics.
Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob
Andreas, Yoshua Bengio, Joyce Chai, Mirella Lap-
ata, Angeliki Lazaridou, Jonathan May, Aleksandr
Nisnevich, Nicolas Pinto, and Joseph Turian. 2020.
Experience grounds language. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 8718–8735,
Online. Association for Computational Linguistics.
Nicolas Carion, Francisco Massa, Gabriel Synnaeve,
Nicolas Usunier, Alexander Kirillov, and Sergey
Zagoruyko. 2020. End-to-end object detection with
transformers. In European Conference on Computer
Vision, pages 213–229. Springer.

5605
Abhishek Das, Satwik Kottur, Khushi Gupta, Avi
Singh, Deshraj Yadav, Jos´e M. F. Moura, Devi
Parikh, and Dhruv Batra. 2017a. Visual dialog. In
2017 IEEE Conference on Computer Vision and Pat-
tern Recognition, CVPR 2017, Honolulu, HI, USA,
July 21-26, 2017, pages 1080–1089. IEEE Computer
Society.
Abhishek Das, Satwik Kottur, Jos´e M. F. Moura, Ste-
fan Lee, and Dhruv Batra. 2017b. Learning coop-
erative visual dialog agents with deep reinforcement
learning. In IEEE International Conference on Com-
puter Vision, ICCV 2017, Venice, Italy, October 22-
29, 2017, pages 2970–2979. IEEE Computer Soci-
ety.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019.
BERT: Pre-training of
deep bidirectional transformers for language under-
standing.
In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Emily Dinan, Stephen Roller, Kurt Shuster, Angela
Fan, Michael Auli, and Jason Weston. 2019. Wizard
of wikipedia: Knowledge-powered conversational
agents. In 7th International Conference on Learn-
ing Representations, ICLR 2019, New Orleans, LA,
USA, May 6-9, 2019. OpenReview.net.
Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-
aodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,
and Hsiao-Wuen Hon. 2019.
Uniﬁed language
model pre-training for natural language understand-
ing and generation.
In Advances in Neural Infor-
mation Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019,
NeurIPS 2019, December 8-14, 2019, Vancouver,
BC, Canada, pages 13042–13054.
Nouha Dziri, Ehsan Kamalloo, Kory Mathewson, and
Osmar Zaiane. 2019a. Augmenting neural response
generation with context-aware topical attention. In
Proceedings of the First Workshop on NLP for Con-
versational AI, pages 18–31, Florence, Italy. Associ-
ation for Computational Linguistics.
Nouha Dziri, Ehsan Kamalloo, Kory Mathewson, and
Osmar Zaiane. 2019b. Augmenting neural response
generation with context-aware topical attention. In
Proceedings of the First Workshop on NLP for Con-
versational AI, pages 18–31, Florence, Italy. Associ-
ation for Computational Linguistics.
Joseph L Fleiss and Jacob Cohen. 1973. The equiv-
alence of weighted kappa and the intraclass corre-
lation coefﬁcient as measures of reliability. Educa-
tional and psychological measurement, 33(3):613–
619.
Marjan Ghazvininejad,
Chris Brockett,
Ming-Wei
Chang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and
Michel Galley. 2018. A knowledge-grounded neural
conversation model. In Proceedings of the Thirty-
Second AAAI Conference on Artiﬁcial Intelligence,
(AAAI-18), the 30th innovative Applications of Arti-
ﬁcial Intelligence (IAAI-18), and the 8th AAAI Sym-
posium on Educational Advances in Artiﬁcial Intel-
ligence (EAAI-18), New Orleans, Louisiana, USA,
February 2-7, 2018, pages 5110–5117. AAAI Press.
Stevan Harnad. 1990.
The symbol grounding prob-
lem.
Physica D: Nonlinear Phenomena, 42(1-
3):335–346.
Xiaowei Hu, Xi Yin, Kevin Lin, Lijuan Wang, Lei
Zhang, Jianfeng Gao, and Zicheng Liu. 2020. Vivo:
Surpassing human performance in novel object cap-
tioning with visual vocabulary pre-training. arXiv
preprint arXiv:2009.13682.
Bernd Huber, Daniel J. McDuff, Chris Brockett,
Michel Galley, and Bill Dolan. 2018. Emotional di-
alogue generation using image-grounded language
models. In Proceedings of the 2018 CHI Conference
on Human Factors in Computing Systems, CHI 2018,
Montreal, QC, Canada, April 21-26, 2018, page 277.
ACM.
Jeff Johnson, Matthijs Douze, and Herv´e J´egou. 2019.
Billion-scale similarity search with gpus.
IEEE
Transactions on Big Data.
Byeongchang Kim, Jaewoo Ahn, and Gunhee Kim.
2020.
Sequential latent knowledge selection for
knowledge-grounded dialogue. In 8th International
Conference on Learning Representations, ICLR
2020, Addis Ababa, Ethiopia, April 26-30, 2020.
OpenReview.net.
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization.
In 3rd Inter-
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings.
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-
son, Kenji Hata, Joshua Kravitz, Stephanie Chen,
Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.
2017. Visual genome: Connecting language and vi-
sion using crowdsourced dense image annotations.
International journal of computer vision, 123(1):32–
73.
Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper
Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab
Kamali, Stefan Popov, Matteo Malloci, Alexander
Kolesnikov, et al. 2018. The open images dataset v4:
Uniﬁed image classiﬁcation, object detection, and
visual relationship detection at scale. arXiv preprint
arXiv:1811.00982.
Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016.
A diversity-promoting ob-
jective function for neural conversation models. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,

5606
pages 110–119, San Diego, California. Association
for Computational Linguistics.
Linxiao Li, Can Xu, Wei Wu, Yufan Zhao, Xueliang
Zhao, and Chongyang Tao. 2020.
Zero-resource
knowledge-grounded dialogue generation.
In Ad-
vances in Neural Information Processing Systems
33: Annual Conference on Neural Information Pro-
cessing Systems 2020, NeurIPS 2020, December 6-
12, 2020, virtual.
Chin-Yew Lin. 2004.
ROUGE: A package for auto-
matic evaluation of summaries. In Text Summariza-
tion Branches Out, pages 74–81, Barcelona, Spain.
Association for Computational Linguistics.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar,
and C Lawrence Zitnick. 2014.
Microsoft coco:
Common objects in context.
In European confer-
ence on computer vision, pages 740–755. Springer.
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan
Lee. 2019. Vilbert: Pretraining task-agnostic visi-
olinguistic representations for vision-and-language
tasks. In Advances in Neural Information Process-
ing Systems 32: Annual Conference on Neural Infor-
mation Processing Systems 2019, NeurIPS 2019, De-
cember 8-14, 2019, Vancouver, BC, Canada, pages
13–23.
Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard
Socher. 2017. Knowing when to look: Adaptive at-
tention via a visual sentinel for image captioning. In
2017 IEEE Conference on Computer Vision and Pat-
tern Recognition, CVPR 2017, Honolulu, HI, USA,
July 21-26, 2017, pages 3242–3250. IEEE Computer
Society.
Nasrin Mostafazadeh, Chris Brockett, Bill Dolan,
Michel Galley, Jianfeng Gao, Georgios Spithourakis,
and Lucy Vanderwende. 2017.
Image-grounded
conversations: Multimodal context for natural ques-
tion and response generation.
In Proceedings of
the Eighth International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 462–472, Taipei, Taiwan. Asian Federation of
Natural Language Processing.
Stephen Mussmann and Stefano Ermon. 2016. Learn-
ing and inference via maximum inner product search.
In Proceedings of the 33nd International Conference
on Machine Learning, ICML 2016, New York City,
NY, USA, June 19-24, 2016, volume 48 of JMLR
Workshop and Conference Proceedings, pages 2587–
2596. JMLR.org.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation.
In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.
Tingting Qiao, Jing Zhang, Duanqing Xu, and Dacheng
Tao. 2019. Mirrorgan: Learning text-to-image gen-
eration by redescription.
In IEEE Conference on
Computer Vision and Pattern Recognition, CVPR
2019, Long Beach, CA, USA, June 16-20, 2019,
pages 1505–1514. Computer Vision Foundation /
IEEE.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Shaoqing Ren, Kaiming He, Ross B. Girshick, and
Jian Sun. 2015. Faster R-CNN: towards real-time
object detection with region proposal networks. In
Advances in Neural Information Processing Systems
28: Annual Conference on Neural Information Pro-
cessing Systems 2015, December 7-12, 2015, Mon-
treal, Quebec, Canada, pages 91–99.
Stephen Roller, Emily Dinan, Naman Goyal, Da Ju,
Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott,
Kurt Shuster, Eric M Smith, et al. 2020. Recipes
for building an open-domain chatbot. arXiv preprint
arXiv:2004.13637.
Iulian Vlad Serban, Alessandro Sordoni, Yoshua Ben-
gio, Aaron C. Courville, and Joelle Pineau. 2016.
Building end-to-end dialogue systems using gener-
ative hierarchical neural network models.
In Pro-
ceedings of the Thirtieth AAAI Conference on Arti-
ﬁcial Intelligence, February 12-17, 2016, Phoenix,
Arizona, USA, pages 3776–3784. AAAI Press.
Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron C. Courville,
and Yoshua Bengio. 2017.
A hierarchical latent
variable encoder-decoder model for generating di-
alogues.
In Proceedings of the Thirty-First AAAI
Conference on Artiﬁcial Intelligence, February 4-9,
2017, San Francisco, California, USA, pages 3295–
3301. AAAI Press.
Lifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neu-
ral responding machine for short-text conversation.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages
1577–1586, Beijing, China. Association for Compu-
tational Linguistics.
Kurt Shuster, Samuel Humeau, Antoine Bordes, and Ja-
son Weston. 2020. Image-chat: Engaging grounded
conversations. In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics, pages 2414–2429, Online. Association for
Computational Linguistics.
Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. In Proceedings

5607
of the 2015 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 196–
205, Denver, Colorado. Association for Computa-
tional Linguistics.
Russell Stewart, Mykhaylo Andriluka, and Andrew Y.
Ng. 2016. End-to-end people detection in crowded
scenes. In 2016 IEEE Conference on Computer Vi-
sion and Pattern Recognition, CVPR 2016, Las Ve-
gas, NV, USA, June 27-30, 2016, pages 2325–2333.
IEEE Computer Society.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Sys-
tems 27: Annual Conference on Neural Informa-
tion Processing Systems 2014, December 8-13 2014,
Montreal, Quebec, Canada, pages 3104–3112.
Hao Tan and Mohit Bansal. 2020. Vokenization: Im-
proving language understanding via contextualized,
visually-grounded supervision.
In Proceedings of
the 2020 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 2066–
2080.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-
9, 2017, Long Beach, CA, USA, pages 5998–6008.
Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. arXiv preprint arXiv:1506.05869.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and tell: A neural im-
age caption generator. In IEEE Conference on Com-
puter Vision and Pattern Recognition, CVPR 2015,
Boston, MA, USA, June 7-12, 2015, pages 3156–
3164. IEEE Computer Society.
Yu Wu, Wei Wu, Dejian Yang, Can Xu, and Zhoujun
Li. 2018. Neural response generation with dynamic
vocabularies. In Proceedings of the Thirty-Second
AAAI Conference on Artiﬁcial Intelligence, (AAAI-
18), the 30th innovative Applications of Artiﬁcial In-
telligence (IAAI-18), and the 8th AAAI Symposium
on Educational Advances in Artiﬁcial Intelligence
(EAAI-18), New Orleans, Louisiana, USA, February
2-7, 2018, pages 5594–5601. AAAI Press.
Saining Xie, Ross B. Girshick, Piotr Doll´ar, Zhuowen
Tu, and Kaiming He. 2017.
Aggregated residual
transformations for deep neural networks. In 2017
IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2017, Honolulu, HI, USA, July
21-26, 2017, pages 5987–5995. IEEE Computer So-
ciety.
Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang,
Ming Zhou, and Wei-Ying Ma. 2017. Topic aware
neural response generation. In Proceedings of the
Thirty-First AAAI Conference on Artiﬁcial Intelli-
gence, February 4-9, 2017, San Francisco, Califor-
nia, USA, pages 3351–3357. AAAI Press.
Can Xu, Wei Wu, Chongyang Tao, Huang Hu, Matt
Schuerman, and Ying Wang. 2019. Neural response
generation with meta-words. In Proceedings of the
57th Annual Meeting of the Association for Com-
putational Linguistics, pages 5416–5426, Florence,
Italy. Association for Computational Linguistics.
Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han
Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He.
2018. Attngan: Fine-grained text to image genera-
tion with attentional generative adversarial networks.
In 2018 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2018, Salt Lake City, UT,
USA, June 18-22, 2018, pages 1316–1324. IEEE
Computer Society.
Ze Yang, Wei Wu, Huang Hu, Can Xu, and Zhoujun Li.
2020. Open domain dialogue generation with latent
images. arXiv preprint arXiv:2004.01981.
Hainan Zhang, Yanyan Lan, Liang Pang, Jiafeng Guo,
and Xueqi Cheng. 2019. ReCoSa: Detecting the rel-
evant contexts with self-attention for multi-turn di-
alogue generation. In Proceedings of the 57th An-
nual Meeting of the Association for Computational
Linguistics, pages 3721–3730, Florence, Italy. Asso-
ciation for Computational Linguistics.
Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,
Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing
Liu, and Bill Dolan. 2020.
DIALOGPT : Large-
scale generative pre-training for conversational re-
sponse generation. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics:
System Demonstrations, pages 270–
278, Online. Association for Computational Linguis-
tics.
Xueliang Zhao, Wei Wu, Chongyang Tao, Can Xu,
Dongyan Zhao, and Rui Yan. 2020a. Low-resource
knowledge-grounded dialogue generation.
In 8th
International Conference on Learning Representa-
tions, ICLR 2020, Addis Ababa, Ethiopia, April 26-
30, 2020. OpenReview.net.
Xueliang Zhao, Wei Wu, Can Xu, Chongyang Tao,
Dongyan Zhao, and Rui Yan. 2020b. Knowledge-
grounded dialogue generation with pre-trained lan-
guage models.
In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 3377–3390, Online. As-
sociation for Computational Linguistics.
Hao Zhou, Tom Young, Minlie Huang, Haizhou Zhao,
Jingfang Xu, and Xiaoyan Zhu. 2018.
Com-
monsense knowledge aware conversation generation
with graph attention. In Proceedings of the Twenty-
Seventh International Joint Conference on Artiﬁcial
Intelligence, IJCAI 2018, July 13-19, 2018, Stock-
holm, Sweden, pages 4623–4629. ijcai.org.

5608
A
Appendices
In this section, we show more examples of word
co-occurrence distributions on Google knowledge
graph and MS-COCO images. Besides, some con-
versation samples produced by Maria and the base-
lines are also presented in Section A.2.
A.1
Word Co-occurrence Distribution
Examples
In Figure 6, we present some supplementary ex-
amples of the word co-occurrence distribution on
Google knowledge graph and MS-COCO images,
including “trafﬁc light”, “bed”, “book”, and “pot
plant”. Figure 6 (a) shows the co-occurrence distri-
butions of “trafﬁc light” and other words on knowl-
edge graph and images, respectively. As we can see,
most of the co-occurred words with “trafﬁc light”
are the related concepts such as “smart trafﬁc light”,
“trafﬁc light protocol”, “trafﬁc light rating system”,
etc. While the co-occurred words on images are
usually “car”, “person”, “truck”, “bus”, etc, which
we often see when walking by the trafﬁc lights. In-
terestingly, we found “umbrella” and “clock” also
co-occurs with “trafﬁc light” in some images. For
the former, the picture we can imagine is that peo-
ple were holding the “umbrellas” when they walked
through a zebra crossing under the “trafﬁc light”.
For the latter, the possible picture is that we can
see both the “trafﬁc light” and the “clock” on the
top of a high building from a certain angle when
walking on the street. Similar observations can be
also seen in other examples.
(a) traffic light
Most of the co-occurrence words on knowledge
graph are logically-related concepts. However, the
co-occurrence relationship of object tags on images
reﬂects some commonsense of our physical world,
which implies some pictures that we human could
easily imagine. This kind of knowledge is unique
and inherent in images, but it can hardly be cap-
tured in the traditional knowledge bases, such as
knowledge graph.
A.2
Case Analysis
Figure 7 shows some cases from the test set of Red-
dit data. We observe that the responses generated
by Maria are more commonsensical and vivid than
those of the baseline methods, which is consistent
with our automatic and human evaluation results.
Interestingly, Maria is able to retrieve correlated
images using the dialog contexts, which makes its
response more human-like. For instance, case (a)
shows that when the dialog context marvels at “the
pass of the world cup”, Maria recalls a football
player and compliments him “the best player in the
world”; case (b) shows that when the dialog context
chats about the “Canada weather”, Maria is aware
of the fact that “Canada” is often “snowy” and then
talks about “Canada” in a funny tone, “I’ve never
been to a place that doesn’t have snow”; case (c)
shows that Maria understands that “swan” is some-
times “dangerous” when they are on the “beach”;
case (d) shows that when the dialog context tries to
guess one type of game, Maria recalls a ping-pong
“ball” game and describes it; and etc.

5609
(b) bed
(d) pot plant
(c) book
Figure 6: Supplementary examples of the word co-occurrence distribution on Google knowledge graph and MS-
COCO images.

5610
Dialog Context:
Maria
A:the pass of the world cup (james rodriguez)
B: absoleutely gorgeous.
ImgVAE: I love the world cup.
DialoGPT: it 's not a pass of the world cup.
Baseline Response:
Detected Concepts: ball, socks, field, shirt, grass, man, shorts, player, sock
Response: he's the best player in the world
Dialog Context:
Maria
A: how is a swan gonna break your arm?
B: smack you with their wings
ImgVAE: i 'm a little nervous about this.
DialoGPT: i 'm a swan.
Baseline Response:
Detected Concepts:
duck, picture, background, land, beach, border, line, ground, bird,
sky, water, birds, frame
Response: 
i'm not sure if you're joking or not, but i'm sure they're going to be
a bit more dangerous if they are on the beach
Dialog Context:
Maria
A: you 've just ruined my canada weather fetish , that it would be ideal place for me to live:
freezing cold winters in mild summers.
B: ya , there are parts of canada with very nice mild summers , but those same places have mild
winters too.
ImgVAE: i 'm from canada , but i 've never seen a cold winters winters in cold winters winters .
DialoGPT: i 'm from the u.s. and i 've never heard of this.
Baseline Response:
Detected Concepts: person, fence, ground, tree, building, snow, border, pole, 
gate
Response: i'm from canada and i've never been to a place that doesn't have snow
(a)
(b)
(c)

5611
Dialog Context:
Maria
A: we all played this game at one point.
B: what game is this ?
ImgVAE: it 's a game game game , game of thrones.
DialoGPT: it 's a meme
Baseline Response:
Detected Concepts: shorts, background, heart, skateboard, 
woman, ball, bench, counter, shirt, hair, wall, net, stands, people
Response: it's a game where you play the ball with your hands
Dialog Context:
Maria
A: what 's the coldest you 've ever been?
B: almost got hypothermia near the arctic circle because my rain jacket failed to be waterproof
and all my layers were soaked.
C: what kind of jacket?
ImgVAE: the one where you ski and the jacket?
DialoGPT: it 's a winter jacket.
Baseline Response:
Detected Concepts: pants, sky, snow, person, clouds
Response: it's a snow jacket
Dialog Context:
Maria
A: what cool gadget under $200 can be life changing?
B: a smartphone.
ImgVAE: i have a smartphone and i 'm a phone.
DialoGPT: that 's not cool.
Baseline Response:
Detected Concepts: button, hand, wall, line, hair, phone, table, 
fingers, handle, thumb
Response: i already have a phone.
(d)
(e)
(f)
Figure 7: Case Study on the Reddit data from test split.

