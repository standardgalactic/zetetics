
and
An Encyclopedia of Economic History
from Tulipmania of the 1630s to the
Global Financial Crisis of the 21st Century
Booms
  Busts
 
James Ciment, Editor

SHARPE REFERENCE
Sharpe Reference is an imprint of M.E. Sharpe, Inc. 
 
M.E. Sharpe, Inc.
80 Business Park Drive
Armonk, NY 10504
© 2013 by M.E. Sharpe, Inc.
All rights reserved. No part of this publication may be reproduced, stored in a retrieval  
system or transmitted in any form or by any means, electronic, mechanical, photocopying, 
recording, or otherwise, without the prior permission of the copyright holders.
Cover photos (background; left to right) provided by Getty Images and the following:  
Mario Tama; Popperfoto; Yoshikazu Tsuno/AFP; Kean Collection/Hulton Archive; Melanie Einzig.
Library of Congress Cataloging-in-Publication Data
Booms and busts : an encyclopedia of economic history from Tulipmania of the 1630s to the 
global financial crisis of the 21st century / James Ciment, editor.
       v. ; cm.
  Includes bibliographical references and index.
  ISBN 978-0-7656-8224-6 (hardcover : alk. paper)
  1. Financial crises—Encyclopedias.  2. Finance—Encyclopedias.  I. Ciment, James. 
  HB3722.B67 2010
  330.03—dc22	 	
	
	
	
	
 	
	
2010021272
Printed and bound in the United States
The paper used in this publication meets the minimum requirements of 
American National Standard for Information Sciences—Permanence of
 Paper for Printed Library Materials,
ANSI Z 39.48.1984.
CW (c)  10  9  8  7  6  5  4  3  2  1
Publisher: Myron E. Sharpe
Vice President and Director of New Product Development: Donna Sanzone
Vice President and Production Director: Carmen Chetti
Executive Development Editor: Jeff Hacker
Project Manager: Angela Piliouras
Program Coordinator: Cathleen Prisco
Assistant Editor: Alison Morretta
Text Design and Cover Design: Jesse Sanchez
Typesetter: Nancy Connick

BOOMS AND BUSTS
Table of Contents
Introduction
A-Z Entries
Africa, Sub-Saharan
Agriculture
AIG
Airline Industry
Akerman, Johan Henryk (1896–1982)
Argentina
Asian Financial Crisis (1997)
Asset-Price Bubble
Australia
Austrian School
Automated Trading Systems
Babson, Roger (1875–1967)
Balance of Payments
Baltic Tigers
Bank Cycles
Bank of America
Banking School/Currency School Debate
Banks, Central
Banks, Commercial
Banks, Investment
Bauer, Otto (1881–1938)
Bear Stearns
Behavioral Economics
Belgium
Bernanke, Ben (1953–)
Bethlehem Steel
Böhm-Bawerk, Eugen Ritter von (1851–1914)
Boom, Economic (1920s)
Boom, Economic (1960s)
Booms and Busts: Cause and Consequences
Booms and Busts: Pre–Twentieth Century
Booms and Busts: Twentieth and Twenty-First Centuries
Brazil
BRIC (Brazil, Russia, India, China)
Brunner, Karl (1916–1989)
Bullock, Charles (1869–1941)
Burchardt, Fritz (1902–1958)
Burns, Arthur (1904–1987)
Business Cycles, International

Canada
Capital Account
Capital Market
Capital One
Catastrophe Theory
Central America
Chile
China
Chrysler
Circuit City Stores
Citigroup
Classical Theories and Models
Collateral
Collateralized Debt Obligations
Collateralized Mortgage Obligations
Colombia
Commodity Markets
Community Reinvestment Act (1977)
Confidence, Consumer and Business
Congressional Budget Office
Construction, Housing and Commercial
Consumer and Investor Protection
Consumption
Corporate Corruption
Corporate Finance
Council of Economic Advisers, U.S.
Countrywide Financial
Creative Destruction
Credit Cycle
Credit Default Swaps
Credit Rating Agencies
Current Account
Debt
Debt Instruments
Deflation
Demographic Cycle
Denmark
Depository Institutions
Dot.com Bubble (1990s–2000)
Dow Jones Industrial Average
Duesenberry, James (1918–2009)
Eastern Europe
Echo Bubble
Eckstein, Otto (1927–1984)
Effective Demand
Efficient Market Theory
Emerging Markets
Employment and Unemployment

Endogenous Growth Models
Enron
European Central Bank
Exchange Rates
Fannie Mae and Freddie Mac
Federal Deposit Insurance Corporation
Federal Housing Administration
Federal Housing Enterprise Oversight, Office of
Federal Reserve System
Fellner, William John (1905–1983)
Financial Development/Deepening
Financial Markets
Financial Modeling of the Business Cycle
Finland
Fiscal Balance
Fiscal Policy
Fisher, Irving (1867–1947)
Fisher’s Debt-Deflation Theory
Fixed Business Investment
Fleetwood Enterprises
Florida Real-Estate Boom (1920s)
Foreclosure
Fragility, Financial
France
Friction, Financial
Friedman, Milton (1912–2006)
Frisch, Ragnar (1895–1973)
Galbraith, John Kenneth (1908–2006)
Geithner, Timothy (1961–)
General Motors
German Historical School
Germany
Glass-Steagall Act (1933)
Goldman Sachs
Goodwin, Richard Murphy (1913–1996)
Government Accountability Office
Great Depression (1929–1933)
Greece
Greenspan, Alan (1926–)
Gross Domestic Product
Growth Cycles
Growth, Economic
Haberler, Gottfried von (1900–1995)
Hansen, Alvin Harvey (1887–1975)
Harrod, Roy Forbes (1900–1978)
Hawtrey, Ralph George (1879–1975)
Hayek, Friedrich August von (1899–1992)
Hedge Funds

Hicks, John Richard (1904–1989)
Hoarding
House Financial Services Committee
Housing
Housing and Urban Development, Department of
Housing Booms and Busts
Iceland
Immigration and Migration
Income Distribution
India
Indicators of Financial Vulnerability
Indonesia
Industrial Policy
IndyMac Bancorp
Inflation
Information Technology
Innovation, Financial
Institutional Economics
Insull, Samuel (1859–1938)
Integration, Financial
Interest Rates
Intermediation, Financial
International Development Banks
International Economic Agreements
International Monetary Fund
International Monetary Fund Mortgage Market Index
International Policy Coordination
Inventory Investment
Investment, Financial
Ireland
“Irrational Exuberance”
Israel
Italy
Japan
Jevons, William Stanley (1835–1882)
JPMorgan Chase
Juglar, Clément (1819–1905)
Kaldor, Nicholas (1908–1986)
Kalecki, Michal (1899–1970)
Kautsky, Karl (1854–1938)
Keynes, John Maynard (1883–1946)
Keynesian Business Model
Kindleberger, Charles P. (1910–2003)
Kondratieff Cycles
Kondratieff, Nikolai Dmitriyevich (1892–1938)
Koopmans, Tjalling Charles (1910–1985)
Korea, South
Kuznets, Simon Smith (1901–1985)

Labor Market
Lachmann, Ludwig Maurits (1906–1990)
Lange, Oskar R. (1904–1965)
Latin America
Law, John (1671–1729)
Leads and Lags
Lehman Brothers
Lerner, Abba P. (1903–1982)
Leveraging and Deleveraging, Financial
Liberalization, Financial
Life Insurance
Linens’n Things
Liquidity Crunch
Liquidity Trap
Loan-to-Value Ratio
Long-Term Capital Management
Lowe, Adolph (1893–1995)
Loyd, Samuel Jones (1796–1883)
Luminent Mortgage Capital
Lundberg, Erik Filip (1907–1987)
Luxemburg, Rosa (1871–1919)
Madoff, Bernard (1938–)
Malthus, Thomas Robert (1766–1834)
Manufacturing
Market Trends
Marshall, Alfred (1842–1924)
Marx, Karl (1818–1883)
Marxist Cycle Model
Merrill Lynch
Metzler, Lloyd Appleton (1913–1980)
Mexico
Middle East and North Africa
Mill, John Stuart (1806–1873)
Mills, Frederick Cecil (1892–1964)
Minsky, Hyman (1919–1996)
Minsky’s Financial Instability Hypothesis
Mises, Ludwig von (1881–1973)
Mississippi Bubble (1717–1720)
Mitchell, Wesley Clair (1874–1948)
Monetary Policy
Monetary Stability
Monetary Theories and Models
Money Markets
Money, Neutrality of
Money Store, The
Moral Hazard
Morgan Stanley
Morgenstern, Oskar (1902–1977)

Mortgage-Backed Securities
Mortgage, Commercial/Industrial
Mortgage Equity
Mortgage Lending Standards
Mortgage Markets and Mortgage Rates
Mortgage, Reverse
Mortgage, Subprime
Myrdal, Gunnar (1898–1987)
Nasdaq
National Bureau of Economic Research
National Economic Council
Neoclassical Theories and Models
Neo-Keynesian Theories and Models
Netherlands, The
New Deal
New York Stock Exchange
New Zealand
Northern Rock
Norway
Oil Industry
Oil Shocks (1973–1974, 1979–1980)
Over-Savings and Over-Investment Theories of the Business Cycle
Overvaluation
Pacific Rim
Panic of 1901
Panic of 1907
Panics and Runs, Bank
Paulson, Henry (1946–)
Penn Central
Philippines
PNC Financial Services
Political Theories and Models
Ponzi Scheme (1919–1920)
Portugal
Poseidon Bubble (1969–1970)
Post Keynesian Theories and Models
Poverty
Price Stability
Production Cycles
Productivity
Profit
Public Works Policy
Real Business Cycle Models
Real-Estate Speculation
Recession and Financial Crisis (2007–2009)
Recession, Reagan (1981–1982)
Recession, Roosevelt (1937–1939)
Recession, Stagflation (1970s)

Refinancing
Regulation, Financial
Resource Allocation
Retail and Wholesale Trade
Retirement Instruments
Risk and Uncertainty
Robbins, Lionel Charles (1898–1984)
Robertson, Dennis Holme (1890–1963)
Robinson, Joan (1903–1983)
Romer, Christina (1958–)
Röpke, Wilhelm (1899–1966)
Rostow, Walt Whitman (1916–2003)
Rubin, Robert (1938–)
Russia and the Soviet Union
S&P 500
Samuelson, Paul (1915–2009)
Savings and Investment
Savings and Loan Crises (1980s–1990s)
Schumpeter, Joseph (1883–1950)
Schwartz, Anna (1915–)
Seasonal Cycles
Securities and Exchange Commission
Securitization
Shackle, George (1903–1992)
Shadow Banking System
Shock-Based Theories
Slow-Growth Recovery
Smith, Adam (1723–1790)
Souk al-Manakh (Kuwait) Stock Market Crash (1982)
South Africa
South Sea Bubble (1720)
Southeast Asia
Spain
Spiethoff, Arthur (1873–1957)
Spillover Effect
Sprague, Oliver (1873–1953)
Sraffa, Piero (1898–1983)
Stability and Stabilization, Economic
Steindl, Josef (1912–1993)
Stimulus Package, U.S. (2008)
Stimulus Package, U.S. (2009)
Stochastic Models
Stock and Bond Capitalization
Stock Market Crash (1929)
Stock Market Crash (1987)
Stock Markets, Global
Stockholm School
Subsidies

Summers, Lawrence (1954–)
Sunspot Theories
Sweden
Switzerland
Systemic Financial Crises
Tax Policy
Technological Innovation
Tequila Effect
Thorp, Willard Long (1899–1992)
Three-Curve Barometer
Thrift Supervision, Office of
Tinbergen, Jan (1903–1994)
Tobin, James (1918–2002)
“Too Big to Fail”
Transition Economies
Treasury Bills
Treasury, Department of the
Tribune Company
Tropicana Entertainment
Troubled Asset Relief Program (2008–)
Tugan-Baranovsky, Mikhail Ivanovich (1865–1919)
Tulipmania (1636–1637)
Turkey
UBS
Unemployment, Natural Rate of
United Kingdom
United States
Veblen, Thorstein (1857–1929)
Venture Capital
VeraSun Energy
Viner, Jacob (1892–1970)
Volcker, Paul (1927–)
Von Neumann, John (1903–1957)
Wachovia
Wages
Washington Mutual
Wealth
World Bank
WorldCom
Zarnowitz, Victor (1919–2009)
Chronology
Glossary
Master Bibliography
Books
Web Sites

Introduction
 
To people whose jobs, lifestyles, families, and futures depend upon the ups and downs of capitalist markets—in
other words, just about all of us—economic booms and busts may seem like elemental forces of nature. For every
action, it would appear, there is an equal and opposite reaction. What goes up must come down. The cycle of
good times and hard times, finally, is beyond human control.
Once upon a time, there was some truth to this view. Prior to the industrial age, when human endeavor was
largely confined to agriculture, natural forces largely determined economic feast or famine. Indeed, one of the first
intellectual efforts to understand the rhythms of market economies was known as the “sunspot theory.” According
to its author, the nineteenth-century British economist William Stanley Jevons, the eleven-year cycle of sunspot
activity, identified by astronomers of the day, influenced the earth’s climate, which affected crops, causing
economies to expand and contract.
Subsequent to Jevons’s cosmological explanation of the boom-bust cycle, other economists pointed to causes
more firmly rooted on Earth and, specifically, in the doings of the planet’s most enterprising species: humans.
Economic cycles, they said, were the result of credit (John Stuart Mill), of fixed capital investment (Clément
Juglar), and of technological innovation (Nikolai Kondratieff). What all of these theories shared was the notion that
there are predictable rules and principles, akin to the laws of nature, that explain economic cycles. These rules
abide because human beings, in their role as homo economicus, or economic man, act rationally. According to this
view, people act as efficient cogs in a great economic machine, their behavior following immutable laws and
principles. This mechanistic view of human economic behavior—the British neoclassical paradigm, as it were—
was very much in sync with the nineteenth-century Newtonian view of how nature itself works: logically and
predictably.
Far from British shores, meanwhile, in the central European city of Vienna, Austria, an alternative paradigm
emerged. For the thinkers of the Austrian school, economics is not driven by natural law but by human
psychology; value is not determined mechanistically by adding up all the costs of making a product but by how
much people want that product. Economic growth and contraction, according to this view, are not determined by
natural law, but by the ambitions, insights, daring, and, yes, error of very human entrepreneurs. Despite these
fundamentally different understandings of what determines the rhythms of an economy, the British neoclassical
and Austrian psychological/entrepreneurial schools of thought did share at least one important assumption and
conclusion about economic cycles. The assumption was that booms and busts, while obviously having beneficial
or deleterious effects on people’s lives, are not central to how economies function but are, instead, self-correcting
anomalies. Based on this assumption, these very different economic schools both concluded that there is very little
governments can or should do to counteract boom-and-bust cycles, or even ease the want and suffering they
cause.
The Keynesian revolution of the middle third of the twentieth century changed both the assumption and conclusion
of the neoclassical and Austrian schools of thought and put the business cycle and efforts to manage it at the
heart of economic theory and practice. Whatever causes booms and busts, argued the British economist John
Maynard Keynes, economic forces can produce a situation in which the price equilibrium set by supply and
demand—that is, the equilibrium that determines the utilization of economic resources, including both capital and
labor—can become stuck well below full an economy’s full capacity to produce, leaving both factories and labor
idle. Thus, the Great Depression formed a backdrop to Keynes’s greatest work, The General Theory of
Employment, Interest and Money (1936). In it, Keynes argued that only government has the power to reinvigorate

demand—through fiscal and monetary means—and thus lift an economy out of economic stagnation.
Keynes offered a series of tools governments can use to smooth out economic cycles and the hardships they
cause—tools that were eagerly taken up throughout the industrialized capitalist West in the decades following
World War II. And so successful were these tools—or so they appeared to be, amid the greatest economic boom
in world history—that economists and policy makers spoke of capitalism having put the economic cycle itself
behind it for good.
That illusion was shattered by the repeated economic contractions of the 1970s and early 1980s, accompanied,
seemingly in violation of basic economic principles, by large doses of inflation. While Keynes’s heirs argued for
wage and price controls to rein in inflation, policy makers largely ignored these ideas in favor of a purely
monetarist approach—making sure that the money supply was kept in sync with economic growth. Once again,
two decades of solid economic growth, with a minimum of economic contractions, led economic thinkers and
tinkerers to believe they had found the economic Holy Grail—a way to avoid or, at least, minimize the boom-bust
cycle.
Contributing to this thinking were key technological and financial innovations of the late twentieth and early twenty-
first centuries. The information revolution created by the personal computer and the Internet offered better and
more easily accessible information to market participants, promising to minimize economic inefficiencies and
market errors. And the securitization of debt—whereby the inherent risk of lending could be minimized through the
marketplace—meant that more credit was available to drive economic growth.
Of course, as the financial crisis and recession of the late 2000s proved, neither technological innovation nor debt
securitization could permanently keep the wolf from the door. Indeed, debt securitization proved to be the wolf
itself, encouraging the kind of reckless financial leveraging that had caused so many booms to turn to busts in the
dark ages, before economists and policy makers hubristically came to believe they could banish the economic
cycle itself.
Conceived and created in the midst of the worst economic recession since the 1930s, this encyclopedia attempts
to explain what the boom-and-bust cycle is all about—in theory, in history, and in real-life implications. Before
describing the content and organization of this work, a brief explanation of its key concepts is in order. A boom is
a period of rising economic expectation and activity triggered by any number of causes—population expansion, a
new technology, the discovery of natural resources, the emergence of new industries, an increase in productivity,
and the like. Booms go through a series of stages, beginning with a period in which investor confidence is
matched by real economic growth. This is followed by a second phase, in which investor euphoria leads to
outsized expectations, speculation, and increasing financial leveraging. At some point, when the savviest investors
begin to recognize the frothiness of the expansion and start to pull out of the market, a decline in prices follows,
which then triggers a mass sell-off and a rapid drop in prices. The bust that follows is marked by a precipitous
decline in financial activity, production, and sales, leading to a broad contraction in business, rising unemployment,
and a proliferation of bankruptcies—until the cycle is repeated.
 Contents
The contents of this encyclopedia—more than 360 articles, 120 images, a chronology, a glossary, and ancillary
materials—take it from there, offering the why’s, how’s, what’s, when’s, where’s, and who’s of economic booms
and busts. Historically, the work begins with the first great episode in modern speculation—the Dutch tulip boom
of the 1630s, and concludes with the Great Recession of the late 2000s, encompassing nearly 400 years of
economic history. Geographically, it includes articles on all the major economies of the world, but with an
emphasis on U.S. economic history. The book extensively covers the housing and securities boom of the mid-
2000s and the financial crisis and global recession that followed. Readers will also find biographical entries on the
important thinkers in the field (most of these are not American, a reflection of the history of the discipline) and the
theories they advanced, with an emphasis on economists whose work has focused on business and trade cycles.
In addition, a number of biographies highlight recent and current economic policy makers and the decisions they

have made. There are also profiles of economic and financial institutions, in both the private sector and
government (most of these are American). In no small measure, this work also presents more abstract and
technical aspects of economics and business cycle theory, with entries, written in laymen’s terms, on essential
ideas, terms, and schools of thought.
Finally, a word on what this encyclopedia is not. Booms and Busts is not a general economics text. As its title
implies, the work at hand focuses on one critical aspect of economic history and theory—the economic cycle. At
the same time, because the theory and reality of economic cycles are so interconnected, a close reading of the
encyclopedia’s contents will provide readers with a general understanding of the last 400 years of both. Readers
are also likely to find articles of varying degrees of difficulty. Many entries, particularly those pertaining to economic
history, can be easily understood by readers with a passing knowledge of the field. Other entries, particularly
those on theoretical subjects, may be elucidated with the help of the Glossary.
In addition to word search—in either Quick or Advanced modes—tools for navigating the contents of this
encyclopedia include an alphabetically arranged Browse list and a Topic Finder, along with hyperlinked cross-
references at the end of each article to other related entries. A comprehensive Chronology recounts the history of
booms and busts from the 1630s to the present day. And a Master Bibliography of books, articles, and Web sites
—in addition to Further Reading lists for each article—directs users to a wealth of recommended resources for
expanded research.
Africa, Sub-Saharan
 
The second-largest continent by landmass and population—after Asia, in both cases—Africa is a land of contrasts.
(All references in this article to “Africa” refer to sub-Saharan Africa; for a discussion of North Africa, see the article
“Middle East and North Africa.”) On the one hand, its vast mineral wealth and abundant supply of arable land
make it potentially one of the richest regions of the world. But centuries of exploitation—including the slave trade
and European colonization—along with corruption, mismanagement, and a lack of political stability since
independence came to most of the continent in the 1960s have left it with the lowest per capita income of any
major region in the world.
Aside from South Africa, the continent is not heavily industrialized and it accounts for a miniscule percentage of
global trade. Only its mineral sector—including a growing oil industry—has garnered significant foreign investment
since independence. But reliance on the export of minerals has also left it vulnerable to global price swings in that
notoriously volatile sector.
Still, by modern globalization standards, Africa’s economy is relatively unintegrated with that of the rest of the
world. That isolation, along with Africa’s growing trade with Asia, has led some economists to argue that Africa
may have become less dependent on the West and less vulnerable to economic fluctuations originating there,
though the global financial crisis and subsequent recession of the late 2000s has tested that assumption.
 Colonial Legacy
Africa is where humankind first evolved, though for much of human history the continent remained largely isolated
from the civilizations of Asia, the Middle East, and Europe, separated by vast oceans and the daunting expanses
of the Sahara Desert. By the European Middle Ages, however, significant parts of the continent had begun to be
integrated into global trading networks, with Arab traders engaging in seaborne commerce along the eastern coast

of the continent and in trans-Saharan commerce with West and Central Africa. Such commerce included trade in
precious metals, exotic tropical goods, and slaves, connecting the African urban centers of Zimbabwe and
Timbuktu.
With the rise of European seaborne commerce in the middle of the second millennium CE, the people of Africa—
particularly those in its coastal regions—found new trading partners interested in the same goods that Arab traders
had been. In particular, Europeans were interested in exporting human beings—in the form of chattel slaves—to
colonies in the Western Hemisphere. Between 1500 and the late nineteenth century, when the international trade
in slaves was effectively banned, tens of millions of Africans were shipped overseas. The negative impact of the
slave trade on Africa’s economic development was huge. Not only did European slavers—aided by African allies—
remove millions of the continent’s most productive residents—persons in the prime of life were obviously more
valuable than children and the elderly—but the trade itself created endemic political instability that hampered
economic development and internal trade.
And just as Europeans abandoned the slave trade, they found a new way to exploit the region. While sizable
European settlements had existed on the continent since the sixteenth century, particularly in southern Africa, it
was not until the latter half of the nineteenth century that new weaponry and public health measures, which
allowed outsiders to overcome African resistance and disease, led to the continent’s colonial subjugation. By
1900, all of sub-Saharan Africa—aside from Ethiopia and Liberia—was controlled by various European powers.
Governance varied widely in quality among the various European colonizers. While Britain and France made some
efforts to build modern infrastructure and train administrators in their holdings, smaller powers, such as Belgium
and Portugal, provided little of either. And even in model colonies, such as France’s Senegal and Britain’s Gold
Coast (later Ghana), much of the infrastructure was built to serve European interests. In particular, transportation
systems were built to bring raw materials to coastal ports rather than to integrate the continent’s economy.
Perhaps even more destructive were the unnatural borders Europe imposed on Africa, dividing ethnic groups
among different nations and throwing sometimes mutually antagonistic groups into single political entities. Thus,
when independence came, the dozens of new African states were nations in name only. Moreover, they were
nations with little infrastructure and few trained administrators.
 Post-Independence Politics and Economics
In the days immediately following independence there was much optimism, on the continent and abroad, that
Africa could leapfrog over the centuries it took Europe to emerge as a modern industrialized region. After all, the
continent was blessed with enormous natural resources, abundant fertile land, and a youthful, vibrant population.
Many of the newly independent states—often led by the charismatic men who had liberated them from European
rule—embraced a statist, even socialist approach to economic development, with the government investing in
heavy industry and infrastructure. The idea was to create self-sufficient economies, where local needs for
everything from steel to consumer goods would be met by manufacturing goods internally, freeing Africa from what
was viewed as capitalist, neocolonial exploitation.
It did not turn out that way for several reasons. First, the internal markets of most African countries were too small
and too poor to sustain much industry and the lack of continent-wide transportation systems made it difficult for
one country to trade with another. Second, there was a lack of trained technicians and managers to run these new
industries. Finally, many of the new leaders and officials proved either incompetent or corrupt or both, pocketing
much of the foreign capital that came into the country for their own personal use.
Continued poverty and corruption contributed to political instability. Beginning with Togo in 1963 and continuing
into the early twenty-first century, African countries have been rocked by violent coups and bloody and destructive
civil conflicts, the latter often inflamed by tribal divisions. Such strife and corruption were often exacerbated from
the 1960s through the 1980s by cold war tensions, as the United States and the Soviet Union propped up
authoritarian and repressive regimes as long as they sided with Washington or Moscow in the superpowers’ global
struggle for political dominance.

With a few exceptions, African economies stagnated in the late twentieth century as well, many actually shrinking
from where they had been at independence. Adding to the continent’s woes, many European countries pursued
agricultural and import policies—including subsidies to domestic farmers and high tariffs on certain goods—that
made it difficult for Africans to export their goods to their former colonizers. Finally, many African countries
experienced rapid population growth—particularly in urban areas—that strained various governments’ ability to
provide sufficient services and overwhelmed the job-creating capacities of underdeveloped economies. Africa, in
the second half of the twentieth century, became highly dependent on foreign aid. All told, nearly one-half of all
Africans subsist on the equivalent of less than $1 per day per person, making it the most impoverished major
region in the world. By comparison, in South Asia—the second poorest region in the world—about one-third of
the population lives on a dollar a day or less, while in Latin America the figure is between 10 and 20 percent.
 Promise and Peril in the Twenty-First Century
As the twenty-first century has dawned, much of Africa continues to struggle with the same economic problems
that plagued it in the latter half of the twentieth century, including the AIDS pandemic, which, like the slave trade
centuries before, has decimated the ranks of Africa’s most productive age cohorts. Despite being a major oil-
producing region, Africa has experienced acute energy shortages, undermining efforts to industrialize. And while
the number of coups has gone down and the number of democracies has gone up, Africa’s reputation for political
instability and its endemic corruption continue to discourage foreign investment. Moreover, despite numerous
efforts to create transnational political organizations, the continent’s various national economies operate largely
independently of one another.
But while such problems have discouraged Western investment in Africa, the continent has found a new and
important trading partner in East Asia. Led by China, a number of rapidly growing East Asian economies have
begun investing heavily in the continent. China, in particular, sees Africa as a strategically important region and
has struck deals with various governments there for long-term access to mineral resources, oil, and timber. Other
East Asian and oil-rich Middle Eastern countries have also obtained long-term leases for agricultural land. In
exchange, many of these new trading partners have committed to infrastructure development even as they pursue
a policy of noninterference in domestic political affairs. That is, while much Western aid comes with requirements
that outside authorities monitor how the money is used, Asian investment capital comes with few strings attached.
The new partnership between Africa and Asia has led some economists to talk of a decoupling of those regions
with the West. It has been argued, for example, that increased integration with Asia has freed Africa from the
effects of economic crises originating in America and Europe. But the financial crisis that began in the United
States in 2007 and spread through much of the industrialized and industrializing world in 2008 has not left Africa
untouched.
While it is true that the crisis that hit the world’s financial system largely bypassed Africa’s relatively unintegrated
and backward financial sector—South Africa’s being a notable exception—the freezing up of the world’s credit
markets has made obtaining necessary loans that much more difficult. At the same time, what limited foreign
investment has come Africa’s way has decreased since 2007 and the global recession has brought down prices
for the mineral exports upon which so many African economies depend.
Meanwhile, the continent faces a new and far more lasting crisis from climate change, as the consensus among
scientists in the field is that Africa is the region of the world that will face the most deleterious effects of rising
temperatures and erratic weather patterns, including drought, spreading disease, and coastal flooding.
James Ciment
 
See also:  Emerging Markets;  Middle East and North Africa;  South Africa;  Transition
Economies. 

Further Reading
Ayittey, George B.N. Africa Unchained: The Blueprint for Africa’s Future. New York: Palgrave Macmillan, 2005. 
Meredith, Martin. The Fate of Africa: A History of Fifty Years Since Independence. New York: Public Affairs, 2005. 
Reader, John. Africa: A Biography of the Continent. New York: A.A. Knopf, 1998. 
Rodney, Walter. How Europe Underdeveloped Africa. Washington, DC: Howard University Press, 1974. 
Agriculture
 
Human civilization’s oldest economic activity, agriculture remained one of the most important components of every
national and regional economy until the industrial revolution of the nineteenth century. Even today, it remains the
key economic activity of many people in developing countries, though the proportion of people who make their
living from agriculture has fallen to just 3 percent in the United States.
Agriculture has always been subject to the vagaries of nature. Weather, insects, and disease have traditionally
produced great variations in output, as have human-caused events such as war, social unrest, and bad farming
practices. In turn, the variations in output have produced great fluctuations in other sectors of the economy,
though their impact in industrialized countries—where smaller numbers of people depend upon agriculture for a
living and where food has become a smaller part of consumers’ budgets—has diminished significantly over the
past century. Indeed, as recently as the 1930s, the average American family spent about one-quarter of its income
on food; today, even with fluctuations in food prices, it spends less than 10 percent, a result of more productive
agricultural practices and crops.
Several reasons explain why food prices have dropped even as the number of people in agriculture has declined:
more efficient agricultural methods, more productive crops, and labor-saving machinery. In addition, most
developed countries, including the United States, provide a variety of subsidies to farmers, both to maintain a
viable agricultural industry and to keep food prices lower for consumers.
Still, in many parts of the developing world, agriculture continues to be a labor-intensive activity with little
machinery, primitive farming practices, and less productive crops. Thus, while famine and even hunger have
largely been eliminated from the developed world, periodic and regional food shortages continue to plague
developing countries, particularly in Africa. Moreover, subsidies to domestic agriculture in industrialized countries
often hurt farmers in developing countries, who find it hard to compete against subsidized food imports from the
developed world or to export their crops to developed countries.
Because agriculture has been so important to regional and national economies historically, economists in the past
spent much time and energy trying to explain how agricultural output affected the business cycle. The three most
important schools of thought are the Malthusian theory, the sunspot theory, and the cobweb theory.
 Malthusian, Sunspot, and Cobweb Theories
In 1798, the British economist and demographer Thomas Malthus developed a doomsday model of economic
fluctuations. His theory was rooted in neoclassical economics, which argues that supply and demand naturally
tend toward price equilibrium. That is, if demand for a product grows, then prices will rise, leading to investment,
increased output, and lower prices. If supply outpaces demand, the opposite occurs.

While significant improvements in British agricultural methods had occurred by Malthus’s time—notably, crop
rotation and fertilizer input—they had occurred long before it. By the late eighteenth century, agricultural output
had increased only incrementally over the previous century, primarily through increased use of land and labor.
Because the amount of capital (land and equipment) remained relatively fixed, and with inputs of labor creating
only arithmetic increases (adding a laborer yields a relatively small increase in production), the supply of food was
limited to slow growth.
The demand side of the equation, however, was quite different. By Malthus’s time, the population of Great Britain
was increasing rapidly as a result of improved diet and health and increased manufacturing output. Thus, demand
would inevitably outstrip supply, sending food prices soaring and sometimes leading to shortages. This, said
Malthus, would lead to poverty, hunger, and starvation, thereby decreasing population. Malthus called this process
the “preventive check.” That is, if supply was limited to slow growth, then the neoclassical equilibrium of supply
and demand required that the latter increase slowly as well. As a demographer, Malthus understood that this was
not the case. In his view, therefore, the tension between food supply and demand explained the peaks and
troughs of Britain’s economic cycle.
There was, however, a basic flaw in the Malthusian theory—the assumption that capital inputs in the form of new
technology were relatively fixed. In fact, with the industrial revolution dawning in Great Britain, new labor-saving
technology arrived on the scene. That technology, along with improvements in agricultural methods and crop
management, meant that a given quantity of fixed capital in the form of land, along with a fixed amount of labor,
could still yield increased amounts of food, thereby increasing the supply to meet rapidly rising demand.
The “sunspot theory” was first developed by another British economist, William Stanley Jevons, in his
posthumously published work, Investigations in Currency and Finance (1882). Jevons argued that sunspots—or
increased solar activity—affected weather patterns and growing conditions on Earth. Increased solar activity
caused agriculture to decline, he argued, which led to downturns in the business cycle. Jevons’s main evidence
was the coincidence of sunspot activity and downturns in business, which he said occurred every 10.45 and 10.43
years, respectively. But Jevons’s argument had two fundamental flaws. First, he miscalculated the cycle of sunspot
activity, which typically peaked every 11.11 years. Second, even if Jevons had been right that commercial activity
and sunspot activity coincided, there was little evidence then—or since—that solar activity directly affects weather
(even if modern science has found that solar activity does affect the ionosphere and the electromagnetic
spectrum).
The Hungarian-born British economist Nicholas Kaldor (1908–1986) first formulated the “cobweb theory” in 1934.
According to that view, disruptions to the stability of agricultural markets do not always correct themselves, as
neoclassical theory assumes, even if prices and output are allowed to move freely without regulation or other
forms of government interference. This is because farmers are literally backward-looking, taking into account last
year’s crop in planting decisions for this year so that prices and output form a “cobweb” of too-high prices one
year, causing a rise in output the next, then too-low prices resulting in a fall in output. If prices for crops were
high the year before, the farmer would plant more the next season. And with other farmers following the same
course, a surplus would be produced, which in turn would drive less efficient farmers into insolvency and out of
business. Such failures might well lead farmers to plant cautiously the following year, leading to shortages and
higher prices—perpetuating the up-and-down cycle. Adding to the problem was the fact that, in Kaldor’s time, it
was difficult or expensive to store crops from one year to the next. In short, rather than moving toward a price
equilibrium, crop prices tend toward volatility, which directly and significantly affects the business cycle.
The cobweb theory has less applicability in the modern world because most commodities, agricultural or
otherwise, can now be stored over a long period of time, and prices are adjusted through futures markets that
anticipate possible surpluses or shortages. Thus, any miscalculation about demand can be more readily smoothed
over from stored inventories of the commodity in question or by sales or purchases in futures markets. If farmers
can be forward-looking, a stable equilibrium is likely to be reached much faster, with less cyclical behavior.
Nonetheless, cobweb-type models are used in modern explanations of bubbles and cyclical behavior or commodity

prices.
The impact of price fluctuations on agricultural commodities is felt far and wide. But while they may be a burden
on the pocketbooks of consumers in developed nations, price fluctuations can be a matter of life and death in the
developing world, particularly in areas where people live on the edge of subsistence, such as the Sahel region of
Africa. Since the 1980s, countries in the horn of Africa at the eastern end of the Sahel—notably, Ethiopia—have
suffered repeated famines that have killed hundreds of thousands of people. Moreover, price fluctuations can
trigger political turmoil, as was the case in Haiti and a few other countries during a run-up in agricultural
commodity prices in 2008.
 Financial Crisis of Late 2000s
The global financial crisis of 2008–2009 had major effects on agricultural markets and the fluctuations in price for
a wide range of farm products. Indeed, the sharp and unexpected fluctuations in global agricultural prices were a
direct consequence of the weakening of global financial markets. As they took their money out of the stock
market, speculators placed an increasingly large portion of it in agricultural commodities markets. They did this in
the hope of gaining back losses in the financial markets by speculating on the future prices of these commodities.
As a result, a bubble on agricultural products began to emerge as demand from speculators inflated the price of
grains and other products far above their natural market value. Such speculative attacks on agricultural commodity
markets became a serious and far-reaching problem, as they contributed directly to hunger, starvation, and related
ills in many vulnerable regions of the world. In December 2008, in large part because of the speculative price
bubble, the total number of hungry people in the world reached an estimated 963 million—40 million more than
the previous year. As agricultural prices came down in the second half of 2008, a global recession had already
started, and decreasing incomes further deepened poverty and hunger, especially in rural areas.
Mehmet Odekon and James Ciment
 
See also:  Commodity Markets;  Seasonal Cycles;  Sunspot Theories. 
Further Reading
Da-Rocha, J.M., and D. Restuccia.  “The Role of Agriculture in Aggregate Business Cycles.” Review of Economic
Dynamics 9:3 (2006): 455–482. 
Ezekiel, Mordecai.  “The Cobweb Theorem.” Quarterly Journal of Economics 52:2 (1938): 255–280. 
Jevons, William Stanley. Investigations in Currency and Finance. London: Macmillan, 1909. 
Kaldor, Nicholas.  “A Classificatory Note on the Determination of Equilibrium.” Review of Economic Studies  1:2 (February
1934): 122–136. 
Magdoff, Fred, John Bellamy Foster, and Frederick H. Buttel, eds. Hungry for Profit. New York: Monthly Review
Press, 2000. 
Malthus, Thomas. An Essay on the Principle of Population. London: John Murray, 1826. 

 
AIG
 
American International Group, or AIG, is both the world’s largest insurance company and a major financial
services institution. At its peak, shortly before the global financial collapse of 2008, the New York–headquartered
company employed more than 80,000 people and had operations in more than 130 countries. Once the tenth-
largest corporation in the United States, AIG possessed hundreds of billions of dollars in assets. As an insurer
and a financial-services institution, AIG invested heavily in exotic and risky securities—including mortgage-backed
securities—which led to its near-collapse in late 2008 as the value of those securities plummeted. Deemed “too
big to fail” by U.S. government officials fearful that its collapse would greatly aggravate the global financial markets
—since AIG insured so many other securities with a protective instrument called a credit default swap—the
company became the largest recipient of U.S. Federal Reserve (Fed) bailout funds that year, more than $150
billion in all.
Media gather outside the Manhattan headquarters of insurance giant AIG in September 2008, after the Federal
Reserve authorized an initial $85 billion to keep the firm afloat. The demise of AIG would have dealt a devastating
blow to the global financial system. (Stan Honda/AFP/Getty Images)
AIG was founded in Shanghai, China, in 1919 by American businessman Cornelius Vander Starr, the first
Westerner to sell insurance to Chinese clients. Known as American Asiatic Underwriters, the company operated in
China until the Communist takeover in 1949. Meanwhile, Starr launched other companies, including the Asia Life
Insurance Company and, with British and Chinese partners, the International Assurance Company. Together they
expanded operations into other parts of Asia, as well as Latin America, Europe, and elsewhere. Starr also founded
such other companies as the American International Underwriters Overseas Company and American International

Reinsurance (AIRCO), which, as its name implies, was involved in the business of backing the policies of other
insurers by spreading the risk inherent in offering insurance among various firms. Through AIRCO, Starr also
expanded his operations in the United States in the 1950s and 1960s by acquiring other insurance companies.
The American holdings did poorly, however, and in 1962 Starr hired a New York lawyer and financier named
Maurice “Hank” Greenberg to turn them around. Five years later, the two partners formed American International
Group, or AIG, as Greenberg reoriented his firm’s business from personal to more lucrative corporate coverage,
offering industrial, commercial property, and casualty insurance. Starr died the following year, and Greenberg took
over as president and chief executive officer. After going public in 1969, AIG expanded into a variety of
businesses in subsequent decades, including credit services, real estate, health care, oil drilling, and the leasing
of commercial jetliners. In retrospect, the most fateful of the new ventures was AIG Financial Products, launched
in 1987, when several traders from Drexel Burnham Lambert, a pioneer in junk bond sales and corporate mergers
and acquisitions, convinced Greenberg to utilize AIG’s sterling credit rating to develop and invest in complex
financial instruments known as derivatives. Akin to reinsurance, derivatives allow investors to take out a kind of
insurance policy on other investments, with the price of the instrument derived from the value of another security.
Initially, derivatives were a useful and relatively low-risk tool for spreading investment exposure. That gradually
changed over the course of the 1990s and 2000s, as traders at AIG and other financial institutions came to realize
that derivatives could provide large returns as well as hedges against other investments. Better still, they were
virtually unregulated in the United States and most other countries, allowing financial institutions to avoid including
them in the assets-to-investment ratios that governments required for more traditional investments. By 2005,
however, despite the favorable regulatory environment, AIG became the subject of government fraud
investigations, culminating in substantial fines and the removal of Greenberg as CEO.
Nevertheless, high returns and lax oversight had helped expand the derivatives market into a multi-trillion-dollar
business, with AIG being one of the most aggressive of the businesses involved in it. Its AAA credit rating allowed
AIG, under standard financial industry practices, to avoid having to come up with collateral to cover its positions in
the derivatives market; the firm therefore was able to invest huge sums against relatively few assets. Thus, when
the crisis hit the financial markets in the summer of 2008, AIG found itself especially exposed. And when its credit
rating was downgraded on September 16, it found itself unable to come up with the required tens of billions of
dollars in collateral. Meanwhile, the company’s stock price had plummeted to just $1.25 by mid-September, a
fraction of its high of more than $70 the previous year, putting further stress on the company’s finances.
AIG was on the verge of collapse, and this was a source of concern not only to the company’s shareholders and
management. AIG was not just a giant company. Because it insured so many securities of all kinds, it was integral
to the very functioning of global financial markets. U.S. government officials, including Secretary of the Treasury
Henry Paulson and Federal Reserve chairman Ben Bernanke agreed that the failure of AIG could destroy
confidence in the financial markets, freeze the credit that drove the global economy, and plummet the world into a
deep recession. The Fed moved quickly, providing some $85 billion in credit to AIG, secured by the company’s
assets, to meet its collateral obligations. In exchange, the government received a about 80 percent equity stake in
the company.
As enormous as the credit line was, it proved insufficient to keep AIG afloat. In October, the government provided
an additional $37.8 billion in credit, and in November it purchased $40 billion in newly issued preferred stock. In
the process, AIG emerged as a symbol of the excesses and greed of Wall Street and as the target of popular
outrage when it was learned that company executives had treated themselves to a lavish retreat at a California
resort days after receiving the first bailout money and paid themselves large bonuses several months later.
With government backing, however, AIG remained solvent through the worst of the financial crisis in late 2008,
though it was a now largely government-owned company. The revelation of AIG’s involvement in high-risk,
unregulated derivatives trading spurred calls in Congress and from the White House for tighter regulation of these
markets. Nevertheless, many of the same executives who had led the company to the verge of bankruptcy were
kept on, as first the George W. Bush and then Barack Obama administrations came to the conclusion that only

they had the knowledge and experience to undo the damage they had caused.
James Ciment
 
See also:  Collateralized Debt Obligations;  Credit Default Swaps;  Financial Markets; 
Mortgage-Backed Securities;  Recession and Financial Crisis (2007-);  “Too Big to Fail.” 
Further Reading
AIG:  www.aig.com
Cohan, William D. House of Cards: A Tale of Hubris and Wretched Excess on Wall Street. New York: Doubleday, 2009. 
Shelp, Ronald, with Al Ehrtar. Fallen Giant: The Amazing Story of Hank Greenberg and the History of AIG.  2nd ed.
Hoboken, NJ: John Wiley and Sons, 2009. 
Airline Industry
 
A relatively stable industry in its first sixty or so years of operation—barring wartime—the commercial airline
industry both in the United States and around the world became much more susceptible to market forces and
fluctuations in the business cycle as a result of government policies to deregulate the industry, beginning in the
United States in the 1970s and soon spreading around the world. Indeed, the airline industry represents a test
case for the pluses and minuses of deregulation: while airline deregulation initially promised more competition and
lower prices for consumers, it eventually led to more bankruptcies, more consolidation, and deteriorating service.
 Growth
Commercial aviation—that is, the carrying of passengers, mail, and freight as a for-profit business—began about a
decade after the first heavier-than-air flight by the Wright brothers in 1903. Government contracts to carry the mail
beginning in 1918 offered a kind of subsidy to help fledgling airlines start and stay in business. But technical
considerations—planes were small, limited to daytime flight, and incapable of long-distance flight without refueling
—limited the industry’s potential until the 1930s and the introduction of Douglas DC-3, which could fly passengers
across the United States in relative comfort, in fifteen to twenty hours with just three refueling stops.
The advent of pressurized cabins in the late 1930s, although not widely used in commercial aircraft until after
World War II, allowed planes to fly higher, escaping the turbulence of low-altitude flight. The introduction of
commercial jet aircraft further revolutionized the industry in the 1950s, with planes capable of transporting
passengers across the continent and overseas in a matter of a few hours and at even higher altitudes.
As the commercial airline industry grew and became international in scope, governments came to realize that they
would have to establish internationally recognized regulations and regulatory bodies. This began with the
Convention on International Civil Aviation known as the Chicago Convention of 1944, and was further developed
with the creation in 1947 of the International Civil Aviation Organization (ICAO), a United Nations agency. ICAO
introduced the principle that air transport services should be arranged between nations through bilateral air service
agreements. Meanwhile, countries set up agencies to regulate domestic air travel in order to ensure safety and
control air traffic. In the United States, this began with the Air Commerce Act of 1926 and the establishment of the

Civil Aeronautics Authority in 1938, the precursor of the current Federal Aviation Administration.
By the late 1950s and 1960s, air travel had become much more common in the developed world, with the United
States leading the way. But it was expensive. Aside from the very wealthy—the so-called jet set—and well-heeled
business travelers, it remained out of reach to most people. Part of this had to do with the introduction of costly
new aircraft, the purchase of which had to be amortized through high ticket prices.
 Regulation and Deregulation
But regulation played a role as well. It required airlines to price tickets dependent on length of flight rather than on
volume of travel. That is, two flights of the same length—one between two small markets (with fewer passengers)
and one between two big markets (with more passengers)—had to be priced the same, even though the
economies of scale would have made the latter flight cheaper under free-market conditions.
More importantly, regulation limited competition. Fare changes, the introduction of new routes, and entry of new
airlines into the marketplace were hampered by all kinds of rules and bureaucracy that made it very difficult for the
industry to respond to market forces and changing customer demands. Adding to the airlines’ woes were
skyrocketing fuel prices, a result of the energy crisis of the early and middle 1970s.
As part of a general trend toward freeing markets from government control, many economists had been arguing
that deregulation would both help the industry grow and provide better and lower-cost services for air travelers.
Responding to such arguments, Congress passed the Airline Deregulation Act in 1978. For small start-up airlines,
the timing could not have been better. A slow economy had weakened the major carriers and made it possible to
purchase idle aircraft at low prices and hire out-of-work airline personnel at lower wages.
Airline deregulation fulfilled many of its supporters’ hopes. All kinds of new low-cost carriers entered the market,
offering fewer frills but cheaper ticket prices, particularly between high-volume markets. This forced the majors, as
the larger, more established airlines were known, to lower their own prices. In addition, airlines were given more
freedom to change routes and add more flights to suit market needs, allowing them to respond to customer
demands more quickly. One of the major innovations brought about by deregulation was the hub-and-spoke
system, in which airlines fed travelers from smaller markets into major markets and then on to their ultimate
destination, rather than directly between smaller markets, thus allowing for fuller flights, lower costs, and lower
ticket prices. The result of all of this was increased capacity and lower costs; a study done by the Government
Accountability Office twenty years after deregulation found that ticket prices (adjusted for inflation) overall were 30
percent lower than they had been before deregulation. These results partly influenced other countries to follow
suit, as was the case in much of Europe beginning in the early 1990s.
But there was also a downside to deregulation. First, by allowing market forces to have a greater impact on ticket
prices, deregulation ensured that people flying between smaller markets—where economies of scale did not come
into play—would pay more proportionally than people flying between major markets, upsetting a long-standing
principle of equitability in transportation that went back to the first regulations of railroads in the late nineteenth
century. In addition, as airlines found their profit margins on each ticket reduced due to competition, they were
forced to cut back on services, making air travel less pleasant. In addition, increased competition taxed the
existing air travel infrastructure, leading to more delays for travelers.
Finally, by increasing competition and the role of market forces in the industry, deregulation led to a series of
high-profile bankruptcies, particularly among the majors, as well as consolidation within the industry, as airlines
attempted to further cut costs by gaining market share and exploiting economies of scale. Among the most high-
profile bankruptcies were two from 1991: Eastern Airlines and Pan American World Airways (Pan Am). Major
consolidation efforts include American Airlines’s acquisition of Trans World Airlines (TWA) in 2001 and Delta
Airlines’s purchase of Northwest Airlines in 2008, the latter deal creating the world’s largest commercial air carrier.
Such consolidation is meant to help airlines weather an ever more volatile era, as carriers respond to political,

health, and economic disruptions that have an impact on the volume of air travelers, revenue streams, and profits.
Indeed, the first decade of the twenty-first century saw a series of unprecedented shocks to the industry, including
the terrorist attacks of September 11, 2001, which undermined air traffic for a time and led to increasing security
delays and costs; the severe acute respiratory syndrome (SARS) outbreak, which undermined travel to the fast-
growing Asia market in 2002–2003; and the financial crisis and recession of 2007–2009, which hit the lucrative
business traveler market especially hard.
James Ciment and M.V. Lakshmi
 
See also:  Oil Shocks (1973-1974, 1979-1980). 
Further Reading
Belobaba, Peter, Amedeo Odoni, and Cynthia Barnhart. The Global Airline Industry. Chichester, UK: John Wiley and
Sons, 2009. 
Cento, Alessandro. The Airline Industry: Challenges in the 21st Century. New York: Springer Heidelberg, 2008. 
Doganis, Rigas. The Airline Business. New York: Routledge, 2005. 
Sheth, Jagdish, Fred C. Allvine, Can Uslay, and Ashutosh Dixit. Deregulation and Competition: Lessons from the Airline
Industry. Thousand Oaks, CA: Sage, 2007. 
Akerman, Johan Henryk (1896–1982)
 
Johan Henryk Akerman was among the economists of the early twentieth century who defined the growing
discipline of econometrics. He applied a rigorous scientific approach, modeled on the study of physics, to economic
research in general, and he was one of the first economists to apply it to the analysis of business cycles.
Akerman was born in 1896 in Sweden, the younger brother of economist Gustav Akerman (1888–1959). His book
Rhythmics of Economic Life (1928), based on his dissertation, examined business cycle theory. While defending
his dissertation, Akerman began a long-running debate with economist and review-board member Ragnar Frisch
over the definition of business cycles. The debate not only served to sharpen the definition of business cycles but
also the methods by which they were studied. Akerman taught at the University of Lund and his approach came to
be known as the Lund school of economics.
Using a combination of deductive and inductive approaches, Akerman suggested that a business cycle was not an
isolated event but rather a collection of smaller business cycles that combined to form larger cycles, which in turn
create still larger cycles. He believed that changes within the smaller cycles created their own variations, which
would eventually cause variances in the larger-scale cycles. There were, he believed, interdependencies between
seasonal and cyclical changes. In spring and fall, there is typically a rise in economic activity. These seasonal
variations, however, become less pronounced—and even nonexistent—during the boom phase of a cycle as well
as during depressions. According to Akerman, economies had seasonal, agricultural, political, “Juglar” or fixed-
investment, and building cycles, each one of a certain duration. His theories of business cycles, published in
Theory of Industrialism (1960), were not met with universal agreement, although the idea of a political cycle (the
four-year period between presidential elections) gained some support in the United States. Akerman died in 1982.

Robert N. Stacy
 
See also:  Classical Theories and Models;  Frisch, Ragnar. 
Further Reading
Akerman, Johan. Economic Progress and Economic Crises. Philadelphia: Porcupine Press, 1979. 
Akerman, Johan. Some Lessons of the World Depression. Stockholm: Nordiska Bokhandeln, 1931. 
Akerman, Johan. Theory of Industrialism: Causal Analysis and Economic Plans. Philadelphia: Porcupine Press, 1980. 
Boianovsky, Mauro, and Hans-Michael Trautwein.  “Johan Akerman vs. Ragnar Frisch on Quantitative Business Cycle
Analysis.” European Journal of History of Economic Thought 14:3 (September 2007): 487–517. 
Argentina
 
Located in the southern cone of South America, and with a population of just over 40 million, Argentina is a
country with vast natural resources and a relatively literate and educated workforce. Yet, because of bad
economic policy and corruption, much of its history has been marked by a series of rapid economic expansions
and severe recessions. Its high inflation rates of the late twentieth century surpassed those of nearly any other
country in history. The periodic economic crises have resulted in forced changes in government, most recently in
2002, when rioting forced the president from office. The rapid shifts in Argentina’s economic fortunes have had a
lasting effect on its population and the way in which lenders regard Argentine investment.
Argentina achieved independence from Spain in 1810. Sparsely populated at the time, it was divided into two
climate zones. The coastal region was temperate with abundant rainfall, while the interior was drier and more
mountainous. A majority of the inhabitants in the interior were associated with the silver mines in Bolivia. When
the silver was depleted, most people migrated to the coast. The interior grasslands then depended on less-labor-
intensive ranching. Until the last quarter of the nineteenth century, Argentina’s economy remained largely
dependent on exports of animal hides, leather, salted beef, and wool.
After 1875, the Argentine economy expanded at a rapid pace. Real income approached that of the United States.
Rapid immigration, particularly by skilled laborers, helped alleviate the chronic labor shortage. Foreign investment,
especially from Great Britain, allowed the development of infrastructure including an extensive railroad network
that opened markets to products from the interior. Wheat, flax, and maize became major exports, thanks to
advances in agriculture. The use of refrigeration permitted the export of meat products, and the more affluent
European population provided the demand.
In 1890, a banking crisis nearly caused the collapse of the Argentine economy. Government bonds promising a
large return had been purchased by many British investors, especially the Baring Bank. When payments on these
bonds became too great, the Argentine government defaulted. Only a bailout by the Bank of England prevented
major bank failures in Great Britain. As a result, foreign investment in Argentina, along with immigration, sharply
declined for several years, until observers were satisfied that stability had returned.
By 1914, Argentina once again had one of the leading economies in Latin America. Although the economy was
dependent mostly on exports, some industrialization had taken place. Printing plants, metalworking factories, and

textile mills had been established to meet domestic demand. When World War I broke out in 1914, many imports
were no longer available. Foreign investment went to supporting the war effort and was not available for
investment. The decline in worldwide demand after the war caused a decline in Argentina’s economy. Conditions
remained poor during the 1930s because of the Great Depression. Subsidies for agriculture and an agreement
with Great Britain to buy some exports helped limit the decline.
In 1930, the Argentine army staged a bloodless coup against the leftist civilian government of Hipólito Yrigoyen
because of the economic conditions. A policy of import-substitution industrialization was adopted by the new
government. Domestic industry was created to produce products formerly imported. These industries were given
tariff protection and subsidies as necessary. When World War II broke out, exports of food products increased,
resulting in a trade surplus of nearly $2 billion by 1945.
In 1946, populist Juan Perón took power in Argentina, promising better economic conditions through greater
government intervention. He nationalized financial institutions like the central bank, as well as the railroads and
public utilities. Perón also created a single government purchaser for all grain products intended for export. The
economy grew at first under Perón, and the quality of life improved, with many schools and hospitals being built.
When the international grain market declined, however, the government limited production. Trade deficits after
1949 led to inflation. Perón adopted more business-friendly policies in 1952, but was overthrown in 1955.
Succeeding governments adopted a policy of “developmentalism.” Investment, particularly from foreign countries,
was encouraged in industry, energy sources, and public works, with assistance in the mortgage and business
lending sectors. When combined with an austerity program for government spending, recession and inflation
resulted. The subsequent foreign investment caused the economy to rebound, with real growth in wages for
workers.
In 1973, Perón returned to power. The oil crisis of that year caused trade deficits resulting in budget deficits and
inflation. Perón’s government responded with poorly timed wage freezes and hikes that did little to improve
conditions. Perón died in July 1974, and the army staged a violent coup in March 1976. The military was unable
to stop the galloping inflation that reached over 100 percent annually. Public confidence declined because of
human rights abuses and repression by the military government. Humiliated after an unsuccessful war with Great
Britain over the Falkland Islands (Islas Malvinas) in 1982, the military was forced to turn power over to a
democratically elected government in 1983. During the rest of the 1980s, Argentina was plagued by inflation, labor
unrest, corruption, and the inability to achieve financial stability.
In 1990, President Carlos Menem adopted new policies. The Argentinean peso was tied to a fixed exchange rate
with the U.S. dollar, and limits placed on unsecured currency. The inflation rate fell from 1,300 percent in 1990 to
almost zero in 1995. Almost all state-run companies were sold to private investors, and their subsidies were
abolished. Trade agreements with Brazil and other Latin American countries increased exports. Although the
Argentine economy improved in the early 1990s, the rest of the decade was marred by sporadic growth because
of external events.
In 1999, Argentina entered a recession largely because other countries’ currency declined relative to the dollar.
Imports increased because they were cheaper, while exports were more expensive for foreign customers. Fearing
a devaluation of the peso, many Argentineans converted their savings to dollars. To prevent money from being
sent out of the country, the government enacted a law known as the corralito, freezing most bank accounts and
making it difficult for businesses to operate. Many Argentineans took to the streets to protest. The most common
form of protest was the cacerolazo, consisting of banging pots and pans together. Frustrated citizens soon began
to destroy property of large and foreign businesses. On December 20 and 21, 2001, violent protests ended with
several protesters being killed and President Fernando de la Rúa resigning. Argentina defaulted on $93 billion in
loan payments that month.
Most Argentine politicians refused to assume a leadership role. Eduardo Duhalde, vice president of Argentina from
1989 to 1991 and governor of Buenos Aires from 1991 to 1999, accepted the presidency and ordered a

devaluation of the peso in January 2002. Inflation, unemployment, and most other economic indicators
skyrocketed. In May 2003, Néstor Kirchner took office. The devalued peso made Argentinean exports more
attractive, and a trade surplus had been created. Kirchner encouraged domestic industry and used the inflow of
dollars to pay off many of Argentina’s international loans. Wages increased and inflation was held largely in check.
Thanks to Argentina’s success in weathering this crisis, it was largely able to avoid the worst effects of the 2008–
2009 recession.
Tim J. Watts
 
See also:  Brazil;  Emerging Markets;  Latin America. 
Further Reading
Coatsworth, John H., and Alan M. Taylor. Latin America and the World Economy Since 1800. Cambridge, MA: Harvard
University Press, 1998. 
Della Paolera, Gerardo, and Alan M. Taylor, ed. A New Economic History of Argentina. New York: Cambridge University
Press, 2003. 
Edwards, Todd L. Argentina: A Global Studies Handbook. Santa Barbara, CA: ABC-CLIO, 2008. 
 
Asian Financial Crisis (1997)
 
The financial crisis that hit many East and Southeast Asian economies in the summer of 1997 was arguably the
worst and most widespread of its kind in the modern history of the developing world. Triggered by a rapid
withdrawal of foreign investment funds in Thailand—which caused a large and rapid devaluation in that nation’s
currency—the crisis quickly spread to other fast-growing economies in the region, such as Indonesia, Malaysia,
and South Korea, among others. In the aftermath, economists pointed to a number of underlying causes—some
endogenous, or internal, and some exogenous, or external—including weak national financial regulatory systems,
real-estate bubbles, excessive foreign borrowing, and panic selling by foreign investors. The Asian financial crisis
of 1997 set back economic growth in many of the affected countries for the rest of the decade and into the early
2000s. The crisis also had political ramifications, particularly in Indonesia, which saw the fall of the more than
three-decade-old Suharto dictatorship. In addition, many countries imposed tighter regulations on their financial
sectors, though calls for larger international reforms—including stricter controls on capital flows—went largely
unheeded.

A tourist exchanges Thai currency, the baht, at an exchange booth in Bangkok in 1997. The collapse of the baht
led to slumping currencies, stock market declines, and economic hardship across East Asia. (Pornchai
Kittiwongsakul/AFP/Getty Images)
 Causes
With economic globalization heating up in the 1980s, many developing economies in Asia experienced a sudden
and massive inflow of investment money from the United States, Western Europe, Japan, and other developed-
world nations. At the time, however, Asian financial institutions were generally ill equipped to deal with the inflow.
Banks, according to many economists, were poorly regulated, and standards for determining the credit of
borrowers were largely inadequate. Meanwhile, financial liberalization across Asia promoted dramatic shifts toward
speculative financing and development, as local banks became heavily involved in risky domestic lending and
local firms gained access to large amounts of foreign capital.
Much of the borrowing and investment was aimed at the real-estate sector. Asian governments, looking to further
attract foreign investment, offered tax breaks and other incentives to local developers to build residential and
commercial properties in the hope of luring developed-world businesses to make direct foreign investments in the
region. Prices for these real-estate assets, as well as the value of stock shares in local companies that owned
them, rose dramatically. The increases in valuation induced further capital inflows. Much of the collateral (or
guarantees) the banks accepted for foreign loans was real estate and equities, assets whose prices included a
large “bubble” element (i.e., prices had risen far above the assets’ intrinsic value).
Adding to this financially risky situation was the heavy borrowing local banks were engaging in, usually with U.S.

banks as creditors. This left the banks exposed. Should Asian currencies weaken relative to the dollar, it would be
that much harder to service the loans because they were in U.S. dollars. In some regional economies, the ratio of
foreign debt to gross domestic product (GDP) skyrocketed, from about 100 percent in the early 1990s to nearly
200 percent by mid-1997.
The crisis began in Thailand in July 1997. Finding itself with a rising foreign debt, the Thai government had been
trying desperately for some time to prop up its currency, the baht, to keep it pegged to the dollar. As these efforts
became too costly, the Thai government decided to float the baht, which triggered its rapid devaluation. Foreign
investors began to panic, pulling their funds out of Thailand. The contagion—foreign capital flight and monetary
devaluation—spread to other emerging Asian economies, such Malaysia, the Philippines, and, especially,
Indonesia. Yet even developed economies in the region—such as those in South Korea, Hong Kong, Taiwan, and
Singapore—were affected. The only relatively unaffected countries were those more closed to unregulated inflows
of foreign investment at the time—including Vietnam and the two Asian giants, China and India—though asset
valuations declined in those countries as well. Meanwhile, the Japanese stock market, full of companies that had
invested massively in other parts of Asia, was also hard hit. Farther afield, stock markets around the world saw
dramatic declines that many economists linked to the Asian financial crisis. In the so-called minicrash of October
27, 1997, the Dow Jones Industrial Average, to take the most closely watched of the world’s major indexes, fell by
more than 550 points, or about 7 percent. In Asia itself, the medium-term effects on the affected economies were
profound. Over the next four years, GDP losses amounted to 24 percent in South Korea, 26 percent in Malaysia,
54 percent in Thailand, and 83 percent in Indonesia, one of the steepest recorded downturns in modern economic
history.
 IMF Response
As the crisis was unfolding, it became clear that local governments were unable to address the financial panic—
some, like Thailand, were even on the verge of bankruptcy—and required outside help. The institution set up at
the end of World War II to offer emergency assistance in such circumstances was the International Monetary
Fund (IMF), an international lender largely financed and controlled by the governments of the major economies of
developed world. Operating under the premise that the causes of the crisis were largely endogenous to the region
—that the crisis arose from structural distortions in the affected nations’ financial systems and economies—the
IMF decided to lend the countries money to shore up and stabilize their currencies.
As is often the case with IMF assistance, the loans were offered on the condition that significant short-and long-
term economic reforms would be made. First, the IMF insisted that central banks raise interest rates to cool the
overheated national economies and fight inflation. Participating governments were also required to cut spending
dramatically and close down struggling financial institutions. Regardless of whether or how effective this proved to
be, it added to the immediate economic pain felt by the citizens of these countries, many of whom had already lost
money in the devaluation and collapse of asset prices.
Many inside and outside the region criticized the IMF for its assessment of the crisis and the requirements it
imposed on rescued governments. Some, such as Nobel Prize– winning economist Joseph Stiglitz, argued that
many of the affected countries already had economies and financial sectors as well run as those in many
industrialized countries and that the source of the crisis was the “herd” mentality of foreign investors who suddenly
panicked and withdrew their funds. What was needed, Stiglitz and other IMF critics said, were better international
controls on the flow of investment capital. Other economists charged that the IMF’s belt-tightening solution was
exactly the wrong one for reviving the affected economies since it strangled demand, which was critical to growth.
Moreover, it was said, the minimal efforts to help the most vulnerable segments of society deal with rapidly rising
prices and falling incomes led to ethnic violence and political unrest. In Indonesia, rioters attacked ethnic Chinese
enclaves, and escalating instability led to the overthrow of the Suharto regime the following year.
The Asian financial crisis provided economists with stark insights into the role of irrational behavior in economic
decision-making and how such behavior affects exchange rates. Foreign exchange markets, some experts argued,

may suffer from herd behavior that gives rise to sudden surges in capital inflow, followed by dramatic capital
outflows as euphoria turns to panic. The consequences of such pronounced fluctuations include persistently
unstable exchange rates, which can result in banking and financial crises with painful consequences for the real
economy. As a result of such psychological forces, speculative bubbles may generate exchange rates wildly out of
sync with such fundamental economic factors as inflation and interest rates, exports and imports, growth and
productivity, and domestic saving and investment rates.
If there is a lesson to be learned from the 1997 crisis, say some economists, it is that developing and transitional
economies need to consider the trade-offs between higher rates of growth, financed by rapid expansion of foreign
and domestic debt, and financial stability. Slower growth may be preferable if it is more sustainable in the long
run.
John Lodewijks and James Ciment
 
See also:  Business Cycles, International;  China;  Emerging Markets;  Korea, South; 
Southeast Asia. 
Further Reading
Arndt, H.W., and Hal Hill, eds. Southeast Asia’s Economic Crisis: Origins, Lessons and the Way Forward. St. Leonards,
Australia: Allen and Unwin, 1999. 
Bhagwati, Jagdish. The Wind of the Hundred Days: How Washington Mismanaged Globalization. Cambridge, MA: MIT
Press, 2002. 
Goldstein, Morris. The Asian Financial Crisis: Causes, Cures and Systemic Implications. Washington, DC: Peterson Institute
for International Economics, 1998. 
McLeod, Ross, and Ross Garnaut, eds. East Asia in Crisis: From Being a Miracle to Needing
One? London: Routledge, 1998. 
Stiglitz, Joseph. Globalization and Its Discontents. London: Penguin, 2002. 
Stiglitz, Joseph E., and Shahid Yusef, eds. Rethinking the East Asian Miracle. New York: Oxford University Press, 2001. 
Asset-Price Bubble
 
An asset-price bubble occurs when the price of a certain asset or class of assets rapidly rises above the
underlying historical value of the asset based on measured criteria other than price or by balancing price with
other criteria. For example, an asset-price bubble in corporate securities would mean that the price of corporate
shares had exceeded normal price-to-earnings or price-to-revenue ratios. In the case of housing, an asset bubble
might be said to occur when the housing prices in a given market rise above their historic ratio relative to rents or
to median income levels.
It is often difficult amid the complexity of market forces to ascertain when a bubble is occurring. That is to say,
economics and other experts often have a hard time defining the intrinsic value of a given asset at a given
moment in time, thereby making it hard to be sure those prices are rising too far and too rapidly above that value.
Thus, many asset-price bubbles are only understood in retrospect.

Economists disagree about the causes of asset-price bubbles, though there is general consensus that excessive
financial leverage plays an important role. In other words, when credit is too widely available and too easy to
access, there will be asset-price bubbles. Two examples from American history back up this idea—one from the
late 1920s and one from the late 2000s.
The middle and latter 1920s saw a dramatic run-up in the price of many corporate securities. Economic historians,
however, differentiate between the price increases in the middle years of the decade and those of the latter years.
Between the end of the recession in 1922 and the height of the economic boom in 1927, the Dow Jones Industrial
Average (DJIA) rose from about 80 to about 150. This was a substantial increase, to be sure, but one merited by
increases in corporate profits and revenues, many generated by the real gains in productivity created by new
forms of energy (electricity) and transportation (motor vehicles). In other words, the rise in corporate securities did
not represent an asset-price bubble. But then, between 1927 and 1929, the DJIA soared from about 150 to more
than 380 points, even as underlying economic growth—including corporate revenues and profits—was slowing
down. In other words, the growth in prices of corporate securities was far outpacing values.
Much of the explanation for this was excessive financial leverage. Low interest rates and the easy credit policies
they permitted encouraged people to borrow for the purpose of investing. Brokerage houses allowed people to buy
stock with just 10 percent down, financing the rest, a transaction known as margin buying. In turn, the brokerage
houses were able to borrow from banks, often because they were part of the same company. Inevitably, with
people able to buy far more stock than their own resources permitted, share prices rose dramatically, far in excess
of their intrinsic value.
A parallel situation occurred in the early and middle 2000s in the U.S. housing market. Low interest rates
orchestrated by the Federal Reserve and a flood of foreign capital coming in to pay for America’s trade imbalance
made it possible for financial institutions to lend on more liberal terms. Rather than buying corporate securities on
margin—an act that had been severely curtailed in the wake of the 1929 stock market crash with margin
requirements being increased from 10 to 50 percent—people began to take out loans to buy homes or refinance
existing mortgages at lower interest rates.
Traditionally, people taking out mortgages were required to come up with 20 percent of the purchase price of the
property from their own funds. But with credit standards loosening, lenders began to drop these standards,
increasing the leverage of homebuyers. In addition, bankers began to lower their standards on who qualified for a
mortgage to persons who previously would have been deemed too risky to lend to. Many of the latter were given
adjustable-rate mortgages (ARMs), in which a low initial interest rate gave way to a fluctuating one. This
increased the overall leveraging of the housing finance markets, since it increased the amount of money being
borrowed to take out or refinance mortgages.
Margin buying in the 1920s and high-risk mortgages in the 2000s did not present a problem as long as the price
of the asset being purchased continued to climb. In the case of the former, investors could pay back the margin
call with the profits they made on the stock. In the case of the latter, mortgagors could refinance using the rising
equity in their homes.
Of course, increasing the level of financial leveraging cannot go on forever. When stock and housing prices began
to decline in 1929 and 2007 respectively, the dangers of financial leveraging were revealed in bankruptcies and
foreclosures. In both cases, prices dropped precipitously precisely because they had been driven up by financial
leveraging. That is, just as leveraging drove prices up rapidly, so deleveraging drove them down just as quickly
and dramatically.
As the Great Depression of the 1930s and the so-called Great Recession of 2007–2009 reveal, the impact of
asset-price bubbles can reverberate throughout the economy and can be felt around the world. The dramatic fall
in corporate securities prices in the early 1930s forced banks—many of them having participated in the margin
buying frenzy—to cut back on lending, leading to a drop in investment and hiring. Moreover, many of the biggest
banks that had been involved in financing the purchase of corporate securities cut back on their credit to smaller

banks, thus sending out shock waves from New York through the rest of the American financial system. Finally,
the tightening of credit in the United States—then the world’s leading creditor nation—led to a dramatic downturn
in much of the world economy.
With the collapse of housing prices in the late 2000s, the scenario played out somewhat differently. Not only did
many banks suddenly find themselves with bad mortgages on their books, they were also burdened with other so-
called toxic assets such as securities that were based on bundled mortgages. (Indeed, the bundling of mortgages
into securities—originally seen as spreading out the risk of default to investors—had encouraged some of the
reckless lending that contributed to the housing asset-price bubble in the first place.) In response, banks began to
tighten credit to businesses—thereby hurting investment—and each other, causing panic in the financial markets
around the world.
In addition, many homeowners had used the rising equity in their homes to go on a buying spree through cash-
out refinancing. Or, as they expected to pay for their retirement with the rising equity in their homes, they saved
less and spent more. With the crash in housing prices, much of this spending stopped. And, as consumer
spending accounts for about 70 percent of economic activity in the United States, this put a major damper on the
U.S. economy. Moreover, since American consumers are an important market for exporters around the world, the
lowered level of spending hurt economies around the world.
James Ciment
 
See also:  Dot.com Bubble (1990s-2000);  Echo Bubble;  Florida Real-Estate Boom (1920s); 
Housing Booms and Busts;  “Irrational Exuberance”;  Mississippi Bubble (1717-1720); 
Overvaluation;  Poseidon Bubble (1969-1970);  Real-Estate Speculation;  Souk al-Manakh
(Kuwait) Stock Market Crash (1982);  South Sea Bubble (1720);  Stock Market Crash (1929). 
Further Reading
Barth, James R. The Rise and Fall of the U.S. Mortgage and Credit Markets: A Comprehensive Analysis of the Market
Meltdown. Hoboken, NJ: John Wiley and Sons, 2009. 
Fox, Justin. The Myth of the Rational Market: A History of Risk, Reward, and Delusion on Wall Street. New
York: HarperBusiness, 2009. 
Galbraith, John Kenneth. The Great Crash, 1929. Boston: Houghton Mifflin, 1997. 
Gramlich, Edward. Subprime Mortgage Crisis. Washington, DC: Urban Institute, 2007. 
Krugman, Paul. The Return of Depression Economics and the Crisis of 2008. New York: W.W. Norton, 2009. 
Posner, Richard A. A Failure of Capitalism: The Crisis of ’08 and the Descent into Depression. Cambridge, MA: Harvard
University Press, 2009. 
Shiller, Robert J. Irrational Exuberance. New York: Currency/Doubleday, 2005. 
Thomas, Gordon, and Max Morgan-Witts. The Day the Bubble Burst: A Social History of the Wall Street Crash of 1929. New
York: Penguin, 1980. 
Thornton, Mark.  “The Economics of Housing Bubbles.” In America’s Housing Crisis: A Case of Government Failure, ed.
Benjamin Powell and Randall Holcombe. Edison, NJ: Transaction, 2009. 
Australia

 
Occupying the entire continent of Australia, the island nation of Australia is a wealthy, industrialized country
located between the Indian and Pacific oceans, south of the equator. Aside from a small minority of native
Aborigine peoples, the country’s 21 million people—or their ancestors—are largely immigrant, hailing from Great
Britain, other European countries, and Asia.
Australia boasts a major manufacturing and service sector, as well as large-scale agriculture, and is also a major
exporter of minerals, particularly to markets in Asia. At various stages in its history since British colonization in
1788, the Australian economy has been characterized by some striking features in comparison with other Western
economies. In the second half of the first century of colonial development, for example, it boasted one of the
highest living standards in the world. During the 2000s, when other developed economies experienced recessions
following the bursting of the dot.com bubble and the onset of the global financial crisis, the Australian economy
avoided recession altogether despite slowdowns in its rate of growth. However, the nation’s economic development
has been subject to significant periods of boom and bust.
 Nineteenth Century
Some of the recurring factors in this experience are illustrated by the country’s first colonial boom-and-bust
episode during the 1820s. The first half of this decade was characterized by an expansion of pastoral activity as
new lands became available beyond Sydney, one of the initial centers of colonization. This expansion was
facilitated by the import of substantial volumes of consumer goods and significant immigration from Britain.
Increased demand for loans to support the growing boom in wool production also gave rise to the establishment of
new banks, supported by the import of financial capital from London. Competition between these banks led to
interest rate reductions, which added further stimulus to the expansion. A series of events in the second half of the
decade interrupted this boom, culminating in the economic depression of 1828–1830. The 1825 economic crisis in
Britain reduced demand for Australian wool, causing a significant decline in the wool price and in pastoral
incomes. The British crisis also reduced the flow of financial capital to the colony, thereby affecting the banks’
ability to lend. The credit crisis was further exacerbated by demands placed on the Bank of New South Wales,
which was in the process of capitalizing newer banks. A drought in 1827–1828 further reduced pastoral output
and incomes, resulting in the subsequent depression.
Both the environment and human factors have had a major impact on Australia’s economy over the past two
centuries. Droughts and domestic financial crises have disrupted the economy, as have international downturns,
which have periodically reduced both prices and demand for Australia’s agricultural exports. These experiences
manifested themselves in three significant and prolonged booms—in 1850–1890, 1950–1970, and 1993–2007—
and two significant economic depressions—in 1891–1899 and 1929–1932—as well as a series of milder economic
recessions.
The long booms all share similar features. Principal among these is the precondition of strong economic conditions
in the rest of the world, leading to rising demand for Australian agricultural commodities and resources. In the
extended boom of 1850–1890, the demand was for wool and gold (discovered at Bathurst and Ballarat in 1851).
During the booms of 1950–1970 and 1993–2007, the demand included a broader range of agricultural produce,
such as wheat, and other resources, such as coal, iron ore, bauxite, uranium, and oil. Other boom features have
included strong growth in investment spending, credit expansions to support this investment, and increased
imports of capital. In some cases, the booms were also associated with the inflation of asset prices, particularly
those of property and stocks. This was the case, for example, in the latter stages of the boom of 1850–1890. The
Melbourne land boom of the 1880s resulted not only from increased wealth accumulated during the broader boom
but from improved transport infrastructure to Melbourne’s outer suburbs, technological innovations that facilitated
industrial development in these areas, and strong population growth. With these developments came an
expectation of higher land prices that, in conjunction with the easy availability of finance, led to the realization and

extension of such expectations. This represented the classic case of what economists call an “asset bubble”; it
also led to a deterioration of credit standards as financial institutions competed with each other for business,
further fueling the bubble.
Australia’s two severe depressions reflect similar forces to those that caused the depression of 1828–1830. In
1890, London’s Baring bank crisis precipitated a contraction of capital flowing to the Australian economy and an
economic downturn in England that reduced the world price of wool. The first of these led in turn to a restriction of
credit in the Melbourne market, stemming property demand, causing property prices to fall and the expectation of
further price declines to reinforce the downward movement. The combined impact of the falls on overextended
borrowers and reduced incomes from lower wool prices led to loan defaults and bank failures that compounded
the economic contraction. National income fell by about 10 percent in 1892 and did not return to its 1891 peak
until 1899. Unemployment also increased significantly during this period.
 Twentieth and Twenty-First Centuries
While not usually included in the list of booms, the Australian economy experienced solid growth immediately
following the end of World War I. This resulted primarily from increased government spending, a significant rise in
population, and a faster pace of growth in the manufacturing sector than previously experienced. Economists
Chay Fisher and Christopher Kent have argued that over the course of the 1920s this growth also led to
increased construction activity and property speculation similar in nature to that of the 1880s property boom.
In 1929, Australia had already been in mild stagnation for a number of years due to a combination of
overinvestment in industrial capacity following World War I, a contraction of demand for Australian exports due to
unfavorable shifts in the ratio of the price of Australian goods to those available from other parts of the world, and
expectations that these forces would lead to depressed economic conditions. The disruptions to financial markets
in New York and London caused by the October 1929 stock market crash led to similarly sharp drops in Australian
stock prices, higher domestic interest rates due to the reduced supply of funds from London, and depressed
expectations that significantly curtailed spending. In conjunction with the existing situation in the Australian
economy, these events caused a similar reduction in economic activity in 1930 to that experienced in 1892, albeit
with an even greater increase in unemployment. The Great Depression was not, however, associated with a
significant number of bank failures in Australia, as was the case in the United States and as had been the case in
the depression of 1891–1899, which some economists argue accounts for the more rapid recovery of the
Australian economy than was the case in the two other episodes.
Following World War II, the Australian economy showed mixed results. Its geographic isolation and limited internal
market made it difficult for its manufacturing sector to grow and increase its competitiveness in international
markets. In addition, the government protected the industry from outside competition, which tended to slow gains
in productivity. But at the same time, its mineral sector boomed, with Australian commodities helping to fuel the
economic miracle in Japan and later East Asia from the 1950s onward.
In addition to these events, a boom in stock and property prices that occurred in the late 1980s is noteworthy.
While this coincided with a short period of strong economic growth between recessions in 1981–1882 and 1991–
1992, this boom is frequently attributed to financial deregulation in the first half of the 1980s. Removal of
restrictions on interest rates led to a rapid expansion of lending that fueled a pattern of increased asset prices,
expectations of further price rises, increased asset demand financed by credit expansion, and further asset price
increases. An additional factor frequently identified by economists in relation to the boom of 1993–2007 is
increased labor market flexibility and reduced levels of industry protection progressively introduced during the
1980s and early 1990s.
And while banks were to come under considerable pressure when the stock and property bubble of the late 1980s
burst following significant increases in interest rates engineered by the central bank, no bank was to fail in that
episode, and its impact on the Australian economy was small by comparison with the Great Depression or the
depression of the 1890s.

The Australian economy was also able to avoid recessions altogether following the bursting of the dot.com bubble
and the global financial crisis of 2008–2009, partly due to carefully regulated banks and partly due to the prompt
and aggressive use of fiscal and monetary policies to boost aggregate demand.
Thus, while the boom-and-bust experience of the Australian economy has largely reflected international economic
forces, the impact of these forces has been significantly magnified or mitigated by domestic events, structures, and
policies.
Peter Docherty
 
See also:  New Zealand;  Poseidon Bubble (1969-1970). 
Further Reading
Boehm, E.A. Prosperity and Depression in Australia, 1887–1897. Oxford, UK: Clarendon Press, 1971. 
Butlin, S.J. Foundations of the Australian Monetary System, 1788–1851. Sydney: Sydney University Press, 1953. 
Edwards, John. Australia’s Economic Revolution. Sydney: University of New South Wales Press, 2000. 
Kindleberger, Charles P. Manias, Panics, and Crashes: A History of Financial Crises. Hoboken, NJ: John Wiley and
Sons, 2000. 
McLean, Ian W.  “Australian Economic Growth in Historical Perspective.” Economic Record 80:250 (September 2004): 330–
345. 
Merrett, David.  “Some Lessons from the History of Australian Banking.” Special Edition, Economic Papers (December
2006): 52–60. 
Schedvin, C.B. Australia and the Great Depression. Sydney: Sydney University Press, 1970. 
Austrian School
 
Founded in the mid-nineteenth century by financial journalist-turned-economics-professor Carl Menger, the
Austrian school of economics—so named because its founder and many of its leading adherents hailed from that
Central European country—emphasized ideas that continue to influence economic thinking. Austrian school
economists maintained that the value of goods and services depend not on the labor and material that went into
producing them, but on the perceived utility they provide to the seller, the purchaser, and the user. Austrian school
economists also pointed to entrepreneurs as key agents of economic change. So central are these ideas that the
Austrian school is referred to as the “psychological school” or “entrepreneurial school” of economics.
In addition, the emphasis on the subjectivity of value has led members of the Austrian school to maintain that
market forces—that is, the decisions made by millions of individuals—always optimize economic performance.
Politically, then, Austrian school economists have argued that government intervention in the economy—beyond
the strict enforcement of contracts reached freely and voluntarily between individuals and maintaining a stable
money supply—inevitably disrupts the workings of the market in negative ways. Fiercely antisocialist as well,
Austrian school economic thinking has been adopted by libertarians and other radical free-market advocates in the
years since its founding. Among its most important practitioners have been Friedrich von Wieser and Eugen Ritter

von Böhm-Bawerk in the late nineteenth and early twentieth centuries, Ludwig von Mises and Joseph Schumpeter
in the first half of the twentieth century, and Nobel Prize–winner Friedrich von Hayek and Murray Rothbard in the
mid-to-late twentieth century. Aside from Rothbard, an American, all of these individuals were born and educated
in the Austro-Hungarian Empire or, after its demise in 1918, Austria.
 Utility and Opportunity Cost
At the heart of Austrian school thinking is the concept of marginal utility, or the amount that utility increases by the
addition of one unit of an economic good or service. For example, if a purchaser of potatoes is hungry, the utility of
the first potato is very high; but as the person’s appetite is satiated, the marginal utility—and, hence, value to the
purchaser—of each additional potato decreases, since the value of the second potato sitting in his or her larder as
a ward against future hunger is not as great as the value of the first potato in satisfying immediate hunger. In
other words, the value of a potato is not determined by how much work went into sowing it, tending it, reaping it,
and bringing it to market, but on how much individual economic agents, at a given point in time, want it.
Also critical to Austrian school economic theory is the idea of opportunity cost—the idea that the value a person
places on a commodity is determined not just by how much that person desires the commodity, but by the
alternative foregone in purchasing that good. An example can be found in the decision made by an hourly-wage
worker who takes a day off and goes to the movies. The value of enjoying the movie, according to the theory of
opportunity cost, is not determined solely by the ticket price, but also by amount of his lost wages.
In his 1871 book The Principles of Economics, Menger was a contributor to the “marginal revolution,” a new
approach that challenged the thinking of classical economists in the British tradition—as well as the Marxist
tradition—who maintained that the value of goods and services can be determined objectively, based on the cost
of the labor that went into them. The marginal revolution represented a major intellectual paradigm shift in that it
placed the study of individual choice and decision-making at the center of economic inquiry.
 Entrepreneurialism and Money Supply
The Austrian school emphasizes the role of entrepreneurs as the agents of economic change, always alert to the
competitive advantages made possible by new technologies or business methods. Collectively, the decisions of
profit-seeking entrepreneurs steer a free market toward the most efficient use of economic resources.
Monetary policy is another focus of Austrian school economics. According to adherents, money is not a neutral
medium, as classical economists argued, but has a direct effect on market decision-making. By increasing the
money supply too fast, they maintain, central banks can trigger inflation. But because inflation affects the price of
some goods faster or more intensely than others, increases in the money supply beyond what is required for the
natural growth of the economy distort the normal exchange of goods and services and have a broad damaging
effect.
The twin focuses on entrepreneurialism and monetary policy have led a number of Austrian school economists to
the study of the business cycle. Entrepreneurial innovation, they argue, is a key ingredient of economic expansion.
Joseph Schumpeter, a leading Austrian school economist of the early twentieth century, qualified this view by
pointing out that too many entrepreneurs seeking to exploit an innovation causes profit margins to shrink,
bankruptcies to multiply, credit to dry up, and a recession to result. Moreover, while entrepreneurial innovation
affects economic growth and contraction over the long term, monetary policy—specifically, the setting of interest
rates by central banks—has a more immediate effect. By setting interest rates too low, central banks, like
America’s Federal Reserve, create too much credit and an “artificial” boom. While this can be politically expedient
for a government in power, it can have disastrous effects on the economy if pursued too aggressively or for too
long. Cheap credit triggers more borrowing, thereby flooding the market with capital, much of which flows
inefficiently after the diminishing profit margins of over-exploited new innovations or, worse, toward purely
speculative purposes. This, say Austrian school economists, is precisely what happened in the mid-2000s.
Inevitably, an overabundance of credit cannot be sustained and the credit markets contract, often sharply and

suddenly—as in 2008 and 2009.
According to the Austrian school, then, the best role of government in helping avoid boom and bust cycles is to
maintain a stable money supply. Interference in the natural workings of the market through economic stimulus
plans like that introduced by President Barack Obama in early 2009 is anathema to Austrian school, as they will
inevitably produce deleterious distortions in the allocation of economic resources. Because of its relatively laissez-
faire approach to government economic policy—as well as its suspicion of the activities of central banks—the
Austrian school has appealed more to the far right side of the political spectrum, with libertarians, such as 2008
Republican presidential candidate Ron Paul, being particularly strong advocates. More traditionally, Austrian school
economics have provided arguments for those critical of socialism and communism, on two fronts. First, as noted,
government involvement in the economy inevitably distorts the free market—the most efficient allocator of
economic resources—in negative ways. Second, as the importance of private property diminishes, so does the
capacity of individuals to make efficient economic choices.
Finally, Menger’s Austrian school was not just revolutionary in the ideas it propounded, but also in its
methodology. Adherents argued that general theories can explain economic behavior in all circumstances. This
contrasted with the view of the contemporary German historical school, which argued that economic behavior is
rooted in specific historical circumstances and that the purpose of economics is to accumulate data on those
circumstances that the government can then use to make effective economic policy. At the same time, Austrian
school economists maintain that economics is not a science in the same sense as natural sciences are. For one
thing, they maintained, standard experimental methodology cannot be reproduced in economics, since individual
factors cannot be isolated. Second, the actions of human beings are of such complexity that strict empirical inquiry
and mathematical models are useless. Instead, as Ludwig von Mises, an influential twentieth-century Austrian
school adherent, argued, economists should focus on the logical processes people use to make economic
decisions—a study he called “praxeology.”
While some aspects of Austrian school thinking have been adopted by mainstream economists —notably, the role
of entrepreneurial decision-making in microeconomic theory—it is largely viewed as an iconoclastic body of
thought by most practicing economists and economic policy makers today.
James Ciment
 
See also:  Böhm-Bawerk, Eugen Ritter von;  Haberler, Gottfried von;  Hayek, Friedrich August
von;  Mises, Ludwig von;  Monetary Policy;  Monetary Theories and Models;  Schumpeter,
Joseph. 
Further Reading
Hutchinson, T.W. The Politics and Philosophy of Economics: Marxians, Keynesians, and Austrians. New York: New York
University Press, 1981. 
Kirzner, Israel M. The Meaning of Market Process: Essays in the Development of Modern Austrian Economics. New
York: Routledge, 1992. 
Meijer, Gerrit. New Perspectives on Austrian Economics. New York: Routledge, 1995. 
Reekie, W. Duncan. Markets, Entrepreneurs, and Liberty: An Austrian View of Capitalism. New York: St. Martin’s, 1984. 
Automated Trading Systems

 
An automated trading system is any computer program that enacts trades on a stock or securities exchange in
response to user-inputted instructions. Such systems match bids and offers automatically and can “make
decisions” based on market behavior, according to the instructions encoded in the software.
Automated trading systems are also known as algorithmic (or algo) trading systems, robo trading, or black-box
trading, each term emphasizing a different aspect. The systems are especially popular among institutional
investors and fund managers, and can accommodate a wide range of risk and yield preference. There is no one
trading system, nor one trading system style; different systems can be designed with different goals, making
comparisons difficult.
The use of trading systems has greatly accelerated since the computerization of financial markets in the 1970s
and 1980s. The decimalization of the U.S. stock market—moving from sixteenths of a dollar to hundredths of a
dollar (pennies) as the basic unit of stock value—encouraged algorithmic trading by creating smaller differences
between bids and offers, as well as more price differentials to be exploited by arbitrage. As more and more traders
use automated trading, trade volume increases, which in turn increases the amount of data to keep track of. This,
in turn, encourages more traders to use automated trading, and so on. The use of software allows real-time
analysis of, and interaction with, the global financial markets at every hour of the day. Today, more than half of the
trades made on most stock exchanges, and about a quarter of trades made on foreign exchange markets, are
entered through trading systems.
Trading systems are rules based and algorithmic. An algorithm is a set of instructions, often represented by
flowcharts or “if... then” statements—used in data processing and other fields. A simple example of an algorithm is
the set of instructions for what to do when a customer orders a cheeseburger: collect further information
(doneness, type of cheese, toppings, special instructions) and then enact specific predetermined procedures
based on that information. This is a deterministic algorithm. A probabilistic algorithm incorporates a degree of
randomness at some point in the instructions. In the rules to Monopoly, for example, each player begins his turn
by rolling the dice to determine how many spaces to move.
Trading systems are particularly well suited to arbitrage, which takes large amounts of capital and uses it to
generate a profit from the price differential among multiple markets, buying low on one market and selling high on
another. “Low” and “high” are relative terms here, and the difference may be very small. But as long as there is a
difference, a trade large enough for the profit to exceed the transaction cost is a trade worth making. Arbitrage is
most common, then, where transaction costs are lowest. When arbitrage is conducted by a trading system, much
less labor is involved, so the price differential can be even lower while still yielding a profit. Further, computers can
identify such price differentials extremely quickly. Of course, this means that all the other arbitrageurs are scouting
out the same “deals.”
 Alternative Trading Systems
Automated trading systems should not be confused with alternative trading systems (ATS), a regulatory term
referring to trading venues that are approved by the Securities and Exchange Commission (SEC) for trading
outside of exchanges. Nevertheless, there is a relationship between automated and alternative trading systems.
Many ATSs operate electronically, such as broker “crossing networks” that use an automated trading system to
match buy and sell orders without publicizing either before the trade is made; this is often what is meant by
“black-box trading.” Many crossing networks are referred to by the ominous term “dark pools of liquidity.” These
are designed to allow for large trades without impacting the market, keeping knowledge of the trade private or
disclosing it at the last possible moment. Generally, the presence of such pools in the market can be inferred by
monitoring market depth—an easy task for trading systems—and noticing that the bid and offer have decreased
simultaneously by the same amount. But because this could just as easily be a coincidence, it is risky to act on
the assumption that major trades are being made in the dark. Dark pools are favored by institutional investors,

who usually make trades in large quantities.
Electronic communication networks (ECNs), authorized by the SEC in 1998, are publicly visible crossing networks
that execute trades in stocks and foreign currencies (and, less commonly, other securities). Well-known ECNs
include NASDAQ (an electronic stock exchange established in 1971) and Baxter-FX (an electronic foreign
currency exchange). Automated trading generally transpires on an ECN (which is not to say that all trading
transpiring on an ECN is automated).
Especially risk-averse traders, such as pension fund managers and other institutional investors, depend on the
Volume-Weighted Average Price (VWAP) benchmark, which is the ratio of the combined value of all the trades of
a stock over the course of the day so far, divided by the total number of shares traded. Brokers using automated
trading systems can offer a “guaranteed VWAP execution,” which means that the investor’s trade is guaranteed to
be performed at the VWAP. For a lower commission, a broker can offer a VWAP target execution, which has a
margin of error but will enact the trade very near the VWAP price.
 Fundamental, Quantitative, and Technical Analysis
Traditionally, investment strategy depended on fundamental analysis, which evaluates the soundness of a stock by
examining the business itself—its financial statements, its industry, its position in the industry, and its plans for the
future—and treating a stock purchase as an investment in the company (which of course it is). Healthy companies
made for healthy investments. Quantitative analysis, while not exactly opposed to this, developed alongside it in
the 1930s as economic science gained new prominence during the Great Depression. Such analysis uses the tool
of calculus to examine stocks and other securities, and is fundamental to most trading systems. The Black-
Scholes model developed out of quantitative analysis.
Technical analysis ignores most of the data crucial to fundamental analysis. Indeed, it barely acknowledges that
there is a company behind the shares of stock. Instead, technical analysis focuses on the performance of a
particular stock in the market. What the company makes and how good a job it does in making it does not really
matter—all that matters is what the stock sold for yesterday, last week, and last year, and how much of it was
sold. Technical analysis worries some investors for exactly that reason, and its claims to predict stock behavior are
often dismissed as pseudoscience. On the foreign exchange market, however, it is quite popular.
Technical analysis, like quantitative analysis, can be built into algorithmic trading systems more easily than
fundamental analysis can be. Moreover, the increased use of automated trading systems has contributed to the
prevalence of technical analysis. The underlying belief of the “chartists,” as technical analysts are often called, is
that investor behavior and market trends can be accurately predicted. This fuels not only many legitimate and
useful trading systems, but stacks and stacks of popular economics pulp, as one author after another claims to
predict the size and shape of the next big crash or the next big boom.
Moreover, the increased use of trading systems leads to the same kind of security-hacker or virus-antivirus “arms
race” as exists in the computing industry at large. While dark pools of liquidity conduct secret trades in the
shadows, as it were, “shark” algorithms try to detect them by making small market orders to locate the pools. The
pools, in turn, can be equipped with pattern-recognition algorithms and avoid or subvert identification. The more
popular an algorithm or algorithm type becomes, the more likely someone will develop and market another
algorithm to subvert or exploit it.
While automated trading has produced much more responsive financial markets in general, it can also lead to, or
accelerate, panics. Many analysts, including those involved in a presidential task force set up to investigate it,
blame the crash that took place on Black Monday, October 19, 1987, in part on automated trading. (On this day,
the Dow Jones Industrial Average experienced its largest single-day fall in terms of percentage.) Huge blocks of
stock were sold off automatically, based on algorithms geared to specific market conditions, contributing to the
downward slide in stock prices. In the wake of Black Monday, New York Stock Exchange officials instituted a rule
which said that if the Dow fell by more than 250 points in a session, program trading would be prohibited, allowing

brokers to contact their clients and infuse the process with human decision-making.
Bill Kte’pi and James Ciment
 
See also:  Stock Market Crash (1987);  Stock Markets, Global. 
Further Reading
Chan, Ernie. Quantitative Trading: How to Build Your Own Algorithmic Trading Business. Hoboken, NJ: John Wiley and
Sons, 2008. 
Kaufman, Perry J. New Trading Systems and Methods. Hoboken, NJ: John Wiley and Sons, 2005. 
McDowell, Bennett A. The Art of Trading: Combining the Science of Technical Analysis with the Art of Reality-Based
Trading. Hoboken, NJ: John Wiley and Sons, 2008. 
Stridsman, Thomas. Trading Systems That Work: Building and Evaluating Effective Trading Systems. New York: McGraw-
Hill, 2000. 
Babson, Roger (1875–1967)
 
Known as the “Seer of Wellesley Hills,” Roger Babson was an entrepreneur, statistician, and business and
economic forecaster who pioneered the use of charts in the analysis and prediction of business cycles. Like many
of his contemporaries in the first half of the twentieth century, including Ragnar Frisch, Babson sought to analyze
business cycles using what he regarded as scientific methodology. Where he differed from his contemporaries
was in adopting a loose—or, according to his critics, pseudoscientific—model, compared to their more rigorous
scientific and mathematical tools and methodologies. He is also known as the founder of Babson Institute—later
Babson College—a private business school established in Wellesley, Massachusetts, in 1919.
Roger Ward Babson was a member of the tenth generation of Babsons of Gloucester, Massachusetts, where he
was born on July 6, 1875. He attended the Massachusetts Institute of Technology and, after graduating in 1898,
worked for investment firms in New York and Boston. In 1904, he founded the Babson Statistical Organization in
Wellesley to analyse stocks and business reports for banks and investors; the firm continues operations today as
Babson-United, Inc. Babson’s interest in forecasting business cycles began to earn him recognition in the late
1920s, especially after September 5, 1929, when he predicted the stock market crash that occurred less than two
months later. Unfortunately, he had been bullish only a few months earlier and would again become bullish by
1930.
Babson’s approach to understanding and forecasting business cycles was founded on the basic concepts of
Newtonian physics, in particular Newton’s third law of motion (“for every action, there is an equal and opposite
reaction”). Babson applied the same principle to cycles of business expansion and contraction. Although his
method did not gain unanimous respect from economists who took a rigorous scientific approach, his weekly
bulletin, Babson’s Report, published under the aegis of the Babson Statistical Organization, attracted a wide
readership and gained a reputation for accurate predictions.
As early as 1912, Babson wrote that a common mistake among businessmen and economists was that they were
interested only in what happened in their own geographic regions, rather than in the country or the world as a

whole. Additionally, he maintained, too many businessmen and economists viewed the behavior of the
marketplace and the economic system from the narrow perspective of their own particular service or product.
Babson urged entrepreneurs and investors to become familiar with all aspects of the business world,
recommending his own publication as a source of such information. The advent of more rigorous and broad-based
models for forecasting business cycles in the later half of the twentieth lent credence to Babson’s approach. By
the late twentieth century, Babson’s holistic approach to forecasting was well established and influential in the
field.
Among Babson’s more than forty books on economic and social issues were the texbooks Business Barometers
Used in the Accumulation of Money (1909) and Business Barometers Used in the Management of Business and
Investment of Money (1923), both of which went into multiple editions. The use of statistics was only part of
Babson’s approach to business analyses and instruction. His so-called optimistic and pessimistic maps showed
areas in which business opportunities were said to exist and those in which they were lacking. In addition to
books, Babson wrote hundreds of magazine and newspaper articles. He was also a popular lecturer on business
financial trends.
Among his many interests and activities, Babson was one of the world’s leading collectors of Newtoniana. In 1940,
he ran against Franklin D. Roosevelt for president of the United States on the Prohibition Party ticket. In addition
to Babson College, he founded Webber College (now Webber International University), in Babson Park, Florida,
and Utopia College (now defunct) in Eureka, Kansas. Roger Babson died on March 5, 1967, in Lake Wales,
Florida.
Robert N. Stacy
 
See also:  Boom, Economic (1920s);  Frisch, Ragnar;  Great Depression (1929-1933). 
Further Reading
Babson, Roger Ward. Actions and Reactions: An Autobiography of Roger W. Babson. New York: Harper & Brothers, 1935. 
Babson, Roger Ward.  “Ascertaining and Forecasting Business Conditions by the Study of Statistics.” Publications of the
American Statistical Association 13:97 (March 1912): 36–44. 
Barsky, Robert B., and J. Bradford DeLong.  “Bull and Bear Markets in the Twentieth Century.” Journal of Economic
History 50:2 (June 1990): 265–281. 
Friedman, Walter A. The Seer of Wellesley Hills: Roger Babson and the Babson Statistical Organization. Boston: Harvard
Business School, 2008. 
Smith, Earl Leo. Yankee Genius: A Biography of Roger W. Babson. New York, Harper, 1954. 
Balance of Payments
 
The balance of payments (BOP) is a financial statement measuring the monetary transactions (or flows) between
the residents of a specific country and the residents of all other countries. The BOP is determined for a specific
period of time, usually a year, but sometimes monthly or quarterly. A transaction is recorded in a specific country’s
BOP only if one party is a resident and the other is a nonresident: transactions between two residents or two

nonresidents are omitted from the calculation.
The BOP reflects the structure of a country’s sources of foreign financing and its use of these sources. It also
helps economists and policy makers understand how a country’s foreign reserves have changed and how its gross
domestic product has risen or fallen. Because most countries follow the same or almost the same rules for
compiling their BOPs—published in the International Monetary Fund’s Balance of Payments and International
Investment Position Manual (sixth edition, 2008)—it is also possible to compare their balances of payments, to
compare saving and investment behaviors, and, in some cases, to predict changes in their economic policies—
that is, whether they will impose import restrictions, provide export subsidies, or change the value of their
currencies.
The main components of a BOP are the current account (export and import of goods and services, as well as
transfer payments) and the financial, or capital, account (outflow and inflow of financial securities). These, in turn,
are divided into several sub-accounts. The financial account, for example, has four main sub-accounts: (foreign)
direct investment(s), portfolio investment(s), financial derivatives, and other investments. Sometimes reserve
assets are recorded as a fifth sub-account of the financial account, while sometimes they are treated as a
separate account.
The BOP indicates flows (such as how much a country exported or invested abroad in a certain year, quarter, or
month) rather than accumulations (such as the total value of the investments it has received from abroad or the
goods it has imported in all years since its independence). The sum and structure of all of the country’s
international financial liabilities and assets can be found from its international investment position.
Although different currencies are used for foreign transactions, a country prepares its BOP in a single currency,
usually its own. Transactions in other currencies are recalculated based on the exchange rate at the time they
took place. Occasionally, BOP is calculated in other currencies, as this makes it easier for one country to compare
its own BOP to that of another country.
In calculating the BOP, the double-entry system is used. This means that every transaction is recorded with two
entries of equal amounts but with different signs: one with a plus sign (indicating a credit, including exports of
goods and services, inflows on transfers and incomes from abroad, decreases in reserves and other external
assets, and increase in foreign liabilities) and the other with a minus sign (indicating a debit, including imports of
goods and services, outflows on transfers and incomes, increase in external assets, and decrease in foreign
liabilities). As a result, the net balance of all entries should equal zero, and it is not correct to say that a country’s
balance of payments has either a deficit or a surplus. That is, in the aggregate, a deficit in the balance of a goods
and services account must be exactly offset by a surplus in the capital account or vice versa. Hence the BOP
always balances.
At the same time, of course, different accounts and sub-accounts may have different signs. For example, a
country might have a current-account deficit that is covered with a positive financial account (capital account
surplus), and these deficits or surpluses may be stable or may change from year to year. Because several data
sources—state agencies, banks, and/or independent surveys—are used for compiling the BOP, and because it is
difficult to get information about absolutely all transactions that took place during a certain period, the net balance
is not always equal to zero. Thus, an additional account—net errors and omissions—is created for balancing the
other accounts. The sign of this account is always opposite to the sign of the sum of all other accounts.
As is often the case, additional information becomes available after compiling and publishing the initial BOP. Thus,
countries typically publish one or several corrected versions of their BOP during the course of a year. In addition
to such regular adjustments, sometimes—because of significant methodology changes, for example—extraordinary
adjustments are also made. In such cases, countries might revise data from earlier years, being careful to identify
the causes and sizes of the alterations.
Tiia Vissak

 
See also:  Capital Account;  Current Account;  Exchange Rates. 
Further Reading
International Monetary Fund (IMF). Balance of Payments and International Investment Position Manual.  6th ed.
Washington, DC: International Monetary Fund, 2010. 
International Monetary Fund (IMF). Balance of Payments Statistics Yearbook. Washington, DC: International Monetary
Fund, 2008. 
 
Baltic Tigers
 
Comprised of the former Soviet republics of Estonia, Latvia, and Lithuania, the Baltic Tigers represented, for a
time, a success story in the transition from communism to capitalism in Eastern Europe. Liberalizing their
economies from state control led to a flood of foreign capital, which, in turn, produced a consumer and
construction boom. But the boom, which began in the wake of the Russian financial crisis of the late 1990s, was
short-lived, undermined by growing deficits, inflation, and the global financial crisis of the late 2000s. (The term
“Baltic Tigers” comes from the countries’ location—on the Baltic Sea—and the fact that, like the East Asian
“Tigers” of Hong Kong, Singapore, South Korea, and Taiwan, they were relatively small political entities that
experienced rapid economic growth over a short period of time.)
Long dominated by powerful neighbors, the Baltic Tigers achieved a brief period of independence after World War
I, only to find themselves occupied and annexed by the Soviet Union at the end of World War II. Between the mid-
1940s and the end of the 1980s, all three states were transformed into Soviet command-style Marxist economies,
where the state controlled most of the land and means of production. Among the most advanced of the Soviet
republics, the Baltic Tigers experienced rapid industrialization in the years immediately following World War II,
although, as with the rest of the Soviet Union, heavy bureaucracy and a lack of market forces led to increasing
economic stagnation in the 1970s and 1980s.
The political liberalization of the Soviet Union under Premier Mikhail Gorbachev from the mid-1980s encouraged a
growing independence movement in these highly nationalist republics. When they declared themselves
independent in 1990 and 1991, Moscow opposed the decision but did not occupy the countries militarily. Finally,
under pressure from European leaders, the rapidly collapsing Soviet government recognized the independence of
the three countries in September of 1991. Most historians consider the declarations of independence of the three
Baltic countries to have been a major contributing factor in the collapse of the Soviet Union in December 1991.
 Post-Independence Slump and Boom

The decade following independence was a trying time for the three Baltic republics. Their economies remained
tightly bound up with Russia and other former Soviet republics at a time when those economies were experiencing
freefalls in industrial output, gross domestic product (GDP), and per capita income. All three were hit hard by the
Russian financial crisis of 1998, when falling oil and commodity prices led to a rapid devaluation of the Russian
currency, massive inflation and joblessness, and near-bankruptcy for the government in Moscow.
But just as the Russian economy rebounded rapidly from the crisis—largely as a result of rising commodity prices
—so too did the economies of the Baltic republics. From 2000 to 2007, Latvia’s GDP per capita (at current prices)
increased from about $7,700 to $17,000, an increase of more 220 percent; Estonia’s went from $9,900 to $20,200,
a rise of more than 200 percent; and Lithuania’s from $8,400 to $18,900, an increase of 225 percent. These
growth rates were among the highest in Europe. But the countries also had to catch up with the more advanced
European economies: before the boom, their GDP per capita was only about a quarter of the European Union
(EU) average.
Local businessmen line up for loans at the Latvian Investment and Development Agency in Riga in 2006. The
newly independent Baltic states—Latvia, Lithuania, and Estonia—thrived in the transition from communism to
capitalism in the early to mid-2000s. (Ilmars Znotins/Stringer/AFP/Getty Images)
To a large extent, the growth of the Baltic Tigers was based on substantial inflows of foreign capital. Their liberal
economic environments, successful economic reforms, favorable location (near Nordic and other European
markets), relatively low taxes, good infrastructure, cheap but skilled labor, and accession to the EU made them
attractive to foreign investors. Moreover, as large Baltic banks were taken over by Scandinavian banks, the latter
increased the lending capacity of their Baltic subsidiaries, which led to a real-estate and economic boom and
increased wage pressures (for instance, while in 2000 Estonia’s average monthly salary was $289, in 2007 it was
already $991: 3.4 times higher). However, foreign trade deficits began expanding; while both imports and exports
grew in these countries, imports increased more, as did inflation. These problems, combined with the financial
crisis and global recession of the late 2000s, ended the Baltic Tigers’ economic boom by late 2007, early 2008.
 Financial Crisis of 2007–2008
Of the three countries, only Lithuania experienced growth—a modest 3.2 percent—in 2008. That same year,
Estonia’s industrial production fell by 6.5 percent compared to 2007. As the real-estate boom ended, the
manufacture of building materials decreased the most: by 28.1 percent. Overall, enterprises have not met their

income predictions as demand has fallen, unemployment has increased, and an increasing share of borrowers is
causing solvency problems.
On the other hand, the decline has also led to some positive changes for Estonia: while in 2008 its inflation rate
was 7.0 percent, in January 2009 prices decreased by 0.6 percent compared to December 2008, and its foreign
trade deficit has also been decreasing. Moreover, the banks have become more conservative in issuing loans as
their Nordic owners have also cut down their financing.
Despite its economic problems, however, Estonia has continued its relatively liberal economic policy to keep up
investors’ confidence. It has not increased taxes (except the value-added tax), and it has made cuts in the budget
to meet the fiscal requirements necessary for adopting the euro (such as a maximum deficit of no more than 3
percent of GDP), which it achieved at the beginning of 2011.
Latvia fared just as badly. In 2008, the country’s annual inflation rate was 15.4 percent, its unemployment rate
increased to 7.0 percent of the economically active population, and the economy shrank, though to what degree
was disputed by various economists. Meanwhile, Latvia’s currency weakened and the government had to
intervene to keep the exchange rate from falling further. Moreover, the government had to take over—or
nationalize—one of the country’s major banks, Parex Bank, to prevent its going bankrupt. Because of a large
budget deficit, Latvia had to take a $2.4 billion loan not only from the International Monetary Fund (IMF), but also
from the central banks and governments of Sweden, Denmark, and Finland. The country also increased taxes and
cut public sector wages by 25 percent. All of this did little to halt a major slide between 2008 and 2010, as GDP
fell from just over $34 billion in the former year to about $24 billion in the latter.
Like Estonia and Latvia, Lithuania also experienced weakening demand at the end of 2008, although in annual
terms, private consumption still increased by 4.7 percent and government expenditure by 4.3 percent. It struggled
as well with high inflation (11.1 percent in 2008), and less favorable borrowing terms. While in the fourth quarter
of 2008, Lithuania’s economy started falling; for the year overall, it grew by 3.3 percent. The Bank of Lithuania
experienced significant GDP decline in 2009 and 2010, from about $47 billion in 2008 to just over $36 billion in
2010. a slow recovery began in 2011. As expected, domestic demand fell in 2009 and 2010, as did exports and
imports.
Clearly, the economic boom time for the Baltic Tigers was derailed by the global economic crisis of the late
2000s, at least temporarily. It is not clear when the decline will end and the new boom will start. Indeed, for these
three countries, predictions have been changed frequently. Their recovery depends not only on their own actions—
including adoption of the euro (although they do not meet all the requirements yet); making investments in
education, infrastructure, and innovation; increasing productivity; creating high-technology clusters; and increasing
firms’ interest in cooperating—but also on the recovery of the economies of their main trade and investment
partners from the European Union.
Tiia Vissak and James Ciment
 
See also:  Eastern Europe;  Emerging Markets;  Russia and the Soviet Union;  Transition
Economies. 
Further Reading
Andersen, Camilla.  “IMF Helping Counter Crisis Fallout in Emerging Europe.” IMF Survey Magazine, January 14, 2009. 
Bank of Estonia.  “Overview of Recent Economic Developments and the Future Outlook.” December 15 2008. 
Bank of Lithuania.  “Economic Outlook for Lithuania.”
Lucas, Edward.  “The Fall and Rise and Fall Again of the Baltic States.” Foreign Policy, June 22, 2009. 

Bank Cycles
 
Bank cycles are periods of expanding and contracting credit. During expansions, lending standards are loosened,
allowing greater borrowing for investment and consumer spending. During contractions, those standards are
tightened, making it more difficult for businesses to borrow for the purposes of investment, expansion, and hiring
and for households to invest in homes or spend more money on goods and services. The housing boom and bust
from 2003 to 2009 was a vivid demonstration of a bank cycle.
 Role of Credit
In modern capitalist economies, there is a close relationship between money and production. To initiate
production, entrepreneurs must have access to money to pay for the factors of production. Bank credit plays an
important role in financing firms’ investment in capital assets in order to expand their production processes. Thus,
bank credit has a critical impact on the business cycle. Indeed, in the early twentieth century, Austrian school
economist Joseph Schumpeter, among others, emphasized the role of credit creation in financing new techniques
of production, which spur innovation.
Investment decisions require that income-producing activities will be sufficient to generate profits in order to
validate current and future commitments of capital. From the bank’s perspective, a loan represents a claim on a
borrower. It is a promise to receive more money later. It involves time, uncertainty, and expectations that the loan
will be repaid when it is due. If the bank’s customer is successful, the loan will be repaid and the lender will make
a profit. Customers also may pledge collateral to borrow from the bank. In this case, the investment decision is
made based on the quality and the liquidity of the pledged collateral. If the collateral is expected to increase in
value in the near future, this may shift the bank’s preference toward collateralized loans.
However, given the nature of investment and uncertainty about future economic conditions, today’s decisions may
stimulate dynamic processes that lead to financial fragility and instability in the future. Thus, the way in which
capital assets are financed and the structure of the credit system have important impacts on the business cycle.
This view is reflected in the works of economists from varying political perspectives, including Schumpeter, John
Maynard Keynes, and Hyman Minsky.
A bank cycle occurs when increases in the availability of credit and the willingness to borrow boost the demand for
financial assets, thereby pushing up asset prices and encouraging further lending. Optimism about future
economic growth has an impact on the expectations of both lenders, who increase the supply of credit at favorable
terms, and borrowers, who are more willing to take out loans. Credit booms can lead to greater financial fragility if
reliance on bank credit increases the overall level of indebtedness and compromises borrowers’ ability to meet
future interest payments when financial conditions change. In short, rising debt service compromises borrowers’
ability to repay their loans. As businesses and households reduce their expenditures to meet increasing financial
commitments, falling aggregate expenditures translate into falling sales and revenues. Firms cut back on
production, investment, and employment, which depresses incomes and reduces spending even further. Rising
unemployment causes substantial reductions in income, forcing borrowers to default on their loans and, in turn,
leading to massive bank losses, insolvencies, and bankruptcies. As a result, banks reduce the supply of credit,
tighten lending standards, and cut credit limits, causing a credit “crunch.”
Monetary policy also can exacerbate the bank cycle. In this case, changes in the money supply by the Federal
Reserve can lead to changes in short-run economic activity and in the level of prices. For example, tightening

monetary policy leads to a contraction of bank credit and slower economic activity—a policy that usually is
employed to cool an overheated economy. However, an overly strict contraction of the money supply can lead to
bank failures and insolvencies that reduce the supply of credit. Many economists believe that the Great
Depression was made worse by the Federal Reserve’s unnecessary tightening of the money supply during a
period of economic contraction.
 Housing Boom and Bust of 2003–2009
The housing bubble that occurred in the first decade of the twenty-first century and the subsequent recession are
an extreme example of a bank cycle. Much of the expansion in U.S. economic activity was boosted by the
leveraging of households, businesses, and financial institutions. The U.S. economy experienced unsustainable
housing price increases, allowing the household sector to increase spending and purchase property and
speculative assets with borrowed funds.
The meltdown in the global financial markets in 2007–2008 and the severe recession that followed challenged
economists worldwide to find its root causes and to formulate policies that would address its consequences. Over
the past several decades, new nonbank financial institutions—such as monoline mortgage lenders, venture capital
firms, and investment companies—engaged in liquidity creation as an alternative to traditional demand deposits for
short-term investors. These nonbank financial institutions operated outside the regulatory structure of the Federal
Reserve and the Federal Deposit Insurance Corporation, and they relied on short-term funds to finance their
activities, which included buying risky mortgage-backed securities.
The crisis began in late summer 2007 as a liquidity problem, triggering the sale of assets at “fire sale” prices. The
major disruption in credit markets caused instability among traditional banks, as they, too, proved unwilling to lend
to—and borrow from—each other, fearing the troubled assets on one another’s books. The result was massive
insolvency in the global financial system. By 2010, the crisis appeared to be over, but without financial reform that
could prevent another rise and fall of the bank cycle.
Felipe C. Rezende
 
See also:  Banks, Commercial;  Capital Market;  Credit Cycle;  Financial Markets;  Financial
Modeling of the Business Cycle;  Leveraging and Deleveraging, Financial. 
Further Reading
Black, William K.  “‘Control Frauds’ as Financial Super-Predators: How ‘Pathogens’ Make Financial Markets Inefficient.”
Journal of Socio-Economics  34:6 (December 2005): 734–755. 
Fisher, Irving.  “The Debt-Deflation Theory of the Great Depression.” Econometrica 1:4 (October 1993): 337–357. 
Hume, Michael, and Andrew Sentance.  “The Global Credit Boom: Challenges for Macroeconomics and Policy.” Discussion
Paper no. 27, Monetary Policy Committee, Bank of England, June 2009. 
Kregel, Jan A.  “Using Minsky’s Cushions of Safety to Analyze the Crisis in the U.S. Subprime Mortgage Market.”
International Journal of Political Economy 37:1 (Spring 2008): 3–23. 
Minsky, Hyman P. Stabilizing an Unstable Economy. New Haven, CT: Yale University Press, 1986. 
Schumpeter, Joseph A. The Theory of Economic Development: An Inquiry into Profits, Capital, Credit, Interest, and the
Business Cycle. Cambridge, MA: Harvard University Press, 1934. 
Wojnilower, Albert M.  “The Central Role of Credit Crunches in Recent Financial History.” Brooking Papers on Economic
Activity, no. 2(1980): 277–326. 
Wolfson, Martin H. Financial Crisis: Understanding the Postwar U.S. Experience. Armonk, NY: M.E. Sharpe, 1986. 

 
Bank of America
 
One of the largest financial institutions in the world, Bank of America (often referred to as BofA) suffered
significant losses during the financial crisis of 2008 and 2009, largely because of the financial setbacks sustained
by Merrill Lynch, a troubled brokerage house it acquired in September 2008, at the height of the crisis. These
losses, which dramatically reduced BofA’s market capitalization, threatened the very solvency of the institution,
leading the bank to accept tens of billions of dollars in funds from the Troubled Asset Relief Program (TARP), a
2008 U.S. federal program designed to shore up the finances of troubled financial institutions.
Bank of America, with headquarters in Charlotte, North Carolina, became the largest financial services firm in the

world by acquiring the ailing investment bank Merrill Lynch in 2008. But massive losses necessitated billions of
dollars in federal bailouts. (Davis Turner/Stringer/Getty Images)
 Origins and Growth
Founded as the Bank of Italy in San Francisco in 1904 by Italian immigrant Amadeo Giannini, BofA began as
institution aimed at serving the financial needs of working-class immigrants in the Bay Area. Giannini’s rescue of
the bank’s records during the San Francisco earthquake of 1906 helped cement the institution’s reputation for
probity, while the founder’s willingness to lend money to individuals and businesses turned down by other banks
gained it a wide customer base.
In 1928, Giannini merged his bank with the Bank of America of Los Angeles, adopting the latter’s name for the
new institution. The latter had pioneered the business of branch banking, which Giannini expanded throughout the
West. Such interstate banking had only recently been allowed for nationally chartered banks under the McFadden
Act of 1927. Federal banking regulations, however, required BofA to divest itself of its branches outside California
in the 1950s. Around the same time, in 1958, BofA pursued a new avenue of business, introducing the
BankAmericacard (renamed Visa in 1975), one of the first consumer credit cards that allowed customers to make
purchases at different participating businesses. In the 1960s, BofA began licensing other banks to issue the card
and then turned over control of the business to a corporation run by the issuing banks.
In the 1970s and early 1980s, BofA branched out into foreign banking, lending billions of dollars to the
government of Mexico, then flush with oil export revenues. One of the two largest lenders to Mexico, along with
Citibank of New York, BofA was hit hard when oil prices collapsed in the early 1980s and Mexico was unable to
service its massive foreign debt and went into default. Accompanied by bad loans to other developing world
countries, the Mexican default forced BofA to sell a number of its operations to raise capital. By the time of the
stock market crash of October 1987, BofA stock had plummeted in value and the institution had lost its position as
the nation’s largest bank by asset holdings.
Under new management, however, BofA took advantage of regulatory changes in the 1980s that once again
permitted interstate banking, acquiring a number of banks in other states and opening branches across the
country. After suffering large losses in the Russian default of 1998, BofA was acquired by NationsBank of North
Carolina, then the nation’s biggest commercial bank, in what was then the largest banking acquisition in history.
While its headquarters was moved to Charlotte, North Carolina, the new institution retained the name Bank of
America. With the merger, BofA once again emerged as the largest bank in the United States, with assets of $570
billion and nearly 5,000 branches in more than twenty states coast to coast.
 Countrywide Acquisition
Through the early 2000s, Bank of America continued to acquire other banks. In 2008, it purchased Countrywide
Financial, a California-based mortgage lender with a reputation for aggressively marketing subprime mortgages
and home equity loans to borrowers with less than sterling credit histories. The early and middle 2000s had seen
a boom in the mortgage industry, fueled by rising home values and the Federal Reserve Bank’s low interest-rate
policies. Countrywide had ridden that boom to become America’s largest home mortgage lender, financing roughly
one in five U.S. home mortgages and creating securitized mortgage instruments that it sold to investors.
By 2007, however, Countrywide was in trouble. Many of both the conforming and subprime mortgages it financed
were adjustable, meaning that an initial interest-only payment—which offered borrowers low monthly payments,
allowing many to purchase homes beyond their means—was adjusted upward after a period of time, increasing
the monthly payment beyond the borrower’s capacity to meet it. That was not a problem as long as credit was
easy to obtain and rising home equity allowed borrowers to refinance. But when home prices began drop in 2007,
many of those borrowers went into default. The rising default rates, in turn, led to tighter credit by lending

institutions, both to mortgage holders and other financial institutions.
By the time BofA got regulatory approval to purchase Countrywide in mid-2008, rumors began to swirl that the
latter was on the verge of bankruptcy—rumors that both BofA and Countrywide’s management vigorously denied.
True or not, access to BofA’s vast assets shored up Countrywide’s finances, and the division was renamed Bank
of America Home Loans as a way to disassociate it from Countrywide’s now besmirched reputation.
 Merrill Lynch Acquisition and the Financial Crisis
Despite the troubles at Countrywide and the collapse of the credit markets in the late summer of 2008, BofA
appeared to be in relatively sound financial shape, even as other major commercial and investment banks began
to fail. In September, BofA took advantage of the crisis to acquire Merrill Lynch, a financial services firm best
known for having pioneered the branch stock brokerage business. Highly exposed to collapsing securitized
mortgage financial instruments, and under threat of government lawsuit for misrepresenting such securities to
investors, Merrill Lynch had seen its stock price plummet to the point that the firm was on the verge of bankruptcy.
Paying a fraction of Merrill Lynch’s once sky-high share price in an all-stock deal, BofA was seen to have
purchased the world’s largest financial services company at a bargain price. (Even at that reduced share price, the
$50 billion price tag represented more than 80 percent of Bank of America’s own falling stock valuation in
September 2008.)
The acquisition proved disastrous for BofA, as it soon became clear that Merrill Lynch was in a far worse financial
shape than originally understood by BofA management, posting more than $20 billion in losses in the fourth
quarter of 2008 alone. Dragged down by the acquisition of Merrill Lynch and Countrywide, as well as its own
plummeting stock price, BofA was in serious financial trouble by the late 2008. It therefore accepted $25 billion
from TARP, with an additional bailout of $20 billion following in January 2009. Another $118 billion guarantee
against potential losses on toxic assets, such as securitized mortgage instruments, was also offered by TARP.
Whether or not the bailout funds actually rescued the giant bank from insolvency, as some financial experts
claimed, BofA survived. In early 2009, however, the Barack Obama administration and Congress included BofA
among the many large financial institutions that it charged with using the bailout money to shore up its assets
rather than to ease credit, as the TARP money was originally intended. For his part, CEO Ken Lewis maintained
in sworn testimony before Congress in April 2009 that the government was at least partly responsibly for the crisis
at BofA. Having learned of Merrill Lynch’s true financial predicament, Lewis testified, he wanted to back out of the
acquisitions deal but was coerced into completing it by former Secretary of the Treasury Henry Paulson, who
feared what the failure of Merrill Lynch would do to the global financial system. In his own sworn testimony,
Paulson denied the accusation. In 2011, Bank of America was hit by new allegations, this time of misleading
Fannie Mae and Freddie Mac, the government-sponsored mortgage-guaranteeing entities, about the quality of the
home loans it had sold to them. The bank was also caught up in the industrywide robo-signing scandal, in which
lending institutions fast-tracked foreclosures on home mortgages without proper review.
Still, by the second quarter of 2009, BofA was back in the black, posting profits of $3.2 billion. This allowed Lewis
to assert that the bank would shortly be able to begin redeeming, in installments, the preferred stock the
government obtained in exchange for the TARP money it lent, which he had done by 2009. But this also led to
complaints from regulators that the bank had paid back the loans prematurely, in order to remove government
oversight, failing to keep sufficient reserves for an expected wave of new home loan defaults. All of this
undermined the bank’s profibility and, by late 2011, it had seen its share price drop by more than 40 percent from
the year before.
James Ciment and Andrew J. Waskey
 
See also:  Banks, Commercial;  Merrill Lynch;  Troubled Asset Relief Program (2008-). 

Further Reading
Bonadio, Felice A. A.P. Giannini: Banker of America. Berkeley: University of California Press, 1994. 
Johnson, Moira. Roller Coaster: The Bank of America and the Future of American Banking. New York: Ticknor &
Fields, 1990. 
Nash, Gerald. A.P. Giannini and the Bank of America. Norman: University of Oklahoma Press, 1992. 
Banking School/Currency School Debate
 
Arising in opposition to the British Banking Act of 1844, which granted the Bank of England (the nation’s central
bank) a monopoly on the issue of banknotes (paper money), the banking school was a group of economists who
held that the Bank of England should not be required to back its notes with full gold parity, a stipulation of the
1844 law. In other words, adherents of the banking school believed that the notes issued by the bank should not
be fully convertible into gold upon demand of the bearer. Opposing them were members of the currency school,
who argued in favor of full gold parity.
The source of the controversy lay in the disruptive, often devastating swings of the business cycle that marked
British and, indeed, transatlantic economic history in the nineteenth century. Members of the currency school
maintained that the excess issuance of banknotes (by many lending institutions before 1844) were the cause of
both inflation and the speculative excesses that triggered financial bubbles and busts. Members of the banking
school argued that speculative excess operated independently of banknote issues and that strict gold parity would
constrain the money supply and hence economic activity. Ultimately, the currency school won the debate, as
Britain maintained gold parity through World War I.
The foundation for the banking school’s argument lay in the so-called real bills doctrine, first enunciated by
Scottish economist John Law in the early eighteenth century and later elaborated by fellow Scotsmen James
Steuart and Adam Smith. According to this doctrine, banks should be allowed to issue notes at will. Proponents
maintained that the money supply was not the result of exogenous forces (those external to the economy, such as
the arbitrary fiat of bank directors) but endogenous ones (according to the actual needs of business). In other
words, banknotes could not produce inflation since they were issued and accepted only as people needed them,
and people needed them because the economy was growing. Even if a bank were to accidentally issue excess
notes, the argument went on, they would not produce speculation or inflation. Instead, the excess notes would be
returned to the banks as merchants and other holders no longer needed them, a process known as the “reflux
principle.”
Currency school advocates—which included English economists Henry Thornton, John Wheatley, and David
Ricardo—maintained that without the requirement of gold parity, banks would be tempted into issuing too many
notes, since doing so would inflate their institution’s profits. Members of the currency school also challenged the
real bills doctrine, pointing out that merchants would have an incentive to request more and more banknotes if the
rate of return on those banknotes exceeded the interest rate the bank charged on them. In other words, the self-
correcting mechanism of the real bills doctrine had a loophole that could lead to a situation where banknotes
flooded the economy, producing speculation and inflation.
Such arguments weighed heavily in the decisions of British policy makers, who, through the eighteenth century,
maintained laws that required banks to keep enough gold in their vaults to redeem all notes. (Scotland had a

partial exemption to this rule until 1765, which explains why so many of the economists who opposed full gold
parity hailed from there.) In 1797, rumors of a French invasion of Britain led to a run on banks, as depositors
sought to convert their notes into gold, leading the government to suspend full convertibility.
As Britain gained the upper hand militarily, the fear passed and the economic crisis ebbed. Nevertheless, the
government continued to suspend full convertibility through the end of the Napoleonic Wars in 1815. By then,
however, a major debate had emerged over the convertibility issue, a debate known as the “Bullionist
Controversy.” The Bullionists—arguing for full gold convertibility—pointed to the inflation of the early 1800s as
vindication of their position; Anti-Bullionists insisted that the inflation was due to government war purchases, which
caused price-hiking shortages. The end of the war offered support for the latter view, as deflation kicked in despite
the fact that full convertibility remained suspended.
Nevertheless, the political forces arrayed behind the Bullionists won the day, gaining passage in 1819 of the
Resumption Act, which returned British banks to full gold convertibility in 1821. For the next two decades, the
debate remained largely confined to economists—until passage of the 1844 Banking Act. While the measure did
not require full convertibility by the Bank of England, it did require a specific ratio of gold to banknotes issued.
Supporters of the currency and free banking schools maintained, on various grounds, that such a ratio was
necessary to avoid excess note issues that would result in inflation, speculation, and swings in the business cycle.
Meanwhile, members of the banking school—including economists Thomas Tooke, John Fullarton, and a young
John Stuart Mill—had modified the arguments of the old Anti-Bullionists. They did not fully accept the real bills
doctrine and argued that a degree of convertibility was probably a good thing. They also held to a new reflux
principle, which stated that even if an excess issue of banknotes produced inflation, the inflation would not last.
Note holders, they reasoned, would rush to redeem them in gold, thereby contracting the money supply and
easing inflationary pressure.
Ironically, various economic crises in the mid-nineteenth century forced the government to suspend the
convertibility clauses of the Banking Act, which seemed to have lent credence to the banking school’s position. By
this time, however, adherence to the principles of the gold standard were too strong to overcome and Britain
maintained convertibility until World War I. Thereafter, huge external debts, largely to the United States, so
undermined the value of the pound that convertibility to gold was no longer tenable. As for the banking school
itself, the ideas it promoted helped lay the foundations for modern monetary economics and policy in Britain and
much of the rest of the industrialized world.
James Ciment
 
See also:  Banks, Central;  Smith, Adam. 
Further Reading
Hixson, William F. Triumphs of the Bankers: Money and Banking in the Eighteenth and Nineteenth Centuries. Westport,
CT: Praeger, 1993. 
White, Lawrence H. Free Banking in Britain: Theory, Experience, and Debate, 1800–1845. Cambridge, UK: Cambridge
University Press, 1984. 

 
Banks, Central
 
Central banks, which exist in most countries today, are institutions granted the exclusive right to create legal
tender by purchasing government securities, giving them control over the nation’s currency. Central banks can loan
funds (reserves) to commercial banks, serving as a “lender of last resort.” Like ordinary banks, central banks
charge interest on the loans they make, both to the government and to commercial banks.
The earliest institution that could be likened to a central bank was Sweden’s Sveriges Riksbank, established in the
mid-seventeenth century. The first reference to the term “central bank” dates to Great Britain in 1873, when Walter
Bagehot, editor of the Economist, used the term in reference to the Bank of England’s monopoly on the issue of
banknotes. The U.S. central bank, the Federal Reserve, or Fed—actually a system of twelve regional banks, with
the New York Federal Reserve Bank serving as first among equals—was established in 1913. The Fed and most
other central banks enjoy a great amount of political autonomy. Although directors are usually appointed by
elected government leaders, they typically have long tenures and experience little oversight by the head of state or
legislature. Such autonomy is essential to the role and purpose of the central bank, ensuring that politicians do not
exert pressure to accelerate economic growth—especially prior to elections—in ways that might damage the
overall long-term health of a nation’s economy.
 Functions and Goals
The most important function of a central bank is to set a country’s—or, in the case of the European Central Bank,
a region’s—monetary policy. In establishing and executing monetary policy, a central bank pursues three basic
goals: price stability (maintaining a low rate of inflation and avoiding deflation); financial stability (preventing
financial crises and ensuring the smooth operation of the credit system); and a strong real economy (maintaining
low unemployment and steady economic growth).
The most important means at a central bank’s disposal for achieving these ends is referred to as “open market
operations,” or the buying and selling of government securities and other financial instruments to other institutions
and individuals. By purchasing government securities, central banks inflate the money supply, a step usually
reserved for periods of slow or negative economic growth that are accompanied by low inflation or, more rarely,
deflation. By buying securities, the central bank effectively lowers the interest rates on government securities,
which reduces the cost of borrowing on other loans as well.
By making it cheaper to borrow, the purchase of government securities spurs business investment, hiring,
consumer purchasing, and, as a result, overall economic growth. Conversely, by selling government securities, a
central bank shrinks the money supply—or, more typically, slows its growth. This tends to slow economic growth
because it raises interest rates, making it more expensive for businesses to borrow for capital improvements and
hiring and for consumers to borrow money to purchase a home, car, or other goods and services typically paid for
with credit. Central banks typically sell government securities during periods of high inflation or when they suspect
inflation is looming because of unsustainable real-economy growth rates.
Other responsibilities of central banks include setting the discount, or interest, rate they charge to commercial

banks to borrow reserve assets. Most central banks also set the required reserve ratio, which will translate to the
amount of reserve assets that banks must keep on hand versus the amount of funds they lend. The goal in
making this determination is twofold. By increasing reserves, the central bank makes it more difficult for
commercial banks to lend, thereby slowing economic activity. In addition, reserve requirements are designed to
ensure stability in the financial markets, so that banks do not overlend and expose themselves to too much risk.
A central bank can also be charged with issuing a country’s banknotes, managing foreign exchange rates, and
regulating commercial banks. Banking regulations include measures to protect customer deposits, maintain the
stability of the financial system, and prevent the involvement of banks in criminal activities. Given that the failure of
even a single major banking institution can trigger a larger-scale financial crisis, financial regulatory measures
seek to ensure that no bank takes on too much risk. They are required to maintain a specified level of cash
reserves against customers’ deposits and to make provisions against prospective losses. Regulators attempt to
make banking operations transparent by setting financial reporting and disclosure requirements.
Finally, central banks often act as lenders of last resort to troubled commercial banks, a role often criticized by
economists and others for contributing to excessive risk taking. If distressed commercial banks are rescued by a
lender of last resort, it is argued, they may be encouraged to continue lending and investing irresponsibly.
Supporters of this central bank function, while they tend to acknowledge the danger, say that the importance of
avoiding the collapse of a major commercial bank outweighs the risk of encouraging bad lending practices.
 Financial Crisis of 2008–2009
The unprecedented scope of the global financial crisis of 2008–2009 led to an expanded role for central banks in
a number of countries. In the United States, for instance, the Fed facilitated merger deals whereby failing
investment banks would be purchased by solvent institutions. It provided hundreds of billions of dollars in bailout
money to troubled financial institutions, with no guarantees the money would be repaid, taking on equity stakes in
those institutions. Indeed, it extended its reach beyond the financial industry per se by rescuing a major insurance
company, AIG, with some $150 billion in federal money and taking a majority stake in the corporation. Under the
Troubled Asset Relief Program (TARP) of 2008, the Fed and the U.S. Treasury provided hundreds of billions of
additional dollars to financial institutions, either in the form of loans or in exchange for equity stakes. The Fed also
created numerous special lending facilities that lent to banks, primary dealers, money market mutual funds, and
commercial paper dealers. The assets the Fed created by the loans more than doubled the assets on the Fed’s
balance sheet in an unprecedented expansion, invoking special powers for crises under the Federal Reserve Act
of 1913. It remains to be seen how these positions will be unwound as the financial system stabilizes and the
need for the emergency facilities lessens.

Central bank governors of the G7 nations—Canada, France, Germany, the United States, Italy, Japan, and the
UK—and officials of the International Monetary Fund meet in Washington, D.C., during the global financial crisis in
October 2008. (Getty Images)
Fed chairman Ben Bernanke defended the expanded role of the Fed by pointing out the dangers of doing nothing
in the face of the greatest financial crisis since the Great Depression. Failure to prop up troubled financial
institutions, he maintained, could have resulted in a compete freeze in the international credit markets, which in
turn could have halted the short-term lending that keeps banks afloat. The result would have been a wave of bank
failures, business bankruptcies, and mass layoffs. In short, Bernanke and other defenders of the expanded Fed
role argued, the financial crisis of 2008–2009 had the potential to plunge the United States and the global
economies into another Great Depression.
At the same time, the expanded role of the Fed and other central banks led to concerns in the United States and
elsewhere that these politically autonomous institutions, which often operate in secrecy, were gaining too much
power and influence. In the United States, there was talk of expanding Congress’s auditing authority over the Fed,
an idea opposed by Bernanke and criticized by many economists. The Barack Obama administration, for its part,
was moving in the opposite direction, arguing that the Fed should be given the right to regulate investment banks
and other major financial institutions outside its traditional purview, though its powers to write and enforce
consumer financial protection rules would be shifted to a new agency. The financial industry reform act introduced
by Senator Christopher Dodd (D-CT) in March 2010 gave the Fed expanded powers to regulate banks and other
financial institutions.
James Ciment and Lyudmila Lugovskaya
 
See also:  European Central Bank;  Federal Reserve System;  Monetary Policy. 
Further Reading
Davies, H.  “What Future for Central Banks?” World Economics 7:4 (2006): 57–85. 
Epstein, G.  “Central Banks as Agents of Economic Development.” In Institutional Change and Economic Development, ed.

H.J. Chang. New York: United Nations University Press, 2007. 
Banks, Commercial
 
Commercial banks are financial institutions—usually chartered by governments and highly regulated—that function
as intermediaries between depositors and borrowers, taking in money from the former and lending it out to the
latter. Commercial banks offer depositors a number of advantages, including safekeeping of their money, interest
on their deposits, and, in the case of checking accounts, the convenience of being able to pay for purchases
through checks or debit cards rather than carrying around large amounts of cash. Banks pool the money of
depositors and then lend it out to businesses and individuals for interest. Commercial banks make their money in
two basic ways: the difference between the rate they charge borrowers and the interest rate they pay depositors—
known as the “spread”—and fees they collect for any number of financial services, such as checking accounts,
overdraft protection, credit cards, automatic teller machine (ATM) fees, and the like.
Almost as old as trade itself, going back thousands of years, commercial banks play several critical roles in
modern economies. As major lenders to businesses, they allocate capital to various sectors of the economy, and
through the loans they offer individuals, they play a critical role in financing consumer spending, especially for
purchases of homes, cars, college education, and other costly goods and services. In these various ways, banks
facilitate economic activity and, hence, economic growth.
At the same time, however, commercial banks can also serve to destabilize economies. By their very nature, they
are highly leveraged institutions, maintaining a small base of liquid assets to meet depositor demands for funds. At
any given time, the bulk of a bank’s deposits have been loaned out to businesses and individuals, making them
unavailable to depositors. Backstopping banks, should there be a loss of confidence that leads to sudden mass
demand by depositors for their money, are a web of short-term interbank loans and, as a last resort in the United
States, the Federal Reserve (Fed), which lends funds to banks. The fact that most banks have deposit insurance
offered by the Federal Deposit Insurance Corporation (FDIC), a federal agency that guarantees depositor
accounts up to a given amount, makes bank runs much more unlikely than before the creation of the FDIC. Many
other countries have similar agencies.
To avoid having to put these protections into action, commercial banks are heavily regulated, either by state
agencies in the case of most state-chartered banks, or, in the case of national banks, by the Fed. These
government agencies require banks to maintain a certain ratio of assets-on-hand to loans. The Fed, America’s
version of a central bank, sets a range of 3 to 10 percent reserves against loans, a figure that varies according to
the institution. Reserves can be held either by the bank as vault cash or by a reserve deposit account at the Fed
itself.
Commercial banks are only one of several types of financial institutions that provide similar services; others
include credit unions (a kind of cooperative bank); savings and loans; and mutual savings banks.
 Financial Products and Risks
Commercial banks typically offer a number of financial products for both depositors and borrowers. For the former,
there are checking accounts, which typically pay little or no interest but give depositors access to their funds to
make purchases; savings accounts, which offer relatively low interest rates but allow depositors to withdraw funds
at any time; and certificates of deposit, which offer higher interest rates but require depositors to maintain

principals for a fixed period of time. In general, the more liquid the deposit, the lower the interest rate the account
provides.
Borrowers can obtain financing from commercial banks in a number of ways, with less secured and riskier lending
coming with a higher interest rate. Secured loans require borrowers to use some of their own assets as collateral;
unsecured loans, usually in the form of purchases or advances made on credit cards, do not require collateral but
usually come with significantly higher interest rates to reflect the greater risk on the part of the bank. Mortgage
loans are a subset of secured loans, with the home or commercial property itself acting as collateral.
For businesses, there are syndicated loans in which a group of commercial banks provides credit to a borrower,
thereby spreading costs and risks. Project financing is a more ongoing type of partnership with the borrower, in
which the bank commits itself to financing a project over the long term, providing credit as necessary.
Many commercial banks also offer trade financing, where the institution acts as an agent in international trade by
making use of a letter of credit. Because the parties who are involved in trade do not know and trust each other,
the seller hesitates to send the goods without some guarantee that payment will be made. In this case,
commercial banks provide the guarantee and charge a fee.
Some activities of commercial banks generate income but do not require the bank to put its own capital at risk.
These are called noncredit services and typically entail some fee or commission. Among such services are
keeping financial documents, financial securities, and other items in safe deposit boxes; providing financial advice
and cash management to bank customers; acting as clearing agents using various payments and clearing systems
around the world through electronic transfers; electronic funds transfer at point of sale (EFTPOS); ATM access;
and currency exchange transactions.
In taking depositor money or lending out their own assets, banks assume a variety of risks. These include credit
risk, the danger that a borrower cannot repay a debt; liquidity risk, or not keeping enough cash and reserves to
meet depositor demands or cover nonperforming loans; interest rate risk, caused by a mismatch in the maturity
and volume of interest-sensitive assets (loans) and interest-sensitive liabilities (deposits); market or price risks,
caused by price fluctuations in capital markets; foreign exchange or currency risks, caused by fluctuations in
currency valuations; sovereign risk, triggered by political or economic conditions in a particular country; operating
risks, involving fraud, unexpected expenses, or technological failures; and settlement payment risk, in which a
borrower fails to meet the terms of a contract.
 History of Banking
Although banks have existed since ancient times to facilitate trade and guard people’s assets, limited trade and
Christian bans on interest during the Middle Ages prevented the establishment of banks in Europe until the
fourteenth century. Early European banks—first in Italy, then spreading to northern parts of the continent by the
sixteenth and seventeenth centuries—offered a number of services, including bills of exchange that allowed
traders from one city to obtain money in another, a form of early checking services. Many banks also issued
notes, which acted as currency in a given locale, though by the nineteenth century this operation had largely been
assumed by governments, usually through central banks.
In the United States, many of the earliest banks were operated by goldsmiths, who accepted precious metal from
their clients and issued receipts against the deposit. The receipts were then used as currency, even as some
goldsmiths began to offer credit against the gold deposits in their safes. By the mid-eighteenth century, land
banks had emerged, issuing notes against land deeds deposited with them. Such institutions issued currency
against the land deposits and offered loans, though most individuals and businesses turned to merchants or
private lenders for credit.
By the early years of the republic, the demand for credit and capital—as well as the growing complexity of
business—had created a greater need for commercial banks. They soon sprouted up across the country, issuing

banknotes, holding depositor money, and making loans. Until the 1830s, the Bank of the United States, an early
central bank, tried to impose rules on how much assets a bank should hold against the notes it was issuing and
the loans it was offering. But such regulations were usually ignored, and banks often participated—indeed, were
often the prime actors—in the varied real-estate bubbles of the period, most of which led to crashes and financial
panics.
By the late nineteenth century, the U.S. government had become the prime issuer of currency, leaving banks in
the business of making money by taking in depositor assets at a given interest rate and making loans at a higher
one. This was also the period in which the modern checking account system emerged. With the advent of the
Federal Reserve System in 1913, U.S. commercial banks came under tighter regulation by the federal
government. Despite such oversight, many lending institutions developed new corporate structures that allowed
them to engage in much riskier investment banking, especially as rules against such activities were lifted during
the probusiness political climate of the 1920s.
But by engaging in securities underwriting and other investment activities, commercial banks were putting their
depositors’ assets at greater risk. This proved disastrous to both the banks and their customers in the wake of the
stock market crash of 1929. The wave of financial bankruptcies that followed prompted the federal government to
pass the Glass-Steagall Act of 1933, which barred commercial banks from engaging in such businesses as
investment banking and insurance, both of which carried greater risks.
From 1933 through the 1970s, U.S. commercial banking returned to its roots of offering customers basic financial
services and interest on their deposits, and then lending the money, usually to businesses. Savings and loans
were left to handle most consumer business, such as mortgage lending. Banks became highly regulated and
stable businesses, offering solid if not spectacular returns to shareholders. By the 1970s, however, the industry
entered a period of crisis. With inflation high and other financial institutions offering higher rates of return on
money market accounts and certificates of deposit, banks began to lose money; regulations either prevented them
from conducting such business or limited the interest rates they could charge on loans and credit cards, even as
the money they borrowed from the Fed became more expensive.
 Deregulation, Mergers, and New Services
The commercial banking industry responded to the crisis in three ways during the 1980s and 1990s. First, it
lobbied the federal government for regulatory relief, allowing it to charge more interest, offer a variety of financial
instruments, and expand across state lines. Meanwhile, the savings and loan industry was granted the freedom to
make investments in more speculative real-estate ventures and other risky assets including junk bonds, a
departure from its traditional role of providing financing for home purchases. Second, banks began to merge with
one another, creating much larger institutions that did business regionally and even nationally. Third, the industry
began to offer more services, taking advantage of new technologies to set up ATMs, Internet banking, and debit
cards.
Banks also began to lower their credit qualification standards, allowing them to expand their customer base. The
number of credit cards increased dramatically, as did the interest payments on those cards, providing banks with
vast new revenue streams. Along with credit cards, banks provided more and more overdraft protection on
checking and debit card accounts, allowing customers to withdraw more money than they had in their accounts
and charging them hefty fees for the service. By offering more credit, of course, banks were also increasing their
exposure to defaults and bankruptcy. But the banking industry also lobbied for stricter rules on the latter, winning
passage of a new bankruptcy act in 2005 that made it more difficult for consumers to liquidate their debts.
Six years before, the commercial banking industry had won another major political victory, overturning the section
of the Glass-Steagall Act that prevented it from engaging in insurance and investment banking activities. Thus, by
the mid-2000s, huge new bank holding companies, such as Citigroup, had subsidiaries that provided everything
from brokerage services to savings accounts to insurance. Banking thereby became a far more lucrative but far
riskier business—as the financial crisis of 2008–2009 made clear.

 Financial Crisis of 2008–2009
The crisis began in the housing sector. After the greatest run-up in home prices in modern U.S. history, valuations
began to slide beginning in late 2006 and early 2007. With many homeowners subject to adjustable rates,
foreclosure rates began to climb, hurting the balance sheets of many commercial banks. Moreover, many of the
mortgages had been bundled into securities that were purchased by the investment-banking arm of many
commercial banks, adding to their exposure. By September 2008, the entire global financial sector was in crisis.
Interbank loans, essential to the functioning of international credit markets, began to freeze up, as confidence in
the solvency of U.S. banks began to sag. Various investment banks, which were especially exposed to the
mortgage-backed securities, teetered on the edge of collapse, culminating in the September 15 bankruptcy filing of
Lehman Brothers. On September 25, the U.S. Office of Thrift Supervision seized Washington Mutual Bank from its
holding company after a ten-day bank run by depositors, placing it in receivership. It was the largest bank failure
in U.S. history.
To prevent further collapses, the George W. Bush Administration and the Fed pushed for a massive bailout
package, which ultimately amounted to some $700 billion. In addition, a large chunk of the money, about $150
billion, went to insurance giant AIG, whose business included collateral debt obligations, a kind of insurance policy
on other instruments, including mortgage-backed securities. Tens of billions of dollars were doled out to some of
the biggest bank holding companies in the country, including Bank of America and Citigroup. The Fed also
established numerous special lending facilities that increased lending to banks, primary dealers, money market
mutual funds, and commercial paper lenders. Such lending injected more than a trillion dollars into the economy to
prop up the financial system.
Although it was politically unpopular, the bailout was a financial success. The feared wave of bank failures was
averted, and the credit markets began to unfreeze, though lending became far more restricted than it had been
before the crisis. Still, some economists argued that the money would have been better spent on helping
homeowners who faced foreclosure; that was, after all, the underlying source of the crisis in the first place.
Nevertheless, by mid-2009, most commercial banks in the United States were no longer facing imminent collapse.
Some even saw a return to profitability, a recovery reflected in the rapid rise of their share prices from the deep
troughs into which they had fallen in 2008.
In the aftermath of the crisis, the new Barack Obama administration and the Democratic majority in Congress
began talking about new regulations on the financial industry, including new forms of executive compensation and
a new agency to regulate the consumer financial industry, so as to prohibit excessive fees and rein in the loose
credit that had contributed to the crisis. Yet few people were talking about a return to the highly regulated days of
the 1970s, and there was little evidence that Congress would pass laws preventing commercial banks from
engaging in investment banking, insurance, and brokerage businesses.
Still, according to many economists, the commercial banking industry was not out of the woods yet. There were
fears of a second crisis hitting the industry as consumers, many of whom were laid off in the deep recession that
accompanied the first mortgage-related financial crisis, defaulted on record high levels of unsecured—largely
credit-card—debt. This, said some of the gloomier prognosticators, could lead to a second wave of bank failures
and yet another government bailout of the financial industry.
James Ciment and Asli Yuksel Mermod
 
See also:  Bank of America;  Bank Cycles;  Capital One;  Citigroup;  Depository Institutions; 
Federal Deposit Insurance Corporation;  IndyMac Bancorp;  Northern Rock;  Panics and Runs,
Bank;  PNC Financial Services;  Regulation, Financial;  Troubled Asset Relief Program (2008-); 
UBS;  Washington Mutual. 

Further Reading
Barth, James R. The Rise and Fall of the U.S. Mortgage and Credit Markets: A Comprehensive Analysis of the Market
Meltdown. Hoboken, NJ: John Wiley and Sons, 2009. 
Cecchetti, Stephen G. Money, Banking, and Financial Markets. Boston: McGraw-Hill/Irwin, 2006. 
Hempel, George H., and Donald G. Simonson. Bank Management Text and Cases.  5th ed. Hoboken, NJ: John Wiley and
Sons, 1999. 
Hughes, Jane E., and Scott B. MacDonald. International Banking: Text and Cases. Boston: Addison Wesley, 2002. 
Krugman, Paul. The Return of Depression Economics and the Crisis of 2008. New York: W.W. Norton, 2009. 
Posner, Richard A. A Failure of Capitalism: The Crisis of ’08 and the Descent into Depression. Cambridge, MA: Harvard
University Press, 2009. 
Rose, Peter S. and Sylvia C. Hudgins. Bank Management and Financial Services.  7th ed. New York: McGraw-
Hill/Irwin, 2008. 
Banks, Investment
 
Investment banks offer services that deal principally with the restructuring of business ownership, including
securities underwriting through initial public offerings (IPOs) and seasoned offerings, and by arranging mergers and
acquisitions, securitization and securities trading, private placements, and leveraged buyouts. Investment banks
typically work for companies and governments, not individuals, and their profits come from fee income for their
securities services, including underwriting, and from fees as strategic consultants. In the United States—which
from the Great Depression until the late 1990s separated the functions of investment banks and commercial banks
—an investment banker acting in an advisory capacity must be licensed as a broker-dealer and performs his or
her job under the oversight of the Securities and Exchange Commission (SEC).
 IPOs
Businesses over a certain size generally benefit from establishing themselves as corporations, while most small
businesses begin as partnerships or proprietorships. The bulk of a corporation’s stock is usually owned by the
company’s founders and loyal employees. However, revenue streams from ordinary day-to-day business activity
often are not strong enough to meet demand, and offering stock to outsiders expands the company’s funding and
reinvestment capacity.
Private placements provide the first source of outside funding. Originally, the initial stock issues in a private
placement could not be resold by their purchasers for a two-year period and were exempt from the costly SEC
registration process. In 1990, the SEC adopted Rule 144A, which allowed large institutions to trade privately
placed securities among themselves with no two-year waiting period. This new rule greatly increased the
participation of investment bankers in the private placement market, which bypassed the registration process.
There is no limit on the number of accredited private placement purchasers—institutional investors such as banks
and pension funds, wealthy individuals, and the officers of the company—but there can be no more than thirty-five
unaccredited purchasers. Private placements are also done by larger firms for the issuance of new securities
where they want to bypass the costly registration process.

After private placements, it is common for investment banks to court a venture capital fund—a limited partnership
that pools the money of its investors (usually institutional investors) and is managed by a venture capitalist who is
an expert in a specific sector of business and can gauge which companies are sound investments for his or her
fund.
After the venture capital stage, an IPO of stock may follow. Unlike private placements, this stock can be traded on
the secondary markets and is registered with the SEC. This is generally the primary activity of investment banks:
preparing companies to go public. The bank helps determine the stock’s initial offering price by comparing the
performance and growth prospects of the company to those of similar corporations, acts as an intermediary in
selling some of the stock to its clients, and maintains investor interest in the stock after the IPO by producing
reports on its prospects.
Investment banks that underwrite IPOs purchase all of the stock being offered and resell it on their own, which
guarantees that the company raises the money it needs. This is the typical scenario. In some cases, a bank will
refuse to make a guarantee and will only offer a best-efforts sale—selling the stock on consignment, as it were,
without putting its own money up front. Underwriting is a risk for the bank. If it misjudges the market price for the
IPO, it can be stuck with unsold stock.
All IPOs over $1.5 million are regulated by the SEC, and state agencies generally have regulations that must be
met as well. The stock is registered with the SEC twenty days before the IPO, at which point the investment bank
sends representatives on what the industry calls “the road show” to present information and sales pitches to
prospective investors in order to promote the company and its stock offering. This not only gets the word out, but
it also gives the bank a sense of what investors will be willing to pay and how many investors are likely to be
interested. The road show cannot include any data that appear in the SEC filing, which prevents spurious claims
and unlikely projections. Thus, the road show generally consists of marketing spin and selective emphasis. If there
is too little interest in the IPO stock, the SEC registration can be withdrawn.
 Mergers, Acquisitions, and Leveraged Buyouts
Investment banks also consult with businesses involved in mergers and acquisitions. In both kinds of transactions,
one company is created where previously there were two or more. A merger is regarded as an equal arrangement
—two companies merge into a new entity, preserving the rights of the original management of both. In an
acquisition, one company absorbs or assumes control of the other. In practice, many arrangements that are really
acquisitions are called mergers as a condition of the deal; this saves face for the management of the absorbed
company.
Mergers and acquisitions are overseen by the antitrust division of the U.S. Justice Department to ensure that the
new entity does not constitute a monopoly. Stockholders, for their part, may have a variety of responses to a
proposed merger or acquisition. Sometimes the deal strengthens a company unequivocally, as when a large,
healthy company buys out small regional competitors to increase its market share or expand into new markets.
Other times, as when a shoe company attempts to buy a motion picture studio, investors fear that the company
has become too diversified and no longer dominant in its field. Investors like to have a clear sense of what they
are investing in—shoe stock or movie stock—and typically shy away from shares in a company without a clear
identity.
Ideally, a merger or acquisition creates a new entity that is greater than the sum of its parts, thanks to an increase
in efficiency and market power, tax benefits, and economies of scale. Many corporations expanded to a national
scale by starting out as local or regional companies and merging with or acquiring other local companies and
restructuring.
Leveraged buyouts constitute a similar form of restructuring, except that in this case, a publicly held company is
wholly purchased by an investor group that wants to take it private. Typically, this occurs when the company’s
senior management wishes to take it private. It must therefore raise the funds necessary to buy the stock back

from the public and thereby retain sole control of the business. Leveraged buyouts tend to be less common when
the stock market is healthy, because a buyout is relatively more expensive. The motivation is usually to take the
company in a direction that stockholders will not agree to, for one reason or another. Because of the potential for
management to drive down the stock of its own company in order to afford the buyout, the transaction must be
handled very carefully so that there is no hint of wrongdoing.
 Securities
Investment banks deal with securities on both the buy side (managing or consulting on hedge funds, mutual funds,
and pension funds) and the sell side, which includes not only the sales and promotion of stock but also the
securitization of various assets. Securitization is the process of creating a publicly traded and more liquid security
out of a debt instrument. The effects of the 2006–2008 subprime mortgage crisis were as widespread as they
were in part because collateralized debt obligations (CDOs) that used subprime mortgages as their backing assets
had become so common. Pools of such mortgages were used as the collateral to issue bonds, and sometimes
these CDOs were themselves bundled into new collateral pools. The credit rating of a bond backed by such
collateral was often higher than the credit rating of the constituent parts, which meant that investors who normally
avoid high-risk investments—especially institutional investors like pension funds—became exposed to the toxicity
of subprime-backed securities that had gone bad.
Securitization is an advanced and increasingly sophisticated process, as is the accurate valuation and rating of
such instruments. Since the late 1990s, investment banks have hired more and more holders of PhDs in math,
physics, and other hard sciences to work as “quants.” In the language of Wall Street, a quant is a quantitative
analyst. The field of quantitative finance, like modern investment banks themselves, began in the 1930s, after the
stock market crash of 1929 created a desire for strong empirical methods that could be used to analyze finance
and financial risk. Today, working as a quant requires advanced computer knowledge, because numerous types of
specialized software have been developed for complex mathematical analysis.
 History of U.S. Investment Banking
Investment banking in the United States began with brokers of government-and railroad-issued bonds, and
became more sophisticated over the course of the nineteenth century thanks to the large amounts of money
changing hands among the robber barons and other tycoons of the industrial revolution. During the Civil War, the
Union Army was funded with the first mass-marketed war bonds. When the war ended, banks continued to mass-
market similar securities to investors in search of something to do with their money. When industrialist J.P.
Morgan opened his New York banking house in the 1890s, it was not to do business as a commercial bank but
primarily to deal in gold, foreign currency, and securities.
Before the Great Depression, most commercial banks in the United States also offered investment-banking
services. Likewise, most investment banks were also commercial banks. The onset of the Depression was only
the latest in a series of banking panics that stretched back decades, and the severity of its consequences further
convinced the public at large and many in public office that banking mismanagement was among the principal
causes of the crash. The federal finance reforms of the early New Deal created the SEC in 1934 and required
banks participating in the Federal Reserve System—commercial banks—to give up their activity in the securities
trade in that same year. In addition, the reforms prohibited excessive interaction between securities firms and
commercial banks. (For example, no individual could serve on the board of both kinds of institutions.) As a result,
banks spun off their investment services into separate entities, just as busted trusts of the era spun off their
regional branches into separate companies.
The U.S. banking industry remained relatively stable until the 1980s, when the combination of inflation and
stagnant economic growth (stagflation) contributed to a new round of bank failures at a time when political
conservatives were rising to power. Although liberals and Democrats had long declared Franklin D. Roosevelt’s
New Deal initiatives largely responsible for pulling the country out of the Great Depression—with the heavy

industrial activity of World War II undeniably responsible for restoring the country to genuine prosperity—various
economists and political conservatives now challenged that notion, denied the efficacy of such regulations and
reforms, and argued against the need to retain them.
From the 1980s through the 1990s, conservatives pushed for and won gradual deregulation of many sectors of
the finance industry. Results included the rise of savings and loans (S&Ls, which were more appealing to
consumers under deregulation); a slew of bank mergers, as lending institutions were allowed to acquire others
outside the state holding their charter; and, eventually, the recoupling of investment banking with commercial
banking. The 1999 Gramm-Leach-Bliley Act, or Financial Services Modernization Act (FSMA), repealed parts of
the Glass-Steagall Act of 1933 that had segregated investment and commercial banking activity. Some restrictions
on banking activity remained intact: banks were not allowed to own nonfinancial companies, and vice versa,
preventing the possibility of an Apple Bank chain or of Capital One buying out Starbucks, for example. In the
interest of financial privacy, the commercial and investment activities of a particular institution had to remain
separate as well; bankers even had to use separate business cards in each context. Much of the debate over the
FSMA in the years since its passage has centered on questions of privacy and the protection of personal data of
banking customers.
Beginning in 1999, banking activities in the United States were allowed to intermingle under common corporate
auspices, which led to the creation of companies like Citigroup (established in 1998 by the merger of Citicorp and
Travelers Insurance, after the latter bought investment bank Salomon Smith Barney). As a result of both
deregulation and the 2008–2009 financial crisis, there are no major Wall Street firms that do business exclusively
as investment banks. Companies like JPMorgan Chase, Citigroup, Credit Suisse, and HSBC had already engaged
in both commercial and investment banking; and as the financial crisis came to a head in September 2008, the
last two stalwarts of pre-deregulation Wall Street—Goldman Sachs and Morgan Stanley—resumed traditional
commercial banking activity. Meanwhile, other investment banks met different fates in 2008. Lehman Brothers
collapsed after various acquisitions deals fell through, and Merrill Lynch became a part of Bank of America.
Bill Kte’pi
 
See also:  Banks, Commercial;  Bear Stearns;  Citigroup;  Glass-Steagall Act (1933);  Hedge
Funds;  Investment, Financial;  JPMorgan Chase;  Lehman Brothers;  Merrill Lynch;  PNC
Financial Services;  Securities and Exchange Commission;  Troubled Asset Relief Program
(2008-). 
Further Reading
Bookstaber, Richard. A Demon of Our Own Design: Markets, Hedge Funds, and the Perils of Financial
Innovation. Hoboken, NJ: John Wiley and Sons, 2007. 
Brigham, Eugene F., and Michael C. Ehrhardt. Financial Management: Theory and Practice. Mason, OH: South-
Western, 2008. 
Chancellor, Edward. Devil Take the Hindmost: A History of Financial Speculation. New York: Farrar, Straus and
Giroux, 1999. 
Madura, Jeff. Financial Markets and Institutions. Mason, OH: South-Western, 2008. 
Shefrin, Hersh. Beyond Greed and Fear: Understanding Behavioral Finance and the Psychology of Investing. New
York: Oxford University Press, 2007. 

Bauer, Otto (1881–1938)
 
A leader of the Austrian Social Democratic Party, Otto Bauer was a major theoretician of left-wing socialists who
followed Austro-Marxist ideology, which sought a “third way” to socialism—between capitalism and communism. In
this role, Bauer became a leading spokesperson for the Marxist view that economic cycles lead to political
instability and social revolution in the capitalist West.
Bauer advanced many of his views in the daily Arbeiter Zeitung (Workers’ Times), of which he was coeditor
beginning in 1907. His writings greatly influenced Marxist economic thinkers in the 1920s, 1930s, and the
decades following World War II. In the 1960s and 1970s, Bauer’s thinking served as inspiration for the New Left
movement in Europe, which advocated social activism. In the 1980s, his ideas inspired the Eurocommunist
movement, which moved away from totalitarian Soviet-style communism toward more democratic social reforms.
Otto Bauer was born on September 5, 1881, in Vienna, Austria. He earned a PhD in law from the University of
Vienna in 1906, and published his first book, Die Nationalitätenfrage und die Sozialdemokratie (Question of
Nationalities and Social Democracy), the following year. In it, Bauer advocated the creation of separate nation-
states as a solution to the conflict among ethnic minorities in the Austro-Hungarian Empire (Czechs, Slovaks,
Ruthenians, Croats, Italians, Hungarians, Roms, and others), which he viewed as class struggles. Anticipating
intense ethnic conflict in the Balkans, he called for a United States of Europe organized on a confederate basis,
much like today’s European Union.
After earning his doctorate, Bauer became active in the Austrian Social Democratic Party and began a rapid rise
through its ranks. He founded a socialist educational movement called Die Zukunft (The Future) and, in 1907,
founded and edited a theoretical party journal called Der Kampf. He held the position of party secretary from 1907
until 1914.
While serving in the Austrian army on the Eastern Front during World War I, Bauer was captured and spent three
years in Russian prisoner-of-war camps in Siberia. After returning to Austria in 1917, he became the head of the
Austrian Social Democratic Party. With the outbreak of revolution in November 1918—which ended the Habsburg
Empire and forced the abdication of Emperor Charles I—Bauer and the Austrian Social Democrats joined forces
with the Christian Social Party to lead Austria as a coalition government. As minister of foreign affairs in the new
regime, Bauer was a leading advocate of unification and signed a secret Anschluss (unification) agreement with
Germany in 1919 that was later repudiated by the Allies. After resigning as foreign minister in 1919, Bauer
became an opposition leader to Austria’s conservative governments and concentrated on developing the foreign
and domestic policies of the socialists.
Among Bauer’s most important published works is DieÖsterreichische Revolution (1923; Austrian Revolution,
1925), in which he identifies Austrian socialists of the time as the “third force” between capitalism and
communism. In 1926 he published a Social Democrat manifesto that had enduring influence on socialist
movements in Europe. Other books include The World Revolution (1919), The Road to Socialism (1919),
Bolshevism or Social Democracy? (1920), The New Course of Soviet Russia (1921), Fascism (1936), The Crisis
of Democracy (1936), and Between Two World Wars? (1937).
In 1933 members of the Christian Social Party and the Heimwehr (demobilized home guards) put an authoritarian
corporatist dictatorship into power that sought to suppress the Social Democrats. After taking part in the abortive
Viennese Socialist revolt of February 1934, Bauer was forced into exile. In Brno, Czechoslovakia, he organized
and ran the Austrian Social Democrats’ resistance movement (AuslandsbüroÖsterreichischer Sozialdemokraten-
ALÖS) from 1934 to 1938, until, in the face of the Nazi threat, he fled to Paris. He died there on July 4, 1938, just
months before Adolf Hitler’s Third Reich united Austria with Germany.
Andrew J. Waskey

 
See also:  Marxist Cycle Model. 
Further Reading
Boyer, John W. Culture and Political Crisis in Vienna: Christian Socialism in Power, 1897–1918. Chicago: University of
Chicago Press, 1995. 
Rabinbach, Anson. Crisis of Austrian Socialism: From Red Vienna to Civil War, 1927–1934. Chicago: University of Chicago
Press, 1983. 
 
Bear Stearns
 
Bears Stearns was a venerable international investment bank, brokerage house, and securities trading firm that, at
the time of its demise, specialized in risky asset-backed securities. Its collapse in March 2008 and subsequent
distress sale to the JPMorgan Chase financial services company helped trigger—or at least portended—the crisis
in the financial industry that paralyzed global credit markets in late 2008 and early 2009.
Bear Stearns was founded in 1923 in New York City as an equities trading firm to take advantage of another
period of rapid growth in the U.S. securities markets. Founded on limited capital, the firm thrived as new small
investors rushed to put their money into corporate stock and government bonds, which sent security prices—and
the company’s profits—soaring. With that money as a foundation, the firm was able to survive the Great
Depression and prosper in the post– World War II economic boom. By the 1970s, Bear Stearns had gained a
reputation as a risk taker, investing in high-yield bonds, including those issued by New York City when it was on
the verge of bankruptcy, and specializing in the corporate takeovers that sent Wall Street prices soaring in the
1980s. To bring in more capital to finance such deals, company executives took Bear Stearns public in 1985,
morphing from a brokerage house to a full-service securities trading firm and investment bank.
Along the way, the company had known its share of scandal. In the mid-1980s, to facilitate corporate takeovers,
Bear Stearns pioneered agreements that allowed clients to buy stock in the firm’s own name—a tactic deemed
illegal by the U.S. Securities and Exchange Commission. In 1997, Bear Stearns was caught in another scandal
when it served as a clearing broker for a smaller house, which subsequently went bankrupt and was found to
have defrauded investors of tens of millions of dollars.
Its reputation and share price declining in the face of such incidents, the company retrenched to avoid a hostile
takeover. The move proved successful, allowing Bear Stearns to avoid the worst of the dot.com crash of the early
2000s. Moreover, as it shifted its emphasis away from mergers and acquisitions in the 1990s, the firm also
averted the effects of an industry-wide slump in that segment of the investment banking business in the early
2000s. Thus, Bear Stearns survived as one of the last independent financial services companies on Wall Street.

By this time, the firm had come to focus on three major activities: the clearinghouse business, where it worked
with securities exchanges to ensure that executed trades were settled efficiently and on time; bond selling; and the
packaging and bulk reselling of home mortgages as asset-backed securities—or the securitization of risky housing
investments. Bear Stearns was among the most aggressive in the latter field, recognizing the enormous profits to
be made in what was becoming a red-hot housing market, driven by low interest rates and increasingly lax credit
standards for borrowers.
The business proved exceedingly lucrative for several years. Bear Stearns invested heavily in mortgage-backed
securities, many of them involving the subprime market. In subprime mortgages, lenders offered homeownership
loans to borrowers with little or even bad credit histories, at variable rates that would rise precipitously after a fixed
time period. At the same time, Bear Stearns also invested heavily in various kinds of financial derivatives, or
investment instruments whose price is derived from the value of another asset—a kind of insurance policy for
investments.
The strategy seemed brilliant, at least for a time. By late 2007, Bear Stearns had assets of nearly $400 billion and
derivative financial instruments whose notational contract value stood at a staggering $13.4 trillion. Meanwhile, the
company had been voted America’s most admired securities firm in the prestigious Fortune magazine’s “America’s
Most Admired Companies” survey three times running, and its “Early Look at the Market—Bear Stearns Morning
View” was one of the most widely read market information publications on Wall Street.
As characterized in a best-selling 2009 account of the firm’s demise, Bear Stearns was a house of cards. Its
nearly $400 billion in securities assets was supported by just $11.1 billion in actual equities positions, giving it a
highly risky leverage ratio of more than 35 to 1. In other words, the firm had borrowed more than $35 for every
dollar it held—and for very risky investments. Thus, when the U.S. housing market began to cool in 2006–2007,
Bear Stearns found itself in deepening trouble. With equity drying up, highly leveraged homeowners were unable
to refinance and, as rates rose on adjustable mortgages, went into default. This undermined the value of the
mortgage-backed securities in which Bears Stearns was so heavily invested. Meanwhile, civil suits were being
pressed against the company for misleading investors regarding the exposure of its investment funds to certain
high-risk securities.
As investors learned of the financial difficulties engulfing the firm, they began withdrawing money from their
investment funds. The stability of Bear Stearns became a source of industry and media speculation, which rapidly
eroded its very foundation—the foundation of any investment firm—public trust. The process devolved into a full-
scale “run” on the company by early 2008. In March, events cascaded rapidly as major banks refused to lend Bear
Stearns the funds it needed to cover losses, resulting in even more investor withdrawals. Unable to raise funds
from other banks, the company turned to the New York Federal Reserve and received a $30 billion emergency
loan on Friday, March 14. It was not enough to stave off investor fears. Concerned about the impact a collapse of
such a major investment bank would have on the global securities markets, the federal government hastily
arranged over the weekend to have JPMorgan Chase buy Bear Stearns for $2 a share. As the firm’s stock had
sold for more than $150 a share less than one year earlier, this provoked a shareholder revolt, forcing JPMorgan
—prodded by Secretary of the Treasury Henry Paulson and New York Federal Reserve Bank president Timothy
Geithner—to up the offer to $10 a share.

Protestors outside Bear Stearns headquarters in New York City demonstrate against the government-backed sale
of the venerable but crippled investment bank and brokerage house to JP Morgan Chase in March 2008.
(Bloomberg/Getty Images)
In the wake of the financial crisis that froze credit markets around the world in late 2008 and early 2009, and the
hundreds billions of dollars in Washington bailout money given to U.S. banks since that weekend in March, there
has been much second-guessing about the federal government’s approach to the Bear Stearns collapse. Some
have argued that a bailout package proportionate to the company’s liquidity shortfall would have reassured the
credit markets early on that the government was ready to take bold action to save major investment banks and
might have helped minimize the subsequent financial crisis. Whatever the case, the Bear Stearns crisis proved to
be a harbinger of dark economic times to come for the U.S. securities markets and for the global economy as a
whole.
James Ciment
 
See also:  Banks, Investment;  Bernanke, Ben;  Paulson, Henry;  Recession and Financial
Crisis (2007-). 
Further Reading
Bamber, Bill, and Andrew Spencer. Bear-Trap: The Fall of Bear Stearns and the Panic of 2008. New York: Brick
Tower, 2008. 

Cohan, William D. House of Cards: A Tale of Hubris and Wretched Excess on Wall Street. New York: Doubleday, 2009. 
Kansas, Dave. The Wall Street Journal Guide to the End of Wall Street as We Know It: What You Need to Know About the
Greatest Financial Crisis of our Time—and How to Survive It. New York: HarperBusiness, 2009. 
Waggoner, John. Bailout: What the Rescue of Bear Stearns and the Credit Crisis Mean for Your Investments. New
York: John Wiley and Sons, 2008. 
Behavioral Economics
 
Behavioral economics is a branch of economics that incorporates behavioral assumptions, including psychological,
sociological, and institutional factors, into the analysis of economic realities. It critiques the conventional economic
assumption that human beings act in a narrow, materially selfish, calculated, and deliberative manner—called
“rationality” in neoclassical economics—and that their actions can and should be reduced to mathematical models
that apply in all circumstances.
This branch of economics made a key contribution to the understanding of business cycles—and, more precisely,
of booms and busts—by introducing realistic behavioral assumptions into business cycle models. A fundamental
premise of behavioral economics is that making correct behavioral and institutional assumptions is vital in order to
understand causality and to generate robust analytical predictions. One cannot arbitrarily make behavioral and
institutional assumptions because they are mathematically convenient, because these assumptions traditionally
have been made, or because they fit into a particular worldview of rational or intelligent behavior. Rather,
behavioral and institutional assumptions must be context specific and must fit the economic reality.
According to behavioral economists, conventional economic theories fail to explain important facts about human
behavior with respect to business cycles. For example, George Akerlof, a pioneer of behavioral macroeconomics,
pointed to the failure of contemporary macroeconomic theory to explain the existence of involuntary unemployment
(conventional economists assume that if the unemployed really wanted work, they would work for less—thus
unemployment represents a strong preference for leisure); the real impact of monetary policy on output and
employment (conventional economists assume that monetary policy has no impact on output and employment in
the long run); the failure of deflation to accelerate when unemployment is high (conventional economists assume
that prices will fall when unemployment is high); and the excessive volatility of stock prices relative to their
fundamentals (conventional economists assume that actual stock prices should reflect fundamentals—part of
efficient market theory). For behavioral economists, what is critical is building models of business cycles that have
a robust foundation in microeconomics, inclusive of their psychological, sociological, and institutional dimensions.
 Assumptions Matter
In conventional economics, which follows the prescriptions of Milton Friedman, behavioral assumptions are not at
all important. Of critical importance, rather, is that individuals behave as if they know and can apply the
mathematical formulas that are consistent with producing, on average, optimal outcomes. Individuals are assumed
to have perfect knowledge of the alternatives relevant to a choice problem and to be able to forecast the
consequences of particular choices in the present and into the future, even when that future is highly uncertain.
These behavioral assertions assume that individuals have an unbounded computational capacity to determine the
outcomes of alternative choices, and that they make choices independent of other individuals. What matters to
conventional economists is that the model predicts well even if the behavioral assumptions are wildly inaccurate. In
other words, a good model predicts well even if it explains nothing. For behavioral economists, on the other hand,

analytical prediction is important, as is the ability to explain economic phenomena. They are interested in
understanding how people actually behave in the real world in the process of generating both optimal and
suboptimal economic outcomes.
Herbert Simon, one of the pioneers of behavioral economics, argued that in constructing economic models of
human behavior, the physiological and institutional constraints that characterize human decision-making must be
taken into account. Real people who are intelligent and rational cannot and therefore will not behave in the
calculating and all-knowing manner prescribed by conventional economics, nor will rational agents behave in the
narrowly selfish, materially maximizing manner that the conventional wisdom assumes must characterize rational
beings. Such thinking produces a different causal and predictive narrative than that generated by conventional
economics.
Simon coined the term “bounded rationality” to refer to rational choice in the context of a decision maker’s
cognitive limitations, as well as the limitations of knowledge and computational capacity. According to Simon’s
theory of bounded rationality, individuals do not maximize but rather satisfy (do the best they can), based on
decision rules according to which a particular choice appears satisfactory given the objectives of the decision
maker. In other words, smart decision makers use context-dependent heuristics, or experience-based techniques,
which Gerd Gigerenzer referred to as “fast and frugal heuristics,” to make investment-and employment-related
decisions.
 Animal Spirits, Herding, and Confidence
To explain booms and busts in the economy, behavioral economics emphasizes behavioral factors along with real
variables such as supply shocks (e.g., a spike in oil prices or technological change). The field seeks to provide the
best possible understanding of business cycles, which, behavioral economists argue, cannot be achieved without
adequately incorporating behavioral variables into macroeconomic modeling. Behavioral economics also is
concerned with the relationship between policy actions and behavioral factors, such as the way in which monetary
and fiscal policies influence consumer and investor behavior given the importance of “animal spirits”—that is,
human emotions and moods.
Behavioral models are influenced by John Maynard Keynes’s General Theory of Employment, Interest, and Money
(1936), which introduced psychological variables into the economic modeling narrative. Keynes argued that
“animal spirits”—whether a person is confident or pessimistic based on imperfect information and emotive factors
—contribute directly to business cycles. Animal spirits play a role in motivating borrowing and spending behavior,
which, in turn, has an impact on business cycle volatility and the degree to which monetary and fiscal policies can
affect macroeconomic outcomes. Keynes referred to animal spirits as behavior that is motivated by emotive as
opposed to calculating or hard-core economic rationality considerations:
Most, probably, of our decisions to do something positive, the full consequences of which will be
drawn out over many days to come, can only be taken as the result of animal spirits—a spontaneous
urge to action rather than inaction, and not as the outcome of a weighted average of quantitative
benefits multiplied by quantitative probabilities.
Keynes’s notion of the “liquidity trap” is also important to understanding macroeconomic outcomes. A liquidity trap
exists when individuals have little confidence in an uncertain future in light of a dismal present, and thus refuse to
borrow money in order to invest or consume, even at very low real interest rates. This behavior plays a role in
sustaining economic downturns. Reducing interest rates to drive economic recovery will not be sufficient if animal
spirits—that is, consumer confidence—are at low levels. Efforts must be made to restore confidence in the
economy as well; monetary policy, which emphasizes the price of borrowing, is only a piece of a much more
complex policy puzzle.
Hyman Minsky introduced the concept of financial fragility, which is closely related to the ideas of animal spirits
and overconfidence or overoptimism. Minsky argued that market economies are naturally subject to business

cycles, largely as a function of psychological variables that might be allayed by smart government policy. As the
economy expands (booms), investors gain confidence and engage in speculative financing, believing that profits
will cover the cost of interest payments. Growth is fueled by speculative investment, and speculative investment is
fueled by growth. Lenders execute loans with confidence that those debts will be repaid. Eventually, however,
loans become risky as more investments (including the purchase of shares) are funneled into assets whose prices
have little relation to their fundamental values. When a negative shock hits the economy—exposing the overall
riskiness of investment—the bubble bursts, and the economy moves into a recession (bust), driven by increasing
defaults and bankruptcies.
Another concept pioneered by Keynes relates to “choice behavior,” which uses predictions about the behavior of
other individuals as a proxy for best practice behavior. For example, an individual is not sure how best to invest in
the stock market. Therefore, he or she follows the leader. In a world of asymmetric information, such “herding
behavior” may make sense, but it is not part of mainstream theory. Keynes used the analogy of a beauty contest:
It is not a case of choosing those [faces] that, to the best of one’s judgment, are really the prettiest,
nor even those that average opinion genuinely thinks the prettiest. We have reached the third degree
where we devote our intelligences to anticipating what average opinion expects the average opinion
to be. And there are some, I believe, who practice the fourth, fifth and higher degrees.
Herding behavior, whereby individuals follow the leader at a rapid clip, generates asset-price cascades. Such
behavior creates asset-price bubbles and crashes that drive assets prices far above their fundamental values in
the short run. Such deviations are inconsistent with an important facet of the efficient market hypothesis—that
asset prices should reflect fundamental value at all points in time.
A very important variable introduced by George Akerlof and Joseph Stiglitz to help explain business cycles is
“asymmetric information,” meaning that different people have access to different and incomplete information that is
pertinent to a particular decision problem. For example, investment bankers know more about the riskiness of
assets than buyers, borrowers know more about their creditworthiness than bankers, sellers of used cars know
more about their vehicles than prospective buyers, and workers know more about their effort inputs than their
employers. In these cases, individuals must make educated guesses to fill in their information gaps.
Other important variables, which behavioral economists refer to as “cognitive biases,” include overconfidence,
money illusion, framing, panic, and fairness. Overconfidence or overoptimism bias is also referred to as “irrational
exuberance,” a term coined by former Federal Reserve chairman Alan Greenspan and adopted by economist
Robert Shiller, a pioneer of behavioral finance. Overconfidence occurs when individuals believe that they can do
better than they actually can, objectively speaking. This drives animal spirits in a manner that yields bubbles.
 Money Illusion
Akerlof argued that individuals tend to suffer from a “money illusion,” causing them to act in a quasi-rational
manner. That is, they do not bother to correct for small decreases in money wages that result from low rates of
inflation. In the view of workers, it is not worth the transaction costs of attempting to secure compensating money
wage increases. Workers also may not notice real wage cuts that are a consequence of low rates of inflation (true
money illusion). Therefore, government can cut real wages by adopting a targeted low inflation policy.
To the extent that real wages must fall in order for employment to increase during a recession, this can be
achieved with the help of smart monetary policy. The contemporary wisdom rejects the hypothesis that money
illusion exists and that monetary policy can reduce real wages, given that individuals are assumed to be rational
(not easily fooled).
Keynes rejected the hypothesis of money illusion as well. He argued instead that workers will knowingly accept
small real wage cuts through inflationary policy during an economic downturn, especially a severe one that is
associated with increases in employment, as this does not reduce their relative wages. However, workers will

resist employers’ efforts to cut their real wages directly. Therefore, Keynes argued that unions and workers “do not
raise the obstacle to any increase in aggregate employment which is attributed to them by the classical school.”
Keynes’s argument builds on the rationality assumption, but, like Akerlof, he regarded monetary policy as one
weapon in a larger arsenal required to move an economy out of a downturn.
 Efficiency Wages, Reciprocity and Fairness, and Business Cycles
Conventional macroeconomists tend to agree that one cause of economic downturns or busts is that workers
refuse to accept wage cuts as aggregate demand falls. Thus, wages are inflexible or “sticky” downward, meaning
that wages cannot always be lowered in response to an economic downturn. In this sense, many economists
argue that workers are responsible for the persistence of unemployment.
Behavioral economists argue that firms will not cut wages during a downturn—as predicted by some conventional
economists (new classical school)—for efficiency wage reasons. Workers will retaliate against wage cuts that are
judged to be unfair by reducing their effort inputs. This is possible in the real world of asymmetric information and
incomplete contracts, and increases the real cost of labor and unit production costs. This provides a rational
reason why nominal wages are sticky downward. Given the money illusion or rational workers’ acceptance of
inflation-generated real wage cuts, monetary policy can be effective at reducing real wages without spurring
workers to retaliate by cutting effort inputs, thereby facilitating rising employment on the supply side.
An alternative efficiency wage argument is that even in the absence of money illusion, employment will grow
without cuts to real wages if firms respond to increasing demand by increasing firm efficiency. Firms will increase
the demand for labor if rising efficiency makes increasing employment cost effective. Higher wages are
sustainable when they provide incentives for sufficient increases in productivity (increasing effort inputs).
Inflationary policy is not a necessary condition for increasing employment in this case (consistent with the
classical economic perspective). But increasing demand to encourage firms to increase output and productivity is
necessary. This is consistent with the evidence that there is a positive relationship between employment growth
and increases and levels of real wages internationally.
 Policy Based on Behavioral Economics Insights
An important school of thought within behavioral economics argues that behavioral variables such as
overconfidence, herding, and greed are cognitive biases or errors in judgment, and therefore are irrational. Thus,
irrationality in decision-making is a leading cause of booms and busts in the economy. This suggests that policy
makers should attempt to modify human behavior so that agents behave rationally, therefore moderating the
extent of business cycle volatility. An alternative perspective argues that these variables represent rational
behaviors in a world of bounded rationality. One cannot reduce the extent of business cycle volatility by modifying
these behavioral characteristics. Rather, the extent to which volatility can be reduced is contingent on changing
environmental variables such as information and legal parameters that, in turn, affect decision-making.
An important cause of excess market volatility, especially severe crashes, is inappropriate regulation of public and
private financial institutions. Many investment decisions that generate large social and private losses are products
of incentive systems that do not internalize gains and losses to individual decision makers. If it is profitable for
individuals to make decisions that predictably can bankrupt their firms by selling toxic assets, for example, then it
is highly rational to engage in these activities, even though it is to the detriment of the firm’s shareholders. Also,
what appear to be irrational decisions often are the products of a lack of knowledge or misleading information.
Assets might be packaged or framed in a positive light even when the fundamentals are weak. From this
perspective, public policy needs be directed toward institutional design as opposed to the reconfiguration of human
behavior.
An important implication for monetary or fiscal policy is that government must consider the impact of policy on
animal spirits, and hence on the propensity to consume and invest. For example, low interest rates alone, given a
pessimistic public, will not generate expected increases in borrowing and expenditure. During an economic

downturn, nonstimulatory fiscal policy or negative financial analysis can, through pessimistic animal spirits, push
the economy further downward even in the face of easy money. To the extent that mild inflation helps grease the
wheels of growth in the labor market, targeting inflation rates at very low levels keeps unemployment
unnecessarily high. Finally, given efficiency wages, efforts to cut real wages will not have the predicted effect of
spurring increased employment. Rather, incentives should be developed to encourage increases in economic
efficiency in the context of increases in aggregate demand.
Booms and busts are part and parcel of vibrant market economies, but their magnitude can be moderated by
public policy, thereby avoiding major crashes, only with a greater appreciation of the behavioral variables
underlying decision-making.
Morris Altman
 
See also:  Confidence, Consumer and Business;  Keynes, John Maynard;  Minsky, Hyman. 
Further Reading
Akerlof, George A.  “Behavioral Macroeconomics and Macroeconomic Behavior.” American Economic Review 92:3
(2002): 411–433. 
Akerlof, George A., and Robert J. Shiller. Animal Spirits: How Human Psychology Drives the Economy, and Why It Matters
for Global Capitalism. Princeton, NJ: Princeton University Press, 2009. 
Altman, Morris.  “Behavioral Economics, Economic Theory and Public Policy.” Australasian Journal of Economic
Education 5(2009): 1–55. 
Altman, Morris. Handbook of Contemporary Behavioral Economics: Foundations and Developments. Armonk, NY: M.E.
Sharpe, 2006. 
Altman, Morris.  “Involuntary Unemployment, Macroeconomic Policy, and a Behavioral Model of the Firm: Why High Real
Wages Need Not Cause High Unemployment.” Research in Economics 60:2 (2006): 97–111. 
Altman, Morris. Worker Satisfaction and Economic Performance: Microfoundations of Success and Failure. Armonk,
NY: M.E. Sharpe, 2001. 
Friedman, Milton.  “The Methodology of Positive Economics.” In Essays in Positive Economics, ed. Milton
Friedman. Chicago: University of Chicago Press, 1953. 
Gigerenzer, Gerd. Gut Feelings: The Intelligence of the Unconscious. New York: Viking, 2007. 
Lewis, Michael. The Big Short: Inside the Doomsday Machine. New York: W.W. Norton, 2010. 
Minsky, Hyman P. Stabilizing an Unstable Economy. New Haven, CT: Yale University Press, 1986. 
Schiller, Robert J. Irrational Exuberance. Princeton, NJ: Princeton University Press, 2001. 
Stiglitz, Joseph E.  “The Causes and Consequences of the Dependence of Quantity on Price.” Journal of Economic
Literature 25:1 (1987): 1–48. 
Belgium
 

With a population of almost 11 million and a landmass the size of Maryland, Belgium is one of the smallest
member states in the European Union (EU). The capital city of Brussels is a major financial center and the
headquarters for the EU and the North American Treaty Organization (NATO). Due to its geographic location at
the crossroads of Western Europe, a well-developed transport network, and the high productivity of its workforce,
Belgium has historically enjoyed a strong economy. In 2008, it was one of the fifteen largest trading nations in the
world and had a gross domestic product (GDP) of $390 billion, which ranked thirtieth in the world. Despite its
comparatively strong economy, the Belgian financial sector suffered significant losses in the global economic crisis
of 2008–2009, with major banks and investment firms reeling from bailouts and bankruptcies like their counterparts
in the United States.
One of the Low Countries of Western Europe, Belgium is part of the Benelux group of nations (along with the
Netherlands and Luxembourg). It was a founding member of the European Economic Community (a precursor to
the modern European Union), created in 1957 to bring about economic integration among Belgium, France,
Germany, Italy, Luxembourg, and the Netherlands; in 1992 it became a founding member of the EU.
The Senne River, which passes through Brussels, divides the country roughly in two. The northern part, Flanders,
speaks Flemish (a form of Dutch) and is primarily Protestant. The southern part, Wallonia, is inhabited by the
French-speaking Walloons, who are primarily Roman Catholic. Despite its ethnic, linguistic, and religious
differences, Belgium has been able to survive as a constitutional monarchy since Napoleonic times. In the days of
European colonialism, it was a minor power ruling over its African colony of the Belgian Congo (present-day
Congo).
Since the Middle Ages, Flanders has been economically important as a center for trade and textiles. As the Italian
city-states declined, Antwerp and Brugge (Bruges), today the capital city of West Flanders, became important
commercial centers for shipbuilding, food processing, and chemicals. In the 1830s, Belgium followed Great Britain
as it experienced its own industrial revolution and became a center for textiles and steel because of its natural
deposits of coal and metal ores. This was particularly true of the French-speaking Wallonia. The Flemish-speaking
north remained mostly agricultural, with some processing facilities. This economic and linguistic split has persisted
through the country’s history, and has led to political stalemate and even talk of separatism into the twenty-first
century. Still, despite such divisions, Belgium boasted one of the most advanced industrial sectors in Europe
through the early twentieth century.
Because it was so heavily industrialized and so dependent on trade, Belgium was hit by the global economic
contraction of the 1930s, before being occupied by the Nazis during World War II. In the wake of that conflict,
however, Belgium participated in the postwar economic miracle in Western Europe, seeing its living standards rise
dramatically and a major chemical and petroleum refining industry develop around the port of Antwerp in the
northwest. Indeed, the rise of Antwerp was part of a general geographic shift in the Belgium economy in the
postwar era, in which the industrial heartland, saddled with older and inefficient heavy industry, began to lag
behind the prospering north.
Overall, by the mid-1990s, manufacturing industries and mining had declined, while the country’s service sector
(consumer and financial services) expanded significantly, accounting for nearly 70 percent of the GDP and
employing 70 percent of the labor force. The oil crisis of the 1970s and economic restructuring led to a series of
recessions during the 1980s. In the early 1990s—in an effort to give its manufacturing regions greater control over
their economic problems—the government extended each region broad economic powers to control trade,
industrial development, and environmental regulation while at the same time privatizing many formerly state-
owned companies.
Belgium has been one of the foremost proponents of regional economic integration, with approximately 75 percent
of its trade with other EU countries, including Germany, the Netherlands, and France. After the initiation of the
euro as the official currency of the EU member states in 2002, currency exchanges became more efficient and
cheaper. Nevertheless, the financial crisis that began in 2008 hit the Belgians hard as shareholders in the nation’s
two largest financial institutions, Fortis and Dexia, saw their assets evaporate in the global monetary meltdown.

Fortis, the largest Belgian financial-services firm with branches in all three of the Benelux countries, had 41.7
billion euros (US$60.7 billion) of structured investments at the end of June 2008, including collateralized debt
obligations and U.S. mortgage-backed securities. Its banking operation was put in jeopardy by the global financial
crisis brought on by the collapse of the U.S. subprime mortgage market. In order to meet the Fortis financial crisis,
in September the Benelux countries invested a total of 11.2 billion euros (US$16.3 billion) into a rescue package;
however, a run on Fortis banking units in the Netherlands forced the government to nationalize the Dutch units.
This meant that Fortis’s branches in the Netherlands were taken away from Fortis by the Dutch government.
(Finally, in early 2009, the Belgian portion of Fortis was sold to the French bank BNP Paribas.)
The 2008 Belgian financial crisis deepened after the Belgian financial institution Dexia, a major lender to
governments and public agencies in Europe, received 6.4 billion euros (US$9 billion) from the Belgian, French,
and Luxembourg governments in order to provide the company with additional capital after it was unable to sell
more stock to increase its capitalization. However, the capital infusion was not enough to keep Dexia solvent, and
in November 2008 it announced that it had lost 1.5 billion euros and was forced to sell its insurance operations.
On December 15, 2008, Dexia announced that some of its financial problems were related to the collapse of the
Wall Street firm Bernard L. Madoff Investment Securities. Madoff, a former chairman of the NASDAQ stock
exchange, resigned as chairman of his firm on December 11, 2008, when it was discovered that he had defrauded
investors of an estimated US$50 billion in a Ponzi investment scheme, of which Dexia was a victim.
A third major shock to the Belgian economy came in October 2008, when the Belgian unit of Ethias, a European
company engaged in banking and insurance, announced that it would be receiving 1.5 billion euros from the
Belgian government in order to shore up its capital reserves. As the country’s financial crisis deepened, news of
the assistance brought confidence in the Belgian banking sector to an all-time low.
Economic growth and foreign direct investment declined in 2008. The depth of Belgium’s financial and economic
crisis remained uncertain for the next several years, as EU leaders formulated an action plan to cope with
Europe’s economic crisis.
Andrew J. Waskey
 
See also:  Netherlands, The. 
Further Reading
Cook, Bernard A. Belgium: A History. New York: Peter Lang, 2002. 
Meerten, Michaelangelo van. Capital Formation in Belgium, 1900–1995. Ithaca, NY: Cornell University Press, 2003. 
Mommen, Andre. The Belgian Economy in the Twentieth Century. London: Routledge, 1994. 
 
Bernanke, Ben (1953–)

 
As chair of the Board of Governors of the Federal Reserve System (Fed) since early 2006, Ben Shalom Bernanke
was a key player in the U.S. government’s response to the financial crisis of 2008–2009, arranging for the bailout
of insurance giant AIG (American International Group) and working with Secretary of the Treasury Henry Paulson
to design and implement the $700 billion federal bailout plan, known officially as the Troubled Asset Relief
Program.
Federal Reserve Board chairman Ben Bernanke testifies before a Senate committee on the $700 billion Wall
Street bailout request by the Bush administration in fall 2008. The Senate elected Bernanke to a second term in
January 2010. (Alex Wong/Getty Images)
Bernanke was born in Augusta, Georgia, in 1953, and grew up in a small town in South Carolina. He received a
bachelor’s degree in economics from Harvard University in 1975 and a doctorate in economics from the
Massachusetts Institute of Technology in 1979; his doctoral dissertation focused on the dynamics of the business
cycle. Bernanke was a professor of economics and public affairs at Princeton University from 1985 to 2002. Before
joining the faculty at Princeton, he was an associate professor of economics (1983–1985) and an assistant
professor of economics (1979–1983) in the Graduate School of Business at Stanford University. He also taught as
a visiting professor of economics at New York University (1993) and the Massachusetts Institute of Technology
(1989–1990). Bernanke is a fellow of the Econometric Society and the American Academy of Arts and Sciences,
and he has served as editor of the American Economic Review.
As an academic, Bernanke focused his scholarly efforts on examining the causes of the Great Depression. He
confirmed the argument presented by Milton Friedman and Anna Jacobson Schwartz in their study A Monetary
History of the United States (1963), that the activities of the Fed to reduce the money supply during the early
1930s contributed significantly to the duration of the Depression. Bernanke also published important works on the
role of financial information and creditworthiness during the Depression. In his 1983 article “Nonmonetary Effects
of the Financial Crisis in the Propagation of the Great Depression,” he highlighted the impact of widespread bank

failures during the Depression. These, he argued, contributed to a loss of invaluable financial information about
the creditworthiness of firms, thereby leading to greater risks in investing and, in turn, causing the real cost of
credit to rise. Bernanke also emphasized the importance of comparative research into prices, wages, and
production across a number of countries in order to better understand the causes and transmission of the
Depression.
While still teaching, Bernanke entered public service as a member of the Academic Advisory Panel at the Federal
Reserve Bank of New York from 1990 to 2002. In the latter year, he took a leave of absence from Princeton to
become chair of President George W. Bush’s Council of Economic Advisers from 2005 to 2006. He was a member
of the Board of Governors of the Federal Reserve System from 2002 to 2006 before succeeding Alan Greenspan
as chair on February 1, 2006.
Upon taking the helm at the Fed, Bernanke instituted a number of reforms that attempted to make the institution
more transparent by declaring specific inflation goals. At first, Bernanke made a point of issuing clearer statements
about Fed policy than had Greenspan, who was famous for his oracular remarks. Some criticized the Fed’s new
openness, arguing that such clear declarations of intent caused fluctuations in the stock market.
But it was Bernanke’s response to the financial crisis of late 2000s that garnered the greatest criticism—and
praise—from economists and other experts. Some contended that he was slow to react to the economic crisis as it
unfolded over the course of 2008. Indeed, many said that his decision to let the investment bank Lehman Brothers
fail in September 2008 helped precipitate the crisis in the first place. However, Bernanke’s research into the
causes of the Great Depression clearly influenced the policy outlook of the Fed during the financial crisis. This can
be seen in the interventionist, and often controversial, stance of both the Fed and Congress in preventing a
contraction of the money supply, and in their actions to “bail out” many financial institutions. Bernanke maintained
—along with many other supporters of the bailout—that this was critical in order to ensure the stability of, and the
public’s confidence in, the larger financial system, thereby limiting the damage and duration of the initial crises.
At first, Bernanke intended to buy up “toxic” mortgage-backed securities that were poisoning the global financial
system. The Emergency Economic Stabilization Act, passed on October 3, 2008, authorized $700 billion to do so.
Within a few days, however, the Treasury Department, in conjunction with the Fed, decided to use an initial $250
billion to purchase preferred stock in American banks. The thinking was that by shoring up the capital of the
largest banks, institutions would begin lending again and the financial crisis would be resolved.
Bernanke was criticized for his role in arranging the acquisition of troubled brokerage house Merrill Lynch by Bank
of America in late 2008. New York State Attorney General Andrew Cuomo, for one, alleged that Bernanke and
Paulson had failed to inform Bank of America officials of the true scope of Merrill Lynch’s losses when persuading
it to take over the company. At congressional hearings on the subject in June 2009, Bernanke insisted that he
had not forced Bank of America president Ken Lewis into taking over Merrill Lynch, and did not admit to covering
up the latter company’s economic woes. Bernanke also was widely criticized for his decision to provide $80 billion,
prior to the passage of the $700 billion bailout legislation, to rescue insurance giant AIG. At least one senator
alleged that Bernanke had made the decision against his staff’s recommendation.
Despite these criticisms, President Barack Obama nominated Bernanke for a second term as Fed chairman in
August 2009. But rising political anger at the bailout, a still sputtering economy, and double-digit unemployment
led to an unusual amount of resistance on Capitol Hill, with both conservatives and liberals questioning the
president’s decision. Nevertheless, Bernanke was confirmed by the Senate in a 70–30 vote in January 2010, the
narrowest margin in the history of Federal Reserve chair confirmation votes.
James Ciment
 
See also:  Federal Reserve System;  Troubled Asset Relief Program (2008-). 

Further Reading
Bernanke, Ben S.  “Nonmonetary Effects of the Financial Crisis in the Propagation of the Great Depression.” American
Economic Review 73:3 (June 1983): 257–276. 
Grunwald, Michael.  “Person of the Year: Ben Bernanke.” Time, December 16, 2009. 
Harris, Ethan S. Ben Bernanke’s Fed: The Federal Reserve After Greenspan. Boston: Harvard Business, 2008. 
Bethlehem Steel
 
Once the second-largest steel manufacturer in the United States, the Pennsylvania-based Bethlehem Steel
corporation was laid low in the late twentieth century by internal management issues, foreign competition, and the
overall decline of the U.S. steel industry that began in the 1970s. Facing huge losses by the 1980s, the company
was forced to shut down most of its plants and restructure. These efforts only slowed its decline, however, and in
2001 the company declared bankruptcy.
Founded in 1857 as the Saucona Iron Company, the company moved to Bethlehem, Pennsylvania, and changed
its name to the Bethlehem Iron Company four years later. The company prospered in the late nineteenth century—
despite intense competition from industry leader Carnegie Steel (later U.S. Steel)—providing iron and then steel for
the nation’s growing rail network, the skyscraper construction boom, and the U.S. Navy, which was replacing its
wooden ships with steel ones and expanding dramatically after the 1880s. The company changed its named to the
Bethlehem Steel Company in 1899, then reorganized under a corporate charter as the Bethlehem Steel
Corporation in 1904. It also grew by introducing innovative steel manufacturing techniques—including the Grey
rolling mill, which revolutionized the manufacture of steel girders—and acquiring other companies in the steel
business as well as in railroads, coal mining, and shipbuilding.
To meet government demand, Bethlehem Steel expanded again during World War II, building more than 1,000
ships for the U.S. armed forces, leaving it with a huge capacity to meet the needs of the postwar economic boom.
Bethlehem Steel became a leader in the production of steel for both old uses (such as building construction) and
new ones (including production of uranium rods for nuclear power plants). The 1950s and 1960s saw some of the
U.S. steel industry’s most profitable years, as the economy boomed and the former steel-making capacity of
Europe and Japan struggled to rebuild after the devastation of World War II. By the mid-1950s, Bethlehem Steel
reached its peak production of more than 20 million tons (18.1 million metric tons) annually. In 1955, it reached
number eight on Fortune magazine’s list of America’s biggest businesses.
Steel was America’s bellwether industry during this period, with both economic analysts and politicians keeping a
close eye on it. In 1962, for example, President John F. Kennedy intervened when the industry—led by U.S. Steel
—raised its prices; Kennedy forced the company to roll back the increases out of fear they might produce
destabilizing inflation throughout the economy.
But America’s dominance of the global steel industry did not last. As European and Asian manufacturers revived,
they soon began to compete both in the American and foreign markets. Steel producers in Europe and Asia
enjoyed lower labor costs—this was particularly true for Asian producers—and more modern manufacturing
facilities. In the long run, the destruction wrought by World War II allowed European and Japanese steelmakers to
introduce cutting-edge technology more quickly in subsequent decades. Thus, by the 1970s, foreign steel of the
same high quality could be produced for substantially less than American steel.

Bethlehem Steel began seeing regular annual losses in the middle years of that decade, a result not just of
foreign competition but of a sputtering American economy and a decline in large, steel-intensive infrastructure
projects. Management contributed to the company’s problems as well, failing to close unprofitable facilities,
upgrade other plants, and adjust to changing market demand for more varied steel products. In 1982, the company
reported more than $1.5 billion in losses, which forced it to close down many of its operations. Along with declines
at other steel and heavy manufacturers, the closings helped create what came to be known as the “rust belt,” a
swath of the Northeast and Upper Midwest pockmarked by empty and decaying factories, declining tax bases, and
persistent high unemployment. More generally, scholars of the era began to talk of an American
“deindustrialization.”
While Bethlehem’s belt tightening returned it to profitability in the late 1980s, the comeback was short-lived.
Further losses in the 1990s forced the company to divest its coal mining, railroad car manufacturing, and
shipbuilding businesses. In 1995, it also closed its main plant in Bethlehem, ending an almost 140-year history of
steelmaking in that city. But the restructuring was a case of too little, too late. In 2001, the corporation was forced
to file for protection under U.S. bankruptcy laws. Two years later, many of its remaining assets were sold to
International Steel Group, which merged with the Netherlands-based Mittal Steel in 2005.
While some of the company’s old plants continue to produce steel, part of the flagship facility in Bethlehem was
turned into an industrial-themed casino and resort in 2009.
James Ciment
 
See also:  Manufacturing. 
Further Reading
Loomis, Carol J.  “The Sinking of Bethlehem Steel.” Fortune,  April 5, 2004. 
Strohmeyer, John. Crisis in Bethlehem: Big Steel’s Struggle to Survive. Bethesda, MD: Adler & Adler, 1986. 
Warren, Kenneth. Bethlehem Steel: Builder and Arsenal of America. Pittsburgh, PA: University of Pittsburgh Press, 2008. 
Böhm-Bawerk, Eugen Ritter von (1851–1914)
 
Eugen Ritter von Böhm-Bawerk was an Austrian economist, minister of finance, and founding member of the
Austrian school of economics. He developed a number of the early theories that influenced such students of
economic growth and business cycles as Joseph Schumpeter.
Böhm-Bawerk was born on February 12, 1851, in Brno, Czech Republic (then Brünn, Moravia). He studied law at
the University of Vienna, receiving a PhD in 1875. There, his views on economics were greatly influenced by
Austrian school founder Carl Menger’s Principles of Economics (1871), and by his classmate and future brother-
in-law, Friedrich von Wieser. From 1881 to 1889, while teaching at the University of Innsbruck, Böhm-Bawerk
published two of the three volumes of Capital and Interest, his most important work. He worked with the Austrian
Ministry of Finance on a proposal for tax reform, and in 1895 he became minister of finance, a post he would hold
off and on until 1904. In this position, he supported the institution of a legally fixed gold standard, a balanced
budget, and financial stability. He strongly opposed government subsidies and spending on large public projects.

He returned to academia in 1904 as a professor at the University of Vienna. There, his students included Ludwig
von Mises, Henryk Grossmann, and Schumpeter.
In Capital and Interest (1884), Böhm-Bawerk examines interest—including use, productivity, and abstinence
theories. For example, people agree to pay interest in order to have immediate access to goods. If they must give
up the privilege of immediate consumption, they in turn demand positive interest. He also maintains that there is a
“technical superiority of present over future goods,” an argument that was both controversial and complex. He
views production as a roundabout process (that is, the process by which capital, such as machinery and
equipment, is produced first and then used to produce consumer goods) that requires time. Thus, investment in
capital is necessary to transform future factors of production (land and labor) into higher output.
Although today Böhm-Bawerk’s theories on the roundabout process are somewhat obscure, prior to World War I
they were much discussed. He was among the first economists to challenge Karl Marx’s view that capitalism
exploits workers. Böhm-Bawerk argued that capitalism actually favors workers, since they are paid based on
expected (future) revenue from the goods they produce. Workers could not receive all the benefits (profits) from
production because some of the product would be necessary to finance the production process. Böhm-Bawerk
criticized Marx’s exploitation theories, arguing that workers, in fact, receive their wages before the owner receives
revenues on goods produced. Hinting at his theory of the roundabout process, Böhm-Bawerk pointed out that
Marx’s theory ignored the time of production and thus the present value factor. Therefore, workers produce only
part of a good’s value; labor can only be paid according to the present value of a future output.
In Karl Marx and the Close of His System (1896), Böhm-Bawerk argues that there is a contradiction between the
law of value, which Marx explained in the third volume of Capital, and his own theory of value. According to Böhm-
Bawerk, the allocation of profits to the different factors of production does not follow political decisions but rather
follows economic imperatives, such as supply and demand.
Positive Theory of Capital (1889), the second volume of Capital and Interest, focuses on the time of production
processes and the need of interest payments to bridge the gap between present and future output. In it, Böhm-
Bawerk supports Menger’s ideas of marginal utility, stating that goods only have value based on what people are
willing to pay for them.
The last volume of Böhm-Bawerk’s Capital and Interest, Further Essays on Capital and Interest (1921), published
posthumously, contains clarifications of his theories and his responses to critics. At the time of his death on August
27, 1914, in Tyrol, Böhm-Bawerk was considered one of the period’s leading economists.
Carmen De Michele
 
See also:  Austrian School;  Hayek, Friedrich August von;  Mises, Ludwig von;  Monetary
Policy;  Monetary Theories and Models;  Schumpeter, Joseph. 
Further Reading
Garrison, Roger W.  “Austrian Capital Theory: The Early Controversies.” In Carl Menger and His Legacy in Economics, ed.
Bruce J. Caldwell. Durham, NC: Duke University Press, 1990. 
Kirzner, Israel. Essays on Capital and Interest: An Austrian Perspective. Brookfield, VT: Edward Elgar, 1996. 
Kuenne, Robert E. Eugen von Böhm-Bawerk. New York: Columbia University Press, 1971. 

 
Boom, Economic (1920s)
 
The period between the end of World War I in November 1918 and the stock market crash of October 1929 saw
one of the most dramatic and rapid economic expansions in U.S. history. Industrial output, corporate profits, and
stock prices all experienced significant and sustained growth, aside from a short but sharp recession from late
1920 through early 1922. So dramatic was the country’s economic performance during these years that the
Roaring Twenties have been forever etched in the public imagination as the epitome of boom times.
In reality, the 1920s were not as universally cheerful as people commonly believe. Large sectors of the economy,
including agriculture, textiles, and coal, remained mired in recession, while the enormous wealth generated during
the period was unequally distributed; most working-class and lower-middle-class Americans saw little growth in
their incomes. And, of course, the boom economy of the 1920s was living on borrowed time, with a number of
important flaws that contributed to the “Great Crash” and the worst catastrophe in U.S. economic history.

Flourishing consumerism, the rebellious exuberance of flappers, and the freedom afforded by automobiles
characterized the Roaring Twenties. An ailing farm sector and a widening gap between rich and poor were among
the underlying economic weaknesses. (Stringer/Hulton Archive/Getty Images)
 Rough Start
The 1920s did not begin with a roar. Indeed, economic performance in the several years immediately following
World War I was shaky at best, largely as a result of a hasty and poorly thought-through demobilization effort.
During the war, which, for the United States lasted from early 1917 through late 1918, federal spending grew
exponentially, from several million dollars annually to more than $2 billion per month. The nation’s factories could
barely keep up with defense demands, and unemployment virtually disappeared (partly due to the fact that nearly
4 million people were in uniform).
With the end of the war, consumers went on a buying spree with their wartime earnings, as defense efforts had
caused a relative dearth of consumer products during the war period. Since companies could not keep up with
demand, prices soared—the inflation rate reached 30 percent in 1919—far outpacing wage growth. The gap led to
what was perhaps the largest wave of strikes in U.S. history to that time. Approximately 4 million workers, or
about 20 percent of the nation’s workforce, participated in some 3,000 strikes during 1919 alone, crippling
productivity and contributing to inflation and demobilization problems.
Determined to bring inflation down, the Woodrow Wilson administration and the semi-independent Federal
Reserve—just six years old in 1919—made what later economists considered several major mistakes. Believing
that federal borrowing dried up capital needed by private enterprise, the Wilson administration quickly moved to
shrink spending, putting many companies out of business and many workers out of a job. Meanwhile, convinced
that its previous low-interest, money-supply-expanding policies, while necessary to pay for the war effort, had
fueled inflation, the Federal Reserve raised interest rates, thereby tightening credit and slowing growth of the
money supply. The subsequent contraction was exacerbated by a drop in exports—from more than $13 billion in
1920 to less than $7 billion the following year—as the economies of many of the European belligerents were
restored and began producing the necessities once imported from the United States. While the administration’s
efforts did have the desired effect—a deflationary cycle wiped out most the gains in prices from the early postwar
years—they also led to a major contraction in industrial output and an unemployment rate of more than 10
percent.
The recession did not last long, however, and by late 1922 the economy was growing again. The trend continued
through virtually the end of the decade, with unemployment never rising above 4 percent and inflation almost
nonexistent. The numbers were impressive, with the economy as a whole expanding by about 40 percent, from a
gross national product (GNP) of $74.1 billion in 1922 to one of $103.1 billion in 1929. What explains this dramatic
expansion? Economists point to two key ingredients: innovation, both of the technological and managerial variety,
and the growth in aggregate demand, as consumption trumped savings and consumers went on a spending spree
for new and exciting products, such as the automobile.
 Innovation
Technological innovation undoubtedly played a key role in two ways. First, new technologies or, more precisely,
the application and expansion of existing technologies on a new scale, led to significant gains in productivity.
Expansion of the electricity grid—from 39 million kilowatts in 1919 to 97 million kilowatts in 1929—allowed industry
to switch from cumbersome steam engines to smaller, more efficient, more versatile electric motors. Expansion in
the number of telephone lines—from 30 million in 1920 to 70 million in 1930—allowed for better communication
within growing corporate enterprises, between businesses, and between businesses and consumers. And while
the 1920s is famous for its embrace of the automobile, less heralded was the growth in internal combustion–

driven trucks, a far more effective form of transportation than horse-drawn wagons and far more adaptable
geographically than railroads. Finally, many companies took a more active role in technological innovation rather
than waiting for independent inventors to come up with new ideas. By the late 1920s, more than a thousand U.S.
corporations had created research and development programs. The most famous of these was Bell Labs, which
American Telephone & Telegraph incorporated as a separate company in 1925.
Technological innovation also spurred demand, as industry introduced—or expanded production and lowered the
cost of—a panoply of new and highly desirable consumer products. Among them was a variety of electrically
driven household appliances, such as the radio. Between 1923 and 1930, approximately six of every ten American
households had purchased a radio set. Even more important to economic growth was the automobile. With Henry
Ford’s Model T leading the way, car purchases increased from about 2.2 million in 1920 to 5.3 million in 1929.
Such growth in aggregate demand provided a major engine of economic growth, allowing industries to take
advantage of economies of scale, thereby lowering prices and spurring new demand in what economists call a
“virtuous cycle.”
Yet as students of economic innovation also note, technological change is usually only half the explanation for an
upturn in the business cycle. New managerial practices, designated under the rubric “scientific management,” also
contributed to the gains in productivity that fueled the economic expansion of the 1920s. These included tighter
managerial control of workers, more on-the-job training, and the implementation of scientifically determined “best
practices” in the workplace, which improved output per worker on assembly lines and in the bureaucracies of
corporate headquarters. At the managerial level, companies—led by the innovative General Motors—began to
separate day-to-day management from long-term planning.
Meanwhile, the financial sector was innovating and expanding as well. Banks began to offer more affordable
mortgages, allowing more Americans to purchase their own homes, even as they helped finance new factories and
corporate headquarters. All of this helped fuel a boom in real estate and construction. The financial sector also
provided the funds for an unprecedented wave of corporate mergers. Indeed, no equivalent period produced more
mergers than the six years from 1924 through 1929—about 1,250 in all. Credit helped make the financial sector—
and Wall Street in particular—a major player in the nation’s economy, with bank assets rising from $48 billion to
$72 billion between 1919 and 1929, half of this controlled by the largest 1 percent of banks.
Government policy under successive pro-business Republican administrations and Congresses also contributed to
corporate growth. The government relaxed its efforts to break up large corporations, allowing for the rapid growth
in business consolidation, even as it cut taxes on the wealthy to free up funds for investment. The Commerce
Department under Secretary Herbert Hoover offered a number of more innovative ideas, particularly through its
efforts to allow companies to share information and collectively market their products at home and overseas—
practices once considered violations of antitrust laws. Washington also supported manufacturers with high tariffs
on imported goods.
 Uneven Growth
For all the wealth and exuberance it generated, the boom of the 1920s was neither a universal phenomenon nor
one built on the most solid foundation, as the stock market crash of 1929 and the Great Depression of the 1930s
proved. First, a number of key industries remained ailing throughout the decade. This included textile production in
the Northeast, as factories moved south to take advantage of cheaper labor; coal, which faced new competition
from oil and gas; and railroads, facing increasing competition from trucks and buses. But no sector suffered more
than agriculture, which still employed about one in four working Americans at the beginning of the decade. Having
greatly expanded output to meet wartime demand—often by going into debt—farmers faced a dramatic collapse in
commodity prices after the war. The prices never really recovered, and the farming sector was left in a state of
recession through the entire decade. Thus, while nonfarm yearly incomes averaged around $750 during the
decade, farm incomes hovered at one-third that level.
Discrepancies in income were not confined to the farm sector. Indeed, many Keynesian economists point to

stagnant wages and the rapidly growing inequality between rich and poor as a major reason why the Wall Street
crash, which directly affected only a small proportion of the population, caused a depression that affected every
corner of the American economy. Despite the enormous productivity gains of the 1920s, the vast majority of
workers saw their wages rise modestly if at all. According to a study released by the National Bureau of Economic
Research (NBER) in 1921, a typical American family of five (the average household was larger than in the twenty-
first century) required at least $2,000 a year in income for basic necessities. In 1919, approximately 70 percent of
American income earners fell below that mark. By 1929, that figure had only fallen to 60 percent, and the drop
occurred mostly among skilled workers, themselves a relatively small portion of the overall working-class
population. Among the unskilled, fully 42 percent had household incomes of less than $1,500. At the other end of
the income scale, meanwhile, the gains were dramatic. Between 1919 and 1929, the average income for the top 1
percent of American households rose some 75 percent. Their share of total national income rose from 12 to 34
percent, the largest ten-year increase in U.S. history. Likewise, household wealth was being distributed more and
more unequally. By 1929, the top 1 percent of Americans held more than 44 percent of the nation’s wealth, while
the bottom 87 percent held just 8 percent.
Several factors contributed to the growing inequality, including a drop in union membership, which undermined
wages; falling income tax rates on the rich; and the spectacular growth in the values of securities, most of which
were owned by upper-middle-class and wealthy households. In fact, the growth in stock values did not just
contribute to greater wealth among the rich; it also resulted from it. In other words, while much of the new wealth
accruing to the top households went into conspicuous consumption, a good deal of it also went into speculative
financing. Such investments drove stock prices spectacularly upward in the late 1920s, especially as the other
avenue for speculation—real estate—went into a slump after 1927. Between that year and the crash of 1929, the
average price of stocks rose 50 percent; high flyers such as RCA rose many times faster. Soon, not only the rich
were investing in corporate securities, so was the middle class. By the time of the crash, some 10 percent of
American households had invested on Wall Street. Encouraging this growth was a particularly dangerous form of
financing, whereby brokerage houses and investment banks offered loans to stock purchasers for up to 90 percent
of the value of the securities they were buying, collateralized by the ever higher valuations themselves.
It was a classic bubble, and it finally burst. The fall in asset values produced a liquidity crisis that reverberated
throughout the economy when confidence in the ability of individuals and institutions to pay back loans and credits
evaporated. The crisis in the financial markets might have remained largely confined there, much as during earlier
panics, had it not been for the underlying weaknesses of the “boom” of the 1920s.
James Ciment
 
See also:  Florida Real-Estate Boom (1920s);  Great Depression (1929-1933);  Stock Market
Crash (1929). 
Further Reading
Bernstein, Irving. The Lean Years: A History of the American Worker, 1920–1933. Boston: Houghton Mifflin, 1960. 
Best, Gary Dean. The Dollar Decade: Mammon and the Machine in the 1920s. Westport, CT: Praeger, 2003. 
Ciment, James, ed. Encyclopedia of the Jazz Age. Armonk, NY: M.E. Sharpe, 2008. 
Goldberg, David J. Discontented America: The United States in the 1920s. Baltimore, MD: Johns Hopkins University
Press, 1999. 
Leuchtenberg, William E. The Perils of Prosperity, 1914–1932. Chicago: University of Chicago Press, 1958. 

 
Boom, Economic (1960s)
 
In the popular imagination, the 1960s are remembered as a time of social upheaval—urban rioting, anti–Vietnam
War demonstrations, feminist protest. For those who study the intersection of politics and economics, however, all
of the protest seems incongruous for the times. Social upheaval, it is understood, tends to occur during times of
great economic upheaval—the industrial union movement and sit-down strikes of the Great Depression being a
classic example. But the 1960s were, arguably, the most prosperous decade in modern American history, with
virtually every economic index—from the Dow Jones Industrial Average (DJIA) to the number of people living
above the poverty line—showing remarkable gains.
 Extent
Before examining why the 1960s were so prosperous, a few words on the time frame and scale of the expansion
are helpful. According to most economists, the recovery from the mild recession of 1957–1958 marks the onset of
the great expansion, and the oil shocks and far deeper recession of 1973–1975 mark the end. Whatever the
specific time frame, the actual decade of the 1960s saw remarkable gains across the U.S. economy. Between
1960 and 1970, the gross national product nearly doubled from $503.7 billion to $976.4 billion in non-inflation-
adjusted dollars. Inflation remained largely in check until deficit spending to fund the Vietnam War sent it higher in
the late 1960s. The unemployment rate was cut in half, from just under 7 percent at the end of the recession to
3.5 percent in 1970, a trend made even more noteworthy by the fact that millions of women were entering the
workforce in these years. In 1960, about one-third of all adult American women worked; a decade later, the figure
was well over 40 percent.
The financial markets boomed as well, with daily shares traded on the New York Stock Exchange nearly
quadrupling from 3 million to almost 12 million. As late as the recession of 1957–1958, the DJIA had stood at just
under 420, up just 10 percent from its pre-Depression high of 381; by 1969, it had more than doubled to nearly
1,000. At the other end of the spectrum, the gains were equally remarkable, as the economic expansion, along
with a host of government antipoverty programs launched by President Lyndon Johnson, collectively dubbed the
“Great Society,” lowered the percentage of households living in poverty from 22.4 percent in 1959 to 12.6 percent
in 1970, the fastest and most dramatic recorded drop in American history.
In fact, the 1960s economic expansion represented the culmination of a much a longer growth period. Referred to
by economists as the post–World War II economic boom, the surge encompassed virtually the entire
noncommunist industrialized world, including Western Europe, Canada, Japan, and elsewhere. Indeed, the growth
rates of such high-flying countries as Italy and Japan outpaced that of the United States in these years. Between
1960 and 1970, Japan posted gross domestic product (GDP) gains of between 7 and 10 percent annually, raising
per capita income from about $6,000 to more than $14,500 in 1970 (or from 40 percent of the U.S. figure to 60
percent). In Italy, per capita income rose from about $9,600 in 1960 to just over $17,000 in 1973, a rise of about
75 percent. During the same period, U.S. per capita income rose from about $17,600 to about $26,000, an
increase of nearly 50 percent. (All figures, except where noted, are represented in 2008 U.S. dollars.) While
Europe and Japan experienced the same forces that propelled the U.S. economy—principally, pent-up aggregate

demand—they also prospered as they rebuilt and modernized economic infrastructure that had been devastated by
the war. U.S. government policy also helped, going beyond the massive foreign aid to Europe and Japan after the
war. As the dominant player in the global economy and international institutions, the United States promoted
international trade and a stable global financial system based on a strong U.S. dollar.
 Causes
What explains these remarkable numbers? Much of it had to do with international circumstances, innovation, and
government policy. The story begins with World War II. While Europe and Japan saw their infrastructures
devastated by the conflict, the United States remained largely untouched. Indeed, its industrial capacity expanded
dramatically as a result of the war effort. At the same time, deprived of consumer goods, Americans accumulated
some $140 billion in savings between 1942 and 1945. When the war ended, pent-up aggregate demand fueled a
consumer-led economic surge as Americans spent lavishly on new automobiles and household durables such as
washing machines, refrigerators, and television sets. Millions of Americans decamped from the cities to the
suburbs, and from the Northeast and Midwest to the South and West, fueling an unprecedented construction
boom.
The federal government contributed to the boom in two major ways. The GI Bill, 1944 legislation designed to help
more than 15 million men and women in uniform readjust to civilian life, provided government-backed mortgages,
making buying a home much more affordable for working-and middle-class Americans. Whereas about four in ten
families owned their own homes in 1940, more than six in ten did so in 1960. Meanwhile, the GI Bill paid other
dividends. By providing scholarships and low-interest loans for veterans to attend college, it greatly increased the
overall productivity of American workers. The second policy was the Interstate Highway Program, launched by
President Dwight Eisenhower in 1956, which not only connected suburbs to cities (and cities to other cities) with
high-speed, limited-access highways, but also fueled a major construction boom.
The government also played a more indirect role in the postwar boom period. Many of the scientific and
technological innovations of World War II—largely financed with government research money—were
commercialized in the 1950s and 1960s. Among the industries that took advantage of wartime innovation were
plastics, synthetic fibers, and aerospace. The latter, in particular, was aided by a massive expansion in defense
spending, as the United States ended its long tradition of maintaining a tiny peacetime military establishment.
Whereas defense spending represented 25 percent of much smaller federal government outlays in 1930—about
$50 billion in all—it commanded more than 40 percent in 1970, or about $700 billion, which went to the Vietnam
War effort.
The dramatic rise in per capita income during the 1960s allowed tens of millions of Americans, including many
blue-collar workers, to join the ranks of the middle class. But not only were members of the vast middle sector of
the U.S. economy seeing their incomes rise, their share of national income remained significantly higher in these
years than in any other period of the twentieth and twenty-first centuries. Whereas the top 1 percent of income
earners garnered 15 to 20 percent of all income in the 1920s (and the 2000s), the figure stood at about 10
percent throughout the postwar boom.
The more equitable distribution of income was the result of several factors, including a lack of competition from
other countries. Even as late as 1970, to take one important example, imported passenger cars (other than those
built by Canadian subsidiaries of U.S. corporations) represented just 13 percent of all new sales. Commanding the
largest market for cars in the world allowed U.S. auto manufacturers to pay high wages and offer plentiful
benefits, for the most part negotiated by a powerful union movement. Indeed, during the 1960s, union
membership in the United States was at or near record levels, with roughly one in three workers organized. High
union membership not only meant a higher percentage of national income going into the pockets of unionized
workers, but nonunionized workers also did well, since union pay scales set the standard in most industries. Thus,
as productivity rose dramatically, so did the income of the working and middle classes, fueling ever more
consumer demand.

Low-interest, government-backed mortgages for World War II veterans, standardized building techniques, new
highways, and growing families gave rise to single-family housing developments across America during the boom
of the 1960s. (American Stock/Hulton Archive/Getty Images)
 Supercharged Late 1960s and Fall of the 1970s
While the postwar boom began in the late 1940s, it became supercharged in the period between the end of the
1957–1958 recession and the oil crisis and persistent recession beginning in late 1973. Much of this was the
result of government policy. To help boost a somewhat sluggish economy in the early 1960s, President John F.
Kennedy proposed the largest tax cut as a percentage of national income in U.S. history, though it did not go into
effect until signed into law by his successor, Lyndon Johnson, in February 1964. The measure significantly
lowered tax rates on all income groups and corporations while liberalizing the rules on the depreciation of capital.
All of the extra after-tax income, corporate profits, and spending produced a sustained period of extraordinary
growth, as the nation’s GDP climbed from less than $3.6 trillion in early 1964 to $5.42 trillion in late 1973— a rise
of nearly 44 percent in one decade, more than economic growth during the 1980s and 1990s.
But it could not last. For even as the huge tax cut was going into effect, Johnson was expanding government
expenditures, both for the War on Poverty and, even more significantly, for the war in Vietnam. Fearing that
opposition to the war and his social programs would be fueled by raising taxes, Johnson instead allowed the
federal debt to grow and put pressure on the Federal Reserve to increase the money supply. This resulted in
rising levels of inflation through the late 1960s and early 1970s, which hit 5.7 percent in 1970. To deal with the
inflation, Johnson’s successor, Richard Nixon, instituted two major policy decisions in August 1971. The first was
a ninety-day wage and price freeze followed by a three-year period of regulated increases. The second was a
devaluation of the U.S. dollar. With inflation running high and the federal debt growing, foreigners were attempting
to convert their dollars into gold, creating unsustainable pressure on the value of the dollar. In 1973, the global
financial system that had been in place since the end of World War II, whereby currency values were pegged to
the U.S. dollar, was replaced by freely floating currencies for most nations.
The final nail in the coffin of the 1960s economic expansion was the oil shock of 1973–1974, whereby major Arab
oil producers raised their prices and cut supplies to the West as punishment for the latter’s support of Israel in the
1973 Arab-Israeli War. As a result, oil prices nearly quadrupled and, since oil was central to just about every facet

of the U.S. economy, the effect was widespread. The inflation rate climbed to more than 11 percent in 1974.
Meanwhile, economic growth slowed, stalled, and then slipped into negative territory through the mid-1970s, not
recovering significantly until after the deep recession of 1981–1982, when inflation was brought to heel. The
economic crises of the 1970s confounded governments around the world, as they combined inflation with slow
economic growth in a pattern that baffled the Keynesian economists who had driven government economic policy
making since the end of World War II.
James Ciment
 
See also:  Recession, Stagflation (1970s). 
Further Reading
Andrew, John A. Lyndon Johnson and the Great Society. Chicago: I.R. Dee, 1998. 
Karier, Thomas. Great Experiments in American Economic Policy: From Kennedy to Reagan. Westport, CT: Praeger, 1997. 
Matusow, Allen J. Nixon’s Economy: Booms, Busts, Dollars, and Votes. Lawrence: University Press of Kansas, 1998. 
Mayer, Thomas. Monetary Policy and the Great Inflation in the United States: The Federal Reserve and the Failure of
Macroeconomic Policy, 1965–1979. Northampton, MA: Edward Elgar, 1999. 
Booms and Busts: Cause and Consequences
 
The historical record reveals certain regularities in alternating periods of widespread economic prosperity and
poverty, expansion and recession, and boom and bust. But economists—and everybody else affected by the ups
and downs of the economy—have long wondered what explains this phenomenon. More recently, since the
financial crisis of 2008–2009 and recession of 2007–2009, economists and policy makers have been grappling
with the question of whether the crisis and recession were the natural consequences of the previous economic
expansion or whether they were simply the result of some bad business decisions.
Consideration of the causes and consequences of booms and busts demands a clear definition of just when these
booms and busts occur and what they look like, for it is the “stylized facts” or interpretations about sinusoidal, or
wavelike, deviations from trend measures of output, employment, wages, interest rates, and the like that provide
the focus of what is to be explained. To complicate matters, according to the twentieth-century American
economist Victor Zarnowitz, while “business expansions and contractions consist of patterns of recurrent serially
correlated and cross-correlated movements in many economic... activities,” these alternations in business
conditions are not really “cycles” since they involve no unique periodicities—their amplitude, scope, and duration
vary considerably over time. So the question of whether booms are even related to busts is an open one.
 Agricultural Versus Industrial Economies
Traditionally, and today, in predominantly agrarian-based market economies, fluctuations in income and
employment were and are directly linked to nature’s rhythm. Indeed, one of the earliest theories of booms and
busts traced an average eleven-year cycle of feast and famine of the harvest back to sunspots and solar flares,
which presumably affected climate and hence agricultural output.

In an industrially based capitalist market economy, one finds no such agreement on the cause of fluctuations.
Economists have long struggled to define, characterize, explain, and model the causes of booms and busts. There
is consensus only insofar as economists are agreed that profits, investment in inventories, plant and equipment,
and financial conditions—such as the availability of credit to support loans to businesses—are central to the
phenomenon.
But which causes which? Are booms caused by credit surpluses, and busts by credit crunches—that is, are these
surpluses and crunches the impulses that in turn cause volatile business investment to propagate the problems?
Or are credit conditions merely responding to and spreading the problems arising from the impulse of volatile
private investment?
Meanwhile, can government policies counteract this instability, or are government policies themselves a source of
the boom-and-bust phenomenon? To what extent are economists even observing objective movements in
business activity and not merely artifacts artificially induced by data measurement techniques? Does a boom
contain the seeds of the subsequent bust and vice versa, or is the boom and bust independent of the preceding
bust and boom, each the result of some other, independent cause?
Beyond the question of measurement, the challenge of understanding causes and consequences of economic
booms and busts is taken up by examining the relationship among profits, investment, and financial conditions,
and in turn, their relationship(s) to output, employment, and income. To sort it all out, theories attempt to tear
apart the impulse or precipitating cause that lies at the root of the boom and bust, from the propagation
mechanism that explains how it spreads to other parts of the economy, and from the factors that explain
persistence and the tendency for the boom and bust to drag on.
Theories of the business cycle that seek to explain the causes of booms and busts may be categorized into (1)
monetary (credit) theories that see financial problems disturbing the entire economy; (2) theories of real
disturbances that unbalance specific major sectors (such as oil shocks that affect the energy sector and
consequently the rest of an energy-dependent economy); and (3) the Keynesian theory of aggregate demand,
which attributes booms (and inflation) to excessive demand, and busts (depression or recession) to a collapse in
aggregate demand.
 Monetary Theories
Monetary theories of the boom and bust see a change in financial conditions as the impulse or precipitating factor,
and view the resulting real economic adjustments as propagating these effects throughout the economy. Theories
of the business cycle that focus on real disturbances unbalancing specific sectors often start, conversely, by
assuming that the impulse stems from changes in investment in a major sector of the economy or from
technological change, and that it is the financial system that transmits and spreads this disturbance.
In the early 1800s, it was recognized that the credit from which to finance investment played a central role in the
state of the economy. In a critical 1826 study, British economist and philosopher John Stuart Mill offered a theory
of “commercial crises” that tied the phenomenon of recession and crises to the contraction of credit in the United
Kingdom. It was not until the turn of the next century that credit theories of crises evolved into monetary theories
of the business cycle with endogenous or self-reinforcing components explaining movements in output, income,
and prices. In his 1913 book Good and Bad Trade, British economist Ralph Hawtrey explored the possibility that
expansions of bank loans lowered the cost of loans, which in turn stimulated the demand for, and then the supply
of, capital goods (plant and equipment) that businesses typically borrowed money to buy. Since the expansion
process would be restricted by a limit on the ability of banks to increase loans, eventually the expansion process
of the boom would reverse itself and cause a bust. In credit theories, then, the cause of the boom lies in an
expansion of credit available for loans; the cause of the bust lies in the exhaustion of that credit and is ensured by
the limits imposed by the availability of monetary reserves.
The Post Keynesian American economist Hyman Minsky, basing his analysis on changes in aggregate demand,

believed that bank credit used to finance investment was the source of business fluctuations, but he focused
attention explicitly on the growing mismatch of investment income and related debt obligations over the course of
the boom as the source of an inherent fragility. Minsky described how a prolonged economic expansion
encourages investors to replace their expectations of a normal business cycle (incorporating an expected
recession) with the expectation of perpetual expansion. The new and (unreasonably) confident expectations of
continuing profits in turn encourage businesses to take on greater debt loads, which over time decrease financial
stability. While funds borrowed are often fixed in nominal, or non-inflation-adjusted terms, profits fluctuate. In a
boom, profits are high and rising; in a downturn, profits fall. If businesses borrow so much that they are unable to
make the payments on their debt when profits weaken, businesses go bankrupt and the economy descends into
recession and possibly crisis.
In addition to the increasingly optimistic expectations of Minsky’s investors, there is the vexing problem of liquidity
(where the easiness of selling/buying an asset can be a good thing for encouraging investment but can cause
greater instability in asset prices) and asset pyramiding (where the capital gains from an asset price increase
provide the base from which to demand more assets and drive the price up even further). In combination, these
financial market factors serve to increase the fragility of the boom and risk a more severe bust.
When economists complicate matters further by considering the possibility that investors often get caught up in
waves of optimism, they look at a market “bubble” as the explanation of the boom. A bubble is said to occur when
investors are so optimistic that they bid asset prices up on the enthusiasm of getting rich by buying low and selling
high, and forget to pay attention to the real economic prospects of what they are buying. But such a situation
cannot go on forever. At some point, the bubble bursts and investors react by becoming pessimistic. At this point,
the economy may go bust. This scenario is most likely to occur when the changes in the underlying real economy
are new (as when a technological innovation results in restructuring large parts of the economy). While such
changes played a role in the collapse of technology stocks in 2000, the primary reason for the failure was over-
investment in the sector.
Asset values generally and bubbles specifically are often the product of emotion as much as anything else, as
economists and pioneering investment analysts Benjamin Graham, David Dodd, and Sidney Cottle noted in their
influential 1962 analysis of securities values. Brenda Spotton Visano later demonstrated how what others think and
do influences investor behavior in periods when investors are particularly uncertain about the future. In works from
the early 2000s, she also explored the role of emotions in creating the panic that emerges when the bubble
bursts.
In short, financial panics appear when investors, fearing a loss in the value of their investments, dump their assets
and in so doing cause the collapse of a bank or stock market. Fear that a bank will fail can cause depositors to
rush to withdraw their funds. Fear that a stock market will crash can cause investors to sell their shares. And then,
in a self-fulfilling way, the large and sudden withdrawal of cash and sale of shares can, by themselves, cause the
very collapse that was feared. A bank run or a stock market crash increases the risk of an economic crisis. The
breakdown of a market or the credit crunch caused by the failure of a bank can adversely affect the day-to-day
operations of business in other sectors of the economy. As American monetary economists Milton Friedman and
Anna Schwartz suggest in their explanation of the Great Depression, financial panics can precede and cause
economic crises in employment, production, and trade.
 Disturbances as Causes
Theories that take instead real economic disturbances as the precipitating factor mostly focus on investment-
related disturbances as the root cause of booms and subsequent busts. Whether the predominant investment
variable is inventory, fixed investment, or technological change, however, relates closely to the “typical” cycle one
is looking at. In a 1923 study, British statistician Joseph Kitchin, for example, identified a 3-to-5-year business
cycle corresponding to the observed fluctuations in business inventories. Clément Juglar’s 7-to-11-year cycle,
developed in the late nineteenth century, corresponded closely with theories related to fixed investment, while

Soviet economist Nikolai Kondratieff identified a 45-to-60-year cycle corresponding to a long technological wave
in his pathbreaking 1928 study. Meanwhile, in his 1939 theory of business cycles, Austrian school economist
Joseph Schumpeter developed a notion that there are interrelated cycles with credit conditions layered on top of a
boom caused ultimately by overinvestment in an original technological innovation.
Whereas Schumpeter developed a theory focused on explaining the booms by investment and technological
advancement, John Maynard Keynes of Great Britain, the dominant economist of the first half of the twentieth
century, and others focused on developing a theory that would attribute the cause of the bust to deficient
consumption demand. While Keynes and Schumpeter shared a belief in the importance of the investment stimulus
to explain the boom, Keynes in particular emphasized an aggregate demand deficiency combined with
complicated labor and capital market problems as the cause of protracted periods of labor unemployment
prevalent in a bust.
There is some debate over whether Keynes’s theory of general (un)employment was adequate as a theory of the
business or trade cycle. Keynes himself saw his theory of employment as adequate to explain business
fluctuations. In his landmark work, The General Theory of Employment, Interest and Money (1936), he wrote,
“Since we claim to have shown in the preceding chapters what determines the volume of employment at any time,
it follows, if we are right, that our theory must be capable of explaining the phenomena of the Trade Cycle.”
John Hicks, a follower of Keynes, disputed the relevance of the latter’s theory for such a purpose. “Keynesian
economics, in spite of all that it has done for our understanding of business fluctuations,” wrote Hicks in A
Contribution to the Theory of the Trade Cycle (1950), “has beyond all doubt left at least one major thing quite
unexplained; and that thing is nothing less than the business cycle itself.... For Keynes did not show us, and did
not attempt to show us, save by a few hints, why it is that in the past the level of activity has fluctuated according
to so definite a pattern.”
The multiplier mechanism of Keynes and others, whereby changes in investment having a multiplicative effect on
output and income, paired with French economist Albert Aftalion’s accelerator principle, whereby an expectation of
future demand serves as an independent investment stimulus, formed the basis of “multiplier-accelerator” models
of business cycles, as American economist Paul Samuelson noted. Multiplier-accelerator models made explicit in
a simple way the idea that Keynes had in mind.
Subsequent developments of Keynesian-type theories of the causes and consequences of booms and busts
spread out in a few different directions. “Old” Keynesians such as American James Tobin focused on an
investment impulse stemming from the relationship between the cost of new capital and the stock market’s
valuation of the firm: as the share price of the firm rises relative to the cost of investing in plant and equipment,
firms expand and that spending contributes to the boom. “New” and “neo” Keynesians seek to explain nominal
frictions and real rigidities by exploring the microfoundations of slow-to-adjust labor markets, for example. When
the demand for investment goods and consumer durables fluctuates, firms adjust only slowly and imperfectly,
which then explains alternating periods of high and low employment. Post Keynesians examine the role of money
in economic decision-making, the cost of making economic decisions in a world where future economic conditions
cannot be predicted, and the reality that a degree of nonstructural unemployment and inequitable distributions of
income are an intrinsic part of modern entrepreneurial economic systems.
 Benefits and Costs
All of these theories, however, address only the economic causes and consequences of the boom and bust.
During a boom period, one sees higher incomes, more employment, and possibly higher prices; during a bust,
one sees lower national income, higher unemployment, and lower prices. Thus, accordingly, the Great Depression
can be viewed as a sharp downturn on a graph. What all of these theories omit are the considerable human
benefits and costs of both booms and busts. The consequences of a low income or job loss can be socially and
personally devastating. Such consequences are not measured by typical economic data and so escape
consideration when economists talk about booms and busts.

Yet these consequences are very real in human terms and in many ways more important than the abstract issues
discussed by economists. If the consequences were clearly separate and apart from all economic considerations,
then one could argue that theorists and policy makers need only solve the economic problem. If society could find
a way to avoid extreme busts, at least, the social and human damage would be avoided. While many economists
are confident that this is indeed the case, others believe that attention to those who suffer and how much they
suffer should be the impetus for policies designed to lessen the dire effects of the bust. It is not enough to say
that the government should step in to bail out a failing business; if a government cannot bail out all of them, how
should it decide which ones will benefit from its largesse?
A fundamentally different approach to causes and consequences of booms and busts has its roots in the 1930s
work of the Norwegian Ragnar Frisch and the Russian Eugen Slutsky, who demonstrated the possibility of
inducing “cycles” by the filtering process used to de-trend random observations. This statistical artifact paved the
way for viewing booms and busts as the result of random real shocks to productivity originating from technology
(such as innovations in information and communications brought on by computing technology) or from the energy
and environment (such as an oil price shock).
The real business-cycle theories of economist and Federal Reserve Bank president Charles Plosser and others
effectively challenged previously held views that booms and busts were the manifestation of cyclical phenomena to
be explained by theories with endogenous, self-perpetuating components. In Plosser’s view, the financial system
—having much in common with earlier classical models of the business cycle—is, like money, merely a veil on the
real economy. Booms and busts are real economic phenomena that occur independently of the financial sector
and are manifestations of the market economy adjusting to a new real economy. Government policies designed to
stabilize the economy actually produce detrimental effects, so the proponents argue, since the policies are too
imprecise and difficult to implement.
Like the 1930s bust in the European and American economies, the global financial crisis of 2008–2009 challenges
this extreme new classical view most profoundly. It may be that those who adhere to real shocks as the root
cause of booms and busts remain unconvinced that the source of the latest problem was anything but another
real shock. At a minimum, however, it will now be difficult to deny that the financial system plays a critical role in
the propagation of those shocks, as evidenced by the economic devastation measured in terms of output and jobs
lost, together with the commensurate increases in social hardship.
Brenda Spotton Visano
 
See also:  Austrian School;  Behavioral Economics;  Catastrophe Theory;  Classical Theories
and Models;  Fisher’s Debt-Deflation Theory;  German Historical School;  Institutional
Economics;  Keynesian Business Model;  Kondratieff Cycles;  Marxist Cycle Model;  Minsky’s
Financial Instability Hypothesis;  Monetary Theories and Models;  Neoclassical Theories and
Models;  Neo-Keynesian Theories and Models;  Over-Savings and Over-Investment Theories of
the Business Cycle;  Political Theories and Models;  Post Keynesian Theories and Models; 
Real Business Cycle Models;  Seasonal Cycles;  Sunspot Theories. 
Further Reading
Friedman, Milton, and Anna Jacobson Schwartz. A Monetary History of the United States, 1867–1960. Princeton,
NJ: Princeton University Press, 1963. 
Frisch, Ragnar.  “Propagation Problems and Impulse Problems in Dynamic Economics.” In Economic Essays in Honor of
Gustav Cassel. London: George Allen & Unwin, 1933. 
Graham, Benjamin, David Dodd, Sidney Cottle, and Charles Tathum. Security Analysis: Principles and Technique.  4th ed.
New York: McGraw-Hill, 1962. 

Hawtrey, Ralph G. Good and Bad Trade: An Inquiry into the Causes of Trade Fluctuations London: Constable, 1913. 
Hicks, John R. A Contribution to the Theory of the Trade Cycle. Oxford, UK: Clarendon Press, 1950. 
Keynes, John Maynard. The General Theory of Employment, Interest and Money. London: Macmillan, 1936. 
Kindleberger, Charles P. Manias, Panics, and Crashes: A History of Financial Crises. New York: Basic Books, 1978. 
Kitchin, Joseph.  “Cycles and Trends in Economic Factors.” Review of Economic Statistics 5(1923): 10–16. 
Kondratieff, Nikolai D. The Long Wave Cycle,  trans. Guy Daniels. New York: Richardson and Snyder, 1984. 
Mill, John Stuart.  “Paper Currency and Commercial Distress.” In The Collected Works of John Stuart Mill. Vol. 4. Essays of
Economics and Society, Part I, ed. John M. Robson. Introduction by Lord Robbins. Toronto: University of Toronto
Press, 1967. 
Minsky, Hyman. Can “It” Happen Again? Essays on Instability and Finance. Armonk, NY: M.E. Sharpe, 1982. 
Samuelson, Paul A.  “Interactions Between the Multiplier Analysis and the Principle of Acceleration.” Review of Economics
and Statistics 21:2 (May 1939): 75–78. 
Schumpeter, Joseph A. Business Cycles: A Theoretical, Historical, and Statistical Analysis of the Capitalist Process.  2 vols.
New York: McGraw-Hill, 1939. 
Slutzky, Eugen.  “The Summation of Random Causes as the Source of Cyclic Processes.” Econometrica 5:2 (April
1937): 105–146. 
Tobin, James.  “A Dynamic Aggregative Model.” Journal of Political Economy 63:2 (April 1955): 103–115. 
Zarnowitz, Victor.  “Recent Work on Business Cycles in Historical Perspective: A Review of Theories and Evidence.” Journal
of Economic Literature 23:2 (June 1985): 523–580. 
 
 
Booms and Busts: Pre–Twentieth Century
 
Human society has always been subject to the rise and fall of fortunes. For the tens of thousands of years of pre-
history, most human communities lived on the edge of survival. If adequate rains fell at the right time, there was
wild food to gather and animals to hunt. If not, there was famine. The rise of agriculture during the late Neolithic
Period, or New Stone Age, mitigated the vagaries of nature by allowing for a greater abundance and predictability
of food supplies and for the provisioning of larders in times of plenty against times of want. Still, climate could
undo the best-laid plans. Prolonged drought could disrupt even advanced civilizations, as appears to have been
the fate of the Anasazi in the American Southwest. The Anasazi built complex societies and sophisticated
architecture around the turn of the second millennium CE, only to disappear two centuries later as the result of an
extended drought and the social unrest it triggered.
 Rise of Civilization

The development of large-scale irrigation also helped remove human society from the immediate effects of climate,
though this did not seem to help the Anasazi. By ensuring a steady source of water, civilizations that mastered the
complex engineering of irrigation could further shield themselves from fluctuations in food supplies. But the
development of large-scale irrigation was premised on the existence of a centralized government authority that
could build and maintain such projects through forced labor and taxes. As civilizations grew more complex, they
became more interdependent, with urban dwellers depending on the surplus produced by farmers and rural folk
depending on the central government to ensure social peace and infrastructure maintenance.
But even civilizations that had largely overcome climatic uncertainty could experience rises and falls based on
environmental factors. In cases as diverse as the Maya of Mesoamerica and the ancient inhabitants of Easter
Island (Rapa Nui) in Polynesia, the archaeological consensus is that these civilizations went into periods of
prolonged decline when they had grown beyond their local environment’s capacity to support them or had caused
their local ecosystems to collapse. In addition, with increasing complexity came greater fragility of the social
authority. When central authority became weak, the social structure often frayed, leading to civil unrest, anarchy,
and disruptions of internal trade. Chinese history, to take one example, was marked for thousands of years by
alternating periods of effective government and economic prosperity followed by weakened central government,
social chaos, and economic want.
In the West, the rise of Roman authority led to an unprecedented period of economic expansion and prosperity
throughout the Mediterranean world and much of Western Europe from the late first millennium BCE to the middle
of the first millennium CE. The collapse of the Roman Empire in the fifth century, however, plunged much of the
region into what is known as the Dark Ages, a roughly 500-year period marked by a near cessation of trade, a
decline in manufacturing, the collapse of cities, and the loss of critical technical skills.
 “External Shocks”
Scholars refer to events that arise from outside the existing economic order as external shocks. For Europeans,
two external shocks ushered in the modern era. Each resulted in the rise and fall of fortunes for all social classes.
The first was the so-called Black Death of the mid-fourteenth century, in which about one-third of the continent’s
population succumbed to the bubonic plague. While the immediate impact on the people of Europe was
devastating, most historians have since concluded that the episode had beneficial long-term economic effects.
Much of Europe was overpopulated when the plague hit, which limited the productivity of land, kept laborers in
poverty and subjugation to lords, and perpetuated an unequal distribution of wealth. Those who owned preciously
scarce land—a class of people invested in the economic status quo—retained the lion’s share of economic power.
The reduction in population as a result of the plague shifted power and wealth from the landlords to laboring and
trading classes, resulting in higher wages, greater mobility, more freedom of movement, greater demand, and
more internal trade. Some economic historians have even argued that the Black Death was a contributing factor to
the advent of the capitalist economic system in early modern Europe.
The second great external event of the middle centuries of the second millennium BCE was the European
“discovery” of the Americas. For Spain, the nation that conquered most of the New World in the first century after
Columbus, the influx of vast quantities of precious metals from mines in Mexico and South America created new
wealth, which led to a steady increase in population. But this expansion led to increased demand for products that
the economy—hurt by the crown’s decision to expel Jews and Moors, key businessmen and craftsmen,
respectively—could not provide, leading to inflation. With costs rising, Spanish goods could not compete with
those from other parts of Europe, leading to an eventual decline of the Spanish economy and its eclipse by those
of Northern Europe.
Even in the latter areas, however, inflation eroded the wealth of those who lived off fixed rent income, such as
landlords. For those who borrowed money, particularly middle-class merchants and craftspeople, the effects of
inflation were positive, since they could pay back their loans in depreciated currency. As with the Black Death,
many economic historians believe that the inflation caused by the influx of precious metals from the Americas had

an immediate devastating impact—particularly on the poor—but offered long-term economic benefits in that it
undermined the power of conservative landlords in favor of more enterprising and innovative merchants and
artisans.
 Early Modern Capitalism and Speculation
Natural catastrophes, environmental degradation, the collapse of central authorities, shocks like the Black Death
and the conquest of the Americas—all of these factors came from outside the existing economic order. It is only
with the modern capitalist order, which began to emerge in Europe early in the second half of the second
millennium CE, that can one speak of a business cycle of expansion and contraction primarily driven not by the
external factors of natural forces and governance issues—though both would continue to play a critical role in such
cycles, the latter into the present era—but by economic forces inherent in capitalism itself.
Still, until the late eighteenth and early nineteenth centuries, the dynamic forces of capitalism—the “creative
destruction,” as Austrian school economist Joseph Schumpeter later put it—had yet to have a major impact on the
economic lives of most people. Economic trends were long-term and often little felt beyond those classes with
money to invest. Most people earned their livelihood through agriculture—consuming most of what they produced
—and participated only tangentially in the larger commercial economy. This norm for much of the world, even into
the nineteenth and twentieth centuries, was also true for residents of Europe prior to the industrial and commercial
revolutions of the late eighteenth and early nineteenth centuries, despite its being the most economically advanced
continent in the early modern era.
Not surprisingly, then, the earliest episodes of boom and bust in Western history were confined to speculators.
These included the Mississippi and South Sea bubbles of early eighteenth-century France and England,
respectively, and, the most infamous of all, the tulipmania episode of early seventeenth-century Holland, all highly
speculative episodes that made and destroyed great fortunes in a matter of a few years and even months. While
tulipmania, involving speculation in exotic tulip bulbs and tulip bulb fortunes, was driven by market forces alone,
the Mississippi and South Sea bubbles involved quasi-governmental enterprises—one dealing in North American
lands and the other South American trade—indicating the large role government played in these mercantilist, early
capitalist economies. And, as with the tulip episode, the financial impacts of the South Sea and Mississippi
bubbles were largely confined to the upper reaches of society, barely affecting the overall national economy and
causing no more than a ripple in the lives of ordinary farmers and workers.
Speculation in tulips became so frenzied in Holland during the mid-1630s that buyers were paying tens of

thousands of florins—or storehouses of food and grain—for a single exotic bulb. A purely market-driven bubble,
“tulipmania” burst suddenly in 1637. (The Granger Collection, New York)
 Commercial and Industrial Revolutions
It is only with the advent of the commercial and industrial revolutions in the late eighteenth and early nineteenth
centuries that one can speak of the modern business cycle. A complicated phenomenon occurring over centuries,
the commercial revolution essentially involved the integration of larger and larger sectors of the populace into the
cash economy, selling their labor or crops on the open market in exchange for money that could be used to
purchase food in the case of urban dwellers, and manufactured goods and imports and commercially grown
foodstuffs such as sugar and tea in the case of both urban and rural households.
The industrial revolution, of course, involved the harnessing of new forms of energy and technology—along with
new methods of organizing production—to mass-produce goods for the capital and consumer markets. Both the
commercial and the industrial revolutions depended on innovation in the agricultural sector, new forms of
transportation, new sources of capital—much of it generated by colonies and slaves abroad—and ever more
sophisticated financial markets to make that capital available where it was needed.
But while these twin economic revolutions created ever-greater prosperity—particularly for a rising middle class—
they also produced more short-term economic volatility. In pursuit of growth, entrepreneurs competed more
intensively with one another, driving down profits and eventually wages, leading to a fall in demand that further
eroded revenues and profits. Ultimately, economies would fall into recession, until new demand stimulated new
growth, leading to a period of economic expansion.
Arguably the first instance of a modern boom-and-bust cycle in U.S. history came with the economic expansion
that followed the end of the Napoleonic Wars in Europe and the War of 1812 in North America, with the
subsequent Panic of 1819. Economic historians disagree over the causes of this cycle, with the more Keynesian-
minded calling it a classic example of falling profits leading to a decline in wages and falling demand. These
forces, in turn, are said to have produced the economic downturn that began in 1819, although Keynesian
economists do note that the expansionary monetary policies of the nation’s nascent credit markets played a role in
creating speculative excesses—particularly in the real-estate market—that resulted in the panic. In contrast,
monetarists put the lion’s share of blame for the speculative boom, and the bust that inevitably followed, on the
heads of central and commercial bankers who pumped too much money into the economy.
 Increasing Financial Integration and Economic Volatility
Ultimately, the Panic of 1819 was short-lived and did not greatly affect those who had not participated in the
speculative excesses of the preceding four years, although higher rates of unemployment could be felt among
urban workers and declining agricultural prices had an impact on the farm households. For the next seventeen
years, however, the U.S. economy prospered, buoyed by investment in transportation, the beginnings of
industrialization, and the great expansion of commercial crop production, particularly of slave-grown cotton. By the
1830s, the nation’s economy was far more integrated, both internally and to markets in Europe. This was
especially the case with financial markets, as U.S. economic growth and prosperity had come to depend on large
infusions of foreign capital—especially from Britain—much in the way that emerging markets depend on flows of
capital from the developed world in today’s global economy.
With greater integration came increasing vulnerability to economic shocks from abroad. In addition, as ever more
Americans entered the commercial economy—as farmers producing crops for sale on the national market and
urban laborers and artisans selling their skills, brain, and brawn for wages—they became more dependent on the
smooth running of financial markets. When the Bank of England decided to curtail investment due to worries that
the American economy was becoming overheated (as evidenced in a massive real-estate bubble in Western
lands) the U.S. financial markets seized up, leading to a tightening of credit that set off a wave of bankruptcies,

layoffs, falling agricultural prices, and collapsed real-estate values. The resulting downturn would drag on for about
six years, in the worst depression experienced by the U.S. economy prior to the Civil War. By comparison, the
Panic of 1857—triggered by a flight of British capital from U.S. banks—was relatively mild and short-lived. In both
cases, there were indigenous causes as well, including the bubble in real-estate prices and prices for agricultural
and manufactured goods.
 Laissez-Faire Economics and Late-Nineteenth-Century Capitalism
By the post–Civil War era, the United States—while still heavily dependent on foreign sources of capital—had
developed large-scale financial markets of its own. Both the Civil War and the boom that followed the cataclysm
resulted in the creation of great fortunes and the rise of large-scale enterprises and corporations, particularly in
the railroad sector. While much of this expansion was based on real economic need—the nation was expanding
westward, rapidly industrializing and urbanizing, and once again drawing in millions of new immigrants—a great
deal of the investment was speculative in nature, much of it the result of a rapidly expanding money supply. When
a number of major financiers became overleveraged in their efforts to create a second transcontinental railroad—
causing a collapse of their banking house in 1873—it triggered a financial panic that once again saw credit
markets dry up, bankruptcies spread, agricultural prices fall, and unemployment among urban workers soar.
A Thomas Nast cartoon titled “The Long and Short of It” comments on the U.S. bank panic of 1873. A result of
overspeculation in the railroad industry, the crisis marked the beginning of a long period of instability and,
eventually, depression. (The Granger Collection, New York)

The 1873 financial crisis and the multiyear depression that followed began a period of extreme volatility in the
history of American and global capitalism—one that would culminate in the worst economic downturn of pre-
twentieth-century history, the depression of the 1890s. (There was a milder panic in 1884 as well, which also
triggered a recession in the middle years of that decade.) Like the financial crisis of 1873, the Panic of 1893 was
triggered by overinvestment and speculative financing in the railroad industry, which in turn set off a crisis in the
credit markets and a prolonged recession that gripped the nation for several years. Economists grappled with the
cause of the volatility in late-nineteenth-century capitalism, with some citing too much production and others too
little consumption, though most agreed that lax credit and investment practices played a key role. Whatever the
reason, business leaders responded to the volatility by attempting to insulate their businesses through the creation
of trusts, holding companies, pools, and other organizational structures that reduced cutthroat competition.
While such efforts might have achieved stability within a specific company or even a whole industry, they could
not address the overall problem of economic volatility. As events of the twentieth century would prove, only the
central government had the means to smooth out the business cycle. But accepting that idea meant dispensing
with the extreme laissez-faire thinking at the heart of late-nineteenth-century American capitalism. At the dawn of
the twentieth century would arise a movement ready to do just that and begin a new economic era, one in which
government played an ever more active—and, to detractors, intrusive—role in the economy.
James Ciment
 
See also:  Mississippi Bubble (1717-1720);  Panics and Runs, Bank;  South Sea Bubble (1720); 
Tulipmania (1636-1637). 
Further Reading
Appleby, Joyce. The Relentless Revolution: A History of Capitalism. New York: W.W. Norton, 2010. 
Beaud, Michel. A History of Capitalism, 1500–2000,  trans. Tom Dickman and Anny Lefebvre. New York: Monthly Review
Press, 2001. 
Bernstein, William J. The Birth of Plenty: How the Prosperity of the Modern World Was Created. New York: McGraw-
Hill, 2004. 
Cameron, Rondo, and Larry Neal. A Concise Economic History of the World: From Paleolithic Times to the Present.  4th ed.
New York: Oxford University Press, 2003. 
Ferguson, Niall. The Ascent of Money: A Financial History of the World. New York: Penguin, 2008. 
Gamble, Andrew. The Spectre at the Feast: Capitalist Crisis and the Politics of Recession. New York: Palgrave
Macmillan, 2009. 
Gills, Barry K., and William R. Thompson, eds. Globalization and Global History. New York: Routledge, 2006. 
Heilbroner, Robert L., and William Milberg. The Making of Economic Society.  12th ed. Upper Saddle River, NJ: Pearson
Prentice Hall, 2008. 
Landes, David S. The Wealth and Poverty of Nations: Why Some Are So Rich and Some So Poor. New York: W.W.
Norton, 1999. 
Maddison, Angus. Contours of the World Economy, 1–2030 AD: Essays in Macro-Economic History. New York: Oxford
University Press, 2007. 
McCraw, Thomas K., ed. Creating Modern Capitalism: How Entrepreneurs, Companies, and Countries Triumphed in Three
Industrial Revolutions. Cambridge, MA: Harvard University Press, 1997. 
Moore, Karl, and David Lewis. The Origins of Globalization. New York: Routledge, 2009. 
Polanyi, Karl. The Great Transformation: The Political and Economic Origins of Our Time.  2nd ed. Boston: Beacon

Press, 2001. 
Pomeranz, Kenneth, and Steven Topik. The World That Trade Created: Society, Culture, and the World Economy, 1400 to
the Present. Armonk, NY: M.E. Sharpe, 2006. 
Stearns, Peter N. Globalization in World History. New York: Routledge, 2009. 
Stearns, Peter N. The Industrial Revolution in World History. Boulder, CO: Westview, 2007. 
 
 
Booms and Busts: Twentieth and Twenty-First Centuries
 
The advent of capitalism in the early modern era, and the industrial and commercial revolutions, which
transformed the economies of Europe and North America from the late eighteenth through the late nineteenth
centuries, created a material abundance for the masses unknown in previous human history. At the same time,
these developments brought the economic volatility of the business cycle, the booms and busts that lifted and
broke individual and business fortunes on a recurring basis. Throughout the nineteenth century and particularly in
its last few decades, the economies of the United States and Europe were periodically rocked by financial panics
and recessions, some of them, such as those of the 1870s and 1890s, deep and lasting.
For the most part, economists, business leaders, and government policy makers of the day believed that the
volatility of the business cycle, while regrettable, was an inevitable part of capitalism and long-term growth.
Proponents of the classical school of economics argued that such booms and busts are, in fact, anomalies and
that capitalist economies tend toward equilibriums of low unemployment and inflation. Any actions on the part of
the government to smooth out the business cycle or to ease the effects of recession through fiscal or monetary
means, they argued, are only likely to distort the natural workings of the marketplace, thereby prolonging
recessions or blunting recoveries. Instead, business leaders moved to insulate their industries from the boom-and-
bust cycle through consolidation and coordination, whether in the form of pools, holding companies, or trusts.
But while the economic history of the nineteenth century was marked by the full flowering of laissez-faire thinking
and practice—and all of the economic dynamism and volatility that brought—that of the twentieth and twenty-first
centuries would be one in which governments—backed by new economic thinking—would increasingly intervene in
the marketplace in an effort to smooth out the business cycle and to ensure maximum sustainable growth along
with low unemployment and inflation.
 Progressivism and the Federal Reserve
In the United States, the first pivotal event in this process was the Panic of 1907. Like several late-nineteenth-
century episodes of chaos in the credit markets, this first major financial panic of the twentieth century was
triggered by the activities of speculators. These activities caused key financial institutions to fail, which in turn set
off a credit crunch and stock market crisis that threatened to drag the economy into recession. To counteract the
panic, an ad hoc consortium of major New York banks, led by financier J.P. Morgan, moved to shore up securities

prices and the credit markets with an infusion of $100 million, an extraordinary sum for the day.
While the effort succeeded in averting a serious economic downturn, the episode concerned policy makers and
leading financiers. The credit market was simply growing too big for any one private financial institution, or even
group of financial institutions, to effectively deal with large-scale crises; the government, even many bankers had
concluded, must play a role in stabilizing the financial markets. At the same time, a new political movement had
emerged in the country in the early twentieth century—Progressivism, which argued in favor of an increasing role
for government in resisting the economic power of corporations and ensuring competitiveness in the marketplace.
While coming from very different ideological origins, both schools of thought held that not only did the government
have a useful role to play in the marketplace but that it was the only institution with the power and resources to
assure the smooth running of the economy.
Out of this new economic thinking came the Federal Reserve System—the central bank of the United States—
created in 1913 by Progressive-minded President Woodrow Wilson and a Progressive-dominated Congress. Like
the central banks of other countries, the Fed, as it came to be called, was given broad power over two facets of
the economy—regulating banks and setting monetary policy. The Fed, of course, was not the first of its kind.
Several European countries—most notably, Great Britain with its Bank of England—had had central banks for
centuries. The United States itself had had a version of a central bank—the Bank of the United States—for
several decades in the early nineteenth century, until the institution was killed by populist politicians and free
financial market advocates. Nearly a century later, there was still a great deal of hostility to the idea of a central
bank, particularly in less-developed areas of the country that depended on easy credit for their growth. The fear
there was that a central bank would serve the anti-inflationary, tight money interests of northeastern financial
interests. Such fears forced a compromise—the Fed would be a decentralized central bank, with great power
residing in the directors and presidents of the twelve local branches.
 World War I and the Roaring Twenties
While such decentralization assured that the Fed would be more responsive to interests beyond those of New
York bankers, it also contributed to the institution’s less-than-effective record during the first several decades of its
existence, contributing both to the speculative excesses in the real-estate and securities markets of the 1920s and
to the economic contraction of the early 1930s.
But there were other factors beyond Fed policy that contributed to the economic volatility of the roughly two
decades between World Wars I and II, both in the United States and globally. The first of these great conflicts had
a transformative effect on the economies of both Europe and the United States. Remaining out of the war for its
first three years and enjoying great prosperity supplying the Allies with food, materiel, and armaments, the United
States became the leading creditor nation in the world as a result of the conflict, with allied countries such as
France and Britain going deep into debt to U.S. financial institutions. With the Allied victory in 1918, both London
and Paris imposed heavy reparations payments on Germany, because they blamed it for starting the war. An
unstable international financial order fell into place, as German reparations payments helped France and Britain
pay their debts to the United States, which then bolstered the German economy with loans after the latter had
tried to get out of its reparations payments by devaluing its currency through hyperinflation.
Meanwhile, the United States, flush with capital, went on a speculative binge in the 1920s, first in the overheated
real-estate market and, more spectacularly, in the latter years of the decade, in a soaring stock market. While
some of the increases in securities prices were justified by improvements in productivity and rising corporate
profits, much was the result of new forms of financing, in which stock purchasers could buy shares on the margin
(paying only a fraction of what the stock cost and borrowing the rest), using the stock as collateral. As long as
stock prices were rising, the system worked. But when prices began to fall in 1929, the whole system came
crashing down, with shareholders unable to meet their margin calls. The stocks were then sold because the
margin calls were not met, causing stock prices to fall even further. Brokers were not able to pay off their credit
lines to banks, thus further jeopardizing the solvency of the affected banks.

 The Great Depression and Rise of Keynesian Economics
While the Great Wall Street Crash of 1929 is associated with the start of the Great Depression—the worst
economic downturn in the history of capitalism—there were other reasons why falling securities prices, which
affected the portfolios of only one in ten Americans, soon brought the U.S. and global economies to their knees.
In the United States, there were deeper economic problems, hidden behind the façade of 1920s prosperity. These
included weaknesses in such key economic sectors as agriculture, coal mining, railroads, and textiles, and
increasing household and corporate indebtedness, the former caused in part by rising inequalities in wealth and
income, and the latter by a wave of business consolidation in the 1920s. With the collapse of credit on Wall Street
came widespread bank failures and a dramatic tightening of the credit markets. With much of the world dependent
on American capital and investment, the crisis quickly spread to Europe and other industrialized economies.
The sheer enormity of the economic catastrophe of the 1930s forced economists and economic policy makers to
rethink both their analyses of the business cycles and their prescriptions for responding to economic downturns. In
Britain, economist John Maynard Keynes challenged the classical notion that economies naturally return to a
supply-and-demand equilibrium that ensures low inflation and unemployment. Keynes argued that the stickiness of
prices and wages could create a supply-and-demand equilibrium marked by profound under-utilization of capital,
productive capacity, and labor.
As a prescription for the crisis, Keynes focused on raising aggregate demand. With businesses and households
either unwilling or unable to do so, this left only the government. Thus, Keynes emphasized both expansive
monetary stimulus (increasing the money supply) and fiscal stimulus (cutting taxes and spending on infrastructure)
measures to jump-start aggregate demand, even if this meant that governments ran large deficits. Such advice
ran counter to the conventional economic thinking of the day, which held that government borrowing in an
economic slowdown soaks up capital that might otherwise be used for private sector investment, thereby
prolonging the slump.
Without necessarily adopting all of Keynes’s ideas, governments throughout the industrialized world began to
employ monetary and fiscal stimulus policies to “prime the pump,” as the contemporary American expression went.
In the United States, the Federal Reserve shifted to a loose monetary policy, while the Franklin Roosevelt
administration, through a series of New Deal programs, pumped billions of dollars into the economy to build
infrastructure and subsidize agriculture, among other things. Still, old orthodoxies died hard. Believing that deficit
spending might yet cripple the economy, the Roosevelt administration cut back on its stimulus programs
dramatically in 1937, causing a second dip in economic performance that historians have come to call the
Roosevelt Recession.

British economist John Maynard Keynes (right), meeting here with U.S. Treasury secretary Henry Morgenthau, Jr.,
in 1944, overthrew classical economic theory by advocating massive government stimulus to raise demand in a
depressed economy. (Time & Life Pictures/Getty Images)
 Triumph of Keynes and the Post–World War II Economic Boom
According to most economic historians, New Deal programs only half-lifted the American economy. While
corporate revenues and overall economic growth returned, unemployment remained stubbornly high through the
end of the decade. Ultimately, however, Keynes was proven right. With enough stimulus, an economy could be
lifted out of a high-unemployment, under-capacity equilibrium. The proof came with the massive defense spending
accompanying America’s preparation for—and entry into—World War II, though putting 16 million Americans into
the armed forces certainly helped lower unemployment as well.
While many economists and policy makers feared that the end of World War II and the contraction in defense-
related stimulus spending would trigger a new period of economic instability and recession, the opposite proved
true. While the first global conflict of the twentieth century ushered in a period of global economic instability, the
second brought about the longest and most dramatic period of economic growth in the history of the world—some
thirty years of sustained, high growth in industrialized economies from Western Europe to Japan to North America,
as well as more modest growth in the developing world that supplied those economies their raw materials.
There were several reasons for this development. First was the massive economic stimulus of rebuilding war-
ravaged infrastructures in Japan and Europe. Second was the huge pent-up demand in the United States and
elsewhere after nearly a generation of privation in the Great Depression and rationing during World War II. Third
was the massive military spending that accompanied the cold war. Fourth was the American-led structuring of an
international economic order that ensured monetary stability and free trade. And finally, virtually all industrialized
economies adopted the Keynesian paradigm of fiscal and monetary stimulus to smooth out the business cycle,

stimulating demand during economic downturns through expansive fiscal and increasingly monetary measures and
pulling back on the monetary reins when inflation threatened.
 “Stagflation” and the Undoing of the Keynesian Consensus
The Keynesian consensus would finally come undone in the 1970s as the era of stable and sustained economic
growth came to an end. As with the onset of the great global expansion, there were a number of contributing
factors to the series of recessions that rocked the industrialized world in the 1970s and early 1980s. In Europe,
there was the end of postwar reconstruction; in the United States, there was the inflation triggered in part by
spending on the war in Vietnam. In addition, there was a slowdown in productivity gains, since new postwar
technologies and managerial stratagems, and the productivity-enhancing benefits they brought with them, had
already become fully assimilated into the economy. Most devastating of all was the sudden and dramatic run-up
in energy and commodity prices, the former triggered by unrest in the petroleum-rich Middle East. The latter was
a form of cost-push inflation that led to a wage-price spiral.
All of this contributed to “stagflation”—a phenomenon that complicated traditional Keynesian stimulus solutions. A
combination of the words “stagnation” (low or negative growth) and “inflation,” stagflation seemed to defy
economic theory. Rapidly rising wages and prices were only supposed to occur during times of economic
expansion, as a near full-employment economy was unable to meet the demand of businesses and consumers.
But in the 1970s, inflation accompanied recession, and Keynes’s prescriptions for deficit spending to lift economies
out of recession became untenable in the face of high inflation.
Many Post Keynesians argue that a Keynesian approach to stagflation—that is, putting a cap on wage and salary
increases—was never tried. Nevertheless, the problems associated with Keynesian economics helped trigger both
a dramatic rethinking of what governments should do to respond to economic crises and a significant political
realignment, as voters rejected the largely liberal, Keynesian-influenced regimes that had presided over both the
boom of the 1950s and 1960s and the bust of the 1970s. Especially in the United States and Great Britain, new
conservative governments moved away from the Keynesian emphasis on stimulus measures that would create
broad-based demand to policies that would spur investment, including large tax cuts for wealthy individuals and
corporations and deregulation of the financial sector. At the same time, to wring inflation out of the economy, the
U.S. Fed pursued tight monetary policies. The result of these steps was a dramatic recession in the early 1980s—
the worst since the 1930s—followed by a buoyant recovery, albeit one accompanied by increasing inequality in
income and wealth.
 Speculation and a Second Post–World War II Boom
Some economists argued that the policies pursued by the conservative administrations of Ronald Reagan in the
United States and Margaret Thatcher in Great Britain contained many Keynesian elements. In the United States,
for instance, the large tax cuts and massive defense spending—both driving government deficits to record levels—
provided a kind of Keynesian fiscal stimulus. Conservatives were loathe to call such policies Keynesian but,
however, they were labeled, they seemed to work. Barring sharp but short recessions in the early 1990s and
again in the early years of the new millennium, the U.S. economy experienced sustained growth from the mid-
1980s through the middle of the new millennium’s first decade, even if that growth was fueled largely by dramatic
productivity gains resulting from the personal computer, mobile telephone, and Internet revolutions.
The period of sustained growth differed significantly from that of the 1950s and 1960s in several key aspects. One
was that the immediate postwar boom saw a narrowing of income and wealth inequality, while the boom of the
late twentieth and early twenty-first centuries brought a trend toward greater inequality. This contributed to other
differences, including much higher rates of household debt and the need for women to enter the workforce to
sustain the lifestyles of middle-class families. And finally, there was the matter of regulation. Under the regime of
New Deal laws and agencies, financial institutions were unable to engage in many highly speculative activities.
Among the initiatives of the new conservative consensus in many countries was a lifting of such regulations. That,

combined with the large amounts of capital in the pockets of the wealthy, contributed to repeated episodes of
speculation—the savings and loan–inspired commercial real-estate boom of the early and mid-1980s, the dot.com
boom of the late 1990s, and, most spectacularly, the housing bubble of 2003–2006. Admittedly, the dot.com boom
was also caused by the introduction of a new technology, but the bubble was inflated by large infusions of venture
capital. Ultimately, all of these periods of speculation inevitably led to busts: the recession of the early 1990s in
the case of the commercial real-estate collapse (though diminished defense spending in the wake of the cold war
was a contributing factor); the recession of 2001–2002 (though the terrorist attacks of September 11, 2001,
played a role); and the “great recession” of 2007–2009, the worst economic downturn to hit the industrialized
world since the Great Depression.
The financial crisis of the early twenty-first century touched off the world’s worst economic downturn since the
Great Depression and recriminations over speculation, deregulation, gimmicky investment instruments, and sheer
greed. (Bloomberg/Getty Images)
Just as the repeated recessions of the 1970s caused both economists and government officials to rethink their
reliance on the Keynesian theoretical and policy-making paradigm, so the financial crisis of 2008–2009 and the
subsequent contraction caused many in the economic community and more liberal government administrations
(the latter put in place by voters fed up with the failures of conservative policy makers) to question the financial
deregulation and supply-side economics that had dominated both since the 1980s. Indeed, the massive stimulus
packages instituted by a number of governments in both the industrialized and developing worlds in the early
twenty-first century seem to hearken back to the old Progressive-Era ideas and Keynesian paradigm first put into
action in the first half of the twentieth century.
James Ciment
 
See also:  Asian Financial Crisis (1997);  Boom, Economic (1920s);  Boom, Economic (1960s); 
Dot.com Bubble (1990s-2000);  Florida Real-Estate Boom (1920s);  Great Depression (1929-
1933);  Oil Shocks (1973-1974, 1979-1980);  Panics and Runs, Bank;  Panic of 1901;  Panic of
1907;  Poseidon Bubble (1969-1970);  Recession and Financial Crisis (2007-);  Recession,
Reagan (1981-1982);  Recession, Roosevelt (1937-1939);  Recession, Stagflation (1970s); 
Souk al-Manakh (Kuwait) Stock Market Crash (1982);  Stock Market Crash (1929);  Stock
Market Crash (1987). 

Further Reading
Aldcroft, Derek H. The European Economy 1914–2000. London: Routledge, 2001. 
Barsky, Robert B., and Lutz Kilian.  “Oil and the Macroeconomy Since the 1970s.” Journal of Economic Perspectives 18:4
(2004): 115–134. 
Clarke, Peter. The Keynesian Revolution in the Making, 1924–1936. New York: Oxford University Press, 1988. 
Gran, Peter. The Rise of the Rich: A New View of Modern World History. Syracuse, NY: Syracuse University Press, 2009. 
Hetzel, Robert L. The Monetary Policy of the Federal Reserve. New York: Cambridge University Press, 2008. 
Karier, Thomas. Great Experiments in American Economic Policy: From Kennedy to Reagan. Westport, CT: Praeger, 1997. 
Kindleberger, Charles P. The World in Depression, 1929–1939. Berkeley: University of California Press, 1973. 
Leuchtenberg, William E. The Perils of Prosperity, 1914–1932. Chicago: University of Chicago Press, 1958. 
Sawyer, James E. Why Reagonomics and Keynesian Economics Failed. New York: St. Martin’s, 1987. 
Shiller, Robert J. Irrational Exuberance. New York: Currency/Doubleday, 2005. 
Temin, Peter. Lessons from the Great Depression. Cambridge, MA: MIT Press, 1989. 
Wilber, Charles K., and Kenneth P. Jameson. Beyond Reagonimcs: A Further Inquiry into the Poverty of Economics. Notre
Dame, IN: University of Notre Dame Press, 1990. 
 
Brazil
 
Located in South America, Brazil, with a population of about 190 million, is one of the large emerging countries
(along with Russia, India, and China) expected to play a prominent role in the global economy in the twenty-first
century. Despite its growing economy and substantial industrial and agricultural exports, Brazil still suffers from
widespread poverty, high illiteracy, and a poor education system. Brazil also has one of the highest Gini
coefficients, the most common measure of income inequality, of any country in the world. This inequality is a
remnant of Brazil’s uneven economic development over the past 500 years.
This is not the first time that Brazil has been characterized as an emerging economic power. Since the explorer
Pedro Cabral first claimed Brazil as Portuguese territory in 1500, Brazil has experienced four major economic
development cycles. The first three cycles were driven by booms in different exports in the seventeenth,
eighteenth, and nineteenth centuries, respectively. In contrast, the last of the four cycles, in the twentieth century,
was stimulated by intentional government policies designed to spur domestic industrial development by restricting
international trade. Each of the development cycles ended with Brazil’s standard of living still far below those of
leading contemporary economies.

Traders at the Brazilian Mercantile and Futures Exchange in São Paolo celebrate a rebound in stock prices in
early 2008 after the U.S. Federal Reserve cut interest rates to stave off recession. The United States is Brazil’s
main export market. (Bloomberg/Getty Images)
 Sugar Cycle
When Pedro Cabral accidentally bumped into the Brazilian coast on his way to India around the Horn of Africa,
Portugal was a small country more interested in establishing trading posts in Africa and the East Indies than in
colonizing vast foreign territories in the Americas, as its Iberian neighbor Spain did after 1500. However, in Brazil
there was little to trade; the most valuable early resource export was brazilwood, the source of a red dye that
proved to be of little value beyond giving the new territory its name. The Portuguese thus switched to the
colonization strategy of awarding large tracts of land (capitanias) to wealthy Portuguese citizens (donatários)
willing to go to Brazil to invest in their properties and attract colonists. In exchange for land titles, donatários
agreed to pay the crown 20 percent of the revenues generated on the land. According to economic historian
William Glade, this system made early Brazilian colonization “a business venture, combined with aspects of private
subgovernment.” A small elite came to dominate Brazil’s society and economy.
With no easy riches to exploit except for fertile tropical lands, the Portuguese donatário introduced crops from
elsewhere in the Portuguese empire, most notably sugarcane from the Portuguese Azores and Cabo Verde
islands in the Atlantic. Sugarcane grew very well in the Northeast region of Brazil where the Portuguese first
settled. But sugar production was labor intensive, and labor was in short supply in the capitanias. There were few
Portuguese settlers, and the native population mostly fled into the interior of the country to escape disease and
hard labor. Brazilian landowners thus began importing slave labor from Portuguese outposts in Africa, initiating the
Atlantic slave trade. By 1600, Brazil was the world’s leading sugar producer, and slaves outnumbered Portuguese
settlers in Brazil.
The Brazilian sugar boom ended in the late seventeenth century when other European colonial powers introduced
sugar to their own colonies. Most colonial empires limited foreign trade to their own colonies, and Brazil’s “mother
country,” Portugal, offered a very small market for sugar. Other products such as tobacco and cotton faced the
same limitations. The Brazilian plantations stagnated, leaving a society characterized by huge income inequalities
and little lasting economic development, although plantation owners had accumulated substantial riches. The
Northeast region of Brazil remains the poorest region of the country today.
 Mining Cycle
During the 1690s, gold and diamonds were discovered in parts of what is today the state of Minas Gerais

(General Mines) in the interior of the country. Nearly half of the world’s output of gold during the eighteenth
century would come from Minas Gerais. Some planters from the Northeast moved their slaves to the region to
work in the gold mines, but mostly the mining boom attracted new settlers from Portugal and elsewhere. The gold
cycle generated more diverse economic development than did the sugar cycle because gold mining was based on
small operations. Miners’ demand for food and transportation stimulated farming and the raising of mules to carry
supplies and gold between Minas Gerais and the coastal city of Rio de Janeiro.
The gold cycle also led to a greater government presence in Brazil as Portuguese bureaucrats were deployed to
collect a 20 percent government tax on gold. To minimize tax evasion, Portugal limited transport to a single mule
trail between Rio and Minas Gerais. Also, ships leaving Brazil had to sail in annual convoys accompanied by
government escorts back to Portugal. Of course, some smuggling occurred, but the economically oppressive
measures mostly fueled resentment of the crown. The first open uprising against the Portuguese colonial
government occurred in Minas Gerais in 1788. Another effect of the gold cycle was the emergence of Rio de
Janeiro as the colony’s largest city, through which all commerce with Minas Gerais passed.
Rio de Janeiro gained a huge increase in stature when, in 1807, the Portuguese royal court fled Napoleon’s
Iberian invasion and made Rio the capital of the Portuguese Empire. The wealthy Brazilian families schemed to
declare independence in 1822 when the Portuguese crown returned to Lisbon and reduced Brazil’s status once
again to a distant colony. Interestingly, the Brazilian elite opted for a monarchy rather than the republican form of
government most former Spanish colonies in Latin America chose after independence. They urged the son of the
Portuguese king to remain in Brazil as Emperor Dom Pedro I, which he did; he was succeeded by his son in 1831.
Brazil remained a monarchy until 1889, effectively ruled by a somewhat shaky coalition of elite business and
agricultural classes. A republican form of government was finally adopted in 1889 after years of debate within the
elite about slavery (abolished in 1888) and monarchy. The republic, like the monarchy, was increasingly
dominated by wealthy coffee barons, the new elite that emerged from the third development cycle.
 Coffee Cycle
Coffee trees were introduced to Brazil from the Middle East early in the eighteenth century. The trees require a
combination of cool but never cold weather, and they die from frost, which is often found in the higher altitudes of
tropical countries. The vast coastal highlands of Brazil fit the requirements for coffee cultivation perfectly.
Worldwide demand for coffee grew rapidly during that nineteenth century as the incomes of middle-class
consumers in Europe and the United States grew and as Brazil’s efficient production lowered costs. The coffee
trade was also helped by Europe’s abandonment of protectionist colonial trade preferences, and by the decline in
shipping costs by over 70 percent between 1840 and the early twentieth century due to the development of metal
steamships. Coffee cultivation spread from the Paraíba valley near Rio de Janeiro southward toward the state of
São Paulo and westward into the former mining region of Minas Gerais. The port of Santos, just below the São
Paulo plateau, became the world’s largest coffee port.
Coffee farming required large amounts of labor, and because the importation of slaves was no longer permitted,
the coffee boom attracted large numbers of immigrants from Portugal, Italy, Spain, Germany, and many other parts
of Europe. The Southern region of Brazil, from São Paulo down to the Uruguayan border, became a melting pot of
European immigrants, augmented later by immigrations from Japan and the Middle East, and by internal migration
of Brazilians from the poor Northeast region. British investment in infrastructure followed, and railroads soon
fanned out from the port of Santos and the nearby city of São Paulo into the interior of the state. By 1900, Brazil
produced half the world’s coffee, and the export accounted for 80 percent of the country’s export earnings.
The wealth accumulated by São Paulo coffee barons laid the basis for Brazilian industrialization. At the start of
the twentieth century, the growing immigrant population was creating a viable market for locally produced textiles,
clothing, footwear, woodworking, and processed foods. Most of the new industries were established in the city of
São Paulo. However, coffee’s role as the “engine of growth,” as one historian called it, was weakened by the
volatility of coffee prices and by overproduction, as other countries, such as Colombia and Mexico, planted more

coffee trees. The politically powerful Brazilian coffee interests pushed the government to guarantee prices and
purchase excess stocks. After the sharp rise of prices in the 1920s following the world economy’s recovery from
the war, there came a price decline of over two-thirds at the start of the Great Depression; the Brazilian
government’s political response to this latest coffee bust would effectively create the next development cycle: the
import substitution industrialization cycle.
 Import Substitution Industrialization
The collapse of coffee prices in 1930 greatly reduced foreign exchange earnings and government tax revenues. In
response, the Brazilian government imposed foreign exchange controls and sharply devalued the national currency
but guaranteed Brazilian coffee farmers the same local currency prices they received before devaluation. By
making imports much more expensive, foreign trade was balanced by causing imports to fall from $417 million in
1929 to just $108 million in 1932. The price support of coffee was a huge subsidy to the largest sector of the
Brazilian economy; it effectively constituted a huge fiscal stimulus. The government’s payments were largely
financed by printing money, not raising taxes, so the fiscal expansion was effectively augmented by an
expansionary monetary policy, just as John Maynard Keynes (1936) would some years later prescribe as the
solution to the Great Depression. Economic historian Celso Furtado writes: “Unconsciously Brazil undertook an
anticyclical policy of larger relative proportions than had been practiced in industrial countries until that time.” The
“accidental” Keynesian program worked, and while much of the world was in the Depression, Brazilian industrial
production grew rapidly throughout the 1930s as augmented internal demand was channeled toward domestic
producers when the price of imports rose sharply after the devaluation of the currency.
The 1930s experience influenced Brazilian policy makers in the 1940s and 1950s, when World War II and the
postwar economic worldwide economy recovery increased demand for Brazil’s raw materials. The value of the
Brazilian currency rose as a result of export expansion, and the prices of domestic goods relative to foreign goods
moved in the opposite direction from what occurred in the 1930s. In response to the pleas from the new Brazilian
industrialists, and justified by various popular economic and political arguments popularized by the United Nations
Economic Commission for Latin America and its head Raúl Prebisch, the Brazilian government imposed strict
import restrictions to protect Brazilian industries. Such intentional trade restrictions to promote domestic industrial
production came to be known as “import substitution industrialization” (ISI).
Brazil passed the “law of similars” in 1948, which explicitly banned all imports of “similar” products as soon as any
domestic firm showed it could supply the domestic market. Under this legislation, any product would be protected,
whether it cost twice as much or five times as much to produce in Brazil as it did in the rest of the world. The law
of similars thus severed the link between underlying comparative advantage and domestic production. It effectively
awarded domestic producers indefinite subsidies paid for by Brazilian consumers, who faced the high prices
charged by producers protected from foreign competition. Despite the apparent cost of ISI, it seemed to work.
Domestic industrialization grew rapidly, and by the late 1950s Brazilian factories, often owned and operated by
foreign multinational firms, were producing automobiles, electrical equipment, nearly all consumer goods,
construction equipment, tools, and, by the late 1960s, aircraft. Brazil’s status today as one of the emerging world
industrial powers has its roots in the ISI period after World War II. Brazil also achieved some of the fastest growth
rates of any developing country during the 1960s and 1970s.
The ISI policies made São Paulo and its surroundings the industrial capital of Brazil. They also made the southern
part of the country much wealthier than the rest of Brazil. The near middle-class living standards in many parts of
southern Brazil combine with the historically based extreme poverty of the Northeast and the poverty-fed slums of
the major cities housing migrants from the country’s poor regions to give Brazil its very unequal income
distribution.
In addition to worsening the income distribution, ISI caused other unsustainable economic changes, as evidenced
by the financial crisis of 1982. That year, the Brazilian government and its agencies defaulted on nearly $100
billion in foreign debt, and the sudden reversal of capital inflows brought Brazilian growth to a complete halt. After

several decades of rapid per capita income growth, between 3 and 6 percent per year in real terms over the three
decades between 1950 and 1980, per capita income actually fell between 1980 and 1990. It grew at just 1 percent
during the 1990s. Economist Victor Elias’s data also show a sharp slowdown in total factor productivity—the rate
at which output increases in excess of the growth of all inputs—from 3.7 percent per year in the 1950s to just 1
percent per year in the 1970s, and to –1 percent by the early 1980s. The rapid economic growth of the 1970s
under ISI thus seems to have led to investment that did little to increase productivity, while foreign borrowing
greatly increased Brazil’s foreign debt burden.
Brazil borrowed overseas because, under ISI, export earnings as a percentage of gross domestic product (GDP)
fell from over 7 percent prior to the ISI period to just 4 percent in the 1950s, and then to just above 2 percent by
the late 1970s. As should have been expected, ISI reduced Brazil’s capacity to produce exports because the
protected Brazilian industry had little incentive to compete in export markets. Yet Brazil had to import increasing
amounts of oil, industrial equipment, parts, components, other raw materials, and technology to sustain the
growing domestic industrial sector. The external imbalance from the ISI period represented a challenge to
Brazilian policy makers after the 1982 debt crisis stopped Brazilian growth and effectively brought the fourth
development cycle to an end.
 A Fifth Economic Development Cycle?
In his analysis of ISI policies, economist Henry Bruton suggests that while “some form of protection is in order to
enable a country to establish its place in the world economy,” the period of protection must also be used to
“establish an economy that is flexible and resilient.” Under pressure to repay loans from the International Monetary
Fund and the World Bank, Brazilian leaders introduced so-called Washington consensus policies designed to
improve its ability to repay foreign debt, namely trade liberalization, free investment flows, balanced government
budgets, and a reduced government role in the economy. During the 1980s and 1990s, Brazilian leaders struggled
to balance the domestic economic interests that had developed under ISI with policies linking the Brazilian
economy more closely to the global economy.
Under the recent presidencies of Fernando Henrique Cardoso and Luiz Inácio (Lula) da Silva, both left-leaning
politicians, Brazilian economic policy has moved toward an eclectic mixture of open trade policies and continued
active government involvement in the economy. Since barely escaping another foreign debt crisis 1998, Brazil has
accumulated foreign reserves of over $200 billion, which has stabilized its currency. Foreign investment increased
after 2000, and transnational firms increased their industrial and agricultural exports from Brazil. Brazil’s own
mixed state-private companies Petrobras (oil) and Vale do Rio Doce (mining) have transformed themselves into
global corporations. A number of private Brazilian firms that grew up under ISI protectionism have also become
“outward oriented,” some even investing overseas, a sign that Brazil has abandoned ISI policies in favor of
integration with the global corporate economy.
Per capita real growth of GDP slowed to almost nothing in 2009 before climbing again by about seven percent in
2010, the latter a significant improvement on the 1980s and 1990s. Meanwhile, international economists had
praised Brazil for getting its macroeconomic fundamentals together, keeping inflation low and even boasting
growth through the financial crisis and global recession of the late 2000s. And this fiscal probity did not come at
the expense of the poor, as is often the case in developing economies seeking to balance budgets, as Brazil saw
a gradual reduction in income inequality and poverty, previously two of the main weaknesses of the country’s
economic and social order. Finally, with the discovery of potentially large oil fields off its southern coast, Brazil
looked to become a major petroleum producer as well.
Now the question for the country, agree economists, is whether the gradual moves toward more income equality
and lower levels of poverty will turn the modest growth recovery of the past decade into a new development cycle
that finally puts Brazil on a consistent path toward economic development.
Hendrik Van den Berg

 
See also:  Argentina;  BRIC (Brazil, Russia, India, China);  Emerging Markets;  Latin America. 
Further Reading
Baer, Werner. The Brazilian Economy: Growth and Development.  4th ed. Westport, CT: Praeger, 1995. 
Elias, Victor J. Sources of Growth: A Study of Seven Latin American Economies. San Francisco: ICS Press, 1989. 
Furtado, Celso. The Economic Growth of Brazil: A Survey from Colonial Times to Modern Times. Berkeley: University of
California Press, 1963. 
Glade, William P. The Latin American Economies: A Study of Their Institutional Evolution. New York: American Book, 1969. 
Maddison, Angus. Monitoring the World Economy, 1820–1992. Paris: OECD, 1995. 
Prebisch, Raúl. The Economic Development of Latin America and Its Principal Problems. Lake Success, NY: United Nations
Department of Social Affairs, 1950. 
Skidmore, Thomas E., and Peter H. Smith. Modern Latin America.  3rd ed. New York: Oxford University Press, 1992. 
Stein, Stanley J. Vassouras: A Brazilian Coffee County, 1850–1890. New York: Atheneum, 1974. 
Summerhill, William R.  “Railroads in Imperial Brazil, 1854–1889.” In Latin America and the World Economy Since 1800, 
ed. John H. Coatsworth and Alan M. Taylor, 383–405. Cambridge, MA: Harvard University Press, 1998. 
Thorp, Rosemary. Progress, Poverty, and Exclusion: An Economic History of Latin America in the 20th
Century. Washington, DC: Inter-American Development Bank, 1998. 
BRIC (Brazil, Russia, India, China)
 
BRIC is an acronym coined by the investment company Goldman Sachs in 2003 to represent the group of four
rapidly expanding economies that consists of Brazil, Russia, India, and China—sometimes referred to as “the
BRIC” or “BRICs.” By the year 2050, each of these developing countries is expected to be wealthier than most of
today’s major economic powers. According to Goldman Sachs and adherents of its view, China and India will
become the world’s largest suppliers of manufactured goods and services, respectively, while Brazil and Russia
will dominate as suppliers of raw materials. Because of their lower labor and production costs, these nations are
also expected to represent a growing opportunity for foreign expansion on the part of firms in the developed world.
The BRIC countries, taken together, have contributed almost 40 percent of global economic growth since 2000.
As of 2008, their combined economies (adjusted for cost of living) exceeded that of the United States and
accounted for 22 percent of the global economy. China has been the single-largest driver of growth, with its share
of world output rising from 6 percent to 11 percent in just one decade.
Although the BRICs are unlikely to organize as a trading bloc like the European Union, or even a formal trading
association like ASEAN (the Association of Southeast Asian Nations), there are strong indicators that the four
countries recognize their growing economic power and look forward to transforming it into greater geopolitical
clout. The BRIC countries encompass more than a quarter of the world’s landmass, account for 40 percent of the
world’s population, and held a combined gross domestic product (GDP) of some $15.5 trillion in 2009. With or
without a formal trading bloc, they constitute the largest economic entity in the global market by virtually any
measure.

According to several forecasts, the BRIC economies together will account for more than half the market size of the
G-8 nations (Canada, France, Germany, Italy, Japan, Russia, United Kingdom, and United States) by 2025 and
exceed them by 2050. (As of 2009, they accounted for less than 15 percent of the aggregate G-8 GDP.) Of the
G-8 nations, only the United States and Japan are expected to be among the world’s six largest economies in
2050. The shift in GDP is expected to be most dramatic through 2030, with a significant deceleration in the
following two decades. Only India is likely to see growth rates above 3 percent in the 2030–2050 period. With the
exception of Russia, individuals in the BRIC countries are likely to remain poorer than their counterparts in the G-
8 economies. China’s per capita income is forecast to grow to about $30,000 by 2050, according to some
economic analyses.
In order to help these nations reach their economic potential, policy makers in the four countries have continued to
focus on education, foreign investment, domestic consumption, and domestic entrepreneurship. India has the
potential to grow the fastest among the BRICs, primarily because the decline in average age will occur later for
India than in Russia, China, and Brazil. According to Goldman Sachs, India’s per capita GDP will quadruple
between 2007 and 2020, the Indian economy will exceed that of the United States by 2043, and the four BRIC
nations as a group will overtake the G-8 by 2032.
 World Rankings (by World Bank) of BRICs, 2010 
Brazil Russia India China
Population
5
9
2
1
Total area
5
1
7
3
GDP (nominal)
7
11
9
2
GDP (PPP*)
9
6
4
2
Exports
24
21
23
2**
Imports
22
18
13
3
Received FDI
12
16
22
8
Foreign exchange reserves*** 6
3
7
1
Number of mobile phones
4
5
2
1
Number of Internet users****
5
11
4
1
* Purchasing power parity.
** European Union is number one, but China ranks first among individual nations.
*** Mid-2011
**** 2009
 Impact of Global Recession
All four countries in BRIC have suffered economic downturns as a result of the 2008–2009 global recession, albeit
to different extents. Brazil was the hardest hit, but the effects of the recession were softened by the boom period
that preceded it and by a relatively strong economic foundation. The Brazilian stock market (Bovespa) had
skyrocketed from 9,000 in 2002 to more than 60,000 in 2008, and government policies, such as lowered interest
rates, favored investment. In addition, much of the nation’s foreign debt was retired and the economy was

bolstered by strong capital inflows and booming exports in 2008, primarily due to the global commodity boom of
the previous year.
Russia, which averaged growth of more than 7 percent over a period of ten years, has shown signs of economic
contraction on par with other hard-hit countries. Exports plummeted, and industrial production fell by almost 9
percent in 2008. Especially damaging to the Russian economy was the collapse of oil prices in mid-2008. Foreign
exchange reserves dropped significantly despite currency depreciation, and refinancing of external loans became
extremely difficult.
India, too, was hurt by the global recession of 2008–2009, suffering a sharp economic deceleration following the
international credit crunch. Merchandise exports fell by 12 percent in 2008, with sales data from a wide array of
industries and services suggesting further slowdowns in the future. India’s currency, the rupee, depreciated by 20
percent in just one year, and stock prices remained low despite a mild recovery in late 2008.
China has been the major economic force among the four BRICs, primarily because of the global growth spurt of
2003–2005, which propelled a massive commodity boom from 2006 to 2008. Yet even China’s export growth
turned negative in 2008, resulting in the closure of thousands of factories that had produced toys, textiles,
electronics, and other labor-intensive exports.
 Future Economic Growth
Among the four countries, China has registered the strongest economic growth in recent years—11.9 percent in
2007 and 9.4 percent in 2008. Forecasts for 2009 by the World Bank, Goldman Sachs, and Citibank ranged from
1.5 to 3.0 percent. India has recorded the second-highest growth among the BRICs—9.0 percent in 2007 and 6.3
percent in 2008, with forecasts of 5.5 to 6.0 percent for 2009. The Russian economy expanded by 8.1 percent in
2007 and 6.0 percent in 2008, with projections of 1 to 4 percent for 2009. The Brazilian economy grew by 5.4
percent in 2007 and 5.2 percent in 2008, and was expected to grow by 2 to 3 percent in 2009.
According to an Ernst & Young research report published in December 2008, titled “For Richer, For Poorer: Global
Patterns of Wealth,” the BRIC countries will contribute 40 percent of global economic growth between 2009 and
2020, with China alone accounting for one-quarter. China is also expected to be the largest economy in the world
in terms of purchase parity by 2019 (taking cost of living into consideration). The BRIC countries are also expected
to account for 65 percent of global basic metal consumption as well as 38 percent of chemical products, 30
percent of motor vehicles, and 28 percent of electronics by 2020. In all sectors, China will contribute the lion’s
share of growth.
 Assumptions and Criticisms
The major assumptions underlying BRIC projections are that the four countries will continue to pursue established
economic policies and to develop institutions supportive of growth. Bad public policy, natural disasters, and any
number of external factors could derail the projections. Proactive growth policies will have to continue at both the
national and the global levels, and the pace of reform must be steady or accelerate, given the political and social
tensions in these countries. Demographic factors such as declining fertility rates represent another area of
potential risk.
Another assumption underlying the BRIC thesis is that the four countries have abundant untapped energy
sources, such as oil, coal, fossil fuels, and natural gas. However, should any of the countries reach peak
production before renewable energy sources can be developed and commercialized, economic growth will be
slower than anticipated. Furthermore, given the current technology, there is a limit to how much the BRICs can
develop their natural resources before exceeding the ability of the global economy to absorb them. These
countries also have enormous populations of impoverished people, which could hinder economic progress,
increase social unrest, and limit market demand. Finally, factors that are impossible to predict or plan for—such as
international conflict, civic unrest, acts of terrorism, or outbreaks of disease—can have an impact on the destiny of

any country.
Abhijit Roy
 
See also:  Brazil;  China;  Emerging Markets;  India;  Russia and the Soviet Union;  Transition
Economies. 
Further Reading
 Bird, Robert C.  “Defending Intellectual Property Rights in the BRIC Economies.” American Business Law Journal 43:2
(2006): 317–363. 
 Tamazian, Artur, Juan Piñeiro Chousa, and Krishna Chaitanya Vadlamannati.  “Does Higher Economic and Financial
Development Lead to Environmental Degradation? Evidence from BRIC Countries.” Energy Policy 37:1 (2009): 246–253. 
 Wilson, Dominic, and Roopa Purushthaman.  “Dreaming with BRICs: The Path to 2050.” Goldman Sachs Global Economics
Paper No. 99, 2003. 
Brunner, Karl (1916–1989)
 
A Swiss-American economist who was a leading early member of the monetarist school, Karl Brunner was
influential in the founding of two economic watchdog groups that monitor global economic policy: the Shadow
Open Market Committee and the Shadow European Economic Policy Committee, both established in the early
1970s. As a monetarist, Brunner believed that prices and inflation are closely tied to the growth of a nation’s
money supply. He was known as a critic of the U.S. Federal Reserve, citing its failure to maintain steady growth
in the money supply as a destabilizing factor in the nation’s economy.
Born on February 16, 1916, in Zurich, Switzerland, Brunner attended the University of Zurich from 1934 to 1937,
studied at the London School of Economics for the next two years, and then earned his PhD back at the
University of Zurich in 1943. After working for the Swiss National Bank, Brunner arrived in the United States in
1948 on a Rockefeller Foundation fellowship. After spending a semester at Harvard, he moved to the University of
Chicago, where he was greatly influenced by Milton Friedman, Frank Knight, and others, who advocated a brand
of free-market libertarianism, took a neoclassical approach to price theory, and attempted to bring economic
reasoning to other realms of social science. Outside the discipline of economics, Brunner also wrote and taught in
such fields as logic and the philosophy of science.
Eventually becoming a U.S. citizen, Brunner went on to teach at the University of California–Los Angeles (UCLA),
Ohio State University, and the University of Rochester, where he served until his death as the Fred H. Gowen
professor of economics at the William E. Simon Graduate School of Business Administration and as director of the
Simon School’s Bradley Policy Research Center.
In addition to his academic work, Brunner advised international governments on economic and financial policy, and
established a number of influential economic research and policy bodies. An active organizer, academic, and
policy advocate, he founded the Carnegie-Rochester Conference Series on Public Policy, the Konstanz Seminar
on Monetary Analysis and Policy, and the Interlaken Seminar on Analysis and Ideology, later renamed the Karl
Brunner Symposium. He also served as an adviser to the Swiss National Bank and other European central banks,

in which capacity he advocated a policy of low inflation as critically important to economic growth.
Much of Brunner’s work was in collaboration with the economist Allan H. Meltzer and in agreement with Milton
Friedman and the Chicago school of economics. Brunner and Meltzer argued that the modern system of monetary
circulation is profoundly affected by the lending practices of banks and the control of the money supply. In
developing his theories on monetary supply and its relationship to business cycles, Brunner examined the activities
of central banks, commercial banks, and other lending institutions in order to gauge the supply of money and
credit at a given time and to understand its effect on business cycles and economic stability within different
countries.
In addition to founding the Journal of Money, Credit and Banking and the Journal of Monetary Economics, Brunner
wrote or co-wrote more than a dozen books, including Monetary Economics (1989), and Money and the Economy:
Issues in Monetary Analysis (1989), both with Meltzer, and well over a hundred published papers. He also edited
The First World and the Third World (1978) and The Great Depression Revisited (1981). He died on May 9, 1989,
in Rochester, New York.
Andrew J. Waskey
 
See also:  Friedman, Milton;  Monetary Theories and Models. 
Further Reading
Brunner, Karl. Money and the Economy: Issues in Monetary Analysis. New York: Cambridge University Press, 1993. 
Brunner, Karl, and Allan H. Meltzer.  “The Uses of Money in the Theory of an Exchange Economy.” American Economic
Review 61(December 1971): 784–805. 
Lys, Thomas, ed. Economic Analysis and Political Ideology: Selected Essays of Karl Brunner. Cheltenham, UK: Edward
Elgar, 1996. 
Bullock, Charles (1869–1941)
 
Charles Jesse Bullock was a Harvard economist and monetary expert whose career flourished during the early
years of the 1900s. His work touched on a number of the important economic issues of the day as well as
significant aspects of the history of economics. His most important achievements centered on the measurement
and forecasting of business cycles.
Bullock was born on May 21, 1869, in Boston. After beginning his higher education through a correspondence
course, he graduated from Boston University in 1892 while working as a high school principal. After obtaining his
PhD from the University of Wisconsin in 1895, Bullock became an instructor in economics at Cornell University,
taught at Williams College from 1899 to 1903, and then took a position at Harvard University, where he remained
for the next thirty-one years.
Bullock’s area of expertise was public finance, though he also notable made contributions to the study of
international economics before 1914 and to the history of economics. He served as a tax adviser to
Massachusetts, among other states, and was a member of Phi Beta Kappa, the American Academy of Arts and
Sciences, the American Economic Association, the Statistical Association, the Harvard Economics Society

(president, 1927), and the National Academy of Arts and Sciences. From 1917 to 1919, Bullock served as
president of the National Tax Association.
As director of Harvard’s Committee on Economic Research from 1917 to 1929, Bullock worked with colleagues to
develop the “barometric” approach to business cycles. Their “three-curve” barometer, also known as the Harvard
barometer, was used to measure expansions and contractions in business cycles. The A curve was based on
stock prices and the loans of New York City banks; the B curve was based on wholesale prices and loan totals of
banks outside New York; and the C curve used short-term interest rates as a reflection of financial conditions.
Together, the A, B, and C curves were grouped into a statistical description that could be used to examine
economic fluctuations as deviations from major trends. For example, seasonal fluctuations in economic areas such
as jobs, harvests, and travel might be grouped into composite indices. The Harvard barometer was widely used in
the 1920s as an economic forecasting tool and became the basis of a variety of other quantitative measures.
Bullock’s committee published its findings in the Review of Economic Statistics (later titled the Review of
Economics and Statistics), which provided a forum for discussions about business cycles, their size, and timing.
In addition to his work on business cycles, Bullock made significant contributions to the study of economic history,
especially financial practices in colonial America. His study of colonial monetary policy examined its impact on the
American Revolution and consequences for the new republic. Bullock concluded that British colonial authorities
had mismanaged the financial resources of their day. His major published works include the Finances of the
United States from 1775–1789, with Special Reference to the Budget (1895); Introduction to the Study of
Economics (1897); Essays on the Monetary History of the United States (1900); and Finances of Massachusetts,
1780–1905 (1907).
In his last years, Bullock focused his studies on the effects of economic and financial practice in politics and
society, resulting in his influential Economic Essays (1936) and Politics, Finance and Consequences (1939).
Bullock died on March 17, 1941, in Hingham, Massachusetts.
Andrew J. Waskey
 
See also:  Fiscal Policy;  Tax Policy. 
Further Reading
Bullock, Charles Jesse. Economic Essays.  Reprint ed. Freeport, NY: Books for Libraries Press, 1968. 
Bullock, Charles Jesse. Elements of Economics. Boston: Silver Burdett, 1905. 
Bullock, Charles Jesse. Essays on the Monetary History of the United States.  Reprint ed. New York: Greenwood, 1969. 
Burchardt, Fritz (1902–1958)
 
German economist Fritz Adolph “Frank” Burchardt is best known for his extensive theoretical work on economic
growth and business cycles. His research encompassed the related areas of employment, monetary policy,
production theory, and international trade. A founder of the so-called Kiel school, whose members included some
of the most important German academic economists, Burchardt became known as a prominent critic of monetary
trade-cycle theories in Weimar Germany.

Born in Barenburg, Germany, in 1902, Burchardt received a doctorate in economics from the University of Kiel
(Germany) in 1925. He remained at Kiel from 1926 to 1933, working closely with sociologist and economist Adolph
Lowe. The Kiel school was affiliated with the Kiel Institute of World Economics, a center for the study of business
cycles founded in 1914 by Bernhard Harms. Under Lowe’s leadership, the school’s reformist mission attracted a
group of brilliant young economists, including Gerhard Colm, Hans Neisser, Wassily Leontief, and Jacob
Marschak. During the same period, Burchardt also pursued postdoctoral training in economics at Frankfurt’s
Goethe University, where his thesis was accepted in the winter of 1932–1933. His studies were cut short with the
rise to power of the Nazi Party and his subsequent dismissal from his academic post.
Fleeing the Nazis, Burchardt moved to England in 1935 and joined the Oxford Institute of Statistics. Four years
later, however, after the outbreak of World War II, he was among 65,000 young Germans interned by the British
as enemy aliens on the Isle of Man. Thanks to the protests of influential colleagues, including British economist
John Maynard Keynes, Burchardt was released from the internment camp in November 1940.
Back at Oxford, Burchardt revamped the Bulletin of the Institute of Statistics and coordinated the research of
various colleagues. At the same time, he was also developing his theories of business cycles and economic
growth, which expanded on his work with Lowe at the Kiel Institute. In 1944, Burchardt edited a famous
cooperative study titled The Economics of Full Employment, with contributors from such notable Central European
émigrés as Thomas Balogh, Kurt Mandelbaum, Ernst F. Schumacher, and Burchardt himself. Their flight from
Central Europe before and during World War II would impoverish the economics departments at German and
Austrian universities for the next generation and beyond.
Burchardt’s work focused on monetary explanations of business cycles, including a comprehensive survey of the
topic from a historical perspective, written in 1928 when he was still in Germany. In 1932, Burchardt made a
significant contribution to the development of economic growth theory with a comparative analysis of Austrian and
Marxist approaches to that subject. He was particularly interested in showing how the study of both vertically
integrated production systems and interindustry linkages informs economists about patterns of economic growth
and the ups and downs of the business cycle.
Burchardt sought to apply the same theories to government. His criticisms of the Weimar Republic’s trade-cycle
theories were in keeping with the reforms sought by the Kiel school, which aimed to develop innovative theories
of growth in light of monetary policy, production structures, and the business cycle. Burchardt was also interested
in applying his research to the issue of international trade. In a May 1943 article in Economica titled “Multilateral
Clearing,” he addressed the problems of international trade and capital movements within the context of Keynesian
economic theory. In 1948, he became the first economist (and nonmathematician) to be named director of the
Oxford Institute of Statistics. Burchardt died on December 21, 1958.
Andrew J. Waskey
 
See also:  Austrian School;  Growth, Economic;  Kalecki, Michal;  Marxist Cycle Model. 
Further Reading
Burchardt, F.A., et al. The Economics of Full Employment: Six Studies in Applied Economics. Oxford, UK: Basil
Blackwell, 1944. 
Lowe, A.  “F.A. Burchardt, Part I: Recollections of His Work in Germany.” Bulletin of Oxford University Institute of
Statistics 21(1959): 59–65. 
Worswick, G.D.N.  “F.A. Burchardt, Part II: Burchardt’s Work in Oxford.” Bulletin of the Oxford University Institute of
Statistics 21(1959): 66–72. 

Burns, Arthur (1904–1987)
 
Arthur Frank Burns was an American economist whose career spanned academia and government. He was
considered an expert on the measurement of business cycles and served as chairman of the Federal Reserve
System (Fed) during the Nixon administration, from 1970 to 1978.
Burns was born on April 27, 1904, in Stanislau, Galicia, in the Austro-Hungarian Empire (now part of Ukraine).
His parents immigrated to Bayonne, New Jersey, when he was a child. Burns attended Columbia University in
New York City, where he received bachelor’s and master’s degrees in 1925, pursued graduate studies under
Wesley Clair Mitchell, and was awarded a PhD in 1934. Burns taught economics at Rutgers University from 1927
until 1944, at which time he joined the faculty at Columbia. There, in 1959, he was appointed John Bates Clark
professor of economics.
Burns’s research at Columbia focused on business cycles. He became a member of the National Bureau of
Economic Research (NBER) in 1933, serving as director of research from 1945 to 1953 and as president from
1957 to 1967. NBER, founded in 1920 as a private, nonprofit research center, sought to promote understanding of
the U.S. economy through rigorous research projects and to share the results of its research as nonpartisan
economic information. A major interest of the NBER was the business cycle and its relationship to long-term
economic growth, which it investigated in detail.
Burns’s work on business cycles, much of it in collaboration with his former mentor at Columbia, Mitchell, explored
the factors that determine booms and busts as an economy expands and contracts. Together, Burns and Mitchell
set the standards for the NBER committee on business cycles as it sought to assign dates to them. Through this
work, Burns became known as an expert on the timing and movement of business cycles.
From 1953 until 1956, Burns served as chairman of the Council of Economic Advisers under President Dwight D.
Eisenhower. He returned to government service three years later as an economic adviser to President Richard M.
Nixon. He was elevated to chairman of the Federal Reserve System in February 1970 and served in that capacity
until late January 1978. As Fed chairman, Burns came under attack for his economic beliefs, often by those who
were in power.
Burns believed in fiscal and monetary restraint (reduced money supply and government spending) as a way to
achieve low unemployment and economic growth, but many thought he was too susceptible to political pressures
and influences. President Nixon, who had attributed his defeat in the 1960 presidential election to the Fed’s tight
money policy, now encouraged Burns to follow an easy-credit policy—the very opposite of Burns’s belief in fiscal
and monetary control. The issue was at the center of U.S. policy debate during the oil crisis of 1973, when the
national economy underwent an unusual and debilitating condition called stagflation—a decline in business activity
and increased unemployment (stagnation) at the same time that consumer prices were rising (inflation).
Contributing to the problem of inflation were government spending on the Vietnam War, congressional deficit
spending, and the ongoing cost of Great Society programs. When Burns’s term as Fed chair expired in 1978,
Democratic president Jimmy Carter chose not to reappoint him to the position. One short-term consequence of
this decision was a plunge in value of the dollar.
After leaving government, Burns joined the conservative think tank American Enterprise Institute, where he was
free to lecture and write, publishing Reflections of an Economic Policy Maker in 1978. He was appointed
ambassador to West Germany by President Ronald Reagan, and served in that capacity from 1981 to 1985. He
died on June 6, 1987, in Baltimore, Maryland.

Andrew J. Waskey
 
See also:  Federal Reserve System;  Monetary Policy;  National Bureau of Economic
Research;  Recession, Stagflation (1970s). 
Further Reading
Tuch, Hans N. Arthur Burns and the Successor Generation: Selected Writings of and About Arthur Burns. Lanham,
MD: University Press of America, 1999. 
Wells, Wyatt. Economist in an Uncertain World: Arthur F. Burns and the Federal Reserve, 1970–1978. New York: Columbia
University Press, 1994. 
Business Cycles, International
 
The study of international business cycles, a relatively new field of inquiry in economics, attempts to explain how
swings in national economies or economic groupings affect other national economies and economic groupings.
The central question in the study of international business cycle studies is whether and how the business cycles
in developed-world economies affect economies in the developing world.
Prior to the rise of international business cycle studies, economists focused their efforts on understanding national
economies or, more typically, trying to develop foundational theories and assumptions for economic conditions and
performance generally. By the 1970s, however, many economists had come to recognize that the global economy
was becoming increasingly integrated. Key to the integration were new communication and transportation
technologies (the Internet and container shipping, for example) that made doing business across continents and
oceans far simpler and easier. At the same time, increasing computer capacity was making it possible to digest,
analyze, and utilize the enormous quantity of economic data being generated around the world—data that made
better investment decision-making possible. A second key factor in the integration of the world economy was the
emergence of new economies in the developing world, most notably those of East and South Asia. Finally, there
was a growing awareness among economists that economic “shocks,” which began in developed-world
economies, could spread to developing world economies, leading economic decision makers in the latter to design
and adopt response strategies.
While developing nations were insignificant players on the world scene as late as the 1960s, their economies have
come to play an increasingly influential role in the global economy. By the late 2000s, China and India alone
accounted for approximately one-quarter of world economic output. Meanwhile, businesses in the developed world
began to take advantage of opportunities in the developing world in the form of direct investment or by
establishing trade links with suppliers. A number of factors play into these decisions, including government policies
in the developed and developing world, geographic location, natural resources, and, importantly, labor markets.
With their huge populations of manual workers—and, in some cases, a growing cohort of educated workers—
developing world countries offer enormous advantages to firms in the developed world trying to lower costs. Even
more rapid has been the financial integration of the world economy, as individuals and institutions take advantage
of new communications to invest in national financial markets, in the process creating an integrated global financial
system.

Students of international business cycles study this process of integration, as well as its wide-ranging
ramifications. The essential question is the extent to which upswings and downswings in one part of the
international economy affect other parts—in particular, how much do trends in developed-world economies affect
their counterparts in the developing world. This question divides the field into two intellectually competing camps.
On the one hand, some theories under the idea of “co-movement” maintain that the more open a country is to
international trade the more vulnerable it becomes to external economic shocks. Specifically, proponents argue,
the economies of developing world countries that depend on investment from and trade with the developed world
are more likely to go into recession when their investment and trading partners do. According to advocates of this
view, the United States is crucial in this regard, as it is not only a major source of investment in the developing
world but also the world’s largest importer, with an increasing portion of that import trade coming from China and
other developing world economies. Thus, they argue, while the recession of 2007–2009 may have begun in the
United States, its effects soon spread to the developing world—evidenced by slower growth rates in China and
other economies dependent on trade with and investment from the United States. As American consumers closed
their wallets and international credit froze up (much of it having come from U.S. financial institutions), so new
investment in China slowed, factories shut down, workers were laid off, and the economy slowed.
An opposing theory points to a phenomenon referred to as “decoupling.” Adherents of this view maintain that,
despite growing economic integration, developing world economies, particularly large ones such as Brazil, China,
and India, have effectively become disconnected from swings in the business cycle of the United States and other
countries of the developed world. Proponents of decoupling note that, despite the deep recession in the United
States, Japan, and Europe, large developing countries either continued to grow or experienced only minor
recessions. Decouplers maintain that government policies, trade among developing countries, and, most
importantly, dramatic growth in internal markets—as evidenced by enormous increases in the number of middle-
class consumers in China and elsewhere—make these countries relatively immune from business downturns in
the developed world, even as severe as the 2007–2009 recession.
Current empirical research paints a mixed picture, with data to support both the co-movement and decoupling
views of international economic influence. In any event, the rigorous study of international business cycles
provides an evolving context for understanding the relationship between the economies of the developed and
developing worlds and other aspects of the emerging global economy in the twenty-first century.
Michael Rawdan and James Ciment
 
See also:  Asian Financial Crisis (1997);  Emerging Markets;  Shock-Based Theories; 
Transition Economies. 
Further Reading
Backus, David, Patrick J. Kehoe, and Finn E. Kydland.  “International Business Cycles: Theory and Evidence.” Available at
www.nber.org/papers/w4493
Crucini, Mario J., Ayhan Kose, and Christopher Otrok.  “What Are the Driving Forces of International Business Cycles?”
NBER Working Paper Series No. 14380, October 2008. 
Engelhardt, Lucas M.  “Business Cycles in an International Context.” Available at
www.mises.org/journals/scholar/Engelhardt.pdf
Kose, M., C. Otrok, and E. Prasad. Global Business Cycles: Convergence or Decoupling? Washington, DC: International
Monetary Fund, 2008. 
Lewis, A. Nobel Lectures, Economics 1969–1980. Singapore: World Scientific Publishing, 1992. 

 
Canada
 
Canada is a land of contrasts. It is both a large nation and a small one. The second biggest in the world by
landmass, it has the least number of inhabitants of the G-8 group of major economies. While its vast expanses
reach to the extreme latitudes of the north, politically, economically, and socially, it is situated somewhere in the
middle of the Atlantic. That is to say, Canada stands halfway between the more statist and socialist polities of
Western Europe and the freewheeling capitalism of its huge neighbor to the south, the United States.
Sharing a 3,000-mile (4,825-kilometer) border with the largest economy on Earth, Canada has often experienced
the same economic ups and downs as the United States, even if its more extensive social safety net has
sometimes better cushioned its citizens from the privations of the latter. Canada was one of the nations hardest hit
by the Great Depression, for example, though it has been less deeply affected by the financial crisis and recession
of the late 2000s.
 Economic History
Canadian history has often paralleled that of the United States. Home to hundreds of thousands of indigenous
people in the pre-Columbian era, it was settled by Europeans beginning in the seventeenth century and became
an important British colony in the eighteenth. Unlike the United States, Canada did not win its freedom as a result
of revolution but was granted increasing autonomy during the nineteenth century, culminating in its independence
in 1867. The late nineteenth and early twentieth centuries saw Canadians settle the vast expanses of the West
and build a substantial industrial and transportation infrastructure. Like the United States, Canada is a land of
immigrants, with similar waves of Eastern and Southern Europeans coming to the country around the turn of the
twentieth century and another wave of peoples arriving from the developing world in the latter half of the century.
The parallels are not perfect. One significant difference between the two countries has to do with ethnicity;
Canada had almost no slavery, little history of racism, and, until recently, few persons of African descent. Instead,
it has a sizable French-speaking population—largely situated in the eastern province of Quebec—which has often
seen itself as an oppressed minority.
The modern Canadian economy owes much to the work of John A. Macdonald, the country’s first prime minister.
In the 1870s and 1880s, he pushed through a transcontinental railroad that helped unite the far-flung reaches of
the continent-wide country economically. Macdonald also pushed for protective tariffs to allow the country’s infant
industrial base to grow. But for all Macdonald’s efforts, Canada remained mired in an economic slump throughout
much of his administration. Most immigrants preferred the warmer climes of the United States, and the long global
recession from the 1870s through the early 1890s meant that there was slack demand for the country’s vast
natural resources—from timber to minerals to fish.
Only with the revival of the world economy in the 1890s did Canada itself truly begin to grow into an economic
power and draw in large numbers of immigrants. Huge new mineral deposits were discovered in northern Ontario,
along with the last great gold discovery of the nineteenth century in the Yukon. A burgeoning population on the
country’s vast great plains led to soaring wheat production while new transportation infrastructure allowed for the

more effective exploitation of the country’s vast timber resources.
In the twentieth century, the Canadian economy became closely linked to that of the United States. As with its
neighbor to the south, Canada enjoyed flush economic times in the 1920s—buoyed by increasing consumer
spending—but was hard hit by America’s Great Depression, experiencing similarly high levels of unemployment
and shrinking gross domestic product (GDP). Although under conservative leadership for the first half of the
1930s, Canada developed its own set of New Deal– like programs to regulate business and provide jobs, though it
was only with the country’s 1939 entry into World War II that unemployment dropped to pre-Depression levels.
Emerging out of World War II as one of the largest economies in the world (like the United States, its
infrastructure was spared wartime destruction), Canada basked in the global prosperity of the 1950s and 1960s,
its natural resources in great demand from industries in the United States, Europe, and Japan, and its industrial
capacity expanding to meet rising consumer demand at home. During these years, Canada expanded its social
welfare system to provide Social Security– like public old-age pension and welfare programs to aid the poor.
Unlike the United States, however, Canada also created a universal health care program from the late 1940s
onward that eventually evolved into a single-payer system, whereby provincial governments—aided by the federal
government—use tax revenues to pay for the health services provided by private doctors and hospitals.
With its economy heavily dependent on natural resources, which experienced steep price hikes in the 1970s, parts
of Canada, particularly the oil-rich province of Alberta, prospered even as industrial Ontario and Quebec
experienced a recession due to a flagging U.S. economy. When commodity prices, including oil, fell in the 1980s,
Canada went into a prolonged period of slow or negative growth, with high unemployment and growing
government deficits.
The slump produced political disaffection, as voters turned from the Progressive Conservatives, the center-right
party, to the Liberals, the center-left party, in the early 1990s. The new government, under Prime Minister Jean
Chrétien, soon began to put Canada’s fiscal house in order, with the federal government posting surpluses in
every fiscal year since 1996. It was aided in this by a rapidly growing North American economy, now more
integrated than ever since the passage of the Canada– United States Free Trade Agreement of 1988, renamed
the North American Free Trade Agreement (NAFTA) with the inclusion of Mexico in the early 1990s.
The dot.com bust of the early 2000s left Canada largely unaffected, though market valuations on the Toronto
Stock Exchange, the country’s main securities exchange, fell. Thus, while the United States fell into recession in
2002, the Canadian economy continued to grow, one of the few times in history that has occurred.
Trade ministers of the United States, Canada, and Mexico (left to right) convene in Vancouver in 2007 for their

annual meeting to oversee the North American Free Trade Agreement. Free trade has been a boon to the
Canadian economy, one of the world’s most stable. (Bloomberg/Getty Images)
 Financial Crisis of the 2000s
Canada has not been as fortunate in the wake of the financial crisis originating in the U.S. housing market in
2007, though the Canadian banking system has weathered the crisis better than that of many other countries.
There are a number of reasons for this. First, Canada has much stricter lending rules for mortgages. By law,
homebuyers must put down 20 percent of the cost of the home and there are strict rules against adjustable rate
mortgages. Unlike Americans, Canadians do not receive tax breaks for mortgage interest, thereby discouraging
people from taking on mortgages they could not normally afford. At the same time, there are tighter rules against
homeowners walking away from mortgages when they cannot make the payments. Such regulations have meant
that Canada avoided the subprime mortgage crisis experienced in the United States.
Canadian banks operate in similarly more cautious ways, both out of custom and because of tighter regulations.
They tend to operate in a more traditional manner, being more liquid and less highly leveraged, though some had
invested in the mortgage-backed securities that brought down a number of major U.S. banks.
Still, Canada is not expected to escape the reverberations of the financial crisis entirely. With more than 80
percent of its exports going to the United States, Canada cannot help but be affected by the deep recession south
of its border. In particular, timber exports have plummeted with the crash in housing starts in the United States.
More generally, falling commodity prices across the board—a result of lessening demand in a global downturn—
have undermined Canada’s economic growth, sending unemployment up from 6.4 percent in 2006 to nearly 9
percent by late 2008. By October 2011, it had fallen back to 7.3 percent.
Still, compared to the United States and other developed-world countries, Canada was in relatively good financial
shape through the first couple of years of the recession. Its government had the smallest debt load per capita of
any G-8 country and its fall in home sales—at 20 percent—was not nearly as dramatic as that in the United
States. And unlike most industrialized world economies, Canada experienced positive growth—2.7 percent—in its
GDP in 2008, but saw that figure fall by 2.9 percent in 2009. For 2010, the country returned to the black, with
GDP growing by 3.1 percent.
James Ciment
 
See also:  United States. 
Further Reading
“Canada’s Economic Growth Offers Hope.” The Globe and Mail,  August 31, 2009.  “IMF Slashes Canada’s GDP Growth for
2009 and 2010.”
Johnson, Harry G. The Canadian Quandary: Economic Problems and Policies. Montreal: McGill-Queen’s University
Press, 2005. 
Naylor, R.T. Canada in the European Age, 1453–1919.  2nd ed. Montreal: McGill-Queen’s University Press, 2006. 
Statistics Canada:  http://www.statcan.gc.ca
Capital Account

 
The capital account constitutes one part of the balance of payments of a given country, the other two components
being the current account and the financial account. The balance of payments is the cumulative measure of net
income coming into a country via international trade, overseas investments, and other foreign sources. The capital
account records transfers of assets, such as patents, trademarks, stocks, and bonds, while the current account
includes payments for exported and imported goods and services, interest, and foreign aid.
The financial-derivatives account records financial instruments that allow or obligate their owners to buy, sell, or
exchange financial assets at an agreed time (or period) in the future, at an agreed price, or in an agreed amount.
Such instruments are known in the financial community as forwards, futures, options, and swaps. Employee stock
options are also a part of this account.
Other investment accounts record all assets and liabilities that are not reflected in the capital, financial, or reserve
assets accounts. Hard currency and deposits, trade credits and advances, loans, and other assets and liabilities
are reflected there.
The reserve-assets account records the assets of a nation’s central bank—such as the U.S. Federal Reserve
(Fed), including the assets that can be used to meet the country’s balance-of-payments needs. The reserve
assets must actually exist and be readily available; potential assets, or those that may be realized in the future,
may not be included. Reserve assets can take a number of forms, including foreign currency, monetary gold,
International Monetary Fund (IMF) reserves, and other liquid assets. A positive sign associated with a reserve-
asset account means that, during the specified period, the country’s reserves have decreased; a negative sign
means they have increased.
A country with a positive capital account balance (and negative current account balance) is called a “net
borrower”; a country with a negative capital account balance (and positive current account balance) is called a
“net lender.” However, because not all capital takes the form of loans, a country with a large current account
deficit may not actually have large foreign loans. Instead, it may have attracted a lot of foreign direct investment,
which can have important economic benefits.
Most economists argue that countries should maintain relatively liberal capital account policies, which allows for
the free flow of capital across borders, stimulating trade and, theoretically, increasing the prosperity of all
countries. But if countries remove virtually all barriers to capital flows, they risk being swept along by market
fluctuations. For example, many East Asian countries removed most of the barriers to capital flows from the late
1980s onward, allowing foreigners to invest heavily in East Asian securities. Soon, these economies became
heavily dependent on having a positive capital account balance. But when fears about the values of those
securities caused foreign investors to pull out their capital, Asian economies found themselves with their foreign
reserves depleted, causing depreciation of local currencies and a regional crisis known as the Asian financial crisis
of 1997–1998. The event has led some financial analysts to call for more capital account controls.
Economists also cite the role of America’s capital account deficit as a factor in the 2008–2009 financial crisis and
the 2007–2009 recession. With the country running a major current account, or balance-of-trade, deficit, it needed
a capital account surplus to finance the gap between the imports and exports. This need drew in large financial
investment from abroad, especially from the exporting economies of East Asia and the oil exporting nations. All of
this new capital flowing into the United States contributed to the run-up in asset prices, including mortgage-backed
securities and, indirectly, housing. But to keep the flow of foreign capital coming, the Fed had to raise interest
rates on government bonds. When it did so, it caused rises in adjustable rate mortgages, which were tied to the
Fed rate. This contributed to the rapid decline in housing prices that most economists say was at the heart of the
financial crisis and recession of 2007–2009.
Tiia Vissak

 
See also:  Balance of Payments;  Current Account;  Exchange Rates. 
Further Reading
International Monetary Fund (IMF). Balance of Payments and International Investment Position Manual.  6th ed.
Washington, DC: International Monetary Fund, 2010. 
International Monetary Fund (IMF). Balance of Payments Statistics Yearbook. Washington, DC: International Monetary
Fund, 2008. 
Capital Market
 
The capital market is the market for both debt and equity securities. It is where businesses and governments turn
when they need to raise long-term capital, usually defined as capital that they can use for more than a year.
Capital markets are divided between primary and secondary markets. In the primary market, businesses,
governments, and public sector institutions sell debt (bonds) or equity (stocks) via securities dealers, such as
brokerages and investment houses. These dealers then sell the stocks and bonds to investors in a process known
as underwriting. Secondary markets are where investors buy and sell the stocks and bonds among themselves.
For stocks this is usually done through exchanges, such as the New York Stock Exchange, while bonds and
structured products, such as derivatives, are sold directly from investor to investor in the over-the-counter market.
In some countries, such as the United States and the United Kingdom, business firms typically use primary capital
markets, whereas in others, such as Germany and Japan, businesses rely on the banking system more. The main
difference between the two ways of raising money is that the access to funds in the former case depends on the
combined judgment of the market participants, whereas in the latter it depends on the judgment of the particular
bank to which the project is presented. The many agents that participate in primary capital markets can provide a
more reliable profit outlook for a project than a particular bank, though the latter may have more accurate
knowledge of a particular local business than capital investors do.
In the primary capital markets, the borrowers get money and the lenders get documents that specify the
conditions under which the money is lent. These documents, or titles, fall into to two basic categories: equity and
bonds. The lender who purchases equity acquires a share in the ownership of the company and, accordingly,
acquires the right to share in its profits. The portion of the profits that the company’s management decides to pay
out to shareholders is called “dividend”; accordingly, an investor who purchases equity acquires the right to be
paid dividends. By contrast, the purchaser of bonds issued by a company (or government entity) does not acquire
an ownership share and, accordingly, acquires no right to dividends. The obligation of the bond issuer to the bond
purchaser is to repay the total amount, plus interest as stipulated. When a company or government borrows
money by issuing bonds, it is said that it finances not with equity, but with debt.
Not every business project or enterprise has access to the primary capital markets. A firm can “float an issue”
(borrow funds) only if the authority that runs the market allows it to do so. A company admitted to a capital market
—often with an initial public offering (IPO)—becomes a “publicly listed firm”; in colloquial terms, it is also said that
the firm “goes public.” This does not mean that the company becomes nationalized, only that it has been admitted
to the capital market, where the public can purchase its issues. Nor is access to the primary capital markets

restricted to private business enterprises. Public institutions and, in particular, the government can float issues—
that is, borrow money—in the primary capital markets.
Shares and bonds, private or public, can be either negotiable or nonnegotiable. Titles are negotiable if the owner
can transfer them to a third party; titles are nonnegotiable if the owner is barred from doing so. Negotiable titles
are sold and purchased in a secondary capital market. More attention is often paid to secondary markets such as
the stock market, perhaps because more than ten times as much wealth exchanges hands in the secondary
market than in primary markets. However, the primary market is equally important because it is the original source
of investment funding. Obviously, there are no secondary capital markets for nonnegotiable titles. Secondary
capital markets arise when the titles issued in the primary market are negotiable and their owners are willing to
exchange them either for other titles or for money. The reason why lenders may want to sell or purchase their
titles is to maximize the yield on their investment. This may require, first, to exchange one’s titles for other titles
that promise larger dividends. Second, the desire to maximize the return on investments in capital markets could
lead investors to exchange titles they currently possess for other titles whose price they expect to rise in the
future; if that expectation if fulfilled, the investor makes a capital gain. Investors who expect changes in the
dividend policy of publicly traded companies or who expect increases in the prices of titles are said to be
speculating. Thus, both primary and secondary capital markets are speculative; in both cases, the investor acts in
accordance with an expectation of future dividends on asset prices.
In addition to equity and debt, participants in the capital markets can issue titles, which confer the right or the
obligation to buy or sell a given title at some future time and/or at some definite price. These titles are called
“derivatives.” If the contract stipulates an obligation to buy or sell the underlying title at some future date, then the
derivative title is called an “option.” If, by contrast, the property of the derivative title confers a right to buy or sell
the underlying title, it is called a “future.” Futures and options are issued in primary capital markets and traded in
secondary capital markets. (With regard to futures, it is also necessary to differentiate commodity markets—in
which the participating agents buy and sell titles that stipulate prices for future deliveries of ordinary commodities,
such as pork bellies or corn—from capital markets—in which the item being traded is money.)
Kepa Ormazabal
 
See also:  Financial Markets;  Stock and Bond Capitalization;  Stock Markets, Global. 
Further Reading
Chisholm, Andrew. An Introduction to Capital Markets: Products, Strategies, Participants.  The Wiley Finance Series.
Hoboken, NJ: John Wiley and Sons, 2002. 
Fabozzi, Frank J., and Franco Modigliani. Capital Markets: Institutions and Instruments.  4th ed. Upper Saddle River,
NJ: Prentice Hall, 2008. 
Capital One
 
Based in McLean, Virginia—located in Fairfax County, the second-richest county in the country and home to
seven Fortune 500 companies and Freddie Mac—the Capital One Financial Corporation is one of America’s most
successful financial companies, built primarily on the mass-marketed credit cards it pioneered in the 1990s.

Thanks to the quantity of direct mail the company sends to offer its services to consumers, Capital One is the
fourth-largest customer of the United States Postal Service and the second largest of the Canadian post office.
Capital One was founded in 1988 by Richard Fairbank (chairman and CEO to the present) and his British partner,
Nigel Morris (chief operating officer until retiring in 2004), to provide more customized financial services to
customers. Shortly after retiring, Morris was involved in the Cash for Honors scandal in the United Kingdom; his
donation of 1 million pounds to the Labour Party was part of an investigation into political contributions given in
exchange for peerages and other special favors.
Originally a monoline business specializing in consumer loans, Capital One weathered the circumstances that
drove other single-service financial companies like the Money Store out of business and saw others like First USA
scooped up in acquisitions. Underlying Capital One’s success, and part of Fairbank’s initial business plan, was a
new sophistication in data collection and information processing. Capital One radically expanded the breadth and
quantity of customer data it collected; ten years after its founding, the company had one of the largest consumer
information databases in the world. This approach, among other things, allowed Capital One to offer loans and
other financial services to prospective customers who represented too great a risk to lenders who relied on
conventional metrics. Capital One’s willingness to extend credit to college students and others with little or no
income, and to offer secured credit cards to those rebuilding bad credit or emerging from bankruptcy, prefigured
the craze for subprime mortgage loans in the early twenty-first century.
As in the case of many subprime mortgage lenders, Capital One’s ethics and practices have been called into
question. In order to offset the risk of issuing credit cards to consumers with little means of paying them off, for
instance, the company charges high penalty fees and interest rates sometimes so high that the cards cannot be
issued in states with strict anti-usury laws. In addition, Capital One has a reputation for offering some customers
multiple cards, each with its own limit and fees, rather than raising the limit on an existing card; this can quickly
lead to hundreds of dollars per month in fees without paying off the principal. Some Capital One cards are issued
with a debt already approaching the credit limit, because of annual and one-time processing fees.
Capital One Financial Corporation is the holding company for Capital One (the original credit card issuer), Capital
One Bank (a retail bank chain), and Capital One Auto Finance (COAF). Like the credit card company, COAF built
its fortune on specifically tailored marketing, offering its services through direct mail targeted at specific consumer
types. Later acquisitions and diversification have made it the largest online lender of car loans, with approvals
communicated online in a matter of minutes. Although Capital One’s international operations are not as significant
as those of other financial companies, Canadian and British operations are extensive.
The second half of the 2000s brought several significant changes at Capital One. The company began retail
banking operations under the Capital One Bank brand after acquiring New Orleans– based Hibernia National Bank
(negotiating a lower price than originally offered, in the wake of Hurricane Katrina) in 2005. Then, with financial
crisis looming later in the decade, the company abandoned subprime mortgage lending in response to investor
demands. In 2009, however, after the collapse of that market and the devastation to the entire credit industry, the
company cut its quarterly dividend by 87 percent in order to preserve capital in a volatile economic environment.
At the same time, Capital One raised interest rates on many existing credit card accounts, offering holders the
option of closing the account and paying off the balance under the previous terms. In November 2008, Capital
One received just over $3.5 billion in bailout funds from the Troubled Asset Relief Program (TARP), established by
the federal government to assist financial institutions facing liquidity problems. In June 2009, Capital One repaid
the federal government, freeing itself from restrictions placed on TARP recipients.
Bill Kte’pi
 
See also:  Banks, Commercial. 
Further Reading

Chin, Amita Goyal, and Hiren Kotak.  “Improving Debt Collection Processes Using Rule-Based Decision Engines: A Case
Study of Capital One.” International Journal of Information Management 26:1 (February 2006): 81–88. 
Clemons, Eric K., and Matt E. Thatcher.  “Capital One Financial and a Decade of Experience with Newly Vulnerable
Markets: Some Propositions Concerning the Competitive Advantage of New Entrants.” Journal of Strategic Information
Systems 17:3 (September 2008): 179–189. 
Catastrophe Theory
 
Catastrophe theory is a branch of mathematics that describes dynamic phenomena in which sudden, dramatic
jumps, or “catastrophes,” may occur in apparently stable, calm situations or during regular, continuous processes.
The theory originates from the works of the French mathematician René Thom in the 1960s, though some
elements had been articulated by earlier thinkers, including the French mathematician Jules-Henri Poincaré in the
late nineteenth century and even, to some extent, Leonardo da Vinci during the Renaissance. In addition to
mathematics and finance, catastrophe theory has been applied to other areas of study, including biology, geology,
chemistry, archaeology, physics, medicine, demographics, psychology, education, politics, road traffic, and
aviation.
Catastrophe theory became popular among academic economists in the 1970s, after the Japanese-born British
mathematician Sir Erik Christopher Zeeman—who is credited with coining the term—used it to study financial
market dynamics, in particular booms and busts in the stock market. For that analysis, Zeeman divided investors
into “fundamentalists”—who can gauge the true value of assets, buying them when the market value is below that
level and selling them when it is above; and “chartists”—who do not know the true value and therefore “chase
trends” (i.e., buy assets after prices rise and sell them after prices fall). In Zeeman’s view, the behavior of the
stock market reflects the interactions of these two types of agents, causing underlying instability and unexpected
crashes in capital markets.
At the macroeconomic level, catastrophe theory has been used to study the relationships among national income,
capital stock, gross investment, savings, and wealth. In addition, it has been used in studying the links between
actual inflation, unemployment, and expected inflation, as well as studies of urban and regional economics, bank
failures, real-estate prices, consumer purchasing behavior, business management, the decision-making of
monopolists, the dynamics of balance of payments (especially trade balance), and foreign currency speculation. An
example is the Asian financial crisis of the late 1990s, in which inflows of vast amounts of capital from outside the
region suddenly evaporated and then reversed themselves as a result of a single government’s—specifically,
Thailand’s—inability to prop up its currency, the baht.
Since the late 1970s, catastrophe theory has also been used as well to explain several cyclic economic
phenomena at the microeconomic level that cannot be adequately described with traditional analytical techniques
—such as why some firms continue producing goods even when market prices do not cover their production costs
(i.e., they are suffering losses), while others do not restart production when prices are considerably higher than
they had been before the shutting down. Thus, during an economic recession, some firms continue producing
goods longer than they should, while during periods of economic recovery, some firms do not resume production
soon enough.
Catastrophe theory has also been used to explain other illogical economic behavior—such as why firms decide to
open new plants too late to maximize their profits (after demand has peaked) or do not close less efficient plants

soon enough (amid declining demand). Likewise, it can show why some companies raise prices as demand
diminishes or lower prices as demand increases. Such decisions are explained by the lack of complete information
regarding costs and revenues, resulting in overproduction during economic recessions and delays or
underproduction during economic recoveries.
The popularity of catastrophe theory has generally declined since the 1980s, the theory having been the object of
questioning, criticism, and even ridicule among some scholars. Critics have variously contended that the theory or
its applications—including those in economics—have relied excessively on qualitative methods, have been
incorrectly calculated (using arbitrary variables or improper statistical methods), or have relied on overly narrow or
restrictive mathematical assumptions. According to another line of criticism, catastrophe theory is suitable only for
the study of very specific—and unrealistic—economic systems and scenarios. Despite the waxing and waning of
academic interest, and despite the various criticisms, catastrophe theory remains actively and widely applied in
economics, business, and related disciplines.
Tiia Vissak
 
See also:  Shock-Based Theories. 
Further Reading
Balasko, Yves.  “Economic Equilibrium and Catastrophe Theory: An Introduction.” Econometrica 46:3 (1978): 557–569. 
Dodgson, J.S.  “Kinks and Catastrophes: A Note on the Relevance of Catastrophe Theory for Economics.” Australian
Economic Papers 21:39 (1982): 407–415. 
Pol, Eduardo.  “Theoretical Economics and Catastrophe Theory: An Appraisal.” Australian Economic Papers 32:61
(1993): 258–271. 
Rosser, J. Barkley, Jr.  “The Rise and Fall of Catastrophe Theory Applications in Economics: Was the Baby Thrown Out
with the Bathwater?” Journal of Economic Dynamics & Control 31:10 (2007): 3255–3280. 
Scott, Robert C., and Edward L. Sattler.  “Catastrophe Theory in Economics.” Journal of Economic Education 14:3
(1983): 48–59. 
Varian, Hal R.  “Catastrophe Theory and the Business Cycle.” Economic Inquiry 17:1 (1979): 14–28. 
Zhang, Wei-Bin.  “Theory of Complex Systems and Economic Dynamics.” Nonlinear Dynamics, Psychology, and Life
Sciences 6:2 (2002): 83–101. 
Central America
 
Consisting of seven small nations—Belize, Guatemala, Honduras, El Salvador, Nicaragua, Costa Rica, and
Panama—Central America, sometimes referred to as Mesoamerica (though the latter appellation also includes
southern Mexico) is an isthmus that connects North America to South America. Approximately 202,000 square
miles (523,000 square kilometers) in area and with a population of about 42 million, Central America is a largely
Spanish-speaking and Roman Catholic region, though there are sizable English-speaking and Protestant
minorities. Most of the people in the region are of mixed Spanish and Indian heritage, or mestizos, with significant
populations of persons of other European and African backgrounds.

Economic development in the region varies. Nicaragua and Honduras are among the poorest nations in the
hemisphere, while Panama and Costa Rica rank as middle-income countries. The economy of the region rests on
commercial agriculture, light manufacturing, tourism, and, in the case of Panama—with its ocean-linking canal—
trade and finance.
Central America was home to millions of indigenous people prior to the arrival of Europeans at the end of the
fifteenth century, and, along with southern Mexico, was home to the Mayan civilization. Aside from Panama, which
offered the quickest overland route between the mines of Peru and the Atlantic Ocean, the region was largely
bypassed by the Spanish in the early years of their conquest of the Americas due to its thick jungles, stifling
equatorial climate, and lack of precious metals. Land was parceled out to conquistadores, who developed large
plantations and ranches worked by indigenous and mestizo workers. In 1540, the Spanish organized much of the
territory into the Captaincy General of Guatemala.
With the independence of Mexico in 1821, the region was absorbed into the Mexican Empire for two years, before
gaining independence as the United Provinces of Central America, with its capital in Guatemala. Ideological and
regional disputes led to great instability and dissolution into five separate countries—Costa Rica, El Salvador,
Guatemala, Honduras, and Nicaragua—between 1838 and 1840. Meanwhile, Panama had become a province of
Colombia with that country’s independence from Spain in 1810. Remote Belize was settled by the British from the
seventeenth century—who began bringing in black Jamaican slaves as timber workers—and became a British
colony (known eventually as British Honduras) in the late eighteenth century, remaining part of the empire until
independence in 1981.
With the breakup of the United Provinces, the various countries of Central America followed parallel but different
economic and political trajectories. With large indigenous and mixed-race populations ruled over by a European-
descended elite, the countries remained economically underdeveloped and politically unstable. They were often
ruled by military dictators who had come to power in various violent and nonviolent golpes, or coups, through
much of the nineteenth century, and they remained underdeveloped, exporting small amounts of agricultural
products, such as coffee, to North America and Europe. With power in the hands of a tiny landed elite,
manufacturing was slow to develop and the middle class remained a tiny minority, largely located in the various
capitals of the region.
Around the turn of the twentieth century, the various countries of the region came increasingly under the political
and economic hegemony of the United States. Guatemala and Honduras, in particular, became major exporters of
bananas to North America with the economies of these two countries, as well as their politics, dominated by the
U.S.-owned United Fruit Company. Nicaragua saw a U.S. occupation from 1912 to 1933, resisted by an
insurgency under Augusto César Sandino, while Panama came into existence as a country separate from
Colombia in 1903 with the assistance of the United States, which soon began construction of a transoceanic canal
that was completed in 1914.
By the middle of the twentieth century, however, two of the region’s countries had embarked on efforts at
economic and political reform. In Guatemala, voters put into power leftist president JacoboÁrbenz, who began to
challenge the United Fruit Company’s power, including its control of about 40 percent of the country’s arable land.
Following a brief civil war in 1948, Costa Rica abolished its army and implemented political reforms that
established a working democracy in the nation. While Costa Rica’s efforts were successful, Guatemala saw its
reformist president overthrown in a U.S.-sponsored coup. Meanwhile, Nicaragua remained under the control of
dictator Anastasio Somoza García, who had been put into power with the help of U.S. occupation forces, while El
Salvador continued to be effectively ruled by the so-called fourteen families of wealthy landowners.
Continuing inequalities in wealth and a lack of land reform led to insurgencies in much of the region from the
1960s through the 1990s, fought vigorously by the military or conservative civilian governments in power, often
with military aid from the United States. Guatemala saw a bitter civil war begin in the 1960s that would eventually
result in the deaths of 200,000 persons, most of them civilians. Meanwhile, in 1968, a populist general named
Omar Torrijos came to power in a coup in Panama, vowing to win back control of the canal from the United

States, which he achieved by treaty in 1977, with a takeover date of 1999. In El Salvador, some 75,000 persons—
again, largely civilians—were killed in an insurgency that began in 1980. And in Nicaragua, the Sandinista
National Liberation Front (FSLN, its Spanish acronym) rose up against the Somoza dictatorship in the 1970s.
The Sandinistas were successful in coming to power, overthrowing the dictatorship in 1979. The new government
then tried to implement land reform and other measures to equalize the great discrepancies of wealth in the
country. But once in power, the FSLN was confronted by an insurgency of its own, consisting largely of disaffected
remnants of the Somoza dictatorship backed by the United States and largely based in Honduras.
U.S. support for right-wing governments in Central America had always been premised on keeping the countries
of the region from falling under the influence of communists, backed by Cuba and the Soviet Union. But with the
end of the cold war that urgency evaporated, and eventually the various conflicts in the region wound down, either
through negotiated settlements, as in El Salvador in 1992 and Guatemala in 1996, or through a change in
government, as was the case of Nicaragua in 1990. A U.S.-led invasion in late 1989 ended the rule of dictator
Manuel Noriega in Panama.
Since the return to peace and civilian government in the 1990s, the various countries in the region have
embarked upon free-market reforms, opening their countries up to foreign investment, which has led to the
development of more light manufacturing. Costa Rica has emphasized ecotourism to draw in foreign capital while
Panama has emerged as a major center of trade and finance, especially since its takeover of the canal. Many of
the countries of the region also depend heavily on remittances from the hundreds of thousands of Central
American immigrants—both legal and illegal—working in the United States.
In 2004, the countries of Central America—along with the Dominican Republic, but minus Belize and Panama—
signed the Central America Free Trade Agreement with the United States. CAFTA, as the agreement is known,
lowered tariffs for Central American imports into the United States while easing restrictions on U.S. investment in
the region. CAFTA has accelerated the economic integration between the United States and Central America and
was seen by many experts as a boon to the latter’s economy. Opponents in the latter region, however, argue that
it will lead to further U.S. domination of the Central American economy. Moreover, they point out, increased
integration will expose Central American economies to the U.S. business cycle. And, indeed, exports to the United
States have declined somewhat as a result of the 2007–2009 recession, and growth rates have slowed
significantly or fallen in all of the Central American countries, barring Panama.
James Ciment
 
See also:  Latin America;  Mexico. 
Further Reading
Desruelle, Dominique, and Alfred Schipke, eds. Economic Growth and Integration in Central America. Washington,
DC: International Monetary Fund, 2007. 
Jaramillo, Carlos Felipe, et al. Challenges of CAFTA: Maximizing the Benefits for Central America. Washington, DC: World
Bank, 2006. 
Pearcy, Thomas. The History of Central America. Westport, CT: Greenwood, 2006. 
Robinson, William. Transnational Conflicts: Central America, Social Change, and Globalization. New York: Verso, 2003. 

Chile
 
A long sliver of a nation, tucked between the Andes Mountains and the Pacific Ocean in the southern cone of
South America, Chile, a nation of about 17 million, has an economic history that shares many experiences with
other Latin American economies. Its economic development has been inconsistent and, at times, volatile: export
booms and busts occurred, import substitution industrialization policies were adopted in the twentieth century, and
those policies were recently abandoned in favor of trade liberalization. Chile’s economic history differs from the
rest of Latin America, however, in that its economic ups and downs did not exactly parallel those of other countries
in the region. Chile’s distinctive domestic economic policies interacted in unique ways with its export booms and
busts. It experienced subpar economic growth for most of the post–World War II period, when most of Latin
America grew rapidly, and its very recent economic growth performance has been substantially better than that of
its neighbors.
 Early Development
Before gaining independence from Spain in 1818, the region that is today Chile was sparsely populated by both
descendants of Spanish settlers and native groups. Probably fewer than 1 million Chileans lived and farmed in the
central valley at the start of the nineteenth century. Central valley farmers exported tallow, beef, and grain to other
Spanish colonies in present-day Peru and Colombia, and there were mines in the Andes Mountains that defined
the long eastern border of the country. Most of the northern deserts remained largely unpopulated, and native
societies continued to occupy most of the southern part of the country until the nineteenth century.
The export of food continued after Chile and Spain’s other South American colonies gained their independence.
Agricultural exports received a boost from the California gold rush in 1849, as the direct ocean supply route from
Chile was more convenient than overland routes to California from the eastern United States.
Chile’s first major export was nitrate. In the 1870s, Chilean entrepreneurs opened nitrate mines on land claimed
by Bolivia and Peru. Chile won the War of the Pacific (1879–1884), fought over the disputed territory, and
extended its national territory to its present northern borders. Bolivia was permanently denied access to the Pacific
Ocean, an issue that troubles Bolivian-Chilean relations to the present day.
Chilean mines soon faced competition from British mining firms that opened large-scale nitrate mines to supply
the growing world market. Nitrate was a critical raw material for manufacturing explosives, and it was a basic
ingredient in agricultural fertilizers that were increasingly applied by farmers in Europe and the United States in
the latter half of the nineteenth century. Nitrate accounted for half of Chile’s exports and up to one-quarter of its
gross domestic product (GDP) by 1890.
The nitrate mines in the remote northern region of Chile are an example of enclave development. British mines
employed few Chileans, and much of the revenue from exporting nitrate accrued to the foreign firms. However, the
Chilean government learned to capture a greater share of the nitrate earnings by raising taxes on nitrate exports
to over 30 percent. In the early twentieth century, nitrate taxes accounted for three-quarters of government
revenues.
 The Second Export Boom: Copper
Chile’s second major export product was copper, mined in the remote Andes. By the early twentieth century, the
U.S. copper companies Anaconda and Kennecott controlled Chile’s major copper deposits. The global market for
copper grew rapidly as electric power and telephone communications expanded throughout the world. The Chilean
government again sought to capture an increasing share of the value of copper exports, but copper tax revenues
never exceeded 20 percent of all government revenues. For one thing, the foreign firms fought back with threats

to leave the country. For another, they actively lobbied domestic politicians to resist calls for higher taxes.
 A Larger Economic Role for Government
Chile’s democracy was dominated by the country’s economic elites and, occasionally, the military. The political
system did transmit enough pressure from all segments of the population to induce the government to undertake
popular programs in education and social services in addition to the infrastructure and transportation projects
favored by businesses and large farmers. Chile was well ahead of most of the rest of Latin America in terms of
literacy, living standards, and health. The military sometimes sided with labor during the frequent labor disputes
early in the twentieth century.
Politicians were able to satisfy most Chilean factions because nitrate and copper export taxes provided the
revenues. The Chilean government spent over 15 percent of national income early in the twentieth century, a very
high share at that time in history, but Chileans did not pay many of the taxes. By the end of the 1920s, Chile’s per
capita real income was equal to that of Germany and the Scandinavian nations.
The government’s role was expanded at the start of the Great Depression of the 1930s. And, even though Chile
owed its relatively high (for Latin America) levels of government-provided education, health care, transportation,
and social services to export tax revenues, when exports fell by 80 percent and GDP per capita fell by 42 percent
between 1929 and 1932, the political response was to reduce Chile’s dependence on exports. Chile introduced
high tariffs and import quotas to protect domestic producers who “substituted” foreign imports. Chile thus
anticipated the import substitution industrialization (ISI) policies popularized by the United Nations Economic
Commission for Latin America and its director, Raúl Prebisch, and embraced by most Latin American countries
after World War II.
Chile soon went beyond restricting imports. In the late 1930s, the government established the Corporación de
Fomento de la Producción (CORFO) to fund government-owned industries in steel, energy, and food processing
when private investors appeared reluctant to expand domestic output. The results of these policies were
disappointing. Chile’s per capita real income grew annually at rates of 1.3 and 1.9 percent during the 1950s and
1960s, respectively, when Latin America’s overall per capita real income grew at 2.2 and 2.6 percent. As a result,
Chile did not sustain its per capita GDP relative to the European countries with which it had enjoyed near parity at
the start of the century. Instead, Chile’s per capita real GDP regressed to the Latin American average.
Increased domestic taxes to pay for growing government activities split political sentiments more clearly between
those who wanted the government to reduce its share of the economy and those who supported more socialistic
policies and the expansion of government social and economic programs. After World War II, left-of-center parties
demanded that the government take over the copper industry so that Chile could capture the full surplus from its
exports. Many politicians and commentators accused the U.S. copper firms of intentionally reducing production
and, therefore, tax revenues in order to pressure the government.
More fundamentally, Chile’s ISI policies were costly because the country’s domestic market of less than 7 million
low-income people was not large enough to sustain large-scale manufacturing industries. An example of the
failure of import substitution was Chile’s attempt to establish a domestic automobile industry in the 1960s. Some
twenty local factories used the ban on imports of complete automobiles to begin assembling autos from imported
parts. A total of 8,180 automobiles were assembled in 1963 and 7,558 in 1964. The near-artisan production
methods resulted in cars costing five times their overseas price, with inferior quality. Once launched, however, ISI
policies were not easily dismantled because protectionism created a constituency of domestic industries and
workers who feared their demise if protectionist import tariffs and quotas were abandoned.
Three political factions fought for power during the post–World War II period: the conservative Right, the socialist
Left, and a “centrist” party that favored some modifications in the ISI regime but also promised to nationalize the
copper industry. The centrists often won by attracting votes from conservatives who feared that the Left would win
if they split the center-right vote. In the presidential election of 1970, however, the conservatives backed their own

candidate, and the left candidate, Salvador Allende, won with only 36.3 percent of the vote, barely above the
Right’s 34.9 percent and the center’s 27.8 percent.
Against strong resistance from established private business and landholders, the Allende administration not only
completed the nationalization of the copper industry started by earlier governments, but began nationalizing many
other foreign and Chilean industries. Allende also honored his campaign promise for a major redistribution of land,
something many earlier governments had promised but never actually carried out. There were reports that some of
Allende’s government officials were urging workers to occupy factories and invade farms in order to spur owners
to agree to government takeovers. In any case, occupations and invasions soon became a common occurrence.
After a 7 percent rise in per capita income in 1971, a direct result of expanded government programs and a hike
in the minimum wage, per capita GDP declined in 1972 when private investment collapsed. The Allende
government accused the business sector of intentionally sabotaging the economy, and it pressed ahead with its
nationalization of private industry. The Right argued that investment fell because of the threats of government
nationalizations and worker occupations. Inflation rose as goods became scarce with the disruptions of production,
and the government began printing money to fund the difference between falling tax revenues and the rising cost
of nationalizations, unemployment benefits, and new government benefits. Under cover of the economic difficulties,
the U.S. Central Intelligence Agency (CIA) helped a Chilean military junta launch a coup in 1973 in which Allende
was killed. General Augusto Pinochet headed the new military government.
The Pinochet regime reversed Allende’s socialist economic policies, but economic growth did not recover. The
privatization of government assets was a major component of Pinochet’s new economic program, designed by
mostly U.S.-educated Chilean economists who came to be known as the “Chicago boys” because several held
degrees from the University of Chicago. Chicago is famous for its advocacy of free-market and free-trade policies,
or what most Latin Americans describe as “neoliberal ideology.” Privatization was difficult because of the lack of
private Chilean capital to buy the government-owned firms, banks, and utilities. At the urging of the government,
purchases were highly leveraged through bank borrowing. A large portion of the bank funds were acquired through
foreign borrowing, enabled by foreign banks anxious to lend to a country led by what they saw as a pro-business
regime. When the worldwide debt crisis caused foreign lending to stop in 1982, Chilean private banks and their
highly leveraged clients defaulted en masse. Domestic lending immediately stopped, real investment declined,
unemployment surged to 20 percent, and GDP per capita fell by 15 percent in 1983 and another 5 percent in
1984. The economic collapse reduced real per capita GDP below its level at the time of the 1973 military coup.
Domestic opposition to the Pinochet regime became quite visible despite the continued political oppression.
The Pinochet government responded with a more gradual approach, and policies to further liberalize foreign trade
and privatize the economy were balanced by more social expenditures to deal with Chile’s poverty, uneven
education, and unemployment. After 1985, Chilean per capita GDP rose again, albeit from low levels. Growth was
spurred in part by rising copper prices and taxes, and there was also rapid growth of agricultural exports, such as
fruits, farmed fish, wood products, and wines, from food-processing industries created decades earlier under ISI
policies. After a century during which Chile sought to reduce the role of exports in its economy, policy had clearly
come full circle. Like 200 years earlier, exports by miners and the central valley’s farmers drove economic
development. Would the nation’s per capita income once again approach that of the world’s most developed
countries as it had 100 years earlier?
After the restoration of democracy with the election of 1990, the new left-of-center government kept many
neoliberal “Chicago” policies in place while increasing the share of government revenue to social programs.
Chilean real per capita GDP grew 4.8 percent during the 1990s, a period when the rate for all of Latin America
grew 1.5 percent. Growth of per capita income was more than 3 percent per year between 2000 and 2008, and
poverty was significantly reduced. The global recession sharply cut Chilean exports in 2009, however, and growth
turned negative in the first quarter of that year despite fiscal stimuli in the form of direct payments to households
and direct support of bank lending. Chile’s return to the ranks of the wealthy countries was further set back by a
massive earthquake and tsunami in February 2010. Despite the setback, the country posted a healthy 5 percent
growth in GDP for that year.

Hendrik Van den Berg
 
See also:  Argentina;  Latin America. 
Further Reading
Collier, Simon, and William F. Sater. A History of Chile: 1808–1994. New York: Cambridge University Press, 1996. 
Economic Commission for Latin America and the Caribbean (ECLAC). Economic Survey of Latin America and the
Caribbean: 2008–2009. Santiago, Chile: ECLAC, 2009. 
Frank, Andre Gunder. Dependent Accumulation and Underdevelopment. London: Macmillan, 1967. 
Glade, William P. The Latin American Economies: A Study of Their Institutional Evolution. New York: American Book, 1969. 
Maddison, Angus. The World Economy: A Millennial Perspective. Paris: OECD, 2001. 
Skidmore, Thomas E., and Peter H. Smith. Modern Latin America.  3rd ed. New York: Oxford University Press, 1992. 
Thorp, Rosemary. Progress, Poverty, and Exclusion: An Economic History of Latin America in the 20th
Century. Washington, DC: Inter-American Development Bank, 1998. 
 
China
 
Encompassing one of the oldest continuous civilizations in human history, the People’s Republic of China is a
hybrid state, blending a dynamic and rapidly growing capitalist economy with a one-party, communist-inspired
political system.
Until about the eighteenth century, China was arguably the richest and most advanced economy on Earth, before
entering a long period of stagnation and foreign occupation, which came to an end with the Communist takeover of
1949. Over the next thirty years, a dictatorial political system and command style economy ensured that land and
virtually all of the means of production were controlled by the state, eliminating the gross inequalities in wealth
that existed prior to the revolution, but at the cost of famine, economic stagnation, and political repression.
An ideological reorientation following Communist Party chairman Mao Zedong’s death in the mid-1970s
encouraged the growth of markets in China and produced astounding growth rates beginning in the 1980s,
although the state retained control of large swaths of the economy. Despite the Asian financial collapse of the late
1990s—as well as various recessions in the West, including the very deep one of 2007–2009—China continued to
grow at near or above double-digit rates into the late 2000s, by which time the country had become a leading
exporter, as well one of the largest economies in the world.

 Pre-Communist Economy
China emerged as one of the first large, organized states around the end of the third millennium BCE and, within
the next thousand or so years, developed an infrastructure of cities, roads, and canals that allowed for extensive
internal trade in textiles, metals, handicrafts, and food, all administered by a centralized government. China was
also the birthplace of many key innovations in human history, including coinage, paper, and gunpowder over the
millennia.
But its history was also punctuated by great upheavals, in which central government control withered, long-
distance trade diminished, and conflict among local authorities hampered agriculture and manufacturing, until a
new strong and centralized authority, known as a dynasty, emerged and the country was once again united and
prosperous. There were roughly a dozen of these dynasties between the formation of the centralized state under
the Xia dynasty around 2,000 BCE and the Qing, or Manchu, dynasty, which began ruling from the late
seventeenth century CE.
While China under the early Qing dynasty saw great advances in internal trade and economic development, it also
turned inward, rejecting many of the new technologies and economic ideas of the burgeoning West. Still, China in
these years enjoyed a favorable trade balance that saw the West exporting large quantities of precious metals to
the country in exchange for China’s many coveted goods, including tea, silk, and various finely crafted artisan
products.
But by the nineteenth century, the Qing dynasty’s resistance to outside ideas left China vulnerable to an
aggressive and militarily advanced West. In the first half of the century, the country experienced a series of
humiliating military defeats—known as the Opium Wars—in which Great Britain forced China to allow Europeans
to export more freely to the country as a way to address the continuing trade imbalance with the West. Among
the products Britain imposed on China was opium from its colonies in South Asia.
Other European imperialists—as well as rapidly modernizing Japan—soon joined Britain and, by the end of the
century, many of the country’s coastal provinces, the most economically dynamic region of China, had been
carved into spheres of influence. Various outside powers were granted exclusive trading rights, known as
concessions, and their nationals were immune from Chinese law.
Traditionally, the Chinese viewed their country as the center of the world—indeed, the Chinese name for the
country was the “middle kingdom”; they believed China to be culturally superior to countries of the foreign
“barbarians,” as they referred to most non-Chinese peoples. The occupation of much of their country, then, was
seen as a humiliation, and led to the failed antiforeign Boxer Rebellion of 1900–1901 and the successful revolution
of 1911, in which the Qing dynasty was overthrown and replaced by a republic.
While the new Republic of China under Sun Yat-sen was less resistant to Western ideas and achieved a
modicum of centralized control over the vast Chinese empire, it was unable to dislodge the foreign concessions
and was ineffective in moving China from a peasant-based agricultural economy to a modernizing industrial one.
The country continued to stagnate economically, remaining a largely feudal state in which the peasants of the
countryside—who made up the vast majority of the population—lived under the iron rule of wealthy landlords and
warlords. Dire poverty for the masses, periodic local famines, and gross inequalities of wealth continued to be the
hallmarks of Chinese life.
 Communist Economy
Such conditions led many Chinese to turn to communism as a way to address continued political subjugation to
foreign powers, ongoing economic privation, and the unaddressed injustices of feudalism. By the 1920s, a
Marxist-inspired movement under Mao had emerged to challenge, both politically and militarily, the nationalist
government and foreign occupiers that were ruling China, in particular the Japanese. By the early 1930s, large
parts of the country were engulfed in a civil war that would continue until the Communist victory of 1949, albeit

with a long hiatus in which Nationalists and Communists united in opposition to Japanese invaders. (At the time of
the Communist victory, hundreds of thousands of Nationalists fled to the island of Taiwan where, under military
leader Chiang Kai-shek, they established an authoritarian capitalist system independent of Beijing. By the early
1990s, under the protective wing of the U.S. military, Taiwan had emerged as both a democracy and a flourishing,
export-oriented free-market economy, though Beijing has always maintained that it has sovereignty over the
“renegade province.”)
Meanwhile on the mainland, the ruling Communist Party utterly transformed the political, social, cultural, and
economic order of China. All political parties, other than the Communist Party of China, were banned, along with a
free press. Dissent of both the ideological and cultural varieties—China is home to a number of restless ethnic
minorities on its fringes—was ruthlessly crushed. At the same time, however, the Communists dismantled the
feudal order, executing or taking away the powers and property of economic elites, freeing peasants from the
arbitrary and brutal rule of local landlords.
Land was nationalized and peasants organized into farming collectives. As per the model of its early mentor, the
Soviet Union, the new People’s Republic of China government instituted tight, centralized control of the economy,
directing it toward rapid industrialization under a series of plans, including the Five Year Plan of the mid-1950s;
the Second Five Year Plan, or “Great Leap Forward,” of the late 1950s and early 1960s; and the Third Five Year
Plan, or “Agriculture First” program, of the mid-1960s.
Much was achieved under these programs, particularly in the early years, as China created a heavy industry
infrastructure and improved agricultural output. But there were also systemic failures and catastrophic missteps.
Despite much emphasis on the farming sector, economic inequality between the relatively prosperous cities and
impoverished countryside remained large. Efforts under the Great Leap Forward to expand industrial production
through decentralization—including such ill-thought-through ideas as “backyard” steel smelters—set back
economic development while a program to foster communal living and farming in the countryside led to massive
famine that cost the lives of millions of peasants.
A further setback occurred with the “Cultural Revolution” of the late 1960s. A largely political event inspired by
Mao’s efforts to renew the country’s revolutionary spirit, the Cultural Revolution saw disruptions in nonagricultural
production due to intensified political agitation and an anti-intellectual crusade that saw older and experienced
technicians replaced by inexperienced youths whose only qualification for running things was a fervent
revolutionary spirit.
 “Market Socialist” Economy
With the deaths of Mao and Premier Zhou Enlai in 1976, the radical forces within both the government and the
party found themselves outflanked by a more pragmatic leadership led by formerly disgraced Communist Party
official Deng Xiaoping, who, beginning at the party’s national conference in 1978, pushed for the introduction of
market forces within a socialist political and economic system. Technocrats rather than ideologues were put in
charge of national economic planning; peasants were allowed to cultivate private plots and sell their surplus on the
open market for profit; and local governments were given more leeway in deciding which industries to invest in.
Within a few years of these reforms, agricultural output, light industry, and the production of consumer goods
expanded dramatically.
The money for all of this investment came from two sources: bank deposits of newly prospering peasants and
workers, and foreign capital. Under Deng, the Chinese government set up a series of Special Economic Zones,
largely in the old trading regions of coastal southern China and centered on the capitalist, British-administered
enclave of Hong Kong, which funneled much of that foreign capital to entrepreneurs and entrepreneurially inclined
local governments. These enclaves also allowed foreign capitalists freedom from many government regulations
and offered them the ability to repatriate profits.
Much of the economic activity in these zones and in other light manufacturing centers was geared for the foreign

market. Between the late 1970s, when the economic reforms were first implemented, and the mid-1980s, the
value of exports and imports rose from about 10 percent of gross domestic product (GDP) to 35 percent.
Meanwhile production for domestic consumption was also expanding dramatically, as restrictions on
entrepreneurial activity, as well as price controls and subsidies, were lifted. By the mid-1980s, China had
developed its own unique brand of “market socialist” economics, with the central government still running much of
the country’s heavy industry and engaged in overall planning, but with microeconomic decisions left in the hands
of local authorities and businesspeople.
The southern city of Shenzhen, China’s first Special Economic Zone and a center of the nation’s economic
transformation, was said to grow by “one high-rise a day, one boulevard every three days” during the 1990s.
(Thomas Cheng/AFP/Getty Images)
The results of these reforms were nothing short of astonishing, as China achieved an annual average growth rate
of nearly 10 percent, measured in real, inflation-adjusted terms, between the early 1980s and the mid-2000s.
Over the same period, GDP by purchasing power parity (PPP)—a measure that equalizes differences in currency
values between countries—rose from just over $250 billion to about $8 trillion in 2008 while per capita income
soared from about $250 annually to more than $6,550 in 2008. By 2008, China had emerged as the second-
largest economy in the world, after the United States.
While the prosperity lifted hundreds of millions out of poverty, new problems emerged, as the economic disparities
between classes and between the city and countryside grew exponentially. Many Chinese also complained of
widespread corruption, in which well-connected officials—oftentimes with links to the Communist Party—used their
power to override local complaints about headlong development that displaced families from their homes with little
compensation. Human rights issues also emerged, as stories about exploited workers, the use of coerced prison
labor, and appallingly dangerous working conditions were exposed. Many of these issues were raised during the
large Tiananmen Square (Beijing) protests of 1989, which were brutally put down in June by authorities at the cost
of hundreds of lives.
There were also domestic and international concerns about the environmental costs of China’s rapid
industrialization—by the late 2000s, China rivaled the United States as the leading producer of greenhouse gases,
for instance—and the poor and sometimes dangerous quality of Chinese manufactured goods and agricultural
products, with stories of tainted food, toys, and cosmetics capturing headlines around the world. Western
countries, most notably the United States, also complained of China’s growing trade surplus, which, vis-à-vis the
United States, grew from $6 billion in 1985 to $273 billion in 2010. Some economists and politicians blamed this

on Beijing’s policy of maintaining an artificially undervalued currency, which made its exports that much more
competitive.
 Financial Crises of the 1990s and 2000s
Yet for all this, China continued to sustain remarkable growth and development, even while its neighbors and
much of the rest of the world stumbled. The financial crisis that hit other Asian economies hard in the late 1990s,
and which was triggered by massive foreign capital flight from local securities, left China largely untouched, since
most of the foreign money coming into the country had not been invested in securities but in brick-and-mortar
factories and other infrastructure. Thus, while many Asian economies went into recession following the 1997 crisis,
China continued to post annual growth rates in the 7 to 8 percent range, before climbing above 10 percent by the
mid-2000s.
Even more astounding was China’s and much of the rest of Asia’s response to the global financial crisis and
recession that began in 2007. With Western economies sinking into negative growth and consumers reducing
spending, many experts predicted a dramatic slowdown in the growth of both Chinese exports and the Chinese
economy overall. Indeed, the growth rate in Chinese exports to the world fell, but still increased at 17.2 percent in
2008. Yet, while growth rates declined somewhat, from 11.4 percent in 2007 to a still dynamic 9 percent in 2008,
they soon rebounded, reaching a blistering annualized rate of more than 15 percent in the second quarter of 2009.
In 2010, the annual growth rate had again climbed into double digits, at 10.3 percent.
Economists offered a number of possible explanations of why their predictions of a slow and tepid Chinese
recovery from the global slump proved so wrong. First, they said, the government actively loosened credit, allowing
for more investment. Second, energy-and resource-hungry China benefited from the falling oil and commodity
prices triggered by recession in Europe and North America. China has aided itself on this front by engineering
long-term deals with resource-rich countries—particularly in Africa and Latin America—to ensure itself not only
access to key commodities but also more stability in their pricing. In addition, China’s huge late-2008 economic
stimulus package of $586 billion—some 15 percent of GDP, as compared to America’s stimulus package of $787
billion in early 2009, representing about 5.6 percent of GDP—appeared to be having a more profound effect in a
country where consumers are less burdened with debt. That is, more of every yuan of stimulus money that landed
in Chinese consumers’ pockets went into purchasing rather than paying off debt.
Inevitably, the United States and other Western economies will emerge from the 2007–2009 recession and begin
growing again. But just as inevitably, say economists, they will not experience the same growth rates as a surging
China. With its huge population, its growing technological expertise, and the entrepreneurial energy of its people,
China is destined, say economic prognosticators, to emerge as the world’s largest economy, outpacing the United
States sometime in the late 2020s or early 2030s.
Not that there are no problems ahead. Some experts argue that unless rapid economic growth is maintained, the
increasing inequalities in Chinese life could lead to social unrest. In addition, there remains the growing
contradiction of a freewheeling economy coexisting with a rigid political autocracy. In many other formerly
authoritarian Asian countries that contradiction was resolved relatively peacefully in favor of democracy by the
1990s. But China has a long history, going back thousands of years, in which diminished centralized authority
leads to economic chaos and decline, a scenario that no doubt concerns both Chinese authorities and the Chinese
people.
James Ciment
 
See also:  Asian Financial Crisis (1997);  BRIC (Brazil, Russia, India, China);  Emerging
Markets;  Southeast Asia;  Transition Economies. 
Further Reading

DeBary, William Theodore, and Irene Bloom, eds. Sources of Chinese Tradition. New York: Columbia University
Press, 1999–2000. 
Eastman, Lloyd. Family, Fields, and Ancestors: Constancy and Change in China’s Social and Economic History. New
York: Oxford University Press, 1988. 
Mengkui, Wang, ed. China in the Wake of Asia’s Financial Crisis. New York: Routledge, 2009. 
Naughton, Barry. The Chinese Economy: Transitions and Growth. Cambridge, MA: MIT Press, 2007. 
Overton, Rachel H. China’s Trade with the United States and the World. Hauppage, NY: Nova Science, 2009. 
Philion, Stephen E. Workers’ Democracy in China’s Transition from State Socialism. New York: Routledge, 2009. 
Schiere, Richard. China’s Development Challenges: Public Sector Reform and Vulnerability to Poverty. New
York: Routledge, 2010. 
Spence, Jonathan. Gate of Heavenly Peace: The Chinese and Their Revolution, 1895–1980. New York: Viking, 1981. 
Spence, Jonathan. The Search for Modern China. New York: W.W. Norton, 1999. 
Wilson, Scott. Remade in China: Foreign Investors and Institutional Change in China. New York: Oxford University
Press, 2009. 
 
Chrysler
 
The smallest of the “Big Three” major American automobile manufacturers in the early twenty-first century,
Chrysler was also the first to go into—and emerge from—bankruptcy when the financial crisis and recession of the
late 2000s dried up credit financing and caused a plunge in automobile sales. In its efforts to survive, Chrysler
lobbied for and received billions of dollars in federal bailout money. This was the second time in the company’s
history that it had turned to Washington for help—the first was in the late 1970s. This time, however, the money
came with a catch. The Detroit-based company had to come up with a viable plan to return to solvency. After
failing to do so, it was forced to declare bankruptcy in 2009.

Chrysler workers leave the Warren (Michigan) Truck Plant as the company faced bankruptcy in April 2009. All
manufacturing was suspended during the restructuring process, 789 dealerships were terminated, and eight
factories were slated for closure in 2010. (Bill Pugliano/Stringer/Getty Images)
 Origins and Growth
Walter Chrysler, founder of the eponymous company, was a kind of white knight of the automotive industry of the
early twentieth century. A former railroad mechanic who had helped streamline production at the Buick division of
General Motors in the 1910s, Chrysler successfully restructured Willys-Overland, an independent manufacturer hit
hard by the recession of 1921–1922. In 1924, he bought a controlling interest in the failing Maxwell Motor
Company, which he closed down and reorganized as Chrysler a year later.
At a time when Americans idolized dynamic business leaders, Walter Chrysler was hailed in the Roaring Twenties
as an industrial hero. By the late 1920s, his company had emerged as one of the leading automobile
manufacturers in America, having introduced two new divisions—Plymouth and DeSoto—and acquiring Dodge.
Chrysler was inspired by similar efforts at industry leader General Motors, which, under his former boss, William
Durant, had introduced the concept of different divisions whose cars appealed to buyers with different budgets. In
1928, the company broke ground on its iconic art deco headquarters, the Chrysler Building in New York City,
which, when it was completed in 1931, was briefly the tallest structure in the world. In 1929, Time magazine
named Walter Chrysler “Man of the Year.”
Like other automobile manufacturers, Chrysler was hit hard by slumping demand during the Great Depression. But
it was not hit as hard as rival Ford, which, by 1936, Chrysler had replaced as the number two American
automobile manufacturer by sales, a ranking it would hold on and off through the end of the 1940s. From the
post– World War II boom of the late 1940s through the early 1970s, Chrysler thrived, emerging as one of the Big
Three while most other major U.S. automakers closed their doors. Chrysler also moved aggressively into the
European market, acquiring controlling interests in British, French, and Spanish firms during the 1960s and
reorganizing them as Chrysler Europe.
 Oil Crisis and the First Government Bailout
The company was less successful seeing its way through the troubled manufacturing climate of the 1970s. Like
other American automobile companies, Chrysler generally produced large, less fuel-efficient models. When oil
prices spiked after the Arab oil embargo of 1973–1974, Chrysler found itself stuck with a substantial inventory of

big cars, which American consumers were spurning in favor of higher-mileage Japanese and European imports.
Meanwhile, in response to growing environmental and consumer concerns, Washington had imposed new
emissions and safety standards on domestically manufactured vehicles. Reengineering cars to meet those
standards—and then introducing new, higher-mileage models—was expensive, especially for Chrysler. Being the
smallest of the Big Three, research and design costs ate up a relatively larger portion of its revenues. Trying to
save money, it introduced poorly designed small cars such as the Dodge Aspen and Plymouth Volaré, which
saddled the company with high warranty costs.
By the time of the second spike in oil prices during the late 1970s, Chrysler was reeling in the United States and
its European operations had collapsed. Some $4 billion in debt and on the verge of bankruptcy by the middle of
1979—its total losses for that year would hit $1.2 billion, the largest in U.S. corporate history to that time—the
company requested $1.5 billion in loan guarantees from the federal government. At the same time, Chrysler’s
board hired Lee Iacocca—a highly regarded marketing executive at Ford, known for having designed and
marketed the hugely successful Mustang in the 1960s—to head the company.
 Renaissance
An effective lobbyist as well, Iacocca won over a reluctant Congress, which passed the Chrysler Corporation Loan
Guarantee Act at the end of 1979. The government also helped by making large purchases of Chrysler vehicles for
its own fleet. Through aggressive marketing with a patriotic message—Iacocca appealed to Americans to buy
domestic rather than foreign cars—and by selling off its lucrative defense subsidiary, Iacocca turned the company
around, aided by a reviving economy and surging car sales. The federal loans were paid off early, allowing
taxpayers to turn a $350 million profit on Washington’s loan guarantees. By 1983, Chrysler had returned to the
black and Iacocca was hailed as a business genius.
With oil prices declining steadily, the nation’s automobile industry thrived from the mid-1980s through the mid-
2000s, though its share of domestic sales continued to be eroded by imports. Chrysler shared in the prosperity,
buying American Motors and its popular Jeep brand of off-road vehicles in 1987, returning to Europe by building a
manufacturing plant in Austria, and introducing one of the most popular new styles of light vehicle—the minivan—
in 1983.
So successful was Chrysler that in 1998 the German automobile manufacturer Daimler-Benz purchased it and
renamed the combined new corporation DaimlerChrysler. The $48 billion deal was intended as a merger of
equals: Chrysler would bring its high-end German counterpart a line of more moderately priced cars and broad
access to the U.S. market; Daimler would provide Chrysler with some of its own top-flight technology and
engineering. But the merger did not work out as intended. A clash of corporate cultures and the weakening of the
U.S. automobile market doomed the enterprise in less than ten years. In 2007, Daimler sold an 80 percent stake
in the company to a private equity group called Cerberus Capital Management for $7.4 billion—a huge loss.
Even at that price, Cerberus’s purchase of Chrysler proved problematic. Along with soaring fuel prices, a
weakening economy beginning in 2007 and the financial crisis of 2008—which dried up the credit most buyers
used to finance their vehicle purchases—produced a 25 percent drop in sales, from just over 2 million in 2007 to
fewer than 1.5 million in 2008. Financial losses mounted into the billions, forcing the company to lay off thousands
of workers.
 Bailout Efforts and Bankruptcy
The new owners tried a number of strategies to stave off bankruptcy, including failed merger talks with General
Motors. With General Motors also in serious financial trouble by late 2008, the George W. Bush administration
announced plans for a bailout of the two automobile giants. Although the $13.4 billion package was a fraction of
the contemporary bailout of failing U.S. financial institutions, the plan met with a political firestorm. As with
General Motors, Chrysler had not helped its case by flying its executives in private jets to testify before Congress
—not the best public relations move for a company begging for taxpayer dollars. The Senate promptly voted

against the package, whereupon Chrysler announced that it was running out of cash and, absent a bailout, would
be forced to declare bankruptcy within weeks.
Chastened, company executives returned to Washington—this time by car caravan—to again plead with
Congress. Fearing the political fallout from seeing a pillar of the American economy collapse at a time of
deepening recession, taking with it hundreds of thousands of jobs, Congress responded with an even bigger
bailout than that proposed by Bush—some $25 billion in all—in September. But the money came with strings
attached. Chrysler would have to present a viable plan for returning to solvency within a matter of months.
The company responded by closing plants—some temporarily and some permanently—and furloughing or laying
off more workers. Its real hope lay in a merger with Fiat, Italy’s largest automobile manufacturer. Sergio
Marchionne, Fiat’s president and the man credited with having returned that company to profitability—demurred,
believing that he would get a better deal once Chrysler had been reorganized under Chapter 11 of U.S.
bankruptcy laws.
Without access to Fiat’s assets, Chrysler did just that on April 30, 2009. Both Chrysler and the new Barack
Obama administration expressed a desire for what they called a “surgical bankruptcy,” a process that would allow
Chrysler to make arrangements with its many creditors, dealers, and workers in a much shorter time than was
usually the case for a bankruptcy of this size and complexity. Such a process, it was hoped, would also serve as
a template for the potential bankruptcy of the much larger General Motors. After a few last-minute hitches,
including protests from some creditors, Chrysler emerged from bankruptcy on June 10 as a new corporate entity,
also named Chrysler. More than half the stock was owned by the pension plan of the United Auto Workers, 20
percent by Fiat, and another 10 percent by the U.S. and Canadian governments.  By 2011, the company had
recovered significantly and posted a $212 million profit in the third quarter, its largest since emerging from
bankruptcy in 2009.
James Ciment
 
See also:  General Motors;  Manufacturing;  Troubled Asset Relief Program (2008-). 
Further Reading
Curcio, Vincent. Chrysler: The Life and Times of an Automotive Genius. New York: Oxford University Press, 2001. 
Langworth, Richard M., and Jan P. Norbye. The Complete History of Chrysler Corporation, 1924–1985. New
York: Beekman, 1985. 
Moritz, Michael, and Barrett Seaman. Going for Broke: Lee Iacocca’s Battle to Save Chrysler. Garden City, NY: Anchor
Press/Doubleday, 1984. 
Circuit City Stores
 
Circuit City provides an important example of how even large, popular retailers can fail when cost-conscious
management shift from their traditional areas of expertise and jettison core competencies during times of economic
contraction. Circuit City was a U.S.-based electronics retailer that sold personal computers, entertainment
software, and other electronic products. The company opened its first stores in 1949 and liquidated its final U.S.-

based stores in March 2009 following a bankruptcy filing and a failed effort to find a buyer.
 History
Circuit City was first founded by Samuel S. Wurtzel as a part of the first Wards Company retail store in Richmond,
Virginia, in 1949. By 1959, it operated four television and home appliance stores in the city. The company
continued to grow over the next two decades, experimenting with several retail formats and names, finally
changing its name to Circuit City and being listed on the New York Stock Exchange in 1984. The early slogans,
for example, “Circuit City—Where Streets Are Paved With Bargains,” touted the company’s everyday low pricing
strategy. By the late 1980s, the company had begun an aggressive national growth policy, highlighting its “plug”
design stores, in which the entrance was in the shape of a plug, drawing consumers in. It also accentuated its
core competency of exceptional service best exemplified by its slogan, “Welcome to Circuit City, Where Service Is
State of the Art.”
In 2000, the company exited the large-appliance market. This had been a profitable business to date, earning it
$1.6 billion in revenues in 1999, but the company decided to focus on its original “plug design” electronic and
computer-based products as well as music and movie sales. The updating of the stores cost $1.5 billion. The
move was met with skepticism from investors, and within a few weeks the value of the company’s stock
plummeted to nearly a third of its one-year high. In 2003, the company moved from having a commissioned sales
force to hourly “product specialists,” resulting in the laying off of 3,900 employees, but the company realized $130
million a year in savings. In 2004, Circuit City co-branded with Verizon Wireless, allowing the latter to operate full-
service sales and service centers in each of its superstores. Firedog, the company’s upgraded in-store and in-
home theater technical support and installation services, was introduced in 2006.
 Decline, Bankruptcy, and Liquidation
Philip J. Schoonover, an executive vice president from rival Best Buy Stores, Inc., took over as chairman of the
board of Circuit City in June 2006 and implemented cost-cutting strategies, which began the gradual decline of the
company. His initiative of slashing salaries of management and sales associates appeared to hurt employee
morale. Starting wages for sales associates in 2007 were dropped from $8.75 an hour to $7.40 an hour. That
same year, the company also closed seven U.S.-based superstores, one Kentucky distribution center, and sixty-
two stores in Canada to cut costs and improve its financial performance.
In Schoonover’s first six months in office, three handpicked senior executives, including its chief financial officer,
left the company, causing concerns among analysts. In the “wage management” program, 3,400 of the better-paid
associates were laid off and were offered to be rehired ten weeks later at prevailing lower wages. The policy
backfired and resulted in even lower sales. Schoonover resigned in September 2008.
On November 3, 2008, hit by falling consumer demand during the worst recession in decades, Circuit City
announced that it would close 155 stores and lay off 17 percent of the workforce by the end of the year. A week
later, it filed for bankruptcy protection under Chapter 11 of the U.S. Bankruptcy Code. The company had assets of
$3.4 billion and a debt of $2.32 billion, including $119 million to Hewlett-Packard and $116 million to Samsung.
Unable to find a buyer, Circuit City decided to close all of its remaining 567 stores on January 16, 2009.
Approximately 30,000 employees lost their jobs in the liquidation.
 Future of Electronics Retailing
Despite the demise of its major competitor, Best Buy cannot afford to rest easy. Analysts expect Wal-Mart to pick
up at least half of Circuit City’s customers and compete fiercely by deeper discounting. Best Buy’s new chief
executive officer as of June 2009, Brian Dunn, is hoping to distinguish his company by turning his stores into a
series of interactive experiences, where customers can step into the world of a new videogame or see their faces
on a high-definition video camera, instead of just walking past aisles stacked with merchandise. His main
philosophy is to rely on the innovations proposed by the frontline workers in his company.

Abhijit Roy
 
See also:  Recession and Financial Crisis (2007-). 
Further Reading
Berfield, Susan.  “Selling Out to the Bare Walls.” BusinessWeek, March 12, 2009. 
Bustillo, Miguel.  “Best Buy Confronts Newer Nemesis—With Circuit City Gone, Electronics Retailer Arms Its ‘Blue Shirt’
Sales Force to Take On Wal-Mart.” Wall Street Journal, March 16, 2009. 
Llovio, Louis.  “Circuit City’s Demise Vacates Lots of Retail Space.” McClatchy-Tribune Business News, March 10, 2009. 
Citigroup
 
A global financial services conglomerate serving 200 million customers in more than 100 countries, New
York– based Citigroup Inc. provides an array of financial services, including retail, corporate, and investment
banking, insurance, and asset management. Created in a 1998 merger of Citicorp, a banking powerhouse, and
the Travelers Group, an insurance and financial services provider, Citigroup prospered mightily during the financial
industry boom of the early 2000s before being hit hard by the financial crisis of 2008–2009. To shore up its assets
in the face of heavy losses from securitized mortgage financial instruments, Citigroup accepted $45 billion under
the federal government’s Troubled Asset Relief Program (TARP) in late 2008.
The Citicorp side of Citigroup began as the City Bank of New York in 1812. Over the years, the bank acquired a
number of other banks in the United States and overseas, even as its corporate named changed at various times.
It emerged as America’s largest commercial bank in terms of assets toward the end of the nineteenth century and
the largest in the world by 1929. (In subsequent years, both distinctions would be lost to other banks from time to
time, but Citicorp always remained among the largest in the United States and the world.) Citicorp did suffer its
share of setbacks, however. Heavily involved in loans to the developing world in the 1970s and early 1980s, it
posted a record $1.2 billion loss in 1987 and was forced to set aside a $3 billion reserve fund after falling
commodity prices forced several countries to default.
As its name implies, the Travelers Insurance Company, founded in 1864, began by offering insurance against
death and personal injury for those traveling on steamboats and railroads. It eventually grew into one of the
largest American insurance companies, though it too had its stumbles. Facing heavy losses during the real-estate
bust of the early 1990s and required to pay vast sums to policyholders in the wake of Hurricane Andrew, which
devastated South Florida in 1992, Travelers formed an alliance, and then merged, with rival Primerica in 1993.
The new company retained the Travelers name, though changed it to the Travelers Group in 1995. Meanwhile,
the company had branched out into the financial services industry, acquiring the brokerage house Smith Barney in
1987 and the investment bank Salomon Brothers in 1998.
Citicorp and the Travelers Group came together as Citigroup in one of the largest mergers in corporate history,
with the new firm having a market value of $140 billion and assets of nearly $700 billion. While the Glass-Steagall
Act of 1933 ostensibly prevented banks from owning insurance companies, Citigroup won government approval of
the merger by promising to sell off its insurance businesses. At the same time, however, it lobbied hard to get the

relevant provision of Glass-Steagall overturned. In 1999, President Bill Clinton signed the Financial Services
Modernization Act, which did exactly that.
While the legal hurdle to the banking-insurance merger had been overcome, other problems emerged. Turf battles
arose among the various divisions of the company, and the hoped-for economies of scale proved elusive. In 2002,
Citigroup spun off the property and casualty part of Travelers into a separate subsidiary, followed three years later
by the sale of the life insurance division to MetLife. With these divestments, Citigroup sought to focus exclusively
on banking and financial services.
Several divisions of Citigroup were implicated in the Enron investor fraud scandal of the early 2000s, which
caused a 25 percent drop in the price of company stock price and forced Citigroup to set aside $1.5 billion for
litigation costs. Nevertheless, the company prospered in the financial industry boom of the early to mid-2000s,
posting profits of more than $15 billion in 2002, at the height of the scandal. From 2000 to 2004, the company
enjoyed a compound annual growth rate of 8 percent in net revenues and 9 percent in operating income.
Through its financial services divisions, however, Citigroup also became deeply involved in collateralized debt
obligations (CDOs), many of which included mortgages bundled into tradable financial instruments. Having
underestimated the possibility of a collapse in real-estate prices and widespread mortgage foreclosure, Citigroup
was heavily exposed when the housing bubble burst, beginning with defaults on subprime mortgages. As the
crisis began to unfold in 2007, Citigroup moved to cut costs, laying off thousands of workers, and assured
investors that the bank was not heavily exposed.
As the crisis deepened through 2008, Citigroup found itself in trouble on a number of fronts. Loans in virtually
every form—from home mortgages it financed directly to CDOs to small-business loans and corporate financing—
went into default. By November, the company was floundering, with losses close to $20 billion for the year; $10.1
billion in losses came in the fourth quarter alone, a company record. To cut costs, the company announced further
layoffs, which put the total for the year at some 75,000.
The cost-cutting measures were not enough, however, and in November the company was forced to take $25
billion in TARP money to shore up its assets and avoid insolvency. When that proved insufficient, another $20
billion was added. Meanwhile, the value of company stock was plunging; total market valuation plummeted from
$300 billion in 2006 to just $6 billion by the end of 2008. In the face of such a decline, the company negotiated a
deal in which the federal government would provide a more than $300 billion guarantee for Citigroup loans and
securities. And in early 2009, the company announced a major restructuring, reorganizing itself into two separate
banking and brokerage units.
In February 2009, the newly installed Barack Obama administration agreed to take a 36 percent equity stake in
the company by converting $25 billion of the aid money into common shares. So reduced were Citigroup’s fortunes
by mid-2009 that the Dow Jones announced it was removing the company from its much-watched Dow Jones
Industrial Average, replacing it with the Travelers Insurance Group it spun off as a subsidiary in 2002.
Yet amid these setbacks, Citigroup made a solid recovery during the first half of 2009, posting significant profits in
the first two quarters of the year. In April, it announced first-quarter profits of nearly $1.6 billion and insisted it
would pay back the entire $45 billion in government loans, which it achieved by the end of 2009. Some market
analysts remained skeptical despite this achievement, saying the company still faced potentially painful write-
downs in its automobile and credit card financing businesses. Such skepticism was borne out when Citi released
its earnings report for the third quarter, showing $3.2 billion in losses, most from its Citi Holdings divisions, which
held most of the troubled assets on the corporation’s books. By 2010 and 2011, however, the company had
returned to the black, posting more than $10 billion in profits in the former year and $3.8 billion in the third quarter
of 2011 alone.
James Ciment
 

See also:  Banks, Commercial;  Banks, Investment;  Troubled Asset Relief Program (2008-). 
Further Reading
Citigroup:  www.citigroup.com
Langley, Monica. Tearing Down the Walls: How Sandy Weill Fought His Way to the Top of the Financial World... and Then
Nearly Lost It All. New York: Free Press, 2004. 
Stone, Amey, and Mike Brewster. King of Capital: Sandy Weill and the Making of Citigroup. New York: John Wiley and
Sons, 2002. 
 
Classical Theories and Models
 
The dominant school of economic thought from the time it arose with the work of Scottish economist Adam Smith
and French economist Jean-Baptiste Say in the late eighteenth and early nineteenth centuries through the
economic catastrophe of the Great Depression of the 1930s, classical economic theory argued that markets,
operating under the principles of supply and demand, naturally tend toward equilibriums of full production and full
employment. More to the point, this is the intellectual legacy of this school even though a careful reading of Smith,
David Ricardo, Thomas Malthus, and Karl Marx—classical economists all—shows that they were quite aware of
the prospects for unemployment and fluctuations in the business cycle.
Nevertheless, classical economics holds that recessions generally tend to be short-lived and self-correcting,
requiring no government interference at the macroeconomic level. Indeed, any such interference is only likely to
distort the proper functioning of the market, thereby prolonging the economic downturn. Although classical
economists recognized the phenomenon of the business cycle, they did not attach great importance to it.

Classical theories of economic behavior begin with Adam Smith’s two-volume Wealth of Nations (1776), which
argues that competition should dictate wages and prices and that government should avoid interfering with market
forces. (The Granger Collection, New York)
According to Jean-Baptiste Say, an early proponent of the self-correcting market economy, supply creates its own
demand. In other words, the income paid by producers to those who own the resources—such as the rent paid by
tenant farmers to a landlord—will be repaid in equal amount by the resource owners who desire the goods or
commodities. Of course, this would apply only in situations where land is the sole resource and farmers are the
only producers. But, according to Say, such a scenario also plays out in the complexity of real-life economies.
Say argued that the production of an economy generates precisely the level of income needed to purchase the
economy’s output of goods and services. Or, put another way, supply generates the demand to keep resources
fully employed.
Thus, supply is the driver of the economy; it is a product of the push and pull of free-market forces. According to
classical economists, an oversupply of goods causes prices to fall. Similarly, when there is excessive demand for
goods—that is, when the demand outstrips the economy’s capacity to produce—prices rise. Consumers react by
buying less, thereby bringing supply and demand back into equilibrium. Conversely, when the economy produces
more than people want, prices fall. This, in turn, triggers a rise in demand, again returning the economy to
equilibrium.
The same rules are said to apply to employment and interest rates, since these constitute the price of labor and
the price of money, respectively. If there are too many workers chasing too few jobs, wages fall, which spurs
employers to increase hiring. When there are too few workers, wages rise until employers can no longer afford to

pay them. Before that happens, however, the higher wages trigger greater demand, causing a rise in prices that
negates the wage increase. Likewise with money, when there is too little in circulation, interest rates rise, which
makes borrowing for investment more difficult. When investment shrinks, production slows, thereby adjusting the
supply of goods to the amount of money in the economy.
While some pre-twentieth-century economists, most notably Karl Marx, argued that the equilibrium can be set at a
level that produces great suffering (indeed, building on the work of Adam Smith and classical British economist
David Ricardo, Marx asserted that capitalist economies naturally tend in that direction) they nevertheless held to
the idea of supply-and-demand equilibrium of high output and low unemployment.
Government involvement in the economy, according to classical economists, should be restricted to creating a
climate in which the markets can operate freely. Traditionally, this meant ensuring that contracts are honored
through the establishment of a court system and ensuring domestic tranquility so that economic agents are not
subject to violence or coercion. More expansively, classical economists embraced government measures at the
microeconomic level. If, for example, a single firm dominates an economic sector—allowing it to set prices at will
and thus distort the proper functioning of the market—then it might be within the government’s purview to take
measures, such as antitrust action, to ensure free competition.
The tendency toward market equilibrium—the heart of the classical economist’s approach to the business cycle—
was sorely tested by the Great Depression of the 1930s. Here was a situation in which the economy operated far
below its productive and consumption capacity, with factories going idle and farm produce left to rot in the fields—
nowhere near equilibrium. The rate of unemployment, meanwhile, hovered at 25 percent in the United States,
more than six times what economists consider economically robust (the percentage of workers normally between
jobs or opting to stay out of the workforce).
This situation led a number of economists at the time to question the classical paradigm of market equilibrium, the
most important being British economist John Maynard Keynes. According to Keynes, the classical theory of
equilibrium was wrong on several critical counts. First, said Keynes, demand is the critical component in the
equation, not supply. Second, he said, aggregate demand—established by the spending decisions of individuals,
businesses, and governments—is the product of a host of independent factors. In addition, Keynes argued, the
classical assumption of wage and price flexibility is off the mark as well, since a number of factors make them
quite inflexible. In short, he said, an equilibrium can be set at which an economy operates well below its capacity
and full-employment level.
Just as classical economics had implications for government policy makers—essentially, leave macroeconomics
alone and let the free market work out the problems for itself—so, too, did Keynesian economics. If the key to
lifting an economy out of a low-employment, low-output equilibrium was to raise aggregate demand—and if
individuals and businesses are incapable of doing so—then the only agent left is government. Thus, Keynes
argued that governments should abandon their classical economics bias and inject large sums into the economy
during downturns as a way to smooth out the economic cycle—a process that classical economists argued could
be achieved only by the market itself.
James Ciment
 
See also:  Law, John;  Malthus, Thomas Robert;  Marshall, Alfred;  Mill, John Stuart; 
Neoclassical Theories and Models;  Smith, Adam. 
Further Reading
Keynes, John Maynard. The General Theory of Employment, Interest and Money. London: Macmillan, 1936. 
Knoop, Todd. Recessions and Depressions: Understanding Business Cycles. Westport, CT: Praeger, 2004. 

Sowell, Thomas. On Classical Economics. New Haven, CT: Yale University Press, 2006. 
Collateral
 
In finance, collateral is property that the borrower of a secured loan uses to back his or her promise to pay back a
debt to a creditor—an asset the creditor can acquire in the event that the borrower defaults on the loan
agreement, thus reducing the creditor’s risk. In most real property mortgages, for instance, the house (or land or
other building) that is being purchased also serves as the collateral for the loan; if the mortgage cannot be repaid,
the bank extending the mortgage takes possession of the house through foreclosure, typically auctioning it off to
recoup its investment. (The process of foreclosure is generally overseen by a court, which presents the borrower
with the opportunity to protest if he or she feels that the foreclosure is unjust or unwarranted; it also helps ensure
that a fair price is obtained for the property.)
On a smaller scale, the vehicle purchased with a car loan acts as the collateral for that loan; reclaiming the
collateral in this case is called repossession, not foreclosure. Most smaller debts are likely to be unsecured,
though items bought on store credit lines, such as furniture and major appliances, can be repossessed in case of
default. While most credit card debt is unsecured, some financial institutions offer secured credit cards (intended
for those repairing their credit), which require an initial deposit equal to a portion of the credit limit. The security
deposit required to open an account with a utility company in the case of bad or no credit serves a similar
function, as does the security deposit that landlords typically require of renters.
Cross-collateralization is the process of using the same property as collateral for more than one loan issued by
the same institution. While this sounds like it works to the consumer’s benefit, it often means that the title of a
smaller purchase—typically a car or truck—remains with the lending institution until the much larger purchase—
such as a house—has been paid off. And because the life of a mortgage usually exceeds the life of a car, this
puts the borrower in the position of never completely owning the vehicle, despite having paid it off. Cross-
collateralization does not apply to real estate; once a house or piece of land is paid off, it remains paid off even if
other outstanding loans are owed to the institution.
 Title Loans
A title loan is a sum of money extended to a borrower who uses the title to his or her car as the collateral.
Typically short-term and high-interest, title loans are offered by companies that fill a niche in the lending
landscape by providing smaller and higher-risk loans than banks typically offer. Such loans are often available
online and granted without a credit check; the interest rate can be staggeringly high compared to that for more
mainstream consumer loans, with annual percentage rates (APRs) well over 100 percent and often three or four
times that. Minimum payments must cover the interest and, if the principal cannot be paid off at the end of term,
the loan may be rolled over into another term, with interest continuing to accumulate. Like payday loans—which
offer fast cash at high interest to people in need of money and anticipating a paycheck—title loans are
manageable for the borrower only if they are paid off quickly. And like check-cashing outlets, the lender profits
from individuals with few financial services options and little financial acumen. State legislation increasingly limits
title loans, both by the amount of the loan that can be extended and by the number of times it can be rolled over.
 Collateralization
Collateralization, or securitization, is the process of creating asset-backed securities—that is, securities backed by

a pool of assets from which the income and overall value of the securities is derived. In recent years, the most
famous example of this is the collateralized mortgage obligation (CMO), a type of security first created by First
Boston and Salomon Brothers on behalf of Freddie Mac, the government-sponsored enterprise, in 1983. CMOs
are legal entities created to serve as the owner of a pool of mortgages, which are used to back bonds issued by
the CMO. Like other asset-backed securities backed by collateralized debt, CMO bonds are structured in complex
ways to allow more adjustments to the risk and return than could be made if investors were to simply pool their
money together and buy a mortgage debt. The process developed for Freddie Mac allowed multiple types of
bonds to be issued by a CMO, at different prices, with different interest rates, and with other different
characteristics. The CMO may issue a hierarchy of bonds, for example, with those in the highest tier paid first; this
runs the risk that there will be no money left to pay off the bonds on lower tiers, which thus sell at a lower price.
(This could occur if a mortgage is paid off early, before the full twenty-or thirty-or forty-year term expires, which
yields less income in the form of interest payments.) CMOs may also be issued that pay investors from the
interest payments on the underlying mortgages—hence the name interest-only CDOs. On the other hand,
principal-only CDOs pay investors from the principal payments on the underlying mortgages only.
Before long, specialists developed pooling techniques to shorter-term structured notes that were similar to CMO
bonds. Then the technique was extended to other debt pools, including credit card debt, insurance contracts,
small business loans, and so on. CDOs were even created that were backed by other CDOs, and those CDOs
backed still other CDOs, to create several layers of investment. The complexity of the instruments resulted in a
significant abstraction of the structured notes from the underlying assets used as collateral. In many cases,
investors did not actually know what they owned—especially given the prevalence of CDOs in hedge funds,
pension funds, and mutual funds. When the subprime mortgage crisis struck, many of the CMO bonds backed by
subprime mortgages became part of the plague of toxic assets that contributed to the global financial crisis in
2008, when exposure to these assets ruined so many banks and other businesses.
Bill Kte’pi
 
See also:  Collateralized Debt Obligations;  Collateralized Mortgage Obligations;  Debt;  Debt
Instruments. 
Further Reading
Ahamed, Liaquat. Lords of Finance: The Bankers Who Broke the World. New York: Penguin, 2009. 
Davenport, Penny, ed. A Practical Guide to Collateral Management in the OTC Derivatives Market. New York: Palgrave
Macmillan, 2003. 
Garrett, Joan. Banks and Their Customers. New York: Oceana Publications, 1995. 
Sena, Vania. Credit and Collateral. New York: Routledge, 2007. 
Collateralized Debt Obligations
 
Collateralized debt obligations (CDOs) are complex financial instruments created from pools of debt securities
(obligations). CDOs are backed by the principal and interest payments made on the underlying securities, which
are passed through to the owners of the CDO. The payments made on the CDOs are divided into different

tranches (classes), or slices, with different risks. Since principal and interest payments are not passed through in a
straightforward, proportional way, various risks can be transferred among investors in different tranches within the
CDO. The CDO specifies the number of tranches, which have ranged from a few to around 100.
The three categories of tranches are the senior tranche, the mezzanine tranche, and the subordinate/equity
tranche. Usually at least one of the tranches pays a floating rate where the interest rate is periodically reset based
on an index. Those in the highest (most secure) tranche would be repaid principal and interest payments first,
while those in the lowest (least secure) tranche would be repaid last and not until the securities in all the other
tranches had been fully repaid. The investor in the most secure tranche would earn a lower return than the
investor in a more risky tranche because of the lower risk. Given the different risks involved in the various CDO
tranches, rating agencies such as Moody’s, Standard & Poor’s, and Fitch Ratings rate each tranche of the security
separately. Investors in high-rated tranches can use the rating system to feel more secure about their investment,
and all investors can more accurately evaluate the risk/return relationship.
 History
The first CDOs were created in 1983 in the mortgage market, where a type of CDO called a “collateralized
mortgage obligation” (CMO) was developed by the government-sponsored enterprise Freddie Mac. The CDO
market was dominated by CMOs through the 1990s, and the CMO market remains the largest part of CDO
market. However, in the 1990s, the pool of assets from which a CDO was created began to spread to other debt
securities, and in the early 2000s, CDOs in nonmortgage-related securities began to grow exponentially. Today,
CDOs have been created from one or more of the following categories of debt instruments:
Investment-grade and noninvestment-grade corporate bonds
Credit card balances, accounts receivable, auto loans, and student loans
Emerging market bonds
Domestic and foreign bank loans
 Example
A CDO consisting of bank loans, called a collateralized loan obligation (CLO), is constructed as follows: Every
CDO has a sponsor. Suppose that the sponsor is a bank that wants to securitize some of its loan portfolio to
reduce its capital. Loans are assets to the bank, and regulatory capital requirements specify a capital-to-assets
ratio that must be maintained. By sponsoring a CDO, the bank can reduce the loans on its balance sheet, thus
increasing its capital-to-assets ratio. At the same time, the bank can get new funds to lend from the sale of the
CDO. A second reason why a sponsor may create a CDO is if the sponsor believes a profit can be made on the
CDO and this profit becomes the motivating force. The CDO will have a collateral manager that purchases the
loans from the bank and issues debt obligations (CDOs) to pay for them. Again, the CDO could be based on a
pool of bonds, mortgages, or pools of other debt instruments.
Consider the above case where the sponsor buys $100 million of loans from Bank A. The loans earn 10 percent
interest per year for the next ten years. Thus, the loans in the pool will earn $10 million per year in interest ($100
million x 10 percent = $10 million). For simplicity, assume that the loans are bullet or balloon loans where the
principal ($100 million in this case) is due at maturity. Assume there are only three tranches and that payments are
made according to the table that follows.
 
Tranche Par
Fixed or
Floating
Coupon Rate (annual percent interest paid as

Type
Value Rate
a percentage of face value of the CDO)
Senior
$70
million Floating
T-bill rate + 150 basis points
Mezzanine $20
million Fixed
8 percent
ubordinate $10
million —
—
The senior tranche pays investors a floating rate equal to the one-year T-bill rate plus 150 basis points. (One
basis point is 0.01 percent, so 150 basis points is 1.5 percent.) Thus, if the one-year T-bill rate is 5 percent, then
senior tranches would pay 6.5 percent. CDOs in the mezzanine class pay investors a fixed interest rate of 8
percent. The subordinate/equity class (that put up $10 million) would get the remainder if any funds are left over.
 Hedging Interest-Rate Risk
In the example above, to hedge the risk that the floating rate payable to investors in the senior tranche might
increase to a rate greater than the 10 percent of the underlying loans in the pool earn, the CDO sponsor can
enter into an interest rate swap agreement. An interest rate swap agreement allows for a fixed interest rate
stream, such as the 10 percent earned on the pool of loans, to be traded for a floating interest rate stream that
resets periodically based on an index. Only the interest streams and not the principal balances that generate the
income streams are traded.
In the previous example, assume that the sponsor enters into an interest rate swap agreement where the fixed
stream of 10 percent earned on the loans in the senior tranche is traded for a floating stream to be paid to
investors in the senior tranche based on the one-year T-bill rate plus 150 basis points. Thus, by entering into the
interest rate swap, the sponsor is able to hedge (or in this case, eliminate) the risk that changes in the interest
payments to the senior tranche will reduce or eliminate potential profits to the subordinate/equity class.
The flows of funds per year resulting from the CDOs are as follows.
 
Tranche Type
Par
Value
Coupon
Rate
Interest
Received
from CDO
Interest Paid
to Investor
Return
Senior
$70
million
T-bill rate +
150 basis
points
$7 million
−$7 million
because of
interest rate swap*
$0
Mezzanine
$20
million
8 percent
fixed
$2 million
−$1.6 million
$0.4
million
Subordinate
$10
million —
$1 million
—
$1
million
Residual funds left over
for subordinate/equity
class
—
—
$10 million
−$8.6 million
$1.4
million

*Note that the sponsor entered into an interest rate swap agreement to pay the fixed interest rate of 10 percent
earned on the senior tranche in exchange for receiving the T-bill rate plus 150 basis points, which is then passed
on to the investors in the senior tranches.
For example, ($70 million × 10% = $7 million) − ($70 million × 10% = $7 million) = $0 million. Therefore, the
sponsor had a net receipt of $90 million.
Thus, in this case, the investors in the subordinate tranche that invested $10 million will get the original $10
million back plus an additional $1.4 million (14 percent) per year for 10 years. This, of course, assumes all the
interest and principal payments on the underlying loans are paid in full. If the principal and interest on the
underlying loans in the CDO are not fully repaid, or if the interest rate swap adversely affects the sponsor, the
return to the subordinate tranche may be reduced or eliminated.
 CDOs and the Financial Crisis of 2007–2009
CDOs played a significant role in the global financial crisis and accompanying recession (2007–2009), when
investors found out that things do not always turn out as hoped for or anticipated. Defaults in the CDO market
significantly contributed to and exacerbated the crisis.
The financial crisis began in the subprime mortgage market. Many of the loans involved in subprime mortgages
were made to borrowers with bad credit and involved little or no down payment. They were often made at low
teaser interest rates that would reset to higher rates within a few years. These and other mortgages were then
repackaged and sold as mortgage-backed securities or as CMOs (a type of CDO) in the global marketplace. Many
of these securities had been rated AAA by the rating agencies and hence were attractive investments for global
investors.
At the same time, the issuances of other types of CDOs (which had been relatively small prior to 2000) were also
increasing. In 2000, the global issuance of all CDOs was about $67 billion. CDO issuances increased to over $250
billion in 2005 and to over $520 billion in 2006. As more and more homeowners had trouble making their house
payments on the reset subprime loans, borrowers started defaulting at alarming rates on loans that had been
securitized into mortgage-backed securities or CMOs. This caused a crisis in these markets, and as housing prices
started to fall, more and more homeowners defaulted. The crisis in the CMO market quickly spread to all CDOs,
as investors became dubious about the creditworthiness of these exotic securities. Prices plummeted as the
securities were dumped in the market. As the crisis spread from the mortgage market to other financial markets
and then to the broader economy, the situation further deteriorated. As the securitized payment on income
streams of all types of debt instruments became more uncertain, defaults on all types of securities spread. The
CDO market exacerbated the crisis by the sheer volume of securities that had been sold into global markets to
investors who really had little knowledge about the risks of the assets in which they were investing. By 2007,
issuances of new CDOs fell to about $481 billion, and then dropped precipitously to $61 billion in 2008 and to just
over $4.2 billion in 2009.
The largest banks in the country were also heavily involved in CDOs. Losses in CDOs became one of the factors
limiting credit extension by the largest banks throughout the crisis and into early 2010, despite the enormous
injections of cash into the banks by the Federal Reserve. Because of the large losses in these markets, questions
have been raised about the appropriateness of the behavior of some of the largest banks. In January 2010, the
Securities and Exchange Commission (SEC) sent subpoenas to Goldman Sachs, Credit Suisse, Citigroup, Bank of
America/Merrill Lynch, Deutsche Bank, UBS, Morgan Stanley, and Barclays Capital to investigate their involvement
with a particular type of CDO called a synthetic CDO, that is, a CDO based on one or more previously existing
CDOs. It is feared that the largest banks could take enormous losses in the future on synthetic CDOs, similar to
the losses they have already taken on CDOs backed by subprime mortgages. It is clear that the risks involved in
these securities were not understood by either the rating agencies or the investors, and that U.S. government
failed to sufficiently regulate these markets.

Maureen Burton and Jamie Shuk Ki Yuen
 
See also:  Collateral;  Collateralized Mortgage Obligations;  Debt;  Financial Markets; 
Recession and Financial Crisis (2007-);  Securitization. 
Further Reading
Burton, Maureen, and Bruce Brown. The Financial System and the Economy.  5th ed. Armonk, NY: M.E. Sharpe, 2009. 
Burton, Maureen, Reynold Nesiba, and Bruce Brown. An Introduction to Financial Markets and Institutions.  2nd ed. Armonk,
NY: M.E. Sharpe, 2010. 
Fabozzi, Frank J., Franco Modigliani, and Frank J. Jones. Foundations of Financial Markets and Institutions.  4th ed. Upper
Saddle River, NJ: Prentice Hall, 2009. 
Lucas, Douglas J., Laurie S. Goodman, and Frank J. Fabozzi. Collateralized Debt Obligations: Structures and Analysis.  2nd
ed. Hoboken, NJ: John Wiley and Sons, 2006. 
“Risk Management of Investment in Structured Credit Products.” Financial Institution Letter,  FIL-20–2009. Washington,
DC: Federal Deposit Insurance Corporation, April 30, 2009. 
Securities Industry and Financial Markets Association:  www.sifma.org
Collateralized Mortgage Obligations
 
A collateralized mortgage obligation (CMO) is a financial instrument (security) created from a pool of mortgages or
mortgage-backed securities (MBSs) that redirects the cash flows (principal and interest) from the underlying
mortgages to different classes of investors. The originator of the CMO takes a pool of mortgages or a mortgage-
backed security and sells new securities based upon the redistribution of the cash flows into different tranches or
slices with differing risks. CMOs meet the demands of investors (usually institutions) for securities with varying
risks and returns. (CMOs should not be confused with collateralized debt obligations, or CDOs, which involve
various types of nonmortgage debt, though the two are similar in structure.)
 History
A mortgage-backed security is a security backed by a pool of mortgages that provides cash flows to investors.
CMOs are securities like mortgage-backed securities in that they are backed by the cash flows from a pool of
mortgages. However, the cash flows of the CMOs are divided into different tranches (slices) or classes to create
new securities with different risks and returns that would appeal to different investors.
For example, if an investor was looking for a short-term investment, he or she could invest in the tranche where
all of the flows of principal went to that tranche first. Investors in other tranches would initially receive cash flows of
interest payments only until after the principal of the previous tranche was paid in full. Those investors that
preferred long-term instruments could purchase securities created from the tranche that was last to receive
principal payments. There are two broad types of CMOs: agency-backed CMOs and non-agency-backed CMOs.
 Agency-Backed CMOs

Agency-backed CMOs are created from pools of mortgages or mortgage-backed securities issued and insured by
the government-sponsored entities Fannie Mae and Freddie Mac, or the government-owned corporation Ginnie
Mae (National Mortgage Association). The mortgages in these CMOs are all insured by these institutions and have
no default risk. That is, their principal and interest payments are fully insured by the government or a government-
sponsored enterprise and will be paid even if the original borrower defaults. However, investors in mortgage-
backed securities with no default risk still face the risk that the mortgages in the pool will be prepaid sooner than
expected because the property is sold or because the mortgage is refinanced due to a decrease in interest rates.
This risk is called “prepayment risk.” If more mortgages than expected are prepaid, then the return falls short of
expectations because the investor receives his or her funds back before the full return is realized. Moreover, if
there have been greater-than-expected prepayments due to decreases in interest rates, then the funds received
back early will have to be reinvested at a lower rate. Likewise, if interest rates go up, fewer mortgages than
anticipated will be repaid and the maturity of the securities will be longer than anticipated.
This dilemma could lead to a problem if the investor had planned to receive the funds back sooner rather than
later. An agency-backed CMO deals with these problems by dividing the cash flows from the pool of mortgages
into different tranches. Those who wanted shorter-term instruments could invest in a tranche that received
principal payments first. Those who wanted longer-term instruments could invest in a tranche that postponed
receiving any principal payments until all other investors in the various tranches had been repaid. For example,
CMO tranches could be designed to amortize sequentially. One class might have the right to all of the initial
principal payments, while other classes have to wait until the first class is paid off before receiving any principal
payments. Once the first class is paid off, the second class begins to receive scheduled and unscheduled principal
payments. By designing the classes to be paid down sequentially, it is possible to create short-term, medium-
term, and long-term securities.
Thus, the CMO took a typical mortgage-backed security or pool of mortgages and divided it into a series of
tranches with different prepayment risks. The newly directed CMO redirects the cash flows (principal and interest)
of mortgage-backed securities to various classes of bondholders, thus creating financial instruments with varying
prepayment risks and varying returns. Those who are most risk averse to a prepayment risk or who want a short-
term investment can choose an instrument wherein the principal will be repaid soonest. Those who are willing to
bear more risk can choose an instrument wherein the principal will not be repaid until later, and hence, is subject
to a greater prepayment risk. In exchange for more prepayment risk, the investor may receive a higher return.
Needless to say, such provisions make attractive choices available to a wider range of investors. Finally, since
CMOs are fixed-rate debt instruments, there is an interest rate risk in that, if the interest rate goes up, the value
of the fixed-rate CMOs will go down. If an investor had to sell the CMO in the secondary market before maturity,
he or she would experience a capital loss.
 Nonagency-Backed CMOs
Nonagency-backed CMOs are similar to agency-backed CMOs with the exception that they are backed by pools of
mortgages that are not insured by Fannie Mae, Freddie Mac, or Ginnie Mae. Thus, in addition to prepayment risk,
there is some credit risk that the mortgages will not be repaid and the cash flows will fall short of anticipated.
Some nonagency-backed CMOs were formed from pools of relatively safe mortgages or mortgage-backed
securities with little default risk. However, others were formed by pools of subprime mortgages where there was a
great deal of default risk. With nonagency CMOs, the cash flows from the underlying mortgages are divided into
tranches that direct principal payments to different classes of investors. Those in senior tranches receive principal
and interest payments first and thus are more likely to be repaid in full. Those in lower tranches receive a higher
return for accepting more default risk. Because of the default risk, each of the tranches in a CMO (except the
lowest tranche) is rated by one or more of the rating agencies—Moody’s, Standard & Poor’s, and Fitch—to give
investors some idea of the risks of investing in the various tranches. The lowest tranche is a subordinate/equity
tranche that is not rated by a rating agency and is often held by the originators of the CMO. This tranche has the
highest risk, but the originators believe it offers the highest possible return.

This shifting of risk creates relatively secure classes called “senior securities” and a relatively risky class called the
“subordinate/equity class.” Between the two lie the so-called mezzanine CMO tranches. Losses resulting from
mortgage loan defaults are first absorbed by the most subordinated CMO class, followed by the next most
subordinated class, and so on. If the loss of mortgage principal is large enough, even the senior class may
experience losses. The ability to reallocate the cash flow in a pool of mortgages or a portfolio of MBSs makes the
market for CMOs much deeper and broader than for simple MBSs. Note that even the senior tranches of the
CMOs formed by pools of subprime mortgages were rated high (low risk) by the rating agencies because it was
believed that the lower tranches would absorb any defaults. The number of tranches in CMOs has increased
dramatically from the original three-class CMO issued by Freddie Mac in 1983. In 2007, CMOs were being issued
with close to 100 classes.
CMO classes can also be structured so that a tranche receives only interest or only principal payments. These are
known as interest-only securities (IOs) and principal-only securities (POs). IOs and POs appeal to investors with
different needs and expectations about the prepayment behavior of borrowers. Investors in POs want the
mortgages to be paid off as soon as possible because that would give them the highest return. If they have to wait
a long time to receive their principal payment, the return is reduced. For example, if an investor invests $10,000 in
a PO to receive $20,000 on some future date, then the return will depend on how soon the principal is paid. If the
principal is prepaid in one year, the investor earns a 100 percent return because his or her investment has
doubled in one year. However, if the principal payment is not received for twenty years, the return is reduced to
about 3.5 percent per year since $10,000 invested at 3.5 percent for twenty years is approximately equal to
$20,000. For IOs, the investor receives interest payments only until the mortgages are paid off. Thus, investors in
IOs do not want mortgages to be paid off early whereas for investors in POs the sooner the mortgages are paid
off, the higher the return.
Finally, since CMOs are predominantly fixed-rate debt instruments, there is an interest rate risk in investing in
either agency-backed or nonagency-backed CMOs. That is, if the interest rate goes up, the value of the fixed-rate
CMOs will go down. If an investor had to sell the CMO in the secondary market before maturity, she would
experience a capital loss.
 CMOs and the Financial Crisis of 2008–2009
CMOs played an integral role in the financial crisis of 2008–2009. Defaults in the subprime mortgage market led to
losses in CMOs that were backed by subprime mortgages, a type of nonagency CMO. The crisis in the subprime
market quickly spread to other mortgage markets and caused defaults on mortgages that were packaged into
agency CMOs and thus guaranteed by Fannie Mae and Freddie Mac. Losses in these markets spread to other
markets and led to the collapse of numerous financial institutions that eventually spilled over to the real estate
sector, causing the severest recession since the Great Depression of the 1930s. Because of the losses to Fannie
Mae and Freddie Mac, the two government-sponsored enterprises were put into conservatorship by the U.S.
government in September 2008.
A look at the spectacular growth of these markets from the mid-1990s until 2009 is also revealing and supports
the notion that the creation of these instruments allowed for huge flows of funds into the mortgage markets and
contributed to the housing price bubble that eventually burst with such disastrous results. For example, the table
below shows the amount outstanding of agency and nonagency MBSs and CMOs for various years from 1995 to
2009. Data for MBSs and CMOs are reported together.
In addition to the tremendous growth of these securities, note that after the financial crisis began in 2007, the
amount outstanding of nonagency-backed securities actually fell by over $600 billion. The amount of agency
securities—those insured by Fannie Mae, Freddie Mac, or Ginnie Mae—actually increased by over $800 billion in
this period. This was despite the fact that Fannie Mae and Freddie Mac were put into conservatorship by the U.S.
government and were virtually insolvent, with over $100 billion of taxpayer funds having been injected into them.
Thus, firms that experienced the most severe strains still managed to increase the pool of assets that they insure

by over $800 billion. Although still buying MBSs and CMOs, Fannie Mae and Freddie Mac were far from out of the
woods when government officials announced in December 2009 that the U.S. Treasury would give them virtually
unlimited support over the next three years. Finally, the Federal Reserve (in addition to the increased purchases
by Fannie Mae and Freddie Mac) has purchased over $1 trillion in mortgage-backed securities and agency-backed
securities as of February 2010. They are authorized and plan to purchase $1.25 trillion of mortgage-backed
securities and agency-backed securities. This reflects the extent to which policy makers have tried to mitigate the
severe downturn caused by the collapse of mortgage market and mortgage derivative markets, including MBSs
and CMOs.
Maureen Burton and Jamie Shuk Ki Yuen
 
See also:  Collateral;  Collateralized Debt Obligations;  Financial Markets;  Housing Booms and
Busts;  Mortgage-Backed Securities;  Mortgage Lending Standards;  Mortgage Markets and
Mortgage Rates;  Mortgage, Subprime;  Recession and Financial Crisis (2007-); 
Securitization. 
Further Reading
Burton, Maureen, and Bruce Brown. The Financial System and the Economy.  5th ed. Armonk, NY: M.E. Sharpe, 2009. 
Burton, Maureen, Reynold Nesiba, and Bruce Brown. An Introduction to Financial Markets and Institutions.  2nd ed. Armonk,
NY: M.E. Sharpe, 2010. 
Fabozzi, Frank J., Franco Modigliani, and Frank J. Jones. Foundations of Financial Markets and Institutions.  4th ed. Upper
Saddle River, NJ: Prentice Hall, 2009. 
 
 
Colombia
 
Unlike most Latin American countries, Colombia—a nation of about 45 million located in northwestern South
America—has enjoyed a relatively stable political and economic history, with no military coups. Still, the country is
deeply divided by wealth, with most Colombians living in great poverty. Political and class violence is not
unknown, and the last decades of the twentieth century were marred by widespread violence from criminal and
revolutionary groups. Despite the disorder, however, Colombia’s export-driven economy avoided the extremes of
other economies in the region. But because of this reliance on exports, Colombia has been hard hit by the U.S.
and global recessions of the late 2000s.

The world’s third-largest coffee producer, Colombia relies heavily on exports for economic stability. The global
recession of 2007–2009 thus had a major impact, compounded by a small coffee harvest at decade’s end.
(Rodrigo Arangua/AFP/Getty Images)
Before independence from Spain in the early nineteenth century, Colombia was a relatively sparsely populated
country. The first decades as an independent country did not change that fact. Most people lived in the
northwestern part of the country, separated into disconnected regions by branches of the Andes. Internal
transportation depended largely upon pack mules, and few goods had enough value to be shipped very far.
Colombia’s most important export was gold. During the 1840s, however, British investment encouraged the
development of tobacco exports. The Magdalena River provided a more economical route to bring tobacco from
the interior to waiting ships, and profits from the trade bought steamboats to speed up the process. Tobacco was
joined by other exports, including cotton, indigo, and cinchona bark, but Colombia remained an insignificant
exporter until the twentieth century.
After 1865, the burgeoning international market for coffee encouraged Colombian landowners to grow it. Mountain
slopes, which could be used for little else, were very suitable for the crop. By 1890, coffee had become
Colombia’s primary export. The first growers were large landowners, but small and medium landowners soon
became the primary growers. Income from the relatively dependable coffee crop provided funds for new industries
and for consumer spending. After 1910, coffee production increased rapidly, until Colombia exported 10 percent of
the world’s supply in 1929.
Agricultural products dominated the Colombian economy for most of the twentieth century. Cotton, sugarcane, and
other tropical products supplemented the coffee exports. Colombian industry remained small and produced goods
primarily for domestic consumption. Tariffs helped protect the small industrial sector from foreign competition. The
economy continued to grow at a steady, moderate pace.
Politically, Colombia was divided between supporters of the Liberal and Conservative parties. Conservatives held

power from 1884 to 1930, but uprisings by Liberals resulted in thousands of deaths in the 1890s. As a result,
some Liberal ministers were included in all cabinets. Liberals regained power between 1930 and 1946. When a
prominent Liberal leader was assassinated in 1948, rioting in the capital of Bogotá and uprisings throughout the
country resulted in thousands of deaths. In 1958, Liberals and Conservatives agreed to a power-sharing
arrangement, in which each party would alternate holding the presidency for four years. Other political groups
were shut out of power. In the late 1960s, a number of rebel groups began a guerrilla war against the established
order.
The great poverty in which most Colombians lived, as well as the widespread violence, led some to move to the
sparsely inhabited lowlands in the southeastern part of the country. They found that coca, marijuana, and other
crops that could become illegal drugs grew well in the climate. Beginning in the late 1960s, the worldwide demand
for recreational drugs increased, and cartels formed to grow, refine, and supply them. By the 1970s, Colombian
drug cartels had accumulated massive amounts of dollars, which they laundered through such legitimate
operations as land purchase. The inflow of dollars helped fuel inflation, while violence associated with cartels that
were protecting their markets helped destabilize the country. Government efforts to stifle this illegal economy were
largely unsuccessful, but did bring increased American aid. When unemployment among the poor in legal
businesses increased, drug cartels were able to provide work and income.
In 1990, President César Gaviria Trujillo implemented a policy of economic liberalism. Tariffs were reduced and
many industries were deregulated. State-owned businesses were privatized and a more liberal foreign-exchange
rate was adopted. Free-trade arrangements were made with other Latin American countries, especially Chile,
Mexico, and Venezuela. To ensure affordable food for the poor, Gaviria also adopted a policy of encouraging the
growth of agricultural products that could readily be exported. Coffee, cacao, and cut flowers found ready markets
in the United States, while wheat, soybeans, and corn were imported. The policy kept food prices low and the
economy grew at an annual rate of 4.5 percent, but made Colombia heavily dependent on imports for survival.
The 1990s also saw the increased export of energy and mineral products. Hydroelectric plants produced
electricity, which was made available to neighboring states. Petroleum deposits were developed and oil became
the leading export in 1991. Oil was sent via pipeline from the interior to the Pacific port of Tumaco for export. Coal
also became a significant export, along with nickel, gold, silver, and emeralds.
Social unrest remained a stumbling block for the Colombian economy in the 1990s, however. Paramilitary groups
organized by the Right assassinated many labor and indigenous leaders who spoke out for greater equality.
Rebels and drug cartels attacked government installations and pipelines carrying oil, prompting U.S. military aid.
Thanks to an overvalued peso and increased government spending, a deficit budget helped bring on a recession
in 1999. Unemployment rose to 20 percent. Foreign investments declined, worsening the situation. In 2002,Álvaro
Uribe was elected president on promises to restore peace and security. Loans from the World Bank, the Inter-
American Development Bank, and the Andean Development Corporation helped Uribe to increase spending on
social programs. Military forces were increased as well, and rebel threats were reduced. The power of the drug
cartels was also curtailed, but not eliminated. The supply of cocaine has been limited, but international demand
means that trafficking will continue.
The worldwide recession beginning in 2007 reduced the demand for Colombian exports and led to economic
hardships. Growth rates slowed dramatically, from more than 7 percent in 2007 to just 2.5 percent in 2008 to –1
percent in 2009, a performance that put Colombia among the hardest hit of Latin American economies during the
recession of the late 2000s. By 2010, the economy had recovered significantly, posting an annual 4.3 uptick in
GDP.
Tim J. Watts
 
See also:  Latin America. 

Further Reading
Hylton, Forrest. Evil Hour in Colombia. New York: Verso, 2006. 
Safford, Frank, and Marco Palacios. Colombia: Fragmented Land, Divided Society. New York: Oxford University
Press, 2002. 
Schneider, Ben Ross. Business Politics and the State in Twentieth-Century Latin America. New York: Cambridge University
Press, 2004. 
 
Commodity Markets
 
The role of a commodity market is to provide a locus for buyers and sellers to trade physical commodities at spot
prices or to trade derivative instruments of the physical commodities (futures and options). Traditional commodities
include agricultural products, livestock and meat, forest products, metals and minerals (precious and industrial),
and energy. Commodity markets serve as mechanisms to provide market price discovery and to transfer risk. In
recent decades, certain nonconventional derivative instruments have been traded on commodity markets.
Nonconventional “exotic” derivatives include derivatives of financial instruments (financial futures and options) and
derivative products related to environmental emissions, telecommunications bandwidth, and weather conditions.
Commodities (or derivatives of commodities) are typically traded in standardized contracts that specify the product,
its deliverable grade (quality), the contract size, the pricing unit, the incremental minimum fluctuation (tick size),
daily price limit trade, and, in the case of futures and options, delivery dates, settlement procedure, and any
special conditions. Commodities and derivatives have individual trading symbols.
There are approximately fifty major commodity markets and exchanges around the world, known by the cities in
which they are located. London and New York are the principal international commodity markets, and Chicago is
the leading domestic commodity market in the United States. Some of these trade in a wide variety of offerings,
while others are highly specialized. Metals are largely traded in London, New York, Chicago, and Shanghai. New
York, London, and Tokyo are centers of energy-based trading. Most of the markets offer electronic trading, auction
trading on a trading floor (public outcry, ring trading, or pit trading), and centrally cleared over-the-counter, off-
exchange trading.
 Major World Commodity Exchanges 
Exchange
Headquarters
Nation
Products Traded
CME (Chicago Mercantile Exchange
United States
Agriculture, timber

and Chicago Board of Trade)
New York Mercantile Exchange
(NYMEX)
United States
Energy, agriculture, metals,
freight, environment, energy
IntercontinentalExchange
United States
Agriculture, energy, environment
London Metal Exchange
United Kingdom Metals, plastics
Minor Metals Trade Association
United Kingdom Metals, metal by-products
Zhengzhou Commodity Exchange
China
Agriculture, plastics
Dalian Commodity Exchange
China
Agriculture, plastics
Multi Commodity Exchange
India
Agriculture, metals, energy,
environment
National Commodity and Derivatives
Exchange
India
Agriculture, metals, energy,
plastics, financial instruments
Abuja Securities and Commodity
Exchange
Nigeria
Agriculture
BM&FBovespa
Brazil
Agriculture, energy, financial
instruments
Dubai Mercantile Exchange
United Arab
Emirates
Energy
Source: Frank L. Winfrey.
 Speculation as a Positive Force
Commodity markets function on speculation. Although this term has a negative popular connotation, the activity
can also serve to enhance overall utility—that is, to maximize the benefits of commodities for society in general.
This does not seem to be the case at first glance, as most speculators who purchase commodity contracts have
no interest in the delivery of the actual goods. Instead, they are hoping to make a profit on price fluctuations,
unloading the contract before the delivery date to someone who actually wants the commodities.
Even barring unforeseen events—like a frost that damages a Florida orange crop or a hurricane that threatens oil
production on the Gulf Coast—commodity prices vary from place to place and over time. Buying and selling
commodities between markets, an activity known as arbitrage, ensures that the cost of a crop in one market will
be about the same, aside from shipping costs, as in another. Similarly, most crops are harvested at a given time
of year; if an entire crop were sold at harvest, the price would drop precipitously. Speculators, then, buy up the
commodity, put it in storage, and sell it later—thereby smoothing out the price over the course of the year. Such
activities can be a boon to consumers.
For producers and processors, the speculation inherent in commodity markets offers a form of insurance.
Commodity prices, particularly those of weather-affected crops, are notoriously volatile. Producers, usually farmer
cooperatives, and processors, such as those who run storage facilities and canning, freezing, or bottling plants,
can lock in a price by buying a commodity futures contract. A futures contract is one in which a buyer and a seller
agree today on the price and quantity of a commodity that will be delivered later. Futures agreements are
standardized with regard to quantities and delivery dates. They are traded on organized commodity exchanges.
Many producers will purchase futures contracts on the type of commodity they produce. Note that prices of futures
contracts are highly correlated with spot prices (the price for immediate delivery). If their crop is damaged—again,
using a frost in Florida as an example—it is likely that other producers will be hurt as well, pushing both spot and
futures prices up. Producers then make a profit on the futures contract because they can sell the contract at a
higher price than they paid, making a profit to offset the losses the frost caused in their own crop. This activity is

known as “hedging.”
Hedging has benefits for processors as well. If the owner of a corn warehouse buys a crop in the fall, hoping to
sell it in the spring, and the price drops significantly in the intervening months, he or she could lose a large
amount of money. To offset that risk, the warehouse owner purchases futures contracts, thereby hedging against
the exposure of all the corn in his or her warehouse to price fluctuations.
The Chicago Mercantile Exchange, founded in 1898 as the Chicago Butter and Egg Board, merged with the
Chicago Board of Trade in 2007 to form CME Group, the largest commodity futures and options exchange in the
United States. (Scott Olson/Stringer/Getty Images)
 Speculation as a Negative Force
While most commodity speculation serves the larger societal good, it can have deleterious effects as well. Efforts
to corner markets, or buy up virtually all of a given commodity, can lead to a buying and selling frenzy that often
drives up prices temporarily, only to crash when the corner fails. Efforts by speculators to corner the gold market,
for example, contributed to the financial panic of 1873, which led to a prolonged economic recession in the United
States. More recently, many economists blame market speculators for the spectacular run-up in oil prices in 2008,
worsening a recession that had begun in late 2007.
Positive or negative, this kind of commodity trading is as old as agriculture and trade itself, though the market in
futures was not fully realized until the development of railroads in the mid-nineteenth century, which allowed for
more timely delivery of goods over greater distances. Over time, markets have been developed for any number of
commodities as they became marketable, such as petroleum in the nineteenth century and uranium in the
twentieth. But these represent mere additions to an existing paradigm. In the 1990s, for example, the Texas-
based energy firm Enron pioneered a host of new products for futures trading, involving energy and
telecommunications bandwidth. The company collapsed in 2001, though not because of the intrinsic risk involved
in futures speculation but because of excessive debt and fraudulent accounting practices.
On a vastly greater scale, financial institutions began to develop futures contracts on the products they specialize
in. Beginning in the 1980s but accelerating in the 1990s and 2000s, investment banks and brokerage houses
created futures contracts on mortgages and interest rates. Known as derivatives because their value is derived
from other assets, these forms of futures contracts were at the heart of the crisis that struck the global financial

markets in 2008. Part of the problem was that these products were so unusual that they fell outside the purview of
government watchdog agencies and were too complicated for many of the institutions investing in them to
understand the risk they entailed.
While financial derivatives have received a black eye because of the crisis, the expansion of futures markets to
other noncommodity products continues in other areas. The cap-and-trade system on carbon emissions being
promoted in various countries, including the United States, would allow companies to buy and sell their emissions.
The underlying principle is that the marketplace provides a more efficient mechanism for allocating the costs of
carbon emission reduction—essential in preventing catastrophic climate change—than taxes, fines, and specific
government-mandated limits.
James Ciment and Frank L. Winfrey
 
See also:  Agriculture. 
Further Reading
 Gallacher, William R.  Winner Take All: A Brutally Honest and Irreverent Look at the Motivations and Methods of Top
Traders.  New York:  McGraw-Hill,  1994. 
 Geman, Hélyette, ed.  Risk Management in Commodity Markets: From Shipping to Agriculturals and Energy.  Hoboken, NJ: 
John Wiley and Sons,  2008. 
 Morrison, Kevin.  Living in a Material World: The Commodity Connection.  Hoboken, NJ:  John Wiley and Sons,  2008. 
 National Futures Association.  Opportunity and Risk: An Educational Guide to Trading Futures and Options on Futures. 
Chicago:  National Futures Association,  2006. 
 Schofield, Neil C.  Commodity Derivatives: Markets and Applications.  Hoboken, NJ:  John Wiley and Sons,  2007. 
Community Reinvestment Act (1977)
 
The Community Reinvestment Act of 1977 (CRA) was passed in the Ninety-Fifth U.S. Congress and signed into
law by President Jimmy Carter. This federal law was designed to encourage commercial banks and savings
institutions to reduce discriminatory lending practices in low-and moderate-income neighborhoods. The act
ensures that lenders “help meet credit needs of the entire community, including low-and moderate-income
neighborhoods in a manner consistent with safe and sound operation of the institution.” Recently, critics of the act
point to the CRA as an important factor in the financial a crisis of 2008, citing the fact that it lowered standards for
mortgage lending, thereby contributing to the spread of subprime mortgages, the financial instrument at the heart
of the crisis.
The goal of the act was to reduce discriminatory credit practices against low-income and minority neighborhoods,
a practice known as redlining. To enforce the statute, federal regulatory agencies were required to assess each
bank’s and savings institution’s record in terms of its compliance with the law before approving the opening of new
bank branches or approving mergers and acquisitions. The law, however, does not entitle institutions to make
high-risk loans that might bring about losses. Recent amendments to the CRA have allowed community groups to
better access CRA information and enabled the lending organizations to increase their activities.

 Does CRA Help or Hurt Lending Practices?
There is no clear consensus as to the effectiveness of the CRA. Some economists maintain that there is no solid
evidence that the CRA was effective in increasing lending and homeownership more in low-income neighborhoods
than in high-income ones. Even Federal Reserve chairman Ben Bernanke recently stated that more lending does
not necessarily produce better outcomes for local communities. However, in some instances, Bernanke notes, “the
CRA has served as a catalyst, inducing banks to underserved markets that they might otherwise have ignored.”
In a hearing on the CRA before the U.S. House Committee on Financial Services in February 2008, the director of
the Division of Supervision and Consumer Protection at the FDIC, Sandra L. Thompson, praised the positive
impact of the CRA by pointing to increases in lending to low-and moderate-income households and minorities in
the decades since the law’s passage. She pointed to data from studies done at Harvard University that showed
that between 1993 through 2000, home purchase lending to low-and moderate-income groups had increased by
94 percent—more than in any of the other income categories.
At the same hearing, New York University professor Larry White stated, “Fundamentally, the CRA is a regulatory
effort to ‘lean on’ banks and savings institutions, in vague and subjective ways, to make loans and investments
that [the CRA’s proponents believe] those depository institutions would otherwise not make.” White felt that such
laws were more likely to drive institutions out of neighborhoods, and that better ways to achieve these goals would
be through vigorous enforcement of antidiscrimination laws and antitrust laws, the latter to promote competition,
and federal funding of worthy projects directly through community development funding.
 CRA and the Subprime Crisis
The CRA was under particular scrutiny after the foreclosure and mortgage crisis of 2008–2009. Some charged
that the CRA was primarily responsible for the financial crisis, as it encouraged the loosening of lending standards
throughout the banking industry and encouraged banks to make bad loans. Others commented that this act, along
with government-backed Fannie Mae, was primarily responsible for pushing banks and mortgage brokers into
granting easy credit and subprime loans to those who could least afford them.
Many housing advocacy groups, like the Association of Community Organizations for Reform Now (ACORN), also
felt that lowered credit standards resulted in unsupportable increases in real-estate values in low-to moderate-
income communities. Ballooning mortgages on rental properties resulted in higher rents for lower-income tenants
who could least afford them.
However, other commentators have noted that CRA-regulated loans tended to be safe and profitable, and that the
subprime excesses came primarily from institutions not regulated by the CRA. A Federal Reserve survey has
shown that 50 percent of the “toxic” subprime loans were made by independent mortgage companies that were
not regulated by the CRA. An additional 25 to 30 percent came from only partially CRA-regulated bank
subsidiaries and affiliates. Finally, others note that it is unfair to place blame on CRA for what other federal
agencies did on their own. In particular, defenders of CRA point out that the Department of Housing and Urban
Development (HUD) and the Office of Federal Housing Enterprise Oversight (OFHEO) allowed Fannie Mae and
Freddie Mac to fulfill their affordable but ill-advised housing goals by buying mortgage-backed securities that were
never part of the CRA. (Note that Congress abolished OFHEO in July 2008, putting its functions under the newly
created Federal Housing Finance Agency, or FHFA, and that Fannie Mae and Freddie Mac were put into
conservatorship by the U.S. government on September 7, 2008. Both actions occurred because of de facto
bankruptcy of the two housing giants and the ongoing financial crisis.)
By late 2009, the House Financial Services committee was discussing an act to update the CRA. The Community
Reinvestment Modernization Act of 2009 would extend the CRA’s lending standard to nonbank institutions, such as
credit unions, insurance companies, and mortgage lenders. It would also make the act more race based, applying
its standards to minorities regardless of income or whether they lived in low-and moderate-income areas.

Abhijit Roy
 
See also:  Housing Booms and Busts;  Mortgage Lending Standards;  Mortgage, Subprime. 
Further Reading
Ardalan, Kavous,  “Community Reinvestment Act: Review of Empirical Evidence.” Journal of Commercial Banking and
Finance  5:1/2 (2006):  115–139. 
Ardalan, Kavous, and Ann Davis.  “Community Reinvestment Act and Efficient Markets Debate: Overview.” Journal of
Economics and Economic Education Research  7:3 (2006):  53–70. 
Black, Harold A., Raphael W. Bostic, Breck L. Robinson, and Robert L. Schweitzer.  “Do CRA-Related Events Affect
Shareholder Wealth? The Case of Bank Mergers.” The Financial Review  40:4 (2005):  575–586. 
Brevoort, Kenneth P., and Timothy H. Hannan.  “Commercial Lending and Distance: Evidence from Community
Reinvestment Act Data.” Journal of Money, Credit, and Banking 38:8 (2006): 1991–2012. 
Singer, Daniel D.  “Online Banking and the Community Reinvestment Act.” Business and Society Review  111:2
(2006): 165–174. 
Title VII of the Housing and Community Development Act of 1977.  Pub. L 95–128; 91 Stat. 1147, 12 USC 2901–05.
Vitaliano, Donald F., and Gregory P. Stella.  “The Cost of Corporate Social Responsibility: The Case of the Community
Reinvestment Act.” Journal of Productivity Analysis  26:3 (2006):  235. 
Vitaliano, Donald F., and Gregory P. Stella.  “How Increased Enforcement Can Offset Statutory Deregulation: The Case of
the Community Reinvestment Act.” Journal of Financial Regulation and Compliance  15:3 (2007):  262–274. 
Confidence, Consumer and Business
 
Consumer confidence, a post–World War II concept that has developed into an important economic indicator in the
United States and throughout the world, measures and describes the degree of optimism or pessimism about both
the national economy and individuals’ personal economic circumstances. This subjective measurement, conducted
monthly by several major organizations and factored into U.S. government economic projections, infers a causal
link between consumers’ economic optimism and greater consumer purchasing and, thus, economic growth.
Demographer Fabian Linden postulated in 1967 that consumers offer clues about the nation’s economic outlook
faster than such “hard” data as gross domestic product growth or retails sales.
 Key Measures and What They Mean
Government, investors, manufacturers, retailers, and financial institutions in the United States closely watch the
principal consumer confidence indices—the University of Michigan’s Consumer Sentiment Index and the
Conference Board’s Consumer Confidence Index—to forecast and plan. If consumers are confident, businesses
take this as a signal to increase production. Conversely, if pessimism prevails or is growing, manufacturers are
likely to cut inventories and delay new investment. Government can project either higher or lower tax revenues
based on consumer confidence. Banks can plan for either increasing or decreasing lending. And the media widely
covers these indices as bellwethers of the state of the economy.

In many ways a combination of economics and psychology that long predates behavioral economic analysis,
consumer confidence reflects the partly rational, partly irrational feelings of the public. Economist John Maynard
Keynes referred to consumer and business confidence as the “animal spirits” that play a significant role in
macroeconomic performance. These indices are predicated on the belief that consumer mood powerfully
influences spending and the economy at large.
“Consumer confidence figures are really a measure of how we feel about ourselves,” said David Wyss, chief
economist at Standard & Poor’s, in 2003. “If consumers are worried, Main Street retailers better get worried too.”
But is this true? As Nobel Prize–winning economist James Tobin asked in 1959: “Are households which express
optimistic attitudes and positive intentions more likely to spend and less likely to save than other households? Do
the answers to attitudinal and intentions questions provide information of value in predicting the buying behavior of
households? If so, does this information supplement or merely repeat the predictive information contained in
financial, economic, and demographic data concerning households?”
 CCI, MCSI, and George Katona
In the United States, the Conference Board releases its Consumer Confidence Index (CCI) on the last Tuesday of
each month. The University of Michigan’s Consumer Sentiment Index (MCSI) is released three days later. Each
index, as well as such lesser-known indices as the Washington Post–ABC News Consumer Comfort Index, is
based on large-scale national surveys addressing a variety of economic topics and disaggregated into several
subsidiary indicators.
The Consumer Sentiment Index is the oldest confidence index, developed in the late 1940s at the University of
Michigan by economist George Katona to probe consumer feelings and behavior, what influences them, and how
this data can be used to forecast and support business and government growth goals. During World War II,
working for the Federal Reserve Board, Katona wrote that rising aspirations will fuel economic growth and that
experts can chart, if not guide, expectations of consumer sentiment and behavior. Trained as a psychologist as
well as an economist, Katona obtained funding for his research from the Fed, enabling him to establish the Survey
Research Center as part of the Michigan’s Institute of Social Research in 1946.
For twenty-six years, beginning in 1952, Katona directed the quarterly Survey of Consumer Attitudes. He
published numerous books and articles, managed a staff of sixty-five plus several thousand part-time interviewers
and a thousand-person consumer focus group, and was celebrated by business and the media as a defender of
America’s abundant capitalist economy. Katona—who claimed to have invented the new field of psychological or
behavioral economics based on the ideas of psychologists Kurt Lewin and Abraham Maslow as much as those of
Keynes—staunchly believed that consumer attitudes are at least as critical as macroeconomic data in
understanding the economy. He was one of the first to detect U.S. consumer optimism immediately after the war,
predicting—against conventional wisdom—that the U.S. economy was headed for a boom rather than a new
depression. He was also one of most perceptive observers of the new mass-consumption economy of the postwar
era.
In such books as The Powerful Consumer (1960), The Mass Consumption Society (1964), and Aspirations and
Influence (1971); in his in-depth analyses of consumer attitudes and demand; and in the Surveys of Consumer
Attitudes, Katona hailed consumers as the dynamo behind the booming U.S. economy. His basic, oft-repeated
formula was that abundance depends on sustained high demand, high demand depends on consumer optimism,
and optimism does not necessarily depend on income.
Contrary to neoclassical ideas of scarcity, Katona said: “Instead of being driven to avoid hardship and being
satisfied with restoring an equilibrium, in an era of prosperity people are spurred by rising levels of aspiration.”
They continually strive for more and more income and consumption, which “may become cumulative and self-
reinforcing.” To Katona, capitalism, consumption, and democracy are all of a piece, complementary ingredients of
an abundant society.

The MCSI is benchmarked at a value of 100 as of December 1964. The index not only fluctuates in tandem with
existing economic conditions, but it has been a remarkably reliable predictor of the nation’s medium-term
economic outlook. Five core questions and a host of industry-specific questions are asked in thirty-minute
telephone interviews of a random sample of more than 500 Americans. The major questions concern individuals’
beliefs about the current business climate, personal finances, and spending. The survey asks people to assess
their current financial situation and to predict what it will be one and five years in the future. It also asks about
expectations of the strength or weakness of the national economy in one and five years, and present intentions to
buy major household items. The MCSI is based on relative scores of the percentage giving favorable replies minus
the percentage giving unfavorable replies, with adjustments derived from the sample composition and prior
surveys. The index also is subdivided into three indices—the Index of Consumer Sentiment, the Index of Current
Economic Conditions, and the Index of Consumer Expectations. The latter is included in the U.S. government’s
Index of Leading Economic Indicators.
The Conference Board, a business-sponsored economic research organization, launched its Consumer
Confidence Index in 1967 as “a monthly report detailing consumer attitudes and buying intentions, with data
available by age, income and region.” The Conference Board mails a questionnaire to 5,000 households each
month. About 3,500 reply, making the sample less representative of the general population. Expectations about
future economic conditions make up 60 percent of the index, and views about current conditions comprise 40
percent. Again, relative values of “positive,” “negative,” or “neutral” responses are calculated against a 1985
benchmark of 100. Two subindices are released—the Present Situation Index and the Expectations Index.
 Other Indices and Underlying Factors
Other opinion research about economic conditions and expectations are conducted in the United States. The
Washington Post–ABC News Consumer Comfort Index asks 1,000 randomly selected adults each month to rate
national and personal economic conditions and whether it is a good time to make major purchases. Many other
countries have developed and issued consumer confidence indices. For example, KBC Bank Ireland and an Irish
think tank have calculated such an index since 1996. An American consulting firm surveys 10,000 people in fifteen
Indian cities to create an index for that country. Most developed nations of the Organisation for Economic Co-
operation and Development (OECD) follow America’s lead in conducting such surveys, and the Nielsen Company
conducts a Global Online Consumer Survey with respondents in more than fifty countries.
Research suggests that the top factor driving consumer confidence is employment. “As the labor market goes, so
goes the confidence index,” says Lynn Franco of the Conference Board. “The three keys to consumer confidence
are jobs, jobs, and jobs.” Stock market swings and geopolitical events also have an effect; for example, after the
September 11, 2001, terrorist attacks on the United States, the CCI fell precipitously, by 17 points. Younger
Americans tend to be most optimistic, with optimism declining with age. Men tend to be more upbeat than women,
and optimism increases with educational attainment. Not surprisingly, lower-income Americans are more
pessimistic.
Similar surveys of business confidence and expectations are conducted by a number of entities, ranging from the
private Moody’s Investors Service to the U.S. Department of Commerce’s Bureau of Economic Analysis (BEA).
The Moody’s Survey, which has been released every Monday since 2006, not only asks business leaders for
sales and investment data, but also about their outlook for business conditions in the coming months. The
BEA/Conference Board survey tracks the relative assessment of business conditions over time among chief
executive officers. Several states, particular industries, and countries outside the United States also conduct
surveys of business leaders’ economic expectations. Data similarly are used to help assess current economic
conditions and forecast future business investment, hiring, production, and other activities. But confidence indices
have their critics.
 Questions and Criticisms

In the mid-1950s, a Federal Reserve–appointed committee questioned the power of such surveys in predicting
business trends and economic conditions. C. Britt Beemer, president of America’s Research Group, has said that
because the questions are not open ended, they do not allow people to explain why they hold their beliefs.
Sampling problems arise with the Conference Board’s CCI, as it relies on a self-selected pool of respondents.
Other critics say that the MCSI misses people in the highest and lowest income brackets. A frequent criticism is
that most people are not well informed about macroeconomic conditions and, therefore, should not be asked
about “business conditions.” Instead, questions about buying intentions and perceived employment prospects are
said to be more predictive. Others suggest that new questions should be asked, particularly ones that gauge
people’s sense of the probability of different economic developments.
Nonetheless, even the moderately skeptical James Tobin concludes that “the [economics] profession owes George
Katona and his colleagues at the Survey Research Center for their imaginative and pioneering work in the
collection and interpretation of buying intentions and attitudinal data.”
Andrew L. Yarrow
 
See also:  Behavioral Economics;  Consumption;  Savings and Investment. 
Further Reading
Conference Board:  www.conference-board.org
Dominitz, Jeff, and Charles F. Manski.  “How Should We Measure Consumer Confidence?” Journal of Economic
Perspectives 18:2 (Spring 2004): 51–66. 
Horowitz, Daniel. The Anxieties of Affluence: Critiques of American Consumer Culture, 1939–1979. Amherst: University of
Massachusetts Press, 2004. 
Katona, George. The Mass Consumption Society. New York: McGraw-Hill, 1964. 
Katona, George. The Powerful Consumer: Psychological Studies of the American Economy. New York: McGraw-Hill, 1960. 
Linden, Fabian.  “The Consumer as Forecaster.” Public Opinion Quarterly 46:3 (1982): 353–360. 
Moody’s. Economy.com, Weekly Survey.  www.economy.com/survey-of-business-confidence.asp
Tobin, James.  “On the Predictive Value of Consumer Intentions and Attitudes.” Review of Economics and Statistics 41:1
(February 1959): 1–11. 
Weiss, Michael J.  “Inside Consumer Confidence Surveys.” American Demographics 25:1 (February 2003): 23–29. 
 
Congressional Budget Office
 

The Congressional Budget Office (CBO) is a nonpartisan agency in the U.S. federal government charged with
providing economic data to Congress, especially for the purpose of helping it formulate the national budget. In that
capacity, it conducts research studies and prepares analytical reports that, among many other things, assess the
timing and duration of past, current, and future business cycles.
The CBO was founded on July 12, 1974, under the Congressional Budget and Impoundment Control Act and
began operations on February 24, 1975. The CBO is located in Washington, D.C., and has about 230 employees,
mainly economists and public policy analysts. The CBO’s director is appointed for a two-year term by the Speaker
of the House and the president pro tempore of the Senate, in consultation with each chamber’s budget committee.
The CBO has six divisions that work both independently and cooperatively: the Macroeconomic Analysis Division,
Budget Analysis Division, Tax Analysis Division, Health and Human Resources Division, Microeconomic Studies
Division, and National Security Division. The agency also includes a Panel of Economic Advisers and a Panel of
Health Advisers, consisting of experts in each area. The panels advise the CBO directorship on ways to improve
transparency, reliability, and professional standards in the agency.
By February 15 of each year, the CBO is responsible for providing Congress—specifically, the House and Senate
budget committees—objective and timely information, estimates, and analyses that help it make fiscally sound
policy decisions and allocations on programs covered by the federal budget. Every year, it testifies before
Congress on a wide variety of issues and policies, including alternative spending and revenue scenarios. It
completes hundreds of formal cost estimates and impact assessments. It estimates the impact of unfunded
mandates (regulations imposing costs on state or local governments for which they do not receive reimbursement
from the federal government) on state, local, and tribal governments and the private sector. It estimates the
president’s budget proposals and generates cost estimates for all bills reported by congressional committees and,
on request, possible amendments to those bills and bills at other stages of the legislative process (including
alternative proposals) with a likely impact on state or local governments or the private sector. When necessary for
clarification, CBO analysts contact the sponsoring legislator or the staff of the appropriate legislative committee; in
all cases, however, the CBO draws its own conclusions and makes its own estimates based on independent
analysis.
The Congressional Budget Office provides detailed, nonpartisan estimates and analyses of federally financed

programs. Its revised budget and economic outlook for 2010 projected a deficit of $1.4 trillion. (Bloomberg/Getty
Images)
Specific legislation over the years has assigned additional tasks to the agency, such as assessing the financial
risks posed by government-sponsored enterprises or the treatment of administrative costs under credit reform. In
addition, the CBO may also conduct analytical studies if requested by individual members, committees, or
subcommittees of the House or Senate. As in economic analysis and reporting, the CBO remains objective and
impartial on all policy matters, offering no recommendations or proposals.
Under the Congressional Budget Act of 1974 and the Unfunded Mandates Reform Act of 1995, the CBO must
fully explain its methodologies and assumptions. The agency obtains data from a variety of sources, including
government statistical agencies, industry groups, and private surveys. Likewise, while it develops some of its own
analytical models, it also relies on those formulated by others. Beyond the expertise of its own analysts, the CBO
also seeks the help of outside experts, especially in the course of examining specific business sectors, such as
agriculture or telecommunications.
The CBO issues a steady flow of studies, reports, briefs, monthly budget reviews, letters, presentations,
background papers, analytic studies, and other publications, all available online. It regularly publishes a short-term
economic and budget outlook report at the end of January, which includes estimates of federal spending and
revenue for the next ten years. It also publishes a long-term budget outlook, with revenue and spending scenarios
—and their economic implications—through the year 2050. Since 2004, the agency has also produced long-term
outlooks for the Social Security program. A variety of supplemental information and revised data is available on
the CBO Web site, including the most up-to-date budget estimates, economic projections, and the status of
discretionary appropriations (those not mandated by existing law). Most of the agency’s work is made readily
available to the public; only cost estimates for legislative proposals may remain confidential until they become law.
CBO assessments of its own economic forecasts for accuracy, balance, and consistency are reviewed both
internally and by outside experts.
While the CBO is widely praised across the political spectrum for its nonpartisanship and the quality of its
research, its assumptions and predictions are, by necessity, politically controversial, as for example ten-year
estimates of the federal deficit. In addition, some in Washington and the media have criticized the CBO for an
overly narrow focus on direct costs and benefits of new programs, ignoring broader savings the programs might
generate. During the health care debate of 2009, for example, the CBO estimated that part of the proposal
circulating in Congress could cost the federal government up to $1 trillion over ten years. Conservative opponents
of the proposal seized on the report as evidence that the health care reform was too costly in an age of
ballooning deficits. Supporters of the reform effort—both in Congress and the Barack Obama administration—
countered that the CBO was ignoring indirect savings, such as those associated with broader preventive care.
Tiia Vissak and James Ciment
 
See also:  Council of Economic Advisers, U.S.;  Fiscal Balance;  Fiscal Policy;  Government
Accountability Office;  National Economic Council;  Tax Policy. 
Further Reading
Congressional Budget Office:  www.cbo.gov
Congressional Budget Office. CBO’s Policies for Preparing and Distributing Its Estimates and Analyses. Washington,
DC: Congressional Budget Office, 2005. 

Construction, Housing and Commercial
 
Among the oldest of human activities, the construction of buildings—for commercial, industrial, residential, and
other uses—displays many continuities as well as many changes over the millennia. An important activity in all
periods of human history, construction is one of the largest industries in the United States in the twenty-first
century. Ironically, for an industry whose aim is to provide one of the most permanent of products, the construction
business has proven to be one of the most volatile, rising and falling dramatically with upturns and downturns in
the overall economy.
 Market Characteristics
Contract construction represented more than $500 billion in value added (current dollars) and 4.6 percent of U.S.
gross domestic product (GDP) from 2001 to 2004. In the United States, nearly 8 million people were employed in
construction and alteration work annually from 2005 to 2006. Added to this are the ancillary industries that supply
construction workers with tools and materials, expanding the construction network dramatically. For example, in
2007, 122 million tons (110.7 million metric tons) of concrete were produced in the United States alone.
But the business is also fraught with instability. There are several reasons for this. For the most part, construction
is a process of building custom-designed products, on location, often with a specific user in mind. Construction
sites vary from the building of a multimillion-dollar urban skyscraper to the putting in place of a sewer on a local
street corner. Installation methods generally are labor intensive, and projects serve as textbook examples of the
law of diminishing returns (marginal productivity theory), whereby a rising labor-to-capital ratio tends to produce
decreasing returns after a maximized point of output is reached. Thereafter, efficient production becomes a
balancing act between capital and labor, requiring careful supervision of workers and materials. This makes
construction projects sensitive to changes in wage rates and capital costs—so sensitive that job sites have been
known to shut down following periods of intense inflation, as the cost of construction rapidly exceeds the original
estimate over time.
A recent snapshot of the sector would show a multitude of firms, some sole proprietorships and others with
thousands of employees spanning markets worldwide. The U.S. Census Bureau reported in 2002 that there were
2,657,360 construction companies in the American market, of which more than 2.4 million were individual
proprietorships. Typically, a small number of firms produce the largest share of revenue. Such an industrial
structure makes the construction industry susceptible to the upheaval of market gyrations. Low capitalization at
one extreme and overexpansion at the other creates a risk profile that makes firm survival tenuous. Limited
barriers to entry, such as licensing requirements and low start-up costs, allow for rapid expansion as demand
rises. Conversely, these financially unsecured firms are vulnerable to downturns, causing equally rapid market exit
as thin profit margins evaporate. The rapidity of turnover and the market structure combine to reinforce the cyclical
nature of the construction contractor’s existence.
While production and market organization are important, the real engine of market movement comes from the
demand side. Construction demand is derived overwhelmingly from the needs of other economic sectors
(speculative commercial construction is one exception). As a derived demand, the “wants” for construction are
subject to forces ranging from a corporation’s expansion plans to tax breaks for specific types of construction
(e.g., health care facilities). Thus, the construction industry feeds on the success of the overall domestic economy.
 Labor Markets
Modern construction is characterized by the use of skilled labor, which is divided among approximately sixteen

trade categories. Trade abilities and knowledge are uniquely defined, and often gained through formal
apprenticeships or informal mechanic-helper arrangements. Such skill compartmentalization requires a fair degree
of coordination by job managers to keep construction projects on schedule and within budget. Total construction
employment hit an all-time high of 7.8 million in 2007, propelled by a housing boom.
Traditionally, the hands-on nature of the production process led to low capital-to-labor ratios as builders and
contractors struggled to impose capital substitutions for labor. The intricacies of on-site installations gave workers
a fair degree of individual control over the pace of the workplace. This fact lent strength to building trades unions
in the twentieth century, which negotiated wage premiums for construction workers. The importance of skilled
labor cannot be overestimated in understanding market-based construction. The time needed to train workers in
the building trades makes labor supply problematic given the wide swings in the building sectors.
Employers are hesitant to invest heavily in training entry-level employees for two reasons. First, they may be
concerned that workers will leave their firm after gaining enough knowledge to find employment with another
company. Second, the employer essentially has trained its competitor, given the ease of market entry. Building
trades unions solved this problem by developing multi-employer apprenticeship programs, in which costs are
shared and skilled labor is made available through a hiring hall. Both the union and nonunion sectors still depend
on itinerant workers to make up shortfalls in labor demand when work is plentiful. Yet when work is scarce, labor
market attachment among union workers remains high because of long-term benefit plans and more equitable
distribution of limited job offerings. Thus, in the long run, the rise and fall of construction activity leads to a steady
flow of workers into and out of the industry as employees react to the boom-and-bust economy, leaving
contractors wanting for skilled labor during upswings and oversupplied when work dries up. All in all, U.S.
construction employment since has risen over time, but with clear evidence of periodic ups and downs in the
market demand for labor.
 Cycles
Cyclical behavior has always been apparent in construction, in the same way that the weather creates optimal
seasons for agricultural production. Weather-oriented cycle durations vary by location, but developers in cold
climates still rush to undertake excavation prior to the onset of the freezing cold. While these cycles are not
economic swings by definition, they serve as parameters for peaks and troughs in employment and output on an
annual basis.
A more practical concern is how cycles in the construction sector are affected by factors in the overall economy.
Downswings produced by a slowing of economic activity cause the building sector to follow suit, while the opposite
is true during rebounds. For example, an overlay of Bureau of Labor Statistics employment data for the
construction industry with the National Bureau of Economic Research (NBER) listing of business cycles shows
clear patterns, such as the examples listed in the following table. There are relative similarities in the first two
examples. In the third, the contraction of 1990 ended in March 1991 for the overall economy, while the decline in
construction lasted considerably longer, from March 1990 until summer 1992.
 Contractions of the U.S. Economy and Construction Employment 
                     U.S. Economy
                     U.S. Construction
Peak
Trough
Peak
Trough
July 1953
May 1954
April 1953
September 1954
November 1973
March 1975
March 1974
September 1975
July 1990
March 1991
March 1990
July 1992

Sources: National Bureau of Economic Research; U.S. Bureau of Labor Statistics.
Timing is of more interest. In part, this is related to the speculative aspect of the construction investment decision,
and in part, it is connected to the timing of the construction process. Once the decision is made to go forward with
a speculative project, the owners are affected by the overall economy. Shifts in demand, changes in use, and the
cost of construction are all variable influences. In response, developers may alter the form of the building through
design or scale switch (e.g., from condominiums to rentals), or shut the job down if the expected revenue
negatively affects the minimum return on investment regardless of the structure’s function.
Construction projects tend to be relatively long-term endeavors compared to most manufacturing processes. Thus,
a time span of eighteen to twenty-four months exposes builders and contractors to a wide variety of economic
factors—consider that the NBER found that post–World War II contractions averaged ten months and expansions
fifty-seven months. Typically, input price change risk, which producers of consumer products (e.g., automobiles or
paper towels) can limit by altering their short-run output or selling prices, negatively affects contractors who have
already submitted bids based on existing costs.
Added to this is the fact that work is awarded to architects and contractors through a bidding process that tends to
create short-term relationships, so that new work is often carried out by different combinations of employers and
workers. Although such continuing competition can benefit the buyers of constructions services, the inefficiencies
caused by the realignment of shops calls into question the accuracy of the price estimates offered by successful
bidders. Clearly, most projects are run successfully, but a by-product of the contracting process is instability when
poor estimates lead to firm failures.
Although the construction industry always has seemed ripe for technological and structural advancement, it is in
the last quarter of the twentieth century and the initial decade of the twenty-first that substantial progress has
been made in the capital/labor trade-off. The key to cutting on-site labor hours is prefabrication. Early on, major
shifts came in building techniques that were associated with breakthroughs in new materials or engineering, such
as steel fabrication in the late nineteenth century and the commercialization of portland cement in the early
twentieth. Though these techniques added new dimensions to the building process, they did not necessarily
shorten the time needed for construction. The development of drywall in the mid-twentieth century and advances
in excavating equipment, such as steam shovels and modern payloaders, cut the time for wall production, land
clearing, and foundation work.
The elusive ability to significantly improve construction productivity came full circle with the advent of off-site
manufacturing of building elements that incorporated many of these early innovations. Prefabricated curtain walls
of brick, stone, or metal, precast concrete slabs, and prefabricated residential units have shortened the time span
for erection. Technology also has been augmented by improved project management methods such as “design
build” and “fast track” construction, again hastening the completion of projects. The result of this increased speed
is faster turnover for skilled labor and greater urgency for employers to have another project lined up for their
capital investment. This, in itself, challenges long periods of expansion and intensifies the cyclical nature of the
construction process.
Yet the greatest threat to the industry in some ways is the most obvious. It is found in the funding pipeline for all
parts of the construction sector. Construction is extremely sensitive to interest rates given its role in capital
formation. Although some projects are self-funded, the vast majority require equity investors and construction
capital. Although each form of investment undergoes its own due diligence, both are constrained by the vagaries
of the broader economy.
Corporate construction (e.g., a new headquarters) or owner-financed residential undertakings often are started
because of specific needs and because sufficient funding already exists. In more speculative sectors of the
industry, builders and developers seek to raise capital through the sale of equity in the project. There are
numerous means of raising capital at this level, but all are tied to some expected return on investment, either

through the sale of finished structures or through leasing income streams. The investment decision is tied to many
traditional economic measures and interest rates, and alternative investment opportunities help determine the end
result. Why is it, then, that construction booms seemingly charge ahead in the face of an impending downturn and
the industry seems slower to react than other economic sectors as expansions begin? The housing market offers
some clues.
 Housing Market
Although private homes have a certain element of use value comparable to consumption goods, the role of the
housing market is accounted for in a manner similar to that of other additions to the capital stock of a nation.
Large amounts of public and private money are invested in home building, and it has long been institutionalized in
public policy around the world. The demand for housing has several components, all of which may fuel and then
retard construction, leading to market oscillations. Waves of immigration, domestic population growth, and rising
disposable income are three well-known influences on demand. The first two demonstrate a need for additional
housing, while the last creates demand as people seek larger living spaces. In addition, all of these factors are
easily affected by government through legislation and executive direction.
The decision to add housing units to the existing supply is guided by interest rates, which affect the cost of
financing for both the builder (loans) and the buyer (mortgages). Given a level of demand, attractive interest rates
are necessary to stimulate home production, although they are not, by themselves, sufficient to do so. The prices
of land, labor, and materials are other aspects of the cost of production that shape a developer’s appetite for
residential construction. Elements from the supply and the demand sides create a roller-coaster ride for builders,
contractors, and workers. The accelerator principle, which holds that growth in real GDP leads to increases in
planned investment spending, is important here. Expected future growth in the demand for housing leads builders
to shift investment capital to residential projects. The timing allows them to break ground for new units that will be
completed during the current expansion or finish as an uptick is getting under way.
It is the unplanned portion of housing investment that can produce instability. If output is steady and population
growth is zero, there will be only replacement additions to capital stock. However, rising standards of living and/or
increases in population create conditions that spur unplanned investment in a nation’s housing stock. The growing
demand depletes existing inventories of homes and, given the time needed for housing production, causes
planned (replacement) investments in the residential market to fall short of demand. This, in turn, sets off a
cyclical upswing in home building.
The housing market then starts to attract additional investment from existing home builders, while the low barriers
to entry allow new entrants in the market, further fueling investment. If, at the same time, low interest rates and
government policies entice more homebuyers, the industry will experience a boom. Hiring increases, the number
of housing starts explodes, and supply rises rapidly. The spigot of economic activity opens wide, and, short of
government controls or a disastrous economic occurrence, such as the credit market debacle of 2008–2009, the
industry will continue to add housing units—but to what point?
Consumption goods produced in factories are regulated easily through information on supply and demand.
Housing, on the other hand, is a durable investment-grade good that often is custom designed, time consuming to
build, and produced by both small-scale firms and large corporate builders. The boom psychology and the
consistent inability to gauge real market demand historically fuel the boom-and-bust nature of housing
construction. In addition, housing is an extremely durable good, making replacement investment far less
predictable, which, in turn, leads to fluctuations rather than steady demand.
It is not much of a stretch to believe that commercial construction operates in a similar way. Research in the early
1960s by economist Moses Abramowitz indicated that all aspects of construction historically have suffered from a
series of long swings. Commercial projects, however, tend to be larger, but more susceptible to replacement
through technologically superior structures or renovations over time. Consider the fact that both the Chrysler
Building and the Empire State Building were started at the end of the 1920s economic boom in New York City

and completed at the onset of the Great Depression, while the Burj Khalifa in Dubai was started during the boom
years of the mid-2000s but finished after the global financial crisis of 2008, which nearly rendered the emirate
bankrupt. The firms in this segment of the industry tend to be bigger, better capitalized, and better able to
withstand swings in demand. However, both residential and commercial construction markets often give the
appearance of a game of “musical chairs,” in which no builder wants to be the last one constructing during the
boom or holding an undesirable structure as the market cycles downward.
 Crisis
Economists long have recognized that crises play a role in the macroeconomy, initiating a catharsis in which
markets are restructured, inefficiencies are exposed, and the groundwork for revival is prepared. Such economic
chaos, even in construction, is far more devastating and socially destructive than the typical troughs of a business
cycle. The construction industry has been caught in the tidal waves of the major downturns and certainly has
faired no better than other sectors.
For example, the stagflation crisis of the 1970s started at a GDP peak in November 1973 and bottomed out
sixteen months later in March 1975. Construction employment shadowed this decline, cresting in February 1974
and reaching a low point in July 1975. The more severe financial crisis that began in December 2007 and
traumatized global market systems was preceded by a precipitous drop in construction employment that
commenced in January of the same year. The early twenty-first-century downturn underscores the dependence of
the building industry on the credit markets and confirms that demand is derived from the needs of the rest of the
economy.
The initial retrenchment in housing demand that stalled the building boom at the beginning of the millennium
resulted from a falloff in the demand for private residences. Rising delinquencies in the subprime mortgage
markets sounded a warning to home builders in an era of easy credit, while a lack of due diligence by financial
institutions created a seemingly unending pool of potential commercial and residential property failures. Builders
applied the brakes to some projects and abandoned others, leading to a drop in residential construction permits
and employment in 2007. Census Bureau data show that annual new, privately owned housing starts fell from a
high of 2.07 million in 2005 to a low of only 554,000 in 2009, a 73 percent decrease in just forty-eight months.
Compounding the situation was the fact that banks and brokerage houses had bundled many of these mortgages
into collateralized debt obligations, which were sold on the open financial markets. As arrears mounted in the
paying of home mortgages, holders of these debt instruments began to suffer significant losses, making lenders at
all levels reluctant to issue more debt. (By mid-2009, the same situation had developed in the commercial sector.)
Builders and developers that were scaling back from a perceived falloff in demand also felt the credit crunch as
liquidity dried up for existing and near-term projects. The collapse of the stock markets further reduced demand,
as widespread layoffs and a dearth of economic activity arrived in the fourth quarter of 2008. Builders of all sizes
were unable to secure equity loans or construction financing, leading to further cutbacks and massive labor
reductions.
Additional evidence of the cyclical nature of market-based construction can be found in the securitization process.
Economic historians have shown that the overly optimistic view of investors during the 1920s fueled a building
boom that was unsustainable in terms of return on investment. In much the same way, the opacity of collateralized
debt obligations in the early twenty-first century helped ensure that a housing bubble would emerge. As financial
institutions raced to sell debt secured at the subprime level, investors failed to understand the extent of the risk to
the underlying mortgages (billions of dollars in mortgages had been issued to individuals who were not
creditworthy). The unrealistic expectation of future returns mimicked the real-estate failure of the Great
Depression. However, the rise in delinquent mortgage holders increased the volume of foreclosed properties on
the market, creating an alternative competitive market for any would-be homebuyers to choose from. This, in turn,
limited the need for new housing and led to a precipitous drop in home building.

 Solutions
Given the labor requirements, market barriers (or the lack thereof) to entry, and site specificity of construction
projects, there is little expectation that the construction industry could be regulated into stability through supply-
side intervention. The length of time for completion and contractors’ inability to divine demand explains the bumpy
road that the industry often takes.
A more appropriate focus, say economists, is the demand for construction services. In this case, there is room for
public and private decision-making as well as input from trade associations and labor organizations. The use of
interest rates as a policy tool has long been the purview of the Federal Reserve, and yet the financial meltdown of
2007–2008 has been attributed, in part, to easy credit terms. In retrospect, moderate home building growth would
have served the economy far better than the binge construction seen at the turn of the twenty-first century.
Greater government oversight in both the credit and securities markets with respect to housing and home
mortgages might have forestalled any economic disasters.
Of course, once the downturn snowballed into a deep recession, it was necessary to develop means to stimulate
economic growth. Construction was curtailed by a lack of funding and, later, by a lack of effective demand as
other sectors were crippled by inactivity. Building permits declined and housing starts fell off dramatically. In the
1930s, the federal government adopted a Keynesian philosophy, bolstering the construction industry through large
doses of infrastructure spending (e.g., the Works Progress Administration built bridges, schools, stadiums, etc.).
Many projects initiated by the Barack Obama administration have received funding from the federal stimulus
program, although not nearly on the same scale as President Franklin D. Roosevelt’s New Deal. In fact, Nobel
Prize–winning economist Joseph Stiglitz has called on the federal government to dramatically expand government
expenditures well beyond the $700 billion stimulus plan of 2009 to revive the American market system.
In contrast, the New York Times reported on January 23, 2009, that the centralized Chinese government
undertook construction projects valued at hundreds of billions of dollars. These were focused largely on improving
the nation’s transportation network, but included environmental projects such as water treatment plants. State-
controlled banks provided funding, whereas free-market banks in the United States have been cautious and slow
to fund projects as a result of market conditions and a backlog of nonperforming loans. Stiglitz noted that the
national Chinese stimulus package was valued at 14 percent of that country’s GDP. An equivalent American
response would have been in the trillions of dollars.
From a market perspective, construction booms are fraught with the inherent dangers of land, labor, and material
price inflation. Limited urban land sites classically raise the price of existing lots for building during any expansion.
The uptick translates into higher demand for skilled workers and building products until the costs of these inputs
invariably put pressure on profit margins. Typically, these market-based price limits set the stage for a barrier to
expansion as high-cost producers begin to fail and market demand for buildings and structures wanes. A lack of
construction investment can be overcome by an influx of government projects (as occurred in China and in the
United States during the Great Depression), although in noncrisis times, such projects can fuel sharply rising
prices of production.
Construction is an important sector of the economy that suffers from a historical pattern of instability while
generating billions of dollars in improvement to the nation’s capital stock. Yet it often gets short shrift from an
economic policy perspective. In part, this is because the cyclical movement stems from the derived demand for
construction services and its own production process. The wide range of market influences, which spur demand
for structures, homes, and facilities, say some economists, need to be considered in light of public policy. The
relationship to government fiscal policy (e.g., tax incentives), public works expenditure (infrastructure), and
legislative actions (e.g., immigration laws) already has been established but needs to be further explored, they
argue. Additional study is also warranted to understand the effects of technology and the de-skilling process as
the industry continues to reduce on-site labor hours through prefabrication and capital for labor substitutions.
Finally, the U.S. economy in general, many students of the industry agree, would be better served by a more
stable construction sector, given that it employs millions of workers and develops opportunities for so many

companies and contractors.
Gerald Finkel
 
See also:  Housing;  Housing Booms and Busts;  Mortgage, Commercial/Industrial;  Mortgage
Markets and Mortgage Rates. 
Further Reading
 Abramowitz, Moses.  Evidence of Long Swings in Aggregate Construction Since the Civil War.  New York:  National Bureau
of Economic Research,  1964. 
 Allen, Steven G.  “Why Construction Industry Productivity Is Declining.” Review of Economics and Statistics 67:4 (January
1986): 661–669. 
 Barth, James R.  The Rise and Fall of the U.S. Mortgage and Credit Markets: A Comprehensive Analysis of the Market
Meltdown.  Hoboken, NJ:  John Wiley and Sons,  2009. 
 Burns, Arthur F., and Wesley C. Mitchell.  Measuring Business Cycles.  New York:  National Bureau of Economic
Research,  1946. 
 Finkel, Gerald.  The Economics of the Construction Industry.  Armonk, NY:  M.E. Sharpe,  1997. 
 Goetzmann, William N., and Frank Newman. Securitization in the 1920’s.  Working Paper no. 15650, National Bureau for
Economic Research, 2010. 
 Krugman, Paul, and Robin Wells. Macroeconomics.  2nd ed. New York: Worth, 2009. 
 Mills, Daniel Q.  Industrial Relations and Manpower in Construction.  Cambridge, MA:  MIT Press,  1972. 
 Stiglitz, Joseph E.  Freefall: America, Free Markets, and the Sinking of the World Economy.  New York:  W.W. Norton, 
2010. 
 Thornton, Mark.  “The Economics of Housing Bubbles.” In America’s Housing Crisis: A Case of Government Failure, ed.
Benjamin Powell and Randall Holcombe. Edison, NJ: Transaction, 2009. 
Consumer and Investor Protection
 
One issue that often arises during or following an economic bust is whether consumers and investors were
properly protected from making bad economic decisions. The degree to which consumers and investors are
protected from the financial losses resulting from a crisis draws scrutiny in post-bust analysis.
Consumer protection refers to the system of safeguards that protect consumers from possible fraudulent and unfair
practices of businesses. It entails enabling the consumers to attain compensation for defective or harmful products
and deficiencies in the delivery of promised goods and services.
 Consumer
A consumer is one who buys products or services for personal consumption. In the present context, the term
“consumer” is used for anyone who makes a purchase. Globally, consumers number in the billions and collectively

have the power to influence the seller. However, at an individual level, there is seldom any interaction and
cooperation among consumers, which limits the scope for collective action. This greatly reduces the market power
of the single buyer and puts him or her at a disadvantage whenever seeking compensation or satisfaction in any
unfair deal or financial loss during or after purchase. Accordingly, sellers may indulge in any number of unfair
practices, including incomplete disclosure of information, use of harmful ingredients or components in products,
selling defective products, and so on.
 Caveat Emptor versus Caveat Venditor
For centuries, the Latin phrase caveat emptor, meaning “buyer beware,” was an accepted business norm, which
meant that the buyer was solely responsible if the merchandise sold were found to be defective or deviated from
the claims made by the seller. Hence, the buyer had to be careful in product selection and purchase. Since the
mid-1960s, however, there has been a growing recognition of consumer rights in the United States, initiated in
part by Ralph Nader, an attorney and pioneering consumer rights activist. Within a few years came the emergence
of a movement of citizen activists, public interest litigants, and consumer advocacy groups throughout the United
States. This crusade for consumer protection gained momentum and intensified into a full-fledged movement
known as consumerism, which spread across the rest of the world. Governments were persuaded and compelled
to pass supporting legislation that gave teeth to the movement. The main outcome of this was a shift in
responsibility for product liability from the buyer to the seller, the philosophy of caveat venditor, or “seller beware,”
has gained ground. Caveat venditor holds the seller responsible for any liability arising out of the purchase
transaction, unless the seller explicitly disclaims any responsibility and the buyer agrees to this prior to the
purchase.
 Rights of Consumers
Articulating the rights of the consumers is an important aspect of protection. In all the stages of purchase—from
the prepurchase decision-making stage to the postusage stage—the consumer is entitled to information and the
opportunity to act in case the promised value is not delivered. The rights of consumers in a number of countries
are as follows:
Right to Satisfaction of Basic Needs. The buyer has access to goods and services that are essential for the
satisfaction of basic needs like adequate food, clothing, shelter, health care, education, public utilities, water
and sanitation.
Right to Safety. The merchandise bought must be safe and healthy for the consumer. Products made with
ingredients or components that are hazardous or potentially harmful must be regulated. For example, children’s
toys painted with colors that have a high lead content and refrigerators that emit high levels of
chlorofluorocarbons are harmful to health and the environment respectively. Hence, right to safety involves the
development of standards by government and compliance with these standards by manufacturers and sellers
to ensure a minimum level of assurance to the consumers.
Right to Information. The buyer has to be informed about the quality, source of supply, dates of production and
product expiration, price, quantity, and so on, in order to protect himself or herself from being misled by
salespersons, distributors, advertisements, and labels.
Right to Choose. Besides the availability of product-related information that will help consumers make an
informed decision, the right to choose also precludes forced choice through tie-ups, that is, the purchase of a
product or service being linked to the compulsory purchase of another product or service.
Right to Seek Redress. If the consumer has a complaint about the product or service following purchase,
representing his or her grievance to an independent body should be possible. This right involves the creation
of regulatory or judicial bodies that have (1) the mandate to hear out the consumer complaints; and (2) the

power to ensure corrective action or compensation from the supplier.
Right to Consumer Education. Consumers can demand information or seek compensation only when they are
aware of their rights. Hence, educating the consumers is an important cornerstone of consumer protection.
Right to a Healthy Environment and Sustainable Consumption. Consumers have a right to expect products and
services that do not endanger the environment or cause erosion of natural resources. Sellers have to ensure
that the products do not cause harm to the environment or rapid resource depletion that can cause ecological
imbalances.
Right to Associate. Consumers must have freedom to associate and form consumer groups and to represent
their views in decision-making processes at the local or federal levels affecting them.
 Consumer Protection Laws and Organizations
Laws for consumer protection are essential for safeguarding consumer rights. In the absence of appropriate laws,
delivering justice to consumers would be left to the discretion of the sellers since corrective action cannot be
enforced. Hence, the first step toward ensuring consumer rights is to develop the necessary legal provisions.
Legislation is usually in the areas of consumer complaint redress, fixing liability of sellers, assuring truth in
advertisements and labels, establishing information disclosure requirements, setting resale price controls, deterring
the creation of monopoly power in an industry, prohibiting unfair and restrictive trade practices, and so forth. To
enforce these laws, appropriate regulatory venues are created in which consumer interests can be fairly
represented.
A variety of organizations are involved in promoting consumer protection. They are in the form of citizen groups or
consumer groups that are nonprofit organizations and often the first places consumers can turn for help following a
purchase of a good or service. These groups gather information and guide consumers in representing their cases
to judicial organizations. They also undertake campaigns for educating consumers about harmful products and
consumption habits. In order to mobilize support for the consumer movement, they seek representation in
decision-making bodies at national and international levels. For instance, Consumers International (CI), founded in
1960, is a nonprofit federation of 220 consumer groups from 115 countries. The organization’s expressed mandate
is “to secure a fair, safe and sustainable future for consumers in a global marketplace increasingly dominated by
international corporations.”
Once a customer case is represented, the actual process of hearing the case, assessment, passing judgment, and
enforcing corrective action are undertaken by judicial organizations. These institutions differ from country to
country. In the United States, the Federal Trade Commission (FTC) is the government agency responsible for
consumer protection through the establishment and enforcement of commercial trade regulations. The FTC
ensures effective law enforcement by sharing its expertise with federal and state legislatures and national and
international government agencies. It also develops policy and research tools and runs educational campaigns.
The Bureau of Consumer Protection, an arm of the FTC, fields complaints about consumer fraud and identity theft
and makes them available to law enforcement agencies across the country. It also conducts investigations and
sues those sellers who violate the law. The seven areas of its work include advertising practices, consumer and
business education plans, enforcement, financial practices, marketing practices, planning and information, and
privacy and identity protection.
The consumerism movement has assumed worldwide proportions, especially with the rise of communications
technology and the Internet. Web-based campaigns and the forwarding of e-mails across continents and blogs are
now a part of global coordination and concerted consumer protection strategies. For instance, World Consumer
Rights Day (WCRD) 2009 was observed on March 15 with the members of Consumers International organizing
supermarket sweeps, lunchbox challenges, marches, press conferences, and guest blogs. The Junk Food
Generation Campaign, a worldwide campaign which calls for an end to the marketing of unhealthy food to
children, was also intensified to mark WCRD. However, consumerism is also associated with the idea that global

economic benefits arise though greater and well-informed consumption. As a consumer rights movement,
consumerism has become a global phenomenon for consumer protection.
 Investor Protection
An investor is one who commits resources to business venues that generate returns in the future. These
commitments can be bank deposits, fixed assets, or financial assets like stocks and bonds or bullion, real estate,
and precious items. From a professional finance standpoint, capital investment refers to the buying of a financial
product or any item of value with an anticipation of earning positive future returns. Unlike a speculator who
accepts a high level of risk in the hope of gaining higher-than-average returns, an investor tries to minimize risk
and maximize returns. Hence the investor tries to balance the twin concerns of risk and return.
Safeguarding the interest of investors and their legal rights is known as investor protection. It includes guiding and
educating investors regarding their rights and legal enforcements in the case of violation of these rights. Investors’
backgrounds usually range from the technically sophisticated to those who are less so and who have resources
but little knowledge in these matters. The need for protection arises due to the information and knowledge gap
between the investors and the group that manages the investors’ funds. If the investment is made in financial
products like bonds or mutual funds, investors deal with providers of financial services; if shares are purchased,
the investors’ entrusted resources are used by the managers of the company. Thus the investors deal with
professionals who possess greater knowledge and experience since individual investors often possess neither the
information nor the capability to evaluate the information. Not all parties they deal with are honest and
straightforward. At times, the self-interest of the vendors or managers may be served better by not communicating
all the information to the investors, or else they may find it difficult to communicate technical information in an
understandable manner to the less knowledgeable investors.
The alternative for investors is either to seek the services of professional investment advisers or to entrust funds to
professional fund managers to make appropriate investment decisions on their behalf. However, even these
solutions are not foolproof. The investment advisers possess better technical and market knowledge but still
cannot match the product knowledge of vendors, while the fund managers may be motivated by their self-interest
rather than maximization of the clients’ interest. Thus the need for investor protection arises due to information
gaps, potential conflicts of interest, and disparate investor capabilities.
 Investor Rights
The explicit rights of the investor include the following:
Right to Complete Information (about):
The firm and the work history and background of the person handling the investor account
Commissions, sales charges, maintenance or service charges, transaction or redemption fees, and penalties
he terms and conditions of transactions the investor undertakes
Any major changes to investments
Right to Receive Advice:
Advice consistent with the investor’s financial needs and investment objectives
Presale advice about the risks, obligations, and costs of any investment
Right to Documents and Statements:

A copy of all completed account forms and agreements
Account statements that are accurate and understandable, with regular updates
Statement for the period during which transactions are made
Right of Access to Funds:
Timely access to funds
Information about any restrictions or limitations on funds access
Right to Be Heard:
Right to patient hearing and clarification of investor queries
Prompt attention to and fair consideration of investor concerns and account problems
In addition, as partial owners of the company, shareholders have all or some of the following rights depending on
the nature of shares held:
Voting rights on issues that affect the organization
Right to the assets of the corporation
Right to transfer the stock
Right to receive dividends as declared by the board of directors of the company
Right to inspect the books and records of the company
Right to sue the company in case of wrongful deeds committed by the directors and officers of the company
Right to share the proceeds recovered in case of liquidation of the company’s assets
Free-market advocates who are averse to government regulation advocate competition as a means of ensuring
investor rights. Their belief is that the government’s role is to foster a competitive environment in which companies
vie with each other to maximize the investors’ wealth. In other words, free-market forces are expected to ensure
protection of investor concerns. In their view, companies that do not divulge all of the relevant information in a
clear and understandable way will be driven out of business because investors will not invest in them. However,
there have been instances of market failures like the financial crisis in the 1930s and the global financial crisis of
2008–2009 that have demonstrated that markets do not always offer protection and hence there is need for
regulation. The extent of the regulation needed is still a highly debated issue and is decided by the governments
of respective countries based on their past experience, public demands, and political requirements.
 Laws and Institutions
Effective investor protection requires a strong legal system and an effective enforcement regime. Clear ownership
rights, contract laws, commercial and bankruptcy codes, and strict enforcement lend strong credibility to a
country’s securities market. Most countries have investor protection regimes comprised of the necessary
legislation as well as the institutions that monitor relevant business practices and enforce the legislation. Countries
like the United States adopt rules pertaining to registration and authorization of financial products and schemes,

which is known as a top-down approach. They specify custodial, redemption, liquidity, disclosure, and reporting
requirements. They also impose restrictions on leveraging, short-selling, management fee arrangements, and
portfolio diversification. The purpose of this kind of regulation is to ensure information availability to investors as
well as limiting their exposure to financial loss.
On the other hand, countries that adopt a bottom-up approach ensure investor protection through disclosures and
self-certification by the companies themselves. In these countries, investors are expected to make their own risk
assessment and judgment about the suitability of the products to their concerns. Under such regimes, like in
Australia, the onus placed on the investors is clearly greater than in countries such as the United States.
The following are some examples of the regulatory provisions that nations adopt in order to protect investors:
Entry norms like capital requirements, promoter’s contribution, and lock-in period
Liquidity norms, for instance, the percentage of the net tangible assets to be held in monetary assets
Eligibility of the companies for public issue or rights issue, denomination of shares for public/rights issue,
stipulations about the manner of specifying the information in the offer document
Disclosure norms, including guidelines for the issue of advertisements and the prospectus, the form or forms in
which the required information should be furnished, the items or details to be shown in the balance sheet and
the earnings statement, etc.
Strict separation of clients’ money, maintenance of accounts, audit, and insurance requirements
The development of such rules and their enforcement is done through institutions specifically created for this
purpose. The Bankruptcy Act of 1938 and Securities Investor Protection Act of 1970 are the major laws aimed at
protecting the investors in the United States. More recently, the Sarbanes-Oxley Act of 2002, also known as the
Public Company Accounting Reform and Investor Protection Act of 2002, was passed in the United States in the
wake of a number of major scandals that eroded billions of dollars of investors’ wealth and public confidence in the
securities markets due to a collapse in share prices of companies like Enron and WorldCom. The Securities
Investor Protection Corporation (SIPC) protects securities investors from financial harm. SIPC organizes the
distribution of the available customer cash and securities to investors. If such cash is not available, SIPC provides
insurance coverage up to $500,000 of the customer’s net equity balance, including up to $100,000 in cash. Note
that the SIPC does not insure against losses in an investment account. It insures against losses involving crimes
that a brokerage firm commits with its clients’ funds, such as embezzlement.
The U.S. subprime mortgage crisis of 2007–2008 and the victory of the more regulatory-minded Democrats in the
congressional and presidential races of 2008 has led to calls for a new government agency—the proposed
Consumer Financial Protection Agency (CFPA), which would oversee a variety of financial products, including
home mortgages, credit cards, and other consumer lending. The subprime mortgage crisis revealed a variety of
problems in the way mortgages were originated and marketed, including overcomplicated contracts and
misleading sales practices, as a well as a general lack of financial understanding among large portions of the
public, particularly those at the lower end of the income spectrum who took on the subprime mortgages.
In addition, consumer groups long complained that banks were too aggressive in marketing credit cards, offering
low teaser rates to get people in debt and then arbitrarily raising interest rates to exorbitant levels. While some of
these activities are regulated by various federal and state agencies, advocates of the CFPA argued that there
needed to be a single agency, so that financial instruments were not only marketed fairly but remain uniform
throughout the country. In addition, they argued that many of the regulatory agencies were created to help the
markets run more smoothly, not to protect consumers, and that they remained too closely tied to the interests of
the financial industry. Detractors of the CFPA idea conceded that some new consumer financial regulation was
necessary but said that creating another government bureaucracy was not the answer and that more regulation

would prevent the development of new and useful financial instruments that benefit consumers.
In July 2011, the Consumer Financial Protection Bureau (CFPB) went into operation, the enabling legislation
having been signed into law by President Barack Obama the year before. While the new entity was given broad
powers to set and enforce rules for consumer financial instruments and the various financial institutions that issue
them, bureaucratic limitations were established to keep that power in check. Bowing to conservative pressure in
Congress, the Obama Administration ditched its plan for an independent agency, placing the new bureau under
the aegist of the Federal Reserve Board, though with a significant amount of independence. More contentious
were the fights over who would be appointed to head the new bureau. Critics of the banking industry wanted
Harvard Law Professor Elizabeth Warren, a bankruptcy expert who had been a strong advocate of tightening
regulation on consumer financial products. But Republicans in Congress threatened to stop her nomination,
pushing for a board to head the bureau, under the assumption that having several directors, rather than one,
would temper its advocacy of new regulation. In the end, the CFPB was headed by a single director, but it was not
Warren, who withdrew her nomination. Instead, Obama appointed Richard Cordray, a former Ohio attorney
general who was more widely acceptable to the financial industry and its allies in Congress.
 Benefits and Costs
Regulation enhances investor security and contributes to the development of financial markets that are considered
imperative for strong economic growth. Inadequate investment protection leads to financing constraints and hence,
increases the cost of capital. Research has shown weak investor protection to be the cause of suboptimal
investment decisions like overinvestment in low-growth industries and underinvestment in high-growth industries.
Low dividend payout, excessive cash holding, and more aggressive earnings management were also found in
countries with weak investor protection regimes. The mandatory information disclosure required by strong investor
protection regimes was found to improve information transfer and financial market liquidity. In the arena of
international business, firms from countries with strong legal systems and shareholder rights were found to make
more profitable overseas investment decisions while firms from weak investor protection regimes often ended up
as targets for acquisition.
A number of administrative and bureaucratic costs are associated with regulation and compliance. Regulation also
leads to market rigidities (inability of the market to react quickly to shifts in supply and demand), weakening of
economic incentives, and increased costs of lobbying by business to influence the regulatory agenda. Obtaining an
optimum balance between protection and cost of regulation is difficult. Despite these dilemmas, most countries opt
for regulation because the social costs associated with real-world market imperfections are greater than the costs
of regulatory intervention. The big issue is the extent of regulation. Historical evidence reveals a retreat of
regulation during financial booms and a surge in regulation in the eras of financial busts. Finally, in an era of
globalized financial markets, more attempts for greater global coordination of regulations are imperative. Without
global coordination, market participants wishing to engage in more risky behavior than domestic regulators allow
can merely take the activity offshore.
A.D. Madhavi and James Ciment
 
See also:  Corporate Corruption;  Securities and Exchange Commission. 
Further Reading
Brian, Harvey W., and Deborah L. Parry. The Law of Consumer Protection and Fair Trading. London: Butterworths, 2000. 
Consumers International:  www.consumersinternational.org
Franks, Julian, and Colin Mayer. Risk, Regulation and Investor Protection: The Case of Investment Management. Oxford,
UK: Clarendon Press, 1990. 

Meier, Kenneth J., E. Thomas Garman, and Lael R. Keiser. Regulation and Consumer Protection: Politics, Bureaucracy and
Economics.  3rd ed. Houston, TX: Dame Publications, 1998. 
Page, Alan C., and R.B. Ferguson. Investor Protection. London: Orion, 1992. 
Reinis, August, ed. Standards of Investment Protection. New York: Oxford University Press, 2008. 
United Nations. Guidelines for Consumer Protection. New York: United Nations, 2003. 
Consumption
 
“Consumption” is a macroeconomic term for the total amount of spending on consumer goods by individuals or
households—or a defined group of individuals and households, such as a nation—over a given period of time.
Although consumption literally refers to the goods and services consumed during the period in question,
economists also include goods that are bought during the period but used or enjoyed over a longer amount of
time, such as cars, appliances, home linens, or shoes. Consumption falls into three basic categories: durable
goods, such as cars and appliances; nondurable goods, such as food, medicine, clothing (though this last item
may be used over a long period of time, economists classify it as a nondurable); and services, such as medical
care, home cleaning, and school tuition. Consumption equals individual or household income, minus savings. The
use of savings is either to purchase financial assets or to purchase newly constructed housing, a component of
investment spending. (Only households engage in consumption spending on consumer goods and services;
business firms and governments do not.)
 Patterns and Trends
As in most developed economies, consumption accounts for the lion’s share of total spending in the United States
—about 70 percent in 2009. Consumption, of course, varies widely among individuals and groups, especially when
it comes to income. For example, wealthier individuals and households with higher incomes tend to spend a
higher percentage on health care, while poorer individuals and households with lower incomes spend a higher
percentage on food. There are several explanations for the difference in consumption patterns—older people, who
require more health care, tend to be wealthier and to have higher incomes; for those with tighter budgets,
nonessential medical services may not be a high priority. And while wealthier people buy higher-quality food and
eat out more often, there is a limit to how much food someone can eat. More importantly, wealthier individuals
with higher incomes tend to put more of their income into savings. Across all demographic and social categories,
aggregate consumption in the United States in recent years breaks down approximately as follows: 12 percent for
durables, 29 percent for nondurables, and 59 percent for services.
Not only do consumption patterns differ among classes, but they also vary over time—in the lives of individuals,
over the course of business cycles, and through historical eras. Thus, younger Americans tend to spend more of
their income on entertainment, shifting the emphasis to health care when they get older. Paradoxically, people
tend to save more and consume less (even as a percentage of income) during downturns in the economic cycle.
While that is not the case among people who have been laid off, who cannot afford to save as much and who
spend all of their unemployment benefits to buy the things they need for themselves and their families, there are
still far more employed people than unemployed people in the worst of recessions. And those who are still
employed tend to save more for a variety of reasons. They may fear losing their jobs or seeing their incomes go
down and decide to put money away for a “rainy day.” Economic downturns also tend to bring declining equity
value in securities and homes; while the losses may only be on paper, they tend to make people feel less secure

about their current and future (retirement) economic situation, compelling them to save more.
The housing bubble and crash of the mid-to-late 2000s illustrates how an economic cycle can play a crucial role in
consumption. During the run-up in housing prices in the early and middle part of the decade, Americans found
themselves with greater amounts of equity in their homes. That led to three developments. First, with more equity
in their homes, many individuals worried less about saving for retirement and increased consumption. Second,
with credit standards loosening, many people were able to take out home equity loans and use the money to buy
all sorts of consumer goods and services. And third, with rising equity, people were able to refinance their homes
at lower interest rates, which lowered their monthly mortgage payments and freed up more money for
consumption. Between 2002 and 2006—at the peak of the housing boom—the aggregate personal savings rate in
the United States fell from more than 2 percent of after-tax income to nearly zero. By 2009, it had climbed back
up to more than 5 percent, a level not seen since the mid-1990s.
Even at 5 percent, however, the aggregate savings rate was relatively low compared to that of much of the post–
World War II era. There were two basic reasons for this. First, median household income has stagnated in real,
inflation-adjusted terms; according to some estimates, it has even fallen since the early 1970s, when the
aggregate savings rate was above 10 percent. Moreover, to sustain even the stagnant or slightly falling income
level, households have had to add an additional breadwinner—usually the wife. With both parents working,
households have to spend more on day care, food (going out or buying prepared food), cleaning expenses, and
other goods and services that had been provided by the stay-at-home spouse in the traditional, one-breadwinner
family. Of course, in some two-breadwinner households, the savings rate may be higher since there are now two
incomes.
Such consumption may be seen as a sheer necessity. But there has also been a major attitudinal change toward
discretionary consumption since World War II, a change fostered by the advertising, consumer goods, and
financial industries. Advertising and marketing have increased the appetite for consumption, while the consumer
goods industry has both met demand and produced new and improved products. Meanwhile, the financial industry
has developed and aggressively marketed a host of new products—credit cards, debit cards, home-equity loans,
and other innovations—that make it easier to borrow and, at least temporarily, hide the costs of consumption by
delaying payment or spreading it over longer periods of time. Between 1980 and 2008, so-called revolving debt
(primarily credit-card debt) increased from $139 billion to $972 billion (in 2008 dollars), a jump of 700 percent.
Consumption in the United States has also been driven by secular trends—that is, long-term historical patterns
unaffected by the business cycle. For example, improved agricultural methods and crops have brought down the
cost of food, reducing that important component of consumer spending. Where Americans spent more than 40
percent of their income on food and drink in the early part of the twentieth century, a hundred years later they
were spending less than 20 percent on such items. In addition, according to some economists, the advent of
Social Security in the 1930s and Medicare in the 1960 lowered savings rates and increased consumption as
people worried less about how they would pay for medical care in their senior years. At the same time,
relentlessly rising medical costs have driven up that component of national consumption. And, of course,
demographic trends also play a vital role. Living on fixed income and savings, seniors tend to consume relatively
more than people at the peak of their earning lives. Thus, as the median age rises in a country, savings rates
decline.
 Differences Among Nations
As a function of both cyclical economic trends and secular factors, savings rates may also differ dramatically from
country to country. With their high consumption rates, Americans are among the least active savers of any
industrialized country, a somewhat paradoxical trend given the relative lack of a social safety net. In other words,
it would seem sensible for American consumers—who receive less in unemployment, retirement, health care, and
educational benefits from the government than their counterparts in Europe, say—to save more against such
contingencies than people who live in countries where the social welfare system is far more generous. Indeed, in

China, where the social safety net is even weaker than in the United States, the national savings rate hovers
above 30 percent—the highest in the world for a major country. Even in the eurozone, the savings rate is about
10 percent, double that of the United States.
Economists offer a number of theories for the differences. On the savings side, they say that Americans are less
likely to put money away because, aside from the occasional recession, Americans have only known prosperity
since World War II. On the consumption side, factors include the effectiveness of advertising, the wealth of new
products constantly being offered, and the innovative new financial products that have made it easier to consume
thoughtlessly. In broad terms, the theorists tend to break down into two camps: liberal voices, which insist the high
consumption-to-saving ratio is caused by stagnating incomes, compounded by soaring medical costs and, until the
late 2000s, rising housing costs; and conservative voices, which contend American consumers have lost their
sense of discipline, refusing to defer immediate gratification for long-term security. The easy availability of credit
also gives some households the ability to increase their consumption beyond what they otherwise would be able to
spend without credit.
Whatever the reasons, consumption patterns have an enormous impact on the national economy, given that they
account for about two-thirds of all spending. High rates of consumption, combined with a decline in manufacturing
capacity, can lead to rising trade deficits in the durable and nondurable goods categories (services are harder to
provide from overseas), while low savings rates can stifle investment as banks and other financial institutions lack
the resources to lend money to businesses for capital improvements.
James Ciment
 
See also:  Confidence, Consumer and Business;  Effective Demand;  Hoarding;  Inflation; 
Retail and Wholesale Trade;  Savings and Investment. 
Further Reading
Attanasio, Orazio P., Laura Blow, Robert Hamilton, and Andrew Leicester.  “Booms and Busts: Consumption, House Prices
and Expectations.” Economica  76:301 (2009): 20–50. 
Calder, Lendol. Financing the American Dream: A Cultural History of Consumer Debt. Princeton, NJ: Princeton University
Press, 1999. 
Censolo, Roberto, and Caterina Colombo.  “Public Consumption Composition in a Growing Economy.” Journal of
Macroeconomics,  30:4 (2008):  1479–1495. 
Strasser, Susan, Charles McGovern, and Matthias Judt. Getting and Spending: European and American Consumer
Societies in the Twentieth Century. New York: Cambridge University Press, 1998. 
Corporate Corruption
 
The fraud perpetrated by financier Bernard Madoff, whose $50 billion to $60 billion Ponzi scheme was uncovered
amid the subprime mortgage crisis of 2008, was but the latest in a long history of scandals associated with
corporate corruption in America. Such scandals have occurred frequently, often resulting in devastation to
individual investors and to the overall economy. Ironically, corporate corruption has helped shape the financial
system in many ways, making it a dominant force in world finance, as institutional and regulatory reforms initiated

in response to corruption have created a more efficient and transparent marketplace that encourages investment.
For example, the stock market manipulations of Assistant Treasury Secretary William Duer in 1792 led to reforms
that laid the groundwork for the establishment of the New York Stock Exchange. That institution would play a key
role in the capital-raising efforts that built the nation’s railroads, funded insurance companies and banks, and
connected isolated local markets to the national economy.
Another example of reform following fraud occurred after the Panic of 1907, a financial crisis touched off by
corruption at the Knickerbocker Trust Company in New York. That panic shook the nation, but also led to the
creation of the Federal Reserve System, which did much to save the economy during the subprime mortgage
crisis that peaked in 2008. Similarly, congressional investigations following the stock market crash of 1929
uncovered widespread fraud and corruption. These revelations led to the passage of federal securities laws and,
in 1934, to the creation of the Securities and Exchange Commission, which reshaped Wall Street and corporate
governance through mandatory full-disclosure requirements.
 Robber Barons
Corporate corruption became a national concern during the era of the “robber barons” following the American Civil
War, when businessmen such as Jay Gould and Daniel Drew plundered the stock markets and looted the
corporations they controlled. In 1872, a scandal at Crédit Mobilier of America, a service company for the Union
Pacific Railroad, set the bar for corporate scandals in future centuries. Crédit Mobilier made profits of some $44
million from Union Pacific construction projects, which were funded in part by the federal government. The
company’s officers gave a number of its shares to Congressman Oakes Ames of Massachusetts in order to buy
influence. Cash was given to other government officials to persuade them to falsely certify completion of
construction and to allow the company to receive payments from Congress. Representative Ames was expelled
from Congress when the scandal became public, as was Representative James Brooks of New York, who also
had accepted bribes. Several other members of Congress were censured, and a motion was entered to impeach
Vice President Schuyler Colfax. Numerous other politicians were found to have been involved in this affair,
including James A. Garfield, the twentieth president of the United States. The secretaries of navy and war were
among those who received bribes.
Another model of corruption before the turn of the twentieth century was the American Ice Company (AIC), which
had a monopoly on all ice business along the Atlantic coast. Ice was then a vital consumer product, and AIC was
the fifth-largest company in the United States. AIC officials bribed New York City dock commissioners to turn back
competing supplies of ice. The ice fields of competitors were smashed by steamships hired by AIC, and the
company lowered its prices until its remaining competitors were destroyed. Once its monopoly was in place, AIC
raised its prices by well over 100 percent, causing much hardship to consumers and stirring controversy in the
press. The scandal blossomed when it was discovered in 1900 that the mayor of New York City, Robert Van
Wyck, and his brother, a onetime Democratic Party gubernatorial nominee, had received AIC stock valued at
almost $900,000. Several other prominent New York politicians also were found to have received large amounts of
AIC stock for their support of the company.
 Twentieth-Century Scandals
The Teapot Dome Scandal during the administration of President Warren G. Harding was another epic case of
corporate corruption. In 1921, it was discovered that Albert Fall, then secretary of the interior and a former senator,
had secretly leased the U.S. Navy’s oil reserves at Teapot Dome in Wyoming to oil tycoons Harry F. Sinclair of
Mammoth Oil and Edward Doheny of Pan American Petroleum. Fall, who was paid $500,000 for access to the
government oil leases, was indicted and convicted for his misconduct. However, he was sentenced to only a year
in prison and fined $100,000. Sinclair and Doheny were acquitted of bribery charges, though Sinclair was found
guilty of jury tampering. The scandal tainted the administration of President Harding, leaving it with a reputation for
corruption.

Another massive scandal involved the failure of the Kreuger & Toll Company in the 1930s. Kreuger & Toll
controlled more than 90 percent of the world’s production of matches. Its head, Ivar Kreuger, known as the “Match
King,” used counterfeit bonds to support the company’s loans—but when its finances collapsed, the company
failed. Although the company was based in Sweden, Kreuger & Toll securities were held widely in the United
States. Claims against the bankrupt firm exceeded a then-astonishing $1 billion. Another corporate scandal in the
1930s involved the Insull Utility Holding Company in Illinois. This company generated some 10 percent of the
country’s electric power through a pyramid of 100 holding companies that controlled more than 250 operating
companies. Investors lost hundreds of millions of dollars when this empire collapsed. The head of the company,
Samuel Insull, was indicted for misleading investors and manipulating the company’s stock price. Insull fled the
country, but soon was captured and returned for trial in the United States, where he eventually was acquitted of
all charges.
An underworld group of individuals—including Lowell Birrell, Ben Jack Cage, Earl Belle, Alexander Guterma,
Serge Rubinstein, and Virgil D. Dardi—weaved a web of corporate corruption in the 1950s. These men were said
to have looted and destroyed seventy-five public companies and caused investor losses of $100 million. Many of
them fled to Brazil, but that exodus slowed when an extradition treaty was signed with that country in 1961. Eddie
Gilbert was at the center of another highly publicized corporate scandal in the 1950s. Gilbert acquired control of
the E.L. Bruce Company, a large manufacturer of hardwood flooring. He amassed a fortune estimated at $25
million, which allowed him to live an opulent lifestyle. Gilbert used $2 million of E.L. Bruce funds to meet margin
calls on stock that he owned. When the loss was discovered, Gilbert fled to Brazil, making front-page news across
the nation. He eventually tired of Brazil, however, and returned to the United States, where he was prosecuted
and jailed for two years. Gilbert later became a successful businessman in New Mexico.
The “go-go” years of the 1960s and the following decade saw corporate fraud ranging from the IOS (Investors
Overseas Services, Ltd.) mutual fund collapse and its looting by Robert Vesco, who fled the country when the
fraud was discovered, to the highly publicized collapses of the National Student Marketing Corporation, the Four
Seasons Nursing Centers, and the pyramid sales schemes of Glenn W. Turner of Koscot. In the 1970s,
“questionable” payments made by the Lockheed Corporation and other large companies to foreign government
officials in order to obtain business resulted in the collapse of several governments. That scandal resulted in the
passage of the Foreign Corrupt Practices Act of 1977, which prohibited such payments.
The 1980s revealed a mass of corporate corruption that threatened the financial system. The events of this
decade included the collapse of Penn Square Bank, a strip mall bank that contributed to the failure of the giant
Continental Illinois National Bank. The Bank of Credit and Commerce International S.A. proved to be a global
criminal enterprise that was involved in money laundering and drug trafficking. The savings and loan (S&L)
debacle in the 1980s cost taxpayers more than $125 billion. The abuses committed by the managers of S&Ls
were legendary—they included hiring prostitutes to entertain customers, leasing or buying Learjets for personal
use, and purchasing extravagant homes and expensive art, all paid for with S&L money. By 1992, more than
1,000 individuals had been charged with crimes in connection with S&L activities—most of whom were convicted.
Two of the individuals charged with crimes were Don Dixon and “Fast Eddie” McBirney. Also implicated was
Charles Keating, Jr., who controlled the Lincoln Savings and Loan Association in Irvine, California. He had paid
himself and his family members $34 million for their services before the failure of that institution cost taxpayers
more than $3 billion. Keating spent five years in jail before a federal court overturned his conviction.
The insider trading scandals of the 1980s involving Ivan Boesky and others, and the prosecution of Michael
Milken, the “Junk Bond King,” were all headline news and the subject of several books. Milken, for a time,
became the high priest of corporate finance through his innovative use of high-yield “junk” bonds to fund corporate
mergers. His annual “Predators’ Ball,” a conference on junk bonds held in Beverly Hills, was attended by
hundreds of institutional investors and individuals involved in mergers and acquisitions. Milken was well
compensated for his efforts to expand the use of junk bonds, receiving more than $120 million in salary and bonus
in 1984 and $550 million in 1987. Milken was indicted in March 1989 on ninety-eight felony counts of securities
violations, mail and wire fraud, and racketeering. The charges brought against Milken involved parking stock

(holding shares controlled by another party to conceal ownership of shares) and other forms of manipulation.
Milken pleaded guilty to six felony counts, agreed to pay a fine of $600 million, and was sentenced to ten years in
prison, which later was reduced to three years.
Modern corporate corruption is marked by the failure of the Enron Corporation, the nation’s seventh-largest
company at the time it declared bankruptcy in 2001. Enron failed amid a sea of corporate corruption that involved
accounting manipulations designed to boost its stock price so that company executives could reap millions of
dollars in compensation from stock options. More corporate corruption was uncovered following the failure of
WorldCom, a telecommunications company whose executives had engineered massive manipulations of its
accounts to boost the company’s share price. At the time, it was the largest bankruptcy in history. WorldCom’s
chief executive officer (CEO), Bernard Ebbers, was sentenced to twenty-five years in prison.
The accounting scandal at WorldCom was accompanied by others, including Tyco International Ltd., whose CEO,
Dennis Kozlowski, was sentenced to more than eight years in prison, and Adelphia Communications Corporation,
whose eighty-year old CEO was sentenced to fifteen years in prison. Other massive accounting scandals arose
during this era at Nortel Network Corporation, Lucent Technologies Inc., Qwest Communications International Inc.,
Global Crossing Ltd., AOL-Time Warner, Cendant Corporation, Hollinger International Inc., and HealthSouth
Corporation. The Sarbanes-Oxley Act of 2002 was passed in response to those scandals. It sought to strengthen
accounting controls at public companies.
New York State Attorney General Eliot Spitzer began a crusade against corporate corruption during this period.
He exposed, among other things, conflicts of interest on Wall Street by financial analysts, who privately were
disparaging the same stocks they were touting to public investors. Spitzer arranged a $1.4 billion settlement
between state and federal regulators and several investment banking firms involved in the scandal. Spitzer also
attacked the fee arrangements of Marsh & McLennan and AIG (American International Group), two large
insurance firms, ousting their chief executive officers and imposing large fines in the process.
Spitzer exposed “late trading” and “market timing” activities by hedge funds in the shares of mutual funds. Those
arrangements allowed hedge funds to profit at the expense of individual mutual fund investors. The mutual fund
investigations led to large settlements with several hedge funds. Spitzer’s aggressive prosecutorial tactics made
him a controversial figure, but he leveraged the publicity associated with his prosecutions of corporate corruption
to catapult himself into the New York governor’s office in 2007. However, he was forced to resign from office the
following year when he was caught up in a scandal of his own involving money laundering activities that were
used to cover up his involvement as a client of a prostitution ring.
The subprime mortgage crisis exposed more corporate corruption, including the largest fraud in history committed
by Bernard Madoff, who was arrested on December 11, 2008, after confessing that he had been running a giant
Ponzi scheme. Madoff was a well-known figure in the securities business. He was a former chair of NASDAQ and
had served on the Board of Governors of the National Association of Securities Dealers, the industry’s self-
regulatory body. Actual out-of-pocket losses to investors were estimated at $19.4 billion. Madoff was sentenced to
150 years in prison. Among Madoff’s victims were a number of Jewish charities, including one sponsored by
Nobel laureate Elie Wiesel. Tufts University lost $20 million, Yeshiva University lost more than $100 million, and
Bard College lost $3 million.
Marc Schrenker, an investment adviser in Indiana accused of defrauding his customers, fled in his airplane,
jumping out of the plane over Alabama. The crashed, but empty, plane was discovered in Florida, 200 miles (320
kilometers) away. Schrenker was found hiding in a campground near Quincy, Florida, where he slit his wrist just
before being captured. Schrenker survived and was jailed. Another massive fraud was revealed on February 17,
2009, after the Securities and Exchange Commission charged Sir R. Allen Stanford with defrauding investors of
some $8 billion. He had promised high returns from certificates of deposit, but actually had invested customer
funds in illiquid assets. Stanford was a high-profile financier who was an international cricket sponsor. He
operated out of the Caribbean island of Antigua through his Stanford International Bank.

The subprime mortgage crisis also gave rise to concerns that executives at large financial service firms had been
corrupted by compensation schemes that induced them to take excessive risks, which crippled or destroyed their
firms when the subprime crisis began. Several financial services firms failed, or had to be bailed out by the federal
government, during the subprime crisis. They included Merrill Lynch, Bear Stearns, Morgan Stanley, Lehman
Brothers, Citigroup, Bank of America, Wachovia, Washington Mutual, Countrywide Financial, IndyMac Bancorp,
and AIG, as well as the government-sponsored enterprises Fannie Mae and Freddie Mac. Those failures
prompted the federal government to place restraints on executive pay at firms bailed out by the government and to
allow shareholder votes on compensation arrangements.
 Conclusion
Corporate corruption has had a significant impact on the American economy, often precipitating market panics.
The Panic of 1884, for example, was touched off by the discovery of massive fraud at Grant & Ward, a brokerage
firm in which former Union general and U.S. president Ulysses S. Grant was a partner. The Panic of 1907 was set
off by the failure of the Knickerbocker Trust Company. The stock market crash of 1929 and the unveiling of much
corruption on Wall Street preceded the Great Depression. The subprime mortgage crisis of 2007–2009 was a
continuing scandal over corrupt lending practices and excessive executive compensation. The unraveling of
Bernard Madoff’s massive fraud followed those scandals.
Corporate corruption frequently leads to new regulations. Corruption on Wall Street resulted in the enactment of
the Securities Act of 1933 and the Securities Exchange Act of 1934, which created the Securities and Exchange
Commission. The Insull scandal led to the enactment of the Public Utility Holding Company Act of 1935, which
subsequently was repealed. The savings and loan crisis of the 1980s resulted in much corrective legislation. The
corruption at Enron and WorldCom led to the enactment of the Sarbanes-Oxley Act in 2002. In the aftermath of
the subprime mortgage crisis, Congress considered a number of measures to prevent corruption and reckless risk
taking by financial services firms.
This cycle of corruption imposes heavy costs on society in the form of onerous regulatory requirements that the
innocent must bear. The Sarbanes-Oxley Act, for example, imposed significant accounting costs on public
companies. This pattern of corruption, market panics, and corrective legislation also has resulted in a multilayered
regulatory system in which numerous state and federal agencies seek to prevent and deter corporate corruption.
At the federal level, regulators include the Federal Reserve Board, the Office of the Comptroller of the Currency in
the Treasury Department, the Federal Deposit Insurance Corporation, the Office of Thrift Supervision, and
FinCEN, an anti–money laundering group located in the Treasury Department. In addition to those bodies are the
Securities and Exchange Commission, the Commodity Futures Trading Commission, the Federal Trade
Commission, the Occupational Safety and Health Administration (for Sarbanes-Oxley whistle-blower claims), and
self-regulatory bodies such as the Financial Industry Regulatory Authority, in the securities industry, and the
National Futures Association in the futures industry. The Justice Department is criminalizing every form of
corporate behavior.
Corporations are policed at the state level by fifty state insurance commissioners, acting collectively through the
National Association of Insurance Commissioners; fifty state securities commissioners (plus the District of
Columbia), acting collectively through the North American Securities Administrators Association; and fifty state
attorneys general. There also are fifty state bank regulators.
Before the subprime mortgage crisis, there was widespread concern that these layers of regulation were affecting
the ability of American corporations to compete in a global economy. Efforts to reduce those burdens were
dropped during the subprime crisis. Congress is now considering even more regulation in this continuing cycle of
scandal and corrective legislation.
Jerry W. Markham

 
See also:  Enron;  Insull, Samuel;  Securities and Exchange Commission;  WorldCom. 
Further Reading
Fox, Loren. Enron: The Rise and Fall. Hoboken, NJ: John Wiley and Sons, 2003. 
Gup, Benton E., ed. Too Big to Fail: Policies and Practices in Government Bailouts. Westport, CT: Praeger, 2004. 
Henriques, Diana B. The White Sharks of Wall Street: Thomas Mellon and the Original Corporate Raiders. New
York: Scribner, 2000. 
Markham, Jerry W. A Financial History of Modern U.S. Corporate Scandals: From Enron to Reform. Armonk, NY: M.E.
Sharpe, 2005. 
Mayer, Martin. The Greatest-Ever Bank Robbery: The Collapse of the Savings and Loan Industry. New
York: Scribner, 1990. 
Partnoy, Frank. The Match King: Ivar Krueger, the Financial Genius Behind a Century of Wall Street Scandals. New
York: PublicAffairs, 2009. 
Vise, David A., and Steve Coll. Eagle on the Street. New York: Scribner, 1991. 
Corporate Finance
 
Corporate finance is the process by which corporations acquire the funds needed to start and grow their
businesses, and to fund their everyday commercial activities. Corporate finance can be complex, but its essential
nature begins with the corporate balance sheet. The right side of that financial statement identifies the
corporation’s sources of funding of its assets, which are liabilities and shareholder equity. Each of these funding
sources has unique characteristics.
During boom periods, when credit comes on easier terms, corporations tend to become more leveraged, or
indebted, using the additional borrowed funds to increase output and even speculate in things like other corporate
securities and real estate. But during recessions, such as the one that began in the United States in late 2007 and
gripped the global economy in 2008–2009, corporations find it increasingly difficult to borrow money to finance
investment, expansion, and hiring. Indeed, the credit markets became so tight in late 2008 that many economists
feared corporations would not be able to access even the normally routine short-term credit they often used to
finance day-to-day operations, including payroll. This freezing up of the global economy was one of the critical
factors leading the United States and other governments to offer bailouts of major financial institutions in late
2008.
 Liabilities
Liabilities shown on the balance sheet are simply borrowings, which may include bank loans (secured or
unsecured), notes, bonds, commercial paper, and a number of other lending arrangements. Interest must be paid
on borrowings, which may be either a floating or fixed rate. Arrangements must also be made for the repayment
of the principal of the loan on its maturity date, or at some earlier time. Earlier repayment of the loan may be
required upon the occurrence of a trigger event specified in the loan documents, such as a credit downgrade.

Short-term working capital needs may be met with a number of loan arrangements, such as a revolving line of
credit from a bank. A line of credit allows the corporation to borrow and repay funds as needed, up to a specified
maximum principal amount set by the lending bank. Another popular short-term lending arrangement is
commercial paper. This is simply a promissory note issued by a corporation to a lender, which may be another
corporation with excess funds on hand. The initial type of commercial paper can range from overnight to 270 days,
but averages about 30 days.
Corporations obtain loans on a medium-or long-term basis through a number of financing techniques, such as
note and bond sales. Bonds sold to the public, sometimes referred to as “debentures,” are sold under a trust
indenture agreement that specifies the terms of the bond. The basic “coupon” note, or bond, pays periodic interest
at a set rate over the life of the note.
The interest rate on bonds is usually set by market conditions and by the creditworthiness of the borrower. That
creditworthiness may be assessed by the rating agencies, such as Moody’s, Standard & Poor’s, and Fitch. The
borrower is assigned a rating by one or more of those rating agencies, reflecting one of several levels of perceived
creditworthiness, ranging from investment grade to “junk.” The lower the credit rating, the higher the interest rate
on the loan. The higher rate is required to compensate for the increased risk of a default on a lower-rated bond.
Bonds paying fixed rates of interest will fluctuate in value with changes in interest rates or the creditworthiness of
the borrower. All other things being equal, the value of a bond will increase if interest rates decrease. This is
because the higher-paying bond is more valuable than comparable bonds paying a lower rate of interest.
Conversely, the value of the bond will decrease if interest rates increase.
Some bonds are “callable.” This means that the corporation may redeem the loan before maturity, allowing a
refunding of its debt at a lower cost if interest rates decrease. To protect the lender, there is often some “call
protection” for these bonds in the form of a premium to be paid on redemption or some minimum period of time
before the bonds can be called. Some debt instruments are convertible into stock. This allows the holder to
receive a fixed rate of return before conversion and to participate in the success of a corporation through the
increased value of its stock upon conversion. Conversion rights, call provisions, and other bells and whistles
added to the plain vanilla bond, are designed to attract capital and to allow flexible funding for the corporation at
the lowest or most acceptable cost.
 Shareholder Equity
Shareholder equity includes capital raised by the sale of stock to shareholders, who are the owners of the
corporation. Typically, stock is issued in the form of “common stock” that is given one vote per share on corporate
matters at shareholder meetings and a pro rata share of dividends. “Preferred stock” is also popular. It is given
priority over common stock for dividends in a specified amount. “Preferred stock” generally has a liquidation
preference over common stock in the event the business of the corporation is terminated, but preferred stock
usually has limited voting rights. Preferred stock dividends may be cumulative, which means that, if a dividend on
the preferred stock is missed, the common stockholders may not receive a dividend until those arrears are paid to
the preferred. Such rights give preferred stock many characteristics associated with debt.
Most corporations start out as private businesses. If they later seek to sell stock or bonds to the public, that
offering must be registered with the Securities and Exchange Commission (SEC). The company will, thereafter, be
required to submit financial reports to the SEC, which are made public on a quarterly and annual basis, and upon
the occurrence of certain special events. Once registered with the SEC, the corporation’s stock may be listed on a
national securities exchange, like the New York Stock Exchange, or traded in the over-the-counter market on
NASDAQ or some other venue, such as an electronic trading platform.
Another component of shareholder equity is retained earnings. These are the profits (or losses) of a company
accumulated over the years, minus dividends paid out to shareholders. The declaration of a dividend is a
discretionary matter for the board of directors of the corporation. Instead of paying out the profits to shareholders,

the board may decide to use corporate profits for capital expenditures that will increase the size or profitability of
the business.
A matter of some concern in corporate finance is the concept of “leverage,” which is determined by the debt-to-
equity ratio of the corporation. A high degree of leverage will increase shareholder profits because the profits of
the corporation will be generated more by borrowed funds than by shareholder equity. The lenders receive only
interest payments. In contrast, if the funds were raised by additional stock sales, the amount of dividends the
existing shareholders would otherwise receive will be diluted. This is because the new shareholders will be entitled
to share in future dividends with the existing shareholders. Leverage is a wonderful thing when there are profits,
but it works both ways. If the corporation is suffering losses, leverage will magnify shareholder losses and present
a danger to creditors.
Corporate finance is also driven by the time value of money. This concept recognizes that a dollar earned today is
worth more than a dollar earned in the future. Corporate finance requires constant attention to the present and
future value of money in assessing the costs of particular financing programs and the viability and effectiveness of
alternative financing programs, mergers and acquisitions, and other aspects of corporate finance.
 Corporate Finance—History
Today a vast, complex, and interconnected global structure exists to meet the financing needs of corporations. It
was not always so. The greatest commercial adventure in all history, Christopher Columbus’s voyages to America,
had to be funded by the Spanish sovereign because there was little private capital available for such an
enterprise. That situation changed with the development of the “joint stock” companies, the predecessor to our
modern corporations, which were used by the Dutch and the English for exploration of the world in the sixteenth
and seventeenth centuries. Joint stock companies, including the Virginia Company, which established the
Jamestown colony, were able to raise private capital to fund their global operations and to colonize America.
The British crown eventually suppressed commercial corporations in the American colonies, but the number of
corporations grew rapidly after the Revolutionary War. These fledgling enterprises demanded increasing amounts
of capital as the need arose for the building of bridges, turnpikes, canals, and other internal improvements. Much
capital was raised in Europe, especially England, to fund the early American enterprises. The arrival of the
railroad increased the demand for capital, with over 1,000 miles (1,600 kilometers) of track per year being laid in
the period leading up to the Civil War.
Bonds, rather than common stock, largely capitalized the railroads. This was because European investors
preferred a fixed return on their money. Railroads were soon issuing many levels of bonds in England, including
sterling bonds, which were payable in British pounds; first, second, and third mortgage bonds; convertible bonds;
and real-estate bonds. The preference for bonds over equity would have an unfortunate result. The individuals
controlling the railroads through stock ownership often contributed little in the way of capital, at least in
comparison to bondholders. The equity owners were all too frequently speculators who had little concern for the
long-term success of the company. These robber barons used their leverage at every opportunity to loot the
railroads or to otherwise abuse their positions.
Private investment banking firms expanded after the Civil War in order to meet the increased demands for capital
that followed the conflict. Jay Cooke, the principal Union military financier, was a leader in one of the first joint
syndicate operations for underwriting securities sold to the public, an event that occurred in 1869. Cooke’s firm
failed in 1873, but other investment bankers led the effort to consolidate enterprises into giant amalgamations.
From 1897 to 1904, more than 4,000 firms merged into 257 surviving entities. During the same period, 319
railroads combined. The hundred largest companies quadrupled in value as a result of these combinations,
controlling 40 percent of the industrial capital of the United States.
J.P. Morgan became the most famous investment banking firm and a leader in corporate finance through its
reorganizations of faltering railroads at the end of the nineteenth century. The firm’s most famous combination was

the creation of United States Steel Corporation as the twentieth century began, with a then staggering total
capitalization of $1.4 billion.
The United States was the largest industrialized country in the world at the beginning of the twentieth century. At
the time, it was producing 24 percent of manufactured goods in the world; by 1913, the figure would increase to
one-third. Corporate finance provided the foundation for that growth. The United States became a net creditor with
the outbreak of World War I. Before that conflict, securities issues over $1 million were considered to be large. By
the 1920s, $25 million issues were not unusual. Stock trading grew during the 1920s, until the stock market crash
of 1929, and the ensuing Great Depression, crippled the economy.
Congress responded to concerns over corporate finance raised by the market crash with the adoption of the
federal securities laws in the 1930s. That legislation included the Securities Act of 1933, which regulated initial
public offerings of stocks and bonds, and the Securities Exchange Act of 1934, which regulated trading of
securities on stock exchanges, and later the over-the-counter market. The latter measure also created the
Securities and Exchange Commission, which has become the nation’s principal regulator of corporate finance for
public companies.
Until the 1970s, corporate finance was a largely unexciting business of raising capital from stock offerings and
borrowing funds through bond offerings, bank loans, or commercial paper. That changed dramatically as a result
of the rampant inflation that arose in the 1970s, reaching 13 percent in 1979, with short-term interest rates
climbing to nearly 20 percent in 1981. This resulted in an increased focus on corporate finance, leading to the rise
of the chief financial officer as one of the leading executives at most public companies. These individuals
employed many imaginative techniques for borrowing, investing, and managing short-term funding needs through
such means as “repos” (repurchase agreements), asset-backed commercial paper, sweep accounts, and other
arrangements designed to minimize borrowing costs and maximize profits from surplus funds.
Corporate finance was also exposing a dark underside of its character. The insider trading scandals involving Ivan
Boesky and other well-known financiers on Wall Street resulted in several high-profile criminal prosecutions in the
1980s. The “junk bonds” promoted by Michael Milken, a broker at the investment banking firm of Drexel Burnham
Lambert, were used to fund highly leveraged corporate mergers. These bonds became highly controversial
because the mergers they financed sometimes resulted in business failures and massive layoffs. The buyouts
relied on heavy borrowing to buy public companies.
A revolution in corporate financing techniques involving derivative instruments was also under way. These
included stock option contracts traded on the Chicago Board Options Exchange, beginning in 1973. The futures
markets also developed a number of innovative contracts, including futures on Government National Mortgage
Association (GNMA, or Ginnie Mae) pass-through securities. More innovation followed with futures on stock
indexes, resulting in a convergence of the stock and commodity markets. That convergence raised concerns with
the effects of new trading techniques in these derivatives, such as “dynamic hedging” and “portfolio” trading
through computerized trading programs on the stock market. Those concerns seemed justified when the stock
market crashed in 1987. However, the stock market soon recovered and financial innovation proceeded apace.
Swap contracts that allowed corporations to hedge their interest rate, currency, and other risks became popular in
the 1980s. For example, a firm with an unwanted floating interest rate exposure could swap that rate for the fixed
rate payments of a firm seeking floating rate exposure. More complex swaps led to problems on the part of some
firms that did not understand their complexities. The variations of derivative instruments multiplied into the
hundreds and included such things as “worthless warrants,” “death-backed bonds,” and even “exploding options.”
An important development in finance was the so-called pass-through security, or collateralized mortgage
obligation, which involved mortgage pools in which investors were sold participations. This process facilitated the
raising of a significant amount of capital for the mortgage market. This “securitization” concept spread to other
asset-backed securities, such as credit cards and other receivables like franchise fees and even royalties. This
corporate financing tool was badly abused by the Enron Corporation before that company’s spectacular failure in

2001. Enron used such structures to conceal debt and to increase its revenues improperly. That and other
financial scandals led to corrective legislation in the form of the Sarbanes-Oxley Act of 2002.
The leveraged loan market became a popular source of funding for private equity acquisitions of public companies
in this century. Those loans were syndicated by a lead bank and sold in pieces to other investors. The credit
crunch that began in the summer of 2007 slowed the leveraged loan market and presaged the subprime crisis that
shocked the nation between 2007 and 2009. The federal government invoked desperate measures in order to
prevent a complete freeze in corporate finance. These government programs included the $700 billion Troubled
Asset Relief Program (TARP), which injected billions of dollars of capital into financial services firms; the Asset-
Backed Commercial Paper Money Market Fund Liquidity Facility (AMLF), which made nonrecourse loans on
asset-backed commercial paper; the Commercial Paper Funding Facility (CPFF), which was created to purchase
unsecured, asset-backed commercial paper from corporate issuers through a special-purpose vehicle (this
program had purchased $334 billion in assets by December 31, 2008); and the $200 billion Term Asset-Backed
Securities Loan Facility (TALF, expanded to $1 trillion in March 2009), which makes secured, nonrecourse loans
available to banks and commercial firms, using as collateral such things as credit card debt, consumer loans, and
student loans.
The subprime crisis gave rise to much concern with, and criticism of, corporate finance. Many high-profile financial
services firms, such as Lehman Brothers, Bear Stearns, Citigroup, AIG, and Merrill Lynch failed or had to be
rescued. These firms were highly leveraged with high multiples of debt to equity, and were thought to have
incurred excessive risk in order to increase the bonus pools of executives. This gave rise to efforts to reform
compensation schemes in order to discourage undue risk taking in corporate finance. Although the economy
seemed to be recovering as 2010 began, Congress was considering legislation that would put new constraints on
corporate finance.
Jerry W. Markham
 
See also:  Credit Rating Agencies;  Debt Instruments;  Fixed Business Investment;  Inventory
Investment;  Leveraging and Deleveraging, Financial;  Production Cycles;  Savings and
Investment;  Stock and Bond Capitalization. 
Further Reading
Markham, Jerry W. A Financial History of the United States.  3 vols. Armonk, NY: M.E. Sharpe, 2002. 
Markham, Jerry W., and Thomas L. Hazen. Corporate Finance, Cases and Materials.  2nd ed. Rochester, MN: West
Group, 2007. 
Ross, Stephen A., Randolph W. Westerfield, and Bradford Jordan. Fundamentals of Corporate Finance.  9th ed. New
York: McGraw-Hill, 2009. 
Council of Economic Advisers, U.S.
 
Along with the Federal Reserve (Fed) and the Department of the Treasury, the Council of Economic Advisers
plays a central role in understanding and managing American business cycles. It does so by helping identify,
formulate, and implement policies that lead to economic growth and minimize contraction.

Created by the Employment Act of 1946, the Council of Economic Advisers (CEA) is an independent, three-
member advisory body in the Executive Office of the President. In advising the president, the CEA represents the
view of the economics profession on national economic policy. In fact, the CEA’s only explicit mandate is to write
the annual Economic Report of the President, which contains past economic performance data, projections of
future macroeconomic performance, and discussion of relevant microeconomic and international issues.
Throughout its history, the CEA has focused primarily on maximizing long-term economic growth. In
macroeconomic policy, this has meant trying to achieve full employment and price stability. In recent decades, the
CEA has increased its scrutiny of microeconomic policy issues with the goal of realizing efficiency gains in
government policy in both the short and long term.
 Membership
The CEA consists of three members and their staffs. All three members are appointed by the president and
confirmed by the U.S. Senate. The three members had equal status under the original legislation, but a
chairperson—likewise designated by the president—was assigned special responsibilities in a 1953 reorganization.
The chair is legally responsible for hiring staff and representing the CEA in dealings with the president and public.
Since 1961, the chairperson has been a regularly attendee at cabinet meetings, albeit without formal cabinet
status. All three members of the CEA oversee the professional staff, which comprises about thirty economists and
statisticians.
According to the Employment Act, a qualified member of the CEA “shall be a person who, as a result of his
training, experience, and attainments, is exceptionally qualified to analyze and interpret economic developments, to
appraise programs and activities of the Government, and to formulate and recommend national economic policy....”
In practice, almost all CEA members have had doctorates in economics, have had significant academic
experience, and have intended to return to academia once their service in Washington was over.
The same is true of professional staff, made up almost entirely of academic economists on one-or two-year leaves
from their teaching positions. These nonformalized constraints mean that in comparison with other political
appointments, the advice provided by the CEA is less biased and more consistent with the best practices of the
economics profession as a whole. Because CEA members and staff typically come from academia and plan on
returning to it, they are sometimes less willing to compromise their academic reputations by supporting policy to
please the current administration. As another result of academic professionalism, Democratic economists have
sometimes worked as members of Republican CEA staffs and vice versa.
 Macroeconomic Efforts
Following the Employment Act of 1946 to “promote maximum employment,” during the Harry S. Truman
administration, the council was concerned primarily with maximizing aggregate output. Hence, during the late
1940s and early 1950s, the CEA focused on such issues as the supply of natural resources, the quality of the
labor force, the development of new and improved technology, and government efforts to sustain investment.
During the administrations of John F. Kennedy, the council continued to focus on keeping unemployment low and
investment high. Walter Heller, the CEA chairman from 1960 to 1964, promoted the adoption of a 4 percent
unemployment target. Likewise, Heller set a targeted annual economic growth rate of 4 percent. These goals were
to be achieved, in part, through discretionary fiscal policy as well as sharp cuts in marginal tax rates. Many
scholars point to marginal income tax cuts in 1962 and 1964 as the fuel of that decade’s economic expansion.
During the 1970s, the CEA stepped back from its advocacy of activist macromanagement, reflecting a shift in
opinion across the economics profession. Among academic economists at the time, the idea of a fundamental
trade-off between inflation and unemployment that could be used to maintain full employment was being
discredited. At the same time, there was increasing recognition of the importance of monetary policy in dictating
aggregate demand. While the CEA did not stop advocating fiscal policy to stimulate aggregate demand, it attached

greater importance to monetary policy and the Fed in determining aggregate economic growth.
Indeed, deference to the Fed and an appreciation of the role of monetary policy in minimizing economic
fluctuations has only increased since the 1970s. In more recent decades, the macroeconomic efforts of the CEA
have focused heavily on structural changes that promote long-term economic stability. Nonetheless, the council
opposed a balanced budget amendment in the mid-1990s over concern about the amendment’s effect on the
stabilization of the macroeconomy, and the CEA approved several small fiscal stimuli in the first decade of the
twenty-first century.
 Microeconomic Policy
Since the 1960s, the CEA’s growth-oriented advice to presidents has also addressed efficiency improvements in
microeconomic policy. During the Kennedy administration, the CEA realized that it had the resources and
expertise to give microeconomic policy advice that would promote long-term gains in overall economic efficiency.
By calling attention to understated costs and overstated benefits of proposed policies and by emphasizing the
importance of incentives to the outcome of spending, tax, and regulatory policies, the CEA has achieved several
important improvements in microeconomic policy. If not always high-profile, such changes have yielded long-term
benefits for the U.S. economy.
During the Richard M. Nixon administration, the CEA successfully advocated for the end of government
subsidization of the politically popular but economically wasteful supersonic transport program. In addition, the
1970s also brought accelerated deregulation of the airline, railroad, and parcel service industries, in part due to
the CEA’s advice on the high cost of existing regulations. In the early 1990s, the CEA helped advance an early
“cap-and-trade” energy program, a market-based approach to mitigating sulfur dioxide emissions from burning
coal while realizing efficiency benefits over more burdensome Environmental Protection Agency regulations.
During the Bill Clinton administration later that decade, the CEA played an important role in two important policy
initiatives: the North American Free Trade Agreement (NAFTA) and welfare reform. The passage of both of these
initiatives required constant explanation to the public and Congress of the benefits, that, at least in theory, trade
liberalization and social service reform would bring to the economy. As a relatively independent organization, the
CEA had more credibility than other government agencies.
Among the first appointments made by incoming president Barack Obama in 2009 was Christina Romer as chair
of the CEA. An adviser to Obama through much of the presidential campaign, Romer was a widely expected
choice, not only because of her association with Obama but because of the economic troubles the new president
inherited. Romer had made a name for herself among economists for her work on the Great Depression. Both as
an adviser to the candidate and as chair of the CEA, Romer strongly advocated targeted tax cuts for middle-and
working-class taxpayers. The advice was based on her analysis of President Herbert Hoover’s tax hikes of the
early 1930s, which, she maintained, helped prolong and deepen the depression. Romer resigned in September
2010, and the chair remained vacant for more than a year.
Joshua C. Hall and Elliott McNamee
 
See also:  Congressional Budget Office;  Fiscal Balance;  Fiscal Policy;  Government
Accountability Office;  National Economic Council;  Tax Policy. 
Further Reading
Council of Economic Advisers:  www.whitehouse.gov/administration/eop/cea
Feldstein, M.  “The Council of Economic Advisers: From Stabilization to Resource Allocation.” American Economic
Review 87:2 (1997): 99–102. 

Feldstein, M.  “The Council of Economic Advisers and Economic Advising in the United States.” Economic Journal 102:414
(1992): 1223–1234. 
Norton, Hugh S.  The Employment Act and the Council of Economic Advisers, 1946–1976. Columbia: University of South
Carolina Press, 1977. 
Porter, R.  “Presidents and Economists: The Council of Economic Advisers.” American Economic Review 87:2 (1997): 103–
108. 
Stiglitz, J.  “Looking Out for the National Interest: The Principles of the Council of Economic Advisers.” American Economic
Review 87:2 (1997): 109–113. 
 
Countrywide Financial
 
Once the largest mortgage lenders in the United States—financing about one in five mortgages in the country in
2006—California-based Countrywide Financial Corporation was badly hit by the collapse of the subprime
mortgage market beginning in 2007. Facing a myriad of problems—shrinking assets, plummeting stock prices, and
a number of state investigations into allegedly deceptive lending practices—Countrywide Financial was absorbed
by Bank of America in January 2008, whereupon its name changed to Bank of America Home Loans.

Mortgage originator Countrywide Financial took advantage of the housing boom of the early 2000s by aggressively
marketing subprime loans. The collapse of real-estate prices spelled ruin for the firm, which was acquired by Bank
of America in 2008. (Bloomberg/Getty Images)
Founded in 1969 by business partners David Loeb and Angelo Mozilo, Countrywide went public that same year.
Although the initial response from investors was lukewarm, Countrywide was a mortgage innovator—and a
successful one—from early on. It opened its first branch office in 1974, and had more than forty before the decade
was out. The company earned a listing on the New York Stock Exchange in 1985 and, within six years of that,
had become the nation’s largest mortgage lender.
Meanwhile, Countrywide was branching out into other businesses. In 1985, it created Countrywide Mortgage
Investment (CMI), which bundled mortgages too large to meet the requirements of the quasi-government
mortgage insurers Fannie Mae and Freddie Mac; it bundled them into collateralized debt obligation instruments
that it sold to investors. In 1997, CMI was spun off as IndyMac. (Under the chairmanship of Loeb, IndyMac would
become one of the nation’s largest thrifts and mortgage originators—only to fail in 2008 and be placed under
receivership by the Federal Deposit Insurance Corporation.)
Countrywide was well positioned to take advantage of the housing boom of the early to mid-2000s. Following the
dot.com stock market collapse of 2000, the terrorist attacks of September 11, 2001, and the recession of 2001,
the Federal Reserve moved to lower the interest rate it charged to member banks—a standard monetary policy
for reviving the economy. Between 2000 and 2003, the rate was lowered from 6 percent to 1 percent. This
historically low rate allowed lenders to charge much less for mortgages. Moreover, government policies to expand
homeownership and loosen regulation encouraged many lenders to innovate with new kinds of mortgages and to

pursue new customers, including those with little or bad credit history. Mortgage originators offered these
customers subprime mortgages, which required little documentation of borrower eligibility. Between 2001 and
2007, subprime mortgages increased from 10 percent to 20 percent of the overall U.S. mortgage market. These
mortgages carried an adjustable rate—after an initial period in which the borrower made monthly payments on the
interest only, he or she was required to pay a higher interest rate and part of the principal as well. Such a
dramatic increase in monthly payments normally would have overwhelmed many homeowners’ budgets. But with
home values rising dramatically—a result, in part, of low rates and aggressive mortgage financing—most
borrowers were able to refinance at lower adjustable rates, using the rising equity in their homes as collateral.
No company was more aggressive or innovative than Countrywide Financial in exploiting this burgeoning home
mortgage market. By 2006, the company had outstanding mortgages valued at $500 billion, assets of $200 billion,
and some 62,000 employees working out of more than 900 offices nationwide. At the peak of operations in 2006,
the company reported nearly $2.6 billion in profits.
By 2007, however, the U.S. housing bubble was bursting, a result of soaring home-price-to-income ratios—
especially in hot markets such as Florida, the urban Northeast, and the Southwest—and a glut of newly developed
properties. As housing prices declined, many of those with adjustable rates found it difficult to refinance, especially
as the credit markets began to freeze up. Foreclosure rates soared, and housing starts plummeted.
Having risen so far and so fast, Countrywide was hit hard by the slump. With revenues declining rapidly, the
company was forced to draw on its entire $11.5 billion credit line from a consortium of banks in August 2007. That
same month, it sold a 16 percent equity stake for some $2 billion in stock from Bank of America. In September, it
cut its workforce by roughly a fifth.
All of these measures proved insufficient. While chairman Mozilo (Loeb had died in 2003) maintained that the
company would weather the crisis, rumors began to float through the financial markets that Countrywide was
either about to be acquired by another institution or was on the verge of bankruptcy. In January 2008, the former
proved true, as Bank of America paid $4 billion in stock to acquire the outstanding equity in the firm. Mozilo left
the firm when the takeover was completed in July. For its part, Bank of America hoped to expand its presence in
the home mortgage market.
Meanwhile, federal and state investigations into various lending and accounting practices were proceeding.
Countrywide was accused by federal authorities of falsifying records before the takeover by Bank of America. More
serious civil charges were filed by the attorneys general of California and Illinois, who charged that the company
had engaged in deceptive lending practices by encouraging people to take out risky mortgages beyond their
means, even when loan officers knew this to be the case. These allegations came in the wake of a 2006 case in
which Countrywide settled with the New York State attorney general’s office on charges of steering minority
borrowers into higher-cost mortgages. In April 2009, Bank of America—eager to disassociate itself from a
company with such a reputation—changed the name of Countrywide Financial to Bank of America Home Loans.
In October 2010, Mozilo settled with the Securities and Exchange Commission on its insider trading and securities
fraud charges, paying $67.5 million in fines and accepting a lifetime ban on serving as an officer or director of any
publicly traded company.
James Ciment
 
See also:  Mortgage, Subprime;  Recession and Finance Crisis (2007-);  Shadow Banking
System. 
Further Reading
Michaelson, Adam. The Foreclosure of America: The Inside Story of the Rise and Fall of Countrywide Home Loans, the

Mortgage Crisis, and the Default of the American Dream. New York: Berkley, 2009. 
Muolo, Paul, and Mathew Padilla. Chain of Blame: How Wall Street Caused the Mortgage and Credit Crisis. Hoboken,
NJ: John Wiley and Sons, 2008. 
Creative Destruction
 
The term “creative destruction” refers to the process by which innovation creates new markets while inherently
destroying old or existing ones. Thus, by the very act of creation, innovators are able to harness wealth for
themselves by destroying the markets for established goods and services. Since the 1930s, economists have
incorporated the concept of creative destruction within their understanding of how business cycles operate.
 Origins
Early expressions of the concept can be traced to the writings of Mikhail Bakunin, a nineteenth-century Russian
revolutionary, and Werner Sombart, a late nineteenth-and early twentieth-century German economist and
sociologist. Bakunin stated in 1842 that “the passion for destruction is a creative passion.” And in 1913, Sombart,
in his book Krieg and Kapitalismus (War and Capitalism), wrote, “out of destruction a new spirit of creativity
arises.”
As commonly used and understood today, however, the term creative destruction was introduced into mainstream
economic thinking and popularized by the famed Austrian-American economist Joseph Schumpeter. In his book
Capitalism, Socialism, and Democracy (1942), Schumpeter characterized capitalism as a “perennial gale of
creative destruction.” In that work and others, Schumpeter examines capitalism as an expression of innovation in
dynamic (unstable) business cycles. Specifically, creative destruction is used to describe the outcome of a
particular kind of radical innovation.
In Schumpeter’s words, “The opening up of new markets, foreign or domestic, and the organizational development
from the craft shop to such concerns as U.S. Steel illustrate the same process of industrial mutation—if I may use
that biological term—that incessantly revolutionizes the economic structure from within, incessantly destroying the
old one, incessantly creating a new one. This process of creative destruction is the essential fact about
capitalism.” In other words, the constant churning of market forces leads to the destruction of the prevailing
economic order and gives rise to a new one. Thus, the ups and downs of the business cycles are seen as part of
an internal or endogenous (not external or exogenous) process that is driven primarily by innovation.
 Creative Destruction and the Business Cycle
Schumpeter drew a vital connection between the concept of creative destruction and patterns of the business
cycle. In particular, he saw radical new technologies competing and winning against existing products and, in the
process, creating waves of economic expansion. In his own time (1882–1952), Schumpeter could look back on
such breakthroughs as the steam engine, steel manufacturing, and the telegraph. In the current age, examples
include the computer, the cell phone, and other information and communications technologies. Whatever the
specific innovations, the cyclical process includes economic contraction when an existing technology matures, and
expansion when—and only when—a new technology comes along to replace it. With the emergence of innovative
new products, the old ones become obsolete and the marketplace stops buying them. Producers of outmoded
products inevitably lose market share, and producers of new products shift consumer preference in their favor.

Schumpeter differentiates between two types of innovation—radical and incremental. Products characterized by
radical innovation fundamentally transform the nature of the industry. Products with only incremental innovation
(small modifications and minor improvements) do not radically transform an industry or affect the broader
economy. Incremental innovations neither create new markets nor eliminate old ones.
The interpretation of economic reality employed in Schumpeter’s explanation of creative destruction differs from
that of mainstream macroeconomic theory. Schumpeter avoids the stylized conceptions of human rationality,
competitive markets, and economies of scale, which have proven problematic for standard macroeconomic theory
following the financial collapse of 2008. Schumpeter’s alternative highlights instead the importance of small,
innovative entrepreneurs, often idiosyncratic and quixotic, driven by a unique spirit of enterprise. Schumpeter uses
the German term Unternehmergeist to refer to this entrepreneur-spirit.
 Costs
Creative destruction is not without costs. Industry erosion, the demise of firms, and lost jobs are all a natural and
essential part of the process. Radical innovation forces the rapid obsolescence of existing products and services
as well as the workers and firms that produce them. During market turmoil, skills and knowledge also become
outdated and lose value, leading to increases in unemployment. In the long run, even the largest and most
powerful companies cannot prevail against innovative products and processes that meet market needs. Unless
workers and firms actively seek to improve their skills and respond in a dynamic and proactive manner, the results
may be bankruptcy and liquidation. All in all, however, evidence suggests that societies in which creative
destruction is allowed to operate freely will eventually reap benefits in productivity, economic growth, and more
jobs.
Manjula S. Salimath
 
See also:  Austrian School;  Schumpter, Joseph .
Further Reading
Foster, Richard, and Sarah Kaplan. Creative Destruction: Turning Built-to-Last into Built-to-Perform. New
York: Currency, 2001. 
Kirchoff, Bruce A.  “Entrepreneurship’s Contribution to Economics.” Entrepreneurship Theory and Practice (Winter
1991): 93–112. 
McCraw, Thomas K. Prophet of Innovation: Joseph Schumpeter and Creative Destruction. Cambridge, MA: Harvard
University Press, 2007. 
Schumpeter, Joseph A. Capitalism, Socialism, and Democracy. New York: Harper, 1942. 
Credit Cycle
 
The term “credit cycle” is used today in reference to any economic process in which credit advances play a key
role in business fluctuations. In particular, a credit cycle refers to how credit availability interacts with aggregate
economic activity to generate business-cycle patterns over time. For instance, a shock to the system could

generate dynamic endogenous, or internal, forces on the basis of the interaction between credit expansion and
the prices of collateralized assets, which are then assumed to have ramifications for the course the real economy
takes. These theories, which have been championed primarily by heterodox economists and followers of such
well-known writers as Hyman P. Minsky, have evolved largely in opposition to traditional mainstream real
business-cycle theories that deemphasize the credit-driven aspects of cyclical fluctuations.
 Evolution: From the Banking School to Keynes
Despite its modern link with business-cycle theory, the concept of a credit cycle finds its origin in the recognition
that credit (the provision of liquidity in a monetary economy for the purpose of financing the spending of economic
agents) necessarily follows a sequential process, with credit coming into being when a bank makes a loan and
advances funds to a creditworthy borrower on the basis of sufficient private collateral. Credit is destroyed when
the borrowing unit extinguishes its double-entry bookkeeping counterpart—debt—at the moment of the
reimbursement of the principal of the loan. This “life cycle” of credit was formally recognized by Thomas Tooke
and the disciples of the “banking school” in mid-nineteenth-century Great Britain. These theorists emphasized the
circular nature of credit by espousing the flux-reflux principle, whereby the initial flow of credit is followed inevitably
by a reflux of funds to the banking sector on a later date when the loan is extinguished. On the basis of this view,
it was an easy step to formulate a theory of cyclical fluctuations that underlined the significance of credit
availability and the tendency for the excessive leveraging of the borrowing unit.
In the nineteenth century, famous adherents of the banking principle, such as John Stuart Mill, developed theories
of how exuberant expectations can feed recurrent waves of credit expansion, while such famous empiricists as
Clément Juglar amassed enormous historical data on the anatomy of “commercial crises” and the bursting of
speculative bubbles that had been fed by excessive credit advances. The periods of “prosperity, crisis, and
liquidation” that characterized the credit cycle were defined and measured by the limited bank data series then
available and, above all, by the course of prices over time. The empirical knowledge provided by the work of
Juglar and such other famous institutionalist economists as Wesley C. Mitchell formed the basis of elaborate
twentieth-century theories of the business cycle, especially during the interwar years.
While many economists of the early twentieth century had already elaborated credit-driven theories of business
fluctuations based on the investment-saving relation, one of the most detailed attempts at developing a
sophisticated theory of the credit cycle is found in John Maynard Keynes’s two-volume work, A Treatise on
Money, published in 1930. In the Treatise, Keynes presents a sophisticated model of the monetary system in
which banks play an essential role in the financing of investment, with an elaborate analysis of the genesis and
recurrent pattern of the credit cycle. Keynes’s definition in volume I of the Treatise is most revealing of how he
conceptualizes the cycle: “We now define the Credit Cycle to mean the alterations of excess and defect in the cost
of investment over the volume of saving and the accompanying see-saw in the Purchasing Power of Money due
to these alterations.”
Influenced by the work of Knut Wicksell, Keynes describes how a shock to the system can generate a complete
cyclical process of expansion and contraction in the flow of investment in relation to saving, which would explain
the recurrent pattern of seesaw price movements that had been documented in the empirical studies of the Juglar
variety. If ex ante investment can deviate from household saving, the gap would have to be filled by bank credit.
For any initial shock that triggers a change in borrowing by business enterprise to finance investment, Keynes
traces through period analysis how the economy moves from its initial primary phase of expansion. It is then
accompanied by strong multiplier/accelerator effects characterizing its secondary phase, until the overinvestment
leads to its collapse, debt liquidation, and subsequent recovery. While analytically different in some details from
the broadly similar theories popularized by Friedrich von Hayek and Irving Fisher during the early 1930s, all of
these writers bestowed on credit the vital role in the genesis, ultimate collapse, and liquidation of the accumulated
credit advances.
 Post Keynesian Views and Contemporary New Keynesian Analysis

Inspired by this Keynes-Fisher tradition of the credit cycle, late-twentieth-century Post Keynesian economist
Hyman P. Minsky developed a more sophisticated model of cyclical fluctuation to describe how an economy
evolves from a state of tranquility to one of instability and crisis. In the Minskian world, investment is propelled
forward by the behavior of asset prices, which rise relative to the supply price of current investment goods when
an economy begins to grow. This leads to increasing corporate indebtedness as credit demand expands during
the upswing, thereby progressively pushing the borrowing unit into ever-higher leverage ratios. This financial
fragility, characterized by higher debt ratios, will continue to deepen until some sudden event leads to an
unwinding of the credit-debt relation through stampede liquidation and a precipitous drop in asset prices,
investment, and production. Accompanying this decline, there would be a concomitant drop in credit financing until
asset prices reach bottom and begin to turn around.
The first decade of the twenty-first century brought growing interest in the behavior of credit and its cyclical
character even among more mainstream neoclassical economists, spurred again by the financial crisis in the latter
part of the decade. Some of the literature has moved forward largely because of the work of such New Keynesian
economists as Nobuhiro Kiyotaki and John Moore, who developed an intricate nonlinear model of the behavior of
various financial and macroeconomic variables to an initial shock affecting the value of collateralized assets.
Mario Seccareccia
 
See also:  Bank Cycles;  Capital Market;  Corporate Finance;  Financial Markets;  Financial
Modeling of the Business Cycle;  Leveraging and Deleveraging, Financial. 
Further Reading
Keynes, John Maynard. A Treatise on Money.  2 vols. New York: Harcourt, Brace, 1930. 
Kiyotaki, Nobuhiro, and John Moore.  “Credit Cycles.” Journal of Political Economy 105:2 (April 1997): 211–248. 
Minsky, Hyman P. Stabilizing an Unstable Economy. New Haven, CT: Yale University Press, 1986. 
Niehans, Jürg.  “Juglar’s Credit Cycles.” History of Political Economy 24:3 (Fall 1992): 545–569. 
Seccareccia, Mario.  “Aspects of a New Conceptual Integration of Keynes’s Treatise on Money and the General Theory:
Logical Time Units and Macroeconomic Price Formation.” In Money, Credit, and the Role of the State,  ed. Richard Arena
and Neri Salvadori, 285–310. Aldershot, UK: Ashgate, 2004. 
Seccareccia, Mario.  “Credit Money and Cyclical Crises: The Views of Hayek and Fisher Compared.” In Money and
Business Cycles: The Economics of F.A. Hayek,  vol. 1,ed. Marina Colonna and Harold Hagemann, 53–73. Aldershot,
UK: Edward Elgar, 1994. 
Credit Default Swaps
 
Most privately issued financial securities carry some default risk—that is, the risk that the issuer will not pay the
principal and interest on the security when it is due. A credit default swap is a contract that transfers the default
risk from the purchaser of a financial security to a guarantor, who receives a fee for accepting the risk that the
returns of the underlying security may fall below a certain amount. By hedging default risk, credit default swaps
often are considered a form of insurance for the buyer and a tool used to manage risk. The buyer of the swap

receives protection that a payment will be made by the seller if a credit “event,” such as a default, occurs.
 History
The first credit default swaps, created by J.P. Morgan and Company in 1997, were concentrated in corporate
bonds. The underlying assets in swaps initially were bond funds that sought to insure their portfolios against
default risk. The default risk is transferred to the seller of the swap in exchange for a premium. If the bonds are
paid back in full, the seller retains the swap contract premium. The credit default swap market expanded in the
early 2000s from corporate bonds to include a variety of other markets, including municipal securities, bank loans,
and other financial assets.
Later, the credit default swap market expanded to include collateralized mortgage obligations (CMOs) and other
collateralized debt obligations (CDOs) among the securities for which default insurance could be purchased. A
CMO is a financial instrument (security) created from a pool of mortgages or mortgage-backed securities that
redirects the cash flows (principal and interest) from the underlying mortgages to tranches (classes) with different
risks. A CDO is a financial instrument created from pools of debt that are backed by the principal and interest
payments made on the underlying securities, which are passed through to the owners of the CDO. Like CMOs,
the payments made on CDOs are divided into tranches (classes) or slices with different risks. In essence, CMOs
and CDOs are derivative instruments whose value is determined by the flows of cash in the underlying pools of
securities. Thus, when a credit default swap is created from a CMO or a CDO, the buyer is literally purchasing a
derivative of a derivative.
Another characteristic of credit default swaps is that an investor, such as a hedge fund, can buy or sell a swap
without having any direct relationship to or ownership of the underlying securities. Thus, a speculator could
purchase a swap based on a CMO if the speculator anticipated that defaults in the mortgage markets would lead
to defaults on the CMOs. If there is a default, a payment would be made to the purchaser of the swap, even
though the purchaser does not own the underlying CMO. This is basically wagering on an assumption. In 2007,
credit default swaps peaked at approximately $62 trillion. The combined total of all U.S. corporate debt and
mortgages was less than half of that amount. This demonstrates the extent to which these instruments were
purchased by investors who did not own the underlying securities. Also, credit default swaps are bought and sold
over and over in secondary markets, making it almost impossible for an investor in a credit default swap to
evaluate the ability of the seller to make a payment if a credit event occurs. The outstanding amount of credit
default swaps from 2001 to 2008 is shown in the table that follows.
 Outstanding Credit Default Swaps, 2001–2008 
Year
Credit Default Swaps Outstanding (billions of dollars)
2001
$918.87
2002
$2,191.57
2003
$3,779.40
2004
$8,422.26
2005
$17,096.14
2006
$34,422.80
2007
$62,173.20
2008
$38,563.82
Source: ISDA Market Survey, 1987–2008, International Swaps and Derivatives Association, Inc.

 Credit Default Swaps and the Financial Crisis of 2008–2009
Credit default swaps, which are largely unregulated and untraceable, played a key role in the financial crisis of
2008–2009. Because there is no regulation, there are no capital requirements for the seller of default protection
and no standard way for a buyer of these instruments to assess the risks. Prior to the financial crisis, the Federal
Reserve announced that credit derivatives were good strategies for hedging risk, and suggested that more banks
use these credit derivatives. The explosion of credit default swaps created a ticking time bomb. If these exotic
derivative-type instruments had not been created, the extent of the downturn caused by the subprime mortgage
crisis would have been much less. This is an example of a situation in which the financial engineers who created
these products were one step ahead of regulators—leading to disastrous results. Regulators and other market
participants did not know the extent of these instruments until problems arose.
During the financial crisis of 2008–2009, the bankruptcy of Lehman Brothers and the bailout of AIG (American
International Group) were caused by losses in swap markets in which these institutions had sold “default
insurance” to speculators, including hedge funds. During AIG’s bailout, Société Générale, one of the largest
European commercial banks, was the largest recipient of both credit default swap collateral postings ($4.1 billion)
and payments ($6.9 billion), which were paid in whole or in part by U.S. taxpayers.
It is apparent, say most experts, that greater global regulation of these markets is needed, including a
standardized clearinghouse that could make sure that sellers of this protection have the ability to pay a
mechanism to insure that the payment is made.
Maureen Burton and Jamie Shuk Ki Yuen
 
See also:  Collateralized Debt Obligations;  Collateralized Mortgage Obligations;  Debt
Instruments;  Recession and Financial Crisis (2007-);  Securitization;  Troubled Asset Relief
Program (2008-). 
Further Reading
 Fabozzi, Frank J., Franco Modigliani, and Frank J. Jones. Foundations of Financial Markets and Institutions.  4th ed. Upper
Saddle River, NJ: Prentice Hall, 2009. 
 Gibson, Michael S.  “Credit Derivatives and Risk Management.” Finance and Economics Discussion Series, Divisions of
Research and Statistics and Monetary Affairs, Federal Reserve Board of Governors, May 22, 2007. 
 Mengle, David.  “Credit Derivatives: An Overview.” Economic Review 92:4 (Fourth Quarter 2007): 1–24. 
 Parkinson, Patrick M.  “Over-the-Counter Derivatives.” Testimony before the Subcommittee on Securities, Insurance, and
Investment, Committee on Banking, Housing, and Urban Affairs, U.S. Senate, July 9, 2008. 
 White, Patricia.  “Over-the-Counter Derivatives.” Testimony before the Subcommittee on Securities, Insurance, and
Investment, Committee of Banking, Housing, and Urban Affairs, U.S. Senate, June 22, 2009. 
Credit Rating Agencies
 
Credit rating agencies are private companies that offer ratings on the risk associated with debt instruments, such

as bonds, issued by private corporations and nonprofit organizations, as well as various government entities.
Credit rating agencies also offer ratings on structured financial instruments—issued by special-purpose entities
created for the task of issuing such instruments—such as mortgage-backed securities and other collateralized debt
obligations. (Credit rating agencies should not be confused with the more familiar credit bureaus or credit reporting
agencies, such as Experian, Equifax, and TransUnion, which provide information to lenders on the creditworthiness
of consumers.)
 Operations
High credit ratings indicate that the debt issuer has a low chance of default; low ratings indicate a relatively high
level of risk. Ratings are determined by a number of factors. For corporations, this includes asset-to-debt ratios,
earnings-to-debt ratios, revenues, past history of meeting debt obligations, and a host of other financial factors.
For governments, a different set of factors come into play in the credit rating process, including public and private
investment flows, foreign direct investment, foreign currency reserves, transparency, political stability, and history
of meeting debt obligations.
Credit ratings serve a number of critical market functions. By offering a tested method for rating credit and
providing simple, letter-based grades on debt, credit rating agencies make it easier for investors to purchase a
range of debt securities, giving them an understanding of the risk they are taking on versus the returns they stand
to make. Credit ratings also give smaller entities, including developing world countries, start-up companies, and
nonprofit organizations, the opportunity to market debt to a wider array of investors.
Credit rating agencies, which make their money by charging fees to issuers of debt, operate in many large
developed and developing countries. Among the most important are three U.S. firms—Moody’s Investors Services
(usually referred to as Moody’s), Standard & Poor’s (S&P), and Fitch Ratings. These agencies rate bonds with
letter grades, much like in a classroom, though the exact methods differ slightly from agency to agency. The
highest rating is “AAA” (Aaa for Moody’s) and the lowest rating is “D.” Any variation on a “D” rating indicates the
instrument is already in default. (Moody’s lowest rating is C, which is for securities that are predominantly
speculative and that may be in default.) There are also variations in the ratings system depending on whether the
debt instrument is short term or long term. In general, the higher the rating the lower the risk, meaning that the
issuer can offer a lower interest rate on top-rated securities. Bonds above BBB– (or Baa3 by Moody’s) are rated
investment grade, meaning that they offer a very low chance of default, while those below that rating are
considered low grade, or “junk bonds,” meaning that the chance of default is high. The sellers of below-
investment-grade junk bonds call these “high-yield” bonds. Historically, about one in fifty investment-grade
corporate bonds has been defaulted on while the figure for noninvestment grade is about one in three. For
municipal bonds, the figures are about 1 in 1,400 for investment grade and 1 in 23 for noninvestment grade.
 History
Credit rating agencies in the United States date back to the nineteenth century when the forerunners of today’s
Dun & Bradstreet and Standard & Poor’s began offering reports to investors and lenders, who paid fees for the
service, about the creditworthiness of various companies and individuals, the latter even being examined for such
nonbusiness-related things as whether too much was spent on entertainment by the company or individual. In
1860, Henry Varnum Poor, founder of the company that would eventually become Standard & Poor’s, published
his History of the Railroads and Canals in the United States, the forerunner of later securities analyses. By the
early twentieth century, the two companies had been joined by the forerunners of Moody’s and Fitch, which also
began to rate bonds, stocks, and other securities. In 1924, Fitch introduced the grade ratings system still in use
today.
Credit rating agencies grew along with the U.S. economy through much of the twentieth century, but underwent
fundamental change in the 1970s. Prior to that period, credit rating companies made their profits by charging
investors and lenders a fee to subscribe to their reports. But the main credit rating agencies came to the

conclusion that their services increased the value of the securities being issued, making the issuing entities more
profitable or, in the case of governments, more cost effective. In effect, the credit rating agencies were increasing
the profits of the companies whose securities they rated with no return to the agencies. In addition, the growing
complexity of the capital markets—along with the growing number of securities being offered—was raising the
costs of providing rating services beyond what investors and lenders were willing to pay. And so, the credit rating
agencies turned their fee system on its head, collecting fees from the issuers of securities.
 Criticisms
This arrangement has led to the predictable criticism that requiring the issuers of securities to pay for their ratings
undermines the impartiality of the rating. More generally, since rating agencies must learn everything they can
about the companies whose securities they are rating, their agents, say critics, establish too-close relationships
with the management of the companies being examined, which also undermines their impartiality. Other experts
offer criticisms of the rating process itself, noting that it inevitably leads to an oligopolistic sector. That is,
companies always seek out one of three leading agencies to rate their securities since only ratings from those
businesses are valued by investors and lenders. Moreover, those agencies take on quasi-regulatory activities,
since they influence how the credit markets work by allowing the investment divisions of banks to use reports from
credit agencies in calculating reserve requirements, as per Securities and Exchange Commission regulations. In
addition, the whole credit rating process can, say some analysts, devolve into a vicious cycle for entities issuing
securities. A lower rating makes issuing securities more expensive, which undermines the creditworthiness of the
company being rated, and so on.
Finally, both the corporate scandals of the early 2000s and the financial crisis in the latter years of the decade
have pointed up new problems with the credit rating business. Some critics have noted that credit agencies have
on occasion been too slow to react to fast-changing economic circumstances. For example, the various credit
rating agencies were giving bonds issued by Enron investment-grade ratings just days before the troubled energy
giant declared bankruptcy in 2001. Far more troubling, according some experts, were the high ratings that credit
agencies gave to some of the riskiest structured financial instruments in the years leading up to the financial crisis
of 2008–2009. Critics charged that this was the result of a cozy relationship between officials of credit rating
agencies and officials of the special-purpose vehicles and financial institutions that created them.
In the wake of Enron’s bankruptcy and other corporate scandals, Congress passed the Sarbanes-Oxley Act in
2002, calling for more transparency in corporate accounting, among other things. Included in the bill was the
requirement for the Securities and Exchange Commission to investigate and correct abuses in the credit rating
process. Subsequently, the collapse of the market in structured financial instruments has brought about new calls
for reform. From the Left has come the idea of some kind of market authority—either a nonprofit entity, consisting
of representative investors, or a public agency—that would manage conflicts of interest between the agencies and
the companies they rate, assure transparency of the rating process, provide ratings information to the public,
assign which agencies rated which securities, and handle the payments made by the companies offering the
securities to the ratings agencies. From the Right, the criticism has focused on the fact that securities regulations
are dependent on credit rating agencies. By getting rid of many of these regulations, the market could more
effectively decide which credit rating agencies provided the best product to investors, thereby eliminating abuses.
Given the scope of the recession and financial meltdown in the 2007–2009 period, it seems unlikely, however, that
Congress will get rid of many of the regulations.
Further controversy hit the industry in August 2011, when Standard and Poors decided to lower its long-term
sovereign credit rating for the United States from the highest triple-A rating to the next highest double-A+. It was
the first time this had occurred in more than 70 years, as a result of a protracted congressional debate over
raising the country’s debt limit. The agency said it took the action because it remained skeptical that the nation’s
political system could resolve its ongoing fiscal problems. The decision left financial markets reeling for a time,
though they soon recovered.

James Ciment
 
See also:  Capital Market;  Consumer and Investor Protection;  Corporate Finance;  Debt;  Debt
Instruments;  Financial Markets;  Indicators of Financial Vulnerability. 
Further Reading
Sandage, Scott A. Born Losers: A History of Failure in America. Cambridge, MA: Harvard University Press, 2005. 
Sinclair, Timothy J. The New Masters of Capital: American Bond Rating Agencies and the Politics of
Creditworthiness. Ithaca, NY: Cornell University Press, 2005. 
Current Account
 
The current account is that part of a nation’s balance of payments that includes imports and exports of goods and
services and transfer payments to and from foreigners. It is an important measure of a country’s international
economic relationships and one that is both a cause of and a response to economic booms and busts.
The current account (CA) is one element of the balance of payments, together with the capital and financial
accounts. The balance of payments is the cumulative measure of how much net income has been coming into a
country from abroad via international trade, foreign investments, and other sources. Often, economists examine
only the trade part of the current account, looking at imports and exports of goods and services, or what is called
the balance of trade. Including incomes such as corporate profits earned in other countries and transfers such as
remittances from workers employed in other countries adds up to the total current account.
A CA surplus means that the sum of these four accounts is positive (greater than zero), while a CA deficit means
that is negative. This does not mean that all four accounts are necessarily either positive or negative. For
example, a country with an aggregate CA deficit may have a foreign trade deficit and a negative income account,
while its services and current transfers accounts may be running surpluses. In 2009, the United States ran up a
deficit in goods—that is, imports over exports—of about $517 billion, but a surplus in services of $136 billion,
producing an overall imbalance of about $381 billion.
 Goods and Services Account
The goods account pertains to income generated from international trade activity; it reflects a country’s net income
from exports and imports. This includes both national exports (goods sold abroad by the residents of a specific
country) and imports for national consumption (goods purchased from abroad by the residents of a specific
country). Exports, essentially the sale of goods for money, generate income for the exporting country while imports
represent a loss of income for the importing country. Thus, a positive goods balance means that, in a given
period, the country in question has exported more (in value) than it has imported. In other words, the country has
generated more income from exporting than it has lost from importing. But a negative imbalance of trade does not
necessarily mean an economy is in bad shape, as prosperous consumers buy up foreign goods. By all accounts,
the U.S. economy in the late 1990s was doing extremely well, with low unemployment, high gains in productivity,
low interests, and a shrinking U.S. budget deficit. Yet between 1995 and 2000, the overall trade imbalance rose
from around $96 billion to just under $380 billion, a roughly 400 percent increase. At the same time, a sagging

economy can depress imports, leading to a positive current account, as was the case during much of the late
1970s and early 1980s.
In addition to final and intermediate goods, the services account reflects international trade in services as well.
Thus, the services account records revenues from the export of services and the cost of imported services. A
positive balance means that a country is a net exporter of services. Services are considered exported if the
residents of a specific country offer those services to the residents of other countries. (For example, if foreign
tourists stay at a hotel in Country A, the latter is technically exporting travel services to the country from which the
tourists came.) Services are counted as imports if residents of a foreign country offer those services to residents of
the home country in question. (For example, if goods from Country B are shipped abroad by ships from Country C,
Country B is said to be importing services and Country C exporting them). The United States has had a service
account surplus as a result of its dominance in banking and other financial services and in entertainment
copyrights such as movies and TV shows. While not as large as the goods account deficit, it has helped to buffer
this deficit. As a result, swings in the value of the U.S. dollar have not been as large as they would otherwise
have been.
 Income Account
The income account reflects the flow of income between residents of a country and nonresidents. It records two
main categories of income: compensation of employees, including wages and benefits; and investments and
income coming to citizens, businesses, and the government from all foreign investments. (Sometimes other
primary income—rent, taxes, and subsidies on products and production—is included as a separate category). A
positive income balance means that the country in question earns more income from abroad than its residents and
businesses pay to citizens, businesses, and governments in other countries. If a country has attracted a lot of
foreign capital but has not invested very much abroad, its income balance will be negative. In other words, it loses
more income to foreigners investing in that country than it has gains from its own investments abroad.
 Current Transfers Account
The current transfers account (sometimes called the secondary income account) records transactions related to
the accumulation of residents’ disposable income but not reflected in other parts of the CA. Current transfers
include such categories as donations, fines, gifts, grants, indemnities, inheritance, insurance fees and premiums,
membership fees, pensions, personal transfers (including worker remittances), social benefits and contributions,
and taxes on income and wealth. A positive balance of current transfers means that a country is a net recipient of
such transfers. The current accounts in many countries in Central America and Eastern Europe get a positive
boost from the remittances sent back to families and friends by guest workers and immigrants—both legal and
illegal—in North America and Western Europe respectively.
 CA Deficits
A CA deficit, or negative CA, means that a country is a net debtor to the rest of the world. Because its income
from exports is not large enough to cover its loss of income from imports, that country must borrow from abroad or
attract foreign investments or foreign aid in order to maintain its consumption level. (The current account balance
can also be caused by low productivity, high inflation, a decline in the world prices of the products the country
exports heavily, an increase in the prices of those it imports heavily, natural disasters, armed conflicts, or an
inappropriately strong domestic currency. In addition, a CA deficit may be caused by large foreign investment
inflows or heavy imports of technology, though in these cases, large exports tend to follow in subsequent years.)
If a country has a large and persistent current account deficit that it is not able to finance itself, its currency may
start to depreciate rapidly. This occurs because other countries—and currency speculators—begin to think that the
country in question might not be able to pay back what it owes and that it might not have enough cash to pay for
needed domestic projects. Other countries might respond by dumping the country’s currency on the foreign

exchange market. The increased supply of the currency on the world exchange—together with plummeting
demand for it—leads to devaluation on international currency markets. To cope with their CA deficit and other
balance-of-payments problems, countries may have to resort to borrowing funds from the International Monetary
Fund. In doing so, they often have to agree to severe belt-tightening measures in order to improve their CA
position.
Tiia Vissak
 
See also:  Balance of Payments;  Capital Account;  Exchange Rates. 
Further Reading
International Monetary Fund. Balance of Payments and International Investment Position Manual.  6th ed. Washington,
DC: International Monetary Fund, 2010. 
International Monetary Fund. Balance of Payments Statistics Yearbook. Washington, DC: International Monetary
Fund, 2008. 
 
Debt
 
A debt is a liability to the borrower (debtor) and an asset to the lender (creditor) who purchases the debt
instrument (loan, bond, etc.), issued by the borrower. The creditor has the expectation of full payback within a
predetermined period of time, often in fixed, regular installments and with additional compensation (interest),
calculated as a percentage of the debt instrument’s value compounded over the time period. The terms of the
debt, including interest, duration, and other considerations, are typically set forth in a written contract before the
debt instrument is issued and made available to be purchased by the creditor. Debts can be accrued by
individuals, households—who take out loans and mortgages, businesses, and governments, which may borrow or
issue bonds.
When an individual, business, or, more rarely, government debtor finds itself unable to meet its obligation to its
creditors, that individual or institution is said to be insolvent, usually requiring it to file for bankruptcy within the
jurisdiction in which it operates. In the United States, all bankruptcies are handled by federal courts.
Borrowing and lending through debt markets are critical components in the functioning of a modern capitalist
economy, allowing individuals to finance major purchases, businesses to conduct daily operations and expand, and
governments to finance war, infrastructure projects, or other extraordinary expenses.

The National Debt Clock in New York City stood at $11.5 trillion in July 2009 and exceeded $12 trillion by year’s
end. As a percentage of gross domestic product, a key economic indicator, the U.S. debt rose to an alarming 67
percent in 2010. (Timothy A. Clary/AFP/Getty Images)
 Types of Debt
Debts come in several overlapping categories. A secured debt guarantees the creditor the right to an asset held
by the debtor should the latter fail to pay back the debt. Common secured debts include home mortgages and car
loans, since the contract contains a proviso that the lender can seize the home or car should the debtor fail to pay
back the full amount on a timely basis. An unsecured debt, by contrast, is one in which the borrower puts up no
collateral to secure the loan. For individuals, the most common unsecured debt is credit card spending; the
financial institution that issues the card cannot seize the assets that was purchased by the funds lent.
Debts can also be held by private individuals and entities or publicly owned corporations. A typical privately held
debt is a loan offered directly by a financial institution to a borrower. Debts held by publicly owned companies,
also known as bonds, represent IOUs issued by businesses to investors, with a fixed amount of interest and a
fixed time in which they must be paid. (Public debts are not the same as “public debt,” which refers to the amount
owed to bondholders by a government.) Bonds, which are issued by businesses and governments, are typically
sold as securities either directly to investors or through brokers. Bonds may be secured or unsecured, meaning
that the bondholder has a higher priority for repayment should a business, or, more rarely, a government, be
unable to pay all of its debt obligations and find itself in bankruptcy.
Finally, private loans may be syndicated or bilateral. A bilateral loan is a loan between one lender and one
borrower. Syndicated loans are those offered by more than one lender to a single borrower. Typically, syndicated
loans involve very large sums of money, either beyond the means of a single institution or individual to lend or,
more typically, above the amount that an institution or individual cares to risk on a single borrower. Syndicated
loans are usually tendered for large-scale building projects or for corporate mergers and acquisitions. In the case
of a syndicated loan, the lead lending institution often makes a guarantee (for a fee) that the full amount of money
will be available, thus providing a kind of bridge loan to the borrower until other lending institutions or individuals
can be brought on board. This service is known as underwriting and helps to expedite large-scale lending.

Debt is essential to the smooth functioning of a modern economy because it facilitates and encourages consumer
spending, and consumer spending is the primary engine of developed-world economies. In the United States, for
instance, about 70 percent of all economic activity is related to consumer spending. Just as importantly,
companies often rely on short-term loans to meet payroll, fill stock, and cover other operational needs and on
bilateral and syndicated long-term loans to make strategic investments, expand operations, and acquire property
or raw materials—all critical for growth and modernization. In addition, without the ability to float bonds,
businesses and governments would be denied a means of obtaining revenues to create new plants and
infrastructure. Finally, debt is critical to the functioning of national and global financial systems, as financial
institutions routinely acquire short-term debt to meet asset requirements against loans they themselves are
making. Indeed, it was the freezing up of short-term, interbank lending and bank-to-business lending in the late
summer and fall of 2008 that prompted the U.S. government and others around the world to inject hundreds of
billions of dollars into leading financial institutions. Should this kind of lending stop, it was feared, the credit
markets would freeze up and the global economy could grind to a halt, plunging the world into a new Great
Depression.
 Financial Crisis of 2008–2009
Indeed, debt was central to the crisis that gripped the world’s financial markets in 2008—either directly, through
nonperforming mortgages when the housing boom collapsed in 2007, or via derivatives, a financial instrument
whose value is derived from some other asset. Because lending is an inherently risky business, creditor institutions
seek ways to spread the risk around. Derivatives that are used to hedge, such as futures options, allow those
most willing and able to bear a risk to do so and hence make financial markets more efficient. Other derivatives,
such as securitizations, take relatively illiquid assets, package them together, and sell them off to investors as
relatively liquid assets.
The idea behind all of these derivatives was that by diffusing risk, financial institutions were protecting themselves
and making it possible to extend credit to more and more individuals. But by spreading risk around, the financial
institutions that originated the mortgages had less incentive to ensure that borrowers were creditworthy, thus
creating a moral hazard problem. When large numbers of high-risk borrowers began to default on their mortgages,
the aggregate losses overwhelmed the system. Derivatives, then, created what economists call “systemic risk,” in
which so many large institutions become insolvent that the global financial system was threatened. Some
economists came to fear a similar scenario with regard to collateralized credit card debt. If enough people default
on their credit card bills, the strain on the global financial system could be as great as that caused by the
mortgage crisis.
 Public Debt
Public, or government, debt operates somewhat differently from private debt, but can also produce systemic
problems when it grows too large. The U.S. government, for example, has operated with large deficits for several
decades, except for a brief period at the turn of the twenty-first century when it ran small surpluses. To reduce the
debt, the government can raise revenues or cut spending, risky choices politically; it can increase the money
supply and thereby inflate its way to a reduced debt, which risks undermining the credibility of the government to
borrow in the future on favorable terms; or it can issue bonds, which only pushes debt repayment into the future.
For domestic holders of U.S. Treasury bonds, there is little risk of losing money, since the U.S. government
cannot default on its debt—barring a catastrophe of unimaginable proportions.
However, for foreign holders of U.S. public debt—or the public debt of any country other than their own—there is
another risk factor, inflation, which can undermine the value of a bond, since a U.S. bond is denominated in
dollars, and dollars may fall in value against the currency in the lender’s country. With the U.S. government
posting unprecedented deficits, which have contributed to a debt load hovering above $12 trillion at the end of
2009, many economists and politicians have come to fear that foreign lenders—notably, the Chinese, who hold
about $1 trillion of that debt in bonds—might shy away from buying the Treasury bonds that help to service the

debt. The huge debt to China is a result of the vast trade imbalance between the United States and that country,
which the United States pays for by borrowing from outside sources, including China itself.
Thus far, China and other foreign holders of U.S. Treasury bonds have continued to invest in them for two
reasons. First, despite the recession of 2007–2009 and the mounting public debt, U.S. Treasury bonds are still
seen as perhaps the most secure investment in the world, since the U.S. dollar remains the world’s reserve
currency; second, selling off the bonds or not buying more would undermine the value of the large quantity of U.S.
Treasury bonds China and other countries already hold.
James Ciment
 
See also:  Corporate Finance;  Debt Instruments;  Fiscal Balance;  Minsky’s Financial
Instability Hypothesis. 
Further Reading
Bonner, William, and Addison Wiggin. Empire of Debt: The Rise and Fall of an Epic Financial Crisis. Hoboken, NJ: John
Wiley and Sons, 2006. 
Burton, Dawn. Credit and Consumer Society. New York: Routledge, 2007. 
Deacon, John. Global Securitisation and CDOs. Chichester, UK: John Wiley and Sons, 2004. 
Foster, John Bellamy, and Fred Magdoff. The Great Financial Crisis: Causes and Consequences. New York: Monthly
Review Press, 2009. 
García, José, James Lardner, and Cindy Zeldin, with assistance from Myra Batchelder and Jennifer Wheary. Up to Our
Eyeballs: How Shady Lenders and Failed Economic Policies Are Drowning Americans in Debt. New York: New
Press, 2008. 
Wiggin, Addison, and Kate Incontrera, with Dorianne Perrucci. I.O.U.S.A. Hoboken, NJ: John Wiley and Sons, 2008. 
Debt Instruments
 
Debt instruments, which may play a central role in the dynamics of business cycles, enable the transfer of
ownership of a debt obligation from one party to another (or to multiple parties). When a homeowner takes out a
mortgage with a local bank, the bank can then sell the debt. Nothing changes for the homeowner—not the
amount, terms, or interest rate of the loan. In that regard, the transfer of the mortgage is not analogous to a
debtor selling off a debt to a collection agency, to which the balance is then owed. Nevertheless, such
arrangements make the bank’s debt-based assets more liquid. In addition to mortgage packages, debt instruments
include any agreement between a lender and a borrower—including bonds, leases, and credit card agreements—
and as well as financial arrangements derived from them.
 Bonds
The most familiar form of debt instrument is the bond. The issuer of a bond assumes a debt to the purchaser that
is collectible at maturity; the interest on the debt is called the “coupon.” Bonds are used for long-term financing
and are especially common as ways for governments to raise funds. Local or county governments, for instance,

might issue municipal bonds to fund a large, one-time expense, such as the construction of a new building or new
infrastructure. The federal government issued war bonds to raise funds for World War II. Corporate bonds are
different from both government bonds and corporate stocks. A corporate bond is a debt owed by the company
(rather than the government) to the bondholder, while a share of stock confers a portion of ownership of the
company. The value of stock shares fluctuates according to market trends, whereas bonds are more stable.
Government and corporate bonds are issued for a specific principal or face amount. This is the amount repaid at
maturity and upon which interest is calculated. The issue price is the amount paid when the bond is issued. The
maturity date is the point in time at which the principal is paid (and interest stops accruing). The coupon is the
interest rate; the term originated from the physical coupons originally given to purchasers, exchangeable at the
bank for an interest payment.
Bonds are generally grouped into three categories based on the term, or duration, of the instrument: short-term
(maturity of one year or less); medium-term (one to ten years); and long-term (more than ten years). Short-term
bonds are often referred to as bills, medium-term bonds are called notes. Extremely long-term bonds remain a
rarity, but the twenty-first century has seen a flourishing market for fifty-year bonds backed by euros.
 Junk Bonds
Bonds, like other securities and instruments, are rated by credit rating agencies, such as Standard & Poor’s.
Government-issued bonds are often considered zero-risk because of the unlikelihood that the issuing government,
at any level, would default (i.e., fail to pay off the principal and accrued interest at the time of maturity). Other
bonds are rated according to the scale used by the particular rating agency. In the United States, the scale is
often (from lowest to highest) C, CC, CCC, B, BB, BBB, A, AA, AAA. Bonds given a low rating (BB or lower) at the
time of issuance are commonly referred to as “junk bonds,” or, more formally, speculative-grade bonds. Junk
bonds have a higher—sometimes significantly so—risk of defaulting, and investors who buy them should expect of
a relatively high yield to make the gamble worthwhile.
Because of the high risk associated with them, junk bonds are generally excluded from mainstream financial
portfolios; pension funds and other investment institutions are often required (by their own rules) to invest in only
high-rated bonds. Junk bonds are especially popular in the United States, and only in the twenty-first century
have they become common in the European and Asian markets. Michael Milken, the “Junk Bond King,” made junk
bonds a household term in the 1980s. Originally an investor in “fallen angels”—bonds that had been highly rated
and lost their perceived value but had the potential to rise again, allowing the savvy investor to profit by buying
them low and selling them high—Milken became attracted to “junks” as a way to quickly raise the large sums of
money needed to finance the era’s flurry of large-scale mergers and acquisitions. He was, in a sense, the
financier of the corporate raider era. Milken’s appetite for high-risk, high-yield ventures would prove his undoing,
however, as he was indicted on nearly 100 charges of racketeering in 1989. His indictment marked one of the first
times the federal Racketeer Influenced and Corrupt Organizations Act (RICO) was used against an alleged
criminal with no organized crime ties. Milken’s crimes were related primarily to insider trading, securities fraud, and
evasion of taxes on the income he had earned illegally. Junk bonds, meanwhile, while they did not shed their
unsavory reputation entirely, reached their peak in trading in 1998.
 Collateralized Debt Obligations (CDOs)
CDOs are structured notes created from other debt instruments that are pooled together and treated as an asset
that backs (acts as collateral for) the notes themselves, increasing liquidity. Mortgage obligations or credit card
accounts, for instance, can be pooled together and used to issue a number of notes that are easier to trade on
the market than the mortgages themselves. There is a wide variety of CDOs, including those backed by bank
loans, by bonds, by credit derivatives, by insurance obligations, and even by pools of other CDOs. One of the
advantages of creating a CDO is that its credit rating is often higher than that of the backing assets. This, in turn,
can make the CDO available to investors, such as pension funds, that are required to stick to high-rated securities.

The term “toxic debt” became another household phrase during the 2008–2009 financial crisis, as subprime
mortgage loans and other unstable assets were used to back collateralized debt obligations. This resulted in the
extraordinarily widespread presence of such CDOs in the portfolios not only of individual investors, but also of
banks, funds, and corporations. What the crisis revealed was that many CDOs had been overvalued—sometimes
willfully, but often as a result of their sheer structural complexity.
Bill Kte’pi
 
See also:  Collateralized Debt Obligations;  Corporate Finance;  Credit Default Swaps;  Debt; 
Mortgage-Backed Securities. 
Further Reading
Bookstaber, Richard. A Demon of Our Own Design: Markets, Hedge Funds, and the Perils of Financial
Innovation. Hoboken, NJ: John Wiley and Sons, 2007. 
Brigham, Eugene F., and Michael C. Ehrhardt. Financial Management: Theory and Practice. Mason, OH: South-
Western, 2008. 
Chancellor, Edward. Devil Take the Hindmost: A History of Financial Speculation. New York: Farrar, Straus and
Giroux, 1999. 
Madura, Jeff. Financial Markets and Institutions. Mason, OH: South-Western, 2008. 
Shefrin, Hersh. Beyond Greed and Fear: Understanding Behavioral Finance and the Psychology of Investing. New
York: Oxford University Press, 2007. 
Deflation
 
The standard definition of inflation is a protracted period of generalized price increase, which can occur when too
much money chases too few products. By the same token, deflation is defined as a protracted period in which the
overall price level falls. Deflation may be due to too little demand for products, or to improvements in technology
that allow for production costs to fall. In general, if output prices are falling due to less demand, wages,
employment, and incomes will also tend to fall or to increase more slowly. Deflation is a problem for an economy
because debts are denominated in dollars, and if there is deflation, there is a real increase in debt levels in the
economy. Deflation should not be confused with disinflation, which refers to declining rates of inflation.
Changes in the supply of money in an economy are a normal part of the market process. For example, when one
nation imports more goods than it exports, there will be a decrease in the money supply as monetary reserves are
exported to pay for imports. As the supply of money decreases, domestic prices tend to decrease, or deflate.
Then, however, lower domestic prices will discourage imports (because they become too expensive for the
country undergoing deflation) and encourage exports (because they are now less expensive for buyers from other
countries). This process is known as the price-specie flow mechanism. Even though money and prices deflate in
one country, strictly speaking this may not be deflation. The reason is that the international trade system corrects
the price imbalance in a relatively short amount of time; thus there is no protracted period of price decline, and
hence the definition of deflation is not satisfied.

 Deflation and the Great Depression
The fear of deflation has been called “apoplithorismosphobia,” based on the severe deflation of the Great
Depression that led some economists to believe that deflation is the cause of severe economic downswings.
According to this view, shocks to the banking and financial system can cause a sharp decrease in lending and
bank failures, resulting in a decrease in the supply of money and credit. This reduction makes it difficult to finance
production and to purchase goods. Lower prices and incomes place individuals, businesses, and banks in danger
of bankruptcy. This, in turn, feeds back into the economy and creates a deflationary depression spiral. Such fears
were particularly pronounced during the Japanese recession of the 1990s and again during the economic crisis
and recession of 2007–2009, with the bursting of the housing bubble.
At the same time, some economists have reevaluated the economic phenomenon of deflation and found that it is
not always the evil or threat that many believe it to be. Rather, in most instances, it is a normal market process
and not a true cause of economic depression. Indeed, these economists maintain, the deflation associated with
depression is best viewed as a consequence that facilitates correction and recovery. More generally, deflation is
associated with prosperity, as when increases in the production of goods due to increases in productivity that
exceed the increase in wages result in lower prices and higher real wages. The result for consumers is that it
costs less to buy a car or a dishwasher or week’s worth of food—hardly cause for fear. In addition, deflation may
be offset by increases in the velocity of money. That is, as the money supply falls, each dollar circulates more
rapidly through the economy, which can help stabilize spending and prices.
 Four Types of Deflation
Deflation of prices implies that a unit of money has greater purchasing power than it did previously. This helps
identify four different types of deflation, two of which operate on the demand for money and two on the supply of
money: growth deflation, cash-building deflation, bank credit deflation, and confiscatory deflation. The first
represents the general, beneficial case of deflation, the second and third are remedial (or corrective) processes
that tend to reverse adverse economic conditions, and the fourth refers to extreme and harmful measures
undertaken during emergencies.
On the demand side, the usual form of deflation is “growth deflation,” which occurs in a market economy when
the production of goods expands faster than the wages increase due to increases in productivity. If production
expands faster than wages, per-unit costs fall, which puts downward pressure on prices and tends to increase the
real wage. This is a process by which economic growth is transmitted into higher standards of living throughout
the economy.
“Cash-building deflation” is another form of demand-based deflation, albeit more unusual. It typically occurs during
economic emergencies in which difficult circumstances, such as the prospects of war or depression, threaten the
standard of living. If the production of goods and money remain the same, but the demand for money increases to
purchase goods in the future, this exerts downward pressure on the price of goods and on wages. This is
pejoratively referred to as “hoarding,” because the additional savings puts short-term pressure on the suppliers of
goods. However, because money is so useful in emergency conditions, it is a perfectly logical strategy that
benefits even nonhoarders because their money also has greater purchasing power.
On the supply side of money, the common form of deflation is “bank credit deflation.” When depositors perceive
that banks have expanded lending for nonprofitable or risky projects, they will withdraw their money, forcing banks
to curtail lending and causing insolvent banks to fail. This depositor-driven process results in an overall decrease
in the supply of money in the economy, falling prices, and a greater purchasing power of money. The resulting
contraction is merely a symptom of, but does not in itself cause, a depression. The actual cause of the Great
Depression of the 1930s was the previous expansion of money and credit into unprofitable or risky investments.
Bank credit deflation, once widely feared and maligned, is actually the correction process of the business cycle.
Since the 1930s, in the United States, federal deposit insurance has greatly reduced fears caused by bank credit
deflation.

The more unusual form of deflation on the supply side is “confiscatory deflation,” in which the government
prevents depositors from accessing their money. Such actions often take place after a bank credit deflation has
revealed banks to be bankrupt. The government’s policy response reduces money in circulation and puts further
downward pressure on prices. It may force people to resort to barter, self-sufficiency, and other primitive means.
Confiscatory deflation represents a transfer of wealth from depositors to bank owners and can lead to social
chaos.
Perhaps deflation’s greatest threat concerns debts. As the value of money increases, it makes the costs of paying
back or servicing debt that much more expensive, pushing more businesses and consumers and other borrowers
into insolvency and bankruptcy. In late-nineteenth-century America, for example, many indebted farmers pushed
for inflationary measures—such as the monetization of silver—to help relieve their debt burden.
All changes in the supply and demand for money, and the subsequent changes in prices that ripple through the
economy, result in winners and losers. The association of deflation and depression has resulted in an unwarranted
phobia of deflation. In reality, with the exception of the confiscatory type, deflation should be seen as a positive
force in an economy, either as a natural component of long-term economic growth or as a corrective process that
addresses the root cause of depressions or economic emergencies such as war.
Mark Thornton
 
See also:  Inflation;  Price Stability. 
Further Reading
Rothbard, Murray N.  “Deflation: Free and Compulsory.” In Making Economic Sense. Auburn, AL: Ludwig von Mises
Institute, 2006. 
Salerno, Joseph T.  “An Austrian Taxonomy of Deflation—With Applications to the U.S.” Quarterly Journal of Austrian
Economics 6:4 (Winter 2003): 81–109. 
Thornton, Mark.  “Apoplithorismosphobia.” Quarterly Journal of Austrian Economics 6:4 (Winter 2003): 5–18. 
Demographic Cycle
 
Demographic cycles are the fluctuations in human population over time, or the continuities and changes in birth
rates, mortality rates, composition, and overall size of a given population. Economists have long noted
connections between demographic cycles and economic developments, with an extensive literature on the subject
dating back to the eighteenth century.
 Pre–Industrial Revolution
For most of human history—from the beginning of human civilization in about 3,000 BCE to the onset of the
industrial revolution in the mid-eighteenth century—the demographic cycle was characterized by three basic
trends: relatively high birth rates (about 40 per 1,000 persons annually, with adult females averaging six to eight
live births in their lifetimes) and high mortality rates (25 to 38 per 1,000 persons per year); a steady, secular rise in
world population over time, from about 25 million at the dawn of civilization to 750 million on the eve of the

industrial revolution; and periodic population catastrophes, usually the result of epidemic, war, or social chaos.
The high birth and death rates were due to a variety of factors. High birth rates resulted from a lack of
contraception; high infant and child mortality rates, which motivated parents to have a lot of children; agricultural
economies that required the labor of children; and social customs that grew out of these necessities. Meanwhile,
the high death rates—which nearly canceled out the high birth rates—resulted from poor diet or periodic famine,
primitive medical practices, and poor hygiene. The long-term secular gain in population was largely due to
expanded agriculture, as humans moved into new areas of the globe, cut down forests and replaced them with
farmland, or used irrigation and other ancient techniques to expand fertile lands. Increasing population densities
were critical to the development of civilization and inter-regional trade, as they allowed for the creation of
agricultural surpluses that could be consumed by artisans, merchants, and other urban dwellers.
Yet while the general trend in preindustrial human history had been gradually upward, periodic cataclysms have
dramatically reduced the population in specific regions and periods of time. In the history of Western civilization,
the most dramatic of these was the coming of the bubonic plague in the mid-fourteenth century, an episode
known as the Black Death. The disease wiped out about one-third of the population of Europe in just a few years,
ending two centuries of solid demographic growth.
While the suffering of victims was ghastly and the psychological impact on survivors was harrowing, economic
historians cite significant benefits resulting from the Black Death. By substantially reducing the peasantry—the
poor, as is often the case in pandemics, are the worst affected—it gave additional bargaining power to those who
survived. Landlords desperate to retain laborers had to concede new freedoms and increased compensation,
leading to the end of the feudal order that had dominated Europe for centuries and allowing for the rise of a more
prosperous peasantry and an increase in urban populations, as newly freed serfs flocked to towns and cities.
These developments, say historians, established the foundation for a modern capitalist Europe.
While the Black Death was a unique episode in European history, its effects were similar to admittedly less
dramatic ones put forward by the first scholar to extensively study the relationship between demographic and
economic cycles, Britain’s Thomas Malthus. In his Essay on the Principle of Population (1798) and The Principles
of Political Economy (1820), Malthus presented a model of population fluctuation based on agricultural output. As
output grows, he hypothesized, so does population, but at a much faster pace. Rising population in turn puts
upward pressure on agricultural prices even as it causes a drop in income, as more farmers try to make due on
less land and more workers compete for existing jobs. Eventually, income drops by enough to cause hunger and
starvation, thereby reducing the population and allowing survivors more land and the ability to negotiate for better
wages. And the cycle continues.
 Industrial Revolution
The industrial revolution, which ushered in the modern demographic cycle, eventually proved Malthus wrong, as it
introduced new agricultural techniques and equipment and created a transportation infrastructure that allowed farm
products to be shipped more efficiently. Population growth went from steady to spectacular. As industrialism took
off, more and more people left the farm for the city, even as those who remained behind became more
productive. Meanwhile, the increasing wealth created by industrialization, as well as new technological and
scientific innovations, allowed for improved public health measures, which dramatically lowered mortality rates.
However, it also allowed for lower-cost contraception.
All of this contributed to a period of dramatic population growth in the areas most directly affected by the industrial
revolution, particularly Europe. While that continent’s population stood at about 160 million in 1750, it climbed to
more than 400 million by 1900. In other words, while it had taken Europe 2,000 years to increase by 100 million
people (from about 60 million at the beginning of the common era), it took just 150 years to climb another 240
million. The continent’s population would rise to approximately 550 million by 1950 and to more 700 million by the
year 2000.

Even as the population rose, however, other developments were slowing the rate. The rise from 160 million to 400
million represented an increase of 250 percent in 150 years, while the rise from 400 million to 700 million
represented an increase of just 75 percent over 100 years. As mortality rates dropped—particularly for infants and
young children—couples felt less compelled to produce as many children, lowering birth rates. In addition, as
people left the land and went to work in factories and businesses, they no longer relied on their children’s labor in
the fields. Children, in a sense, became liabilities rather than assets, economically speaking, as they increasingly
came to spend long years in school, a nonremunerated occupation but one critical to the child’s future success in
an urbanized, industrialized economy. Over time, as parents came to recognize these costs and the fact that their
children were more likely to survive to adulthood, they reduced the number of children they had.
Thus, the modern demographic cycle is marked by dramatic population growth with the advent of industrialization
and modern public health measures, and a tapering off as couples make the decision to have fewer children. And
just as the economy has a profound impact on demography, so demography has a major impact on the economy.
Increased populations allow for the creation of larger internal markets and economies of scale.
 Post–World War II Era
Gradually, what happened in Europe spread to the rest of the world, though with a difference. In much of the
developing world, industrialization lagged behind dramatic population growth, as public health measures and
modern medicine lowered the death rate before economic modernization could lower the birth rate. Thus,
populations in the developed world exploded after World War II, as the new health measures—along with the
more bounteous harvest the green revolution in agricultural crops produced—spread around the globe. From 1950
to 2000, Asia’s population grew from 1.4 billion to 3.6 billion, Africa’s from 220 million to 770 million, and Latin
America’s from 170 million to 510 million.
The relationship between dramatic population growth and economic performance is murky, however. In some
instances, including various countries in Asia and Latin America, rapid population growth has not stunted
economic development. At the same time, it is clear that overpopulation can produce economic ills, including
unemployment and poverty. Such is the case in much of the Middle East and Africa, which continue to experience
some of the highest population growth rates in the world. In addition, overpopulation can put enormous strains on
the natural environment.
Meanwhile, the growing prosperity of East Asia has produced the same results as in Europe—though in a more
truncated period of time, reflecting the region’s much more rapid industrialization. As those societies become more
prosperous and economies modernize, parents come to the realization that it makes more economic sense to
have fewer children, who can then be educated more effectively. Indeed, across East Asia, rapid population
increase is giving way to more gradual gains and in some cases, such as South Korea, declines. In China, the
large decline in the population growth rate was also the result of draconian laws limiting most urban couples to a
single child.
Indeed, population declines seem to be a hallmark of many countries in the industrialized world, the most notable
exception being the United States. And just as dramatic population increases can place a burden on an economy
to provide enough goods, jobs, and services for the rising numbers, so ebbing populations—which also mean
aging populations—can create problems as well. Not only do they shrink internal markets and reduce the labor
force, but they also require each worker to support more retirees, usually through taxes or public insurance
schemes such as America’s Social Security.
James Ciment
 
See also:  Malthus, Thomas Robert. 
Further Reading

Chesnais, Jean-Claude. The Demographic Transition: Stages, Patterns, and Economic Implications: A Longitudinal Study of
Sixty-Seven Countries Covering the Period 1720–1984. New York: Oxford University Press, 1992. 
Demeny, Paul, and Geoffrey McNicoll. The Political Economy of Global Population Change, 1950–2050. New
York: Population Council, 2006. 
Easterlin, Richard A. Reluctant Economist: Perspectives on Economics, Economic History, and Demography. New
York: Cambridge University Press, 2004. 
Malthus, Thomas Robert. An Essay on the Principle of Population, ed. Philip Appleman. New York: W.W. Norton, 2004. 
Simon, Julian L., ed. The Economics of Population: Classic Writings. New Brunswick, NJ: Transaction, 1998. 
Denmark
 
Denmark is a northern European country, made up of a peninsula and an archipelago of more than 400 islands.
The population of Denmark is approximately 5.5 million people, one-fifth of whom reside in the capital of
Copenhagen and surrounding regions. The country is governed by a constitutional monarchy with a unicameral
parliament.
Denmark offers a high standard of living for its citizens through a well-developed social welfare system. In 2007,
Denmark had the highest tax-to-gross domestic product (GDP) rate among the 30 Organisation for Economic Co-
operation and Development (OECD) countries, at 48.9 percent. Despite this tax burden, Danes report a high level
of contentment. Government expenditure in the economy for the years 1999 to 2008 constituted an average of
52.95 percent of the nominal GDP (the value of goods and services produced in the domestic economy during a
given year, measured in current prices), compared with 42.54 percent for the euro area countries. Denmark has
been a member of North Atlantic Treaty Organization (NATO) since its inception in 1949 and was the first of the
Nordic countries to join the European Economic Community, now the European Union (EU), in 1973. However,
along with two other EU members, Sweden and Great Britain, Denmark has not joined the eurozone, despite two
domestic referendums on the question and a possible third in the offing.
The service sector makes up most of the economy, with finance and business accounting for the lion’s share.
These functions are enabled by the country’s robust communications and technology infrastructure, which boasts
the highest broadband penetration among OECD countries. In manufacturing, machinery and transport equipment
account for over one-quarter of the exports. Denmark has a large fishery, a high-tech agricultural sector, and is a
leading exporter of milk, dairy, and pork products. It is a net exporter of energy and has been self-sufficient in
consumption since 1997, drawing on the North Sea for crude oil and natural gas, and on wind power for
renewable energy.
Denmark has a long history of banking and entrepreneurship with significant periods of economic growth. Between
1995 and 2000 real GDP grew at an average rate of 2.9 percent. (Real GDP is the market value of all the goods
and services produced within a country during a given year and measured in constant prices, so that the value is
not affected by changes in the prices of goods and services.) With the onset of the recession in 2001, real GDP
grew at an average rate of only 0.53 percent over 2001–2003, returning to a healthier average of 2.6 percent over
2004–2006, concomitant with the construction boom related to housing and domestic engineering projects. The
rate of growth of real GDP slowed to 1.7 percent in 2007 and an anemic 0.2 percent in 2008. After years of
expansion and maintaining a healthy surplus, growth is expected to slow and stagnate with the end of the

construction boom and the slump in the housing market, which has been overvalued in recent years. Danish
homeowners are among the most heavily indebted in the world. Denmark’s real GDP was expected to decline in
2009, in step with the euro area. There was a tangible loss in consumer confidence beginning in the summer of
2008 as Danes experienced deterioration of personal financial circumstances, followed by a fourteen-year high
rate in personal and business bankruptcies.
 Banking Sector Vulnerable
In the banking sector, credit expanded greatly in the mid-2000s, with resulting overextension of credit, mainly to
property developers. This, along with other factors, put a number of small banks at risk, most notably Roskilde, a
regional bank, the tenth largest in Denmark, which experienced significant losses on property loans in summer
2008. By November, Roskilde and two other banks were taken over by the Nationalbank (Denmark’s central
bank). In early October 2008 the government passed the first bank rescue package in an attempt to build
confidence in the financial markets, guaranteeing the claims of depositors and unsecured creditors. A second bank
rescue package of DKK100 billion (5.5 percent of the GDP), in effect February 2009, is aimed at bank and
mortgage lenders with a view to stimulate lending. This enables the government to buy up bank stocks and sell
them when conditions are warranted, thus recapitalizing the financial system. In return, the government requires
financial institutions to maintain lending. The package also places limits on banking executives’ pay and dividends
and aims to improve supervision and regulation of financial institutions. At the same time, the export industry was
given a boost.
The Danish krone is pegged to the euro as the Nationalbank tends to follow rates set by the European Central
Bank. As the world’s major banks cut their interest rates, the Nationalbank initially raised its rates in an effort to
support the krone, resulting in a significant difference in interest rates between Denmark and the eurozone. There
since has been a downward adjustment in rates.
Demographically, Denmark is faced with a shrinking workforce as evidenced by record low unemployment in 2008,
leading to upward pressure on wages. However, despite this and the global financial crisis, due to its receptivity to
foreign investment and well-developed infrastructure, Denmark continues to be one of the best countries in the
world for business, with its global ranking for business (among eighty-two countries) projected to slip from second
place in 2004–2008 to third place during the 2009–2013 period.
Marisa Scigliano
 
See also:  Finland;  Iceland;  Norway;  Sweden. 
Further Reading
Central Intelligence Agency. The CIA World Factbook. New York. Skyhorse, 2009. 
Economist Intelligence Unit (EIU). Country Report—Denmark.  London: EIU, 2009. 
Economist Intelligence Unit (EIU). ViewsWire—Denmark. London: EIU, 2009. 
Statistical Office of the European Communities (Eurostat). Eurostat Yearbook 2009.  http://epp.eurostat.ec.europa.eu
Organisation for Economic Co-operation and Development (OECD). OECD Economic Outlook,  no. 84. Paris: OECD, 2008. 
Depository Institutions

 
A depository institution is any financial intermediary that accepts customers’ deposits and uses those deposits to
fund loans and other investments. Nondepository intermediaries include insurance companies, pension funds,
investment funds, and finance companies. Depository institutions include banks, savings associations, and credit
unions. The vast majority of commercial banks and savings associations purchase insurance from the Federal
Deposit Insurance Corporation (FDIC). Credit unions can purchase deposit insurance from the National Credit
Union Administration. Deposits are insured up to a current limit of $250,000. Not all accounts and funds held by a
depository institution are insured by the FDIC—only checking accounts, negotiable order of withdrawal accounts,
savings accounts, money market accounts, time deposit accounts (such as CDs), and negotiable accounts drawn
on the bank’s accounts. (The Securities Investor Protection Corporation does protect investors from loss in the
case of brokerage failures but not for losses in investment accounts.) Until 1989, savings and loan institutions
(S&Ls) were insured separately by the Federal Savings and Loan Insurance Corporation, but this agency was
dissolved by the Financial Institutions Reform, Recovery, and Enforcement Act (passed in response to the S&L
crisis) and its responsibilities transferred permanently to the FDIC.
 Deposit Accounts
The defining characteristic of the depository institution is of course the deposit account, of which there are three
main types: demand accounts, savings accounts, and time deposit accounts. Balances in deposit accounts are
part of the nation’s money supply. Different account types place different restrictions and fees (or interest rates) on
the deposits, but the depositor remains the owner of the money held, while it becomes a liability for the bank.
Typically, under the fractional-reserve banking system that characterizes modern banking, the institution holds
reserve assets equal to a fraction of the deposit liabilities in reserve and loans the rest out to other customers,
earning a profit on it. It is from that profit that banks are able to pay interest on some accounts (most savings and
time deposit accounts, some checkable accounts), which protects the customer by negating the effects of inflation
—without which there would be no incentive to save money, which would depreciate over time, perhaps faster
than those things it might be spent on. With liquidity comes the loss of evaporation. Interest, safety, and the
insurance of the FDIC are the primary incentives for using banks for long-term money storage, instead of stuffing
it under the mattress. In addition, interest is the reward depositors receive for postponing present consumption in
favor of future consumption.
Though they are not involved in monetary policy and do so incidentally rather than to enact macroeconomic effect,
depository institutions have the power to increase the money supply. Most of the money supply, in fact, is held in
accounts at depository institutions. Every time a bank grants a loan, it is increasing the money supply by a certain
amount, essentially “creating money” by increasing the amount of money available to that customer without
increasing, dollar for dollar, the amount of cash held by the bank.
Demand accounts are more typically known as checking accounts. While savings accounts earn interest and thus
make a profit or protect against inflation, checking accounts held by members of the Federal Reserve System are
prohibited from earning interest by Regulation Q. In some cases this prohibition is bypassed by banks that do not
participate in the Federal Reserve System (interest-earning checking accounts are one of the selling points of the
new wave of online-only bank accounts such as ING Electric Orange) or by offering negotiable order of withdrawal
(NOW) accounts, which function similarly but are allowed to pay interest. NOW accounts were developed in the
1970s to circumvent Regulation Q by creating deposit accounts that included the ability to issue “withdrawals” that
could be given to third parties—more or less a trick of semantics.
The main benefit of a checking account is the convenience. Most debit cards draw from checking accounts, so
even customers who rarely write checks have come to rely heavily on checking accounts for easy access to funds.
Many checking accounts charge fees—either flat or associated with usage (such as a cost per check)—unless a
minimum balance is maintained.

Savings accounts are the simplest form of a bank account: money is deposited, and can be accessed only by
dealing directly with the bank (through an in-person withdrawal, electronic balance transfer, or use of an ATM). A
small amount of interest accrues on the account—less than would be expected to be earned from even risk-
averse investing, but enough to offset inflation. Banking regulations limit the number of transfers and withdrawals
that can be made on a savings account to six per month.
Time deposits function much like more restricted savings accounts: they earn interest but cannot be withdrawn
from until a specific term has ended. The most common time deposit account in the United States is the certificate
of deposit, which has a fixed term (typically three or six months, or one to five years) and a fixed interest rate. The
interest rate is based, generally, on both the length of the term and the size of the principal (the deposit). Higher
principal and longer terms mean higher interest rates. Typically, the highest interest rates are offered by smaller
institutions in more need of liquid cash.
CDs can sometimes be closed early, but at a substantial penalty, in the form of loss of a certain amount of the
interest that had so far accrued. Interest can be paid out as it accrues, and some customers prefer this option;
however, it means they earn less over time, since the interest is not being “reinvested” through compounding.
 Types of Depository Institutions
Commercial bank. This is the ordinary bank most people are familiar with, offering savings and checking accounts,
mortgage and small business loans, and so on; the term distinguishes it from investment banks.
Savings and loan association. Also known as a thrift, an S&L operates primarily by accepting savings deposits and
making mortgage loans. Although S&Ls were a prominent part of American banking in the early twentieth century,
the advent of checking accounts offered by other types of banks saw their decline until the 1980s, when S&Ls
were greatly deregulated. Suddenly they were allowed to do nearly anything other banks could do, but without
being run by executives who had experience at doing it; the S&L crisis inevitably followed, resulting in the
dissolution of the Federal Savings and Loan Insurance Corporation and the collapse of half the country’s thrifts.
Mutual savings bank. Primarily located in the northeastern United States, mutual savings banks (MSBs) are
designed to encourage savings and prioritize security over profit. They offer few of the bells and whistles of
commercial banks, but because they are not chasing profit, they are risk averse and tend to survive most banking
crises better than other depository institutions.
Credit union. A cooperative depository institution owned and operated by its members, a credit union is much like
an MSB in risk aversion and promotion of savings. In addition, members of credit unions share a bond beyond
being clients, as they often work at the same company or are members of the same union. Credit union fees are
typically lower than those of commercial banks. Credit unions are “not for profit” but not “nonprofit” (a designation
that applies to specific charities); they are, however, tax-exempt organizations. Federally chartered credit unions
and most state-chartered credit unions are insured by the National Credit Union Administration, which actually
often enjoys a higher ratio of insurance fund to capital insured than does the FDIC. Many credit unions serve
members of the armed forces; the world’s largest credit union, for instance, is the U.S. Navy Federal Credit
Union.
Bill Kte’pi
 
See also:  Banks, Commercial;  Savings and Investment;  Savings and Loan Crises (1980s-
1990s). 
Further Reading
Cottrell, Allin F., Michael S. Lawlor, and John H. Wood, eds. The Causes and Costs of Depository Institution Failures. New

York: Springer, 1995. 
Sicilia, David, and Jeffrey Cruikshank. The Greenspan Effect. New York: McGraw-Hill, 2000. 
 
Dot.com Bubble (1990s–2000)
 
Tied to the early commercialization of the Internet, the dot.com bubble, which lasted from the mid-1990s through
2000, was one of the most exuberant speculative episodes in the history of American capitalism. During the peak
years of the bubble at the end of the twentieth century, tens of billions of dollars were invested in thousands of
start-up online and technology-oriented companies, a portion of which went public. When it became clear around
the turn of the millennium that many of these companies were not only overvalued but poorly run—and when
expectations about the revenues the early Internet was capable of generating proved overly optimistic—the bubble
burst, with many of the start-up firms going bankrupt, taking hundreds of billions of dollars in market valuation with
them.
 Origins
Invented in 1969, the Internet—called the ARPANET in its early years—was originally a creation of the federal
government, allowing for the transmission of digitized information from one defense-related computer to another.
At first largely confined to government and university computers, the Internet was transformed into a commercial
phenomenon by three developments. First was the personal computer revolution of the 1980s, in which small,
inexpensive, easy-to-operate computers became ubiquitous in workplaces, schools, and homes. Second came the
development in the late 1980s and early 1990s of graphical interface technology, culminating in the World Wide
Web, which brought the intuitive point-and-click system familiar to many computer users to the Internet. The final
piece came with the National Science Foundation’s 1994 decision to open the Internet for commercial purposes.
In August 1995, an initial public offering (IPO) was tendered for Netscape Communications Corporation, the
developer of the first commercial graphical interface browser. This was the first time the public could buy into an
Internet-connected company, and they did so with gusto. Within fifteen months, the company was valued at more
than $2 billion, making founder Marc Andreessen a billionaire almost overnight. The success of Netscape’s IPO
loosened the floodgates, as other companies with Internet connections went public, often reaping fortunes for the
founders and their financial backers. While Netscape provided access to the Internet, other companies began to
set themselves up to market products over the new medium. Perhaps the best known of these early Internet
marketers was Amazon.com, which began as a bookselling outlet in 1994 and then branched out into other forms
of retail. After its IPO in 1997, the company had a market valuation of nearly $500 million.
The Internet did offer clear advantages to retailers. A company such as Amazon.com could offer a far wider
selection than even the biggest “brick-and-mortar” retailer, as dot.com advocates referred to traditional businesses
operating out of actual stores. Not having to pay for the associated costs of retail space, Internet businesses

could also operate in a leaner fashion, saving on rent, labor, and utility costs—“friction free commerce,” as it was
called. They could cut out the middleman, providing customers direct access to manufacturers and content
originators—a process that came under the buzzword “disintermediation.” And dot.com businesses had access to
a far greater market, since customers could be located anyplace with Internet service.
Dot.com advocates, in both the business world and the many media outlets springing up to cover it, began to talk
of a “new economy.” Not only was Internet commerce destined to replace the traditional kind, but it appeared to
be creating a whole new paradigm, freed from old notions of what made a business profitable. The slogan of the
day was that you either “got it”—meaning, understood that retail and even capitalism as a whole was undergoing
a fundamental shift—or you did not and were doomed to be left behind economically. Proponents of Internet
commerce grew breathless in their estimates of its marketing potential—hundreds of billions, they said, maybe
even trillions, and all within a few years.
Despite what the more ebullient advocates of the dot.com business model believed, Internet commerce still
required real world infrastructure—specifically, networks of powerful computers, known as “servers,” to store the
huge amounts of data contained on millions of Web pages and the high-speed telecommunication lines that kept
the whole system connected. With expectations so high for dot.com commerce, various telecommunications
companies—including some of America’s most prestigious names—began to invest heavily in that infrastructure.
Jeff Bezos founded the online bookstore Amazon.com in 1994 and took it public three years later. Many other
Internet start-ups of the mid-to-late 1990s shut down after spending millions in venture capital but not earning a
penny. (Paul Souders/Getty Images)

 Venture Capitalists and IPOs
Venture capitalists, or those who wanted to invest directly in dot.com start-ups, rushed to take advantage of what
was called “first-mover advantage.” It was widely believed that the first company to take advantage of an Internet
marketing niche was the most likely to succeed, as word of mouth spread ever more rapidly by yet another
relatively new Internet phenomenon—email. All of the hype and sense of urgency drew in an estimated $120
billion in venture capital at the height of the boom between 1998 and early 2000, financing more than 12,000
dot.com start-ups. These businesses fell into several basic categories: “marketplace” sites such eBay, which
brought together buyers and sellers and took a fee on transactions; “portals” such as Yahoo!, which provided
users a guide to Internet sites and earned money from online advertising; “aggregators”—Amazon.com being the
best known early example—which put customers in contact with a number of different suppliers, in this case,
publishers; “content providers” such as traditional media sources that wanted to place their written and visual
materials online and charge a fee for looking at them; and Internet service providers (ISPs), such as America
Online (AOL), which gave people access to the Internet and charged a periodic fee, as phone companies did for
their services.
Roughly one in ten of the dot.com businesses went public, utilizing brokerage houses and investment banks to
offer shares, most of which were bought and sold on the National Association of Securities Dealers Automated
Quotations (NASDAQ), an exchange founded in 1971 and largely devoted to technology stocks. Between the end
of 1995 and its peak in March 2000, the NASDAQ Composite Index quintupled, from just over 1,000 to 5,048.62.
At the same time, Internet public company shares represented 20 percent of the trading volume of all shares on
U.S. stock exchanges and 6 percent of capitalization, an enormous figure given the total capitalization of publicly
trade companies. Investors also bought into the idea that vast amounts of new telecommunications infrastructure
was needed; between 1997 and 2000, the Dow Jones Total Market subindex for the telecom sector more than
doubled in value.
 Bursting
The market was in a classic bubble phase during the last years of the twentieth century, as market valuations far
outstripped the underlying values of companies. There were a host of reasons for this. First, many of the
companies whose stock was rising the fastest—or which were garnering the large amounts of private venture
capital—were operating deep in the red. Amazon.com, arguably the most successful of the aggregators, was
bleeding money during the boom years of the dot.com bubble, some $750 million worth of capital from 1996 to
2000. And while Amazon.com kept its marketing costs down, other Internet start-ups did not. Many spent lavishly
on advertising. OurBeginning.com, an online stationery vendor, took out ads during the 1999 Super Bowl alone
worth four times its annual revenues. The idea was to jump-start the process of first-mover advantage. But
combined with other expenses, such as lavish spending on company headquarters, the advertising put the
companies deeper into red ink. Moreover, many firms lacked feasible business models and prospects for
sustainable growth, as they were often run by young owners and managers who had great technological savvy but
lacked business training or experience. To take perhaps the best-known example of such a company—Pets.com
—market analysts began to question whether a firm devoted to selling pet supplies over the Internet, no matter
how clever its advertising, was really worth the $300 million it had garnered in venture capital and through share
sales following its early 2000 IPO.
Larger forces were also at work that helped deflate the dot.com bubble. In March 2000, when publicly traded
companies published their data on revenues and profits, the picture they collectively presented was dismal.
Predictions that the Christmas retail season of 1999 would demonstrate the viability of these companies and of
Internet marketing generally were proved false. Meanwhile, to cool what was perceived as an overheated,
potentially inflationary market—the much more substantive Dow Jones Industrial Average of more traditional
companies was also hitting record highs—the U.S. Federal Reserve (Fed) posted no less than six interest rate
increases between early 1999 and early 2000.

The bursting of the bubble began over the weekend of March 11–12, 2000, as major institutional shareholders put
in automatic sell orders for key high-tech stocks—though not necessarily dot.com stocks—such as IBM, Cisco,
and Dell. Many savvy market analysts had come to the conclusion that, even among these more solid firms,
market valuations were too high. With preparations for the Y2K switchover done—in which millions of computer
systems had to be adjusted to account for the change from the twentieth to twenty-first centuries—many investors
came to believe that there would be a drop-off in the need for technology products and services. As is often the
case, the fall in prices for the shares of high-profile companies dragged down the rest of the market sector they
belonged to. Combined with the poor earnings reports and the growing sense that many of the dot.com start-ups
were poorly run, the sell-off produced a panic in the market as investors desperately sought to unload shares and
venture capitalists closed their wallets. Meanwhile, as the dot.com bubble burst so too did expectations about the
need for the infrastructure to support it. Just as there was not enough consumer demand to sustain some of the
more frivolous dot.com companies, so there was not enough Internet traffic to justify the huge amount of money
invested in infrastructure.
The result of all of this was a dramatically deflated NASDAQ index—which fell by more than half by the end of
2000—and a wave of bankruptcies. By late 2002, with heavy ongoing declines in the telecom sector, the total loss
in market valuation was estimated at an astonishing $5 trillion, or more than 10 percent of total U.S. wealth. Total
online sales amounted to just $36 billion, a tiny fraction of what people had projected back at the height of the
boom in the late 1990s. The bursting of the dot.com bubble helped trigger the recession of the early 2000s.
 Legacy
Economists and analysts who study the dot.com boom—and the bust that followed—point to a more long-lasting
legacy. While predictions that dot.com marketing would quickly outstrip traditional retailing were grossly overstated,
online sales in the years since 2002 have garnered an increasing market share, and companies that survived the
crash—including eBay and Amazon.com—became well positioned to take advantage of that growth. Moreover,
both the technological and marketing innovations of the dot.com boom laid the foundations for an Internet
marketplace that many experts agree may, in the long run, fulfill some of the more expansive predictions made for
it in the late 1990s.
On the negative side, some economists lay blame for the housing bubble of the mid-2000s—which led to a
market correction and recession in the late 2000s far worse than those of the early 2000s—at least in part on the
dot.com bust. With stock prices falling by some 80 percent on the NASDAQ, and a smaller but still significant
percentage on other major exchanges—the Dow Jones Industrial Average dropped by about 25 percent between
2000 and 2003—venture and speculative capital that had gone into corporate securities may have found its way
into the housing sector. In addition, in responding to the dot.com-induced recession, the Fed lowered interest
rates to record lows of just 1 percent in 2004, bringing down the cost of home mortgages and thereby
encouraging home purchases among many people who otherwise (or in other times) could not have afforded
them. In a sense, then, echoes of the dot.com bubble and burst continue to reverberate through the American
economy.
James Ciment
 
See also:  Asset-Price Bubble;  Information Technology;  Technological Innovation;  Venture
Capital. 
Further Reading
Cassidy, John. Dot.con: The Greatest Story Ever Sold. New York: HarperCollins, 2002. 
Henwood, Doug. After the New Economy. New York: New Press, 2003. 
Marcus, James. Amazonia. New York: New Press, 2004. 

Stross, Randall E. eBoys: The First Inside Account of Venture Capitalists at Work. New York: Crown Business, 2000. 
Dow Jones Industrial Average
 
The Dow Jones Industrial Average (DJIA) is one of the most widely reported and monitored stock market
indicators in the world. An index of the stock value of thirty major U.S. corporations, its goal is to broadly
represent the performance of the stock market by measuring the performance of companies with established track
records that are dominant players in their respective industries. As of July 2009, a majority of the DJIA 30 were
companies that manufactured and sold industrial or consumer goods ranging from construction equipment to
household goods and pharmaceuticals, with the rest in industries including financial services, information
technology, and entertainment. Although other indexes are better measures of the stock market, the DJIA is
influential simply because it is so widely reported and closely watched. While economists dismiss fluctuations in
the DJIA in assessing the overall health of the economy, broad trends in the index may indicate future corporate
profitability.
The ancestor of the modern Dow Jones index was first published in 1884 by Wall Street Journal founder Charles
H. Dow and consisted of eleven companies, nine of which were railroads. In 1896, the first version of what is now
known as the Dow Jones Industrial Average debuted with twelve companies listed by Charles Dow and statistician
Edward Jones. It grew to twenty companies in 1916 and expanded to thirty in 1928. To pursue its objective of
measuring the average performance of the domestic stock market, the list has undergone more than two dozen
revisions since 1928 (not counting company name changes) in response to shifts in the national economy or the
ranks of corporate giants. Indeed, of the original twelve companies listed on the 1896 DJIA, only General Electric
is still included today. Changes to the index are made by the editors of the Wall Street Journal when something
happens to one of the component companies (if it is acquired by another company, for example) or when the
change would reflect a shift in the nontransportation, nonutility profile of the economy. Recent changes include the
removal of troubled financial services company Citigroup and bankrupt automobile manufacturer General Motors,
both in 2009, and their replacement by the computer networking and services provider Cisco Systems and the
insurance giant Travelers.
 Component Companies of the Dow Jones Indutrial Average, November 2011 
Company
Ticker Symbol
3M
MMM
Alcoa
AA
American Express
AXP
AT&T
T
Bank of America
BAC
Boeing
BA
Caterpillar
CAT
Chevron Corporation
CVX
Cisco Systems
CSCO

Coca-Cola
KO
DuPont
DD
Exxon Mobil
XOM
General Electric
GE
Hewlett-Packard
HPQ
The Home Depot
HD
IBM
IBM
Intel
INTC
Johnson & Johnson
JNJ
JPMorgan Chase
JPM
Kraft Foods
KFT
McDonald’s
MCD
Merck
MRK
Microsoft
MSFT
Pfizer
PFE
Procter & Gamble
PG
Travelers
TRV
United Technologies
UTX
Verizon Communications
VZ
Wal-Mart
WMT
Walt Disney
DIS
Other stock indices, such as the Standard & Poor’s 500 and the New York Stock Exchange, index track more
stocks than the thirty in the DJIA. In addition, these and other stock indices take into account the relative size of
corporations rather than simply measuring the value of one share as in the DJIA. Thus, based on February 2009
figures, components Kraft Foods and JPMorgan Chase would have roughly the same weight in the DJIA (both
share prices were in the low $20s) even though Kraft’s market cap ($34 billion) was less than half of JPMorgan’s
($75 billion). As a result, most financial studies do not rely on the DJIA—even though it remains the most widely
watched indicator of stock market performance.
One of the reasons for its ongoing status is that adjustments to the calculations underlying the DJIA have
remained consistent and can be interpreted over more than a century of existence. When a new company
replaces an old one, the price differential between the two would cause the overall index to fluctuate, even though
no market changes had occurred. Adjustments to the denominator have also been made when company stock
prices change because of stock splits, mergers and acquisitions, large dividends, or other events that change
share price but do not reflect market valuation of those shares.
 Greatest One-Day Gains and Losses, Dow Jones Industrial Average 
Measure/Date
Percentage Change
Point Change
Gain by percentage
   1. March 15, 1933
15.34
8.26
   2. October 6, 1931
14.87
12.86

   3. October 30, 1929
12.34
28.40
Gain in points
   1. October 13, 2008
11.80
936.42
   2. October 28, 2008
10.88
889.35
   3. November 13, 2008
6.67
552.59
Loss by percentage
   1. October 19, 1987
22.61
508.00
   2. October 28, 1929
12.82
38.33
   3. October 29, 1929
11.73
30.57
Loss in points
   1. September 29, 2008
6.98
777.68
   2. October 15, 2008
7.87
733.08
   3. September 17, 2001
7.13
684.81
Over the years, the DJIA has provided a useful if imprecise reflection of the overall health or weakness of U.S.
corporations, though its fluctuations tend to be greater than those of overall corporate well-being. This is because
the share price of a specific stock reflects less the underlying value of a company than perceptions of that value,
which are subject to speculation. In addition, stock prices tend to reflect investor predictions about a company’s
future prospects, which by definition is a more speculative enterprise. Thus, for example, while overall U.S.
industrial output fell by about 50 percent between its peak in mid-1929 and its trough in mid-1932, the DJIA fell by
nearly 90 percent. Or, to take a more recent example, the 13.3 percent decline in U.S. industrial output from the
beginning of the most recent recession in December 2007 to April 2009 was accompanied by a nearly 50 percent
drop in the DJIA.
John J. Neumann and James Ciment
 
See also:  New York Stock Exchange. 
Further Reading
 Bodie, Z., A. Kane, and A. Marcus.  Essentials of Investments.  Boston:  McGraw-Hill/Irwin,  2008. 
 Dow Jones Indexes:  www.djaverages.com
 Shiller, Robert J.  Irrational Exuberance.  Princeton, NJ:  Princeton University Press,  2005. 
Duesenberry, James (1918–2009)
 
James Stemble Duesenberry was a professor of economics at Harvard University from 1955 to 1989 whose most
significant contribution to the field was the relative income hypothesis, with which many economists were
uncomfortable. He served on President Lyndon Johnson’s Council of Economic Advisers, and subsequently was

chairman of the Federal Reserve Bank of Boston.
Duesenberry was born on July 18, 1918, in West Virginia. He earned bachelor’s (1939), master’s (1941), and—
after serving in the Air Force during World War II—doctorate (1948) degrees from the University of Michigan. His
doctoral thesis, published as Income, Saving, and the Theory of Consumer Behavior (1949), was an important
contribution to the Keynesian analysis of income and employment. Duesenberry taught at MIT (1946) and at
Harvard (1955–1989). While at Harvard, he chaired the Department of Economics from 1972 to 1977. He served
on the President’s Council of Economic Advisers from 1966 to 1968, along with Otto Eckstien, with whom he had
collaborated on a 1960 article in Econometrica about recession; and, from 1969 to 1974 he was chairman of the
board of the Boston Fed. Following his retirement from Harvard, he continued to serve as a consultant at the
Harvard Institute for International Development.
While such economists as Milton Friedman and John Maynard Keynes became familiar to many people in the
twentieth century, Duesenberry was known mainly among other economists for his writings on economic concepts
and theories. The field of economics since the 1920s, and especially since the Great Depression, had aspired to
be the hardest, or most scientific, of the social sciences. It was the first field to integrate game theory, and it
aimed to remain distinct from such “soft” sciences as psychology and sociology, which examine motive and other
intangible forces to explain behavior. Even John Maynard Keynes’s theories were modified by more scientific
principles as his followers deemphasized what Keynes viewed as irrational “animal spirits” that caused consumers
and investors to make decisions based on emotional, rather than purely rational, impulses.
But Duesenberry’s relative income hypothesis deals specifically with motivation. In the 1960s—a decade during
which the “Other America” and the Great Society gained prominence and the issues of poverty and the
disenfranchisement of minorities were the focus of much public attention—sociologists examined the effects of
income inequality. Rises in crime, for instance, were predicted to occur when the economically disadvantaged
were socialized to desire the same material things and lifestyles as those with more money, and this socialization
was seen to be encouraged partly by television and the national media.
Similarly, in Duesenberry’s view, attitudes toward purchasing and saving were influenced by decision makers’
perceptions of others’—and their own recent—standards of living. He believed that even when individuals’ needs
could be met by spending less, they continued to spend more than necessary—even buying on credit—because
they aspired to a more affluent lifestyle. Duesenberry sought to explain why spending habits tended not to react
“rationally” to recessions, and why the affluent were more likely to save money than the less affluent (who would
benefit more from saving).
The similarity of Duesenberry’s relative income hypothesis to sociological approaches is apparent in his language;
he refers to a “demonstration effect” (a term common in sociology and political science) in the relationship
between the consumption behavior of the wealthy and the awareness of such behavior by the less affluent and
their subsequent attempts to emulate it. (There is a similar effect when children mimic adult behavior through such
make-believe play as “house” or “doctor,” or by setting up “businesses” such as lemonade stands.) Such
observations seem to place Duesenberry more in the annals of sociology than economics.
Friedman’s permanent income hypothesis became the canonical theory of consumption, displacing, although not
entirely replacing, Duesenberry’s. Extensions of Duesenberry’s theory have been used in the globalist era to
describe intangible economic relationships between developed (economically advantaged) and developing
(economically disadvantaged) nations, and the macroeconomic behavior of both.
Bill Kte’pi
 
See also:  Eckstein, Otto;  Friedman, Milton. 
Further Reading

Duesenberry, James S. Income, Saving, and the Theory of Consumer Behavior. Cambridge, MA: Harvard University
Press, 1962. 
Frank, Robert H.  “The Mysterious Disappearance of James Duesenberry.” New York Times, June 9, 2005. 
Gilpin, Robert. Global Political Economy: Understanding the International Economic Order. Princeton, NJ: Princeton
University Press, 2001. 
Mayer, Thomas, James S. Duesenberry, and Robert Z. Aliber. Money, Banking, and the Economy. New York: W.W.
Norton, 1996. 
 
Eastern Europe
 
Eastern Europe is a region made up of countries that were, for much of the post–World War II period, organized
under communist-type economic and political systems. These include Albania, Bulgaria, the Czech Republic,
Hungary, Poland, Romania, Slovakia, and the states of the former Yugoslavia (Bosnia, Croatia, Macedonia,
Montenegro, Serbia, and Slovenia). The former eastern half of Germany, once known as the German Democratic
Republic, or East Germany, was also part of this region. (East Germany is discussed in the article “Germany.”) In
addition, the three former Soviet Baltic states of Estonia, Latvia, and Lithuania are also usually included (although
in this encyclopedia they are discussed separately in the article “Baltic Tigers.”) More debatable is the inclusion of
the other former Soviet republics located on the eastern fringes of Europe: Belarus, Moldova, and Ukraine, though
they are included in the discussion here.
With the collapse of both the Soviet Union and Soviet-backed regimes in the late 1980s and early 1990s, all of
these countries, with the exception of several former Yugoslavian republics, transitioned peacefully from
communist to free-market systems in the early 1990s, though in most cases with enormous economic dislocations.
While levels of economic development vary widely among the different Eastern European states, the region as a
whole lags significantly behind Western Europe in terms of gross domestic product (GDP) per capita and other key
economic indicators to the present day.

Members of the Hungarian Trade Union Association gather in front of parliament in downtown Budapest to protest
government economic policy in 2009. Hungary and other Eastern European nations were among the hardest hit by
the global financial crisis. (Attila Kisbenedek/AFP/Getty Images)
 Origins of Eastern Europe’s Economic Lag
While virtually all economists agree that the communist-inspired, command-style economic systems stunted
economic development in the region—especially when compared to the mixed market, capitalist-socialist hybrid
systems of Western Europe—the origins of Eastern Europe’s relative economic backwardness go back to at least
the Middle Ages, when instead of gradually winning new freedoms, as in Western Europe, peasants in Eastern
Europe found themselves under new landlord-driven restrictions that resulted in serfdom or near-serfdom in much
of the region. The enhanced power of the landlords stunted the growth of an innovative merchant class while
slowing the development of cities, both keys to industrialization. While several Eastern Europe countries had
thrown off many of the restrictions by the nineteenth century and begun to industrialize, the damage, say
historians, was already done. As late as the 1930s, on the eve of World War II and the subsequent subjugation to
Soviet power after the war, the economies of Eastern Europe lagged significantly behind those of Western
Europe.
World War II inflicted further blows on the region in several critical ways. First, the war heavily damaged the
economic infrastructure of Eastern Europe, particularly in the Soviet Union, while the Nazi occupying forces
rounded up hundreds of thousands of Eastern Europeans, most notably Jews, for slave labor and for
extermination in death camps. Besides being crimes against humanity, such practices resulted in an incalculable
loss in economic productivity, as much factory production went to servicing the German war machine rather than
creating economic growth in the region.
Causing longer-term economic damage, say economists, was the legacy of the war. As the Soviet Red Army
marched into the region, pushing the German army out, it imposed pro-Soviet, communist governments across the
region. Only in Yugoslavia (and Albania), which had large and well-organized anti-Nazi resistance forces during
the war, were the Soviets unable to impose their will. Still, both countries chose to adopt the same centrally
controlled, command-style economic systems, though Yugoslavia’s did allow for a modicum of market forces to
operate.

 Communist Era
Whether under Soviet-controlled regimes or independent communist ones, the economic development of Eastern
European countries followed similar trajectories, as various forms of the Soviet prewar economic system were
imported into the region. Economic planning was instituted, usually through adoption of what were often known as
Five Year Plans. Economic experts and political appointees, working for the central government, developed long-
term blueprints for where investment was to be made. The emphasis was on heavy industry, which, in itself, was
not a bad idea, given how Eastern Europe had often lagged in this sector, and given the enormous destruction to
what infrastructure it did have during World War II.
Meanwhile, agriculture was collectivized, though in some countries, such as Poland, much land remained in
private hands. But whatever the case, relatively few resources were allocated to the agricultural sector. Similarly,
there was little emphasis on consumer goods, particularly in the early postwar decades.
As most noncommunist economists point out—particularly those of the Austrian school—such command-style
economics fail because they dispense with normal market forces in the allocation of resources. That is, economic
authorities set goals for final output, then try to find the economic resources to meet those goals. But such
centralized planning and direction is not an effective substitute for such market forces as profits and prices in
allocating resources and motivating firms to become more efficient or meet the needs of consumers. And with few
consumer goods available, workers lack the motivation to be particularly productive. Eventually, economic planners
in various Eastern European countries tried to make adjustments, by allocating more resources to consumer
goods. But again, the command-style system did not find a particularly efficient means for making sure the right
goods to meet demand were being made in sufficient quantities, or at all.
Also setting back postwar Eastern European economic development were geopolitical factors. Fearing that too
much economic integration with Western Europe would undermine their control, the Soviet Union and pro-Soviet
regimes in much of Eastern Europe severed traditional trade ties between the two halves of the continent. In
addition, the Soviet Union spurned any cooperation with the U.S. Marshall Plan of the late 1940s, a massive
injection of U.S. capital that did much to lay the foundations for Western Europe’s economic “miracle” of the 1950s
and 1960s.
Thus, by the end of the communist era in the late 1980s, Eastern European economies consistently lagged behind
those of the West, though some, particularly in Central Europe and the Baltic republics of the Soviet Union, did
achieve a certain degree of economic prosperity. In general, GDP per capita in communist Eastern Europe was
roughly one-fourth that of Western Europe, though with much variation on both sides of what was then known as
the “Iron Curtain.”
 Fall of Communism and Market Reforms
While some Eastern European countries were beginning to experiment with the introduction of market forces
before the fall of communism, the collapse of communist regimes in Eastern Europe and the Soviet Union
between 1989 and 1991 greatly accelerated the process. Meanwhile, the fall of communism in Eastern Europe
coincided with a move toward more market forces among the stagnating hybrid economies of Western Europe and
a new consensus in the West that less government interference in the economy was best for economic growth.
To varying degrees of speed and thoroughness, every postcommunist Eastern European regime began freeing
prices from centralized control, turning over state enterprises to private owners, and attempting to strengthen their
currencies and balance their budgets, often taking advice from strongly pro–free-market advisers from the United
States and other countries in the West.
The impact of these market reforms on the lives of ordinary Eastern Europeans was often harsh. While a few well-
connected individuals—often members of the old communist leadership—were able to buy up state enterprises at
fire sale prices, most citizens faced unemployment, rising prices, and a deteriorating social safety network. With

access to superior Western-made goods for the first time, many Eastern Europeans spurned local products,
leading to the closing of factories, many of which were inefficient and technologically backward after years of
operating in a command-style economy.
By the late 1990s to early 2000s, however, many of the more advanced Eastern European economies had
recovered from the shock of transitioning to free-market mechanisms of resource allocation. Meanwhile, Western
European companies began investing heavily in existing Eastern European firms or in setting up their own
operations in Eastern Europe, both to gain access to consumers with rising incomes and to take advantage of
relatively low-cost yet highly educated workers. Indeed, during this period, real GDP growth in Eastern Europe
usually outpaced that of Western Europe by a significant margin. Between 2002 and 2006, for example, the more
advanced Eastern European economies experienced real GDP growth rates averaging about 5 percent while the
core countries of the European Union (that is, not including those Eastern European countries that had joined
during this period) averaged less than 2 percent GDP growth per annum.
Meanwhile, as the accession of many of these countries to the European Union and World Trade Organization
indicated, Eastern European countries had turned away from the former Soviet economic orbit and joined the
globalized economy. But while market reforms produced substantive growth and the new openness to global trade
improved standards of living, it also exposed the region to fluctuations in the global financial system.
During the boom years of the late 1990s through mid-2000s, many Eastern European countries borrowed heavily
from Western banks to help develop their economies and provide a rising standard of living for their citizens. In
doing so, they were aided by the loose credit policies of Western financial institutions eager to earn profits in
rapidly growing emerging markets. This inflow of capital prompted much speculation in stocks, particularly in the
housing sector, leading to price bubbles.
When the credit crisis in the global financial markets emerged and intensified between 2007 and 2009, many
Eastern European banks and companies found themselves unable to obtain new loans to service their existing
debts to Western financial institutions. The liquidity crisis sent the value of local currencies plunging against the
euro. Many homeowners, having taken out mortgages with Western financial institutions in order to get lower
interest rates, were forced into foreclosure.
The problems in Eastern Europe’s liquidity crisis were not confined to that region, of course, since Western
European—and some U.S.—financial institutions found themselves exposed to massive amounts of bad loans and
toxic securities assets originating in Eastern Europe. Meanwhile, the European Central Bank decided in early 2009
not to provide a massive bailout to troubled Eastern European financial institutions, as had the Treasury
Department to institutions in the United States, but instead to analyze each institution’s need on a case-by-case
basis.
James Ciment
 
See also:  Baltic Tigers;  Emerging Markets;  Russia and the Soviet Union;  Transition
Economies. 
Further Reading
Aligica, Paul Dragos, and Anthony J. Evans. The Neoliberal Revolution in Eastern Europe: Economic Ideas in the Transition
from Communism. New York: Edward Elgar, 2009. 
Gros, Daniel, and Alfred Steinherr. Economic Transition in Central and Eastern Europe: Planting the Seeds.  2nd ed. New
York: Cambridge University Press, 2004. 
Lane, David, ed. The Transformation of State Socialism: System Change, Capitalism or Something Else? New
York: Palgrave Macmillan, 2007. 

Roche, David.  “Eastern Europe and the Financial Crisis.” Wall Street Journal, March 28, 2009. 
Wagener, Hans-Jürgen. Economic Thought in Communist and Post-Communist Europe. New York: Routledge, 1998. 
Echo Bubble
 
An echo, or secondary, bubble is a smaller speculative bubble that follows a major financial bubble that has burst.
These secondary bubbles behave as aftershocks within the same financial markets. Echo bubbles often result from
the conditions that created the original bubble, such as excessive speculation in real estate, new issues of stock,
or derivatives. However, an echo bubble also may be stimulated by the policies used to counter the effects of the
original bubble. For example, a central bank might increase the supply of money and credit or maintain a low
interest-rate policy in order to stabilize financial markets after a bubble bursts. Such policies, however, might
increase liquidity in financial markets, spurring a new speculative surge in the same markets. This secondary
bubble “echoes” the original bubble, but often with a lower volume of speculation.
The most famous example of an echo bubble occurred in 1930. Between 1927 and 1929, corporate securities in
the United States experienced one of the greatest speculative bubbles in history, with the Dow Jones Industrial
Average soaring from about 160 to more than 380. This rapid growth came to an end in October 1929 when the
stock market crashed, and the Dow Jones fell below 200 in a matter of days. A variety of factors, including bullish
talk by leading financial experts, along with a cut in the Federal Reserve’s interest rate, touched off a new
speculative rally, driving the Dow Jones back up 50 percent to nearly 300 in the first two quarters of 1930. But the
rally proved short-lived. Ultimately, the Dow Jones would hit bottom at just over 40 in mid-1932.
The discovery of echo bubbles resulted from the laboratory work in economics of Nobel laureate Vernon Smith. He
conducted experiments that simulated financial market decision-making as early as the 1960s. Smith’s original
experiments were conducted in the classroom, but then were extended to groups of financial market professionals.
Based on his experiments, Smith was able to identify the existence of echo bubble patterns. An echo bubble
expands before it bursts. Financial professionals analyze the recovery periods in stock markets after a crash or a
bust in terms of major adjustments that occur within financial markets. During some recoveries, stock markets
experience “corrections.” If the markets are moving toward a secondary bubble, the correction stops the echo
bubble. In this context, stock market corrections indicate that the recovery is unsustainable. There is a direct
connection between stock market corrections and the end of an echo bubble.
The existence of an echo bubble indicates that the experience from a major speculative bubble’s failure has not
changed the behavior of financial market participants. Thus, during an echo bubble, investors follow the same
patterns of behavior as in the original bubble. In other words, the experience of the original bubble does not alter
the speculative motivations in financial markets; as a result, a correction will take place, causing the echo bubble
to burst. It is as if a second round of irrational exuberance must take place before the speculative drive in financial
markets can come to an end. In experimental economics, these secondary bubbles show up about half of the time
in the aftermath of a major speculative bubble. Empirical evidence from financial markets to support the theory of
echo bubbles is more limited, but professional observations regarding the need for “market corrections” after a
major bubble support the existence of echo bubbles.
The speculative behavior reflected in echo bubbles suggests that efficient market theory, the standard model of
finance, is flawed. This model assumes that individuals behave rationally, and that they use all available
information to form expectations about financial prices. Thus, financial prices reflect all available information.

However, echo bubbles indicate that speculative behavior is driven by overly optimistic expectations of future
prices of specific financial assets. The historical experience of major speculative bubbles and their collapse does
not necessarily change the speculative motivation in financial markets. Instead of a readjustment of expectations,
the underlying pattern of speculation carries over into a new bubble period. This supports the observation made by
John Maynard Keynes that there are extended periods of time when “animal instincts”—that is, emotions—
overwhelm rational calculation in investment decisions. This is also consistent with Hyman Minsky’s theory that
financial bubbles—and the underlying behaviors that create them—are an inherent part of financial markets.
William Ganley
 
See also:  Asset-Price Bubble. 
Further Reading
Ball, Sheryl, and Charles Holt.  “Speculation and Bubbles in an Asset Market.” Journal of Economic Perspectives 12:1
(Winter 1998): 207–218. 
Durlauf, Steven N., and Lawrence E. Blume. Behavioural and Experimental Economics. Hampshire, UK: Palgrave
Macmillan, 2010. 
Foster, John Bellamy, and Fred Magdoff. The Great Financial Crisis: Causes and Consequences. New York: Monthly
Review, 2009. 
Galbraith, John Kenneth. A Short History of Financial Euphoria. New York: Penguin, 1990. 
Garber, Peter M. Famous First Bubbles: The Fundamentals of Early Manias. Cambridge, MA: MIT Press, 2000. 
Kindleberger, Charles P. Manias, Panics and Crashes: A History of Financial Crises.  3rd ed. Hoboken, NJ: John Wiley and
Sons, 1996. 
Magdoff, Fred, and Michael Yates. The ABCs of the Financial Crisis. New York: Monthly Review, 2009. 
Smith, Vernon.  “The Effect of Market Organization on Competitive Equilibrium.” Quarterly Journal of Economics 78:2 (Fall
1964): 181–201. 
Smith, Vernon.  “Experimental Methods in Economics.” In Behavioural and Experimental Economics, ed. Steven M. Durlauf
and Lawrence E. Blume. Hampshire, UK: Palgrave Macmillan, 2010. 
Eckstein, Otto (1927–1984)
 
Harvard economist and professor Otto Eckstein introduced the concept of core inflation and developed large-scale
macroeconometric models, or measurements of macroeconomic data. He was a member of the President’s
Council of Economic Advisers during Lyndon Johnson’s first full term (1964–1968) and, in 1969, a co-founder with
businessman Donald Marron of Data Resources Inc. (DRI), which became the world’s largest nongovernmental
supplier of economic data.
Eckstein was born on August 1, 1927, in Ulm, Germany. His family fled Germany in 1938 and settled in the
United States. Eckstein graduated in 1946 from Stuyvesant High School in New York City, received a bachelor of
arts degree in economics from Princeton University in 1951, and master’s (1952) and PhD (1955) degrees from
Harvard University, where he served as a professor from 1955 to 1984 (he died on March 22 of that year). From

1957 to 1966, he was a consultant to the Rand Corporation, a think tank.
Eckstein’s work focused principally on macroeconometrics. His concept of “core inflation” became especially
important in American economics in the 1960s and 1970s, when inflation was a serious concern. Core inflation is
an inflation metric that excludes from consideration those items that are especially volatile, such as food and
energy. Energy prices—particularly that of oil—are subject to volatility independent of the factors that drive the
inflation of other prices, and can often go up or down even when the prices of other goods are moving in the
opposite direction. Seasonal food prices are subject to volatility as a result of climate issues, and usually go up
when energy prices do because of transportation costs. The official measure of core inflation used by the Federal
Reserve since 2000 is based on the core personal consumption expenditures price index (PCEPI, which includes
data from the consumer price index and the producer price index), a switch from the previously used consumer
price index (CPI). Unlike the CPI, the PCEPI accounts for substitution; in other words when the price of a
particular good goes up, consumers often buy a different good instead. Thus the PCEPI is less affected than the
CPI by volatile price shocks. The switch from CPI to PCEPI, occurring sixteen years after Eckstein’s death,
reflected the issues he worked on. Although inflation may seem simple to understand, the question of how to
measure it, what to measure, what data to consider, and what time period to measure, is complicated and subject
to frequent revision.
Bill Kte’pi
 
See also:  Council of Economic Advisers, U.S.;  Deflation;  Inflation. 
Further Reading
Eckstein, Otto. The Great Recession. Amsterdam, NY: North-Holland, 1978. 
Eckstein, Otto. Industry Price Equations. Cambridge, MA: Harvard Institute of Economic Research, 1971. 
Wilson, Thomas A.  “Otto Eckstein: Applied Economist Par Excellence.” Review of Economics and Statistics 66:4
(1984): 531–536. 
Effective Demand
 
During the first half of the twentieth century, the concept of effective demand was invoked by economic theorists—
in particular the great John Maynard Keynes—as an explanation of the extended economic depression of the
1930s.
The term “effective demand” has two different meanings, one related to microeconomics (the study of economic
decision-making at the level of individual households, firms, and markets), and the other to macroeconomics (the
study of the performance of the economy as a whole). The concept of effective demand in microeconomics is
concerned with two ideas: (1) the consumer’s or producer’s willingness to pay; and (2) the consumer’s or
producer’s ability to pay for a good or service at various prices, with all other factors affecting demand held
constant.
Effective demand starts with the consumer’s willingness to pay, and then identifies potential demand that cannot
be realized due to insufficient income. The term is widely used in development economics to describe the inability

of households, small businesses, and farmers in less developed economies to grow because of constraints on
demand by low earning potential.
The macroeconomic concept of effective demand has its origins in classical economic literature, most notably
Thomas Malthus’s Principles of Political Economy (1820). Although Malthus is more known for his theories of
population, his writings in Principles of Political Economy about the lack of “effectual demand” laid groundwork for
Keynes’s pathbreaking work, The General Theory of Employment, Interest and Money, more than a century later,
in 1935.
Malthus was concerned about the long-term implications for individuals and families in a capitalist society. His
analysis demonstrated that a capitalist economy can indeed grow due to improved access to resources and the
application of improved technology. However, Malthus noted, the lack of “effectual” or sufficient demand—now
referred to as effective demand—places limits on economic growth. Malthus argued that the purchasing power of
the “large body of very poor workmen” and the luxury spending of “a comparable small body of very rich
proprietors” would be insufficient to employ the expanding workforce, thus creating unemployment and misery.
Malthus’s ideas were in direct opposition to those of the classical economic school, led by David Ricardo, which
held that unemployment is impossible in a properly functioning, nonregulated economy. Following the work of
Jean-Baptiste Say, the classical economists believed that the act of production generated an equal amount of
spending on consumption and investment. This basic principle—that supply creates its own demand—became
known as Say’s law or Say’s identity. According to that proposition, only workers who are unwilling to work at the
going wage will be unemployed.
Keynes criticized classical Ricardian theory as overly “optimistic” and not in tune with reality. Confirmation, at least
in Keynes’s view, came in the form of the Great Depression of the 1930s, in the midst of which he wrote his
General Theory. Classical economists, meanwhile, continued to maintain that the capitalist economies of the
Western world would “self-correct” and eventually return to full employment.
According to Keynes, the essential fact that was not being considered by the classical adherents to Ricardo was
that a capitalist economy can in fact remain at a stagnant equilibrium with a high rate of unemployment for a
substantial period of time. Thus, Keynes’s key theoretical innovation can be understood as a deepening of
Malthus’s concept of effective demand. According to the model developed by Keynes, the plans of business
enterprises to produce and invest—which, according to classical economic rules, should lead to a correspondingly
higher level of consumption—may in fact be insufficient to employ all workers who wish to be employed at the
going wage. In considering the full impact of effective demand, Keynes maintained that market forces in this
situation do not necessarily budge the economy from an underemployed state. “[T]he paradox of poverty in the
midst of plenty,” he wrote, is a consequence of “the mere existence of an insufficiency of effective demand [that]
may, and often will, bring the increase of employment to a standstill before a level of full employment has been
reached.”
Keynes’s work led to the adoption of discretionary fiscal policy by Western governments during the Great
Depression to make up for the deficiency of aggregate (or effective) demand; this resulted in the creation of public
works spending in economic downturns and social safety nets such as social security systems and unemployment
insurance. The success of these economic and social policies, the expansion of economic activity, and the
improving living standards for the balance of the twentieth century has largely muted the interest in insufficient
effective demand. The recession and economic crisis of 2007–2009, however, sparked new interest in some
circles as debt-ridden and unemployed consumers cut back on spending, reducing demand and employment in
the process.
Derek Bjonback
 
See also:  Confidence, Consumer and Business;  Consumption;  Keynesian Business Model. 

Further Reading
Hartwig, Jochen.  “Keynes Versus the Post Keynesians on the Principle of Effective Demand.” Journal of the History of
Economic Thought 14:4 (December 2007): 725–739. 
Keynes, John Maynard. The General Theory of Employment, Interest and Money. New York: Harcourt, Brace, and
World, 1935. 
Malthus, Robert. The Principles of Political Economy.  2 vols., reprint. Cambridge and New York: Cambridge University
Press, 2008. 
Schumpeter, Joseph. History of Economic Analysis. New York: Oxford University Press, 1954. 
Efficient Market Theory
 
Efficient market theory holds that in an efficient stock market, prices reflect all available information. In other
words, price changes occur only as a result of new information entering the market. This theory classifies markets
into three levels of efficiency: strong-form efficiency, semi-efficiency, and weak-form efficiency. In the strong-form
efficient market, prices reflect all information, including public and nonpublic information. In the semi-efficient
market, prices reflect all available public information. In a weak-form efficient market, prices reflect only past
security prices.
 Origins
Efficient market theory first was expressed in the doctoral dissertation of French mathematician Louis Bachelier,
written in 1900. Bachelier concluded that stock market prices are unpredictable, but his thesis went largely
unnoticed until the 1960s, when the “random walk” theory of stock market prices became popular. This theory
posits that stock prices are random and that past prices cannot be used to predict future prices. Efficient market
theory evolved from this concept. Economists associated with the efficient market theory school of thought include
Milton Friedman, Eugene Fama, and George Stigler.
Classical economics turned on the belief that encouraging the pursuit of economic self-interest was the most
favorable approach to government economic policy, allowing the so-called invisible hand of the market to work
unimpeded by regulation. Neoclassical economists sought to make the invisible hand visible in the form of efficient
market theory. In academia, the difference between the school of thought advanced by the neoclassicists and that
of the classical economists was referred to as the “freshwater versus saltwater” debate. This was a reference to
the fact that many efficient market theorists were concentrated inland—most notably at the University of Chicago—
while the classic economists were concentrated on the coasts.
 Legal Implications for Corporate Governance
Neoclassical economics spread deep into law faculties, especially at the University of Chicago, giving this school
of thought the moniker “Chicago school of law and economics.” The professors who were attracted to this
movement sought to induce a paradigm shift that would use efficient market theory to guide the regulation of
financial markets. They believed this theory provided a sound basis to support their argument that markets are
largely self-policing, thus reducing the need for government regulation.

Efficient market theory also was applied to corporate governance. Neoclassical economists argued that corporate
managers should have incentives to align their interests with those of stockholders. This resulted in the
widespread use of stock options as compensation for managers. These professors argued that the corporation
was really a “nexus of contracts” that could be negotiated with each party’s best interests in mind. Leaders in the
Chicago school on matters involving corporate governance included Richard A. Posner and Frank H. Easterbrook,
both of whom are now federal appellate court judges, and the former dean of the University of Chicago Law
School, Daniel R. Fischel.
These legal theorists argued that market forces ensure that managers do not overreach in negotiating their
contracts with shareholders—otherwise, no one would buy the company’s stock. Under the contractual theory,
shareholders are assured only of the rights they might have under a contract with the corporation. The courts
would not be called on to create rights that the parties themselves did not establish by contract. This theory of
corporate governance proved to be highly controversial. Traditional legal theorists applied fiduciary duties
(borrowed from trust law) that required corporate managers to act in good faith, use due care in making decisions,
and avoid conflicts of interest that would undermine their duty of loyalty to the firm. The neoclassicists believed
that the application of fiduciary duties to managers was just an “off the rack” guess by judges about what the
parties would have agreed to had they thought to contract over the matter at issue. Instead, they argued that the
focus should be the presumed intentions of the parties to any particular dispute.
Supporters of the Chicago school contended that fiduciary duties are created and applied by judges with no
knowledge or experience in business. They pointed out that bondholders are protected only by contract terms, and
not by fiduciary duties. Why should shareholders have any greater rights or fare any better than creditors? Critics
of fiduciary duties also pointed to what they considered a particularly egregious decision, in which the Delaware
Supreme Court (Smith v. Van Gorkom, 488 A.2d 858 [1985]) ruled that a board of directors had breached its
fiduciary duty of care in too hastily approving the sale of a public company. However, the board members were
highly experienced, they had been considering selling the company for some time, and the sale price was at a
substantial premium. The Delaware legislature responded to that decision by passing a statute allowing Delaware
corporations to waive the duty of care. Delaware is the favorite place of incorporation for large corporations, and
most of those businesses obtained shareholder approval for such a waiver.
Efficient market theory reached its apex when the U.S. Supreme Court adopted a “fraud-on-the-market” theory
(Basic Inc. v. Levinson, 485 U.S. 224 [1988]) that relieved plaintiffs of the burden of showing reliance on false
statements in company press releases. Now, plaintiffs need not have read or heard the information claimed to be
false. Rather, reliance is presumed because, in an efficient market, the price of a stock reflects all available
information, including the information that is alleged to be false. The effect of the false information is transmitted
through changes in the price of the stock.
The fraud-on-the-market theory presumes that the market is efficient and that the market does not discount false
statements. That presumption was challenged by Justice Byron White, who pointed out that, “while the
economists’ theories which underpin the fraud-on-the-market presumption may have appeal of mathematical
exactitude and scientific certainty, they are, in the end, nothing more than theories which may or may not prove
accurate on further consideration” (485 U.S. at 254). He noted that the fraud-on-the-market theory is at odds with
federal policy because it seeks market discipline instead of government regulation and that “some of the same
voices calling for acceptance of the fraud-on-the-market theory also favor dismantling the federal scheme which
mandates disclosure” (485 U.S. at 259).
Justice White was right to be concerned. The efficient market hypothesis was undermined by the market
breakdown during the stock market crash of 1987. That market event did not appear to be the result of any new
information that would have justified such a sharp market contraction. Instead, economists identified something
called “noise trading”—a concession that some trading in the market is irrational and uninformed, reducing market
efficiency and creating unpredictable volatility. By the mid-1990s, efficient market theory was being attacked from
all quarters as overly simplistic and even wrong. Even its strongest proponents acknowledged that the theory was
riven with flaws. Eugene Fama, the leading proponent of the efficient market hypothesis, was among those who

conceded its shortcomings.
One example of its failings was the movement to align the interests of corporate managers with those of
shareholders through stock options. This effort turned into a disaster after the Enron Corporation, WorldCom, and
numerous other public companies were found to have manipulated their accounts in order to boost stock prices
and garner massive bonuses through exercisings. Market efficiency was further called into question during the
financial crisis of 2008–2009, when markets worldwide proved to be remarkably inefficient.
A new school of “behavioral economics” led by Richard Thaler attacked the efficient market theory. This school of
thought argues that human behavior affects market prices, that such behavior is not always rational, and that
humans make systematic errors in judgment that affect prices. In 1999, economists Jon D. Hanson and Douglas A.
Kysar noted that although many jurists subscribed to a view of economic actors as purely logical and analytical,
scholars in other social science disciplines were coming to a very different conclusion. “Those scientists—cognitive
psychologists, behavioral researchers, probability theorists, and others—were discovering powerful evidence that
the rational actor model, upon which the law and economics project depends, is significantly flawed,” Hanson and
Kysar wrote. “In place of the rational actor model, those scientists were developing a human decision maker
model replete with heuristics and biases, unwarranted self-confidence, a notable ineptitude for probability, and a
host of other nonrational cognitive features.”
Jerry W. Markham
 
See also:  Consumer and Investor Protection;  Information Technology;  Neoclassical Theories
and Models. 
Further Reading
Cunningham, Lawrence A.  “From Random Walks to Chaotic Crashes: The Linear Genealogy of the Efficient Capital Market
Hypothesis.” George Washington Law Review 62(1994): 547–608. 
De Long, J. Bradford, Andrei Schleifer, Lawrence H. Summers, and Robert J. Waldman.  “Noise Trader Risk in Financial
Markets.” Journal of Political Economy 98:4 (August 1990): 703–738. 
Hanson, Jon D., and Douglas A. Kysar.  “Taking Behavioralism Seriously: The Problem of Market Manipulation.” New York
University Law Review 74(1999): 630–749. 
Malkiel, Burton G. A Random Walk Down Wall Street. New York: W.W. Norton, 1973. 
 
Emerging Markets
 
A term coined by World Bank economist Antoine van Agtmael in the 1980s, “emerging markets”—or emerging

economies—refers to developing world countries notable for their high economic growth rates in recent decades,
which places them in a kind of economic limbo between the developing and developed worlds. Most emerging
markets are located in Asia, Latin America, and the former communist bloc of Eastern Europe and the Soviet
Union, with a few cases in Africa and the Middle East as well. Large and high-profile members of the emerging
markets group include Brazil, Russia, India, and China—the so-called BRIC nations—but there are numerous
smaller ones as well. In addition, the grouping has changed over time, with most economists now placing the
emerging markets of the 1980s—such as the Asian Tigers (Hong Kong, Singapore, South Korea, and Taiwan)—
on the list of developed-world countries.
Leaders of the world’s top emerging economies—Brazil, Russia, India, and China (BRIC)—held their first official
summit in June 2009 in Yekaterinburg, Russia. Discussions focused on ways to increase their influence during the
global economic crisis. (Vladimir Rodionov/Stringer/AFP/Getty Images)
Developing world nations face a host of problems, from low educational attainment to widespread poverty to lack
of infrastructure. Many find themselves trapped in a vicious economic cycle—little education and few skills keep
income levels low, which results in an inadequate savings rate, which means low investment. With little
investment in capital goods, productivity also remains low, which in turn keeps income levels down—and the cycle
repeats.
 From Underdevelopment to Emerging Market Status
A country’s shift from underdevelopment to emerging market status can result from a host of factors, though all

emerging markets have one thing in common—they have managed to lift the rate of investment substantially and
have made sure to put a lot of resources into improving education and skill levels, as well as the overall health of
the populace. Poor health often impedes growth because it lowers productivity. In addition, emerging markets
have emphasized basic macroeconomic fundamentals, making sure domestic savings rates stay high and inflation
is kept in check, as the latter can undermine savings by forcing people to spend now rather than risk higher prices
later. In many cases, emerging markets have had the advantage of being able to adopt and imitate technologies
pioneered in the developed world.
Beyond these fundamentals, however, the path from underdevelopment to emerging market status can vary
greatly. In some countries, a valuable resource base is key, as in the case of hydrocarbons in the Persian Gulf
states. Of course, simply having a lot of oil is not enough to guarantee emerging market conditions. Nigeria has
long been one of the world’s largest producers of oil and yet its economy remains mired in underdevelopment.
In other cases, economic liberalization has helped achieve emerging market status. In the late 1970s and 1980s,
for example, Chile privatized much of its industry, lowered tariffs and other barriers to foreign investment, and saw
economic growth take off at one of the fastest rates in the world. The Asian Tigers followed a very different path
to rapid economic growth, maintaining punitive tariffs and other restrictions on imports and a high level of
government direction of the economy, though the free market was generally left on its own to create wealth. The
example of China presents yet another means of attaining emerging market status, taking a mixed economic
approach to development—government ownership of key industries, with local managers setting production and
marketing policies to meet demand.
 Foreign Investment
Whatever path they take, emerging markets are generally successful at luring foreign investment and capital,
critical in low-income countries. This is nothing new, of course. A key element in the emergence of the United
States as an industrial powerhouse in the nineteenth century was the high level of foreign—often British—capital.
In the case of the Persian Gulf states, bringing in foreign capital was often simply a function of selling the oil
beneath their feet. But in cases where natural resources are not enough to lift countries out of the vicious cycle of
underdevelopment, the key element in attracting foreign capital is the lure of high returns for overseas investment.
One important factor here is keeping corruption under control. If foreign investors believe the benefits of
investment are monopolized or stolen by insiders, they are unlikely to put their money into a country.
Even more important is the investment a country makes in a healthy and highly educated workforce and
adherence to macroeconomic fundamentals, since inflation can destroy the value of foreign investment. If all of
these things come together at once, the vicious cycle of underdevelopment turns into a virtuous cycle of rapid
economic growth. As investment levels increase, so, too, does productivity. And since wages often lag behind
productivity growth, profits increase, which lures in more foreign investment. As a country grows richer, it can
invest more in education and infrastructure, thereby increasing productivity, and the cycle repeats.
But foreign investment can also subject emerging markets to the vagaries of international finance. With
improvements in communications and technology from the 1980s onward, it became easier for investors in the
developed world to put their money into developing world economies as they were lured by the possibility of profit
levels unattainable in more mature economies. Poor countries, of course, were eager for such investment and
lowered barriers to it. From the late 1980s onward, for example, vast amounts of foreign capital were invested in
the emerging markets of East Asia, creating a speculative bubble in everything from housing to commercial
property to manufacturing capacity to stock market securities.
The presence of all of this foreign capital was premised on high growth rates, which assured high profits and
financial market stability. But foreign investors can be fickle. Any perception of instability or slowing growth rate will
cause them to pull their money out of high-risk emerging markets. This is exactly what happened in Thailand and
then much of East Asia during the so-called Asian financial crisis of 1997–1998. As foreign capital pulled out,
governments tried to bolster their own currencies—many were on fixed-rate exchange systems to attract foreign

capital in the first place—which further depleted their foreign currency reserves. Desperate, the governments then
turned to the International Monetary Fund (IMF) for loans. But the IMF required the various governments to
impose fiscal policies that led to economic contraction, causing more foreign capital to flee. Depending on the
country, it took several years for many of these emerging markets to return to positive economic growth.
The social and political ramifications of emerging market status tend to be more similar than dissimilar among
countries. The key outcome is the growth of an urban middle class, and in cases as diverse as Chile and South
Korea, the authoritarian governments that created the conditions for economic takeoff are pushed to cede
increasing political power to an educated and prosperous middle-class electorate. Whether or not emerging
market status always guarantees such an outcome, however, is being tested by the largest emerging market of all
—China.
James Ciment
 
See also:  Africa, Sub-Saharan;  Argentina;  Asian Financial Crisis (1997);  Baltic Tigers; 
Brazil;  BRIC (Brazil, Russia, India, China);  Central America;  Chile;  China;  Colombia;  Eastern
Europe;  India;  Latin America;  Mexico;  Middle East and North Africa;  Russia and the Soviet
Union;  South Africa;  Southeast Asia;  Transition Economies;  Turkey. 
Further Reading
Emerging Markets Monitor  www.emergingmarketsmonitor.com
Enderwick, Peter. Understanding Emerging Markets: China and India. New York: Routledge, 2007. 
Manzetti, Luigi. Neoliberalism, Accountability, and Reform Failures in Emerging Markets: Eastern Europe, Russia,
Argentina, and Chile in Comparative Perspective. University Park: Pennsylvania State University Press, 2009. 
Mavrotas, George, and Anthony Shorrocks, eds. Advancing Development: Core Themes in Global Economics. New
York: Palgrave Macmillan, 2007. 
Motamen-Samadian, Sima, ed. Capital Flows and Foreign Direct Investments in Emerging Markets. New York: Palgrave
Macmillan, 2005. 
 
 
 
 
Employment and Unemployment
 
Employment and unemployment are two distinct labor market states in which individuals of working age can find
themselves. In the case of the former, an individual holds a job and is said to be employed. In the case of the
latter, a person does not hold a job but is available for work and is actively looking for work. The sum of these
two measures of labor market participation constitutes the total labor force.

 Measurement
Labor market variables are measured by a household survey that national statistical agencies conduct monthly in
most industrialized countries based on a representative sample of the civilian noninstitutional working-age
population. In the United States, the Current Population Survey, which is conducted by the Census Bureau (on
behalf of the Bureau of Labor Statistics), surveys approximately 50,000 households each month to generate
estimates of labor market variables such as employment, unemployment, and labor force participation. The survey
covers a sample of the population that is sixteen years of age and older, excluding members of the armed forces
and inmates of institutions.
While the specifics of the sample and the definition of “working-age population” vary from country to country (e.g.,
in Canada, the survey samples about 50,000 households from a working-age population of individuals fifteen
years of age and older), most industrialized nations rely on household surveys rather than statistical information
based on records from unemployment insurance offices and employment centers to calculate their estimates of
monthly labor force statistics. This is because of possible statistical bias—for instance, an individual may have
exhausted his or her unemployment insurance benefits, and yet still would be considered unemployed according to
the household survey by virtue of his or her continued job search. This suggests that unemployed individuals
receiving assistance constitute only a portion of the actual total unemployed.
Job seekers scan the listings at an employment office in San Francisco in November 2009. The U.S.
unemployment rate reached 10.2 percent that month, the highest since 1983. (Justin Sullivan/Getty Images)
 Definitions
The U.S. Bureau of Labor Statistics defines the number of employed as including all individuals who, during the

reference week in which the statistical survey was conducted,
1. Held a job and worked at least one hour for pay or profit, or were unpaid and contributed at least fifteen
hours in a family business;
2. Did not work, as a result of nonlabor market reasons such as vacation, illness, bad weather, strike or lockout,
or other family or personal reasons that temporarily kept the individual off the job.
Hence, regardless of the skill requirements for a particular occupation, the number of hours an individual works
(part-time or full-time), or the number of jobs held by an individual participant in the labor market (single versus a
multiple job holder), everyone counts as one employed person in the official employment statistics. According to
the Organisation for Economic Co-operation and Development (OECD), about 538 million people were employed
in the OECD countries out of a civilian labor force of 572 million individuals in 2008. This indicates that the vast
majority of individuals who were actively participating in the labor market were employed, regardless of whether
they worked part-time or held multiple jobs. On the other hand, the employment rate, also referred to as the
employment/population ratio, is defined as the number of employed persons as a percentage of the civilian
noninstitutional working-age population—that is, the number of employed persons as a share of the total number
of individuals in an economy who potentially could offer their labor services. In 2008, the employment rate in the
United States was 62.2 percent, which was down from 63.0 percent in 2007.
According to the Bureau of Labor Statistics, unemployment is a particular labor market state that encompasses all
individuals in the working-age population who, during the reference week, held no job, were available for work,
and actively engaged in a job search during the preceding four weeks. According to this definition, in 2008,
approximately 34 million people in the OECD countries were without work and were actively seeking work out of a
civilian labor force of 572 million. In contrast to the employment rate, the unemployment rate is defined as the
percentage of the actual labor force that is unemployed. For all of the OECD countries together, the weighted
average unemployment rate in 2008 was 5.9 percent, ranging from a low of 2.5 percent in Norway to a high of
11.4 percent in Spain.
 Patterns of Unemployment and Underemployment
Both employment and unemployment show strong seasonal, cyclical, and long-term patterns. Depending on
climate and the structure of industry, in some countries, the magnitude of the seasonal difference could far
exceed the cyclical variations because of the booms and busts that regularly afflict advanced market economies.
However, the long-term behavior of unemployment differs somewhat among these countries. The succeeding
figure illustrates the trend in the unemployment rate in the major industrialized countries between 1970 and 2008,
adjusted to the U.S. statistical measure. Each of the three panels depicts the U.S. unemployment rate series (the
continuous line, as compared to the broken lines for other countries).


International Comparison of the Unemployment Rate Experience of the United States in
Relation to its G-7 Partners, 1970–2008  (adjusted to U.S. statistical measure)
Source: U.S. Bureau of Labor Statistics.
The top panel of the figure shows the unemployment rate of the largely English-speaking members of the G-7
countries from 1970 to 2008. It depicts a relatively stationary trend with local spikes during major recessions, as
occurred in the mid-1970s, early 1980s, early 1990s, and early 2000s. Unemployment rates converge at less than
6 percent by the end of the period, before the deep recession that originated in the United States in late 2008 and
spread internationally throughout 2009. In contrast, in the middle and lower panels, one can compare the historical
trend in the U.S. unemployment rate to that of continental Europe and Japan, respectively. In the early 1980s, the
major countries of the eurozone departed significantly from the U.S. pattern, experiencing rates far surpassing
those in the United States since the 1980s; in the case of Japan, unemployment rates trended upward throughout
the period, especially during the 1990s and the first half of the 2000s, rising from levels of less than 2 percent in
the early 1970s to converge on U.S. rates during the last decade.
Although labor market variables such as the employment and unemployment rates display strong cyclical patterns,
they generally are known to be lagging indicators of the business cycle compared to, say, real gross domestic
product (GDP), which roughly coincides with the reference cycle. This is because in times of recession, as firms
cut production, they do not shed employment proportionally, but first choose to utilize their skilled personnel and
supervisory staff less intensively in order to retain essential human capital within the enterprise. This is why we
often witness real GDP declining well before unemployment begins to rise in a recession, with the ratio of output
per employed person falling sharply. On the other hand, during a period of recovery, firms will choose to employ
their underutilized workforce more intensively before hiring more workers, resulting in what observers refer to as a
“jobless” recovery, with output per employed person rising.

 Other Measures of Underutilization of the Labor Force
While the unemployment rate is the most widely used measure of the underutilization of human resources in an
economy, there are other useful measures of labor market performance that add dimension to the problems of
underemployment and underutilization of labor. Because firms choose to employ their workforce less intensively
during a recession, this suggests that work hours should decline during recessions and the incidence of part-time
employment should rise. While many part-time workers choose part-time jobs and thus are voluntarily
underemployed, the number of involuntary part-time employed could be especially high during recessions. For
instance, in any given month in 2009, close to half of all part-time workers (i.e., those who were employed for
fewer than thirty-five hours per week) in the United States worked on a part-time basis for “economic reasons”—
that is, they could not find full-time jobs because of slack labor market conditions resulting from cyclical (or even
seasonal) factors affecting labor demand. The importance of involuntary underemployment, which persists even
during periods of growth and prosperity, points to a bias toward focusing only on the official employment and
unemployment rates as appropriate indicators of labor market performance.
The behavior of firms in hiring involuntary part-time workers also can lead to a substantial underestimation of the
magnitude of the underutilization of labor in an economy. Another type of underestimation could occur as a result
of the cyclical behavior of participation in the labor market. Out of the total civilian working-age population (i.e., the
potential labor force), there are a significant number of individuals who are not counted as part of the current labor
force because they are not searching for a job, but who are available for a work and have looked for work in the
past twelve months. The Bureau of Labor Statistics classifies these individuals as “persons marginally attached to
the labor force.” Officially, they are classified as not in the labor force because of they are not searching for a job;
however, some argue that these individuals constitute a “disguised” or “hidden” form of unemployment. Indeed, a
subset of these marginally attached individuals who give a strictly job-related reason for not looking for work are
described as “discouraged workers.” These are individuals who stopped looking for work because of their negative
expectations of finding a job vacancy, became discouraged, and officially dropped out of the labor force. The
proportion of marginally attached labor market participants displays a strong cyclical pattern, rising during times of
increasing unemployment and falling during periods of high growth in employment.
To understand the magnitude of involuntary part-time employment and marginal attachment to the labor force, it is
useful to compare the official unemployment rate with an expanded labor underutilization rate that adds these two
groups to the estimate of the measured unemployed. For instance, in January 2010, the official seasonally
adjusted unemployment rate was estimated at 9.7 percent, according to the Bureau of Labor Statistics. When
estimates of the total unemployed, plus all persons marginally attached to the labor force, plus the total involuntary
part-time employed are added together and calculated as a percentage of an expanded denominator (which
includes the official labor force as well as all persons marginally attached), we get a broad, seasonally adjusted
estimate of underutilization of labor of 16.5 percent in January 2010. This means that as many as one out of every
six individuals was directly affected by the recession. Like the unemployment rate, the gap between this broad
estimate of underutilization of labor and the official unemployment rate widens during recessions and narrows
during periods of strong growth in labor market demand.
 Economic Concepts of Unemployment
There exists a whole typology of unemployment concepts referring both to the characteristics and the underlying
mechanisms of unemployment in an economy. Economists normally classify unemployment into four main
categories:
1. Seasonal unemployment is the least controversial concept. This type of unemployment is becoming less
significant, both because of the relative decline in primary activities such as agriculture and because of
changing technology, which allows many building activities to be carried out during the winter months in
regions where seasonality is important. It is because of seasonal variations that the raw monthly labor force

statistics normally are seasonally adjusted.
2. Frictional unemployment recognizes that in a dynamic labor market, there is continual turnover as individual
labor market participants voluntarily engage in job searches. Mainstream economists emphasize this
voluntary search activity to explain patterns of unemployment using a choice theoretic framework of analysis.
3. Structural unemployment refers to the constant churning between the structure of demand and the structure
of labor supply that leads to a mismatch between people and jobs. This mismatch between the skills required
and those supplied by labor force participants can be the result of technological change (often referred to as
technological unemployment) or other factors that lead to changing industrial, occupational, and/or regional
patterns of demand.
4. Cyclical unemployment relates to changes in aggregate demand that reflect the general pattern of booms
and busts in an economy; this also is referred to as deficient-demand unemployment. Although economists
recognize that both frictional and structural unemployment are sensitive to overall business fluctuations,
traditionally, they have sought to identify cyclical unemployment as characteristically different from other
categories of unemployment. Indeed, much debate in macroeconomics centers on the existence and nature
of cyclical unemployment.
 Classical/Neoclassical Versus Keynesian/Post Keynesian Concepts of
Unemployment
Because it uses an individualist methodology, traditional mainstream economic analysis, which is referred to in
modern macroeconomics textbooks as “classical” or “neoclassical” economics (which dominated economic thinking
both immediately before the publication of John Maynard Keynes’s General Theory of Employment, Interest and
Money in 1936 and since the rise of monetarism and the new classical school in the 1970s and 1980s),
historically has had difficulty grappling with the category of cyclical unemployment. This is a type of unemployment
that Keynes originally described as “involuntary” unemployment.
To modern mainstream neoclassical economists, who analyze economic behavior within a purely choice theoretic
framework of analysis, demand-deficient unemployment is an aberration. In times of recession, unemployment
rises, not because of a shortfall in demand, but because of an inadequate supply-side response to the decline in
labor demand. As long as wages are sufficiently downward flexible, this should preclude the existence of any
demand-deficient unemployment. If cyclical unemployment exists, it can be attributed to an insufficient cut in
wages as a result of workers’ bargaining strength, or to institutional impediments such as minimum wage
legislation or the generosity of unemployment insurance benefits. Although not all unemployment is “voluntary,”
the existence of involuntary unemployment is attributable to institutional imperfections in the labor market that
prevent the competitive downward bidding of wages.
Keynes and Post Keynesians reject this neoclassical conception of the labor market as a self-correcting
mechanism. Keynes argued that the existence of involuntary unemployment was not attributable to wage rigidity,
but to the incapacity of the labor market to clear on its own because of the negative aggregate demand-side
feedback effect that a decrease in wages would have on the demand for labor, and thus on the equilibrium level of
employment. Hence, even if wages were downward flexible, the economy would not necessarily return to a state
of full employment—that is, a level of unemployment consistent with the existence of frictional unemployment. This
is because wages are not just an element of cost to business enterprises, but they are also an element of
aggregate demand arising from workers’ consumption spending, with the latter depending on household
employment income. A cut in wages would make it less expensive to hire labor, thereby putting upward pressure
on employment, as the economy moves along a downward-sloping demand curve for labor. However, the
negative feedback effect of the wage cut through the demand side also could shift the demand curve for labor
inward, thereby putting downward pressure on employment at the lower level of wages because of the shrinking
aggregate consumption demand resulting from lower overall labor income. Depending on the strength of the

downward shift in labor demand, falling wages could exacerbate the overall state of cyclical unemployment.
These Keynesian ideas about the determination of employment and unemployment, which are reflected in the
threefold classification of frictional, structural, and cyclical unemployment, became popular in the decades
following World War II. In this framework, full employment means the absence of cyclical unemployment, or
unemployment attributable to the business cycle. However, by the 1970s, some neoclassical economists were
seeking to revive the classical theory of employment by fusing the established concept of frictional unemployment
with what they dubbed the “natural rate” of unemployment, a concept this is associated with economist Milton
Friedman.
Frictional unemployment can be described as a desired or “equilibrium” state in which, at any give time, some
labor market participants may find themselves temporarily moving “between jobs.” Friedman accepted the notion of
equilibrium turnover. However, he appended an important condition pertaining to wages and prices to the concept
of frictional unemployment. He concluded that, when wages and prices are not accelerating in an economy and
when expectations about the future course of wages and prices are being fully realized by individual participants,
the economy rests at its natural rate of unemployment (also often referred to as the nonaccelerating inflation rate
of unemployment, or NAIRU). Cyclical deviations from this steady-state rate of unemployment occur when
expectations regarding the future course of wages and prices are not being realized because of short-term,
unanticipated fluctuations in demand. Later, new classical macroeconomists such as Robert Lucas ruled out even
short-term fluctuations in involuntary unemployment, on the strict assumption of continuous market clearance and
strong information about the future course of the economy. However, if this is so—if cyclical unemployment really
is of little importance—is this natural rate of unemployment a constant, or does it vary cyclically over time?
Most mainstream economists who subscribe to the concept of a natural rate of unemployment argue that as long
as the underlying structural characteristics of the labor market in terms of labor mobility and labor turnover remain
unchanged, so will the natural rate of unemployment.
However, a new literature that has emerged during the last two decades among New Keynesians and Post
Keynesians sought to restore the concept of involuntary unemployment by showing that the natural rate of
unemployment closely tracks the actual rate of unemployment. This often is referred to as a “hysteresis” model of
the labor market, and it points to the permanent effects that cyclical changes in actual unemployment have on the
natural rate of unemployment because of phenomena such as skill deterioration resulting from long spells of
unemployment. It may be argued that the endogenization of the natural rate of unemployment to the behavior of
the actual rate of unemployment revived the Keynesian notion of deficient demand, or involuntary unemployment,
that had been eclipsed in the economics literature since the 1970s.
Mario Seccareccia
 
See also:  Unemployment, Natural Rate of;  Wages. 
Further Reading
Bean, Charles R.  “European Unemployment: A Survey.” Journal of Economic Literature 32:2 (June 1994): 573–619. 
Freeman, Richard B. America Works: Critical Thoughts on the Exceptional U.S. Labor Market. New York: Russell Sage
Foundation, 2007. 
Galbraith, James K.  “Time to Ditch the NAIRU.” Journal of Economic Perspectives 11:1 (Winter 1997): 93–108. 
Organisation of Economic Co-operation and Development (OECD). Employment Outlook: Tackling the Jobs
Crisis. Paris: OECD, 2009. 
Osberg, Lars.  “The ‘Disappearance’ of Involuntary Unemployment.” Journal of Economic Issues 22:3 (September
1988): 707–727. 

Solow, Robert M. The Labor Market as a Social Institution. Oxford, UK: Basil Blackwell, 1990. 
U.S. Bureau of Labor Statistics:  www.bls.gov
Endogenous Growth Models
 
Endogenous growth models are a class of economic growth theories, and those associated with them, that
emerged in the 1980s. The main premise of these models is that economic growth is an endogenous outcome of
an economic system, a result of forces operating within the economy. Simply put, the main idea behind
endogenous growth models is that knowledge—as embodied in greater productivity—drives economic growth.
Because the emphasis on productivity connects endogenous growth models to real-world circumstances,
endogenous growth models have gained widespread popularity and acceptance in the economic profession.
 Underlying Concepts
If one could synthesize hundreds of years of research on economic growth, especially in the post–industrial
revolution period, three fundamental ingredients would dominate the discussion: the capital-to-labor ratio, the
concept of diminishing returns, and the growth in productivity or knowledge. Beginning with Adam Smith’s Wealth
of Nations (1776), in which Smith identified an increase in the well-being of the population as the true wealth of a
nation, particular importance has been attached to capital accumulation and the capital-to-labor ratio for an
economy’s growth. The concept of diminishing returns is more closely associated with the English economist
David Ricardo, who argued that an increase in output diminishes as more capital is put into the production
process—to the point that it will ultimately come to a standstill. Finally, the neoclassical growth model, which
dominated the growth literature after its introduction by the American economist Robert Solow in 1956, added a
third key ingredient to the growth equation: technological progress. According to this view, as long as the growth
in productivity resulting from technological change is greater than diminishing returns to physical capital, an
economy will experience long-term growth.
Endogenous growth models were introduced in the 1980s in an attempt to reconcile theory with a more
statistically based analysis of the economic growth process. Two main assumptions of the neoclassical model
were that technology was exogenously determined—that is, arising from forces that arose independent of
economic forces, such as genius—and that all countries have the same level of technology available to them.
Given these two assumptions, along with the assumption of perfect competition, one of the main implications of
the neoclassical growth model was that of convergence. Convergence implies that poorer countries will grow
faster than wealthier countries and, over time, will catch up with wealthier countries. Even though initial estimates
gave empirical support to this theory, it did not hold up when the economic researchers broadened their sample of
countries outside the West and the time period was lengthened. Attempts to compare incomes across countries
found evidence of divergence rather than convergence—in other words, that poorer countries do not necessarily
catch up with wealthier ones but often lag further and further behind.
To explain the apparent contradiction between the neoclassical theory and empirical evidence, American
economist Paul Romer, considered one of the pioneers of endogenous growth theory, suggested that technological
progress is determined internally by positive knowledge spillovers. Positive spillovers (also known as positive
externalities) are benefits accruing to all firms due to knowledge accumulation—that is, productivity gains—within
one firm. Endogenous growth models were so named because of the endogenous nature of technology—it is
driven by economic decision-making. Additionally, given the nature of knowledge creation, there are increasing

returns associated with new knowledge or new technology. Thus, it is possible to show that investment in
knowledge creation allows for growth in productivity that can offset the effects of diminishing returns as originally
conceived by Ricardo.
The reason behind increasing returns to knowledge is the nonrival nature of knowledge. Knowledge and ideas,
once produced, can be shared free of cost or very cheaply. It is this nonrival nature of knowledge that drives
growth. This quality implies that knowledge may be path-dependent in the sense that it may be possible to
develop new technologies based on what is learned from existing ones. In other words, decisions made for any
given circumstance are limited by decisions made in the next, so that economic practices become self-reinforcing.
This may explain why certain technologies prevail longer than others. Another implication of the nonrival nature of
knowledge is that it is relatively easy and cheap to replicate a new technology once it has been produced.
However, even though knowledge is nonrival, it is also what economists call “excludable or partially excludable,”
given socially and legally determined property rights. Licenses, patents, and trademarks allow the creator of new
knowledge to exclude others from the ideas or allow others to share such ideas (after an initial time period) at an
additional cost. Without this protection, private or public entities would not have the incentive for new knowledge
creation.
The nonrival nature of knowledge implies that countries can continue to grow prosperous by augmenting their
knowledge of how to increase output with ever-decreasing inputs. However, given the unique characteristic of
knowledge, markets may tend to underinvest in knowledge. It is much cheaper for a firm to duplicate an idea after
it has been produced rather than invest in research and development of such knowledge, which may or may not
bear results. Thus, given the initial high cost of investing in knowledge creation, the larger the market share, the
greater the profits. This is because a greater market share lowers the average cost of producing such knowledge.
Additionally, given the excludable nature of knowledge and the exclusive monetary gains from creating such
knowledge, there is a tendency for monopolies to be formed in the market. This is why countries introduce and
execute antitrust laws to prevent the creation of such monopolies.
 Categories
Endogenous growth models can be grouped into four categories, depending on their emphasis. The first type of
model emphasizes the size and productivity of the research sector. In this view, economic growth depends on the
ability of this sector to provide new ideas to the other sectors that produce goods and services.
The second group of endogenous growth models emphasizes new products that make older ones obsolete. These
models are similar in concept to Austrian school economist Joseph Schumpeter’s concept of “creative destruction,”
according to which new technologies lead to overall economic growth even as they produce specific winners and
losers in the economy. Here, too, there are spillovers from the research sector, which lead to new innovations and
products.
A third group of endogenous growth models recognizes the difference between fundamental or theoretical
research and the application of such research to the real economy. According to these models, theoretical
research does not add value unless it can be applied to secondary innovations that lead to the production of new
goods. These models highlight the importance of learning by doing. They are also known as two-stage innovation
models.
The fourth category attempts to explain why most economic growth occurs at intervals rather than evenly over
time. Fluctuations in growth are related to vertical innovations whereby the possibility of new innovations dampens
current research efforts, as researchers fear that existing technologies and products will soon become obsolete,
making it less than worthwhile to improve them. Another explanation of uneven growth lies in the positive
externality effects of general-purpose technologies in one sector, which can lead to improved technologies in many
sectors. These may be due to the learning-by-doing nature of innovations. A good example would be the invention
of the steam engine or the invention of computers and how they have revolutionized production and technology in
many sectors of the economy.

Not all innovation is positive, however. As noted earlier, innovation can lead to economic instability. And as the
financial crisis of 2008–2009 has demonstrated, innovations in the financial sector can be especially destabilizing.
As finance is the lubricant and energy source for all economic activity, innovation in this sector has an impact on
virtually all economic activity. Given that, it is not surprising that the worst financial crisis since the Great
Depression—a crisis brought on, in part, by innovation in the financial sector—produced the worst overall
economic downturn since the Great Depression.
Sharmistha Self
 
See also:  Growth Cycles;  Growth, Economic. 
Further Reading
Aghion, Philippe, and Peter Howitt. Endogenous Growth Theory. Cambridge, MA: MIT Press, 1998. 
Aghion, Philippe, and Peter Howitt.  “A Model of Growth Through Creative Destruction.” Econometrica 60:2 (1992): 323–
351. 
Grossman, Gene, and Elhanan Helpman.  “Technology and Trade.” NBER Working Paper no. W4926. Washington,
DC: National Bureau of Economic Research, 1994. 
Rivera-Batiz, Luis A., and Paul M. Romer.  “Economic Integration and Endogenous Growth.” Quarterly Journal of
Economics 106:2 (1991): 531–555. 
Romer, Paul M.  “Endogenous Technological Change.” Journal of Political Economy 98:5 (1990): S71–S102. 
Romer, Paul M.  “The Origins of Endogenous Growth.” Journal of Economic Perspectives 8:1 (1994): 3–22. 
Schumpeter, Joseph.  “The Creative Responses in Economic History.” Journal of Economic History 7:2 (1947): 149–159. 
Solow, Robert M.  “A Contribution to the Theory of Economic Growth.” Quarterly Journal of Economics 70(1956): 65–94. 
Enron
 
Once one of the largest and apparently most innovative corporations in America, Houston-based Enron was
originally an energy company that had diversified from the 1980s through the early 2000s into a number of
businesses, the most lucrative of which was the supply and trading of natural gas and electricity. Hailed as an
innovator, Enron also engaged in a number of illegal activities, including falsification of its accounting records for
the purposes of driving up its stock price and manipulation of the California electricity market. As revelations of
these practices became public in 2001, the company was forced into bankruptcy, decimating the assets of
thousands of investors, including many of the company’s own employees. Several high-level executives were
charged and found guilty of various forms of fraud. The scandal also destroyed the venerable accounting firm of
Arthur Andersen, which had been tasked with overseeing Enron’s books, and led to far-reaching reforms for
corporate responsibility.
 Origins and Growth
The origins of Enron date to 1985, when InterNorth, an Omaha-based natural gas pipeline operator, merged with

Houston Natural Gas. The resulting company was soon renamed Enron by its chief executive officer (CEO),
Kenneth Lay, formerly head of Houston Natural Gas. The merger came during a period of deregulation of the
natural gas industry. In 1978, Congress passed the Natural Gas Policy Act, which created a national market for
natural gas. This was followed by other laws and regulatory changes that allowed pipeline operators to get into the
business of buying and selling natural gas. The idea behind natural gas deregulation was that market forces,
rather than government regulators, would be better able to ensure that consumers—both individuals and industries
—got the most competitive price for their energy supplies.
In 1989, Enron began trading in natural gas futures, soon diversifying into a variety of other forms of energy,
including electricity, an industry that was also undergoing deregulation in many states. In 1990, Lay hired an
energy consultant named Jeffrey Skilling to head Enron’s commodities trading department. By 1994, the company
had become the largest seller of electricity both in the United States and the United Kingdom, which had also
deregulated much of its energy markets.
By the late 1990s, the company—now consistently rated “America’s Most Innovative Company” by Fortune
magazine—was building and operating power plants and gas pipelines across North America and in many other
parts of the world. At the same time, Enron was developing new derivative commodities products that investors
could buy and sell, including those based on weather. That is, people could bet on how weather conditions would
affect the price of commodities, including energy, and purchase derivatives—a form of insurance—against those
conditions. Always technically savvy, the company opened what would become the largest Web-based
commodities trading market in the world in 1999. By that same year, the company had become so large and
influential that it had a hand in about one-fourth of all major energy deals in the world. In August 2000, its share
price reached its all-time high of about $90 a share; the company reported revenues of $100 billion that year and
employed more than 20,000 people in over thirty countries.
 Questionable Business Practices
At the same, however, Enron was engaging in dodgy business activities. It began to set up a web of subsidiaries
and partnerships, with interlocking ownership, with which it engaged in all kinds of business and trading activity.
All of this was perfectly legal except for the fact that Enron was using the subsidiaries to hide losses and debt,
allowing it to maintain the illusion of ever-increasing revenues and profits. Enron was also engaged in rigging
markets, most notably in the California electricity market, which had been deregulated in 1996, permitting energy
wholesalers more leeway in the prices they charged and allowing California utilities to sell their electricity to other
states. Although the facts only came out later, by 2000, Enron and other electricity suppliers were gaming the
system, either holding off supplies or diverting them elsewhere to create false shortages that would send prices
spiking. Enron and other wholesalers made huge windfall profits as a result.
But the revenues were not enough to save the company. By 2001, Enron was sustaining large losses in its
various business endeavors, including broadband provision, and through its partnerships, saddling the company
with unsustainable debt. This was never divulged to investors, however, including employees whose retirement
accounts often consisted largely of Enron stock. Previously, the company had hidden much of this debt with its
subsidiaries, but by the summer of 2001 the losses and debt had become so large that this was no longer
possible.
As rumors circulated through the financial markets that Enron was in serious trouble, Skilling, now CEO, resigned,
replaced by former CEO Lay. In October, the company reported a $638 million loss for the third quarter and a
$1.2 billion reduction in shareholder equity, much of the losses caused by the various partnerships and
subsidiaries set up by Andrew Fastow, a Skilling protégé and the company’s chief financial officer. That same
month, the Securities and Exchange Commission launched an investigation into Enron’s accounting practices as
the company fired Fastow.
With its finances collapsing, and its share price falling below $10, Enron began borrowing from banks, to the tune
of several billion dollars. In November, Enron made public the extent of its debt, and the company’s bonds were

downgraded by the rating agency Standard & Poor’s to that of no-investment grade, or “junk bond” status, the
latter a popular term for the highest-risk bond. Then, after a contentious takeover bid by a small rival named
Dynegy fell through, Enron filed for bankruptcy protection under Chapter 11 of U.S. bankruptcy laws on December
2, 2001. It was the largest such filing to date in U.S. history.
Meanwhile, revelations soon emerged—some by the company itself—that huge bonuses had been paid to
executives even as the company was becoming insolvent. Later, it would be learned that executives and their
families had sold off huge blocks of stock as they were telling investors and their own employees that the
company was still profitable.
 Prosecutions and Legacy
With the company in bankruptcy, Lay resigned as CEO in February 2002 while various lower-level executives at
Enron and its accounting firm, Arthur Andersen, pleaded guilty to charges of obstructing justice, for destroying
crucial documents, as well as money laundering and conspiracy. In late 2002, Fastow was indicted on seventy-
eight charges of conspiracy, fraud, money laundering, and other crimes, pleading guilty to two counts of
conspiracy in January 2004. He received a ten-year prison sentence. A month later, Skilling—indicted on multiple
counts of insider trading, fraud, and conspiracy—would plead innocent. In July, the FBI indicted Lay for
participating in a conspiracy to falsify the company’s financial statements. He, too, would plead innocent. Both
men, however, would be found guilty. In May, Skilling was convicted on nineteen counts of security and wire fraud
and sentenced to more than twenty-four years in prison while Lay was convicted on six similar counts and faced
up to forty-five years in prison. However, Lay died of a heart attack in July, before sentencing could proceed.
In the end, some twenty-one individuals were found guilty of various forms of securities malfeasance and fraud,
including four at the brokerage firm of Merrill Lynch, which had marketed some of Enron’s equity. While the firm of
Arthur Andersen was found guilty of obstructing justice by destroying documents, the conviction was later
overturned. Nevertheless, the bad publicity surrounding the scandal led most of the accounting firm’s clients to
abandon it, forcing it to close down operations, though it never dissolved or declared bankruptcy.
Aside from the nearly $75 billion lost by investors, nearly two-thirds of which investigators said was attributable to
fraud, the main outcome of the Enron scandal were twofold: it invigorated calls to re-regulate energy markets in a
number of states, including California, and it led to tougher federal corporate accounting and financial reporting
standards with congressional passage of the Public Company Accounting Reform and Investor Protection Act of
2002, better known as Sarbanes-Oxley, after its co-sponsors, Senator Paul Sarbanes (D-MD) and Representative
Michael Oxley (R-OH).
James Ciment
 
See also:  Corporate Corruption. 
Further Reading
Bryce, Robert. Pipe Dreams: Greed, Ego, and the Death of Enron. New York: PublicAffairs, 2003. 
Fox, Loren. Enron: The Rise and Fall. Hoboken, NJ: John Wiley and Sons, 2003. 
McLean, Bethany, and Peter Elkind. The Smartest Guys in the Room: The Amazing Rise and Scandalous Fall of
Enron. New York: Portfolio, 2004. 

 
European Central Bank
 
Established in June 1998, several months ahead of the launch of Europe’s common currency, the euro, the
European Central Bank (ECB) conducts monetary policy for the countries of the eurozone (officially the euro
area), which encompasses those European Union (EU) member states that have adopted the euro as their
currency, forming the European Monetary Union (EMU). As of late 2009, sixteen of the twenty-seven EU countries
have adopted the euro. The sixteen countries that participate in the EMU are Austria, Belgium, Cyprus, Finland,
France, Germany, Greece, Ireland, Italy, Luxembourg, Malta, Netherlands, Portugal, Slovakia, Slovenia, and
Spain. The eleven that have not adopted the single currency either do not want to or do not meet the
requirements set by the EU to join. They are Bulgaria, Czech Republic, Denmark, Estonia, Hungary, Latvia,
Lithuania, Poland, Romania, Sweden, and the United Kingdom.
A sculpture of the euro symbol stands outside the headquarters of the European Central Bank in Frankfurt,
Germany. The ECB administers monetary policy for the sixteen European Union nations that have adopted the
euro. (Bloomberg/Getty Images)
The ECB has played an increasingly important role in monitoring and attempting to adjust for economic
fluctuations within the business cycles in the eurozone. The ECB is one of the world’s most important central
banks as the euro area is the world’s largest economy after the United States. The euro has become the second
most important international currency after the U.S. dollar. The adoption of the euro by the EMU member

countries has promoted foreign trade and foreign investment flows, enhanced competition, and as a result, also
contributed to higher economic growth. The euro area is expanding as several other EU member states plan to
adopt the euro in the near future.
The headquarters of the ECB is located in Frankfurt am Main, Germany, continental Europe’s leading banking
center. The current president of the ECB is Jean-Claude Trichet. He is the second holder of this position: until
November 2003, Willem (Wim) Duisenberg held this post. The main decision-making body of the ECB is the
Governing Council, consisting of the six members of its Executive Board—including the president, the vice
president, and four other members—and the governors of the national central banks of the sixteen euro area
countries. The Governing Council usually meets twice a month.
The primary objective of the ECB is maintaining price stability (keeping inflation below 2 percent or close to it)
over the medium term and, through that, achieving sustainable economic growth and prosperity in Europe. The
ECB also has several other important tasks. It is responsible for defining and implementing the monetary policy for
the euro area, conducting foreign exchange operations, holding and managing the euro area countries’ official
foreign reserves, ensuring the smooth operation of payment systems, and monitoring progress in financial
integration. The ECB has the exclusive right to authorize the issuance of banknotes within the euro area (member
states can issue euro coins, but they have to inform the ECB beforehand and get its approval). The ECB also
cooperates with relevant EU and international institutions, bodies, and forums. In addition, the ECB collects
monetary and financial statistical information from national authorities or economic agents. Every year, the ECB
has to give an overview of its monetary policy and other activities to the European Parliament, the EU
Commission, and the European Council. The ECB publishes monthly bulletins, statistics pocket books, annual
reports, financial stability reviews, research, and occasional and legal papers. Thus, the ECB is quite active in
knowledge creation and dissemination.
The ECB is politically independent. It and its member national central banks are not allowed to take instructions
from any national governments, EU institutions, or anyone else. This independence is critical; otherwise, politicians
might be tempted to increase output and decrease unemployment for the short-run election cycle, for example, by
printing more money, despite creating higher inflation in the longer run. This helps the ECB to maintain price
stability and retain the credibility of the single monetary policy. To ensure its political independence, the ECB has
its own budget. It gets its capital from the national central banks of the euro area.
One of the ongoing issues surrounding the ECB is whether its policies and regulations are, or can ever be, optimal
for all of the European countries that have adopted the euro. Critics point out that Europe is not what economists
refer to as an “optimal economic area,” meaning that what policies may work well for some countries may not work
well in others with different types of economies, and may in fact prove economically destructive to them. A second
controversy is whether the ECB and the U.S. Federal Reserve can coordinate responses to crises, such as in
dealing with the global financial meltdown of 2007–2008. In the early months of the crisis, Germany—as the
dominant player in the ECB—resisted U.S. calls for decisive monetary stimulus action to fight the growing
recession. But as the economic downturn deepened, the ECB began to loosen credit more, as the United States
had urged.
In 2011, the ECB found itself at the center of an even more largescale and urgent problem—the sovereign debt
crisis facing a number of eurozone countries, especially Greece. The crisis pointed to the basic problem inherent
in the plan that established the ECB and the Euro, specifically, that the ECB was responsible for assuring the
Euro’s stability but had little control over the fiscal policies of the countries that had adopted the currency.
Countries that either acted fiscally irresponsible, as many alleged was the case with Greece, or who had simply
been hit hard by recession, such as Ireland, might find themselves unable to pay their debts, thereby threatening
the overall stability of the Euro.
 
The economic crisis provoked a political one as well. In general, more fiscally sound northern European countries,
most notably Germany, Europe’s largest economy, resented having to provide financial bailouts or loan

forgiveness to countries of the continent’s Mediterranean littoral, which the northerners regarded as fiscally
irresponsible. The German government of Chancellor Angela Merkel resisted efforts by France, which was heavily
exposed to potential bad loans to Mediterranean neighbors, to make the ECB the lender of last resort. This was
partly because Germany was the largest contributor of funds to the bank. But as the sovereign debt crisis of 2011
threatened to spread to economies much larger than Greece, such as Spain and Italy, pressure mounted for the
ECB to spearhead the creation of larger funds to shore up the finances of these troubled economies.
Tiia Vissak
 
See also:  Banks, Central;  Monetary Policy. 
Further Reading
Dutzler, Barbara. The European System of Central Banks: An Autonomous Actor? The Quest for an Institutional Balance in
EMU. New York: Springer, 2003. 
European Central Bank:  www.ecb.int
Issing, Otmar, Vitor Gaspar, and Oreste Tristani. Monetary Policy in the Euro Area: Strategy and Decision Making at the
European Central Bank. New York: Cambridge University Press, 2001. 
Kaltenthaler, Karl. Policymaking in the European Central Bank: The Masters of Europe’s Money. Lanham, MD: Rowman &
Littlefield, 2006. 
 
Exchange Rates
 
Exchange rates—the values of individual currencies in terms of others—are a critical element in the international
transmission of business cycles and in modern international financial crises. The exchange rate system that exists
between international currencies lies at the heart of the economic crises experienced since the 1980s in Latin
America, Eastern Europe, and Asia, since the exchange rate affects the capital flows, imports, exports, and terms
of trade among countries in those regions.
Exchange rates play a major role in international business. If, for example, an American-made washing machine
costs US$500, while a similar Mexican-made appliance costs 6,000 pesos, the Mexican product will be cheaper
for an American to purchase if the exchange rate is 25 pesos per dollar (or 4 cents per peso), while the American
machine will be cheaper for the American to buy if the exchange rate is 10 pesos per dollar (10 cents per peso).
In the first case, if the exchange rate were 25 pesos per dollar, the American would have to pay only US$240 for
the 6,000 pesos to purchase a Mexican appliance. If the exchange rate were 10 pesos per dollar or 10 cents per
peso, the American would have to pay US$600 for the 6,000 pesos to purchase the machine. Hence in this case,
the American would not purchase the machine made in Mexico. Thus, which machine is cheaper (the best buy)

depends on the exchange rate at the time of the purchase.
Fluctuations in the exchange rate thus play a major role in determining the profitability of international trade, the
direction of foreign investment, and the attractiveness of foreign tourism. Economists cite three major categories of
exchange rates: nominal, real, and effective.
The euro is the second most traded currency in the world after the U.S. dollar. A relatively strong exchange rate
has yielded increased foreign investments for many eurozone countries, but acute economic problems have
persisted for some. (Michel Porro/Stringer/Getty Images)
 Nominal Exchange Rates
Exchange rates as described above—between the currencies of two countries without regard to inflation—are
known as nominal exchange rates. Nominal rates between the U.S. dollar and major foreign currencies are
reported daily by financial publications such as the Wall Street Journal and the Financial Times.
Exchange rates are set in a number of different ways. Some countries, like the United States, Canada, and Great
Britain, let their currencies float freely, with the rate being determined entirely by supply and demand. Under such
a system, if Americans want more British pounds—to buy British goods, to invest in Britain, or to travel in Britain—
the increased demand will raise the price of the pound. Since demand and supply change from day to day,
exchange rates also fluctuate on a daily basis in a freely floating system. For a floating currency, a rise in the
currency’s value is called appreciation, while a fall in value is called depreciation. For example, if the yen/dollar
exchange rate rises from 140 to 150, then the dollar has appreciated, since after the rise it is worth more yen (150
versus 140). Likewise, the yen has depreciated, because after the change it takes 150 yen to get US$1 while
before the change it took only 140 yen to get US$1.
On the other hand, some countries adopt a fixed or pegged exchange rate mechanism, tying the value of their
currency to another currency. Saudi Arabia, for example, has pegged its currency to the U.S. dollar at 3.75 riyals
per dollar, a level it has maintained since the mid-1990s; similarly, the Lithuanian currency has been fixed at 3.5
litas per euro since 2002. A country with a fixed exchange rate maintains that rate artificially no matter what
changes take place in the demand and supply of that currency by using its reserves of foreign currency. If, on a
given day, demand for the Saudi riyal falls, leading to downward pressure on the price of the currency, the Saudi

government sells some of its foreign currency reserves (U.S. dollars) and buys riyals. This puts more foreign (non-
Saudi) currency into the foreign exchange market, thereby increasing the supply of those currencies; it also
reduces the supply of riyals in the foreign exchange market. The result is an increase in the value of the riyal
relative to the U.S. dollar, thus making up for the previous fall in price and maintaining the relationship between
the two currencies, as required by the fixed relationship between the two.
Similarly, if demand for the riyal rises, creating upward pressure on its price, the government sells riyals and buys
foreign currency. To maintain a fixed exchange rate, the government must have adequate reserves of foreign
currency. If reserves are depleted as a result of supporting the value of the domestic currency, it may have to be
pegged at a lower level. This kind of adjustment is known as devaluation, while repegging at a higher value is
called a revaluation.
A country can also follow an exchange rate system known as a managed float. This has elements of both floating
and fixed rate regimes. Many emerging economies, such as Brazil, China, and Singapore, follow a managed float.
In this system, the currency does not have an official value that the government is committed to maintain, but it is
not allowed to fluctuate freely in response to demand and supply, as the government tries to steer the value in a
particular direction by buying and selling it in the foreign exchange market (using reserves). Thus, for example,
China, which switched from a pegged system to a managed float in July 2005, allowed the yuan to rise gradually,
from 12.3 U.S. cents to 14.6 cents in July 2008, before intervening to prevent a further rise over the next six
months.
In all three of the above systems, the exchange rate is determined by market forces. In a freely floating system,
only market forces come into play, while in a pegged rate or a managed float, governments intervene by buying or
selling currencies to maintain the pegged rate. In some countries, however, market forces have little or no role, as
the government sets a value for its currency and declares all other rates to be illegal. This typically leads to the
rationing of foreign currencies, and often to black markets. In the past, many countries, such as China, India, and
Russia, had such controlled exchange rates, but today they are relatively rare; Iran and Venezuela are among the
few countries that maintain such a system.
Recent research shows that, under the flexible exchange rate system, business cycles are more synchronized,
especially among the industrial countries.
 Real Exchange Rates
Real exchange rates are used by economists to analyze the competitiveness of a country’s products, based on
changes in the nominal exchange rate as well as inflation in the country and its trading partners. Referring to the
example of the American-made washing machine costing US$500 and a similar Mexican washing machine costing
6,000 pesos, one can see that the two products cost the same at an exchange rate of 12 pesos per dollar (or
8.3333 cents per peso). If the peso appreciates to 10 cents (one peso is now worth more—10 cents versus
8.3333 cents), the Mexican machine now costs the equivalent of US$600, or 20 percent more than its American
counterpart. Mexican products thus become less competitive.
If, with the exchange rate unchanged, inflation in Mexico is higher than in the United States, the effect is the
same. If inflation in the United States is zero, the American machine will cost the same (US$500) a year later. If
inflation in Mexico is 20 percent, the Mexican machine will cost 7,200 pesos a year later—6,000 pesos plus (6,000
pesos times 20 percent)—or the equivalent of US$600 if the exchange rate remains unchanged at 12 pesos per
dollar. Since the Mexican product is now less competitive by 20 percent, the peso is said to have undergone a
real appreciation of 20 percent. Similarly, if the Mexican inflation of 20 percent is accompanied by U.S. inflation of
2 percent, Mexican products become less competitive by 18 percent—or there has been a real appreciation of 18
percent of the peso.
If there are changes in the nominal exchange rate and differences in inflation rates, the real exchange rate is
especially useful for tracking competitiveness. In the situation where there is 20 percent inflation in Mexico and 2

percent inflation in the United States, a 10 percent nominal depreciation of the peso against the dollar—from
8.3333 cents to 7.5 cents, say—will reduce the price disadvantage faced by Mexican products by 10 percent.
Specifically, the disadvantage will decrease from 18 percent to approximately 8 percent; the peso will have
experienced a real appreciation of 8 percent.
Real exchange rates are closely monitored by international economists and organizations like the International
Monetary Fund, since a country whose currency has undergone a strong real appreciation is going to find its
products becoming less competitive on the international market. This is likely to lead to decreased exports and
increased imports, and may ultimately generate a balance-of-payments crisis. Several transition economies in
Eastern Europe, which tried to keep their currencies stable against the euro while undergoing higher inflation,
found themselves in this predicament in the 1990s.
 Effective Exchange Rates
An effective exchange rate is an index used to determine how a currency’s value has changed against (or
compared to) a group of currencies, usually those of major trading partners. In 2008, for example, the U.S. dollar
rose by 24 percent against the Canadian dollar and by 4 percent against the Malaysian ringgit. If Canada and
Malaysia had been the only trading partners of the United States, and if each accounted for half of U.S. trade,
they would have been assigned equal weights and the effective value of the dollar would have risen by half of 28
percent, or 14 percent.
In practice, of course, effective exchange rates are calculated against a much larger number of currencies, with
different weights. The Federal Reserve System, for example, publishes a trade-weighted index of the effective
exchange rate of the U.S. dollar against a basket of twenty-six currencies, with weights varying from 17 percent
for the euro to 0.5 percent for the Colombian peso. Similarly, the European Central Bank compiles an index
showing the effective exchange rate of the euro against a basket of twenty-one currencies, with weights varying
between 24 percent for the United States to 0.1 percent for Latvia.
 Exchange Rates and Financial Crises
Exchange rates pose enormous risks for national economies, as well as for international businesses. Speculators
can intensify—or even cause—a financial crisis by buying and selling currencies in anticipation of a specific event.
Even if the expected event, such as political upheaval in a particular in a country, does not take place, the
damage is done. Speculators have already bought and sold currencies on the foreign exchange market and have
thereby weakened the value of that country’s currency relative to other currencies. Moreover, less developed
countries that borrow money from lenders in a more developed economy run the risk that foreign exchange
movements will go in the wrong direction for them, making it harder to repay the loans and sending them deeper
into debt and financial crisis. Note that the development of foreign exchange forward and options markets has
been in response to the need of market participants in international trade to be able to reduce or hedge the risk of
changes in the exchange rate eliminating their profits. However, these markets can also be used for speculation.
Just such developments occurred in the Asian financial crisis of 1997–1998. Massive overdevelopment of
commercial real estate throughout Asia in the 1990s led to anticipation by currency speculators of large-scale
foreclosures as developers could not repay loans to Asian banks. As a result, speculators started getting rid of
Asian currencies by selling them onto the foreign exchange market. This led to rapid devaluation of Asian
currencies, beginning with the Thai baht. The situation was made even worse by the so-called spillover or
contagion effect. Asian banks, which had borrowed dollars from the United States and converted them into their
own local currencies, now had to pay back the U.S. lenders in dollars—which proved more difficult as the value of
their own currencies continued to plunge. As a result, the Asian economies, one by one, spun out of control. The
disaster continued to resonate a decade later as the global financial crisis of 2008–2009 began heating up.
Animesh Ghoshal

 
See also:  Balance of Payments;  Capital Account;  Current Account. 
Further Reading
Ho, Lok-Sang, and Chi-Wa Yuen. Exchange Rate Regimes and Macroeconomic Stability. Boston: Kluwer Academic, 2003. 
Krugman, Paul R., and Maurice Obstfeld. International Economics: Theory and Policy. Boston: Pearson Addison
Wesley, 2009. 
OANDA, The Currency Site:  www.oanda.com
Rajan, Ramkishen, and Reza Siregar.  “Choice of Exchange Rate Regime.” Australian Economic Papers 41:4 (2002): 538–
556. 
Yarbrough, Beth V., and Robert M. Yarbrough. The World Economy: Trade and Finance. Mason, OH: South-
Western, 2006. 
 
Fannie Mae and Freddie Mac
 
Freddie Mac and Fannie Mae are two housing-related government-sponsored enterprises (GSEs) listed on the
New York Stock Exchange. Both have the same charters, and both purchase and securitize home mortgages to
ensure that private institutions that lend money to homebuyers have the funds to do so. At one time, Fannie Mae
bought mortgages primarily from savings and loans (S&Ls), while Freddie Mac bought them from commercial
banks. Today, the difference no longer holds. Both GSEs were implicated in the mortgage crisis of the late 2000s.
According to some critics, their practices had encouraged too many aspiring homebuyers—including those with
poor credit histories or inadequate incomes—to take out mortgages. Even so, backing about half of all mortgages
issued in the United States inevitably left both institutions vulnerable to the wave of defaults that followed the
bursting of the housing bubble beginning in 2007. In both cases, the federal government was forced to step in to
avert financial catastrophe. Fannie Mae and Freddie Mac were placed under the conservatorship of the Federal
Housing Finance Agency (FHFA) in September 2008.
 Fannie Mae
The first of the two institutions, Fannie Mae (the Federal National Mortgage Association), was established as a
federal agency in 1938 to support housing by increasing the supply of mortgage credit. In 1968, it was chartered
by Congress as a private, shareholder–owned, government-sponsored company and split into two parts: Ginnie
Mae (Government National Mortgage Association, which continued as a federal agency and concentrated on
special assistance programs) and Fannie Mae. Until 1968, Fannie Mae could only buy mortgages insured by the
Federal Housing Administration (FHA); after that it was also allowed to buy other mortgages.

Washington, D.C.–based Fannie Mae and its sister organization, Freddie Mac, backed more than half of all
mortgages in the United States before the housing bust of the late 2000s. The government assumed control of
both institutions to avoid further turmoil. (Karen Bleier/AFP/Getty Images)
Fannie Mae does not offer loans directly to homebuyers. Thus, even if Fannie Mae has bought a homeowner’s
mortgage, the borrower still sends his or her monthly mortgage payments to the loan servicers, who then forward
them to Fannie Mae, which in turn passes them on to the holders of mortgage-backed securities, minus service
fees. Fannie Mae also operates in the secondary mortgage market by packaging home loans into mortgage-
backed securities, which make them easier to sell to investors. Selling the securities provides additional capital to
lenders, which allows them in turn to lend money to new low-, moderate-, and middle-income customers. For
better or for worse, mortgages become more available and affordable to prospective homebuyers.
The activities of Fannie Mae were modernized in 1992, providing a variety of financial services, products, and
solutions to lenders and housing partners. It securitizes both single-and multifamily mortgage loans into Fannie
Mae mortgage-backed securities funded by issuing debt securities in domestic and foreign capital markets. As the
events of 2007–2008 made painfully clear, securitizing home mortgages puts the securities holder at greater risk
should too many borrowers default. In addition to making homeownership possible for more Americans, Fannie
Mae has also supported rental, workforce, and supportive housing for homeless people.
 Freddie Mac
Freddie Mac (originally the Federal Home Loan Mortgage Corporation, but officially doing business as Freddie

Mac since 1994) was founded by Congress in 1970 to support homeownership and rental housing, ending Fannie
Mae’s monopoly. Freddie Mac reduces the costs of housing finance, increases stability in the secondary market
for residential mortgages and the liquidity of mortgage investments, improves the distribution of investment capital
for residential mortgage financing, and thus helps more families (including those with low and moderate income) to
buy, rent, and keep their homes if they have undertaken mortgage obligations. Freddie Mac is one of the biggest
buyers of home mortgages in the United States. To raise funds, it issues debt securities.
The customers of Freddie Mac are predominantly mortgage lenders in the primary mortgage market, including
mortgage bankers, commercial banks, savings institutions, credit unions, and state and local housing finance
agencies. Freddie Mac buys mortgage loans from lenders, packages the mortgages into securities, holds some in
its retained portfolio for investment purposes, and sells the rest to investors, including banks, pension funds, and
others. Moreover, it guarantees that investors will receive payment of principal and interest on time. The lenders
can then use the received funds for lending to other customers. In this way, Freddie Mac helps finance one out of
six American homes—not only single-family houses, but buildings with rental housing as well.
Freddie Mac’s role is not directly visible to homebuyers because, like Fannie Mae, it does not offer loans directly.
Nevertheless, its activities result in more readily available home mortgage credit, lower mortgage interest rates, a
broader selection of mortgage products, and reduced origination costs. Thanks to Freddie Mac, economists
estimate that homebuyers save up to 0.5 percent on their mortgage rate, helping them collectively to save at least
$23.5 billion every year.
Although Freddie Mac does not prescribe to lenders how much they should lend and to whom, it does offer
guidelines and has developed a system called Loan Prospector to help lenders make sound financial decisions.
The system helps lenders determine whether a borrower will be able to repay the mortgage on time and if the
property value is sufficient to pay off the mortgage if the borrower is not able to continue regular payments.
 Mortgage Crisis
In the late 1990s and early 2000s, several developments encouraged Fannie Mae and Freddie Mac to begin
purchasing and securitizing riskier home mortgages. Under pressure from both the Bill Clinton and George W.
Bush administrations to expand homeownership, the two GSEs began to ease credit requirements on the
mortgages they would buy from lending institutions, allowing the same institutions to offer mortgages to riskier
clients at higher interest rates. With housing prices rising steadily, this seemed safe to Fannie Mae and Freddie
Mac investors, who drove up the price of the GSEs’ stock.
Although one of the goals was to provide safe standards for subprime mortgages, the opposite occurred. The
lending industry began offering all kinds of adjustable mortgages not backed by Fannie Mae and Freddie Mac that
could hit borrowers with huge increases in their monthly payments once the low initial “teaser” rates expired.
When housing prices began to deflate beginning in 2007, many of these borrowers found that they lacked the
equity needed to refinance and went into default.
While Fannie Mae and Freddie Mac were not exposed to the worst of the subprime mortgage business, the sheer
volume of loans they purchased proved devastating when the wave of foreclosures began to affect lower-risk
mortgages in 2008. By July, the situation had become so perilous for the two GSEs that the federal government
announced plans for a possible takeover, sending the share prices of Fannie Mae and Freddie Mac plunging.
Meanwhile, the government restructured its regulatory and supervisory oversight of the GSEs, shifting
responsibility from the Office of Federal Housing Enterprise Oversight (OFHEO) in the Department of Housing and
Urban Development (HUD), where it had resided since 1992, to the new Federal Housing Finance Agency
(FHFA). The latter agency was created by the Housing and Economic Recovery Act of 2008, taking over the
functions of OFHEO and the Federal Housing Finance Board. FHFA was granted the authority to set minimum
limits on Fannie Mae and Freddie Mac capital, to regulate the size and content of their portfolios, and to approve
or disallow new mortgage products.

In February 2009, Freddie Mac and Fannie Mae began participating in President Barack Obama’s Homeowner
Affordability and Stability Plan, designed to help 4 million to 5 million solvent homeowners refinance their
mortgages and reduce monthly payments through the two institutions, and to help another 3 million to 4 million at-
risk homeowners avoid losing their properties. To help reach these goals and to reassure investors that the two
GSEs have the full backing of the federal government, the Treasury Department increased its funding commitment
to Freddie Mac and Fannie Mae. In late 2011, Fannie Mae and Freddie Mac agreed to relieve lenders from certain
risks associated with the Obama Administration’s effort to allow home owners with mortgages owned or insured by
the two GSEs to refinance their mortgages at new record-low interest rates. The idea behind the plan was to allow
homeowners who would normally not qualify for refinancing, because their homes were worth less than their
loans, to get new mortgages with lower monthly payments, thereby freeing up income and spurring consumer
demand. At the same time, many conservatives had come to blame Fannie Mae and Freddie Mac for easing up
on lending standards and thereby contributing to the housing bubble that led to the financial crisis of 2007–2008.
As the 2012 election season heated up, there was much talk, particularly among Republicans, of closing down the
two GSEs.
Tiia Vissak and James Ciment
 
See also:  Housing Booms and Busts;  Mortgage Lending Standards;  Mortgage Markets and
Mortgage Rates;  Mortgage, Subprime;  Recession and Financial Crisis (2007-). 
Further Reading
Fannie Mae:  www.fanniemae.com
Federal Housing Finance Agency:  www.fhfa.gov
Freddie Mac:  www.freddiemac.com
Ginnie Mae:  http://www.ginniemae.gov
Office of Federal Housing Enterprise Oversight:  www.ofheo.gov
Federal Deposit Insurance Corporation
 
Created at the height of the banking crisis in 1933, the Federal Deposit Insurance Corporation (FDIC) is an
independent government agency responsible for maintaining public confidence by providing deposit insurance for
banks and thrifts, monitoring and dealing with risks to depositor funds, and ensuring that bank or thrift failures
have minimal impact on the economy and financial system. The FDIC is self-supporting, funding its activities
through premiums paid by banks and thrifts for insurance on their deposits. The FDIC also invests in U.S.
Treasury securities. During the financial crisis of 2007–2009, the FDIC was forced to assume control of several
troubled banks, though no depositor lost any money in the process.
The FDIC has approximately 5,000 employees, six regional offices, and multiple field offices. Its five-member
board of directors is appointed by the president and confirmed by the Senate. The FDIC supervises more than
half of all U.S. banking institutions; it also acts as a backup overseer of other thrifts and banks and as a regulator
of state-chartered banks that are not members of the Federal Reserve. When a chartering authority—state
regulator, Office of Thrift Supervision, or comptroller of the currency—closes a bank or thrift, the FDIC commonly

sells deposits and loans to another institution and transfers customers automatically to the new bank.
The origins of the FDIC go back to the depression of the 1890s, when several leading politicians began to talk
about federal protection for depositors of failed banks. Democratic presidential standard-bearer William Jennings
Bryan, for one, proposed that funds be set aside by the federal government to help banks withstand financial
panic-induced runs by depositors. At about the same time, states began establishing deposit security programs.
With the creation of the Federal Reserve System in 1913, the United States created a central bank that would
become a lender of last resort to member banks, though these were larger and usually safer institutions.
The state–Federal Reserve combination worked well until the Great Depression. Between the stock market crash
of 1929 and the bank holiday of 1933, when newly inaugurated president Franklin Roosevelt closed the nation’s
banks to stop panic withdrawals, more than 9,000 banks across America shut down. The federal government
responded by merging failed banks with stronger ones and, after months of delay, paid depositors about 85
percent of their deposits.
While Roosevelt himself remained skeptical of a federal protection on bank deposits, many in his administration
and in Congress believed it was necessary. In 1933, Congress passed the Glass-Steagall Act, which established
the FDIC to restore public confidence in the financial system by insuring deposits. Although many bankers
opposed the concept of an insurance fund, the FDIC quickly covered 19,000 banking offices, guaranteeing
deposits up to $2,500 per depositor. With the establishment of the FDIC, the nation’s banking system stabilized
and only nine insured banks failed in 1934. Moreover, since the FDIC opened for business on January 1, 1934,
no depositor has ever lost any deposit in an insured banking account.
The FDIC became permanent under the Banking Act of 1935. The FDIC’s initial funding was $289 million, lent by
the U.S. Treasury and Federal Reserve, which it repaid in 1948. By the early 2000s, the insurance fund
exceeded $45 billion, covering more than $5 trillion in deposits.
The insurance ceiling guaranteed by the FDIC has risen periodically over time. Under the Banking Act of 1935, it
was raised to $5,000. The Federal Deposit Insurance Act of 1950 raised the limit to $10,000 and authorized FDIC
lending to a member bank in danger of closing if it were deemed essential to its community. Additional insurance
limit increases came in 1966, 1969, and 1974, with the $100,000 limit set by the Depository Institutions
Deregulation and Monetary Control Act of 1980. The current insurance limit was set at $250,000 per depositor on
October 3, 2008, as a temporary measure in response to the financial crisis.
The limit covers all accounts of a single depositor, but a depositor who owns multiple types of accounts—single or
joint accounts or retirement IRAs and Keoghs, for instance—is covered to the maximum for each type. Accounts
in separate banks are insured separately, but accounts in separate branches of the same bank are treated as one
account. FDIC insurance does not cover securities, mutual funds, or other bank and thrift investment vehicles.
The FDIC’s protections were utilized relatively infrequently during the economic boom of the post–World War II
era. Commercial banks, heavily regulated, were conservative in their lending practices and barred from engaging
in other potentially riskier businesses, such as investment banking. The savings and loan (S&L) crisis of the late
1980s and early 1990s was the first major postwar test of the FDIC. While thrifts are protected by their own
federal insurance program—Federal Savings and Loan Insurance Corporation (FSLIC)—the FDIC was forced to
step in to protect depositors when its sister agency ran out of funds. During the S&L crisis, lax regulation opened
the way to a glut of bad loans that cost the industry hundreds of billions of dollars.
The FDIC has also played a key role in the financial crisis of the late 2000s. Under its widely respected director,
Sheila Blair, the agency, in the spring of 2008, became one of the first to warn that the U.S. banking industry was
facing unprecedented stress due to the collapse of the subprime mortgage market. By the summer, the crisis was
full blown and the FDIC was forced to step in and cover the deposits of a number of failing banks. The best-
known case was that of IndyMac, a California bank with some $19 billion in deposits. The FDIC took over the
institution, creating what was known as a “bridge bank” to manage its assets and liabilities, before transferring its

assets to the newly created OneWest Bank.
By mid-2009, the FDIC had taken over more than seventy banks during the financial crisis. Although this was a
historically high number, it was well below the total for the early 1990s, during the aftermath of the S&L crisis.
Nevertheless, the failures left the agency’s Deposit Insurance Fund nearly depleted, down from more than $45
billion in mid-2008 to just over $13 billion in mid-2009. In response, the FDIC imposed an emergency fee on
member banks to shore up the fund. Under the Dodd-Frank Wall Street Reform and Consumer Protection Act of
2010, the FDIC was to establish a fund minimum of designated reserve ration (DRR) of 1.35 percent of all insured
deposits and provide dividends to the banking industry should the DRR exceed 1.5 percent. Prior to the law, the
DRR was 1.25 percent.
James Ciment and John Barnhill
 
See also:  Banks, Commercial;  Great Depression (1929-1933);  New Deal;  Regulation,
Financial. 
Further Reading
FDIC:  www.fdic.gov
Seidman, L. William. Full Faith and Credit: The Great S&L Debacle and Other Washington Sagas. New York: Times
Books, 1993. 
Federal Housing Administration
 
An agency of the U.S. Department of Housing and Urban Development (HUD), the Federal Housing Administration
(FHA) insures mortgages on homes, multifamily dwellings, manufactured (or mobile) homes, and hospitals. By
doing so, the FHA allows lenders to offer mortgages at more competitive rates for homebuyers with low incomes
or limited credit histories.
Lenders whose loans meet FHA standards enjoy several benefits. They are insured against default by mortgagors,
which allows lenders to be more flexible in calculating payment ratios and household income. For their part,
buyers can qualify more easily because the government, not the lender, assumes the risk. The mortgagor pays
the monthly insurance premium as part of the mortgage.
As of 2009, the FHA required less than a 3.5 percent down payment, compared to a conventional down payment
of 5 percent or more. Mortgagors can also borrow the costs of mortgage insurance and closing fees, whereas
conventional loans require payment of these costs with the down payment. The FHA also allows a larger
percentage of personal income to be spent on housing than would normally be the case with private lenders.
Since its creation in 1934, to help jump-start a housing industry crippled by the Great Depression, the FHA has
insured some 35 million mortgages, with nearly 5 million currently on its books. Because the agency maintained
strict standards regarding the types of mortgages it would insure, the FHA was not overly exposed to the
subprime mortgage crisis that began in 2007. Yet with the recession that followed, especially high unemployment
rates, many economic analysts fear a wave of foreclosures on FHA-insured properties, which might subject the
agency to tens of billions of dollars in losses. Typically, FHA-backed loans require a very small down payment,

leaving many homebuyers in upside-down mortgages—those in which the homeowner ends up owing more than
the house is worth, which lead many to abandon their properties rather than struggle to pay a mortgage they can
no longer afford.
Prior to the creation of the FHA, home mortgages were difficult to finance for most working-and lower-middle-
class Americans. Lenders often required a down payment of 50 percent or more, followed by several years of
interest-only payments, with a large balloon payment at the end. Thus, when the Great Depression began in the
1930s, just 40 percent of Americans owned the homes they lived in. With the credit markets frozen during the
early years of the Great Depression, the housing market collapsed as home-buyers found it difficult to get
financing to make their balloon payments. Foreclosure rates soared, dampening lender interest in financing new
construction. Annual housing starts fell from about 700,000 in the late 1920s to just 93,000 in 1933. Millions of
construction workers were laid off in the slump, adding to an unemployment rate that topped 25 percent.
The FHA offered a revolutionary new approach to mortgage financing. By backing loans 100 percent, it reassured
lenders that they would not be stuck with foreclosed properties. The agency also helped promote a new form of
mortgage, which became standard—one with a relatively small down payment, followed by principal and interest
payments that would amortize over a longer period, typically twenty or thirty years.
The FHA has helped promote homebuying ever since. In the late 1940s and early 1950s, it provided the insurance
for mortgages taken out by hundreds of thousands of returning World War II veterans. Beginning in the 1950s, it
was active in helping marginalized groups, including minorities and the elderly, secure mortgages. Today, African-
Americans and Latinos generate about one-fourth of FHA business, compared to less than one-tenth of
conforming conventional mortgages. Through the decades, FHA standards have fluctuated. When standards are
high, the number of loans drops; when standards are low the number of loans rises. In addition, during slumps in
the housing market, such as those in states hit hard by the oil price slump of the 1980s, the FHA has stepped in
to replace private mortgage insurers, thereby steadying housing markets.
While the FHA has backed millions of mortgages over the years, this figure is small compared to the overall
mortgage market. But by helping to foster easier mortgage terms for those who are eligible, the FHA has played
an outsized role in the U.S. housing boom of the post–World War II era, which has seen homeownership rates
(the percentage of occupied housing units being occupied by the owner) climb to more than two-thirds. But as
private lenders began to market mortgages to more and more homebuyers, conservative politicians and
commentators began to wonder why the federal government was in the business of insuring home mortgages,
and there were calls in Congress to abolish the FHA in the 1990s and early 2000s.
The subprime mortgage crisis largely ended such talk, as it became clear that the FHA’s twin role as a setter of
lending standards and as a backer of mortgages to financially less secure homebuyers were more crucial than
ever. Indeed, in 2008, the FHA came forward with two new programs to help troubled mortgagors. One allowed
the agency to refinance adjustable rate mortgages—whose monthly payments could soar once the initial “teaser”
rate expired—to fixed-rate thirty-year mortgages. The applicants, however, had to have at least 3 percent equity
in their homes and had to meet standard income qualifications. These requirements left many of the most
vulnerable unable to qualify.
For homeowners in difficult circumstances—specifically, those whose mortgage balance exceeded the market
value of the home and whose monthly mortgage payments exceeded 31 percent of gross income—a second
program was initiated. If the mortgagee could get the lender to agree to a 90 percent settlement on the principal,
the homeowner could refinance with an FHA-guaranteed mortgage at a thirty-year fixed rate. However, housing
experts worried that because lenders were not mandated to accept the partial write-down on the principal, few
would participate in the program.
Meanwhile, the FHA was facing operational problems that made its job even more difficult and its programs less
effective. Short of adequate staff, the agency was unable to properly police the mortgages it guaranteed, meaning
that many were offered by private lenders to persons who did not meet FHA standards for potential solvency.

This, say economic analysts, means that the FHA could be exposed to up to $100 billion in losses by 2015, as
many of the mortgage holders go into default and the agency is left to foot the bill to private lenders.
John Barnhill and James Ciment
 
See also:  Mortgage Lending Standards;  Mortgage Markets and Mortgage Rates;  New Deal. 
Further Reading
Federal Housing Administration:  www.fha.com
Hays, R. Allen. The Federal Government and Urban Housing. Albany: State University of New York Press, 1995. 
Wright, Gwendolyn. Building the Dream: A Social History of Housing in America. Cambridge, MA: MIT Press, 1983. 
Federal Housing Enterprise Oversight, Office of
 
In operation from 1992 through 2008, the Office of Federal Housing Enterprise Oversight (OFHEO) was
responsible for maintaining a stable housing sector, largely by collecting data and through oversight of the quasi-
governmental mortgage insurers the Federal National Mortgage Association (Fannie Mae) and the Federal Home
Loan Mortgage Corporation (Freddie Mac). Lacking enough funding and the power to impose changes on the
insurers, OFHEO, say many experts, was unable to prevent the excesses of the subprime mortgage market in the
mid-2000s—excesses that helped lead to the financial crisis of 2008–2009.
OFHEO was established by the Federal Housing Enterprises Financial Safety and Soundness Act in 1992 as an
agency within the Department of Housing and Urban Development (HUD), with the broad mission of supporting a
stable housing sector. To that end, it promoted more available and affordable home financing, meeting affordable
housing and homeownership goals set every year by the secretary of HUD and, most importantly, ensuring that
Fannie Mae and Freddie Mac, which owned or guaranteed more than 40 percent of residential mortgages in the
United States, remained financially safe and sound. OFHEO did not receive taxpayer dollars but instead was
funded through assessments on Fannie Mae and Freddie Mac. (Note that Fannie Mae and Freddie Mac were put
into conservatorship by the U.S. government on September 7, 2008.)
After 1996, OFHEO also published the quarterly (later, monthly) house price index, measuring average seasonally
adjusted price changes in repeat sales or refinancing on the same single-family properties in different geographic
areas. The index was based on purchase prices of houses backed by mortgages that had been sold to or
guaranteed by Freddie Mac or Fannie Mae since 1975. Between 1975 and 1995, there were 6.9 million of these
repeat transactions. The sole use of these repeat transactions in the house price index minimized the problem of
quality differences and thus, the index also has been called a “constant quality” house price index.
To fulfill its goals, OFHEO had to examine Fannie Mae and Freddie Mac regularly and provide an annual report of
the results of these examinations to Congress. In addition, OFHEO had to adjust Fannie Mae and Freddie Mac
loan limits every year, develop new risk-based capital standards, calculate capital adequacy, simulate stressful
interest rate and credit risk scenarios, prohibit excessive executive compensation, issue regulations concerning
capital and enforcement standards, oversee the reporting of suspected or actual mortgage fraud (in cooperation
with the Financial Crimes Enforcement Network), and take necessary enforcement actions.

According to its strategic plan for 2006–2011, OFHEO initiated a special examination of Fannie Mae and Freddie
Mac in 2003 and identified serious accounting, internal control, and management weaknesses. Misstated earnings
were estimated to be $16 billion, resulting in fines of $500 million and lawsuits totaling over $1 billion, while
remedial costs exceeded $2 billion. Moreover, their portfolios of mortgage assets grew at annual rates that
considerably exceeded residential mortgage market growth. As a result of these findings, OFHEO strengthened its
efforts to address these problems and to check that Freddie Mac and Fannie Mae were making required
improvements. However, it lacked independent funding authority and bank regulator–like powers to reduce the
possibility of a systemic disruption in the financial sector. This limited its capacity to implement long-term planning
and negated its ability to react quickly if serious problems emerged with the two enterprises—for instance, if they
needed assistance during an exceptional economic crisis.
With the Housing and Economic Recovery Act of 2008 (signed by President George W. Bush on July 30, 2008),
Congress abolished OFHEO and combined its functions with those of the Federal Housing Finance Board (FHFB)
and HUD’s mission group to form the new Federal Housing Finance Agency (FHFA). All regulations, orders, and
determinations of these agencies continued to be incorporated into the new agency while its authority was
expanded to increase its ability to oversee the country’s secondary mortgage markets.
FHFA started regulating the activities of the Office of Finance and fourteen housing-related, government-
sponsored enterprises (GSEs): Fannie Mae, Freddie Mac, and the twelve Federal Home Loan Banks (in Atlanta,
Boston, Chicago, Cincinnati, Dallas, Des Moines, Indianapolis, New York, Pittsburgh, San Francisco, Seattle, and
Topeka). These banks were created in 1932 for providing additional funds to local lenders for financing loans for
home mortgages. They provide liquidity for more than 8,000 member lenders, mainly through two key housing
programs—the Affordable Housing Program and the Community Investment Program—and this allows those
lenders to continue financing the purchase, construction, or rehabilitation of affordable owner-occupied or rental
housing and the economic development of low-to moderate-income neighborhoods during economic crises. In
June 2008, the combined debt and obligations of these fourteen GSEs was $6.6 trillion. They also purchased or
guaranteed 84 percent of new mortgages. The regulation of twelve Federal Home Loan Banks was previously
performed by FHFB.
The Federal Housing Finance Agency also began publishing the FHFA monthly index (the former OFHEO monthly
house price index) for nine census divisions, the fifty states and the District of Columbia and all 363 metropolitan
statistical areas (11 of the metropolitan statistical areas are further divided into 29 metropolitan divisions). Index
values are only provided for periods where at least 1,000 transactions have been made.
Tiia Vissak
 
See also:  Fannie Mae and Freddie Mac;  Housing Booms and Busts. 
Further Reading
Federal Housing Finance Agency:  www.fhfa.gov
Federal Housing Finance Board:  www.fhfb.gov
Smith, John, ed. The Rescue and Repair of Fannie Mae and Freddie Mac. Hauppauge, NY: Nova Science, 2009. 

 
Federal Reserve System
 
The Federal Reserve System—often referred to as the “Fed”—is the American approximation of a central bank.
Like the central banks of many other countries, the Fed regulates banks and other financial institutions and sets
the federal government’s monetary policy, with the goals of maintaining a stable financial sector, reining in
inflation, and assuring sustained economic growth. The Fed differs from most other central banks in that it has a
decentralized structure with twelve regional banks, but its most important decision-making remains with the central
board of governors.
The Federal Reserve System—consisting of twelve regional banks—was established in 1913 to serve as the
central bank of the United States. The board of governors meets at the Marriner S. Eccles Federal Reserve Board
Building in Washington, D.C. (Bloomberg/Getty Images)
 Operations
Founded in 1913, the Federal Reserve System consists of a board of governors seated in Washington, D.C., and
twelve regional Federal Reserve banks. The Fed is the primary institution for setting monetary policy in the United
States. It has several tools at its disposal to achieve this, the most important being its purchase and sale of U.S.
Treasury and other federal agency securities. The Fed also establishes the minimum capital requirements its
member banks must maintain against outstanding obligations and it sets the discount rate—the rate it charges
thousands of private member banks to borrow money. (All federally chartered banks must be members of the Fed,

while state-chartered banks can apply for membership.) The discount rate—which is nominally set by the Reserve
banks but is in fact controlled by the board of governors—is more of a symbol, signifying the Fed’s open market
actions of buying and selling federal securities. In addition, since the 2008 financial crisis, the Fed has also
supported bank liquidity by directly buying bank assets, a new policy whose future is unclear.
Regulating the expansion of the nation’s money supply, a key tool in fighting inflation or spurring economic growth,
is the responsibility of the Federal Open Market Committee (FOMC), which consists of the board of governors, the
president of the Federal Reserve Bank of New York—the most important of the regional banks—and, on a
rotating basis, the presidents of the other eleven regional banks.
The board of governors oversees the operations, reviews the budgets, and otherwise controls the activities of the
twelve regional banks. At the same time, those regional banks act as the operating arm of the Fed. The regional
banks resemble private corporations—and are often confused as such—issuing stock to member banks. However,
the stock is not issued for the purposes of making a profit but because member banks by law must hold a specific
amount of Fed stock. Dividends are set at 6 percent per annum, and the stock cannot be sold, traded, or used as
security. The Fed is both a not-for-profit and a nonprivate entity.
The Fed enjoys a great deal of independence, sometimes attributed to the goal of political independence,
preventing, for example, a situation in which politicians might want to expand the money supply and produce
economic expansion in pre-election periods, thereby jeopardizing long-term stability. Members of the board of
governors are appointed by the president—and confirmed by the Senate—for a single fourteen-year term, with the
seven members’ appointments staggered every two years. The president also chooses—and the Senate also
confirms—the chair and vice chair from the board for a renewable four-year term. Along with these relatively long
terms of office, the board’s freedom of action is assured by its independent source of funding. Rather than relying
on tax dollars—and, hence, Congress—the board pays for its operations through interest on federal securities,
interest on foreign currency holdings, and loans to member banks, investments, and fees for services, such as
check clearing and transfer of funds. Any excess of income beyond the Fed’s own needs goes to the U.S.
Treasury.
Critics of the Fed point out that independence gives the Fed wide latitude to follow economic policies in line with
its primary constituency, the banking system, thus perhaps paying less attention to consumer interests and
worrying more about banks’ fears of inflation than concerns about unemployment. There is potential oversight of
the Fed, including most importantly, twice-annual reports to Congress and occasional congressional debate when
a new governor is under consideration or new responsibilities are considered for the Fed. In addition, the Fed
banks and board are reviewed annually by an outside auditor. Other watchdogs include the Government
Accountability Office and the board’s own inspector general.
 History
Antecedents of the Fed go back to 1791 and the creation of the First Bank of the United States. Modeled after the
Bank of England, it was part of Secretary of the Treasury Alexander Hamilton’s efforts to secure a sound financial
footing for the nascent republic. Funded largely by government capital, along with some from private investors, the
bank issued notes, or currency, and lent money to the government. In 1811, the bank’s charter lapsed and it took
five more years for the Second Bank of the United States—which was also given a twenty-year charter—to begin
operation. Like the First Bank of the United States, the second was not popular with those who felt it gave the
federal government too much control over the nation’s economy. In 1836, President Andrew Jackson vetoed its
rechartering, ending America’s first attempts at maintaining a central bank.
For the next twenty-seven years, the U.S. banking system remained a hodgepodge of state-chartered or privately
owned banks, each issuing its own banknotes, and all competing to keep their money at face value. The diverse
types of money and highly variable reliability of the different banks made interstate economic activity difficult. In
1863, Congress established a system of “national” banks, with standard operations, minimum capitalization, and
rules for lending and administering loans. A 10 percent tax on nonfederal currency effectively removed all but the

federal currency from circulation.
Still, many economists argued then and since that the lack of a central bank exacerbated the volatility of the U.S.
economy in the nineteenth and early twentieth centuries, since it denied the federal government any effective
means for setting a monetary policy that might smooth out the business cycle.
While private bankers, led by J.P. Morgan, were able to rescue the nation’s financial sector from the effects of the
Panic of 1907, many financiers and economists concluded that it was time America—now the world’s leading
industrialized country—had a central bank. But the politics were tricky. While easterners wanted the economic
stability a central bank would provide, with its power to regulate the money supply, westerners and southerners
feared that the bank would choke off the easy credit they needed to expand their relatively underdeveloped
economies. In 1913, Congress compromised by creating the noncentralized central bank known as the Federal
Reserve.
But if stabilizing the economy via the money supply was the intended goal of the Fed, it did not do such a good
job in its first decades of operation. Many economic historians say its rapid expansion of the money supply fueled
the real-estate and stock market bubbles of the 1920s, while its rapid contraction of the money supply deepened
the effects of the Great Depression.
After World War II, the Fed adopted the dominant Keynesian economic paradigm of countercyclical monetary
policy. That is, it lowered interest rates during recessions, hence expanding the money supply, and it raised the
rates during times of expansion. But the paradigm was tested in the 1970s, when such countercyclical actions
failed to control an economic phenomenon known as “stagflation,” a combination of slow or negative growth and
high inflation.
The monetary school rose up in the postwar era in response to the Keynesians. Led by University of Chicago
economist Milton Friedman, the monetarists argued that economic growth was best assured by creating a stable
monetary supply, expanding only to meet the needs of a growing economy rather than trying to control economic
expansion. Such thinking came to be incorporated into Fed decision-making during the late 1970s and early
1980s, but this was short-lived.
Under the chairmanship of Paul Volcker, which began in 1979, the Fed imposed a dramatic tightening of credit—
and the money supply—as a way to wring inflation out of the economy. It was effective, though at a cost—the
1981–1982 recession that many economists agree was triggered by the Fed’s action was the worst since the
Great Depression.
During the 1980s, the focus gradually shifted toward attaining a specified level of the federal funds rate, a process
that was largely complete by the end of the decade. Beginning in 1994, the FOMC began announcing changes in
its policy stance, and in 1995 it began to explicitly state its target level for the federal funds rate. Since February
2000, the statement issued by the FOMC shortly after each of its meetings usually has included the committee’s
assessment of the risks to the attainment of its long-run goals of price stability and sustainable economic growth.
 2007–2009 Financial Crisis
Many also blame the Fed for the dot.com stock market bubble of the late 1990s and early 2000s and the housing
bubble of the early and mid-2000s that triggered the worst financial crisis since the Great Depression. Eager to lift
the economy out of the recession of 2001–2002, Chairman Alan Greenspan dramatically lowered the prime
interest rate the Fed charged member banks, from 6 percent in 2000 to just 1 percent in 2003. Moreover, the Fed
was very slow to raise the rate again, despite growing evidence that the low rates were fueling an unsustainable
run-up in housing prices. (Since most people borrow money to buy homes, interest rates are a key factor in
housing prices.)
When the housing bubble burst, setting off the financial crisis of 2007–2009, the Fed, which had been gradually
raising rates, reversed itself again. In 2002, before becoming Fed chairman, Ben Bernanke, a student of the Great

Depression, indicated that he supported the theory Milton Friedman and Anna Schwartz set forth in their Monetary
History of the United States, 1867–1960, which said that rather than keeping the downturn from taking banks
under, as was its mandate, the Fed exacerbated the Great Depression by contracting the money supply by one-
third and letting one-third of U.S. banks, generally smaller ones, fail under a philosophy of weeding out the unfit.
Bernanke indicated that he did not intend the Fed to be the culprit when the next cycle turned downward. In 2007
he got his chance. The Fed has historically intervened aggressively in America’s financial crises, and the bust of
2007–2009 was no exception. In December 2008 the board cut the key interest rate to what it called a “target
range” of zero to 0.25 percent. The cut was the ninth in fourteen months. The FOMC cited deteriorating labor
markets and slowed consumer spending, business investment, and industrial production, as well as stressed credit
and financial markets. At the same time it indicated that it did not intend to raise rates anytime soon.
But the Fed did more than merely lower interest rates. In November 2008, it began buying the mortgage-backed
securities at the heart of the crisis in an attempt to shore up the housing market. The November purchases were a
partial realization of the Fed’s decision to spend $500 billion on mortgage-backed securities backed by Fannie
Mae and Freddie Mac, the quasi-governmental mortgage insurers, and to spend another $100 billion buying
mortgages held by Fannie Mae, Freddie Mac, and the Federal Home Loan Banks directly. The loans were
reported as investment grade, not the subprime packages that had sparked the crisis in 2007. The intent was to
lower the cost of mortgages and make loans more readily available.
Announcement of the plan cut rates about half a percentage point and produced a noticeable increase in
mortgage refinancing. However, while many economists believe the Fed’s moves helped slow the rapid decline in
housing prices, it was unable, as of late 2009, to lift that key sector of the economy out of its worst slump of the
post–World War II era.
John Barnhill and James Ciment
 
See also:  Banks, Central;  Banks, Commercial;  Bernanke, Ben;  Burns, Arthur;  Greenspan,
Alan;  Monetary Policy;  Regulation, Financial;  Volcker, Paul. 
Further Reading
Federal Reserve System:  www.federalreserve.gov
Greider, William. Secrets of the Temple: How the Federal Reserve Runs the Country. New York: Simon & Schuster, 1989. 
Hetzel, Robert L. The Monetary Policy of the Federal Reserve. New York: Cambridge University Press, 2008. 
Fellner, William John (1905–1983)
 
William John Fellner earned a doctorate in economics from the University of Berlin in 1929, joined the faculty of
the University of California– Berkeley, in 1939, became a U.S. citizen in 1944, and became a full professor at
Berkeley in 1947. He left Berkeley for Yale University, becoming a professor of economics there in 1952, and
retiring in 1973.
Beginning in 1964, Fellner chaired, with Princeton University professor Fritz Machlup, a series of conferences that
drew thirty-two economists from academic institutions around the world for the purpose of discussing alternative

exchange rate regimes to replace the gold standard that had been established by the Bretton Woods Agreement
in 1944. The group, known as the Princeton-Bellagio Study Group on International Monetary Reform, met
eighteen times between 1964 and 1977 in Bellagio, Italy, Washington, D.C., and Princeton, New Jersey, as well as
in eight other European centers.
In 1969, Fellner became president of the American Economic Association. He was appointed in 1973 to the
Council of Economic Advisers, taking on the assignment in the same year that Richard M. Nixon resigned the
U.S. presidency, and concluding his tenure in 1975. Thereafter, he consulted for the Congressional Budget Office
on issues of taxation and inflation and returned to a life of scholarship with the American Enterprise Institute.
 Competition Among the Few
Fellner’s book Competition Among the Few (1949) describes the tacit collusion of big firms within the same
industry to protect their common interests, control competition, and avoid conflict, taking their prices from the most
dominant firm in the industry without any direct contact. Fellner’s work made a significant contribution to our
understanding of industry concentration and isomorphism (sameness), and of the impact of large firms on wages,
prices, and inflation.
 Wage and Price Controls in a Full Employment Strategy
Fellner believed that the pursuit of a full employment strategy—a condition in which every individual who is willing
and able to work at the prevailing wages and working conditions does so—led monopolistic groups of industrialists
and workers to consistently raise wages and prices, thereby accelerating inflation and resulting in a government
policy of wage and price controls. In Fellner’s view, unemployed individuals holding out for higher wages should
not be included among measures of the involuntarily unemployed. He advised raising the target unemployment
rate from 4 percent to 5 percent to deal with the inevitable frictions in the employment market. Fellner advocated
implementing a federally subsidized employment program to offset the hardship of the 5 percent target and
government spending aimed at avoiding big recessions while letting small ones run their course.
 Rational Expectations and Credibility: Impact on Inflation
Fellner is associated with the rational expectations theory, which asserts that economic outcomes do not differ
regularly or predictably from what people expect them to be, and with the concept of “credibility” as it is applied to
policy makers. In his book Towards a Reconstruction of Macroeconomics (1976), Fellner argued that there is a
game of strategy going on between policy makers and the public: each anticipates and acts on assumptions about
the other’s future responses. Further, the public attaches probability judgments to the way in which the behavior of
the authorities may be influenced by the behavior of the public. Hence, expectations based on presumed future
behavior underscore the importance of credibility. For example, government policy to combat inflation will be
effective only if businesses and workers are convinced that the rising unemployment and declining real output
brought on by demand-management strategies will not be followed by a reversal of policy. If government policy
lacks credibility, then wages and prices will be rolled back, and a reduction in real output will be required to
control inflation.
Although Fellner had reservations about some of the propositions of rational expectations theory and the emerging
monetarist view, he felt that both offered policy guidance that was superior to the mix of policy solutions known as
“neo-Keynesian fine-tuning” because monetarism focused on a single policy variable—namely, the money supply.
Carol M. Connell
 
See also:  Council of Economic Advisers, U.S.;  Deflation;  Fisher’s Debt-Deflation Theory; 
Inflation;  Price Stability;  Wages. 

Further Reading
Fellner, William J. Competition Among the Few: Oligopoly and Similar Market Structures. New York: Alfred A. Knopf, 1949. 
Fellner, William J. Towards a Reconstruction of Macroeconomics: Problems of Theory and Policy. Washington,
DC: American Enterprise Institute for Public Policy Research, 1976. 
Sobel, Robert, and Bernard S. Katz, eds. Biographical Directory of the Council of Economic Advisors. New
York: Greenwood, 1988. 
Spulber, Nicolas. Managing the American Economy, from Roosevelt to Reagan. Bloomington: Indiana University
Press, 1989. 
Financial Development/Deepening
 
The concept of financial development/deepening has two defining aspects. First, financial development/deepening
represents growth in the quantity of financial assets relative to gross domestic product. The higher this ratio, the
greater the opportunity for individuals to use their savings productively and for investors to obtain financing for
their investment plans. Second, financial development/deepening yields an increase in the variety of assets
available to savers and investors, thus allowing for risk diversification. In short, financial development/deepening
involves an increase in both the quantity and the variety of assets that the financial system offers to savers and
investors.
The role of the financial system in any market economy is to serve as an intermediary between those who wish to
save (consume less than their income) and those who wish to invest. Many individuals who save are not prepared
to invest because they lack the skill, knowledge, or inclination to incur risk. Those who are willing to invest may
not be able to finance their ventures with their own savings, as the investment may require levels of funding that
are beyond any individual’s saving capability. Thus, the financial system collects the resources of the savers and
makes them available to investors. A return is paid to those who save, and interest is charged to those who borrow
(investors). The interest rate charged to borrowers (investors) exceeds that paid to savers—this is how the
financial system profits by acting as an intermediary.
The financial system offers savers a variety of assets or forms in which to hold their savings. These include
simple savings accounts offered by banks; accounts at nonbank intermediaries, such as brokerage firms, credit
unions, and savings and loan institutions; and accounts at stock and bond exchanges. These assets vary in the
degree of risk that is borne by the purchaser of the asset. This makes possible the diversification of individual
wealth holdings, which, in turn, reduces the overall risk borne by savers and investors.
 Economic Impact: Long Run
The economic impact of financial development/deepening is best understood by distinguishing between short-run
and long-run impacts. Analysis of the short-run impact focuses on the role of financial development/deepening in
the business cycle. The long-run impact refers to the cyclical rise and fall to which all market-based economic
systems are subject. That is, market-based systems go through periods of expansion, in which unemployment
falls, followed by periods of contraction, in which unemployment rises. These cyclical episodes typically last three
to ten years, depending on a diverse set of factors including credit markets and levels of inventory. Analysis of the
long-run impact of financial development/deepening focuses on the growth of gross domestic product over periods

of ten years or more. One can view the economy as cycling up and down around, it is hoped, a steadily rising
standard of living (in both the short and long run).
What is the impact of financial development/deepening on long-run growth? There are two broad schools of
thought on this question. The first view sees financial development/deepening as a result of economic growth. In
this view, as the real economy (production and distribution) grows and develops, businesses and individuals find
themselves in need of more and a greater variety of financial assets. This creates a profitable opportunity for the
financial system to expand and diversify to meet the growing demand. According to this explanation, growth of the
real economy causes the development/deepening of the financial sector, not the other way around. An economist
would say that causality runs from the growth of the real economy to the development/deepening of the financial
sector.
The second school of thought argues the opposite view. That is, financial development/deepening enhances the
rate of growth in the real economy. More explicitly, financial development/deepening causes economic growth,
rather than the reverse. The financial sector enhances the efficiency of the economy by making it easier to match
savings with investment opportunities. In addition, saving and investment depend on the amount of financial
assets available and their diversity—more of the latter creates more of the former. Finally, financial
development/deepening is likely to increase the rate of technical innovation in an economy. Investors not only
build more factories, shops, and so on, they also build better ones. Investment usually brings new technology as
well as expanded facilities. The easier it is for investors to tap into a pool of savings, the faster new innovations
will be introduced. Thus, financial development/deepening leads to more savings, greater efficiency, and more
rapid innovation—enhancing long-run growth.
These opposing schools of thought set off a series of experiments to empirically test which hypothesis is correct.
Does growth cause financial development/deepening, or does financial development/deepening cause growth?
The consensus is that causality runs in both directions, but the evidence remains inconclusive as to which
direction is more important. Nevertheless, economists agree that financial development/deepening is a critical
component of long-term economic growth.
 Economic Impact: Short Run
While financial development/deepening has a positive role in the long-term growth of the economy, in the short
run, it is thought to increase the fragility of the economic system. That is, as the quantity and variety of assets
expand, this results in a greater likelihood of increased financial volatility or instability, a greater likelihood of
booms (expansions) and busts (contractions), and intensified levels of booms and busts. The following discussion
will focus on a bank-centered financial system.
Banking systems operate on the “fractional reserve” principle. That is, individuals deposit money into a bank, in
return for which they hold some sort of account. The bank, in turn, keeps reserve assets—such as cash in its
vault—in an amount equal to a fraction of the deposit liabilities on reserve at the bank, and it loans out the rest of
the money. When banks make loans, they create money. All banks operate in this manner. Because the proceeds
of the loans made by banks often are deposited at other banks, those banks can use the new deposits to make
more loans. The result is that an initial deposit into the banking system creates a multiple expansion of loans
(credit). In addition, banks within the system are linked by a web of financial relationships—the loans of Bank A
may serve as the reserves for Bank B, and so on.
At any point in time, no bank has enough reserves on hand to pay all of its depositors should they demand their
money. But under normal circumstances, people leave most of their money in the financial system. There may be
some withdrawals, but these generally are offset by new deposits.
During periods of expansion, the economy is growing, profits for businesses are rising rapidly, and the prospects
for future investment look bright. In these periods, banks seek to minimize their reserves in order to expand
lending and credit. The information that is available to the financial system tends to reinforce views of a continued

expansion, leading to further lending. The deeper and more developed the financial system becomes, the more
extensive this process will be. The quality of the loans made by banks for investment tends to decline as
information becomes less reliable.
The turning point in the foregoing process can occur in several ways. If some of the investments financed through
bank lending fail, the depositors of these banks may seek to withdraw their money, fearing a collapse. The
threatened banks may call in their loans (or stop making new loans) in order to generate funds to pay depositors.
Other firms in the economy will find their position threatened as well. As credit becomes scarcer, some of these
firms will default on loans to other banks. Those banks, in turn, will recall loans and cut back on credit. As one
can see, a self-reinforcing contraction begins.
This process may unfold in different ways. The expansion of credit may cause a general inflation in prices, or it
may cause the prices of particular assets, such as real estate, to rise. As the latter occurs, the owners of this
asset (real estate) find that the increased value can be used as collateral for additional borrowing, and banks, in
an expansionary phase, are likely to increase lending. As the value of real estate continues to increase, an asset
bubble begins to form—that is, the asset’s value rises above its long-term value, as determined by its
income/earning prospects.
When the bubble bursts—that is, when real-estate values begin to fall back toward their long-run equilibrium
levels—the collateral behind the loans extended by the financial system begins to shrink. Once again, a number
of banks will find their situation deteriorating, and depositors will begin to withdraw their deposits, leading to further
financial contraction.
In an attempt to keep financial systems stable, most governments act as guarantors. When credit begins to
contract, depositors, fearful for the value of their deposits, often will run to draw their money out, thus
exacerbating the contraction. Most central governments in developed countries promise to step in and provide the
funding so that depositors always will be able to get their money, no matter what (this is known as deposit
insurance). Assured by the government’s guarantee, depositors have no reason to rush to the financial system to
withdraw their money. This tends to stabilize the system.
However, the problem with this system is that it creates “moral hazard.” Banks and other financial institutions know
that the government will step in and solve the problem. As a result, they have an incentive to increase the
riskiness of their investments. If things work out, they will make extremely high profits. If they do not, the
government will step in and provide the necessary resources in times of financial difficulty. In addition, depositors
have no incentive to verify the financial viability of their bank, because the state has insured them against default
by the lending institution. Thus, the entire financial system becomes subject to even greater probability of failure.
The implication is that the stability of the financial system depends critically on its regulatory institutions. This
institutional structure must act as a watchdog to restrain banks and other financial institutions from engaging in
increasingly risky investments. This is difficult, as the financial system is constantly developing new types of assets
for diffusing risk and mobilizing savings. In the long run, this will enhance overall growth, but if regulatory
institutions are unable to adjust (keep up), then the riskiness of the financial system will increase, thus raising the
probability of collapse. Thus, it is essential that regulatory institutions be given the resources necessary to keep
up with technical innovations in the financial sector.
Ultimately, however, the stabilization of the financial system depends on the ability to restrain credit expansion so
as to prevent a rapid expansion of credit (which ultimately will lead to a contraction). In many countries, the central
bank plays this role. Using monetary policy tools, the central bank can drain reserves from the financial system,
reducing the extent of credit expansions. In turn, during contractions, the central bank can inject reserves into the
system, giving banks additional means for extending loans. The problem, of course, is determining when it is the
right time to restrain a credit expansion. If the central bank acts too soon, it will restrain normal growth processes,
reducing the future standard of living. If it acts too late, the likelihood of financial collapse increases dramatically,
as does the severity of the collapse.

 Implications
Financial development/deepening occurs as the quantity of financial assets grows and as the variety of such
assets expands. In the long run, this increases economic growth by matching savings with investment, stimulating
increased saving (and investment), and promoting investment in new technology. In the short run, however,
financial development/deepening tends to increase financial instability. More assets and a greater variety of assets
lead to greater credit expansions and contractions. In order to dampen this instability, say many experts, the state
must invest sufficient resources in financial regulatory institutions. Central banks also must become more adept at
dampening excess credit expansion.
Richard Grabowski
 
See also:  Financial Markets;  Savings and Investment. 
Further Reading
Cooper, George. The Origin of Financial Crises: Central Banks, Credit Bubbles and the Efficient Market Fallacy. New
York: Vintage, 2008. 
King, Robert.  “Finance and Growth: Schumpeter Might Be Right.” Quarterly Journal of Economics 108:3 (August
1993): 717–737. 
Rajan, Raghuram G., and Luigi Zingales.  “Financial Dependence and Growth.” American Economic Review 88:3 (June
1998): 559–586. 
Shaw, Edward S. Financial Deepening in Economic Development. New York: Oxford University Press, 1973. 
Valderrama, Diego.  “Financial Development, Productivity, and Economic Growth.” Federal Reserve Bank of San Francisco
Economic Letter 2003:18 (June 2007): 1–3. 
Financial Markets
 
Financial markets are the figurative space in which financial instruments—such as corporate equities and bonds—
are bought and sold. Highly regulated in the aftermath of the financial crisis that led to the Great Depression in the
1930s, financial markets in the United States and in much of the world were liberated from government constraints
beginning in the 1970s. That deregulation, along with the introduction of new technologies and new financial
instruments, allowed for much innovation and rapid growth in financial markets, but also increased instability, as
the financial crisis of 2008–2009 demonstrated.
 Four Types of Finance
Finance means different things in different contexts. Consider the following four types of finance: corporate,
personal, public, and international.
Corporate finance concerns the means by which firms raise capital to make investments. It examines the relative
efficacy of issuing bonds (debt) or stock (equity) and the impact of the method of finance on firm performance and
investor returns. It also concerns the use and valuation of other financial claims such as options, convertible

bonds, and preferred stock. Personal finance looks at the way in which individuals or families save money and
insure themselves. It deals with matters such as pension plans, deferred savings plans, insurance, and income
taxes. Public finance pertains to government’s use of taxes to raise revenues and its expenditure of these funds. It
deals with deficits and debt, as well as the connection between the timing of benefits from government projects
and the collection of funds for their financing. International finance focuses on exchange rates and international
financial flows. It considers the relationship between capital flows and trade in goods and services, as well as
interest rate differentials across countries. All four types of finance are interrelated and deal with the same
fundamental issues: time, risk, and uncertainty.
 Financial Prices
The most common financial price is the interest rate, or “rate of return.” This rate may be viewed as a contingent
claim. For example, if the interest rate is 25 percent, the price of 100 current dollars is equal to 125 future dollars
—(1 + 0.25) 3 100—and the price of 100 future dollars is equal to 80 current dollars—[1/(1 + 0.25)] 3 100.
Economists typically consider the pure interest on risk-free assets, independent of any default risk. For a lender
buying a contingent claim for future payment, uncertainty regarding the future payment (default risk) will increase
the interest rate that the lender requires. In this case, the interest rate includes two components: a risk-free return
and a risk premium.
In insurance markets, a premium is the price charged for monetary payments in the event of a bad outcome.
Consider a policy for which the customer pays a $1,000 premium in exchange for an $8,000 claim to be paid in
the event of a bad outcome. Suppose the probability of a bad outcome is 10 percent. The insurance company
(insurer) sells a contingent claim of $8,000 to the customer (insured) with an expected payout of $800 ($8,000 3
10 percent = $800) for a price of $1,000 with certainty. Although the bad outcome, if it occurs, will take place in
the future, this time dimension typically is not considered in an insurance context. With perfect information, the
insured is willing to pay $1,000 for a contingent claim with an expected value of $800 in order to reduce his or her
risk of an $8,000 loss if the contingency (bad) outcome occurs.
 Micro/Macro, Real/Nominal Distinctions
The purchase or sale of insurance, and the saving and lending behaviors of individual decision makers, may be
analyzed in detail in a microeconomic context. In this case, preferences, present value calculations, and
expectations of the probabilities of different outcomes are the focus. Transactions that are based on differences in
preferences, a desire to pool or diversify risk, or different productive real investment opportunities benefit both
parties to the transaction, even if they possess the same information. Such transactions represent a positive-sum
game because both parties benefit. However, some financial transactions may be based on differences in beliefs
regarding changes in future conditions or the probabilities of particular outcomes. Trades based on such
differences generally represent a zero-sum game—that is, the better-informed party will benefit at the expense of
the other.
As is true in nonfinancial markets, when aggregated, individual decisions and transactions by individuals, firms,
governments, and foreigners will have broader macroeconomic effects. Because financial assets may be “moved”
across locations at virtually no cost, financial prices such as interest rates are nearly identical across national
markets. Thus, the macroeconomic and microeconomic views of finance are particularly intertwined. Moreover,
financial markets greatly affect other markets throughout an economy. For example, if equity prices on the New
York Stock Exchange increase, high-tech firms in Silicon Valley will find it easier to raise investment funds through
initial public offerings. This may influence their decision to expand production. In addition, if potential customers
are stockholders who experience an increase in wealth, they may increase their demand for the high-tech firms’
output.
Some classical economists speak of a dichotomy between real and nominal factors, in which real values are not
affected by monetary values. For example, if the money supply doubles, and all prices (including wages and

nominal wealth) double, in theory, there need not be any real effect. However, real-world experience is filled with
cases in which monetary or, more generally, financial problems have had a significant, measurable impact on
“real” values such as output and employment. Simple analogies may be instructive in conceptualizing the relation
between monetary and nominal factors.
An automobile engine needs an appropriate amount of oil for lubrication in order to operate properly. Although the
physical engine parts, when combined with gasoline, provide the power, if there is too little oil, the engine will
experience increased friction. This could impair its performance and cause permanent damage. Similarly, too much
oil may adversely impact the real workings of the engine as well. Just as an engine needs an appropriate amount
of oil to operate efficiently, an economy needs an appropriate amount of money, or “financial liquidity.” By this
analogy, the real effects of money become visible when there is too much or too little.
Another analogy might be considered, this time with “liquidity” representing the availability of credit in an economy.
Visualize a real economic landscape comprising productive farmland and some arid, nonproductive wasteland. The
proper amount of liquidity (credit) channeled toward the farmland (truly productive sectors of the economy) could
lead to a more verdant landscape (vibrant real economy with rapid growth). Too little credit channeled toward
productive sectors could spur a drought, with dry and unproductive but fertile farmland. Excessive credit, however,
could flood the farmland. And liquidity funneled toward the arid land could cause erosion and flash floods—
analogous to harmful speculative bubbles.
 Financial Intermediaries as “Middlemen”
In a well-functioning economy, financial intermediaries channel investment funds from savers toward borrowers
with productive real investment projects. They act as “middlemen,” not directly producing goods and services, but
enhancing the real productivity of other economic agents. They provide real benefits to savers, reducing individual
risks or transferring risk to parties who are better able and willing to assume it. A middleman earns a profit by
buying at a low (wholesale) price and selling at a higher (retail) price. In a competitive market, a successful
middleman must provide truly valuable services, such as moving products geographically or repackaging them.
Similarly, although financial intermediaries do not produce physical goods themselves, they provide very real
benefits to market participants.
Banks, savings and loan institutions, and credit unions all accept deposits, make loans, and facilitate payments by
customers. They reduce transaction costs by providing checking and credit services, and by reducing the risk
faced by individual investors or depositors. Although the funds provided to a bank by depositors may be loaned
out, each individual depositor faces virtually no default risk, as the bank pools the funds of many depositors
together (in addition, most developed countries offer deposit insurance). Moreover, a bank’s experience and
expertise in lending allows it to better evaluate the likelihood that any one borrower will default. Financial
conglomerates, securities firms, and mutual funds accept investment funds, pool them, and channel them toward
truly productive investments. Their expertise should enable them to “pick the winners” and reduce transactions
costs. Investors may be individuals or other intermediaries such as insurance companies or pension funds. By
pooling the risk of individual policyholders, insurance companies aim to reduce risk for their customers.
It is useful to conceptualize financial intermediaries as simplified balance sheets. Variation in the types of assets
and liabilities distinguishes the different types of financial intermediaries. A difficulty may arise from a mismatch
between the maturities of assets and liabilities. In the past, banks held shorter-term liabilities (customer deposits)
and longer-term assets (loans), and so were hurt by unanticipated increases in interest rates.
Financial intermediaries face two fundamental, interrelated problems: insolvency and illiquidity. Insolvency occurs
when a firm’s assets are worth less than its liabilities; in this case, its “capital” (net worth or owner’s equity) is
negative. Technically, it is bankrupt. Illiquidity occurs when a firm’s assets are worth more than its liabilities, but
the assets cannot be liquidated quickly without severe losses. One reason that the Federal Reserve was
established in the early twentieth century was to provide liquidity to solvent firms with illiquid assets.

 Markets, Financial and Nonfinancial
In modeling markets for goods and services, economists typically consider private goods, for which ownership and
property rights are clear and well defined. When buyers know exactly what they are getting and sellers know
exactly what they are giving up (i.e., all participants are rational and well informed), each voluntary trade benefits
both parties. Competitive auction markets are the standard benchmark, in which buyers and sellers periodically
transact at prices that no one party can affect individually. With the “market-clearing” assumption that all
transactions occur at equilibrium prices, markets instantaneously move from one equilibrium to another as
conditions change, with no shortages or surpluses. A complication arises for goods that last more than one
“period,” as the future expected prices of both buyers and sellers will influence the current equilibrium. If the
current equilibrium price changes, this may alter future expected prices.
Financial markets involve trade in contingent claims—for example, the delivery of a dollar “tomorrow” or in a
particular “state of nature” (e.g., one’s car is damaged in an accident). Thus, uncertainty and expectations of
future conditions play central roles. If the supply of a financial asset increases and its equilibrium price falls, this
may cause a shift in current demand if investors revise their future expected prices downward as a result. This
tends to amplify the decline that is required to reach a new equilibrium. Thus, the current prices of “storable,”
“nonperishable” financial assets are particularly sensitive to expected future prices.
Financial assets generally are not demanded for their intrinsic value, but for their associated financial flow. Their
demand is “derived” from the anticipated value of associated future payments. A change in the rate of discount will
affect all assets providing future payments. Financial claims are highly substitutable in a way that is qualitatively
different than claims in nonfinancial markets. A future dollar provided by one financial asset does not differ from
that provided by another—thus, the prices of these assets tend to move together. Differences in maturity and risk
across assets affect the magnitude of price changes.
Markets for nonfinancial assets usually are conceptualized in terms of “flows”—the amount of the good or service
that is bought and sold per period. In financial markets, long-lived “stocks” of assets are transacted, further
increasing the connection between the expectations of future conditions and the current market equilibrium. This
issue exists in nonfinancial markets for durable goods as well. For example, an increase in car sales in one
period may reduce the demand for cars in the next. In the housing market, one may consider only those homes
potentially bought or sold, so the equilibrium quantity is the number transacted, not the stock of homes in
existence. Similarly, one may consider the “market for loanable funds” as a flow of financial assets transacted
rather than the stock of assets in existence. The relation between the stocks and flows of durable assets is one
aspect of financial markets not typically seen in most nonfinancial markets for goods and services.
 Deregulation, Leverage, and the 2008–2009 Financial Crisis
In the aftermath of the stock market crash of 1929 and subsequent Great Depression, regulations were put in
place to stabilize the financial system and the economy. These included deposit insurance (to stop bank runs),
separation of investment and commercial banking (to minimize risk for depository institutions), increased auditing
and oversight (to prevent fraudulent bookkeeping), restrictions on the types of assets that financial intermediaries
could own (again to reduce risk), increased reserve requirements (to protect against illiquidity), increased capital
requirements (to protect against insolvency), and interest rate ceilings and segmentation of financial markets (to
reduce competition and increase stability).
By the end of the 1970s, however, these regulations were coming under attack on two fronts. First, firms were
using new technology and mounting legal challenges to circumvent the intent of the regulations. Second, the belief
that financial markets require regulation to remain stable was declining in popularity. Policy errors of the 1960s
and 1970s—such as excessive deficit spending associated with the Vietnam War, rapid monetary growth before
President Richard M. Nixon’s reelection bid, and wage and price controls to fight inflation—had convinced many
that government was the problem and that unregulated markets were the solution. The trend toward deregulation
moved forward, with a slight break in the late 1980s, until the financial crisis of 2008–2009.

Deregulation, advances in technology, huge flows of financial capital from abroad, an increasing belief that major
recessions and financial crises were a thing of the past, and perhaps an exaggerated belief in the Federal
Reserve’s ability stabilize the economy all created an environment in which financial intermediaries, particularly
investment banks and hedge funds, were taking on excessive risks in search of ever-higher returns.
Increased leverage—more borrowing and expansion of assets for a fixed amount of capital—can increase the
return to capital for a given return on assets. One factor that mitigates the excessive use of leverage is a fear that
this amplification is working in reverse, so when asset values fall, bank capital becomes negative and the firm
becomes bankrupt (insolvent). By the early 2000s, the fear of bankruptcy seemed to have diminished, and
deregulation allowed financial intermediaries to take on greater leverage and assume greater risk.
A general fall in home prices, which spurred a decline in the value of assets backed by home mortgages, led to
the forced sale of investment bank Bear Stearns to JPMorgan Chase in March 2008 and the outright failure of
Lehman Brothers in September 2008. In the ensuing panic, asset prices unrelated to home mortgages began to
fall as well. The Federal Reserve and the Treasury Department stepped in to loan against otherwise illiquid assets
in an effort to forestall a greater financial collapse. This event illustrated that the basic fragility of financial markets
had not been conquered, and that financial crises still have a significant impact on the overall economy.
Bruce Brown
 
See also:  Asian Financial Crisis (1997);  Bank Cycles;  Banks, Central;  Banks, Commercial; 
Banks, Investment;  Credit Cycle;  Efficient Market Theory;  Fragility, Financial;  Investment,
Financial;  Liberalization, Financial;  Money Markets;  Panics and Runs, Bank;  Regulation,
Financial;  Stock Markets, Global;  Systemic Financial Crises. 
Further Reading
Bagehot, Walter. Lombard Street: A Description of the Money Market. New York: Scribner, 1912. 
Burton, Maureen, Reynold Nesiba, and Bruce Brown. An Introduction to Financial Markets and Institutions.  2nd ed. Armonk,
NY: M.E. Sharpe, 2010. 
Eatwell, John, Murray Milgate, and Peter Newman, eds. Finance: The New Palgrave. New York: W.W. Norton, 1989. 
“A Short History of Modern Finance.” The Economist, October 16, 2008. 
“Why Is Finance So Unstable?” The Economist, January 22, 2009. 
Financial Modeling of the Business Cycle
 
Financial data models of the business cycle combine mathematical techniques and data from the financial markets
to illustrate and predict expansions and contractions. Such models are useful because financial market activity is
intimately connected to larger economic and business indicators, which themselves are the key components for
measuring business cycle stages. This is especially the case in economies where the financial sector plays an
outsized role, such as that of the United States in recent years. According to experts who utilize mathematical
models to understand the business cycle, the huge growth of the U.S. financial sector—from 16 to 40 percent of
overall domestic corporate profits between the late 1990s and the late 2000s, to take just one measure—helps

explain why the crisis in the financial markets triggered such a deep recession in the United States starting in late
2007.
Financial models of the business cycle are analogous to experiments in the natural sciences. That is, they study
the relationship between a dependent variable and its related independent variables. In economics, however, it is
impossible to fully isolate a single variable as in a scientific experiment. Usually, historical data are utilized to form
and develop a model in order to reach a conclusive and axiomatic statement about a given economic
phenomenon.
Among the most important of the economic and business indicators studied through financial modeling are
investment, consumer credit, and the indices of economic indicators. Investment activities, which are essential
factors of a nation’s gross domestic product and closely related to business cycles, occur largely in the capital or
financial markets. Technically speaking, investments are savings or unconsumed incomes reallocated for higher
returns. Since the reallocation of past income is usually associated with such capital market products as bonds,
stocks, and other financial instruments, investments provide a critical link between financial market performance
and fluctuations in the business cycle. Thus, models generated from financial market performance data provide
useful tools for predicting business cycles and offer guidance to governments in their fiscal and monetary
decision-making.
Consumer credit, both a key financial resource and an engine of economic activity, is made up of two
components: secured debt, such as mortgages and home equity lines of credit, and unsecured debt, such as credit
card balances. Outstanding credit balances and variables derived from them, such as consumer credit usage or
payment patterns, represent various types of consumption and default-risk behavior patterns. The latter can take
the form of defaults on home mortgages or car loans, and missed payments on credit card balances. Major banks
and financial research institutions utilize such proprietary information in their financial modeling to forecast phases
of the business cycle; lending and investment institutions often use it to determine their own investment activity. In
addition, the Conference Board—a nonprofit, nongovernmental business research organization—combines
aggregations of consumer credit data with leading economic indicators to create financial models that can
anticipate movement in the domestic and international capital markets.
 Economic Indicators
Indices, which combine several economic indicators, are critical both for and to financial modeling. They consist of
leading indicators, coincident indicators, and lagging indicators—each measuring data in relation to business cycle
fluctuations. Leading indicators—bond yields and stock market valuations are good examples—anticipate and often
predict near-term changes in the economy. The Index of Leading Economic Indicators, a widely followed and
much-respected measure calculated and published monthly by the Conference Board, is based on ten variables:
the S&P 500 Index of major stock prices, the money supply, the spread between long-and short-term interest
rates, unemployment insurance claims, consumer sentiment, new building permits, manufacturers’ new orders for
consumer goods, delivery of new merchandise from suppliers to vendors, new orders for capital goods, and
manufacturing working hours. The index is closely monitored by the government to determine fiscal, monetary, and
other economic policies, such as changes in the interest rate the Federal Reserve charges to member banks, the
tax code, unemployment insurance compensation and, during recessionary periods, various forms of economic
stimulus. While generally a reliable indicator, the Index of Leading Economic Indicators is not perfect and has
failed at times to provide policy makers with sufficient warning of economic downturns and upturns.
Coincident indicators provide a picture of the economy in the current state. The Conference Board’s Index of
Coincident Indicators includes data on nonfarm payroll workers, personal income (minus government transfer
payments such as welfare, Social Security, unemployment compensation, and disability), industrial production, and
trade sales. High figures in these categories, especially personal income, indicate that an economy is in an
expansion stage of the business cycle.
Lagging indicators measure past performance and help economists predict what will occur later on during the

business cycle. For example, unemployment rates can be expected to remain high even as the overall economy
recovers. The components of lagging indicators include employment, outstanding commercial and industrial loans,
the consumer price index for services, labor cost per unit of output, ratio of manufacturing and trade inventories to
sales, ratio of consumer credit outstanding to personal income, and the prime lending rate, which is based on the
federal funds rate and the interest rates banks charge for home equity lines of credit and some credit cards.
Indicators are also divided into two other categories: procyclical and countercyclical—that is, whether upticks
reflect economic expansion or not. Good examples of the former include manufacturers’ new orders for consumer
goods, industrial production, and the ratio of consumer installment credit to personal income. Key countercyclical
indicators are unemployment claims, outstanding commercial and industrial loans, and changes in labor cost per
unit of output.
While many people watch stock prices vigilantly and the financial media often assumes that changes in stock
market indices offer signs of future economic performance, economists are more skeptical of share prices as a
reliable leading indicator. Some believe that falling indices do increase the likelihood of a recession, especially in
the U.S. economy, where stock prices play a more important role in predicting changes in the business cycle.
Internationally, however, near-term fluctuations in stock prices are less telling, since the global financial system is
much more deeply affected by changes in the interest rate.
In the United States, for example, interest rates or interest rate term structure (the difference between short-and
long-term rates), do have an important predictive value, as adjustments in the interest rate can reduce business
cycle volatility. In the 2008–2009 financial crisis, the Federal Reserve Bank dropped the interest rate it charged
member banks to near zero in order to ease the recession. However, lowering interest rates risks flooding the
economy with money, increasing the possibility of inflation. The net benefit of the trade-off between interest rate
smoothing and inflation instability therefore depends on the timing and level of interest rate–adjustment policies
from the Federal Reserve Bank.
The interest rate spread—that is, the difference between low-risk and high-risk loan rates—is also a useful
predictor of future business cycle phases because it measures the fluctuation of real output or investment, which
is contingent on innovations in the financial markets. For example, an increase in high-risk loan generation
increases the likelihood of default risk; this, in turn, causes a decrease in investment or output, as investors are
unable to adjust their investment decisions without the assistance of external risk–reducing mechanisms. In recent
years, the securitization of subprime mortgages—whereby such mortgages were bundled and sold to investors to
help spread the financial risk of default—led investors to increase their investments, which produced a boom in
the housing market. Nevertheless, the absolute amount of risk—the aggregate risk of all subprime mortgages—
remained the same. Thus, when homeowners began to default on their mortgage payments, investors pulled
back, producing the financial shock of 2008.
 2008–2009 Financial Crisis
The financial shock of 2008 and its role in the recession that followed revived interest in the role that financial
markets play in the business cycle. As American economist Hyman Minsky argued in the mid-twentieth century,
during prosperous times corporations tended to overinvest in productive capacity, saddling themselves with debt
that they had a hard time repaying when revenues sank during economic downturns. The result was a financial
crisis that saw banks and other lenders shutting off credit.
Earlier, in the first half of the twentieth century, Austrian-American economist Joseph Schumpeter highlighted the
importance of innovation—including financial innovation—in economic development. The point was dramatized in
the early to mid-2000s, as rapid-fire innovations in the financial markets produced an economic boom. As
subsequent events proved, however, it was a shaky expansion since some of the new financial instruments—such
as mortgage-backed securities—increased risk dramatically. Because of the importance of these financial
innovations to the boom of the early to mid-2000s, some economists argued, better financial modeling—or more
attention to such financial modeling by policy makers—could have predicted that the boom would not last and that

it would inevitably lead to a collapse in the financial markets. Indeed, an economic strategy group at IBM created
a financial model that predicted just such a collapse, though their warnings were largely ignored by the Federal
Reserve Bank, which maintained the low interest rates that had fueled the mortgage securitization boom.
Traditionally, risk aversion in the investment decisions by firms during a recession phase is seen as having a
major influence on business indicators, such as employment or new durable goods orders. During economic
contractions, investors require higher returns—that is, higher interest rates or dividends—to account for the greater
risk of investing at a time when borrowers have a greater likelihood of financial loss or outright failure. Such risk
aversion heightens the cost of borrowing, leading businesses to forego projects they would have undertaken when
credit was cheaper, as during periods of economic expansion. This, then, leads to declines in such indicators as
employment or durable goods orders. In short, the higher cost of financing during a recession has important
consequences for the business cycle.
As recent economic history indicates, fluctuations in the financial markets have become an increasingly important
factor in determining trends in the business cycle. Indeed, some economists have argued that the economic boom
of the early to mid-2000s was largely a creation of the financial markets. As Schumpeter and others students of
the business cycle have contended, technology innovation leads to economic expansion, while maturation of a
given technology and investment saturation in that technology lead to economic contractions. Nothing illustrates
this phenomenon better than the dot.com boom and bust of the 1990s and early 2000s. Yet the boom-and-bust
cycle that followed depended not on new technology but on new financial instruments, as well as government
interest rate policies that made them possible and profitable. The heightened role of the financial sector has made
the need for financial modeling even more imperative.
Beryl Y. Chang and James Ciment
 
See also:  Financial Markets. 
Further Reading
Buch, Claudia M., and Christian Pierdzioch.  “The Integration of Imperfect Financial Markets: Implications for Business Cycle
Volatility.” Journal of Policy Modeling 27(2005): 789–804. 
Faia, Ester.  “Finance and International Business Cycles.” Journal of Monetary Economics 54:4 (2007): 1018–1034. 
Fuerst, Michael E.  “Investor Risk Premia and Real Macroeconomic Fluctuations.” Journal of Macroeconomics 28:3
(2006): 540–563. 
Kato, Ryo.  “Liquidity, Infinite Horizons and Macroeconomic Fluctuations.” European Economic Review 50:5 (2006): 1105–
1130. 
Kwark, Noh-Sun.  “Default Risk, Interest Rate Spreads, and Business Cycles: Explaining the Interest Rate Spread as a
Leading Indicator.” Journal of Economic Dynamics & Control 26:2 (2002): 271–302. 
Schumpeter, Joseph A. The Theory of Economic Development. New Brunswick, NJ: Transaction, 1983. 
Sensier, Marianne.  “The Prediction of Business Cycle Phases: Financial Variables and International Linkages.” National
Institute Economic Review, October 1, 2002. 
Tallarini, Thomas D., Jr.  “Risk-Sensitive Real Business Cycles.” Journal of Monetary Economics 45:3 (2000): 507–532. 
Vercelli, Alessandro.  “Structural Financial Instability and Cyclical Fluctuations.” Structural Change and Economic
Dynamics 11:1 (2000): 139–156. 

Finland
 
Finland is a northern European country surrounded by the Baltic Sea and the gulfs of Finland and Bothnia. Its
shares borders with Sweden to the west, Norway to the north, and Russia to the east. The population of Finland
is approximately 5.3 million, one-fifth of whom live in the capital of Helsinki and the surrounding region. Finland is
a parliamentary republic. Government expenditure in the economy for the years 1999 to 2008 constituted an
average of 49.09 percent of the nominal gross domestic product (the value of goods and services produced in the
domestic economy during a given year measured in current prices), compared with 42.54 percent for the euro
area countries.
Finland joined the European Union (EU) in 1995 and, unlike the other Nordic countries in that body, chose to
adopt the euro as its currency in 1999. Finland has been offered membership in NATO a number of times, most
recently in January 2009, but has declined to join in deference to its neighbor to the east.
Finland is heavily forested, with little arable land. Its economy depends on forestry to a large extent, with exports
of forestry-related products per capita almost three times greater than those of Canada. In fact, Finland’s forestry
exports in 2007 accounted for almost 10 percent of global exports in this sector. Finland is an energy-poor country
with no reserves of gas or oil, a fact that poses a major challenge for a country that has one-third of its land above
the Arctic Circle and experiences prolonged periods of darkness and cold. The latter factors account for Finland’s
high energy consumption—one of the highest in the world, in fact, and nearly two-thirds higher than the EU
average. To minimize dependency on a specific energy source, usage is spread across a variety of resources,
including oil, wood fuel, natural gas, coal, and peat, as well as nuclear energy. Mining and related processing also
contribute to the national economy.
In recent years, the economic hegemony of natural resources in Finland finally has been eclipsed by the
information and communications technology sector. Along with a significant output of radio and television
equipment, Finland is also home to Nokia Corporation, the world’s largest mobile-telephone handset producer.
Until the recession of 2008–2009, Finland recorded large trade surpluses. Government plays a significant role in
the economy and, despite a broad privatization program launched in 1991, state ownership remains substantial.
After prospering in the 1980s, Finland in the early 1990s suffered one of the worst recessions of any Organisation
for Economic Co-operation and Development (OECD) country since the end of World War II, largely because of
the collapse of the Soviet Union, one of its major trading partners. That loss was compounded by the banking
crisis that swept Scandinavia during the same period. The Finnish economy rebounded in 1994, and over the next
five years (1995–2000), real GDP grew at an average rate of 4.62 percent. This was the strongest expansion
among the Nordic countries and especially favorable compared to the euro area and OECD, where the average
growth rates were 2.68 percent and 3.23 percent, respectively, for the same period. (Real GDP is the market
value of all goods and services produced in a country during a given year, measured in constant prices so the
value is not affected by changes in price.)
With the onset of the recession in 2001, real GDP grew at an average rate of 2.00 percent for 2001–2003, then
returned to a healthier 3.83 percent. The rate of real GDP growth was 4.4 percent in 2007 but started a downward
trend in 2008. Although the forecasts for Finland at the beginning of the global financial crisis were relatively
optimistic, the tide began to turn for this country as well. Both imports and exports fell by more than one-third in
2008, compared to the previous year. The global crisis situation put Finland’s economy at greater risk than that of
other countries, given its new reliance on the vulnerable information, communications, and technology sectors. The
country officially entered a recession at the end of 2008, after two consecutive quarterly declines in GDP. The
silver lining was an easing of inflation, which had been significantly higher in Finland than in the euro area.
Demographically, Finland faces the economic challenges of an aging population, which is maturing sooner and

more quickly than the populations of other Western European countries. In response, the government has
promoted a number of measures to keep individuals working longer and postponing retirement.
Despite the economic crisis, government finances remained relatively balanced, and the banking sector was
relatively immune from the crises faced in other countries, largely because of reforms instituted after the banking
crisis of the early 1990s. Nevertheless, in 2009, the Finnish government introduced tax cuts and announced a
stimulus package of 3 billion euros to encourage construction, among other initiatives. The nation’s banking sector
emerged relatively unscathed, as it did not invest heavily in the Baltic States. In fact, at least one Finnish bank
turned misfortune into opportunity with the acquisition of the Swedish subsidiary of Iceland’s bankrupt Kaupthing
Bank.
Marisa Scigliano
 
See also:  Denmark;  Iceland;  Norway;  Sweden. 
Further Reading
Central Intelligence Agency. The CIA World Factbook. New York. Skyhorse, 2009. 
Economist Intelligence Unit (EIU). Country Report—Finland. London: EIU, 2009. 
Economist Intelligence Unit (EIU). ViewsWire: Finland. London: EIU, 2009. 
Organisation for Economic Co-operation and Development (OECD). OECD Economic Outlook,  no. 84. Paris: OECD, 2008. 
Statistical Office of the European Communities (Eurostat). Eurostat Yearbook 2009.  http://epp.eurostat.ec.europa.eu
Fiscal Balance
 
“Fiscal balance” is a phrase usually used to refer to the net spending of the federal government, but it can also be
applied to the finances of lower levels of governments. As such, it is the difference between the government’s
receipts and disbursements; thus, it can be either positive or negative. When a government’s outlays exceed what
it collects in taxes, it is said to be running a deficit, in which case it will have to find other sources of financing
such as borrowing and/or creating new money. Accumulated deficits lead to what is called the “public debt.” Some
economists regard the public debt as a burden on society and actively lobby for its reduction or even elimination.
Others argue that budget deficits (and therefore the public debt) serve an important function by allowing for the
continuation of capital projects and expansionary fiscal policy during economic downturns. The ultimate impact of
deficits and debt on the economy depends on two issues: Will public investment hinder or help the economic
growth? And is the debt a drag on economic growth, by crowding out private borrowing or driving up interest
rates, or a necessary stimulant for economic growth, by easing the tax burden on households and businesses and
spurring employment and economic activity through government spending?
 Sound Finance
The idea that “responsible” governments should not spend more than they are able to receive in the form of taxes
is not new and can be traced back to classical economists such as David Ricardo, who argued back in the early
nineteenth century that any government disbursements, including reimbursement of debt and payment of interest,

must ultimately be paid for by taxes. More recently, economists—sometimes known as “deficit hawks” and
including people like Stephen Friedman and N. Gregory Mankiw—have elaborated on this idea, arguing that taxes
are the only legitimate source of government revenue; such economists are therefore opposed to any deficits at
all. For instance, in the United States, although unsuccessful, many attempts have been made (such as the
Balanced Budget Act of 1997) to impose binding constraints on federal spending. In Europe, the 1992 Maastricht
Treaty creating the European Union requires that budget deficits in member countries participating in the monetary
union not exceed 3 percent of gross domestic product (GDP) and that public debt not exceed 60 percent of GDP.
In reality, however, such legal limits are largely irrelevant because governments often find it necessary to increase
their spending (and incur deficits) in order to stabilize or stimulate the economy—as they did during the 2008–
2009 financial crisis.
More conservative proponents of sound finance put forward two major arguments to justify their opposition to
public deficits and debt. First, they argue, when the government borrows on financial markets, it competes with
private investors and forces the rate of interest to rise. In this view, higher interest rates discourage private
investment and lower aggregate demand, which leads to a fall in total production (GDP), an idea known as
“crowding out.” The second argument is that if the government finances its deficit by printing more money, the
increase in monetary circulation leads to inflation. Since domestic products will consequently be more expensive
than foreign ones, producers will suffer a loss of competitiveness and sell less both domestically and abroad. As a
result, they produce less, and aggregate output (GDP) will fall again. Thus, it is concluded, if governments want to
avoid these negative effects, they must follow a policy based on balanced budgets or budget surpluses.
 Alternate View
Other economists, generally on the more liberal end of the political spectrum, view the crowding-out hypothesis as
one of the more misleading conceptions in modern economic thought. For instance, some argue, because
government expenditures generate income for the private sector and taxes reduce the disposable income, there
will necessarily be a net addition to incomes whenever government spending exceeds tax revenue (a budget
deficit). Higher income will increase consumption and have further positive effects as households spend more on
goods and services. For this reason, as the history of the United States suggests, many economists maintain that
deficits stimulate the economy whereas surpluses are harmful. Indeed, as economist Randall Wray wrote in
Surplus Mania (1999), “since 1776 there have been six periods of substantial budget surpluses and significant
reduction of the debt.... Every significant reduction of the outstanding debt has been followed by a depression, and
every depression has been preceded by significant debt reduction.”
In a similar way, some empirical evidence also shows that budget deficits have been followed by periods of
economic expansion. The theoretical explanation for the association between public deficits and private sector
performance is easer when one takes a closer look at what happens in reality. For example, public infrastructure
projects such as the construction or improvement of roads, schools, and public transportation require the creation
of millions of new jobs. In addition to reducing unemployment, government spending in general—and public
investment in particular—stimulates private investment. Consider, for example, the construction of an airport. By
the time the facility opens, private investors will build hotels, open restaurants, introduce a taxi service, and
launch other related businesses. These companies, in turn, will hire workers and pay them wages, which helps
expand the local economy. Furthermore, the new money created by the government to pay for all these
expenditures will increase the amount of available liquidity in the system, which helps lower interest rates. This
makes the cost of borrowing cheaper and encourages both investors and households to acquire assets and build
wealth. It is important to note that the money supplied in this process is exactly equal to the amount that was
demanded as wages, salaries, and other payments. Therefore, it is impossible to have an excess supply of money
and, as has been documented, the fear of inflation appears to be unfounded.
Hassan Bougrine
 

See also:  Fiscal Policy;  Tax Policy. 
Further Reading
Bougrine, Hassan.  “The Stabilizing Role of Public Spending.” In Introducing Macroeconomic Analysis: Issues, Questions
and Competing Views,  ed. Hassan Bougrine and Mario Seccareccia, 165–175. Toronto: Emond Montgomery, 2009. 
Ricardo, David.  “Essay on the Funding System.” In The Works of David Ricardo, ed. J.R. McCulloch  [1846]. London: John
Murray, 1888. 
Seccareccia, Mario.  “Keynesianism and Public Investment: A Left-Keynesian Perspective on the Role of Government
Expenditures and Debt.” Studies in Political Economy 46 (Spring 1995): 43–78. 
Wray, L. Randall. Understanding Modern Money: The Key to Full Employment and Price Stability. Aldershot, UK: Edward
Elgar, 1998. 
Fiscal Policy
 
Fiscal policy is viewed by economists as one-half of macroeconomic policy, the other half being monetary policy.
Fiscal policy concerns how a government raises revenue—such as through taxes—and how it spends that revenue
on services and other expenditures. Putting together a government budget thus constitutes a major step in fiscal
policy making. Despite their fundamental differences, fiscal policy and monetary policy go hand in hand because a
country’s currency will lose its value if its fiscal policy is not seen as sustainable, especially over the long term.
Fiscal policy is seen as sustainable or rational when the general public and those buying government debt
instruments perceive that the policy is predictable and that the government will continue to make payments on its
debt. Under the most rational fiscal policy, a government’s revenue sources (taxes, tariffs, user fees) will roughly
equal its expenditures on government programs—in other words, the budget will be balanced. Under a rational
fiscal policy, the government will also make its budgets—and the process by which the budgets are formulated—
open to public scrutiny. In addition, it is common practice and rational fiscal policy for governments to prepare
financial statements and to have these statements audited.
The business cycle can have a major effect on fiscal policy. During economic contractions, taxes and other
sources of government revenue typically go down as people become unemployed and corporate profits shrink and
disappear. At the same time, such periods often see heightened demand for government services, such as
unemployment compensation. In addition, many governments seek to enhance aggregate demand during slumps
by increasing spending on things like infrastructure, which creates investment and jobs, and by cutting taxes. An
expansive fiscal policy can help to lift an economy out of recession but it can also exacerbate a downturn if the
government is forced to borrow too much, making it more expensive for businesses to obtain the credit they need
to operate, invest, expand, and hire.
 Keynesian Fiscal Policy
Keynesian economists—those who adhere to the principles of John Maynard Keynes—believe that fiscal policy
can help governments manage the economy to ensure full employment. Keynesians believe that when private
economic activity is not creating enough demand to allow for full employment, fiscal policy can be used to
increase demand, stimulate production, and create more jobs. The idea of active economic management by the

government was articulated in Keynes’s landmark work, The General Theory of Employment, Interest and Money,
published in 1936 during the Great Depression. The prolonged high unemployment rates of that period prompted
Keynes to recommend that governments take action to create jobs. The General Theory marked the first time that
a comprehensive theoretical framework described how fiscal policy can be used to create full employment. The
book is also regarded among economists as the beginning of macroeconomics as a subdiscipline of the field.
An active fiscal policy, then, is the use of government taxing and spending policy to stimulate economic demand.
In political discourse, the use of government fiscal policy to increase demand came to be known as “fiscal
stimulus.” If the private sector is not investing enough in productive assets and not spending enough to increase
output, including the hiring of employees, Keynesians believe that a fiscal stimulus on the part of government will
trigger an upward cycle of economic activity.
As government spending increases through fiscal stimulus, the argument goes, private individuals will see an
increase in income and spend more on consumer goods. The increase in consumer spending and demand, in
turn, will encourage private companies to spend more on output—including the hiring of more people. The newly
hired employees will then spend more themselves, boosting demand even higher and feeding the upward cycle of
economic activity. The principle underlying active fiscal policy is also known as “demand management,” as it is
predicated on boosting consumer demand and the necessary production to meet it.
Keynesian economists believe that a fiscal stimulus is created when the government spends more than it receives
in taxes and other revenue sources—a practice called “deficit spending.” A government can use deficit spending to
create a fiscal stimulus in several ways. The differences between these strategies are often the subject of political
debate in the formulation of government policy. Economists who believe that government programs help people
urge more spending on such programs. Aside from the help to ordinary people, they maintain, the increased
government spending also fuels greater demand. Other economists recommend fiscal stimulus through a reduction
in taxes, because taxes are seen as a drain on private economic activity. Thus, deficit spending can be created in
any of three ways: (1) increasing government spending while keeping taxes the same; (2) reducing taxes while
keeping spending the same; or (3) reducing taxes and increasing government spending simultaneously.
Deficit spending is also widely referred to as “priming the pump” of economic activity. The term implies a process
analogous to the flow of water through a closed system, the priming of which starts an upward cycle and restores
the flow of water (or money). The first overt use of active fiscal policy in the United States—when, say some
economic historians, Keynesian economics first became institutionalized in American policy—was the Pump
Priming Act of 1938, which increased the federal government budget by 5 percent over the previous year. In
hindsight, a spending increase of this magnitude does not seem substantial enough to pump-prime an economy
facing above-average levels of unemployment—indeed it is not much greater than the general increase in
government spending from year to year—but it marked a significant departure at the time.
 Criticisms
Fiscal policy as a tool for managing the economy has been criticized on a variety of grounds. Perhaps the most
cogent argument against Keynesian economics is that political reality does not allow it to work in practice as it is
proposed in theory. The purpose of a fiscal stimulus—running a deficit in the short term to boost economic activity
in the short term—is to create a temporary increase in private sector demand, fueling production, job creation, and
broad-scale economic growth. However, history has repeatedly shown that once increased government spending
has been put in place, deficit spending generally does not decrease once full employment is achieved. The “public
choice” school of economics explains why it is difficult to reduce spending on any government program due to the
special-interest groups created by it.
Many economists argue that the private sector, rather than government, can best create long-term productivity
increases (which is what supports higher standards of living) through the natural profit incentives of the
marketplace. Thus, it is argued, when governments continue to run budget deficits, increasing their debt levels
from year to year, society’s resources are diverted from vital private sector activity (including investment) to paying

the interest on government borrowing. In the United States, for example, the federal debt prior to the adoption of
Keynesian economics (except during times of war) averaged less than 10 percent of national income. After the
1930s (again excepting periods of war), the national debt as a percentage of national income has averaged more
than 35 percent.
Another common criticism of demand management as fiscal policy is what is known as the “lag time” involved in
implementing public policy. In order for a fiscal stimulus to be effective, the deficit spending must take place when
the economy is performing poorly. However, given the months or even years that it often takes for fiscal policy to
have an effect, the period of greatest potential benefit has already passed. Whatever the causes of booms and
busts in the economic cycle, the periods of growth and contraction are extremely difficult to forecast. It is all but
impossible to time a fiscal stimulus so that it will take effect at the precise moment of downturn—or even close to
it. In the first place, it takes time for the government to gather the economic data required to determine when
unemployment is increasing. Then it takes time to put together a budget proposal recommending the stimulus,
more time for the legislative body to approve the budget, and even more time for the government to spend the
additional funds so as to increase demand.
In short, the lag times in public policy make it exceedingly difficult for the government to manage the economy
purely through fiscal policy. It is often the case that fiscal stimulus spending on a government program flows
through the economy at the same time that the private sector is already increasing its economic activity. When
this occurs, both the government and the private sector are competing for the scarce economic resources of a
society at the same time. Deficit spending on the part of the government may therefore be counterproductive,
worsening the effects of the boom-and-bust cycle because productivity gains in the up-cycle are not as great as
they could be due to government using scarce economic resources at the wrong time. In economic thinking this is
known as a negative “unintended consequence” of public policy.
Every government by definition creates fiscal policy—the management of government revenues and expenditures
—as part of its budget formulation process. Specifically, it is the use of fiscal policy to manage demand that is still
under debate in economic circles, even after being common political practice for more than seventy years.
Cameron M. Weber
 
See also:  Fiscal Balance;  Monetary Policy;  Public Works Policy;  Stimulus Package, U.S.
(2008);  Stimulus Package, U.S. (2009);  Tax Policy. 
Further Reading
Borcherding, Thomas E., ed. Budgets and Bureaucrats: The Sources of Government Growth. Durham, NC: Duke University
Press, 1977. 
Keynes, John Maynard. The General Theory of Employment, Interest and Money. New York: Harcourt Brace, 1936. 
Mankiw, N. Gregory. Principles of Macroeconomics.  5th ed. Mason, OH: South-Western, 2009. 
 

Fisher, Irving (1867–1947)
 
Irving Fisher was an American economist and monetary reformer who developed the modern quantity theory of
money and made contributions to the study of the relationship between money, inflation, interest rates, and
economic activity. He was also an early exponent of the application of mathematics to economic theory and a
world-renowned expert on the theoretical and statistical properties of index numbers. An early neoclassical
economist, he is credited with having laid much of the groundwork for modern monetary economics.
Monetary theorist Irving Fisher, a pioneer of modern quantitative economics, explained fluctuations in the economy
as a function of avoidable disturbances in monetary policy and of “debt-deflation” theory. (The Granger Collection,
New York)
Irving Fisher was born on February 27, 1867, in Saugerties, New York. He received a bachelor’s degree from
Yale University in 1888 and was awarded the school’s first PhD in economics in 1891. He spent his entire
teaching career at Yale, as an instructor, tutor, assistant professor, professor of political economy (1898–1935),
and professor emeritus (1935–1947). Fisher was elected president of the American Economic Association in 1918,
and in 1930 became a founder and the first president of the Econometric Society. In addition to economics, he
had a keen interest in such areas as eugenics, nutrition, pacifism, and the environment.
Among his most important contributions to mathematical economics is the so-called Fisher equation of exchange.

According to the equation, MV = PQ, the money supply (M) multiplied by the velocity of circulation (V, or the
number of times a unit of currency purchases goods and services within a specified period of time) is equal to the
total output of goods and services (P, the average price level, multiplied by Q, quantity—measured by real gross
domestic product). Put more simply, the Fisher equation states that the amount of money spent over a given
period of time must equal the amount of money used during the same period. For example, if the U.S. money
supply is $1 trillion and each dollar is used twelve times per year to purchase goods and services, then $12 trillion
worth of goods and services must have been purchased. The Fisher equation is the cornerstone of the modern
quantity theory of money and provides an explanation for the causes of inflation. If V and Q remain constant over
a period of time, any increase in the money supply will lead to an increase in the price level.
A second element of Fisher’s work was the discovery of “money illusion.” This refers to the confusion that arises
from changes in monetary terms and changes in real terms. For example, if a worker’s salary increases by 8
percent but inflation increases by 16 percent over the same period, the worker would be suffering from “money
illusion” to believe that he is better off; in reality, his new salary will buy him less than his old salary did before the
inflation.
An important example of Fisher’s contribution to macroeconomics is his debt-deflation theory of economic
fluctuations. Fisher initially attributed the onset of the Great Depression to the actions of the Federal Reserve,
which unnecessarily contracted the money supply (a view later reaffirmed by Milton Friedman and Ben Bernanke).
The severity of the Depression, however, eventually led Fisher to develop his debt-deflation theory. The central
element of this theory is that the downswing of the cycle (leading to possible depression) results from the
discovery of overindebtedness throughout the economy. Given that overindebtedness is unanticipated when debts
are incurred, the discovery of the problem is followed by rapid attempts at correction through liquidation and
possible bankruptcies. Since debts are commonly denominated in nominal terms, a massive sale of assets,
leading to a broad-based drop in the price level, would only place greater emphasis on the real value of the debt
burden. A vicious downward spiral would therefore be generated by a combination of excessive real-debt burden
and deflation.
Fisher’s reputation was damaged by his misjudgment of the stock market in 1929 and his insistence during the
course of the Great Depression that recovery was imminent. In 1913, Fisher made a fortune with his patented
index-card system (the forerunner of the Rolodex), but lost half of it through speculation in the stock market.
Adamant that stock prices would quickly rebound after the crash of October 1929, he borrowed heavily to invest
further, but lost his entire fortune as prices continued to slide. Following the collapse of Fisher’s personal fortune,
Yale University was forced to purchase his house in order to save him from eviction. Fisher, who died on April 19,
1947, in New Haven, Connecticut, remained heavily in debt for the rest of his life. It is only in recent years that he
has received proper recognition, not only as the father of modern monetary economics but as one of America’s
most important economists.
Christopher Godden
 
See also:  Debt;  Deflation;  Fisher’s Debt-Deflation Theory. 
Further Reading
Dimand, Robert W., and John Geanakopolos, eds. Celebrating Irving Fisher: The Legacy of a Great Economist. Oxford,
UK: Blackwell, 2005. 
Fisher, Irving.  “The Debt-Deflation Theory of Great Depressions.” Econometrica 1:4 (1933): 337–357. 
Thaler, Richard.  “Irving Fisher: Modern Behavioral Economist.” American Economic Review 87:2 (1997): 439–441. 

Fisher’s Debt-Deflation Theory
 
Developed by American economist Irving Fisher in the Great Depression of the early 1930s, Fisher’s debt-
deflation theory examines how a high level of indebtedness in an economy can, when hit by a shock such as the
stock market crash of 1929, enter a vicious cycle in which the indebtedness triggers crippling deflation, which, in
turn, leads to additional indebtedness. That is because as deflation sets in, money becomes more valuable,
making it more expensive for borrowers to pay off their debts. Because debts are denominated in dollars, when
the price level falls (deflation), there is a real increase in debt levels, making the debts much more difficult to
service. The result can be massive defaults and a prolonged downturn, like that of the period in which Fisher
developed his theory.
A successful businessman and one of the most highly respected economists of the 1920s, Fisher had a gift for
rendering complex theory into terms understood by the lay reader. He was especially hard hit by the Wall Street
crash of 1929 and the Great Depression that followed, losing both his fortune and his reputation as an economist
in the downturn. In an article titled “The Debt-Deflation Theory of Great Depressions,” published in 1933 in
Econometrica, the journal of the Econometric Society, he critiqued the prevailing classical paradigm of economic
equilibrium, according to which the normal push and pull of supply and demand inevitably leads to a balance, or
equilibrium, of high output and low unemployment. The period of equilibrium cannot be maintained for any length
of time, Fisher contended, since the economy is usually at a point where there is over-or underconsumption, over-
or undersavings, over-or underproduction, and so forth. Normally, such unstable states do not lead to recession,
even when the economy is buffeted by the normal flux of the short-term business cycle.
But, Fisher argued, should the economy be burdened with a lot of indebtedness, then any shock to the system
that undermines the confidence of creditors and debtors—such as the stock market crash of 1929—can set off a
deflationary cycle that can plunge an economy into a prolonged downturn like the Great Depression. Fisher laid
out nine cascading steps in the process:
1. Debt liquidation leads to distress selling.
2. As bank loans are paid back, the deposit currency contracts and the circulation velocity of money slows.
3. Decreasing velocity and contraction of deposits create a falling price level and a swelling in the value of
money.
4. The net worth of businesses declines, leading to bankruptcies.
5. Losses in profits create greater anxieties about future losses.
6. Such anxiety reduces output, trade, and employment.
7. People lose confidence about their economic futures.
8. People tend to hoard their money, further reducing the velocity of circulation.
9. Disturbances in interest rates follow, such as a fall in the nominal rate or a rise in real rates.
On the basis of this analysis, Fisher urged that the government should take action to stop the deflationary cycle
as quickly as possible. The credit system has to be restarted, he insisted, because deflation can lead to its
complete collapse. Alternatively, the collapse in the credit system is not reversed until deflation is under control.

Fisher’s remedies for policy makers of the 1930s included taking the United States off the gold standard and
getting rid of fractional banking reserves—requirements that banks hold on to a certain amount of assets against
which they lend money. Fisher opposed the fiscal stimulus policies of the Franklin Roosevelt administration—in
which the federal government pumped money directly into the economy to put people to work—arguing instead
that the government should restrict itself to controlling the money supply and coordinating the reform of the
financial system.
Just as Roosevelt largely ignored Fisher’s recommendations, so the economic community generally disregarded
his debt-deflation theory. According to at least some economic historians, this was because Fisher’s theory raised
uncomfortable questions about the role the Federal Reserve and Wall Street had played in creating excess debt
in the 1920s and thereby triggering debt deflation in the 1930s.
Long overlooked by economists who adhered to British theorist John Maynard Keynes’s theories of countercyclical
spending (even though Keynes himself described Fisher as the “great-grandparent” of some of his own ideas),
Fisher’s theory has been resurrected by a number of economists in more recent years, including James Tobin and
Hyman Minsky in their work on financial instability. In addition, the financial crisis of 2008–2009 has led some to
reexamine Fisher’s analysis of how high debt levels in an economy—as was the case with rising mortgage levels
—can lead to debt deflation and a prolonged economic downturn.
Bill Kte’pi and James Ciment
 
See also:  Debt;  Deflation;  Fisher, Irving;  Great Depression (1929-1933). 
Further Reading
Allen, W.R.  “Irving Fisher, F.D.R., and the Great Depression.” History of Political Economy 9:4 (1977): 560–587. 
Fisher, Irving.  “The Debt-Deflation Theory of Great Depressions.” Econometrica 1:4 (1933): 337–357. 
“Irving Fisher: Out of Keynes’s Shadow.” The Economist, February 12, 2009. 
King, Mervyn.  “Debt-Deflation: Theory and Evidence.” European Economic Review 38:3–4 (1994): 419–445. 
Wolfson, Martin A.  “Irving Fisher’s Debt-Deflation Theory: Its Relevance to Current Conditions.” Cambridge Journal of
Economics 20:3 (1996): 315–333. 
Fixed Business Investment
 
Fixed business investment (FBI) is, as the name implies, the investment that businesses make in fixed assets,
primarily buildings and equipment (the latter including transportation and computing equipment) that have a useful
life span of more than one year. Gross FBI is defined as the purchase of new fixed assets and the depreciated
cost of used fixed assets. Net FBI is gross FBI minus the depreciated cost of used fixed assets. Net FBI
represents the net addition to the capital stock. FBI, along with residential investment, is defined as total fixed
investment in the national economic accounts. Residential investment includes new construction of residential
single and multifamily dwellings.
The other component of gross private domestic investment in the economic accounts is inventory investment,

which is not “fixed” within the business sector but a product destined for sale to other business, consumers,
government, or exports. In summary, gross private domestic investment includes FBI, residential investment, and
changes in inventories, the first two of which are considered fixed.
FBI is broken down into two basic categories—buildings and equipment. Nonresidential building investment
includes new structures (including own-account production); improvements to existing structures; expenditures on
new mobile structures; net purchases of used structures by private businesses and by nonprofit institutions from
government agencies; and broker fees and sales commissions connected to all of the above. Nonresidential
structures also include equipment considered an integral part of a structure, such as plumbing, heating, and
electrical systems.
Investment in equipment and software consists of capital account purchases of new machinery, equipment,
furniture, vehicles, and computer software; dealers’ margins on sales of used equipment; and net purchases of
used equipment from government agencies, persons, and the rest of the world. Own-account production of
computer software is also included. Information-processing equipment and software includes computer equipment
and software, medical equipment, and office and accounting equipment. Industrial equipment includes fabricated
metal products, engines, metalworking machinery, materials-handling equipment, and electrical transmission
equipment. Trucks, buses, truck trailers, automobiles (purchased by businesses rather than households), aircraft,
ships and boats, and railroad equipment are included in transportation equipment. The “other equipment” category
includes furniture, fixtures, and household appliances (purchased by businesses), and agricultural, construction,
mining, and service industry equipment.
FBI accounts are useful for a wide variety of purposes. For the business owner and investment analyst, the
elements in such accounts are often leading indicators of future economic activity. For example, if the accounts
detect the decision to postpone capital spending on new equipment, this is commonly perceived as a signal that
economic and business conditions will slow in the near-term future. Likewise, if economic conditions have been
slow, an uptick in fixed business investment suggests increased optimism by business and improved conditions in
the short-term future.
At the macroeconomic level (i.e., regarding the national economy as a whole), the data on investment are vital to
the study of productivity improvements in the economy. The quantity and quality of FBI is a forerunner of improved
productivity growth, output per worker, and future economic growth in terms of gross domestic product (GDP) and
per capita GDP. The exact mechanisms of these processes are under continuous scrutiny. The performance of
business investment as a source of macroeconomic instability is also an important area of research. In the late
1990s, for example, investment in computing equipment and software was a leading cause of the overheating of
the economy and the subsequent decline when the dot.com bubble burst.
At the microeconomic level, economists study the effect of business investments on economic growth for a
particular metropolitan area, state, group of states, or region. In particular, economists search for models that link
the investment performance of firms and their influence on overall regional productivity and growth. Development
policies by states and communities have been crafted that provide financial incentives for firms to invest in new
plant and equipment as a means of creating higher-paying jobs and generating higher rates of income growth.
In short, trends in fixed business investment are often important indicators of economic expansions or contractions
to come. As such, they are of considerable interest to economists who study the timing and intensity of business
cycles. During the economic expansion of 2005–2007, for example, FBI constituted about 25 percent of net
growth in the GDP, the key measure of national economic output. As real (inflation-adjusted) GDP grew at a pace
of 2 to 3 percent per year, about 0.5 to 0.8 percentage points of this growth was accounted for by FBI.
As the economy cooled during 2008, however, FBI also declined (with the exception of the second quarter),
contributing directly to the contraction. This was especially the case as the recession intensified following the
financial crisis of late 2008. Still, the decline in FBI played a relatively minor role in the fall in GDP that quarter,
compared to the much greater downturn in personal consumption expenditures.

Derek Bjonback
 
See also:  Mortgage, Commercial/Industrial;  Savings and Investment. 
Further Reading
Bondonio, Daniele, and Robert T. Greenbaum.  “Do Business Investment Incentives Promote Employment in Declining
Areas? Evidence From EU Objective 2 Regions.” European Urban and Regional Studies 13:3 (2006): 225–244. 
Doms, Mark.  “The Boom and Bust in Information Technology Investment.” Economic Review, Federal Reserve Bank of San
Francisco, January 1, 2004. 
Khan, Aubhik.  “Understanding Changes in Aggregate Business Fixed Investment.” Business Review, Federal Reserve Bank
of Philadelphia,  second quarter, 2001. 
Kopche, Richard W., and Richard S. Brauman.  “The Performance of Traditional Macroeconomic Models of Business
Investment Spending.” New England Economic Review, no. 2(March/April 2001): 3–40. 
U.S. Department of Commerce, Bureau of Economic Analysis. A Primer on GDP and the National Income and Product
Accounts. Washington, DC: U.S. Government Printing Office, 2007. 
Fleetwood Enterprises
 
Formerly based in Riverside, California, Fleetwood Enterprises was, until its 2009 bankruptcy, the largest producer
of recreational vehicles (RVs) and manufactured (mobile) homes in the United States. Fleetwood experienced
great swings of fortune through much of its history, and its fate illustrates the impact volatile fuel prices and tight
credit can have on business.
Founded in 1950 under the name Coach Specialties Company, the firm specialized in the manufacture of window
blinds for travel trailers. After its owner John Crean built his own travel trailer in the early 1950s, the company won
a contract to build the vehicle for a local dealer. Crean did well, but the seasonal nature of the business led him
to expand into the production of manufactured homes, a growth industry during the early baby boom years.
Cheaper to build and not reliant on weather conditions to construct, manufactured homes grew in popularity in the
first decades after World War II.
In 1957, the firm reincorporated as Fleetwood Enterprises and began to buy up other travel trailer firms. In 1965,
the firm went public. These were boom years for both the recreational vehicle and manufactured home industries.
But the oil shocks of the 1970s and early 1980s—which sent gas prices soaring—hit the RV industry hard, as did
the harsh recession of the early 1980s. For the first time in its history, Fleetwood had to cut back on production,
closing nine of its manufacturing plants.
While low oil prices and a booming economy allowed Fleetwood to prosper through much of the 1980s, it also
faced growing government scrutiny. In 1988, the company paid a multimillion-dollar fine after investigations by the
U.S. Justice Department and the Housing and Urban Development Department led to charges that its
manufactured homes were defective. In addition, several company subsidiaries were fined after pleading guilty to
overcharging veterans for financing.

By the boom years of 1990s, Fleetwood was once again prospering, with a 21.6 percent share of the
manufactured home business and a 34.6 percent share of the motor home market, the most lucrative market in
the RV sector. At its peak, it employed some 21,000 workers. But it was also facing increased competition in both
sectors. In response, the company teamed up with the Michigan-based Pulte Corporation to set up a nationwide
network of retail centers as a way to counter competition. It also bought out competitors. But the rapid expansion
saddled the company with a significant debt load that it was never able to get out from under.
By 2001, the increased competition and the servicing of Fleetwood’s debt sent the company permanently into the
red. Two events in 2008 sealed the company’s fate. In the summer peak season for RV sales, gas prices soared
to more than $4 per gallon nationally, crippling demand for the fuel-hungry RVs the company manufactured. Then,
in the fall, credit markets collapsed. As with standard homes, most manufactured homes and RVs are largely
purchased on credit. When that became much harder to obtain—particularly for the typically lower-income buyers
of manufactured homes—the bottom fell out of that market as well.
By November 2008, Fleetwood’s shipments of RVs had fallen to their lowest level since 1978 and it was forced to
shut down its RV division, closing plants and laying off 2,500 workers. This was not enough. By March 2009,
when the company filed for protection under Chapter 11 of U.S. bankruptcy law, it had assets of $558.3 million
and debts of $518 million. In June, the bankruptcy court approved the sale of the company’s shut-down RV
operations to a New York private equity firm for $53 million, which reopened several of the company’s former
plants in Decatur, Indiana, while the manufactured housing division was sold to Cavco Industries of Phoenix.
James Ciment
 
See also:  Manufacturing;  Recession and Financial Crisis (2007-). 
Further Reading
“As Fleetwood Enterprises Fades from Inland Area, It Leaves Trail of Prosperous Times.” The Press Enterprise, August 18,
2009. 
 
 
Florida Real-Estate Boom (1920s)
 
One of the greatest land-sale booms in twentieth-century America, the Florida real-estate frenzy of the early
1920s saw a huge run-up in land prices in Miami and other parts of South Florida—much of it fueled by canny
developers—before crashing in 1925. Although the boom was short-lived, it increased public awareness of Florida
as an attractive place to live and left behind a number of new towns that would grow into major population centers
after World War II.

 Underlying Factors
The Florida real-estate boom was a direct outgrowth of the economic prosperity and easy money culture of the
United States in the 1920s. Between 1921 and 1929, the U.S. gross national product rose from $74.1 billion to
$103.1 billion, an increase of 40 percent, one of the fastest expansions in history. Meanwhile, per capita income
rose from $640 to $850, a nearly one-third increase. In the low tax, pro-business environment of the day, most of
the gains accrued to the wealthy, but the middle class and skilled members of the working class saw significant
gains as well.
Moreover, following the sharp but brief recession of 1921–1922, the newly created Federal Reserve Board rapidly
increased the money supply by lowering the interest rate it charged member banks to borrow money. This
produced a flood of cheap credit for borrowers throughout the country, as banks became more amenable to
loaning money for real-estate purchases and, in the case of Florida, speculation.
For much of its history since joining the Union in 1845, Florida had been a relatively small and insignificant state,
with most of the population residing in its northern half, where they engaged in agriculture. Hot, humid, and
swampy, the southern half of the state was seen as disease ridden and unsuitable for settlement. In 1900, Florida
still had the smallest population of any state east of the Mississippi and, as late as 1910, its population had yet to
reach 1 million.
Among the residents at the time, however, were a number of wealthy and socially prominent individuals, drawn to
the state as a winter home by its subtropical climate and transported there on a new railroad line that, by the mid-
1890s, had reached Palm Beach and Biscayne Bay, where Miami was located. Over the course of the early
twentieth century, the Atlantic Coast of Florida began to gain a reputation as a warm-weather paradise.
Beyond the railroad and a few luxury hotels, there was little infrastructure. In 1910, an automotive parts millionaire
named Carl Fisher bought a winter home in Miami. Eyeing a barrier island several miles across Biscayne Bay,
Fisher decided it would make an ideal beachside resort. With developer John Collins, Fisher built a bridge and
began dredging swampland, planting trees and other plants to stabilize the expanded island.
While Collins undertook much of the civil engineering, Fisher focused on promotion, utilizing newly developed
advertising and public relations techniques to sell Miami Beach—the island had been incorporated in 1915—to
northerners looking for a warm winter home. Celebrities were encouraged to visit, and pictures of bathing beauties
were placed in newspapers. All kinds of stunts were tried, including the use of baby elephants as golf caddies to
promote South Florida as the ideal place for those interested in the sport. In general, Miami was sold as an exotic
tropical outpost that one could reach by train from the Northeast in a little over one day.
 Boom
At first, conditions were not conducive to Fisher’s campaign, particularly after the United States entered World War
I. He even offered free lots to anyone who would build on the island, but got few takers. Things changed when
the war came to an end, and especially when the recession of 1921–1922 gave way to the “Coolidge prosperity”
of the mid-1920s. By the end of 1922, the boom was in full tilt.
Over the next two years, Florida’s population increased by 300,000, with most of the increase coming in the
southern part of the state. Yet while the population increase contributed directly to the land boom, what really
drove up prices was speculation. Investors large and small began to see Miami real estate not as a place to build
a home but as an opportunity to make money. Lax regulation meant that banks—some of them founded
specifically to cash in on the boom—could offer easy-term loans of 10 percent down (an almost unheard-of deal
in 1920s America) with little capital in their vaults to safeguard depositors against default. Indeed, default seemed
a distant possibility in the early 1920s, as investors bought property for just months or even weeks at a time
before selling it for a profit and investing in more. So contagious was Florida land fever that most of the investors
bought property sight unseen: fully two-thirds of sales were done by mail.

High-rise construction fills the Miami skyline during the Florida real-estate boom of the 1920s. Easy credit,
rampant speculation, and rapidly escalating property values ended abruptly in 1925, but the growth and
development had lasting effects on the state. (The Granger Collection, New York)
 Bust and Aftermath
By 1925, signs began to appear that the market was overheated. Land prices had risen so high—into the
hundreds of thousands of dollars for some parcels (or millions in 2009 dollars)—that the middle class was priced
out of the market. Adding to inflation was a shortage of building materials. So overburdened was the railroad
connecting Miami with points north that, at one point, it refused to transport any freight other than essential goods.
The situation was made worse by the sinking of a large ship at the entrance to Miami Harbor, blocking sea access
to the city, in January 1926. Meanwhile, Miami Beach was not the only new resort municipality in South Florida.
Other developments along the Atlantic Coast, such as Boca Raton and Hollywood, along with Tampa and Marco
Island on the Gulf Coast, were increasingly competing for investor dollars.
By late 1925, the bubble was beginning to deflate, as financial advisers began to warn investors that rising land
prices were not based on the actual value of the property but on the prospect of that land being quickly resold. As
prices began to deflate and people were unable to sell the land, they could not service their loans. This forced
much property into foreclosure and put further downward pressure on prices. Local banks, already hurt by a slump
in agriculture, which had been hit by a series of devastating freezes, began to go under. At the same time, a
slump in the bond markets made it difficult for new municipalities to borrow money to pay for the improvements all
the newcomers demanded and needed, further rubbing the shine off Florida as a place to winter or live year-
round.
According to many accounts, a devastating hurricane that struck Miami in September 1926 caused the bursting of
the bubble. In reality it was only the last pinprick. Prices had already deflated significantly in Miami and many of
the other South Florida developments when the storm laid waste to the east coast and panhandle of Florida,
causing some $14 billion in damage statewide (about $170 billion in 2008 dollars).
While the Florida real-estate boom was short-lived, it left a lasting legacy in the form of new counties, new towns,
and, most importantly, an image of the southern half of the state as a carefree getaway or living destination with a
warm year-round climate. With the national economic boom and the development of low-cost air-conditioning after
World War II, that image would pay great dividends in an expanding population and economy that would make
Florida the fourth-largest state in the Union by the early 2000s.

James Ciment
 
See also:  Asset-Price Bubble;  Boom, Economic (1920s);  Housing Booms and Busts;  Real-
Estate Speculation. 
Further Reading
Foster, Mark S. Castles in the Sand: The Life and Times of Carl Graham Fisher. Gainesville: University Press of
Florida, 2000. 
Frazer, William Johnson, and John J. Guthrie, Jr. The Florida Land Boom: Speculation, Money, and the Banks. Westport,
CT: Quorum, 1995. 
Nolan, David. Fifty Feet in Paradise: The Booming of Florida. New York: Harcourt Brace Jovanovich, 1984. 
Foreclosure
 
A foreclosure occurs when a lender (bank or secured creditor/investor) regains control over, or repossesses,
property used as collateral for a loan. The lender’s objective is to resell the property in an attempt to recover the
amount owed against it. Foreclosure begins after a borrower defaults on a loan payment and the lender gives
public notice to the borrower that his or her right to redeem the mortgage has been severed.
 Role in the 2008–2009 Recession
A fall in residential real-estate prices and a sharp rise in home mortgage foreclosures played an important role in
causing and perpetuating the recession that began in the United States in December 2007. In a modern economy
characterized by well-developed capital markets, ample liquidity, and widespread access to credit, consumption
expenditures are a function not only of income, but also of wealth and consumer confidence. Income matters, but
consumption growth can rise above income growth for long periods of time. Prior to the recession, rising housing
values were tapped through home equity lines of credit to finance household expenditures. These credit-fueled
expenditures were used to maintain standards of living during a period of flat or declining wage growth. As long
as housing prices continued to increase, consumer expenditures could rise, and they did. In the United States,
consumption has risen from 63 percent of gross domestic product (GDP) in the 1950s to 70 percent today. Thus,
over the last several decades, economic growth has been largely driven by consumption expenditures. These in
turn have been driven by rising consumer confidence and the home equity credit made available from the upward
trend in home prices. Both came to an end in 2007.
Between 2007 and early 2010, median U.S. real-estate values fell by over 20 percent. According to Federal
Reserve estimates using Mortgage Bankers Association data, in the first half of 2007, 650,000 foreclosures were
initiated. By the first half of 2008 this nearly doubled to 1.2 million. The share of total mortgages in foreclosure
increased from 1 percent in 2005 to 3.3 percent in 2008. Among subprime mortgages, the share in foreclosure
increased from 3.3 percent in 2005 to 13.7 percent in 2008. These trends led to a recessionary cascade of
decreasing consumption, falling GDP, growing unemployment, falling incomes, and many households owning more
against their homes than what the homes were worth. The result was a further increase in foreclosures. To stop a
downward spiral such as this, housing prices must stabilize to prevent further erosion in wealth. In addition,

household expenditures must be buoyed by increases in wage income rather than by further increases in
household debt.
 The Foreclosure Process
The foreclosure process varies depending on state laws as well as lenders’ specific policies. However, there is a
typical process that applies to most. Generally, after the first two months of missed payments, the borrower will
receive letters and phone calls from the lender. Late fees will apply and begin to accrue. After the third month of
nonpayment, the borrower will likely receive a demand letter from the lender requesting that the borrower make
his or her loan current within thirty days. If the borrower fails to do this, then the lender sends out a foreclosure
package and gives public notice that the loan is being called. The full amount is now due, and the foreclosure
process has been initiated.
 Types of Foreclosure
There are three primary types of foreclosures: judicial, power-of-sale, and strict foreclosure. Judicial foreclosure
takes place through the use of the court system. The lender (or lender acting on behalf of a secured
creditor/investor) initiates the foreclosure by filing a claim with the court. After the claim has been filed, the
borrower will receive a notice from the court requesting payment. If the borrower does not make a monthly
mortgage payment within thirty days, the house can be auctioned by the sheriff’s office. When the house is sold,
the individuals living in house will receive an eviction notice from the sheriff’s office and will be forced to leave the
property. Typically, the borrower has a 180-day redemption period following the sheriff’s sale. During this period
the borrower can pay off the loan in full and would then be allowed to reclaim the property. This rarely occurs.
A power-of-sale foreclosure can take place when there is a clause included in the deed of trust or mortgage that
grants the lender the right to sell the property without judicial proceedings in case of default. This is similar to a
judicial foreclosure, but the lender demands payment directly from the borrower rather than working through the
court. The lender, rather than the sheriff, is also the one to carry out the auction of the property after the borrower
has failed to make payments within the specified time. This type of foreclosure generally takes place more quickly
than judicial foreclosures.
A strict foreclosure occurs when the lender itself takes possession of the property after the borrower fails to make
a payment within the court-ordered time period. This differs from judicial foreclosure in that there is no auction of
the house, but the lender, instead, takes direct possession of the property. Although strict foreclosure was the
original form of foreclosure, it is now limited to a few northeastern states. It is also typically limited to situations in
which the mortgagee owes more on the property than it is worth. Despite the differences in process, each of
these forms of foreclosure requires a public notice of foreclosure. These public records provide the basis for the
widely reported numbers and trends in foreclosures.
If the amount of money made on the sale of the property exceeds what is needed to cover the amount of the
mortgage and costs of foreclosure—which rarely occurs—the borrower will receive the surplus. If there is not
enough money to compensate for the foreclosure costs and mortgage, then the lender can try to receive additional
funds through a deficiency judgment in most states. The deficiency judgment is a separate legal action, and it
gives the lender the right to take other property from the borrower in order to satisfy the remaining debt. In most
cases this is not pursued because the borrower could and would claim bankruptcy, making the deficiency
judgment moot. In some states, first mortgages are nonrecourse loans, meaning that if the original mortgage has
not been refinanced, then the only recourse for the lender is to seize the home; in this case, the lender is not
able to “go after” the personal assets or income of the borrower.
The borrower’s final option to retain his or her home can occur during a redemption period. After the sale has
been made, some states refrain from transferring the title of the house until after the specified redemption period
is complete. If the borrower can repay the full amount of the mortgage as well as the foreclosure costs, he can
reclaim the house.

 Recent Trends and Areas of Contention
The number of foreclosures continued to rise in the wake of the housing crisis of 2007. According to the
Economist, more than 5 million homes in the United States entered the foreclosure process between 2006 and
2008. The International Business Times reported that the total number of foreclosures in 2009 reached 2.8 million,
which was 21 percent higher than the number of foreclosures in 2008 and 120 percent higher than in 2007. In
2010, the number climbed to a record high 2.9 million. Many experts said this would have been even higher if
banks had not slowed down their foreclosure efforts in the wake of an ongoing scandal involving overly hasty and
improper foreclosure proceedings. This involved so-called “robo-signing,” whereby the paperwork involved in
foreclosure was not properly vetted, resulting in some homeowners being improperly foreclosed upon.
Meanwhile, certain areas of the country were far more affected by foreclosures than others. Nevada, Arizona,
Florida, California, and Utah had the highest foreclosure rates. Coupled with the increasing number of foreclosures
was a decrease in home prices. From 1999 to the summer of 2006, home prices doubled, making housing a
valuable investment. However, according to the New York Times, housing prices fell by about 27 percent between
the summer of 2006 and the end of 2009. The freefall slowed somewhat in 2010, with housing prices falling by
about eight percent. A similar annually adjusted drop was experienced in the first three quarters of 2011.
The reason for the decrease in housing prices and increase in the number of foreclosed homes stems in part from
lending practices. Subprime loans were one area of contention. “Subprime” refers to the perceived lower quality of
the loan. Many subprime loans had low initial “teaser” rates to entice borrowers; these loans were usually
characterized by higher fees and interest rates. Furthermore, interest rates on loans with the teaser rates usually
escalated to very high rates in two to three years. As a result, many borrowers who would not traditionally have
been approved for a mortgage were able to purchase homes. An ongoing question is what share of these
borrowers could have qualified for traditional, prime mortgages had they been steered in that direction.
Adjustable rate mortgages were also utilized. These are loans with low initial rates that increase after a few years
or in response to changes in overall interest rates. Additionally, a number of “liar” and/or “NINJA” loans were also
originated. Liar loans are mortgages granted to borrowers who gave information on their income and assets
without providing documentation. Similarly, NINJA loans were mortgages obtained by individuals with no proof of
income, job, or assets. These loans were given to borrowers who then purchased homes that they were unable to
afford. When the owners of these homes began to default, it led to problems for the buyer, the bank, and the
businesses and individuals who had invested in mortgages or securities backed by those mortgages. Since
housing prices had decreased, many borrowers were now “under-water,” owing more on their house than it was
worth. Selling or refinancing the home was not a realistic option because the borrower would still not be able to
pay the mortgage. Thus, many of these loans resulted in foreclosure, which further depressed housing prices.
 Proposed Policy Solutions
In order to rectify this downward spiral of decreasing housing prices and increased foreclosures, several
government programs have been proposed. Hope for Homeowners and FHASecure were two initiatives developed
during the Bush administration. Both of these programs provided avenues for borrowers to refinance their loans to
government-secured, fixed-rate, thirty-to forty-year mortgages administered through the Federal Housing
Administration (FHA). Both of these programs were severely underutilized, and the FHASecure program was
terminated in 2008. The Hope Now Alliance has been more successful, as the coalition of mortgage industry
executives, counseling agencies, investors, trade groups, and mortgage companies provided a hotline for
borrowers to call and be connected with a mortgage counselor. Representative John Conyers, Jr., supported the
Homeowners Protection Act of 2008, which would have allowed bankruptcy judges to modify mortgages by
reducing the amount owed, extending the life of the loan, or adjusting the interest rate. This practice is also known
as the “cram-down,” meaning the lender would be forced to accept treatment of a loan that it did not agree to. By
allowing judges to modify mortgages, homeowners could remain in their homes while they worked to pay off the

modified mortgage. This proposal did not become law.
President Barack Obama has also proposed a plan for keeping people in their homes. One part of his plan was to
allow Fannie Mae and Freddie Mac loans that are underwater to be refinanced. Another component of his plan
was to decrease monthly payments for individuals who were near foreclosure. With the help of Fannie Mae,
lenders are now routinely doing loan modifications before the foreclosure process commences. Interest rates are
lowered, terms extended, and in some cases principal amounts are reduced in an attempt to reduce mortgage
payments to 31 percent of a borrower’s gross monthly income. Unfortunately, many borrowers seeking loan
modifications are also burdened by credit card, auto, and other personal loans. Thus, the hope that modifications
would greatly reduce foreclosures has not been realized.
Homeowners were not the only ones who received assistance through government programs. Banks were also
lent funds to help with the losses they suffered during the housing crisis. Former Treasury secretary Henry
Paulson initiated the Troubled Asset Relief Program, also known as TARP. The original plan for TARP consisted
of banks auctioning their bad loans and other struggling assets at a price that would benefit the bank in the short
term and the government in the long run. Additionally, the TARP Capital Purchase Program allowed the
government to infuse capital into banks through purchases of senior preferred stock, which became the main focus
of TARP. TARP led to the government owning preferred stock in hundreds of banks as well as providing funding
to insurance giant AIG, General Motors, and Chrysler. Another step taken was to place Fannie Mae and Freddie
Mac under government conservatorship in September 2008. This conservatorship resulted in the replacing of the
board of directors and chief executive officers with individuals appointed by the Federal Housing Finance Agency,
and quarterly capital investments by the government. President Obama’s plan also provided some assistance for
banks. His plan of reducing monthly payments for mortgage holders came with incentive payments for each
modified loan the lender completed. The plan was estimated to provide relief for 3 million to 4 million people. As
of December 2009, there have only been 66,465 permanent modifications and 787,231 active trial modifications.
Andrea Krogstad and Reynold F. Nesiba
 
See also:  Housing Booms and Busts;  Mortgage Lending Standards;  Mortgage, Subprime; 
Recession and Financial Crisis (2007-). 
Further Reading
Burton, Maureen, Reynold Nesiba, and Bruce Brown. An Introduction to Financial Markets and Institutions.  2nd ed. Armonk,
NY: M.E. Sharpe, 2010. 
Mortgage Bankers Association:  www.mortgagebankers.org
 
Fragility, Financial
 
The term “financial fragility” refers to the degree to which a nation’s financial system—made up of stock markets,
currency markets, banks, and the like—is vulnerable to collapse. Some economists believe that financial fragility
lies at the heart of the “bust” phase of the boom-and-bust cycle. While not all explanations of booms and busts
support the superiority of financial causes, almost everyone agrees that the collapse of a large bank, for example,

can make a bad economic situation worse.
Among the most prominent of recent theorists to explore financial fragility in depth was the American economist
Hyman Minsky (1919–1996). In his financial instability hypothesis (FIH), Minsky describes how a prolonged
economic expansion encourages investors to replace their expectations of a normal business cycle (incorporating
an expected recession) with the expectation of ongoing expansion. The expectations of continuing profits in turn
encourage businesses to take on greater debt, which, over time, leads to higher outflows relative to inflows, and
hence greater financial instability. Where initially investors and businesses are cautious about the amount of funds
they borrow and on what terms, borrowers become much less wary as the expansion continues. More and more
funds are borrowed for shorter and shorter periods, with shorter loans rolled over and the loan payments
becoming much more dependent on the delivery of capital gains. Over time, a given economic slowdown that
eliminates capital gains and lowers business profits becomes that much more threatening to the overall system.
“Thus,” wrote Minsky, “after an expansion has been in progress for some time, an event that is not of unusual size
or duration can trigger a sharp financial reaction.”
A second source of fragility stems from the liquidity of marketable financial assets such as stocks, currencies, and
derivatives (futures, option contracts, and the like). While the ease with which one can buy and sell part ownership
of a company, for example, makes it considerably easier to raise funds to finance business operations and
expansion (since with each investor committing only a fraction of his or her wealth to owning part of the business,
the risk is reduced for each person), the greater liquidity also makes it much easier to sell the shares if one is
dissatisfied with the company’s performance and the return on one’s investment. If profits are weak, dividends are
low and the stock price increases that generate capital gains for the investor will be minimal or nonexistent.
Facing low profits and minimal capital gains or even capital losses, shareowners are much more likely to sell their
shares than they are to step in and try to help improve company performance. Although the company is itself a
longer-term venture, there are, in a system of finance capitalism, fewer longer-term individual stakeholders in its
success. The result is a greater impatience and movement of capital of the type stressed by British economist
John Maynard Keynes in the 1920s. In this way, following George Edwards and other early writers on finance
capitalism, the conversion from real to financial equity alone introduces an independent source of fragility.
The possibility that one can borrow to purchase liquid financial assets introduces a third source of financial
instability. Borrowing to buy financial assets raises asset prices in a boom and lowers them in a bust. Suppose, for
example, that stock in a company costs $100 per share to buy and that it is purchased on a 60 percent down
payment or margin—that is, with $40 of borrowed money. If the stock price rises to $150, the investor still owes
only $40 but now has $110 of equity or margin collateral in the stock. The investor may use the $50 capital gain to
leverage the purchase of an additional share in the company. Many investors acting to leverage their purchase of
more shares in this way will drive up stock prices even further. But there is symmetry in this pyramiding process.
When the trend is reversed, a fall in the market price of shares eliminates margin collateral. If the original stock
price drops instead from $100 to $65, say, the investor still owes $40 but on a much devalued asset. Based on a
value of $65, the maximum margin loan would be 40 percent of $65, or $26. In liquid-asset markets (stocks,
currencies, options), lenders will demand that the investor pay off at least $14 of the $40 debt. If many investors
sell off the stock to pay for such debt, stock prices will fall even further.
It is well known that financial liabilities incurred by a margin investor are fixed in nominal terms ($40 in the above
example), but market prices are used to value the collateral asset. By forcing sales when asset prices are low or
falling (which drives down asset prices even further), the market creates the paradoxical result that “the more
debtors pay, the more they owe”—the Irving Fisher paradox (1932). A debt-deflation spiral of this type will bring
distress both on lenders, who do not recoup the total amounts owed through sales of collateral assets, and on
other asset holders, whose real value of outstanding debt (nominal debt divided by the value of the collateral
asset) rises with each fall in asset prices.
Increasingly optimistic expectations, liquidity, and pyramiding all serve to drive the expansion into an area of
increasing fragility. Where the changes in the underlying real economy are new (as when a technological
innovation causes restructuring in large parts of the economy), the fragility is further enhanced by the intrusion of

emotion and the increasing importance of the opinion of others. Economists Benjamin Graham, David Dodd, and
Sidney Cottle noted in their influential analysis of securities values of the early 1960s the effect emotion can have
on asset values, while Brenda Spotton Visano has demonstrated how a social dimension of investing serves to
augment a Minsky-type analysis of fragility when investors are facing a radically new—and unpredictable—
economic environment.
In short, financial fragility is an aspect of the boom-and-bust phenomenon that attributes the fluctuations in output,
employment, and prices to an inherently unstable financial system. It is a perspective that suggests the economy
under finance capitalism is essentially more fragile than the underlying real economy alone.
Brenda Spotton Visano
 
See also:  Minsky’s Financial Instability Hypothesis. 
Further Reading
Edwards, George. The Evolution of Finance Capitalism. New York: Augustus M. Kelley, 1938;  reprint 1967.
Fisher, Irving. Booms and Depressions: Some First Principles. New York: Adelphi, 1932. 
Graham, Benjamin, David L. Dodd, and Sidney Cottle. Security Analysis: Principles and Techniques.  4th ed. New
York: McGraw-Hill, 1962. 
Keynes, John Maynard. A Treatise on Probability. Vol. 8,The Collected Writings of John Maynard Keynes, ed. D.
Moggeridge and E. Johnson. London: Macmillan, 1973, 1921. 
Minsky, Hyman. Can “It” Happen Again? Essays on Instability and Finance. Armonk, NY: M.E. Sharpe, 1982. 
Visano, Brenda Spotton. Financial Crises: Socio-economic Causes and Institutional Context. New York and
London: Routledge, 2006. 
 
France
 
France has been one of Europe’s and the world’s great economic and political powers since the end of the Middle
Ages and its consolidation as a nation-state. However, from monarchy to republic, and through devastating wars,
the French economy has experienced marked periods of both economic growth and stagnation. Perhaps Europe’s
greatest power in the sixteenth century, France fell behind Great Britain and Germany industrially in the
nineteenth century, only to experience remarkable economic growth from World War II to the 1970s. Although it
lagged behind the United States, Germany, Japan, and smaller nations in the late twentieth century and, like other
developed countries, was challenged by the rise of China and East Asia by the turn of the twenty-first century,
contemporary France remains one of the world’s wealthiest, most productive economies, a global leader in many

economic sectors.
French president Nicolas Sarkozy announces a 35-billion euro ($52 billion) government investment plan in 2009 to
boost the nation’s economic growth in the medium term. The program targeted sectors of declining
competitiveness, such as universities. (Eric Feferberg/AFP/Getty Images)
 Middle Ages to Napoleonic Rule
During the sixteenth, seventeenth, and eighteenth centuries, France moved from feudalism to a powerful,
centralized monarchy, epitomized by the reign of Louis XIV (ruled 1643–1715). Economically, France was guided
by mercantilism, a philosophy put into practice by Louis’s long-serving finance minister, Jean-Baptiste Colbert.
Mercantilism brought strict controls over the national economy, an influence that continues to the present day, but
also sought to discourage imports and maximize exports. Colbert instituted protectionist policies, the state issued
directives to guide production, industries were organized into guilds, and internal trade was encouraged with the
reduction of tariffs, creation of ports, and the building of roads and canals. New or newly state-controlled
industries included the royal tapestry works at Beauvais, the Gobelins tapestry works, marble quarries, and
purveyors of luxury goods. As the French Empire began to grow, the French East India Company was established
in 1664 and opened trade with West Africa. While France arguably was at the apogee of its power, Colbert’s
policies were a mixed blessing, encouraging industry but also incurring significant debt. The expulsion of the
Huguenots in 1685 and war drained France of talent and expanded its debt.
The early eighteenth century brought the introduction of the taille, or tax, and a system of monetary stability based
on conversion to gold and silver. However, the French economy did not begin to expand until the end of the
1730s, as agriculture, mines, metallurgy, and textiles became profitable industries. Technological development
from abroad, such as John Kay’s flying shuttle and James Watt’s steam engine brought the first signs of the
industrial revolution to France. Joseph-Marie Jacquard’s loom for weaving figured fabrics sparked a burst of early
development in the textile industry, and the first machines were introduced into production. Cities such as Lyons,
an early home to textile mills, as well as Marseille, Bordeaux, and Nantes, became important commercial centers.
Trade with North America and the Antilles grew significantly, with France importing sugar, coffee, cotton, and
slaves. Paris became a center of international banking by the 1780s. France and Britain were the world’s two
richest countries in the mid-1700s, but France was slower to move to industrial production than its rival across the
English Channel.

Economic reversals dominated the mid-to-late eighteenth century, as the Seven Years’ War (1756–1763) resulted
in the loss of most of France’s North American colonies and a huge increase in public debt. Financial support for
the American Revolution, together with several agricultural crashes and harsh winters in the 1770s and 1780s,
brought the French to the verge of economic crisis. Louis XVI’s ministers Anne-Robert-Jacques Turgot and
Jacques Necker introduced reforms, paper money, and increased taxes through a program known as the
vingtième. This combination of factors, together with an increase in poverty and the spread of Enlightenment ideas
about liberty and democracy, led to the beginning of the French Revolution in 1789 and the fall of the monarchy.
The twenty-six years of revolution and Napoleonic rule brought French industrial development to a standstill,
leaving France even farther behind Britain in economic development. The radical, early years of the revolution had
sought to bring economic equality, but the Reign of Terror attacked merchants and other “enemies of the
revolution,” imposed wage and price controls, and brought anarchy, looting, and plundering to much of France.
The manorial system was ended, giving peasants ownership of their land. The military buildup under Napoleon
after 1795 briefly stimulated the economy, but the success of British blockades, high inflation, and military defeat
in 1815 left France in a shambles.
 From Laggard to Leader: Since 1815
The Restoration of the monarchy and the early nineteenth century saw a time of economic stability. France
remained largely an agricultural economy, with peasants living much as they had for hundreds of years. France
was Europe’s second most populous nation, after Russia, with about 30 million people, but barely 7 percent lived
in towns of 20,000 or more. Yet canal-and road-building proceeded apace under the restored Bourbon monarchy
of Louis-Philippe as well as during Napoleon III’s Third Empire. However, the years from the 1840s to 1870
ushered in significant industrialization as roads and railroads were built, factories opened, and educational reforms
were designed to raise students’ knowledge and skills. The cotton and textile industries flourished, with a well-
developed domestic supply of manufactured wool, yet French metallurgy and shipbuilding fell behind those of
Britain and Germany.
Indeed, by the time of the Franco-Prussian War (1870–1871), France had fallen significantly behind these
industrial powers, and people lamented le retard français, or French backwardness. While coal output increased,
France lagged behind other European powers in power generation, including hydroelectric, and its steel and iron
industries were laggards compared to its two major European rivals. The same was true in the new chemical and
electronics industries, despite earlier French inventiveness. By the turn of the twentieth century, most French
machinery was imported. In short, as large-scale industrialization proceeded apace in Britain, Germany, and the
United States, France remained a country of ateliers, or small workshops, with few employees.
Although Germany, the superior economic power on the eve of World War I, was defeated, the Great War was
catastrophic for France. More than 1.3 million Frenchmen died and 3 million were injured during the war, and the
country lost 27 percent of its eighteen-to twenty-seven-year-olds, leaving the labor force in decline into the 1930s.
The reparations extracted from Germany in the Treaty of Versailles failed to boost France’s economic recovery,
while ravaging Germany, a potentially peaceful trading partner. Industrial production did increase in the late 1920s,
but France’s combination of demographic stagnation, a huge and inefficient agricultural sector, and many poorly
equipped industries was indicative of the economy’s continuing structural weaknesses. France’s colonial empire—
in Africa, Asia, the Pacific, and the Caribbean—was as much a burden as a boon to the French economy,
although it reached its peak just before the rise of Adolf Hitler. The French Empire, second in size only to the
British, covered nearly 5 million square miles (13 million square kilometers), or 9 percent of the earth’s land area.
The Great Depression hit France later and with less severity than the United States or Britain, as the franc was
undervalued and its economy relied less on trade. Nonetheless, between 1931 and 1939, the French economy
was in decline, as the country’s production index declined about 10 percent and hundreds of thousands lost their
jobs, though fewer than in other countries. German reparations, a prop to the economy, ended, small and
medium-sized businesses suffered, and France’s relatively backward economy led it to emerge from the

Depression more slowly.
In the 1930s, the government nationalized industries such as railways, coal, banking, electricity, and natural gas.
The leftist Popular Front government, elected in 1936, introduced the forty-hour workweek and vacations with pay,
and responded to strikes by backing pay raises, but there was widespread civil unrest.
French occupation by the Nazis during World War II, the destruction of one-quarter of the nation’s wealth, and
immediate postwar privation gave way to one of the great miracles of economic development. In the late 1940s,
Keynesian ideas of state intervention to promote growth, the development of successful national planning by civil
servant and economic planner Jean Monnet (best known for leading the cause of post–World War II European
unity), a baby boom, and U.S. aid through the Marshall Plan jump-started what the French economist Jean
Fourastié was to call “les trentes glorieuses” (the glorious thirty years). During these three decades, the French
economy grew faster than that of the United States, Britain, and, for much of the period, even West Germany.
Anticapitalist sentiment, together with the heritage of Colbertism, made France ripe for state intervention. Monnet’s
“Plans” came to be known as “indicative planning,” in concert with industry and labor, and in contrast to
authoritarian Soviet-style planning. The Plans defined economic priorities, collected and disseminated a massive
array of economic statistics, did extensive economic forecasting, and brought together big business, labor unions,
and government to create an encouraging climate for business.
Although many industries and banks were nationalized, France achieved financial stability during the Fourth
Republic and was increasingly integrated into the European and global economies. The European Coal and Steel
Community, also devised by Monnet to unite Western Europe in peace and prosperity, led to the 1957 Treaty of
Rome, creating the European Economic Community (now, the European Union). Tariffs were eliminated, French
export industries blossomed, and the country’s sacred agricultural sector was protected by massive subsidies.
Although the state’s share of investment fell from 38 percent to 28 percent between the early 1950s and early
1970s, the government accounted for about half of the French economy by the latter third of the twentieth century.
The government invested heavily in prestigious projects and “national champions” such as Airbus, nuclear power,
transportation, information technology, and armaments.
With the most rapid economic growth between the early 1950s and 1973, and per capita income doubling in the
fifteen years after 1960, French average income grew from about one-half that of the average American in the
early 1950s to four-fifths by the 1970s, and France passed Britain to become the world’s fourth-largest economy.
Technological innovation, government planning, pent-up demand, and the growth of huge multinational French
industries contributed to the growth. By the beginning of the twenty-first century, France was a world leader in
industries such as aerospace, rail, luxury goods, tourism, nuclear power, automobile production,
telecommunications, pharmaceuticals, engineering, retail, capital goods, and banking, and its financial markets
grew dramatically. About a dozen of the world’s 100 largest companies and four of the top twenty-five were
French in 2009, ranging from oil giant Total, retailer Carrefour, automaker Peugeot, and banks such as BNP
Paribas and Société Générale. Other French giants include Saint-Gobain, Renault, Air France, Alstom, Christian
Dior, Alcatel, Michelin, L’Oréal, EDF, LVMH, and Sodexo. While supporting business and investing in research,
the government has helped maintain French education, health, and infrastructure among the world’s best. The
“French model” of balancing the dynamism of capitalism with a strong sense of social “solidarity” was supported
across the political spectrum. France has traditionally also been one of the strongest supporters of the European
Union and has benefited greatly by the increased access the union has given it to markets in other parts of
Europe—today some 60 percent of the country’s trade is with other EU members—and the subsidies the EU has
lavished on the French agricultural sector, the largest in the union.
Despite such benefits, since the 1980s French economic growth has slowed, as policy makers began to reform
labor markets and privatize many companies in the face of rapid global competition. High unemployment and a
sense of economic “malaise” gripped France by the early twenty-first century. During the global financial crisis that
began in late 2007, the French economy suffered, yet, as during the Depression, less than those of the United
States and Britain. Significantly lower household debt and higher savings, together with a $40 billion stimulus that
left government deficits at half the U.S. level, helped the economy emerge from recession, albeit with sluggish

growth. Despite long-term structural weaknesses in the French economy and the need for economic reform,
French productivity is the world’s highest and the average worker enjoys considerably more leisure and state
benefits than their counterparts in other large, rich countries; income statistics thus understated the strength of the
French economy and its people’s high standard of living.
In 2011, however, the French economy faced new troubles, as the sovereign debt crisis of its fellow Eurozone
members, most notably, Greece, threatened the French financial sector, which had heavily invested in the
securities issued by Italy, Greece, Portugal, Spain, and other troubled Eurozone economies. In response, the
government of Nicolas Sarkozy pushed for an aggressive response and a major bailout effort, but was met with
resistance by the German government of Angela Merkel, who was feeling pressure from taxpayers over rescuing
what were perceived as countries with profligate spending habits.
Andrew L. Yarrow
 
See also:  Germany;  Italy;  United Kingdom. 
Further Reading
Ardagh, John. France in the New Century: Portrait of a Changing Society. London: Penguin, 2000. 
Cameron, Rondo, ed., with Franklin F. Mendels and Judith P. Ward. Essays in French Economic History. Homewood,
IL: R.D. Irwin for the American Economic Association, 1970. 
Dormois, Jean-Pierre. The French Economy in the Twentieth Century. New York: Cambridge University Press, 2004. 
Hough, J.R. The French Economy. New York: Holmes & Meier, 1982. 
Melitz, Jacques, and Charles Wyplosz. The French Economy. Boulder, CO: Westview, 1985. 
Friction, Financial
 
In economics, the term “friction” refers to anything that slows or hampers trade, business, or exchange. It can be
related to cost, time, or any number of other factors. Capital gains taxes, a long line at a store, a new business’s
lack of reputation, or a student’s lack of access to loans are all forms of friction.
Financial friction, then, is any cost or other obstacle that causes a person or institution to not invest in something
or hold onto an asset that they would normally sell. As such, financial friction can either heighten or lower a
market participant’s level of risk. In short, financial frictions generate costs—loosely defined—that interfere with an
economic activity that a person would make in the absence of such friction.
When investors decide what to buy and sell, they normally balance risk against return. Riskier investments
promise potentially higher returns and vice versa. Financial frictions distort this decision-making process. For
example, in a theoretically frictionless world, there is nothing to stop an investor from shifting money from
corporate equities to bonds during a downturn in the stock market. But that investor might hesitate to do so if
selling the stocks triggers a broker’s fee or a capital gains tax.
Financial frictions have both direct and indirect costs. The former include the capital gains tax, while the latter
include the losses incurred by making less-than-optimal financial decisions. At the same time, financial frictions

can also bring direct and indirect gains. For every investor who pays a broker’s fee, say, there is a broker
collecting that fee. That would be a direct gain. In addition, a mutual fund that can lower what it charges to
investors in fees can increase its competitiveness, all other things being equal, vis-à-vis other mutual funds,
thereby heightening its profitability.
Moreover, over time, financial frictions change. Not only does Congress pass and repeal, and raise and lower
taxes but new technologies and new financial instruments also come into play. For example, computers have
made it cheaper to compare investment strategies, thereby lowering the costs charged by financial analysts. At the
same time, the growing complexity of financial instruments raises those costs.
Financial frictions fall into five basic categories: transaction costs; taxes and regulations; asset indivisibility;
nontraded assets; and agency and information problems. Transaction costs, the cost in money and time of making
a transaction, are generally relatively low, and new technology is bringing down those costs even further. Taxes
and regulations are self-evident; the capital gains tax, for example, might discourage an investor from selling an
asset in a given year to avoid having to pay it.
The financial friction arising from asset indivisibility arises because some assets simply cannot be divided into
portions small enough for every investor to own one—for example, a parcel of commercial real estate in
Manhattan. Thus, while an optimal investment strategy would dictate that investors own such an asset, the latter’s
indivisibility makes that difficult or impossible. Mutual funds and other collective investment schemes, such as real-
estate investment trusts (REITs), can overcome this friction since, by pooling investors’ money, they can buy a
large, indivisible asset and then divvy up returns on a pro rata basis.
Nontraded assets are those assets that simply cannot be traded or cannot be traded easily. For example, a person
invests tens of thousands of dollars in gaining education and skills but cannot sell that “human capital,” at least,
not since Abraham Lincoln signed the Emancipation Proclamation. However, constant financial market innovation
is ever expanding what can and cannot be traded. The explosion in debt-backed securities—whereby people
invest in the revenue streams arising from mortgages or credit card debts—shows the ingenuity of financial
institutions and their employees in overcoming the friction inherent in nontraded assets. Indeed, even human
capital has become a tradable commodity, as musicians such as David Bowie and James Brown have shown by
creating bonds to be paid off by the future earnings their skills and talents are expected to bring in.
Agency and information problems deal with the issue of incentive. It is a long held truism in the financial markets
—and life itself—that people are more likely to make wise decisions when it comes to their own money than when
it comes to the money of others. While it may make rational sense to purchase an asset controlled by another,
investors may hesitate to do so and give up direct control of their money, or they may worry that the seller knows
more about the liabilities that come with a particular asset and thus may hesitate to invest in it.
Financial frictions can also play a role in the business cycle, as the dot.com and housing booms and busts of the
1990s and 2000s make clear. The former boom and bust involved the dramatic run-up in the value of Internet and
other technology-related stocks in the mid-1990s, followed by their dramatic crash in the early 2000s. One of the
reasons for the sudden crash in share prices was due to the fact that little financial friction was involved in selling
off shares—transaction costs were minimal and people taking losses were not subject to capital gains taxes
(indeed, they could write off their losses against gains made elsewhere in their portfolio). Thus, when stock prices
began to decline, people rushed to sell off their shares.
As housing prices took off in the mid-2000s, some economists feared that this sector too was experiencing an
unsustainable bubble. They called for the Federal Reserve to raise interest rates so as to cool the inflation. But
defenders of the low interest rates, including, for a time, Federal Reserve chairman Alan Greenspan, argued that
housing prices were unlikely to experience the same dramatic swing as dot.com shares. They based their
argument on the steep transaction costs involved in buying and selling a home—broker commissions, taxes,
inspection fees, and so on. In other words, fears of housing market speculation were grossly exaggerated, they
said, because financial frictions in that sector limited them. But as subsequent events illustrated, when potential

returns are great enough—or when losses are substantial enough—investors will decide that the costs inherent in
financial frictions are not substantial enough to alter their investment behavior.
James Ciment
 
See also:  Financial Markets;  Savings and Investment;  Tax Policy. 
Further Reading
DeGennaro, Ramon P.  “Market Imperfections.” Journal of Financial Transformation 14(August 2005): 107–117. 
DeGennaro, Ramon P., and Cesare Robotti.  “Financial Market Frictions.” In Economic Review  of the Federal Reserve Bank
of Atlanta (third quarter 2007).
Luttmer, Erzo G.J.  “Asset Pricing in Economies with Frictions.” Econometrica 64:6 (1996): 1439–1467. 
 
Friedman, Milton (1912–2006)
 
One of the most influential economists of the late twentieth century, the Nobel Prize– winning Milton Friedman is
best known in the profession for his monetarist theories, which emphasized a gradual expansion of the money
supply as the best way to control inflation and create sustained economic growth. Outside of academia, Friedman
was a controversial figure, both an outspoken opponent of government intervention in the economy and the
intellectual mentor for conservative free-market politicians such as British Prime Minister Margaret Thatcher and
President Ronald Reagan.

The conservative, free-enterprise views of Milton Friedman, associated with the Chicago school of economics,
helped shape U.S. and international policy during the 1980s. His monetary approach provided a counterpart to the
fiscal policy of the Keynesians. (George Rose/Getty Images)
The son of Jewish immigrants, Friedman was born in Brooklyn, New York, in 1912, and grew up in suburban New
Jersey. A mathematics major at Rutgers University, he earned his master’s degree in economics at the University
of Chicago in 1933 and, after a number of years working for the federal government in a variety of economics-
related posts, he received his PhD in economics from Columbia University in 1946. That same year, he took a
teaching post at the University of Chicago, where he would remain for the next thirty years, helping to turn the
economics department there into a powerhouse of monetarist, free-market theory.
By his own admission, Friedman started out as a supporter of Keynesian ideas about the need for government to
stimulate demand as a way to lift economies out of downturns. As part of New Deal Washington in the 1930s, he
recalled later, he supported the job creation programs of President Franklin Roosevelt but questioned the
administration’s efforts to fix prices and wages, saying that it distorted more efficient market mechanisms for
allocating resources where they were needed.
Even as Friedman was beginning his teaching career, he was recruited by National Bureau of Economic Research
(NBER) head—and future Federal Reserve chairman—Arthur Burns. At the NBER, Friedman began in earnest his
study of the role the money supply played in the business cycle, research that would culminate in his
pathbreaking 1963 book, A Monetary History of the United States, 1867–1960, cowritten with his longtime
collaborator, economist Anna Schwartz. While, as the title indicates, the work covered the history of money and
monetary policy for the previous century, its most important findings concerned the Great Depression.

Conventional economic wisdom of the day had it that monetary forces played a minimal role in the economic
downturn of the 1930s. Friedman and Schwartz brought those forces to the forefront and blamed the Federal
Reserve (Fed) for making things worse by not keeping the money supply steady and not taking on the role of
lender of last resort.
Few books on economics have been more influential. Not only did it revive monetary theory as a key component
of economic thinking but it also influenced many future policy makers in times of economic crisis. Current Fed
chairman Ben Bernanke—himself a student of the Great Depression—has said that the book and Friedman’s work
generally helped lead him to embrace the activist role the Fed assumed during the financial crisis of the late
2000s.
Important as the book was in economic circles, it had little effect on economic policy makers of the day. The
Keynesian paradigm of activist government held sway for the first several decades after World War II, as both
Republican and Democratic administrations sought to stimulate demand during times of economic contraction by
direct spending, tax-cutting, and expansion of the money supply. But the “stagflation” of the 1970s—in which,
contrary to Keynesian theory, high unemployment was accompanied by high inflation—undermined the prevailing
liberal consensus and led to the triumph of conservatism at the ballot box.
Both Thatcher and Reagan adopted Friedman’s ideas. In the United States, the Fed raised interest rates—thereby
limiting the money supply—as a way to wring inflation out of the system. While this produced record postwar high
unemployment in the short term, it did tame inflation. In general, Friedman’s conservative argument that
government efforts to stimulate demand were counterproductive held sway in policy-making circles during the
1980s. Outside of the Anglo-American world, Friedman and the Chicago school of economics had great influence
over the Chilean government under dictator Augusto Pinochet in the 1970s and 1980s, which dismantled many
social programs, privatized industries, and emphasized free-market forces.
Friedman also did important research into economic questions beyond monetary policy, developing his “permanent
income hypothesis,” which stated that most consumers saved rather than spent windfall gains, such as those
provided by tax cuts, and that government could not reduce long-term unemployment through inflationary fiscal
policies.
In 1976, Friedman was awarded the Nobel Prize in Economic Sciences “for his achievements in the fields of
consumption analysis, monetary history and theory and for his demonstration of the complexity of stabilization
policy.” He retired the following year and became affiliated with the Hoover Institution, a conservative think tank at
Stanford University in California.
Aside from continuing his economic work, he became a public intellectual from the late 1970s onward. His 1980
book Free to Choose, cowritten with his wife Rose Friedman, was a paean to the efficacy of free-market
economics and became the basis for a much-watched documentary series on PBS. Friedman also became well
known for his espousal of libertarian ideas, not only advocating limited government in the economy but in the
social sphere as well, calling for the legalization of prostitution and drugs. He died in San Francisco in 2006.
James Ciment
 
See also:  Monetary Policy;  Monetary Stability;  Monetary Theories and Models;  Schwartz,
Anna. 
Further Reading
Butler, Eamonn. Milton Friedman: A Guide to His Economic Thought. New York: Universe, 1985. 
Ebenstein, Lanny. Milton Friedman: A Biography. New York: Palgrave Macmillan, 2007. 
Friedman, Milton, and Rose Friedman. Free to Choose: A Personal Statement. New York: Harcourt Brace

Jovanovich, 1980. 
Friedman, Milton, and Anna Jacobson Schwartz. A Monetary History of the United States, 1867–1960. Princeton,
NJ: Princeton University Press, 1963. 
Frisch, Ragnar (1895–1973)
 
Norwegian economist Ragnar Frisch is among those credited with introducing the use of mathematical formulas
and a scientific approach to the study of economics. He is responsible for coining the terms “macroeconomics,”
“econometrics” (which he defined as a scientific methodology and approach to studying economics, not as a
subdiscipline of the field), and “macroeconometrics,” as well as for making significant contributions in each of these
areas. In 1969, he was a co-recipient with Jan Tinbergen of the first Nobel Prize in Economic Sciences for their
work in macroeconometrics, in which they explored variations in business cycles largely as a response to the
worldwide economic problems of the Great Depression. Frisch is proud of the Antonio Feltrinelli Prize awarded to
him in 1961 by the Accademia Nazionale dei Lincei, the famous Italian society of which Galileo Galilei was a
member.
Ragnar Anton Kittil Frisch was born in Oslo, Norway, on March 13, 1895. He graduated from Oslo University in
1926, spent a year as a visiting professor at Yale University in New Haven, Connecticut, and then was named
professor of social economy and statistics at Oslo University, a position he held until his retirement in 1965; he
also served as director of the Institute for Social Economy at the university.
Frisch was a pioneer in using mathematical tools in the study of economic problems. Although others had
explored that approach since the nineteenth century, Frisch was able to achieve substantive breakthroughs using
mathematics. Underlying his approach was a belief that economics could be an exact, quantitative science, much
like physics. The field of econometrics was a direct result of his scientific, empirical, and quantitative approach,
with the critical benefits of providing statistical data that could be used as a reliable basis for testing economic
theories and rigorous models capable of predicting changes in an economy.
Among the specific areas that Frisch explored was demand theory, centering on consumer behavior. Additionally,
he developed a comprehensive theory that explained production from the perspective of the processes
themselves. In collaboration with economists Frederick Waugh and Michael Lovell, he developed a complex and
influential econometric formulation called the Frisch-Waugh-Lovell theorem.
Frisch made important contributions to the study of business cycles. His interest stemmed from concerns that
fluctuations in prosperity and depression could not be controlled and thus had a dramatic impact on employment.
To address this issue, he developed a novel method of analyzing time-series data and used it to analyze business
cycles so as to explain why prosperity or recession began or ended. He published his theory of business cycles in
a 1933 article in Econometrica titled “Propagation and Impulse.” Frisch served as editor of Econometrica, a
leading journal in the field, for more than twenty years.
In the years following World War II, Frisch focused his efforts on ways to modernize the economies of other
countries, particularly such developing nations as India and Egypt. In his later years, he opposed Norway’s
participation in the European Common Market. Frisch died in Oslo on January 31, 1973.
Robert N. Stacy

 
See also:  Akerman, Johan Henryk;  Babson, Roger. 
Further Reading
Andvig, Jens Christopher. Ragnar Frisch and the Great Depression: A Study in the Interwar History of Macroeconomic
Theory and Policy. Oslo: Norsk Utenrikspolitisk Institutt, 1984. 
Bjerkholt, Olav.  “Ragnar Frisch’s Business Cycle Approach: The Genesis of the Propagation and Impulse Model.”
European Journal of the History of Economic Thought 14:3 (September 2007): 449–486. 
Johansen, Leif.  “Ragnar Frisch’s Contributions to Economics.” Swedish Journal of Economics 71:4 (December 1969): 302–
324. 
Strøm, Steinar. Econometrics and Economic Theory in the 20th Century: The Ragnar Frisch Centennial Symposium. New
York: Cambridge University Press, 1998. 
Galbraith, John Kenneth (1908–2006)
 
A widely read Post Keynesian economist of the post–World War II era, John Kenneth Galbraith was a major figure
in the institutionalist school, a historically oriented school of economic thinking that examined the role of
institutions—both social and economic—in shaping how economies operated. While Galbraith is best known for his
work on income disparities, private wealth, and corporate power, he also examined the role of speculation in
financial crises, most notably in his books The Great Crash, 1929 (1955) and A Short History of Financial
Euphoria (1993).
Galbraith was born in a small town in Ontario, Canada, in 1908. Befitting his rural upbringing, his education
focused on farming issues. He received his bachelor’s degree from the Ontario Agricultural College and his
doctorate from the University of California, the latter in 1934, and both in agricultural economics. He became a
U.S. citizen in 1937. Combining academics, public service, and journalism, Galbraith had a variegated career,
teaching economics at Harvard University from the 1930s onward, editing Fortune, a business magazine, in the
mid-1940s, serving as deputy head of the Office of Price Administration, a price-setting federal agency, during the
first half World War II, and then taking up the post of ambassador to India in the John F. Kennedy administration.
Galbraith had the rare knack of making economics accessible to ordinary readers, and several of his books—most
notably, The Great Crash, The Affluent Society (1958), and The New Industrial State (1967)—topped nonfiction
best-seller lists. While not as widely read as the above, his book American Capitalism: The Concept of
Countervailing Power (1952) contended that the economic power of big business was increasingly held in check
by countervailing institutions, such as government regulatory agencies and unions. In The Affluent Society,
Galbraith bemoaned the danger of an American society in which increasing private wealth and an increasingly
impoverished public sector perpetuated disparities in income and wealth. The New Industrial State argued how
large corporations used sophisticated marketing and advertising techniques to shape demand, thereby distorting
the ordinary workings of the marketplace. A 1973 work, Economics and Public Purpose, offered solutions to
corporate power in the form of nationalizing the health and defense industries and putting in place wage, price,
and profit controls.
Two of Galbraith’s books focused primarily on the boom-and-bust cycle. In The Great Crash, he examined the

worst financial crisis in the history of capitalism, laying the blamed for the catastrophe on out-of-control
speculation in the corporate securities market, as crowd psychology replaced rational economic calculation,
leading otherwise sane investors—both big and small—to conclude that wealth could be accumulated quickly and
without any increase in productive output, and that share prices would consistently go up, outpacing real economic
growth.
In A Short History of Financial Euphoria, Galbraith expanded his study of speculative episodes across centuries of
capitalist history, noting that psychological factors make price-asset bubbles an intrinsic element of free-market
economies. Moreover, he argued that investors participating in such bubbles have a “vested interest in error,” that
is, in perpetuating the illusion that wealth can be created out of nothing, since that illusion is what serves to send
asset prices and investor returns upward. Presciently, Galbraith also used A Short History to deride the value of
new and more exotic financial instruments, pointing out the dangers of using excessive debt to finance
speculation, and noting how these developments could lead to disastrous collapses in asset prices that could drag
down whole economies.
James Ciment
 
See also:  Institutional Economics;  Post Keynesian Theories and Models. 
Further Reading
Galbraith, John Kenneth. A Short History of Financial Euphoria. New York: Whittle Books, in association with Viking, 1993. 
Galbraith, John Kenneth. The Great Crash, 1929. Boston: Houghton Mifflin, 1955. 
Keaney, Michael, ed. Economist with a Public Purpose: Essays in Honor of John Kenneth Galbraith. New
York: Routledge, 2001. 
Parker, Richard. John Kenneth Galbraith: His Life, His Politics, His Economics. New York: Farrar, Straus, and Giroux, 2005. 
Sharpe, Myron E. John Kenneth Galbraith and the Lower Economics.  2nd ed. White Plains, NY: International Arts and
Sciences, 1974. 
Williams, Andrea D., ed. The Essential Galbraith. Boston: Houghton Mifflin, 2001. 
 
Geithner, Timothy (1961–)
 
Timothy F. Geithner was named secretary of the treasury by president-elect Barack Obama in November 2008,
barely three weeks after Obama won the election and in the midst of the national economic crisis. Geithner’s
nomination won prompt confirmation by the Senate, and he was sworn in on January 26, 2009. Geithner
previously had served as undersecretary of the treasury (1999–2001) in the Bill Clinton administration and later as

director of the Federal Reserve Bank of New York (2003–2008). In the latter capacity, he worked with then
Treasury secretary Henry Paulson and Federal Reserve chairman Ben Bernanke in 2007–2008 on managing the
early stages of what would become the global financial crisis.
In February 2009, barely two weeks after being sworn in, Treasury Secretary Timothy Geithner introduced a
financial stability plan aimed at unfreezing U.S. credit markets. Geithner had been involved in the financial crisis as
president of the Federal Reserve Bank of New York. (Win McNamee/Getty Images)
Timothy Franz Geithner was born on August 18, 1961, in New York City. As the son of an international
development expert for the Ford Foundation, he was raised in the United States, Asia, and Africa. After graduating
from the International High School in Bangkok, Thailand, he attended Dartmouth College, where he was a double
major in government and Asian studies, with a concentration in Chinese. After graduating from Dartmouth in 1983,
Geithner attended the School of Advanced International Studies at Johns Hopkins University and earned a
master’s degree in international economics and Asian studies in 1985. In the course of his academic career, he
studied both Chinese and Japanese.
Before joining the Treasury Department, Geithner joined Kissinger and Associates, a leading policy consultancy in
Washington, D.C., as an Asia expert and research assistant. In 1988, he took a position in the International Affairs
division of the Department of the Treasury, serving as an attaché at the American Embassy in Tokyo and
witnessing firsthand the onset of a decade of economic stagnation in Japan. Geithner rose to assistant secretary
and then under secretary for International Affairs (1997–2001) in the Clinton administration, serving under
Treasury secretaries Robert Rubin and Lawrence H. Summers, both of whom have been identified as mentors.
Rising through the ranks of the International Affairs division, Geithner was at the heart of U.S. financial policy
making in the bailout of Mexico (1995), the Asian financial crisis of 1997–1998 (Thailand, Korea, and Indonesia),
and the 1998–1999 currency crisis in Brazil. In 2001, with the end of the Clinton administration, Geithner moved to
the International Monetary Fund, where he served as director of policy development and review. In November
2003, he was named president and chief executive of the Federal Reserve Bank of New York, serving as vice
chairman and a permanent member of the Federal Open Market Committee, which formulates the nation’s

monetary policy.
As the credit crisis broke out in 2007, Geithner found himself at the epicenter of the rapidly worsening financial
turmoil. Along with Treasury Secretary Paulson and Federal Reserve chairman Bernanke, he agreed to handle the
crisis at the investment bank Bear Stearns by providing emergency funding and then a $30 billion credit line to
enable a rescue takeover by JPMorgan Chase. In September 2008, Geithner and Paulson attempted to make a
stand against any additional government bailouts by allowing the troubled investment banking firm Lehman
Brothers to file for bankruptcy. The failure of Lehman had a tsunami-like effect on the insurance-reinsurance giant
AIG (American International Group) and the brokerage firm Merrill Lynch, among others, because of their heavy
exposure in derivative markets. With fear spreading that other major firms—and much of the U.S. financial sector
in general—were at risk, global capital markets began to seize up. And with the collapse of the global financial
system considered a very real possibility, Paulson, Geithner, and Bernanke pushed through a buyout of insurance
giant AIG with taxpayer money and the emergency sale of Merrill Lynch to Bank of America. Timothy Ryan, chief
executive of the Securities Industry and Financial Markets Association, echoed the observation of many that
Geithner was “one of a core group of government executives who’s been part of every decision [in the current
crisis].”
Immediately after taking office in January 2009, President Obama officially nominated Geithner as the new
secretary of Treasury, declaring, “he will start his first day on the job with a unique insight into the failures of
today’s markets and a clear vision of the steps we must take to revive them.” Geithner’s vision clearly echoed the
reformist approach declared by candidate Obama in March 2007: “[W]e need to regulate institutions for what they
do, not what they are.” Speaking to the Economic Club of New York in June of that year, Geithner stated that
regulators needed to make it “more difficult for institutions [such as investment banks, hedge funds, and private
equity firms] with little capital and little supervision to underwrite mortgages.” As Treasury secretary, Geithner has
promoted increased supervision and examination of such entities, as well as other banklike regulations, such as
capital requirements, liquidity requirements, and leverage limits in exchange for government capital infusions to
investment companies amounting to almost $2 trillion. In early 2009, he directed the allocation of the second
tranche of money, totaling $350 billion, from the $700 billion bank bailout passed in October 2008 and
administered under the Troubled Asset Relief Program, and addressed such other major issues as government
support for the U.S. automobile industry, executive bonuses in financial companies, bolstering the mortgage and
housing industries, tax policy, and foreign trade.
Frank L. Winfrey
 
See also:  Federal Reserve System;  Recession and Financial Crisis (2007-);  Stimulus
Package, U.S. (2009);  Troubled Asset Relief Program (2008-). 
Further Reading
Guha, Krishna, and Gillian Tett.  “Man in the News: Timothy Geithner.” Financial Times, March 21, 2008. 
Kuttner, Robert.  “Meet the Next Treasury Secretary.” American Prospect, September 22, 2008. 
Labaton, Stephen.  “The New Team: Timothy F. Geithner.” New York Times, November 7, 2008. 
Landler, Mark, and Jackie Calmes.  “Geithner, Rescue-Team Veteran, Has Head Start in Seizing Reins.” New York
Times, November 24, 2008. 
“Timothy F. Geithner.” New York Times, March 24, 2009. 

 
General Motors
 
The world’s largest automobile manufacturing corporation for most of the twentieth century, General Motors (GM)
was also a leading innovator in the engineering and marketing of motor vehicles for much of its history. For years
it dominated the global automobile industry, building up to half of all passenger cars in the world shortly after
World War II. But with the rise of European and Japanese auto manufacturing beginning in the 1950s, GM saw its
market share erode significantly, despite aggressive expansion into Europe and Asia. At home in the United
States, just one in five cars sold was built by GM by the mid-2000s. The decline left the Detroit-based firm
especially vulnerable to the global financial crisis and recession of the late 2000s, forcing GM to close plants and
furlough workers, plead for billions of dollars in government loans, and ultimately place itself under the protection
of U.S. bankruptcy laws in 2009.
A General Motors dealership in Creedmoor, North Carolina, survived a major cutback in May 2009. The
automaker announced that 1,100 dealerships would be dropped from its retail network—with more cuts to come.
GM filed for bankruptcy protection on June 1. (Bloomberg/Getty Images)
 Origins and Rise
Founded in 1908 by horse-carriage manufacturer William Durant as a holding company for Buick, General Motors
expanded through the 1910s and 1920s, acquiring or starting a number of other motor vehicle manufacturing
companies in the United States and Europe, including Chevrolet, Oldsmobile, Pontiac, Cadillac, Opel (Germany),
Vauxhall (Great Britain), and companies that would later be incorporated into GMC, GM’s truck and bus

manufacturing division.
Under its visionary chief, Alfred P. Sloan, General Motors developed a number of innovative strategies for
marketing automobiles in the first half of the twentieth century. First, it created the model year, upgrading its cars
annually to spur consumers to replace their automobiles before they broke down. Second, with its many divisions,
GM could offer a range of automobiles for every budget, so that as customers’ incomes rose and they upgraded
their vehicles, they would remain GM customers. The company was often the first to introduce attractive amenities
and moved early into credit financing, offering potential customers a way to make what for most was the second
most expensive purchase (after a house) of their lives. Such innovation helped GM surpass rival Ford in the
1920s to become the largest automobile manufacturer in the United States and the world.
General Motors also became one of the most vertically integrated companies in American manufacturing, having
developed a number of subsidiaries to provide parts, such as ACDelco, and financing, through the General Motors
Acceptance Corporation (GMAC). GM also branched out into non-automobile-related businesses, most notably its
Frigidaire line of household appliances.
After retooling for defense during World War II, GM continued its upward trajectory in the early postwar era. After
briefly resisting unionization in the late 1930s, the company came to an understanding with the United Automobile
Workers (UAW) following a long and costly strike in 1945–1946. In exchange for giving up its demand to have a
seat on GM’s board—and a say in the way the company was run—the UAW won some of the highest wages ever
earned for unskilled manufacturing workers along with generous health and pension packages. With other
American automobile manufacturers forced to meet the GM/UAW terms—and with little competition from foreign
carmakers—GM could afford such largesse. By the mid-1950s, it had the largest private workforce in the world,
and its U.S. market share peaked at more than 50 percent.
 Gradual Decline
While the company faced setbacks in the 1960s and early 1970s—including a devastating exposé on the safety of
one its most popular vehicles and rising environmental protests about its failure to engineer less polluting cars—
GM continued to thrive. Its first real stumble came with the oil crises of the 1970s, when political upheaval in the
Middle East—the world’s largest oil exporting region—sent crude prices skyrocketing from less than $4 a barrel in
1973 to more than $40 a barrel in 1980 (from $19 to $100 in 2008 dollars). Like other American automobile
manufacturers, GM emphasized comfort and horsepower over fuel economy, leaving itself vulnerable to high
gasoline prices. Making things worse, both European and Japanese manufacturers, having fully recovered from
World War II, aggressively moved into the U.S. market with their fuel-efficient and reliable small vehicles.
While GM responded with small cars of its own—until plummeting oil prices in the 1980s and 1990s led it to
reemphasize big cars, pickups, and a whole new type of large vehicle, the sports utility vehicle (SUV)—it failed to
reverse its decline in market share. American customers increasingly came to see foreign cars, especially
Japanese ones, as better built and better styled, even as those same foreign manufacturers took a page from
GM’s playbook and began introducing a line of cars to fit every budget.
Aside from increased foreign competition, GM’s problems were also of its own making. GM’s corporate structure—
with each division largely running its own affairs—created independent fiefdoms that resisted efforts by corporate
management to streamline the company in the face of growing competition; divisions resisted consolidating their
operations and cannibalized each other’s sales. The UAW also contributed to GM’s decline, opposing changes in
work rules that would allow assembly lines to work more efficiently, and threatening—or occasionally calling—
strikes to resist cutbacks in wages or benefits, even as Japanese and European manufacturers opened nonunion
plants in the United States that paid their workers less.
Despite such problems, GM continued to do well, aided by a booming economy and low gas prices from the mid-
1980s through the late 1990s. While its U.S. market share shrank, GM was successful in Europe and aggressively
moved into emerging markets, particularly in East Asia.

By the early 2000s, a new problem emerged. With a growing population of retirees eligible for generous pension
and health care benefits, GM found itself saddled with enormous financial obligations. Combined with its declining
U.S. market share, such costs devastated the corporate bottom line, as GM posted a record $10.6 billion in losses
for 2005. GM responded by divesting its stakes in several foreign auto firms, reducing its annual dividends, and
selling off much of its GMAC financing arm. Meanwhile, for some time, GM had been reducing its manufacturing
capacity and workforce, especially in the United States. While GM employed nearly 900,000 workers worldwide in
1979, its peak year, the payroll had shrunk to about 300,000 workers in the mid-2000s.
 Financial Crisis, Bailout, and Bankruptcy
In short, GM was in a weak position when a series of crises hit the entire U.S. auto industry in 2007 and 2008.
The new challenges included a sudden spike in oil prices—reaching a record high of nearly $150 a barrel in the
summer of 2008—which did the most damage to companies with big inventories of larger, less fuel-efficient
vehicles, such as GM. But even when gas prices came down later in the year, GM faced two new problems: the
economic recession and the global financial crisis. With consumer demand undermined and credit hard to come
by, U.S. new light-vehicle sales plunged from more than 16 million in 2006 to fewer than 11 million in 2008.
While every automaker, both domestic and foreign, suffered in the face of these problems, GM’s sales declines
outpaced those of the industry overall. In 2007, GM lost its standing as the world’s largest auto manufacturer—to
Toyota—for the first time since 1926. Worse, collapsing sales and continued high costs produced record losses for
GM—$38.7 billion in 2007 and $30.9 billion in 2008. Meanwhile, the company’s stock price plummeted from a
high of more than $80 a share in the late 1990s to less than $3 a share in November 2008, representing a more
than sixty-year low. By May 2009, the share price would fall below $1.
As the financial crisis deepened in late fall 2008—and as the George W. Bush administration established the $700
billion Troubled Asset Relief Program for ailing financial institutions—General Motors, along with equally
distressed Chrysler, announced that it was almost out of cash and would be forced into bankruptcy in 2009 unless
it received aid from the government. Despite some popular and media outrage, especially after auto executives
flew to Washington, D.C., on private jets to plea for taxpayer aid, the outgoing Bush administration—supported by
president-elect Barack Obama—agreed to provide $9.4 billion in loans to GM.
But there were strings attached, including a provision that the company come up with a restructuring plan by
March 31, 2009, that would allow it to repay the debt and return to profitability. As the deadline loomed, the
Obama administration pressured GM chief executive officer Rick Wagoner to resign and then offered the company
a further sixty days to come up with a plan. Despite substantial cost-cutting efforts, the downturn in the
automobile industry proved too strong and GM was unable to avoid collapse. On June 1, it filed for federal
bankruptcy protection under Chapter 11, becoming the fourth-largest U.S. corporation in history to do so.
With the swift bankruptcy organization of Chrysler as a model—the smaller auto manufacturer had filed at the end
of April and emerged from bankruptcy in early June—GM and the Obama administration worked to achieve what
they called a “surgical bankruptcy” for GM, hoping that a swift emergence from bankruptcy would reassure
potential automobile buyers that GM would continue as a going concern that could stand by the warranties on its
automobiles.
During the course of the bankruptcy proceedings, GM was split into two entities: the General Motors Company,
which purchased most of the old GM’s assets and emerged from bankruptcy on July 10, an amazingly fast forty
days after filing; and the Motor Vehicles Company, which remained in bankruptcy in order to settle the liabilities of
the old GM with bondholders and others.
By 2010, it appeared that the bailout and restructuring of GM had succeeded in returning the car manufacturing
giant to profitability. After losing $21 billion in 2009, it posted a profit of $4.7 billion in 2010. In early 2011, it
announced plans to spend $17 billion on upgrading various plants around the United States, thereby creating or
preserving more than 4,000 jobs.

James Ciment
 
See also:  Chrysler;  Manufacturing;  Troubled Asset Relief Program (2008-). 
Further Reading
Chandler, Alfred D., ed. Managerial Innovation at General Motors. New York: Arno, 1979. 
Farber, David. Sloan Rules: Alfred P. Sloan and the Triumph of General Motors. Chicago: University of Chicago
Press, 2002. 
General Motors:  www.gm.com
Langworth, Richard M., and editors of Consumer Guide. GM: 100 Years. Lincolnwood, IL: Publications International, 2008. 
German Historical School
 
Emerging in the nineteenth century, the German historical school was a school of economic thinking that
emphasized empiricism rather than theory to explain how economies operate. German historical school thinkers,
who utilized a cross-disciplinary approach in their research, argued that economic behavior and laws grow out of
specific historical, social, and institutional contexts. Overshadowed by the more theoretical Austrian school and the
classical economics of Britain, the German historical school had little influence on subsequent economic theory,
though its approach was widely taken up by historians and sociologists.
The origins of the German historical school are based in German Romanticism, which questioned abstract
theorizing, and the philosophy of GeorgW.F. Hegel, who emphasized the historicity of philosophical thought and
discounted theoretical systems that applied in all places and times. Among the first practitioners of what would
later be known as the German historical school was Wilhelm Roscher, a disciple of Hegel’s. Producing his most
influential work in the mid-nineteenth century, Roscher argued that one can understand economies only by closely
examining historical and sociological evidence. Following Hegelian ideas about the cyclical nature of history,
Roscher argued that economies also go through stages, which he likened to the human life cycle: youth,
adulthood, and old age, and then, as with a new generation, back to youth again.
Roscher and other early German historicists were, by definition, modest in their claims and aspirations. Since they
did not believe in universal economic laws, they were careful not to apply historical lessons to current economic
situations, and they offered little in the way of advice to policy makers. Not so the next generation, or the so-
called young historical school. Led by Gustav von Schmoller, this new generation of historical thinkers maintained
that economics is by nature a normative, or prescriptive, discipline, drawing general rules about economic
behavior from historical examples. Unlike Roscher, Schmoller and other young historical school thinkers were not
shy about offering advice to economic policy makers and opened the Verein für Sozialpolitik (Association for
Social Politics) in 1872 to do just that.
Despite the establishment of the Verein, most practitioners did not define themselves as members of a distinct
school of economic thinking until forced to defend themselves when attacked by Carl Menger and his Austrian
school followers. Contrary to the Germans, Menger argued for a deductive, rather than inductive, approach to
economic theorizing, developing abstract principles that applied in various historical and national contexts. Known

as the Methodenstreit, or Methods Debate, the late-nineteenth-century struggle of ideas ended with most
economic departments at German universities falling under the influence of the Austrians.
Many of the German historicists then immigrated to the United States, where they found a conducive atmosphere
for their kind of thinking in a country that emphasized a pragmatic and problem-solving approach—rather than
theoretical approach—to the social sciences. Out of the German historical school and its related English historical
school grew the American institutional school of the early twentieth century, with its emphasis on empirical
research and its belief that economic behavior is shaped by specific historical situations.
James Ciment
 
See also:  Austrian School;  Institutional Economics. 
Further Reading
Roscher, Wilhelm. Principles of Political Economy. New York: Arno, 1972. 
Schmoller, Gustav von.  “The Idea of Justice in Political Economy.” In Annals of the American Academy of Political and
Social Science, vol. 4(1893–1894). 
 
Germany
 
The modern German economy displays a variant of capitalism that is deeply shaped by its historical experiences
of the twentieth century, and it is distinct from American-style capitalism. The three most significant events of
Germany’s twentieth-century economy are dramatic examples of the ways in which an advanced industrial power
descended into the deep recessions of the business cycle. These events were World War I and the hyperinflation
of the 1920s; World War II and the accompanying hidden inflation; and more recently, reunification, federal
deficits, and the recession in 1992. The events were accompanied by government deficits that led to harmful
inflation and currency reform, and their effect was to foster a cautious attitude toward monetary and fiscal policy.
Today in Germany, there are institutional as well as historical impediments to running large federal deficits.
However, the financial crisis of 2008–2009 has proven serious enough that these restrictions are gradually falling
by the wayside.
 National Economy Before 1945
With its unification in 1871, Germany became a leader in the global market and by 1914 had become the world’s
second-largest industrial economy after the United States. During the second industrial revolution of the late
nineteenth century in the steel, engineering, machinery, electrical, and chemical industries, Germany laid the
institutional foundations for a coordinated market economy. With its large social insurance system, strong unions

and trade associations, large and influential cartels, and the production of high-quality specialized products,
Germany developed a style of capitalism that differed from that of the United States.
The next three decades, from 1918 to 1948, were traumatic ones that deeply shaped the attitude of Germany’s
post–World War II leaders toward economic policy. Massive government spending during World War I, financed
primarily by the printing of money, and political conflicts after 1918, led to hyperinflation in Germany in 1923 and
the need for a new currency in 1924. The Great Depression, beginning in Germany in 1929, reached its height in
a wave of bank failures across central Europe in 1931. Only the suppression of wages and a huge fiscal stimulus
under Germany’s National Socialist (Nazi) regime, through public works and above all military spending, led
Germany to a quicker recovery than the United States or Great Britain. Yet this military spending before and
during World War II led to a new round of inflation, currency depreciation, black markets, and ultimately to another
currency reform in 1948.
Residents of Berlin line up at the state loan office in 1924. The introduction of the gold-backed Reichsmark that
year ended Germany’s post–World War I inflation crisis, but the savings of a whole generation were lost. (The
Granger Collection, New York)
 Social Market Economy
After World War II, the West German state, under the guidance of Economics Minister Ludwig Erhard, developed
the theory of the social market economy (SME) as a way to promote stability and growth. This aimed to guarantee
long-term economic growth by establishing proper ground rules for competition and industrial organization and by
providing state assistance in infrastructure investment, redistribution of wealth, and social welfare. The SME
ushered in the Wirtschaftswunder, a period of impressive economic growth during the 1950s and 1960s that
reestablished West Germany as a capital-intensive, export-oriented economy that sent automobiles, high-end
machinery, and chemical products around the globe.
The institutions and characteristics of the SME—strong unions, cooperation between employers and employees,
the representation of workers in management, rigid labor markets, vocational education, a direct and long-term
relationship between banks and firms, and a tendency toward diversified and flexible quality production—
distinguish Germany’s economy from that of the United States. The latter has historically been characterized by
weaker unions, more flexible labor markets, less cooperation between workers and management, banks that look

more for indirect and short-term relationships with firms, and mass production that, at least until the rise of flexible
manufacture and mass customization, adapted less quickly to changes in market demand.
German leaders believed that the institutions of the SME would enable the market to correct itself during swings in
the business cycle. The state would stabilize production over the long term, but would not provide economic
stimuli in the short term through tax breaks and public works projects. Direct involvement in the business cycle
was to be more the purview of monetary rather than fiscal policy. The two experiences with inflation and currency
reform in 1924 and 1948, both of which resulted from large fiscal deficits and devastated the savings of many
Germans, led Erhard and the German central bank to emphasize currency stability and, whenever possible, to
avoid deficit spending to manage the business cycle. Germany’s earliest post–World War II economists saw the
SME as an alternative to the Keynesian approach of using fiscal and budgetary policy to influence business
cycles.
This outlook changed in 1967 in the face of West Germany’s first real post–World War II recession, when the
federal government changed course in Keynesian fashion and invested in infrastructure and unemployment
benefits to combat the downturn. Thus, a new belief in managing the business cycle persisted throughout the
1970s, spearheaded by Economics Minister Karl Schiller and bolstered by his relative success in overcoming the
1967 recession. However, the 1970s proved to be a decade of slower growth, higher unemployment, higher
inflation, and lower profits. In 1982, after a decade of “stagflation,” low productivity growth, two oil crises, and a
burgeoning federal debt, the new government under Helmut Kohl moved away from Keynesian demand
management, refocused on stabilizing production, and called for a reduction in the federal debt. This return to the
core tenets of the SME remained the guiding principle behind economic policy throughout the 1980s.
 Reunification and Lessons of the Past
In the early 1990s, the reunification of East with West Germany presented a unique situation that required deficit
spending on a large scale. In 1991 and 1992, in hope of raising the productivity of former East German workers,
the newly united Germany ran large federal deficits in transfer payments and infrastructural investment in the
former East. High expectations for a quick convergence of East Germany’s economy with that of West Germany
were not met. Instead, overinvestment in the construction sector, rising prices, high interest rates, the recession in
1992, and the threat all of this posed to monetary unification with the European Union (EU) left a bad taste for
fiscal stimulus among many German economic thinkers.
What lessons were learned from Germany’s deviation into using fiscal stimuli and undertaking demand
management during the late 1960s and 1970s and its reunification experience? By and large, the majority of
German economists in the 1990s and early 2000s concluded that demand management was ineffective and even
harmful. In commenting on Germany’s fiscal policies in the 1970s, Peer Steinbrück, Germany’s finance minister
from 2005 to the present, remarked that “government debt rose, and the downturn came anyway.” The fiscal
deficits of the 1970s and the 1990s left Germany with a large federal debt, which rose from about 20 percent of
gross domestic product (GDP) in 1970 to 60 percent in 2008, or 1.5 trillion euros. And although Germany ran a
budget surplus in 2007, it was the first one in nearly two decades; until 2006 there had been an erosion of
confidence in the government’s ability to run a balanced budget during periods of economic growth. Many German
economists continue to maintain that long-term growth requires long-range planning to cultivate production, not
short-term stimulus to create demand.
In addition, Germany’s membership in the EU places institutional constraints on the size of its deficits and its
ability to pursue short-term fiscal stimuli. Under the Stability and Growth Pact of 1997, the EU penalizes its
member states for deficits in excess of 3 percent of GDP. While this has not been strictly enforced, it illustrates
how much the EU and the European Central Bank are concerned that national debts could grow large enough to
negatively impact future growth by crowding out private investment.
 Financial Crisis of 2008–2009

Throughout the first phase of the financial crisis, during October and November 2008, German chancellor Angela
Merkel and Finance Minister Steinbrück showed reluctance toward any large tax breaks or public spending,
believing that Germany’s traditional variety of capitalism was less vulnerable to a recession than that of either the
United States or Great Britain. Both resisted calls for a Europe-wide fiscal stimulus package. This reflected fears in
Germany that such a package would endanger the EU’s Stability and Growth Pact, and that the potential for a
large deficit and high interest rates would outweigh any immediate benefits.
Yet by January 2009, the financial and economic crisis had become severe and global enough for German leaders
to act. As the world’s leading exporter, Germany is highly dependent on the rest of the world—and Europe in
particular—to buy its products: exports account for roughly one-third of German GDP. The vulnerability of
Germany’s banking sector, alongside predictions of negative growth and rising unemployment in Germany and
across Europe, led Merkel and Steinbrück to pass Germany’s largest postwar stimulus package of public
investment and tax breaks. While many criticize this as too small, it nevertheless represents a break with
Germany’s post-1945 reluctance to engage in Keynesian demand management.
In 2011, Germany faced a new crisis in the form of sovereign debt crises in various Eurozone members, most
notably, Greece, but also Portugal, Ireland, and, as of late 2011, potentially Italy and Spain as well. As the
wealthiest country in Europe and the leading contributor to the European Central Bank, which backs the Euro,
Germany had a lot at stake in making sure the common currency remained stable. But German voters resented
the idea of bailing out and forgiving the debt of what they saw as more profligate Eurozone countries. This led
Merkel’s government to hesitate in giving crucial support to a plan to shore up the finances of weaker Eurozone
members. At the same time, Germany remained committed to saving the Euro and so agreed to contribute to a
fund worth hundreds of billions of Euros aimed at guaranteeing the sovereign debt of weaker Eurozone member
countries and preventing their default. The latter outcome would have uncertain but potentially catastrophic
ramifications for the European and world economies.
Stephen Gross
 
See also:  European Central Bank;  France;  Italy;  United Kingdom. 
Further Reading
Abelshauser, Werner. The Dynamics of German Industry: Germany’s Path Towards the New Economy and the American
Challenge. Oxford, UK: Berghahn, 2005. 
Braun, Hans-Joachim. The German Economy in the Twentieth Century: The German Reich and the Federal Republic. New
York: Routledge, 1990. 
Eichengreen, Barry. The European Economy Since 1945. Princeton, NJ: Princeton University Press, 2007. 
Hall, Peter A., and David Soskice, eds. Varieties of Capitalism: The Institutional Foundations of Comparative
Advantage. New York: Oxford University Press, 2001. 
Siebert, Horst. The German Economy: Beyond the Social Market. Princeton, NJ: Princeton University Press, 2005. 
Glass-Steagall Act (1933)
 

An early component of Franklin Roosevelt’s New Deal agenda, the Glass-Steagall Act—officially the Banking Act
of 1933—was legislation aimed at addressing failures in the nation’s banking system that many economists and
policy makers of the day believed contributed to the financial crisis of the early 1930s. It was named after its
authors, Senator Carter Glass (D-VA), a former secretary of the treasury, and House Committee on Banking and
Currency chairman Henry Steagall (D-AL).
A complex bill, the measure included provisions that established the Federal Deposit Insurance Corporation
(FDIC), creating deposit insurance for bank deposits, and separated commercial banking from riskier brokerage
and investment bank services, as well as insurance underwriting. The latter provision was largely overturned with
passage of the Gramm-Leach-Bliley Act of 1999, a repeal that many contemporary economists say contributed to
the financial crisis of the late 2000s. (The Glass-Steagall Act of 1933 should not be confused with a similarly
named act of 1932 expanding the powers of the Federal Reserve, or “Fed.”)
 Origins
The prelude to the act was the great 1920s bull market on Wall Street. Between 1927 and 1929, the New York
Stock Exchange experienced one of the greatest percentage run-ups in securities values in its history, with
average share prices climbing by 50 percent and such high-flying stocks as RCA seeing their values go up by 600
percent. Overall, the Dow Jones Industrial Average climbed from about 125 in 1925 to over 380 at the peak of the
market shortly before the crash of October 1929.
While there had been solid gains in productivity, production, and profits during much of the 1920s, the economy
weakened after 1927, making the spectacular gains of the late 1920s largely a speculative bubble. Pumping air
into the bubble was the practice of buying equities on margin. Investors could take out loans from brokers and
investment bankers to purchase stock, putting as little as 10 percent of their own money down. Brokers offered
such loans because stock prices continued to rise, meaning that investors could pay back the hefty interest rates
and fees with gains they made in the value of the shares they sold.
But when prices fell in the crash of 1929, a vicious cycle ensued. First, investors could not meet margin calls. For
example, say an investor put down $100 on 100 shares valued at $10 each, for a total stock purchase price of
$1,000, of which $900 was paid for with a loan from a broker. If the value of the shares fell by more than 10
percent, then the investor now owed more than the stock was worth, triggering a margin call by the lender. As
many investors found themselves in this position, brokers and investment bankers, in turn, could not pay back the
loans they had taken out from other financial institutions to lend out to investors.
According to many economic historians, the margin call contagion might have been confined to the relatively small
pool of stock market investors—less than one in ten Americans had money invested in securities in 1929—had it
not been for the fact that many commercial banks had gone into the brokerage and investment banking
businesses. This meant that the money of ordinary depositors had disappeared in the great bear market that
followed the crash of 1929. At first, only the largest institutions in New York were affected. As those banks began
to tighten credit, however, the crisis spread to large and small banks across the country. Panicky depositors began
to pull out their money, forcing more than 2,000 financial institutions into bankruptcy in 1929 and 1930 together.
The mass withdrawal of funds, along with the bankruptcies, froze the nation’s credit markets, drying up the funds
available for investment. This inevitably led to dramatic falls in production, corporate revenues, output, and
employment. The deepening economic gloom only created more fear, which prompted more depositors to
withdraw their money from banks. By early 1933, as the country waited for president-elect Roosevelt to take office
—in those days, the inauguration was held in March—the nation’s banking system was on the verge of collapse,
a situation made worse by the Fed’s decision to hike the interest rate it charged on loans to member banks.
Upon taking office in early March, Roosevelt promptly declared a bank holiday, closing the nation’s lending
institutions for several days. During the hiatus, Congress passed the Emergency Banking Relief Act, which
allowed solvent banks to reopen with a Treasury Department license, while nonsolvent banks were reorganized

under the department’s aegis. These measures halted the panic but did nothing to address the underlying
problems that had led to the crisis in the first place.
That’s where the Glass-Steagall Act came in. Signed into law on June 16, 1933, the law established the FDIC,
which provided federal insurance on bank deposits up to a specified amount ($2,500 at the time, raised to
$100,000 in 1980 and to $250,000 in 2008—the latter temporarily, to the end of 2013). The FDIC was intended to
reassure depositors that they would not lose their money if a commercial bank should fail.
In the decades since, the FDIC has been one of the most successful agencies created by the federal government.
Not a single cent of insured deposits has ever been lost, and depositors find almost no delay in accessing their
money. This was evident during the financial crisis of 2008–2009, when such major institutions as the California-
based IndyMac Bank and Washington State–based Washington Mutual (WaMu), among other smaller ones, failed.
In both cases, the FDIC put the banks in receivership, reorganizing IndyMac as OneWest Bank and selling off
Washington Mutual’s commercial banking operations to the holding company JPMorgan Chase. Note that in the
case of IndyMac Bank, depositors with deposits above the insurance limit did lose some of their deposits. Since
Washington Mutual’s commercial banking operations were sold to JPMorgan Chase, rather than reorganized by
the FDIC, depositors with deposits above the insurance limit did not lose.
The other key provision of the Glass-Steagall Act prohibited commercial banks from engaging in investment bank
and brokerage services. Although there were a number of reasons for this, the main one was that investment
banking and brokerage services are inherently speculative activities, as they entail the underwriting of corporate
securities whose value can rise and fall precipitously in a short period of time, thereby putting bank depositors’
money at risk. Supporters of the Glass-Steagall Act argued that it was necessary to prevent a repeat of the crisis
that had hit the nation’s banks following the crash of 1929. Moreover, if the federal government was going to
insure depositors’ money, then it had an interest in making sure those deposits were not put at inordinate risk,
thereby requiring taxpayer money to cover for speculative losses.
 Repeal of Provisions
The Glass-Steagall Act remained in effect and largely untouched through the 1970s. By that time, much of the
financial industry was chafing under the restrictions of the legislation, a discontent that received a sympathetic ear
in an increasingly conservative political environment, where government regulation was seen as hampering private
industry. In the early 1980s, two pieces of federal legislation—the Depository Institutions Deregulation and
Monetary Control Act of 1980 and the Garn-St. Germain Depository Institutions Act of 1982—began to whittle
away at some of the provisions of the act. The former removed the Fed’s power to regulate interest rates at
savings and loans, and the latter removed many other regulations on savings and loans.
But the major repeal legislation was the Gramm-Leach-Bliley Act of 1999 (officially the Financial Services
Modernization Act)—named after its sponsors, Senator Phil Gramm (R-TX), Representative Jim Leach (R-IA), and
Representative Thomas J. Bliley, Jr., (R-VA)—which allowed for the formation of bank holding companies that
could simultaneously engage in commercial banking, investment banking, brokerage services, and insurance. The
reasons for the repeal were both immediate and long term. As to the former, it allowed for the merger of Citicorp,
a bank holding company, and Travelers Group, an insurance company that owned the brokerage company Smith
Barney, to form the conglomerate Citigroup, which would offer commercial and investment banking services, as
well as brokerage and insurance businesses.
The longer-term factors behind the repeal were based on the idea that, with financial information made more
accessible by innovations in technology and communications, investment banking and brokerage services were not
nearly as risky as they once had been. Moreover, conservative economic thought held that the market could sort
out the risk better than blanket government restrictions, as depositors and investors would stay away from
institutions with unsound reputations. Finally, it was argued, allowing banks to diversify actually moderated risk.
The repeal of restrictions on commercial banking led to mergers throughout the financial industry and seemed to

be working as the new holding companies became enormously profitable in the early and mid-2000s. With the
financial crisis of 2008, however, many economists and policy makers began to rethink Gramm-Leach-Bliley. The
fact that Citigroup received some $50 billion in Troubled Asset Relief Program (TARP) money in 2008—the
largest amount given to a single institution under the aegis of the program (insurance giant AIG received more,
but only $40 billion of it came from TARP)—led many commentators to conclude that some financial institutions
had grown “too big to fail.” In other words, by allowing giant bank holding companies to engage in both
commercial banking and riskier investment banking and brokerage services, the repeal had created institutions
whose failure so threatened the global financial system that the government would inevitably be forced to rescue
them should they approach insolvency. Despite such reservations, the Barack Obama administration and
Democratic-controlled Congress were silent on the prospect of reinstituting the old Glass-Steagall restrictions on
bank holding company activities.
James Ciment
 
See also:  Banks, Commercial;  Banks, Investment;  Great Depression (1929-1933);  New Deal; 
Regulation, Financial. 
Further Reading
Benston, George J. The Separation of Commercial and Investment Banking: The Glass-Steagall Act Revisited and
Reconsidered. New York: Oxford University Press, 1999. 
Kaufman, Henry. The Road to Financial Reformation: Warnings, Consequences, Reforms. Hoboken, NJ: John Wiley and
Sons, 2009. 
Peláez, Carlos M., and Carlos A. Peláez. Regulation of Banks and Finance: Theory and Policy After the Credit Crisis. New
York: Palgrave Macmillan, 2009. 
 
Goldman Sachs
 
Until its restructuring as a bank holding company in late 2008, Goldman Sachs—usually referred to in the industry
as simply Goldman—was one of the oldest, wealthiest, and most influential investment banks in the United States.
Many of its top executives have served in high-level government economic positions in both Republican and
Democratic administrations, including Treasury secretaries Robert Rubin of the Bill Clinton administration and
Henry Paulson of the George W. Bush administration.
Founded just after the Civil War and based in New York City, the company offers a wide variety of financial
services, including the facilitation of corporate mergers and acquisitions, asset management, financial underwriting,
securities trading, private equity deals, and the marketing of U.S. Treasury securities.

While not as hard hit as other financial institutions by the financial crisis and recession of 2007–2009, Goldman
nevertheless found itself in need of billions of dollars in federal bailout money under the Troubled Asset Relief
Program (TARP). The crisis also prompted the decision to reorganize as a bank holding company, which would
allow it to open a commercial bank, giving it access to borrowing at the discount window and in the fed funds
market, but also subjecting it to increased regulation and government oversight.
The investment banking and securities giant Goldman Sachs opened its New World Headquarters in New York’s
Battery Park City in 2010. Despite repaying government TARP money, the company faced criticism over the issue
of executive bonuses. (Bloomberg/Getty Images)
Goldman was founded by German immigrant Marcus Goldman in 1869. When his son-in-law Samuel Sachs joined
the firm in 1882, the current name was adopted. Goldman Sachs was a pioneer in the private bond market and
was among the first Wall Street firms to conduct initial public offerings (IPOs) in the early twentieth century,
including those of Sears, Woolworth, and later Ford. Having set up a kind of early mutual fund in 1928, the
company was hard hit by the market crash of the following year.
The company survived the Great Depression and thrived in the bull market of the 1950s and 1960s and began
establishing offices in other financial centers, beginning with London in 1972. The company also branched out into
other businesses, including commodities trading and asset management in the 1980s. In 1999, the firm went
public, though it only offered up 12 percent of its equity in its first IPO. At the same time, the company made huge
profits managing the IPOs of other companies, particularly in the tech sector. Still, Goldman had its stumbles
along the way, most notably during the collapse of Penn Central railroad in 1970. Holding tens of millions of

dollars of the company’s bonds, Goldman was nearly driven into bankruptcy itself.
It looked in far better shape during the initial months of the subprime mortgage market collapse of 2007, having
gone “short” on mortgage-backed securities, meaning it bet on their market values going down, which they indeed
did, making Goldman some $4 billion in profit in the process. This was at a time when other investment banks and
financial institutions were teetering on the brink of insolvency because of their exposure to subprime mortgage-
backed securities.
While revenues were down significantly during the first three quarters of 2008, the firm was still posting a modest
profit, even as the financial crisis saw investment banks Lehman Brothers go bankrupt and Merrill Lynch bought
out at fire sale prices by Bank of America. By September, however, the contagion of investor fear finally hit
Goldman Sachs, especially after insurance and financial services giant American International Group (AIG) took
$85 billion in bailout money from the federal government. The company was still highly respected and in the black,
but investors nevertheless feared no stand-alone investment bank could thrive or even survive in the current
crisis. As its stock price went into freefall and its assets diminished, Goldman stepped up to receive some $10
billion in government bailout money under TARP.
At the same, the company decided to radically restructure. On September 21, 2008, it reorganized itself as a bank
holding company. A bank holding company can control one or more commercial banks or other bank holding
companies when it owns 25 percent or more of the other companies. As with any major change, there were
benefits and costs. On the plus side, Goldman could now enter commercial banking, allowing it access to a whole
new revenue stream and potentially vast assets. On the downside, at least as far as its stockholders, executives,
and traders were concerned, as a bank holding company Goldman would be subject to greatly increased
government scrutiny and regulation. The freewheeling investment banking days, in which the company could take
greater risks for greater profits, were now in the past. Goldman’s decision—along with a similar move by rival
Morgan Stanley—ended the era of stand-alone giant investment banking firms on Wall Street.
Still, investors retained their faith in Goldman’s legendary ability to make money in good times and bad; just after
their reorganization announcement, widely watched investor Warren Buffett put $5 billion in the firm. The company
raised an additional $5 billion in a stock offering.
After suffering a major loss in the last quarter of 2008, the company posted more than $1.6 billion in profit in the
first quarter of 2009, a result more related to rebounding markets and a lack of competition than to commercial
bank operations, which had yet to commence in earnest. In early 2009, the company also announced that it
planned to raise capital in the securities markets to allow it to pay back the federal bailout money before the end
of the year.
Goldman Sachs continued to profit as the financial markets and the economy as a whole recovered, posting $3.5
billion in profits in the first quarter of 2010. But troubles also continued to plague the firm, including outrage at its
continued bonuses to executives—some $5 billion for the first quarter of 2010—and, more seriously, a decision by
the Securities and Exchange Commission to charge the company with fraud in April. The regulators alleged that
Goldman, in league with a hedge fund, sold mortgage-related financial instruments to investors that it knew were
designed to fail. Goldman denied the allegations.  In July, Goldman agreed to pay a record fine of $550 million to
settle charges of security fraud brought by the SEC.
James Ciment
 
See also:  Banks, Investment;  Troubled Asset Relief Program (2008-). 
Further Reading
Ellis, Charles D. The Partnership: The Making of Goldman Sachs. New York: Penguin, 2008. 

Endlich, Lisa. Goldman Sachs: The Culture of Success. New York: A.A. Knopf, 1999. 
Taibbi, Matt.  “The Great American Bubble Machine.” Rolling Stone, 1082–1083, July 2009. 
Goodwin, Richard Murphy (1913–1996)
 
Richard Murphy Goodwin was an American economist and mathematician. He is best known for having developed
models of endogenous economic cycles, the Goodwin class struggle model, and Goodwin’s nonlinear accelerator.
Goodwin was born in Indiana in 1913. He received a Bachelor of Arts and a doctorate degree from Harvard
University, where he taught from 1942 until 1950. He taught next at Cambridge University (England) until 1979,
then at the University of Siena (Italy) until his retirement in 1984. He died on August 13, 1996, while vacationing in
Siena.
Goodwin’s class struggle model was first proposed in 1967. Drawing on the work of Michal Kalecki, “the Marxist
Keynes,” and on the math used to model the relationship between predators and prey in biology, Goodwin
constructed a nonlinear model of the cyclical relationship between wage share and employment. According to this
model, high levels of employment lead to wage inflation (through supply and demand), which increases workers’
wage share. This, in turn, reduces profits and demotivates future investment. Lowered investment reduces output,
thus reducing labor demand, employment, and wages. A reduction in employment and wages leads to increased
profits, investment, and employment, thereby completing the cycle, which begins again. The exogenous growth
components in Goodwin’s model are productivity growth and labor supply growth. The two classes of income
recipients (predators and prey, in essence) are profit-earning capitalists (business owners) and wage-earning
workers. A Phillips curve, described as an inverse relationship between the rate of unemployment and the rate of
inflation, determines the growth of wages. The Goodwin model proved popular and enduring both among
Keynesians and Marxists, and has had the benefit of functioning without fixed floors or ceilings, and without
exogenous shocks.
Another of Goodwin’s contributions to Keynesian economics is his nonlinear accelerator, first illustrated in a 1951
article. Accelerators are a key to the theory of investment (the change to capital stock over time). The Keynesian
answer to the question “What is investment?” is “Investment is what capitalists do.” Investors purchase stocks and
other items for their portfolios. Business owners invest in their companies by funding such areas as business
activity or expansion, workers’ wages, and equipment improvements and purchases. Keynesians tend to focus on
investment as a type of behavior.
In the accelerator theory of investment, investment behavior is seen as a response to changing conditions of
demand. For example, when there is greater demand for a product, a business will invest in accelerating
production in order to increase supply. In this case, as in the field of physics, the term “accelerate” means “to
change speed” (“decelerate” is a neologism coined by those who understood accelerate to mean “to increase
speed”). Thus, the accelerator theory of investment says that accelerating demand results in accelerating supply,
through the mechanism of investment.
Goodwin’s nonlinear accelerator consisted of an investment function, an accounting identity, and a consumption
function. One result of the nonlinear accelerator was that, once proved valid, it demonstrated that very little could
be done to the system to affect the timing of the business cycle. In other words, the severity of an economic boom
or bust had almost no relationship to how long it would last. In the July 1953 issue of Econometrica, a team of
economists confirmed this theory by computer.

Bill Kte’pi
 
See also:  Endogenous Growth Models;  Kalecki, Michal;  Keynesian Business Model;  Marxist
Cycle Model. 
Further Reading
Flaschel, Peter, and Michael Landesmann, eds. Mathematical Economics and the Dynamics of Capitalism: Research in
Honor of Richard M. Goodwin. New York: Routledge, 2008. 
Goodwin, Richard M. Chaotic Economic Dynamics. New York: Oxford University Press, 1990. 
Goodwin, Richard M., and Lionello F. Punzo. The Dynamics of a Capitalist Economy. New York: Westview, 1987. 
Landesmann, Michael, and R. Stehrer.  “Goodwin’s Structural Economic Dynamics: Modelling Schumpeterian and
Keynesian Insights.” Structural Change and Economic Dynamics 17:4 (2006): 501–526. 
Strotz, R.H., J.C. McAnulty, and J.B. Naines, Jr.,  “Goodwin’s Nonlinear Theory of the Business Cycle: An Electro-Analog
Solution.” Econometrica 21:3 (July 1953): 390–411. 
Government Accountability Office
 
The Government Accountability Office (GAO) advises Congress and the heads of executive branch agencies
about ways to make government more efficient, effective, ethical, equitable, and responsive. In recent years,
“about four out of five GAO recommendations” over a range of public policy trends, challenges, and opportunities
likely to affect the United States and the federal government have been implemented.
The GAO has played an important role in analyzing the major events in and causes of U.S. and international
economic downturns over the past eight decades. It has been especially active in analyzing the many facets of the
financial crisis of 2007–2008. In doing so, it provides critical data and insights to economists who investigate
business cycles.
The GAO was authorized by the Budget and Accounting Act of 1921. Originally called the General Accounting
Office, its name was changed in 2004 to better reflect its “accountability” functions and activities. The agency that
works for the U.S. Congress is independent and nonpartisan with a broad mandate to provide information and
recommendations to control of government expenditures, Originally the agency was charged with checking the
legality of government expenditures by auditing and reviewing fiscal records and vouchers. Today, financial audits
represent only about 15 percent of the agency’s workload.
The contemporary mission of the GAO is to support the Congress “to help improve the performance and ensure
the accountability of the federal government for the benefit of the American people” by “providing timely
information that is objective, fact-based, nonpartisan, non-ideological, fair, and balanced” over “a broad range of
financial and performance audits and program evaluations.” The GAO currently organizes its resources in support
of three broad external strategic goals: helping to address the challenges to the well-being and economic security
of the American people, U.S. national and homeland security efforts, and modernizing government to meet current
and emerging issues. The GAO’s basic work products are reports, testimonies, correspondence, and legal
decisions and opinions to provide “oversight, insight, and foresight.”

The GAO is headquartered in Washington, D.C., and has eleven field offices in major cities around the country. It
employs approximately 3,200 career civil servants whose knowledge, training, and skills cover a wide range of
academic, professional, and scientific disciplines. The comptroller general of the United States is the leader of the
GAO, and its staff is organized around thirteen research, audit, and evaluation teams that support the three
strategic goals. The teams are (1) Education, Workforce, and Income Security; (2) Financial Markets and
Community Investment; (3) Health Care; (4) Homeland Security and Justice; (5) Natural Resources and
Environment; (6) Physical Infrastructure; (7) Acquisition and Sourcing Management; (8) Defense Capabilities and
Management; (9) International Affairs and Trade; (10) Applied Research and Methods; (11) Financial Management
and Assurance; (12) Information Technology; and (13) Strategic Issues.
 GAO Foresight and Oversight
The GAO Strategic Plan (2007–2012) identified and described seven key themes that will affect the United States
over the next several decades: (1) safety and security threats requiring attention in a number of areas—terrorism,
violent crime, natural disasters, and infectious disease; (2) fiscal sustainability concerns as government spending
on social insurance programs create a potential tax gap and environmental sustainability concerns as economic
growth puts stress on air and water quality and induces climatic change; (3) economic growth and competitiveness
because the saving and investment behavior of U.S. citizens is inadequate to provide sufficient capital for
investment in research, development, and productivity enhancements from domestic sources; (4) global
interdependency that requires reducing impediments to the exchange of people, ideas, goods, and capital while
simultaneously requiring secure borders for the safety of the nation; (5) stresses from societal change due to an
aging population and an increasingly diverse population, which will strain social programs; (6) threats to the quality
of life raised from perceptions of increasing income insecurity and the gap between the “haves” and the “have-
nots”; and (7) the ethical and moral questions that society must confront due to advances in science and
technology. The Strategic Plan outlines key efforts, potential outcomes, and suggested government performance
goals to address each of the themes.
The GAO has played an important role in analyzing the 2007–2009 financial crisis and reporting its findings to
Congress and the public. These findings include testimony before congressional committees as well as written
reports on mortgage-based financial activities, unregulated participants in the financial system, the bailout of
American International Group (AIG), the execution of the Troubled Asset Relief Program, assessing current
regulatory oversight and necessary reforms, and developing evaluation criteria for the process of disbursing
economic relief funds to state and local governments. In a July 2007 report, the GAO concluded that improved
federal oversight of the leveraging of assets by key financial institutions—as well as leveraging issues for the
financial system as a whole—was necessary to prevent similar financial crises in the future.
Frank L. Winfrey
 
See also:  Congressional Budget Office;  Council of Economic Advisers, U.S.;  Fiscal Balance; 
Fiscal Policy;  National Economic Council. 
Further Reading
Government Accountability Office:  www.gao.gov.
United States Government Accountability Office.  “Financial Markets Regulation: Financial Crisis Highlights Need to Improve
Oversight of Leverage at Financial Institutions and Across System.” Report to Congressional Committees, GAO-09-739.
Washington, DC: United States Government Accountability Office, July 22, 2009. 

 
Great Depression (1929–1933)
 
The worst economic catastrophe in modern human history, the Great Depression devastated the United States
and other industrialized countries in the early 1930s and beyond. Triggered by the sudden and dramatic collapse
in the American stock market in late 1929, the Great Depression actually originated in a “perfect storm” of
underlying economic problems and, according to economic historians, was worsened and perpetuated by
misguided government policies.
The Depression wreaked havoc on virtually every sector of the global economy, hitting the United States
especially hard by bankrupting small businesses, wiping out farmers, freezing credit markets, erasing corporate
profits, and sending unemployment rates soaring to more than 25 percent. Between the Wall Street crash of
October 1929 and the inauguration of President Franklin Roosevelt in March 1933, at the depths of the downturn,
the U.S. gross domestic product fell by nearly one-third—the steepest decline in the nation’s history.
The impact of the Depression on subsequent American and world history is almost incalculable. In the United
States and other industrialized countries, it led to a dramatic expansion in the power and scope of central
governments, as they took a more hands-on approach to their respective economies and became far more
involved in the everyday lives of ordinary citizens. The Depression—or, more accurately, the responses of the
Herbert Hoover and Franklin Roosevelt administrations to it—forged a general consensus among a generation of
policy makers that the countercyclical fiscal policies advocated by English economist John Maynard Keynes—
whereby governments should borrow and spend to stimulate the economy during downturns in the business cycle
—offered the best way to fight recession and maintain growth.
Apart from economics, the Great Depression had profound political and geopolitical ramifications. In the United
States, it led to a far-reaching political realignment, as Roosevelt built a coalition of low-income workers, white
ethnics, Northern blacks, and Southern whites into a left-of-center Democratic Party majority that would hold
through the 1970s. Overseas, the Depression contributed to the rise to power of the Nazis in Germany and the
onset of World War II. For economists and economic historians, the Great Depression remains the benchmark
against which all subsequent economic downturns are measured, offering lessons and warnings about how to
avoid them and how to deal with them if they do occur.

Only the massive military buildup of World War II provided the economic stimulus that finally ended the Great
Depression, according to most economic historians. (Gordon Coster/Time & Life Pictures/Getty Images)
 Origins
In the popular imagination, the era that preceded the Great Depression was a time of great economic prosperity.
The image of the Roaring Twenties—in reality fostered amid the gloom of the 1930s—did carry some truth. After
a brief but sharp recession in 1921–1922, U.S. economic growth was substantial and sustained for much of the
rest of the decade. Corporate profits were up, construction boomed, stock prices soared, and unemployment held
steady at historically low levels of 4 percent or under.
Yet the prosperity—unequally distributed and unregulated—was superficial. First, there was the prosperity gap
between urban and rural America. The roughly one in four U.S. households that still supported themselves
primarily by farming did not prosper during the boom years of the 1920s. Having expanded production during
World War I—often by going into debt for new land and machinery—American farmers were unprepared for the
dramatic and sustained drop in agricultural prices that occurred after the conflict ended. Between 1919 and 1930,
U.S. farm income as a percentage of national income fell by almost half, from 16 percent to 8.8 percent. As some
economic historians maintain, American agriculture was in a state of depression a full ten years before the rest of
the country. Nor was farming the only sector that lagged during the 1920s; textiles, the railroad industry, and coal
mining were also hurting. Even prosperous industries were weaker than they appeared, as a wave of corporate
mergers left many companies saddled with large debts.
Nevertheless, broad swaths of the economy prospered through much of the 1920s, fueled by rapid growth in
consumer durables—such as appliances and radios—and motor vehicles, along with supporting industries like
steel, rubber, and petroleum refining. The decade also brought significant increases in productivity, as advances in
communication, transportation, and electrification increased worker output. Still, the gains were not equally
distributed. Weak labor unions and falling tax rates for corporations and the wealthy meant that most of the
income gains of the decade accrued to the upper reaches of the economy. By 1929, the bottom 40 percent of the
population received just 12.5 percent of aggregate personal income, while the top 5 percent received 30 percent,
the most acute inequality in the twentieth century. With their incomes rising by 75 percent between 1920 and
1929, and the marginal tax rates falling, the top 1 percent put much of their gains into speculative activities.
Such activities helped hide the underlying weaknesses in the economy, as speculation-fueled rises in real-estate

values and especially stock prices became vertiginous in the latter half of the decade. Between 1924 and 1929—
the peak years of the bull market—the Dow Jones Industrial Average (DJIA) increased fivefold, climbing above
381 in early September of the latter year. Some of the upswing was based on legitimate gains in productivity and
profit, but most of it was speculative, fueled by both the public and private sectors, especially after key industries
like automobiles and consumer durables saw slower growth after 1927. The newly created Federal Reserve Board
(Fed) contributed to the speculation by keeping interest rates low, which enabled individuals and corporations to
borrow money more cheaply and easily—often, in the case of the former, for speculative purposes and, for the
latter, to finance a wave of corporate mergers. Only in 1928 did the Fed begin to tighten credit and warn about the
dangers of stock speculation. By then, however, the bubble was self-perpetuating.
Other government policies—or the lack of them—contributed to the precariousness of the nation’s economy. In an
age of pro-business Republican Party ascendancy in the White House and on Capitol Hill, there was little
inclination or will to impose government regulation or even oversight of the financial sector. Corporations routinely
misrepresented their finances to obtain loans, while investment banks and brokers encouraged margin buying.
Loans were extended for up to 90 percent of the value of equities purchased on the stock market, with lenders
and borrowers sharing the assumption that values would continue to rise. Indeed, by 1929, many Americans had
come to believe that the country had achieved sustained prosperity. On the eve of the crash, observed the noted
economist Irving Fisher, “stock prices have reached what looks like a permanently high plateau.”
 Stock Market Crash and Banking Crises
In retrospect, it is clear that America had entered a recessionary period by mid-1929, as construction and
consumer spending—two key growth sectors—went into decline and businesses cut back on production and
employment to reduce their growing inventories, further depressing demand. By early September, stock prices had
begun to sag. On October 28, “Black Tuesday,” they fell precipitously. Literally overnight, the paper value of
stocks sank from $87 billion to $55 billion. And with that drop, the inherent perils of buying equities on margin
began to be realized. In the previous environment, with brokerage houses and investment banks extending credit
to clients, an investor had needed to put down just $100 to buy $1,000 worth of stock; if the value of the stock
went from, say, $10 to $20 per share, the investor made a 1,000 percent profit. But if the stock should fall from
$10 to $5, the investor had to come up with $400 to pay back the loan, in addition to the $100 investment that had
now vanished. Now, suddenly, overextended brokers and investment bankers were making margin calls—
demanding that investors pay back their loans. With stock prices falling, however, they could not do so. This set
off new panic selling and further drops in stock prices. By mid-November, the DJIA had fallen by a third, to under
200. By mid-1932, the index sank to 41 points, a level not seen since the nineteenth century and down a
staggering 89 percent from its peak on September 3, 1929.
As devastating as these losses were to investors, the impact of the crash might have been contained had the
underlying economy not been riddled with the weaknesses outlined above. After all, just one in ten American
households owned stock shares; of these, just one in three had substantial investments on Wall Street. But the
inherent weaknesses of the economy proved fatal, as most households—having experienced few of the
productivity gains made during the 1920s—had little discretionary income to sustain demand. The problem was
compounded by the fact that American industry had expanded capacity dramatically during the previous boom
period. Thus, the vicious cycle that had begun in the summer of 1929—declining demand leading to layoffs,
leading to further drops in consumer demand—sent the U.S. economy into free fall through 1930 and early 1931.
A brief recovery then set in, as businesses began to rebuild inventories and low prices fueled renewed demand.
The stock market recovered, too, regaining most of its postcrash losses by April 1931.
But it was a false dawn. The Depression—a term popularized by Hoover, who thought it was less ominous-
sounding than “crisis” or “panic”—was still on in earnest, perpetuated by four key forces. One was rising
unemployment and its devastating impact on consumer demand. During 1930, the national unemployment rate
averaged 8.7 percent; for 1931, the figure was 15.9; by 1932 and 1933, it hovered around 25 percent. A second
factor was the continuing weakness in the agricultural sector, as crop prices reacted to falling demand and

plummeted even more steeply through 1930 and 1931. Heavily in debt and with their income falling, many farmers
were pushed into bankruptcy; hundreds of thousands of farms were foreclosed by rural banks, many of which fell
into bankruptcy as well. Since many rural banks borrowed money from their larger urban counterparts, the crisis
soon spread to the entire banking system as frightened depositors began to withdraw their savings.
Adding to the nation’s economic woes were the Fed’s monetary policies. In retrospect, economists from across the
political spectrum generally agree that the Fed’s policies had disastrous effects. First, the Federal Reserve Bank of
New York, the most important of the regional banks in the system, significantly increased its discount rate—the
rate it charges member banks to borrow money—and cut back the amount of money it put into circulation through
the purchase of government bonds. The Federal Reserve Bank of New York believed that the most critical
problem was industrial overcapacity rather than consumer underdemand, and that making credit harder to come by
would reduce capacity and thereby lift prices and profits. To be fair, the bank was hamstrung by federal legislation
that required it to maintain a certain level of gold deposits before it could issue credit. At the time, gold deposits
were shrinking, as domestic and foreign holders of gold-backed U.S. currency demanded payment in the precious
metal, leaving the Fed unable to loosen credit. Whatever the reason, the Fed’s efforts to shrink the money supply
exacerbated an already bad situation as depositors withdrew their money from the banking system, preferring to
hide it under their figurative mattresses instead. From August 1929 to March 1933, the money supply in the
United States fell by fully one-third, driving down prices and drying up funds available to businesses for investment
and hiring.
The Hoover administration’s response also proved inadequate to the problem. Steeped in ideas of “rugged
individualism” and encouraged by conservative Treasury secretary Andrew Mellon, who advocated a laissez-faire
approach to the crisis, the president refused to increase government spending or launch jobs programs, fearful
that they would create a dependency among ordinary citizens. Meanwhile, he hoped the private sector would take
up the slack and tried to win pledges from industrial leaders to maintain wages and production. Finally realizing
that such voluntary measures were not enough, Hoover in 1932 set up the Reconstruction Finance Corporation,
which provided loans to big business in the hope that they would use them to invest, extend credit, and hire the
jobless. Hoover also launched some limited works programs, commissioning such projects as the Hoover Dam on
the Arizona-Nevada border. Most economic historians agree, however, that it was too little, too late.
Finally, there was the international dimension to the crisis. The global economic order of the 1920s had been built
on a precarious foundation. Determined both to punish Germany—which they blamed for the war—and to force it
to pay for the rebuilding of the their war-wracked infrastructure, the victorious Allies imposed vast reparations on
Berlin. When Germany proved unable to pay the required amounts in the early 1920s, the United States—which
had emerged from the war as the world’s largest creditor nation—stepped in, providing loans to Germany to pay
the Allies so that they, in turn, could pay their loans to the United States. Complicating the equation, the United
States maintained high tariffs, making it difficult for countries to sell in the United States and obtain the dollars
they needed to pay back their loans to the U.S. government and U.S. financial institutions.
The economic crisis of the early 1930s only exacerbated the situation. With the crash on Wall Street, U.S. foreign
investment dried up, further depriving other economies of the dollars they needed to repay their loans. Declining
consumer purchasing power and corporate profits shrank America’s appetite for imports, a trend that hit Latin
American economies with particular ferocity. As American investors came to fear the ability of foreign governments
and industries to pay dividends and repay loans, they began to divest themselves of foreign holdings, pulling back
virtually all of the foreign investment made during the boom years of the 1920s (about $11 billion in all). Policies
out of Washington, D.C., only added to the woes. In a move aimed to protect American industries and jobs,
Congress passed the Smoot-Hawley Act in 1930, imposing some of the highest tariffs in U.S. history and further
undermining the ability of foreign industries to sell in the United States. The tariff also triggered retaliatory moves
by other countries, putting a new crimp in international trade.
All of these problems came to a head in the crisis affecting the international gold standard in 1931. Pegging
national currencies to gold had been a sacrosanct economic principle for hundreds of years, as nations based the
value of their currency on the gold reserves held in their vaults. During the 1920s, the system caused problems

for many countries, ranging from inflation in Germany to unemployment in Great Britain. In September 1931, Great
Britain went off the gold standard, followed by more than forty countries in late 1931 and 1932. Hoover refused to
follow suit, though most international investors were convinced that it was only a matter of time before the United
States did the same. In any event, Hoover monetary policy caused a massive outflow of gold from the United
States. One of President Roosevelt’s first moves after taking office was to take the United States off the gold
standard in June 1933.
In the long run, the decision to abandon the gold standard was a smart one. Indeed, according to some
economists, it was the single most important factor in ending the global depression, as it allowed governments to
increase the supply of money more easily and thereby stimulate economic activity. It also removed one of the
main impetuses for raising tariffs and imposing exchange controls, since countries no longer had to fear runs on
their gold reserves. In the short term, however, abandoning the gold standard created uncertainty in the value of
various currencies, thereby disrupting trade and international finance.
By 1933, the world economy had hit bottom, with economists estimating a total drop in output of 38 percent since
1929. Unemployment had skyrocketed in virtually every industrialized economy, bank failures had become
widespread, and international trade had fallen dramatically. As in the United States, many governments worsened
the situation by raising taxes and decreasing spending to balance their budgets, in the hope that lower
government borrowing would free up capital for private investment. Fearing social unrest at home, many
governments also turned inward, passing tariffs and exchange controls in the hope of saving businesses and jobs,
and thereby placating angry citizens. This beggar-thy-neighbor approach underscored what some economists
regard as the single most important factor in turning what might have been a sharp economic downturn into a
prolonged global depression—the lack of international leadership. In previous economic crises, Great Britain had
repeatedly stepped up to impose order on chaotic global markets. Now, however, Britain was weakened by World
War I, deeply in debt, and no longer in a position to do so. Its obvious successor as a global power, the United
States, facing its own grim economic outlook and gripped by isolationist politics, declined the role.
While 1933 marked the low ebb of the global economy, it also represented a turning point, as the abandonment of
the gold standard and new stimulative measures began to kick in. No longer fearing that a run on gold reserves
would undermine its currency, the Bank of England, for example, was able to lower interest rates, promoting
investment and consumption at home. Under its new National Socialist (Nazi) regime, Germany launched a major
government spending program—including a huge military buildup—that put millions of citizens back to work. In the
United States, Roosevelt’s New Deal policies helped stabilize the financial markets and banking sector with new
regulations and protections for investors and depositors and pumped funds into the economy through new
spending and jobs programs, although only World War II expenditures would be sufficient to pull the economy
back to its prior level.
All of these measures represented the triumph of what would become known as Keynesian economics, which
advocated that in a time of high unemployment and underutilized industrial capacity, the government had to step
in and stimulate the economy when private industry would not or could not. Government, the Keynesians said,
had to abandon the time-honored idea that the best means for lifting an economy out of a downturn was
constricting the money supply and balancing the budget. Instead, governments had to act countercyclically
—“priming the pump,” as the American expression went—growing the money supply and investing in the
economy, even if it meant borrowing to do so. This new Keynesian consensus would dominate economic policy in
the industrialized, noncommunist world through the 1970s, when another downturn, this one marked by an
unprecedented combination of high inflation and slow growth, once again tested the premises of the existing
economic policy paradigm.
James Ciment
 
See also:  Boom, Economic (1920s);  New Deal;  Recession, Roosevelt (1937-1939);  Stock
Market Crash (1929). 

Further Reading
Aldcroft, Derek H. The European Economy, 1914–2000. London: Routledge, 2001. 
Bernanke, Ben. Essays on the Great Depression. Princeton, NJ: Princeton University Press, 2000. 
Bernstein, Michael A. The Great Depression: Delayed Recovery and Economic Change in America, 1929–
1939. Cambridge, UK: Cambridge University Press, 1989. 
Eichengreen, Barry. Golden Fetters: The Gold Standard and the Great Depression, 1919–1939. New York: Oxford
University Press, 1992. 
Feinstein, Charles H., Peter Temin, and Gianni Toniolo. The European Economy Between the Wars. Oxford, UK: Oxford
University Press, 1997. 
Friedman, Milton, and Anna Jacobson Schwartz. The Great Contraction, 1929–1933. Princeton, NJ: Princeton University
Press, 2009. 
Galbraith, John Kenneth. The Great Crash, 1929. Boston: Houghton Mifflin, 1997. 
Kindleberger, Charles P. The World in Depression, 1929–1939. Berkeley: University of California Press, 1973. 
Shlaes, Amity. The Forgotten Man: A New History of the Great Depression. New York: Harper Perennial, 2008. 
Temin, Peter. Lessons from the Great Depression. Cambridge, MA: MIT Press, 1989. 
 
Greece
 
One of the birthplaces of Western civilization, Greece is a small country of about 11 million people located at the
extreme southern end of the Balkans, consisting of a mainland, the large Peloponnese Peninsula, and dozens of
inhabited islands in the Aegean, Ionian, and Mediterranean seas.
Despite its geographic location in Eastern Europe, Greece remained outside the Soviet orbit during the cold war
and was considered part of Western Europe. Its economy, one of the weakest in Western Europe, traditionally
relied on two main industries—shipping and tourism—though the industrial and agricultural sectors remain
significant as well. Although it has one of the highest rates of institutional and household borrowing in the
European Union (EU), and although its banking sector was heavily exposed to financially turbulent markets in the
Balkans, the Greek economy weathered the global financial crisis that began in 2007 relatively well at first,
according to economists, though a heavy public debt left the government little room to employ countercyclical
economic stimulus policies and the country nearly experienced default in 2010.

Shareholders of the National Bank of Greece, the country’s largest commercial banking group, met in Athens in
January 2010. The government had just announced a three-year plan to reduce a runaway budget deficit—the
highest in the European Union. (Bloomberg/Getty Images)
 Historical Foundations
Divided into often mutually antagonistic city-states in ancient times, Greece emerged as a cultural center in the
middle of the first millennium BCE, when philosophers and artists laid the cultural foundations of Western
civilization. Because of its fragmented geography, however, the inhabitants of Greece were rarely able to unite
politically, leaving the region vulnerable to conquest by larger civilizations. After being taken over by the Romans
in the second century BCE, Greece fell under the political hegemony of that empire and its successors until the fall
of the Byzantine Empire in the mid-fifteenth century CE. For the next 300 years, the country was ruled by the
Ottoman Turks, who were finally ousted in the Greek War of Independence in the 1820s.
While retaining its independence, except for a brief period of occupation by the Nazis during World War II, Greece
remained one of Europe’s poorer countries through the first half of the twentieth century. Aided by the United
States, Greece’s government was able to defeat a communist insurgency after World War II. This allowed the
country to defy its geography and join the Western Europe community of capitalist democracies, even though it
was ruled by a military dictatorship in the late 1960s and early 1970s.
 Postwar Industrial Economy
Whether under democratic or military rule, Greece participated in the postwar economic boom in Western Europe,

achieving one of the highest growth rates in the world during the 1950s and 1960s. Several factors contributed to
what became known as the “Greek economic miracle.” First was the country’s participation in the Marshall Plan, a
massive influx of U.S. capital aimed at jump-starting Western European economies in the wake of World War II
and preventing leftist revolutions.
The Greek government also instituted a number of policies to lure foreign investment, largely from Western
Europe, including a dramatic devaluation of the Greek currency, the drachma. With the infusion of capital, Greece
developed a major chemical manufacturing sector and built up its infrastructure. Concurrently, the prosperity of
Western Europe led to a massive influx of tourist money that further aided development and lifted the standard of
living. Whereas the Greek per capita income stood at about 40 percent of France’s just after World War II, it had
reached about 80 percent by the time Greece entered the European Community (later the European Union, or
EU) in 1981.
As in much of Western Europe, Greece’s economy stagnated in the 1970s. Productivity gains began to lag, and
the country was hit by rapidly rising oil prices. To lift itself out of the economic doldrums, the government
significantly increased public spending in the 1980s, paying for the expansion by borrowing from abroad. But all
the borrowing and spending led to high inflation and high public debt, the latter climbing above total gross
domestic product (GDP) by the early 1990s, which in turn triggered several currency crises during the 1990s.
To curb inflation, which peaked at nearly 25 percent in the early 1990s, the Greek government instituted a
number of fiscal reforms: reductions in government spending, limits on public borrowing, wage and price controls,
and a bolstering of the drachma. All of these policies were instituted not just to lower inflation—which they did, to
about 2 percent by decade’s end—but to bring the country into line with public spending and inflation mandates
set by the EU for membership in the new eurozone. Greece became a member of that monetary union in 2001,
converting its currency to the euro and turning over interest-rate-setting authority to the European Central Bank.
Fiscal restraint and renewed European economic growth in the early and middle years of the twenty-first century’s
first decade helped revive the Greek economy, as did infrastructure development and other preparations for the
Olympic Games of 2004, held in and around Athens. That year, the country achieved a near 5 percent growth
rate, one of the highest in the European Union. By 2007, just before the global financial crisis, Greece’s per capita
GDP had achieved parity with the average for the EU as a whole, though the latter had been weighed down by
the entry of several relatively poor former Eastern bloc countries in 2004 and 2007.
Still, the Greek economy was not as strong as the numbers suggested. Household, business, and public
borrowing remained high, and low interest rates had fueled an unsustainable run-up in real-estate prices during
the middle of the new century’s first decade. In addition, many Greek banks had loaned substantial amounts to
rapidly growing but volatile economies in the Balkans, especially since the end of large-scale civil conflict in the
former Yugoslavia in 1999. In short, the Greek banking system was highly leveraged by the close of 2000’s first
decade. Thus, when the global financial crisis hit in 2008, the Greek government responded forcefully by
becoming the second EU country, after Ireland, to guarantee savings in its domestic banks. With this move, the
government sought to reassure skittish foreign depositors and investors that the nation’s banking sector was
secure.
At the same time, the high levels of public debt made it difficult for the government to engage in economic
stimulus spending. In 2009, the Greek government’s deficit was running at 12.7 percent of GDP while its overall
debt was 113 percent of GDP; of the latter figure, roughly two-thirds was owed to foreigners. With such a huge
debt load and worker unrest in the country making it seem that the government would not be able to impose the
austerity measures debt holders would like to see to bring down the deficit, fears began to spread in the bond
markets that Greece would be unable to refinance bonds worth some $23 billion and that the government would
go into default.
Ordinarily this would be a problem for Greece alone but, of course, Greece was part of the eurozone and was not
alone among eurozone members in having shaky finances. Spain, Portugal and Ireland faced similar

circumstances. Thus, Greece’s default might shake confidence in the eurozone’s ability collectively to deal with a
serial default.
Germany and France, the largest economies in the eurozone, were at first reluctant to help Greece but, as many
economists pointed out, Greece’s membership in the eurozone had come with the implicit assumption—at least to
those buying its bonds—that it would never be allowed to default, that it would be saved by the European Central
Bank. This allowed Greece to borrow money at much cheaper rates, and the country had binged for a time on this
cheap credit. Now the bills were coming due.
By early 2010, it was becoming clear that Greece would be rescued by the stronger eurozone economies, but
only if Athens took the necessary measures to rein in government spending. In March, the Socialist Party
government of Prime Minister George Papandreou did impose tough austerity measures as well as substantive tax
hikes, though how effective the latter would be in raising revenue was open to question. Greece had the lowest
level of tax collection and the highest level of tax avoidance of any country in the eurozone.
Nevertheless, the moves were enough to convince French president Nicholas Sarkozy and German chancellor
Angela Merkel to arrange a financial rescue of the country, though Merkel in particular imposed tough conditions
and insisted that much of the bailout package would come not from the European Central Bank but from the
International Monetary Fund, which is well known for imposing tough austerity measures on countries that come
looking for funding in times of fiscal crisis. The agreement allowed Greece to borrow money in the bond markets
but only by offering significantly higher interest rates, some 3 percent above what Germany paid on its bonds. The
money helped Greece avoid default, but whether the country was on the long and painful road to fiscal recovery—
as opposed to merely postponing default—would depend upon whether the austerity measures and enhanced tax
collection truly took hold.
In May 2010, the various Eurozone countries and the IMF agreed on a 110 billion Euro loan to Greece, aimed at
shoring up its finances. At the same time, the Eurozone members established the European Financial Stability
Facility (EFSF), with commitments by various countries of 780 billion euros and a lending capacity of 440 billion
euros. The impetus for the facility was to create a fund that could shore up the finances of troubled Eurozone
countries that might find it difficult to sell their bonds on the open market without such backup. As the interest
rates various Eurozone members were forced to offer on their bonds made clear, the Greek crisis had a
contagious effect on other governments’ investors suspected of having fiscal problems. Hence, the need to
provide a fund to provide assurances that the bonds would be repaid and the governments would not go into
default. Then, in October 2011, the Eurozone leaders agreed to write off 50 percent of Greek debt owed to private
creditors and to increase the EFSF to one trillion euros. They also required banks to increase their capitalization to
9 percent to reduce their risk of exposure should any other governments experience a similar crisis to that of
Greece.
But these rescue packages took a toll politically. Many voters in Germany and other countries with more sound
finances resented the idea of bailing out Greece and other countries that were perceived as having profligate fiscal
policies. Thus, in order to achieve a political consensus behind the Greek bailout and loan forgiveness packages,
various Eurozone countries required Greece to tighten its belt even further, imposing severe austerity measures.
Papandreou continued to advocate such policies until the widespread outbreak of popular protests, some of them
violent, in the summer and fall of 2011. In October, the Papandreou government made a unilateral announcement
that it would put its austerity plan to a national referendum, raising uncertainty in international financial markets.
Already seesawing dramatically in response to developments in Greece, the markets panicked, leading Eurozone
leaders, headed by Sarkozy and Merkel, to insist Papandreou drop his plans for the referendum, which he quickly
did. But facing a hostile citizenry, Papandreou agreed to resign, once a coalition government was formed.
Greece is a tiny economy, representing just a fractional percent of the Eurozone GDP. A default there would
certainly roil markets but would probably not produce a continent-wide financial crisis. Indeed, some experts said
that the bailout and loan forgiveness represented a default by another name. The fear was that the investors
would pull out of the bonds issued by larger Eurozone countries that also faced fiscal crises. As the cost of

borrowing goes up, the fiscal crisis grows worse. Should Spain and especially Italy experience the same kind of
run on their bonds, the crisis might not be able to be contained, since even the Eurozone community as a whole
would find it difficult to come up with the funds needed to shore up their finances.
 
James Ciment
 
See also:  Ireland;  Portugal;  Spain. 
Further Reading
Dimitrakopoulos, Dionyssis G., and Argyris G. Passas. Greece in the European Union. New York: Routledge, 2004. 
Kalaitzidis, Akis. Europe’s Greece: A Giant in the Making. New York: Palgrave Macmillan, 2010. 
 
 
Greenspan, Alan (1926–)
 
One of the most influential Federal Reserve Board (Fed) chairs in U.S. history, Alan Greenspan was a key
decision maker during the unprecedented expansion of the nation’s economy in the 1990s. Widely credited with
policies that helped America prosper, Greenspan came under criticism both before and after he left office in 2006,
with many economists and others blaming his loose monetary policies and failure to enforce bank regulations as
key reasons for the dot.com bubble of the 1990s and early 2000s, and the housing bubble of the early and mid-
2000s, contributing to the financial crisis that crippled the global economy beginning in 2007.

Federal Reserve Board chairman Alan Greenspan testifies before the Senate Banking Committee in July 2005.
Greenspan’s comments on economic trends were closely parsed in the financial community and often had a direct
effect on markets. (Mark Wilson/Getty Images)
Greenspan was born in New York City in 1926. An accomplished musician in his younger years, he became
interested in economics in his teens and enrolled in New York University (NYU), earning bachelor’s and master’s
degrees. Forced to leave the economics PhD program at Columbia University in 1951 because he could not
afford the tuition, he eventually earned his doctorate from NYU more than a quarter century later.
Meanwhile, upon graduation from NYU, Greenspan joined the National Industrial Conference Board, a nonprofit
business research group, and then opened his own economic consulting firm, Townsend-Greenspan & Company,
in 1954. He served as director of policy research for the 1968 presidential campaign of Richard Nixon and then
was appointed chair of the Council of Economic Advisers by Nixon’s successor, Gerald Ford, in 1974. Thirteen
years later, President Ronald Reagan appointed Greenspan chairman of the Fed, where he would serve until
2006, the second longest tenure in the institution’s history. He was reappointed five more times, by Republican
and Democratic presidents alike.
Greenspan faced a test of fire within months of his appointment when, on October 19, 1987, the Dow Jones
Industrial Average posted its greatest one-day drop in history, both in absolute and percentage terms to that time.
The new chair’s forceful statement that the Fed would provide a source of liquidity to the financial markets is
widely credited for preventing the stock market crash of 1987 from triggering a full-blown recession.
Although willing to expand the money supply in times of crisis, Greenspan saw himself primarily as an inflation
fighter. Heavily influenced by his mentor, former Fed chairman Arthur Burns, with whom he studied at Columbia,
Greenspan pursued tight money policies during the recession of the early 1990s and has been blamed by his
fellow Republicans for bringing about the defeat of incumbent George H.W. Bush in the presidential election of
1992.
Half a decade later, however, Greenspan pursued a diametrically opposed policy during the Asian financial crisis
of 1997, when he flooded the world’s financial markets with dollars in an effort to slow the capital flight from
affected economies in that part of the world. Greenspan also took an activist role in the 1998 collapse of Long-
Term Capital Management, a major U.S. hedge fund. Fearing its bankruptcy could have a chilling effect on global

financial markets, Greenspan orchestrated a bailout of the firm by commercial and investment banks.
Despite such disruptions, the U.S. economy boomed in the 1990s and Greenspan’s stewardship of the money
supply was often credited with sustaining the expansion. Thus, while Fed chairmen always have enormous
influence over the economy, Greenspan seemed to exert more than most, more even than the presidents under
whom he served. Investors worldwide heeded and hung on his carefully parsed—and some would say, cryptic—
pronouncements on the economy. When, in one of his most famous utterances, he mentioned in 1996 that an
“irrational exuberance” had inflated share valuations too high, stock markets around the world tumbled.
Nonetheless, Greenspan did not propose regulations that might have tempered the dot.com stock bubble that
collapsed finally in 2001.
But in the wake of that collapse the U.S. economy entered recession, Greenspan reversed course, lowering the
fed funds rate to just 1 percent, a historic low. The rate would remain at or below 3 percent through the middle of
2005.
This lengthy run of historically low rates not only made it cheaper for banks to borrow money but to lend it as well.
Many economists cite Fed policy as contributing to the mortgage lending boom of the early and mid-2000s, which
sent house prices soaring across the country, as banks and other financial institutions began to devise ever-riskier
loans—including adjustable rate mortgages—and to offer mortgages to ever-riskier borrowers, with so-called
subprime mortgages. At the same time, there was an ever-increasing trade in mortgage-backed securities, as
lenders bundled the mortgages and sold them to investors, for the purpose of spreading risk around.
Even as housing prices soared, some critics began to argue that Greenspan’s loose monetary policies were
contributing to the deterioration in lending practices and thus inflating a housing bubble. Greenspan argued that
unlike overvalued securities, housing prices had never collapsed on a national scale and thus there was little
danger of a major crisis in the housing markets.
In the wake of the collapse, other critics such as Nobel Prize–winning economists Joseph Stiglitz and Paul
Krugman have argued that Greenspan failed to use his powers to rein in deceptive and dangerous lending
practices. For his part, Greenspan has said that he did seek tighter regulation of Freddie Mac and Fannie Mae,
the government mortgage insurers that back a large portion of the nation’s mortgages. But he has also admitted
fault. In 2008, he agreed with some of his critics and said that he could have done more to push for regulation of
derivatives such as mortgage-backed securities. It is the collapse in the value of those derivatives that was the
prime cause of the financial crisis that began in 2007.
In the wake of his departure from the Fed, Greenspan has served as an economic adviser to a number of financial
institutions.
James Ciment and John Barnhill
 
See also:  Banks, Central;  Federal Reserve System;  “Irrational Exuberance”;  Monetary
Policy. 
Further Reading
Andrews, Edmund L.,  “Greenspan Concedes Error on Regulation.” New York Times, October 23, 2008. 
Fleckenstein, William A., and Frederick Sheehan. Greenspan’s Bubbles: The Age of Ignorance at the Federal Reserve. New
York: McGraw-Hill, 2008. 
Greenspan, Alan. The Age of Turbulence: Adventures in a New World. New York: Penguin, 2007. 
Woodward, Bob. Maestro: Greenspan’s Fed and the American Boom. New York: Simon & Schuster, 2000. 

Gross Domestic Product
 
Gross domestic product (GDP) is the sum of the monetary value of all final goods and services produced within a
country (or region) during a specified period of time, usually a year but sometimes a quarter. It does not matter
who produces the goods and services—citizens or foreign workers, private companies or government institutions,
local or foreign-owned enterprises. Goods and services are included in the calculation of GDP simply if they are
produced within the particular territory and time frame.
GDP measures the final production, or the value of products and services bought by end users. Thus, for
example, the value of a jacket as the final product is counted once; the value of the fabric, buttons, and other
components that are included in the jacket are not counted a second time. According to the production approach,
GDP is the sum of added value—total output minus intermediate products—at all stages of production (from a
piece of wood or plastic to a button, and from a button to a jacket), plus taxes but minus production subsidies.
According to the income approach, GDP equals the sum of profits, specifically firms’ operating profits—minus rent,
interest on debt, and employee compensation; interest income of households through their loans to business firms;
rental income households receive from property, as well as royalties from patents and copyrights and income
received by employees, including salaries wages, and fringe benefits, plus unemployment and Social Security
payments. The methodology for calculating GDP and its components is established by an internationally agreed-
upon framework known as the System of National Accounts.
GDP is a widely used measure of the economic size of a country, and per capita GDP is calculated as a measure
of the economic welfare of its population. Economists, policy makers, and business leaders rely on GDP, per
capita GDP, and GDP growth for several purposes: comparing different countries and regions (assuming they use
the same definitions and methodologies for calculating GDP and that the same currency is used and fluctuations
in exchange rates and prices are eliminated); formulating economic policy; and making private business decisions
about where to invest, export, or locate production facilities.
GDP measures the value of the goods and services at the time they were sold. Thus, if the prices increase by 5
percent but the production level remains the same, nominal GDP also increases by 5 percent—the inflation rate.
Real GDP (or GDP in current prices) is equal to nominal GDP adjusted for changes in prices. Thus, if only prices
change but the production level does not, real GDP remains the same. For example, if the overall price level
doubles and nothing else changes, nominal GDP doubles but real GDP stays the same.
Economic growth is often measured by the growth rate of real GDP (real GDP growth). Growing real GDP is a
sign of increasing output; declining real GDP is a sign of economic recession or even depression.
According to the expenditure approach, GDP can be also measured as the sum of private consumption
expenditures or household consumption (C), private investment expenditures including gross capital formation,
changes in inventories, and newly constructed residential housing (I), government expenditures on goods and
services excluding transfers (G), and exports of goods and services (EX) minus imports of goods and services
(IM): GDP = C + I + G + EX – IM. This does not mean that countries should restrict imports in order to increase
their GDP. Total imports are subtracted from GDP because the other components already include imports—the
consumption totals include both goods and services imported from abroad, as well as those produced domestically
—and GDP measures only the value of goods and services produced at home and used for private consumption
or investment, government purchases of goods and services, or exports.
GDP is linked to a country’s balance of payments because it is the sum of private consumption and investment

expenditures, government consumption expenditures, and the goods and services account. Moreover, gross
national disposable income is the sum of GDP and the balances of the income and current transfers account. The
four latter accounts also form the current account.
There is also a link between GDP and welfare. If welfare is expressed as the sum of domestic spending—private
consumption plus investment expenditures plus government purchases of goods and services (C + I + G)—one
can say that GDP is the sum of welfare and net exports (EX – IM). Thus, in countries with trade deficits, welfare
is larger than GDP; in countries with trade surpluses, GDP exceeds welfare. This does not mean that all countries
should strive for large trade deficits. They may run out of funds for financing them, and their ability to import may
decrease in the future. Moreover, if they take large loans for financing their foreign trade deficits, they will have to
repay those loans, including principal and interest, in the future. The composition of imports is another important
factor: current imports of machinery, for example, may increase future exports and future GDP. If an economy
grows very rapidly, its welfare does not have to decrease in the future when it has a foreign trade surplus and
when it pays back its loans. But if it grows slowly, or if its economy declines, it may face substantial decreases in
welfare.
If one country has a GDP twice as large as that of another country, this does not mean that personal income or
purchasing power is twice as high in the first country. If the former country has a much larger population than the
latter country, GDP on a per capita basis may actually point to a lower average income. For example, if the GDP
of Country A is $200 million and the GDP of Country B is $100 million, but Country A’s population is 400,000 and
Country B’s population is only 100,000, then Country A’s per capita GDP is $500 and Country B’s per capita GDP
is $1,000. Moreover, there may be differences in price levels as well. Because a cup of coffee, or an automobile,
or a taxi ride does not cost the same in every country, an equal per capita real GDP in two countries does not
mean that it is possible to buy the same goods and services in those two countries on the same income.
It is also important to note that GDP does not measure the value of goods produced in previous years even
though some products—such as buildings, cars, and computers—are used for several or many years. Nor does
GDP reflect the distribution of income among the population (income is distributed more evenly in some
economies than in others, lowering the overall poverty rate) or government spending (some countries spend more
on health and education, for example, while others invest more heavily in their military). Various forms of labor
(services) are excluded entirely from GDP, including volunteer work, raising one’s family, and illegal activities.
GDP also takes no measure of the amount of pollution, environmental degradation, or depletion of natural
resources caused by economic activity—all of which reduce human welfare in the long run. And finally, GDP tends
to increase in the aftermath of armed conflicts and natural disasters, as large construction projects may have to be
launched to rebuild facilities that were destroyed.
Tiia Vissak
 
See also:  Growth, Economic. 
Further Reading
Gutierrez, Carlos M., Cynthia A. Glassman, J. Steven Landefeld, and Rosemary D. Marcuss. Measuring the Economy: A
Primer on GDP and the National Income and Product Accounts. Washington, DC: Bureau of Economic Analysis, U.S.
Department of Commerce, 2007. 
International Monetary Fund. Balance of Payments and International Investment Position Manual.  6th ed. Washington,
DC: International Monetary Fund, 2010. 
System of National Accounts. Brussels: Commission of the European Communities, International Monetary Fund,
Organisation for Economic Co-operation and Development, United Nations and World Bank, 1993. 

Growth Cycles
 
A growth cycle tracks the change in an economy, typically measured by gross domestic product (GDP), in the
context of a long-term trend. In other words, as the economy grows along an expected path, the growth cycle
shows how the economy may, at times, grow a bit faster or a bit slower. Growth cycles are often confused with
business cycles, because they are closely related. The business cycle shows expansions (increases) and
contractions (decreases) of a variable such as GDP over a designated period of time. Growth cycles, by contrast,
reflect the fluctuations that occur within the expansion phase of a business cycle, when the average trend line is
rising. The “peaks” and “troughs” of the growth cycle are defined only relative to the general upward trend line.
 Cycles and Phases
Business cycles and growth cycles in a nation’s economy are determined by examining the fluctuations in several
variables, indicating whether overall business conditions are expanding or contracting. For the purposes of the
present explanation, however, gross domestic product is used as a single measure of overall U.S. economic
activity.
Consider the time period between the second quarter of 2006 and the first quarter of 2007, when the business
cycle showed the economy still in an expansionary phase. The growth cycle, however, showed that the economy
was in a “growth recession.” During this period, the U.S. economy grew at rates of 2.7 percent, 0.8 percent, 1.5
percent, and 0.1 percent, respectively—all positive, but below trend. Because growth cycles are characterized by
changes in the rate of economic growth, not just whether the economy is expanding or contracting, they can
indicate important phases in the cycle.
 Determining the Trend: Moving Averages
One of the major difficulties in analyzing the growth cycle is that the underlying trend to which growth is compared
is not constant, but continually changing. Economists used the concept of a moving average to measure the trend.
For example, in 2007, GDP grew at an average rate of 2.38 percent. Thus, in the fourth quarter of 2007, when
GDP fell by 0.2 percent, growth was 2.58 percent below average (0.2 percent plus –2.38 percent). Moving on to
the first quarter of 2008, economists compute a new moving average, adding one new observation (0.9 percent for
the first quarter of 2008) and dropping the oldest data point (0.1 percent for the first quarter of 2007). Thus, the
average growth rate for the year ending the first quarter of 2008 was 2.58 percent (4.8 percent + 4.8 percent –
0.2 percent + 0.9 percent, divided by 4). For the purpose of studying the growth cycle, growth in the first quarter
of 2008 was 1.68 percent below average (0.9 percent – 2.58 percent).
To calculate moving averages, economists often use more complex mathematical methods called “filters.” This
more sophisticated technique enables them to better isolate the difference between a long-run trend line and the
short-run fluctuations around it.
 Short-Run Fluctuations
An important concept in understanding growth cycles is the so-called output gap, which is the deviation from the
potential level of output in an economy at any point in time. The output gap can either be positive or negative. A
negative output gap does not necessarily mean a recession, but merely slower growth than the average within a
growth cycle.

Analysis of growth cycles leads to an understanding of the relationship between the output gap and the cyclical
unemployment rate. Okun’s law, named for mid-twentieth-century U.S. economist Arthur Okun, connects the
output gap and the cyclical unemployment rate. Cyclical unemployment occurs when workers lose their jobs
because of poor business conditions and a weak overall economy. According to Okun’s law, when the output gap
is 2.0 percent below potential, the cyclical unemployment rate rises by 1.0 percent. Increases in cyclical
unemployment are typical during recessions in both the business cycle and the growth cycle.
The Phillips curve—named for New Zealand economist Alban William Phillips—is a theoretical relationship
between the change in the rate of inflation and the short-term output gap. The Phillips curve shows that when an
economy is producing at an above-average rate with a positive output gap, inflation will increase. Another
implication of the Phillips curve is that a negative output gap will reduce the rate of inflation. This implies that
peaks within the growth cycle will likely lead to inflation; by the same token, troughs in the growth cycle will help
reduce the rate of inflation. It also means that the study of growth cycles can help predict when inflation will be on
the rise in an economy and suggest ways in which it can be minimized or avoided altogether. While the Phillips
curve has been a hotly debated theory among economists, most agree that there are short-run trade-offs between
the output gap and inflation.
Andre R. Neveu
 
See also:  Growth, Economic. 
Further Reading
Burns, Arthur F., and Wesley C. Mitchell. Measuring Business Cycles. New York: National Bureau of Economic
Research, 1946. 
Jones, Charles I. Macroeconomics. New York: W.W. Norton, 2008. 
Klein, Philip A., and Geoffrey H. Moore. Monitoring Growth Cycles in Market-Oriented Countries: Developing and Using
International Economic Indicators. Vol. 26, Studies in Business Cycles. Cambridge, MA: Ballinger, 1985. 
Moore, Geoffrey H., and Victor Zarnowitz.  “Appendix A: The Development and Role of the National Bureau of Economic
Research’s Business Cycle Chronologies.” In The American Business Cycle: Continuity and Change, ed. Robert J.
Gordon. Chicago: University of Chicago Press, 1990. 
Growth, Economic
 
Economic growth is defined as an increase in the output of goods and services in an economic system. For
nations, it is usually measured by changes in gross domestic product (GDP). The term “gross” is used to indicate
that the value of the capital that was used up in producing the GDP was not accounted for. Net domestic product
(NDP) is GDP minus the value of the capital that was used in the process of producing the GDP.
Economic growth and productivity are two separate but related measures. Economic growth can come from
increases in productivity (technological change) or from increases in the quality and amount of labor and capital
that are used to produce the output. Economists are most interested in the growth of per capita GDP (also called
“intensive growth”), but the growth in GDP itself (or aggregate GDP, also called “extensive growth”) is also a
closely followed statistic.

Despite the close relationship between economic fluctuations and economic growth, economists have tended to
study these separately and in quite different ways. Most theories of booms and busts stress the demand side:
Why do consumers and investors change how much they want to spend from year to year? Theories of economic
growth instead tend to stress the supply side: Why are firms producing more this year than last year?
Nevertheless, it is possible to outline several ways in which growth and business cycle fluctuations interact.
 Effects of Economic Growth on Business Cycles
One might expect that there would be little or no economic fluctuation in a world without economic growth. If
everyone earned the same income year after year, they would—at least on average—make similar expenditures
year after year. Producers would thus know the total market for the goods and services they produce. In such a
world, one might expect all producers and consumers to continue happily from year to year producing and
consuming what they did the year before. In such a scenario, governments would have little reason to change the
annual money supply or government budget, in which case these sources of economic cycles would disappear as
well. The “cycle” would be nothing more than a nearly flat line.
Small fluctuations might still persist. Competition among firms for an unchanging market sometimes might lead to a
bunching of investment expenditures. Likewise, consumer expenditures on expensive durable goods that are
purchased only occasionally (such as houses and cars) might not occur evenly through time. In the absence of
growth, however, one could anticipate that these booms and busts would be relatively small and inconsequential.
Intensive economic growth occurred slowly if at all before the early nineteenth century, though there were certainly
economic fluctuations. The oscillations are often referred to as “harvest cycles,” as they tended to be driven by
variations in agricultural output from year to year. Now that agriculture has come to comprise only a small
percentage of GDP in developed countries, annual changes in the harvest alone are not likely to cause booms
and busts.
Scholars of economic growth increasingly recognize that the output of individual goods and services does not
expand at the same rate. Economic growth is associated with innovation: firms develop new goods and services
through innovations in technology or firm organization, or by forging new ties with other firms. Sometimes the
impact on other sectors is positive, as when increased production of automobiles is associated with increased
purchase of oil or rubber for tires. Often, though, the impact is negative, as when the automobile displaced the
manufacture of carriages, buggy whips, and horseshoes.
Such changes in the output of goods and services mean that employment and output expand rapidly in some
sectors of the economy while contracting in others. Even within sectors, the successful development of new goods
or services by some firms may squeeze other firms out of the market. Since these changes both across and
within sectors are difficult to predict, neither capital investment nor workers are likely to flow quickly from declining
firms to growing firms. As a result, the process of economic growth creates periods of unemployment.
 Medium-Term Growth Cycles
Economists have begun to connect their theories of growth with their theories of cycles in discussing the idea of a
“medium term.” That is, economists expect that there will be periods of a decade or more in which economies
either grow fast and have mild and short business downturns, or grow slowly (or decline) and have severe and
long business downturns. The 1950s and 1960s were characterized in most developed countries by rapid growth
and mild business cycles; conversely, the 1930s and 1970s were characterized by limited or negative growth and
severe cycles. In studying economic fluctuations, economists have tended to emphasize the shorter business
cycles rather than the arguably more important medium-term fluctuations. Because of this emphasis, they have
treated all business cycles as essentially the same despite the fact that they appear to be quite different in length,
intensity, and the nature of constituent phases.

In terms of the argument above—that fluctuations are largely a result of growth—one might well expect that
periods of rapid growth would also be periods of severe fluctuation. Recall, however, that the link from growth to
booms and busts depends on the uncertainties surrounding the different experiences across firms and sectors.
One might therefore expect that periods in which investors and workers are able to identify with some accuracy
the firms and sectors that can grow quickly will be characterized by both growth and limited fluctuations. If, on the
other hand, there are difficulties in achieving expansion in some sectors or firms, perhaps because workers need
to move geographically or receive extensive training, then a lengthy period of sluggish growth and severe cycles
may result.
It may be that the relative balance between growing sectors and declining sectors differs across the medium term.
If many sectors are growing at once, rapid growth with limited fluctuations results. If few sectors are growing but
many are declining, the reverse occurs.
 Long Growth Cycles
The idea that the medium term merits specific study has existed on the fringes of economic science for some
time. The question then arises if a longer cycle—the one beyond the medium term—also deserves special
examination. For example, if “good” decades of economic expansion are observed to alternate with “bad” decades,
then one begins to wonder if there is some regular, long cycle at work. Accordingly, the Russian economist Nikolai
Kondratieff developed the idea of long waves (Kondratieff cycles) in the early twentieth century. Long waves are
generally believed to be about half a century in duration. Long wave theory has been little discussed in recent
decades, with interest in the phenomenon tending to peak during periods of poor economic performance, such as
the 1930s and 1970s. Thus, with the global financial crisis of 2008–2009, it may stage a comeback.
Long wave theory is challenged on both empirical and theoretical grounds. Theoretically, economists wonder what
forces could drive regular cycles of such a length. Empirically, modern economic growth has occurred only long
enough to generate a handful of such cycles, complicated by such shocks to the global economy as two world
wars. In other words, at least some of the “good” and “bad” of the past century may have been caused by such
unusual events.
Technological innovation has been a particular area of emphasis in many long wave theories. One might
reasonably expect that a wave of innovations would encourage both investment and consumption. But why would
technological innovation be more likely to occur when the boom generated by the previous cluster of innovations
had turned to bust? At such times, money is scarce as firms struggle to finance innovative activity. According to
one argument, firms are then more willing to take chances, making radical innovations more likely. Examining the
historical evidence, however, it is not at all clear that technological innovations (radical or otherwise) are more
common during periods of stagnation or decline.
Other long wave theorists talk of cycles in resource prices (of energy, for example), investment rates (interest
rates), or credit availability (available cash for expansion). In all of these cases, however, both the theoretical
arguments and empirical evidence are viewed with grave suspicion by the vast majority of economists. In fact,
economists have come up with no compelling or widely accepted explanation of even medium-term cycles, let
alone a cogent explanation for the more elusive and difficult to understand long wave patterns.
 Sectoral Interactions and Growth Cycles
Standard economic theory suggests that workers and investment will flow quickly from declining sectors to growing
sectors. In reality, much can be learned about booms and busts by looking at how sectors (and firms) differ in
their growth performance. For most of the twentieth century, though, economists tried to understand economic
fluctuations only in terms of the movements of a handful of economy-wide variables such as consumption,
investment, and money supply. Only within the last several years have some experts in economic fluctuations
examined how varying performance across sectors can cause booms and busts. If many sectors are declining but
few are growing, unemployment will be a likely result. Moreover, even if certain growth sectors are able to absorb

workers from declining sectors, this will not happen overnight. Workers will face many barriers—retraining and
relocation, most obviously—in moving from one industry another, making a smooth, quick transition unlikely.
Standard economic theory suggests that wage rates will fall in the face of unemployment and that, as wages do
fall, more workers will be hired. Again, this logic ignores the challenges of moving across sectors. Empirically,
there is a further puzzle: during bad periods, wages often do not fall even when unemployment is high. During the
Great Depression, for example, real wages (that is, wages adjusted for changes in the price level) actually rose.
 Savings, Investment, and the Growth Cycle
If the amount that people save exceeds the amount that people invest, then an economic downturn will result. Of
course, classical economic theory suggests that interest rates will fall in such a situation, thereby discouraging
saving (the reward for which would decrease) and encouraging investment (because the cost of borrowing would
fall). As with falling wage rates, though, this mechanism does not seem to work fast enough in real life to prevent
busts.
Personal savings decisions generally reflect an individual’s expectations regarding the future. A person is most
likely to save if he or she is worried about the future. Investment decisions are even more future oriented. A
person is most likely to invest if he or she is confident about the future. Investment decisions also depend on the
rate of innovation: investment is often called forth because new technology must be embodied in new equipment.
Thus, savings are more likely to exceed investments if expectations of future growth falter, and/or the rate of
innovation slows. (Note also that a declining rate of innovation might itself lead to low expectations.)
Savings and investment are connected by financial institutions of various sorts. Rates of investment depend on
both the institutional structure and the level of trust and confidence within the financial system. Financial
innovations such as hedge funds or derivatives trading may support growth by increasing the supply of funds to
investors. Yet if lenders lose confidence in these instruments, the result may be a financial crisis that triggers a
broad economic contraction.
 Effects of Booms and Busts on Growth
Although booms and busts are largely a result of the process of economic growth, the cycle affects that process in
many ways as well. There have been several analyses of the effect of business cycle volatility on growth. The
empirical results are diverse and seem to depend a great deal on the precise specification of the equations
employed. This may be because different sources/types of volatility—changes in government policy, trends in
technology—have different effects. Moreover, if cycles are different during medium-term upswings than during
medium-term downswings, analyses that lump the two types together are likely to generate misleading results.
Theoretically, though, it is not hard to imagine a variety of connections between fluctuations and growth. On the
positive side, there remains the possibility that certain types of innovation may be more likely during economic
downturns or upswings. Then, too, it is often suggested that economic fluctuations serve a kind of cleansing
function: weaker firms are put out of business, and surviving firms are forced to make tough decisions and thus
jettison their least productive workers. Such a shaking-out process could then lead to a period of economic
growth. There is evidence for both of these scenarios. More controversially, firms may introduce productivity-
enhancing innovations under the pressure to reduce costs during periods of poor economic performance. In all of
these ways, then, busts serve the longer-term process of growth by rooting out unproductive practices. The effect,
though, should not be exaggerated: some very productive firms and workers may also suffer during economic
downturns.
It would seem natural to expect that booms and busts have a negative rather than positive impact on economic
growth. Lengthy periods of unemployment can lower GDP—the most common measure of economic growth—
below what it could have been. Moreover, to the extent that learning by doing is an important component of
productivity advance—such as when workers develop better methods of production in the course of producing a

good or service—then any reduction in output reduces economic growth.
Levels of investment are also affected by interest rates. During boom periods, central banks generally raise
interest rates in order to discourage investment and restrain inflationary pressures. Before the 1930s, economists
tended to urge low interest rates to foster growth. Since then, economists and central banks have focused
primarily on stabilizing business cycles.
Some economists have urged an emphasis on fiscal policy (government spending and taxation) rather than on
monetary (interest rate) policy during boom times. Tax increases that are clearly temporary might cause
consumption rather than investment to fall. If these taxes were used to finance government expenditure during the
next recession, the government would not have to borrow (and thereby help avoid rising interest rates).
The hardest link to establish is possibly the most important: cycles affect expectations. Investors (and innovators)
must worry that a future recession will cause their otherwise sensible investment plans to become unprofitable. If
investors need to borrow, they may find that banks are unwilling to lend if they fear a severe economic downturn is
just around the corner. It is not clear how important these cyclically related expectations are. Investors have a
host of other things to worry about: the future prospects of technology, institutions, tastes, and competitor
behavior. But fears regarding cycles may nevertheless cause some dampening of the long-term trend in
investment.
 The Demand Side
Business cycle theory to date has tended to emphasize the demand side, while growth theory has tended to stress
the supply side. The discussion of expectations should serve as a reminder that the two are closely linked. Thus,
under at least some circumstances, increases in demand may stimulate increases in supply. Moreover, growth can
be sustained in the long run only if demand and supply expand at similar rates: firms that produce more will suffer
rather than prosper if consumers are not willing to buy their output. Economists as yet understand poorly how
demand and supply interact through time.
Double-digit inflation adds a great deal of uncertainty to investment decisions. It increases the cost of negotiating
business deals, as one cannot easily specify a future delivery price. It also instigates financial speculation, which
tends to destabilize the economy. Empirical studies have suggested that lowering inflation from 50 percent to 5
percent increases growth by 1–2 percent. However, this result is largely taken from a small sample of countries
that experienced both very high inflation and very slow growth. While the negative impact on growth of high
inflation seems fairly clear, it is not at all clear that moderate rates of inflation (below, say, 5 percent) have any
negative impact on growth.
Indeed, some economic theories suggest a possible positive impact of moderate levels of inflation: Workers may
be fooled by rising wages into taking high-productivity jobs that they otherwise would have rejected, even though
the real wage (the purchasing power of a worker’s income) is unchanged. In addition, producers may be fooled by
rising output prices, which they believe will result in higher profits. Not realizing that wage and other costs are
going up as much as prices, producers might respond by expanding production. All in all, the effects of moderate
inflation can lead to greater economic growth overall.
Economists worry about deflation as well. The Great Depression was a time of rapid deflation, which likely
contributed to its severity by encouraging people and businesses to hoard money rather than invest it. The
deflation also increased the real value of debts, which were denominated in dollars, forcing many businesses into
insolvency. The much lower levels of deflation observed in the late nineteenth century (likely driven by
technological innovation) had little or no negative consequences. The lesson seems to be that, as in the case of
inflation, moderate levels of deflation pose no great danger.
Rick Szostak
 

See also:  Gross Domestic Product;  Growth Cycles. 
Further Reading
Blanchard, Olivier. Macroeconomics.  5th ed. Upper Saddle River NJ: Prentice Hall, 2009. 
Pasinetti, Luigi L. Structural Change and Economic Growth: A Theoretical Essay on the Dynamics of the Wealth of
Nations. Cambridge, UK: Cambridge University Press, 1981. 
Szostak, Rick. The Causes of Economic Growth: Interdisciplinary Perspectives. Berlin: Springer, 2009. 
Weil, David N. Economic Growth. Boston: Addison-Wesley, 2005. 
Haberler, Gottfried von (1900–1995)
 
Economist Gottfried von Haberler was a member of the intellectual group known as the Mises Circle (Mises-Kreis)
led by Austrian economist and philosopher Ludwig von Mises. The group was influenced by the Austrian or
Vienna school of economics (also known as the psychological School), which flourished from the late 1800s until
the early 1930s.
Austrian school economists believed and taught that the price level is key to understanding economic phenomena
in free markets. Because prices are determined by people making subjective decisions in dynamically changing
markets, Mises maintained, prices are thus determined by human psychology. Mises’s influence on Haberler led
the latter to propose policies in keeping with Mises’s free-enterprise ideas. Haberler, however, eventually came to
reject some aspects of the Austrian school’s views, such as the use of the gold standard as the measure for the
value of currency. Because of this and other positions, Haberler is sometimes labeled a right-wing Keynesian.
Gottfried von Haberler was born in Austria on July 20, 1900. He was educated at the University of Vienna, where
he earned PhD degrees in economics and law and in 1928 joined the faculty. Haberler left Vienna for Switzerland
in 1934, and two years later he emigrated to the United States, where he became a professor of economics at
Harvard University. At Harvard, he worked closely with Joseph Schumpeter, another Austrian émigré. The two
economists had been influenced by the Austrian school and both opposed Keynesian support of government
intervention in the economy.
Haberler’s most influential economic theories pertained to international trade and business cycles, as presented in
his major published works, Theory of International Trade (1936) and Prosperity and Depression (1937). While in
Vienna, he had argued for a policy of free trade to allow the development of an international division of labor. In
Haberler’s view, such a policy would lead to greater productivity and a greater abundance of goods for all. His
argument, while based on classical economic theories, was strengthened by his belief in the Austrian school
theory of value and price.
Haberler made a strong case for allowing businesses to engage in free enterprise as a way to promote economic
growth. He also developed a method of analyzing the business cycle based on older economic theories. He was
opposed to the Keynesian reliance on political manipulation of the money supply, especially where such policies
would lead to inflation. He was also opposed to protectionism and to mixing capitalist, socialist, and communist
polices in free-market economic systems.
In the 1950s and 1960s, Haberler rejected the gold standard for currencies and argued for the trading of national

fiat currencies in exchange programs, thereby joining with Milton Friedman and other monetarists. In the 1970s he
studied the problem of stagflation and the role of labor unions in causing inflationary recessions. In 1971, Haberler
retired from his position at Harvard and became a resident scholar at the American Enterprise Institute for Public
Policy Research, a free-enterprise think tank based in Washington, D.C. Haberler died on May 6, 1995, in
Washington.
Andrew J. Waskey
 
See also:  Austrian School;  Mises, Ludwig von;  Schumpeter, Joseph. 
Further Reading
Baldwin, Robert E.  “Gottfried Haberler’s Contributions to International Trade Theory and Policy.” Quarterly Journal of
Economics 97:1 (February 1982): 141–148. 
Gillis, Malcolm.  “Gottfried Haberler: Contributions upon Entering His Ninth Decade.” Quarterly Journal of Economics 97:1
(February 1982): 139–140. 
Haberler, Gottfried. Prosperity and Depression: A Theoretical Analysis of Cyclical Movements.  5th ed. New
York: Atheneum, 1963. 
Haberler, Gottfried. The Theory of International Trade: With Its Applications to Commercial Policy. Trans. Alfred Stonier and
Frederick Benham. New York: Augustus M. Kelley, 1968. 
Hansen, Alvin Harvey (1887–1975)
 
Economist Alvin Hansen was the leading American interpreter of Keynesian theory in the 1930s and 1940s. He
spearheaded important advances in the exposition of Keynesian theory, enabling widespread acceptance of
Keynesian public policy.
Alvin Harvey Hansen was born in Viborg, South Dakota, on August 23, 1887. He graduated from Yankton College
in 1910 and, after working for several years in South Dakota public schools, received his PhD in 1918 from the
University of Wisconsin. He taught at Brown University in Providence, Rhode Island, and the University of
Minnesota in Minneapolis before joining Harvard University in 1937 to fill the Littauer chair in political economy.
When he arrived at Harvard, Hansen was considered part of the older generation that supported the political
economy of John Maynard Keynes. Hansen’s teaching on fiscal policy influenced a rising generation of important
economists, many of whom helped transform Keynesian economic ideas into public policy. Initially, Keynes’s
General Theory of Employment Interest and Money (1936) was not well received in the United States, but Hansen
expounded, defended, and modified Keynesian thought to meet the demands of the U.S. economy. His future
writings were developments of Keynesian thought.
By 1938, Hansen had focused his efforts on the pressing problem of economic stagnation. Three years later, he
published Fiscal Policy and Business Cycles, in which he gave his full support to Keynes’s analysis of the causes
and cures for the Great Depression. Because the book was written by an economist and past president of the
American Economic Association, it gained the attention of a wide audience. According to Keynes, without
government intervention, the business cycle would eventually turn upward but would not do so quickly, nor would it

rise to its previous heights. While Hansen was not fully convinced that all of Keynes’s ideas were correct, he
believed that underemployment made government intervention necessary.
In 1953, Hansen published A Guide to Keynes, a chapter-by-chapter exposition of Keynes’s General Theory of
Employment Interest and Money, which provided American students with a clear understanding of Keynes’s
theories. Hansen also continued to address the problem of full employment for decades to come.
Hansen generally ignored the views of the Austrian school economists, but he agreed with the Austrian-American
economist Joseph Schumpeter, who also taught at Harvard, that the Great Depression marked a confluence of
several economic cycles, including the long Kondratieff cycle (40 years), the mid-range Juglar cycle (about 10
years), and the shorter Mitchell-Persons cycle (about 40 months). Hansen hypothesized that these cycles followed
a long-term price decline that had been caused by a shortage of gold that could be used for exchange.
During the course of about three decades, from about 1935 to 1965, Hansen’s views found practical application in
U.S. economic policy. He held several positions in the Franklin Roosevelt administration, playing a key formative
role in the creation of the Social Security system and the Council of Economic Advisers. A prolific writer, popular
professor, and frequent testifier before Congress, Hansen was once called “the American Keynes.” A Guide to
Keynes was vital in advancing Keynesian theory in the United States and throughout the world. In the economic
community, Hansen is perhaps best known for his IS-LM model, or Hicks-Hansen synthesis, regarding the effect
of fiscal and monetary policies on national income (where I stands for investment, S for savings, L for liquidity, and
M for money supply). The model combined elements of both Keynesian and neoclassical theories in a single
economic framework. Alvin Hansen died on June 6, 1975, in Alexandria, Virginia.
Andrew J. Waskey
 
See also:  Boom, Economic (1920s);  Great Depression (1929-1933);  Keynesian Business
Model;  Neoclassical Theories and Models. 
Further Reading
Breit, William, and Roger L. Ransom. The Academic Scribblers.  3rd ed. Princeton, NJ: Princeton University Press, 1998. 
Hansen, Alvin Harvey. Guide to Keynes. New York: McGraw-Hill, 1953. 
Metzler, Lloyd Appleton. Income, Employment and Public Policy: Essays in Honor or Alvin H. Hansen. New York: W.W.
Norton, 1948. 
Rosenof, Theodore. Economics in the Long Run: New Deal Theorists and Their Legacies, 1933–1993. Chapel
Hill: University of North Carolina Press, 1997. 
Harrod, Roy Forbes (1900–1978)
 
Roy Forbes Harrod, a follower of the British economist John Maynard Keynes—particularly Keynes’s theory of
economic expansions and contractions—added significantly to the field of economics with his investigations into
the interactions between so-called dynamic versus stable economic systems.
Harrod was born on February 13, 1900, in Norfolk, England. He was educated at Westminster School, attended

Oxford University’s New College, and briefly studied economics at King’s College, Cambridge University, under
Keynes. In 1922, Harrod was named a fellow and tutor in economics at Christ College, Oxford, and from 1938 to
1947, he was a fellow at Oxford’s Nuffield College. During World War II, he served in a variety of posts on behalf
of the prime minister and in the admiralty. Following the war, Harrod returned to Christ College until 1952, when
he was appointed Nuffield Reader of International Economics. He was a founder of the Oxford Economics
Research Group.
Among Harrod’s important contributions to the discipline of economics was his view that economics is “dynamic,”
or continually unstable and changing, rather than “static,” or stable and unchanging. (His views, formed at Oxford,
were later adopted by the Cambridge University economists.) In his dynamic model, Harrod showed that economic
expansions (booms) and contractions (busts) were deviations from the equilibrium rate of growth. Therefore, his
theory included both a stable (nondynamic) trend line—the average long-run growth in the economy—and
unstable (dynamic) localized and short-term deviations from the trend line.
Harrod presented his thoughts in The Trade Cycle: An Essay (1936), which he further developed in “An Essay in
Dynamic Theory.” Independent of the economist Evsey Domar, he developed an economic model now known as
the Harrod-Domar model (he would have received full credit for this insight—as well as for others of his ideas—
had the publication of his works not been delayed). The Harrod-Domar model shows that the economy does not
naturally have a balanced rate of growth, nor does it naturally find full-employment equilibrium.
In his later work, Towards a Dynamic Economics (1948), Harrod explained that an economy can grow generally
over time, but does so in unstable ways—owing, for example, to inflation or too rapid an expansion. Such
instability results in a failure of that cycle of the economy to reach its potential.
By the time Towards a Dynamic Economics was published, Harrod’s model of economic growth incorporated the
concept of warranted rates of growth. Harrod defined the term warranted rates of growth as the satisfaction that all
producers have produced exactly what is needed for the market, a condition in which inventories and output are
in perfect balance; it also includes the satisfaction that all households have saved at the desired level. Harrod
showed that when these conditions are not met and imbalances occur, unstable cycles form around the general
trend line, thus slowing overall growth.
By the late 1940s and early 1950s, Harrod had begun researching the life and work of Keynes. He became
Keynes’s biographer, publishing The Life of John Maynard Keynes in 1951. In addition to John Richard Hicks and
James E. Mead, Harrod corresponded with Keynes until the end of Keynes’s life. Harrod continued teaching,
researching, and writing until his death on March 8, 1978, in Norfolk.
Andrew J. Waskey
 
See also:  Keynesian Business Model. 
Further Reading
Harrod, Roy Forbes. Collected Interwar Papers and Correspondence of Roy Harrod. Northampton, MA: Edward
Elgar, 2003. 
Harrod, Roy Forbes. Trade Cycle: An Essay. Cranbury, NJ: Scholar’s Bookshelf, 1965. 
Rampa, Giorgio, A. P. Thirlwall, and Luciano Stella. Economic Dynamics, Trade and Growth: Essays on Harrodian
Themes. Port Washington, NY: Scholium International, 1998. 
Young, Warren. Harrod and His Trade Cycle Group. New York: New York University Press, 1989. 

Hawtrey, Ralph George (1879–1975)
 
Ralph George Hawtrey was a follower of the Alfred Marshall neoclassical economic tradition of free markets. He is
best known for developing the “overconsumptionist” monetary theory of business cycles, as well as the concept
that became known as the multiplier, which showed the effect of a change in total national investment on the
amount of total national income.
Hawtrey was born on November 22, 1879, in Slough, Buckinghamshire, England. He studied at Eton College
before attending Cambridge University, from which he graduated in 1901 with honors in mathematics. From 1928
to 1929, he was a visiting professor at Harvard University. Hawtrey spent most of his career with the Treasury of
Great Britain. He also studied and wrote about economics for much of his life. After World War II, he became Price
Professor of International Economics at the Royal Institute for International Affairs (now Chatham House). He was
knighted in 1956 for his work in economics.
Hawtrey’s first book on economics, Good and Bad Trade (1913), focused on the trade cycle. He developed a
theory that variations in the money supply, or the amount of currency, in an economy cause the cycle of growth,
peak, recession, and bottom in economic expansions and contractions. His ideas differed from those of the
Continental (French, German) economists, who believed that the factors driving the phases of the business cycle
were so-called real (that is, non-monetary) phenomena.
In contrast to the Continental economists, Hawtrey followed the Anglo-American tradition, which emphasized
psychological and other factors as the causes of economic cycles. This view was built on the teachings of Alfred
Marshall, who argued that economic as opposed to monetary factors such as business confidence and credit can
cause disequilibrium in an economy, which in turn drives the business cycle.
Hawtrey’s next and very influential book, Currency and Credit, was published in 1919 (revised 1927, 1950). It
presented a purely monetary theory of the economic cycle. The most important of its several distinctive features
was its stress on “effective demand,” meaning the total of both consumption spending and investment spending.
Hawtrey linked changes in effective demand with changes in the money supply and showed how this relationship
caused changes in credit availability and production output within business cycles. He also noted that expansions
and contractions in available credit within an economy did not generally occur at the same time as changes in
prices and wages. Such time differences created lags in the economy, which caused the phases of the business
cycle.
Hawtrey’s ideas had a great deal of influence in the United States, especially during the Great Depression, when
American economists applied his monetary theory to the actions of the Federal Reserve. He died in London on
March 21, 1975.
Andrew J. Waskey
 
See also:  Classical Theories and Models;  Financial Modeling of the Business Cycle; 
Marshall, Alfred;  Neoclassical Theories and Models. 
Further Reading
Capie, Forrest, Geoffrey Edward Woods, and R.G. Hawtrey. The Development of Monetary Theory, 1920 & 1930. Vol. 1
[1938]. London: Routledge, 2000. 

Davis, E.G.  “R.G. Hawtrey (1879–1975).” In Pioneers of Modern Economics in Britain, 203–233, ed. D.P. O’Brien and J.R.
Presley. London: Macmillan, 1981. 
Deutscher, Patrick. R.G. Hawtrey and the Development of Macroeconomics. Ann Arbor: University of Michigan
Press, 1990. 
Saulnier, Raymond Joseph. Contemporary Monetary Theory: Studies of Some Recent Theories of Money, Prices, and
Production. New York: Columbia University Press, 1938. 
 
Hayek, Friedrich August von (1899–1992)
 
An Austrian-born economist who spent much of his working life in Great Britain, the United States, and West
Germany, Friedrich August Hayek is best known in economic circles for his critique of socialism and his advocacy
of free-market economics. The Nobel Prize–winning economist was also something of a Renaissance man,
making important contributions to the fields of political theory and psychology over a career that spanned most of
the twentieth century. His most famous work, The Road to Serfdom (1944), which argued that collectivist
economics inevitably lead to political tyranny, was a major influence on the thinking of libertarians and such later
free-market conservatives as Ronald Reagan and Margaret Thatcher.

A proponent of the unfettered free market and a principal rival of John Maynard Keynes, Austrian-born economist
Friedrich August von Hayek blamed economic downturns on government interference in the market system. He
won the Nobel Prize in 1974. (Hulton Archive/Stringer/Getty Images)
The son of a doctor, Hayek was born in Vienna on May 8, 1899. After serving in the Austro-Hungarian army in
World War I, he attended the University of Vienna, where he earned doctorates in both law and political science in
the early 1920s. His first academic publications came in the field of psychology, but he gradually drifted toward
economics, influenced by the strongly free-market, antisocialist theories of Ludwig von Mises, a leading light of the
Austrian school of economics. He moved to the London School of Economics in 1931 and became a British
subject in 1938 after Austria merged with Nazi Germany. Twelve years later, he moved to the University of
Chicago, where he helped make the economics department a major force in free-market theory. In 1962, Hayek
returned to Europe, becoming professor of economics at the University of Freiburg, West Germany, and the
University of Salzburg, Austria. He died in Freiburg on March 23, 1992.
Hayek began his professional career amid a great debate in Europe over the ability of a socialist economy to
allocate resources as effectively as a capitalist economy. Those on the left side of the political spectrum argued
that governments could create and run large and efficient enterprises by incorporating capitalist-style pricing
mechanisms while avoiding the free market’s tendency to concentrate too much power in the hands of wealthy
capitalists. Building on the work of Austrian school economists, who emphasized subjective individual decision-
making in setting market prices, Hayek argued that many of the factors that went into investment decisions were
beyond the ken of individuals and individual firms. Only through the unfettered free market as a whole—consisting
of individuals driven by the profit motive and fear of business failure—could resources be effectively allocated.
These insights have provided the foundation for modern economists’ understanding of the key role of information

—how it is dispersed and how some economic agents may know more than others—in economic decision-making.
Like other economists, Hayek also had to contend with what appeared to be a failure of the free market in the
1930s—why, during the Great Depression, the marketplace seemed so ineffective in allocating resources
efficiently, as evidenced by the widespread idling of factories and workers. In his 1931 book Prices and
Production, Hayek laid the blame on central banks, such as the U.S. Federal Reserve, for driving interest rates to
artificially low levels. This led businesses to over-invest—especially in long-term projects where interest rates
played a greater role in the decision-making—leading to an artificial boom. Eventually, the boom could not be
sustained and a bust ensued. This was not necessarily bad, in Hayek’s view, as it would reallocate resources to
more efficient activities. In effect, Hayek said that the way to avoid recessions was to avoid policies that led to
economic booms. In other words, governments should use monetary policy to smooth out the business cycle.
Hayek also came to disagree with John Maynard Keynes’s argument that governments can help lift economies out
of recession by borrowing and spending to stimulate demand. That, Hayek maintained, would increase the money
supply and lead to runaway inflation.
Hayek also had to contend with pro-socialist critics of capitalism who gained increasing legitimacy as the Great
Depression persisted. Socialists argued that government planners could effectively run an economy by collecting
and analyzing data and then making appropriate decisions. But like other members of the Austrian school, Hayek
maintained that such mathematical modeling of the economy was an illusion, since the data were so extensive
and largely based on subjective decision-making as to evade analysis by experts. Only the free market, in his
opinion, could utilize such data effectively.
With the The Road to Serfdom, Hayek pursued another tack in his assault on socialist thinking. In that work, he
argued that government control of the economy is a form of totalitarianism, since the ability to make money and
provide for oneself is the central pillar of freedom. Moreover, he said, while democratic socialists, like those in
Britain, might have good intentions, they were embarking on a slippery slope by trying to regulate and control the
free market. Such tinkering, he wrote, inevitably leads to political tyranny and the gross inefficiencies that were a
hallmark of centrally planned economies, such as that of the Soviet Union.
In the first decades after World War II, Hayek became a kind of voice in the wilderness as Keynesian economics
came to be adopted, in various forms, in most of the world’s major noncommunist industrial economies.
Nevertheless, he was awarded the Nobel Prize for Economics in 1974, along with the Swede Gunnar Myrdal, for
his work on the role of money in the business cycle. With the persistent recession of the 1970s, which seemed to
defy Keynesian nostrums, and the triumph of conservative politics in Britain and the United States during the late
1970s and early 1980s, Hayek was restored to influence and widely celebrated. And while the recession of 2007–
2009 revived interest in Keynesian economics, Hayek’s work on how central banks can, by excessively loose
monetary policy, trigger wild swings in the business cycle seemed as pertinent as ever.
James Ciment
 
See also:  Austrian School;  Mises, Ludwig von. 
Further Reading
Caldwell, Bruce. Hayek’s Challenge: An Intellectual Biography of F.A. Hayek. Chicago: University of Chicago Press, 2004. 
Feser, Edward. The Cambridge Companion to Hayek. New York: Cambridge University Press, 2006. 
Hayek, Friedrich. The Road to Serfdom. Chicago: University of Chicago Press, 1980. 

Hedge Funds
 
Like a mutual fund, a hedge fund is an open-ended investment fund in which various parties pool their money and
turn it over to a manager who places it in a portfolio of investments. In both cases, investors typically pay
management fees, though with hedge funds these are often tied to performance as well as to the net asset value
of the fund. As hedge funds are private endeavors, largely outside government purview, it is difficult to precisely
gauge how much money they control. Estimates in late 2009 put that sum at about $2 trillion worldwide.
Economists disagree about the role hedge funds play in the business cycle. Some say their emphasis on
speculation drives up asset prices to unsustainable levels while others argue that because the funds are so
heavily leveraged, they contribute to financial instability.
Hedge funds differ from mutual funds in several key ways. First, hedge funds, which are usually limited
partnerships rather than general funds, involve smaller pools of investors and have minimum investment amounts
far higher than those of mutual funds, putting them out of reach of most middle-class investors. In addition, since
2004, hedge fund investors must be accredited by the government as having a certain amount of income, wealth,
and investment experience. Third, hedge funds can engage in certain types of investments, such as buying
derivatives and options and short-selling stocks, which mutual funds are prohibited from by law.
Investors choose hedge funds over mutual funds for two basic reasons. First, because hedge funds can engage in
more speculative investment, they potentially offer far higher returns over a shorter period of time. Second,
because hedge funds can engage in short-selling, they offer the potential to make money in a down market,
thereby hedging against the rest of the portfolio, which consists of ordinary purchases of securities. In a short sale,
the investor, or hedge fund manager, sells a security he has purchased from a third party, promising to buy it back
at a later date. If the price goes down, the short-seller pays less money than he sold it for, making a profit on the
deal.
Like mutual funds, various hedge funds engage in different kinds of investment strategies. Some focus on regular
corporate securities; others on derivatives. Some are global in scope; others focus on a given national market.
There are hedge funds that engage in commodity and currency speculation, while others focus on securities in
particular economic sectors. In addition, some hedge funds pursue event-driven strategies, others attempt to
gauge the direction of markets, and still others engage in arbitrage, attempting to capitalize on price differentials
between markets. Some seek to become actively involved in the management of the companies they buy into,
while others simply invest.
Hedge funds generally engage in riskier investment activity than mutual funds. Not only does this involve short-
selling, purchases of volatile derivatives, and investment in riskier corporate equities—sometimes even buying
whole companies—but hedge funds also typically leverage their assets. That is, they borrow large amounts of
money against the assets they own, allowing them to engage in even more investment activity. Of course, in
leveraging their assets, they become even more exposed should those investments lose money. That is, a hedge
fund that borrows $10 against $1 in assets can lose all of its initial assets if the investments fall just 10 percent in
value. In 1998, the Connecticut-based hedge fund Long Term Capital Management (LTCM), which was leveraged
by a factor of more than 30, nearly failed after suffering losses stemming from the Russian financial crisis. Fearful
that the failure of the fund, which invested in all kinds of major financial institutions and financiers, might cause a
panic in the credit markets, the Federal Reserve Bank of New York organized a $3.6 billion bailout by various
financial institutions. Many financial experts feared that as LTCM sold off its assets, particularly its corporate
securities, to pay off its debts, it would lead to a dramatic sell-off of such securities, a subsequent collapse in
prices that would force other investors to sell their stocks. Ultimately, said some, the collapse of LTCM could have
resulted in a stock market crash of such scope as to trigger a recession in the larger economy.

Since hedge funds are private entities and investors in them are presumed to be wealthier and more
sophisticated, there is far less government oversight and regulation of the funds and those who manage them,
thereby adding to the risk investors sustain. Specifically, mutual funds are regulated by the Securities and
Exchange Commission (SEC), while hedge funds are not. In addition, hedge funds lack the transparency of mutual
funds, often making it difficult for investors to divine the reasoning behind the investment decision-making of
managers.
The origin of the modern hedge fund dates back to 1948. It was the brainchild of Australian-born U.S. financial
journalist and sociologist Alfred Jones, who realized that the risk associated with long-term stock positions could
be mitigated through short-selling. Four years later, Jones reorganized his fund along lines similar to current hedge
funds, turning it into a limited partnership and adding incentive fees for the managing partner.
But Jones’s business remained unique until profiled by Fortune magazine in 1966, which noted that his fund
significantly outperformed every mutual fund on the market. Two years later, there were well over 100 hedge
funds in the United States alone. Losses during the economically volatile 1970s kept the hedge fund market small
until a new journalistic profile in 1986 once again touted the phenomenal returns of some funds. By this time,
many hedge funds had diversified far beyond Jones’s original strategy of balancing long-term corporate equities
with short-sold stocks. By the 1990s, hedge funds were employing leading portfolio managers, many of them lured
from the mutual fund industry by the huge incentives hedge funds offered.
While hedge funds have won plaudits from the many investors who have made money from them, they have also
come under heavy criticism from several sectors of society. Many economists and financial experts fear that their
leveraging activity adds undue risk to financial markets, while labor leaders complain that hedge funds, in pursuit
of fast profits, often take over companies only to strip them of their assets—including lucrative pension funds—and
then liquidate them, costing jobs.
More recently, like many other financial institutions, hedge funds have been implicated in—and have suffered large
losses from—the financial crisis of 2008–2009 and accompanying recession. Many hedge funds had invested
heavily in the subprime mortgage–backed securities that were at the vortex of the crisis. As a result, losses and
withdrawals from hedge funds have reduced the industry from its estimated $2.5 trillion peak in late 2007 to about
$2 trillion by the end of 2009.
Still, say those who support seeing more regulation of hedge funds, including many officials in the Barack Obama
administration, hedge funds are of such size as to collectively pose a systemic risk. To rein in what is seen as too
freewheeling an industry, Secretary of the Treasury Timothy Geithner has spoken of requiring larger hedge funds
to register with the SEC, as mutual funds are required to do. Hedge funds would then have to disclose their
investor positions to regulators, on a confidential basis, allowing the latter to assess whether those investments
pose a risk to the financial system as a whole. But hedge fund managers have argued that no such regulation is
needed because, unlike huge commercial banks, no single hedge fund is large enough to pose a systemic risk
and those who invest in hedge funds are sophisticated and wealthy individuals who can look after their own
interests and do not need government regulators to do so. With the Obama administration experiencing increasing
headwinds in Congress against more financial industry regulation, it remains open to question whether new
controls on hedge funds can be imposed.
James Ciment
 
See also:  Banks, Investment;  Long-Term Capital Management;  Recession and Financial
Crisis (2007-). 
Further Reading
Agarwal, Monty. The Future of Hedge Fund Investing: A Regulatory and Structural Solution for a Fallen Industry. Hoboken,

NJ: John Wiley and Sons, 2009. 
Altucher, James. SuperCash: The New Hedge Fund Capitalism. Hoboken, NJ: John Wiley and Sons, 2006. 
Bookstaber, Richard. A Demon of Our Own Design: Markets, Hedge Funds, and the Perils of Financial
Innovation. Hoboken, NJ: John Wiley and Sons, 2007. 
Burton, Katherine. Hedge Hunters: How Hedge Fund Masters Survived. New York: Bloomberg, 2010. 
Snider, David, and Chris Howard. Money Makers: Inside the New World of Finance and Business. New York: Palgrave
Macmillan, 2010. 
Temple, Peter. Hedge Funds: Courtesans of Capitalism. New York: John Wiley and Sons, 2001. 
Hicks, John Richard (1904–1989)
 
Nobel laureate John Hicks is considered one of the preeminent economists of the twentieth century. Among his
many contributions to economic theory, Hicks resolved the fundamental conflicts between business cycle theory
and equilibrium theory, which views macroeconomic forces as balancing out one another to establish a steady-
state (that is, noncyclical) condition.
John Richard Hicks was born on April 8, 1904, in Warwick, England. He attended Clifton College from 1917 to
1922, and completed his education in 1926 at Balliol College, Oxford, where he studied mathematics, philosophy,
politics, and economics. Hicks’s specialty during these years was mathematics, and his education was supported
by scholarships in that field. In the early 1930s, Hicks was a temporary lecturer in economics at the London
School of Economics. Beginning as a labor economist, undertaking qualitative analysis on industrial relations, he
soon gravitated toward his passionate interest in the application of quantitative methods to economic theory. His
first major work, The Theory of Wages in 1932 (1932), was followed in 1934 by the publication in Economica of “A
Reconsideration of the Theory of Value,” with R.G.D. Allen.
From 1935 to 1936, Hicks was a fellow of Gonville and Caius College at Cambridge University, where he worked
on a manuscript that would be published in 1939 as Value and Capital (a second edition was issued in 1946). The
book was based on research he had done in London, examining the close interrelationships between markets. His
central thesis posits that economic equilibrium, or a balanced economy, is the product of the interaction between
mutually canceling forces and that the establishment of such an equilibrium does not preclude—and may be used
to help forecast—the cyclical development of an economy. In the book, which helped introduce the notion of
general equilibrium theory to English-speaking audiences, Hicks refined general equilibrium theory to
accommodate mathematical modeling. As a consequence, he greatly influenced the direction that general
economic theory would take, and how it would be taught in the United States and internationally.
Between 1938 and 1946, Hicks taught at Victoria University in Manchester, where he did groundbreaking research
on welfare economics and social accounting. In 1939, he and Nicholas Kaldor developed the Kaldor-Hicks
measure for economic efficiency. With Kaldor-Hicks, an economic outcome is deemed more efficient if a Pareto
optimal outcome (in which at least one person is better off and nobody is worse off) can be reached through a
“fairness” compromise. This takes place when compensation is taken from those who have fared better from a
particular outcome and given to those who have fared worse. Hicks’s work in this area has been applied to a wide
range of issues, including assessments of damages for victims of environmental pollution.
Hicks returned to Oxford, first as a research fellow of Nuffield College (1946–1952), then as Drummond Professor

of Political Economy (1952–1965), and finally as a research fellow of All Souls College from 1965 until his
retirement in 1971. Knighted in 1964, he was a co-recipient with American economist Kenneth Joseph Arrow of
the 1972 Nobel Prize in Economics for his numerous contributions to general economic equilibrium theory and
welfare theory. Hicks died in Blockley, England, on May 20, 1989.
Important works by Hicks include The Social Framework: An Introduction to Economics (1942), A Contribution to
the Theory of the Trade Cycle (1950), A Revision of Demand Theory (1956), “The Measurement of Real Income,”
in Oxford Economic Papers (1958), Essays in World Economics (1959), Capital and Growth (1965), and A Theory
of Economic History (1969).
Andrew J. Waskey
 
See also:  Kaldor, Nicholas;  Keynes, John Maynard. 
Further Reading
Hamouda, O.F. John R. Hicks: The Economist’s Economist. Oxford, UK: Blackwell, 1993. 
Hicks, John Richard. Collected Essays on Economic Theory. Vol. 1, Wealth and Welfare. Cambridge: Harvard University
Press, 1981. 
Hicks, John Richard. Collected Essays on Economic Theory. Vol. 2, Money, Interest, and Wages. Cambridge: Harvard
University Press, 1982. 
Hicks, John Richard. Collected Essays on Economic Theory. Vol. 3, Classics and Moderns. Cambridge: Harvard University
Press, 1983. 
 
Hoarding
 
A natural instinct in humans and some animals, hoarding refers to a behavior in which an organism gathers and
stores up supplies—usually food—to meet future perceived needs. In this context, hoarding may be seasonal or,
in the case of humans, triggered by social or environmental catastrophe or fear of such catastrophe. In humans,
hoarding may also reflect compulsive behavior divorced from real-world circumstances. Such pathology aside,
hoarding is seen by economists as essentially rational behavior by individual human beings, even if the collective
effects can be socially and economically disruptive.
Hoarding has an important role in economics, though it can mean different things in different contexts. The
hoarding of goods such as food and other necessities in anticipation of a catastrophe, such as a hurricane, can
distort the normal functioning of supply and demand dynamics. Not only can it lead to shortages but, by
dramatically increasing the demand side of the equation over a short duration—too short for expansion of

manufacturing or distributing to meet the increased need—mass hoarding can drive up prices, leading to intense
but usually short-term bouts of inflation. Because such hoarding often occurs in the context of a disaster,
governments often impose emergency regulations that make it a crime, or an act subject to civil penalties, to profit
unduly from the situation by price gouging. Retailers may also act to limit the impact of emergency hoarding by
imposing limits on the amount of critical goods individuals are allowed to buy during the emergency. In addition,
during times of war, when certain scarce goods are necessary for defense, governments may impose rationing to
limit the amount of those goods an individual can buy over a certain period of time.
 Money and Gold
Hoarding may also be triggered by economic crisis. Under these circumstances, the items usually hoarded are
money or a commodity, such as gold, that can serve as a substitute for it. The hoarding of money or gold almost
always occurs during times of great economic uncertainty, though there are usually opposing factors for hoarding
one or the other. Gold is frequently hoarded as a hedge against inflation. During periods in which the money
supply is expanding too rapidly—or because social unrest, war, or some other circumstance has led to supply
shortages—prices rise quickly as the value of money declines. For thousands of years, gold has possessed a
value beyond its intrinsic worth as a metal. If individuals expect money to become less valuable, they may hoard
gold as a hedge against inflation, particularly the virulent and rapid form of inflation known as hyperinflation. In
times of inflation, people may also hoard goods as a hedge against future price increases, a process that can
feed upon itself as more and more consumers chase fewer and fewer goods, thereby accelerating the inflationary
process. Such a situation occurred during the oil crises of the 1970s.
While most individuals recognize inflation as an economic threat, fewer understand the dangers of deflation.
Indeed, in some ways, deflation can be more dangerous for an economy than inflation, as long as the latter is
kept in relative check. During periods of deflation, the value of money goes up as prices go down. At first glance,
deflation sounds like a good thing—commodities, products, and services get cheaper. Indeed, say economists, if it
remains moderate, deflation (like inflation) is not necessarily a terrible thing. However, with deflation, the real
value of debt increases because the debt is denominated in the nominal currency. The real increase in debt can
lead to numerous bankruptcies and an economic crisis.
On the other hand, inflation means that those who owe money can pay it back with funds not worth as much as
they were lent in real terms. While this helps borrowers, it can prove disastrous for lenders and for the economy
generally, since banks and others lenders may freeze credit if they believe every loan will create a loss on their
balance sheets. This, in turn, can choke off the investments businesses are willing and able to make—investments
that would lead to growth and employment. As for deflation, it can also disarm the most important tool central
banks have for lifting an economy out of recession—that is, lowering interest rates—since deflation may render
nominal positive interest rates negative in real terms.
As the economy shrinks in an economic downturn, consumers stop spending and start putting money into savings
as a hedge against future needs. During such periods, if there is no deposit insurance, there may be a common
perception that banks and other financial institutions are insecure places to put money. If lending institutions are
no longer able to offer attractive returns on deposited money, this further encourages customers to take their
money out of banks—sometimes quite suddenly. Banks, of course, traditionally earn most of their profit on the
spread between the lower interest rate they pay depositors and the higher interest rate they charge lenders. If
they do not see profitable lending opportunities, however, then they cannot pay depositors any return and may
eventually find themselves insolvent. Moreover, if interest rates are very low, money may just be “hoarded” as it
sits in a checking account where it earns no interest and does not serve as a basis for new lending.
Such situations lead to hoarding, that is, keeping money under the proverbial mattress in a noncirculating,
nonperforming state. Individuals may also hoard money in anticipation of further drops in prices, making it cost-
effective to hold onto money and buy nonessential goods in the future. Such actions, if widespread, can drive
down aggregate demand and starve the economy of funds needed for lending and investment.

This was the situation that gripped the United States and many other industrialized economies in the early years
of the Great Depression and led British economist John Maynard Keynes to develop his critique of the equilibrium
theory of classical economics. According to the latter view, the forces of supply and demand always lead to an
equilibrium of high output and low unemployment. Keynes, instead, argued that aggregate demand can fall so
precipitously—in part, because of hoarding—that an equilibrium may be reached in which output is low and
unemployment is high. Keynes then argued that only the government has the capacity in such a circumstance to
lift aggregate demand by the central bank pumping money into the economy or through fiscal policy, which
Keynes preferred.
 “Cornering Markets”
Finally, hoarding can also refer to the effort by investors to capture a market in a critical commodity. The idea here
is that, by buying up all or most of some commodity and hoarding it for a time, a supplier of goods or an investor
in them can corner the market on a particular item, thereby driving up its price and gaining a windfall profit. Efforts
to corner a market can be successful in local situations or in exceptional circumstances. In the early 2000s, for
example, energy companies such as Enron were able to create a situation in California’s newly deregulated
electricity market in which they were able to hold back the flow of electrons—in a sense, hoarding it—thereby
compelling utilities and other consumers to pay highly inflated rates for electricity. This cornering effort was
particularly effective since electricity is essential and cannot be effectively stored. Enron ended up generating huge
windfall profits from the episode, though not enough to save the company when its huge debt load and poor
accounting practices drove it into bankruptcy shortly thereafter, in late 2001.
In general, however, efforts to corner markets are usually futile in the long run, for two basic reasons. First,
because cornering the market drives up prices, it spurs those not otherwise economically motivated to make an
extra effort to produce or find the cornered commodity. Second, since cornering a market on a vital commodity can
cause social or economic disruption, it may lead to collective or governmental action to prevent the effort from
succeeding. Such was the case in 1869, when American financiers Jay Gould and Jim Fisk attempted to corner
the gold market, triggering a decision by the federal government to release part of its gold stores for sale on the
open market.
In an 1869 political cartoon, financier Jay Gould (left) attempts to corner the gold market, represented by bulls and

bears in a cage. President Ulysses Grant (rear) carries a bag of federal gold stores released on the open market
to counter Gould’s ploy. (The Granger Collection, New York)
Ulku Yuksel
 
See also:  Confidence, Consumer and Business;  Consumption;  Savings and Investment. 
Further Reading
Burdekin, Richard C.K., and Pierre L. Siklos, eds. Deflation: Current and Historical Perspectives. New York: Cambridge
University Press, 2004. 
Friedman, Milton, and Anna Jacobson Schwartz. The Great Contraction, 1929–1933. Princeton, NJ: Princeton University
Press, 2009. 
Jarrow, Robert A.  “Market Manipulation, Bubbles, Corners, and Short Squeezes.” Journal of Finance and Quantitative
Analysis 27:3 (September 1992): 311–336. 
McLean, Bethany, and Peter Elkind. The Smartest Guys in the Room: The Amazing Rise and Scandalous Fall of
Enron. New York: Portfolio, 2004. 
Renehan, Edward. Dark Genius of Wall Street: The Misunderstood Life of Jay Gould, King of the Robber Barons. New
York: Basic Books, 2005. 
Siklos, Pierre L., ed. The Economics of Deflation. Northampton, MA: Edward Elgar, 2006. 
House Financial Services Committee
 
The U.S. House Committee on Financial Services (or House Banking Committee) is the second-largest committee
in the House of Representatives. It oversees the entire financial services industry, including the securities,
insurance, banking, and housing industries. The committee also oversees the work of the Federal Reserve (Fed),
the Department of the Treasury, the Securities and Exchange Commission, and other financial services regulators.
 History and the Committee System
The House Committee on Financial Services (HFSC) was created in 1865 as the Banking and Currency
Committee, which assumed responsibilities spun off from the Ways and Means Committee (which still has
responsibility for tax, tariff, and revenue raising–related issues). It assumed its current name in 1968. Such
congressional committees are legislative subunits of their respective branches of Congress, and especially since
the early twentieth century they have acted with increasing autonomy. President Woodrow Wilson famously said
that “Congress in session is Congress on public exhibition; Congress in its committee rooms is Congress at work.”
The jurisdiction of the American legislature is so broad that no one can be well versed on every issue, nor does
the legislative process permit time for all representatives to become well versed on the issues underlying each bill
as it is introduced; this is especially true in the House, where terms are only two years. Committees meet to
review legislative matters pertaining to their area of focus. The current structure of congressional committees
dates from the 1946 Legislative Reorganization Act, making the HFSC one of 21 House committees; there are 20

Senate committees and 4 joint committees with members from both branches.
 The HFSC and Its Subcommittees
The HFSC, then, is extremely influential in legislation dealing with the financial services industry, which includes
banking, housing, insurance, and securities, as well as oversight of regulators and federal bodies related to that
industry, including the Securities and Exchange Commission (SEC), the Treasury Department, and the Federal
Reserve. Committee members generally serve on more than one subcommittee— for example, a congressman
might chair the Subcommittee on Oversight and Investigations and also serve on the Subcommittee on Financial
Institutions and Consumer Credit. As of the 111th Congress (2009–2011), there are six subcommittees:
The Subcommittee on Capital Markets, Insurance, and Government-Sponsored Enterprises reviews matters
related to the securities industry and capital markets (as well as bodies like the SEC and the New York Stock
Exchange), the insurance industry apart from health insurance, and government-sponsored enterprises like Fannie
Mae and Freddie Mac (though not Ginnie Mae).
The Subcommittee on Financial Institutions and Consumer Credit oversees financial regulators like the Federal
Deposit Insurance Corporation (FDIC) and the Federal Reserve (Fed), the banking system and its health and
efficiency, and all matters related to consumer credit.
The Subcommittee on Housing and Community Opportunity oversees the Department of Housing and Urban
Development (HUD), matters related to housing, government-sponsored insurance programs, and Ginnie Mae (the
Government National Mortgage Association, a government-sponsored enterprise within HUD).
The Subcommittee on Domestic Monetary Policy and Technology handles domestic monetary policy and those
bodies related to it or that impact it, as well as matters related to currency (including the Bureau of the Mint).
The Subcommittee on International Monetary Policy and Trade oversees international monetary policy and matters
related to international trade and institutions, like the International Monetary Fund (IMF) and the World Bank.
The Subcommittee on Oversight and Investigations conducts the oversight of all matters within the HFSC’s
jurisdiction.
 Housing Bubble of 2002–2007
The previous chair of HFSC, from 2001 to 2007 and thus during the housing boom and the early stages of the
financial crisis, was Mike Oxley (R-ID), best known as the cosponsor of the Sarbanes-Oxley Act of 2002, which
strengthened the oversight of public companies in response to the massive accounting fraud scandals of the first
few years of the century. The HFSC is the body before which investigations into such cases take place; in the
summer of 2002 the HFSC regularly made the news because of the WorldCom hearing. Like other committees,
the HFSC has the power to subpoena individuals to appear to testify before the committee; earlier in 2002, Enron
chief executive officer Kenneth Lay was compelled to appear before both the HFSC and the Senate Commerce
Committee during the Enron hearings that helped inspire Sarbanes-Oxley.
At other times, testimony may be given to the committee in writing. That same summer, Fed chairman Alan
Greenspan submitted written testimony to the HFSC stating that the accounting scandals had done no serious
harm to the “remarkably efficient and productive economy,” and that though they had temporarily undermined
investor confidence, that confidence would be regained thanks to the natural economic health of the country.
 Financial Meltdown of 2007–2008
Barney Frank (D-MA) became the chairman of the HFSC in 2007, having previously been the ranking Democrat
on the committee. Frank has been criticized for his possible contribution, through his leadership on the HFSC, to
the conditions that led to the global financial meltdown of 2007–2008. For instance, he opposed a proposal to

create a new administrative agency within the Treasury Department that would have overseen HUD, Fannie Mae,
and Freddie Mac. Frank’s opposition was on the grounds that the troubles faced by Fannie Mae and Freddie Mac
were exaggerated, and that putting them under greater scrutiny would reduce the availability of affordable housing.
A few years later, of course, those companies were hit hard by the subprime mortgage crisis and in September
2008 put into receivership by the U.S. government.
On the other hand, Frank did show concern with policies that he felt were leading to economic troubles. For
example, he has been critical of the Fed, and was one of the few Democrats to openly criticize Alan Greenspan
during the housing boom. Since Greenspan’s departure from the Fed in 2006 and the subsequent deflation of the
housing market, many economists have joined Frank in assigning some of the blame for the bubble to
Greenspan’s decisions, including an overly expansive monetary policy.
As chairman of the HFSC, Frank was also instrumental in winning passage of the American Housing Rescue &
Foreclosure Prevention Act of 2008 (AHRFPA). Unlike the Troubled Asset Relief Program (TARP), which provided
billions in aid to leading financial institutions that were exposed to mortgage-backed securities, the new legislation
provided mortgage refinancing assistance to homeowners threatened with foreclosures. Like many on the political
left, Frank and members of the Democratic majority on the committee believed that direct aid to mortgagees was
the best way to address the critical issue in the financial crisis—the failure of ordinary borrowers to make their
mortgage payments for a variety of reasons.
Bill Kte’pi
 
See also:  Regulation, Financial. 
Further Reading
House Financial Services Committee:  http://financialservices.house.gov
Weisberg, Stuart E. Barney Frank: The Story of America’s Only Left-Handed, Gay, Jewish
Congressman. Amherst: University of Massachusetts Press, 2009. 
Housing
 
No sector of a nation’s economy is more important than housing. Not only are vast amounts of a nation’s wealth
tied up in housing stock and residential real estate but also housing supports huge industries, including
construction, finance, real-estate sales, home improvement, and consumer durables (such as appliances and
furniture). For most individuals and households, the purchase of a home is the single-largest investment they will
make in their lifetime, and the equity contained in their home represents their greatest asset.
Private homeownership generally is considered to be a positive in most societies, as it is seen as contributing to
economic prosperity and social stability. Not surprisingly, many governments, including that of the United States,
attempt to promote homeownership through a variety of means, including tax breaks, monetary policy, financial
regulation, and transportation policy. In many countries, this has led to a vast expansion of homeownership,
particularly since the end of World War II. Moreover, there is a general political consensus that government should
play a role in homeownership. A minority view, however, holds that government efforts to promote homeownership
can have negative consequences, distorting markets and encouraging individuals for whom renting might be a

wiser economic choice than purchasing homes.
Housing, of course, is a necessity of life; everyone has to live somewhere. But housing also represents an
investment opportunity. Throughout much of modern history, housing has been seen as a sure and steady, though
not particularly rapid, means of amassing wealth. That is because housing prices are related to incomes and
rents. Generally, when homes become too expensive or when the monthly costs of financing a home far outstrip
rents, people refrain from purchasing homes, bringing down their prices. However, when credit becomes loose
(easy to obtain), as it did during the mid-2000s, individuals can pay more for homes than their incomes normally
would permit. During such periods, housing prices tend to rise more rapidly, leading to a housing “bubble” that
inevitably deflates when credit tightens again.
 Types and Value
Housing comes in several varieties. The most common form is the residential home for a single household,
situated on land that is held in fee simple ownership. These may be detached homes, physically disconnected
from other homes, or attached or semi-attached homes, such as townhouses. Condominiums and co-ops
represent another form of housing. Condominiums are housing units that are owned by individual households,
while the common areas, including the land, are owned collectively by all of the unit owners. In a cooperative, all
of the units are owned by a legal entity—typically, a corporation—that is under the ownership of the residents of
the co-op. Another major category of housing is manufactured, or mobile, homes. While these may be situated on
private parcels of land, most are located in mobile home parks, where the mobile homeowner leases land on
which his or her manufactured home sits. A final category of housing—a form of nonprimary residence—is the
time-share. Here, a group of owners collectively purchase a residence, usually a condominium, with their shares
representing the amount of time they may occupy it.
In the United States, these forms of housing together represented about $20 trillion in value in 2010, about 150
percent of annual gross domestic product. Of this total, slightly less than half, or $9.6 trillion, represented equity
held by homeowners, while $10.4 trillion consisted of mortgage debt.
 Finance
As these figures indicate, to pay for their homes, most people must obtain financing. Typically, most households
do not have enough cash on hand to pay the full price of the home, and even when they do, they may choose to
keep some of their wealth in more liquid forms than housing. This requires home purchasers to obtain mortgages,
which banks offer on different terms.
A traditional mortgage—the kind that dominated housing finance until the 1990s—is a fixed rate, thirty-year
mortgage. Usually, the mortgagor must put down 20 percent of the value of the home, which is determined by a
government-licensed assessor, at the time of purchase. The mortgagor makes a monthly payment on the loan,
which remains the same throughout the thirty-year life of the mortgage. That payment covers the interest—which
remains at the same rate for the life of the mortgage—and a portion of the principal. Over time, the percentage of
the payment that goes toward paying the interest decreases, and the amount that goes toward paying down the
principal increases. Most banks offer variations on the traditional mortgage, such as a fifteen-or twenty-year fixed
rate mortgage, which allows the borrower to pay off the mortgage more quickly and usually at a slightly lower rate
of interest, though at the cost of a higher monthly payment.
Beginning in the early 1980s, variable rate mortgages were introduced. With these loans, the interest rate is reset
periodically based on a margin above an index that represents the cost to the lender of obtaining the borrowed
funds. These loans are made at lower initial rates, but the borrower assumes the risk that the interest rate will
adjust upward, increasing the payment, at a later date. Variable rate loans are attractive to home buyers who do
not expect to own the property for an extended period of time and expect to resell the property before the interest
rate adjusts upward.

To encourage homeownership and to increase the number of mortgagors—and, of course, to expand their
markets and profitability—financial institutions, including banks as well as monoline institutions specializing in
mortgage financing—began to develop new, hybrid forms of mortgages in the 1990s, although many did not
become widespread until the 2000s. The most common of these loans is the interest-only adjustable rate
mortgage (ARM). With an interest-only ARM, the mortgagor pays a low initial interest rate for a fixed period of
time, typically one to five years. For that period of time, the mortgagor is required to pay only the interest on the
loan—no payments are made on the principal—allowing for a substantially lower monthly payment. At the end of
the introductory period, two things happen. First, the mortgagor is required to begin paying a portion of the
principal each month. Second, the interest rate adjusts, usually upward, in relation to an index that reflects the
cost of funds to the lender. What this means for the mortgagor is a much higher monthly payment. During the
housing boom of the early and mid-2000s, many of these ARM loans were subprime mortgages, that is,
mortgages offered to borrowers with little or bad credit history—individuals who traditionally would not have been
able to obtain a mortgage.
Another hybrid mortgage that was offered during the 2000s was the Alt-A, or “stated income” mortgage, which
was intended for borrowers who had good credit but could not document their income (such as self-employed
workers and employees who work on commission). The loan was made on the basis of the borrower’s stated
income, with no verification. The number of Alt-A loans increased dramatically during the housing price run-up of
2005–2007, leading many to suspect that these loans were really “liar” loans. These mortgages contributed to the
housing boom and its later bust.
Homeowners, of course, can refinance their homes, and many choose to do so, especially when interest rates are
falling or when the amount of equity in the home as a result of rising value allows the owner to secure a lower
interest rate. In other words, because the mortgagee is taking on less risk—the mortgagor now may be taking out
a loan for 50 percent of the home’s value as opposed to 90 percent—it can offer a lower rate. This offers two
benefits to the mortgagor: a lower monthly payment and/or cash in hand, if the mortgagor decides to take out a
loan for a larger amount than is owed. Such a transaction is known as a cash-out refinancing.
Finally, a homeowner may take out a second mortgage, or home equity loan. That is, as the mortgagor pays
down the principal or as the value of the home rises, the owner’s equity (share of ownership) in the home grows.
Equity represents collateral that a mortgagor can use to borrow more money from a lender. With a second
mortgage, the borrower usually pays a fixed interest rate. But a second mortgage is “second” in more ways than
one. Not only does it come after a first mortgage, but also it is second in line should the mortgagor default. That
is, if a mortgagor cannot make the payment on either of the mortgages, the holder of the first mortgage is first in
line to secure its exposure through seizure of the property. Because of the greater risk involved with second
mortgages, they usually come with a higher interest rate. The same risk to the lender applies to home equity
loans, though in this case, the interest rate usually is tied to an index that reflects the cost of funds to the lender
and requires the borrower to make interest payments as long as he or she owns the property securing the home
equity loan.
Historically, mortgagees, or mortgage originators, such as commercial banks and savings and loan institutions,
held onto mortgages for the life of the loan, earning their profit from the margin between interest payments and
their cost of funds. Of course, this exposes to them default risk. That is, when a mortgagor cannot make his or her
monthly payment, the holder of the mortgage can pursue legal means to seize the property, which the mortgagor
offered as security to obtain the loan in the first place. When the mortgagee moves to seize the property, the
property is said to be in foreclosure. Upon obtaining title to the property, the mortgagee usually sells the property
to recoup whatever funds it can to cover the unpaid loan.
Another risk that lenders face in offering long-term, fixed rate mortgages is the risk that the interest rate will go
up, meaning that the cost of funds to provide the mortgage exceeds the interest rate the lender is earning on the
mortgage. This is called interest rate risk, and the longer the term of the loan, the greater the risk. Thus, it would
be advantageous to lenders to be able to sell the mortgages in secondary markets or to a government agency to
reduce the interest rate risk and to get more funds to originate more mortgages. (Lenders earn income from fees,

such as underwriting and processing fees, that they charge to mortgagors.)
Beginning in the early 1980s, Fannie Mae (Federal National Mortgage Association) and Freddie Mac (Federal
Home Loan Mortgage Corporation), government-sponsored enterprises launched by the federal government,
started buying mortgages from lenders and developing mortgage-backed securities of various types. This allowed
banks and other mortgagees to bundle mortgages and sell them to investors, thereby spreading out the default
and interest rate risk. The idea behind mortgage-backed securities was that this process would lower the level of
exposure and hence the risk taken on by mortgagees. And, as basic economics dictate, where there is a lower
risk, there are lower returns. In other words, by lowering their risk, mortgage originators would lower the returns,
based on interest rates, that they were willing to accept from mortgagors.
 Government Homeownership Policies
The U.S. government actively encourages homeownership. But repurchasing mortgages and securitizing them, as
Fannie Mae and Freddie Mac do, is just one of the many direct and indirect means by which the government does
so. Because most homeowners finance their homes, interest rates are crucial to homeownership. By lowering
those rates through the monetary policy tools at its disposal, the Federal Reserve and other central banks around
the world can make financing a mortgage less costly, thereby expanding the number of individuals who can buy
homes and, as most mortgagors are primarily concerned with the monthly payment they are required to make on
their mortgage, increasing the value of the home that a given borrower can afford.
Tax policy also comes into play in encouraging homeownership. Since the income tax was introduced in 1913, it
has included a home mortgage deduction. That is, a mortgagor can deduct the interest payments made on a
primary residence from his or her taxable income. (The United States is not alone in offering a mortgage interest
deduction, but it offers some of the most generous terms.) In 1986, changes to the tax code enhanced the value
of the mortgage interest deduction, thereby encouraging mortgage lending and hence homeownership. Prior to that
year, the interest paid on all personal loans—from student loans to credit card bills—was deductible. By limiting
the deduction to mortgage interest only, the government influenced borrowing practices, prompting many people to
take out larger mortgages, with greater interest payments, and using the cash saved to pay for other purchases or
pay down other loans.
The U.S. government has been especially active over the past fifty or so years in encouraging homeownership.
Since the 1960s, the government has instituted a number of fair housing laws—for example, forbidding racial
covenants that allow home sellers or real-estate agents to discriminate against individuals to whom they sell or
offer homes. In addition, with the Community Reinvestment Act of 1977, the government put in place penalties on
financial institutions that engage in redlining—that is, limiting or refusing loans in so-called high-risk areas, usually
minority neighborhoods in the inner city.
Finally, there are a variety of indirect means by which governments can encourage homeownership. Chief among
these in the United States is transportation policy. By providing massive financing for limited-access, high-speed
roadways through the Interstate Highway System from city centers to outlying areas in the decades after World
War II, the federal government allowed for the development of new suburbs, where detached, residential homes
were both cheaper and more easily available.
 Economic Impact
Like that other great consumer purchase, the automobile—which generates economic activity in the steel, rubber,
and other industries—housing has a major ripple effect throughout the economy. Mortgages sustain a significant
portion of the financial sector in most countries. There is also the real-estate industry—that is, the marketing and
selling of homes—which accounts for about 500,000 jobs in the United States.
Of course, there is more to housing than finance and sales. The houses people live in have to be built by
someone, as does the infrastructure of streets, utilities, and other services that support those homes. Revenues in

the residential construction industry amounted to about $300 billion in 2007, with the total number of individuals
working in the industry estimated at 800,000, although most economists believe these numbers have come down
significantly as the decline in the housing market has deepened. In addition, housing remodeling remains a major
industry, with about $32 billion in annual revenue and 280,000 workers. Finally, there are all of the furniture and
consumer durables, such as refrigerators and washers and dryers, with which people fill their homes. Electronic
and appliance stores did about $110 billion in sales in 2007, while furniture and furnishing establishments did
about $33 billion in sales.
Beyond these specific industries, housing has a broader impact on national economies and societies. Most
sociologists confirm that homeownership has a positive impact on neighborhoods and communities—that is, when
people have a vested economic interest in maintaining the property value of their homes, they work harder to
ensure that their neighborhoods remain clean and safe. This is one reason why governments encourage
homeownership—particularly in lower-income areas—through the many programs and policies noted earlier.
In terms of the economy, housing equity represents a vast pool of wealth. When housing prices are rising and
equity is accumulating, people tend to spend more and save less because they feel that their financial situation is
more secure. As consumer spending generates about two-thirds of all economic activity in most advanced
economies, the effect of rising house prices ripples throughout the economy. Falling house prices, of course, can
have the opposite effect. Moreover, many economists point out that housing plays a major role in the labor market
as well. Exceedingly high house prices make it difficult for communities to attract workers, while falling house
prices may discourage workers from moving to new locales to find jobs or better-paying work because they fear
losing the money they have invested in their homes. Such phenomena can produce labor market rigidity that
dampens economic growth or recovery. For these reasons, some economists argue that homeownership is not an
unalloyed good for economies and societies and that governments should rethink the incentives they provide to
homeowners and potential homebuyers.
James Ciment
 
See also:  Housing Booms and Busts;  Mortgage Markets and Mortgage Rates. 
Further Reading
Floyd, Charles F., and Marcus T. Allen. Real Estate Principles.  9th ed. Chicago: Dearborn Real Estate Education, 2008. 
Guttentag, Jack. The Mortgage Encyclopedia: An Authoritative Guide to Mortgage Programs, Practices, Prices, and
Pitfalls. New York: McGraw-Hill, 2004. 
Morris, Charles R. The Trillion Dollar Meltdown: Easy Money, High Rollers, and the Great Credit Crash. New
York: PublicAffairs, 2008. 
Shiller, Robert J. Subprime Solution: How Today’s Global Financial Crisis Happened and What to Do About It. Princeton,
NJ: Princeton University Press, 2008. 
Housing and Urban Development, Department of
 
The federal government of the United States has undertaken efforts to provide housing assistance to those with
low incomes since the 1930s. In the 1960s, however, most federal efforts toward housing were brought under the

administration of the newly formed U.S. Department of Housing and Urban Development (HUD). A cabinet-level
agency, HUD was created as part of the Department of Housing and Urban Development Act of 1965 and was
given the broad charter to develop and coordinate U.S. government policy toward housing and urban concerns.
Upon its creation in 1964, HUD took over the operation and coordination of the many existing federal government
housing programs. Thus HUD adopted the missions of the departments that came under its control, such as the
Federal Housing Administration (FHA), which was created in 1934 to provide a uniform system of mortgage
insurance. Since the creation of HUD did not interrupt many of the programs that existed prior to 1965, scholars
often treat the pre-1965 programs as HUD programs, although HUD was not yet in existence.
HUD’s initial focus in the mid-1960s was on three key areas: (1) increasing homeownership rates for all
Americans, (2) assisting low-income renters in finding and affording safe and adequate housing, and (3) improving
the condition of U.S. cities. Fighting racial discrimination would later be added to HUD’s mission as part of the
Civil Rights Act of 1968. Title VIII of the act prohibited any form of housing discrimination based on race, religion,
or national origin, and gave HUD the duty to investigate violations of the law. Finally, in 1987 Congress made
HUD responsible for assisting homeless individuals with housing and support services.
HUD has approached the objective of increasing homeownership in a variety of different ways. Homeownership
was encouraged through the creation of mortgage insurance (first started as part of the FHA in 1934). By insuring
private mortgages against the risk of default, the federal government created an incentive for lenders to extend
more loans to a larger pool of potential homeowners. This likely contributed in part to the homeownership rate’s
rise from almost 44 percent in 1940 to nearly 63 percent in 1970, although a much larger factor was probably the
tremendous increase in household incomes during the 1960s.
Today, HUD still promotes homeownership through its management of the FHA and its mortgage insurance
programs. In addition, HUD has regulatory oversight of Fannie Mae and Freddie Mac, the two government-
sponsored enterprises (GSEs) that exist to help expand homeownership opportunities. Fannie Mae was
established in 1938 to help provide mortgages to low-income households, while Freddie Mac was created in 1970
to broaden the secondary mortgage market and thus increase the amount of funding available for new mortgages.
Assistance for low-income renters was initially provided through public housing. Public housing is housing that is
owned and operated by the government. Prior to the creation of HUD, public housing in the United States was the
primary way that the federal government provided housing assistance to the poor. Beginning with the creation of
HUD, however, the link between low-income subsidies and government ownership began to be severed. Through
the Section 23 Leased Housing Program, HUD began in the early 1960s to lease housing units from private
landlords and rent them out to low-income individuals at a subsidized rate.
The health and vibrancy of U.S. cities was also addressed, in part, through the public housing program. In the
1960s, much of what critics would call “substandard” housing was in cities. Through HUD and its various
departments, grants were provided to cities for so-called slum clearance and redevelopment. Public housing units
for low-income city residents were typically part of the redevelopment. More recently, however, urban
redevelopment programs have become more decentralized and have tried to provide incentives for private
redevelopment in certain areas of cities, typically known as Empowerment Zones.
As HUD is the manager of public housing in the United States, criticism of this public policy often falls on the
department itself. Much of the research investigating the theoretical benefits of such projects has concluded that
the magnitude of provision is probably unwarranted, and perhaps even counterproductive. Many of the health,
environmental, economic, and social problems that public housing was intended to help resolve have been linked
to the nature of public housing projects. In a 2006 ranking of New York City’s ten worst landlords published in The
Village Voice, HUD topped the list. Furthermore, the success stories of urban revitalization programs have been
marred by accusations that they gentrify urban areas, causing the poor to be displaced rather than becoming long-
term beneficiaries.

HUD’s regulatory responsibility over Fannie Mae and Freddie Mac has come under fire for helping cause the 2008
financial panic. In trying to meet its mission of expanding homeownership to minorities and low-income
households, HUD required these GSEs to purchase more of the loans made to subprime borrowers. To do this,
HUD allowed the GSEs to count mortgage-backed securities for subprime loans toward their affordable housing
credits. As a result, the GSEs ended up holding the securities instead of the loans, which was problematic
because of the increased risk associated with these securities. Note that Fannie Mae and Freddie Mac were put
into conservatorship by the federal government on September 7, 2008. Critics have also accused HUD of
inadvertently encouraging lax mortgage lending and underwriting standards, a charge that HUD officials deny.
Joshua Hall and Justin Ross
 
See also:  Fannie Mae and Freddie Mac;  Federal Housing Administration;  Housing;  Housing
Booms and Busts. 
Further Reading
Bachhuber, Jay, and Samara Smith.  “HUD.” The Village Voice, June 27, 2006. 
Department of Housing and Urban Development:  www.hud.gov
Hall, Joshua, and Matt Ryan.  “The Economics of Government Housing Assistance for the Poor.” In Housing America:
Building Out of a Crisis, ed. R. Holcombe and B. Powell. New Brunswick, NJ: Transaction, 2009. 
Leonnig, Carol D.  “How HUD Mortgage Policy Fed the Crisis.” Washington Post, June 10, 2008. 
“63 Years of Federal Action in Housing and Urban Development.” Cityscape: A Journal of Policy Development and
Research 1:3 (1995): vi–ix. 
 
 
Housing Booms and Busts
 
In ordinary economic times, housing represents a relatively staid market, with prices rising steadily but gradually
over the years. Prices are kept in check by incomes—when prices rise too much, people cannot afford to buy
homes—and by rents—if monthly mortgage payments rise too much in relation to rents, people will choose the
latter option for their housing needs. In addition, the housing market has built-in “frictions” (an economic term for
things that slow or hamper business and trade) that limit speculation. That is, buying and selling a house involves
large expenses—broker’s commissions, fees, moving costs—hassles—packing, finding new schools for one’s
children—and sentiments—neighborhood friends, memories—that slow people’s propensity to buy and sell their
homes to cash in on market trends.
But there are periods when housing prices fluctuate dramatically, when speculation in homes and other forms of

residential real estate influences the market more than usual, and when housing valuations go up rapidly and then
fall off suddenly. While there have been a number of housing booms and busts in modern economic history, none
has involved more money and none has been more widespread than the boom and bust that occurred during the
first decade of the twenty-first century. As with any other episode of speculation, economists debate the causes of
this most recent housing bubble, but most agree that excessive financial leveraging was the critical factor. Money
for investment in housing, the argument goes, simply was too cheap and easy to obtain, creating a sudden
upsurge in demand that drove up prices and led to more speculative investment in housing. When credit became
tighter, as is inevitable, prices dropped just as rapidly.
 Historical Ups and Downs
With its abundant land, rapidly growing population, and entrepreneurial spirit, the United States has experienced
many speculative real-estate episodes in its history. Indeed, many of the great booms and busts of the country’s
first century of existence—including those of the 1790s, 1810s, and 1830s—were driven largely by people
engaged in the rapid buying and selling of lands on the urban fringes of major cities, in lots proposed for new
towns in the West or for farmland. Inevitably, the booms led to busts. With much of the financing coming from
European and particularly British banks—through the U.S. financial system—the booms inevitably turned to busts
when overseas bankers became concerned about financial leveraging.
By the early twentieth century, with the United States emerging as the world’s largest creditor nation, financial
bubbles were generated by excesses of domestic capital flowing into the housing market. A stunning example of a
speculative bubble occurred in coastal Florida during the 1920s, as middle-class investors from around the
country, prospering from a booming economy, poured capital into residential land in Miami and other growing
Florida cities. Land prices skyrocketed, leading more speculators to move in, and sending prices up further still—
until infrastructure problems, weather catastrophes, and exceedingly high prices began to scare away new
investors, causing prices to plummet.
Still, for all of these episodes, owning one’s own home remained within the financial reach of only a minority of
Americans through World War II. In 1940, just 44 percent of all families lived in houses that they owned. Prior to
and during the Great Depression, mortgages were expensive, often requiring down payments of as much as 50
percent of the price of the home, short repayment periods, and a balloon payment at the end. To expand
homeownership, the federal government established the Federal National Mortgage Association (Fannie Mae) in
1938. The agency bought mortgages from banks, allowing those institutions to offer more mortgages and on easier
terms—often with just 20 percent down, a thirty-year payment period, and no balloon payment at the end. In 1944,
the Servicemen’s Readjustment Act, or GI Bill, offered low-interest loans to veterans for the purchase of homes.
These programs worked: by 1970, homeownership had expanded to 65 percent, even amid a rapidly growing
population, and remained at that level through the early 2000s.
Despite the increasing demand created by the growing prosperity of middle-class families, house prices in the
1950s and 1960s grew steadily but unspectacularly. The median price climbed from about $55,000 in 1950 to
about $80,000 in 1970 (figures in 2009 dollars), a gain of 54 percent, or an average of 2.7 percent per year.
House prices continued to climb steadily over the next few decades, reaching $150,000 in 2000, a rise of about
87 percent, or 2.9 annually. This steady rise, however, disguises a number of spikes in the mid-1970s and mid-to
late 1980s, as well as some significant declines, including those associated with the high interest rates and
recessionary economy of the late 1970s and early 1980s and the economic downturn of the early 1990s.
However, during the period from 2000 to 2007—the height of the most recent housing bubble—the median home
price rose from $150,000 to $266,000.
 Origins of the Housing Boom
A number of factors, originating in both the public and private sectors, explain this spectacular run-up in prices. All
of these forces contributed to a significant increase in financial leveraging—that is, a huge increase in the amount

of credit available for home financing and in the amount of borrowing—which led to a dramatic drop in the cost of
that credit. With so much cheap money pouring into the housing market, economists argue, it was all but
inevitable that home prices would rise quickly and dramatically.
The origins of the housing boom and bust of the 2000s go back roughly forty years to the federal government’s
decision in 1968 to turn Fannie Mae into a private shareholder-owned, government-sponsored enterprise. At the
same time, the government created the Government National Mortgage Association (Ginnie Mae), a government-
owned corporation that continued to guarantee mortgages for new and low-income homebuyers. This assured
those holding the mortgages that they would be paid their principal and interest, allowing issuers to offer
mortgages on better terms.
In 1970, the federal government created a new government-sponsored enterprise, the Federal Home Loan
Mortgage Corporation (Freddie Mac), to provide competition for Fannie Mae. Around the same time, Fannie Mae
expanded the range of mortgages it could buy. Then, in 1977, Congress passed the Community Reinvestment
Act, which penalized financial institutions that failed to provide mortgages in low-income, minority neighborhoods.
All of these actions were designed to increase the number of homeowners, especially among those sectors of the
population—the working class and minorities—who previously had been unable to obtain mortgages. Together,
these agencies helped expand the secondary mortgage market, including the market for bundled mortgage-
backed securities. Over subsequent decades, both Fannie Mae and Freddie Mac expanded the types of
mortgages they bought and marketed as securities, including, by the 1990s and 2000s, mortgages given to
borrowers with less than sterling credit. These would come to be known as subprime mortgages.
The secondary mortgage market is critical to housing around the world because it allows mortgage originators,
such as banks, savings and loan institutions, and monoline companies (those specializing in mortgage lending), to
offer more mortgages by selling existing ones. It also ensures that the risk inherent in the primary mortgage
market—specifically, default and foreclosure risk—is spread among a broad array of investors, lowering the risk
for the originator and allowing mortgagees to offer loans with lower interest rates and more affordable terms.
In the 1990s and 2000s, however, successive presidential administrations—that of both Democrat Bill Clinton and
Republican George W. Bush—pursued policies aimed at expanding homeownership. Fannie Mae and Freddie Mac
began to ease credit requirements on the mortgages they bought from mortgage originators, including many
subprime and adjustable rate mortgages (ARMs). The latter offered low initial, or “teaser,” interest rates, as well as
interest-only payments, that expired after a certain period of time, whereupon the rate would become adjustable,
usually in relation to some kind of index reflecting the cost of funds to the lender. With an ARM loan, if the lender’s
costs of funds go up, the interest rate charged to the borrower (and hence the mortgage payment) also will rise.
This lowering of standards by the government-sponsored enterprises had a major impact on both the primary and
secondary mortgage markets. Mortgage originators began to lower their own standards for verifying the
creditworthiness of mortgagors. They began to offer so-called NINA (no stated income, no asset) mortgages,
which required no documentation of the mortgagor’s income or assets, and NINJA (no income, no job or assets)
loans, for which the mortgagor could simply attest that he or she had a job. Mortgage originators, or mortgagees,
felt comfortable doing this because they did not hold on to the mortgages—and with them, the risk of default—for
very long, instead quickly selling them to one of the government-sponsored enterprises or on the greatly expanded
secondary mortgage market. Indeed, the market for mortgage-backed securities exploded in the early 2000s, from
about $60 billion annually in 2000 to $570 billion in 2006, an increase of more than 800 percent. Many of these
mortgage-backed securities were made up of NINA, NINJA, and other subprime mortgages.
The monetary policy of the Federal Reserve (Fed) also helped inflate the mortgage credit markets. To help lift the
economy out of the recession of 2001 and 2002—a downturn triggered by the collapse of technology stock prices
and, to a lesser extent, the terrorist attacks of September 11, 2001—the Fed, under the leadership of Alan
Greenspan, lowered interest rates dramatically by mid-2003. By taking actions to lower rates, the Fed makes it
easier for people to borrow money—for mortgages, among other things—thereby stimulating the economy, albeit
at the risk of spurring inflation. Many economists argued, both at the time and in retrospect, that the Fed kept

rates too low for too long, even after the economy had revived and even after it had become clear that housing
prices were rising too rapidly.
Such criticisms aside, both the Fed’s monetary policy and the loose credit markets were doing what policy makers
wanted: increasing homeownership, which rose from the 65 percent rate that held from the 1960s through the
1980s to 70 percent by the mid-2000s.
 Housing Boom of the Early and Mid-2000s
Meanwhile, housing prices were going through the roof. Between 2000 and 2007, the median home price in the
United States rose from about $150,000 (2009 dollars) to $266,000, a gain of about 77 percent, for an average
year-over-year increase of 11 percent—this at a time when the overall inflation rate was averaging about 3
percent annually. The increases were even more spectacular in “hot” real-estate markets of the Southwest and
Florida. In Los Angeles and Phoenix, home prices nearly doubled between 2003 and 2007; in Las Vegas and
Miami, they more than doubled. The United States was not alone in experiencing a housing price bubble. The
easy credit made possible by the monetary policies of the world’s central banks and the international market in
mortgage-backed securities were pushing up housing prices in markets from Australia to Great Britain to Spain.
The global housing bubble, however, was not universal; there were major exceptions, particularly in countries
where credit standards did not fall and financial leveraging did not increase substantially, including France,
Germany, and—in the aftermath of a massive collapse in property prices in the 1990s—Japan.
But in buoyant markets, like that of the United States, the bubble became self-sustaining. With housing prices
continuing to rise, credit became even easier to obtain and lenders dropped their underwriting standards, giving
out larger and larger loans with smaller and smaller down payments. After all, if house prices continued to rise and
demand remained strong, the holder of the mortgage could sell the property for a profit should the borrower
default. Meanwhile, with low monthly payments—a result of low interest rates or ARMs—homeowners could buy
more and more expensive homes, even beyond what their income justified. And, even if the ARM threatened to
adjust the monthly payment upward, the homeowner could refinance on perhaps even better terms, as now he or
she had more equity.
In the heady days of the U.S. housing boom, which peaked in early 2005, many new homes were sold while still
under construction. Mortgages rates were artificially low, and prices were at an all-time high. (Dave
Einsel/Stringer/Getty Images)
 Housing Bust of the Late 2000s

Of course, house prices could not go up forever, nor could interest rates remain low and credit terms easy.
Economists debate the exact cause of the housing price crash that began in late 2006 and accelerated in 2007
and 2008. (The timing of the crash varied from market to market.) Some contend that it had to do with increasing
interest rates in the mid-2000s, while others argue that it had to do with a weakening of the overall economy and
rising unemployment. Still others point to housing market fundamentals—housing prices in many markets rose so
high that potential buyers were priced out of the market, or they rose too high in relation to rents, making
homeownership less attractive. In either case, the result was lowered demand. In addition, in many areas,
particularly the lower-cost urban fringes of major metropolitan areas, developers had built too many homes,
creating a glut that falling demand could not meet. Contributing to the problem on the exurban fringe was a spike
in gas prices in 2008, which made the long commutes from such areas that much more expensive.
The fall in prices was more precipitous than the rise had been. The median price for a home fell from its peak of
$266,000 in 2007 to about $165,000 by early 2009, a decline of about 38 percent, before recovering somewhat by
the end of the latter year. In particularly frothy markets, the fall was even steeper. In California, the median home
price fell from just under $500,000 at the height of the market in early 2007 to less than $250,000 in early 2009—
a full 50 percent drop, unprecedented in the modern history of the state’s housing market. Thereafter, the market
stabilized somewhat. As of October 2011, the median house price in the state stood at $240,000, down just four
percent from early 2009.
Once housing prices began to fall, the engine that had driven them up in the first place—easy credit terms and
the expanding secondary mortgage market—went into reverse. As credit became more expensive and more
difficult to obtain, fewer people were able to buy homes, causing prices to drop further and wiping out vast
amounts of equity. This, in turn, made it more difficult for homeowners to refinance as the monthly payments on
their ARMs shot up. This forced more homeowners into foreclosure, putting more properties on the market and
exerting further downward pressure on prices, especially as the mortgage holders tried to unload them at prices
below falling market rates. Many homeowners found themselves “underwater”—that is, owing more than their
newly depreciated home was worth, causing many to put their credit standing at risk and walk away from their
homes. By 2009, roughly one-fourth of all homeowners in the United States were “upside down,” owing more than
the value of their property. By mid-2011, the figure stood at about 28 percent.
Meanwhile, in the secondary mortgage market, another crisis emerged. With foreclosure rates skyrocketing beyond
those normally factored into the risk profiles of mortgage-backed securities, investors became wary of these
financial instruments. Because many banks had a lot of mortgage-backed securities in their portfolios, their
financial health was put in jeopardy. And with nobody able to ascertain what these securities were worth, nobody
could ascertain the depth of the problems facing financial institutions. Banks stopped lending to one another,
threatening a global meltdown in the credit markets—the primary reason for the $700 billion federal government
bailout of late 2008. While the bailout stabilized the banks, banks became increasingly hesitant to lend money,
either for mortgages or for business investment. This dearth of credit helped plunge the United States and much
of the global economy into the worst economic downturn since the Great Depression, putting millions of people
out of work in many countries. With no jobs, homeowners could not afford their mortgage payments, leading to
more foreclosures. Overall, foreclosures soared from 1.2 million in 2006, or 1 in 150 homes, to 3.1 million in 2008,
or 1 in 53 homes; in the third quarter of 2009 alone, 937,000 went into foreclosure, or 1 in 136 homes. The pain
was felt strongest in markets where the bubble had inflated the most. In 2009, for example, California and Florida
accounted for no less than 40 percent of all foreclosures.
The epidemic of foreclosures and rapidly falling home prices exerted a major drag on the economy—and not just
because the housing market crash dried up credit throughout the financial markets. Buoyant home prices keep the
economy active, as homeowners tend to spend more when they believe they are enjoying rising equity in their
homes. Homeowners either do cash-out refinancing, allowing them to make purchases, or spend more because
they believe that the rising equity in their homes will allow them to save less for retirement. Indeed, at the height
of the housing market boom, U.S. saving rates were near zero and sometimes even negative, meaning that
people were spending more than they earned, with homeowners financing the difference through low-interest

home equity lines of credit. Overall, the value of residential real estate in the United States fell from $24 trillion in
2007 to about $20 trillion in 2009, while home equity fell from $16 trillion to $9.6 trillion. The difference between
the former and latter figures represents total mortgage amounts.
Real-estate brokers lead prospective buyers on a tour of foreclosed properties in Las Vegas, Nevada, in 2009.
The number of foreclosures skyrocketed—there and across America—as more and more homeowners failed to
keep up with monthly mortgage payments. (Ethan Miller/Getty Images)
 Obama Administration’s Stabilization Efforts
The danger of rising foreclosures to home values and to the housing market generally—and the overall importance
of the housing market to the U.S. economy—prompted the incoming administration of President Barack Obama to
launch a program that would offer government guarantees to lenders that allowed mortgagors owing up to 105
percent of the value of their homes to refinance at lower interest rates, thereby lowering their monthly housing bill.
The program, according to President Obama, would help as many as 4 million homeowners stay in their homes.
But the program was voluntary, lenders proved slow to take up the offer, and the extensive paperwork slowed
down the process, so that only a small fraction of the 4 million were getting relief by the end of the year. In early
2010, the administration proposed a new, more aggressive plan, called the Homeowner Stability Initiative, funded
with $75 billion in bailout money repaid to the federal government by major financial institutions. The
administration estimated that the program could save between 7 million and 9 million American homes from
foreclosure.
Critics of the plan pointed out its flaws and dangers. Some, particularly on the conservative side of the political
spectrum, argued that it was not fair to help some homeowners with their mortgages—especially those who had
taken out mortgages they could not really afford—while leaving more responsible borrowers to subsidize them
with their tax dollars. In addition, some feared that the relief would go to speculators as well as to homeowners
who were threatened with losing their primary residence, increasing moral hazard—the possibility that people will
behave in a more risky fashion in the future if past experience leads them to believe they will be bailed out. To
these arguments, President Obama countered that foreclosures threatened not only the people losing their homes
but also all homeowners as foreclosures put downward pressure on overall housing prices.  Facing criticism that

the requirements for refinancing were too stringent, the Obama Administration offered a new plan in late 2011.
Known as the Home Affordable Refinance Program, it increased the amount an eligible homeowner owed to 125
percent of the value of the home, though again it required that the homeowner be current on all home payments.
The impetus for the program was record low interest rates. Allowing underwater homeowners to refinance at such
low rates would reduce their monthly rates, thereby achieving two key goals—keeping people in their homes and
freeing up money for consumer spending. But the plan faced the same doubt as the 2009 plan: Would lenders go
along and provide the financing?
More progressive critics argued that the rising foreclosure rates were not attributable to excessive mortgage
payments, but to a lack of income. That is, President Obama’s plan to provide mortgage relief in the form of lower
interest rates, lower monthly mortgage payments, and even lower principal would not matter to a person who was
out of a job and could not pay his or her mortgage no matter how much it was reduced. Instead, these critics
argued that the administration needed to be more aggressive in lowering the unemployment rate through tax cuts
and stimulus spending, which would accelerate economic growth. Only then, they said, could the avalanche of
foreclosures be halted, housing prices be stabilized, and the drag that the housing bust was having on the overall
economy be overcome.
James Ciment
 
See also:  Florida Real-Estate Boom (1920s);  Housing;  Mortgage Lending Standards; 
Mortgage Markets and Mortgage Rates;  Mortgage, Subprime;  Real-Estate Speculation; 
Recession and Financial Crisis (2007-). 
Further Reading
Barth, James R. The Rise and Fall of the U.S. Mortgage and Credit Markets: A Comprehensive Analysis of the Market
Meltdown. Hoboken, NJ: John Wiley and Sons, 2009. 
Bitner, Richard. Confessions of a Subprime Lender: An Insider’s Tale of Greed, Fraud, and Ignorance. Hoboken, NJ: John
Wiley and Sons, 2008. 
Fox, Justin. The Myth of the Rational Market: A History of Risk, Reward, and Delusion on Wall Street. New
York: HarperBusiness, 2009. 
Gramlich, Edward M. Subprime Mortgages: America’s Latest Boom and Bust. Washington, DC: Urban Institute, 2007. 
Krugman, Paul R. The Return of Depression Economics and the Crisis of 2008. New York: W.W. Norton, 2009. 
Muolo, Paul, and Mathew Padilla. Chain of Blame: How Wall Street Caused the Mortgage and Credit Crisis. Hoboken,
NJ: John Wiley and Sons, 2008. 
Posner, Richard A. A Failure of Capitalism: The Crisis of’08 and the Descent into Depression. Cambridge, MA: Harvard
University Press, 2009. 
Shiller, Robert J. Irrational Exuberance.  2nd ed. New York: Currency/Doubleday, 2005. 
Shiller, Robert J. The Subprime Solution: How Today’s Global Financial Crisis Happened and What to Do About
It. Princeton, NJ: Princeton University Press, 2008. 
Spotgeste, Milton R., ed. Securitization of Subprime Mortgages. Hauppauge, NY: Nova Science, 2009. 
Thornton, Mark.  “The Economics of Housing Bubbles.” In America’s Housing Crisis: A Case of Government Failure, ed.
Benjamin Powell and Randall Holcombe. Edison, NJ: Transaction, 2009. 
Zandi, Mark M. Financial Shock: A 360° Look at the Subprime Mortgage Implosion, and How to Avoid the Next Financial
Crisis. Upper Saddle River, NJ: FT Press, 2009. 

 
Iceland
 
Iceland, a tiny Scandinavian country of about 300,000 people, situated in the North Atlantic, experienced one of
the most dramatic boom-and-bust cycles in world history in the 1990s and 2000s. The Icelandic boom began in
the early 1990s, as the government ended the post–World War II economic and capital controls that had lasted
from June 1944 independence from Denmark until the 1980s. The boom led to one of the world’s highest levels of
gross domestic product (GDP) per capita ($63,830 in 2007); adjusted for purchasing power parity, this put Iceland
fourth in the world after Luxembourg, Norway, and the United States. Iceland also had one of the highest levels of
economic and political freedom in the world, and the world’s third-highest score on the 2009 Human Development
Index. In 2008, however, the Icelandic economy collapsed, producing a major political crisis.
Residents of Reykjavik, Iceland, take to the streets in 2008 to call on the government to resign and banks to be
more open about the nation’s economic plight. The currency and banking system both collapsed, and the
government resigned in early 2009. (AFP/Stringer/Getty Images)
 Foreign Capital-Driven Boom
Until the 1980s, Iceland’s economy centered on fishing and was notable primarily for its extensive state
involvement; the country was experiencing the beginnings of economic decline by the late 1980s. Under the
leadership of the Independence Party and its leader, Davíð Oddsson, Iceland embraced the global market in the
early 1990s, opening its domestic economy and joining in the European Economic Area. As a result, Iceland’s

economy soared. Oddsson’s government privatized state enterprises and cut subsidies and taxes dramatically
(corporate tax rates dropped from 45 percent to 18 percent, individual rates from over 30 percent to under 23
percent). Iceland embraced European Union (EU) regulatory standards with respect to financial markets, allowing
capital mobility and giving foreign investors confidence in Icelandic assets. Inflation, which had been over 50
percent in the early 1980s, fell to close to zero by the mid-1990s. Unemployment also declined to levels well
below the Organisation for Economic Co-operation and Development (OECD) average in the late 1990s and early
2000s.
This economic liberalization occurred at a time when world interest rates were low and world capital markets were
highly liquid. Thus, when the Icelandic government privatized the country’s banking system in the early 2000s, the
banks were able to borrow heavily abroad. Rising interest rates in Iceland, introduced by the Central Bank of
Iceland to ward off a return to inflation, brought a rapid influx of foreign capital invested in Glacier Bonds, fueling a
soaring Icelandic Stock Exchange. Icelandic bank assets grew from 96 percent of GDP at the end of 2000 to eight
times GDP at the end of 2006, as the banks bought financial firms outside Iceland and established foreign
branches (including Internet banks). Icelandic entrepreneurs such as Bakkavor Group, FL Group, and Baugur—
often styled “Viking Raiders” in the press—used the capital available from Icelandic banks to purchase foreign
assets. For example, among the British firms bought by Icelandic investors were Singer, Hamleys, and easyJet.
Icelanders also bought “trophy assets” such as the British West Ham United soccer team. The boom produced
multiple Icelandic tycoons, including Bjorgolfur Thor Bjorgolfsson, who at age forty in 2007 had assets estimated at
more than $3 billion, and Lydur Gudmundsson and Agust Gudmundsson, who together founded Bakkavor, which
operated 55 factories in 8 countries and was the largest provider of fresh prepared foods and produce in Britain.
 Financial Crisis and Economic Collapse
By 2005, Iceland had the highest current account deficit in the OECD, and, in a “mini-crisis” in 2006 that
foreshadowed later problems, the Icelandic stock market and exchange rate fell sharply, causing inflation to hit 8
percent. The economy briefly recovered when the banks brought in foreign deposits through international
subsidiaries like Icesave, Edge, and Save & Save, and the boom continued until the world financial crisis hit in
2008 and liquidity dried up globally.
By March 2008, markets recognized that the outstanding Glacier Bonds were unlikely to be rolled over and that
Icelandic firms would face a liquidity crisis as the foreign capital invested in the bonds (an amount equal to nearly
100 percent of GDP) flowed out of the country. Anticipating a fall in the krona, Icelandic banks hedged the
currency, which contributed to the krona’s collapse in March 2008. Like the rest of the world, Iceland’s banks and
political system were unprepared for the global credit crisis that began in 2008. When interest rates began to rise
and asset prices to fall, the highly leveraged Icelandic debtors were unable to adequately roll over their loans and
began to come under stress. As the credit crunch drove investors to seek safety, Icelandic borrowers found their
situation rapidly deteriorating.
The political response to the crisis, both in Iceland and abroad, exacerbated the problem. The Icelandic
government moved hesitantly at first, and then suddenly nationalized Glitner, one of the three large Icelandic
banks. This produced an immediate collapse of the krona and stock prices, and it led international credit agencies
to downgrade Icelandic debt. This in turn pushed the other two banks, Landsbanki and Kaupthing, into insolvency
in early October and led to their nationalizations. In desperation, the Icelandic government attempted to secure a
loan from Russia, but this fell through.
 IMF Rescue
The Icelandic government then sought assistance from the International Monetary Fund (IMF) to deal with the
liquidity crisis, but the British government blocked IMF assistance as a means of obtaining bargaining power in
their efforts to force the Icelandic government to guarantee the savings of British depositors in Icelandic banks.
Since these depositors included British local governments’ funds, as well as individuals who had been drawn by

the high returns, the foreign governments had a strong incentive to attempt to shift the problem to the Icelandic
government. British prime minister Gordon Brown even invoked an antiterrorism law against the Icelandic banks,
seizing both Icelandic government and many private assets in the UK. This prompted an Internet petition featuring
“postcards” of ordinary Icelanders holding signs with sentiments like “[Prime Minister] Gordon [Brown], We Are Not
Terrorists.” When the Icelandic government agreed to Britain’s terms, the IMF assistance began to flow. It proved
too little, too late, to save the Icelandic economy, however, and the collapse produced a political crisis that was
only partially resolved with the ouster of the Independence Party, in power for 18 years and steward of the
country’s transition to a finance-oriented economy—and its replacement by a left-of-center coalition of the Social
Democratic Alliance and the Left-Green Movement.
Andrew P. Morriss
 
See also:  Finland;  Norway;  Sweden. 
Further Reading
Boyes, Roger. Meltdown Iceland: How the Global Financial Crisis Bankupted an Entire Country. New
York: Bloomsbury, 2009. 
Lewis, Michael.  “Wall Street on the Tundra.” Vanity Fair, April 2009. 
OECD. Economic Survey of Iceland 2006: Policy Challenges in Sustaining Improved Economic
Performance. Paris: Organisation for Economic Co-operation and Development, 2006. 
Portes, Richard, and Friorik Már Baldursson. The Internationalization of Iceland’s Financial Sector. Reykavijk: Icelandic
Chamber of Commerce, 2007. 
Immigration and Migration
 
A part of the human experience since prehistoric times, immigration and migration have played a major role in the
economic destiny of regions and nations in the modern era—both in places that tend to receive immigrants and
migrants and in those that send them. In the modern era, migration and immigration have tended to flow in two
basic directions—from rural to urban areas and from economically developing countries or regions to developed
ones.
According to the International Organization for Migration, there are more than 200 million international migrants—
that is, people migrating between countries—in the world today, with Europe accounting for more than 70 million
(both sending and receiving), North America for some 45 million, and Asia for another 25 million. But experts say
that the greatest mass migration in history has occurred within a single country, as China has seen more than 100
million people move from rural areas to urban ones since the advent of economic modernization in the late 1970s.
 Causes
While migration and immigration can be induced by noneconomic factors such as war, social chaos, environmental
degradation, and political, ethnic, or religious persecution, large-scale, sustained movement tends to be rooted in
economics. And here, there are both push and pull factors, specifically, the lack of opportunity in the sending
region or nation and the perceived economic vitality of the receiving place. (Slave trading, while usually economic

in nature, was, of course, an involuntary form of migration or immigration and is therefore outside the scope of this
article.)
Throughout much of human history, immigration and migration have tended to be either local or to have taken
place gradually, over long spans of time. This was due to the technological limitations of pre–industrial revolution
transportation systems and the slow pace of economic change. In the modern era, improved transportation
systems as well as the faster pace of economic change have greatly accelerated the numbers of people who
migrate and immigrate as well as condensing the time frame in which they do so. As immigration scholars have
long noted, socially disruptive economic developments—for good and bad—are a more important impetus for
immigration and migration than poor but stable economic conditions. In other words, mere poverty alone is not as
critical a factor in sending people away as is economic transformation.
For example, southern Italy has been an economically disadvantaged region for centuries. But it was only with the
changes economic modernization set in motion—specifically, nineteenth-century changes in agriculture and land
law that made thousands of peasants economically redundant—that large-scale immigration and migration both to
Italian cities and to foreign countries, largely in the Western Hemisphere, began to occur. Similarly, Mexico has
been a less developed nation than the United States for virtually all of these two countries’ histories. And while
there has been substantial immigration from Mexico to the United States for much of the twentieth century, it was
the increased pace of their economic integration in the last two decades of the century—as well as in the early
years of the twenty-first—that greatly accelerated the process, as large numbers of Mexican farmers found it
increasingly difficult to compete with low-cost agricultural imports from the United States.
Over a shorter time frame, economic cycles can have a major impact on levels of migration and immigration. For
example, immigration to the United States stood at about 300,000 annually in the prosperous late 1920s—even
after strict quotas were imposed in the early years of the decade—but fell to less than 50,000 in the depths of the
Great Depression in the early and mid-1930s. More recently, during the economic boom times of the 1990s and
early 2000s, over 1 million immigrants entered the United States each year. In economically troubled 2008,
however, the foreign-born population of the country grew by just half that number. Economic downturns have
particularly harsh effects on immigrants. According to a 2008 Pew Hispanic Center study, tens of thousands of
Hispanic immigrants withdrew from the U.S. labor market since the recession began in late 2007. Between 2007
and 2008, the number of illegal immigrants declined by about 11 percent, from 12.5 million to 11.2 million.
 Impact
Migration and immigration can have positive and negative economic effects on both sending and receiving regions
and countries. For the former, the positive effects can include the relief of population pressures as well as the
input of capital in the form of remittances or investments by immigrants. On the negative side of the ledger is the
loss of productive people. Migrants and immigrants tend to be young, healthy, and ambitious, exactly the kind of
people an economy needs to prosper. Moreover, sending countries have invested in the upbringing and education
of emigrants, investments that accrue to the receiving country. This is especially the case with the phenomenon
known as “brain drain,” in which educated people from the developing world or rural areas immigrate or migrate to
countries and regions where their skills or learning are better remunerated. Shortages of engineers, health care
professionals, financial experts, and other highly skilled and educated people can significantly retard economic
growth and modernization in many developing world countries and regions.
Still, migration and immigration can have a positive effect on the national social ledgers of sending countries and
regions. In places of high unemployment or population density, emigration can relieve social pressures that often
lead to unrest and even civil war. Moreover, modern technology has made it much easier in recent years for
emigrants to send remittances home, or to return home and invest money made while abroad.
The World Bank reports that migrant workers send back $600 billion a year to their home countries worldwide.
This sum can represent up to three times the money sent by governments as overseas aid and by businesses as
foreign direct investment, and is very critical to the developing economies. During the global economic recessions

of 2007–2009, the growth of remittances globally fell to close to zero for the first time since these money flows
have been tracked.
The case of El Salvador illustrates the importance of remittances. Approximately 2.5 million legal and illegal
Salvadoran immigrants, equivalent to more than one-third of the population of El Salvador, live in the United
States and remit an estimated $2.5 billion annually, or 17.1 percent of the country’s gross domestic product
(GDP), to their family members. The remittances have grown at the rate of over 6 percent per year since the late
1990s and as of early 2010 almost one in four households receives money from relatives in the United States, the
most of any Latin American country. Three-quarters of the money goes to paying for household expenditures;
hence, along with a 13 percent sales tax, the remittances in many ways subsidize the Salvadoran government’s
budget.
For other countries, such as Yemen and Gambia, remittances amount to more than 5 percent of GDP. However,
remittances have the greatest overall impact in Asia, with China and India being the top recipient countries. The
Center for Global Development noted that a Mexican male in his mid-thirties with nine years of education is likely
to make 132 percent more working in the United States than in his home country. For a Bolivian and a Haitian,
the increases would be close to 270 percent and 740 percent, respectively.
There are also benefits and liabilities for the countries and regions that take in immigrants. These places reap the
benefits of young, healthy, ambitious workers—as well as skilled technicians and professionals and entrepreneurs
—without having paid for some or all of their upbringing, education, and training. Approximately four in ten PhD
scientists working in the United States, for instance, were born abroad. The Kauffman Foundation’s index of
entrepreneurial activity is nearly 40 percent higher for immigrants than for U.S. natives. In the last three decades,
the Chinese and Indian immigrants have grown in importance as drivers of U.S. innovation. Chinese immigrants
contributed to just 2 percent of the innovations in 1975, and that figure has grown to over 8 percent today, while
the Indian immigrants’ innovation figure has grown to almost 5 percent during the same time period. Still, the
contributions of these two immigrant groups has begun to level off in the past few years, raising concerns about
the ability of the United States to innovate in the future.
While the United States has been a draw for immigrants since its founding, for other developed-world countries
the phenomenon of mass immigration has more recent origins. After World War II, many European countries were
desperate to rebuild and were experiencing labor shortages; many used immigration to continue the pace of
development. Britain, for one, passed the Nationality Act of 1948, which gave all citizens of Commonwealth
countries and colonial subjects the right of unrestricted entry into the United Kingdom and led to a dramatic upturn
in immigration through the rest of the twentieth century and into the twenty-first.
Moreover, immigration can help balance out demographic imbalances. As the population in developed-world
countries ages in coming decades, the immigration of younger workers from the developing world could help
contribute to the tax base necessary to support the generous public pension and health care systems in Europe.
Conversely, many experts say that Japan’s restrictive immigration rules might hamper its ability to fund
pensioners, who could account for as many as one in three citizens by mid-century.
Still, immigration does not come without costs. Many residents of developed-world countries—including already
settled immigrants—resent newcomers, whom they see as competitors, cultural threats, and sources of crime,
though statistical evidence does not support the latter fear, as newly arrived immigrants in the United States tend
to have lower crime rates than the rest of the population. Illegal immigrants are especially seen as a problem,
particularly in the United States, where many people believe—and some studies bear out—that they end up
costing more in terms of education, health care, and criminal justice than they contribute in taxes.
Whatever the costs or benefits—and in spite of temporary immigrant-reducing downturns such as the financial
crisis and recession of the late 2000s—immigration from the developing world to the developed world, as well as
internal migration from farms to city in such places as China, is expected to continue accelerating into the
foreseeable future, as economic globalization and the increased environmental stresses of climate change spur

tens of millions of people to pick up stakes and move about the planet.
James Ciment and Abhijit Roy
 
See also:  Labor Market;  Wages. 
Further Reading
Borjas, George. Heaven’s Door: Immigration Policy and the American Economy. Princeton, NJ: Princeton University
Press, 1999. 
Ciment, James, ed. Encyclopedia of American Immigration. Armonk, NY: M.E. Sharpe, 2001. 
Council of Economic Advisers. Economic Report of the President,  Chapter 9, “Immigration.” Washington, DC: U.S.
Government Printing Office, 2007. 
Lee, Ronald, and Timothy Miller.  “Immigration, Social Security, and Broader Fiscal Impacts.” American Economic
Review 90(May 2000): 350–354. 
Panayiotopoulos, Podromos. Immigrant Enterprise in Europe and the USA. New York: Routledge, 2006. 
Parson, Craig A., and Timothy M. Smeeding. Immigration and the Transformation of Europe. Cambridge, UK: Cambridge
University Press, 2006. 
 
Income Distribution
 
Income is the amount of money received by an individual over a given period of time, usually measured in yearly
increments. It includes wages and other forms of compensation to labor, such as tips, commissions, and bonuses,
as well as earnings from the ownership of real and financial assets, including interest payments, dividends, rents,
capital gains, and profits. In addition, income includes government compensation in the form of Social Security,
welfare, unemployment payments, and, in the United States, earned income tax credits. Nations also can be said
to have incomes. A nation’s income is the total of all wages (labor income) and all interest, dividends, rents,
capital gains, and profits (capital income).
Distribution refers to the way in which that income is apportioned among different groups of people within a
particular geographic area, usually a nation. These groups may be based on race, gender, or age, though the
most commonly used distribution group is socioeconomic class. When income is distributed fairly evenly across
socioeconomic groups, a society is said to be more egalitarian; when income disparities between groups are large,
a society is said to be less egalitarian. Income distribution varies widely among countries—and even within
different parts of the same country—and among regions of the world. Within a given country, income distribution
tends to shift over time, sometimes trending toward greater equality and sometimes toward greater inequality.

Income distribution and the business cycle affect one another, although economists disagree over what degree of
inequality is opportune for sustained economic growth.
It is important to note the distinction between income and wealth. The latter refers to the net value of assets
owned by an individual (or a nation) at a given point in time. Wealth can be distributed unevenly as well. In fact,
wealth tends to be distributed more unevenly than income, as discrepancies in wealth represent, in part, the
accumulation of unequal distributions of income over many years and even generations. Other than labor income
such as wages and salaries, income flows such as interest payments, rents, and capital gains result from the
ownership of real or financial assets that are included in wealth.
Economists use two tools to measure income distribution—one graphical and one statistical. The former is the
Lorenz curve, which illustrates how national income (vertical axis) is distributed among households (horizontal
axis). A perfectly straight diagonal line from bottom left to top right would represent a perfectly even distribution of
income, with the degree of sag in the line representing the level of income inequality.
In the following figure, if income is distributed equally, every quartile of households (representing one-quarter of
the population) would receive 25 percent of total income. This is shown by the dotted lines converging on the 45-
degree line in the bottom-left corner. The actual distribution of income is shown by the curved line, which indicates
that the bottom quartile of households receives only about 5 percent of total income, while the top 5 percent of
households receive about 25 percent of total income.
Concept of Income Distribution 
Statistically, income distribution is measured by the Gini coefficient. Here, perfect equality—all people receive the
same amount of income—is represented by 0, and perfect inequality—one person makes all the money—is
represented by 1. Thus, a lower Gini coefficient represents greater income equality, and vice versa. For example,
the Gini ratio for the United States increased from 0.408 to 0.443 between 1997 and 2007, indicating a long-term
increase in income inequality.
 Causes of Income Inequality

Income inequality occurs for a variety of reasons. Primary among these is the matter of birth. Inheriting a great
fortune all but assures an individual of a steady income of interest, dividends, rents, and capital gains. But
entrepreneurial activity may be just as important, as successful entrepreneurs tend to have very large incomes.
For all but the very wealthy, however, income comes largely in the form of wages. Here, several factors come into
play. Higher-skilled occupations tend to pay more because there are fewer workers with those skills, allowing them
to command higher pay. Even within a profession, those with greater talent and skills—or a willingness to work
longer hours—tend to earn more than their colleagues.
While most people would agree that higher levels of skill, talent, and work ethic should be rewarded, other—less
equitable—factors also come into play, such as racial and gender discrimination and, of course, the accident of
birth. Even excluding the very wealthy—who derive a large proportion of their wealth from capital income—those
who are born into higher-income families are more likely to receive the kinds of education and training that lead to
higher incomes from labor.
As noted earlier, income distribution can be measured across different groups. In the United States, whites tend to
earn more than other ethnic groups—with the exception of some Asian-American groups—and men tend to earn
more than women, although in both cases, the gap has been shrinking gradually over the last several decades. In
addition, people tend to earn more as they get older until, reaching retirement, their income falls off as they
become more dependent on capital income from retirement accounts and from Social Security payments.
Income distribution also differs among countries. In general, less developed countries have greater levels of
income inequality because there are fewer professionals and skilled workers per capita, and because income
derived from wealth tends to be distributed even more unequally. But even within the developed world, income
distribution varies widely. Income redistribution policies are the chief reason this is so. While the market may
decide that the heir to a great fortune deserves a much greater income than the hardworking offspring of a manual
laborer, society may disagree and institute policies to redress the inequality through such means as progressive
taxation, educational subsidies, and social welfare payments. Thus, countries with general social welfare programs
and highly progressive income taxes tend to see income distributed more evenly than those without.
At the same time, as American economist Arthur Okun argued in his classic 1975 work Equality and Efficiency, a
dollar taken from the rich does not always translate into a dollar received by the poor. The “leaky bucket,” as Okun
called it, means that increasing income equality often comes at the price of economic efficiency and lower overall
national income because it may lead to lowered incentives to work and earn money—as more of it is taken in
taxes—and contributes to labor shirking, as people accept a lower standard of living from unemployment or social
welfare benefits but more leisure time.
Income distribution also can change over time in a given country. For example, the United States saw income
distribution begin to equalize during the first few decades after World War II, a result of rising education levels,
government social welfare policies, strong labor unions, and a steeply progressive income tax. By the mid-1970s,
however, the trend had begun to reverse, with rising levels of income inequality. This, most economists agree,
resulted from lowered taxes on the wealthy, less government assistance to poor families, a rise in the number of
households headed by single mothers, and a growing discrepancy in the level of income earned by college-
educated versus non-college-educated workers.
The last factor can be accounted for, say many economists, by globalization. As manufacturing moved increasingly
to the developing world, this contributed to a decline in manufacturing jobs in the United States—jobs that paid
relatively well but did not require high levels of education. But globalization was not the only factor. Important, as
well, was the decline in unionization rates; rising levels of immigration, which brought in more low-wage labor
competition; and the computer revolution, which increased the number of jobs requiring higher levels of education.
 Income Distribution and the Business Cycle
Economists vigorously debate the impact of income distribution on the business cycle, and vice versa. While all

agree that extremes of income distribution (too much equality or too much inequality) have a deleterious effect on
economic growth, they do not agree on what level of inequality is optimal. Liberal economists in the Keynesian
tradition argue that because those who earn lower incomes tend to spend more of their income, greater income
equality creates greater aggregate demand, leading to higher levels of economic growth and more economic
stability. Thus, they advocate measures and policies that tend to make income distributions less unequal, such as
income redistribution and progressive income taxes. Conservatives, on the other hand, argue that government
policy should emphasize supply-side factors—that is, create the conditions for capital accumulation that allow for
higher levels of investment, which stimulates higher employment and, in turn, contributes to higher wage levels. In
other words, greater income inequality leads to a faster-growing economy, which leads to higher income levels for
all, even if the distribution becomes more skewed.
As for the effect of the business cycle on income distribution, the picture is equally mixed. On the one hand,
periods of rapid growth contribute to greater equality in income distribution, as low unemployment levels give wage
earners a stronger bargaining position in the marketplace, thereby ensuring that more of the national income goes
to wages as opposed to profits, which generally accrue to wealthier individuals. Recessions, on the other hand,
tend to worsen the bargaining positions of wage earners, lowering how much they earn. In addition, recessions are
marked by higher levels of unemployment, further depressing the amount of income earned, especially among
low-wage workers, who often are the first to be laid off.
Of course, periods of economic growth also see increases in corporate profits, which largely accrue to high-
income individuals, while recessions see decreased profits and capital income. Moreover, wages tend to be, in
Keynesian terms, “sticky”—that is, both employees and employers are unwilling or unable, because of contracts, to
lower wages as profits decline. And because higher-income individuals tend to derive more of their income from
interest, dividends, rents, profits, and capital gains than from wages, their share of national income tends to go
down more quickly during recessions than those whose primary source of income is wages. Indeed, this seems to
be the case for the very worst of economic contractions, as income equality in the United States rose during the
Great Depression and, as preliminary findings seem to reveal, rose during the “Great Recession” of 2007–2009 as
well.
In late 2011, the nonpartisan Congressional Budget Office issued a report noting that U.S. income inequality had
grown dramatically between 1979 and 2007. The top one percent of income earners seeing their incomes go up
by 275 percent during that period, while the middle 60 percent of income earners saw theirs go up by less than 40
percent. Indeed, the report said, income inequality in 2007 was at its greatest level since the 1920s. While critics
noted that the 2007 cutoff date exaggerated the gap, since top income earners saw many of their capital gains
shrink as a result the deep recession that began that year, the numbers nevertheless backed up the perception of
a majority of Americans who felt that income disparities were too high in the United States. Income and wealth
inequality was the key grievance of a popular movement—Occupy Wall Street—that began in New York City in
October 2011 and spread to more than 100 cities across the country in the weeks and months that followed.
James Ciment and Derek Bjonback
 
See also:  Poverty;  Wages;  Wealth. 
Further Reading
Frank, Robert H. Falling Behind: How Rising Inequality Harms the Middle Class. Berkeley: University of California
Press, 2007. 
Kelly, Nathan J. The Politics of Income Inequality in the United States. New York: Cambridge University Press, 2009. 
Okun, Arthur M. Equality and Efficiency: The Big Tradeoff. Washington, DC: Brookings Institution, 1975. 
Pontusson, Jonas. Inequality and Prosperity: Social Europe vs. Liberal America. Ithaca, NY: Cornell University Press, 2005. 

Ryscavage, Paul. Rethinking the Income Gap. New Brunswick, NJ: Transaction, 2009. 
Wolff, Edward N. Poverty and Income Distribution.  2nd ed. Malden, MA: Wiley-Blackwell, 2009. 
 
India
 
Located in South Asia, India is the second-largest nation in the world by population, with an estimated 1.2 billion
people. Home to some of the oldest civilizations in human history, it is a multi-ethnic democratic state with a free
market emerging from decades of centralized planning.
Until the eighteenth century, India had one of the most vibrant economies in the world, before being gradually
absorbed into the British Empire from that period through independence in 1947. India’s economy, like that of
many other colonies, was restructured by its European conquerors to serve the needs of the colonizing power. In
India’s case, this meant a gradual weakening of its once-dominant textile industry, which was seen by British
manufacturers as a serious source of competition.
After achieving independence, India’s government pursued a policy common in newly emerging nations in Africa
and Asia of centralized planning, import substitution, and poverty alleviation through subsidies on basic
commodities and other measures. The result of such policies was stability, but slow economic growth. Beginning in
the early 1990s, the country embarked on free-market reforms that, according to many economists, have helped
spur more rapid growth and turned India into one of the most powerful emerging markets in the world.
In 2010, India enjoyed gross domestic product (GDP) growth of 9.7 percent, second only to China’s among major
developing world countries, and surpassed the growth rates of the United States and all other developed countries
in 2010. India’s economic growth, especially in the last two decades, has been remarkable. The nation has gone
through several phases of economic expansion in its history, going back to the Middle Ages and earlier. The
colonization process, which lasted about 200 years, was not beneficial for India, and it emerged weak and
poverty-stricken when it gained independence in 1947. But changes soon followed. Today, the Indian economy is
growing at a significant rate and seems poised for further growth.
It is sometimes assumed that India, prior to colonization, represented a region caught in a population trap that
prevented it from growing economically. The bulk of its population merely subsisted. However, there is evidence
that, prior to the middle of the eighteenth century, southern India was a dynamic hub of economic activity. There
was significant growth in the agricultural sector, with more modest expansion in manufacturing and trading.
According to some reports, the Indian subcontinent accounted for fully a quarter of world manufacturing output in
1750.
Some Indian nationalist writers noted that the nation underwent a process of deindustrialization during the period
of colonization in the late eighteenth and nineteenth centuries. This is said to have occurred in two stages. In the
first stage, the collapse of the Mughal Empire drove down grain productivity, which hurt India’s competitiveness in

terms of manufactured textiles. In the second stage, productivity advances stemming from the adoption of the
factory system in England drove down the price of one of India’s main exports, textiles, which caused further
deindustrialization.
 India After Independence
Independence from the British in 1947 was regarded throughout India as a triumph of the Gandhian strategy of
nonviolence. Yet the fruits of independence were bittersweet, since the freedom was tied to the partition of the
country along communal lines. The Republic of India was established on January 26, 1950, with a bicameral
parliament representing one of the world’s largest electorates. The two legislative houses were the Rajya Sabha,
or Council of States, and the Lok Sabha, or House of the People.
India has witnessed sweeping changes since independence. In 1947, the national literacy rate was only 18
percent, the investment rate about 9 percent of GDP, life expectancy at birth around 32 years, and the annual
economic growth rate about 3 percent. Within 50 years, the nation’s literacy rate had risen to 60 percent, the
investment rate had grown to 30 percent of GDP, life expectancy at birth had climbed to about 63 years, and the
annual GDP growth rate stood at about 8 percent.
At the time of independence, India’s per capita annual income was extremely low (US$95 in 1974 prices).
Achieving economic growth following independence was a priority for political leaders, whose model for success
was the Soviet Union. The result was a reluctance to rely on private entrepreneurship and the free markets.
Instead, India adopted a development strategy that has been referred to as “import substitution industrialization,”
which relied on tariffs and quotas to protect new domestic industry.
In the political sphere, India faced several threats in the initial period after independence. The hostility between
India and Pakistan escalated from the time Pakistan achieved independence, also in 1947. And India entered into
conflict with China during the 1950s over land in its northeast region. The dispute escalated into a full-fledged
attack by Chinese forces in 1962 and came to an end with a Chinese declaration of a cease-fire only after India
appealed to the Western world, particularly the United States, for involvement and military aid.
India’s economic development projects in the 1950s, 1960s, and 1970s were characterized by heavy state
involvement, again following the Soviet model. There was relatively strong agricultural growth, mainly due to the
success of the Green Revolution movement launched in the 1970s, but the public sector seemed to become
increasingly inefficient and tainted by corruption, both political and bureaucratic. Yet, in some sense, the strategy
proved successful. India did establish a growing industrial base, there was a moderate increase in savings, and
from 1950 to 1980, growth rates for real GDP and per capita GDP were 3.7 percent and 1.5 percent, respectively.
Although these were dramatic increases from colonial times, they were extremely weak compared with the
economic performance of other East Asian nations. For the most part, India remained poor and largely dependent
on agriculture, regional inequality mounted, and the global oil crisis of 1973 created inflationary pressures as well
as balance-of-payment difficulties.
While the annual growth rate in the 1950s, 1960s, and early 1970s fluctuated right around 3.5 percent, there was
steady growth from the mid-1970s to the 1980s, and annual increases of about 6 percent between 1980 and
2005. India’s shift to a pro-market economy—heralding the growth it enjoys today—is generally considered to
have begun in the early 1990, but many economists point to the start of real growth somewhat earlier, during the
1980s. Indeed, several major economic reforms were instituted in the 1980s. Although basic restrictions were not
eliminated and the market forces were not set free, the state regulatory apparatus was fundamentally reoriented.
Rather than being aimed at boosting production for the domestic market, the new reforms promoted export
production. This shift in attitude, according to some economists, was responsible for the first spurt of economic
growth, even though the results were somewhat nebulous. India’s economy did show a healthy increase in growth
of 5.6 percent between 1980 and 1985, but the fiscal deficit reached 12 percent of GDP and the current accounts
deficit, as a share of GDP, expanded from 1.7 percent to 3 percent in the latter part of the 1980s. These
macroeconomic problems were occurring at a time of escalating political and social unrest in different parts of the

country and increased tension with Pakistan. Additionally, internal and international borrowing funded most of the
policy reforms, which created a fiscal crisis that left India on the verge of bankruptcy in 1991.
 Globalization
India witnessed sweeping economic changes during the 1990s, with export-led growth seen as the preferred
course for the economy, along with increased foreign direct investments to reduce the trade deficit. At about the
same time, the decline of the Soviet Union and the collapse of communism in Eastern Europe led to a drop in
trade with these markets. Thus, India had no choice but to enter the world market, bringing a marked uptick in the
GDP growth rate during the early 1990s—to 6.7 percent.
The dramatic policy shift and trend toward globalization brought fundamental changes to the structure of the
Indian economy. Among these were the increasing importance of external trade, external capital flows, and a
remarkable growth in the service sector. By the early 2000s, the nation’s information technology (IT) industry had
made huge leaps into the world market, bringing in billions of dollars from foreign firms outsourcing IT jobs. India’s
foreign exchange reserves also grew at a rapid rate, exceeding $100 billion annually by 2004. The economy as a
whole grew at rates of 5.2 percent and 4.6 percent for 2001 and 2002.
And the trend continued. From 2003–2004 to 2007–2008, the share of merchandise trade to GDP increased from
23.7 percent to more than 35 percent; including trade in services, the latter figure was 47 percent of GDP.
Meanwhile, net foreign capital inflows grew from 1.9 percent of GDP in 2000–2001 to 9.2 percent by 2007–2008.
India’s capital markets also flourished, with a strong trend in outbound direct investment flow as Indian capitalists
invested heavily in foreign countries. The largest growth by far, however, was seen in the service sector. This was
partly due to the globalization process and partly due to India’s demographic dividend through a large, young,
well-educated labor force.
White-collar employees enter Technopark, the home of more than forty hardware and software companies in
India’s Kerala state. With Bangalore and other areas, Kerala is a center of the nation’s burgeoning IT service
sector, a source of increasing foreign revenue. (EyesWideOpen/Getty Images)
 Financial Crises of the 1990s and 2000s

Since India’s entry into the global economy, two major crises have rocked the developed and developing world.
The first was the Asian financial crisis of 1997, which affected mostly countries in East Asia. The second was the
subprime crisis that began in the United States and Europe in 2006–2007 and eventually spread to other countries
as well.
In the crisis that swept the so-called Asian tiger nations and others in 1997, exchange rates tumbled, output fell,
and unemployment rates increased—with political instability inevitably ensuing. The crisis was relatively short-
lived, however, and most of the affected countries bounced back relatively quickly. India, in spite of its geographic
proximity to several of the affected countries, escaped relatively unscathed. Its growth rate dipped marginally in
1997, primarily due to domestic factors rather than regional ones. The economy had not yet made the full
transition to exports, which accounted for only about 8 percent of GDP at the time. Moreover, only 13 percent of
its exports were with the Asian countries affected by the crisis. Thus, India’s balance of payments was not greatly
affected by the 1997 downturn in these economies. Capital controls that were still in place helped to shield India
from abrupt changes in short-term capital.
India was much more closely integrated with the global economy by the time the subprime mortgage crisis began
to surface. Initially, at least, India did not seem to be greatly affected by the crisis. As the global financial markets
began to be affected, however, the net flow of capital to India turned negative, as foreign institutional investors
began selling their assets in an attempt to salvage overseas cash balances. By the end of 2008, India’s current
account felt the impact of the slowdown in its exports, a direct result of recessionary trends in developed
countries. In 2008–2009, exports to the United States—India’s largest buyer—fell by 1.6 percent. The impact of
the global recession was relatively smaller on India’s service exports, mainly due to the growth in software and
financial services.
Prior to the crisis, India’s central bank, the Reserve Bank of India (RBI), was more intent on controlling money
supply growth in its attempt to reduce inflationary tendencies in the economy. However, RBI shifted toward an
expansionary policy to deal with the liquidity crunch and near freezing of international credit. Fiscal measures
were also undertaken to deal with the impact of the crisis on the Indian economy by increasing the fiscal deficit,
much like stimulus packages introduced in the United States and elsewhere. The overall balance-of-payments
situation remained quite steady in spite of strains on the capital and current accounts. FDI flows began to increase
in 2008–2009 and crude oil prices were low, in part a function of reduced imports.
The end of 2009 brought good news—a return to economic growth—even though a crisis in the Dubai market in
late 2009 threatened prospects for the 4.5 million Indians living in the Persian Gulf as well as India’s exports to
the region. All in all, India was generally less affected by the financial crisis and global recession of the late 2000s
than most other countries. Reasons included its large and diversified consumption base, a relatively moderate
trade-to-GDP ratio, the relative insulation of India’s financial markets, a healthy balance of external reserves, and
less than complete capital account convertibility. Nevertheless, India continues to face some chronic economic
problems: widespread poverty; economic, regional, and social inequality; lack of infrastructure; poor education for
the masses; corruption; and inadequate employment opportunities for a large and growing labor force.
Sharmistha Self
 
See also:  BRIC (Brazil, Russia, India, China);  Emerging Markets;  Transition Economies. 
Further Reading
DeLong, J. Bradford.  “India Since Independence: An Analytical Growth Narrative.” In Modern Economic Growth: Analytical
Country Studies, ed. Dani Rodrik. Princeton, NJ: Princeton University Press, 2003. 
Grabowski, Richard, Sharmistha Self, and Michael P. Shields. Economic Development: A Regional, Institutional, and
Historical Approach. Armonk, NY: M.E Sharpe, 2006. 

Panagariya, Arvind.  “India in the 1980s and 1990s: A Triumph of Reforms.” IMF Working Paper no. WP/04/43. International
Monetary Fund, 2004. 
Parthasarathi, P. The Transition to a Colonial Economy: Weavers, Merchants, and Kings in South India 1720–1800. New
York: Cambridge University Press, 2001. 
Prasan, A., and C.P. Reddy.  “Global Financial Crisis and its Impact on India.” Journal of Social Science 21:1 (2009): 1–5. 
Indicators of Financial Vulnerability
 
Recent financial crises, such as the Asian currency crisis of 1997–1998, the stock market downturn of the early
2000s, and the global financial meltdown of 2008–2009, have refocused the attention of economists on
determining how to predict future problems. To do this, they examine what are known as indicators of financial
vulnerability. The basic idea is that if economists can identify problematic trends, they can provide early warning
signals to government officials who would in turn implement policy changes so as to avoid a crisis.
As the crisis of 2008–2009 indicated, systemic financial problems can affect any size or kind of economy, from
those in the developing world, to emerging markets, to advanced industrialized economies like the United States.
Nevertheless, emerging market economies tend to be more vulnerable to crises because of their greater reliance
on external funding and other capital inflows for economic growth.
As the premier global institution for dealing with financial crises, the International Monetary Fund (IMF) has
outlined vulnerability factors in government policy, financial sector activities, corporate policy, and household
behavior. The IMF has also set up a system for monitoring the contagion of crises across economic sectors,
tracking whether a country’s fiscal deficit is having an impact on currency exchange rates, whether the country’s
banking sector is vulnerable because it holds a large amount of government debt, and other economic trends.
The four key indicators that the IMF monitors are levels of external and domestic debt; monetary reserve
adequacy; financial sector strengths and weaknesses; and the soundness of corporate finances. External and
domestic debt issues include repayment schedules and interest rate and currency fluctuations. The ratio of
external debt to exports, and of external debt to GDP, are especially helpful indicators of trends in debt and
repayment capacity. The ratio of debt to tax revenue is particularly critical when gauging a country’s repayment
capacity. Indicators of reserve adequacy are instrumental in determining a country’s ability to avert liquidity crises.
Specifically, the ratio of reserves to short-term debt is important in assessing the vulnerability of countries with
significant but uncertain access to capital markets. Strengths and weaknesses in a nation’s financial sector include
the quality of assets, profitability and liquidity, and the pace and quality of credit growth. Other market risk factors,
such as changes in interest rates and exchange rates, are also monitored. Corporate sector indicators pertaining
to leverage, profitability, cash flow, and financial structure are also important considerations.
British economist E. Philip Davis has identified a set of seven generic indicators derived from the theory of
financial instability and empirical studies of financial crisis incidents. His leading indicators approach is used in
predicting turning points of the business cycle, primarily in industrialized countries, and has been effective in
detecting early warning signs of a financial crisis. That is, when a particular indicator exceeds a critical threshold,
a warning signal is given. Davis’s indicators include corporate and household indebtedness relative to assets and
income; prices of equities of various kinds; how much money is in the economy; the health of financial institutions,
as measured by capital adequacy, the amount of nonperforming loans, and other indicators; external financial
indicators, such as trade flows and balance of payments; overall macroeconomic indicators, including GDP growth,

business investment, and inflation; and qualitative social and political indicators, such as easing of financial
regulations, removal of entry barriers to markets, health and coverage of the social safety net, and perceptions of
the government and central bank’s willingness to make sound fiscal and monetary decisions.
In recent decades, great advances have been made in incorporating vulnerability assessments into the financial
surveillance systems. As the global financial crisis of 2008–2009 showed, such indicators are as important for
emerging market economies as they are for developed economies. Early warning system (EWS) models
incorporating the above indicators are used by international financial institutions such as the IMF and World Bank
and by national central banks in predicting the likelihood of impending crises. But these models have their limits.
While they offer a systematic, objective, and consistent method of predicting crises, they have a mixed record of
forecasting accuracy. Economists and policy makers use them alongside other inputs in their surveillance
programs.
But developing-world economies are not the only ones that benefit from effective analysis of indicators of financial
vulnerability, as shown by the failure to foresee recent crashes in asset prices in the United States, and the
degree of economic chaos they can produce. Some economists, for example, were warning of overly inflated
corporate equity prices, particularly in the technology sector, during the late-1990s run-up in stock prices known
as the dot.com bubble. Indeed, it was in reference to this phenomenon that Alan Greenspan uttered his famous
“irrational exuberance” remark in a 1996 speech.
Just over a decade later, however, Greenspan himself was coming under criticism for not heeding the warnings of
some economists, notably Robert Shiller and Dean Baker, that the low interest rate policy of the Federal Reserve,
which Greenspan headed through 2006, was contributing to a housing bubble that was leaving the U.S. economy
vulnerable to a sudden collapse in housing prices. In fact, there were other indicators of financial vulnerability
during the mid-2000s housing bubble beyond overly inflated asset values, including high levels of household
indebtedness and large numbers of questionable loans and securities on the balance sheets of financial
institutions—most notably, collateralized debt obligations. But few in policy-making positions appeared to take
heed of the warnings. Thus, as it became clear during the financial crisis beginning in late 2008, and the deep
recession that accompanied it, indicators of financial vulnerability are only useful if they are heeded.
Abhijit Roy
 
See also:  Asian Financial Crisis (1997);  International Monetary Fund. 
Further Reading
Athukorala, Prema-chandra, and Peter G. Warr.  “Vulnerability to a Currency Crisis: Lessons from the Asian Experience.”
World Economy  25:1 (2002):  33–57. 
Davis, E. Philip.  “Financial Data Needs for Macro-prudential Surveillance: What Are the Key Indicators of Risks to Domestic
Financial Stability?” Center for Central Banking Studies London: Bank of England, 1999. 
Grabel, Ilene.  “Predicting Financial Crisis in Developing Economies: Astronomy or Astrology?” Eastern Economic
Journal  29:2 (2003): 243–250. 
Hardy, Daniel C., and Ceyla Pazarbasioglu.  “Determinants and Leading Indicators of Banking Crises: Further
Evidence.” IMF Staff Papers  46:3 (1999):  247–258. 
International Monetary Fund (IMF).  “Vulnerability Factors.” www.imf.org/external/np/exr/facts/vul.htm

 
Indonesia
 
The fourth most populous nation in the world, with an estimated 240 million people, Indonesia consists of an
archipelago of thousands of large and small islands stretching from west of the Malay Peninsula in Southeast Asia
to the Arafura Sea north of Australia. It is a polyglot nation, with a variety of ethnic, linguistic, and religious
constituencies, though ethnic Javanese and practitioners of Islam predominate. The country’s geographic
expansiveness and cultural diversity have fueled a number of armed separatist movements over the years, though
most have become quiescent in recent times.
A longtime colony of the Netherlands, Indonesia won its independence just after World War II and has been either
democratically or autocratically ruled ever since. Economically underdeveloped during its first decades of
independence, Indonesia is now considered a middle-income country with a significant industrial base and large
oil reserves, though it is still home to many people living below the poverty line.
The Jakarta government has opened up the economy to foreign investment, which has contributed to growth but
exposed the country to fluctuations in the world financial system. Indonesia was hard hit by the Asian financial
crisis of 1997–1998 and has seen its growth rate drop significantly as a result of the global financial crisis and
recession of 2008–2009.

Sales promotion representatives await customers at the Sharia Finance Exhibition in Jakarta, Indonesia, in 2009.
The government launched its first retail Islamic bond to help fund a $6 billion economic stimulus package and
offset a mounting budget deficit. (Adek Berry/Stringer/AFP/Getty Images)
 Economic History
Indonesia’s archipelagic geography stifled political unity until the modern era, though the various islands have
conducted extensive trade with other parts of Asia for thousands of years. Indeed, it was traders who brought the
major religions to the islands—first Hinduism and Buddhism before the common era, and then Islam in the
eleventh century CE.
European explorers began to arrive in the islands—known at the time as the East Indies—in the sixteenth
century, largely in pursuit of exotic and lucrative spices. By the early seventeenth century, the Dutch came to
dominate trade in the islands. At first ruled by a private trading concern, the Dutch East India Company, the
islands were turned into a nationalized colony of the Netherlands at the beginning of the nineteenth century.
Indonesia remained in Dutch hands, aside from a brief Japanese occupation during World War II, until 1949, when
the colonial power was thrown out after several years of armed struggle by nationalist insurgents. Among the
leaders of the independence movement was Sukarno (many Indonesians go by a single name), who served as
the country’s first president from 1945 through 1967. (He declared Indonesia independent in 1945, but the
Netherlands did not concede until four years later.)
Sukarno was a leftist who pursued a statist path to economic development, with the government emphasizing
industrial development for the purpose of economic self-sufficiency. Although he succeeded in establishing a
heavy industrial base, the economy stagnated, with gross domestic product (GDP) standing at just $70 billion by
the end of his reign. Sukarno’s politics had grown increasingly radical, becoming more hostile to the West and
friendlier with China, his rule becoming increasingly autocratic. He was finally ousted in 1965 by Suharto, the head

of Indonesia’s military, who had initiated a bloody, anti-leftist coup two years earlier that resulted in the deaths of
hundreds of thousands of Communists, trade unionists, and others.
Under Suharto, the Indonesia government pursued its “New Order” economic policy. To curb the rampant inflation
of the Sukarno era, Suharto instituted tight fiscal policies and backed a strong rupiah, Indonesia’s national
currency. Aided by the spike in oil and raw material prices in the 1970s—Indonesia also has significant timber,
rubber, and mineral resources—the economy flourished for a time. The dramatic decline in oil prices during the
1980s, however, led to a 20 percent drop in per capita GDP.
To compensate for the lost oil revenues, the Suharto government instituted free-market reforms, opening the
country to greater foreign investment outside the oil sector and promoting tourism. Overall, Suharto’s more open
policies led to economic gains, as per capita GDP climbed to about $1,000 by the mid-1990s.
But there were problems with the Suharto regime as well. Aside from being even more authoritarian than his
predecessor, Suharto presided over an extremely corrupt political and economic system, in which insiders,
including many members of his own family, gained control of strategic businesses and made fortunes siphoning
money from foreign investments. The commercial legal system was flawed as well, making it difficult for people to
enforce contracts and collect debts. And despite economic reforms, there were other distortions in the
marketplace, including a variety of nontariff barriers to free trade, subsidies to inefficient state-owned enterprises,
export restrictions, and domestic subsidies on basic goods. The financial sector was extremely weak, with little
regulatory oversight and a great deal of government corruption leading to the manipulation of banking balance
sheets by politically connected insiders.
 Financial Crises
With the Asian financial crisis that began in Thailand in July 1997, the weakness of Indonesia’s financial sector
soon became apparent. With its huge foreign reserves (largely from petroleum exports) and low inflation, Indonesia
at first seemed immune from the panic in other regional economies that had caused vast outflows of foreign
capital. But the contagion soon spread, sending the rupiah spiraling downward, despite government efforts to
bolster it by raising interest rates. Ultimately, Indonesia was forced to go to the International Monetary Fund (IMF)
for an emergency $23 billion loan, but this did little to stop the rupiah’s devaluation. In all, Indonesia’s GDP fell by
some 13 percent following the onset of the Asian financial crisis.
Meanwhile, as the rupiah fell and the economy tumbled, inflation flared, unemployment rose, and poverty became
widespread. Under IMF dictates, the government tried to put its fiscal house in order by cutting subsidies on food
and fuel, which triggered massive rioting in a number of cities. While angry with the government, many of the
rioters also turned on ethnic Chinese, who controlled much of the nation’s business.
In spring 1998, the economic crisis and political turmoil led to Suharto’s ouster and Indonesia’s emergence as the
world’s third-largest democracy. Under various administrations from the late 1990s through the mid-2000s, the
nation worked to put its economic and legal house in order. The government cut back on subsidies, reduced
government debt, and instituted much-needed regulatory oversight of the financial system, thereby reassuring
foreign investors. All of these reforms contributed to robust economic growth, which averaged between 5 and 10
percent annually. Also assisting Indonesia’s recovery were steadily rising commodity prices through the mid-
2000s, though these hurt the nation’s poor by raising fuel and food prices.
Indonesia was not hit as hard by the global financial crisis of the late 2000s as some other Asian nations, its
economy growing by more than 6 percent in 2008 and more than 4 percent, annually adjusted, in the first quarter
of 2009. Nevertheless, foreign investment fell as investors remained skittish about emerging markets and the
prices of critically important natural resources declined. To counteract the impact of the global financial crisis, the
Indonesian government instituted a $6.9 billion stimulus package in the first half of 2009. The stimulus package
and renewed high growth rates in China and other developing world economies with which Indonesia traded
helped raise GDP growth rates from 4.6 percent in 2009 to 6.1 percent in 2010.

James Ciment
 
See also:  Emerging Markets;  Southeast Asia;  Transition Economies. 
Further Reading
Ariff, Mohamed, and Ahmed M. Khalid. Liberalization, Growth and the Asian Financial Crisis: Lessons for Developing and
Transitional Economies in Asia. Northampton, MA: Edward Elgar, 2000. 
“Indonesia Upgraded: A Brightening Outlook of Indonesia’s Economy.” The Economist, May 22, 2009. 
“Indonesia’s Economy and the Election: So Far So Good.” The Economist, January 8, 2009. 
Ricklefs, M.C. A History of Modern Indonesia Since c. 1200. Palo Alto, CA: Stanford University Press, 2008. 
Taylor, Jean Gelman. Indonesia: People and Histories. New Haven, CT: Yale University Press, 2003. 
Industrial Policy
 
Industrial policy is the set of plans, policies, and programs initiated by a national government to enhance industrial
output and strengthen the economy. The term implies that government should play an active, participatory role in
guiding the direction of a nation’s industrial economy.
In the United States, an earnest debate over a national industrial policy—and whether there should even be one—
began in the early 1980s. Robert Reich, then a Harvard University professor and later U.S. secretary of labor,
advocated the need for a comprehensive industrial policy in his book The Next American Frontier (1983) as did
left-leaning economists Barry Bluestone and Bennett Harrison in their influential and widely read The
Deindustrialization of America (1982). In the aftermath of the 1981–1982 economic recession, Reich and other
industrial policy proponents argued that many of the nation’s most important industries, such as steel, textiles, and
rubber, had failed and that the U.S. economy had been deindustrializing since the 1960s. This, they say, had
resulted in increasing unemployment, mounting business failures, and declining labor productivity. Furthermore,
they argued, other basic industries were no longer competitive in the global economy due to overemphasis by
management on short-term profitability rather than long-term innovation.
Critics of industrial policy, on the other hand, present a less optimistic view. During times of economic crisis, as in
the 1930s, the 1980s, and the financial meltdown of 2007–2008, these economists strongly opposed the bailing
out of troubled companies and industries. They argued, basically, that government only makes matters worse and
that the free market should be left alone to do its job. However painful this may be in the short term, the result in
the long run will be a better, sounder economy.
 Definition
Industrial policy can be defined either broadly or narrowly. The broad definition focuses on the public policies and
private sector strategies that affect the nation’s economic development and international competitiveness. In the
United States, this perspective encompasses large-scale economic policies, labor-management relations,
education and scientific research, production technology, and business and civic cultures. However, because of
the sheer breadth of topics and issues it covers, the broad definition of industrial policy in the United States loses

much of its usefulness as a basis of policy discussion.
A narrower definition is often more useful. From this perspective, industrial policy focuses on measures taken by
the government to improve the country’s economic health through the industrial sector in general or through
specific industries. The narrower definition, which has gained wide acceptance among scholars, business
executives, and public policy makers, focuses on selective government policies that stimulate the development of
newly emerging technology industries; the transference of new knowledge and industry “best practices” to enhance
the competitiveness of mature, slow-growth industries; and efforts to maintain employment and existing
companies in declining industrial sectors. In the United States during the twentieth century, the narrow definition of
industrial policy has found expression in several examples of public policy application. Such policies generally
focus on direct subsidies, tax credits and deductions, and other incentives for a range of industry sectors,
including agriculture, automobiles, steel, telecommunications, and synthetic fuels, with varying degrees of long-
term, economic success.
 International Industrial Policy
Outside the United States, industrial policy has been common practice for many decades in Europe and Asia. In
Great Britain, industrial policies were designed to improve declining productivity and market share of global trade
through coordination between government and industry, consolidation of existing industries, special preference to
domestic firms on government contracts, and direct subsidies and tax credits for declining and emerging
industries. In Germany, the federal government has provided significant subsidies and a guaranteed domestic
market to existing industries, such as coal, steel, and shipbuilding. Moreover, the German government has
supplied a policy “basket” of subsidies, project grants, and tax incentives to emerging biotechnology, computer,
aerospace, and nuclear energy industries. In Japan, the powerful Ministry of International Trade and Industry
(MITI) targeted its post–World War II policies at a number of industries whose advancement was deemed critical to
the nation’s economic success. The Japanese government assisted firms in narrow segments of these targeted
industries in capturing market share through tax incentives, special depreciation rules, government-funded
research assistance, and direct financial subsidies.
 Pros and Cons
Proponents of industrial policy generally believe that national governments should be directly involved in
establishing and achieving national goals for high-growth industries and increasing employment. Relying solely on
the free market and large-scale economic policies, they argue, fails to address the specific problems of important
sectors in American society and does not fully recognize the involvement of foreign governments in international
economic competition. Finally, proponents maintain, if major corporations and key industrial sectors are allowed to
fail, the economic disruption to the American economy will cause panic in the financial markets and raise the
costs of unemployment assistance, employment re-training, and corporate pension bailouts.
Opponents of industrial policy argue that government management of specific industry sectors—that is, picking
“winners and losers”—is a recipe for long-term economic failure. Reliance on “corporate welfare,” in the form of
billions of dollars in short-term aid, will not cure the long-term, structural problems of ailing corporations and
industries. Because politicians and government bureaucrats lack the experience and knowledge to properly
manage private sector organizations, they are likely to channel scarce public resources to inefficient but politically
influential industries, while increasing costs to the beleaguered taxpayer. As economist Charles Schultze,
chairman of the Council of Economic Advisers under President Jimmy Carter, argued in 1983:
One does not have to be a cynic to forecast that the surest way to multiply unwarranted subsidies
and protectionist measures is to legitimize their existence under the rubric of industrial policy. The
likely outcome of an industrial policy that encompasses some elements of both “protecting the
losers” and “picking the winners” is that the losers would back the subsidies for the winners in return
for the latter’s support on issues of trade protection.

Since 1995, international industrial policy has been subordinated to tax, tariff, and trade rules of the General
Agreement on Tariffs and Trade and other free-trade pacts. As a result of the global financial crisis of 2008–2009,
however, the United States, Great Britain, France, Japan, Korea, and other national governments have provided
hundreds of billions in their respective currencies for public bailouts of failing financial sectors, with the U.S.
government also providing multi-billion-dollar direct loans to its flagging auto industry—all public policy decisions
heralding a new era in industrial policy.
Thomas A. Hemphill
 
See also:  Manufacturing. 
Further Reading
Lehne, Richard.  “15 Industrial Policy and High-Tech Industries.” In Government and Business: American Political Economy
in Comparative Perspective. New York: Seven Bridges Press, 2001. 
Johnson, Chalmers, ed. The Industrial Policy Debate. San Francisco, CA: ICS Press, 1984. 
Norton, R.D.  “Industrial Policy and American Renewal.” Journal of Economic Literature 24:1 (March 1986): 1–40. 
Reich, Robert B. The Next American Frontier. New York: New York Times Books, 1983. 
Schultze, Charles.  “Industrial Policy: A Dissent.” Brookings Review 2(October 1983): 3–12. 
 
IndyMac Bancorp
 
The IndyMac Bancorp was the parent company of IndyMac Bank, the largest savings and loan association (S&L)
serving the Los Angeles area, and the seventh-largest mortgage lender in the United States in the mid-2000s.
IndyMac Bank closed down its retail lending and wholesale divisions in July 2008—constituting the fourth-largest
bank failure in U.S. history—and was placed under conservatorship by the Federal Deposit Insurance Corporation
(FDIC). The holding company filed for Chapter 7 bankruptcy. As of mid-2009, IndyMac Federal Bank existed for
the sole purpose of managing IndyMac accounts until they can be redistributed.

Customers line up in front of an IndyMac branch in Southern California after the bank was shut down and placed
under FDIC control in July 2008. The failure of IndyMac, following a wave of mortgage defaults, was one of the
largest in U.S. banking history. (Gabriel Bouys/AFP/Getty Images)
IndyMac was spun off from Countrywide Mortgage Investment in 1997. “Mac” was a contraction of “Mortgage
Corporation,” paralleling the designation Freddie Mac for the Federal Home Loan Mortgage Corporation.
Countrywide itself had been founded in 1985 to collateralize mortgages originating with Countrywide Financial. The
age of the collateralized mortgage obligation—which backed bonds with pools of mortgage debts—had just begun,
and Countrywide was eager to participate. After twelve years as part of Countrywide, IndyMac had come into its
own and was launched as an independent company just as the subprime mortgage market was exploding.
The Pasadena-based bank prospered, operating as both a savings and loan and a mortgage lender, primarily for
residential mortgages. The holding company made a number of acquisitions over the next decade, including SGV
Bancorp, Financial Freedom, the New York Mortgage Company, and the Barrington Capital Corporation. Many
economic analysts, including those at the U.S. Treasury Department, have argued in retrospect that the company
may have been too aggressive in its acquisitions. Moreover, they say, the company was not diligent enough when
it came to ensuring that its investments were safe and its borrowers were able to repay their debts.
By early 2008, IndyMac was reeling from the wave of defaults on home mortgages it had financed—especially
subprime mortgages. Thus, the institution was on shaky ground when, on June 26, 2008, Senator Chuck Schumer
(D-NY) released letters he had written to federal regulators calling into question IndyMac’s ability to remain
solvent. Some commentators have argued that Schumer’s comments sealed the bank’s fate; Treasury Department
officials later concluded that his disclosure was a minor factor compared to the debt IndyMac had accrued in its
acquisitions and the lax lending practices it engaged in to help finance that debt, both of which left it vulnerable
when the housing market collapsed in 2007.
The bank took severe losses throughout the fourth quarter of 2007 and into 2008, as the subprime mortgage crisis
made it impossible to securitize most of IndyMac’s mortgage loans. (Nearly all of them were issued for single-
family residences, with few safer commercial mortgages to balance the risk.) The bank sought large infusions of
capital but proved unsuccessful in finding investors. Nonperforming loans rose 40 percent in a single quarter, and
the company admitted that it expected further losses. Hastening the bank’s demise, the ratings of $160 million
worth of mortgage-backed securities issued by IndyMac were downgraded in April 2008. This brought the bank’s

risk-based capital below the 10 percent required by federal regulations to qualify an institution as “well-capitalized”;
now at the 8–10 percent level, it was designated as “adequately capitalized,” meaning that it would cost the bank
even more to borrow.
Investigators later discovered that, in order to maintain appearances of health, IndyMac had backdated an $18
million contribution from IndyMac Bancorp. The holding company had transferred the money to the bank shortly
before IndyMac disclosed information about its performance in the first quarter of 2008 health; the transaction was
backdated to make it appear the funds had been received during that quarter in order to stay above that “well-
capitalized” minimum. Darrel Dochow, western regional director of the Treasury Department’s Office of Thrift
Supervision (the regulatory agency for S&Ls), had permitted the deceptive backdating and was forced to resign in
February 2009.
In the meantime, beginning in July 2008, depositors rapidly began withdrawing their money from IndyMac
accounts, as bad news about the company began to leak out to the public. Job cuts came just as rapidly, and the
company’s stock price plummeted to 44 cents a share—down from $50 two years earlier. IndyMac’s credit rating
was downgraded to CCC, one of the lowest available without being in default. On July 11, the Federal Deposit
Insurance Corporation (FDIC) put IndyMac Bank in conservatorship and established IndyMac Federal Bank to
manage its assets until they were transferred to the new OneWest Bank in March 2009. Meanwhile, several
lawsuits were filed alleging fraud and other wrongdoing connected with IndyMac’s accounting and lending
practices.
Bill Kte’pi
 
See also:  Banks, Commercial;  Panics and Runs, Bank;  Recession and Financial Crisis (2007-
). 
Further Reading
Krugman, Paul. The Return of Depression Economics and the Crisis of 2008. New York: W.W. Norton, 2009. 
Shiller, Robert J. The Subprime Solution: How Today’s Global Financial Crisis Happened, and What to Do About
It. Princeton, NJ: Princeton University Press, 2008. 
Soros, George. The New Paradigm for Financial Markets: The Credit Crisis of 2008 and What It Means. Jackson,
TN: PublicAffairs, 2008. 
Inflation
 
Inflation refers to an overall rise in the average level of prices of goods and services in an economy over a period
of time. During periods of inflation, currency decreases in value—that is, each unit of currency buys fewer goods
and services. The opposite of inflation is deflation, when the average level of prices falls and each unit of currency
buys more.
Mainstream economists point to the fundamentals of supply and demand as the source of inflation, either through
increased costs of production at the supply end or through increases in aggregate demand that are not
compensated for by an increasing supply of goods and services. Monetarists, however, insist that inflation is

brought on by increasing the amount of money in circulation and that it is a purely monetary phenomena.
In the mainstream view, inflation usually, but not always, accompanies periods of economic expansion because
demand is rising faster than supply. In the monetarist view, however, inflation is a result of government policy, and
thus it is somewhat independent of the business cycle, although governments often expand the money supply as
a means to lift economies out of recession. Economists disagree as to the effects of inflation. Keynesians even
argue that modest inflation can be good for an economy. Regardless, all agree that high rates of inflation are
harmful because they create uncertainty about future prices that discourages saving and investment, among other
effects. In periods of extreme inflation, known as hyperinflation, entire economies can be wrecked, leading to
political and social turmoil.
 Measuring Inflation
Economists use a number of indices to measure inflation. The most widely known measure in the United States is
the consumer price index (CPI), maintained by the Bureau of Labor Statistics, which measures the prices of some
80,000 goods and services. Although this measure is followed closely by economists and policy makers, the CPI
has its faults, as it does not take into account the quality of goods. For example, cars today are far more
expensive than they were twenty years ago, but they are also more reliable and longer lasting. Is the level of
inflation in automobile prices offset, either partially or wholly, by the fact that people now spend less on
maintenance and replacement costs? The CPI does not address such questions. In addition, the CPI includes
products in highly volatile sectors such as food and energy. As a result, the Federal Reserve (Fed), which sets
monetary policy in the United States and thus exerts a major influence on inflation rates, prefers to look at “core
inflation,” or core CPI, a measure that excludes products in the sectors that the Fed has no control over. Thus, the
Fed monitors the core CPI to determine whether inflation in commodity markets (food and energy) is spilling over
to produce inflation in the broader economy.
In addition to the CPI, economists analyze the producer price index, also maintained by the Bureau of Labor
Statistics, which tracks changes in wholesale prices, and the gross domestic product (GDP) deflator, maintained
by the Bureau of Economic Analysis, which tracks price changes in everything that is included in gross domestic
product—a far broader array of goods and services. By using the GDP deflator, economists can determine real
GDP, that is, the growth in the economy adjusted for changes in overall prices. Nominal GDP measures overall
growth, or shrinkage, without taking into account inflation or deflation. Looking at nominal GDP, growth may come
from either higher prices or expanded output. Likewise, changes in real GDP come from an expansion or
contraction of actual output, not just changes in prices.
 Causes
Economists disagree as to the causes of inflation. Mainstream economists argue that inflation is caused by rising
production costs, rising aggregate demand, or both. Production costs may rise for a number of reasons, including
increases in the price of raw materials, the cost of borrowing money, labor costs, energy costs, and other factors.
When these costs rise, producers tend to pass the increase on to consumers in the form of higher prices, in order
to maintain revenue and profits. In classical economics, this reduces the quantity of goods and services
demanded, which, in turn, brings down input prices as supply and demand adjust to a new equilibrium. However,
producers often reduce production as their costs rise, leading to a shortage of supply. If the money supply stays
the same as the supply of goods drops, this produces a phenomenon that economists call “cost-push” inflation.
Conversely, increases in aggregate demand also may cause inflation. If spending by households, businesses,
and/or government increases—and supply fails to keep up—this results in what economists call “demand-pull”
inflation.
Monetarists differ from mainstream economists in that they lay the blame for inflation largely or even exclusively
on monetary policy. Milton Friedman, perhaps the most famous monetary economist, once observed that inflation
“is always and everywhere a monetary phenomenon.” In other words, if governments increase the total amount of

money that people have to spend, entrepreneurs will find that they do not have as many goods to sell as people
can afford to buy. Because entrepreneurs make more profit by raising prices, they will react to increased consumer
spending by raising their prices. As all or most prices begin to rise, the price level increases, and the economy
experiences inflation.
According to monetarists, central banks, such as the Federal Reserve in the United States, cause inflation. Central
banks increase the money supply through open market operations. In the United States, open market operations
refer to the buying or selling of Treasury securities by the Fed. When the Fed buys Treasury securities from the
U.S. Department of the Treasury or from the public, it pays for them using newly created money. This newly
created money enters and circulates in the economy, chasing too few goods and causing prices to rise.
A small minority of economists take a slightly different position on the monetary causes of inflation. They argue
that inflation can be caused by increases in the money supply within the private banking system. This view is
called the endogenous theory—that is, internally created—money by the private sector. An example of this theory
in operation is the housing price bubble of the mid-2000s, which was created, these economists say, by the
financial industry’s overly lenient credit policies that inflated the amount of money chasing the supply of homes.
According to this theory, even if the Fed takes action to decrease the amount of money and credit, the private
sector will find ways around the tightening and continue to increase credit despite central bank restraint. An
example would be banks increasing their use of eurodollar borrowings to fund domestic loans. There are many
other instances in which the private sector has found ways to work around regulations that could restrict lending.
 Inflation and the Business Cycle
Inflation usually occurs in periods of economic expansion, when aggregate demand rises faster than producers
can meet it. In this way, inflation can become self-generating: as prices rise, workers demand wage increases,
which businesses accept because they are earning solid revenues. In other times, however, inflation may
accompany periods of economic stagnation or even contraction. The best-known episode of so-called stagflation
occurred in the 1970s when rapidly rising energy costs slowed the U.S. economy even as they sent the prices of
energy-dependent goods and services upward. As prices rose, so did the demands of labor, furthering the inflation
cycle even as overall economic growth remained anemic. To get out of this vicious cycle, the Fed dramatically
hiked interest rates and tightened credit, setting off a recession so deep that it cooled inflationary pressures as
producers were forced to rein in costs to meet lowered demand and workers resisted calls for higher wages for
fear of risking layoffs.
All economists agree that excessive inflation is bad for the economy. The reason for this is simple: when
households believe that money is declining in value, they lower their saving rate, as the money they put away will
have less value in the future. Businesses feel the pinch, too. Lower saving rates make borrowing money costlier,
discouraging investment. And without investment, production lags, which can fuel inflation. In extreme cases of
inflation, consumers may even begin to hoard goods for fear of extreme price increases, creating shortages that
bring on or worsen the very hyperinflation they fear.
There is less consensus among economists about the positive effects of inflation. Some argue that there are no
positive effects, while others say that inflation can be used to pull economies out of recession. If an economy is
being dragged down by excessive debt, for example, inflation can help reduce that debt load. That is, because
debt usually is paid back over time, the real level of debt goes down as the value of money decreases. Inflation
also raises nominal interest rates, giving central banks the opportunity to lower the rates they charge commercial
banks; lower rates make borrowing cheaper and encourage the kind of investment that helps lift economies out of
recession. Keynesian economists argue that because nominal wages are “sticky”—that is, slow to adjust to
economic changes—inflation allows for the lowering of real wages—that is, wages adjusted for inflation—thereby
bringing labor markets into equilibrium faster than would occur without inflation. Finally, American economist
James Tobin has theorized that because inflation has a greater impact on monetary assets, it encourages
investment in capital goods over financial products, which also can lead to economic recovery.

James Ciment and D.W. MacKenzie
 
See also:  Deflation;  Price Stability. 
Further Reading
Friedman, Milton.  “Unemployment Versus Inflation? An Evaluation of the Phillips Curve.” Occasional Paper no. 44, Institute
of Economic Affairs, 1975. 
Frisch, Helmut. Theories of Inflation. Cambridge, UK: Cambridge University Press, 1983. 
Hayek, Friedrich A. Prices and Production.  1931. New York: A.M. Kelley, 1967. 
Okun, Arthur. Prices and Quantities: A Macroeconomic Analysis. Washington, DC: Brookings Institution, 1981. 
Palley, Thomas I.  “Competing Theories of the Money Supply: Theory and Evidence.” Metroeconomica 45:1 (February
1994): 67–88. 
Phelps, Edmund S.  “Anticipated Inflation and Economic Welfare.” Journal of Political Economy 73:1 (February 1965): 1–
17. 
Phelps, Edmund S.  “Phillips Curves, Expectations of Inflation and Optimal Unemployment over Time.” Economica 34:135
(August 1967): 254–281. 
Samuelson, Paul, and Robert M. Solow.  “Analytical Aspects of Anti-Inflation Policy.” American Economic Review 50:2 (May
1960): 177–194. 
Tobin, James.  “Inflation and Unemployment.” American Economic Review 62:1 (March 1972): 1–18. 
 
Information Technology
 
Information technology (IT), a term first coined in the 1980s, refers to the software and hardware of computer
information systems, either in stand-alone form or linked in networks. Information technology incorporates the
design, development, application, support, and management of computer information systems.
The economic impact of information technology is varied and immense. First, since the widespread adoption of
user-friendly, personal computers in the 1980s and the Internet in the 1990s, information technology has emerged
as a vast economic sector in and of itself. Second, information technology has reshaped the way the world does
business, as there is virtually no industry, no job, and no sector of the economy that has been unaffected by the
revolution in information technology, allowing for enormous gains in productivity, on the one hand, but disruptions
of old business models, on the other.
Finally, information technology has transformed the relevance of economic theory. Information, of course, is crucial

to the decisions made by economic players. Some schools of classical economics presume that buyers and
sellers have access to all pertinent information about transactions before they make their economic decisions. By
amassing more information and making it more widely available, computer technology puts economic theory and
practice more in line with each other.
San Jose, California—seen from a bird’s-eye view, through a fish-eye lens—is the self-proclaimed capital of the
Silicon Valley, the epicenter of the Internet dot.com boom of the 1990s. (David McNew/Getty Images)
 IT Sector
The modern computer era began in the 1940s, with the development of so-called electronic brains, huge and
expensive vacuum tube–driven machines. Transistor technology in the 1950s and integrated circuit technology in
the 1960s made computers smaller and cheaper. By the 1970s, this process, along with developments in
software, allowed for the desktop computer to emerge. Over the course of the 1980s and 1990s, such computers
became ubiquitous in households and businesses throughout the developed world. At the same time, the
development of computer networks and the Internet allowed for the transfer of information from one computer to
another. The first development allowed for huge amounts of information to be amassed, while the latter allowed for
its dispersion.
Defining what constitutes the information technology sector—and therefore its size—is difficult. Moreover, much
economic activity related to information technology occurs within companies that primarily are devoted to other
activities. That is, companies as diverse as automobile manufacturers and coal mining may have large IT
departments. But looking at just one part of the IT sector gives a sense of its overall size. According to the U.S.
Census Bureau, in the late 2000s, computer design and related services—including software development and
support, the design of computer systems, and on-site management and operation of client computer systems and
data processing facilities—generated about $275 billion in annual revenues, or 2 percent of U.S. gross domestic
product (GDP). Most economists, however, take such figures with a grain of salt. The constantly falling cost of IT
infrastructure—as well as the exponential gains in output with each new generation of IT systems—makes it
difficult to measure the relative share of IT in GDP and in productivity.
 Productivity and “Creative Destruction”
Information technology has transformed virtually every sector of the economy. Among the most important effects
has been the increased productivity that IT allows through reductions in coordination costs within companies,

between companies, and between companies and customers. An analysis of just one period of the IT revolution in
the United States—1995 to 2000—reveals the kinds of productivity gains made possible by IT. Many economists
point to these years as the period in which the spread of computers over the previous two decades finally made
its impact on productivity, as companies and workers began to make full use of their potential. Thus, while
productivity gains averaged about 1.35 percent annually between 1973 and 1994, they jumped to 2.87 percent
between 1995 and 2000. Moreover, economists note that industries that incorporated IT more readily—or simply
were more suited to implement IT—made productivity gains far in excess of industries that did not or could not
incorporate it as readily.
One of the industries that made the most of the new technology was finance. By the 1980s, many large
institutional investors had developed complex computer programs to oversee their portfolios and automate their
trading decisions. That is, investors allowed computers to automatically sell or buy shares in response to market
trends. The stock market crash of 1987 made the dangers of that approach apparent, as a sell-off in corporate
equities triggered a cascade of computer-generated selling orders that caused the Dow Jones Industrial Average
to plunge from 2,246 to 1,739—the largest single-day percentage loss in the index’s history. In the aftermath of
the crash, the New York Stock Exchange and other exchanges instituted rule changes to prevent such an
occurrence in the future.
Like all new technologies and the entrepreneurial activity that makes use of them, IT has had a disruptive effect as
well, serving as a major force in what twentieth-century economist Joseph Schumpeter called the “creative
destruction” of capitalism. Online retailing, for example, has had a major impact on traditional “brick and mortar”
stores, particularly in industries with vast arrays of different products, such as bookselling. While major chain
bookstores have seen sales remain flat since the late 1990s, the Internet bookselling giant Amazon.com
increased sales from less than a $1 billion in North America in 2000 to well over $6 billion in 2009. Content
providers also have felt the “creative destruction” of IT. The music industry, for example, saw revenues decline by
roughly one-third between 1999 and 2008, a result, most experts agree, of the free and largely illegal practice of
sharing and downloading, a development made possible by IT. Newspapers have seen such huge losses in
circulation, as readers access their content for free on online, and advertising revenues—as customers stop
buying classified ads and sell their goods and services at online sites such as eBay and Craigslist—that some
experts talk of an industry-wide collapse.
By the 1990s, these transformations had led many investors to conclude that IT had changed the very nature of
modern capitalism. To get in early on the economic revolution promised by IT and the Internet, many began to put
their money into firms developing these new technologies and firms utilizing them to pursue other businesses. By
the end of the decade, vast sums of money were pouring into these businesses, either in the form of venture
capital or purchases of equity shares once the companies became publicly listed, leading to an asset price bubble
in the IT sector. The Nasdaq stock market—where many of these firms were listed—soared from about 800 at the
beginning of 1995 to nearly 4,700 at its peak just after the turn of the millennium, an increase of nearly 600
percent. But the revolution wrought by IT was slower in coming than many had anticipated; traditional market
forces—in the form of poor earnings reports and tighter credit policies implemented by the Federal Reserve—soon
exerted themselves, leading to many IT bankruptcies and a crash in the overall value of IT equities. By mid-2002,
the Nasdaq index had fallen to under 1,200, as the dot.com bubble burst.
 IT and the Markets
More long lasting is the IT revolution’s impact on how financial markets operate. Information, say economists, is
critical to the smooth functioning of the financial marketplace, as buyers and sellers determine how much they will
pay for and charge for financial instruments, respectively. First conceived in the early twentieth century, efficient
market theory—which argues that the prices of traded assets reflect all available information—was given new
credence by the IT revolution at the end of the century. With computers and the Internet, economic agents now
truly had access to all information available, guaranteeing that they would act in a more rational fashion. Because
buyers and sellers had access to all of this information and acted rationally, there was no need for government

regulation, said proponents of the theory. However, in the aftermath of the financial crisis of 2008–2009, many
economists have abandoned the theory, noting that psychology and behavioral habits undermine the notion that
perfect access to information guarantees that economic agents will act rationally. The IT revolution may be a
game changer as far as markets are concerned, these economists said, but it still could not change human nature.
James Ciment
 
See also:  Dot.com Bubble (1990s-2000);  Technological Innovation. 
Further Reading
Cassidy, John. Dot.con: The Greatest Story Ever Sold. New York: HarperCollins, 2002. 
Metz, Tim. Black Monday: The Catastrophe of October 19, 1987, and Beyond. New York: William Morrow, 1988. 
Samii, Massood, and Gerald Karush, eds. International Business and Information Technology: Interaction and
Transformation in the Global Economy. New York: Routledge, 2004. 
Turban, Efraim, Dorothy Leidner, Ephraim McLean, and James Wetherbe. Information Technology for Management:
Transforming Organizations in the Digital Economy. Hoboken, NJ: John Wiley and Sons, 2007. 
Innovation, Financial
 
Financial innovation is generally recognized as a response on the part of the financial industry to regulatory and
environmental challenges. Financial institutions, like other businesses, attempt to respond to challenges to the
environment and regulatory framework. But financial institutions face far greater challenges than most other
businesses, as the financial services industry has traditionally been one of the most heavily regulated in the United
States and most other capitalist economies despite three decades of deregulation.
Many instruments in the financial marketplace that are now regarded as ordinary or standard were in fact the
result of financial innovation. For example, the market for the eurodollar—that is, a U.S. dollar deposited in a
European Bank and a frequently used international currency—was established as a way to circumvent Regulation
Q, instituted in 1933 to restrict the maximum interest rate that a bank could pay on deposits. Similarly, off-
balance-sheet financing/lending (assets or debts not on a firm’s balance sheet) and offshore banking (in which the
branch of a bank is located in a less regulated offshore financial center, such as the Cayman Islands)—now
common practices—developed in response to the tight regulatory capture by federal banking regulators. Indeed, a
great many innovations have been devised by financial institutions in recent times in response to regulatory
challenges and decreasing earning margins. According to some economists, the variety of innovative instruments
and practices provides a strong indication that the current regulatory structure is obsolete and in need of overhaul.
Innovation has been a vital factor in the modern financial system of the twentieth and twenty-first centuries. It
might be said, in fact, that a basic loan is the only true, classical banking product and that all other offerings have
been the result of financial innovation. According to the industry, modern banking is driven by financial innovations
and their ability to meet customers’ needs and fuel the economy at large. Central bankers and financial regulators
take a somewhat more cautious view, pointing out that financial innovations tend to weaken the effectiveness of
monetary policy.

Although the conflict between modern economic reality and rigid, obsolete regulations often gives rise to financial
innovation, the emergence of new instruments and practices also has been triggered by different kinds of
challenges. Changes in the macroeconomic environment—the economy as a whole—induce financial institutions
to seek innovative products and the processes to support them, such as adjustable rate mortgages (ARMs). ARMs
—in which a low initial interest rate readjusts with shifts in a pegged index, such as the federal funds rate, after a
given period of time—were designed, in theory at least, to offer lower rates to first-time homebuyers and those
with poor or little credit history. Likewise, changes in the macroeconomic environment—such as high interest
rates, high inflation, and increases in government deficit financing—often give rise to financial innovations as well.
Indeed, ARMs were first created during a period of high interest rates in the 1990s, to make mortgages more
affordable to middle-income homebuyers.
Technology is another major force that drives innovation. Responding to technological challenges may lead to the
modification not only of financial instruments, but also of delivery channels and the way the financial institutions
relate to the customer. Regulatory environments in particular have been made obsolete by technical innovations
such as electronic funds transfers. Changes in perceived market conditions have also traditionally contributed to
the innovative behavior of financial institutions and the development of financial innovations such as ARMs after
the inflation of the 1970s and early 1980s. Financial institutions, being primarily established as profit-making
entities, are, in a modern economy, market driven. Firms at the micro level design, develop, and launch new or
modified products because they are thought to be more profitable; and as there is no competition, the company
launching them can reap the (usually short-term) benefits of being a market leader. In a competitive market,
market participants are in constant search of new, innovative ways to make greater profits; financial innovations
arise as a result of that drive.
Financial innovations can be classified in a number of categories, and a distinction can be drawn between product
innovations and process innovations. Product innovations appear when a financial institution launches a new
product in the market that bears similarities with existing products but in fact falls outside the current regulatory
capture and opens new market potential for the institution launching it. For example, the financial derivative—a
financial instrument whose value is “derived” from another financial product or index—is related to the much older
futures contract on a commodity, in which an investor agrees to purchase a certain quantity of a commodity in the
future at a fixed price. In both cases, the value of the instrument is derived from something else. Process
innovation, by contrast, changes the way the institution manages an existing financial product or performs a
particular financial process so as to increase efficiency, expand the market, and improve profitability. Process
innovations often focus on transaction costs and how they can be reduced. Debit cards, for example, offer banks,
merchants, and consumers a much less expensive and time-consuming method of paying for goods and services
than the traditional written check.
Other analysts recognize another general category of financial innovation—system or institutional innovations. This
type affects the financial sector as a whole, generating a wave of changes that affects the entire financial sector.
System innovations in the financial industry tend to take hold quickly and become difficult if not impossible for a
single institution to resist implementing.
Some economists have pointed out that a significant financial innovation can provide a shock, or series of
sustained shocks, to an economy. According to Benjamin Graham and David Dodd, a financial innovation can be
understood as a “deviation from the normal patterns.” Although financial innovations generally arise in response to
market imperfections, such as taxes, regulations, moral hazard, and the like, they are not entirely driven by them.
Instead, financial innovations are more directly dictated by the profit-maximizing principle that drives the
institutions as businesses. Concern over transaction costs is often underplayed in the analysis of financial
innovation and of what triggers such costs. Although transaction costs arise in response to regulatory limits, the
ultimate goal of financial institutions is to increase profitability and to control these costs.
Globalization, liberalization, and deregulation, which marked the years of expanded American banking from the
mid-1980s to early in the new century’s first decade, have created a particularly conducive climate for financial
innovation. The demand for a new round of financial regulation after the crisis of 2007–2009 was generally

expected to limit the innovative behavior of banks, who fully expected the regulator authorities to resist new
instruments and processes. This is not to say, however, that financial innovation will come to a stop—only that it
might not be in the forefront for the time being.
Željko Šević
 
See also:  Collateralized Debt Obligations;  Collateralized Mortgage Obligations;  Credit
Default Swaps;  Debt Instruments;  Financial Markets;  Liberalization, Financial;  Regulation,
Financial. 
Further Reading
Allen, Franklin, and Douglas Gale. Financial Innovation and Risk Sharing. Cambridge, MA: MIT Press, 1994. 
Geanuracos, John, and Bill Millar. The Power of Financial Innovation. New York: HarperCollins Business, 1991. 
Graham, Benjamin, and David Dodd. Security Analysis. New York: Whittlesey House, 1934. 
Miller, M.H.  “Financial Innovation: Achievements and Prospects.” Journal of Applied Corporate Finance 4:4 (1992): 4–11. 
Miller, M.H.  “Financial Innovation: The Last Twenty Years and the Next.” Journal of Financial and Quantitative Analysis 21:4
(1986): 459–471. 
Institutional Economics
 
Institutional economics was a school of economic and political thought embraced by U.S. scholars in the early
twentieth century that focused on the role of institutions—social as well as economic—in shaping how economies
operate. Emerging out of the German historical school, it eschewed the theoretical, mathematical modeling of
mainstream classical and neoclassical economics, instead emphasizing that the economic behavior of people and
institutions was rooted in specific historical circumstances. Practitioners of institutional economics looked for
pragmatic solutions to real-life, time-and place-specific economic problems, rather than formulating concepts that,
in theory, applied to all situations. Many institutional economists—among them Adolf Berle and Wesley Mitchell—
took up policy-making or influential positions in government or the private sector.
The three major practitioners of institutional economics were Thorstein Veblen, a founder of the New School for
Social Research in New York City; John Commons, a labor studies pioneer at the University of Wisconsin; and
John Kenneth Galbraith, a widely read and influential writer on American capitalism in the immediate post–World
War II era.
In his most famous work, The Theory of the Leisure Class (1899), Veblen argued that consumer behavior is not
purely rational and utilitarian, but is shaped by social institutions and traditions. Specifically, he said, those with
disposable incomes tend to spend their money in ways that publicly display their wealth. Thus, people might
spend their money on flashy clothes rather than paying the rent, thereby distorting the smooth running of the
economy. Moreover, people might borrow to sustain a particular lifestyle, which could have a negative effect on
financial markets. In making such arguments, Veblen was critiquing the work of the classical economists, whose
mathematical models depended on consumers behaving in predictable ways across class, time, and space.
Moreover, Veblen contradicted mainstream economists—who insisted that government interference in the

economy only distorted markets and led to inefficiencies—by arguing that government had a role in making sure
that more of society’s resources went toward providing for the basic needs of the less fortunate than toward the
“conspicuous consumption” of the well-off and those aspiring to appear well-off.
Not a major economics thinker, Commons is best known for two accomplishments. First is the establishment of
labor history as discipline; in doing so, Commons emphasized that members of the working class were not purely
economic actors, always seeking the highest remuneration for the least possible effort, but rather had other
interests and values—including dignity and security—that were shaped by their involvement in institutions such as
labor unions and factories. Second, Commons put institutional economic thinking into practice by pushing for
government regulations and agencies—and helping to create them—that would provide a counterbalance to the
power of private economic institutions such as corporations. Commons believed that economies could provide the
highest standard of living to the largest number of people when they operated not by purely antagonistic, market-
driven forces, but by negotiation and compromise among institutions, such as corporations, unions, and
government.
Galbraith’s writings focused on the workings of the huge corporations that dominated economic and social life in
the years after World War II. According to Galbraith, the power of these vast institutions was more likely to shape
the operation of the market than vice versa, as classically trained economists insisted. Their advertising shaped
consumer behavior; their control of the production process distorted the normal workings of supply and demand;
and their huge bureaucracies sought long-term stability rather than immediate profit, as mainstream economists
argued.
No longer influential, institutional economic thinking lives on in the work of behavioral economists, who share their
predecessor’s belief that economic agents do not always act in purely rational and utilitarian ways, but often make
economic decisions based on psychological and sociological factors.
James Ciment
 
See also:  Behavioral Economics;  Galbraith, John Kenneth;  German Historical School; 
Veblen, Thorstein. 
Further Reading
Commons, John R. Institutional Economics: Its Place in Political Economy. New York: Macmillan, 1934. 
Galbraith, John Kenneth. The New Industrial State. Boston: Houghton Mifflin, 1967. 
Veblen, Thorstein. The Theory of the Leisure Class: An Economic Study in the Evolution of Institutions. New
York: Macmillan, 1899. 
Yonay, Yuval P. The Struggle over the Soul of Economics: Institutionalist and Neoclassical Economists in America Between
the Wars. Princeton, NJ: Princeton University Press, 1998. 
Insull, Samuel (1859–1938)
 
A celebrated business leader of the 1920s, Samuel Insull built a vast utilities empire that by the end of that
decade was valued at more than $2 billion, making its money by providing electricity to more than 4 million

customers in 32 states. But the assets of the interlocking web of companies he created were over-leveraged and
when the credit markets froze up after the Wall Street crash of 1929, the empire collapsed, wiping out the savings
of millions of small investors and leaving Insull himself impoverished.
Born in London in 1859, Insull went to work for inventor and entrepreneur Thomas Edison’s British representative
at the age of twenty-one. His business acumen soon caught the attention of the famed inventor, who made Insull
his personal secretary and brought him to the United States in 1881. Insull rose through the ranks of Edison’s
wide-ranging business, helping to build power stations around the United States and participating in the formation
of Edison General Electric (later General Electric).
In 1892, Insull left General Electric and Edison’s employ and moved to Chicago, where, taking out a personal loan
for $250,000—then a small fortune—he launched Chicago Edison and purchased one of the city’s many
independent power-generating plants. (Although the company bore the famed inventor’s name, it was not owned
by him.) Unlike in later years when utilities became a highly regulated and stable business, electrical generation at
the dawn of the electrical age was a highly speculative enterprise with lots of competition. Insull prospered
through aggressive acquisition and innovation, of both the technical and entrepreneurial variety. When it became
clear that the alternating current (AC) system pioneered by George Westinghouse was superior to the direct
current (DC) advocated by Edison, Insull abandoned his former employer’s technology and became among the
first to adopt AC for his power systems.
In 1894, he built the Harrison Street Power Station, then the largest in the world. It was a gamble. Electricity, of
course, cannot be stored, so supply and demand have to be evenly matched. Build too large a station and much
of it can sit idle; build too small a station and risk losing customers when the power supply fails. Insull came up
with an idea to make sure that his station ran as close to capacity as possible. By charging lower prices for
electricity at off-peak hours, he could even out usage and lower prices even for peak use hours. To spur demand
further, Insull offered a low-cost home-wiring service and even gave away appliances for free.
Insull realized that economies of scale applied to electricity as they did to other businesses and, from the 1890s
through the 1920s, he built numerous power stations or purchased those of competitors. In 1907, he amalgamated
his largest holdings into a new company known as Commonwealth Edison and, by the middle of the next decade,
possessed a virtual monopoly over the Chicago electricity business. Insull also bought major interests in urban
and inter-urban electric train systems in the Midwest.
By the 1910s and 1920s, Insull was starting or purchasing utilities across the Midwest and around the country,
creating a web of companies linked together into five separate electricity-generating and-distributing systems.
Insull became the major figure in a burgeoning business; between 1919 and 1929, electrical production went from
38.9 million kilowatt hours to 97.4 million. By the latter year, Insull controlled roughly one-eighth of the country’s
electricity-generating capacity.
While Insull’s electricity-generating empire was real, the legal and financial framework for it was of more
questionable value. Insull set up holding companies that owned other holding companies that owned still other
holding companies; there were sometimes four or five levels of ownership between Insull and the firm that actually
supplied electricity to consumers. The reason for this complicated ownership pattern was to get around rules
established by local and state governments to make sure that these natural monopolies did not charge customers
excessive fees for electricity. The strategy worked. By 1930, Insull’s assets were valued at a then-staggering $2.5
billion and “Insullism” became a byword for conglomerate-style corporations.
But to build this empire, Insull borrowed heavily, using the assets of one company as collateral to buy others. This
worked as long as the economy was expanding, credit was free-flowing, and his customer base was growing and
able to pay its bills. But with the stock market crash of 1929 and subsequent economic depression, all of that
came to a halt. Unable to pay his many creditors, Insull saw his empire begin to collapse while he himself was
removed by the boards of many of the various holding companies he had created.

Indicted on charges of embezzlement and investor fraud, Insull fled to Europe in 1932, returned voluntarily to the
United States, and eventually stood trial. Acquitted, he returned to Europe a virtually penniless man and died in
Paris in 1938. To prevent such utilities empires from arising again, Congress passed the Public Utility Holding Act
of 1935, limiting utilities to specific geographic regions and subjecting them more thoroughly to state regulations.
The act was eventually repealed and replaced by a weaker law, the Public Utility Holding Company Act of 2005.
James Ciment
 
See also:  Boom, Economic (1920s);  Corporate Corruption;  Great Depression (1929-1933). 
Further Reading
McDonald, Forrest. Insull. Chicago: University of Chicago Press, 1962. 
Platt, Harold L. The Electric City: Energy and the Growth of the Chicago Area, 1880–1930. Chicago: University of Chicago
Press, 1991. 
Wasik, John F. The Merchant of Power: Samuel Insull, Thomas Edison, and the Creation of the Modern Metropolis. New
York: Palgrave Macmillan, 2006. 
Integration, Financial
 
The term “financial integration” refers to the synthesis or unification of a country’s financial markets with the
markets of other countries, whether located in the same or another geographic region. Financial integration may
therefore be understood as the homogenizing process of a given financial area.
As the global economic crises of recent years have shown, financial integration resulting from the force of
globalization can be a double-edged sword: on the one hand, it can bring great efficiencies into regional financial
operations; on the other hand—as evidenced by the Asian financial crisis of 1997–1998 and the global financial
meltdown of 2008–2009—it can also accelerate the spread of a financial crisis from one country to another.
Financial integration results from the removal of the barriers to financial transactions through deregulation and
privatization (the transfer of companies from public to private ownership), and through the elimination of barriers
preventing foreign institutions from offering cross-border financial services. Financial integration has also been
facilitated through technological progress (especially digital processing and the Internet), which has allowed
financial transactions between countries to be performed almost instantaneously. Moreover, the adoption of a
single currency in economic regions (such as portions of the European Union) has made cross-border transactions
much easier to achieve through increased competitiveness and financial transparency.
In these various ways, financial integration is achieved through the enactment of formal agreements between
countries, as when Chile and Mexico concluded a series of agreements in the wake of the North American Free
Trade Agreement (NAFTA, signed 1994), or when China signed trade agreements with countries gathered at an
Association of Southeast Asian Nations (ASEAN) summit meeting in 2004. Financial integration can also be the
result of less formal arrangements between countries. Even broad coordination of standards, rules, and regulations
provides efficient support for financial integration.

 Europe and the United States
A good example of financial integration can be found in Europe, with the adoption of the Markets in Financial
Instruments Directive (MiFID) in 2004 and its implementation in 2007. MiFID promotes harmonized regulation of
firms dealing in securities markets and offering investment services to clients in other European countries. Thirty
European countries have adopted MiFID. Home countries supervise the standarized regulations. Nevertheless,
MiFID raised a number of concerns—as efforts at financial integration often do—regarding the fragmentation of
markets. According to theory, financial integration tends to increase competition between countries. And indeed,
the greater trade efficiency resulting from MiFID and other integration measures has raised the level of
competition.
In the United States, a greater degree of financial integration emerged with the Riegle-Neal Interstate Banking and
Branching Efficiency Act of 1994. The legislation repealed earlier legal restrictions, namely the McFadden Act of
1927, that prohibited interstate branching—that is, prevented banks from operating in more than one state. The
Riegle-Neal Act allowed lending institutions to acquire other banks or to set up branches in other states. It
promoted financial integration by allowing banks to operate “cross-border” (interstate) accounts. Likewise, the
Gramm-Leach-Bliley Financial Modernization Act of 1999 allowed banks, securities firms, and insurance
companies to merge, thus permitting greater integration of financial services by a single firm. This legislation
repealed the Glass-Steagall Act of 1933, which had separated commercial and investment banking.
 Africa, Latin America, and Asia
African countries have encountered greater obstacles in the pursuit of financial integration. Even the agreements
that they do reach often do not fully deliver their benefits. Political cohesion and stability—which are critical to
financial integration—are all too rare on the African subcontinent. On the positive side, countries in eastern and
southern Africa signed a treaty of economic cooperation in 1993, opening the way toward greater financial
integration. The Southern African Development Community was established in 1996 and the East African
Community in 1999. If falling short of fully integrated common markets, such initiatives held promise for greater
regional cooperation in trade and finance.
Latin America has witnessed a de facto kind of financial integration, primarily as a result of foreign investments
beginning in the 1990s. A liberalization process initiated in the 1990s led to the listing of financial instruments on
foreign stock markets. In the Caribbean, financial integration has taken the form of cross-border ownership
involving both financial and nonfinancial firms in the tourism and leisure industries.
In Asia, a measure of financial integration came as a result of the globalization movement that began in the
1990s, but the extent and effects of the changes are debated. According to some observers, Asian financial
integration lags far behind that of Europe before 1993, relying more on the development of global markets and
foreign investments in the nations of the region. For large-scale financial integration to occur in Asia, many agree,
major political, economic, and social obstacles have to be overcome.
Marc Lenglet
 
See also:  Asian Financial Crisis (1997);  Business Cycles, International. 
Further Reading
Bowden, Roger J., and Vance L. Martin.  “International Business Cycles and Financial Integration.” Review of Economics
and Statistics 77:2 (1995): 305–20. 
Galindo, Arturo, Alejandro Micco, and César Serra.  “Financial Integration.” In Beyond Borders: The New Regionalism in
Latin America. Social Progress in Latin America. Washington, DC: Inter-American Development Bank, 2002. 

World Bank. Global Development Finance: The Role of International Banking. Vol. 1: Review, Analysis and
Outlook. Washington, DC: World Bank, 2008. 
Interest Rates
 
The interest rate is the amount, expressed as an annual percentage, that a borrower pays in addition to the
principle (the amount borrowed in a loan), for the privilege of borrowing money. Generally, the interest rate both
protects against inflation—so that the amount the lender gets back is still worth as much as the amount lent (the
real interest rate)—and guarantees the lender a profit, in lieu of, or perhaps in addition to, other fees. (A credit
card company, for instance, receives penalty fees and possible membership fees, as well as fees from the
vendors that use their services, on top of the interest the cardholder pays; this helps ensure that the card issuer
earns a profit even if the cardholder pays off the balance every month in order to avoid interest.) Depending on
the loan, interest can be compounded annually, monthly, daily, or at other frequencies.
 Nominal and Real Interest Rates
The nominal—or payable—interest rate is the one defined in the appropriate contract or other formal agreement.
For example, a credit card that charges 16.9 percent on the balance has a 16.9 percent nominal interest rate. The
real interest rate does not show up anywhere in the literature pertaining to the debt. It represents the purchasing
power of the interest payments received—that is, the nominal interest rate adjusted for inflation over the period of
time in which the interest is paid.
Interest rates that are compounded at different frequencies can be converted to a “common denominator,” called
the effective interest rate or annual equivalent rate (AER), so named because it restates the nominal interest rate
as an interest rate with annual compounding. The AER is similar to the APR, or the annual percentage rate, the
terms in which credit card rates, mortgage rates, and other loans rates are usually expressed. The Truth in
Lending Act requires that loan and credit card paperwork disclose the APR of the loan, as the periodic interest
rate times the frequency with which the interest is compounded during the year. Various finance charges are
taken into account in determining the APR so as to make the cost of borrowing as transparent as possible.
However, the APR does not include the possibility of high penalty fees for late or missed payments, with the risk
of a permanent increase in the interest rate, and thus may understate the true cost of borrowing.
 Federal Funds Rate and the Discount Window
One of the most important interest rates in the U.S. financial system is the federal funds rate. One of the essential
monetary policy targets of the Federal Reserve (Fed), the fed funds rate is the interest rate paid on federal funds.
These are loans (usually one-day, “overnight” loans) made among banks for the purposes of maintaining the
minimum reserve required by regulation (up to one-tenth of the bank’s demand accounts, such as checking
accounts). Banks that have more reserves on hand than they need to meet that minimum can make a small profit
on loans to banks that come up reserve-short for the day. Institutions involved in the federal funds system, and
affected by this interest rate, include not only federal agencies and government-sponsored enterprises, but also
commercial banks, savings and loans, many investment banks, and foreign banks that operate branches in the
United States. By extension, most transactions in the U.S. economy “touch” the federal funds system through
some degrees of separation—every dollar in circulation passes through hands participating in this system.
When the media reports the Federal Reserve raising or lowering interest rates, it is the fed funds rate they are

referring to—specifically the nominal rate, which is a target range determined twice per quarter by the Federal
Open Market Committee (FOMC). A committee within the Federal Reserve, the FOMC consists of the seven
members of the Federal Reserve Board, the president of the Federal Reserve Bank of New York, and four other
presidents of Federal Reserve banks, filled by one-year rotating terms. Raising the rate contracts the money
supply, discouraging institutions from borrowing from other banks; lowering the rate encourages such borrowing.
The discount window is similar to the fed funds rate. It is the interest charged on loans made by the Federal
Reserve to banks and can take any of three forms: the primary credit rate and, for less sound banks, the
secondary credit rate for overnight loans, and the seasonal credit rate for loans of up to nine months. All three
interest rates are somewhat higher than the fed funds rate.
During the 2008–2009 global financial crisis, one of the ways the Fed responded was to reduce the discount
window—lowering it in small increments, from 6.25 percent in July 2007 to 0.50 percent in December 2008—and
to extend the length of primary credit loans to ninety days. The goal was to drastically increase the availability of
funds to institutions in order to prevent insolvency and the need for further bailouts by an already taxed federal
government.
Interest rates, particularly those set by central banks like the Fed, can both affect and be affected by the business
cycle. High rates increase the cost of borrowing, thereby stifling investment; low rates do the opposite. Central
banks will often raise rates during periods of rapid economic expansion, since inflation becomes a concern at such
times. By making loans more expensive, they can put a check on price and wage increases. During times of
economic contraction or, more rarely, during deflationary episodes, the bank will lower the rates, making loans
cheaper, with the goal of increasing economic output, employment, and mild inflation.
The size and direction of the interest rate hike or decrease, as well as its timing, can be critical in averting inflation
or recession. Many economic historians have cited the Fed’s high interest rate policy as a key factor in deepening
and prolonging the Great Depression in the 1930s. Conversely, the Fed has been blamed for contributing to the
housing bubble of the early 2000s by maintaining historically low interest rates even when it was clear to many
economists that the housing market was overheated, a key factor in the crisis that struck the world financial
system in 2008.
Bill Kte’pi and James Ciment
 
See also:  Banks, Central;  Debt;  Federal Reserve System;  Monetary Policy;  Mortgage
Markets and Mortgage Rates;  Savings and Investment. 
Further Reading
Bernstein, Peter L. A Primer on Money, Banking, and Gold. Hoboken, NJ: John Wiley and Sons, 2008. 
Brigo, Damiano, and Fabio Mercurio. Interest Rate Models: Theory and Practice: With Smile, Inflation, and Credit. New
York: Springer Finance, 2007. 
Gali, Jordi. Monetary Policy, Inflation, and the Business Cycle: An Introduction to the New Keynesian Framework. Princeton,
NJ: Princeton University Press, 2008. 
Homer, Sidney, and Richard Sylla. A History of Interest Rates. Hoboken, NJ: John Wiley and Sons, 2005. 
Langdana, Farrokh K. Macroeconomic Policy: Demystifying Monetary and Fiscal Policy. New York: Springer, 2009. 

Intermediation, Financial
 
Contemporary financial markets are complex structures that bring together many different players and allow the
exchange of financial instruments representing shares, debt, commodities, and other underlying financial objects
and contracts, which are thereby made negotiable. Essential to these markets are financial intermediaries between
suppliers and users of capital. Suppliers are institutions such as central banks, the Federal Reserve, commercial
banks, insurance companies, credit unions, pension funds, real-estate investment trusts, and mortgage pools,
which distribute capital to users through investment in loans, bonds, and stocks; the users, in turn, use these
financial resources to fund their economic activity. Such users include businesses, which use the funds for
investments, expansion, and operating; consumers, who use capital for things like home purchases and higher
education costs; and governments, which use the capital for infrastructure improvements and other public needs.
Intermediation occurs in a variety of ways, through such devices as checks, credit cards, stocks, and financial
contracts. Although all of these are recognized mediums of exchange, it is important to distinguish between
different forms of financial intermediation—specifically, those closely related to markets (such as the buying and
selling of corporate securities) and those directly related to the transformation of corporate balance sheets (through
direct lending). Both relate to credit, but while intermediation markets transform credit over time, banks create new
money.
Among the different groups of participants in capital markets, banks are generally viewed as intermediaries
through which money is created, distributed, and stored. They act as intermediaries between lenders and
depositors, managing assets funded by deposits (liabilities) and redistributing those assets through lending. As a
result, banks offer payment services to their customers. Banks can follow a generalist strategy, offering a full array
of financial services, from deposit accounts to diversified loans, to real estate or life insurance at the wholesale or
retail level (“universal banks”). Banks can also take a more restrictive approach, focusing on high-net-worth
individuals (private banks), mergers and acquisitions, syndication, or other specialized activities (investment
banks). All of these institutions differ from central banks, which serve as creditors to both private banks and
governments. Central banks (such as the U.S. Federal Reserve) play a major role in regulating a nation’s money
supply.
Brokerage houses such as Morgan Stanley or the largely online firm Charles Schwab constitute another class of
intermediaries, facilitating exchanges within financial markets. Brokers, as defined in the United States, are entities
other than banks that are in the business of buying and selling securities for others. They do not necessarily need
to be in direct contact with customers, though this is usually the case. They conduct research on issuers and offer
order routing, order taking, and execution services to their retail, corporate, and institutional clients (asset
managers, hedge funds, proprietary trading desks, or third brokers). In addition, they may develop a whole range
of ancillary services, such as trading algorithms packages, facilitation services, clearing or prime brokerage
services that are marketed to clients who need them in order to conduct their businesses in financial markets.
Brokerage houses may also provide their customers with administrative support, such as regulatory reporting. For
all the services they deliver, brokerage houses usually take a commission—the expression of intermediation
service. Because their business is at the heart of the financial markets, they are often exposed to conflicts of
interest. In fact, brokers may be full subsidiaries of investment banks working with issuers. In this case, brokers
implement policies and procedures known as “Chinese walls” to prevent private information from being used by
analysts or traders to perform their duties, whether writing or making transactions for customers.
Marc Lenglet
 
See also:  Savings and Investment. 

Further Reading
Allen, Franklin, and Douglas Gale.  “Financial Intermediaries and Markets.” Econometrica 72:4 (2004): 1023–1061. 
Bond, Philip.  “Bank and Non-Bank Financial Intermediation.” Journal of Finance 59:6 (2004): 2489–2529. 
De Goede, Marieke. Virtue, Fortune and Faith: A Genealogy of Finance. Minneapolis: University of Minnesota Press, 2005. 
Heffernan, Shelagh. Modern Banking in Theory and Practice. Hoboken, NY: John Wiley and Sons, 1996. 
Mizruchi, Mark S., and Linda Brewster Stearns.  “Money, Banking and Financial Markets.” In Handbook of Economic
Sociology, ed. Neil J. Smelser and Richard Swedberg. Princeton, NJ: Princeton University Press, 1994. 
Spencer, Peter D. The Structure and Regulation of Financial Markets. New York: Oxford University Press, 2000. 
 
International Development Banks
 
International development banks provide loans to developing countries from resources contributed by both
developing and developed countries. The loans provided come in two varieties: long-term loans charging market
rates of interest and long-term loans charging below-market rates of interest. They also provide financial resources
in the form of grants. The purpose of these loans and grants is to increase the level of development, or rate of
growth, of developing countries. These banks generally come in two varieties: multilateral and subregional. Some
examples of the former are the World Bank, African Development Bank, European Bank for Reconstruction and
Development, and the Inter-American Development Bank Group. Examples of subregional banks include the West
African Development Bank, East African Development Bank, and the Caribbean Development Bank.

The Asian Development Bank, with 67 member nations and headquarters in Manila, the Philippines, is one of a
number of global and regional institutions that promote economic and social development through loans, grants,
and other assistance programs. (Bloomberg/Getty Images)
 Rationale
Poor countries face a number of barriers to growth and development. Most suffer from a lack of savings.
Economists generally argue that capital—manmade means of production (machinery) —is a critical input in the
production process. In order to increase labor productivity and growth, the labor force must be better equipped
with capital. To create capital, firms must invest, and this investment must be financed by savings. A lack of
savings will dramatically reduce investment, which, in turn, will reduce capital formation. Workers will be ill
equipped with machines, and growth (at least in the short run) will be reduced.
Many developing countries are also lacking a highly developed physical infrastructure such as roads,
communication systems, and so on. In order to build these critical components, public investment must occur and
this will have to be financed via savings. Without savings, these investments will not occur. Overall growth will be
lowered if producers find it difficult to transport products or to communicate with each other.
Many developing countries also lack social infrastructure, an important component of which is the educational
system. In the long run, it is the rate of technical innovation that is the key component of long-term growth. In
order to be able to innovate or borrow technology from elsewhere, it is essential for the labor force to rapidly
accumulate human capital via education. This requires investment and savings.
In light of the above, development banks can play an important role. They can augment the savings available to
poor countries such that investment in capital, physical infrastructure, and social infrastructure can be increased. In

addition, these banks can provide the resources and information necessary to increase the efficiency of
government bureaucracy in developing countries. As a result, economic growth can be increased and economic
development enhanced.
There are, however, critics of development banks and their policies. Some scholars have argued that the impact of
investments by development banks has been meager. One of the reasons for this is that there is often little follow-
up work accompanying projects funded by these banks. In other words, once projects are completed there is little
attempt to see or calculate the actual benefits generated by such activity. Did the investment actually succeed in
raising educational standards, enhancing productivity, and so on? Without this kind of evaluation the banks are
likely to continue some projects that generate very little economic and social return to society.
Some have also argued that the activities of development banks often create systems of incentives that are
inimical to long-term economic growth. Ruling groups in developing countries and the bureaucracies that
implement policy have their own interests in mind when making and implementing policy. Their interests include
expanding the power and influence of their particular parts of the government as well as their long-term political
survival. The interests of society in general are important only to the extent that the survival and success of the
political elite depends upon the general well-being of society. If the political elite has to extract revenue from
society at large in order to generate the revenue necessary for political survival, then the former will be interested
in promoting the interests of the latter. If, instead, members of the political elite can get the revenue they need
from external sources, then their political prospects are not dependent on the well-being of their own society, and
they are unlikely to engage in socially productive policies.
The above argument is generally made with respect to foreign aid. Foreign aid represents a flow of revenue into a
developing country. This eases the constraint on the receiving government and allows it to reduce its dependence
upon the savings of its own population for development. This is likely to create an environment inimical to good
policy. To the extent that loans from development banks incorporate an element of aid, they may very well
undermine the foundations for good governance.
There are, of course, reforms in lending practices that can be utilized to minimize these negative effects. The more
successful these reforms, the more likely the activities of development banks will be successful in promoting
growth and development.
 Development Banks and Financial Crises
In the past, developing countries in various parts of the world have been subject to periodic financial crises that
were devastating to their growth process. Generally, such crises have led to the collapse of the domestic banking
system and foreign exchange crises, characterized by dramatic falls in currency values and increasing difficulty in
making international payments. This in turn has caused dramatic declines in domestic and foreign investment
accompanied by declines in growth, and rising unemployment and inflation. Latin America, in particular, has been
subject to a series of crises resulting in a “stop-and-go” pattern of economic growth.
The financial crisis of 2008–2009 originated in the developed world and initially had little or no impact on the
developing world. The financial sectors in most developing countries were not exposed to the type of toxic assets
that undermined the financial systems in the United States and much of Europe. Nonetheless, the significant
economic downturn in the United States and Europe had a negative impact on developing countries. Export
markets for developing nations shrank as growth in the developed world slowed down. In addition, remittances to
developing countries by their workers who have migrated to the developed world fell dramatically. Finally, capital
inflows into developing countries from the developed countries declined.
Development banks can play an important role in limiting these negative effects. These organizations are intent
upon increasing the flow of loans and investments into developing countries, with a focus on programs aimed at
the poorest countries. This will ensure that infrastructure and technology projects in developing countries continue,
as they are likely to be especially important in terms of providing employment opportunities. The resources

currently available to the development banks are limited, however. But coordination of programs across banks can
enhance their impact.
One further point needs to be made. The current financial and economic crisis will certainly have a negative
impact on growth prospects in developing countries, but this negative impact is likely to be dampened by a
number of factors. First, the extent to which capital markets in developing countries were linked to those in
developed countries was limited, thus limiting the impact of the crisis. Second, growth rates in the developing
world have increasingly become decoupled from those in the developed world. The trade linkages for the former
have been shifting away from the United States and Europe and toward rapidly growing Asia. Third, the prospects
for rapid growth in India and China remain high. This will partially affect the negative consequences of economic
slowdown in the developed countries. Fourth, the institutional structure and policy mechanisms in many
developing countries have improved significantly. Finally, significant countercyclical fiscal packages have been
implemented not only in the developed world, but also in rapidly growing economies such as China, India, and
Brazil.
Richard Grabowski
 
See also:  International Monetary Fund;  World Bank. 
Further Reading
Griffith-Jones, S., and J.A. Ocampo.  “The Financial Crisis and Its Impact on Developing Countries.” United Nations
Development Program Working Paper, 2009. 
Nandé, W.  “The Financial Crisis of 2008 and the Developing Countries.” UNU-Wieder. Discussion Paper No. 2009/01,
January 2009. 
Overseas Development Institute.  “The Global Financial Crisis and Developing Countries.” Background Note, October 2008. 
www.odi.org.uk/resources/download/2462.pdf
International Economic Agreements
 
An international economic agreement establishes reciprocal conditions between two or more nations for trade,
investment, currency and exchange policy, labor mobility and working conditions, economic relief, environmental
regulation, or common management of waterways and resources.
Narrowly defined, an international agreement is a treaty with bilateral principles for trade in which the designation
of “most-favored-nation” status introduces a commercial advantage over tariff reductions among reciprocal
transactions. Historically, this type of positive economic discrimination has evolved from the architecture of
strategic international alliances, with enforcement clauses being strictly secured by national government’s
oversight.
From another perspective, an international economic agreement represents a shared commitment to a cluster of
principles or related international policies—covering trade, investment, currency, and economic relief—signed on a
multilateral basis and enforced by specific international regulatory institutions. Whereas simple bilateral trade
accords played a key role in most international relations during the nineteenth and early twentieth centuries,
extended multilateral and regional agreements, broader in scope, became the mainstay of international economic

relations after 1947.
Trade agreements come in many varieties within the two basic bilateral and multilateral frameworks. In free trade
area agreements, like the North American Free Trade Agreement (NAFTA), signers pledge to reduce or eliminate
tariffs and other barriers to trade between them, but leave up to individual countries control over tariffs on goods
from nonsignatory countries. In a customs union, all internal tariffs are removed and a single tariff structure is
created for trade with nonsignatory nations. A common market agreement, like that of the predecessor
organizations to the European Union (EU), differs from a customs union in that barriers to the movement of capital
and labor are lowered or removed along with tariffs on trade goods. With a monetary union, such as the EU’s
eurozone, a single currency and monetary policy is adopted, usually run by a common central bank. Finally, with
an economic union, like the EU, all kinds of economic policies—from regulations to taxation—are harmonized.
These various forms of trade agreements have their benefits and their drawbacks. On the plus side, trade
agreements allow for the freer movement of goods, services, capital, and even labor, so as to create greater
efficiencies of production, distribution, and marketing. More comprehensive agreements, such as monetary and
economic unions, offer the opportunity to coordinate economic and other policies across a broad area, as well as
enhance the power of unions vis-à-vis nonmember states and trading blocs. But trade agreements also have their
critics, particularly in cases where one signatory is seen to have some economic advantage over another. Many
people in the United States and Canada, especially in the labor community, for example, criticized NAFTA out of
fear that Mexico’s substantially lower labor costs would deplete jobs north of the border. On the other side, some
in Mexico have noted that the availability of cheaper American grains have driven thousands of less-efficient
Mexican farmers off the land.
 Interwar Period: Failure of International Agreements
With the Treaty of Versailles in 1919, the turmoil of World War I was settled, if not fully resolved. The agreement
proved not entirely satisfactory, as some European leaders realized that further negotiations would be needed if
the European markets, destroyed by war, revolution, and the collapse of empires, were to be rebuilt. What ensued
was an attempt to promote new international forums, such as the League of Nations, and to sponsor a series of
international conferences that would resolve economic problems and disputes. But as these initiatives faltered, one
after another, governments began to back away from their international commitments. Suspicions, misgivings, and
military rivalry became the driving forces of international relations.
The failure of economic diplomacy during the interwar period sheds light on some factors that have magnified
short-term conflicting interests rather than the commitment to general cooperation. First and foremost, the political
margins of negotiators became narrow, both internally and externally, when governments yielded to the pressure
of economic and social crisis—as seen in the traumatic inflationary cycle of 1918–1924 and, later, in the Great
Depression of 1929–1933.
Second, agreements also became increasingly difficult when no clear leadership became evident on the
international scene. In that regard, the 1920s revealed a widening political gulf among the core winning powers,
together with the decline of British authority and the withdrawal of the United States from the European political
“mess.”
Third, and finally, agreements became more problematic when no international organization existed to frame the
arrangements, either through long-term goals for economic relations, a legal framework of liability, or compliance
and enforcement through oversight and information gathering. The League of Nations, created in 1919, soon
backtracked from this direction and became known as a political forum that strived for peacekeeping rather than
an intervening power in the sensitive area of policy making. When it came to trade and monetary policy, every
nation envisaged that the strong hand should belong to the independent central banks of the world’s major
economies, not to a scattered diplomatic institution.
Recurring economic crises, a lack of leadership, and a lack of international institutions thus constituted three main

factors that thwarted the prospects for overarching agreements. In this context, the international architecture that
surfaced in the 1920s and 1930s was one of imperial and regional trade preferences and bilateral protectionist
trade agreements. The Customs Union formed by France with members of its empire in 1928, and the
Commonwealth system established by Great Britain in 1932, are paramount examples of the former. The bilateral
commercial treaties signed by the United States, mostly with Latin American countries, and the bilateral trade
blocs forged by Nazi Germany, exemplify the latter.
 Post–World War II: Multilateral Relations and Trade Liberalization
In the eyes of many observers, the trade discrimination of the 1930s led to the armed aggression of World War II.
Import quotas, imperial and regional trade preferences, currency controls, and other discriminatory practices were
perceived as the harbingers of war. And if preferential trade unleashed the evil of world conflicts, the best way to
safeguard a peaceful future was through the promotion of free trade and multilateral relations. This view,
particularly entrenched in the U.S. State Department, paved the way for postwar reconstruction plans based on
the nondiscrimination of third partners, indivisibility of agreements, and reciprocity of treatment.
Two pillars of the new world order—the International Monetary Fund and the World Bank—emerged from the
Bretton Woods Conference of 1944 with the mission of backing up currency stability, financial integration, and
multilateral aid among the adherent countries. The third pillar was a common agreement signed by twenty-three
nations in 1947, called the General Agreement on Tariffs and Trade (GATT), which covered 60 percent of world
trade.
Central to this agreement was the restoration of the world’s international trade on a nondiscriminatory basis:
concessions granted by each country to a single partner were henceforth extended to all signatories of GATT
accords. In this manner, GATT set the stage for a trade-off between reductions in import restrictions, on the one
hand, and reciprocal market access to other trading countries, on the other. The conventional approach of mutual
commitment to a fixed set of rules also underwent significant change, being replaced by the more flexible and
cooperative principle of periodical multilateral negotiations, or the so-called GATT rounds.
The historical agreement signed in 1947 benefited mostly from the combination of export boom and strong
economic growth that followed World War II. This resulted in broadening the scope of international economic
accords along a double path: more goods and trade issues included on the agenda and more countries brought
into the GATT fold.
The first phase of the GATT rounds consisted of negotiations over tariff reductions on industrial products, following
an item-by-item approach. After that, the Kennedy Round (1963–1967) pioneered a formula for gross average
cuts on imported goods and extended the agenda to antidumping, agriculture, subsidies, technical barriers to
trade, and countervailing duties. The Tokyo Round (1974–1979) reinforced the previous tariff cuts and adjoined
the issues of public procurement, safeguard, and revision of GATT articles.
The Uruguay Round (1986–1993) debated nontariff barriers, intellectual property, services, and trade-related
investments. The previously neglected sectors of agriculture and textiles, in which the developing economies had
clear advantages, also came to occupy a central place in negotiations. The full membership of GATT rose to 113
countries, accruing the representation of less-developed economies. To tap the mounting inclusiveness, GATT
became a permanent institution, renamed the World Trade Organization (WTO), in 1995.
 Late Twentieth and Early Twenty-First Centuries: Regionalism and
Globalization
Despite the unprecedented height of multilateral openness, other tendencies were also wending their way,
particularly among the less-developed countries. A first wave of preferential trade agreements occurred in the
1960s and early 1970s, in an attempt to liberalize commerce between regional trading partners, while

discriminating against third parties. This was a defensive move to counteract worldwide multilateralism led by the
United States, and to draw up a network of neighboring alliances. By forming preferential trade agreements, the
less-developed economies could reduce the cost of achieving any given level of import-competing industrialization,
improve the member’s terms of trade vis-à-vis the rest of the world, benefit from local economies of scale, and
cope with political isolationism.
The second surge of preferential trade agreements arose in the early 1990s, part and parcel of the ongoing
changeover of the world order. Rather than being simply opposed to multilateral initiatives, regional integration
arrangements forged complementary networks that strengthened national participation in the overriding multilateral
environment.
In a period marked by burgeoning economic interdependence—the end of the cold war, the technological
revolution in telecommunications, and a growing share of the less-developed economies in world trade—the
forging of regional trade preferences became the flip side of increasing the scope of GATT/WTO. Moreover, the
wave of preferential trade agreements did not curtail the path of export growth, but actively contributed to its
enhancement. Even though the long-term welfare consequences of this economic “regionalism” are still unclear, its
role remains undisputed.
The most important preferential economic agreements today govern such free-trade regions and customs unions
as the European Community, NAFTA, MERCOSUR, Association of Southeast Asian Nations (ASEAN), Andean
Pact, and the Caribbean Community (CARICOM). Beyond such regional security measures, multilateralism still
persists as an inescapable path to solving international problems. The decision of the 1997 Climate Convention in
Kyoto, at which industrialized nations committed themselves to reducing their emissions of greenhouse gases by
an average of 5 percent compared with 1990 emissions, is the best example of ongoing international agreements
that require inclusive cooperation on a world scale.
Nuno Luis Madureira
 
See also:  International Policy Coordination. 
Further Reading
Goldstein, Judith, and Joanne Gowa.  “U.S. National Power and the Post-War Trading Regime.” World Trade Review 1:2
(2002): 153–170. 
Goldstein, Judith L., Douglas Rivers, and Michael Tomz.  “Institutions in International Relations: Understanding the Effects
of the GATT and the WTO on World Trade.” International Organization 61:4 (2007): 37–67. 
Mansfield, Edward D., and Eric Reinhardt.  “Multilateral Determinants of Regionalism: The Effects of GATT/WTO on the
Formation of Preferential Trading Arrangements.” International Organization 57:4 (2003): 829–862. 
McCarthy, Dennis. International Economic Integration in Historical Perspective. New York: Routledge, 2006. 
 

International Monetary Fund
 
An international financial institution established just after World War II, the International Monetary Fund (IMF)
studies national economies, provides technical assistance and training to governments, and, most importantly,
lends money to nations in need, particularly in the developing world. Like its sister institution the World Bank, the
IMF is funded chiefly by member states, with the bulk of its money coming from developed nations. While the
World Bank generally focuses on long-term development, IMF aid is typically extended at times of crisis in a
nation’s economy. Since the late 1940s, the IMF has played an important and often controversial role in business
cycles around the world by responding to banking and currency crises of great magnitude and consequence.
The “intellectual founding fathers” of the IMF were two economists, John Maynard Keynes of the United Kingdom
and Harry Dexter White of the United States. The idea of the fund was discussed by delegates from forty-five
countries attending the Bretton Woods Conference in New Hampshire in July 1944. The IMF was officially
founded on December 27, 1945, when representatives of twenty-nine countries signed the Articles of Agreement.
Operations got under way in May 1947. As of 2009, the ever-expanding organization had 185 member states.
Delegates of the International Monetary Fund and World Bank convene in Singapore for the annual joint meeting
of the two groups in September 2006. The fall event has been a focal point of antiglobalization demonstrations.
(Bloomberg/Getty Images)
 Structure and Operations
Although the IMF is a specialized agency of the United Nations, it has its own charter, finances, and governing
structure. The IMF is funded by member countries, whose annual payment, called a “quota,” depends on the size
and strength of the member country’s economy. The same factors also determine a country’s voting power and
the maximum sum it can receive from the organization in loans. The IMF also borrows money from international
lending institutions such as the central banks of member countries. The IMF has three main functions: (1)
economic surveillance, (2) lending, and (3) providing technical assistance and training. Toward those ends, the

IMF publishes compilations of research bulletins, staff papers, working papers, economic outlooks, policy papers,
financial market updates, manuals, guides, and reports; it organizes conferences, seminars, and workshops; and it
combats money laundering and terrorism. The IMF headquarters is located in Washington, D.C., with additional
offices in Paris, Warsaw, Tokyo, and New York. The organization has approximately 2,500 employees from more
than 140 countries. Its managing director since November 2007 has been Dominique Strauss-Kahn of France.
The IMF was created to promote global growth by overseeing the international monetary system. As such, it seeks
to ensure the stability of exchange rates between national currencies, to encourage member states to remove
exchange restrictions that hinder foreign trade, and—especially in the first decade of the twenty-first century—to
facilitate financial transactions between countries. By creating the IMF, the founding members sought to avoid a
repetition of the situation in the 1930s, when several countries devalued their currencies and raised foreign trade
barriers to increase their export competitiveness. These steps brought a dramatic decline in world trade,
increased unemployment, and deepened the economic recession in many countries. Financial isolationism and
worldwide recession, in turn, helped pave the road to World War II.
Although the Articles of Agreement have been amended several times, the IMF’s main goals—the stability of
exchange rates and removing exchange restrictions—have remained unchanged through the years. In addition,
the organization has taken on new tasks. Until 1971, the IMF supervised a system of fixed exchange rates tied to
the U.S. dollar (which in turn was pegged to the value of gold). However, the aftermath of the Vietnam War, the
oil shocks of the 1970s, and the printing of money to pay for President Lyndon B. Johnson’s Great Society
triggered a period of uncontrolled inflation in the United States, which in turn caused a precipitous devaluation of
the dollar against other currencies. Neither the central banks of other industrialized countries nor the IMF itself
could stop the decline of the dollar. First Canada and then the European industrial nations, no longer seeing the
dollar as a stable currency, let their own currencies float freely.
With the breakdown of the dollar-based international monetary system, the role of the IMF was expanded. During
the oil crisis in the 1970s, the IMF increased institutional lending and began helping poor countries by providing
emergency financing to solve their balance-of-payments difficulties. Again in the 1980s, the IMF increased lending
in reaction to a global financial crisis. In the 1990s, it helped the former Soviet bloc countries in the transition to
market-driven economies by providing financial support, policy advice, and technical assistance. The organization
proved highly successful in the latter efforts, as several of these countries joined the European Union in 2004.
Meanwhile, in 1996, the IMF began coordinating efforts with the World Bank to help poor countries reduce their
debt burden to manageable levels.
Loans granted by the IMF are typically provided under special arrangement. If a country wants to borrow money
from the IMF, it has to agree to institute prescribed economic policies and austerity measures. The IMF’s twenty-
four-member executive board (large economies like the United States have their own seats, while smaller ones
are grouped) must approve the country’s Letter of Intent and a Memoranda of Economic and Financial Policies
listing these measures.
The IMF employs several different loan instruments, called “facilities.” The Poverty Reduction and Growth Facility,
established in 1999, is a low-interest (0.5 percent annually) lending mechanism for low-income countries. Those
with a per capita income of less than US$1,095 are eligible for these loans; 78 such countries were eligible in
2008. Such loans are used for reducing poverty, strengthening governance, and spurring economic growth.
Another instrument, the Exogenous Shocks Facility, supports low-income countries that face unexpected and
uncontrollable emergencies, such as a natural disaster or declining global commodity prices. Again, the annual
interest rate is low (0.5 percent). In addition to these two types of loans, Stand-by Arrangements are geared to
countries with short-term balance-of-payments problems, such as reduced exports. Extended Fund Facilities were
established for helping countries with long-term balance-of-payments problems. Supplemental Reserve Facilities
provide short-term financing for countries with sudden crises, as in the case of massive capital outflows.
Compensatory Financing Facilities assist countries with sudden decreases in exports or sudden increases in the
price of imported cereals. Finally, additional emergency assistance is offered to countries recovering from armed
conflicts and natural disasters.

 Controversies and Criticisms
The policies and requirements of IMF loans have come under criticism among some economists and public policy
officials. Indeed, critics maintain, the IMF can actually hurt an already troubled economy through the imposition of
inappropriate “one-size-fits-all” policies. IMF policy prescriptions that impose tight spending restrictions on a
country to which it lends money, for example, may be more effective for economies suffering from excessive
government spending and inflation rather than for economies that are going through a severe debt crisis and
declining prices. The issue was debated in the mid-1990s, for example, when the IMF demanded that the
government of South Korea undertake what appeared to be inappropriately tight financial policies given the
particular nature of its economic troubles. Advocates of the IMF counter such criticism by pointing out that the
economy of South Korea did recover relatively quickly, arguing for the IMF approach.
During the financial crisis of 2008–2009, the IMF provided loans valued at more than $50 billion to emerging
countries—though the largest loan, $39 billion, went to Greece in 2010—to help them cope with declines in capital
inflows, exports, and local demand, and to support banks with financial difficulties. It also offered advice to
advanced countries in designing more effective stimulus packages and provided all members with economic
analyses and forecasts.
Tiia Vissak
 
See also:  International Development Banks;  World Bank. 
Further Reading
Andersen, Camilla.  “Changing IMF Works Hard to Combat Global Crisis.” IMF Survey Magazine, February 26, 2009. 
Boughton, James M., and Domenico Lombardi, eds. Finance, Development, and the IMF. New York: Oxford University
Press, 2009. 
Hill, Charles. International Business: Competing in the Global Marketplace.  7th ed. New York: McGraw-Hill/Irwin, 2009. 
Humphreys, Norman K. Historical Dictionary of the International Monetary Fund. Lanham, MD: Scarecrow, 1999. 
International Monetary Fund:  www.imf.org
Pauly, Louis W. Who Elected the Bankers? Surveillance and Control in the World Economy. Ithaca, NY: Cornell University
Press, 1997. 
International Monetary Fund Mortgage Market Index
 
The International Monetary Fund (IMF) Mortgage Market Index is a composite index conceived and compiled by
the IMF, published for the first time in the April 2008 issue of World Economic Outlook. The index quantifies the
institutional features of mortgage markets and the ease with which mortgage credit can be accessed in eighteen
industrial countries. Some have argued that the index has been signaling the deterioration of risk conditions in
housing markets and rising volatility in deregulated credit markets, especially in the United States, since the mid-
2000s.

The Mortgage Market Index (MMI) is a simple mathematical average of five important institutional indicators of the
housing markets: mortgage equity; the existence of early payment fees; loan-to-value ratio; average mortgage
term; and the level of development of secondary mortgage markets. Mortgage equity measures the amount of
money a homeowner can borrow against the net current value of the property, or the difference between the
current market value of the property minus the outstanding principal of the mortgage. The higher the current
value, the higher the equity build-up and the higher the amount one can borrow. Fee-free repayment enables the
homeowner to refinance the existing mortgage at a lower rate, thereby reducing the mortgage payment without
paying a prepayment penalty. The loan-to-value ratio indicates the percentage of the value of the property
financed by mortgage. The length of the mortgage term, whether it is for 15 or 30 years, for instance, affects the
debt-service-to-income ratio, or the ease with which one repays the mortgage; the shorter the mortgage term, the
higher the debt-service-to-income ratio and the higher the financial burden of the mortgage. The level of
development of secondary mortgage markets affects the lenders’ ability to refinance the mortgage. The higher the
level of development, the easier it is for the originators of the mortgages to sell them in the secondary markets
where they are repackaged and sold to hedge funds, pension funds, or other secondary investors.
Although industrial countries are generally characterized by well-developed financial markets, the institutional
features of national mortgage markets as measured by MMI differ significantly. Whereas mortgage markets are
easily accessible in the United States, they are not in such countries as Germany, Italy, and France. In these and
other countries, homeowners can neither borrow against accumulated equity nor refinance without paying extra
fees. In addition, the average mortgage term in these countries is fifteen years, with a loan-to-value ratio of 75
percent. The result is a relatively high debt-service-to-income-ratio. In the United States, by contrast, a home
mortgage term is typically thirty years (or adjustable) and the loan-to-value ratio is 80 percent, lowering the debt-
to-income-ratio significantly and enabling more homeowners to finance their mortgage. In addition, whereas
secondary mortgage markets are nonexistent in most European countries, they are well developed in the United
States; in 2008, mortgage-backed security issues in the United States constituted about 20 percent of outstanding
residential loans. In France and Germany in the years leading up to the financial crisis of 2008, mortgage-backed
security issues constituted between 0.2 and 0.3 percent of outstanding residential mortgage loans. Accordingly,
the United States ranks highest among the eighteen nations for which the IMF calculates the Mortgage Market
Index, followed by the Scandinavian and non-U.S. Anglo-Saxon countries such as Australia and Canada.
 Mortgage Market Index (MMI) Ranking of Industrial Countries, 2007 
Country
Mortgage Market Index
United States
0.98
Denmark
0.82
Netherlands
0.71
Australia
0.69
Sweden
0.66
Norway
0.59
United Kingdom
0.58
Canada
0.57
Finland
0.49
Spain
0.40
Ireland
0.39
Japan
0.39
Greece
0.35
Belgium
0.34

Austria
0.31
Germany
0.28
Italy
0.26
France
0.23
According to IMF data, countries with a high MMI also show strong positive correlations between consumption and
housing wealth, and between housing prices and consumption. Thus, appreciation in housing prices and
subsequent rises in equity appear to have facilitated higher household consumption. Under the assumption that
higher future income and/or increasing housing wealth can make the repayment of debt relatively easier,
homeowners have accumulated more and more debt to finance present consumption. Not surprisingly, data for the
United States support these findings. Between 1995 and 2006, homeownership in the United States jumped to 69
percent, a significant increase of about 6 percent. With home equity constituting a large portion of the total wealth
held by middle-income groups, the bursting of the housing bubble beginning in 2007 left homeowners with
unprecedented debt but without the inflated housing wealth to fall back on. All in all, the housing bubble
undermined consumer discipline in borrowing and spending, which contributed to untenable levels of debt.
As a simple mathematical average, the IMF Mortgage Market Index treats each indicator in the index with equal
weight. It might be argued, however, that the relative importance of each indicator may be different in individual
countries and that the weights should be adjusted accordingly. Nevertheless, the MMI remains a useful and
informative economic index that may provide an early alarm for future volatility in the housing market.
Mehmet Odekon
 
See also:  International Monetary Fund;  Mortgage Markets and Mortgage Rates. 
Further Reading
 Carderelli, Roberto, Deniz Egan, and Alessandro Rebucci.  “Press Points for Chapter 3: The Changing Housing Cycle and
Its Implications for Monetary Policy.”
 International Monetary Fund (IMF).  World Economic Outlook: Housing and the Business Cycle.  Washington, DC: 
International Monetary Fund,  2008. 
 
International Policy Coordination
 
Globalization and its close cousin, regionalization, have increasingly linked the economies of the world so that the

rise and fall of a business cycle in one country more readily induces similar movements in the cycles of other
countries.
Again and again during the course of the twentieth century, it became clear that the nations of the world were
bound more and more closely by trade, finance, migration, and environmental issues. This is not to say that what
happens to one happens to all, nor that what is good for one is good for all. What it does say is that, increasingly,
what happens to one affects all. There have been a variety of responses to this, from protectionist impulses as
nations seek to build a shell around themselves and prevent the impact, to internationalist efforts to eliminate or
change the relevance of national borders. Somewhere in between lies international policy coordination, or the
open discussion and collective debate of national policies in an international context, not with the goal of making
all nations adopt the same policies, but simply to encourage them to develop their policies with an awareness of
their potential impact elsewhere. Even when national governments have different goals, it is possible to negotiate
so that various national policies can be designed to fit together more or less harmoniously.
 Twentieth Century
The Great Depression and World War II were two key events in the development of economic policy competition
and coordination among nations. The severe economic downturn of the 1930s set in motion the forces of
economic nationalism as various nations erected higher and higher trade barriers in order to protect national
industries and to bring down politically destabilizing levels of unemployment. But many economic histories argue
that this protectionism only deepened the Depression while increasing international tensions, contributing to the
outbreak of the most destructive war in human history.
The end of World War II saw a more urgent desire for international cooperation and coordination, not only in the
political and legal contexts that were the main focus of the United Nations, but in economic and other areas as
well. Even during World War II, delegates from all forty-four Allied nations met at the Mount Washington Hotel in
Bretton Woods, New Hampshire, to hammer out the details of what has since been called the Bretton Woods
system—an international economic system requiring each country to maintain a fixed exchange rate for its
currency relative to the only currency considered at the time sufficiently stable to act as an anchor for the
international monetary system, the U.S. dollar, itself to be fixed to the price of gold. The International Monetary
Fund and the World Bank were established by the conference as well. By the early 1970s, in the face of high
inflation brought on by the costs of the Vietnam War, the Great Society, and the energy crisis, and suffering from
a widening and unprecedented trade deficit, the United States could not protect the dollar from a steep decline in
value. As a consequence, it was the United States that broke the Bretton Woods accord by abandoning the
dollar/gold standard in 1971. The decision was made by President Richard Nixon—without consulting with the
other Bretton Woods nations or even the U.S. State Department—in response to the dollar’s rapid depreciation
with respect to other currencies and gold and the increasing demands of other nations for the United States to
make good on its debts in gold. Within five years, none of the world’s major currencies was fixed anymore, either
to gold or to the U.S. dollar.

In July 1944, at the United Nations Monetary and Financial Conference in Bretton Woods, New Hampshire,
delegates of the 44 Allied nations of World War II laid the foundations of the postwar international monetary
system and open trade. (Alfred Eisenstaedt/Time & Life Pictures/Getty Images)
 Supranational Unions
Various supranational unions, also known as regional trade blocs, have provided their member nations with the
opportunity to coordinate their policies or have required a certain amount of coordination as a prerequisite for
membership. The European Union (EU) is the best-known example, consisting in 2009 of twenty-seven member
states with a common trade policy, various international bodies governing interactions between member states,
and a common currency used by sixteen of the member states. More than just an economic union, the EU also
guarantees the freedom of movement of people, goods, services, and capital among its member states, and helps
to guide the foreign policies of its member states. As of 2009, the members of the EU are Austria, Belgium,
Bulgaria, Cyprus, the Czech Republic, Denmark, Estonia, Finland, France, Germany, Greece, Hungary, Ireland,
Italy, Latvia, Lithuania, Luxembourg, Malta, the Netherlands, Poland, Portugal, Romania, Slovakia, Slovenia,
Spain, Sweden, and the United Kingdom. Croatia, Macedonia, and Turkey are official candidates for membership.
To join the EU, a candidate state must be a stable, functioning democracy; must demonstrate a respect for
human rights and the rule of law; and must have an economy that can participate competitively in that of the EU.
Other supranational unions are less developed than the EU, which has spent decades working toward the unity it
currently enjoys. The North American Free Trade Agreement (NAFTA), an economic agreement between Canada,
Mexico, and the United States, and in effect since 1994, provides an example of less integrated economic policy.
While NAFTA has lowered numerous trade barriers and tariffs and helped coordinate industrial and financial policy
among the three nations, its goals are more modest than those of the EU and do not include efforts to develop a
common currency or eliminate barriers to the free movement of workers.
Nevertheless, agreements such as NAFTA promote social and economic progress among member states and
encourage coordination of regional development and various spheres of policy among those members. The
Association of Southeast Asian Nations (ASEAN) includes the nations of Brunei, Cambodia, Indonesia, Laos,
Malaysia, Myanmar, the Philippines, Singapore, Thailand, and Vietnam. The African Union includes fifty-three
African states, with the possibility of moving toward an EU-like unity. The Union of South American Nations
(USAN) is specifically modeled after the EU and integrates two regional customs unions, the Andean Community

and Mercosur (Mercado Común del Sur). Member states include Argentina, Bolivia, Brazil, Chile, Colombia,
Ecuador, Guiana, Paraguay, Peru, Suriname, Uruguay, and Venezuela. A paper entity only at the time of this
writing, the hope is for USAN to eliminate tariffs among member states by 2019, integrate infrastructure such as
highways and pipelines, and move toward an EU-style political-economic community that coordinates the
development of foreign policy and trade with entities outside the union.
Economists and political scientists wonder how great economic stresses, such as the financial crisis and global
recession of the late 2000s, have affected and will continue to affect international policy coordination. Given the
vulnerabilities the crisis revealed, and the protectionist pressures the recession has presented, many world leaders
would agree with French president Nicolas Sarkozy’s sentiments that, “We must rethink the financial system from
scratch, as at Bretton Woods.”
Bill Kte’pi and James Ciment
 
See also:  International Economic Agreements. 
Further Reading
Branson, William H., Jacob A. Frenkel, and Morris Goldstein, eds. International Policy Coordination and Exchange Rate
Fluctuations. Chicago: University of Chicago Press, 1990. 
Haas, Peter M. Knowledge, Power, and International Policy Coordination. Columbia: University of South Carolina
Press, 1997. 
Krasner, Stanley, ed. International Regimes. Ithaca, NY: Cornell University Press, 1983. 
Ruggie, John. Constructing the World Polity: Essays on International Institutionalization. New York: Routledge, 1998. 
Inventory Investment
 
In a company, inventories are stored quantities of raw material, work in progress, or finished products. Raw
materials are goods bought from a supplier that must be transformed during the production cycle in order to create
finished products that may then be sold. Works in progress are unfinished products. A good is not a raw material,
a finished product, or a work in progress by virtue of its inherent properties, but by nature of the company’s
business. Thus, coffee may be a finished product for a producer or retailer, but a raw material for a company like
Starbucks. Inventory investment generally turns back into cash in the short term through the sale of finished
products. Inventory may become a permanent investment if the production cycle requires more than one year, or
because the company is not able to sell finished products within one year. The latter case may be the result of
strong competition, a product’s obsolescence, or economic recession.
Inventory investment also plays a role in macroeconomic analysis, as inventories represent a key bellwether for
national economies. Companies generally tend to accumulate inventory during economic downturns, while
maintaining optimal levels of inventory, or even experiencing shortages, during prosperous periods. Collectively,
these changes are referred to as “inventory cycles” and represent the shortest of the major business cycles the
economy experiences. Aggregate inventory levels are also a leading indicator that economists use to ascertain in
which direction the economy is likely to move in the short term.

For example, between November of 2008 and November of 2009, U.S. business inventories fell about 11 percent,
from $1.476 trillion to $1.313 trillion, indicating that companies were selling off goods and that the economy was
coming out of recession. However, the figures also help to explain why employment figures were slower to
respond to the recovery, as companies chose to sell off the goods they already had on hand rather than hire new
workers to make more. As a general rule, a company is successful if it is able to generate good returns using
minimal resources, including inventories. Resources invested in inventories may increase or decrease. An
increase in inventories, however, may be a result of good news (an increase in sales), a change in inventory
policy (a larger inventory may increase customer satisfaction), or a negative event (a reduction in sales). While
rises in inventory can sometimes bode ill for a company or the economy for a while—indicating slackening
demand—this is not always the case. Companies may expand inventory if they expect the economy to turn
around and demand to increase.
Inventory investment includes carrying costs such as insurance, warehousing, handling, goods deterioration, and/or
theft. Despite the costs, however, inventories are necessary in order to avoid interruptions in output, to maintain
sales, and to ensure consumer trust. Moreover, placing orders is costly because of the shipping and time required
to manage the order. Finally, like any other investment, inventories must be financed with capital, either in the
form of debt or equity—that is, by borrowing money thorough the issuance of bonds and by taking out loans from
banks or through issuing shares.
But capital is costly. With this in mind, companies must identify their optimal inventory level, which depends
primarily on the industry. A retailer needs inventories, for example, while an insurance company does not.
Additionally, inventory depends on the kind of policy companies wish to implement. If a company prefers an
aggressive management policy, it will minimize inventories, otherwise it will accept higher levels of inventory with
higher carrying costs but usually fewer lost sales.
Traditionally, an optimal inventory level could be identified by means of the economic order quantity (EOQ) model,
which takes into consideration carrying costs (which increase with higher quantities in stock) and restocking costs
(which decrease with higher quantities), in order to identify the quantity of inventory that minimizes the total cost.
More recently, beginning in the 1980s, a new and innovative approach in inventory management called “just in
time” (JIT) was implemented by Japanese companies. This approach, which seeks to reduce inventories to the
very minimum level (zero if possible), is based on a new kind of relationship with suppliers, with whom a high level
of coordination is needed. In the new relationship, suppliers are expected to provide raw materials only for
immediate need—thus, the number of orders is at the maximum level and inventory turnover is at a maximum.
This policy is not easy to implement, but highly efficient and profitable. To take one of many examples, JLG
Industries, a Pennsylvania-based manufacturer of aerial work platforms, was able to reduce inventory from 40
percent of sales in 1990 to 9 percent in 1998, with a net income that increased from $3.2 million in 1991 to $ 46.5
million in 1998, largely through the use of just-in-time inventory management.
Companies with low inventory levels may react faster to unexpected drops in product demand. They may
immediately reduce orders for raw materials, for example, and thus have less unsold product. Moreover, such
companies generally have fewer debts with suppliers in the short term, reducing the risk of becoming low on cash.
Companies with a high level of inventory (or with long-term purchasing contracts for fixed quantities) are less
flexible. If the economy slows down, they may experience cash stress. A drop in sales implies a reduction in cash
proceeds. If, in the meantime, they have a lot of raw materials and, consequently, outstanding payables, it may be
difficult to meet their obligations.
An increase in inventory always leads to cash absorption because of the increase in purchasing costs that must
be paid to suppliers. When this increase also corresponds to an increase in sales, the cash absorption is offset by
an increase in cash generated by collected sales. Otherwise, the company has less cash available for new
investments, meeting debt obligations, and paying shareholders.

A useful performance indicator for inventory management is inventory turnover, or the ratio of cost of goods sold to
inventory level. The ratio represents the number of times inventories are renewed in one year: the higher the ratio,
the more efficient the management.
Comparing the ratio of one company with that of others can help management determine if the inventory level is
typical for the industry, what the market trends are, and whether or not the company is being efficiently managed.
With respect to the latter, there might be a chance to improve profitability by reducing inventory stock. Generally
speaking, a company should keep inventories low, except when a more aggressive management policy negatively
affects profitability instead of improving it.
Laura Peressin, Giorgio Valentinuz, and James Ciment
 
See also:  Savings and Investment. 
Further Reading
Arvan, L., and L.N. Moses,  “Inventory Investment and the Theory of the Firm.” American Economic Review 72:1
(1982): 186–193,  and 73:1 (1983): 251. 
Followill, R.A., M. Schellenger, and P.H. Marchard.  “Economic Order Quantities, Volume Discounts and Wealth
Maximization.” Financial Review 25:1 (1990): 143–152. 
Johnson, H. Thomas, and Robert S. Kaplan. Relevance Lost: The Rise and Fall of Management Accounting. Cambridge,
MA: Harvard Business School Press, 1991. 
Sadhwani A.T., M.H. Sarhan, and D. Kiringoda.  “Just in Time: An Inventory System Whose Time Has Come.” Management
Accounting 67(December 1985): 36–44. 
Schonberger, Richard L. Japanese Manufacturing Techniques. New York: Free Press, 1982. 
Treml, H.E., and K. Lehn.  “Decentralization, Incentives, and Value Creation: The Case of JLG Industries.” Journal of
Applied Corporate Finance 13:3 (2000): 60–70. 
Investment, Financial
 
Financial investment is the acquisition of instruments such as corporate stock, government and private bonds,
foreign exchange, bank deposits, and other forms of loans by financial firms, businesses, organizations, and
individuals. Financial assets such as these are often referred to as “cash instruments” because their values are
determined in markets where they can be bought and sold relatively easily for cash. Also included among financial
investments are derivative instruments, also known as “derivatives,” such as options, futures, swaps, and other
instruments whose values are derived from such other assets as cash and real estate. Derivative instruments are
acquired in order to reduce risk, to provide insurance against certain losses and negative events, or for sheer
speculation. They may be bought and sold, but not as readily as cash instruments.
The proliferation of ever more exotic and complicated financial instruments over the past decade or so—such as
the aforementioned derivative instruments—has been widely blamed for the financial crisis that gripped world
markets in late 2008. Because the risks associated with these instruments were poorly understood—or
miscalculated—many investors, including institutional ones, found themselves in financial difficulty when the

market for these complicated instruments crashed. Moreover, burdened by having these instruments on their
balance sheets and unable to determine their value or to sell them, many financial institutions were forced to rein
in the credit they offered businesses and households, a phenomenon that helped turn the financial crisis into a
global recession.
 Uncertainty and Expectations
The prices of financial instruments generally depend on expectations of uncertain future outcomes. For example,
stock prices depend on the expected future performance of corporations, and a derivative such as a forward
foreign exchange contract depends on expected future exchange rates. Information about the future is incomplete,
and prices of financial instruments and their derivatives therefore may be driven by heuristics (simple rules of
thumb) or emotional sentiments. Price movements can be volatile when there is uncertainty about the future and
may diverge from long-run trends for extended periods of time.
Financial investment is distinct from what economists define as investment, which is the direct creation of physical
capital such as a factory, a machine, or a bridge. The purchase of newly issued stock or bonds may, of course,
end up funding the building of the factory or the purchase of tools, but this type of financial investment and
economic investment are different concepts. Much financial investment has a relationship to economic investment,
as when someone buys stock issued years ago or opens a savings account at a bank that uses the money to
provide revolving credit to consumers. In these latter cases, no new productive capital is created when the
financial investment is made.
 The Financial Sector
Purchases and sales of financial instruments and derivatives occur in the economy’s financial sector. When the
financial sector consists of mostly private firms, the term “financial industry” may be used in place of financial
sector. In most countries, the financial sector consists of intermediaries, formal exchanges, and over-the-counter
markets.
Intermediaries are institutions that offer one set of financial arrangements to those contributing funds and another
set of arrangements to those borrowing the funds. For example, commercial banks offer depositors a variety of
checking and savings accounts and certificates of deposit with different interest rates and maturities, and they offer
borrowers a variety of loans of different lengths, methods of payment, and charges. Intermediaries channel funds
from one set of financial assets to another set of financial assets, usually deposits to business and consumer
loans, and in the process they assume risks and incur costs.
Exchanges such as stock markets, options markets, and some futures markets are centralized markets in which
large numbers of buyers and sellers interact directly and set asset prices through a process of supply and
demand. Over-the-counter markets are dealers who link buyers and sellers one transaction at a time. Most bonds,
foreign exchange, and financial derivatives are traded in over-the-counter markets. These are less transparent
than exchanges because they reveal little information about prices and trading volumes; buyers and sellers only
see the price of one particular transaction.
 Economic Purposes
The fundamental economic purpose of the financial sector, and thus of financial investment, is to channel savings
to economic investment. The financial sector’s role in channeling funds to new projects and research activities is
critical to the economy’s long-run rate of growth. For example, without financial intermediaries to connect savers
and investors, there would be less investment and innovation, all other things being equal, because investors and
innovators would also have to be savers.
Financial investment also channels funds to cash-constrained consumers, a process that is economically
beneficial if it permits consumers to better allocate their purchases over time. For example, without mortgage loans

people would have to build their homes gradually as their income flows permit the purchase of building materials
and construction services. Also, financial investment reduces life’s risks because it permits people to save for
contingencies and borrow during emergencies.
 Costs
Financial intermediaries, exchanges, and markets are costly to operate, and they may introduce risks and
uncertainties when investments are bought and sold. In today’s high-income countries these sectors have a direct
cost of several percent of gross domestic product to operate and even more if regulatory costs are included.
Banks earn a spread between deposit rates and borrowing rates, and brokers, dealers, and financial firms charge
high fees for their services.
Modern financial sectors are also prone to systemic instability. For one thing, all inter-temporal transactions, or
those that take place over time, such as mortgages, are subject to default. In addition, the complexity of today’s
financial instruments and derivatives makes it impossible for any one investor, financial firm, or government
regulatory agency to fully grasp the risks of every financial investment. The many levels of derivative instruments
available in the modern financial sector imply that any one default or market failure can trigger many more defaults
and failures throughout the system. From an economic perspective, when the financial sector falters in its role of
channeling savings to investment, innovations, and consumption, there are very real economic consequences.
Instability in financial intermediaries, exchanges, and markets also results from the divergence in purposes of
sellers and purchasers of financial assets. In explaining the financial collapse during the Great Depression, British
economist John Maynard Keynes observed in his pathbreaking 1936 book, The General Theory of Employment,
Interest and Money, that before the development of modern financial systems,
enterprises were mainly owned by those who undertook them or by their friends and associates,
investment depended on a sufficient supply of individuals of sanguine temperament and constructive
impulses who embarked on business as a way of life….
Today, however,
as a result of the gradual increase in the proportion of the equity in the community’s aggregate
capital investment which is owned by persons who do not manage and have no special knowledge
of the circumstances, either actual or prospective, of the business in question, the element of real
knowledge in the valuation of investments by those who own them or contemplate purchasing them
has seriously declined.
Keynes feared that “when the capital development of a country becomes a by-product of the activities of a casino,
the job is likely to be ill-done.” Although Keynes focused on the stock market as a source of instability, the same
problem exists in all financial markets. For example, in government bond markets, prices are determined by
savers, speculators, financial firms, gamblers, and business firms that have no interest in the underlying
government agency that issued the bond.
Financial investment has thus become a major contributor to the booms and busts observed in modern
economies. It has proved difficult to find the optimal balance between the need for larger financial sectors to
facilitate the flow of funds from savers to investors, innovators, and consumers on the one hand, and the
increasing complexity that seems to generate occasional economic crises on the other.
 Why Financial Transactions Fail
There are several basic difficulties in carrying out a financial investment; all are related to asymmetric information,
which describes a situation in which one side of a financial transaction has more information about future profits
and the likelihood of repayment than the supplier of the savings.

There are two factors that play into the problem of asymmetric information situations, including:
1. Adverse selection in which persons who take out an insurance policy are likely to be those people who may
need it, undermining the actuarial statistical analysis insurance companies make to spread out risk; and
2. Moral hazard in which persons are likely to engage in risky behavior when they know the costs of that risky
behavior.
Information asymmetries point to a role for government policy. In a developed economy such as that of the United
States, the Securities and Exchange Commission (SEC) was created to oversee financial markets. Among other
things, it requires firms that issue stock or bonds to provide financial information to prospective buyers. In most
countries the government agencies that supervise banks require that financial statements be public so that
depositors and other holders of bank liabilities can judge the bank’s ability to meet its obligations. Government-
mandated information permits financial transactions to be completed where the fear of default, adverse selection,
or moral hazard would otherwise cause prospective buyers or sellers to shy away.
Financial intermediaries introduce a “principal– agent problem.” Banks, money managers, and hedge funds (the
agents), among other intermediaries, effectively play with other people’s (the principals’) money, and there may be
incentives that lead them to treat funds differently from what financial investors would prefer. For example, a profit-
maximizing bank might be tempted to invest in excessively risky assets because if things work out, the bank
owners stand to enjoy high profits, but if things do not work out, it is the depositors who suffer most of the losses.
The fear of bank failures has led many countries to provide depositors with deposit insurance. But unless banks
and other intermediaries are closely regulated, such insurance can worsen the principal–agent problem because
principals have less motivation to monitor agent activity.
The U.S. savings and loan (S&L) crisis, which occurred after a weakening of banking regulations in the early
1980s, illustrates some of the weaknesses of financial intermediaries. S&Ls were suddenly permitted to freely
determine interest rates on deposits and to make commercial loans after decades of regulated interest rates and
lending restricted to home mortgages. The deregulated S&Ls began expanding deposits by offering higher interest
rates on government-insured accounts and making risky commercial loans even though they had no experience in
assessing business risks. In a few cases, corrupt individuals acquired savings and loans to channel the savings of
depositors protected by deposit insurance to their business friends. U.S. taxpayers paid more than $100 billion to
cover the bank losses and the stolen deposits.
 Explaining the Variety
The problems of moral hazard, adverse selection, asymmetric information, fraud, and contract enforcement explain
why intermediaries, exchanges, and over-the-counter markets coexist: each has its advantages and
disadvantages in dealing with these problems. For example, relatively inexpensive financial markets such as bond
and stock markets can exchange the stocks and bonds of well-known corporations, whose value can be easily
judged by most savers. Less-well-known borrowers rely on banks, which devote resources to investigating and
monitoring small business firms and their projects. Financial intermediaries such as banks, pension funds, mutual
funds, and insurance companies are good at pooling risk.
The creation of new financial institutions and instruments is called “financial innovation.” An economy that has
experienced a large amount of financial innovation and thus has a variety of intermediaries and markets is said to
have a “deep” financial sector.
 Problems of Financial Depth
A recent example of financial innovation is the collateralized debt obligation (CDO), a derivative that is a claim to
some share of a large bundle of financial instruments. For example, a CDO is created when a bank originates

mortgages or auto loans, puts them together into one large bundle, and then sells shares in the earnings from the
mortgages or auto loans to investors, pension funds, hedge funds, and other banks. But CDOs are not simple
shares. To make them as profitable as possible for the loan originators, CDOs are split into separate “tranches,”
each with a different rate of return and a different priority for receiving the returns on the underlying mortgages.
Purchasers of a share in the top tranche are the first to get paid from the returns on the whole bundle of
mortgages, and the purchasers of the other tranches are paid only after the higher tranches receive payment. The
bottom tranche, sometimes referred to as “toxic waste,” stands to earn a relatively high stated interest rate but
only after all the other tranches are paid. The tranches are carefully structured to gain the highest possible ratings
for each one. The top tranche is normally awarded an AAA rating, given only to financial instruments with no risk.
Its share of the total bundle must be small enough to make it highly unlikely that the total returns on the whole
bundle will not be large enough to fully service that upper tranche.
The AAA tranche can be quite large even when the underlying instruments are risky. For example, the top tranche
of U.S. CDOs of subprime mortgages, home loans to relatively risky borrowers, issued during 2005–2007 included
about 80 percent of all the mortgages in the total CDO. Only if more than 20 percent of subprime borrowers
stopped servicing their debt would the AAA-rated tranche no longer earn full returns, which was considered highly
unlikely during the optimistic early 2000s.
CDOs of subprime mortgages played a central role in causing global financial markets to collapse in 2007 and
2008. It turned out that many banks that originated subprime mortgages had enticed borrowers with easy
introductory interest charges for the first two or three years, while U.S. housing prices were clearly in a bubble in
many parts of the country. Securitization also led banks to encourage loan officers to issue mortgages with little
concern for borrowers’ ability to service the debt, since the bank would not have any risk once the loans were
bundled and sold as CDOs. Regulators and the banks themselves should have become suspicious when loan
officers openly began to refer to the subprime mortgage market as “the liar’s market.” When the housing price
bubble burst, defaults became much more likely than the ratings suggested.
 Complexity and Risk
There was a second financial innovation meant to enhance the safety of CDOs that also failed: credit default
swaps (CDSs). CDSs covering CDOs were options that paid out the full value of the CDO in the case of default.
Interestingly, these derivative instruments were not only purchased by investors in CDOs, but also by hedge
funds, speculators, and plain old gamblers who did not own any CDOs but just wanted to place a bet that the
CDOs would default in the future. Such gambles are comparable to the purchase of a fire insurance policy on a
neighbor’s house; if the neighbor’s house burns down, the policy holder receives a windfall equal to the value of
the house without actually owning and losing it. Of course, the neighbor also might have purchased insurance, in
which case the insurance company has to pay out twice the value of the house. CDSs worth many times the
value of the underlying CDOs were sold, exposing the sellers to huge potential payments in the case of the
default of specific CDOs.
The over-the-counter CDS market is another example of Keynes’s point about the divergence of interest in the
financial instruments and the economic activities that underlie those financial instruments. By 2007, it is estimated,
more than $50 trillion in credit default swaps had been contracted by investors, hedge funds, banks, and other
assorted gamblers throughout the global financial industry.
One of the largest sellers of CDSs for tranches of the subprime mortgage CDOs was the U.S. insurance firm AIG,
which sold the CDSs through its London-based Financial Products Division. Britain did not require AIG to hold
reserves on the CDOs as long as the firm’s own financial models showed reserves were not necessary. While
AIG’s customers clearly thought there was risk, since they paid billions of dollars in premiums for the CDSs, the
company’s London office set no reserves aside and booked all premiums as pure profit. AIG’s management paid
the 400 employees in its London office yearly bonuses equal to about one-third of the premiums collected on the
CDSs, or more than $1 million per year per employee.

By 2008, the AAA-rated tranches of the subprime mortgage CDOs had proved to be risky after all, and AIG was
called on to pay out the losses. Since there were no reserves to cover the losses, the U.S. government had to
channel over $180 billion to AIG to keep the firm solvent. With its widespread life insurance, fire insurance, auto
insurance, and other insurance businesses, the firm was deemed to be too important for the U.S. economy to fail
and leave millions of people and businesses uninsured.
In some instances the innovations raised incomes of bankers and financial executives at the long-run expense of
taxpayers, duped investors and pensioners, and foreclosed homeowners. Overall, rather than spreading risk and
permanently increasing homeownership, financial innovation created a global financial system that threatened the
stability of the world’s economy.
 The Future
In the aftermath of the 2008–2009 financial crisis, many governments grappled with the need for financial
regulation and reform. Should financial markets and intermediaries be more closely regulated to prevent excesses
like subprime mortgages and the derived CDOs and CDSs that spread the losses throughout the world? Should
certain instruments or markets simply be prohibited? At the time of this writing, it was not yet clear where each
national government would set the balance between the further financial innovation favored by the financial
industry and the economic stability favored by most savers, workers, and pensioners.
Hendrik Van den Berg
 
See also:  Financial Markets;  Savings and Investment. 
Further Reading
Greenlaw, D., J. Hatzins, A. Kashyap, and Y.S. Shin.  “Leveraged Losses: Lessons from the Mortgage Market Meltdown.”
Proceedings of the U.S. Monetary Policy Forum, 2008. 
Keynes, John Maynard. The General Theory of Employment, Interest and Money. London: Macmillan, 1936. 
Minsky, Hyman P. Can “It” Happen Again? Armonk, NY: M.E. Sharpe, 1982. 
 
Ireland
 
Once one of the economically backward countries in Western Europe—a nation whose poverty sent millions of
inhabitants abroad in search of opportunity—Ireland emerged in the 1990s as one of the world’s fastest-growing
economies. It was a mecca for high-tech and other foreign companies seeking a well-educated, English-speaking,
low-cost workforce with access to the European Union market. Between 1990 and 1995, the Irish economy grew

at an impressive 5.14 percent annually. In the second half of the decade, it expanded at rates unseen outside the
developing dynamos of Asia, earning Ireland the nickname “Celtic Tiger” (after the fast-growth Asian Tigers of the
1980s).
As it turned out, however, the rapid growth of the Irish economy in the 1990s and early 2000s was fragile and
unsustainable, dependent on a prosperous world economy and pumped up by a bubble in property values. When
the world’s credit markets seized up in late 2008, Ireland was particularly hard hit, experiencing such an enormous
drop in economic output and rise in unemployment that some economic analysts began speaking of an “Irish
Depression.”
Construction projects stand idle along the River Liffey in downtown Dublin in 2009. After a decade of spectacular
growth beginning in the mid-1990s, Ireland suffered one of the worst economic collapses of any developed nation
since the Great Depression. (Bloomberg/Getty Images)
 The Transformation
How was the Irish economy transformed from a sleepy economic backwater into the Celtic Tiger in the first place?
Part of the story is explained by convergence—the idea that it is easy for countries that are behind to “catch up”
by adopting the latest technology and production methods developed by more economically advanced countries.
Convergence was certainly part of the story in Ireland’s case, as the country was able to catch up with the rest of
the industrialized world by implementing cost-saving technology developed elsewhere.
Yet the extraordinary growth of the 1990s and early 2000s cannot be explained entirely by convergence, since
Ireland not only converged on other developed countries; it also surpassed many of them. In per capita terms,
Ireland’s gross domestic product (GDP) rose to at least 20 percent higher than that of Germany, France, Italy, and
the United Kingdom. In just two decades, Ireland not only caught up with other Western industrialized giants, but
also became a frontrunner. To understand the economic growth in Ireland, one must look at how the institutional
framework or “rules of the game” have changed there over the past several decades.
The roots of Ireland’s transformation began in the 1950s, when it experienced poor economic performance relative
to other European countries. While the Continent profited by a postwar economic boom, average annual growth
rates were an anemic 2 percent in Ireland. As a result, about one-seventh of the entire national population
emigrated to other countries between 1950 and 1960. Beginning in the mid-1960s, however, the Irish government
began a series of policy changes that lowered trade barriers and made the country more attractive for direct

foreign investment. These policy changes included the lowering of tariffs starting in 1964 and the signing of the
Anglo-Irish Trade Agreement in 1965. As a result of these changes, annual growth rates rose to comparable
levels with the rest of Western Europe. In the 1960s, Ireland increased GDP by an average of 4.2 percent each
year. And although Irish growth rates from 1950 to 1973 were not high enough to catch up with countries like
France and Belgium, an institutional foundation was laid that would lead to greater growth in subsequent years.
Following the 1973 oil shock, the Irish government attempted to boost overall demand in the country by
implementing a series of policies aimed at jump-starting aggregate demand and, according to Keynesian theory,
the economy as well. National pay agreements forced wages and salaries to rise, government agencies hired
workers in an effort to fight unemployment, transfer payments swelled, and public infrastructure projects increased
capital expenditures. In the end, however, these macroeconomic policies were not effective at stimulating the Irish
economy. The nation’s average annual growth rate was a meager 2.2 percent from 1973 to 1992. To make
matters worse, efforts to boost consumer demand in the country led to a financial crisis. To finance its
expansionary fiscal policy, the Irish government had borrowed heavily, which in turn led to a high debt-to-GDP
ratio. The government was facing a serious budget deficit problem and had to cut spending drastically.
 Crisis Creates Opportunity
Irish policy makers aggressively cut government spending in nearly all areas during the late 1980s. Outlays for
agriculture, roads, housing, education, and the military all were cut by at least 5 percent; and in 1987, the entire
operating budget was cut by 3 percent. In addition, the scope of the government was shrunk, as numerous
agencies were eliminated and thousands of government employees were forced to return to the private sector. As
a result of these sweeping changes, the federal deficit was eliminated by 1987 and the debt-to-GDP ratio fell to
manageable levels by the early 1990s.
Ireland was poised for its economic takeoff. With government spending under control, policy makers were able to
create a more competitive tax system. From 1989 to 2000, the standard income tax rate was cut from 35 percent
to 24 percent. Perhaps even more important was a reduction in corporate taxes, from 40 percent in 1996 to 12.5
percent in 2003. The latter cut helped spur capital investment, especially in high technology.
Combined with Ireland’s new openness to trade in the 1960s and its entry into the European Community (now
European Union, EU) in 1973, the new tax policies proved highly effective in spurring economic growth. According
to the Fraser Institute’s economic freedom index—which measures the degree of competitiveness in national
economies—Ireland rose from the sixteenth freest economy in the world in 1980 to seventh in 2000. Many
analysts point to this as the foundation of Ireland’s newfound prosperity, as policies consistent with economic
freedom—steady taxation, responsible government spending, and minimal barriers to trade—allowed
entrepreneurial activity to thrive. Other factors in the economic boom included the effects of EU subsidies on
education and infrastructure spending and the role of industrial policy. For example, the availability of an educated
workforce, combined with low corporate taxes and access to European markets through the EU, gave Irish state
development organizations such as the Industrial Development Agency the ability to lure high-tech firms such as
Dell and Microsoft to the country.
 From Boom to Bust
The dramatic economic growth achieved by Ireland between the mid-1990s and the mid-2000s was followed by
an equally dramatic economic collapse, with many of the gains in employment and GDP nearly wiped out.
Unemployment nearly doubled, to more than 11 percent by the end of 2008, while the overall GDP shrank by 7.1
percent during the fourth quarter of that year alone. In September 2008, the government of Ireland officially
declared the country to be in a recession. So bad were the indices and forecasts, however, that many economists
referred to conditions as the Irish Depression.
Some of this catastrophe was, no doubt, a result of the global economic downturn. Heavily dependent on exports,
Ireland suffered mightily as demand for its products shrank and as foreign and domestic manufacturers cut

production. At the same time, some of the forces that made Ireland among the hardest hit of European Union
economies in the 2007–2009 recession were home-grown. As in the United States—and, closer to home, Spain—
the main domestic source of the trouble was a bubble in the housing market that had been inflated by rising
incomes and employment, a growing population of young people eager to buy their own homes, loose credit
standards, and speculative frenzy. Land and property values in Dublin, its suburbs, and even far-flung agricultural
areas of the country had seen enormous increases in the early 2000s. Developers built thousands of new homes
and millions of square feet of commercial space to capitalize on the nation’s growing prosperity—to which all the
construction contributed further. Irish banks contributed to the speculative frenzy by offering mortgages equal to
100 percent of the value of a home and loosening standards on who could obtain a mortgage. In addition, interest
rates fell on the heels of the Irish economy’s linking to the economy of the EU with the introduction of the euro in
2002. This encouraged more people to borrow money, both to buy homes and to borrow against them.
By 2007, the tide in the property market began to turn. With too many housing units on the market and with the
ratio of home prices to income at an all-time high, prices began to drop. Many who had speculated in real estate,
as well as all those who purchased homes too expensive for their budgets, now proved unable to meet their
mortgage obligations—which forced even more property onto the market. Nor was the crisis confined to
homeowners and speculators. At the height of the property boom, the construction sector had an outsized role in
the national economy, accounting for some 10 percent of GDP and more than 12 percent of employment. With the
bust in the property market, many construction workers suddenly found themselves unemployed (though some
were foreigners who left the country when the work dried up).
The bursting of the property bubble escalated into a broad-based financial crisis as lending institutions were left
with massive bad loans on their books. The crisis worsened with the freezing up of the international financial
system in 2008, as Irish financial institutions, operating in a relative small national economy, had been heavily
dependent for their liquidity on international monetary transfers. When that liquidity dried up, the banks found
themsleves heavily exposed, which forced the government to bail out some institutions with multibillion-euro loans
and to nationalize others. Although the infusion of money temporarily halted the crisis, it did little to lift Ireland out
of its economic morass. Indeed, economic forecasters predicted even worse times to come, with some forecasting
as much as a 25 percent drop in GDP by the end of 2010—a rate of contraction not experienced by other
industrialized economies since the Great Depression of the 1930s. Indeed, from its peak of $263.7 billion in 2008,
Ireland’s GDP fell to just $203.9 billion in 2010, a drop of 22.7 percent.
Joshua C. Hall and William J. Luther
 
See also:  Greece;  Portugal;  Spain;  United Kingdom. 
Further Reading
Crafts, N.  “The Golden Age of Economic Growth in Western Europe, 1950–1973.” Economic History Review 48:3
(1995): 429–447. 
Gwartney, J., and R. Lawson. Economic Freedom of the World: 2008 Annual Report. Vancouver, Canada: Fraser
Institute, 2008. 
Powell, B.  “The Case of the Celtic Tiger.” Cato Journal 22:3 (2003): 431–448. 
“Irrational Exuberance”

 
The phrase “irrational exuberance” has come to be associated with any unexplained (at least in pure economic
terms) major upturn in the stock market or, indeed, any other real or financial market, or the economy as a whole.
An implied warning, the term was coined by Federal Reserve chairman Alan Greenspan in a dinner speech at the
American Enterprise Institute in Washington, D.C., during a stock market boom in late December 1996. The
phrase almost immediately gained popularity in the financial markets, serving as a wake-up call for investors at
the time. The run-up taking place in the stock market, Greenspan implied, did not make sound economic sense.
Either the comment was prescient or it had a direct effect on market perceptions—perhaps both. In any event,
financial markets around the world underwent a significant decline in value in the days that followed. Although the
run-up resumed and even accelerated shortly thereafter, Greenspan’s term had become a catchphrase among
market analysts and commentators to describe steep, unexpected, unexplained increases in the price of any real
or financial assets.
“Irrational” refers specifically to investor decisions that are not rooted in logic or sound economic theory.
“Exuberance” refers to the high levels of positive emotion—specifically, enthusiasm and excitement—that attend a
run-up in prices of real or financial assets. In conjunction, the words were applied in some quarters to the dot.com
bubble of the late 1990s, when Internet start-up companies and technology stocks in general were finding hordes
of investors and attracting rampant speculation. Amid talk of a “new economy”—in which value would be
measured not in terms of production or even profits, but online traffic and the potential it implied—traders became
unduly excited about and inordinately invested in vague financial prospects. Economic fundamentals generally did
not support this view and, according to some analysts (especially in retrospect), may have actually pointed in the
opposite direction. In short, stock market traders and investors in general were being irrationally exuberant about
the pricing of these stocks at the time. Once this realization set in, a different psychology took hold and stock
market prices dropped sharply.
Notable among the follow-up voices invoking Greenspan’s phrase was that of Yale University economist Robert
Shiller, who used it as the title of his 2000 book about the dot.com phenomenon. Published at the peak of that
boom in March 2000, Shiller’s book presented a series of arguments for why stocks—of technology companies in
particular—were overvalued at the time. His work like Greenspan’s 1996 dinner speech, proved prescient, as the
dot.com bubble burst in the weeks and months that followed.
In the first decade of the twenty-first century, economists and journalists began invoking “irrational exuberance” in
reference to another kind of bubble—this time in the housing market. Shiller, for example, issued a second edition
of his book in 2005 predicting the collapse in home prices that began the following year. In another expression of
irrational exuberance in the financial marketplace, the price of houses across the country rose to untenable levels.
Fueled by easy credit, median housing prices in certain areas of the country were six to nine times higher than
median annual income. This was just one of a multitude of measures reflecting the fact that property values were
no longer conforming to sound economic theory. Inevitably, when prices began to plummet in the most inflated
areas, the ripple effect was felt throughout the economy.
Irrational exuberance invariably results in asset pricing bubbles like those in the dot.com and housing markets
during the late 1990s and mid-2000s. Yet the phenomenon is hardly new and hardly confined to the United
States. Indeed, it has recurred throughout the world since the advent of the market economy in the Middle Ages.
A manic surge in the price of tulips in Holland during the 1630s was perhaps the first example of an asset price
bubble and the irrational exuberance that gave rise to it. At the height of the tulip craze, speculators were trading
small fortunes in gold, jewelry, and land for a single bulb. Indeed, the term “tulipmania” came to be used for the
very phenomenon later encapsulated in the phrase “irrational exuberance.” In the interim—and into the twenty-first
century—the phenomenon became increasingly frequent and widespread. As early as 1841, the Scottish journalist
and poet Charles Mackay reflected on such episodes of human folly in a classic book titled The Extraordinary
Popular Delusions and the Madness of Crowds. It might have been titled Irrational Exuberance.
Some analysts and economic experts refer to such episodes as essentially rational “mistakes”—that is, that traders

who fuel the run-ups are, in fact, acting rationally based on the information they have at the time. Only in
hindsight, it is argued, can their decisions be characterized as irrational. According to the countervailing view,
however, many of these episodes can be predicted to end badly—even while they are occurring. The use of the
term “irrational exuberance” reflects the latter view. Those who invoke the phrase clearly imply the need for more
careful and deliberate valuations and projections in the rise and fall of asset prices.
Omar J. Khan
 
See also:  Asset-Price Bubble;  Dot.com Bubble (1990s-2000);  Greenspan, Alan. 
Further Reading
Dash, Mike. Tulipomania: The Story of the World’s Most Coveted Flower and the Extraordinary Passions It Aroused. New
York: Random House, 2001. 
Mackay, Charles. The Extraordinary Popular Delusions and the Madness of Crowds. Amherst, NY: Prometheus, 2001, 
reprint.
Shiller, Robert J. Irrational Exuberance.  2nd ed. Princeton, NJ: Princeton University Press, 2005. 
Israel
 
A geographically small nation located at the eastern end of the Mediterranean Sea, Israel—with a population of
approximately 7.3 million—was established as a homeland for the Jewish people in 1948. With its network of
kibbutzim, or producer cooperatives, Israel began its existence as a semi-socialist state. Economic stagnation in
the 1970s and 1980s led the government to liberalize the economy, and by the 1990s the country had emerged
as one of the most dynamic economies in the Middle East, with a vigorous agricultural, tourism, and
manufacturing sector, the latter including world-class defense, medical technology, and software sectors. Israel
has also benefited from the Jewish Diaspora, which has provided much-needed capital throughout the country’s
history.
Home to the ancient Hebrew tribes in the second and first millennium BCE, Israel was occupied by the Romans in
the second century BCE, who eventually expelled many of the Jews after a series of unsuccessful revolts. For the
next 2,000 years, Jews lived in exile, largely in Europe, Southwest Asia, North Africa, and eventually North and
South America. Their religious tradition, however, spoke of a return to the “promised land.” Ironically, however, it
was secular Jews, influenced by the nationalist currents sweeping Europe in the nineteenth century, who
developed Zionism, the political movement for a modern Jewish state in the historical land of Israel.
Zionists’ efforts to encourage Jews to return to move to Palestine were given a tragic boost by the Holocaust, and
by the late 1940s, hundreds of thousands of Jews had settled in British-controlled Palestine, a development that
antagonized local Arabs. Unable to reconcile the two groups, the British abandoned Palestine in 1948 but not
before the Jewish population declared an independent state. War between the Jews and surrounding Arab states
ensued, leading to a Jewish victory and the exile of hundreds of thousands of Palestinian Arabs. Relations with
both the Palestinians and surrounding Arab states have remained tense, with periodic wars and rebellions
occurring through the present day.

Many of the Zionist leaders who founded Israel were influenced by socialist ideals, and the economy they
established in Israel included a major role for the state in directing economic development, along with the above-
noted network of kibbutzim and a powerful union sector. Emerging from war, with a large population of refugees,
Israel in its early years was forced to implement strict austerity measures, rationing, and price controls. By the
early 1950s, however, the government was able to ease restrictions while the economy benefited from U.S. aid,
large transfers of capital from the Jewish Diaspora, German reparations for the Holocaust, and the sale of Israel
bonds abroad, largely to Jews living outside Israel. All of this capital allowed for massive investment in
infrastructure and education.
The government also instituted a policy of import substitution, which included high tariffs on imported goods, and
state support for critical industries and subsidies to the export sector, including agriculture. Between 1950 and
1965, Israel enjoyed one of the fastest gross domestic product (GDP) growth rates in the world, averaging 11
percent per annum, though with the huge influx of immigrants this only amounted to GDP per capita growth of
about 6 percent. Despite these gains, Israel was hampered by one critical drawback—its need to spend large
amounts on defense. In the wake of the 1967 and 1973 wars, Israel’s defense budget amounted to an astonishing
30 percent of gross national product, though much of this was offset by military aid from the United States.
In 1977, Israel decided to replace a fixed exchange rate for its national currency, the shekel, with a floating one.
The result was a crippling inflationary spiral that dramatically reduced growth rates through the mid-1980s and
required the government to put into place restrictions on capital movements into and out of the country. At the
same time, the government moved to liberalize the economy, deregulating many industries, privatizing some
government enterprises, and allowing more market forces to determine the allocation of economic resources. But
while reducing its role in directing economic development, the government expanded its social welfare obligations,
introducing a national health system and increasing social security–style payments to the elderly and disabled,
thus helping to ease the dislocations and growing income inequality resulting from economic liberalization.
The policies were a major success. By the late 1990s, Israel had emerged as a high-tech leader, as well as a
leading exporter of defense and medical technology, cut diamonds, and winter and citrus crops. At the same time,
the country also came to attract much foreign investment, particularly in its software and Internet sectors, which
drew large amounts of venture capital from the United States, though this left the country exposed to the dot.com
bust of the early 2000s. In 2003, for example, the Israeli economy shrunk by more than 1 percent.
As for the global recession of the late 2000s, Israel has weathered it relatively well, posting GDP growth of about
5 percent in 2008 and a contraction of 0.3 percent in 2009, relatively small by developed-world standards for that
troubled year in the global economy. By 2010, it was back to 4.7 percent. Economists cited several reasons for
this success, including the government’s conservative macroeconomic policies and its tight regulation of the
financial sector, which meant that Israeli banks did not engage in the kinds of risky investment decisions that
brought so much grief to financial institutions in the United States and several countries in Europe. By 2010, Israel
was ranked forty-first in the world in terms of total GDP, a remarkable achievement for such a small nation, and
twenty-seventh by GDP per capita, putting the country at the lower end of industrialized nations.
James Ciment
 
See also:  Middle East and North Africa. 
Further Reading
Ben-Bassat, Avi, ed. The Israeli Economy, 1985–1998: From Government Intervention to Market Economics.  Cambridge,
MA: MIT Press, 2002. 
Nitzan, Jonathan, and Shimshon Bichler. The Global Political Economy of Israel. Sterling, VA: Pluto, 2002. 
Senor, Dan, and Saul Singer. Start-Up Nation: The Story of Israel’s Economic Miracle. New York: Twelve

Publishers, 2009. 
 
Italy
 
Home to the Roman Empire and birthplace of the Renaissance, Italy has been central to the development of
Western civilization for more than two thousand years. Beginning in the late Middle Ages, various city-states in
Italy established trade links throughout the Mediterranean region and as far away as India and even China. Early
Italian bankers, particularly in Venice, pioneered modern accounting, bookkeeping, and business finance—
innovations that later spread to other parts of Europe and led to the birth of modern capitalism.
For all of these innovations, Italy lagged behind Northern Europe both politically and economically in the
eighteenth and nineteenth centuries. It remained divided among various states until unification in the latter half of
the nineteenth century and was far poorer than much of the rest of the Continent. The post–World War II
economic boom, however, lifted Italy to the front ranks of industrialized countries—today it ranks seventh
worldwide by gross domestic product (GDP), with an economy based on manufacturing, services, tourism, and the
production of high-end artisan goods.
The Italian economy has several structural weaknesses, including a stark division in wealth between the northern
and southern halves of the country and a rapidly aging population whose pensions are eating up an ever-larger
share of the national budget. Since the 1990s, the country has experienced stagnant growth, a situation
compounded by the financial crisis of the late 2000s. Nevertheless, because of its general economic conservatism
—practiced by both households and financial institutions—Italy has not experienced the same economic tumult as
the United States and a number of other European economies.
 A History of Division
Inhabited since Paleolithic times, the Italian Peninsula was the center of the Roman Empire, which ruled the
Mediterranean world and much of the Middle East from the second century BCE through the fifth century CE. After
the fall of the Roman Empire, Rome itself became the home of the Roman Catholic Church, one of the only pan-
European institutions to survive imperial collapse. At the same time, however, Italy went into a prolonged period of
conflict and stagnation, divided politically among warring city-states.
From the 1400s through the 1600s, the peninsula underwent an economic and cultural rebirth, known as the
Renaissance. In this period, Italian artists created some of the greatest masterpieces of Western civilization even
as their more commercially minded countrymen developed the foundations of the modern capitalist economic
order.
After centuries of political division and occasional conquest by outsiders, Italy was unified under royal rule in the
1860s. But the country remained economically divided between an industrializing and modernizing North and an

agriculturally based South, where landownership was consolidated in the hands of a small landlord class and
feudal economic relations persisted. Hundreds of thousands of impoverished peasants fled the region in the late
nineteenth and early twentieth centuries, either for the cities of northern Italy or across the Atlantic to the
Americas.
Economic and political chaos in the wake of World War I led to the rise of an Italian fascist regime. Under dictator
Benito Mussolini, Italy pioneered a corporatist economic and political order in which society was organized into
economic interest groups controlled by the state. While promising prosperity and liberation from class struggle,
corporatist fascism as developed under Mussolini largely benefited the interests of the state and its business elite
allies.
 Modern Industrial Democracy
With the end of World War II and the defeat of fascism, Italy was established as a democratic republic and
incorporated into the emerging Western European economic and political order. It was an original member of the
European Economic Community (later the European Union), whose founding documents were known as the
Treaties of Rome, after the Italian capital, where they were signed.
Italy also became an integral part of the transatlantic political and economic system that emerged in the wake of
World War II. The nation’s rapid economic growth during that period—often referred to as the “Italian economic
miracle”—saw per capita GDP rise from $3,800 in 1950 to $10,400 in 1973. As for other Western European
nations, Italy’s recovery was aided in the late 1940s by the U.S. Marshall Plan, which included massive U.S.
economic aid aimed at jump-starting war-torn economies and preventing the spread of leftist and communist
governments. Italy’s extraordinary growth in the first several decades after World War II was assisted as well by
the extraordinary economic performance of much of Western Europe and North America, which was a boon to
Italian exports.
Rapidly rising oil prices and widespread recession in much of the industrialized world led to stagnant growth in the
1970s. But with the return of global economic prosperity and falling oil and natural resource prices in the 1980s,
Italy prospered, for a time reaching GDP per capita parity with Great Britain.
During the postwar era as a whole, Italy developed a unique form of industrial capitalism. Unlike those of other
major European countries, Italy’s economy was dominated by small and medium-sized companies, many of them
family-owned and-run. While Italy was home to a variety of mass-production industries—everything from cars to
tires to household appliances—it specialized in the production of high-end consumer goods, including clothes,
shoes, and other leather products.

The Italian automobile industry, known for some of the world’s most popular brands and stylish designs, helped
fuel the nation’s “economic miracle” of the post–World War II era. Fiat has bought several major domestic
manufacturers and a stake in Chrysler. (Damien Meyer/AFP/Getty Images)
By the late 1990s, however, Italy was once again stagnating economically, dragged down by the impoverished
South (or Mezzogiorno), corruption, massive government debt (reduced somewhat before Italy’s conversion to the
euro at the beginning of 1999), political paralysis, and an aging population. While much of Europe was
experiencing modest growth from the late 1990s to the mid-2000s, Italy saw almost none, averaging well under 2
percent per year for most of that period.
Economists point out, however, that the Italian economy is often in better shape than statistics suggests because
of its large informal sector, which may account for as much as 15 percent of national economic activity. In
addition, Italian households tend to save more and spend less than those in the United States and many other
advanced industrialized countries. Credit card and household debt remain a fraction of that in the United States,
for instance. Italian homeowners also tend to carry smaller mortgages than those in many other countries, which
helped the country avoid the housing bubble experienced in the United States, Spain, and the British Isles in the
early and mid-2000s.
All of these factors helped Italians weather the financial crisis and recession of 2008–2009 better than citizens of
the United States and other European countries. In addition, Italy’s conservative bankers largely stayed away from
the more exotic financial instruments, such as mortgage-backed securities, which brought down financial
institutions in the United States and other Western European countries during the 2008–2009 financial crisis.
Still, the country’s weak fiscal picture in 2011—its debt stood at 116 percent of GDP, second highest in the
European Union after Greece—put its economy in jeopardy. The Greek sovereign debt crisis made investors wary
of bonds issued by other Eurozone members with troubled finances, forcing them to pay more to borrow money to
keep their governments solvent, thereby exacerbating their fiscal problems. Of all the Eurozone members facing
such a crisis, Italy was by far the weakest. And because so much of its debt was financed by other Eurozone
lenders, notably France, its fiscal situation threatened to send the European Union economy and, indeed, that of
the rest of the world into a new recession. As with Greece, part of the problem was political, in this case a
government, run by longtime Prime Minister Silvio Berlusconi, mired in corruption scandals and unable or unwilling
to impose the harsh austerity measures demanded by foreign lenders, including other Eurozone governments,
such as Germany and France. In November 2011, the crisis came to a head, forcing Berlusconi to resign after 17

years in power, replaced by an economist named Mario Monti.  
James Ciment
 
See also:  France;  Germany;  United Kingdom. 
Further Reading
Hearder, Harry, and Jonathan Morris. Italy: A Short History.  2nd ed. New York: Cambridge University Press, 2001. 
Organisation for Economic Co-operation and Development (OECD).  “Economic Survey of Italy 2009.”
Schioppa, Fiorella P. Italy: Structural Problems in the Italian Economy. New York: Oxford University Press, 1993. 
 
Japan
 
An island nation of about 125 million people, Japan has the third-largest economy in the world, with a gross
domestic product (GDP) of about $4.3 trillion annually in the late 2000s. An ethnically homogenous country with a
history stretching back thousands of years, Japan emerged onto the world stage as a rising industrial power in the
late nineteenth century, the first non-European (or non-European-settled) nation to achieve such a status.
After a period of militaristic rule and imperial expansion in the first half of the twentieth century, and defeat in
World War II, Japan emerged as a thriving capitalist democracy in the second half of the century, even challenging
U.S. economic hegemony for a time in the 1980s. But the boom led to speculative excess, particularly in real
estate, setting off a financial crisis and a period of long-term stagnation in the 1990s and early 2000s.
Just when the Japanese economy began to recover, it was hit by the global financial crisis and recession of the
late 2000s. While the Japanese financial sector has not been as deeply affected as its counterparts in the United
States and some European countries, the economy as a whole—deeply dependent on exports—has been hard hit
by falling demand, sinking it into a new slump.
 Economic History to the Meiji Restoration
Human habitation of the Japanese archipelago stretches back to the Paleolithic Era. Heavily influenced by
Chinese civilization, with peoples and ideas arriving via the Korean Peninsula, Japanese civilization and the first
Japanese state emerged around the third century CE, with Buddhism following several hundred years later.
Another import was the Chinese form of bureaucratic government—which laid out codes for private
landownership, granting equal land plots to all adult males, and a taxation system to finance the imperial
government—under Prince Shotuku in the seventh century.

By the eighth and ninth centuries, however, the equal land allotment system had broken down, as the central
government weakened. Monasteries, imperial retainers, and local landlords, or daimyo, began consolidating more
and more land and forcing peasants to work for them, though these peasants never became serfs like their
European counterparts in the same period. To maintain order, local landlords organized armies of warriors known
as samurai.
During this period, there was often great tension between the centralizing efforts of the shogunate, or imperial
household, and local landlords, leading to periodic strife that further impoverished the peasantry. Japan’s feudal
order of powerful landlords, a weak central government, and a cowed peasantry persisted for centuries after it had
disappeared in Europe. Under the Tokugawa Shogunate, which emerged in the late 1500s, there was some effort
to curb the power of the daimyo, though most avoided taxation and other forms of control from the central
government, with the burden of supporting that government falling on the peasantry.
But there were also countervailing forces at work during the Tokugawa Shogunate, which lasted until the Meiji
Restoration of 1867. While the economy largely rested on agriculture, especially rice, trade in other goods—cloth,
cooking oil, sugar, paper, and iron—emerged. Urbanization also accelerated, with the commercial center of Osaka
boasting a population of roughly 1 million people by 1750 while the imperial capital of Kyoto and the administrative
center of Edo (later Tokyo) each had populations of about 400,000. Manufacturing and a financial sector also
became key ingredients of the growing Japanese economy in these years.
Europeans first arrived in the islands in the early 1500s, but after a period of growing influence they were
restricted to a few tiny islands where they could conduct only limited commerce with licensed Japanese traders.
By these means, Japan largely isolated itself from the outside world until the mid-nineteenth century, when
American naval ships and traders forced its doors open. This opening up was part of a long tradition whereby
Japan periodically adopted foreign technologies and ideas—usually from China and Korea—and then closed itself
off again.
 Emergence as Industrial Power
In 1867, the feudal Tokugawa Shogunate was overthrown and replaced by a new imperial order, an event known
as the Meiji (“enlightened”) Restoration. The new imperial government based in Tokyo recognized that to avoid the
fate of other Asian countries, such as China and India, which were either colonized by Europeans outright or had
their economies dominated by them, Japan would have to adopt the political, economic, and military practices of
the West if it was to remain independent and achieve prosperity.
With its long tradition of borrowing innovations from China, the Japanese were highly successful in adapting
themselves to the modern capitalist and imperialist world order of the late nineteenth and early twentieth centuries.
The old feudal order was replaced by a modern bureaucratic state endowed with great powers to set economic
policy. A modern legal framework was established, abolishing a system that subjected people of different classes
to different rules, and the rudiments of democracy were put into place.
With the government providing various forms of economic stimulus, many of the old daimyo turned themselves into
industrialists, building factories, railroads, and corporations, some of which survive to the present day. The
Japanese also built a modern army and embarked on imperial expansion, seizing Korea and Formosa (modern-
day Taiwan), establishing a “sphere of influence” in Mainland China, and even defeating Russia in a war for
influence in East Asia, the first time in the modern era that a non-European country had beaten a European one
in a major conflict.
Japan prospered mightily, joining the front ranks of industrialized countries by World War I. But the expansion
created problems. Poor in natural resources, Japan found itself having to export more and more to pay for the
resources it needed, fueling fears among the elite and the larger electorate that unless it continued its overseas
expansion its fortunes would be reversed. As in other newborn democracies, such as Germany and Italy, many
Japanese came to question whether a democracy could secure Japan’s rightful place as a leading power, a fear

that became fused with an ultra-nationalist ideology that endorsed the notion of the Japanese people’s innate
racial superiority over others.
By the 1930s, the militarists had captured power in Japan and embarked on a massive expansion of the armed
forces and the defense industry while launching an invasion of China. Japan’s rising power and belligerency raised
tensions with another great power of the Pacific, the United States. When Washington attempted to restrict
Japanese militarism by limiting key exports, the Japanese military determined that it had to neutralize U.S. power
in the western Pacific. In late 1941, it launched a surprise attack against American forces in Hawaii, triggering the
United States’ entry into World War II. At the same time, Japan launched a massive invasion of Southeast Asia,
seizing the territory from European colonizers, all under the banner of creating the “Greater Co-Asian Prosperity
Sphere,” a euphemistic title for an East Asian and western Pacific economic community, free of Western powers
but utterly dominated by Tokyo.
The Japanese militarists’ decision to go to war with China and the United States proved disastrous in the end.
Bogged down in the vast reaches of China, Japan was overwhelmed by the greater industrial resources and
population of the United States. Its cities and industrial centers were bombed to annihilation, with the aerial attack
culminating in two atomic bomb blasts that forced Japan to surrender unconditionally to Washington in late 1945.
Occupied by the U.S. military after World War II, Japan adopted a democratic constitution written for it by U.S.
authorities.
 Postwar Economic “Miracle”
While the country lay in ruins, it did enjoy certain advantages even in the immediate postwar era of inflation,
unemployment, and shortages of consumer goods. Its populace was highly educated and highly skilled, it had
centuries of manufacturing experience behind it, it had a host of major corporations that had been highly profitable
before the war, and it had a seventy-five-year tradition of effective economic policy making by the central
government. Determined to prevent a leftist takeover of Japan in the early years of the cold war, the United States
did much to bolster the Japanese economy. Especially during the Korean War from 1950 to 1953, U.S. military
procurement provided a much-needed boost to manufacturing and agriculture, at one point accounting for more
than a quarter of all exports.
Unlike the United States, however, the Japanese were not reluctant to employ central planning to direct the
economy. Central to this effort was the establishment of the Ministry of International Trade and Industry (MITI) in
1949. MITI not only developed the economic blueprint for Japan’s economic recovery from war but also it made
sure that the nation’s banking and manufacturing sectors cooperated with government in developing key heavy
industries. The government also encouraged the importation of the latest manufacturing technologies, which
forced industry to modernize. MITI set import and export policies to make sure that nascent domestic industry was
protected and the exports that had long been essential to Japan’s economic prosperity were promoted.
The Japanese central bank contributed as well, providing easy credit so that industry could grow, while the
legislature eased up on antimonopoly rules. Huge, vertically integrated conglomerates in shipbuilding, steel, and
consumer goods production known as keiretsu emerged. Industrial peace was achieved, largely by reining in the
power of unions while guaranteeing workers steadily rising wages and lifetime employment, though this only
applied to major corporations that, in turn, relied on a network of smaller suppliers and subcontractors hiring
people without such benefits.
The result of all this was the so-called Japanese miracle of the 1950s and 1960s, when the country consistently
ranked at the top of the world nations in GDP growth. By the early 1970s, Japan had emerged as the second-
largest noncommunist economy in the world. And while the country faced setbacks in the 1970s—a result of
rapidly rising oil prices and economic stagnation in much of the industrialized world that undermined exports—it
roared back to life in the 1980s.
Adopting technologies from the West and then improving on them—as well as developing some of its own under

MITI’s guidance—Japan’s export-driven economy became a world manufacturing leader in such fields as
shipbuilding, steelmaking, electronic consumer goods, and automobiles. So successful was the Japanese model of
manufacturing that U.S. firms looked to the country for inspiration. The American automobile industry, for one,
tried to re-create the team-oriented production techniques of Japanese assembly lines while that industry and
others adopted Japan’s “just-in-time” inventory control methods, which streamlined the supply process. For the
Japanese people, the success was registered in rising standards of living, with per capita income rising to 80
percent that of the United States by 1990.
Government integration of manufacturing, finance, and labor for targeted industries—such as consumer electronics
—led Japan’s emergence as a major industrial power in the 1950s and 1960s. Matsushita epitomized the
conglomerates known as keiretsu. (Bill Ray/Time & Life Pictures/Getty Images)
 Stagnation and the “Lost Decade”
But there was a darker side to the boom as well—speculative excess and overcapacity. With so much capital on
hand, Japanese individuals and businesses began to speculate in securities and real estate, driving up both,
especially the latter, to unsustainable levels. At one point in the late 1980s, it was estimated that the paper value
of real estate in the greater Tokyo area surpassed that of the entire United States. Ultimately the bubble burst,
wiping out the assets and portfolios of individuals and businesses alike. A nation of savers became even more
frugal as a result, undermining consumer demand.
Meanwhile, having overbuilt industrial capacity, Japanese companies found themselves saddled with debt, forcing
some to abandon traditional guarantees of lifetime employment for their workers. Rising Asian economies were
also eating into Japanese overseas markets for consumer goods, though these same economies were also
becoming customers for Japanese exports of machinery and other industrial goods.
The Japanese banking system was overleveraged as well and suffering from a lack of liquidity. But rather than
root out the weaker institutions, the Japanese central bank and financial authorities did everything they could to
keep them limping along, creating uncertainty in the financial markets and undermining credit. Moreover, the

central bank was slow to lower interest rates, say many economists, further contributing to the stagnation in the
Japanese economy through the early 1990s.
The Japanese government tried to compensate for the slump in other ways, embarking on a massive public
building project. But it was not enough to overcome the slowdown in the private sector, which was also suffering
the effects of lowered demand from a shrinking and ageing population. Finally, in the late 1990s and early 2000s,
the government embarked on the “structural reforms” international economists and institutions had been advising
for years, in an effort to dispose of the many toxic assets on the books of financial institutions. But such efforts
only contributed to more deflation and a further bout of zero and negative growth.
Gradually, however, demand began to revive, reducing the problem of overcapacity, and the banks started to
purge their books of bad assets. Some of this reversal was achieved by a central bank policy known as
“quantitative easing.” While the Bank of Japan had already overcome its reluctance to lower interest rates in the
1990s, bringing down the rate it charged banks to zero or near zero, this had failed to stimulate credit, investment,
and spending. In the early 2000s, it employed the new “quantitative easing” strategy, pumping new money into the
economy by buying up corporate and government debt. The idea was to stimulate inflation—though not too much
—thereby getting consumers to spend and businesses to invest. The strategy appears to have worked. By 2005,
the economy had returned to sustained growth, even coming to surpass the rate of economic expansion in the
United States and the European Union.
The “lost decade,” as the period of stagnation from the early 1990s to the early 2000s is sometimes called, did
have one positive side-effect, strengthening Japan’s financial institutions by requiring them to reduce their
exposure to debt and increase their capital. With banks holding far less debt than their Western counterparts, not
only did they weather the financial crisis of the late 2000s better, but also they were able to take advantage of the
turmoil in the United States and certain European banking sectors by snapping up financial assets at reduced
prices.
For a time, in the first months of 2008, it appeared that Japan might even be able to sustain positive economic
growth despite the global financial crisis. But, inevitably, its export-driven economy felt the impact of the global
recession engulfing the United States and European Union, though exports to the still buoyant Asian economies
helped offset some of the losses in exports to the West. By 2009, Tokyo was experiencing its first trade deficit in
more than thirty years while major corporations began to report significant losses. In the last quarter of 2008, even
the iconic Japanese automaker Toyota posted a loss, its first since before World War II.
Adding to Japan’s woes was the massive earthquake and tsunami of March 11, 2011, which devastated the
northeast coast of the country and sparked the world’s worst nuclear reactor crisis since Chernobyl in 1986. The
multiple disaster sent the Japanese economy reeling, even as it struggled to emerge from the global recession
that began in 2007. After seeing its GDP fall by 6.3 percent in 2009 and then regain ground with net positive
growth of 5.1 percent in 2010, the country’s economy shrank by 3.7 percent in the two months following the
disaster. Moreover, the disaster’s impact was felt far beyond Japan’s shores, as companies around the world—
particularly those in the electronics and automobile sectors—saw their supply chains disrupted. Still, it appeared to
many economists that the global economic effects of the tsunami would be shortlived and might even provide, in
the form of reconstruction funds, a needed stimulus for the Japanese economy.
James Ciment
 
See also:  China;  Korea, South. 
Further Reading
Bank of Japan.  “Financial System Report (September 2009): The Current State of Japan’s Financial System and
Challenges: An Overview.”

Hirschmeier, Johannes, and Tsunehiko Yui. The Development of Japanese Business, 1600–1980. Boston: G. Allen &
Unwin, 1981. 
Koo, Richard C. The Holy Grail of Macroeconmics: Lessons from Japan’s Great Recession. Hoboken, NJ: John Wiley and
Sons, 2008. 
Mosk, Carl. Japanese Industrial History: Technology, Urbanization, and Economic Growth. Armonk, NY: M.E.
Sharpe, 2001. 
Wood, Christopher. The Bubble Economy: Japan’s Extraordinary Boom of the’80s and the Dramatic Bust of the’90s. New
York: Atlantic Monthly, 1992. 
Jevons, William Stanley (1835–1882)
 
Studies by the English neoclassical economist and statistician William Stanley Jevons on the causes of rapid
fluctuation in the value of British currency added significantly to the understanding of business cycles in the
nineteenth century. Along with Leon Walras of France and Carl Menger of Austria, Jevons was also a major
contributor to the theory of marginal utility, which hypothesizes that utility determines value.
Jevons was born on September 1, 1835, in Liverpool, England. He graduated from the University of London in
1852 and then spent five years in Australia as an assayer at the Sydney Mint. In that capacity, he collected
extensive data on the Australian climate, which began his interest in statistics. He was also fascinated with the
economic impact of the Victoria gold rush and its effects in Sydney, which led to his booklet Remarks on the
Australian Goldfields, published in 1859. Returning to England, Jevons attended University College, London,
receiving a bachelor’s degree in 1860 and a master’s degree in 1863. From 1866 to 1875, he served as a
professor at Owens College, Manchester, and for the next five years as professor of political economy at
University College, London.
Jevons’s research on the fluctuating value of the British currency compared to gold, for which he collected and
collated statistics, helped form the basis of his 1862 paper for the British Association, “On the Study of Periodic
Commercial Fluctuations.” This was followed by the book A Serious Fall in the Value of Gold, Ascertained, and Its
Social Effects Set Forth (1863), which earned the admiration of fellow economists. Its success encouraged Jevons
to write “On the Variation of Prices and the Value of Currency Since 1782,” which he read to the London
Statistical Society in May 1865.
By this time, Jevons’s work had started to attract widespread attention, especially when economist John Stuart
Mill cited his statistics to explain why Britain’s national debt should be systematically reduced. British prime
minister William Gladstone also drew from Jevons’s ideas in his plans to reduce the country’s national debt over
thirty-nine years.
While at Owens College, Jevons had written his best-known work, The Theory of Political Economy (1871), which
integrated the various strands of his research on money supply, currency fluctuations, and business cycles.
Jevons believed that business cycles were not random events but influenced by seasonal variables. For example,
he argued, sunspots affect the weather, which in turn has an impact on agricultural production and the economy.
On August 13, 1882, two years after retiring from University College, London, Jevons drowned while swimming
near Hastings. His eldest son, Herbert Stanley Jevons (1875–1955), a respected economist, geologist, and
educator, edited his father’s papers.

Justin Corfield
 
See also:  Seasonal Cycles;  Sunspot Theories. 
Further Reading
Jevons, H.A. Letters and Journals of W.S. Jevons. London: Macmillan, 1886. 
Keynes, John Maynard. Essays in Biography. London: Hart-Davis, 1951. 
Peart, Sandra. The Economics of W.S. Jevons. London and New York: Routledge, 1996. 
Peart, Sandra. W.S. Jevons: Critical Responses. London and New York: Routledge, 2003. 
Robbins, L.C.  “The Place of Jevons in Economic Thought.” Manchester School 50(1982). 
JPMorgan Chase
 
JPMorgan Chase was an active, and successful, player in the global financial crisis of 2008–2009. The firm
showed the other side of a serious recession: the bargains to be made, and the long-term strategic advantages to
be gained by those properly positioned during unstable economic times.
As a bank, JPMorgan Chase traces its origins back to 1799, and was formed through a series of mergers and
takeovers. The name comes from the J.P. Morgan & Co. bank and the Chase Manhattan Bank, and as such it is
one of the oldest financial services companies in the world, with total assets of $1.78 trillion (as of June 30, 2008)
and 228,452 employees.
The Bank of the Manhattan Company was established in 1799 and was founded by Aaron Burr, U.S. vice
president under Thomas Jefferson. In 1955, it merged with the Chase National Bank (founded 1877) to become
the Chase Manhattan Bank, which emerged during the 1970s as one of the best-known and most widely
respected banking houses in the United States. However, its exposure to “toxic” real-estate mortgages resulted in
its purchase by the Chemical Bank of New York (founded in 1823 as the New York Chemical Manufacturing
Company), although it continued to trade as the Chase Manhattan Bank.
Drexel, Morgan & Co. was established in 1871. It helped finance a number of financial institutions during the
depression of 1895 and in that same year was renamed J.P. Morgan & Company under the leadership of J.
Pierpont Morgan (1837–1913), who helped finance the career of Andrew Carnegie. J.P. Morgan & Company
emerged from the crisis as one of the biggest and most powerful banking institutions on Wall Street (New York
City). Its headquarters, built in 1914 at 23 Wall Street, was known as the “House of Morgan.” By that time J.P.
Morgan had established extensive political links in Western Europe and helped finance British and French war
bonds during World War I.
 Twentieth Century
The first decade following the war was a busy one for both banks as the economy expanded for most of the
1920s. Then came the most important—and disastrous—event in the banks’ history. On what is known as “Black
Thursday,” October 24, 1929, the U.S. stock market underwent the biggest crash in history. After the fall in share

prices in the morning, J.P. Morgan himself tried to reverse the slide. He attempted to inject some confidence,
albeit only in the short term, back into the market through cash infusions and continued assurances to the
financial community of more help to come. But a larger crash in share prices took place the following week that
sent his bank reeling. The Great Depression had begun and, in a few years, Morgan’s bank found itself at a
crossroads in its history. In 1935 it had no choice but to separate its investment banking operations from the main
company and overall reduce its level of operations. This was the case as well for Chase Manhattan and other
banking operations.
The post–World War II decades of expansion saw banks grow again and attempt diversification. The impressive
growth in the U.S. economy during the 1990s instilled even greater confidence in the banking community. By this
decade, many banks had once again joined investment banking with their regular banking operations. In order to
continue growing and become more efficient over a wide range of business activities, banks began to merge. In
1999 J.P. Morgan & Co. merged with the Chase Manhattan Bank to become JPMorgan Chase.
By the first decade of the twenty-first century, JPMorgan Chase completed its acquisition of other banks, notably
the Bank One Corporation, which had only been formed six years earlier from a merger of Banc One of Ohio
(formerly the City National Bank & Trust Company and the Farmers Savings & Trust Bank) and the First Chicago
NBD (created from a merger of First National Bank of Chicago, and the NBD Bancorp, formerly the National Bank
of Detroit).
 Financial Meltdown of 2008
While the financial crisis of 2008 is most often associated with contracting and failed firms, it also provided
opportunities to more well-positioned companies for accelerated expansion and market control. Eager to continue
to expand, JPMorgan Chase took advantage of the collapse in the share price of Bear Stearns to take over what
had been the fifth-largest investment bank in the United States in March 2008. The takeover came after Bear
Stearns had suffered major problems. On March 14, 2008, the company lost 47 percent of its market value as
rumors spread about investors desperate to withdraw their capital. With the realization that Bear Stearns might
well be insolvent, on Sunday, March 16, JPMorgan Chase announced that it was prepared to buy Bear Stearns
and two days later offered 0.05472 of their shares for one in Bear Stearns, valuing Bear Stearns shares at $2
each. Six days later the offer was revised to $10 a share, with the merger formally completed on June 2, 2008.
The collapse of Bear Stearns was one of the first major signs of the impending economic downturn in the U.S.
financial system, but it was far less dramatic than that of many other companies, as it was absorbed into another
institution.
The takeover of other banking institutions continued, and on September 25, 2008, JPMorgan Chase bought the
Washington Mutual Bank from Washington Mutual Inc. Founded in 1889 as the Washington Mutual Building Loan
and Investment Association, the Washington Mutual Bank was the largest savings and loan bank in the United
States.
 Implications
The case of JPMorgan Chase shows an important distinction between the stock market crash and Great
Depression of the early 1930s and the financial meltdown of 2008–2009. The capital destruction in the earlier
disaster was so deep and widespread, there were very few pockets of economic activity that could take advantage
of falling prices. This was not the case in the more recent crisis. Financial institutions that had steered relatively
clear of the more toxic assets, and that had slowly and methodically expanded operations over the previous half
century, like JPMorgan Chase, were in a perfect position to absorb operations that they could get on the cheap
and use to their advantage in the years to come. On the other hand, this situation often results in a less
competitive industry, which has consequences for consumers.
Justin Corfield

 
See also:  Banks, Investment;  Bear Stearns;  Recession and Financial Crisis (2007-); 
Troubled Asset Relief Program (2008-);  Washington Mutual. 
Further Reading
Chernow, Ron. The House of Morgan: An American Banking Dynasty and the Rise of Modern Finance. New York: Simon &
Schuster, 1991. 
Crisafulli, Patricia B. The House of Dimon: How JPMorgan’s Jamie Dimon Rose to the Top of the Financial World. Hoboken,
NJ: John Wiley and Sons, 2009. 
McDonald, Duff. Last Man Standing: The Ascent of Jamie Dimon and JPMorgan Chase. New York: Simon &
Schuster, 2009. 
Juglar, Clément (1819–1905)
 
Clément Juglar was a French physician turned economist who came to be regarded as a founder of modern
business-cycle theory. Indeed, he is credited with being one of the first economists to recognize the existence of
business cycles and to describe them based on collected data.
Juglar was born on October 15, 1819, in Paris, France. Trained as a physician, he wrote a thesis on the
pulmonary effect of heart disease. His studies in epidemiology and demographics prompted in him an interest in
economics during the national recession of 1847 and the revolution of 1848. While researching the effects of
catastrophe (such as war and famine) on the size of the French population from the early eighteenth to the mid-
nineteenth century, Juglar expanded his inquiry to determine their impact on trade. Based on his findings, he
began writing a series of articles on economics during the early 1850s.
In 1857, Juglar published an article titled Des crises commerciales et monétaires de 1800 à 1857 (“Business and
Monetary Crises from 1800 to 1857”) in the Journal desÉconomistes, articulating his theory of the cyclical nature of
economic growth and decline. He elaborated his findings in the book Des crises commerciales et de leur retour
périodique en France, en Angleterre et aux Etats-Unis (Periodic Business Crises in France, England, and the
United States) (1862). Juglar’s thesis, which he expanded and refined significantly in succeeding years, suggested
that business goes through regular and repeatable cycles that can be described and predicted. According to
Juglar, business cycles occur at intervals of eight to eleven years, and comprise three distinct phases—prosperity,
crisis, and liquidation. Within these longer cycles are shorter cycles, which explains why there are temporary
reverses during economic recoveries. Juglar’s cycles are sometimes compared to Kondratieff cycles, which are
considerably longer. The Norwegian economist John Akerman later hypothesized shorter sets of cycles that
combine to form Juglar cycles, which he then combined into longer cycles of sixteen years.
Yet Juglar was an empiricist more than a theorist, much as such later economists as Frederick Mills, Wesley Clair
Mitchell, and other statisticians. His approach was to draw conclusions from masses of data collected over long
periods of time. This approach was applauded in some circles and criticized in others as an effort to produce
results that could be defended rather than advancing a theory that was open to dispute. As a result, Des crises
commerciales was a seemingly convincing, albeit difficult, book to read.
While Juglar was acknowledged as having made substantial contributions to the understanding of economic

panics and crises, his work was not universally accepted. One reviewer of Des crises commerciales accused him
of overestimating the importance of banking. Indeed, Juglar posited a strong relationship between banking activity
and business cycles without offering a full explanation or analytical proof. His view was rooted in empirical data,
such as banking statistics, and was based on the notion that banking crises generally precipitated commercial
crises.
Clément Juglar died on February 28, 1905. Although his investigations into the business cycle remained
influential, the importance of his work may have been exaggerated by a later misunderstanding. In the 1930s, the
great economist and political scientist Joseph Schumpeter, referring to a contemporary edition of Juglar’s writings,
cited the Frenchman’s contributions without realizing—or perhaps choosing not to state—that the publication
incorporated work by later economists as well.
Robert N. Stacy
 
See also:  Kondratieff Cycles;  Schumpeter, Joseph. 
Further Reading
Groenewegen, Peter D. Physicians and Political Economy: Six Studies of the Work of Doctor-
Economists. London: Routledge, 2001. 
Juglar, Clement. A Brief History of Panics: And Their Periodical Occurrence in the United States.  3rd ed. New
York: Forgotten Books, 2008. 
Kaldor, Nicholas (1908–1986)
 
British economist Nicholas Kaldor’s most significant work concerned the relationship between national economic
growth and the way in which capital and labor resources are used and distributed in an economy. His wide-
ranging interests included trade cycles, the causes of the Great Depression, welfare economics, and public
finance. He is regarded as one of the most original and innovative economists of the post–World War II era.
Kaldor was born on May 12, 1908, and educated in Budapest, Berlin, and London. In 1930, he earned his
doctorate from the London School of Economics, where he lectured from 1932 to 1947. It was there that he made
his greatest contributions to trade cycle theory, at first under the influence of Friedrich Hayek and the Austrian
school of economics, and then as a convert to Keynesian theory.
Early in his career, Kaldor published articles on regional problems in economic development, the first of which
examined the Danube region of Europe. In 1947, Kaldor left academia to work for Swedish economist and
politician Gunnar Myrdal as director of the Research and Planning Division of the Economic Commission for
Europe (a predecessor of the European Union). Kaldor served as an economic adviser to a number of
governments, including those of Great Britain, India, Iran, Mexico, Turkey, and Venezuela. His advisory work
proved largely ineffectual, however, because the nations he counseled often did not follow through on his
proposals, usually for political rather than economic reasons. Nevertheless, Kaldor’s research during this period
was important to his later work in theoretical economics.
In 1950, after serving for two years on the Economic Commission, Kaldor took a teaching position at King’s

College, Cambridge University, where he became a full professor in 1966 and taught until his death in 1986.
There, Kaldor enhanced his already strong reputation as a supporter of Keynesian theories and an expert on
growth and resource distribution. His application of Keynesian principles to the study of trade cycles and interest
rates, presented in several published works, drew widespread interest in the economics community. However, it
was in the study of wealth accumulation and distribution that Kaldor made his most important contributions. His
initial research on wealth focused on capitalist countries, but he later expanded his research to include developing
nations as well.
Beginning with a groundbreaking essay published in 1932, Kaldor examined the effects of technical progress on
the economy, challenging the position of many economists at the time who claimed that technical advancements
had created a disturbance in the economic equilibrium that had contributed to the Great Depression. Like the
Austrian School economist Joseph Schumpeter, Kaldor believed that when technical progress caused a
disturbance or “disequilibrium” in an economy, this often produced economic expansion in the long run. Kaldor
believed investment, especially in innovation, to be a critical factor in determining profits. In what became known
as Kaldor’s law, he maintained that the primary requirement for economic growth is full employment and that the
manufacturing sector is usually the most critical to economic growth—a point of view that sparked debate and
criticism among many economists, including American Nobel laureate Paul Samuelson.
Described as one of the last of the great economic generalists in an age of increasing specialization, Kaldor
collaborated with a number of other influential economists, including Piero Sraffa and Joan Robinson. Made a life
peer, or baron, in 1974, Kaldor died on September 30, 1986, in Cambridgeshire, England.
Robert N. Stacy
 
See also:  Hicks, John Richard;  Keynesian Business Model. 
Further Reading
Kaldor, Nicholas. Essays on Economic Stability and Growth.  2nd ed. New York: Holmes & Meier, 1980. 
King, John Edward. Nicholas Kaldor. Basingstoke, UK: Palgrave Macmillan, 2008. 
Lawson, Tony, J. Gabriel Palma, and John Sender, eds. Kaldor’s Political Economy. London: Academic, 1989. 
Nell, Edward J., and Willi Semmler, eds. Nicholas Kaldor and Mainstream Economics: Confrontation or Convergence? New
York: St. Martin’s, 1991. 
Skott, Peter. Kaldor’s Growth and Distribution Theory. New York: Peter Lang, 1989. 
Turner, Marjorie S. Nicholas Kaldor and the Real World. Armonk, NY: M.E. Sharpe, 1993. 
Kalecki, Michal (1899–1970)
 
The Polish economist Michal Kalecki has been accurately characterized as a Keynesian. In fact, Kalecki published
many of the ideas for which John Maynard Keynes became known before the release of Keynes’s General Theory
of Employment, Interest and Money (1936). But Kalecki’s work was published in Polish and French rather than
English, and consequently went largely unnoticed. It was not until the 1990s, with the publication of The Collected
Works of Michal Kalecki, that much of his work became available in English.

Kalecki was born in Lodz, Poland, on June 22, 1899. He attended Warsaw Polytechnic Institute and, following
several years in the Polish armed forces, attended Gdansk Polytechnic but did not receive a degree. He worked
for the Institute of Studies of Economic Conditions and Prices in Warsaw from 1929 to 1936, during which time he
produced much of his important work and writing on business cycles.
Kalecki’s theories were based on the premise that political factors are as important as economic ones, especially
where there is imperfect competition or conflict between management and the working class. In his writings,
Kalecki maintained that the level of investment coupled with consumption played a key role in business cycles. If,
for example, during a depression the rate of investment were to increase, the corresponding increase in demand
for consumer goods would exceed that of the investment. This situation would come about when workers
producing investment goods eventually spent their money on consumer goods. Workers making consumer goods
would also buy more consumer goods. In this way Kalecki, like Keynes, believed that government deficit spending
on public works—that is, investment and the creation of investment goods—would stimulate the economy.
Although Keynes and Kalecki shared similar views, they came from very different backgrounds. Keynes’s frame of
reference was the generally thriving capitalist economy of Great Britain. While Kalecki had lived and worked in
Britain from 1936 to the end of World War II—first at the London School of Economics and later at Cambridge
University—his earliest work was shaped by his experience in Poland, which had been part of Prussia, Russia,
and Austria before briefly gaining independence after World War I. When Kalecki returned to Poland after the war,
its economy was a state-run socialist system based on the Soviet model.
Like Keynes, Kalecki believed that saving money was not an unalloyed virtue because if everyone saved—rather
than spent—it would impede economic growth. Thus, what is good for the individual is bad for the whole.
Similarly, both men argued that cutting wages, while possibly good for an individual company, would be bad for
the economy as a whole since it would decrease consumer demand for goods.
Kalecki, who died in Warsaw on April 18, 1970, greatly influenced such Cambridge economists as Joan Robinson
and Nicholas Kaldor. It has been suggested that among the reasons the COMECON (Council for Mutual Economic
Assistance) nations of Eastern Europe began to suffer economic problems by the late 1960s and early 1970s was
that the older generation of economists was dying, leaving no adequate successors. Whether or not that is true,
Kalecki, sometimes known as the “left-wing Keynes,” left a conspicuous void after his death.
Robert N. Stacy
 
See also:  Burchardt, Fritz;  Goodwin, Richard Murphy;  Keynesian Business Model. 
Further Reading
Bhaduri, Amit, and Kazimierz Laski.  “Relevance of Michal Kalecki Today.” Economic and Political Weekly 29:7 (February
12, 1994): 356–357. 
Blaug, Mark. Michal Kalecki. Brookfield, VT: Edward Elgar, 1992. 
Kriesler, Peter. Kalecki’s Microanalysis: The Development of Kalecki’s Analysis of Pricing and Distribution. New
York: Cambridge University Press, 1987. 
Sadowski, ZdzisŁaw, and Adam Szeworski, eds. Kalecki’s Economics Today. New York: Routledge, 2004. 
Sawyer, Malcolm C., ed. The Economics of Michal Kalecki. Basingstoke, UK: Macmillan, 1985. 
Sawyer, Malcolm C., ed. The Legacy of Michał Kalecki. Cheltenham, UK: Edward Elgar, 1999. 
Steindl, Josef.  “Personal Portrait of Michal Kalecki.” Journal of Post Keynesian Economics 3:4 (Summer 1981): 590–596. 

Kautsky, Karl (1854–1938)
 
Karl Kautsky was a prominent Marxist political and economic theorist whose work is most notable for linking long-
term political, social, and historical forces with economic cycles. That link, he believed, is especially clear in
particular economic contractions in capitalist systems, which he believed to be in unstoppable decay.
Kautsky was born on October 16, 1854, in Prague, then part of the Austrian Empire. His family moved to Vienna
when he was a child, and he studied history and philosophy at the University of Vienna, where he joined the
Social Democratic Party of Austria in the mid-1870s. He went to Zurich, Switzerland, in 1880 and, influenced by
the German political theorist Eduard Bernstein, became a Marxist. Kautsky traveled to London the following year
to meet Karl Marx and Friedrich Engels, and in 1883 he founded the Marxist journal Die Neue Zeit (The New
Times), which he edited until 1917.
An orthodox Marxist regarded as the intellectual heir and successor to both Marx and Engels, Kautsky was also
influenced by the work of the eighteenth-century British economist Thomas Malthus and was for many years
prominent as a socialist intellectual. In his writings, Kautsky extended Marx’s ideas and critiqued the ideas of
others. His work on what Marx and Bernstein believed was the coming crisis of capitalism appeared in the 1890s
and was favorably reviewed by Vladimir Lenin. By the time of the Bolshevik Revolution in 1917, however, Kautsky
had become marginalized by the train of events and the direction of communism in the Soviet Union. He was
labeled an apostate for his criticisms of the Communist Party.
In the years leading up to World War I, Kautsky was a political activist as well as an intellectual, favoring
revolution rather than accommodation and opposed to any alliance with organizations that were not orthodox
Marxist. He lobbied for the Socialist deputies of the German Reichstag to abstain rather than vote against
Germany’s entry into World War I in 1914. Although he changed his position months later, his support for the war
was the kind of position that led Lenin to distance his party from other socialist organizations across Europe.
In his essay “Finance-Capital and Crises” (1911), Kautsky addressed the problem of periodic economic crises,
emphasizing a distinction between industrial cycles, which he regarded as harmful to workers and beneficial to
capitalists, and agricultural cycles, which he believed distributed benefits evenly regardless of one’s position in the
cycle. In his view, cyclical crises had not been addressed by early economists because they had not occurred
before the advent of the industrial revolution. Contemporary economists, he maintained, were in a state of denial;
what they called crises were actually part of the demise of capitalism.
Kautsky’s view was opposed to that of economist Clément Juglar, who believed that the cycles of boom and bust
would continue. Kautsky argued that the end of capitalism would begin with what he called the anarchy of the
production of commodities—in other words, overproduction by and lack of coordination between individual
producers unaware of the activities of others. That situation, combined with a rapidly growing labor force and the
development of technology that could speed up production, would overload the system with goods and be followed
by a drop in consumption. Supply and demand would eventually come into equilibrium, but at great cost to the
laboring masses.
In 1922, Kautsky published an article in Foreign Affairs titled “Germany Since the War,” in which he examined
political and economic conditions in that country in the aftermath of World War I. In it he described in great detail
the problems facing Germany as a result of the harsh terms of the Treaty of Versailles. Among the issues he
identified were the Allied policy of holding an entire nation responsible for the mistakes of its government and,
following comments by John Maynard Keynes, the reparations program. The vast sums Germany was forced to

pay led to large budget deficits, disastrous inflation, and an unfavorable balance of payments, which made
payment of the reparations all the more onerous. At the conclusion of the article, Kautsky predicted—eleven years
before the rise of Adolf Hitler and the Nazis—that the misery of the German people resulting from Allied policies
would eventually give rise to armed opposition and revenge. After living in Vienna since 1924, Kautsky and his
family left the city upon Hitler’s annexation of Austria in 1938. They traveled first to Czechoslovakia and then to
Amsterdam, where Kautsky died on October 17 of that same year.
Robert N. Stacy
 
See also:  Malthus, Thomas Robert;  Marxist Cycle Model. 
Further Reading
Kautsky, John H. Karl Kautsky: Marxism, Revolution & Democracy. New Brunswick, NJ: Transaction, 1994. 
Salvadori, Massimo L. Karl Kautsky and the Socialist Revolution, 1880–1938. London: NLB, 1979. 
Steenson, Gary P. Karl Kautsky, 1854–1938: Marxism in the Classical Years. Pittsburgh, PA: University of Pittsburgh
Press, 1991. 
 
Keynes, John Maynard (1883–1946)
 
One of the most influential economists of the twentieth century, John Maynard Keynes challenged the classical
economic paradigm of self-regulating markets, arguing that during times of economic recession, aggregate
demand lags behind aggregate supply. To encourage the former, he argued, government stimulus was needed,
with each dollar authorities pumped into the economy having a multiple effect in creating demand as it is spent by
successive individuals and businesses, minus that portion that is saved. Keynes called this phenomenon the
multiplier. Hesitantly adopted by several governments, including that of Franklin Roosevelt in the 1930s, Keynes’s
ideas became the foundation of economic policy in much of the noncommunist industrialized world for several
decades after World War II.

In addition to charting a major new direction in macroeconomic theory, John Maynard Keynes advised the British
and U.S. governments on economic policy and was a leading architect of the monetary system and global
financial institutions for the postwar world. (George Skadding/Time & Life Pictures/Getty Images)
Keynes was born in Cambridge, United Kingdom, in 1883, his father a lecturer in economics at the university
there. After attending Eton, the most prestigious English private boarding school—on a scholarship—Keynes
enrolled at King’s College, Cambridge University, as a mathematics major. While earning his bachelor’s degree in
1904, Keynes also became interested in economic studies—encouraged by the then-dean of English economics,
Alfred Marshall—and social philosophy. After a short stint in the civil service, Keynes returned to Cambridge
University to study, and began publishing articles on economics. During these years, Keynes also became
involved with the influential social and literary circle known as the Bloomsbury Group, which included such
luminaries as E.M. Forster, Lytton Strachey, and Virginia Woolf.
During World War I, Keynes was recruited by the government to work out economic arrangements with Britain’s
allies and, at war’s end, was appointed by the Treasury Department as a representative to the Versailles Peace
Conference of 1919. Although British and French political leaders ignored Keynes’s warning that large reparations
payments from Germany would cripple that nation’s economy, the conference—or, more precisely, its
consequences—would establish his reputation as a major economic thinker. His book The Economic
Consequences of the Peace (1919), reiterating the position he took at the conference, proved prescient later in the
1920s, when Germany was unable to meet its reparations quotas and descended into economic chaos.
Keynes became a successful investor during the 1920s even as he continued to research and write in the fields of
mathematics and economics, publishing work on probability theory and the need for an inflationary monetary
policy that would help reduce Britain’s nagging unemployment problem. But his call in the acerbically titled

Economic Consequences of Mr. Churchill for Britain not to go back on the gold standard—it had abandoned it at
the outset of the war—went unheeded. The 1920s also saw Keynes begin his studies on the relationship between
employment, money, and prices—a subject that he would continue to pursue into the 1930s and that would
establish his reputation. In his Treatise on Money, published in 1930, Keynes argued that high personal savings
rates—caused by tight money and high interest rates—could impede investment and lead to higher unemployment.
Personally affected by the Great Depression—his investments of the 1920s being wiped out by the stock market
crash of 1929—Keynes turned from theory to advocacy with his 1933 work The Means to Prosperity, where he
began to argue for countercyclical public spending, the hallmark of what would become known as the Keynesian
economic model. It was in this book also that Keynes explained the multiplier effect for the first time, arguing that
when government injected one dollar into the economy by hiring workers it produced a greater economic stimulus,
as that worker might spend the money at a store, thereby aiding a storekeeper who might order more goods from
a manufacturer, and so on.
In 1936, Keynes published the work for which he is best known—The General Theory of Employment, Interest
and Money—a book that provided the theoretical underpinnings for the recommendations he first offered in The
Means to Prosperity. In it, he dismissed the accepted wisdom of classical economics that economies tend toward
a high-employment, high-output equilibrium where free wage and price competition produces a balance between
supply and demand. That is, when demand is low, prices drop, which re-stimulates demand. Government efforts
to stimulate demand, either by expanding the money supply or putting money directly into the economy, will only
affect price, not output or employment, according to this school of thought. Indeed, such remedies will only make
things worse, by distorting the natural workings of the marketplace.
Keynes argued quite the opposite. Focusing on the demand side of the equation, he insisted that aggregate
demand worked independently of supply and was the result of millions of individual decisions. Thus, during
downturns like the Great Depression, economies may find their equilibrium at a level of high unemployment and
low output. It was at such times that outside stimulus was needed and that only the government could provide
effective amounts of it. Although some of his ideas were adopted in Britain and the United States during the Great
Depression, it was only after World War II that the Keynesian economic model became the new paradigm of
academics and policy makers in the industrialized world.
As for Keynes himself, he was sidelined during the great debate around his ideas in the late 1930s as he
recuperated from a 1937 heart attack. And with the outbreak of World War II, he focused his energies on practical
solutions, such as those offered in his 1940 book How to Pay for the War, in which he called for higher taxes and
compulsory savings, not only to pay for the war, but to control inflation by limiting the growth of aggregate
demand. In 1944, he headed Britain’s delegation to the Bretton Woods Conference, a meeting of various allied
governments in the New Hampshire community of the same name where the postwar global economic order was
to be planned. There, Keynes was a radical voice, calling for a common world currency and international financial
regulatory bodies. But as representatives of the world’s leading economy, more moderate U.S. delegates won the
day. Still, Keynes was satisfied with the results of the conference, which included mechanisms for currency
stabilization among countries and the establishment of financial institutions—notably, the International Monetary
Fund and what is popularly known as the World Bank—designed to smooth out economic crises and aid
development.
By this time, however, Keynes was not a healthy man. His work at the conference and his efforts to secure an
American loan for Britain at war’s end further exhausted him, leading to his death in 1946 at the age of sixty-two.
James Ciment
 
See also:  Great Depression (1929-1933);  Keynesian Business Model. 
Further Reading

Clarke, Peter. The Keynesian Revolution in the Making, 1924–1936. New York: Oxford University Press, 1988. 
Felix, David. Keynes: A Critical Life. Westport, CT: Greenwood, 1999. 
Skidelsky, R.J.A. John Maynard Keynes: A Biography. London: Macmillan, 1983. 
Skidelsky, R.J.A. Keynes: The Return of the Master. London: Public Affairs 2009. 
Keynesian Business Model
 
According to the Keynesian business-cycle model—named for its architect, the early-twentieth-century British
economist John Maynard Keynes—economic downturns are due primarily to falling demand, which, in turn,
reduces real output and employment, leading to a further fall in demand. In his influential and pathbreaking book
of 1936, The General Theory of Employment, Interest and Money, Keynes asserted that this negative economic
cycle can be mitigated and even reversed by large-scale government spending as well as reductions in interest
rates on the part of central banks. The Keynesian model dominated the thinking of economic policy makers in
much of the industrialized world from the late 1930s through the early 1970s, until a combination of stagnant
growth and high inflation seemed to undermine its basic premises. In the wake of the 2007–2009 global financial
meltdown and recession, however, Keynesian ideas have once again become the basis for government economic
policy decisions in the United States and elsewhere.
Before Keynes, most economists and governments held to the basic tenets of classical economics, a body of
macroeconomic theory first developed by British and French economists—including Adam Smith, David Ricardo,
John Stuart Mill, and Jean-Baptiste Say—in the latter half of the eighteenth and first half of the nineteenth
centuries. According to this model, economies tend toward a high-employment, high-output, supply-and-demand
equilibrium. In the words of Say, “supply creates its own demand.” In other words, where there is free wage and
price competition, an increase in production will lower prices, which in turn increases demand and thus
employment. Conversely, a decline in production will raise prices, which in turn increases production and also
employment. According to classical economics, then, increasing aggregate demand, by expanding the money
supply (via lower interest rates) or by direct infusions of government spending, affects only wage and price levels;
it does not affect the level of unemployment or real economic output. Indeed, according to classical economic
theory, these two government remedies will have a negative impact on the economy by drying up investment
funds that could be better used by private industry. The policy implications of the classical model are clear:
governments have little ability to effect overall economic output, and the measures they take are usually harmful.
The Great Depression of the early 1930s, however, challenged the presumptions of classical economics, as the
prolonged slump and the unprecedented levels of unemployment undermined the idea that economies tend toward
a full-employment, high-output equilibrium.
 Focus on Demand
Unlike classical economists, Keynes focused on the demand side of the equation or, more specifically, the
aggregate demand that results from the spending decisions made by all players in an economy—consumers,
businesses, and government. According to Keynes, reduced spending leads to reduced demand, which in turn
leads to further reductions in spending and so forth. This cycle, he argued, was responsible for the prolonged
economic slump of the 1930s. Most economists of Keynes’s day asserted that reduced spending was caused by a
tightened money supply as banks became more cautious in their lending, thereby making it hard for businesses to

invest and hire. Expanding on the work of earlier business cycle theorists, Keynes contended that larger factors
send economies into slumps. Specifically, he said, the millions of decisions made by consumers and businesses
for any number of reasons lead to the reduced aggregate demand that is the primary cause of an economic
downturn. Yet Keynes did not ignore the supply side of the equation, which lay at the heart of classical
economics. Whereas the latter assumed price and wage flexibility, Keynes argued for their inflexibility. In other
words, demand is not created by supply, as Say maintained; it works independently of it. Thus, Keynes
maintained, an economy can find its supply-demand equilibrium far below the full-employment and high-output
level insisted upon by classical economists.
Just as classical economic theory had important policy ramifications, so did the Keynesian business cycle model.
Because it argued that expansion and contraction cycles are the economic norm—as opposed to the high-output,
high-employment equilibrium asserted by classical economists—the Keynesian model held that government should
intervene in the economy to flatten that cycle and the human misery it causes. He also argued that there are times
when only government action can lift an economy out of a low equilibrium slump through fiscal policy. That is, by
pumping expenditures directly into the economy—Keynes advocated direct employment on infrastructure projects
as one way to do this—the government can increase demand. Moreover, every dollar spent would have a
multiplier effect, as contractors buy supplies and the people who are hired spend more money in stores.
The Keynesian model had a dramatic impact on the economists who shaped government policy during the post–
World War II era, even if his recommendations were only tepidly applied in the United States and other
industrialized nations during the Great Depression itself. Moreover, there was nothing in the Keynes model that
said increased government spending had to be on infrastructure. Military spending, he implied, would have much
the same economic effect, even if the social consequences were not as positive. Indeed, the mass spending of
World War II, which effectively lifted the United States out of the Great Depression, seemed to confirm this view.
For the next few decades, national governments applied Keynesian ideas, effectively tempering the business cycle
and, say proponents, contributing mightily to the West’s postwar economic boom of the 1950s and 1960s.
 Challenges
Keynesian economics immediately faced challenges both from the Left and the Right, and itself was split between
competing interpretations in Cambridge, England, and Cambridge, Massachusetts. The most critical challenge to
the model came with the persistent economic troubles of the 1970s, in which high inflation co-existed with
sluggish growth. The Keynesian solution of pumping money into the economy—in the United States, through
military spending on the Vietnam War and expanded social programs—only seemed to exacerbate inflation without
easing stagnation. To address this problem, a number of governments, including that of the United States, shifted
to an economic policy based on monetarist theories. According to these theories’ most forceful proponent,
American economist Milton Friedman, the money supply was the primary determinant of change in output and
prices. Thus, increasing the money supply during economic downturns—by lowering interest rates or through
government spending—had an inevitable inflationary effect that fed on itself. (Consumers tended to spend more
before their dollars lost value, and workers demanded higher wages.) What the government should do instead,
said the monetarists, was adhere to a stable and predictable monetary increase, so as to wring inflation out of the
economy and produce stable growth. In fact, the monetarist approach of the U.S. Federal Reserve during the late
1970s and early 1980s did lower the inflation rate dramatically, if at the cost of temporarily high unemployment
rates.
The monetarist approach was not entirely triumphant, however, as governments in the West continued to use both
Keynesian and monetarist—or modified monetarist—approaches to effect stable growth. With the economic crisis
of the 2000s, the pendulum began to shift back toward a Keynesian model in the United States and elsewhere, as
governments began to increase spending to counteract the dramatic fall in aggregate demand and rising
unemployment set off by a sudden contraction in the credit markets. Whether such measures—including President
Barack Obama’s unprecedented $787 billion Economic Stimulus Package of 2009—would have the effect
assumed by the Keynesian model remained to be seen.

James Ciment
 
See also:  Classical Theories and Models;  Great Depression (1929-1933);  Keynes, John
Maynard;  Neoclassical Theories and Models;  New Deal. 
Further Reading
Cate, Thomas, ed. An Encyclopedia of Keynesian Economics. Cheltenham, UK: Edward Elgar, 1997. 
Clarke, Peter. The Keynesian Revolution and Its Economic Consequences. Cheltenham, UK: Edward Elgar, 1998. 
Felix, David. Biography of an Idea: John Maynard Keynes and the General Theory of Employment, Interest and
Money. New Brunswick, NJ: Transaction, 1995. 
Keynes, John Maynard. The General Theory of Employment, Interest and Money. London: Macmillan, 1936. 
Skidelsky, Robert. John Maynard Keynes: A Biography. London: Macmillan, 1983. 
Kindleberger, Charles P. (1910–2003)
 
The American economist and economic historian Charles Kindleberger was a leading architect of the Marshall
Plan, the U.S. economic and technical aid program to rebuild Europe after World War II, and a prolific writer. He
authored more than thirty books, the best known of which is Manias, Panics and Crashes: A History of Financial
Crises (1978). Revised editions of the work appeared after the burst of the dot.com bubble in 2001 and
(posthumously, with updates by Robert Z. Aliber) amid the recession of 2007–2009. Kindleberger taught
economics at the Massachusetts Institute of Technology (MIT) for thirty-three years.
Charles Poor Kindleberger II was born on October 12, 1910, in New York City. He graduated from the University
of Pennsylvania in 1932 and received his master’s degree from Columbia University in 1934, where he also
completed his doctorate, under the monetary theorist James W. Angell, in 1937. His thesis, International Short-
Term Capital Movements, was published that same year.
As a researcher in international trade and finance for the Federal Reserve Bank of New York from 1936 to 1939,
Kindleberger spent a year in Switzerland with the Bank of International Settlements. He served as a research
economist for the board of governors of the Federal Reserve System from 1940 to 1942, leaving that position for
a naval commission in World War II. Dissatisfied with his desk job, he joined the Office of Strategic Services
(OSS), the precursor to the Central Intelligence Agency, as an intelligence officer. He went on to become a major
in the 12th Army Group in Europe, identifying enemy supply lines for Allied bombing missions.
With the end of the war, Kindleberger was appointed chief of the Division of German and Austrian Economic
Affairs at the U.S. Department of State in Washington, D.C., where he played a key role in devising the European
Recovery Program, or Marshall Plan. Returning to academia in 1948, Kindleberger joined MIT as an associate
professor of economics and became a full professor in 1951; he was the Ford international professor of
economics until his retirement in 1976, and maintained his ties with the university as a professor emeritus.
Unlike the many economists who seek to support their theories through statistical data and economic modeling,
Kindleberger looked for historical parallels to explain his views. The first of his major works in the field of economic

history to gain notice outside academia was The World in Depression 1929–1939 (1973). In that book, he argued
that the Great Depression of the 1930s resulted from the decline of British economic dominance after World War I
and the failure of Republican administrations in the United States to take up the lead in global affairs. That
reluctance, he maintained, was evidenced by the isolationist policies of the Warren G. Harding and Calvin
Coolidge administrations, the lack of U.S. interest in maintaining foreign exchange rates, and the unwillingness of
President Herbert Hoover to support failing banks at the start of the Depression. Kindleberger also criticized the
work of economist Paul A. Samuelson and dismissed John Maynard Keynes’s position that the Depression was
caused by a lack of demand. The liberal economist John Kenneth Galbraith praised the book, though he found
Kindleberger’s conclusions about the difficulties in the New York Stock Exchange “cautious.”
Kindleberger’s next major work, Manias, Panics, and Crashes (1978), examined stock market crashes through
history and the problems of rampant speculation. The book sold well when it was first published and was released
in four new editions thereafter. Other notable works by Kindleberger include Europe’s Postwar Growth: The Role of
Labor Supply (1967), American Business Abroad (1969), and World Economic Primacy: 1500–1990 (1996). He
died in Cambridge, Massachusetts, on July 7, 2003.
Justin Corfield
 
See also:  Great Depression (1929-1933);  Panics and Runs, Bank. 
Further Reading
Galbraith, John Kenneth. Money: Whence It Came, Where It Went. London: Andre Deutsch, 1975. 
Kindleberger, Charles. The Life of an Economist: An Autobiography. Oxford: Basil Blackwell, 1991. 
Kindleberger, Charles, and Robert Z. Aliber. Manias, Panics, and Crashes: A History of Financial Crises.  6th ed.
Basingstoke, UK: Palgrave Macmillan, 2010. 
 
Kondratieff Cycles
 
Kondratieff cycles—also known as supercycles, long waves, or k-waves—are long-term trade cycles affecting the
global capitalist economy, each just over a half a century in duration, that follow a predictable pattern of expansion
and stagnation and/or decline. Because they involve such profound economic change, Kondratieff cycles are also
linked to political upheavals, such as war and revolution. The cycles are named for Soviet economist Nikolai
Kondratieff (also spelled Kondratiev), who conceived of them in the 1920s as a way to explain the history of
capitalism from the French Revolution of the late eighteenth century through his own time. In the decades since,
other social science theorists have elaborated on Kondratieff’s original concept, reconsidering the causal factors of
the cycles and extending them to later time periods.

 Foundations and Dynamics
Like many innovative theories, Kondratieff’s was not entirely original. Earlier economists, such as the Frenchmen
Clément Juglar in the 1860s, had suggested that capitalist economies operate in shorter expansion-contraction
phases. Two Dutch economists of the early twentieth century, Jacob van Gelderen and Samuel de Wolff, had
proposed somewhat longer phases as well. Still, the most widely circulated and influential articulation of the idea
of long-term economic cycles was Kondratieff’s.
His theory—first presented in a 1926 article titled “The Long Waves in Economic Life”—was based on an analysis
of the economies of Great Britain, France, and, to a lesser extent, the United States. (These countries, he noted,
offered the richest mine of economic data for that period.) Using these numbers, Kondratieff calculated that, in the
previous 130 or so years, the three countries had experienced two-and-a-half long-term economic cycles: a broad
upswing from 1789 to 1814, and a downswing from 1814 to 1849; another upswing from 1849 to 1873, and a
downturn from 1873 to 1896; and an upswing from 1896 to the early 1920s, which Kondratieff predicted would
soon turn to another downswing. He also asserted that, if the economic data were available, the era prior to 1789
would reveal the same pattern. Kondratieff did not insist that periods of upswing and downswing are uniform and
consistent. He did say that, in the former period, years of growth outnumbered years of contraction, while
recessions tend to occur more frequently during the downswing phase of the cycle.
Periods of economic expansion, according to Kondratieff’s theory, are characterized by falling interest rates,
increased wages, rising prices, and the increased production and consumption of basic commodities. (His analysis
focused on data for coal, pig iron, and lead.) Other characteristics of upswing periods include the widespread
introduction of new technologies and increased gold production. In downswings, the agricultural sector experiences
long-term depression, new technologies are invented, and gold production falls. Finally, Kondratieff offered the
paradoxical idea that political tensions rise during times of economic expansion and ease during times of
economic contraction.
Kondratieff was most emphatic about the impact of economic cycles, arguing that they result in many phenomena
that, in a mistaken logical order, often are identified as causes of the cycle. For example, he argued, while
individual creativity certainly plays a role in technological innovation, the conditions that allowed for that creativity
and, more importantly, the application of that technological creativity to the broader economy are a “function of the
necessities of real life” as determined by long-term economic cycles. In other words, as a Marxist, Kondratieff
emphasized the importance of great historical forces beyond the influence of individual economic agents. Similarly,
political tensions typically do not trigger long-term upswings or downswings but tend to be caused by upswings.
Thus, wars are a result of increased competition for resources and markets, and revolutions are produced by the
“social shocks” associated with the “new economic forces” of upswing periods. Nor did the inclusion of new
countries into the global economy, such as the United States in the mid-nineteenth century, produce upswings.
Again, the long-term phases in the cycle, Kondratieff insisted, created the conditions for their inclusion.

A comparison of actual U.S. wholesale prices and Nikolai Kondratieff’s idealized long wave shows close
correspondence in roughly half-century cycles: upwaves of inflation/expansion, price/growth peaks, downwaves of
contraction, and recessionary troughs.
 Influences and Variations
As history would have it, Kondratieff’s theories brought great personal misfortune to their author. Originally a
proponent and architect of the five-year economic plans of the Communist Soviet Union, Kondratieff became
convinced, as a result of his research into long-term cycles, that the plans were misguided and ineffective. This
position put him at odds with Soviet dictator Joseph Stalin, who had him imprisoned and then executed during the
political purges of the late 1930s.
Outside the Soviet Union, meanwhile, Kondratieff’s theory was gaining adherents, among them Austrian-American
economist Joseph Schumpeter. While accepting the existence of Kondratieff cycles—indeed he gave them their
name as a homage to the recently executed Russian—Schumpeter argued in the late 1930s that the pivot
between periods of economic decline and expansion is caused by the activities of entrepreneurs and business
innovators. Usually these activities entail the introduction of technological innovation to the wider economy, such
as the application of steam power in the late eighteenth century (corresponding to the Kondratieff upswing
beginning in 1789) or the spread of railroads in the mid-nineteenth century (the upswing beginning in 1849).
In the years immediately following World War II, as advocates of the Keynesian economic consensus promoted
government stimulus as a way to avoid or dampen the effects of market forces, Kondratieff cycles fell out of favor.
In the mid-1960s, a Marxist economist named Ernest Mandel revived interest in them with his prediction that the
latest upswing cycle, which began in the late 1940s, would come to an end within five years. Mandel was not far
off the mark, as the early 1970s brought an end to the postwar global economic expansion.
Other economists have elaborated on Schumpeter’s interpretation of Kondratieff’s work. Shifting the focus from
entrepreneurs to the large-scale industrial firms of the post–World War II era, Christopher Freeman of Great
Britain and Carlota Pérez of Venezuela argued in the 1980s that the phases of the Kondratieff cycle are
associated with new methods of business organization and management, themselves triggered by the constraints
and opportunities brought by fundamental technological change.
Kondratieff’s theory continues to intrigue economists to the present day, partly because it has accurately predicted
long-term change in the global capitalist economy. The late 1920s did, indeed, usher in a lengthy downturn in the
global economy, just as the late 1940s brought a turnaround that lasted until the early 1970s. Thus, according to

Kondratieff’s theory—and, specifically the Schumpeter-Freeman-Pérez reinterpretation—a new long-term
expansion should have begun in the 1990s, following the introduction of new information technologies and the
long-term managerial and organization responses to them. Other disciples of Kondratieff emphasize the inclusion
of new economies—such as China’s—as a factor in the new growth phase. By Kondratieff’s reckoning, then, the
deep recession of 2007–2009 did not signal an end to the long-term expansion of the global economy, as many
feared, but rather a rough patch—albeit it a rocky and jarring one—on a quarter-century-long road of expansion
that would come to an end in the 2020s.
James Ciment
 
See also:  Juglar, Clément;  Kondratieff, Nikolai Dmitriyevich;  Seasonal Cycles;  Sunspot
Theories. 
Further Reading
Barnett, Vincent L. Kondratiev and the Dynamics of Economic Development: Long Cycles and Industrial Growth in Historical
Context. New York: St. Martin’s, 1998. 
Freeman, Chris, and Francisco Louçã.  As Time Goes By: From the Industrial Revolutions to the Information Revolution.
New York: Oxford University Press, 2001. 
Mandel, Ernest.  Long Waves of Capitalist Development: A Marxist Interpretation. New York: Verso, 1995. 
Solomou, Solomos.  Phases of Economic Growth, 1850–1973: Kondratieff Waves and Kuznets Swings. New
York: Cambridge University Press, 1988. 
Kondratieff, Nikolai Dmitriyevich (1892–1938)
 
The Russian economist Nikolai Dmitriyevich Kondratieff was one of the most influential thinkers on business
cycles in the twentieth century, known for his work on a long-wave theory of growth. A prominent critic of Western
capitalism and an active socialist revolutionary, Kondratieff applied his formidable research and theoretical acumen
to the early economic planning agenda of Soviet Russia. One of the leading proponents of Soviet leader Vladimir
Lenin’s New Economic Policy (NEP), Kondratieff was to die in Joseph Stalin’s Great Purge of the late 1930s at the
age of only forty-six.
Born on March 4, 1892, north of Moscow, Kondratieff came from a peasant family but managed to secure tuition
at the University of St. Petersburg from the Ukrainian economist Mikhail Tugan-Baranovsky, who got to know the
boy and was impressed with his potential. Kondratieff initially specialized in agricultural economics and, as deputy
minister of food, was involved in conducting studies of food shortages during World War I. The Russian Revolution
of 1917 forced Russia to end its participation in that conflict and eliminated Kondratieff’s position.
Because his political views were in agreement with those of the revolution, Kondratieff was allowed to return to
academia. He began teaching at the Agricultural Academy of Peter the Great in 1919, and the following year
became founding director of the Moscow Business Conditions Institute, which planned, monitored, and forecast
economic and business activity in Soviet Russia; he served in that position until 1928. With the Communist regime
keen to introduce a five-year plan for the nation’s economy, Kondratieff became integrally involved in establishing
the underlying principles and theories. Toward that end, he traveled to Great Britain, Germany, and North America

in the early 1920s to gain firsthand knowledge of Western business-cycle theory.
Kondratieff formulated his general thesis in works titled The World Economy and Economic Fluctuations in the
War and Post-War Periods (1922), On the Notion of Economic Statics, Dynamics and Fluctuations (1924), and
The Great Economic Cycles (1925), published in Russian. In them, he outlined his idea of what economists came
to call “Kondratieff cycles” or “Kondratieff waves.” Every fifty or sixty years, he showed, Western capitalist
economies experienced periods of economic depression after periods of rapid growth. Although earlier economists
had suggested a similar concept, Kondratieff devised and elaborated the theory independently, proving far more
insightful and influential than any predecessors.
Kondratieff’s research greatly impressed the Soviet regime, which adopted his ideas as the theoretical foundation
for the nation’s economic policy in the mid-1920s. The New Economic Policy that evolved from Kondratieff’s work
assumed the primacy of agricultural production over heavy industrial manufacture. But by the late 1920s, Vladimir
Lenin, the founder of Soviet communism, was dead, and Joseph Stalin had taken control of the government. The
new leader was suspicious of officials who had worked under his predecessor, and Kondratieff’s political fortunes
took a turn for the worse. His influence as a government economist greatly reduced, Kondratieff was fired as
director of the Business Conditions Institute in 1928. The following year, the Great Soviet Encyclopedia declared
his theory “wrong and reactionary.”
Kondratieff was arrested in July 1930 and eventually charged with being a member of the Peasants Labor Party,
an illegal organization that, according to some scholars, might have never even existed. He was sentenced to
eight years in prison, but Stalin—who viewed him, like so many other intellectuals, as an enemy—wanted him
executed. While in prison, Kondratieff managed to complete several more works on economic theory, which were
published posthumously. On September 17, 1938, Kondratieff was tried for a second time, sentenced to ten years
in prison, and barred from writing to anybody outside the prison. That same day, he was shot dead by a firing
squad. It was not until July 1987 that Kondratieff was officially rehabilitated in the Soviet Union. Eleven years later,
with the Soviet Union dissolved, his collected works were translated into English by Stephen S. Wilson and
published in London by Pickering & Chatto.
Justin Corfield
 
See also:  Kondratieff Cycles. 
Further Reading
Barnett, Vincent L. Kondratiev and the Dynamics of Economic Development: Long Cycles and Industrial Growth in Historical
Context. New York: St. Martin’s, 1998. 
Barnett, Vincent L.  “Which Was the ‘Real’ Kondratiev: 1925 or 1928?” Journal of the History of Economic Thought 24:4
(December 2002): 475–478. 
Jasny, Naum. Soviet Economists of the Twenties: Names to Be Remembered. Cambridge, UK: Cambridge University
Press, 1972. 
Louca, Francisco.  “Nikolai Kondratiev and the Early Consensus and Dissensions About History and Statistics.” History of
Political Economics 31:1 (1999): 169–206. 
Koopmans, Tjalling Charles (1910–1985)

 
The Dutch-American economist Tjalling Charles Koopmans was a co-winner, with Leonid Kantorovich of the
Soviet Union, of the 1975 Nobel Prize in Economic Sciences. The two were awarded the prize for their
“contributions to the theory of optimum allocation of resources,” or, according to the Royal Swedish Academy of
Sciences, “how available productive resources can be used to the greatest advantage in the production of goods
and services.” Beginning with his work on the efficient use of shipping facilities, Koopmans was said to apply
“brilliant mathematical techniques to develop the complicated equations in this field.” His work in econometrics and
mathematical programming helped open a new area of economic studies.
Tjalling Koopmans was born on August 28, 1910, at’s-Graveland, The Netherlands. At age seventeen, he entered
the University of Utrecht, where he studied mathematics and theoretical physics. In 1933, after meeting the Dutch
economist Jan Tinbergen, Koopmans moved to Amsterdam to work on mathematical economics and statistics.
After completing his doctoral thesis at the University of Leiden in 1936, he served as a professor at the
Netherlands School of Economics from 1936 to 1938, and as the specialist financial secretary at the League of
Nations from 1938 to 1940.
Moving to the United States in 1940, Koopmans worked as an economic analyst for the Anglo-American
Combined Shipping Adjustment Board. Near the end of World War II, he joined the Cowles Commission for
Research in Economics at the University of Chicago, where he served as a professor from 1944 to 1955. He
became a naturalized U.S. citizen in 1946. When the Cowles Commission moved to Yale University in 1955,
Koopmans went with it; he was a professor there from 1955 until his death in 1985.
Koopmans’s growing interest in the economics of transportation led to his study of optimal routing. At Yale, he
devoted much of his research to the economics of optimal economic growth and the development of a
comprehensive theory to determine the proper allocation of resources—labor, capital, and natural resources—to
ensure optimum growth in an economic system. This led to his Nobel Prize–winning study of the optimum
allocation of resources, in which he used his background in mathematics to provide a system of interacting
equations that took into account the cost of materials at their source, then the cost of transporting them using
alternative routes.
In addition to his research, Koopmans wrote extensively on economic theory and the major issues facing
twentieth-century economists. His best-known book, Three Essays on the State of Economic Theory (1957),
continues to be widely read. Koopmans received honorary doctorates in economics from the Netherlands School
of Economics; the Catholic University of Louvain, Belgium; Northwestern University; and the University of
Pennsylvania. He died on February 26, 1985, in New Haven, Connecticut.
Justin Corfield
 
See also:  Growth, Economic. 
Further Reading
Koopmans, Tjalling. The Scientific Papers of Tjalling C. Koopmans. New York: Springer-Verlag, 1970. 
Koopmans, Tjalling. Three Essays on the State of Economic Theory. New York: McGraw-Hill, 1957. 

 
Korea, South
 
South Korea, officially the Republic of Korea, is a geographically small nation of about 48 million people, located in
the southern half of the Korean Peninsula, east of China and west of Japan. Inhabited by an ethnically
homogenous people with a civilization going back thousands of years and heavily influenced by China, the Korean
Peninsula was largely isolated from the outside world until the late nineteenth and early twentieth centuries, when
it became a colony of Japan. Following the latter nation’s defeat in World War II, the peninsula was divided into
two countries—capitalist South Korea and communist North Korea.
In the wake of a brutal three-year war between the two countries (1950–1953), South Korea was left an
impoverished, largely agricultural nation ruled by a repressive dictatorship. Beginning in the 1960s, however, the
country began to industrialize, emerging as one of the world’s largest economies and, in the 1980s, as a nascent
democracy. While suffering a major setback with the Asian financial crisis of the late 1990s, the South Korean
economy continued to grow at breakneck speed until slowed by the global financial crisis and recession of 2008–
2009.
 Economic History to the Korean War
Korean history dates back to the migration of people from China in the early Neolithic Era, with the first agricultural
settlements appearing around 6,000 BCE. Koreans date the beginning of a distinct Korean civilization to the
founding of the Gojoseon state in 2333 BCE. Chinese social, cultural, and economic influences remained strong—
particularly Buddhism, which arrived in the fourth century CE—though various Chinese efforts to conquer the
peninsula in the first millennium CE were repulsed by temporary confederations of Korean kingdoms. Despite such
invasions, Korea became a center of manufacturing, especially known for its ceramics, and the seeds of a national
commercial system emerged.
Following a period of civil conflict and chaos, much of Korea was united under the Koryo dynasty from the tenth
through fourteenth centuries. Korean society became increasingly stratified during this period, with a wealthy and
powerful aristocracy largely situated in the capital and an impoverished peasantry, consisting of serfs and large
numbers of privately held and government-owned slaves, in the countryside. There were no cities other than the
capital, Songdo (now Kaesong); the use of money waned and commerce nearly died out.
In the late fourteenth century, the Koryo dynasty was overthrown (with the help of the Ming dynasty in China) and
replaced by the Choson (or Yi) dynasty, which would rule the country until 1910, when the Japanese occupied the
peninsula and made it an imperial colony. Choson rulers, employing Confucian bureaucratic methods, imposed a
strong centralized government on the peninsula from its capital Seoul. They replaced the system of tributes paid
by local landlords—who imposed them on peasants—to a universal tax on agricultural harvests. Such reforms
eased the financial burden of peasants and brought a measure of prosperity to the countryside. Choson
bureaucrats also granted merchants the freedom to accumulate and invest capital, though it required them to be
licensed by the government.
Korea prospered in the seventeenth and eighteenth centuries with the adoption of new, more productive

agricultural methods, new crops from the Americas, and a merchant-led commercial system that expanded trade
domestically and with China and Japan, exporting tobacco, cotton, ginseng, ceramics, and paper. Seoul emerged
as a major manufacturing center, with more than a thousand markets by 1800.
Politically, however, the Choson dynasty increasingly came under the control of China, becoming a mere vassal by
the 1700s. At the same time, Korea became closed off to the world, earning the title “hermit kingdom.” By the
1800s, the country’s economy began to slide into stagnation and its people into poverty. Not until the Sino-
Japanese War of 1894–1895 was Korea finally liberated from Chinese rule, with the nation declared an
independent republic.
Liberation proved limited and short-lived, however, as Japanese political and economic interests came to dominate
the peninsula, culminating in Korea’s formal annexation as an imperial colony in 1910. While the Japanese did
much to develop the country, building the beginnings of a modern industrial and transportation infrastructure and
opening it up to outside trade, the effort was largely to serve the colonizers’ interests. Korean nationalists
remained harshly opposed to Japanese rule, especially during World War II, when the Japanese used Korean
males as slave laborers and Korean females as sex slaves for imperial troops.
With Japan’s defeat in 1945, the Korean Peninsula was divided at the 38th parallel, with areas to the north under
the Soviet sphere and areas to the south within the U.S. sphere. The division was supposed to be temporary, until
elections could be held to choose a government for a unified Korea. But the two superpowers and the two
governments of Korea could not be reconciled, and the division was formalized in 1948.
Two years later, North Korea, under the leadership of Kim Il Sung, invaded the South in the hope of unifying the
country under communist rule. Nearly successful, Kim’s troops were driven back by South Korean, U.S., and other
allied forces. After a newly communist China entered the fray on behalf of the North, the war settled into a
stalemate, finally halting with an armistice (not a formal peace treaty) in 1953. To this day, the two countries
remain technically at war and the border between them highly militarized.
 Economy Since the Korean War
South Korea emerged from the war with its infrastructure decimated and its people impoverished. It was among
the poorest nations in the world, with a per capita gross domestic product (GDP) on par with much of Africa. It did
not stay that way for long. Using massive amounts of U.S. aid, dictator Syngman Rhee, while ruthlessly repressing
political dissent of any kind, built a broad-based educational system and a modern transportation and
communications infrastructure that unified the mountainous country. South Korea also received significant amounts
of aid from Japan during this period, as reparations for the years of exploitative colonial rule.
Like the Japanese government, the South Korean regime engaged in strategic economic planning for the country,
working with major family-owned industrial conglomerates known as chaebol to develop key heavy industries and
a strong export sector. It was a smart strategy, according to economic historians, since South Korea had to export
to pay for the natural resources it lacked and had a high savings rate among its still low-income citizenry, which
lowered domestic demand. Starting in areas such as textiles and clothing, Korea moved into the production of
steel, ships, cars, and electrical and electronic items and components. The government assisted companies by
intervening to maintain quiescent workforces, protecting them from competition and granting financial aid to
promote chaebol.
Chaebol are large business groups with a plethora of diversified subsidiaries, including chemicals, heavy industry,
electronics, and services. They are family founded, owned and supported by complex cross shareholdings,
subsidies, and loan guarantees. Much of the large business sector was part of a chaebol network and the network
exerted widespread, ingrained, and deep influences on society. Practices included lifetime employment,
“seniorityism,” and extensive in-house inculcation in company history, vision, and songs. Chaebol dominated some
localities, leading to the emergence of company towns such as Woolsan (Hyundai) and Pohang (POSCO) to
house and serve the needs of employees.

There were more than sixty chaebol in all, although a few dominated. By the 1990s, the top five (Hyundai,
Daewoo, Samsung, LG, and SK) accounted for about 9 percent of South Korea’s GDP; the top 30 accounted for
15 percent of GDP and spread across over 800 subsidiaries and affiliates. Some became major global companies
engaging in production, acquisitions, and investments.
The emphasis on industrialization created a growing disparity in wealth between the countryside and city, causing
a mass exodus from the former to the latter. But the policy paid off, as South Korea experienced GDP growth
consistently around 10 percent annually from the 1960s through the 1980s, rising from about $30 billion in 1960 to
$340 billion in 1989. During the same period, the per capita GDP climbed from $1,200 to more than $8,000, one
of the fastest increases in human history and one that pulled South Korea—one of the four economic “tigers” of
East and Southeast Asia—into the ranks of the developed world.
 Cultural Factors
Culture and custom aided the government in its efforts to make Korea an economic powerhouse. Today it is
currently the ninth-largest economy in the world. Deeply influenced by Confucian attitudes about work and
education, South Koreans are known to put in extraordinarily long hours at work and at school, and to save their
money assiduously.
The nation’s economic system and success were also influenced by cultural heritage. Especially influential were
Buddhism (from 372 CE, and especially 935 to 1392) and Confucianism, the state religion for more than 500
years, to the early twentieth century. The Confucian code of personal and social behavior was maintained by a
hierarchical, authoritarian structure. Social values included an emphasis on family, close relationships between
father and son, differential gender roles, precedence of elders, and mutual trust among friends. Also held in high
regard was educational attainment, one of the best and shortest ways to social status and jasusungga (making
one’s own fortune).
In addition, Korean society was resistant to foreign peoples, countries, and cultures for several reasons. First, the
population was ethnically and linguistically homogeneous to a strong degree. Second, the prevalent agrarian
society was characterized by passive, closed, and insular perspectives. The climate favored rice cultivation, which
was labor-intensive, time-intensive, and centered along rivers and deltas in isolated communities. These
communities fostered close-knit, interdependent groups that emphasized collectivism and inter-group
responsibilities. Third, antagonistic memories and feelings toward foreign interventionist powers were deeply
entrenched.
The historical and cultural legacy has influenced the modern South Korean economy through the predominant
corporate culture, management values, and organizational structures and practices. One long-standing expression
of that legacy has been a group-orientated approach to business. Traditionally, Korean workers have tended to
sacrifice their personal goals for collective ones, in return for which they have been taken care of by the business
or community. In-group harmony (inhwa) was important, with mutual independence making out-group boundaries
more salient. Commercial enterprises were highly centralized and vertically organized, with family-style hierarchical
principles and relationships making for more predictable behavior, obligations, and indebtedness.
Authoritarianism and paternalism have been much in evidence in Korean culture, with companies assuming the
role of parents and employees the role of family members. Important positions traditionally were filled by kinship-
based recruiting from extended clans (chiban) or regions, dominated by kinship-based relationships with owners
(hyulyon). Ideas of harmony and family-oriented management had seniority as the primary factor. Thus, Korean
organizations were, according to Korea scholars Y.H. Cho and J.K. Yoon, “like families as well as armies.”
Such influences have continued into contemporary society, guiding daily life and social mores, values, ways of
thinking, and modes of conduct, with family, hierarchy, seniority, and traditions paramount. Nevertheless, cutting
against these cultural elements are more contemporary developments and trends, such as Western approaches to
education and employment, globalization, the internationalization of business, and opening up to other cultures.

Indeed, from the early 1990s, South Korean governments explicitly employed a policy of globalization (segyewha)
that facilitated more communication and interaction with other countries. Companies adopted similar policies,
sending employees abroad for exposure to different cultures.
 Government
While the Seoul government was highly successful in promoting industrial development, it was not always as
disciplined with its own finances. It ran up major budget deficits in the 1970s—exacerbated by the sudden rise in
global energy and raw material prices—which forced the government, under pressure from international financial
institutions, to put its fiscal house in order. A conservative monetary policy helped rein in inflation but also
triggered widespread unrest, which eventually forced the regime to democratize (the desire to present its best face
to the world for the 1988 Seoul Olympics was also a factor). By the late 1980s and early 1990s, the country was
once again achieving remarkable levels of growth, with chaebol such as Samsung, Daewoo, and Hyundai
establishing themselves as globally recognized brand names in consumer electronics, shipbuilding, and
automobiles, respectively.
In 1997, however, the Korean economy stumbled as a result of the Asian financial crisis that began in Thailand.
As foreign investors pulled their money out of high-growth Asian economies, such as South Korea’s, the national
currency, the won, began to depreciate rapidly. The situation was made worse by the fact that Korean banks were
saddled with large portfolios of nonperforming loans. In addition, some of the chaebol found themselves unable to
meet their own debt obligations. One of the largest, Daewoo, was ultimately dismantled and sold off in pieces by
government regulators. By the end of 1997, South Korean leaders were forced to go to the International Monetary
Fund for a bailout, which ultimately amounted to nearly $60 billion. Contributing to the problem, according to some
Korea experts, was the nepotism and “crony capitalism” inherent in the country’s close political-business
connections, opaque structures, and corporate governance characterized by circular investments and complicated
inter-company relations.
Union employees of a debt-burdened South Korean bank protest IMF policies during the Asian debt crisis of 1997.
As a result of conditions imposed by the IMF in exchange for its bailout package, Koreans feared foreign
domination of the nation’s financial system. (Choo Youn-Kong/AFP/Getty Images)

The troubles and humiliation the Asian financial crisis brought to South Korea were relatively short-lived. With new
sources of foreign and domestic capital pouring in, the nation’s merchants and manufacturers once again began to
expand operations, contributing to a GDP growth rate of nearly 10 percent in 2000. Meanwhile, the government
embarked on an extensive restructuring of the financial sector, imposing new rules that made the cozy and often
corrupt relationships between bankers and industrialists more difficult to sustain.
Despite such restructuring, the South Korean economy and its financial sector were especially hard hit by the
global financial crisis of 2008–2009 and the recession that grew out of it. Fearing a repeat of 1997, foreign
investors pulled massive amounts of capital out of Korean securities and banks, triggering a 40 percent drop in
the country’s main stock index and a more than 25 percent decline in the value of the won. Heavily dependent on
exports, the South Korean economy was disproportionately hurt by the global recession, forcing the government to
bolster bank reserves and pump money into the economy through aggressive stimulus measures. Still, economic
experts expected South Korea to come out of the crisis more quickly than the United States and the European
Union, since so much of its economy was now connected to China’s (which continued to surge).
James Ciment and Chris Rowley
 
See also:  Asian Financial Crisis (1997);  China;  Japan. 
Further Reading
Buzo, Adrian. The Making of Modern Korea. New York: Routledge, 2007. 
Cho, Y.H., and J. Yoon.  “The Origin and Function of Dynamic Collectivism: An Analysis of Korean Corporate Culture.” Asia
Pacific Business Review 7:4 (June 2001): 70–88. 
Cumings, Bruce. Korea’s Place in the Sun: A Modern History. New York: W.W. Norton, 2005. 
Rowley, C.  “The Credit Crunch and Impacts in Asia.” Professional Manager 14:7 (2008). 
Rowley, C., and J. Bae, eds.  Korean Businesses: Internal and External Industrialisation. London: Frank Cass, 1998. 
Rowley, C., and Y. Paik, eds. The Changing Face of Korean Management. London: Routledge, 2009. 
Rowley, C., T.W. Sohn, and J. Bae, eds. Managing Korean Businesses: Organization, Culture, Human Resources and
Change. London: Frank Cass, 2002. 
Stiglitz, Joseph E., and Shahid Yusef, eds. Rethinking the East Asian Miracle. New York: Oxford University Press, 2001. 
Kuznets, Simon Smith (1901–1985)
 
The Russian-American economist Simon Smith Kuznets was awarded the 1971 Nobel Prize in Economics in
recognition of “his empirically founded interpretation of economic growth.” Kuznets is credited with revolutionizing
the field of econometrics and developing the concept of gross national product (GNP). His research also had a
profound impact on economists’ understanding of how business cycles work.
Kuznets was born on April 30, 1901, in Pinsk, Russia (now Belarus), to Abraham and Pauline Friedman Kuznets.
When his father immigrated to the United States in 1907, he changed the family name to Smith; Kuznets, who

remained in Russia, retained his original surname. After beginning his higher education in 1922 at Kharkov (now
Kharkiv), Ukraine, he moved to the United States and completed his bachelor of science degree at Columbia
University in 1923, his master’s the following year, and his doctorate in 1926. Kuznets then became a research
fellow with the Social Science Research Council, where his research on the cyclical pattern in prices led to his first
book, Secular Movements in Production (1930).
Kuznets taught at the University of Pennsylvania from 1930 to 1954. He then became a professor of political
economy at Johns Hopkins University, where he remained until 1960. He was the Frank W. Taussig research
professor in economics at Harvard University from 1960 until his retirement in 1971.
Heavily influenced by economist John Maynard Keynes, much of Kuznets’s early work concerned the study of
prices. His book Commodity Flow and Capital Formation was published by the National Bureau of Economic
Research (NBER) in 1938; three years later he completed National Income and Its Composition, 1919–1938
(1941), which described trends in gross national product (GNP) during the years between World War I and II.
From this work, Kuznets developed the model of the business cycle known as the Kuznets curve, which identifies
increases or acceleration in GNP during boom periods and declines or slowdowns in GNP during downturns.
Studying Keynes’s Absolute Income Hypothesis (1936), Kuznets found that Keynes’s predictions did not hold up
under careful examination. Expanding Keynes’s empirical work on the subject to cover the period from the 1870s
until the 1940s, Kuznets showed that in spite of very large changes in income, the savings ratio remained
constant throughout the seventy years in question. This became the basis of Kuznets’s book Uses of National
Income in Peace and War (1942) and influenced Milton Friedman’s later work on the relationship between income
and savings.
Kuznets’s extensive research on the national income accounts of the United States—calculating national income
back to 1869 broken down by industry, product, and usage, and measuring the distribution of income between the
rich and the poor—earned widespread academic acclaim. Capital in the American Economy was published in
1961, and Economic Growth of Nations: Total Output and Production Structure appeared ten years later. By this
time, Kuznets had convinced the U.S. Department of Commerce to standardize its measurement of GNP. At the
same time, he argued in print and before a U.S. Senate hearing that GNP was not the sole indicator of economic
health in the United States or, especially, developing nations.
Although much of his work was devoted to the study of U.S. economic health, during the 1960s Kuznets began to
study developing countries and concluded that the problems facing most of them were the same as those faced by
countries in the industrialized world before they had become economically developed. His concern that less
developed nations would be left behind the economically developed world led to a major study in which Kuznets
examined empirical data on income disparity in developing nations and identified a rising middle class as those
countries became industrialized.
In his last years, Kuznets enjoyed a flurry of recognition for his achievements. He was awarded the Robert Troup
Paine Prize in 1970, followed by the Nobel in 1971, and the Francis A. Walker Medal in 1977. Simon Kuznets
died in Cambridge, Massachusetts, on July 8, 1985.
Justin Corfield
 
See also:  Friedman, Milton;  Gross Domestic Product;  Keynesian Business Model. 
Further Reading
Abramovitz, M.  “The Nature and Significance of Kuznets Cycles.” Economic Development and Cultural
Change 9(1961): 225–248. 
Bird, R.C., M.J. Desai, J.J. Enzler, and P.J. Taubman.  “Kuznets Cycles in Growth Rates: The Meaning.” International

Economic Review 6(1965): 229–239. 
Kuznets, Simon S. Capital in the American Economy: Its Formation and Financing. Princeton, NJ:  Princeton University
Press, 1961. 
Kuznets, Simon S. Economic Change: Selected Essays in Business Cycles, National Income, and Economic Growth. New
York: W.W. Norton, 1953. 
Labor Market
 
The labor market is the theoretical “place” where the supply of and the demand for labor determine compensation
and employment levels, usually measured by the number of hours worked.
Modern economies have many labor markets, varying in size, with a host of geographic and political factors
coming into play. Labor markets are also highly segmented. While many unskilled jobs can be filled by almost any
adult, most professions require skills and education, which means that only a certain segment of the labor market
can work in them.
The supply of labor is determined by a host of factors, including population, demographics, education, immigration,
and labor participation rates. In the United States, the working population is defined as all adults over the age of
sixteen. And while mandatory retirement age has largely been outlawed or discontinued in the United States,
elderly persons are generally less likely to work than younger persons. Thus, all things being equal, as America’s
population ages, its labor market shrinks. Immigration also plays a role in two ways. First, relatively more
immigrants than native-born citizens tend to be of working age, so that a country with high immigration levels
tends to have a faster-growing labor market than a country with low immigration. In addition, immigrants tend to
cluster in certain occupations, leading to greater labor market segmentation.
Labor market participation rates are subject to a host of variables beyond age. Wages are a critical factor; as they
go up, labor market participation tends to rise. Economists refer to this relationship as “labor elasticity.” The more
likely workers are to join the labor market in response to higher wages, the more “elastic” the market is. But
demographic factors also play a role. Adult males, because most have to support themselves or a family, tend to
have higher participation rates and are less likely to be affected by wage levels. In other words, male adult labor
participation is highly “inelastic.” Before the 1960s and the mass influx of women into the labor force, the women’s
labor market participation was more elastic, as many chose—or were compelled by social factors to choose—
unpaid wifely or motherly activities and were less likely to join the labor force in response to higher wages. Today,
with more women supporting themselves or contributing to the support of their families, their labor elasticity has
decreased. For the young, who are supported by their parents, and the old, who may receive income from various
forms of pension or feel they are too old to work, labor elasticity is also higher.
Thus, labor supply is subject to a host of factors, making determination of the labor supply a complex science.
Similarly, labor demand is affected by a number of factors, most importantly labor’s productivity and the economy’s
demand for the goods and services produced in a particular labor market. Finally, both labor supply and demand
are affected by laws and institutions that determine how work is done and how much workers are paid.
Consider first productivity, for which capital equipment is critical. Workers today are far more productive than their
counterparts of a century ago, in large part because of new technologies and infrastructure. For example, a truck
driver on modern superhighways can haul far more goods, farther and faster, than a wagon master on dirt tracks

could in 1900. Likewise, an economist working with computers and having access to whole databases of
information can be more productive than one ensconced in a library using pencil and paper. Moreover, workers in
the twenty-first century are generally better educated and more skilled than their counterparts at the turn of the
twentieth century, contributing to higher productivity.
When workers are more productive, they are generally in higher demand. This also means that, all things being
equal, they can command higher wages. Such differences are a function of places as well as time. Workers in the
United States are in higher demand and can command higher wages than their counterparts in Mexico because of
the skills they possess and the technologies and infrastructure available to them. Of course, with the spread of
high technology and improving education in developing countries, the comparative advantages enjoyed by
American workers are shrinking by the decade.
Occupation is another fundamental factor in determining labor demand. Occupations that require higher skill levels
and more education usually pay better, since fewer people spend the time and money necessary to obtain them.
Moreover, it is not easy for workers to shift from one occupation to another; a coal miner cannot simply switch to
nursing because the coal industry is shrinking and the health care industry is expanding. Economists divide labor
markets into two segments: a primary market in which skilled and therefore difficult-to-replace workers have better
pay, and a secondary market in which unskilled, replaceable workers are paid less.
Labor markets differ from other markets for a number of reasons. Most importantly, because labor is not owned by
the purchaser as with traditional goods, the employer must encourage the laborer to work with a certain level of
effort, skill, and honesty. As a result, labor’s price may be somewhat higher at what economists call an “efficiency
wage.” Furthermore, labor markets are affected by social norms and government regulations on issues such as
the rights of workers to organize in unions, to receive the minimum wage, and to certain protections under rules
regarding racial, sexual, or other forms of discrimination.
Upturns and downturns in the economic cycle can also affect the labor market. When aggregate demand drops
(for any number of reasons), the economy enters a period of diminished growth or outright contraction, also
known as a recession. Diminished demand causes businesses to reduce production, and hence employment, in
an effort to cut inventories or reduce output. Economists refer to this phenomenon as “cyclical unemployment.”
Two examples, one from economic history and one from the 2007–2009 recession, illustrate that any number of
factors can come into play during economic crises. A dramatic drop in aggregate demand during the Great
Depression led to unprecedented levels of unemployment—up to one-fourth of the U.S. workforce at the trough of
the downturn in early 1933. In the classical economic model, the lower demand for labor would bring down wages
and increase employment. As Keynes argued, however, wages sometimes did not fall for a variety of social and
economic reasons—including employment contracts, collective bargaining agreements, worker morale, and others
—and when wages did fall, they further reduced aggregate demand, thus worsening the Depression. To get out of
this economically crippling situation, Keynes advocated government spending to increase aggregate demand and,
thus, output and employment.
The 2007–2009 recession, while nothing on the scale of the Great Depression, showed a similar pattern of
persistent unemployment even as the economy recovered. The drop in aggregate demand resulted in higher
unemployment, and while companies were shedding workers, wage inelasticity appeared to ease as companies
and governments chose—and got workers to accept—reduced pay and shorter hours (the latter often in the form
of unpaid furloughs). A series of federal government stimulus packages attempted to ease unemployment by
maintaining aggregate demand, but were offset by reductions in state and local government spending and ongoing
decline in the key auto and housing sectors.
James Ciment
 
See also:  Employment and Unemployment;  Unemployment, Natural Rate of;  Wages. 

Further Reading
Ashenfelter, Orley C., and David Card, eds. Handbook of Labor Economics. Amsterdam: North-Holland, 1999. 
Head, Simon. The New Ruthless Economy: Work and Power in the Digital Age. New York: Oxford University Press, 2005. 
Killingsworth, Mark R. Labor Supply. New York: Cambridge University Press, 1983. 
Mincer, Jacob. Studies in labor supply. Aldershot, UK: Edward Elgar, 1993. 
Lachmann, Ludwig Maurits (1906–1990)
 
Ludwig Maurits Lachmann was a German economist who was profoundly influenced by the work of Friedrich
Hayek and the Austrian school, of which he became an important, if somewhat unorthodox, member. Lachmann,
who helped revive interest in the Austrian school in the 1980s, is perhaps best remembered for his ideas about
economic expectations, which he viewed as neither hard data nor mathematical variables but as subjective
interpretations.
Born on February 1, 1906, in Berlin, Germany, Lachmann studied at the Askanisches Gymnasium and the
University of Berlin, where he completed his doctorate in economics in 1933. While at the University of Zurich,
Switzerland, in the summer of 1926, he was influenced by Hayek’s work and by the Austrian school, which would
have an impact on the rest of his life.
With Adolf Hitler’s rise to power in Germany, Lachmann moved to England and studied at the London School of
Economics under Hayek. As a student, he traveled to the United States from November 1938 to April 1939,
completing his master of science degree upon his return to London. After completing a research fellowship at the
University of London, he served from 1943 to 1947 as acting head of the Department of Economics and
Commerce at the University College of Hull (later the University of Hull). With his wife, the former Margot Wulff,
Lachmann moved in 1949 to South Africa, where he joined the University of Witwatersrand, Johannesburg, as a
professor and remained there until his retirement in 1972. From 1961 to 1963, he also served as president of the
Economic Society of South Africa.
Throughout his academic career, Lachmann had been intrigued with the ideas of Austrian school economist Carl
Menger. Indeed he believed that the school had, from the mid-twentieth century, deviated from Menger’s original
ideas about the construction of a marginal utility theory of value. In his writings, Lachmann prominently supported
the use of hermeneutic methods in the study of economic phenomena. He was dubbed a “fundamentalist
Austrian” for his opposition to the neoclassical school, his research and writing on economic subjectivism,
imperfect knowledge, methodological individualism, and strong support for the “radical subjectivist” strand of
Austrian economics.
As a professor at the University of Witwatersrand, Lachmann traveled regularly to New York City, where, from
1974 to 1987, he collaborated on research with Israel Kirzner to reinvigorate the Austrian school. The revival of
that movement was evident during the Austrian Economics Seminar at New York University from 1985 until 1987,
which Lachmann helped organize.
Lachmann had formed his own views on the business cycle, arguing for radical subjectivism, a concept he traced
back to Menger, and discarding the “elaborate formalism” of what was then regarded as orthodox economics.
Following Lachmann’s death on December 17, 1990, his widow established the Ludwig M. Lachmann Research

Fellowship at the London School of Economics.
Justin Corfield
 
See also:  Austrian School;  Hayek, Friedrich August von. 
Further Reading
Lachmann, Ludwig Maurits. Capital and Its Structure. London: Bell and Sons, 1956. 
Lachmann, Ludwig Maurits. Capital, Expectations, and the Market Process: Essays on the Theory of the Market
Economy, ed. W. Grinder. Kansas City, MO: Sheed, Andrews, and McMeel, 1977. 
Lachmann, Ludwig Maurits.  “From Mises to Shackle: An Essay.” Journal of Economic Literature 14(1976): 54–62. 
 
Lange, Oskar R. (1904–1965)
 
Oskar Richard Lange was a Polish economist who believed in the possibility, and indeed necessity, of
incorporating aspects of free-market principles into socialist governing systems to ensure the economic success of
socialism. The major goal of this theoretical work was to help avoid disastrous upswings and downturns in a
socialist government’s business cycles.

Polish-born economist Oskar Lange taught at the University of Chicago, became a diplomat back in Poland, and
returned to the United States in 1945 as ambassador. He advocated flexible market pricing in socialist economic
systems. (Howard Sochurek/Time & Life Pictures/Getty Images)
He was born Tomaszow Mazowiecki in central Poland on July 27, 1904, to Arthur Julius Rosner, an affluent textile
manufacturer, and Sophie Albertine Rosner. He completed his bachelor’s degree at the University of Krakow
(Poland) in 1926; two years later he earned a master’s of law. After briefly working at the Ministry of Labor in
Warsaw, he became a research assistant at the University of Krakow, where he worked from 1927 until 1931.
Winning a Rockefeller fellowship in 1934, he went to Great Britain and then to the United States in September
1935. He was named a professor at the University of Chicago in 1938 and became a U.S. citizen five years later,
changing his name to Oskar Richard Lange.
It was during his time at Chicago that Lange developed his most important ideas on economics. Although he was
a socialist, Lange disagreed with the economic theories of Karl Marx, the father of socialist thought. In his first
major work, On the Economic Theory of Socialism, published in 1938, Lange argued that the centralized control of
an economy must be flexible and realistic, otherwise countries like the Soviet Union would decline as economic
powers. Lange believed that it was important for socialist countries to relax their grip on the economy and let the
free market dictate policy.
For example, he believed that fixing prices artificially through centralized control (as done in the Soviet Union)
would damage the general economic welfare of a country. Rather, he thought there had to be flexible pricing that
would actually reflect increased production or demand, or shortages. In this way, a socialist economy could
automatically deal with shortages in products and goods and also eliminate surplus production for the optimal
economic benefit of society. Lange argued that such a socialist market economy could operate more effectively
than capitalist economies because the latter often feature business monopolies that create artificial shortages in

order to raise prices, or cut prices to increase market share and eliminate competition. As a result of more flexible
control, market socialism would be able to triumph and avoid the boom-and-bust cycle that occurred with
uncontrolled capitalism, such as existed in the United States.
Lange’s work attracted the attention of Joseph Stalin, then leader of the Soviet Union. In March 1944, with World
War II still raging, Stalin, then a U.S. ally against Nazi Germany, asked President Franklin Roosevelt to allow
Lange to visit the Soviet Union to brief him on his economic ideas.
At the end of World War II, Lange played an increasingly important role as a government official in Poland, then
under Soviet control. He renounced his U.S. citizenship, and the pro-Soviet Polish government appointed him as
its first ambassador to the United States. He also served as the Polish delegate to the United Nations Security
Council. From August 7 to August 12, 1964, following the death of Polish president Aleksander Zawadzki, Lange
was one of the four acting chairmen of the Council of State. That year, as a tribute to Lange, the University of
Warsaw published a special volume of academic economics papers containing contributions by forty-two leading
economists and statisticians. Lange died following a long illness on October 2, 1965, in London, England. In 1974,
Warsaw’s University of Economics was named in his honor.
Justin Corfield
 
See also:  Marxist Cycle Model. 
Further Reading
Friedman, Milton.  “Lange on Price Flexibility and Employment: A Methodological Criticism.” American Economic
Review 36:4 (1946): 613–631. 
Hull, Cordell. The Memoirs of Cordell Hull, Vol 2. London: Hodder & Stoughton, 1948. 
Sadler, Charles.  “Pro-Soviet Polish-Americans: Oskar Lange and Russia’s Friends in the Polonia, 1941–1945.” Polish
Review 22:4 (1977): 25–39. 
 
Latin America
 
A vast region, defined as much by history and ethnicity as it is by geography, Latin America stretches southward
from the Mexican border with the United States to the tip of South America, encompassing that continent, Central
America, and much of the Greater Antilles in the Caribbean basin.
Most of the 570 million people in the region can trace their descent to one or more of three ethnic groups:
Europeans, initially from colonial powers Portugal and Spain beginning in the sixteenth century and later from
many other countries; African slaves, who were forcibly brought to the region in significant numbers from the

sixteenth to nineteenth centuries; and indigenous peoples, who first settled in the region in Paleolithic times. Many
of the countries have large ethnically distinct populations of so-called mestizos, or persons of mixed European and
indigenous heritage.
With the exception of isolated pockets where indigenous languages predominate, the inhabitants of Latin America
speak either Spanish or Portuguese and participate in a culture that is an amalgam of European, African, and
indigenous American elements. The majority of Latin Americans practice Catholicism, though significant numbers
are Protestants or followers of indigenous or Afro-Christian hybrid religions. (This shared Iberian-influenced
linguistic and cultural heritage does not apply to those inhabitants of countries or territories of the region colonized
by the English, Dutch, and French, such as Haiti, Jamaica, Belize, or Suriname.)
Inhabited by a variety of indigenous cultures at first contact with Europe in 1492—including high civilizations of the
Andes, Central America, Mexico, and possibly the Amazon basin—much of the region was conquered over the
course of the sixteenth century by Spain and Portugal. Contact led to the mass extermination of indigenous
peoples and the influx of colonizers from Europe and slaves from Africa. The Spanish and Portuguese built an
economy based on commercial crop production, including sugar, tobacco, grains, and livestock, much of it
exported to Europe.
Vast plantations and ranches predominated, with a small, largely European elite ruling over a vast peasantry of
indigenous and mestizo workers or African slaves, with gross inequalities in wealth between the two classes. Such
patterns persisted even after much of the region achieved independence from Spain and Portugal in the early
nineteenth century. With power in the hands of commercial agricultural interests, Latin America was slow to
develop a significant middle class and was late to industrialize, its economy largely geared to the production and
export of minerals or agricultural commodities.
Only in the twentieth century did some areas—such as Argentina, Brazil, and Mexico—see the development of an
industrial base, though the region’s economy continued to rely on the export of raw materials. Such reliance—as
well as ill-considered fiscal policies by various governments—resulted in uneven economic development and a
series of financial crises. By the end of the century, however, some of the more advanced economies of the
region had developed a significant middle class and a substantial industrial sector, though large inequalities in
wealth persisted.
 Conquest, Colonialism, and Mercantilist Economics
It is generally believed that the region was first inhabited between about 15,000 and 12,000 BCE by peoples who
migrated across the Bering Strait from East Asia and then down the spine of the Western Hemisphere. The first
civilizations emerged in eastern Mexico in the second millennium BCE and, by the time of first contact with
Europeans in 1492, there were major civilizations in the Andes (Inca), the Valley of Mexico (Aztec), and the
Yucatan region of Mexico and Central America (Maya), and perhaps also the Amazon basin. While the difficult
geography of the region made intra-American contact difficult, there were wide-scale trading networks that tied
various ethnic groups to the centers of civilization. The civilizations of the Valley of Mexico, for example, traded as
far away as the present-day U.S. Southwest, Central America, and even the islands of the Caribbean.
The coming of Spanish and Portuguese (and later Dutch, English, and French) colonizers beginning in the late
fifteenth century CE utterly transformed the region and its economy. First, with the colonizers came European
diseases, which wiped out large portions of the indigenous population. It is estimated that the number of
indigenous people living in the Valley of Mexico, for example, fell from 25 million in 1519, when the first Spanish
conquistadores, or conquerors, arrived, to just 1 million by the early seventeenth century. Contributing to this
“Native American holocaust” was the encomienda system the Spanish set up in their American colonies, whereby
by the conquistadores were given huge land grants or mines and the right to tax or employ the local indigenous
peoples. While enslaving the latter was formally outlawed, the encomienda system operated as a kind of legalized
form of slavery, since the natives could not look for other work and were legally bound to the land.

Second, to compensate for a rapidly declining population of indigenous people, the Spanish and Portuguese
imported millions of Africans, largely to work on plantations in the Caribbean, coastal regions of mainland Spanish
America, and Portuguese Brazil. Finally, the conquerors brought new crops, such as wheat and sugar, and new
livestock, primarily cattle and sheep, which transformed farming in the region. (There was also a transfer of Native
American crops, such as potatoes and corn, that transformed agriculture in the Eastern Hemisphere as well.)
The conquest also had a profound effect on European economies. The flood of precious metals from American
mines created high inflation in Spain and Portugal, raising the cost of production and thereby undermining the
manufacturing sectors in those countries. At the same time, rising prices helped traders in all parts of Europe,
laying the foundations for a middle class, especially in Northern Europe. The influx of precious metals also
provided Europe with the currency to purchase goods from China, helping to spur international trade.
Precious metals from Latin American sources—such as the Potosí silver mine in Upper Peru (now Bolivia)—
brought a windfall to sixteenth-century Spain. The new wealth increased demand for products and raised prices,
ultimately leading to economic decline. (The Granger Collection, New York)
While indigenous peoples and African slaves suffered, the Latin American economy slowly grew and diversified
through the sixteenth and seventeenth centuries, producing crops and minerals not only for export but for
domestic consumption as well. By the early 1700s, Latin Americans had built a thriving local economy based on
intercolonial trade in foodstuffs, textiles, and manufactured goods, in spite of the mercantilist policies of Spain.
Under mercantilism, colonial powers attempted to direct all benefits to themselves by restricting their colonies’
trade with other powers and by stifling the production of manufactured goods there, the latter in order to turn the
colonies into captive markets for goods from the mother country.

This mercantilist policy was largely honored in the breach until the eighteenth century, when Madrid initiated
reforms intended to direct more trade to Spain. As a result, the value of exports soared but domestic
manufacturing collapsed, as cheaper and better-made European goods flooded local markets. By the latter part of
the century, Spain had created a functioning mercantilist system, but when war came to Europe and the
transatlantic sea lanes were closed around 1800, it could not sustain it.
And just as the Latin American economy began to shrink, Spain imposed new taxes to help pay for defenses
against other European powers eyeing the rich colonies for conquest and trade. The result was widespread
dissatisfaction among colonial elites, which by the early nineteenth century had led to a wave of national
revolutions that saw Spain driven from South and Central America in the early nineteenth century, though it
retained control of its Caribbean territories. Brazil, too, achieved independence from Portugal in this period.
 From Neocolonialism to Economic Nationalism
While many of the independence leaders dreamed of a unified and free Spanish America, the region soon came
to be divided into more than a dozen small and medium-sized states. Equally disappointing for Latin Americans
was the political turmoil that enveloped most of the nations through much of the nineteenth century, until put down
by military strongmen, or caudillos, after the 1870s. The turmoil also contributed to economic stagnation through
much of the century even as the encomienda system and its postcolonial avatars discouraged the rise of a
prosperous peasantry, while the power of landowners stifled the rise of an independent merchant and
manufacturing sector.
With Europeans largely interested in their own internal development or the development of their colonies in other
parts of the world, and the United States preoccupied with settling and developing its vast internal markets, there
was little foreign capital to be invested in the region. But as Europe and the United States grew richer, there was
more excess capital to invest in the late nineteenth century; Latin America, with its vast mineral and agricultural
resources, seemed an excellent place to do so, especially after the return of domestic peace to the region in the
late nineteenth century.
What ensued was a new form of outside exploitation, one economic historians refer to as neocolonialism, whereby
political sovereignty is maintained in a country but the heights of the economy come under the control of foreign
interests, along with a small local elite. In Mexico and Central America, the dominant power in the late nineteenth
and early twentieth centuries came to be the United States, which developed sugar, rubber, and banana
plantations as well as Mexican oil fields. And while Americans made substantial investments in South America as
well, Europeans—especially the British—predominated here, developing sugar, coffee, and cotton plantations in
Brazil, mining in the Andean countries, livestock raising and meatpacking in Argentina and Uruguay, and
transportation and communications infrastructure throughout the continent.
While the economies of the region prospered and attracted millions of immigrants from Southern and Eastern
Europe, the development of more commercialized forms of plantation agriculture resulted in even more
concentrations of wealth and land in the hands of local elites, resulting in more peasants forced off their lands and
into the status of an agricultural proletariat. The state, controlled by landed elites, helped facilitate the process by
passing laws that favored the rich and by sending in the military whenever the poor rose up to challenge the
system. Meanwhile, in the growing cities of Latin America, the influx of European immigrants and capital—and the
infrastructure that capital financed—led to the development of an industrial base, particularly in the south of Brazil
and the nations of the “southern cone” of South America—Argentina, Chile, and Uruguay.
By the early years of the twentieth century, then, Latin American economies had become somewhat modernized
and integrated into the global trading network. While such integration fostered continued growth of the middle
class, it also exposed the region to the fluctuations in the global economy. With its economy built on exports, Latin
America was hard hit by the dramatic drop in world demand during the Great Depression, which especially
affected the middle class. The result was a strong shift to economic nationalism in many Latin American countries,

most of which tried to gain more control over their resources—most notably, Mexico’s nationalization of its oil
industry in the late 1930s—and to expand their industrial base to meet the domestic need for both consumer and
capital goods. World War II aided this effort, as demand for Latin American resources soared and competition from
European and U.S. manufacturers was sidelined either by the conflict itself or, in the case of the United States,
for meeting defense needs.
Much of this economic nationalism was forwarded by authoritarian regimes, such as that of Juan Perón in
Argentina, and was continued in the immediate postwar era. While much of Latin America thrived in the 1950s
and 1960s, as governments attempted to jump-start heavy industry and a booming global economy soaked up
Latin American natural resource exports, the wealth continued to accrue largely to the upper reaches of society.
This triggered a period of political turmoil that saw major guerrilla movements develop in several countries, though
most were eventually crushed by the military.
 Debt Crisis, Neoliberalism, and Beyond
While the industrialized world went into a prolonged period of stagnation in the 1970s and early 1980s, many Latin
American countries thrived. The differing fates were related, as the soaring cost of natural resources produced
both high inflation and unemployment in the West while pouring capital into resource-rich Latin American
countries. With many experts concluding that a period of sustained high prices for natural resources was part of
the world’s economic future, many of the authoritarian regimes in the region were able to borrow large sums of
capital for internal development and consumption. The result was a greatly rising debt load in many Latin
American countries that became unsustainable when resource prices fell in the early 1980s, and resulted in what
came to be known as the “debt crisis.” During this time, many countries in the region—most notably, Mexico—
teetered on the edge of bankruptcy. The crisis forced many countries to turn to loans from multilateral financial
institutions, which imposed harsh austerity measures that deeply affected workers and consumers and required
governments to curtail the economic nationalist policies of high tariffs and nationalization of industries and finance.
The new economic policies of the 1980s and 1990s went by a number of names: one was the “Washington
Consensus” (since much of it was pushed by the United States and the Washington-based multilateral financial
institutions of the International Monetary Fund and the World Bank); another was “neoliberalism.” (The latter was
used in the European sense of the term, meaning more laissez-faire–oriented economics.) Whatever the term, the
policies called for industries and banks to be privatized, subsidies on basic consumer products removed, and
tariffs designed to protect local industries eased or ended. To supporters of these policies, the process was a
painful but necessary step toward greater economic efficiency and profitability and a fuller integration of Latin
American economies into the world economy, all of which would eventually benefit Latin Americans of all classes.
To detractors, the policies allowed wealthy local elites and foreign investors to seize control of formerly publicly
held properties for a song and to repatriate billions in profits out of the country.
Whether a period of necessary adjustment or plunder, the 1980s and 1990s saw Latin America go into a period of
slow and uneven negative economic growth, increasing inequities in wealth and income, and periodic financial
crisis. The worst of these came in 1998 and 1999, in the wake of the Asian financial crisis, when panicky foreign
investors began to pull out massive amounts of capital from Latin American financial institutions and stock
markets, causing local currencies to collapse and setting off widespread recession in much of the region.
By the early 2000s, many Latin American governments had turned away from neoliberalism, either because they
believed it had failed or because they held the political costs to be too high. In some countries, such as
Venezuela, the government completely repudiated the policies, embarking instead on massive spending schemes
and subsidies, paid for by oil revenues, to bolster development and ease poverty. But in other countries, the move
away from neoliberalism was more deliberate, with the rebuilding of social welfare safety nets, targeting of key
industries for development, and attempts to build intra-regional trading networks, all the while fostering increased
economic integration with the world. Whether bolstered by these policies alone or aided by a growing world
economy and new demand for raw materials from rising powers like China, Latin American economies generally

prospered in the early and middle 2000s, with countries like Brazil and Mexico emerging onto the international
scene as rising economic powers invited into such clubs of major industrialized and industrializing countries as the
G-20.
While the Latin American financial sector was not heavily exposed to the exotic financial instruments—such as
mortgage-backed securities—that led to the crisis in the financial sector of the United States and a number of
European countries, nor did the region experience a housing bubble as inflated as that of the United States and
some other regions of the industrialized world, Latin America was hard hit by the global financial crisis of 2008–
2009 and subsequent recession nonetheless. As investors around the world panicked, they pulled out of markets
that were deemed riskier, such as those in Latin America. The result was capital flight and a major drop in
securities valuations on local stock markets. This led to drops in currency values, which increased debt loads in
several countries, though nations that relied on natural resource exports were cushioned from the impact for a
time by high prices. The global recession that followed the crisis pulled down those prices, with the result that
most of the economies of the region experienced negative growth in 2008 and part of 2009. Nevertheless,
ongoing demand by burgeoning Asian economies for Latin America’s agricultural products and raw materials, as
well as sound fiscal policies by many of the governments in the region, brought renewed growth to the continent
by late 2009 and 2010.
James Ciment
 
See also:  Argentina;  Brazil;  Central America;  Chile;  Colombia;  Emerging Markets;  Mexico; 
Tequila Effect. 
Further Reading
Chasteen, John Charles. Born in Blood and Fire: A Concise History of Latin America. New York: W.W. Norton, 2006. 
Eakin, Marshall C. The History of Latin America: Collision of Cultures. New York: Palgrave Macmillan, 2007. 
Langley, Lester D. The Americas in the Modern Age. New Haven, CT: Yale University Press, 2003. 
O’Brien, Thomas F. Making the Americas: The United States and Latin America from the Age of Revolutions to the Era of
Globalization. Albuquerque: University of New Mexico Press, 2007. 
Skidmore, Thomas E., Peter H. Smith, and James N. Green. Modern Latin America.  7th ed. New York: Oxford University
Press, 2010. 
Topik, Steven, Carlos Marichal, and Zephyr Frank, eds. From Silver to Cocaine: Latin America Commodity Chains and the
Building of the World Economy, 1500–2000. Durham, NC: Duke University Press, 2006. 
Law, John (1671–1729)
 
An economist before economics was an established discipline, Scotsman John Law wrote about and studied the
relationship between money, real goods, and wealth. Today he is best remembered for creating one of the first
financial bubbles in history, the so-called Mississippi Bubble of 1716–1720, which led to a major financial crisis in
France.
Law was born into a family of bankers on April 21, 1671, in Edinburgh, Scotland. It was assumed that he would

go into the family business, but following his father’s death in 1688, with inheritance money in his pocket, Law
chose a path that involved gambling, romance, and violence. In 1694, he killed a rival in a duel over a woman, for
which he narrowly escaped the death penalty and was fined on charges of manslaughter. He then moved to the
Netherlands, the financial center of Europe, where he saw firsthand fortunes being made (and lost) through
financial speculation.
From the Netherlands, Law moved permanently to France, where the economy was in a severe crisis due to the
cost of its role in the War of Spanish Succession. Philippe, Duke of Orleans, who was effectively in control of the
French government at the time, appointed Law controller-general of finances. Law had impressed the duke with
his argument that the only way to improve France’s economic situation was to end the private tax farms, abolish
minor monopolies, and establish a central bank to oversee government finances. He also proposed the institution
of a large state trading company to generate profits to pay off the national debt. This led to the creation, in May
1716, of the Banque Générale Privée (General Private Bank), which was financed by government-printed paper
money. Legally it was a private bank, but 75 percent of its capital came from the government in the form of bills
and notes.
The General Private Bank would prove to be Law’s undoing. In August 1717, he used the institution to fund the
so-called Mississippi Company and start a French colony in North America’s Louisiana Territory. The aim was to
establish a business venture that would rival the British East India Company by taking control of the lands around
the Mississippi, Ohio, and Missouri rivers.
The Duke of Orleans granted Compagnie d’Occident (Company of the West) a trade monopoly in North America
and the West Indies, prompting speculation in shares of the new enterprise. Law combined the Banque Royale
with the trading company and issued more and more shares to raise capital, which was then used to pay off
government debt. A significant portion of the money also found its way into the pockets of government officials,
and, some believed, Law himself. Share prices more than tripled by 1720, and the scheme finally collapsed when
investors realized that the business ventures in America were not as successful as originally thought and graft had
depleted the profits. As word spread, the bubble burst and the value of the stock plummeted to virtually nothing.
Law left France in disgrace. He received an official pardon from the British courts in 1719 and spent four years in
England before finally settling in Venice, Italy, where he died, destitute, on March 21, 1729.
Justin Corfield
 
See also:  Classical Theories and Models;  Mississippi Bubble (1717-1720). 
Further Reading
Gleeson, Janet. Millionaire: The Philanderer, Gambler, and Duelist Who Invented Modern Finance. New York: Simon &
Schuster, 2000. 
Minton, Robert. John Law, The Father of Paper Money. New York: Association, 1975. 
Montgomery Hyde, H. John Law: The History of an Honest Adventurer. London: W.H. Allen, 1969. 
Murphy, Antoin E. John Law: Economic Theorist and Policy-Maker. New York: Oxford University Press, 1997. 
Leads and Lags

 
Leads and lags refer to the expediting and delaying, respectively, of the settlement of debts in international
business trade. Specifically, the premature payment for goods purchased in another country is known as a “lead,”
while the delayed payment for such goods is known as a “lag.”
The basic concept behind leads and lags is simple. If a purchaser of goods in Country A expects the value of his
currency to rise in the coming days or weeks, he may choose to delay payment for the goods manufactured in
Country B, since such a delay will make purchasing the goods less costly. Conversely, he may choose to pay
ahead of time if he thinks Country A’s currency is going to decline in value, thereby making the cost of the goods
more expensive in the future.
Obviously, betting on leads and lags is a highly speculative activity that can hurt the purchaser financially should
currency exchange rates take an unexpected turn. For the seller, there is much less risk, since he will be paid the
agreed-upon amount in his own currency regardless of fluctuations in the exchange rate.
Whether or not a purchaser engages in lead or lag payments depends on several factors. First, it generally has to
be agreed upon by both the purchaser and the seller. Even though the risk accrues to the purchaser alone, the
seller might have cash-flow problems and need prompt payment for goods sold. Second, the use of leads and
lags is usually confined to big-ticket items—such as aircraft, expensive machine tools, and military hardware—
since only then do the savings of leads and lags become large enough to make it worth the purchaser’s while (or
offset the special fees or interest costs that a delayed payment may entail).
Leads and lags also require a fluctuating exchange-rate system. Leads and lags were relatively rare during the
period from the end of World War II through the beginning of the 1970s, when the fixed exchange rates of the
Bretton Woods international economic system were in effect. Thus, during that period, extreme fluctuations in
currency exchange rates were quite rare. With the decline of the Bretton Woods system in the 1970s, currencies
began to float in value against one another, rising and falling with increasing volatility. Under those conditions,
lead and lag payments became more common. In the decades since, technological innovations have made
exchange rate information more widely available, making it possible to calculate fluctuations more rapidly—again
contributing to an increase in lead and lag payments.
Leads and lags can affect the business cycles of a particular country in a number of ways. If too many foreign
consumers are holding back on payments to Country A, for example, then businesses in that country can suffer
financially from the disruption of cash flow. Just such a situation arises when the currency of a particular country
has been weakening, or even if it is expected to weaken. The result is a snowballing of the economic downturn. A
similar kind of vicious cycle can be caused by government action. If Country A’s economy is softening due to an
expected weakening of the currency, the government might decide to devalue the currency even further in order to
spur exports and bolster manufacturing. Such a move, however, could then encourage the consumers in other
countries to delay payments, resulting in cash-flow problems for sellers in Country A and hastening economic
contraction.
Justin Corfield and James Ciment
 
See also:  Exchange Rates. 
Further Reading
Chipman, John S. The Theory of International Trade. Cheltenham, UK: Edward Elgar, 2008. 
Kemp, Murray C. International Trade Theory. London: Routledge, 2008. 
Williams, John Burr. International Trade under Flexible Exchange Rates. Amsterdam: North-Holland, 1954. 

 
Lehman Brothers
 
As one of the oldest and largest investment banks on Wall Street—with origins dating back to before the Civil War
—Lehman Brothers’ filing for Chapter 11 bankruptcy protection was the largest bankruptcy in U.S. history and a
key triggering event for the fall 2008 meltdown in the global financial markets. It was also the largest failure of an
investment bank since the collapse of Drexel Burnham Lambert in 1990.
The firm Lehman Brothers was founded in 1850 when Mayer Lehman emigrated to the United States from Bavaria
(in present-day Germany) to join his brother Henry Lehman, who had arrived six years earlier to run a store in
Montgomery, Alabama. The two went into the business of trading in cotton and, in 1858, moved their operation to
New York. They were so prosperous that they were able to loan money to the state government of Alabama to
help pay for reconstruction.
In 1887 the firm gained a seat on the New York Stock Exchange, and from 1906 it became involved in
underwriting the flotation of companies on the stock market. Although hit by the Great Depression, Lehman
Brothers nevertheless made a small fortune from carefully investing in the venture capital market in the late
1930s. The last member of the Lehman family to control the company was Robert Lehman, who died in 1969.
Under the management of Peter Peterson from 1973, the firm grew, acquiring other investment banking
companies. They bought Kuhn, Loeb & Co. in 1977, and were soon the fourth-largest investment bank in the
United States, after Salomon Brothers, Goldman Sachs, and First Boston. Following the economic downturn of the
early 1980s, Peterson was ousted and Lewis Glucksman took over after a power struggle that began in 1983
when Glucksman was appointed as a co-CEO.
The company was not in very good financial shape after the battle, and Glucksman decided to sell it for $360
million to Shearson, an electronic transaction firm backed by American Express. In 1994, however, Lehman
Brothers Kuhn Loeb was spun off into a separate entity and finally sold publicly on the stock market as Lehman
Brothers Holdings, with Richard S. Fuld as the chairman and CEO. The company then grew rapidly. Although it
had left asset management in 1989, in 2003 it decided to reenter the field and before long it had a vast sum of
assets under management. In 2007 this generated the company $3.1 billion in net revenue, and just before it went
bankrupt, it had $275 billion in assets under management.
In 2003, the Securities and Exchange Commission became concerned about what it saw as undue influence by
the company’s investment banking section over reports put together by the firm’s research analysts. Regulators
found that the firm had published improper financial advice as had many other companies, which together paid out
$1.4 billion in fines, of which Lehman had to contribute $80 million to settle conflict-of-interest charges.
By mid-2007, economic problems resulting from the collapse of the collateralized debt obligation market had
begun to plague the company, which in response axed its subprime lender, BNC Mortgage in August, shedding

1,200 employees. At the same time, it quickly moved to reduce its exposure to the subprime market. But
problematic loan debts remained on its balance sheets and during the first part of 2008 it was clear that the
company was still heavily exposed to the subprime loan market. In early 2008, it sold $6 billion in assets, creating
losses in the second quarter of $2.8 billion. As this news went public, the stock in Lehman Brothers fell
dramatically, losing 73 percent of its value between January and June 2008. In August 2008, Lehman Brothers
initiated new cost-cutting measures, laying off 1,500 people just ahead of the deadline for reporting for the third
quarter in September. With the share slide, some companies started to consider buying Lehman Brothers. The
Korea Development Bank was seriously interested but then decided against it; the same was true for Bank of
America and Barclays Bank.
The collapse of the bank came in September 2008. On September 13, the president of the Federal Reserve Bank
of New York, Timothy F. Geithner, called a meeting with Lehman Brothers to try to work out a financial solution to
prevent bankruptcy. The efforts came to naught, and on Monday, September 15, 2008, the company sought
bankruptcy protection. At the time, it had assets of $639 billion but had incurred a bank debt of $613 billion and
bond debt of $155 billion. That same day had seen Lehman shares fall by 90 percent of their remaining value. On
the following day, Barclays PLC announced that it was prepared to buy sections of Lehman Brothers for $1.75
billion.
An employee of Lehman Brothers carries a box of his possessions from company headquarters in New York City
on September 15, 2008, the day the investment firm filed for Chapter 11 protection in bankruptcy court. (Chris
Hondros/Getty Images)
The collapse of Lehman Brothers caused share prices to plummet across the board on the New York Stock
Exchange, particularly for financial sector stocks. Some economic analysts argue that the company’s collapse was

perhaps the single most important triggering event of the financial crisis and the seizing up of the international
credit market in the early fall of 2008, itself a major contributing factor to the deep global recession of that same
year. In retrospect, say some analysts, the government’s decision not to bail out Lehman Brothers helped
precipitate the crisis, forcing a much bigger bailout of the banking industry several weeks later. Others, however,
counter that the problems in the financial sector were systemic and that Lehman Brothers’ collapse was more a
symptom than a cause of that weakness.
Justin Corfield and James Ciment
 
See also:  Banks, Investment;  Recession and Financial Crisis (2007-). 
Further Reading
Auletta, Ken. Greed and Glory on Wall Street: The Fall of the House of Lehman. New York: Random House, 1986. 
A Centennial: Lehman Brothers 1850–1950. New York: Lehman Brothers, 1950. 
McDonald, Lawrence G., with Patrick Robinson. A Colossal Failure of Common Sense: The Inside Story of the Collapse of
Lehman Brothers. New York: Crown Business, 2009. 
Lerner, Abba P. (1903–1982)
 
The Russian-American economist Abba Ptachya Lerner was one of the most influential theorists in the field during
the twentieth century. While he did not contribute directly to the literature of business cycles, his work in such
areas as general equilibrium and international trade theory influenced the thinking of several prominent economists
in the area of cycles. Although he was a socialist, Lerner championed free markets and opposed the minimum
wage and any sort of price controls.
Lerner was born in Bessarabia (then part of the Russian Empire) on October 28, 1903, and his family migrated to
London’s East End three years later. After holding a variety of jobs during the 1920s, Lerner enrolled at the
London School of Economics in 1929. He studied there under Austrian economist Friedrich Hayek, who influenced
much of his later work. Lerner distinguished himself as a student, though it took until 1944 to complete his doctoral
thesis, titled “The Economics of Control: Principles of Welfare Economics.” He also attended Cambridge University
for a period in 1936, where he was influenced by John Maynard Keynes. The following year he moved to the
United States and became something of an academic nomad, teaching economics at the University of Kansas City
(1940–1942), the New School for Social Research in New York City (1942–1946), Roosevelt University in Chicago
(1947–1959), Michigan State (1959–1965), the University of California at Berkeley (1965–1971), Queen’s College,
City University of New York (1971–1978), and Florida State University (1978 until his death on October 27, 1982).
While best remembered for developing a model of market socialism, Lerner was a master at tackling a broad
variety of economic issues and concepts with a fresh eye. He cogently revised such controversial economic
theories as Wilhelm Launhardt’s representation of the dynamics of international trade, and those of Alfred Marshall.
His work on the latter led to the Marshall-Lerner principle, which linked elasticity conditions, exchange rates, and
fluctuations in the balance of trade.
A prodigious writer, Lerner published many important papers on economic theories, even as a student. His

doctoral thesis, for example, expanded on Marshall’s work. In the 1940s and 1950s, Lerner addressed a dizzying
array of economic and social issues and ideas. Notable books include The Economics of Control (1944), The
Economics of Employment (1951), and Essays in Economic Analysis (1953). He also contributed to the Lange-
Lerner-Taylor theorem, which used a trial-and-error approach to analyze levels of public ownership and determine
levels of output and equilibrium.
Lerner’s socialist leanings were a source of opposition by the political right, including many in the field.
Nevertheless, he became a friend of Republican senator from Arizona Barry Goldwater and economist Milton
Friedman—though he and Friedman had disagreed about Lerner’s use of the “equal ignorance” assumption to
support his argument that equal distribution of income is optimal.
Justin Corfield
 
See also:  Hicks, John Richard;  Keynes, John Maynard;  Marshall, Alfred. 
Further Reading
Bronfenbrenner, Martin.  “Abba Lerner, 1903–1982.” Atlantic Economic Journal 11(March 1983). 
Lerner, Abba. Selected Writings of Abba Lerner, ed. David Colander. New York: New York University Press, 1983. 
Samuelson, Paul A.  “A.P. Lerner at Sixty.” Review of Economic Studies 31:3 (1964): 169. 
Scitovsky, Tibor.  “Lerner’s Contribution to Economics.” Journal of Economic Literature 22:4 (1984): 1547–1571. 
Leveraging and Deleveraging, Financial
 
Financial leverage is the use of borrowed money to finance an investment. The larger the proportion of funds
contributed by creditors relative to that by owners in the enterprise, the greater the financial leverage. An
investment financed with 100 percent of owners’ equity is unleveraged. Individuals, corporations, and governments
all use financial leverages when they borrow. Consequently, financial leverage is a necessary practice in modern
economies without which long-term investment would be impossible. At the same time, however, financial leverage
increases return volatilities and bankruptcy risk. It is one of the main causes of many financial crises in history.
Recent examples include the Asian currency crisis of 1997–1998 and the subprime mortgage crisis in the United
States. Financial deleverage refers to the attempt or process of reducing existing debt.
 Benefits and Costs
Financial leverage magnifies investment gains and losses. On the one hand, financial leverage increases returns
of equity. When equity holders borrow to invest, they pay fixed obligations to creditors regardless of investment
outcomes. If the return earned on an investment is greater than the cost of borrowing, the return to equity holders
will be proportionally higher after the fixed interests are paid. On the other hand, financial leverage can escalate
losses. It increases financial distress and bankruptcy risk of equity holders. When an investment is highly
leveraged, payments to creditors become burdensome. Fixed interests will be paid even if an investment is at a
loss. The more loss occurs, the more equity holders have to use their own capital to fulfill debt payments. Default
on a debt will cause bankruptcy of the borrowing parties.

Financial deleverage reduces the total amplifying effect of volatility, both its potential upside gains and downside
losses. Involuntary financial deleverage—that is, selling assets to repay debt when refinancing of debt is difficult
during financial distress—will cause asset prices to decline steeply. If such deleverage happens at a broader
economic scale, it may cause a financial crisis or deepen an existing one.
 Types
Individuals, companies, and governments all engage in financial leverage. Individuals who use borrowed funds to
invest are leveraged. Real-estate investment is a well-known example; properties are purchased with the lender’s
capital so that borrowers can use real estate at the same time it is being paid for. Another example is when
individuals engage in financial transactions using margins. They borrow cash from the counterparty to buy
securities in order to boost returns. Investors also invest in various leveraged financial instruments such as options
and futures to increase potential gains. In the United States, credit scores measure probabilities of individuals’
defaulting on their loans and thus affect individuals’ ability to borrow from financial institutions. However,
government incentive policies or fraudulent mortgage practices may cause some to neglect this risk guideline.
Companies invest in assets by either selling equity or borrowing funds. A leveraged firm takes on debt to purchase
assets while an unleveraged firm uses its equity. Leverage ratios measure a company’s debt level; the best known
are debt-to-equity ratios and debt ratios (total debt divided by total assets).
Firms use financial leverages to accelerate potential growth or returns. Certain tax policies also encourage debt
financing. Interest payments are considered expenses of conducting business and are therefore exempt from
corporate tax payments. This is similar to the situation in which individuals are encouraged to buy instead of rent
houses because mortgage interest payments are exempt from personal taxes. Companies in certain industries—
for example, utility companies or real-estate companies—borrow more than companies in other industries on
average because they have more stable earnings or can provide more collateral for higher leverages.
Companies can borrow from corporate bond markets. Corporate bonds are standard debt securities traded at the
over-the-counter market or through private placements where debt contracts are negotiated among private parties.
Standard & Poor’s, Moody’s, and Fitch are rating agencies that assign letters ranging from AAA, AA, A, BBB, BB,
B, CCC, CC, C, to D to various bonds. AAA is the highest rating. D is the lowest. Ratings of corporate bonds
indicate their levels of default risk. An investment-grade bond is rated BBB or higher. A high-risk bond, which has
a rating lower than BBB, is called a junk bond. It is also called a high-yield bond because it offers a high return to
compensate for its high risk.
Companies deleverage to reduce risks by using cash flows generated from operations or by selling off assets.
Deleverage is considered a red flag to investors, who expect growth in the companies they invest in. It can be a
warning signal of a company in financial distress.
Public sectors, such as municipal and federal governments, or government agencies, have been using financial
leverage extensively to finance roads, education, and other public projects when tax revenues are not enough to
cover expenses. A debt security issued by a city, county, state, or other local government entity is called a
“municipal bond.” Interests earned on municipal bonds are exempt from federal taxes and from most state and
local taxes as well. A debt security issued by the U.S. government to meet federal expenditures is called a
“treasury security.” Treasury securities are categorized as bills, notes, or bonds based on their length of maturity.
A national government may issue a debt security denominated in a foreign currency. This is called a “sovereign
bond.” A sovereign credit rating is the rating that indicates the risks of a country’s investing environment. Debt-to-
GDP ratio, in which total debt of a nation is divided by its gross domestic product (GDP), is commonly used in
macroeconomics to indicate the degree of a country’s ability to pay back its debt.
 Financial Leverage and the Subprime Mortgage Crisis
Just as excessive borrowing led to the Asian currency crisis of 1997–1998, financial leverage was one of the

major factors causing the subprime mortgage crisis in the United States that began in 2007. Both individual and
corporate borrowing contributed to the crisis.
First, some homebuyers put little or no down payment to purchase homes that they could not afford, expecting to
make profits when house prices continued to rise. The lower underwriting standards and nontraditional mortgages
allowed many homebuyers with below prime credit scores to purchase homes with adjustable rate mortgages that
are subject to higher interest rates and default risks later on. When house prices began to fall in mid-2006 and the
Federal Reserve set higher benchmark interest rates, short sales, foreclosures, and loan defaults became
widespread.
Second, many financial institutions in the United States were highly leveraged during the housing boom. For
example, investment banks had an average debt-to-equity ratio of 25:1, which means that for every dollar of
equity, investment banks borrowed $25 on average. Government-sponsored enterprises (GSEs) such as Fannie
Mae and Freddie Mac also had high leverages. In 2008, Fannie’s total assets to capital ratio was about 20:1, while
Freddie’s was about 70:1. GSEs could borrow at very low cost because many investors believed their debts were
guaranteed by the federal government. During the housing boom of 2002–2005, many banks, hedge funds,
insurance companies, and other financial institutions invested heavily in mortgage-backed securities with borrowed
funds, which earned them high returns but caused them to become vulnerable to a downturn in the real-estate
market.
When the house bubble began to burst in 2006, many homeowners were left in a situation where the value of
their homes was lower than the mortgage they owed. Since many loans were highly leveraged with little equity,
some homeowners simply defaulted on their loans and walked away. Failure of the mortgage market drove down
values of financial firms’ assets, which had been backed by the mortgage revenues. Companies’ equities were
quickly wiped out because the proportion of equity to debt was very low. Existing loans were unable to renew,
forcing the companies to raise additional capital from the markets. Most firms were unable to obtain external funds
in the midst of a crisis and had to sell assets to pay off debts. This involuntary financial deleverage caused steep
declines in asset prices, which deepened the crisis even further.
The negative impact of financial leverage was severe. By the end of September 2008, there were no large
investment banks left in the United States. Two of them (Morgan Stanley and Goldman Sachs) converted to bank
holding companies so they could receive federal assistance, but then were subject to more regulations. Three of
the five largest investment banks either went bankrupt (Lehman Brothers) or were bought out by other banks
(Merrill Lynch and Bear Stearns). In an attempt to ease the crisis, the U.S. Treasury took over Fannie Mae and
Freddie Mac in the same month.
 Future Possibilities
Financial leverage magnifies investment gains. It can lead to excessive risk taking. Extreme borrowing usually
occurs when there is an upward trend of asset pricing or during asset bubbles, when downside losses seem
unlikely. Since the subprime crisis, the U.S. government has been fixing and establishing laws and regulations to
restrict excessive leverages and to ensure that individuals and companies have enough equity or capital to cushion
against potential negative shocks.
Consumers and corporations have been deleveraging starting in 2007. Deleveraging will improve financial health
in the long run, but it can have detrimental effects in the short term. Household deleveraging reduces loans and
consumer spending, which hurts banks and retail businesses. Corporate deleveraging reduces investments, which
slows economic growth.
While the private sector has been deleveraging, the public sector in the United States has leveraged up. The
national debt increased dramatically in the aftermath of the subprime crisis, passing $12.3 trillion in January 2010,
or about 83 percent of GDP. A significant increase in national debt not only results in large interest payments, but
also may have adverse effects on the economy. Higher interest rates lead to the possibility of higher taxes. If the

government cannot make the payments, more borrowing has to be done. This will raise interest rates and slow the
domestic economy. Furthermore, large increases in debt are often associated with a rise in inflation and a
depreciation of the national currency. As the currency depreciates, demand for the country’s debt will decrease,
which drives up interest rates even more and causes further currency depreciation, ultimately risking a downward
spiral. Despite these possible problems, some economists maintain that the U.S. government had little choice but
to increase debt. In this view, less government spending would have worsened the recession and jeopardized the
shaky financial system.
Priscilla Liang
 
See also:  Corporate Finance;  Financial Markets;  Investment, Financial;  Mortgage,
Subprime;  Recession and Financial Crisis (2007-). 
Further Reading
Acharya, Viral, and Matthew Richardson, ed. Restoring Financial Stability: How to Repair a Failed System. Hoboken,
NJ: John Wiley and Sons, 2009. 
Antczak, Stephen, Douglas Lucas, and Frank Fabozzi. Leveraged Finance: Concepts, Methods, and Trading of High-Yield
Bonds, Loans, and Derivatives. Hoboken, NJ: John Wiley and Sons, 2009. 
Bernstein, William. The Investor’s Manifesto: Preparing for Prosperity, Armageddon, and Everything in Between. Hoboken,
NJ: John Wiley and Sons, 2009. 
Brigham, Eugene F., and Michael C. Ehrhardt. Financial Management: Theory & Practice. San Diego, CA: South-
Western, 2008. 
Neftci, Salih. Principles of Financial Engineering. San Diego, CA: Academic, 2004. 
Liberalization, Financial
 
Financial liberalization is the process of removing government regulations or restrictions on financial products,
institutions, and markets. Through the early twentieth century, there was little regulation of the financial
marketplace in the United States and most of the world. But economic downturns such as the Great Depression,
many of which were triggered by financial upheavals, convinced economists and policy makers that rules and
regulations were needed to rein in the kinds of speculative excess and malfeasance in the financial markets that
could disrupt the rest of the economy. Beginning in the 1970s, however, the new consensus was that regulation
stifled innovation and growth in financial markets, and this led to a wave of liberalization lasting into the early
twenty-first century. While it is too soon to assess the full shakeout of the financial crisis of 2008–2009, it appears
to have created a new policy consensus around the idea of re-regulation.
(A note on terminology: In a European context, “liberalization” signifies less government involvement in economic
affairs. Thus, American conservatives—known as economic “liberals” in Europe—support liberalization while
liberals, in the American sense of the term, oppose it.)
 Pre-Twentieth-Century Regulation

Rules and regulations on financial dealings are as old as trade itself, with rules on usury, or excessive interest or
even interest itself, going back to ancient times. During the Middle Ages, the Catholic Church regarded the
charging of interest as a sin and forbade church members from engaging in interest-earning money lending. While
the church gradually lifted such proscriptions in the early modern era—an early example of financial liberalization
—other faiths, most notably Islam, maintain them to the present day.
Regulation of the financial markets fluctuated in nineteenth-century America. For much of the period through the
mid-1830s, the country had a kind of central bank, known as the Bank of the United States, which acted to
stabilize U.S. currency by collecting the notes issued by commercial banks—which were widely used as currency
—and demanding payment in silver or gold. This made sure that commercial banks operated in a sound fashion,
maintaining adequate reserves of specie against the notes they issued. Such restrictions, however, were not
popular among many commercially active people outside the major cities of the East, since they saw it as
restricting expansion of the money supply and hence economic growth. Responding to such appeals, President
Andrew Jackson vetoed the re-chartering of the bank in 1832—allowing it to close down four years later. This left
the U.S. banking system largely unregulated through the early twentieth century.
Only during the Civil War did the national government attempt to regulate banks, mainly as a way to raise revenue
for the war effort. With the National Banking Act of 1864, Congress offered state banks national charters—allowing
them to issue national notes—but only if they sunk one-third of their capital into U.S. bonds. When few state
banks took up the offer—most opposed the requirement and other federal regulations—Congress passed another
law the following year, imposing a crippling tax on state bank notes, pushing many banks to re-charter as federal
banks. However, the innovation of checkable deposits at this time allowed those state banks that did not re-
charter as national banks to survive without issuing their own banknotes. Prior to checkable deposits, banks made
loans by issuing their own banknotes. With checkable deposits, state banks that received deposits of national
banknotes from other banks could hold reserve assets equal to a fraction of their deposit liabilities and make loans
by creating checkable deposits rather than issuing their own currency. This has resulted in the dual banking
system (a network of federally and state chartered banks) that exists today.
 Early Twentieth-Century Regulation
The extreme economic volatility of the late nineteenth and early twentieth centuries led to new calls for financial
regulation. In 1913, Congress responded with the Federal Reserve Act, which established the Federal Reserve
System—also known as the Fed—America’s decentralized version of European central banks. The Fed included
twelve regional banks, to ensure that the financial interests of various parts of the sprawling republic were
represented. Far more powerful than the earlier Bank of the United States, the Fed lent money to member banks
—all federally chartered banks and any state banks that wished to join—at varying interest rates, which allowed it
to control the nation’s money supply. It also regulated member banks by ensuring that they maintained adequate
reserve assets against deposit liabilities.
Still, other aspects of the banking system went unregulated. Key among these was the ability of commercial banks
to engage in investment bank activity—that is, the underwriting and trading of corporate securities, both inherently
speculative activities. While such activities proved lucrative for banks in the bull stock market years of the late
1920s, they proved disastrous in the wake of the Wall Street crash of 1929. Losses on the stock exchange
caused major commercial banks either to fail outright or cut back on the credit they offered regional and local
banks. The result was a wave of bank runs and failures at financial institutions across the United States in the
early 1930s.
The Emergency Banking Act of 1933 gave the Fed unprecedented power to certify solvent banks and reorganize
insolvent ones. Later that year, Congress passed the Glass-Steagall Act, officially the Banking Act of 1933,
prohibiting commercial banks from engaging in brokerage, investment banking, and insurance businesses. In
addition, Glass-Steagall established the Federal Deposit Insurance Corporation (FDIC), providing federal
guarantees on deposits up to a certain amount (originally $2,500, rising to $100,000 by century’s end, and

temporarily increased to $250,000 as a response to the financial crisis of 2008–2009). Together, the two
measures helped stabilize the banking system, even as they placed new restrictions on how banks could operate
and the reserves they were required to hold. With the Banking Act of 1935, the federal government extended its
control over the nation’s banking system. The legislation transferred power over interest rates from the regional
banks of the Fed to a centralized board of governors appointed by the president. In addition, the act required large
state banks to join the Fed in order to use the FDIC, subjecting them to its oversight.
 Liberalization and Deregulation in the Late Twentieth Century
The financial regulatory system established by Franklin Roosevelt’s New Deal of the 1930s held through the
1970s, assuring a stable banking system. But the galloping inflation and economic downturns of the latter decade
created a need—and political consensus—for financial liberalization. Many banks and savings and loans (S&Ls)
found themselves in a quandary as inflation rates rose above the interest rates they were permitted to charge on
loans and offer on deposits. With depositors putting their money into other financial institutions—such as the
higher-paying money market accounts being offered by brokerage houses—banks and particularly S&Ls found
themselves short of liquidity.
The liberalization of the nation’s financial system began with the passage of the Depository Institutions
Deregulation and Monetary Control Act (DIDMCA) of 1980. Its numerous provisions reflect the compromises
necessary to enact such an all-encompassing piece of legislation. As its title suggests, however, the major
provisions of interest to us can be divided into two groups:
1. Deregulation: The remaining Regulation Q ceilings (interest-rate ceilings on the interest depositors could be
paid) were phased out over a six-year period that ended in 1986. Asset and liability powers of banks and
thrifts were expanded. S&Ls and savings banks were allowed to extend loans to businesses and offer more
services to customers. All depository intermediaries were permitted to offer NOW accounts (interest-bearing
checkable deposits) to households. State usury ceilings (maximum interest rates financial institutions are
allowed to charge borrowers on certain types of loans) were suspended.
2. Monetary control: All depository institutions were subject to reserve requirements (so-called universal reserve
requirements). Reserve requirements were to be the same on particular types of deposits across institutions
(so-called uniform reserve requirements); this provision was phased in over an eight-year period that ended
in 1987.
The liberalization of the nation’s financial system continued with the S&Ls, which were granted “regulatory relief”
by the Garn–St. Germain Depository Institutions Act of 1982. Under this law, S&Ls—originally restricted largely to
home mortgages—were permitted to invest in riskier commercial real estate and businesses. With little oversight,
such financial liberalization led to excessive speculation by S&Ls and a wave of bankruptcies by the late 1980s
and early 1990s. In response, Congress created the Office of Thrift Supervision to oversee S&Ls.
Still, efforts at re-regulation paled in comparison to the continued push for financial liberalization. By the late
1990s, many large commercial banks were lobbying to end the provision of Glass-Steagall that barred commercial
banks from engaging in investment banking activity. They offered two key arguments to make their case. First,
they said, in a rapidly evolving financial marketplace, the distinctions between traditional deposits, loans, and
securities were blurring, making it difficult for regulated commercial banks to operate and costing them business,
as less regulated investment banks and brokerages took advantage of these new financial instruments. Second,
they argued, there was greater safety in diversification.
The arguments of the commercial banks won the day and, in 1999, Congress passed the Gramm-Leach-Bliley
Act, officially known as the Financial Services Modernization Act. The legislation repealed all provisions of Glass-
Steagall that prevented commercial banks from engaging in investment banking, brokerage, and insurance
activities, or from setting up bank holding companies that would allow them to do so.

 Financial Crisis of 2008–2009
While such financial liberalization led to greatly enhanced profits for bank holding companies in the early twenty-
first century, it also created new dangers and risks. As banks diversified, it became increasingly difficult for
government regulators to figure out how to impose existing rules. In other words, engaging in so many financial
activities allowed bank holding companies to choose from a host of competing agencies as to who should regulate
them. Some turned to the Office of Thrift Supervision, which had a well-deserved reputation for imposing a light
regulatory hand.
In addition, the new liberalization allowed bank holding companies to invest in a host of increasingly exotic
financial instruments, most notably mortgage-backed securities, or mortgages bundled together and then sold to
investors. Many financial institutions also began to invest in credit default swaps, essentially insurance policies on
other securities. Much of this financial innovation was rooted in the booming housing market of the early and mid-
2000s. But when housing prices collapsed in the latter years of the decade, these financial instruments lost their
value. Worse, it became difficult to assess what these different securities were even worth.
Uncertain how many so-called toxic assets were on the books of other financial institutions—and hence, how
solvent those institutions were—many banks stopped lending money to each other. Because lending between
banks is a critical activity that keeps the credit markets operating, fear spread that the loan freeze could plunge
the global economy into another Great Depression. As banks refused to lend to businesses as well, choking off
investment and payrolls, the Fed and other central banks created vast bailout funds to rescue financial institutions
of all sizes. At first, the main goal was to buy up the “toxic assets” and thereby bolster the solvency of financial
institutions. That proved unrealistic, however, as it became hard to assess what was toxic and what was not, or
how much the assets were worth. The Fed therefore shifted gears and began buying equity stakes in financial
institutions.
Such intervention in the financial marketplace was unprecedented, especially in the relatively laissez-faire financial
environment of the United States. It also raised questions about how much control the federal government—now a
partial owner of major financial institutions—should have over their operations. Hewing to the line that financial
institutions should be free of political control, the federal government took a relatively light hand, beyond restricting
the bonuses that top executives of bailed out firms could receive.
Over time, a consensus began to build for a new set of regulations and regulatory institutions, especially after the
Democratic landslide in the 2008 elections that put Barack Obama in the White House and gave the party even
greater majorities in both houses of Congress. While such reform was still a work in progress a year after the
election, two basic strands were becoming clear: a streamlining of the regulatory process that would make it
difficult for financial institutions to play off one agency against another, and new means for assessing and
responding to systemic risk such as speculative behavior by large, diversified institutions whose collapse could
bring down the global financial system.
James Ciment
 
See also:  Financial Markets;  Innovation, Financial;  Regulation, Financial. 
Further Reading
Alexander, Kern, Rahul Dhumale, and John Eatwell. Global Governance of Financial Systems: The International Regulation
of Systemic Risk. New York: Oxford University Press, 2006. 
Barth, James. The Rise and Fall of the U.S. Mortgage and Credit Markets: A Comprehensive Analysis of the Market
Meltdown. Hoboken, NJ: John Wiley and Sons, 2009. 
Krugman, Paul. The Return of Depression Economics and the Crisis of 2008. New York: W.W. Norton, 2009. 

Spencer, Peter D. The Structure and Regulation of Financial Markets. New York: Oxford University Press, 2000. 
White, Lawrence J. The S&L Debacle: Public Policy Lessons for Bank and Thrift Regulation. New York: Oxford University
Press, 1991. 
Life Insurance
 
The life insurance industry plays an important role in the life of a business cycle. Through their large-scale
investments, insurance companies have, at times, contributed to swings in the economic cycle. Moreover, when
insurance companies raise or lower premiums as a result of gains or losses on the stock market, they may spur
or decrease disposable income, employment, and, in turn, consumer spending within an economy, an important
determinant of economic growth or contraction.
 Background
Life insurance is essentially a contract between an insurer and a policyholder whereby the insurer agrees to pay a
sum of money on the occurrence of the death of the individual, or in some cases upon the individual’s sustaining
a terminal or critical injury. To achieve this, the policyholder, or another person such as a family member or an
employer, pays an amount known as a premium at regular intervals, or in some cases, a lump sum.
In some cases there are variations, which may include the funeral and burial expenses. In most cases, however,
the life insurance policy calls for the payment of a single lump sum by the insurance company if various conditions
in the policy have been met. Over time, actuaries working for insurance companies have been able to assess the
risk of death to people of particular age groups and occupations with a high degree of accuracy, as well as the
risk to people living in a specific area, people suffering from similar health complaints, or those involved in a
particular lifestyle activity such as smoking.
This has led to policyholders often filling in specific application forms in which the individual risk is assessed by
the insurer, and the premium or payout adjusted accordingly. Similarly, there are often exceptions to payout by the
insurer, such as in cases of suicide, or where a person has been taking part in a specific high-risk activity
prohibited by the policy or has provided misleading or inaccurate information.
Insurance companies assess the risk of the insured, and over time, as more and more statistics have been
collected, actuaries have been able to assess a more and more specific level of risk.
People seeking insurance often contact a range of insurance companies, and generally start paying premiums to
the company that offers the lowest premium level and/or the highest payout for their individual circumstance.
Unlike choosing other kinds of insurance, such as car insurance, where people might prefer a company that
advertises how easily it deals with problems, a person considering life insurance is not going to be the one
submitting the claim, so other factors are considered. Most life insurance companies therefore focus on their
ability to charge low premiums as their primary marketing strategy.
 Insurance Companies and Economic Cycles
The nature of the life insurance business means that companies have vast sums of capital to invest, since they
are collecting premiums well in advance of paying out benefits when a customer passes away or becomes

incapacitated. Because of this, insurance companies run large investment divisions. With their vast assets, these
companies can have a major impact on the valuation of securities and other traded assets and can, during times
of speculative excess, contribute to rapidly rising bull markets. In periods of economic boom, such as the late
1990s and the early 2000s, life insurance companies (and other insurance companies) collected premiums and
then were able to make large profits on their investments and speculation. Because of these profits, some
companies were able to lower premiums for policyholders. The lower the premiums, the more people would take
out policies with a specific company, thereby allowing the company to increase its market share.
This led to some insurance companies becoming involved in heavy speculation on the stock and commodity
markets. For a time, when returns were good, this led to the lowering of premiums paid, as noted above, a
situation welcomed—in the short term at any rate—by most consumers. However, when smaller profits were made
in the investment or speculation, or when, on occasions, losses were accrued, this caused many problems for the
insurance companies. At the depths of the bear market in late 2008, many insurance companies, including such
giants as MetLife, reported huge hits to the corporate securities part of their portfolios. Nevertheless, most major
life insurance companies maintain well-diversified portfolios overall and none came close to insolvency.
However, unlike an investment bank where people have invested sums of their own money, if a life insurance
company does close down, except for those customers who have paid their policy in terms of a lump sum (which
is unusual), the loss to the consumer, in the short term, is not as much. Yet, in the long term, this could result in
life insurance companies significantly increasing their premiums charged, and this in turn costs the consumer
much more. The increase in cost can be dramatic in the case of companies paying life insurance for their
employees, when the life insurance is essentially factored into the cost of hiring labor. A rise in life insurance
premiums translates into a rise in the cost of employment, and this in turn can effect underlying employment rates.
Justin Corfield
 
See also:  AIG;  Retirement Instruments;  Savings and Investment. 
Further Reading
Black, Kenneth, Jr., and Harold D. Skipper, Jr. Life Insurance. Englewood Cliffs, NJ: Prentice Hall, 1994. 
“Financial Crisis Poses Greatest Threat to Insurance Industry, Study Says.” Insurance Journal, June 23, 2009. 
Rotman Zelizer, Viviana A. Morals and Markets: The Development of Life Insurance in the United States. New Brunswick,
NJ: Transaction, 1983. 
 
Linens’n Things
 

Founded in 1975 and based in Clifton, New Jersey, Linens’n Things, Inc., was a chain of retail stores that sold
housewares, decorative accessories, and small appliances for the home. It quickly became popular throughout the
United States and Canada, at its peak operating 589 stores in 47 U.S. states and 6 Canadian provinces. By the
first decade of the twenty-first century, however, the company had fallen on hard times and finally went bankrupt
in 2008.
The home-products retail chain Linens’ n Things, which once operated 589 stores and employed 19,000 workers,
liquidated its remaining outlets and ceased operations amid the economic recession in late 2008. (Joe
Raedle/Getty Images)
Linens’n Things was one of the first retail companies to provide, in one store, a wide range of low-priced, high-
quality home products such as bedding, towels and other bathroom accessories, crockery and dinnerware, kitchen
appliances and accessories, home electrical items (especially fans, air conditioners, and small kitchen items),
curtains, and blinds. It also specialized in bridal registries.
In February 2006, Apollo Management, a private equity limited partnership run by Leon Black, bought Linens’n
Things for $1.3 billion, and as late as 2007 the home goods retailer was on the Forbes list as the 114th-largest
private company in the United States. At that time, Robert J. DiNicola was the company’s chief executive officer
(CEO). It employed 19,000 people, held assets valued at $1.65 billion, and earned revenue in 2005 of $2.7 billion
(an increase of 1.2 percent from the previous year). However, the company’s net profits were only $36 million, or
less than 1.4 percent of earnings—well below average for a large discount-based retail company.
The financial precariousness of Linens’n Things became evident in the middle of the new century’s first decade as
the economic downturn kicked into gear and American consumers started cutting back on buying housewares and
other company items, resulting in overstocked stores and declining sales. In March 2007, management began a
series of layoffs that would reach the tens of thousands as the situation continued to deteriorate. On May 2, 2007,
the company filed for Chapter 11 bankruptcy protection. In the petition made to a court in Delaware, the company
said that it would have to close 120 stores, with the largest number in California (27) and Michigan (10), where
the downturn in sales had been most severe. Michael Gries of the financial advisory firm Conway Del Genio Gries
was then brought in as chief restructuring officer and interim CEO, with DiNicola becoming executive chairman.
Gries closed 200 “underperforming” stores to slow financial hemorrhaging of the company’s funds and to put what
was left of the business up for sale.
The bankrupt company announced in October 2008 that it would hold a massive closeout sale of all products,
fixtures, furniture, and equipment at its remaining 371 stores. The liquidation continued on the Linens’n Things

Web site until early 2009. According to retail analysts, the company’s main downfall was that it had moved from its
original mainstream business of stocking high-quality, low-priced items into the promotion of new products and
large clearance sales. At the same time, it had started to face increased competition from similar chains, including
Bed Bath & Beyond, at a time of economic downturn.
Justin Corfield
 
See also:  Recession and Financial Crisis (2007-). 
Further Reading
Bernfield, Susan.  “Selling Out to the Bare Walls.” Businessweek, March 16, 2009, p. 42. 
Genoways, Ted.  “The Hard Sell.” Mother Jones 34:3 (May–June 2009): 71–74. 
Linens’n Things:  www.lnt.com
Walker, Rob.  “Cleaned Sheets.” New York Times Magazine, August 30, 2009, p. 18. 
Liquidity Crunch
 
In microeconomics (the economics of individuals, households, and firms), liquidity signifies the amount of cash on
hand, or the existence of assets, such as stock shares, that can easily be converted into cash in order to meet
expenses. A liquidity crunch, then, occurs when a company or household runs out of such assets or cash to meet
expenses, and can no longer secure credit. In macroeconomics (the study of the economy as a whole), a liquidity
crunch occurs when lenders in general become skittish about offering credit, choking off investment and payrolls
and contributing to an economic downturn.
Most firms need credit to function—for investment, operating expenses, and payroll—especially because they are
often awaiting payment from clients. When those payments do not come, or if a firm seeks to expand, it will turn to
lenders for credit. But if lenders perceive that a company is having trouble meeting payments, it may be more
demanding on the terms of the loan, requiring more collateral or higher interest rates to make up for the additional
risk. The additional requirements are often so burdensome that a company may choose to reorganize under
bankruptcy law rather than try to obtain credit under these conditions.
When such circumstances apply to an entire national economy, or sector of it, the ramifications of a liquidity
crunch may become dire. Large-scale liquidity crunches often occur during times of economic recession, as many
companies come to face their own shortages of cash or readily convertible assets, as customers disappear, as
clients delay payment or fail to pay altogether, and as convertible assets drop in value. When this occurs, lending
institutions often become more selective about whom they lend money to and how much they offer in credit,
limiting loans to individuals and companies with substantial secured assets and better credit ratings. This
phenomenon is known as a “flight to quality.”
The subprime mortgage crisis that began in 2006 offers an extreme example of this kind of macroeconomic
liquidity crunch. In the early part of the decade, credit was loose and mortgages were easy to obtain, fueling a
run-up in housing prices. This eased credit even further, since homebuyers automatically found themselves with a
secured asset in the form of rising equity in their property. Lenders worried less about the creditworthiness of their

borrowers—even subprime borrowers with low income or uneven credit histories—since they could seize the
property of anyone who failed to make their mortgage payments and sell the property at a profit.
But when housing prices peaked and then began to fall in 2007, the situation reversed itself. Lenders became
more hesitant to offer mortgages to riskier customers and even to more creditworthy ones. As the housing crisis
began to put a damper on consumer spending and the economy sank into recession, people began to lose their
jobs and became increasingly unable to service their mortgages. This created a liquidity crunch for some financial
institutions, forcing them to cut off credit. Even sound lenders became more wary of offering credit, not just to
homebuyers but to other individuals and firms, leading to a liquidity crunch throughout the economy. With money
becoming less available to finance mortgages, housing prices fell even further—a situation made worse by the
many foreclosed properties coming onto the market.
In addition, many financial institutions had invested in securities that were backed by mortgages. As these
securities dropped in value—or when the value of the securities became difficult to assess—banks further
tightened credit, even to each other. Lenders began to fear that borrowers had too many mortgage-backed
securities—which came to be called “toxic assets”—on their books and might be unable to pay back their loans.
The investment bank Lehman Brothers became the best-known victim of the 2007–2009 liquidity crunch,
collapsing in the late summer of 2008. Beyond the financial sector, companies found it increasingly difficult to
borrow, limiting investment and hiring. Individuals, too, found it harder to obtain credit or saw their credit card limits
shrink, dampening consumer demand.
While the liquidity crisis of the end of the century’s first decade is just beginning to be studied, economists have
long debated what triggers liquidity crises on a macroeconomic level and what measures should be taken to ease
them. Most agree, however, that liquidity crunches are an inherent part of business cycles, providing short-and
long-term corrections after periods of loose credit and rapid economic expansion.
James Ciment and Justin Corfield
 
See also:  Corporate Finance;  Credit Cycle;  Debt;  Financial Markets. 
Further Reading
Cooper, George. The Origin of Financial Crises: Central Banks, Credit Bubbles and the Efficient Market Fallacy. New
York: Vintage, 2008. 
Elliott, Larry. The Gods That Failed: How Blind Faith in Markets Has Cost Us Our Future. New York: Nation Books, 2009. 
Gold, Gerry, and Paul Feldman. A House of Cards: From Fantasy Finance to Global Crash. London: Lupus Books, 2007. 
Spotton Visano, Brenda. Financial Crises: Socio-economic Causes and Institutional Context. New York: Routledge, 2006. 
Turner, Graham. The Credit Crunch: Housing Bubbles, Globalisation and the Worldwide Economic
Crisis. London: Pluto, 2008. 
Liquidity Trap
 
Governments have a number of tools at their disposal to help the economy during recessionary periods. They can
utilize tax policy to promote investment or initiate stimulus programs to increase demand for goods and services

and put people to work. But the most readily available tool—and the one most commonly employed—is monetary
policy.
Sometimes, however, monetary policy is ineffective, and the reason may be that the economy is in a liquidity trap.
In this situation, efforts to increase the money supply have failed to encourage borrowing that could stimulate the
economy, either because interest rates are so close to zero that they cannot fall any lower, or because banks do
not want to make loans. In either case, the economy is awash in liquid assets and cash that are readily
convertible into other assets. However, the increase in liquidity does not have the expected positive impact on
lending, investment, and economic growth.
Thus, a liquidity trap can, in broad terms, represent a situation in which monetary policy is ineffective or, more
narrowly, in which interest rates are at or near zero. Technically speaking, a liquidity trap occurs when increases
in the money supply—as effected by a central bank’s buying and selling of government bonds—do not reduce the
interest rate. In other words, the central bank has expanded the money supply by providing additional cash
reserves to commercial banks, but these same institutions do not lend the money to businesses and households,
either because they are worried about the capacity of borrowers to repay or because businesses and households
evince no demand for such credit.
During extreme periods of contraction, such as the recession of 2007–2009, the U.S. Federal Reserve reduced
the nominal interest rate—that is, the interest rate not adjusted for inflation—to near zero as a way to stimulate
investment, demand, and hiring. The United States then entered a liquidity trap because the Federal Reserve
could not lower the interest rate any further even as the economy remained mired in slow or negative economic
growth.
A liquidity trap occurs because commercial banks—though they are able to borrow money more cheaply from the
central bank—remain hesitant in their own lending. They may be facing a liquidity crunch of their own due to bad
loans on their books or because they question the creditworthiness of potential borrowers, a situation particularly
acute during recessionary periods when households and businesses are experiencing higher rates of insolvency
and bankruptcy. Moreover, a liquidity trap may also coincide with deflation and an economy-wide preference for
liquidity, as households and firms choose to save the extra money in the economy rather than spend it, thereby
increasing the value of the money in circulation. Deflation can often dampen investment, since the future returns
on an investment are worth less. The Japanese economy of the late twentieth century—during the so-called lost
decade of the 1990s—provides a classic example of a liquidity trap. Fueled by favorable trade balances and a
booming economy, the Japanese went on a buying spree in the 1980s, driving up the prices of all kinds of assets
—from corporate securities to commercial real estate to golf club memberships—to unsustainable levels. When the
bubble burst in the early 1990s, financial institutions became hesitant to offer credit and the economy began to
contract. With deflation setting in, Japanese households and firms hesitated to make long-term investments,
fearing a loss in value. In addition, many banks held vast quantities of depreciated assets on their books, further
reducing their willingness to lend. The result was an extended period of stagnation, with the economy failing to
grow or diversify.
The Bank of Japan undertook the classic solution of lowering interest rates as a way of stimulating lending,
investment, demand, and hiring. But because of the many bad assets on their books—and because of fears of
deflation and continued economic stagnation—commercial banks failed to respond by making credit more
available, even when they themselves were able to borrow money at a central bank interest rate of near zero. In
addition, massive stimulus programs undertaken by the government also failed to lift the economy. Only when
commercial banks began purging their books of bad assets did they begin to lend more freely, restoring modest
growth to the nation’s economy in the early 2000s.
The Japanese experience of the 1990s weighed heavily on the thinking of economic policy makers in the United
States and other industrialized countries as they grappled with the financial crisis and recession of the late 2000s.
Many came to recognize that the unwillingness of the Japanese government and commercial banks to write off
bad loans contributed to the long period of stagnation, and that simply lowering central bank nominal interest rates

to near zero was not sufficient to lift the economy out of recession. During the financial crisis of 2008–2009, the
lessons from Japan prompted government to take more aggressive steps to deal with “troubled assets” as a way
for economies to avoid falling into the liquidity trap.
James Ciment and Justin Corfield
 
See also:  Banks, Central;  Federal Reserve System;  Interest Rates;  Monetary Policy. 
Further Reading
Sevensson, Lars E.P.  “Escaping from a Liquidity Trap and Deflation: The Foolproof Way and Others.” Journal of Economic
Perspectives 17:4 (Fall 2003): 145–166. 
Tirole, Jean. Financial Crises, Liquidity, and the International Monetary System. Princeton, NJ: Princeton University
Press, 2002. 
Wilson, Dominic. Is Shutting Krugman’s Liquidity Trap the Answer to Japan’s Problems? Canberra: Australia-Japan
Research Centre, 1999. 
Loan-to-Value Ratio
 
The loan-to-value ratio (LTV) is the ratio of the amount of a loan to the value of the asset being purchased by
that loan. Along with the creditworthiness of the borrower, the type of asset being purchased, and the overall
credit situation in the economy at large, LTV is a critical factor in the assessment lenders make in deciding
whether to offer a loan in a given situation. LTVs apply to all loan situations, whether the borrower is a firm or an
individual, or whether the loan is for a car, a house, commercial real estate, corporate securities, a business, or
business equipment.
All other factors being equal, low LTVs usually mean less risk for the lender and lower interest rates for the
borrower. To take an extreme example, an LTV of 1:10 on a $20,000 loan means that the bank is lending $20,000
against a $200,000 asset. Unless the value of the asset falls by more than 90 percent—a highly unlikely scenario
in most lending situations—the lender is unlikely to lose money on the deal, even if the borrower fails to service
the loan. That is because the lender can take possession of the asset and sell it off for more than the loan was
worth.
High LTVs reflect the creditworthiness of the borrower in several ways. First, by putting down a higher percentage
of the asset’s value in cash, the borrower has demonstrated sufficient assets to make a large down payment,
indicating a strong financial situation. For many individuals, this means having made an effort to save and good
financial habits. Moreover, a higher down payment commits the borrower to servicing the loan, since he or she
risks losing that money in the eventuality of default.
For home mortgages, the most common type of loans, LTVs have fluctuated over the years, though the general
trend has been toward higher ratios. In the United States, through the early part of the twentieth century, banks
generally required substantial down payments on a home mortgage—in the range of 50 percent or more, keeping
the LTV at 5:10 or less. This discouraged home buying and kept homeownership rates low. In the late 1930s, the
federal government established Fannie Mae to provide support for mortgage. In the post–World War II era,

generous loans to returning veterans and federal guarantees on home mortgages permitted commercial lenders,
such as banks and, increasingly, savings and loans, to offer higher LTV ratios, prompting a surge in
homeownership. Fannie Mae purchased, held, or sold to investors Federal Housing Administration (FHA) and
Veterans Administration (VA) loans that provided liquidity in the mortgage and an expansion of mortgage debt and
homeownership.
Still, through the 1970s, most lenders insisted on an LTV of no more than 8:10. In other words, they required
borrowers to put down at least 20 percent of the value of the loan. With various reforms to savings and loan (S&L)
regulations, which made it easier for the industry to provide loans not only on home mortgages but on commercial
real estate as well, the LTV climbed higher. But the S&L crisis of the late 1980s, combined with declining real-
estate values in the early 1990s, reversed that trend.
Even as the S&L crisis broke, however, innovations in the financial markets were beginning to send LTVs back up
in the 1990s and early 2000s. Specifically, financial institutions began to bundle home mortgages into financial
products that could be traded like any security. Aside from the profits to be made, the impetus for so-called
mortgage-backed securities was that they spread the risk of default from the issuing institution to investors. For
proponents of such securities, this meant that banks could offer loans to less creditworthy customers and raise
the LTV ratio, since the issuer of the mortgage was not entirely liable in case of default.
By the early 2000s, the securitization of mortgages, along with historically low interest rates from the Federal
Reserve, had led the financial industry to begin offering mortgages with LTVs of 10:10 and even higher. That is,
they began offering loans in excess of the value of the home, giving the borrower money to make improvements
or to spend the money on anything else he or she desired.
In retrospect, LTVs this high seem foolhardy. But at the time—during the housing boom of the early and mid-
2000s—lenders believed that rising house prices provided adequate security against their loans. That is, if a
borrower took out a $200,000 loan for a $180,000 house and defaulted, it did not matter because the price of the
house was likely to rise above $200,000 over a relatively short time period. Of course, when housing prices began
to fall in the late 2000s and the recession caused many people to default, many banks found themselves with
assets worth less—sometimes far less—than the loans that had been extended. Indeed, many borrowers simply
walked away from their homes when their mortgages came to exceed the value of their properties—a situation
known as being “upside down” or “underwater.”
With so many bad loans on their books, banks began to rein in their lending and to demand lower LTVs on the
mortgages they offered. By the late 2000s, it was increasingly common for banks to insist on a traditional LTV of
no more than 8:10 and sometimes, for less creditworthy customers, even lower than that. Lower LTVs put an
additional damper on home sales and contributed to falling values in a vicious cycle that devastated the housing,
real estate, and home construction industries.
James Ciment and Justin Corfield
 
See also:  Collateral;  Debt;  Mortgage Lending Standards;  Mortgage Markets and Mortgage
Rates. 
Further Reading
Goodhart, C.A.E., and Boris Hofmann. House Prices and the Macroeconomy: Implications for Banking and Price
Stability. New York: Oxford University Press, 2007. 
Shiller, Robert J. Subprime Solution: How Today’s Global Financial Crisis Happened and What to Do About It. Princeton,
NJ: Princeton University Press, 2008. 
White, Lawrence J. The S&L Debacle: Public Policy Lessons for Bank and Thrift Regulation. New York: Oxford University

Press, 1991. 
Long-Term Capital Management
 
Long-Term Capital Management (LTCM) was a hedge fund that nearly went bankrupt in 1998. Its bailout by major
U.S. banks—a bailout coordinated by the Federal Reserve (Fed)—is said, by some economists, to have
contributed to the 2008–2009 financial crisis, as many financial institutions came to believe that even if their high-
risk investment strategies should fail, they would see themselves rescued with the help of the federal government.
 LTCM’s Investment Strategies
A hedge fund is a nontraditional type of mutual fund formed as a partnership of up to ninety-nine investors.
Partners in hedge funds are wealthy individuals and institutions with significant net worth. Minimum investments
start at $1 million, and many hedge funds have much higher minimum requirements to participate. Hedge funds
attempt to earn high or maximum returns regardless of whether prices in broader financial markets are rising or
falling. The funds trade securities and other creative financial instruments and try to outperform traditional
investment funds by employing novel trading strategies. Because of their limited number and the wealth of their
participants, hedge funds are not regulated in the same way that traditional mutual funds are.
Some of the strategies employed by hedge funds include the following: selling borrowed securities (selling short)
in the hope of profiting by buying the securities at a lower price on a future date; exploiting unusual price
differences between related securities in anticipation of making a profit when the prices come into more traditional
alignment; trading options and derivatives; and borrowing to invest so that returns are increased.
One of the best-known hedge funds, and ultimately an infamous type, was Long-Term Capital Management. It
was founded in 1994 by John Meriwether, formerly of the investment bank Salomon Brothers, and Robert Merton
and Myron Scholes, who shared the 1997 Nobel Prize in economics for their work in modeling financial risk. LTCM
required investors to make a minimum investment of $10 million for three years. LTCM raised $3 billion over the
course of a few months in 1994, as investors—drawn by the company’s stellar management—clamored to be a
part of this hedge fund.
They were well rewarded for doing so. In their first year, investors made a 20 percent return, followed by 43
percent in the second year, and 41 percent in the third. LTCM achieved these spectacular returns by using trading
strategies that suggested that the prices of the different securities being invested in should be related to one
another, based on risk, maturity, and liquidity. If rates among securities got out of alignment, LTCM would, in
effect, place bets that rates would return to the traditional or historical alignment. In fact, the fund was purchasing
bonds it believed to be overpriced and selling them short (that is, borrowing and selling, then buying them back
later when, it was hoped, their price had dropped). Using such a strategy, LTCM made profits as long as the
spread between the two types of bonds narrowed, regardless of the direction in which financial prices or interest
rates moved. In addition, LTCM relied on short-term bank loans to leverage, or increase, the amount of investable
funds.
 Crisis
Although returns fell to 17 percent in 1997 (due, said the company, to increased competition as other funds copied
its strategies), the strategies worked well until late summer 1998, when Russia defaulted on its debt, throwing

global financial markets into turmoil. The default by Russia was related to the Asian financial crisis of 1997 and
caused interest rates and financial prices to move in nontraditional ways. The prices of the securities that LTCM
thought would rise relative to U.S. government securities did the opposite because of perceived increases in risk.
In short, the spread between the financial prices widened rather than narrowing as expected. There was a “flight
to quality” as funds flowed into U.S. government securities (pushing their prices up) and out of other securities
(pushing their prices down). LTCM was particularly vulnerable because it had borrowed about 50 times its capital
(investors’ funds). When prices failed to move in the expected direction, the fund’s capital base was swiftly
depleted. Losses were magnified because of the high degree of leveraging (reliance on borrowed funds). With the
value of LTCM’s securities falling, banks suggested that the fund should liquidate its positions so that the bank
loans could be repaid.
 Bailout
To ward off the fund’s certain bankruptcy, a consortium of sixteen leading U.S. banks agreed to a $3.6 billion loan
package to bail out LTCM on September 24, 1998. Thus, the securities whose prices had fallen did not have to
be sold at a loss but could be held until their prices moved into more traditional alignment. In an extremely
unusual and controversial move at the time, the Federal Reserve (Fed) arranged the bailout. LTCM had tried to
arrange a deal on its own for more than a month but had failed. The Fed brokered the deal to prevent the
liquidation of LTCM’s $200 billion in securities and avoid what it thought might be a frantic reaction in financial
markets. It was alleged that LTCM was linked to about $1.25 trillion worth of positions in global financial markets.
The liquidation of LTCM’s positions would have exacerbated the price falls and led to losses by the banks that had
loaned to the hedge fund.
There was also concern that if LTCM were forced to liquidate, a chain reaction could be set off. The dramatic
drop in the prices of the securities that LTCM was liquidating could cause the crisis to spread to other hedge
funds that employed similar strategies. In that event, other banks that had loaned to hedge funds could also
experience major losses. By 1999, LTCM was able to repay the $3.6 billion bailout by the sixteen banks. When
the loans were repaid, LTCM quietly closed.
As financial analysts note, the LTCM rescue was not exactly like the bailout of major financial institutions in 2008.
In the LTCM case, the Fed merely “arranged” the bailout, unlike the 2008 bailouts of Bear Stearns, Fannie Mae,
Freddie Mac, AIG, Citigroup, Bank of America, and many other of the nation’s largest banks, where the Fed and
Congress orchestrated and underwrote the bailouts.
The LTCM episode caused some analysts to call for more oversight and rules for the very lightly regulated hedge
fund industry. But others argued that this was not a good solution because hedge funds would merely go
“offshore” where they could escape regulation. The problem was—and continues to be—not so much that the
wealthy investors in hedge funds might lose their money but that the banks that loan to the funds to enable them
to leverage their bets would also lose, putting them and the financial system they buttress at risk. Perhaps, say
some analysts, the solution would be for banks to be more judicious in lending to hedge funds and to more fully
disclose their exposure to potential losses.
In 2009, some analysts argued that the collapse of LTCM was a harbinger of the later financial crisis of 2008–
2009, with many suggesting the company’s collapse should have been a wake-up call to the financial industry and
regulators. Others wondered if the 2008–2009 crisis could have been avoided if the Fed had not orchestrated the
1998 bailout of LTCM, causing market participants to believe that the Fed would intervene if large insitutions got
into trouble—a situation known as moral hazard. If regulators had let LTCM fail (and lived with the consequences),
or if they had recognized lapses in the financial regulatory structure, perhaps the later crisis could have been
avoided. One thing is certain: the $3.6 billion private sector bailout of LTCM, which seemed monumental at the
time, was dwarfed by the later direct public sector bailouts of the crisis of 2008–2009, which total in the hundreds
of billions of dollars.
Maureen Burton

 
See also:  Hedge Funds. 
Further Reading
Burton, Maureen, and Bruce Brown. The Financial System and the Economy.  5th ed. Armonk, NY: M.E. Sharpe, 2009. 
Burton, Maureen, Reynold Nesiba, and Bruce Brown. An Introduction to Financial Markets and Institutions.  2nd ed. Armonk,
NY: M.E. Sharpe, 2010. 
Dowd, Kevin.  “Too Big to Fail? Long-Term Capital Management and the Federal Reserve.” Cato Institute Briefing
Papers, September 23, 1999. 
Lowenstein, Roger. When Genius Failed: The Rise and Fall of Long-Term Capital Management. New York: Random
House, 2000. 
Lowe, Adolph (1893–1995)
 
German economist and sociologist Adolph Lowe was a pioneer in elucidating and analyzing the dynamics of
business cycles. He made significant contributions to understanding the ways in which socio-behavioral changes—
in addition to purely economic factors—influence the rate and direction of business cycles.
Adolph Lowe was born on March 4, 1893, in Stuttgart, Germany, where he was also raised. He studied in Munich
before becoming a student of political economist Franz Oppenheimer at the University of Berlin, where he focused
on economics and sociology. He served in the German military during World War I, after which he became a
financial adviser to the Weimar Republic.
Lowe joined the Ministry of Economics in 1922 and worked there for four years before leaving for a position at the
Institut für Weltwirtschaft (World Economic Development Institute) in Kiel. At the Institute, he researched business
cycles in collaboration with economists Fritz Burchardt, Gergard Colm, Hans Neisser, and Jacob Marschak. In
1926, he published his first major essay on business cycles, titled “Wie ist Konjunkturtheorie uberhaupt moglich?”
(How Is Business Cycle Theory Possible at All?). The essay, in opposition to the prevailing economic thinking at
the time, proved that there existed a vital, intricate connection between research on business cycles and that on
the general equilibrium economic theory. It profoundly influenced Friedrich Hayek and later members of the
Austrian school of economics and their thinking about business cycles.
In 1931, Lowe moved to the University of Frankfurt, where he was influenced by the philosophers Max
Horkheimer and Theodor W. Adorno of the Frankfurt school. Two years later, however, with the rise to power of
Adolf Hitler, Lowe was forced to leave his position because of his support for the Social Democratic Party, his
previous membership in the Socialization Committee, and his Jewish background. He fled Germany with his
family, living briefly in Geneva, Switzerland, before moving to England. There he worked at the London School of
Economics and the University of Manchester. He remained in Britain until 1940, when he was interned as an
enemy alien in spite of his opposition to Hitler. He moved to the United States at the invitation of Alvin Johnson,
where he served as the director of the Institute of World Affairs at the New School for Social Research in New
York City until 1983.
Although he was initially influenced by the work of John Maynard Keynes, at the New School Lowe remained

faithful to his research at Kiel, which had provided the underpinnings for many of his theories on the nature of
business cycles, the importance of changes in social and behavioral structures, and on pure economic forces—
and had overturned some of the fundamental thinking on orthodox economic theory. Lowe attempted to restructure
the Institute of World Affairs based on the Kiel model. Using what he termed “instrumental analysis,” he combined
research on behavioral patterns with economic analysis in his best-known book, On Economic Knowledge (1965).
This was followed by the publication in 1969 of his influential article “Toward a Science of Economics,” and, in
1976, with the publication of The Path of Economic Growth. These works did not immediately sit well with the
academic establishment in economics, which was more comfortable with mechanistic views of economic theory.
Additionally, because Lowe was a socialist and supported the use of government policy instruments to improve
the economic well-being of the country, some of his ideas were unpopular during the cold war. Although he
pointed out that the U.S. economy was already heavily regulated by such legislation as antitrust laws, it did not
stop free-market economists and politicians from criticizing him. Lowe died on June 3, 1995, in Wolfenbüttel,
Germany.
Justin Corfield
 
See also:  Behavioral Economics;  Keynesian Business Model. 
Further Reading
Hagemann, H., and H.D. Kurz.  “Balancing Freedom and Order: On Adolph Lowe’s Political Economics.” Social
Research 57(1990): 733–753. 
Lissner, Will.  “In Memoriam: Adolph Lowe 1893–1995, Economist.” American Journal of Economics and Sociology (January
1996). 
Lowe, Adolph. Essays in Political Economics: Public Control in a Democratic Society. New York: New York University
Press, 1987. 
Lowe, Adolph. On Economic Knowledge: Toward a Science of Political Economics. Armonk, NY: M.E. Sharpe, 1977. 
Lowe, Adolph. The Path of Economic Growth. Cambridge, UK: Cambridge University Press, 1976. 
Loyd, Samuel Jones (1796–1883)
 
Samuel Jones Loyd, First Baron Overstone, was a British financier, politician, and leading authority on the nation’s
banking during the mid-nineteenth century. An influential force in British government in the 1840s, he was
responsible for the introduction of the Bank Charter Act of 1844, which strengthened and extended the power of
England’s central bank. Loyd was also an effective champion for the role of a national central bank in maintaining
a healthy financial system.
Born in Lothbury, England, on September 25, 1796, Samuel Loyd was the only son of Lewis Loyd, a clergyman
turned banker, and Sarah Jones Loyd, whose father owned the Manchester bank where Lewis worked. Samuel
Loyd attended Eton College and Trinity College, Cambridge, from which he graduated in 1818. That same year he
joined Jones, Loyd & Co. A member of the Liberal party, he was elected to Parliament in 1819 and continued to
serve until 1826. He married the daughter of a Nottingham banker in 1829 and amassed a personal fortune over
the course of the next twenty years.

In December 1832, Loyd was defeated for a parliamentary seat for Manchester but remained a powerful figure in
the government because of his wealth, connections, and understanding of economic processes. A member of the
elite Political Economy Club from 1831 to 1872, Loyd was appointed an exchequer bill commissioner in 1831 and
appeared before a parliamentary committee the following year to discuss renewal of the charter of the Bank of
England. This led, eight years later, to his book Thoughts on the Separation of the Departments of the Bank of
England, which heavily influenced the passing of the Bank Charter Act of 1844.
In his book, Loyd argued that the role of the Bank of England was to provide a safe place of deposit for
government and public money and to ensure the strength of the British currency. He strongly opposed other banks
issuing their own notes, believing that the central bank’s control of the country’s money supply would prevent the
economy from entering a period of inflation or slipping into a destructive period of deflation—either of which would
lead to economic instability and contraction.
In 1850, Loyd became First Baron Overstone, with his seat at Wolvey Hall, Warwickshire; with peerage came
permanent membership in Parliament’s House of Lords. The London branch of Jones, Loyd & Co. was taken over
by the London and Westminster Bank in 1864, which left him no longer directly involved in the banking industry.
Nevertheless, Lloyd continued to influence the country’s financial policy. Through the 1860s and 1870s, the
government regularly consulted him on banking-related matters, especially during financial crises and when issues
involving monetary policy came up for consideration by Parliament.
At his death on November 17, 1883, Loyd’s personal fortune was valued at about £5.2 million, making him one of
the richest people in England. He is remembered for recognizing the importance of central banks to strong
national economies.
Justin Corfield
 
See also:  Banking School/Currency School Debate;  Banks, Central. 
Further Reading
Clapham, Sir John. The Bank of England: A History. London: Cambridge University Press, 1970. 
Loyd, Samuel Jones. Tracts and Other Publications on Metallic and Paper Currency, ed. J.R. McCulloch. Englewood Cliffs,
NJ: A.M. Kelley, 1972  (reprint of 1857).
O’Brien, D.P., ed. The Correspondence of Lord Overstone. Cambridge, UK: Cambridge University Press, 1971. 
Luminent Mortgage Capital
 
Luminent Mortgage Capital, Inc., was a real-estate investment trust with mortgages centered in San Francisco
that relied on investor money from California, and gradually elsewhere throughout the United States, to buy
mortgages and related instruments. Formed in 2003 to invest in highly rated mortgage-backed securities, Luminent
is an example of a company that was born and thrived in the real-estate boom of 2003 to 2006 and died in the
bust that followed.
A publicly traded company, Luminent took in money to fund the boom in property development in the San

Francisco Bay area. While its base of operations was located in Philadelphia, the company incorporated in
Maryland. At the height of the property boom in the new century’s first decade, it gained a triple-A rating and
enjoyed rapidly rising investments, earnings, and share prices.
With the downturn in the property market in early 2007, however, investors began to pull back their funds, causing
the flow of new capital into the company to slow. Also, fewer and fewer borrowers applied for or were granted new
loans. With a number of Luminent’s clients failing financially, it was not long before the company faced severe
financial difficulty. In the first half of 2007, the company sold only $31 million of its mortgage securities, compared
to sales of $3.1 billion the previous year. Luminent was able to buy only $1.26 billion in loans to be held for
investment in the first half of 2007, compared to $3.14 billion in the first half of the previous year.
With the advent of the full-blown subprime mortgage crisis, Luminent faced continued devaluation of its assets and
a lack of new business. Finally, when the markets opened on Monday, August 6, 2007, the company announced
that it was forced to suspend dividend payments to investors and seek fresh sources of capital to alleviate what it
hoped would be a short-term cash-flow problem.
Later in August 2007, a major attempt was made to salvage the business by offering the San Juan–based
investment company Arco Capital Corp. a majority stake in the company, including about half its stock and control
over four seats on the board, in exchange for a loan of $60 million and a sale of some of the investments at a
massively reduced price of $65 million. Arco became the major secured creditor, followed by Washington Mutual’s
WaMu Capital Corporation, which also provided funds to keep Luminent solvent. Ultimately, however, these
agreements did not bring in sufficient capital to keep the company afloat.
On September 8, 2008, Luminent Mortgage Capital filed for Chapter 11 bankruptcy protection. As of July 31,
2008, it had debts of $486.1 million and assets of $13.4 million. Its shares reached a low of 7.7 cents, and the firm
became, through restricting, a publicly traded partnership rather than a publicly traded real-estate investment trust.
Justin Corfield
 
See also:  Mortgage Markets and Mortgage Rates;  Mortgage, Subprime;  Recession and
Financial Crisis (2007-);  Shadow Banking System. 
Further Reading
Colter, Allison Bisbey.  “Pipeline.” American Banker, September 11, 2008, p. 12. 
“Problems Mount for 2 Mortgage Firms.” New York Times, September 27, 2007, p. C4. 
“Speculation on Asset Cap Boosts Freddie, Fannie.” Los Angeles Times, August 7, 2007. 
“Two U.S. Companies Involved in Mortgages Move to Raise Cash.” New York Times, August 21, 2007. 
Lundberg, Erik Filip (1907–1987)
 
Swedish economist Erik Lundberg was one of the foremost thinkers in what became known as the Stockholm
school of economics, which included such other notables as Nobel laureates Gunnar Myrdal, Bertil Ohlin, and Dag
Hammarskjöld. The Stockholm school—of which Lundberg was the last surviving member—was known for laying

the theoretical underpinnings of the northern European welfare state and, among economists, for pioneering
sequence, or process, analysis. Although Lundberg spent his entire career in Sweden and was the only member of
the group to remain in academia, his work proved influential around the world, in policy as well as in theory.
Erick Filip Lundberg was born on August 13, 1907, in Stockholm, Sweden. He earned his doctorate in economics
in 1937 and was appointed director of Sweden’s National Institute of Economic Research, a post he held for the
next eighteen years. In 1946, he accepted a chair in economics at Stockholm University, where he lectured until
1965. From the mid-1960s to his retirement in 1974, Lundberg was a professor at the Stockholm School of
Economics. In addition to his academic posts, Lundberg served as an economic adviser to a large Swedish bank
beginning in the 1950s. He also served as president of the Royal Swedish Academy of Sciences (1973–1976)
and as a member of the academy’s Economics Prize Committee, which selects the winners of the Nobel Prize in
Economic Sciences (1969–1979, chairman 1975–1979).
As a working theoretical economist, Lundberg became known for formulating models of macroeconomic
fluctuations, which helped the Swedish government, among others, devise resource allocation policies. His first
major work, Studies in the Theory of Economic Expansion (1937), appeared a year after John Maynard Keynes’s
General Theory of Employment, Interest, and Money. By the time a second edition of Studies in the Theory of
Economic Expansion was published in 1955, Lundberg had become well known and widely regarded for his
writings on business cycles and economic growth, notably Business Cycles and Economic Policy (1953) and
Instability and Economic Growth (1968).
In his research, Lundberg expanded the methods that economists used to analyze business cycles. He
incorporated a “multiplier” into his models that took into consideration the effects of changes in exports and
investment on business fluctuations. He also demonstrated how to apply price mechanisms and their time lags
when adjusting for changes in demand to an understanding of business cycles. Lundberg died in Stockholm on
September 14, 1987.
Justin Corfield
 
See also:  Stockholm School. 
Further Reading
Baumol, William J.  “Erik Lundberg 1907–1987.” Scandinavian Journal of Economics 92:1 (March 1990): 1–9. 
Laursen, Svend.  “Lundberg on Business Cycles and Public Policy.” Quarterly Journal of Economics 69:2 (May 1955): 221–
234. 
Lundberg, Erik Filip. The Development of Swedish and Keynesian Macroeconomic Theory and Its Impact on Economic
Policy. Cambridge, UK: Cambridge University Press, 1996. 
Lundberg, Erik Filip. Instability and Economic Growth. New Haven, CT: Yale University Press, 1968. 
 

Luxemburg, Rosa (1871–1919)
 
Rosa Luxemburg was a Marxist economist, philosopher, and revolutionary who, in the study of economic cycles,
became best known for developing theories of overproduction/underconsumption and the need for imperialist
capitalist economies to move production to their colonies. She also is remembered for cofounding, with Karl
Liebknecht, Die Internationale, which became the Spartakusbund (Spartacist League), and later—in collaboration
with independent socialists and the international communists of Germany—the Communist Party of Germany.
Marxist theoretician and firebrand Rosa Luxemburg addresses a 1907 gathering of the Internationale, a precursor
of the Communist Party of Germany she helped found. Steady equilibrium growth, Luxemburg argued, is not
possible in a closed capitalist economy. (ullstein bild/The Granger Collection, New York)
Luxemburg was born on March 5, 1871, in Zamość, in Russian Poland, and educated in Warsaw. At the age of
fifteen, she joined the Proletariat Party, a left-wing Polish political group, and took part in strikes that they
organized. A crackdown by Russian authorities forced her to flee to Switzerland in 1889, where she attended the
University of Zurich. There, her studies included philosophy, mathematics, and economics—with a particular
interest in stock exchange crises. In Switzerland, she met other exiled Russian socialist revolutionaries, including
Leo Jogiches, with whom, in 1893, she founded Sprawa Robotnicza (Workers’ Cause), a newspaper that opposed
the nationalist policies of the Polish Socialist Party and advocated socialist revolution in Germany, Austria, and
Russia to ensure Polish independence.
Luxemburg married the German Gustav Lübeck in 1898 and became a German citizen, settling in Berlin. As a
member of the left wing of Germany’s Social Democratic Party, she worked with Karl Kautsky to expel from the
party those who supported Eduard Bernstein’s revisionist theory, which called for trade union activity and

parliamentary politics as a means to achieve socialism. She opposed Germany’s establishment of a colonial
empire in Africa and China, arguing against its involvement in China in 1900, and against the increasingly
expensive arms race, which she believed would lead inevitably to war. At the same time, she wrote articles about
European socioeconomic issues for a variety of newspapers. Luxemburg and Jogiches were arrested in Warsaw
during the failed 1905 revolution in that city. Meanwhile, with revolutionary activism rising in less developed parts
of Europe, Luxemburg was coming to the conclusion that socialism was more likely to arise in an underdeveloped
country such as Russia than in more industrialized countries. She published The Mass Strike, the Political Party,
and the Trade Unions in 1906, in which she advocated a general workers’ strike.
Vladimir Lenin, exiled from Russia, met Luxemburg in Munich in 1907 while she was attending the Russian Social
Democrat’s Fifth Party Day. Luxemburg taught at Berlin’s Social Democratic Party school from 1907 to 1914, and
in 1913, she published The Accumulation of Capital, a book about economic imperialism.
Luxemburg became more internationalist in her thinking, working with, among others, Liebknecht, Jogiches, and
French socialist politician Jean Jaurés to unify European socialist parties and workers’ groups to stop World War I,
which she believed would be fought over imperialism and nationalism. Although she had predicted it, she was
devastated by the onset of the war, particularly when the Social Democratic Party supported Germany’s invasion
of France and French socialists supported the French war effort. In 1916, Luxemburg was imprisoned for her
efforts to organize antiwar demonstrations and a general strike and to encourage young men to refuse the draft.
During this time, she wrote The Russian Revolution (1922), a book critical of Lenin and the methods of the
Russian Bolsheviks, which she saw as a move toward dictatorship. In it, she famously declared, “Freedom is
always the freedom of the one who thinks differently.”
After Luxemburg was freed from prison in November 1918, she and Liebknecht founded the newspaper Red Flag,
which supported amnesty for political prisoners. The Spartacist League joined with other groups to become the
Communist Party of Germany, led by Luxemburg and Liebknecht, who favored violent revolution. Although
Luxemburg supported the formation of the Weimar Republic, she was outvoted, by other members of the party,
and in January 1919, the Communists tried to seize power in Berlin. Luxemburg and Liebknecht joined the
attempted revolution and were captured and killed by the anti-Communist Freikorps (Free Corps), under the
direction of Social Democratic leader Friedrich Ebert.
Justin Corfield
 
See also:  Marxist Cycle Model. 
Further Reading
Basso, Lelio. Rosa Luxemburg, A Reappraisal. London: Deutsch, 1975. 
Bronner, Stephen Eric. Rosa Luxemburg: A Revolutionary for Our Times. University Park: Pennsylvania State University
Press, 1997. 
Ettinger, Elzbieta. Rosa Luxemburg: A Life. London: Pandora, 1988. 
Hudis, Peter, and Kevin B. Anderson, eds. The Rosa Luxemburg Reader. New York: Monthly Review, 2004. 
Nettl, J.P. Rosa Luxemburg. London: Oxford University Press, 1966. 
Shepardson, Donald E. Rosa Luxemburg and the Noble Dream. New York: Peter Lang, 1996. 

 
Madoff, Bernard (1938–)
 
An influential securities industry executive on Wall Street, Bernard Madoff became a prominent symbol of
corporate corruption in December 2008, when he confessed to running the largest Ponzi scheme in history, said to
have bilked investors of more than $50 billion. For decades, Madoff had run a successful brokerage firm alongside
a secretive investment management business that billed itself as a hedge fund. On March 12, 2009, Madoff
pleaded guilty to eleven federal counts of fraud, and on June 29 of that year, he was sentenced to 150 years in
prison.
New York financier Bernard Madoff, charged with running a Ponzi scheme that cost investors an estimated $50-
$65 billion, was seen as a symbol of the greed and corruption endemic to the American financial community.
(Hiroko Masuike/Stringer/Getty Images)
Born on April 29, 1938, Madoff grew up in Laurelton, a small town in Queens, New York. He married Ruth Alpern
in 1959 and graduated from Hofstra College the following year. In 1960, he founded a broker-dealer firm, Bernard
L. Madoff Investment Securities, with a $50,000 loan from Ruth’s parents. At first working alone, and later with his
brother Peter and other family members, Madoff ran this legitimate brokerage firm until his arrest in December
2008.
Although Madoff likely will be remembered for the illegal Ponzi scheme that he operated, he was an influential
figure on Wall Street for years before he became a household name. His early adoption of computerized trading
helped his firm gain a reputation as an efficient provider of quick trades. Madoff also helped popularize the

nascent Nasdaq stock market, which was founded in 1971 as an alternative venue for stock trading.
Madoff’s promotion of the Nasdaq won him favor with the Securities and Exchange Commission (SEC). During the
1970s, he essentially was running a “third market” for stock trades. SEC regulators, attempting to crack down on
monopolies, were glad to take advantage of the competition and transparency that this new market offered. Madoff
also popularized the now-standard practice of paying customers a nominal amount in return for their business
(paying for “order flow”), served on several self-regulation boards within the securities industry, donated to both
Republican and Democratic politicians, and lobbied for restructuring of the stock market.
The early success of Madoff Investment Securities was aided by the support of Madoff’s father-in-law, Saul
Alpern. Managers at Alpern’s accounting firm acted as unlicensed money managers for their clients, sending
business to Madoff in return for commissions. From the 1960s to 1992, when the SEC intervened, workers at
Alpern’s firm generated more than half a billion dollars for Madoff Investment Securities through referrals.
These referrals also provided a steady stream of potential clients for the investment management business that
Madoff had begun running on the side. Madoff offered an improbably high rate of return on investments, but he
was extremely private about the details of his investment strategies. The secretive nature of Madoff’s operation
lent it the appeal of an elite club, and Madoff soon built a large network of satisfied and loyal clients. Meanwhile,
his brokerage business offered a convenient cover for his unregulated investment management activities. Madoff
moved money between the firms as needed, using profits from one operation to cover cash shortages in the
other.
At the start of his career, Madoff seems to have invested his clients’ money legitimately. At some point, however,
he began operating a Ponzi scheme. Madoff testified that he began the scheme in 1991; others charged that it
actually began sometime around the stock market crash of 1987, possibly earlier. Ponzi schemes use money
received from new investors to pay “returns” to current investors. This allows the operator to simulate huge
returns, when in fact those returns are just money borrowed from future investors. Ponzi schemes are difficult to
get out of once they are begun, because all investors eventually will demand to be paid. Because the operator is
promising more money than he actually has, he can never make good on all of the promises and must keep
taking in more money from new investors in order to fund payouts to current investors.
By 2008, Madoff’s Ponzi scheme was falling apart. The tight capital market, fueled by the financial crisis,
prompted many of his investors to withdraw their money, and Madoff could not keep up with the demand. In
addition, regulatory changes had reduced the profitability of brokerage activities, meaning that Madoff could no
longer use excess profits from his legitimate broker-dealer business to pay out promised returns.
On December 10, 2008, Madoff’s sons, Andrew and Mark Madoff, contacted the Federal Bureau of Investigation,
saying that Madoff had confessed to them that he had been running a Ponzi scheme for years. On March 12,
2009, Madoff pleaded guilty to eleven federal charges of fraud, and on June 29, U.S. District Court Judge Denny
Chin sentenced him to serve 150 years in prison and forfeit $170 billion in assets.
Madoff’s Ponzi scheme was unprecedented in scale. The actual dollar amount of the fraud likely will never be
known, but some estimate that Madoff went to prison owing investors more than $50 billion (including fabricated
returns). Pension funds, university endowments, charities, and individual investors lost millions to the scheme.
Exactly how Madoff managed to operate the scam on such a large scale and for so long remains unknown. It is
also unclear to what extent other executives in the firm knew about his illegal activities. Madoff’s guilty plea meant
that he was not required to cooperate with investigations into others’ involvement.
Many observers regarded the Madoff case as symptomatic of inadequate regulation in the financial sector.
Regulatory agencies failed to investigate his operations despite many red flags. For instance, many of Madoff’s
family members held key positions in Madoff Investment Securities—an unusual arrangement for a financial firm.
Madoff also reported suspiciously high returns in his investment management business, but the SEC never
scrutinized these investment activities. Beginning in 2000, former financial industry executive Harry Markopolos

repeatedly went to the SEC with accusations and evidence that Madoff was engaging in illegal activities, but his
warnings went unheeded.
For many observers, Madoff became a symbol of rampant greed and corruption on Wall Street, and his scheme
came to light in the midst of a national reassessment of the U.S. financial system. Against a backdrop of corporate
wrongdoing and crumbling financial institutions, Madoff’s actions fueled public cynicism about the financial sector
in the United States. The Madoff scandal, remarkable for its magnitude, likely will remain an enduring symbol of
the worst excesses of the financial system of the early twenty-first century.
Suzanne Julian
 
See also:  Hedge Funds;  Ponzi Scheme (1919-1920). 
Further Reading
Arvedlund, Erin. Too Good to Be True: The Rise and Fall of Bernie Madoff. New York: Portfolio, 2009. 
Kirtzman, Andrew. Betrayal: The Life and Lies of Bernie Madoff. New York: HarperCollins, 2009. 
Strober, Deborah H. Catastrophe: The Story of Bernard L. Madoff, the Man Who Swindled the World. Beverly Hills,
CA: Phoenix, 2009. 
 
Malthus, Thomas Robert (1766–1834)
 
Thomas Robert Malthus was a British demographer and economist best known for his belief that unchecked
population growth always exceeds the means of subsistence or support. This theory was important to Malthus’s
concept of economic crises within the business cycle, such as it was understood in the early nineteenth century.
He extended this theory—and his inquiries into economic processes in general—to formulate the famous theory of
classical economics, which became the basis, directly or indirectly, of later business-cycle theory.
Malthus was born on February 14, 1766, near Guildford, Surrey, England. He first was home-schooled, then
attended Jesus College, Cambridge University, graduating in 1784; he received a master’s degree from
Cambridge in 1791 and became a fellow in 1793. He was ordained in 1797 and briefly served as a country
parson. He married in 1804, and in 1805, he was named a professor of history and political economy at
Haileybury, where he remained until his death on December 29, 1834, near Bath, England.

Thomas Malthus’s theory of human demography—that population unchecked by disease, war, or famine increases
much faster than food supplies—has had an enduring influence on the study of history and economics, including
the business cycle. (The Granger Collection, New York)
 Theory of Population Growth and Economic Survival
Malthus was a product of the Age of Enlightenment, a period when intellectual pursuits and rational discourse
were encouraged as a way to unlock the secrets of nature. His views, first articulated in An Essay on the Principle
of Population (1798), contradicted those of such eighteenth-century notables as Jean-Jacques Rousseau, William
Godwin, and Benjamin Franklin, who believed that high fertility rates caused economic booms by increasing the
number of available workers. In Malthus’s view, high growth rates led to serious economic problems, as a
country’s population continued to grow at a much faster rate than its food supply. Such an imbalance in the
growth rates of population and food supply would lead to a scarcity of food, resulting in starvation, poverty,
disease, and war. He believed that the only way to avoid such terrible consequences was to slow population
growth through “moral restraint” (including late marriage and abstinence) and the use of contraception.
Malthus’s theory, advanced for its day, provoked resistance within the Church of England as well as in more
conservative sectors of society. He was reviled by many contemporaries as hard-hearted, a prophet of doom, and
an enemy of the working class. More forward-looking thinkers recognized his work as the first serious economic
study of the welfare of the lower classes. Later, many twentieth-century economists, including Julian Simon,
dismissed Malthus’s prophecies of disaster on empirical grounds. In their view, massive population growth had not
led to catastrophe in more developed economies, primarily because the industrial and technological revolutions of
the late nineteenth and early twentieth centuries had increased productivity across economic sectors, including
agriculture. However, twentieth-century social scientists researching economic development pointed to Malthus’s

theory as a good predictor of what could happen in economically underdeveloped countries.
 Trade
Malthus believed that international trade policy, especially regarding food, was necessary to delay the dire
economic and social consequences anticipated by his theory of population growth. However, his views on
international trade were inconsistent. He started out as a “free-trader,” supporting, in 1814, free trade in corn (i.e.,
grain) by eliminating tariffs because the cultivation of the British variety of corn was increasingly expensive. He
supported free-trade economists—most notably Adam Smith and David Ricardo—who believed that it was
economically beneficial for a country (Great Britain) to rely on foreign sources for food if other nations could
produce food more efficiently and cheaply.
At first, Malthus saw free trade in food as a way to alleviate the burden of feeding a rapidly growing population.
However, he changed his position in 1815 when he threw his support to the protectionists, thus acknowledging the
realities of international trade. He argued that other countries often prohibited the import of British goods, or raised
taxes on imported goods (corn) to make it too expensive to buy. If Britain were to continue accepting imports of
foreign corn from uncooperative countries, it would be at a trade disadvantage and its food supply would be held
captive to foreign politics, potentially leading to food shortages in England. Malthus encouraged the support of
domestic food production to guarantee that Britain would remain self-sustaining.
 Other Economic Inquiries
Malthus was interested in other economic issues as well. In a pamphlet published in 1800, The Present High Price
of Provisions, he advanced a theory that linked price levels with the rise and fall of an economy’s money supply,
arguing that rising prices were followed by increases in the money supply. He also investigated the causes of
fluctuations in property values (such as the price to purchase or rent land). He determined that such fluctuations
resulted from the combined factors of agricultural yields and the availability (or scarcity) of land.
Economist David Ricardo, a contemporary, incorporated Malthus’s theory of “rents” with his own theory of “profits”
to create an early version of the classical theory of economics, which linked optimal prices with supply conditions.
But Malthus was not entirely comfortable with Ricardo’s theory. In his own treatise, Principles of Economics
(1820), Malthus took an important step beyond Ricardo—and a major stride toward formulating the complete
theory of classical economics—by introducing the idea of a “demand” schedule, in which supply and demand
together determine the optimal pricing of goods.
Abhijit Roy
 
See also:  Classical Theories and Models;  Demographic Cycle. 
Further Reading
Brander, James A.  “Viewpoint: Sustainability: Malthus Revisited?” Canadian Journal of Economics 40:1 (February
2007): 1–38. 
Brown, Lester R., Gary Gardner, and Brian Halweil. Beyond Malthus: Sixteen Dimensions of the Population
Problem. Washington, DC: Worldwatch Institute, 1998. 
Dixon, Robert.  “Carlyle, Malthus and Sismondi: The Origins of Carlyle’s Dismal View of Political Economy.” History of
Economics Review 44(Summer 2006): 32–38. 
Hollander, Samuel. The Economics of Thomas Robert Malthus. Toronto: University of Toronto Press, 1997. 
Sachs, Jeffrey D.  “The Specter of Malthus Returns.” Scientific American 299:3 (September 2008): 38. 
Turner, Michael, ed. Malthus and His Time. Houndmills, UK: Macmillan, 1986. 

Manufacturing
 
In economics, manufacturing refers to the application of capital goods, labor, and resources to the production of
goods. These goods can be divided into two basic categories: consumer goods, such as appliances and clothes,
that are destined largely for consumers, and capital goods, such as machines, tools, and equipment, that are used
in the manufacture of other goods and purchased by businesses. Manufacturing represents one of the main pillars
of activity in any economy, along with agriculture, extractive industries (such as timber or mining), service
industries, and the financial sector. Manufacturing, which both affects the business cycle and is affected by it, has
undergone a long-term transformation, rising to importance in the developed, or industrialized, countries of
Europe, North America, and Japan from the late eighteenth to the mid-twentieth century, before undergoing a slow
decline in those areas and an increase in regions of the developing world, most notably East Asia.
 History
Traditionally, manufacturing was the province of skilled craftsmen, who usually worked by themselves or in small
shops, employing hand-held tools. While this model still survives in both the developed world (usually for highly
specialized and often elite goods, such as concert-quality violins or haute couture fashion) and the developing
world (for many simple and inexpensive products, such as basic clothing or utensils), it gradually has been
replaced—beginning with the first industrial revolution of the late eighteenth and early nineteenth centuries—by a
very different model of production. The industrial model of manufacturing utilizes new sources of energy (primarily
coal and oil) to run machines in order to mass-produce goods.
Besides making goods cheaper and more plentiful, industrialization deeply affected the labor force as well.
Workers no longer labored in small shops but in factories with many employees. Moreover, workers were de-
skilled after a fashion, as they no longer had to learn the complex tasks of hand manufacturing; instead, they only
needed to be trained in the relatively simple tasks associated with operating machinery and performing assembly
line work.
While the technology involved in manufacturing has undergone countless improvements and innovations over the
past 250 years, the manufacturing process itself was based on two revolutionary ideas. The first was the concept
of interchangeable parts, developed in the late eighteenth century. With components made to set specifications,
complex goods could be manufactured more cheaply, as the same kind of part would always fit in the same
finished good. The second innovation—the assembly line—came in the late nineteenth and early twentieth
centuries, beginning in the American meat-packing industry but perfected, most famously, by American auto
manufacturer Henry Ford. By breaking down the process of building highly complex products, such as
automobiles, into simple tasks and bringing the materials to the worker, rather than having the worker fetch them,
Ford dramatically reduced the amount of time—and hence labor costs—necessary for the manufacture of
automobiles, putting these once elite consumer goods into the hands of working-and middle-class purchasers.
These innovations—new forms of energy, new technologies, and new production models—could only fully be
realized by those who had—or had access to—large amounts of capital. Industrialization, then, removed the
means of production from the hands of workers and put it into the hands of those who owned the capital, that is,
the factory owners. Unable to compete—both because machine manufacturing produced goods cheaper than
workers could by hand and because ordinary workers could not afford to buy the machines—workers were forced
to sell their labor to the highest bidder. Thus, the modern wage system of employment was born. At the same

time, innovations in manufacturing steadily lowered the prices of goods, making it possible for ordinary people to
afford them, thus ushering in the modern consumer age.
 De-Industrialization
England was home to the first industrial revolution, but the technologies and methods pioneered there soon were
adopted in other parts of the world during the nineteenth century—notably in continental Europe, North America,
and Japan. In the years since World War II, however, industrialization has spread to all parts of the world, most
notably East Asia and, to a lesser extent, the more economically advanced countries of Southeast Asia and Latin
America.
This has led to a phenomenon known as de-industrialization in parts of the older industrialized countries of
Europe, North America, and Japan, where economies have become more dependent on the provision of services
rather than the production of goods. Whereas roughly one in three employed Americans worked in manufacturing
in 1950, by the first decade of the twenty-first century, that figure had declined to about one in eight. This decline
is attributable to several factors, among them new technologies that improved productivity—that is, allowed each
worker to produce more output in a set period of time—and the outsourcing of manufacturing to low-wage
countries such as Mexico, India, Southeast Asia, Taiwan, and, eventually, mainland China.
While the decline undoubtedly has hurt those areas that once were centers of manufacturing—such as the so-
called rust belt states in the Midwest and Northeast or the Midlands and north of England—economists vigorously
debate the impact of de-industrialization on the U.S. economy as a whole and whether that impact has been good
or bad. Some argue that the loss of high-paying manufacturing jobs has hollowed out the middle class and
contributed to the stagnation of real wages (i.e., wages accounting for inflation) since the 1970s, despite growth in
real gross domestic product per capita of more than 50 percent. They also point to the huge trade deficits that the
de-industrialization process has contributed to as dangerous to the country’s long-term economic health, as they
require large influxes of capital to finance, undermining the country’s ability to set its own monetary policy.
Others, however, argue that the shift of manufacturing to low-wage countries has benefited Americans in several
ways. Most importantly, they say, the shift of manufacturing to low-cost countries has brought down the price of
goods, making Americans better off in material terms, even if wages have remained stagnant. As for trade
deficits, they maintain that these have more to do with the imbalance of savings and investment at home. If
domestic capital can be utilized more profitably in other sectors, such as housing or finance, then this necessitates
borrowing from abroad. Of course, the recent shift of large sums of capital into these two sectors has been blamed
for both the housing bubble and the financial crisis of the late 2000s. Moreover, those who say that de-
industrialization is not necessarily a bad thing contend that services are an ever more critical element in advanced
economies and, here, the United States runs a significant surplus.
How an economist or economic policy maker views de-industrialization and trade often determines what he or she
thinks should be done—or not done—about them. Virtually all experts and policy makers agree that the free flow
of capital at the heart of this globalization process is, in sum, a positive force, enriching both developing and
developed economies. But those who worry about de-industrialization—particularly labor unions and union-oriented
economists (unionized workers in the private sector are highly concentrated in manufacturing)—say that the
United States should strictly enforce international wage and environmental standards, so that developing-world
countries do not have an unfair advantage over the United States.
 Business Cycle
Along with these long-term trends, manufacturing is deeply affected by the business cycle. When consumer
demand flags and business inventories build up, manufacturing output usually goes into decline, leading to
increased unemployment, which can contribute further to waning demand. In addition, because manufacturing is
so dependent on financial capital—most industries need steady flows of credit to meet payroll, purchase raw
materials, and purchase capital equipment—the tightening credit markets that often are associated with—or that

trigger—recessions can make it difficult for manufacturers to operate at full capacity or expand, leading to
reductions in investment and hiring.
For example, during the 1930s, when manufacturing represented a much greater component of the American
economy, the collapse in the credit markets and weakening consumer demand led to a massive drop in industrial
production, about 37 percent between the peak of August 1929 and the trough of March 1933. By comparison,
during the most recent recession, industrial production in the United States was off by about 15 percent between
the onset of the downturn in the fourth quarter of 2007 and its end in the second quarter of 2009.
James Ciment
 
See also:  Bethlehem Steel;  Chrysler;  Fleetwood Enterprises;  General Motors;  Industrial
Policy;  Production Cycles. 
Further Reading
Beatty, Jack, ed. Colossus: How the Corporation Changed America. New York: Broadway, 2001. 
Bluestone, Barry, and Bennett Harrison. The Deindustrialization of America: Plant Closings, Community Abandonment, and
the Dismantling of Basic Industry. New York: Basic Books, 1982. 
Kenton, Lawrence V., ed. Manufacturing Output, Productivity, and Employment Indications. New York: Novinka/Nova
Science, 2005. 
Whitford, Josh. The New Old Economy: Networks, Institutions, and Organizational Transformation of American
Manufacturing. New York: Oxford University Press, 2005. 
 
Market Trends
 
Usually referring to the valuation of corporate securities as listed on key market indices, such as the Dow Jones
Industrial Average (DJIA), market trends—that is, the direction of key market indices—fall into two basic
categories: “bull” and “bear.” A bull market is a period in which the value of corporate securities, or stocks, is
collectively trending upward; a bear market is a period in which those stocks are collectively trending downward.
The identification of a trend is based on real money terms, accounting for inflation. In other words, if stock prices
remain flat for an extended period of time, inflation is actually lowering their value, producing what is, in effect, a
bear market. Although market trends are widely reported in the media, the whys and hows of their connection to
the real economy are disputed among economists.
Since bull and bear markets concern collective market trends, it is possible that a specific stock or even an entire
stock sector, such as technology, may decline in an overall bull market or rise in an overall bear market. Even in

the strong bull markets, there may be particular companies or sectors of the economy that are suffering;
conversely, even in the bleakest bear markets, companies and sectors may be found that are doing quite well.
Moreover, a bull market or bear market does not necessarily mean that the overall price index will go up or down
every trading day—only that it is trending in one direction or the other over time.
(The origins of the terms “bull” and “bear” are obscure and disputed. “Bull” may derive from the German word
buellen, meaning “to roar.” “Bear” may have come from the pessimistic attitudes of bearskin wholesalers on the
European exchange in the eighteenth and nineteenth centuries. Others suggest that “bull” and “bear” reflect the
animals’ methods of attack—a bull thrusts upward with its horns; a bear swipes downward with its claws.)
The two great symbols of trends in the financial marketplace—bulls and bears, captured in this 1879 painting by
William H. Beard—are of uncertain origin. According to one explanation, a bull throws you up in the air and a bear
knocks you down. (The Granger Collection, New York)
 Relationship to General Economy
The relationship between market trends and the performance of the actual economy—productivity, output, growth,
employment, and other such factors—is complicated and not always synchronous. That is because market trends
are driven more by expectations of economic performance than by the real-time performance of the overall
economy. Stock market investing is, after all, a form of gambling, albeit one based on informed decision rather
than guessing. In other words, bull markets are driven by an expectation that the economy—or a sector of it, in
the case of a more selective investment—will turn around or continue to expand. In addition, market trends are
based on changes in corporate profitability that may not be indicative of the overall economy if, for example,
profits rise or fall because of changes in conditions in other countries, or because of changes in tax laws. Finally,
the market trends may not indicate overall well-being of the typical citizen if, for example, corporate profits
increase while take-home pay falls.
Thus, bull markets are often related to overall business cycles but generally not in real time. Bull markets begin
when indices begin to trend upward from a trough or a low point in overall securities valuations. Conversely, bear
markets begin when market indices begin to trend downward from a plateau or a high point in overall securities
valuations. Thus, bull markets tend to begin during periods in which the overall economy is doing badly but
investors come to believe that an economic turnaround is imminent. For example, despite mass layoffs in 2009
and anemic or negative economic growth, the Dow Jones Industrial Average (DJIA) increased by some 50 percent
between March and October. In such a scenario, investors have come to believe that a period of corporate

profitability is about to begin, even if companies are losing money at that particular time. By this reasoning, stocks
are seen as undervalued. Alternatively, bear markets begin when investors come to believe that stock prices are
overvalued and that profits are likely to decline in the future. In this respect, bull and bear markets presage
general trends in the economy. It remains true, of course, that an extended period of economic expansion is likely
to be accompanied by a bull market, and an extended period of economic contraction is likely to be accompanied
by a bear market.
 “Efficient Market Theory”
Mainstream economists invoke the concept of “efficient market theory” to explain stock prices and overall market
trends. According to this view, securities markets are highly proficient at absorbing the kinds of financial
information that affects stock prices. In other words, new information affects a given stock price or index value but
is quickly absorbed. Thus, at any given time, the stock market is likely to have acknowledged all relevant
information and factored it into the price of a given security or index. According to the efficient market theory, a
theory of market behavior dating from the 1960s, current prices include all past information so that new
information is unpredictable and arrives “unannounced,” causing prices to rise as often as it causes prices to fall.
As a result, price changes look random, resembling the pattern of somebody rambling without a specific
destination (hence the name “random walk” for this phenomenon). In other words, there is no magic formula for
beating the market. Especially in the long term, an investor can only do as well as the overall market by absorbing
and correctly analyzing relevant information. At any given time, of course, an investor can beat the market by
predicting or guessing at the random movements better than others. (This ignores the possibility of “insider
trading,” whereby an investor acts on knowledge not available to the investment community at large; such unfair
practices undermine faith in the markets and are punishable by severe criminal penalties.)
Nevertheless, economists also observe that certain anomalies undermine efficient market theory. These anomalies
derive from the fact that investors are human beings and their decisions subject to emotion, personal psychology,
or sheer impulse. It has been long noted, for example, that certain characteristics of stocks—such as high
dividends or a high earnings-to-price ratio—attract investors, pumping their perceived value beyond the level
suggested by other indicators.
 Herd Mentality
The herd instinct—another human trait—plays an important role as well. If a given investor has come to believe
that, in a bull market, others are likely to buy a particular stock, the investor is likely to pay more for the stock than
the available information indicates it is worth. In other words, all indicators may suggest that the stock is worth
$10 per share, but if an investor comes to believe—based on past performance or future expectation—that others
are willing to pay more than $10, he or she may pay $20 for the stock in the hope that its price will rise to, say,
$30.
This mentality is precisely what causes bubbles, such as the one that drove stock prices to dizzying heights in the
1920s, far above their intrinsic value. The DJIA soared 150 percent between 1925 and 1929, far in excess of the
rise in corporate earnings. The same mentality, in reverse, pertains in bear markets, where investors sell shares
below their intrinsic value because others are selling. Thus, in the aftermath of the Great Crash of 1929, the DJIA
fell by more than 80 percent—also well in excess of the decline in corporate earnings.
A more recent example of the herd mentality and market bubbles occurred in the dot.com boom and bust of the
late 1990s and early 2000s. With incessant talk of the Internet “changing business as we know it,” investors
rushed to buy shares in companies exploiting the new technology, even if those companies were losing money
and had business models that left management experts shaking their heads. High-tech stock prices soared on this
herd mentality, as investors came to believe in “first mover advantage”—that the first company to exploit an
Internet niche was likely to thrive. In other words, with investors wanting to get in on the ground floor, they flocked
to every initial public offering (IPO) of shares in a high-tech firm. Federal Reserve Board chairman Alan

Greenspan characterized the phenomenon as “irrational exuberance.” Inevitably, as in the crash of the 1930s, the
crash in Internet stocks that began in early 2000 sent the price of even tried and tested Internet companies
plummeting. The herd of investors hastened the sell-off just as aggressively as they had fueled the buy-up, fearful
that they would lose even the meager remains of their initial investment. In short, investors in bubble and bust
markets tend to base their buy and sell decisions on what other investors are doing rather than on the intrinsic
value of the stock or the market generally.
Economists also debate the cause-and-effect relationship between market trends and real economic performance
overall. In other words, they question whether market trends merely reflect overall performance or play a role in
shaping it. Most economic historians have come to the conclusion, for example, that the stock market crash of
1929 was a contributing factor to the Great Depression but not the most important one. That is because so few
people—about one in ten Americans—actually had money in the markets at the time and because other factors,
such as growing inequalities in wealth and income and lagging sectors such as agriculture, played a more
important role in the economic collapse of the 1930s. In the early 2000s, however, far more people have a
financial stake in the equities market, particularly for their retirement savings. If people come to believe that their
retirement is in jeopardy, say economists, they may curtail current spending, thereby dampening or reversing
economic growth. In such cases, market trends can affect real economic performance in a significant way.
James Ciment
 
See also:  Confidence, Consumer and Business;  Financial Markets;  Stock Markets, Global. 
Further Reading
Cassidy, John. Dot.con: The Greatest Story Ever Sold. New York: HarperCollins, 2002. 
Galbraith, John Kenneth. The Great Crash, 1929. Boston: Houghlin Mifflin, 1997. 
Gross, Daniel. Pop! Why Bubbles Are Great for the Economy. New York: Collins Business, 2007. 
Spencer, Roger W., and John H. Huston. The Federal Reserve and the Bull Markets: From Benjamin Strong to Alan
Greenspan. Lewiston, NY: Edwin Mellen, 2006. 
Marshall, Alfred (1842–1924)
 
The theories of Alfred Marshall dominated British economic thought from the late nineteenth century to the late
1930s, when the views of John Maynard Keynes gained prominence. As one of the founders of the “neoclassical
school,” Marshall made enormous contributions to the development of the field of economics, including its
establishment as a separate academic discipline.
Born in London on July 26, 1842, Marshall studied mathematics and political economy at the University of
Cambridge. He became professor of political economy at the University of Bristol in 1882, before accepting the
position of professor of political economy at Cambridge in 1885. During his time there, he published his major
work, Principles of Economics (1890), which became the leading British textbook in economics for decades; it ran
to eight editions between 1890 and 1920. Marshall was also a tutor to Keynes and active in creating a separate
degree for economics within the university. Although he retired early in 1908, his work provided the foundation for
much British economic thought throughout the 1920s and 1930s. Marshall died in Cambridge on July 13, 1924.

The centerpiece of Marshall’s work was what economists refer to as partial equilibrium analysis. His theories
focused on individual commodity markets, ignoring the influence that changes in these markets had on other
markets, and vice versa. In other words, he considered individual commodity markets in isolation.
In his research on economics, Marshall applied the methodology of a physical scientist by isolating physical
systems from external influences to better analyze selected critical variables. This allowed him to study in detail
how the economic “laws” of supply and demand worked within a particular market, and to better understand the
relationships between the two critical economic variables: price and quantity.
The law of supply states that as prices rise, producers will increase output to get more of their goods on the
market. The law of demand states that as prices fall, consumers will buy greater quantities of a good. In
microeconomic textbooks, this simplified system is represented by a graph with an upward-sloping supply curve
and a downward-sloping demand curve—the famous “scissors” model of supply and demand. The interaction of
supply and demand, therefore, determines both market prices and quantities. Market equilibrium (the point at
which producers will sell all their goods) occurs when the supply curve and the demand curve intersect (since this
is the only point at which the conditions of both laws are satisfied). Market competition (influenced by the wants
and incomes of consumers) is assumed to drive actual prices to their equilibrium price. This model is also used to
explain other economic and historical phenomena.
Another of Marshall’s contributions to economics, which grew out of his research on the laws of supply and
demand within markets, was the idea of price elasticity. Now a fundamental concept for economists, price
elasticity relates to the relative responsiveness (or sensitivity) of one variable, such as demand, to changes in
price. For example, if the price elasticity of demand for X is given as 3.1, a 1 percent increase in price will, if all
other factors remain constant, generate a 3.1 percent reduction in the quantity demanded. In this example, the
relationship is defined as elastic because the change in price has a significant effect on the quantity demanded. If,
however, the price elasticity of demand for good Y is given as 0.7, the same 1 percent increase in price will
generate only a 0.7 percent reduction in the quantity demanded. In this example, the relationship between price
and demand is “inelastic” because the change in price has a relatively minor effect on the quantity demanded.
Principles of Economics was initially intended as a two-volume work, but Marshall never completed the second
volume. However, material that was to have been included in Volume 2 found its way, in a somewhat fragmented
fashion, into his last two works, Industry and Trade (1919) and Money, Credit and Commerce (1923). Marshall’s
discussion of the business cycle, for example, was noticeably disjointed. It provided more of a description of the
sequence of a cycle than an explanation of the various forces that are at work in the cycle. In his model of the
business cycle, Marshall describes the following sequence: Rising business confidence leads to increased
borrowing from banks, and hence an increase in the price level. The growing demand for loans pushes interest
rates very high. Borrowers must sell goods in order to pay their debts. Prices fall, and business failures increase.
Nevertheless, Marshall’s contributions to economic analysis continued to influence the teaching of the subject
throughout the world well into the twentieth century.
Christopher Godden
 
See also:  Classical Theories and Models;  Hawtrey, Ralph George;  Lerner, Abba P.; 
Neoclassical Theories and Models. 
Further Reading
Groenewagen, Peter. A Soaring Eagle: Alfred Marshall, 1842–1924. Cheltenham, UK: Edward Elgar, 1995. 
Marshall, Alfred. Industry and Trade. London: Macmillan, 1919. 
Marshall, Alfred. Money, Credit and Commerce. London: Macmillan, 1923. 

Marshall, Alfred. Principles of Economics. Amherst, NY: Prometheus, 1997.  First published 1890.
Reisman, David. The Economics of Alfred Marshall. London: Macmillan, 1986. 
 
Marx, Karl (1818–1883)
 
Karl Heinrich Marx was a highly original thinker whose writings on the nature and limitations of the capitalist
system were fundamental to the rise of socialist thought and government—including the communist state—in the
twentieth century. His writings continue to provide essential insights for students of economics, philosophy, history,
politics, and law. Within the realm of economics, Marx’s analysis of the exploitation of labor in the capitalistic
system served as the basis for political theories of business cycles.

For Karl Marx, business cycles in the capitalist system are predicated on the exploitation of labor. Because
business owners profit by exploiting workers, he said, shifts in the balance between capital and labor in the
production process cause profits to rise or fall. (Time & Life Pictures/Getty Images)
Born on May 5, 1818, in Trier, Germany, Marx was educated at the University of Bonn and the University of
Berlin, where he received a doctorate in philosophy in 1841. He became the editor of a radical newspaper,
Rheinische Zeitung, which was shut down by the Prussian government in 1843 for promoting dangerous
viewpoints. Marx fled to Paris, where he became involved with a number of early socialist and communist groups.
His friendship and working association with German social scientist Friedrich Engels led to the publication of the
Communist Manifesto in 1848, a work that develops the political and governmental implications of his fundamental
ideas, calling for a workers’ revolution. After being expelled from Paris for such incendiary work, Marx and his
family settled in London in 1849.
Through his growing interest in political economy, which focuses on the role of the state in economic matters,
Marx came to the belief that in order to change the institutional framework of the capitalist system, it was first
necessary to develop a solid understanding of how capitalism functioned. After much study in the reading room at
the British Library, he produced the first volume of his famous critique of capitalism and political economy, Das
Kapital, in 1867. Two further volumes, left incomplete at the time of his death, were edited by Engels and
eventually published in 1885 and 1894.
The Marxist interpretation of history emphasizes the importance of, and the close link between, social and
economic relationships. Marx focused on the exploitation of one group by another, especially of labor by
capitalists. Within the capitalist system of commodity production and exchange, in Marx’s view, workers (the

“proletariat”) are forced to sell their labor to capitalists (the “bourgeoisie”). Given that the capitalists are the sole
owners of land and capital—the critical factors of production—workers possess no means of support other than to
offer themselves as wage labor. The large pool of readily available labor places downward pressure on wage
rates, which reduces worker income. This increases the profits of factory owners, which come through extracting
surplus value, or the value created by the worker minus the amount paid to the worker. Capitalists can also
increase surplus value by utilizing capital equipment or speeding up the rate of production, while keeping wages at
the same level. At the same time, these capitalist forces further immiserate the working classes. Marx considered
this to be exploitation of the worker, pure and simple. In his view, the capitalist system is founded at its core upon
the vulnerability of the laboring classes.
A central element of Marx’s economic philosophy was his theory of the trade cycle and economic development.
During the upswing of the cycle, he maintained, competition between entrepreneurs for labor—due to increased
economic activity—leads to sharp increases in wages. The rise in costs to capitalists causes them to modify the
production process, incorporating more capital equipment and allowing them to cut back on labor. This was
precisely the trade-off that occurred during the industrial revolution. However, Marx observed, given that capitalists
derive profit from the exploitation of labor, any change in the relative size of capital and labor in the production
process will cause the rate of profit to decline (because there would be less labor to exploit).
Because this tendency would be associated with the downswing of the cycle, it would lead to an increase in
unemployment. The resulting downward pressure on wages—given the lower demand for labor—would counteract
the tendency of profits to decline by reducing the costs of production. This, in turn, would constitute an incentive to
expand production once again and thereby trigger a new upswing of the cycle.
The repetition of these cycles, Marx argued, would cause the growing misery of the working class and the
increasing concentration of economic and political power in the hands of a few capitalists. Rising social unrest, he
argued, would eventually lead to the collapse of the capitalist mode of production. Although Marx detailed what he
believed to be the long-term “laws of motion” associated with capitalism, he provided little analysis of the
economic structure that would eventually replace it.
Despite the obvious limitations of Marx’s analysis—most noticeably his failure to anticipate the flexibility of
capitalism and the rigidity of socialist governments—his work continues to generate a vast literature, and Marxist
theory continues to provide important insights into the global nature of modern-day capitalism.
Christopher Godden
 
See also:  China;  Classical Theories and Models;  Eastern Europe;  Marxist Cycle Model; 
Russia and the Soviet Union. 
Further Reading
Berlin, Isaiah. Karl Marx: His Life and Environment. Oxford, UK: Oxford University Press, 1978. 
Marx, Karl. Das Kapital.  3 volumes. London: Penguin, 2004. 
Marx, Karl, and Frederick Engels. The Communist Manifesto. London: Longman, 2005. 
McLellen, David. Karl Marx: His Life and Thought. London: Macmillan, 1973. 
Sweezy, Paul. The Theory of Capitalist Development: Principles of Marxist Political Economy. New York: Monthly
Review, 1970. 

Marxist Cycle Model
 
Although he was trained in the classical tradition, the nineteenth-century German political economist Karl Marx,
whose ideas inspired socialist parties around the world as well as communist revolutionaries from Moscow to
Beijing to Havana in the twentieth century, offered a radical reinterpretation of the classical understanding of the
business cycle. Rather than establishing a stable equilibrium of supply and demand, as classical economic
thinkers asserted, capitalist economies in Marx’s view are characterized by periodic crises and a secular trend
toward diminishing profits and increasing exploitation of workers. A political thinker as well as an economist, Marx
argued that such tendencies would inevitably lead to revolutionary upheaval, as workers rose up and overthrew
the capitalist system, replacing it with a socialist model of economics. Marx’s most developed theory of capitalist
economic crisis exists only in sketchy form in Volume 3 of Capital, published after his death based on incomplete
manuscripts.
Marx’s life corresponded in time with the rapid expansion of the industrial revolution from its birthplace in England
—where Marx spent many years working out his theories—to continental Europe. Technological advances, he
noted, permitted capitalists (those who owned the means of production) to replace workers (those who were
forced to sell their labor) with machines. While this process allowed for increased profits at the microeconomic
level of individual firms, it came with great macroeconomic costs for society in general.
Rising profits provided the capital for further investment. But as more capitalists invested, competition rose and
profits fell, leading to periodic economic busts until new advances allowed the process to begin again. Meanwhile,
all of the capital investment in equipment put more and more workers out of a job, creating what Marx called “the
reserve army of the unemployed.” The masses of desperate people inevitably drove down wages, as workers
competed with each other for the dwindling supply of available jobs. Moreover, working conditions would become
more onerous as capitalists, desperate to preserve profits in the face of competition, accelerated the pace of work
and imposed conditions that made workplaces ever more dangerous and unpleasant.
The general trend in capitalism, then, would be toward a smaller and richer capitalist class, as those able to
survive the periodic busts in the business cycle bought up smaller firms and the capital equipment of failed firms
at fire sale prices even as they captured ever greater shares of the market. Meanwhile, an expanding working
class would become poorer and more immiserated. Impoverished laborers would no longer be able to buy the
goods produced by capitalists, leading to ever more volatile swings in the business cycle.
There was a way out of this vicious circle, argued Marx. Capitalists could invest abroad in new markets, where
profits were higher. Vladimir Lenin, Marx’s early-twentieth-century disciple and leader of the Russian Revolution of
1917, elaborated on this idea, using it to explain why great economic powers seized colonies and then exploited
their surplus value of labor and resources. But it was only a temporary solution, Lenin argued, as the same
“contradictions” of capitalism in the metropolis inevitably would spread to other exploited countries and colonies.
Ultimately, Marx argued, there was no way to save capitalism from itself. Like other economic systems before it,
he contended, capitalism contained the seeds of its own destruction. As workers became poorer, they would also
become more politicized, recognizing that their condition could only improve with the overthrow of both the
capitalist class and the capitalist system. The latter’s replacement would come with socialism, where the means of
production are owned by the workers themselves, thereby ending class tensions and the periodic crises of
capitalism.
Marx never went very far into the specifics of how the socialist system would operate, and those who sought to
put it into practice generally instituted command-style economies that failed to provide the carrots and sticks
necessary to promote economic innovation, effective management, worker productivity, responsiveness to
consumer needs and demands, or sustained economic growth. Moreover, Marx’s critique of capitalism has also

proved less than accurate; technological innovation has created unprecedented wealth, and workers, at least in
industrialized countries, have seen their standards of living generally rise and their working conditions improve
since the dark days of the industrial revolution of Marx’s time. Still, Marx’s idea that economic interests inform
people’s political and social values—a variation on the classical precept of self-interest driving economic growth—
has become a bedrock of political science and sociological thought.
James Ciment
 
See also:  China;  Classical Theories and Models;  Eastern Europe;  Marx, Karl;  Russia and the
Soviet Union. 
Further Reading
Glombowski, Jörg.  “A Marxian Model of Long Run Capitalist Development.” Journal of Economics 43:4 (December
1983): 363–382. 
Hollander, Samuel. The Economics of Karl Marx: Analysis and Application. New York: Cambridge University Press, 2008. 
Maksakovsky, Pavel V. The Capitalist Cycle, trans. Richard B. Day. Chicago: Haymarket, 2009. 
Marx, Karl. Capital: A Critique of Political Economy, trans. Ben Fowkes. New York: Vintage, 1976–1981. 
Merrill Lynch
 
Now an investment banking and wealth management division of Bank of America, Merrill Lynch was once the
largest commercial brokerage house in the world and a major independent investment bank. Heavily exposed to
mortgage-backed securities and other financial instruments whose value was undermined by the collapse in
housing prices during the late 2000s, Merrill Lynch nearly went bankrupt before being acquired by Bank of
America in September 2008, in a controversial deal brokered by Secretary of the Treasury Henry Paulson and
Federal Reserve chairman Ben Bernanke.
From its origins in the late eighteenth century through the early years of the twentieth, the stock market was
largely the domain of wealthy businessman and financiers in New York City and other financial centers in the
Northeast and Midwest. Charles Merrill, the company’s founder, and his partner Edmund Lynch, who joined a year
later, realized that the American middle class represented a major pool of potential investors in corporate and
other forms securities.
Drawing on the reserves of these modest investors, as well as major investors on Wall Street, Merrill Lynch
capitalized on the newly emerging retail chain store industry, purchasing a controlling interest in Safeway in 1926,
then a small group of local grocery stores in the West, and turning it into the nation’s third-largest supermarket
chain. Merrill Lynch prospered mightily on the deal and on gains from the stock market boom of the 1920s.
Fearing that a bubble was emerging, however, the company began to urge its clients to divest themselves of risky
investments in 1928 and shifted its own investments to low-risk financial instruments.
Despite these prudent moves, the company was hard hit by the stock market crash of 1929, and sold its retail
brokerage business in 1930 to E.A. Pierce and Company in order to concentrate on investment banking.
Throughout the 1930s and 1940s, Merrill Lynch—which joined with E.A. Pierce in 1940 and then with the New

Orleans–based commodities trading and branch stock brokerage firm of Fenner & Beane in 1941—perfected its
strategy of offering investors what it called a “department store of finance.” In other words, the firm did not just buy
and sell securities for investors, but also provided small investors with advice, education, seminars, and literature
on how to buy, hold, and sell securities wisely. In 1959, the firm—now known as Merrill, Lynch, Pierce, Fenner,
and Smith, after Beane dropped out of the firm and financier Winthrop Smith became a partner—became the first
brokerage house to incorporate, and went public (sold ownership shares to the public) in 1971.
During the 1950s and 1960s, the company prospered by establishing a network of branch offices across the
country, operated by more than 15,000 brokers. This allowed the company to place, or sell, the securities it
underwrote directly, rather than having to go through independent brokers, as was the case with most investment
banks. At the same time and into the 1970s and 1980s, the company also expanded aggressively abroad.
Its growth to become the largest investment bank in the world by the mid-1990s did not come without
embarrassments and setbacks, however. In 1986, its reputation was besmirched when one of its brokers was
arrested in a major securities fraud operation and its finances took a huge blow in the stock market crash of 1987.
On Black Monday—October 19—the company lost $377 million, largely in mortgage-backed securities, the biggest
single-day loss of any investment bank in U.S. history. In 1994, Merrill Lynch was implicated for having advised
Orange County, California, treasurer Robert Citron to invest in high-risk securities, in violation of state law. The
company ultimately settled with the county for $400 million, though it did not admit any wrongdoing, in 1998. But
all of these problems would pale in comparison to the events of 2008.
By the early twenty-first century, Merrill Lynch had become Wall Street’s number one underwriter of collateralized
debt obligations—complex financial instruments that are backed by other securities, often mortgage-backed ones.
Many of the collateralized debt obligations underwritten by Merrill Lynch were backed by subprime mortgages.
These securities offered huge potential profits, but, as is the case with all such securities, they came with high risk
—in this case, the potential that the low-income, poor-credit-history mortgage holders would default on their
loans.
As housing prices collapsed in 2007, taking the secondary mortgage market with them, the company began to
hemorrhage money. In January 2008, Merrill Lynch reported a fourth-quarter loss for 2007 of nearly $10 billion,
most of it attributable to writing down subprime mortgage-related assets. Between mid-2007 and mid-2008, the
company lost a total of nearly $20 billion, as its stock price plummeted. By late summer, financial analysts were
predicting more losses and even bankruptcy for the firm.
Meanwhile, in mid-September, other investment banks were reeling from the subprime mortgage crisis. Lehman
Brothers was forced to declare bankruptcy—the largest in U.S. history. To avoid the same fate, Merrill Lynch
opened acquisition talks with Bank of America, the nation’s largest commercial bank, with Paulson and Bernanke
acting as mediators. The two policy makers feared that the collapse of the world’s largest investment bank would
spur panic in the financial markets and plunge the global economy into depression. To use a phrase that grew in
popularity during the crisis, Merrill Lynch was an institution that was simply “too big to fail.”
On September 15, the same day Lehman Brothers declared bankruptcy, Bank of America announced that it was
acquiring Merrill Lynch in a $50 billion, all-stock deal. At the time, the merger was hailed for helping to ease the
growing panic in the global financial markets. But in subsequent months, details about the deal emerged that cast
doubt on the motives and methods of mediators Paulson and Bernanke. Both, it was alleged, had strong-armed
Bank of America’s chairman, Kenneth Lewis, into making the deal, threatening him with increased regulatory
scrutiny and action if he did not comply. Nor did Bank of America come off well. In early 2010, the Securities and
Exchange Commission (SEC) charged the bank’s executives with hiding the huge losses sustained by Merrill
Lynch in order to get shareholders to approve the merger. The case was quickly taken care of when Bank of
America agreed to pay a $150 million settlement to the SEC.
James Ciment

 
See also:  Bank of America;  Banks, Investment;  Recession and Financial Crisis (2007-). 
Further Reading
Perkins, Edwin J. Wall Street to Main Street: Charles Merrill and Middle-Class Investors. New York: Cambridge University
Press, 1999. 
Story, Louis, and Julie Creswell.  “For Bank of America and Merrill, Love Was Blind.” New York Times, February 7, 2009. 
U.S. Securities and Exchange Commission. Securities and Exchange Commission v. Bank of America Corporation,  Civil
Action no. 09-6829 (JSR) (S.D.N.Y.). Litigation Release no. 21371, January 11, 2010. 
Metzler, Lloyd Appleton (1913–1980)
 
The American economist Lloyd Appleton Metzler made notable contributions to several areas of economics but is
best remembered for his studies of international trade. Paul Samuelson called him one of the half-dozen most
important economists in the world during the 1940s and 1950s.
Metzler was born in Lost Springs, Kansas, in 1913. He attended the University of Kansas, receiving a bachelor’s
degree in 1935 and a master’s in business administration in 1938. He was awarded a PhD from Harvard
University in 1942. His doctoral thesis, “Interregional Income Generation,” dealt with the foreign-trade multiplier in
a theoretical two-country world, from a Keynesian perspective. The first chapter, which examined the stability
properties of the two countries, was published in the April 1942 issue of Econometrica; the second chapter,
exploring inter-country transfers, was published in the June 1942 issue of the Journal of Political Economy. Both
articles were well received in the academic community, and Metzler was heralded as one of the bright young
scholars in his field.
After a year in Washington as a government economist during World War II, and another year as an assistant
professor at Yale University, Metzler joined the faculty of the University of Chicago in 1947. On a faculty that
included Milton Friedman, Franco Modigliani, and Kenneth Arrow, Metzler stood as a self-described “token New
Dealer,” comfortable with Keynesian and neoclassical economics and decidedly not a member of the Chicago
school. There was no tension or friction between Metzler and his colleagues, however, and much of his work was
accessible to and respected by all, regardless of school of thought.
In 1949, Metzler articulated what came to be called the “Metzler paradox.” Although it rarely occurs in practice, he
identified the phenomenon as a potential factor in the Heckscher-Ohlin (H-O) general equilibrium model of
international trade, developed by Swedish economists Eli Heckscher and Bertil Ohlin of the Stockholm school. The
H-O model was designed to predict patterns of production and trade according to the theory of comparative
advantage. In other words, it is based on the assumption that a country will find it advantageous to export
products that involve its most abundant or least expensive resources (natural resources, domestic expertise, or
cheap labor, for example); and, conversely, that it will import products that, if produced domestically, would
consume the country’s scarcest or most expensive resources. Within this model, according to the Metzler
paradox, if the exporting country’s offer curve is sufficiently inelastic (inflexible), a tariff on imports by the importing
country can result in a reduction of the relative price. Even though the paradox is rarely if ever seen in real-world
situations, its theoretical validity has proved useful in economic analyses of international trade based on the H-O
model. Also in 1949, Metzler compiled an extensive survey of international trade theory that not only popularized

his own theories about stability in the foreign exchange market, but also influenced a generation of graduate
students who were able to benefit from access to a broad array of theories in one place.
Outside the realm of international trade theory, Metzler’s 1951 essay “Wealth, Saving, and the Rate of Interest”
addressed arguments on the “neutrality of money” and examined various monetary policies to demonstrate their
effects on interest rates and relative prices. Although this was not Metzler’s primary area of study, the essay
became one of the most important in the debate over monetarism—the school of thought, advocated by
Samuelson and others, that controlling the money supply is the most effective way of controlling short-term
demand and economic activity.
Although he continued to teach, Metzler took a hiatus from publishing after discovering he had a brain tumor upon
his return from a lecture tour of Scandinavia in 1952. When he resumed writing in the 1960s, he focused on an
earlier area of interest—mathematical economics, specifically the unification of comparative static stability and
dynamic stability analysis. The Social Science Research Council funded his work on The Mathematical Basis of
Dynamic Economics, which explored the relationship between differential and difference equations in price theory
and the analysis of business cycles. Lloyd Metzler died on October 26, 1980.
Bill Kte’pi
 
See also:  Balance of Payments;  Business Cycles, International;  Capital Account;  Current
Account;  Exchange Rates. 
Further Reading
Horwich, George, and Paul Samuelson, eds. Trade, Stability, and Macroeconomics: Essays in Honor of Lloyd A.
Metzler. New York: Academic, 1974. 
Metzler, Lloyd A. Collected Papers. Cambridge, MA: Harvard University Press, 1973. 
Tvede, Lars. Business Cycles: History, Theory, and Investment Reality. New York: John Wiley and Sons, 2006. 
Weintraub, E. Roy. How Economics Became a Mathematical Science. Durham, NC: Duke University Press, 2002. 
 
Mexico
 
The largest Spanish-speaking country in the world, with more than 110 million people, Mexico is located between
the United States to the north, Central America to the south, the Pacific Ocean to the west, and the Gulf of
Mexico and Caribbean Sea to the east. The country is highly diverse ecologically, ranging from the deserts of the
northwest to the rain forests of the south, though most of its people inhabit a belt of states across the central part
of the country.

Humans first began inhabiting what is now Mexico during the Paleolithic Period, and the country is believed to be
home to the first great civilization of the Americas, the Olmecs, beginning in the middle of the second millennium
BCE. In the early 1500s, the central part of the country was conquered by the Spanish, a process that resulted in
the loss of up to 90 percent of the indigenous people.
The colonizers imposed a feudal order known as the encomienda system, whereby Spanish landlords were given
vast land grants or control of the country’s valuable mines and then allowed to employ peasants in serf-like
conditions. Frustration with Spanish policy led to a revolt by the local elite and independence in the early 1820s,
though economic conditions changed little. Thus, political turmoil and insurrection recurred sporadically through the
early twentieth century, culminating in the Mexican Revolution (1910–1920), which resulted in widespread land
reform.
Throughout much of the twentieth century, Mexico was ruled by a single party—the Partido Revolucionario
Institucional, or Institutional Revolutionary Party (PRI), which nationalized the vital oil industry in the 1930s and
generally pursued a policy of economic autarky (self-sufficiency), with high tariffs, nationalized industries, subsidies
for basic consumer goods, and heavy government involvement in the economy.
Falling energy prices in the 1980s forced Mexico to seek international financial assistance and helped promote a
transition to more free-market, trade-oriented policies, culminating in the 1992 signing of the North American Free
Trade Agreement (NAFTA), which sought to create closer economic ties between Mexico, Canada, and the United
States. The new reforms created a more dynamic and wealthier economy, but one that saw increased divisions
between rich and poor. More integrated into the world economy, Mexico was hard hit by the 2007–2009 financial
crisis and recession, which saw demand for Mexican exports contract in their biggest market, the United States.
 Economic History Through World War II
Mexico was home to some of the most vibrant civilizations in the pre-Columbian history of the Americas and, at
the time of first contact with Europeans in the late 1400s and early 1500s, had two major centers of urban
civilization: that of the Aztecs in the Valley of Mexico in the central part of the country, and that of the Maya in the
Yucatan Peninsula region in the south. Both controlled widespread trading networks that linked them to regions as
far away as what is now the southwestern United States and Central America.
In 1519, the first Spanish conquistadores arrived in the Valley of Mexico. With the help of superior weapons, the
pathogens they carried, and indigenous allies—who resented harsh Aztec rule—the Spanish quickly defeated the
Aztecs and established dominion over the region. While Spanish authorities quickly outlawed slavery of indigenous
peoples, the encomienda system imposed working conditions not unlike those of slavery. Under the Spanish labor
system, the peasantry could not leave their employers, who had virtually total control over their lives both on and
off the job. Many landowners and mine owners worked these quasi serfs to death with long hours and harsh
conditions. In the century following the conquest, it is estimated that the indigenous population of the Valley of
Mexico fell from about 25 million to 1 million.
Initially, the Mexican economy under the Spanish was based chiefly on the export of precious metals, particularly
silver. Gradually, commercial agriculture—including sugar plantations along the Gulf of Mexico and livestock
raising in the Valley of Mexico—became important components. What barely changed was the social order, in
which a handful of very powerful ethnic Spanish ranchers and mine owners ruled over a vast peasantry and
mining proletariat consisting primarily of mestizos, or people of mixed Spanish and indigenous heritage.
Spain was a relatively weak colonial power and, for much of the period in which it ran what was then called New
Spain, it imposed little control over the economic and political life of the colony. To counter the rising power of
Britain and France, and the trade inroads those countries were making in its American colonies, Spain in the mid-
eighteenth century began to impose tighter mercantilist rule, attempting to direct more of the colonies’ trade to the
mother country. But when the French Revolution and the Napoleonic wars of the late eighteenth and early
nineteenth centuries once again weakened Madrid’s control, local elites—many inspired by the revolutionary

rhetoric and ideas of France—broke free and established an independent Mexico in 1821.
For the first half-century of independence, Mexico’s economy remained rooted in large-scale agriculture and
mining, with modernization stunted by political turmoil. Under dictator Porfirio Díaz, who ruled the country from
1876 to 1911, Mexico embarked on a modernization program that included high tariffs to protect local industries
but also increased foreign investment, particularly from the United States, to modernize mining and commercial
agriculture industries and build the beginnings of a railroad and communications network. The policies worked, but
created great discrepancies in wealth and did little to address the most contentious of domestic economic issues
—landlessness among the peasantry. The Mexican Revolution that ensued in the second decade of the twentieth
century saw Díaz overthrown and a massive redistribution of land.
Renewed unrest developed as a result of the Great Depression, during which the Mexican economy, increasingly
dominated by the United States, was dealt a heavy blow from the downturn north of the border. Thousands of
Mexican workers who had migrated to the United States during the prosperous years of World War I and the
1920s were forced to return home, as unemployment rose and per capita income fell. The result was a political
turn to the Left under the PRI and the initiation of more autarkic economic policies, including nationalization of the
oil industry in 1938 and an emphasis on import-substitution industrial development.
 Oil Boom and Bust
The policies initiated in the 1930s paid dividends in the immediate post–World War II era, with the government
using rising oil revenues to expand education and build a heavy industry infrastructure. Still, by the 1960s, those
revenues were shrinking as existing oil fields went into decline. By 1966, Petróleos Mexicanos (Mexican
Petroleum, or Pemex) was barely producing enough to meet domestic demand.
Between 1971 and 1973, however, oil and natural gas were discovered in the areas near Tampico, Reynosa, and
Poza Rica, and the first discoveries were made in the states of Tabasco and Chiapas. These areas, together with
the Cantarell field discovered in 1976 near Campeche in the Gulf of Mexico, have become the country’s highest
crude producers. Since 1978, this region has provided 79 percent of Mexico’s total production and is the principal
source of the nation’s hydrocarbon reserves.
In the mid-1970s, Pemex undertook an aggressive strategy to accelerate the export of crude oil and bring in more
foreign currency. In this way, the government hoped to finance new industrial and commercial development. The
decision would prove disastrous. Throughout the world, the early 1970s was characterized by extreme volatility in
both finance and production. The decision of U.S. president Richard Nixon to end the Bretton Woods monetary
system in 1971, precipitated in large part by a growing tendency toward the deregulation of global finance, threw a
relatively orderly international system of trade and finance into disarray. That situation was exacerbated by the oil
embargo of 1973. World oil prices skyrocketed in deregulated markets, and the financial windfall of oil exporters
flooded the eurodollar market. U.S. and European international banks, seeking to recycle so-called petrodollars,
looked to Latin American countries as potentially lucrative new markets.
With international funding, Mexico experienced a second oil boom from 1974 to 1982. Crude exports during this
period jumped from 5.8 million barrels per day to 544 million per day. Once again, the movement of international
markets determined the fate of the Mexican economy. As the effects of the 1979 energy crisis began to recede
and oil prices began to decline, the U.S. economy continued to suffer from stagflation—sluggish economic growth
along with rising inflation. In response, the U.S. Federal Reserve raised interest rates to unprecedented levels—
over 19 percent by July 1981. Faced with lower oil prices and drastically higher interest rates on international
loans, Mexico declared a moratorium on its loan payments. Other Latin American nations quickly followed suit, and
the 1982 debt crisis was born, with profoundly negative consequences for the region.
Increased exports during the boom were not matched by equally ambitious goals in domestic refining and the
production of petroleum-based products. In 1974, almost 800,000 barrels were refined daily, and the construction
of new refineries pushed refining capacity to 1.5 million barrels per day in 1981. This relatively modest increase

ended with the debt crisis, and no new capacity has been added in the years since. Likewise, petrochemical
production reached its highest point in 1982—14.5 million tons (13.2 metric tons)—and has held constant at that
level ever since.
With the collapse in oil prices in the early 1980s, however, Mexico experienced a growing balance-of-payments
shortfall, resulting in massive capital flight abroad. By 1982, the country had virtually depleted its foreign-capital
reserves in an effort to shore up the peso and to prevent rapid inflation. Fears that the government could not pay
its debts and would have to raise taxes only exacerbated the problem, forcing the government to devalue the peso
several times. These moves resulted in inflation rates approaching 100 percent annually in the middle and latter
years of the decade. Both domestic and foreign investment faltered, and the economy went into a period of
stagnation that Mexicans call the “lost decade.”
The Mexican oil industry, vital to the country’s economy, was nationalized in 1938. Through the state-owned
company Pemex, petroleum accounts for about one-third of government revenues. Declining capacity has been a
problem in the twenty-first century. (Steve Northup/Time & Life Pictures/Getty Images)
 Market Reforms, Foreign Investment, and the Crisis of 1994
To improve the economy’s performance and comply with the strictures of international lending institutions, which
the country’s finances had come to rely on, Mexico embarked on a program of pro-market reforms, including
privatization of state enterprises—though never in the politically sensitive oil industry—and deregulation. The
government also established a policy of encouraging U.S. corporations to establish assembly factories, or
maquiladoras, which were largely free of Mexican taxes and regulations on the repatriation of profits. Soon, large
zones along the northern border were filled with such assembly plants, which manufactured everything from
clothing to automobiles.
By the early 1990s, when Mexico signed NAFTA, which put it on the road to fuller integration with the economies

of Canada and the United States, the economic outlook had improved and the country was once again
experiencing growth, though on a modest scale. But 1994, the year in which NAFTA went into effect, proved rocky
for the Mexican economy. The situation looked good at the start of the year, with inflation slowing, the currency
stable, and the overall economy growing steadily since 1988. The trade deficit caused some concern about the
balance of payments, but this situation was eased with significant foreign investment coming into the country. The
foreign capital inflows, stimulated by expectations about NAFTA, took the form of both direct investment
(ownership of companies and factories) and portfolio investment (stocks, bonds, and such). The result was a boom
in Mexican securities, and the stock market was one of the best performers in the world.
As the year unfolded, however, Mexico experienced a number of political shocks that had devastating economic
consequences. In January, an uprising broke out among the indigenous population in the southern state of
Chiapas—the first armed revolt in the country since the Mexican Revolution of the 1920s. In March, the leading
candidate in the presidential election, scheduled for that summer, was assassinated. In September, the general
secretary of the ruling PRI was also assassinated. In November, the chief investigator of the murder resigned,
charging a cover-up. And in December, there were reports of renewed fighting in Chiapas. Meanwhile, there was
a spate of kidnappings of foreign businessmen in Mexico City.
Not surprisingly, confidence among foreign investors plummeted. Since stocks and bonds can be sold much more
quickly than physical assets like factories, portfolio investment is more volatile than direct investment. Thus,
portfolio inflows, which had reached $9 billion in the fourth quarter of 1993, began to dry up. Then, with investors
pulling out of the country in droves, the inflow turned into an outflow of almost $6 billion in the fourth quarter of
1994. This served once again to weaken the peso.
Initially, the Mexican Central Bank tried to support the currency with U.S. dollar reserves, selling dollars and
buying pesos to prop up the value of the latter. But the reserves were quickly depleted, falling from $26 billion in
the first quarter of the year to $6 billion in the fourth quarter. After losing $2 billion in a week, the government
stopped supporting the peso on December 20, 1994. The currency fell from 3.5 pesos per dollar to 3.9 pesos
immediately, and to 5.2 pesos by December 31, 1994.
To make matters worse, the government had been borrowing in the form of tesobonos, short-term bonds linked to
the dollar, where bondholders did not have any currency risk; interest rates on these bonds were lower than for
cetes, the normal Mexican government bonds. This transformed the currency crisis into a debt crisis, since the
government did not now have enough dollars to repay tesobono holders.
International financial markets reacted immediately and violently. The sharp fall in the peso after seven years of
stability fundamentally altered expectations of currency traders, who had come to believe that sudden devaluations
in Mexico were a thing of the past. As they withdrew funds, the peso (and Mexican securities) fell further.
Moreover, the shock waves provoked by the devaluation reached well beyond Mexico. In the two weeks after the
devaluation, other Latin American financial markets collapsed as well because of the capital being pulled out—
what became known as the “Tequila effect.” Emerging markets in Asia were hit too, as were European markets.
To prevent Mexico from defaulting, and to protect the rest of the world from the contagion effect of such a default,
an international loan package of $50 billion was put together. The United States committed $20 billion, the
International Monetary Fund (IMF) $17 billion, and other countries and international institutions $13 billion. The
loan was secured by revenues from Mexico’s oil exports, which were put into an escrow account in New York.
Mexico also undertook an IMF-approved stabilization program, with severe belt-tightening in government spending
and bank lending. This reduced the purchasing power of Mexicans, and the fall in the peso made foreign products
much more expensive. Imports shrank, and exports, now more competitive, increased; by the second quarter of
1995, the nation’s trade deficit had turned into a surplus. Portfolio investment returned by the end of 1995, and the
currency, now allowed to float freely—it was no longer pegged to the dollar or any other currency—stabilized at
between 6 and 7 pesos per dollar. Mexico repaid the entire amount of the emergency loan, with interest, before it
was due. Thus, one might conclude that the crisis was dealt with effectively and quickly.

The resolution, however, came at a high price. Due to reduced bank lending, interest rates rose sharply, exceeding
100 percent on consumer loans. Business investment plunged, and there was a wave of bankruptcies and bank
failures. The unemployment rate almost doubled, and the gross domestic product (GDP) fell by 7 percent, a very
large decline for a modern economy.
 Growth and Crisis
By 1996, the Mexican economy was once again on the mend, buoyed by exports to a surging U.S. economy and
the rising powers of Asia, especially China. Helping the situation was a renewed interest by the Mexican
government—which, since the late 1990s, was no longer under the exclusive control of the PRI—in establishing
fiscal probity. The government replenished its foreign reserves and abandoned its policy of supporting a strong
peso, all the while implementing tight controls on wage and price increases. All of this resulted in a new wave of
foreign investment and a surge in exports, helping the country avoid the massive balance-of-payments problems
in Argentina and Brazil that triggered the South American economic crisis of 2002.
By the mid-2000s, Mexico had emerged as the second-largest economy in Latin America, after that of the far
more populous Brazil. It had the highest per capita income in the region, which by 2007 reached nearly $8,000
annually, placing Mexico among the top middle-income countries in the world. Still, the nation’s economy was
plagued with problems. The high per capita income masked significant wealth and income gaps, not just between
classes but between regions, with the north and Mexico City doing far better than other parts of the country.
Continuing poverty continued to plague about 40 percent of the country, and the unemployment rate even in the
best years rarely fell below 25 percent. Millions migrated north, legally or not, to seek work in the United States.
Their remittances home, valued at some $20 billion annually in the mid-2000s, provided the second-largest
source of foreign capital into Mexico, after petroleum exports.
While Mexico’s increased integration with the economy of the United States since the 1980s has brought
substantial economic growth, it has made the country far more vulnerable to fluctuations north of the border.
Although the government imposed a number of measures in the wake of the 1994 financial crisis to ensure that its
banks were not over-leveraged with speculative investments, and the country avoided the kind of housing price
bubble experienced in the United States, Mexico’s financial sector was nevertheless highly exposed to the latest
global financial crisis because so many of its banks were owned by institutions in the United States and Spain, two
countries hard hit by the mortgage and housing problems at the heart of the crisis.
Even more devastating, according to economists, was the subsequent recession in the United States, which led to
falling exports, higher unemployment, declining petroleum prices, lackluster foreign investment, reduced income
from tourism, and diminishing remittances from Mexicans working north of the border. After posting an average 4
percent growth rate through the mid-2000s, the Mexican economy expanded by just 1.3 percent in 2008 and
declined by 6.8 percent in 2009. But demand from burgeoning Asian economies and a gradual turnaround in the
U.S. economy helped lift Mexico back into the black and, in 2010, it posted a 5.5 percent gain in GDP.
James Ciment and Animesh Ghoshal
 
See also:  Central America;  Latin America;  Tequila Effect. 
Further Reading
Moreno-Brid, Juan Carlos, and Jaime Ros. Development and Growth in the Mexican Economy: A Historical
Perspective. New York: Oxford University Press, 2009. 
Lustig, Nora. Mexico: The Remaking of an Economy. Washington, DC: Brookings Institution, 1998. 
Organisation for Economic Co-operation and Development (OECD).  “Economic Survey of Mexico 2009: Overcoming the
Financial Crisis and the Macroeconomic Downturn.” July 30, 2009. 

Schlefer, Jonathan. Palace Politics: How the Ruling Party Brought Crisis to Mexico. Austin: University of Texas
Press, 2008. 
Servín, Elisa, Leticia Reina, and John Tutino. Cycles of Conflict, Centuries of Change: Crisis, Reform, and Revolution in
Mexico. Durham, NC: Duke University Press, 2007. 
Teichman, Judith A. Policymaking in Mexico: From Boom to Crisis. New York: Routledge, 1988. 
 
Middle East and North Africa
 
An expansive geographic area that stretches across northern Africa and Southwest Asia, from Morocco in the west
to Iran in the east, from the Sahara Desert and Indian Ocean in the south to the Mediterranean Sea, Turkey, and
the states of the former Soviet Union to the north, the Middle East and North Africa (hereinafter referred to as the
Middle East) share a common faith, Islam, and, with some major exceptions, a common language and culture,
Arabic.
The Middle East, the birthplace of Western civilization, was under the rule of Rome for centuries (with the
exception of Persia, modern-day Iran). The region came under the rule of various Arabic caliphates through the
middle ages, before being incorporated into the Ottoman Empire in the fifteenth and sixteenth centuries. Over the
course of the nineteenth century and following the collapse of the Ottomans, the region was colonized by
European powers, largely Britain and France.
Independence came to most of the Middle East immediately before and after World War II, a period that also saw
the emergence of a modern economic order throughout much of the region, built on its vast oil and natural gas
reserves. Unevenly distributed, these reserves have provided great wealth to some national economies, leaving
much of the rest of the region struggling to modernize and develop. Political turmoil has also been a hallmark of
the region, some because of the conflict with Israel but much due to conflicts between and within Arab states and
with Iran.

Leaders of the Gulf Cooperation Council nations—the six Arab states of the Persian Gulf—held the group’s
thirtieth annual summit in Kuwait City in December 2009. Discussions focused on the global economic downturn,
which did not spare the region. (Yasser Al-Zayyat/Stringer/AFP/Getty Images)
 Economic History Before the Oil Boom
After Africa, the Middle East was the first place to be inhabited by human beings several hundred thousand years
ago. Its great river valleys—the Nile in Egypt and the Tigris and Euphrates in Iraq—were home to humanity’s first
major civilizations. By the latter centuries of the first millennium BCE, much of the region had come under the sway
of the Roman Empire, which remained dominant until the fifth century CE.
The event that shaped the region’s subsequent history was the birth of Islam in the early seventh century CE in
what is now Saudi Arabia, and the impetus the faith gave to the tribes of the Arabian Peninsula to unify and
conquer much of Southwest Asia and Africa north of the Sahara from 633 through the early eighth century. The
region was unified politically for several centuries under successive caliphates, or states ruled by a caliph (an
Arabic term that combines leader and prophet), before gradually breaking apart into several caliphates in the ninth
and tenth centuries.
Still, even after the collapse of a central authority, the region remained unified by faith (virtually all people in the
region converted to Islam), a common Arabic language and culture (with large pockets of indigenous language and
culture surviving), great centers of learning and science, and an interwoven, trade-based economy.
Islam, the dominant cultural element in the region, differed from medieval Christianity on issues of money. While
the latter frowned upon the acquisition of wealth beyond a person’s basic needs, Islam emphasized the
importance of using one’s wealth for the common good—in charity, a central tenet of the faith, and good works.
As the founder Prophet Muhammed himself was a merchant, so trade was seen as a godly activity, especially
when it resulted in the spread of the faith. And indeed, Islam’s spread had as much to do with merchants as it did
with armies.
Under both unified and separate caliphates, the region was interwoven by a vast trading network of land routes
and sea lanes that linked its various parts with each other and with regions as far away as China, India, West
Africa, and Christian Europe. Merchants, often doubling as government representatives, facilitated the trade in a
vast array of products, including precious and nonprecious metals, oil, dyes, textiles, and drugs. Arab traders also
introduced a number of financial innovations, including the check, the bill of exchange, and the joint stock

company. Great cities emerged, such as Cairo in Egypt and Baghdad in Iraq, as centers of trade, with vast
marketplaces known as souks where almost any product or service available in the medieval world could be
obtained.
Even the onslaught of the Mongols, which led to the sacking of Baghdad in 1258, only served to reroute the
intricate trading networks of the region. Still, assaults by Mongols and Turkic peoples undermined the stability in
the region and began the long-term decline in its economic and political fortunes. By the late sixteenth century,
virtually all of the region, with the exception of Persia, had come under the suzerainty of the Ottoman Turks, who
shared a common faith but spoke a different language and possessed a different culture than their Arab subjects.
At the same time, the rise of European maritime power and the discovery of the Americas shifted the Middle East
from the center of world trade to its periphery by the late 1600s and led to its economic and technological decline
vis-à-vis Europe and the West. From the seventeenth through the nineteenth centuries, the region languished
under Ottoman rule, leaving it vulnerable to Western colonization. In 1830, France occupied Algeria—later
expanding its conquests to include much of northwest Africa, while Britain seized possessions along the periphery
of the Arabian Peninsula.
Other parts of the Ottoman Empire—most notably, Egypt—broke free of the sultan’s rule. Under the breakaway
governor Muhammad Ali, Egypt developed the beginnings of industrial infrastructure and instituted land reforms in
the mid-nineteenth century, a process that drew in large numbers of European traders, professionals, and capital
—some of it to build the Suez Canal. Under Ali’s successor and grandson Ismail, however, Egypt fell into debt to
European financiers and came under the control of European colonizers by the 1880s.
With the collapse of the Ottoman Empire after World War I, much of the rest of the region, from Iraq in the east to
Palestine in the west, also came under British and French rule, though under the League of Nations mandate
system this was expected to be temporary. By World War II, Egypt, Iraq, and Saudi Arabia had all achieved
nominal independence, though they continued to be ruled by pro-British regimes.
 Oil Boom
It was also during this period that the first great oil discoveries in the region were made, with vast reserves being
found and exploited in Iran, Iraq, Saudi Arabia, and other Gulf territories from the 1910s through the 1930s—
reserves that would later be found to be the largest in the world by a significant margin.
The exploration and exploitation of this carbon wealth was conducted by Western oil companies, who also took the
lion’s share of the profits, fueling nascent nationalist movements both in colonized and noncolonized countries and
territories in the region—movements that would see the toppling of many pro-Western regimes after World War II,
including those in such key countries as Egypt (1952) and Iraq (1958). Efforts to politically unify the Arab world
came to naught, however, partly because of differences between countries with great oil resources and those with
little or none.
Over the course of the 1950s and 1960s, the region participated in the global economic boom, though not nearly
as much as Europe or East Asia. Led by nationalist leaders, many of the countries, such as Algeria, Egypt, and
Iraq, promoted autarkic economic development, whereby national economies were to be made self-sufficient
through centralized planning and state-led development of heavy industry. Such policies met with mixed success,
achieving better results in countries where large oil revenues could fund the development.
The event that utterly transformed the region economically, however, came in the early 1970s. Through much of
the 1950s and 1960s, two trends could be seen in the region’s key oil and natural gas industries. The first was
the increasing control national governments asserted over Western oil companies, with many regimes
nationalizing their local industries and others demanding ever greater percentages of the revenues those
resources provided.
At the same time, however, a glut in world energy supplies kept prices low, so that even where Middle Eastern

oil-producing countries had come to control more of the revenues, these remained relatively small. By the early
1970s, however, ever greater energy demand in the West had eliminated the supply surplus. Thus, when various
Arab oil exporters united to punish the West for its support of Israel during the Arab-Israeli War of 1973, the price
of crude oil surged, from about $3 a barrel to $12. Six years later, during the Islamist revolution in Iran,
disruptions in supply and panic on world markets would again send oil prices soaring, to more than $40 a barrel
(over $100 in 2009 dollars).
These “oil shocks” and the “energy crisis” they triggered helped end the post–World War II economic boom in the
West, creating a decade of “stagflation,” a period of slow or negative growth combined with inflation. For the
Middle East, the picture was more mixed. Like other developing countries around the world, oil-poor Middle
Eastern countries were hit with dramatically higher prices that undermined economic growth. But for those
countries with huge oil and natural gas reserves, the spike in oil prices produced an unprecedented economic
boom.
Resource-rich countries around the Middle East, and particularly in the Persian Gulf region, spent huge sums of
money building modern housing, schools, manufacturing infrastructure, and transportation and communications
networks in an effort to join the ranks of the developed nations in a single bound. Even non-resource-rich Middle
East countries benefited somewhat from the boom in the form of increased aid from oil-rich countries and, even
more so, from the remittances of nationals who went to work in those countries. But there was also a great deal of
conspicuous consumption in the oil-rich countries, as much of the wealth accrued to elites with close connections
to the royal families or dictatorial regimes that ruled them. Speculative excess also ensued, especially in Kuwait,
where the unregulated Souk al-Manakh stock market saw securities valuations inflate to unsustainable levels
before crashing in 1982.
 Boom-and-Bust Cycle
Inevitably, the boom led to bust, however, as the oil industry responded to high prices by bringing new fields in
other parts of the world into production. The huge run-up in oil prices in the 1970s was followed by a collapse in
the 1980s and 1990s. While the reserves of the region were so vast that revenues continued to pour in, many
countries had to scale back some of their most ambitious development projects and their social welfare networks.
In non-resource-rich countries, economic stagnation set in, as corruption, a lack of skilled workers, and
overspending on arms stunted economic growth.
The situation helped foster a new militancy in the region. Whereas in the 1950s and 1960s, the calls for political
and economic reforms had been promoted by those with a socialist, Pan-Arabist, or nationalist agenda, now the
activism was coming from Islamists, many of whom called for a return to Koranic economic principles. With the
exception of Iran, where an Islamist revolution overthrew a free market–oriented dictator in 1979, the Islamist
militancy did not succeed in toppling the largely pro-Western regimes of the region. But it did result, among other
changes, in an effort within the business community of the Islamic world to create financial instruments that
adhered to religious principles, including bans on interest-bearing loans, bonds, and other securities.
By the early 2000s, the economies of many oil-rich Middle Eastern countries were rebounding, as rising demand
from newly emerging industrializing countries in Asia put upward pressure on energy prices. In the Persian Gulf
region, there was a new spate of construction and infrastructure expansion as some states, most notably in the
semi-independent principalities of the United Arab Emirates, including Dubai and Abu Dhabi, which attempted to
turn themselves into centers of finance.
But the new boom did not survive the global financial crisis of 2008–2009, which deflated the real-estate and
finance bubbles that had been inflating in the Persian Gulf region since the 1990s, largely based on loose credit
and the inrush of oil revenues. In addition, the recession that followed the crisis reduced economic output around
the world and lowered demand for oil, bringing down energy prices and reducing revenues in oil-rich countries.
For non-resource-rich Middle Eastern countries, the recession of the late 2000s has resulted in lowered exports,

tighter credit, stagnating domestic economies, and a drop in remittances from nationals who once worked on the
vast construction projects of the Persian Gulf region. Although down, oil revenues will continue to bolster Middle
Eastern economies—at least, those with significant reserves—but growth rates for the region as a whole are
expected to fall below 4 percent for 2009, down by nearly one-half from the boom years of the early and middle
2000s.
Beginning in early 2011, a number of countries across the region experienced a new round of political turmoil, as
popular protests and armed rebellions overthrew and challenged long-standing autocracies, dictatorships, and
monarchies. Countries affected included Bahrain, Egypt, Libya, Syria, Tunisia, and Yemen. Egypt, Libya, and
Tunisia saw governments overthrown. In Bahrain, the popular protests were crushed. In both Syria and Yemen,
the situation remained fluid as of late 2011.
While political freedom was a critical component in all of the rebellions, so were economic factors. In all of these
countries, people expressed their dissatisfaction with high unemployment, particularly among the young, income
and wealth inequality, and favoritism, in which government largesse tended to accrue to the politically well-
connected. Whether new governments, which, by most expert accounts, were likely to be at least somewhat more
democratic, could address these problems remained to be seen. But, as is often the case, the popular protests
negatively affected growth in the short-term. Unrest in Egypt undermined the country’s lucrative tourism business,
while the eight-month civil war in Libya disrupted the production and shipment of the country’s vast hydrocarbon
reserves. Egypt, for one, saw its annualized GDP growth rate drop from 6 percent in the fourth quarter of 2010 to
-4.2 percent in the first quarter of 2011, at the height of the revolution. In Libya, high oil and gas prices contributed
to a 7.4 percent growth rate in 2010, but the violence associated with the anti-Muammar Qadafi uprising
devastated the economy, with many experts predicting it would shrink by as much as 20 percent in 2011. Still, the
consensus was that once the dust had settled both countries were likely to benefit from renewed growth.
James Ciment
 
See also:  Africa, Sub-Saharan;  Israel;  Oil Shocks (1973-1974, 1979-1980). 
Further Reading
Lewis, Bernard. The Middle East: A Brief History of the Last 2,000 Years. New York: Scribner, 1995. 
Ochsenwald, William, and Sydney Nettleton Fisher. The Middle East: A History.  7th ed. Boston: McGraw-Hill, 2010. 
Pappé, Ilan. The Modern Middle East. New York: Routledge, 2005. 
Safadi, Raed, ed. MENA Trade & Investment in the New Economy. New York: American University in Cairo Press, 2003. 
 
Mill, John Stuart (1806–1873)

 
Through his penetrating and comprehensive study of economics and political economy, the English economist,
philosopher, and reformer John Stuart Mill had a great influence, albeit indirectly, on thinking regarding the nature
of free markets, the equilibrium condition of supply and demand, and the various forces underlying the rise and fall
of business cycles.
At the time of his death on May 8, 1873, the sixty-six-year-old Mill (born May 20, 1806) was regarded as the
greatest British philosopher of the nineteenth century. A Renaissance man, he wrote prolifically and on a wide
range of subjects, including politics, social issues, economics, women’s rights, civil rights and slavery, and religion.
He also served for a time as a member of Parliament, held an academic position, and worked in commerce for the
British East India Company.
Mill regarded economics as a separate science, a self-contained and complete discipline that did not rely on other
fields to make sense. Yet he also referred to economics as an inexact science in which not all laws governing the
phenomena being studied could be known. The result, he acknowledged, was that any conclusions economists
might draw can only be approximate and must always leave room for doubt. Finally, Mill saw economics as a
discipline that relies on deduction to reach valid conclusions—even if his own methods, according to some
commentators, could at times be inconsistent. In that deductive method, as defined by Mill, the first step was the
statement of a law or laws by induction. Once the law was stated, the next step was to deduce the laws by
means of observation. The final step was verification. Although various elements of Mill’s approach have been
superseded, some economists believe that his view of economics as a separate discipline has aided in the
development and interpretation of both microeconomic and macroeconomic equilibrium models.
British philosopher and economist John Stuart Mill believed in the classical, free-market paradigm. In his view,

disruptions in the equilibrium of supply and demand—economic cycles—are caused by the overreaction of traders
to unexpected market shocks. (The Granger Collection, New York)
Mill’s background was unique in that he was a child prodigy who was purposefully and specifically trained to
become a genius. He was the son of a Scottish philosopher and historian, James Mill. His father sent him to be
educated by the English philosopher and social reformer Jeremy Bentham, whose ideas on utilitarianism formed
the basis of much of Mill’s thought for several years. Under Bentham’s curriculum, Mill read Roman and Greek
classics and studied mathematics intensively. His study of economics focused on the works of Adam Smith, whose
practical approach he admired, and David Ricardo, who extended Smith’s theories on free markets.
Mill’s own great work in the field, Principles of Political Economy, was published in 1848; another seven editions
would appear during his lifetime, the last in 1871. The book was used as the principal economics text at Oxford
University until 1919, nearly fifty years after Mill’s death. The work that replaced it, written by Alfred Marshall, was
strongly influenced by Mill and his views.
Like Smith and Ricardo, Mill was a believer in free markets and, therefore, free trade and no or low tariffs
between nations. That economic philosophy had begun to predominate in England during the mid-1840s with the
abolition of the Corn Laws, which had kept agricultural prices artificially high. Mill wrote that a laissez-faire policy
on the part of the government should be a general practice, as any departure from it was bound to bring evil
consequences in the form of a reduction in aggregate social welfare. Nevertheless, Mill did accept the use of
government intervention on certain occasions and, like many intellectuals of the nineteenth century, looked for
economic means to alleviate poverty that arose from inequalities in the demand for labor.
While Mill did not write as extensively on business cycles as his contemporary, French economist Clément Juglar,
he did explore the dynamics of supply and demand, the two major forms of money (currency that circulates and
credit), and some of the causes and consequences of financial booms and busts. Mill believed that an excess of
credit creates a temporary state of overproduction, that overproduction ends when prices fall, and that a steady-
state equilibrium is eventually reached in the labor market—a classical economic paradigm that has survived into
the twenty-first century.
Robert N. Stacy
 
See also:  Classical Theories and Models. 
Further Reading
Balassa, Bela A.  “John Stuart Mill and the Law of Markets.” Quarterly Journal of Economics 73:2 (May 1959): 263–274. 
Harris, Abram L.  “John Stuart Mill: Government and Economy.” Social Service Review 37:2 (June 1963): 134–153. 
Hausman, Daniel M.  “John Stuart Mill’s Philosophy of Economics.” Philosophy of Science 48:3 (September 1981): 363–
385. 
Hollander, Samuel. The Economics of John Stuart Mill. Toronto: University of Toronto Press, 1985. 
Mill, John Stuart. Principles of Political Economy. New York: Oxford University Press, 1998. 
Mills, Frederick Cecil (1892–1964)

 
Economist and statistician Frederick Mills, a colleague of Wesley Clair Mitchell and Willard Thorp, was a leading
researcher at the National Bureau of Economic Research (NBER) from 1925 to 1953, and a faculty member at
Columbia University from 1919 to his death in 1964. In professional circles, he is best known for his use of
statistics to analyze economic data, his work on the cyclical behavior of production and prices, and as a leading
authority on inflation.
Mills was born on March 24, 1892, in Santa Rosa, California. He attended the University of California, from which
he received a bachelor’s degree in 1914 and a master’s in 1916. During that same period he worked as an
investigator for California’s Commission of Immigration and Housing and served on the U.S. Commission on
Industrial Relations. His work focused on issues of unemployment, migratory labor, and immigration. From 1916 to
1917, he was a Garth fellow in political economy at Columbia University in New York, where he studied under
such notable scholars as Wesley Clair Mitchell (economics), John Dewey (philosophy), and Franz Boas
(anthropology). He received a PhD from Columbia in 1917, publishing his dissertation that year under the title
“Contemporary Theories of Unemployment and Unemployment Relief.”
Mills’s book Statistical Methods, published in 1924, described the use of quantitative techniques to analyze
problems in business and economics. It provided insights into the specific types of data gathering and analysis—
including such methods as time-series studies—that were performed at the NBER in the early 1920s. In the highly
regarded book, which demonstrated the advantages of using statistics in economic analysis, Mills also explained
cases in which the methodology was not ideal. Nevertheless, it formed the basis for the methodology he would
employ consistently for his entire career.
Another of Mills’s well-known studies, The Behavior of Prices (1927), presented data on the prices of 200 items.
Although the work was criticized for presenting data in isolation without taking into consideration outside factors,
Mills had stated at the outset that the study was only preliminary and would eventually include a fuller examination
of prices in a broader context. His work on prices and business cycles, based on statistical research and an
empirical foundation, led him to conclude that there were certain regularities in cycles that could be identified.
From that proposition, he believed a larger set of conclusions could be drawn.
Despite the development of econometrics and the increased use of statistics in economic analysis in the early
twentieth century, Mills’s approach was not universally accepted. Many disagreed with his methods—including his
very use of statistics to derive meaning from economic and business events. Mills served as president of the
American Statistical Association (1934) and the American Economic Association (1940).
Robert N. Stacy
 
See also:  Mitchell, Wesley Clair;  National Bureau of Economic Research;  Thorp, Willard
Long. 
Further Reading
Bye, Raymond T. An Appraisal of Frederick C. Mills’ The Behavior of Prices. New York: Social Science Research
Council, 1940. 
Mills, Frederick C. The Behavior of Prices. New York: Arno, 1975. 
Mills, Frederick C. Productivity and Economic Progress. New York: National Bureau of Economic Research, 1952. 
Mills, Frederick C. Statistical Methods Applied to Economics and Business.  3rd ed. New York: Holt, 1955. 
Mills, Frederick C. The Structure of Postwar Prices. New York: National Bureau of Economic Research, 1948. 
Woirol, Gregory R.  “The Contributions of Frederick C. Mills.” Journal of the History of Economic Thought 21:2 (June

1999): 163–185. 
Minsky, Hyman (1919–1996)
 
American economist Hyman Minsky proposed influential ideas about the workings of national economies that
touch directly on business cycles in general. His research and theories on financial crises received much attention
during the financial meltdown of the early 2000s.
Minsky was born in Chicago on September 23, 1919. He received his bachelor’s degree from the University of
Chicago and his master’s and doctorate degrees from Harvard University, where he studied under Joseph
Schumpeter and Wassily Leontief and was influenced by, among others, Irving Fisher and Jacob Viner. Widely
viewed as the outstanding financial Keynesian of the last quarter of the twentieth century, Minsky taught at Brown
University, the University of California at Berkeley, Washington University in St. Louis, and the Jerome Levy
Economics Institute at Bard College in New York. Although he was not as familiar to the general public as some
other economists, he was well known and highly influential in the academic and financial communities.
Minsky believed that capitalism was inherently fragile and that the back-and-forth movement from fragility to
robustness created business cycles. The behavior of bankers and businesspeople—particularly their uncertainty
about financing and investing during booms and busts—also influences the cycle. According to Minsky, these
financial uncertainties and the responses to them “called the tune for aggregate demand and employment.” In
times of prosperity, more money is available than is needed to meet obligations, leading to increased speculation.
Lending increases until it exceeds what borrowers can pay. At that point, lenders decrease the amount of available
money (sometimes referred to as a “Minsky moment”) and the economy contracts. Thus, how well the financial
world performs has much to do with how the business world performs. Minsky called this approach “financial
Keynesianism.” In the 1960s and 1970s, when he was advancing these arguments, such connections were not
well established. Minsky also suggested that the prices of outputs and capital assets are separate entities that are
determined differently and not completely linked in an economy. He argued that there is a connection between
business decisions and financial relationships, although the connection might not always be well understood or
optimally coordinated.
Minsky believed that business cycles, with their extreme swings from boom to bust, will always occur in a free-
market economy. Furthermore, he maintained, government intervention can do a great deal to counteract the
violent swings, and that regulation, the use of the Federal Reserve, and other government actions are the best
means to accomplish this end. He supported government-created deficits, as Franklin D. Roosevelt’s
administration instituted in the 1930s, believing that a large debt would provide a safe investing haven for cautious
investors. While Minsky’s work was generally seen as pioneering, not all economists agreed with his ideas.
Minsky died in Rhinebeck, New York, on October 24, 1996, but his theories have remained influential. His notion
that debt accumulation drives an economy toward the brink of disaster received especially wide attention during
the financial crisis of the early 2000s. He also identified three types of borrowing that contribute to a situation
marked by crippling, insolvent debt—specifically, borrowing related to hedging, or selling short and buying long to
balance investment risk; speculating; and Ponzi schemes, in which earlier investors are paid high returns with
money from new investors. And, indeed, the financial meltdown that began in 2007 was largely precipitated by the
collapse of the housing and subprime mortgage markets in which there were poorly understood hedges,
speculation, and even Ponzi schemes.

Robert N. Stacy
 
See also:  Fisher, Irving;  Kindleberger, Charles P.  Minsky’s Financial Instability Hypothesis; 
Post Keynesian Theories and Models;  Viner, Jacob. 
Further Reading
Bellofiore, Riccardo, and Piero Ferri. The Economic Legacy of Hyman Minsky, Volume 1: Financial Keynesianism and
Market Instability. Northampton, MA: Edward Elgar, 2001. 
Bellofiore, Riccardo, and Piero Ferri. The Economic Legacy of Hyman Minsky, Volume 2: Financial Fragility and Investment
in the Capitalist Economy. Northampton, MA: Edward Elgar, 2001. 
Dymski, Gary, and Robert Pollin, eds. Monetary Macroeconomics: Explorations in the Tradition of Hyman P. Minsky. Ann
Arbor: University of Michigan Press, 1994. 
Minsky, Hyman P. Can “It” Happen Again? Essays on Instability and Finance. Armonk, NY: M.E. Sharpe, 1982. 
Minsky, Hyman P. Inflation, Recession and Economic Policy. Brighton, UK: Wheatsheaf, 1982. 
Minsky, Hyman P. John Maynard Keynes. New York: Columbia University Press, 1975. 
Minsky, Hyman P. Stabilizing an Unstable Economy. New Haven, CT: Yale University Press, 1986. 
Taylor, Lance, and Stephen A. O’Connell.  “A Minsky Crisis.” Quarterly Journal of Economics 100, Supplement (1985): 871–
885. 
Minsky’s Financial Instability Hypothesis
 
Minsky’s financial instability hypothesis is a theory of financial market instability named for its originator, twentieth-
century American economist Hyman Minsky. According to classical economic equilibrium theory, the forces of
supply and demand inevitably lead to a price equilibrium in which there are no shortages or surpluses of goods—
in other words, where quantity demanded equals quantity supplied. Moreover, efficient market theory states that
the values assigned to assets and liabilities by financial markets cause efficient use of resources because known
information is always reflected in prices and new information is absorbed instantly. By contrast, Minsky’s financial
instability hypothesis rejects both of these views, arguing instead that capitalist economies are fundamentally
unstable, exhibiting periods of inflation and debt deflation that have the potential to spin out of control.
Minsky was not the first economist to explore the idea of financial instability, but while similar ideas were
developed by earlier theorists, today’s theories about financial instability are most closely associated with the
ideas he presented in his 1986 book Stabilizing an Unstable Economy and elaborated on in a 1992 paper titled
“The Financial Instability Hypothesis.” In the two works, Minsky discussed the impact of debt on the behavior of
economic system as a whole. In his view, a prolonged period of tranquility, when the system is stable and returns
are normal, leads financial innovators (whether banks or brokers or dealers) on a quest for higher returns. This, in
turn, leads to a period of high risk-taking, financial innovation, and unsustainable levels of debt that ultimately
disrupt stability, raise asset prices, and create speculative booms. When the boom finally ends, it is followed by a
period of reduced asset valuation, intense credit contraction, and a financial crisis in which the unsustainable
levels of debt cannot be serviced. Minsky saw a critical role for government in stabilizing the economy by running

sizable deficits during economic downturns and then accumulating a surplus during inflationary booms.
While many economists focus their attention on consumer behavior, understanding it as the core of the “real
economy,” Minsky focused on Wall Street and the money flows that make investment, production, employment,
salaries, and purchases possible, all of them financed in some way by credit. Credit itself is one of the financial
innovations that generate wealth and fuel economic growth. However, at the same time, credit creates the
instability at the heart of Minsky’s hypothesis because it allows businesses and individuals to take advantage of
investment opportunities that arise, whether or not they have money available in a savings or checking account.
Credit can take other forms that similarly lead to Minsky-type instability. Other innovations include money market
funds, bonds, options, hedge funds, and a wide range of derivatives, including collateralized mortgage obligations
(CMOs) and collateralized debt obligations (CDOs), as well as investment financing options—from margin
accounts to credit cards, and home equity lines of credit for the individual—that institutional lenders use to reduce
their risk and provide cash for lending purposes. Like the businesses and individuals to which they lend, banks
seek profits by financing activity. And like all entrepreneurs, bankers are aware that innovation assures profits.
Thus bankers, whether they are brokers or dealers, are merchants of debt who strive to innovate in the assets
they acquire and the liabilities they market.
Minsky distinguishes three kinds of financing—hedge, speculative, and Ponzi. Hedge financing units are those that
can fulfill all contractual payment obligations by their own cash flows. Speculative finance units can meet interest
payments on debt but cannot repay the principle out of income cash flows and must issue new debt to meet
commitments on maturing debt. Ponzi financing must borrow or sell assets to pay interest on outstanding debts.
Minsky argues that an economy’s financing regions can be stable (hedge financing) or unstable (speculative or
Ponzi financing). Over periods of prolonged prosperity, the economy makes a transition from financial relations that
make for a stable system to financial relations that make for an unstable system. During a protracted period of
good times, capitalist economies tend to move from a financial structure dominated by hedge finance units to a
structure in which there is a preponderance of units engaged in speculative or Ponzi financing.
Minsky believed that sound fiscal and monetary policies—in particular, public spending to offset reductions in
private spending and central bank lender-of-last resort interventions—can steady financial markets and restore
stability. In Stabilizing an Unstable Economy (1986), he discusses the nine contractions and the even greater
number of domestic or international financial crises since 1950 to demonstrate that big government played a
significant role in avoiding a repeat of the 1929–1933 macroeconomic collapse. Many economists have used
Minsky’s hypothesis to help explain the financial crisis of 2008–2009 and to offer solutions to prevent a
reoccurrence.
Carol M. Connell
 
See also:  Asset-Price Bubble;  Debt;  Deflation;  Innovation, Financial;  Minsky, Hyman; 
Regulation, Financial;  Systemic Financial Crises. 
Further Reading
Cooper, George. The Origin of Financial Crises: Central Banks, Credit Bubbles, and the Efficient Market Fallacy. New
York: Vintage, 2008. 
Minsky, Hyman P.  “The Financial Instability Hypothesis.” The Jerome Levy Economics Institute,  Working Paper No. 74
(May 1992). 
Minsky, Hyman P. Stabilizing an Unstable Economy. New Haven, CT: Yale University Press, 1986. 
Pressman, Steven. Interactions in Political Economy: Malvern After Ten Years. New York: Routledge, 1996. 
Whalen, Charles, and Jeffrey Wenger.  “Destabilizing an Unstable Economy.” Challenge 45: 5 (2002): 70–92. 

Mises, Ludwig von (1881–1973)
 
Ludwig von Mises was an economist of the Austrian school who made pathbreaking contributions to monetary
(money supply) theory and business-cycle theory. He is credited with developing the Austrian theory of the
business cycle. An ardent advocate of free markets, Mises participated in the famous socialist calculation debate,
arguing that socialist planners were incapable of efficiently coordinating modern economies. Though Mises’s
influence on economics diminished with the rise of Keynesianism in the mid-twentieth century, his work received
renewed attention as Austrian economics returned to favor in the last decades of the century.
He was born Ludwig Heinrich Edler von Mises on September 29, 1881, in what is now Ukraine but was then part
of the Austro-Hungarian Empire. He studied at the University of Vienna and received his Doctor of Laws degree in
1906. Mises was influenced by the founders of the Austrian school of economics, Carl Menger, and Eugen von
Böhm-Bawerk. Employed by the Austrian Chamber of Commerce, Mises was also a long-serving, though unpaid,
faculty member at the University of Vienna, where his private seminars cultivated the next generation of Austrian
economists. Though an influential adviser to the Austrian government, Mises left Austria for Switzerland as the
Nazis rose to power. Eventually emigrating to the United States, he was a visiting professor at New York
University from 1945 until his retirement in 1969. For virtually his entire adult life, Mises was actively engaged at
the highest levels in both the intellectual and policy struggles of Europe. He died on October 10, 1973.
Mises’s first important contribution to economics was The Theory of Money and Credit (1912), in which he
provided the first modern treatment of monetary theory. In doing so, he bridged the gap between microeconomics
(economics at the individual, organizational, or company level) and macroeconomics (the economy of a nation or
region as a whole) by establishing what determines the value of money. His solution was the regression theorem,
in which he suggested that the value of money is, and always must be, based on the value that the market gives
to money as a commodity.
The most important contribution of The Theory of Money and Credit, however, would come to be known as the
Austrian theory of the business cycle. Integrating the contributions of such earlier economists as Richard Cantillon,
Knut Wicksell, and Eugen von Böhm-Bawerk, Mises developed the theory that economies undergo business
cycles primarily due to the policies of the central bank. Easy credit policies on the part of the central bank will be
felt first in banking and financial markets, where an artificial increase in loanable funds will reduce the going rate
of interest below the natural rate established by natural supply and demand. Lower interest rates, in turn, increase
borrowing and investment, which results in decreased saving and increased consumption. The signals of
increased consumption, the theory goes, encourage entrepreneurs to invest in processes that change the structure
of production and the array of capital goods away from the underlying preferences of consumers. Eventually, it is
said, such changes will be revealed as bad investments and will require a painful process of reallocating labor and
capital via unemployment and bankruptcy. One of Mises’s students, Friedrich August von Hayek was awarded the
Nobel Prize in 1974 for his elaborations of the theory.
Mises’s other writings made significant contributions on a variety of economic issues. He wrote one of the few
economic treatises of the twentieth century, Human Action: A Treatise on Economics (1949), which presented a
systematic exposition of the principles of economics from methodology to policy conclusions. In addition, he
greatly influenced a new generation of American economists who firmly established the Austrian school of
economics in the United States. The Ludwig von Mises Institute was founded in 1982 in Auburn, Alabama.

Mark Thornton
 
See also:  Austrian School;  Böhm-Bawerk, Eugen Ritter von;  Hayek, Friedrich August von. 
Further Reading
Hülsmann, Jörg Guido. Mises: The Last Knight of Liberalism. Auburn, AL: Ludwig von Mises Institute, 2007. 
Kirzner, Israel M. Ludwig von Mises: The Man and His Economics. Wilmington, DE: ISI Books, 2001. 
 
Mississippi Bubble (1717–1720)
 
An episode of speculative excess in early-eighteenth-century France, the Mississippi Bubble made and destroyed
fortunes for thousands of investors across Europe, lured by the promise of mineral wealth in the country’s imperial
holdings along the Mississippi River. More than a mere mining and real-estate boom and bust, the Mississippi
Bubble was an early example of how easy credit policies and a lack of hard economic data can run up the market
price of an investment far above its real value.
After the War of Spanish Succession of 1701–1714 and as a result of the excessive spending of King Louis XIV,
France was in bad financial shape. In 1714, a Scottish economist with connections to the duke of Orleans,
nephew of King Louis XIV, arrived in Paris with a scheme to revive the French treasury and economy. John Law
was the thinker behind the “real bills doctrine,” an economic theory stating that a money supply should grow with
the economy, if necessary by allowing banks to issue their own notes. Law insisted this would not fuel inflation
because businesses would accept the notes only as they needed them—that is, as they expanded production.
In 1715, upon the death of Louis XIV, the duke of Orleans became regent to the young King Louis XV. A year
later, Law received a royal charter to open the Banque Générale (General Bank), with the power to issue bank
notes. This was a major departure for France, which had always placed its faith in specie money (gold and silver).
But Law had convinced the duke that an expanded money supply would spur commerce and, hence, taxable
revenues for the national treasury.
In 1717, Law organized another chartered enterprise, the Compagnie d’Occident (Company of the West), which
was given exclusive rights to trade with France’s territories in North America, stretching from Canada to the
modern-day state of Louisiana. Rumors had swirled through France for some time that the southern reaches of
this territory were rich in gold and silver. The rumors, of course, turned out to be false.
To finance his scheme, Law sold shares in the company both for cash and in exchange for state bonds, which
offered a way for the French government to finance its debt. The cash was supplied by the Banque Générale; in
other words, Law was extending credit to investors from his bank to buy shares in his trading company. As

investors, lured by the idea of great mineral wealth, threw their money at Law, who then invested in bonds, the
government granted the Scotsman more exclusive trading rights with other imperial holdings, which in turn lured
more investors.
In 1717, economist and financier John Law set up a joint-stock company in France for reclaiming colonial land in
Mississippi. Law issued more and more shares, whose price continued to rise. The bubble finally burst, and Law
became a hated figure. (MPI/Stringer/Hulton Archive/Getty Images)
In early 1719, the government took over the Banque Générale, renaming it the Banque Royale (Royal Bank),
leaving Law in charge, and backing its issues of bank notes, which now became the de facto paper money of the
state. Meanwhile, the expanding Compagnie d’Occident was renamed the Compagnie des Indes (Company of the
Indies) to reflect its far-flung trade, though most Frenchmen continued to call the whole enterprise the Compagnie
du Mississippi (Mississippi Company).
Appointed controller general and superintendent general of France in January 1720, Law emerged as the most
powerful economic figure in the country, controlling its finances and, through the Compagnie des Indes, its trade
with the non-European world. Just as important, he now held much of the government’s debt, which gave him a
steady source of interest income to finance his ever-expanding business empire.
Meanwhile, Law’s growing wealth and power had attracted investors who sent share prices in the Compagnie des
Indes from 500 livres to more than 10,000 livres over the course of the year 1719. Fortunes were made overnight,
as the French press coined a new word for those suddenly wealthy individuals: “millionaires.”

But there was an underlying flaw in Law’s scheme: the questionable value of the notes the Banque Royale was
issuing. As long as they were viewed as solid, people would continue to take them and use them to buy stock in
the Compagnie des Indes. By early 1720, however, some investors began to grow uneasy, selling their shares to
obtain gold and silver. Soon the sell-off was snowballing. Law as banker responded by limiting gold payments to
100 livres; Law as finance minister declared the Banque Royale’s notes legal tender, good for all debts public and
private. The bank also promised to redeem Compagnie des Indes shares for the going rate of 10,000 livres,
thereby flooding the French economy with money and triggering hyperinflation.
In response, the Banque Royale began lowering the amount it would pay for shares, ultimately to 1,000 livres by
the end of 1720. Meanwhile, lawsuits burgeoned as Law’s growing legion of opponents got the courts to declare
null and void the shares of investors who could not prove they actually owned them—that is, the many investors
who had purchased shares with notes from Banque Générale and Banque Royale prior to the time Law declared
those notes legal tender. Despite the two-thirds reduction in outstanding shares, the value of a single share
continued to drop. By late 1721, they fell back to the 500 livres at which they initially had been offered, in the
process destroying the fortunes of numerous “millionaires.”
Along with a similar scheme in Britain in 1720 known as the South Sea Bubble, the Mississippi Bubble has gone
down in history as one of the great speculative episodes of the early modern era in capitalism. Meanwhile, Law—
though still respected for his economic theories—has gone down in history as a financial charlatan of the first
order, though some economists say his intentions may have been good even if the methods he used were
questionable. As for paper money in France, it was buried for another eighty years. This, argue some economists,
put the country at a financial disadvantage against its main economic and imperial competitor, Great Britain, which
allowed the issue of paper bank notes, though restricting their issue after 1844 to the central Bank of England.
James Ciment
 
See also:  Asset-Price Bubble;  South Sea Bubble (1720);  Tulipmania (1636-1637). 
Further Reading
Fiske, Frank S., ed. The Mississippi Bubble: A Memoir of John Law; to Which Are Added Authentic Accounts of the Darien
Expedition, and the South Sea Scheme. New York: Greenwood, 1969. 
Garber, Peter M. Famous First Bubbles: The Fundamentals of Early Manias. Cambridge, MA: MIT Press, 2000. 
Gleeson, Janet. Millionaire: The Philanderer, Gambler, and Duelist Who Invented Modern Finance. New York: Simon &
Schuster, 1999. 
Mitchell, Wesley Clair (1874–1948)
 
American economist and institutionalist Wesley Clair Mitchell was a leading expert on business cycles and a
founder, in 1920, of the National Bureau of Economic Research (NBER). Mitchell played a leading role in
monitoring and understanding business-cycle activity in the United States.
The second of seven children, Mitchell was born on August 5, 1874, in Rushville, Illinois. His father, a farmer, had
been a doctor in the Civil War. In 1899, Mitchell was awarded a doctorate from the University of Chicago, where

he studied under Thorstein Veblen and John Dewey. He taught at the University of California, Berkeley, and at
Columbia University and was one of the founders and a director of the New School for Social Research in New
York City.
Mitchell is best known for his quantitative studies of business cycles in the United States. His 1913 book Business
Cycles articulated the basic methodology, conditions, and assumptions that he would follow for the rest of his life.
In that work, he examined American business from 1890 to 1911, a period marked by an increase in the accuracy
and frequency of record keeping. Indeed, prior to 1890, business record keeping in America was so meager, that,
as Mitchell noted, there was no real index to indicate whether prices rose or fell. The greater reliance on detailed
statistics to analyze business cycles helped define and describe cycles of boom and bust and what came in
between.
According to Mitchell, the business cycle follows a track from prosperity to crisis to depression, and back to
prosperity. Within each phase, he noted actions and triggers that move the cycle from one stage to the next. For
example, an increase in business activity leads to general prosperity, but as profits rise, costs begin to increase as
well. This leads to declining profits and tightening restrictions on credit. This phase is followed, in turn, by
decreased production, prices, and costs as businesses struggle to remain solvent. The cycle then continues from
depression to prosperity.
Mitchell’s influential work at the NBER included gathering data from hundreds of areas and subjecting it to time-
series studies. These studies—based on the collection of data at relatively uniform points in time—became the
means by which Mitchell and the NBER could identify business cycles, quantitatively measure and explain their
characteristics, and predict future occurrences. Thus, Mitchell’s conclusions were based on empirical rather than
theoretical principles.
In addition to teaching and research, Mitchell served on numerous government committees. He served as chair of
the President’s Committee on Social Trends (1929–1933), as president of the American Economic Association
(1923–1924), and, in 1941, as a member of the original standing committee of the Foundation for the Study of
Cycles.
Mitchell’s book What Happens During Business Cycles, published posthumously in 1951, was described by one
reviewer as a contribution to the study of business cycles, but not a major one. Mitchell’s method, which by then
was being replaced by models, econometrics, and deductive rather than inductive approaches, perhaps had gone
as far as it could go. Mitchell’s method could identify business cycles, but could not give a “Newtonian” set of
rules about them, which became the backbone of modern economic analysis. Nevertheless, Mitchell’s work
continued to have an enormous influence on later economists who studied business cycles, including Simon
Kuznets and Arthur Burns. Mitchell died on October 29, 1948, in New York City.
Robert N. Stacy
 
See also:  Burns, Arthur;  Kuznets, Simon Smith;  National Bureau of Economic Research. 
Further Reading
Burns, Arthur F. Wesley Clair Mitchell: The Economic Scientist. New York: National Bureau of Economic Research, 1952. 
Klein, Philip A.  “The Neglected Institutionalism of Wesley Clair Mitchell: The Theoretical Basis for Business Cycle
Indicators.” Journal of Economic Issues 17:4 (December 1983): 867–899. 
Mitchell, Wesley Clair. The Backward Art of Spending Money, and Other Essays. New York: A.M. Kelley, 1950. 
Mitchell, Wesley Clair. A History of the Greenbacks: With Special Reference to the Economic Consequences of Their Issue,
1862–65. Chicago: University of Chicago Press, 1960. 
Mitchell, Wesley Clair. What Happens During Business Cycles, a Progress Report. New York: National Bureau of Economic

Research, 1951. 
Monetary Policy
 
Governments utilize monetary policy to make changes in the supply, availability, and cost of money and credit. By
effecting such changes, policy makers attempt to stimulate or discourage the consumption of goods and services
by households and firms, in hopes of achieving price stability in times of inflation and economic growth in periods
of slow or negative economic growth.
Monetary policy is usually decided upon and carried out by central banks, such as the Federal Reserve System
(Fed) of the United States, though other agencies and departments in the executive branch, such as Treasury, as
well as legislature, may use monetary policy to effect economic change. While the ability of government to pursue
monetary policy differs from country to country, the Fed has three main tools at its disposal: changing the interest
rates it charges commercial banks; purchasing or selling government securities; and raising or lowering the
reserve requirements of commercial banks.
 Policy Tools
In the United States, all federally chartered commercial banks—along with major state-chartered banks—are
members of the Fed. This arrangement means that they are allowed to borrow funds from the Fed. When the Fed
changes the interest rate it charges member banks, the entire financial system can be affected. This is because
both member and nonmember banks are continually making short-term loans to each other in order to meet
reserve requirements, operational needs, such as restocking automatic teller machines, or sudden payment
demands by other institutions. Banks that have excess reserves can loan them out to those that need to increase
their reserves.
The Fed sets its own rate—known as the “discount rate”—which can also affect the federal funds rate. When the
Fed reduces the discount rate, it often lowers the federal funds rate, since banks will turn to the Fed if they can
get money cheaper there. Conversely, by raising the discount rate, the Fed raises the floor for all federal funds
rates. Reducing target interest rates enables commercial banks to reduce their lending rates to customers. It is up
to the banks, and not the Fed, to determine how much they want to lower those rates, but competition usually
dictates that they stay in line with other banks or risk losing customers. In addition, lower discount rates loosen
credit, thereby making banks less selective about whom they offer credit to.
The second means by which the Fed can affect monetary policy is through the purchase or sale of government
securities, such as long-term Treasury bonds (T-bonds) or short-term Treasury bills (T-bills). (This activity is
conducted by the Federal Reserve Bank of New York, the most influential of the twelve regional Fed banks.)
When the Fed buys government securities, it increases the money supply because those holding the securities
now have cash in hand instead—cash that they can lend out. Conversely, by selling bonds, the Fed takes in cash,
lowering the money supply and tightening available credit. As a guide for its purchase and sale of government
securities, the Fed uses the federal funds rate, the rate at which banks borrow from each other.
Finally, the Fed can affect the money supply by raising or lowering the reserve requirements of commercial banks,
that is, the money a bank must hold against liabilities such as checking and savings accounts. By raising the
reserve requirement, the Fed in effect makes less money available for credit, thereby shrinking the available funds
in circulation.

When the Fed takes on any or all of these measures to increase the money supply, it is said to be pursuing an
expansionary monetary policy; when it takes measures to reduce the supply, it is pursuing a contractionary policy.
When the Fed pursues an expansionary policy, it is making money cheaper and hence more available for
business investment, operations, and hiring, as well as for household spending. Such a policy is usually pursued
during periods of slow or negative economic growth as a means of stimulating the economy. Conversely, the Fed
undertakes a contractionary policy during periods when it fears that accelerated economic growth is triggering or
threatening to trigger excessive wage and price inflation. In general, Fed policy aims to smooth out the business
cycle and achieve sustained economic growth accompanied by low inflation and low unemployment.
Fed monetary policy achieves changes in the economy through a variety of direct and indirect means. By making
money cheaper and more available, it can directly affect consumer demand and business investment. It can even
affect productivity indirectly by allowing businesses to invest in new and more efficient equipment.
The Fed’s interest rate policies can have a major effect on the business cycle, as the events of the late 1970s and
early 1980s make clear. As a result of the energy crisis, sagging productivity, and other factors, the U.S. economy
suffered from both slow or negative economic growth and high inflation during this period. The Keynesian
economic consensus for fighting recession, which advocated fiscal and monetary stimulus, was at a loss, since
such stimulus would exacerbate the already high rate of inflation. Monetarists, on the other hand, argued that the
Fed could return the economy to steady and sustainable growth by slowing the growth rate of the money supply
to what economic growth required. These monetarists argued that inflation was the chief bane, since it created
uncertainties in the markets and stymied savings and investment.
With newly appointed chairman Paul Volcker at the helm, the Fed decided to wring inflation out of the system by
drastically raising interest rates, thereby making it more expensive to borrow and thus shrinking the amount of
money in circulation. With money more expensive, consumers spent less and businesses invested less, and wage
and price gains fell off dramatically. The end result was that inflation was brought under control by the early 1980s
but at the cost of the deepest economic downturn—and highest level of unemployment—of the postwar era. Over
the next two decades, the Fed moved toward attaining a more stable and predictable funds rate as a way to grow
the money supply in response to growth in the real economy.
 Housing Boom
While the decisions undertaken by Volcker in the 1980s to rein in inflation are widely praised, this is not the case
with more recent Fed decisions. Mistakes in monetary policy, as economists understand, can also have major
negative effects on the economy that cannot always be reversed by changing policy. A recent example of this is
the housing bubble of the early and mid-2000s. From late 2001 to late 2004, the Fed set its funds rate at
historically low levels of 2 percent or less, partly to revive an economy hard hit by the dot.com bust and the
recession of 2000–2001. By making money cheaper and more available, the Fed made it easier for people to
obtain low-cost mortgages, spurring a boom in the housing market that drove up prices to unsustainable levels.
But the boom continued even after the Fed raised the funds rate. By this point, the upswing was self-sustaining.
Even though credit became theoretically more expensive and scarce—which normally should have made banks
more careful in their lending—rising house prices reassured lenders that they could always get their money back
should a borrower default. From the perspective of the housing bust years of the late 2000s, many economists
and policy makers have argued that the Fed kept interest rates low for too long a period, thereby failing to burst
the housing price bubble. In doing so, it also created conditions for an inevitable bursting of the housing price
bubble, the single most important factor in the deep recession that hit much of the world economy in 2008 and
2009.
In the wake of the financial crisis triggered by the crash in housing prices, the Fed moved aggressively, agree
most economists, though not without controversy. It lowered the benchmark rate it charged member banks to near
zero, as a way to loosen credit markets and avert a deepening recession. This was a traditional move and did not
meet with much criticism. Perhaps its most controversial move was to provide $85 billion in bailout money to

troubled insurance giant AIG, which had in effect insured many of the derivatives and other complicated securities
at the heart of the crisis, a move Chairman Ben Bernanke justified by pointing to the catastrophic effect the failure
of AIG would have on financial markets worldwide. The Fed would later be criticized for focusing on helping the
biggest financial institutions while ignoring the economic plight of ordinary borrowers. Moreover, the Fed would
also be criticized for steps it did not take, with many in the media and policy-making circles blaming it for
exacerbating the growing financial crisis in late summer of 2008 by not bailing out the major investment bank of
Lehman Brothers.
 International Factors
Monetary policy makers also have to consider international factors in their decisions. And here the currency
system of the country plays an important role. If a nation has a fixed exchange rate—usually against the dollar—
lower interest rates are likely to reduce international capital inflows as lenders shy away from places where their
money brings in lower returns. Lower rates may also produce domestic capital outflows, as financial institutions,
investors, and depositors send their money abroad in search of higher returns. These flows can damage a
country’s balance of payments and reduce foreign currency reserves, as central banks have to supply the foreign
currencies demanded by consumers (to buy foreign goods) and investors (to make foreign investments).
To do this, the bank has to reduce the local currency in circulation—or risk inflation—and thus undermine the
expansionary policy of lowering interest rates it undertook in the first place. The effects of the contractionary
monetary policy are the opposite of the effects of expansionary monetary policy, but again, the economy returns
to the state it was in before the central bank launched its policy. In other words, in a system of fixed exchange
rates, the total effect of monetary policy—both expansionary and contractionary—can be zero, and it is not
effective for influencing the economy.
If a country has a floating exchange rate, then increasing the money supply decreases interest rates but increases
bank lending, consumer spending, imports, and capital outflows. This leads to the depreciation of the local
currency. The value of the domestic currency decreases while the value of foreign currencies increases, because
the demand for foreign currencies increases as they are needed for importing goods and services and investing
abroad. This, in turn, may increase local firms’ competitiveness on foreign markets—as the new exchange rate
causes local goods and services to become relatively cheaper than foreign ones—and once again may lead to
economic growth, increased spending, and higher interest rates. Contractionary monetary policy—decreasing the
money supply—has the opposite effects. In short, if the country’s exchange rate is floating, monetary policy may
be more effective than it would be in countries with fixed exchange rates.
Tiia Vissak and James Ciment
 
See also:  Balance of Payments;  Banks, Central;  Federal Reserve System;  Inflation;  Interest
Rates;  Price Stability. 
Further Reading
Fleckenstein, William A., and Frederick Sheehan. Greenspan’s Bubbles: The Age of Ignorance at the Federal Reserve. New
York: McGraw-Hill, 2008. 
U.S. Monetary Policy: An Introduction. San Francisco: Federal Reserve Bank of San Francisco, 2004. 
Woodward, Bob. Maestro: Greenspan’s Fed and the American Boom. New York: Simon & Schuster, 2001. 

Monetary Stability
 
Monetary stability, one of the central goals of a nation’s monetary policy, means that the value of nation’s currency
—or, in the case of the euro, a region’s currency—remains relatively stable over time, both in terms of purchasing
power and vis-à-vis other national currencies. By achieving monetary stability, governments hope to assure steady
and sustainable economic growth, with low levels of inflation and unemployment.
 Role of Central Banks
In most countries, monetary policy is set by central banks—in the United States, the central bank is the Federal
Reserve (usually referred to as the Fed)—though in some nations monetary policy is controlled by monetary
boards or institutes. In virtually all countries, central banks, while they are government institutions, enjoy relative
autonomy from politics. The reason for this is that politicians, eager to be reelected, might unduly influence central
banks to pursue dangerously expansive monetary policies in hopes of producing short-term surges in economic
activity during election years, even if such policies might endanger the long-term economic health of the nation.
Monetary stability is generally achieved by a set of policy measures undertaken by the central bank of a country.
Central banks usually have the power to set interest rate targets, control discount policy by lending directly to
commercial banks, and set reserve requirements for commercial banks. In the United States, for example, the Fed
has three means to implement monetary policy. It can change the rate it charges member banks—that is, all
federally chartered and many large state-chartered banks—to borrow money, the so-called discount rate. By
charging more, it makes money more expensive to borrow, tightening credit, hiking interest rates, and hence
contracting the amount of money in the economy. Charging less produces the opposite effect. While widely noted
in the media, the discount rate is largely symbolic, indicating the Fed’s overall monetary policy.
The Fed can also increase reserve requirements pertaining to the amount of money commercial banks must hold
—their liabilities—against their outstanding loans, or assets. By increasing the requirement, the Fed makes it more
difficult for banks to lend money, thereby contracting the amount of money in circulation. Again, by doing the
opposite, the Fed indirectly puts more money in the system.
But the most important and effective tool at the Fed’s disposal is its power to buy and sell government securities,
its so-called open-market actions. By selling securities, the Fed, in effect, sops up money; by buying them back, it
releases money into the system. Finally, since the financial crisis of 2008–2009, the Fed and other central banks
have added a fourth weapon to their monetary policy arsenal—the buying up of bank assets and equity. This
latter tool has been used to increase bank liquidity at a time of credit contraction, in the hope that commercial
banks lend more money.
 Historical Federal Reserve Actions
By hiking the discount rate, pushing up reserve requirements, and selling government securities, the Fed attempts
to shrink or slow the growth of the money supply. This is usually done during periods of economic growth, when
the Fed fears that too much investment and spending may lead to too much inflation. In the early 1980s, for
example, with the nation experiencing high rates of inflation, the Fed moved decisively in the above-noted ways.
The effort worked and the consumer price index measure of inflation dropped from around 13.5 percent in 1980 to
3.5 percent in 1985, though at the cost of the highest levels of unemployment and the worst economic downturn
since the Great Depression.
This action was inspired, in part, by the monetarist school of economics—its best-known advocate being Nobel
Prize–winning economist Milton Friedman—which argued that the best way to assure sustained economic growth
was by maintaining monetary stability. Inflation has a crippling effect on economies in two major ways. By creating

uncertainties over future prices and profits, it dampens business investment. At the same time, households tend to
spend more, since they anticipate higher prices in the future. Weak investment and rising demand created a
vicious cycle of inflation that the Fed aimed to stop in the early 1980s.
By making the opposite moves, the Fed attempts to speed up the increase in the money supply. This is usually
done in periods of economic contraction, such as during the post-dot.com recession of the early 2000s, when the
Fed lowered the discount rate from 6.5 percent in May 2000 to 1 percent in June 2003.
While the Fed changes its monetary policy in response to changes in the economy, and in hope of affecting the
economy, the long-term goal is monetary stability, where the money supply grows at the same rate as the
economy, thereby assuring that both inflation and unemployment remain low. In both cases, it should be noted the
goal is not zero inflation or zero unemployment although, according to economists, both would be the desired goal
in an ideal world. There is always some frictional unemployment as people move from job to job. And mild levels
of inflation are generally considered beneficial by economists. This is because wages are “sticky,” as workers and
firms are reluctant to lower them. If there were no inflation, there would have to be no rises in wages, but that is
very unlikely as workers expect to receive more in wages for things like seniority, and firms generally have policies
to meet this desire. If inflation were zero, then firms would have to lay off workers to keep prices absolutely
steady. Monetary stability, then, is about maintaining a growth in the money supply commensurate with economic
growth, and taking into account the fact that mild levels of inflation are necessary.
By achieving monetary stability, central banks also hope to maintain a currency’s value vis-à-vis other major
currencies. By maintaining a steady ratio, the central bank of a country hopes to ensure that the country can
export its goods competitively and borrow money from abroad at reasonable interest rates. If a nation’s currency
becomes too valuable, then the goods it produces become too expensive in comparison to similar goods
produced in other countries. If, on the other hand, the value of the currency falls too much, international investors
will be hesitant to buy that nation’s government securities, forcing that nation’s central bank to raise its interest
rates to attract capital and thereby putting a damper on economic growth.
Finally, in the wake of the 2008–2009 financial crisis, many central banks moved to buy up troubled assets and to
take equity stakes in commercial banks. This was done because many of the latter had large amounts of
mortgage-backed securities, derivatives, and other financial instruments whose value dropped dramatically in the
wake of the U.S. and global housing crisis. By buying these assets, the Fed was directly pumping money into
commercial banks in the hope that they would lend more money, as the sudden and dramatic tightening of credit
in late 2008 threatened to push the global economy into a deep recession, if not depression. Such action was
intended, among other things, to increase the money supply as a means of countering recessionary forces.
James Ciment and Željko Šević
 
See also:  Banks, Central;  Federal Reserve System;  Inflation;  Interest Rates;  Monetary
Policy;  Price Stability. 
Further Reading
Balino, T.J.T., and Carlo Cottarelli. Frameworks for Monetary Stability: Policy Issues and Country Experiences: Papers
Presented at the Sixth Seminar on Central Banking Washington, D.C. March 1–10, 1994, Washington, DC: International
Monetary Fund, 1994. 
Cukierman, Alex. Central Bank Strategy, Credibility, and Independence: Theory and Evidence. Cambridge, MA: MIT
Press, 1992. 
Hetzel, Robert L. The Monetary Policy of the Federal Reserve. New York: Cambridge University Press, 2008. 
U.S. Monetary Policy. An Introduction. San Francisco: Federal Reserve Bank of San Francisco, 2004. 

Monetary Theories and Models
 
Business cycle theory aims at explaining the causes of periodic ups and downs in an economy. Some
explanations center on the use of money in modern industrial economies. Since the use of money pervades
modern economies, the idea that money is deeply involved with industrial cycles might seem obvious. However,
some business cycle theories center on actual goods used in production (industrial equipment and labor) rather
than on the money used in buying and selling real goods.
 Nineteenth-Century Theories
Several prominent nineteenth-century economists developed monetary (money-based) theories of business cycles.
Thomas Malthus explained business cycles in terms of an oversupply of money followed by underspending.
Malthus thought that rising prices in a booming economy cause the total supply of money to expand. Booms could
therefore be self-financing by inducing more money to be injected into the economy. Conversely, Malthus
maintained, booms lead to underconsumption and, due to excessive saving by capitalists, economic busts. By
saving money, he argued, capitalists reduce the demand for goods. Reduced spending on goods (or
underconsumption) will cut profit rates and send the economy into a crash.
John Stuart Mill offered a different monetary theory of cycles. In his view, business cycles are driven by investor
speculation and bank credit expansion. Mill argued that excessive optimism on the part of “rash speculators”
causes banks to overextend credit. Excessive bank credit causes a boom, and the economy appears sound
during the upswing. However, the undue optimism of rash speculators means that their investment plans are
faulty—and that they will ultimately fail. Once speculators start to fail, prices fall, credit contracts, and the economy
goes into a downward spiral. And the crisis will affect more than just speculators. Even sound businesses can be
caught up in it, leaving many workers unemployed.
In the second half of the nineteenth century, Karl Marx proposed yet a third monetary theory of cycles—one also
tightly linked to political forces. According to Marx, the use of money in commerce set capitalism on an unstable
path of booms and busts. Capitalists reap profits by investing money (M) in commodities (C), which are then sold
for even more money (M’). Since M is less than M’, the “capitalist” cycle effectively trades less money for more;
the difference between M and M’ is defined as “surplus value.” Marx maintained that capitalists seek to gain
surplus value by exploiting workers.
There are two important factors in Marx’s theory of exploitation. First, the existence of some unemployment keeps
wages low. And since workers fear becoming unemployed, they will accept wages lower than the value of what
they produce. This loss of workers’ wages is an important source of surplus value that accrues to capitalists.
Second, competition for monetary profit among capitalists causes the rate of profit to decline. Capitalists try to
maintain their rate of profit by investing in more capital—including new technology—but they can only invest in
capital through further exploitation of workers, and there is a limit to how much they can extract from workers.
Once capitalists have pushed the exploitation to its limits, the economy will crash and many capitalists will
become bankrupt. The surviving capitalists will take over capital from those who have failed and start the process
all over again. Thus, the use of money puts capitalism on a boom-and-bust cycle whereby ownership of industry
becomes more concentrated over time.

 Twentieth-Century Theories
Business cycle theories of the nineteenth century generally lacked a sound theoretical basis and were at best
superficially plausible. By 1890, however, basic economic theory had improved and economists were in a position
to construct better business cycle theories.
Modern monetary business cycle theory was founded by the Swedish economist Knut Wicksell in the late 1890s
and early 1900s. Wicksell maintained that the supply of household savings and the demand for business loans for
investment determine the “natural” level of interest rates. Specifically, natural interest rates occur where consumer
savings and business investment (and production) are in balance. In such an economy, Wicksell contended,
steady growth occurs as households buy more of the output that businesses turn out. Wicksell further maintained
that a too-rapid increase in the money supply will drive a wedge between normal consumer saving and business
investment, causing a fundamental imbalance that disrupts industry and leads to economic busts.
According to Wicksell’s critics, the relationship between an economy’s money supply, household savings (and
spending), and business investment is not so simple. Among other things, they argue, Wicksell did not sufficiently
take into account the possibility of inflation. If households buy more while businesses expand to produce more,
then increases in the money supply simply deliver more goods to consumers.
At the same time, however, it is important to remember that industrial production has limits. As businesses and
households all try to spend more, prices will rise as too much money chases too few goods. Inflation results from
excessive spending by households and businesses. Low interest rates would ultimately cause severe inflation,
known as hyperinflation. Conversely, artificially high interest rates reduce demand for loans by entrepreneurs and
cause price deflation, or a general fall in prices and wages.
The twentieth-century Austrian school economists Ludwig von Mises and Friedrich von Hayek took Wicksell’s
ideas a step further by focusing on the complexities inherent in the relationship among interest rates, business
investment, and consumer spending, especially as regards the role of inflation. In the Austrian version of monetary
trade-cycle theory, low interest rates cause business investment and consumer spending to rise together. While
this is good for short-term growth, such high spending will ultimately cause inflation. The only way to stop this
inflation is to raise interest rates by restricting the money supply. But higher interest rates and restricted money
supply will force capitalists to liquidate many of their projects. Mises and Hayek therefore advocated preventive
measures for the trade cycle. Since creating large amounts of new money causes upswings in the economy that
inevitably end in disaster, central banks should be restrained from doing so.
According to the British economist John Maynard Keynes, the main cause of trade cycles has to do with the
demand for money rather than the supply of money. With less spending by households and businesses and with
more money simply being held by speculators, the money supply will circulate at a slower pace. Thus, a general
lack of money demand causes downturns in the business cycle.
 The Great Depression: Keynes versus Hayek
The Great Depression of the 1930s provided economists with a prime battleground on which to test their monetary
theories of business cycles. Hayek, for one, explained business cycles in terms of variations in the money supply.
The fact that the Federal Reserve Bank increased the money supply during the 1920s and pulled back on the
money supply in 1929 supports Hayek’s case. Indeed, most economists initially subscribed to Hayek’s view.
Keynes, meanwhile, explained business cycles in terms of variations in money demand. In his view, crises are
caused by low spending and speculative cash hoarding. According to Keynes, the persistence of the Great
Depression supported that argument. By 1931, the Federal Reserve had tried to lower interest rates to stimulate
investment, but the economy slid further into depression. By the late 1930s, most economists accepted Keynes’s
business cycle theory.
If Keynes is right, the remedy for the trade cycle is for government to boost its spending whenever private

spending falls short. The government can then close the gap between private saving and private investment and
in so doing restore steady circulation of the money supply. Keynes’s followers also believe that the government
can make limited use of its control over the money supply to counteract business cycles.
 Milton Friedman
University of Chicago economist Milton Friedman challenged post–World War II supporters of Keynes, arguing that
business cycles are caused by changes in the money supply. According to Friedman, money demand is relatively
stable and consumption does not rise and fall exactly with current income. When people lose current income, he
argued, they try to maintain most of their planned consumption. They do so by either liquidating past savings or by
borrowing. As the economy slows, consumers will automatically close any gap between saving and investment.
Friedman, like Hayek, blamed businesses cycles on manipulation of the money supply by central banks. His
solution to radical swings in the economy was to grow the money supply at a slow and steady rate. Friedman’s
policy of constant money supply growth might not eliminate the trade cycles entirely, he recognized, but it would
make them less severe.
Although many economists today still accept modified versions of Keynes’s theory, the academic and policy-
making community in the late twentieth century began discounting the role of money in trade cycles. Many insisted
that money has no real effect on the economy and that the business cycle is caused by technological or regulatory
shocks or by problems in labor markets.
 Financial Crisis of 2008–2009
The most recent financial crisis has renewed interest in the original ideas of Keynes and Hayek. Some economists
view the subprime mortgage crisis of 2007–2008 and its economic repercussions as another example of a
Keynesian collapse of private spending. Other economists blame the Federal Reserve for funding the subprime
boom in the years leading up to the crisis. Although economists would continue to disagree as to which monetary
theory of trade cycles is correct, the general idea that money is central to explaining trade cycles was back in
vogue.
D.W. MacKenzie
 
See also:  Austrian School;  Friedman, Milton;  Hayek, Friedrich August von;  Monetary Policy; 
Monetary Stability. 
Further Reading
Friedman, Milton. Studies in Quantity Theory. Chicago: University of Chicago Press, 1956. 
Friedman, Milton. A Theory of the Consumption Function. Princeton, NJ: Princeton University Press, 1957. 
Garrison, Roger. Time and Money: The Macroeconomics of Capital Structure. New York: Routledge, 1999. 
Hayek, Friedrich August von. Monetary Theory and the Trade Cycle. New York: Harcourt, Brace, 1933. 
Hayek, Friedrich August von. The Pure Theory of Capital. London: Macmillan, 1941. 
Keynes, John Maynard. The General Theory of Employment, Interest and Money. New York: Harcourt, Brace, 1936. 
Keynes, John Maynard. A Treatise on Money. New York: Harcourt, Brace and Company, 1930. 
Malthus, Thomas R. Principles of Political Economy: Considered with a View to Their Practical
Application. London: Longman, Hurst, Rees, Orme and Brown, 1820. 
Marx, Karl. Capital: A Critique of Political Economy, trans. Ben Fowkes. New York: Vintage, 1976–1981. 
Mill, John Stuart. The Collected Works of John Stuart Mill, ed. J.M. Robinson. Toronto: University of Toronto Press, 1963–

1991. 
Mises, Ludwig von. The Theory of Money and Credit. London: J. Cape, 1934. 
Sandelin, Bo, ed. Knut Wicksell: Selected Essays in Economics. New York: Routledge, 1999. 
White, Lawrence H. A Theory of Monetary Institutions. New York: Wiley-Blackwell, 1999. 
Money Markets
 
Money markets are financial markets for short-term financial instruments in various countries in which different
types of short-term debt securities, including bank loans, are purchased and sold. Money market instruments are
highly liquid and marketable; companies that would like to have a high degree of liquidity prefer to invest in money
market securities because they are easy to convert into cash.
 Characteristics and Availability
Money market investments are also called cash investments because of their short maturities. Large corporations
or governments need to keep sufficient cash to manage their daily operations. These daily operations comprise
paying back their short-term debts and tax obligations and the ablity to acquire their daily purchases to run their
day-to-day business. As money markets are short term and less risky, their return is lower than the longer-term
debt instruments such as bonds and equities. The difference between the money market and the capital market is
that the money market utilizes short-term debt securities. Money market securities must have a maturity of less
than one year, thus money market instruments are less risky than other financial market instruments. Therefore,
the default risk is very small. Remembering the golden rule of risk and return in finance, the less risky the
investment, the less profitable it is, so their return is relatively small, too.
In sharp contrast to investments made in money markets, investments made in capital markets are long term
(more than one year). Market participants in capital markets are also expected to invest in some money market
securities for their short-term liquidity needs. Money market instruments are traded in large denominations (often
in units of $1 million to $10 million). Because of these high denominations, money markets are not available for
small individual investors. However, individuals can still participate in money markets through their bank accounts
or mutual funds.
Participants prefer to invest in money markets for their requirement of urgent cash and to fight against the
opportunity cost of holding monetary assets such as demand deposits or cash. Even if the interest return is small
in money markets, it is better than keeping excess cash on hand. Keeping cash under a mattress does not have
any return but has an opportunity cost, namely the interest rate offered for that amount in money markets.
Because money market transactions involve relatively large amounts, participants are generally large institutions or
government institutions, such as the U.S. Treasury, central banks (e.g., the Federal Reserve), commercial banks,
brokers, dealers, and large financial and nonfinancial corporations.
 Instruments and Securities
Money market instruments are investment securities that have a maturity of not longer than one year, bear low
credit risk (default risk), and are well known for their ready marketability.

Certificates of deposit (CDs) are debt instruments issued by banks and other depository institutions (savings
associations and credit unions) to investors. In other words, CDs are time deposits at a bank with a specific
maturity date. Like all time deposits, the funds may not be withdrawn on demand. The main advantage of CDs is
their safety and the ability to know what the return will be in advance. As for the disadvantages to CDs, the
money is attached to the maturity of the CD and to withdraw it prior to the maturity date results in a loss for the
investor.
Negotiable certificates of deposit are time deposits issued by banks and other depository institutions whose terms
are negotiable. They are like CDs, but with a secondary market whereby the buyer can resell the security if the
funds are needed before maturity. Thus, if an investor purchases a 90-day negotiable CD and finds out that the
funds are needed in 30 days, there is a secondary market where the CD can be resold to another investor with 60
days remaining, and the original purchaser can get the funds back before the maturity date.
Treasury bills (T-bills) are short-term government securities that are issued with original maturities of one year or
less. T-bills are the most marketable and safest money market securities as they are issued by governments. The
reason behind their attractiveness is their simplicity and safety. Basically, T-bills are used by governments to
collect money from the public. T-bills have a par value (face value), but are sold for a lower price than the par
value; those who buy T-bills make a profit from the difference between the par value and the payment they made
for purchasing the T-bill. The only disadvantage of T-bills is that investors do not earn a very high return because
treasuries are extraordinarily safe and liquid.
The federal (fed) funds market is the financial market where banks and other depository institutions borrow and
lend reserves among themselves. The loans are unsecured and usually overnight. Thus, a bank with excess
reserves can loan them to another bank for interest. Likewise, a bank that is short of reserves can borrow from
another bank. The fact that the Federal Reserve now pays interest on excess reserves may limit the supply of fed
funds since banks can earn interest from the Fed rather than lending their excess reserves to another bank.
Repurchase agreements (repos) are agreements whereby government securities are sold with the simultaneous
agreement to buy the securities back at a higher price on a later date (usually the next day). In reality, the buyer
of the repo has made a loan to the seller of repo and the government security serves as collateral. The difference
between the selling price of the securities and what they are bought back for is the interest on the loan. Hence,
repos are very short-term borrowing instruments backed by government securities. The maturities are usually
overnight but may be up to thirty days or more.
Commercial paper is a short-term, privately issued, and unsecured promissory note issued by a large corporation
to raise short-term cash, most of the time to finance accounts receivable and inventories. Commercial paper has a
fixed maturity of no longer than 270 days and is usually sold at a discount rate from the face value. Face value is
the value of a security written on the security, but when sold at a discount rate, the difference between the face
value and the discounted cost gives the investor his/her income from this transaction. Commercial paper is
considered a very safe investment because the financial situation of a corporation can easily be foreseen for the
next few months. Additionally, the creditworthiness and credit ratings of the corporation are usually very high;
therefore, the investment is not very risky. Commercial paper is frequently issued in denominations of $100,000 or
more. Smaller companies have also found their way into the commercial paper market by getting a backup letter
of credit or guarantee from a bank stating that the bank will pay the investor if the issuer of the commercial paper
defaults.
Bankers’ acceptances are time drafts issued by a bank guaranteeing to a seller of goods that the payment to be
made is guaranteed by a bank.
Eurodollar deposits are U.S. dollar–denominated deposits placed in foreign banks outside the United States. This
market developed in Europe, but the name has nothing to do with the euro or European countries. A eurodollar
deposit is very large in scale and has a maturity of less than six months. Because eurodollar CDs are less
liquid, they are more likely to offer higher earnings in comparison to the other money market instruments. They

can also pay a higher interest rate, because the deposits are not insured (and hence a deposit insurance premium
does not have to be paid) and they are not subject to reserve requirements. Therefore, they are subject to less
costly regulation than domestic deposits would be. Large banks in London have organized an interbank eurodollar
market. This market is now used by banks around the world as a source of overnight funding. The rate paid on
these funds is known as the London interbank offered rate (LIBOR).
Money market deposit accounts are short-term deposit accounts issued by depository institutions (banks, savings
associations, and credit unions) that pay a competitive interest rate. These deposits have limited check-writing
privileges and are insured up to the deposit insurance limit.
Money market mutual funds are shares that collect small sums from individuals and small corporations, pool them
together, and invest them in short-term marketable debt instruments on behalf of the customers. For individuals,
the best way to exist in the money markets is through money market mutual funds or through a money market
deposit account. The development of money markets is unavoidable because the opportunity cost of keeping cash
on hand can be quite high.
While money market funds are considered one of the most secure investments, the financial crisis of the late
2000s shook investor confidence in them somewhat when they experienced severe strains. Money market funds
seek a net asset value of at least one dollar. That is, investors can expect that even in troubled economic times,
their investment of one dollar will return at least one dollar and, ideally, one dollar plus a small gain. But in the
wake of the collapse of Lehman Brothers and the severe crisis that struck the global credit markets in September
2008, two key money market funds fell below one dollar—or, in market parlance, they “broke the buck”—an
unprecedented occurrence. As a result, the Federal Reserve stepped in to allow money market funds to purchase
deposit insurance for their deposits. This was done to avoid a crisis in this market and a run on these funds. To
many analysts, this event was one of the many signs that the financial crisis of 2008–2009 was of a ferocity
unmatched since the Great Depression of the 1930s.
Asli Yuksel Mermod
 
See also:  Banks, Commercial;  Capital Market;  Collateral;  Debt Instruments;  Depository
Institutions;  Financial Markets;  Stock Markets, Global;  Treasury Bills. 
Further Reading
Cecchetti, Stephen G. Money, Banking and Financial Markets. Boston: McGraw-Hill/Irwin, 2006. 
Fabozzi, Frank J., Franco Modigliani, and Michael G. Ferri. Foundations of Financial Markets and Institutions. Upper Saddle
River, NJ: Prentice Hall, 2009. 
Moffett, Michael, Arthur I. Stonehill, and David Eiteman. Fundamentals of Multinational Finance. Boston: Pearson Prentice
Hall, 2009. 
Rose, Peter S., and Sylvia C. Hudgins. Bank Management and Financial Services. New York: McGraw-Hill, 2008. 
Saunders, Anthony, and Marcia Millon Cornett. Financial Markets and Institutions: An Introduction to the Risk Management
Approach. New York: McGraw-Hill, 2007. 
Money, Neutrality of

 
Neutrality of money is a controversial concept in economics that bears directly on economists’ policy
recommendations for controlling economic fluctuations. “Neutrality” refers to the belief that changes in the money
supply effect only nominal economic variables, such as prices, wages, and exchange rates, but have no effect on
the actual level of output—that is, on real values, or prices corrected for inflation. If money neutrality exists, then
efforts to control economic fluctuations through monetary policy will be ineffective, whereas if money is not
neutral, then monetary policy has much greater potential to influence economic indicators such as gross domestic
product and employment.
For example, if the stock of money is increased by 3 percent, then, according to the neutrality of money theory,
prices also should increase by 3 percent, while production levels—and, as a result, the number of employees
needed to produce these amounts—should remain exactly the same. Likewise, if the stock of money is decreased
by 5 percent, then prices and wages also should decrease by 5 percent, and production levels and employment
should not change at all. Some economists argue that a permanent change in the rate of growth of money, not
just in the size of the money supply, likewise has no effect on long-term real output. This concept is called the
superneutrality of money.
Both neutrality and superneutrality of money are based on the assumption that consumers, firms, public
institutions, and others active in the markets for goods, labor, and capital are fully aware of all changes in the
stock of money. Furthermore, these economic actors adjust prices based on what economists call “rational
expectations” about the economy’s future. As a result, real values—that is, prices adjusted for the inflationary
impact of the growing money supply—remain unchanged. Although this scenario may seem far-fetched, it is
supported by some empirical studies.
On the other hand, many economists, in particular those in the Keynesian tradition, maintain that money is not
fully neutral—that is, increases in the money supply may affect real variables. Consequently, monetary policy can
be used to increase output during economic busts by increasing the stock of money and to cool overinflated
booms by decreasing the stock of money. According to these economists, money is not fully neutral because
market players have imperfect information and an insufficient understanding of the consequences of making
changes in the stock of money. After all, in the real world, all people are not trained economists—even if an
individual is informed that additional money has been printed or that banks’ reserve requirements have been
lowered or strengthened, he or she may not fully understand what those changes mean and, consequently, may
not act as rationally as classical economists would expect.
In addition, not only can real variables be affected by changes in the stock of money, but also nominal variables
in a real-world economy may not be responsive to changes in the stock of money. For example, wages tend to be
sticky—it is not easy to lower them, especially if trade unions are strong and unemployment is very low—and
thus, even if the stock of money is reduced, wages do not always drop. Likewise, as all employers are not
enthusiastic to increase wages if unemployment is high and trade unions are weak, wages do not always increase
after the stock of money is expanded. Both phenomena run counter to the concept of neutrality of money.
Moreover, many companies do not make minor upward or downward price changes when the stock of money
changes slightly because of so-called menu costs: they would have to print and attach new price tags, which is
quite time-consuming and costly, especially if they sell a large number of goods (as in a supermarket). In addition,
some prices are also sticky—for instance, during economic recessions, all homeowners do not lower the prices of
their houses, even if the stock of money is reduced considerably. Indeed, some may not even be aware of such a
change.
Research on the neutrality of money is contradictory; the results depend on the country, the level of inflation, and
the length of time studied. Most economists conclude that money is not perfectly neutral, at least not in the short
run, and therefore monetary policy can influence real output and employment.
Tiia Vissak

 
See also:  Gross Domestic Product;  Inflation;  Keynes, John Maynard;  Monetary Policy. 
Further Reading
Hayek, Friedrich A.  “On ‘Neutral Money.’” In Money, Capital, and Fluctuations: Early Essays, ed. Roy McCloughry.  1933.
Chicago: University of Chicago Press, 1984. 
Lucas, Robert E., Jr.  “Expectations and the Neutrality of Money.” Journal of Economic Theory 4:2 (1972): 103–124. 
Saving, Thomas R.  “On the Neutrality of Money.” Journal of Political Economy 81:1 (1973): 98–119. 
Serletis, Apostolos, and Zisimos Koustas.  “International Evidence on the Neutrality of Money.” Journal of Money, Credit and
Banking 30:1 (1998): 1–25. 
Siegel, Jeremy J.  “Technological Change and the Superneutrality of Money.” Journal of Money, Credit and Banking 15:3
(1983): 363–367. 
Tyran, Jean-Robert. Money Illusions and the Strategic Complementarity as Causes of Monetary Non-Neutrality. New
York: Springer, 1999. 
Money Store, The
 
A pioneer in offering subprime home mortgages, the Sacramento-based Money Store became a major lender in
the 1990s, before being purchased by First Union, a North Carolina banking corporation, in 1998 and then shut
down because of sustained losses two years later.
Opened in 1967 by entrepreneur Alan Turtletaub, The Money Store originally focused on providing second
mortgages to persons whose poor credit history made them ineligible for financing by traditional banks. While the
second mortgage offered by the company usually came with interest rates several points higher than the traditional
loan, the funds could be used by mortgage holders to pay off higher interest credit card debt, making such
financing popular.
In 1989, son Marc Turtletaub took over from his father, presiding over the company’s boom years in the 1990s. By
1998, The Money Store had revenues of more than $800 million and some 5,000 employees in offices around the
country. But with success came the inevitable competition. As profit margins declined, the company began to
experience cash-flow problems and started to sell its portfolio of loans to secondary markets to get the money
necessary to lend to new clients. The Money Store bundled the loans and sold them as mortgage-backed
securities.
For a time, the strategy worked, largely through aggressive marketing to win new customers. Some $40 million in
ads in its last years—featuring pitches by baseball icon Phil Rizzuto—made it simple for potential borrowers to
obtain a loan through a national 800 number. With the advent of the Internet in the late 1990s, the company took
its publicity online in a major way, eventually developing an interactive site where the customer could pick the
terms he or she wanted.
But even as the company was selling itself to potential customers, investors in the late 1990s remained wary of
mortgage-backed securities and the company found itself facing liquidity problems. With the Asian and Russian

currency crises of 1997 and 1998 further freezing up the markets for risky securities, the company was forced to
sell itself to First Union for $2.1 billion.
While the infusion of cash from First Union, then the sixth-largest bank in the United States, helped The Money
Store remain solvent, it still sustained losses in 1999 and 2000 as new players entered the subprime and second
mortgage markets. First Union tried to cut costs by halving its workforce, but to no avail. In 2000, First Union
closed down The Money Store, paying out some $1.7 billion in severance pay to employees.
In 2004, former company chairman Morton Dean, now head of MLD Mortgage, purchased the name The Money
Store and reopened it for business as a mortgage financer, with a check-cashing operation that allows people to
get short-term loans, or advances—usually at very high interest—against future paychecks.
John Barnhill and James Ciment
 
See also:  Mortgage, Subprime;  Shadow Banking System. 
Further Reading
“First Union to Acquire Money Store for $2.1 Billion.” New York Times, March 5, 1998. 
“First Union to Shut Down Money Store.” New York Times, June 27, 2000. 
Moral Hazard
 
Moral hazard occurs when people or institutions take greater risks than they ordinarily would because they know
that they are not going to be held fully accountable—or even accountable at all—for the costs of their risky
behavior. Moral hazard is a critical factor in the insurance business, as underwriters must take into account the
fact that the policies they offer may encourage risky behavior. For example, if someone has an auto insurance
policy that provides full coverage for theft, the policy holder may be lackadaisical in locking his or her car. Indeed,
if the owner is interested in replacing the old car with a new one, he or she may even feel the incentive to behave
in a risky fashion.
A similar situation applies to depository institutions with deposit insurance that might act in inordinately risky ways
if they know they (or their depositors) are not going to suffer the full consequences of that behavior. For example,
a bank may be enticed to take on high levels of risk with its depositors’ funds because the bank knows its
depositors will not lose if the investments turn sour. Depositors do not worry about the risky investments the
banks make because they know their deposits are insured. Banks get to keep the higher return if the risky
investments pay off but depositors are shielded from the losses if they do not.
Moral hazard has also been a concern of economists and policy makers for several decades with regard to the
international financial system. Many analysts worry that developed countries are encouraged to borrow recklessly
because they know that the International Monetary Fund (or a government) would step in to rescue them should
they be on the verge of bankruptcy during a financial crisis. Moreover, moral hazard encourages international
banks and other financial institutions to make such loans in the developed world for the very same reason.
The financial crisis of 2008–2009 brought the dangers of moral hazard to the attention of the public at large and

highlighted a type of moral hazard relating to mortgage securitizations. The securitization of mortgages—the
bundling and reselling of mortgages to investors as securities—encouraged greater risk taking among mortgage
originators. Prior to when the securitization of mortgages became widespread, lenders were careful about whom
they offered mortgages to, since they would be responsible should the borrower default. But by passing on the
risk to others, lenders could act in less responsible ways. At the same time, the originators were more aware of
the risks than those assuming them, since the latter did not have specific information about the original
mortgagors. Such moral hazard also applied to the many derivatives taken out against securities investments, as
these derivatives acted like insurance policies on those investments, shifting some of the risk from one set of
investors to another.
Then, in the wake of the financial crisis of late 2008 came the massive $700 billion federal bailout of banks and
other financial institutions—a move replicated by central banks and governments in many other countries afflicted
by the global credit crisis. Many economists have since theorized that major financial institutions acted in riskier
fashion than they should have because they knew that their governments would never allow them to become
insolvent or collapse. They were—to use a contemporary financial turn of phrase—“too big to fail.” In other words,
they knew that their sheer size and involvement in so many aspects of the global financial markets created a
situation in which their failure would create economic and political consequences so dire that no government would
risk such an occurrence. They had more information than the party who would bear the brunt of their risky
behavior—the government and ultimately the taxpayers.
The costs of moral hazard were borne out by the cascade of events in 2008 that led up to the global financial
crisis. When the investment bank Bear Stearns became insolvent in March 2008, the government quickly moved
in to arrange its rescue by offering guarantees to its ultimate purchaser, fellow investment bank JPMorgan Chase.
This, say economists, sent a signal to the markets that the government would come to the rescue should a major
investment bank risk going under.
With the impending collapse of the far bigger Lehman Brothers in September 2008, however, Treasury Secretary
Henry Paulson and Federal Reserve chairman Ben Bernanke were loathe to step in, fearing that they were
encouraging moral hazard throughout the financial sector. As it turned out, the reluctance to rescue Lehman
Brothers sent shock waves through the financial markets and contributed to the crisis of late 2008 and early 2009
that prompted the $700 billion bailout. In other words, according to some economists, fears of encouraging moral
hazard led to a response that greatly exacerbated the financial crisis and pushed the global financial system to the
brink of collapse. In other words, it seemed that the failure of government to bail out Lehman Brothers in order to
discourage moral hazard led to a worse outcome than if the government had bailed out the excessively leveraged
institution, which had taken on excessively high levels of risk.
James Ciment
 
See also:  Risk and Uncertainty;  Troubled Asset Relief Program (2008-). 
Further Reading
Fenton-O’Creevy, Mark, Nigel Nicholson, Emma Soane, and Paul Willman. Traders: Risks, Decisions, and Management in
Financial Markets. Oxford, UK: Oxford University Press, 2005. 
Ferguson, Niall, and Laurence Kotlikoff.  “How to Take Moral Hazard Out of Banking.” Financial Times, December 2, 2009. 
Kamin, Steven.  “Identifying the Role of Moral Hazard in International Financial Markets.” International Finance 7:1 (Spring
2004): 25–59. 
Lane, Timothy, and Steven Phillips. Moral Hazard: Does IMF Financing Encourage Imprudence by Borrowers and
Lenders? Washington, DC: International Monetary Fund, 2002. 
Stern, Gary H., and Ron J. Feldman. Too Big to Fail: The Hazards of Bank Bailouts. Washington, DC: Brookings

Institution, 2004. 
 
Morgan Stanley
 
One of America’s most successful investment banks from the 1930s through 2008, Morgan Stanley was hit hard
by the financial crisis of the late 2000s, as many of its investments in securitized debt obligations lost value and
uncertainty in the markets brought down its share price by nearly half. Nevertheless, Morgan Stanley was one of
two major firms that survived the winnowing of the U.S. investment banking industry during the crisis, along with
Goldman Sachs, though not before transforming itself into a traditional bank holding company with commercial
bank operations.

Morgan Stanley, with headquarters in New York Times Square, survived the financial cataclysm that struck down
so many other U.S. investment firms in 2008, but reconstituted itself as a bank holding company—which meant
tighter federal regulation. (Bloomberg/Getty Images)
Morgan Stanley’s story begins with the Glass-Steagall Act of 1933, which required that the investment and
commercial banking operations of a financial holding company be separated into separate entities. Morgan Stanley
grew out of J.P. Morgan and Company, the wealthiest and most powerful financial institution of the late nineteenth
and early twentieth centuries, operating both investment and commercial banking operations. In the wake of the
1929 stock market crash and amid the Great Depression that followed, many economists and many in the public
came to believe that investment banks and commercial banks should be separated, as the risky behavior of the
former jeopardized the solvency of the latter when they were part of the same company. This, it was argued, had
caused the sell-off in securities values to lead to an overall collapse of the country’s banking system in the early
1930s.
With passage of Glass-Steagall, J.P. Morgan and Co. was forced to split off its investment bank into a new
company, headed by Henry S. Morgan (J.P. Morgan’s grandson) and Harold Stanley, two former partners at J.P.
Morgan. Although Morgan Stanley was largely funded by J.P. Morgan and Co., the split was still in compliance
with Glass-Steagall because the company was issued only nonvoting preference shares in Morgan Stanley. At
least on paper, J.P. Morgan and Co. could not decide the policies or practices of Morgan Stanley.
From the time it opened its doors on September 16, 1935, Morgan Stanley prospered, aided by the fact that it had
acquired much of its parent company’s investment banking business. In its first year, Morgan Stanley handled
approximately one-quarter of all the securities issued in the United States. From the outset, Morgan Stanley was
so successful that it could demand to be the lead underwriter for all of its client offerings. At the same time, the
company benefited from the collegial investment banking atmosphere of the times, whereby rival firms rarely
poached clients from each other.
By the early 1940s, however, competition began to increase, as some clients would request bids from various
investment banks before making a purchase. Still, most of Morgan Stanley’s blue-chip clients continued to find
comfort in the prestige and cachet of the Morgan name. In the 1950s, Morgan Stanley’s client roster was the
envy of Wall Street and included such giants as General Electric, U.S. Steel, and General Motors. The firm did not
need to trade or distribute securities or even resort to advising newly formed companies. This allowed it to avoid
undue market exposure or company risk. Major financial innovations had not yet arrived, and competition was not
overly intense. Issuing securities was a relatively straightforward process, so clients saw little benefit in having the
investment banks fight over business.
The 1960s brought significant changes both at Morgan Stanley—as it expanded into Europe—and in the
investment banking business generally. At the prompting of clients, the industry took on a new aggressiveness,
and firms began to arrange company takeovers. In addition, competitors such as Salomon Brothers and Goldman
Sachs began to gain ground on Morgan Stanley by setting up securities trading departments.
At first, Morgan Stanley looked down on what it regarded as the huckstering of securities. By the 1970s, however,
the company realized that it needed to participate in order to keep up with competitors, so Morgan Stanley set up
trading and sales desks of its own. Securities trading brought increased risk and fundamentally changed the
culture of investment banking. Once dominated by a small group of individuals from top families, investment
banking became more of a meritocracy, with traders from all walks of life soon dominating the business.
The 1970s and the 1980s also saw Morgan Stanley move aggressively into the mergers and acquisitions (M&A)
business. Initially, the M&A department would provide advice to clients interested in acquiring other companies.
But the business proved so lucrative that Morgan Stanley soon began pitching its own ideas to clients in order to
ignite further M&A activity. M&A continued to flourish through the 1980s and into the 1990s, expanding through

the issue of high-risk (“junk”) bonds. Because such activities required considerable capital, Morgan Stanley in
1986 sold 20 percent of its shares to the public.
As with much of the rest of the investment banking community, success followed upon success for Morgan
Stanley through the mid-2000s, though the company lost more than a dozen employees in the terrorist attacks of
September 11, 2001. In 2004, Morgan Stanley handled the highly successful initial public offering (IPO) of the
Internet search engine company Google and traded in the flourishing collateralized debt obligation instrument
market, whereby home mortgages were bundled and sold to investors.
With the meltdown of the subprime mortgage market in 2007, Morgan Stanley found itself forced to write down a
number of bad investments, requiring a $5 billion cash infusion from a Chinese investment company in exchange
for nearly 10 percent equity. The financial crisis of 2008 continued to batter the firm, which saw its share price fall
nearly 50 percent by the early fall. As other “too big to fail” investment banks did exactly that, there was talk that
Morgan Stanley would merge with a major commercial bank. Instead, the company sought to reconstitute itself.
On September 22, 2008, it announced that it would be doing business as a bank holding company, bringing it full
circle to the structure of the original J.P. Morgan back before Glass-Steagall. (The law’s stipulations against
combining investment and commercial banking under one firm had been overturned with passage of the 1999
Financial Services Modernization Act.)
While the new Morgan Stanley would be subjected to the much tighter federal regulation surrounding commercial
banks, directors felt that the benefits outweighed the risks, especially in an era of financial volatility. As a bank
holding company, it would have access to far greater assets and a more diversified business, thereby reducing its
exposure to the risks of being an investment bank only. Morgan Stanley was not alone in making this decision;
Goldman Sachs did the same. With other investment banks having collapsed or been taken over by commercial
banks, the decision of Morgan Stanley and Goldman Sachs to become bank holding companies ended an era of
stand-alone investment banks on Wall Street.
Patrick Huvane and James Ciment
 
See also:  Banks, Investment;  Troubled Asset Relief Program (2008-). 
Further Reading
Chernow, Ron. The House of Morgan: An American Banking Dynasty and the Rise of Modern Finance. New York: Grove
Press, 2010. 
Gordon, John Steele. The Great Game: The Emergence of Wall Street as a World Power, 1653–2000. New York: Simon &
Schuster, 1999. 
Morgan Stanley:  www.morganstanley.com
Morgenstern, Oskar (1902–1977)
 
The German economist Oskar Morgenstern, together with Hungarian economist John von Neumann, is known as
the co-founder of game theory, the branch of applied mathematics that translates situations in the social sciences
into game-like strategic scenarios. Morgenstern and von Neumann co-authored Theory of Games and Economic

Behavior (1944), the first book-length treatment of the subject.
Morgenstern was born on January 24, 1902, in Gorlitz, Germany, to the illegitimate daughter of German emperor
Frederick III. He attended the University of Vienna, Austria, earning a PhD in 1925. He became the director of the
Austrian Institute for Business Cycle Research in 1931 and a professor at the University of Vienna in 1935. Three
years later, he received a Rockefeller Foundation fellowship to study in the United States, where he chose to
remain when the Nazis invaded Vienna and forced his dismissal from the university. He took a teaching position in
economics at Princeton University in New Jersey and soon joined the staff of its Institute for Advanced Study. By
this time, the institute had become home to a number of scholars who had emigrated from troubled Europe,
including Albert Einstein and Morgenstern’s future research partner, von Neumann.
In the kind of collaboration promoted by the institute, von Neumann’s strength was his expertise in mathematics,
while that of Morgenstern—who had already written a book on the science of economic predictions—was
economics. In Theory of Games and Economic Behavior, Morgenstern and von Neumann discussed competition,
utility, domination, and standards of behavior in mathematical terms; explained how economic situations could be
modeled as games; and discussed what could be learned by “solving” these games. The work was revolutionary.
Although World War II was the overwhelming focus of public attention in 1944, the book and its authors received
widespread acclaim and attracted a level of public interest unlike that of any other publication on the mathematics
of economics—and unlike no mathematician other than Einstein.
Theory of Games also reformulated the expected utility hypothesis, which had been originally described in 1738 by
Dutch-Swiss mathematician Daniel Bernoulli. The hypothesis concerned human behavior during gambling and
other situations in which the outcome is uncertain, when the size and probability of risk (the consequence of
losing) and reward (the consequence of winning) influence the likelihood of someone making a wager.
Morgenstern and von Neumann modernized the hypothesis to describe mathematically a rational decision maker
according to four axioms. According to the first axiom, completeness, the decision maker prefers one outcome to
another; the second axiom, transitivity, holds that the individual’s preferences are consistent (for example, if he
likes vanilla better than strawberry and strawberry better than pistachio, then he likes vanilla better than pistachio);
the third, independence, says that adding an equal amount to each of the two outcomes does not change their
order of preference; and the fourth, continuity, says that there is a possibility that a combination of the individual’s
second and third choice will be equal to his first choice. If these four axioms are true, then the individual’s
behavior is rational and can be represented mathematically. The resulting mathematical function takes into
account the individual’s risk aversion; in other words, the suggestion is not that all individuals will make the same
choices, but rather that the traits determining how an individual makes choices can be mathematically modeled.
The expected utility hypothesis has been applied to economics, politics, psychology, and other social sciences.
Researchers have explored its ramifications in such areas as consumer behavior, decisions about medical care,
and investing. Toward the end of his career, Morgenstern explored the economics of defense, among other fields.
He died in Princeton on July 26, 1977.
Bill Kte’pi
 
See also:  Risk and Uncertainty;  Von Neumann, John. 
Further Reading
Henn, R., and O. Moeschlin, eds. Mathematical Economics and Game Theory: Essays in Honor of Oskar Morgenstern. New
York: Springer, 1977. 
Morgenstern, Oskar.  “The Collaboration Between Oscar Morgenstern and John von Neumann on the Theory of Games.”
Journal of Economic Literature 14:3 (1976): 805–816. 
von Neumann, John, and Oskar Morgenstern. Theory of Games and Economic Behavior. Princeton, NJ: Princeton

University Press, 2007. 
Mortgage-Backed Securities
 
A mortgage-backed security (MBS) is a debt instrument whose value is based on a pool of mortgages. Investors
in MBSs buy an undivided share of a pool of specific mortgages. MBSs derive their cash flows from the underlying
mortgages in the pool. The most basic form of MBS simply passes the cash flows from the pool of mortgages to
investors.
The U.S. MBS market was jump-started and has been sustained by the activities of three government-sponsored
enterprises (GSEs): the Government National Mortgage Association (GNMA, or Ginnie Mae), the Federal National
Mortgage Association (FNMA, or Fannie Mae), and the Federal Home Loan Mortgage Corporation (FHLMC, or
Freddie Mac). Fannie Mae and Freddie Mac issue MBSs directly from pools of mortgages they have purchased
from private lenders. Fannie Mae and Freddie Mac are publicly traded, government-sponsored companies, the
stocks of which are traded on the New York Stock Exchange. Their securities were always assumed to have an
implicit guarantee from the government since they were issued by a government-sponsored enterprise, even
though there was no explicit guarantee. This proved to be correct when Fannie Mae and Freddie Mac were put
into receivership by the federal government in late 2008 due to the unprecedented crisis in the mortgage market.
Although shareholders in Fannie Mae and Freddie Mac saw the value of their stock decline, Fannie Mae and
Freddie Mac securities were indeed fully guaranteed by the government. Ginnie Mae, which is part of the
Department of Housing and Urban Development, guarantees the timely payment of interest and principal of MBSs
issued by private lenders who purchase the Ginnie Mae guarantee. The private MBSs with the Ginnie Mae
guarantee are known as Ginnie Mae securities and are backed by the full faith and credit of the U.S. government.
Fannie Mae, Freddie Mac, and Ginnie Mae MBSs make up the core of what is known as the agency/GSE for
MBSs. They were valued at a massive $5.3 trillion on March 31, 2009, compared to $7.5 trillion of outstanding,
publicly held debt owed by the U.S. government on the same date.
 Background
In 1970, Ginnie Mae launched an important financial innovation that was to set the credit and banking markets on
a new path. The innovation was the Ginnie Mae–guaranteed pass-through certificate, issued by private lenders
but with the Ginnie Mae guarantee, commonly referred to as the Ginnie Mae MBS. In 1981, Fannie Mae launched
the Fannie Mae–guaranteed pass-through certificate, which significantly expanded the scale and scope of the
MBS by directly buying mortgage pools from private lenders. Freddie Mac soon followed with its own pass-
through MBS product. GSE involvement greatly expanded the scale of the MBS market and propelled its growth.
Fannie Mae and Freddie Mac expanded the secondary market for mortgages by buying pools of mortgages that
did not qualify for the Ginnie Mae guarantee. Fannie Mae and Freddie Mac have their own underwriting standards
with which borrowers and originators must comply. Fannie Mae and Freddie Mac buy qualifying mortgages—also
called conforming fixed-rate and variable-rate mortgages—from approved originators. Fannie Mae exchanges
MBSs in the form of pass-through certificates for pools of conforming mortgages originated by financial
institutions. Freddie Mac does the same, but its basic guaranteed MBS is called a participation certificate, which
operates like a pass-through certificate. The other option for originators of mortgages that conform to GSE
underwriting standards is to sell the mortgages to Fannie Mae and Freddie Mac for cash. Note that Fannie Mae
and Freddie Mac issue their own securities, called agency securities, to get the funds to buy the mortgage pools
from private lenders. With Ginnie Mae, private lenders sell their own MBSs but with the Ginnie Mae guarantee.

Mortgages that do not conform to the underwriting standards of the two GSEs or qualify for the Ginnie Mae
guarantee are used as collateral for private-label MBSs, issued by private institutions with no government
involvement. Subprime mortgages are a subsegment of the private label market. Not all nonconforming mortgages
are subprime. Loans over a certain limit, called jumbo loans are not subprimes, for instance. In 2009, that limit
was up to $625,250, depending on the area. Underwriting standards constrain how much a homeowner can
borrow, what percentage of the value of a home (called the loan-to-value ratio) the person can borrow, and what
percentage of the person’s income is available to pay interest and principal on the mortgage loan. In addition, the
credit profiles of prospective mortgagors are examined.
The MBS market is important because it facilitates the flow of capital from investors all over the world to families
that would like to finance the purchase of a home. Banks originate mortgage loans that supply the capital needed
by prospective homebuyers. The mortgage loans are then transformed into MBSs by a process known as
securitization. MBSs are sold to investors such as mutual funds, hedge funds, pension funds, corporate treasuries,
insurance companies, banks, and private individuals all over the world. Funds invested in MBSs flow back to the
banks, which can then originate more mortgage loans. Without the ability of financial institutions to securitize and
sell their mortgages, the amount of loans they can make becomes constrained.
Interest payments are determined by taking the product of the contractual monthly interest rate of the mortgage
loan and the outstanding mortgage balance. There are two components of principal payments. The first is
scheduled repayment of principal. This is the amortized amount necessary to pay off the mortgage loan in equal
monthly payments of principal and interest over the life of the loan, typically thirty years (360 months). Note that
as the principal declines each month, the level payment consists of slightly larger amounts of principal and a
smaller amount of interest since the outstanding balance is falling. The other component of principal payments is
unscheduled payments, which come from borrowers who pay off more than is required in a given month so that
the loan is paid off before the final maturity date. When a mortgagor defaults and the foreclosed home is sold by
the lender, the proceeds from the sale count as unscheduled repayment of principal. In the declining real-estate
market of the late 2000s, the sale of foreclosed properties from subprime borrowers who were unable to make
their mortgage payments were often insufficient to cover the outstanding loan balance. The loss must be borne by
the investors in the MBS or by a financial institution that has agreed to guarantee the payments to MBS investors.
 Risks
Mortgage-backed securities expose investors to interest rate risk, prepayment risk, liquidity risk, and credit risk.
When market interest rates increase, such as rates on new mortgages, the value of outstanding MBSs will fall.
This happens because competing newly issued MBSs backed by higher-yielding mortgages will offer higher
interest rates than will outstanding MBSs. The only way investors will be willing to buy the MBS that pays a lower
interest rate is if the price of the MBS declines. Indeed, the price must fall to a level that compensates investors
for the lower income from interest the MBS will generate.
The prepayment risk on MBSs derives from the mortgagor’s option to prepay the loan before it matures. Most
mortgage loans have original maturities of either fifteen or thirty years. The prepayment option gives the
mortgagor the right to prepay all or part of the mortgage principal before its final maturity date, and the
unscheduled principal payment must be distributed to owners (investors) of the MBS. Prepayments also result
when homeowners sell the home and pay off the mortgage. Investors are then forced to reinvest the prepaid
mortgage balance. If these funds cannot be invested in securities that yield the same as the MBS did, then the
investor is worse off due to the mortgagor’s prepayment.
Liquidity risk refers to the possibility that an investor in MBSs or any security cannot sell his or her position for a
reasonable price because few if any competing bids are being offered. In 2007, the private-label market became
extremely illiquid and nearly shut down due to the crisis in the mortgage market.
Credit risk refers to the possibility that the principal value of an MBS is not repaid to investors. This would occur if

the mortgages backing the pool default and the proceeds from foreclosure sales are insufficient to cover the
balance of the mortgage loan.
The fundamental difference between the agency market and the private market for MBSs is how credit risk is
managed and distributed. In the agency market, credit risk is absorbed by the guarantor—Fannie Mae, Ginnie
Mae, or Freddie Mac. The credit risk embedded in a pool of nonconforming mortgages financed with a private-
label MBS is typically shifted onto the investors of the MBSs.
Private-label MBSs generally divide the greater risks of the securities they issue into various classes, depending
on the degree of risk. The simplest case is for one class of MBS to be placed in a subordinated position with
respect to another security that takes a senior position. The subordinated note is designed to protect the investors
in the senior security position from credit losses. As long as losses are below the principal amount of the
subordinated class, the senior class will not expect any losses. For example, a mortgage loan pool of $1 million is
divided into a $900,000 senior class and a $100,000 subordinated class. If $90,000 of mortgage principal is not
repaid, the subordinated class will lose $90,000 (90 percent of its principal) while the senior class will not lose any
principal. This is called the senior/subordinated structure. It enables the senior class of MBSs to attain a very high
credit rating, perhaps AAA. The subordinated class will receive a much lower credit rating, perhaps BBB, because
it is more likely to experience losses of principal. The only way investors are willing to buy a subordinated class of
MBSs is if the security offers a high enough return to compensate for the additional risk. It is possible to pay the
subordinated security a higher yield only if the senior class receives a lower yield. The pool of mortgages backing
an MBS generates only so much interest each period. It can be redistributed but not increased.
Credit ratings are evaluations performed by private companies that assess the quality of securities. The three
principal rating agencies in the United Sates for MBSs are Moody’s, Standard & Poor’s, and Fitch. The rating is a
quantitative measure of the reliability of the security according to the particular agency. Triple A (AAA) is the
highest rating, indicating that the security is unlikely to experience delays or defaults in repayment. D is the lowest
rating, indicating that a security has already defaulted and remains in default.
 Types
The basic MBS, whose cash flow has the same profile as the underlying mortgages, is called a pass-through
security. The pass-through security is the building block of more complex MBSs created by bankers. It is called a
pass-through security or pass-through certificate because the interest and principal payments are passed through
to investors.
Another significant financial innovation expanded the secondary market in mortgage loans. Freddie Mac issued the
first collateralized mortgage obligation (CMO) in 1983, backed by a pool of thirty-year fixed-rate mortgages. A
collateralized mortgage obligation redirects the cash flows (principal and interest) of MBSs to various classes of
investors, thus creating financial instruments with varying prepayment risks and varying returns. A CMO is a multi-
class or multi-tranche issue of MBS to finance a single pool of mortgage collateral. (The terms “tranche” and
“class” are used interchangeably.) Each class of the CMO is a separate security. The classes of securities in a
CMO each have the right to receive a different portion of the monthly interest and principal payments of a pool of
mortgages or pass-through securities. Investors who are most risk averse can choose any instrument wherein the
principal will soon be repaid. Those who are willing to bear more risk can choose an instrument wherein the
principal will not be repaid until later and, hence, is subject to a greater prepayment risk. In exchange for the
greater prepayment risk, the investor receives a higher return. Needless to say, such provisions make attractive
choices available to a wider range of investors. CMOs thus create instruments with varying prepayment risks and
varying returns, so that investors can choose the risk/return combination they are most comfortable with.
On a final note, an interesting phenomenon has occurred in the MBS market due to the financial crisis of 2008–
2009. The outstanding amount of private-label securities has decreased from just over $4.5 trillion at the end of
2007 to just under $3.7 trillion on March 31, 2009. During the same period, Fannie Mae, Freddie Mac, and Ginnie
Mae MBSs increased from just over $4.4 trillion to just under $5.3 trillion—despite the fact that Fannie Mae and

Freddie Mac were put into receivership in September 2008 due to their insolvency. Notwithstanding the
government takeover, they still purchased large quantities of previously held private-label securities, which in
reality is an attempt by the government to lend support to the crisis-ridden mortgage market.
Charles A. Stone
 
See also:  Collateralized Debt Obligations;  Collateralized Mortgage Obligations;  Debt
Instruments;  Housing Booms and Busts;  Mortgage Lending Standards;  Mortgage Markets
and Mortgage Rates;  Mortgage, Subprime;  Recession and Financial Crisis (2007-);  Real-
Estate Speculation. 
Further Reading
 Fabozzi, Frank J., ed.  The Handbook of Mortgage-Backed Securities.  New York:  McGraw-Hill,  2006. 
 Federal Home Loan Mortgage Corporation (Freddie Mac).  “Mortgage Securities Products.” Available at
www.freddiemac.com/mbs/html/product
 Federal National Mortgage Association (Fannie Mae).  “Mortgage-Backed Securities.”
www.fanniemae.com/mbs/index.jhtml?p=Mortgage-Backed+Securities
 Government National Mortgage Association (GinnieMae):  www.ginniemae.gov
 Lucas, Douglas J., Laurie S. Goodman, Frank J. Fabozzi, and Rebecca Manning.  Developments in Collateralized Debt
Obligations: New Products and Insights.  Hoboken, NJ:  John Wiley and Sons,  2007. 
 Spotgeste, Milton R., ed.  Securitization of Subprime Mortgages.  Hauppauge, NY:  Nova Science,  2009. 
 Stone, A. Charles, and Anne Zissu.  The Securitization Markets Handbook: Structures and Dynamics of Mortgage-and
Asset-Backed Securities.  New York:  Bloomberg,  2005. 
Mortgage, Commercial/Industrial
 
Investments in commercial or industrial real estate are normally financed by borrowing. There are many types of
commercial and industrial real estate, including multifamily rental housing, office buildings, retail properties,
warehouses, hotels, medical office facilities, student housing, recreational facilities, and so on. The acquisition of
any of these types of properties involves a “capital stack,” in which a primary mortgage loan rests at the bottom,
the equity of the investors rests at the top, and a mezzanine loan may fill any gap between the primary loan and
equity.
 Leveraging
Real-estate investors use borrowing to increase financial leverage, defined as the ratio of the total investment to
the equity invested. For example, suppose that an investor has $1 million to invest in real estate, and conventional
primary loans are available to finance 60 percent of the purchase price. This means that an investor can purchase
real estate worth $2.5 million with the equity of $1 million (since $2.5 million x 60 percent = $1 million). However,
suppose that mezzanine financing is also available and that the investor finances 20 percent of the total real-
estate investment from this source. Combining this amount with the 60 percent primary loan, the investor now can

borrow 80 percent of the purchase price of the investment property, and therefore can purchase real estate in the
amount of $5 million (since $5 million x 80 percent = $1 million). The total value of the real-estate investment is
sensitive to the degree of leverage in this range. Use of leverage increases the expected return to the $1 million in
equity, provided that money for the mezzanine finance can be borrowed at an interest rate (after tax) that is less
than the after-tax return to equity. However, a greater expected return to equity comes at the cost of greater risk.
The main sources of loans for commercial/industrial real estate are commercial banks, life insurance companies,
and investors in asset-backed securities (pension funds, insurance companies, and other long-term investors). As
of December 31, 2009, the institutions listed in the following table held outstanding commercial mortgage debt of
$2,485.5 billion in the United States.
 Outstanding Commercial Mortgage Debt, United States (June 30, 2011) 
Commercial banks
$1,117.4 billion
Savings institutions
$116.1 billion
Life insurance companies
$255.8 billion
Asset-backed securities holders
$520.2 billion
Other sources
$263.8 billion
Source: Board of Governors of the Federal Reserve System, Flow of Funds Accounts of the United States, Z1,
September 16, 2011, 99.
Primary mortgages for commercial/industrial properties differ substantially from residential mortgages granted to
homeowners. These loans are less standardized, require extensive and detailed documents upon submission of
an application, and often involve a lengthy approval process. No consumer protection laws apply in this loan
market. The loans are of shorter duration—usually 5 to 10 years—and are not fully amortizing (i.e., require a
balloon payment at maturity). Therefore, if the investor intends to hold the property for a longer period, a new loan
must often be obtained. Borrowers face a prepayment penalty with real teeth. Often that penalty is sufficient to
ensure that the yield for the lender is the same whether the loan is held to maturity or paid back before maturity.
This is known as the yield maintenance penalty. Furthermore, because investors in commercial or industrial
property often view defaulting on a loan simply as a business decision based on financial costs and benefits, the
loan-to-value (LTV) ratio is lower than with residential mortgages. LTV is normally 60 to 70 percent for primary
mortgages, but can be as high as 80 percent. Lastly, the ability of the borrower (and the property) to pay the debt
service on the loan is based on a detailed analysis of the income stream that the property can be expected to
generate. This point requires some detailed discussion.
The relevant income stream is the annual cash flow called net operating income (NOI). NOI is the actual gross
income of the property (gross rents and other income sources such as parking fees and so on) minus operating
expenses, local property taxes, leasing expenses, and funds set aside as reserves for replacement or repair of
capital equipment. Estimation of the gross income of the property involves detailed analysis of the existing leases
and their expiration dates. Operating expenses include fixed expenses (e.g., insurance) and variable expenses
(utilities, management fees, janitorial service, and so on). Leasing expenses included commissions to leasing
agents and incentives to tenants (moving allowances, upgrades to their space, months of free rent, and so on). As
defined, NOI has only two uses—debt service and before-tax return to equity. Lenders require that NOI exceed
debt service payments (interest and reductions in principal); debt service coverage ratio (DCR) is defined as NOI
divided by debt service payments. DCR often is set at 1.25 to 1.4, and depends upon the type of property involved
and the amount of risk that is perceived.
Mezzanine loans stand after the primary mortgage loan in priority, and thus are much riskier. Mezzanine loans
involve higher interest rates and substantial fees. As commercial real-estate experts note, while the interest rate

on a first mortgage is usually well below 10 percent, the mezzanine rates can easily rise above 10 percent. A
mezzanine loan sometimes is secured by voting stock in the company of the borrower. The mezzanine loan is
separate from the primary loan, with a separate promissory note, loan document, and collateral. The loan normally
is nonrecourse (the lender cannot sue for any asset beyond the collateral). Prepayment terms are comparable to
those for the primary mortgage loan and are constructed so that the lender’s rights terminate with repayment.
An alternative to borrowing at arm’s length is an equity participation loan. The lender offers a lower interest rate in
return for a share in the income of the property or a share in the appreciation in the value of the property. The
lender receives a portion of the income or appreciation in value if it exceeds some base amount. In effect, the
lender is providing both debt and equity. The larger equity component is shared between owner and lender. An
equity participation loan that involves sharing current income means that the debt service coverage ratio is
increased.
The conduit loan became increasingly important in the United States in the first decade of the 2000s. The conduit
loan is a mortgage that becomes part of a pool of mortgages that is sold as a commercial mortgage-backed
security (CMBS). Underwriting standards are set by the secondary market—the CMBS market—and conduit loans
must be structured and documented for easy sale to the agent that buys up the loans, holds them, and sells
securities based on the cash flows provided by the loans. Those agents are called real-estate mortgage
investment conduits (REMIC) or financial asset securitization investment trusts (FASITs). Rating agencies such as
Standard & Poor’s, Moody’s, or Fitch assign credit ratings to the various classes of bonds. Fannie Mae and
Freddie Mac issue these securities for small residential rental properties, but otherwise the CMBS market is
private.
A simple version of a CMBS would divide the returns to the pool of mortgages equally among those who purchase
shares in the pool. However, since the preferences of investors differ, the returns to the mortgage pool are divided
into “tranches.” The collateralized mortgage obligation (CMO) is a version of the CMBS in which several tranches
are formed with different return characteristics. For example, the top tranche (Tranche A) is of short maturity and
would receive interest, amortization, and any prepayments for a certain number of years. The next tranche
(Tranche B) would be of longer duration and receive only interest payments until Tranche A is paid off, and so on.
Any residual after payments are made to the tranches accrues to the equity interest of the issuer. The CMBS is a
complex instrument, and the market collapsed completely in the financial crisis of 2008–2009.
 U.S. Market Trends
Commercial real-estate markets in the United States experienced rising market values after the recession of 2001,
with an increase in prices of 50 to 60 percent (depending upon the type of real estate) from 2002 to the peak in
2007. Many economists attribute these increases largely to a decline in the capitalization rates applied to the net
income streams of the properties as loans became increasingly available at lower rates.
Commercial real-estate markets in the United States began to experience sharp declines in rents and prices with
the onset of the recession in December 2007. The immediate cause of the recession was the drop in housing
construction that resulted from the end of the housing market bubble—a topic covered in other entries in this
volume. Commercial real-estate prices had been inflated by the easy availability of credit and the attendant high
degrees of leverage. As noted above, many of the loans for commercial real estate remained on the balance
sheets of lenders. Sharp declines in market values reduce asset values in banks and other financial institutions,
placing them in danger of insolvency. Since these loans tend to be of shorter maturities than housing loans, a
major problem of refinancing (or default) of these properties is expected to arise in the years 2011–2015.
The sources of the sharp decline in market values can be examined in a basic model of commercial real-estate
price. The value of a commercial property is estimated by industry professionals as: Value = NOI/Capitalization
rate.
Capitalization rate (or cap rate) is the risk-adjusted cost of capital minus the expected percentage increase in

value over the coming year. Some economists have found that cap rates (for office buildings in downtown
Chicago) depend upon the borrowing rate, the implicit cost of equity, and recent changes in local market
conditions—changes in the vacancy rate and office employment. In other words, market participants were found to
use recent changes in local market conditions as predictors of the changes in market value. An increase in the
vacancy rate of one percentage point was associated with an increase in the cap rate of 67 basis points. The
average capitalization rate in this particular market during 2006–2007 was 6.7 percent. Since then, the vacancy
rate has increased by 2.8 percent, which means that the cap rate has increased by 1.9 to 8.6 percent. Also, NOI
has declined as rents have declined and vacancies have increased.
For example, an office building with NOI of $1 million in 2007 might have had a value of:
$1,000,000/0.067 = $14.93 million. Then suppose that NOI is $900,000. The current value of that same building is
then estimated as: $900,000/0.086 = $10.465 million
This demonstrates a decline of 30 percent. A decline in value of this magnitude likely puts the building
“underwater,” meaning that the building is worth less than the balances on the outstanding loans. Recent reports
indicate that market values for commercial real estate have dropped by larger amounts in other locations. One
study notes a 50 percent decline in office building values in Phoenix, Arizona, for example.
Ultimately, commercial real estate depends upon employment and the overall level of economic activity.
Commercial real-estate markets will recover as the general level of business activity recovers. In the meantime,
property owners and their lenders will undergo the painful process of deleveraging.
John F. McDonald
 
See also:  Fixed Business Investment;  Mortgage Markets and Mortgage Rates;  Savings and
Investment. 
Further Reading
 Bergsman, Steve.  Maverick Real Estate Financing.  Hoboken, NJ:  John Wiley and Sons,  2006. 
 Clauretie, Terrence, and G. Stacy Sirmans.  Real Estate Finance: Theory and Practice.  Mason, OH:  Cengage Learning, 
2010. 
 Downs, Anthony.  Real Estate and the Financial Crisis.  Washington, DC:  Urban Land Institute,  2009. 
 Grubb & Ellis.  “Office Trends Report—Fourth Quarter 2009: Chicago, IL.” Chicago: Grubb & Ellis, 2010. 
 McDonald, John.  “Optimal Leverage in Real Estate Investment with Mezzanine Lending.” Journal of Real Estate Portfolio
Management 13:1 (2007): 1–5. 
 McDonald, John, and Sofia Dermisi.  “Office Building Capitalization Rates: The Case of Downtown Chicago.” Journal of
Real Estate Finance and Economics 39:4 (2009): 472–485. 
 Rudolf, John,  “Phoenix Meets the Wrong End of the Boom Cycle.” New York Times, March 17, 2010. 
Mortgage Equity
 

Mortgage equity, also referred to as home equity, is the value of the unencumbered part of a homeowner’s
mortgaged property—in other words, the market value of the home (including building, land, or other assets)
minus the outstanding balance of the mortgage owed. This is the amount the property owner could expect to
recover if the property is sold; it is sometimes called “real property value.”
Mortgage equity represents hypothetical liquidity—it is not a liquid asset itself—and is considered to have a rate of
return (profit) of zero. Home equity loans, or second mortgages, provide a way of “extracting” liquid equity from the
illiquid equity of a mortgage property. Homeowners may use such loans to cover a variety of expenses, such as
renovations to the property, paying down debts, paying for a child’s college, and other major expenses. Investors
may use home equity loans to extract equity from their properties in order to make other investments with high
enough rates of return to pay the interest on the loan and still make a profit. In such loans, the equity of the
property—rather than its full value—is used as collateral; thus, the amount available to be borrowed will be limited
to the former. Home equity loans are usually for a shorter term than the first mortgage.
Home equity loans are often used to pay down bills that have accumulated over time and that carry significant
interest rates or penalty fees, such as medical bills, student loans, or credit card debts. This strategy, according to
personal finance experts, has both pros and cons. On the one hand, because it is a secured loan where the
lender has a lien on the property, a home equity loan will usually carry a lower interest rate than other interest-
bearing debts, thus effectively reducing the amount of money needed to pay off the debt. Moreover, in the United
States, interest paid on home equity loans is an income tax deduction that reduces tax liabilities. Interest
payments on most other loans, such as credit card and automobile loans, are not. However, home equity loans
are secured recourse debts, meaning that if they are not paid off, the lender can seize (foreclose on) the property
used as collateral. Furthermore, “recourse” means that the borrower is personally liable for the debt if there is
insufficient equity to pay off the loan after the property has been seized by the lender. By contrast, a first mortgage
(as long as the original mortgage has not been refinanced) is typically a nonrecourse debt; thus, foreclosure of the
property may discharge the debt of the initial mortgage (even if the lender does not recoup the full amount owed)
but does not affect any remaining debt on the home equity loan if there are insufficient funds to pay off the home
equity loan issuer. Note also that when a property is foreclosed upon, the first or original mortgage issuer has first
claim on the proceeds from the sale of the property before the issuer of the home equity loan is entitled to be
repaid.
State laws govern how much homeowners can borrow against their property. Most states allow loans of up to 100
percent of the property’s equity; some place the limit lower; and still others permit “over-equity loans,” which
extend a sum greater than the value of the equity serving as collateral. Over-equity loans have become much
less common as banks tightened credit standards in the wake of the subprime mortgage crisis and credit crunch
of 2007–2009. The amortization period of a home equity loan is usually ten to fifteen years if the loan is for the
full amount available; it may be much shorter if a smaller loan is needed. (In the case of emergency home repairs,
a home equity loan is considered by financial counselors to be much smarter than using a credit card, as it carries
a lower interest rate; moreover, the interest expense may be offset by tax benefits.) Shorter amortizations usually
include a balloon payment at the end of term.
Open-ended home equity loans are also called home equity lines of credit—meaning that the borrower can draw
funds on an as-needed basis; these typically carry a variable interest rate. Although they can be useful when the
exact amount necessary is not known in advance, the interest rate will often be higher than that of a traditional
home equity loan. Because the interest rate is variable, it is not possible to “lock in” a favorable percentage when
applying for the credit, as it is with a second mortgage. Whether on a second mortgage or line of credit, a home
equity credit incurs a variety of transaction fees in addition to the interest on the loan, such as appraisal fees,
originator fees, title fees, closing costs, and so forth.
The rate of mortgage equity withdrawal—the amount of equity collectively extracted by means of home equity
loans in a given country in a given period—is a telling macroeconomic statistic that economists monitor carefully.
During the U.S. housing market boom from the 1990s through part of the first decade of the twenty-first century,
for instance, the contribution of mortgage equity withdrawals to total personal consumption expenditures nearly

tripled.
Mortgage equity can also have a significant, if indirect, effect on economic growth. Many economists believe that
rapidly rising U.S. housing values in the early and mid-2000s—which, of course, meant rising levels of equity—
spurred a burst of consumer spending. And since consumer spending represents about two-thirds of all economic
activity, this contributed to the solid economic growth experienced in the United States and many other countries
during this period. There were two reasons for this. First, rising equity levels allowed homeowners to take out
home equity loans, which they could then use to purchase big-ticket consumer items, such as cars and
appliances. Second, many homeowners saw their rising equity levels as a way to finance retirement, meaning that
they could save less and spend more. Of course, when housing prices began to fall in 2007, so too did equity
levels, contributing to a significant drop-off in consumer spending and higher savings rates.
Bill Kte’pi and James Ciment
 
See also:  Consumption;  Housing Booms and Busts;  Mortgage Lending Standards;  Mortgage
Markets and Mortgage Rates;  Mortgage, Subprime;  Recession and Financial Crisis (2007-); 
Savings and Investment. 
Further Reading
Guttentag, Jack. The Mortgage Encyclopedia: An Authoritative Guide to Mortgage Programs, Practices, Prices, and
Pitfalls. New York: McGraw-Hill, 2004. 
Kratovil, Robert, and Raymond Werner. Modern Mortgage Law and Practice. Englewood Cliffs, NJ: Prentice-Hall, 1981. 
Morris, Charles R. The Trillion Dollar Meltdown: Easy Money, High Rollers, and the Great Credit Crash. New
York: PublicAffairs, 2008. 
Mortgage Lending Standards
 
Mortgage lending standards are used by banks and other lending institutions to determine the fitness of a potential
borrower for a prospective loan. The structure of a mortgage loan—its principal, interest rate, amortization, and
term—affects the level of risk the bank is willing to take on, as do the financial health and future prospects of the
borrower, and the effects of the real-estate market on the value of the home that serves as collateral. When
housing prices are on the upswing, lenders balance the risk of a loan by the prospect of recouping or even
profiting through foreclosure. This process and that of collateralization—the bundling and sale of mortgages as
securities—were key in lowering mortgage standards during the housing boom that led to the subprime mortgage
crisis of 2007–2008.
 Risk-Based Pricing
Loan approval is not a binary, “yes” or “no” process. The applicant’s creditworthiness determines not simply
whether or not he or she is approved for a mortgage, but which mortgage terms are approved. Applicants who
present a greater risk but still fall within acceptable credit standards are given less favorable terms—generally
higher interest rates. Sometimes condemned as a predatory lending practice, risk-based pricing is defended by
the banking industry as a compromise measure that makes credit available to those to whom it would not

otherwise be extended.
Subprime mortgages are an obvious example of risk-based pricing. These mortgages were historically offered to
individuals with poor credit ratings but high incomes. Over time, however, especially during the early twenty-first-
century housing boom, the “high income” part of the requirement was increasingly overlooked or disregarded. For
example, NINA loans—No (declared) Income, No (declared) Assets—had been offered to applicants with a high
credit score and some explanation for the lack of income (such as being self-employed or prohibited from
disclosing their compensation), but the housing bubble saw the popularization of NINJA loans—No (declared)
Income, No (declared) Job, No (declared) Assets—which could be repaid only by selling the house.
 Credit Scores
One of the criteria used to evaluate a potential mortgage loan, especially for a residential property, is the
borrower’s credit score. Although awareness of credit scores has been raised in recent years, they are still widely
misunderstood. A credit score is based on a statistical analysis of information in the subject’s credit report. In the
United States, there are three major credit bureaus—Equifax, Experian, and TransUnion—and a number of third-
party scoring systems. The major credit rating bureaus make use of the FICO model, offered since 1958 by a
consumer credit scoring company called Fair Isaac. The information in a person’s credit report includes past
borrowing and credit accounts, bank information, outstanding debts, payment delinquencies, and so on. The
various pieces of information are weighted differently, according to proprietary and secret formulas. While the
consumer can obtain access to his or her credit report, there is no transparency to the credit score itself, nor any
way to reverse it. Subjects cannot learn how many points they may have lost for one reason or another, and there
is no disclosure regarding which pieces of information have been weighted most heavily in calculating the overall
credit score. Specific reasons will be given if and when credit is denied, but the exact scoring mechanism is not
disclosed.
According to Fair Isaac, the FICO score is calculated according to the following broad formula: 35 percent is
punctuality of payment; 30 percent is amount of debt (specifically, how close the subject is to meeting his or her
credit limit); 15 percent is length of credit history; 10 percent is types of credit used; and 10 percent is recent
activity, such as credit card applications. However, each of these elements is based on multiple factors, and the
relative percentages apply only in “typical” cases. In the many atypical cases, the weighting is affected by a
variety of unusual circumstances, such as a court judgment or bankruptcy filing.
For decades, the use of credit scores as a mechanism for determining creditworthiness was praised for being blind
to gender, ethnicity, or an interviewer’s personal preferences. In theory, at least, items that pertain most to credit
risk are weighted most heavily. In practice, credit scoring systems have come under critical scrutiny, especially as
their use has broadened in recent years. Insurance companies have started using credit scores to set premiums
for homeowner insurance, and some employers have begun running credit checks on job applicants as an
indication of character and responsibility. The increasing reliance on credit scores has focused attention on a
variety of problems with the systems of information gathering and calculation. To begin with, creditors do not
always update information in a timely manner, so old debts may continue to be reported after they have been
resolved. In addition, because credit limits can be increased in order to impact the 30 percent debt limit, third-
party agencies sell “credit boosting services” that artificially improve the customer’s credit score by opening an
unusable account with a high credit limit. More importantly, credit scores are simply inaccurate as predictive
measures. Studies indicate that the accuracy of credit scores in predicting whether or not the subject will be
delinquent has been steadily declining since at least the turn of the century.
 Redlining
Redlining is the discriminatory and illegal action whereby financial institutions limit or eliminate the credit they offer
to borrowers in impoverished urban neighborhoods, usually those inhabited by ethnic minorities. Legally,
mortgages cannot be denied on the basis of ethnicity, just as discrimination is outlawed in other contexts. In

practice, however, there is a disproportionate rate of foreclosure in the United States on black-and Hispanic-
owned homes, even relative to those owned by poor whites. Furthermore, the practice of redlining has made it
more difficult for middle-class nonwhites to obtain a mortgage than for lower-class whites. A perennial issue,
pervasive redlining has come to light again in the aftermath of the subprime mortgage crisis. In certain parts of the
country, high-risk subprime mortgages were disproportionately issued in black neighborhoods. Within a certain
median-income bracket, it has been alleged, race was a greater determining factor in the issuance of subprime
mortgages than median neighborhood income. Several class-action discrimination claims have been filed against
lending institutions for discriminatory predatory practices.
Redlining itself began in 1935, when the new Federal Home Loan Bank Board (FHLBB) commissioned color-
coded maps of American cities, with the colors of various neighborhoods indicating relative credit risk. “Declining
neighborhoods,” for example, were outlined in yellow, while affluent suburbs and newly developed neighborhoods
were outlined in blue; those deemed too risky for standard mortgages were outlined in red. These maps were
made by a variety of groups and experts, with no overarching guidance as to the standards that should be used in
determining what distinguishes a creditworthy neighborhood from a risky one. A number of the maps were
constructed with race as a factor in assessing credit risk. In the East and the industrial cities of the Midwest, this
generally meant that black neighborhoods, and many immigrant precincts, were redlined. In the Southwest,
Mexican-American neighborhoods were so identified.
The result was to discourage loans to nonwhite neighborhoods in many parts of the country, contributing
significantly to urban decay and the difficulty of maintaining a healthy nonwhite middle class. The same kind of
discrimination applied to small business loans as well as residential mortgages. A practice analogous to redlining
—not using FHLBB-commissioned maps per se, but still using neighborhood boundaries as indicators of race—
has been alleged in the credit card industry as well, with worse credit terms offered to nonwhites than to whites of
equal income. Realtors are sometimes accused of practicing unofficial redlining by steering prospective
homebuyers toward one neighborhood or another in order to preserve a racial status quo.
Bill Kte’pi
 
See also:  Housing Booms and Busts;  Mortgage-Backed Securities;  Mortgage Markets and
Mortgage Rates;  Mortgage, Subprime;  Recession and Financial Crisis (2007-);  Real-Estate
Speculation. 
Further Reading
Dennis, Marshall W., and Thomas J. Pinkowish. Residential Mortgage Lending: Principles and Practices. Mason,
OH: South-Western, 2003. 
Koellhoffer, Martin. A Mortgage Broker’s Guide to Lending. New York: Mortgage Planning Solutions, 2003. 
Morgan, Thomas A. The Loan Officer’s Practical Guide to Residential Finance. Rockville, MD: QuickStart, 2007. 
 

Mortgage Markets and Mortgage Rates
 
The residential mortgage market in the United States consists of two components: the market in which loans are
originated, called the primary mortgage market, and the market in which loans are bought and sold, known as the
secondary mortgage market. Mortgage loans are originated by commercial banks, savings and loan associations,
savings banks, credit unions, mortgage companies with lines of credit, and mortgage brokers that prepare
applications (but are not lenders). The first four types of originators are depository institutions that make loans to
home-buyers. Mortgage companies with lines of credit are not depository institutions, but have lines of credit with
major financial institutions. Mortgage brokers do not make loans themselves, but work with real-estate brokers
and buyers to prepare loan applications that are forwarded to commercial banks, thrifts, and mortgage companies.
 Primary Mortgage Market
There are many types of mortgage loans available. The standard loan has a fixed interest rate, a long term
(usually fifteen or thirty years), and equal (level) monthly payments. The monthly payment is equal to the principal
of the loan multiplied by a mortgage constant. The formula for the mortgage constant is as follows: MC = r/{1 –
[1/(1 + r)n]}.
In this equation, r is the monthly interest rate and n is the number of time periods (months). For example, if the
annual interest rate is 6 percent and the term is thirty years, then r = 6 percent/12 = 0.005 and n = 360. The
mortgage constant is 0.0059955, and the monthly payment on a loan of $100,000 is $599.55 ($100,000
x.0059955), which is sufficient to pay the interest on the loan and the principal in exactly thirty years. This type of
loan is fully amortized because the principal is paid off over the term of the loan. The first payment is made at the
end of the first month. The interest on the loan for the first month is $500 (0.005 x $100,000 = $500), leaving
$99.55 to go toward reducing the principal of the loan. The borrower enters the second month with a balance of
$99,900.45 ($100,000 – $99.55 = $99,900.45), pays interest of $499.50 (0.005 x $99,900.45 = $499.50), and
reduces the principal by $100.05 ($599.55 – $499.50 = $100) at the end of the month, and so on. The mortgage
constant is the stream of monthly payment for 360 months that has a present value of $1 when discounted at an
annual rate of 6 percent. The mortgage constant increases as the interest rate rises and as the term of the loan
becomes shorter (smaller n). If the term of the loan is infinite, then the mortgage constant is simply the interest
rate, and the principal is never repaid.
One difficulty with the standard fixed rate mortgage is interest rate risk. Financial institutions are in the business of
“borrowing short and lending long.” For example, the major liabilities of a bank are deposits, and most deposits
are held in the form of short-term certificates of deposit (CDs) that mature in six months, one year, and so on. If
interest rates increase, then banks must increase the rates they offer on CDs in order to keep their deposits.
However, the income from their assets—the fixed rate mortgages—will not increase. If the increase in interest
rates is sufficiently large, a bank will discover very quickly that it is losing money because it is paying more for its
liabilities than it is earning on its assets.
Lenders can avoid this problem by offering adjustable rate mortgages (ARMs), which became popular with
borrowers during the housing boom of the late 2000s, as they offer lower initial monthly payments. The interest
rate on an ARM is tied to an index and adjusts after a specified period of time. The lender bears less risk
because the income from its assets will increase as interest rates rise. Because an ARM involves less interest rate
risk for the bank, the initial interest rate is lower than the rate charged for fixed rate mortgages. The standard
terms for ARMs include a periodic adjustment (annual, semi-annual, or monthly, as defined in the contract) of the
interest rate that is tied to an index, such as the cost of funds index provided by the Federal Home Loan Bank, the
one-year Treasury bill rate, or the London Interbank Offered Rate (known as the LIBOR). The contract rate
normally equals the index rate plus a margin that is set at 150 to 275 basis points (i.e., 1.5 percent to 2.75

percent). The terms usually include a cap on the size of the annual adjustment and a cap on the total adjustment
that can occur over the life of the loan. Some ARMs have an initial-period discount (“teaser” rate), during which
time the initial rate is less than the index rate plus the margin. These mortgages switch to the index rate plus the
margin after an initial period of two to three years. Some ARMs include a provision stipulating that the loan can be
converted to a fixed rate mortgage within a certain time period (for a fee). Also, a few ARMs have a cap on the
size of the monthly payment. Such a cap can lead to what is called negative amortization—the size of the
outstanding loan balance increases because the monthly payment is less than the interest on the loan after
adjustment.
Most lenders offer a menu of mortgages that may include fixed rate mortgages with thirty-year terms, with
different combinations of interest rates and discount points; fixed rate mortgages with fifteen-year terms; and
adjustable rate mortgages with annual interest rate adjustments and a lifetime cap. The interest rate is highest on
thirty-year fixed rate mortgages, lower on fifteen-year fixed rate mortgage, and lowest on adjustable rate
mortgages.
Because of the risk of default, lenders must evaluate both the creditworthiness of the borrower and the quality of
the property. A mortgage is a type of contract in which the property serves as collateral for the loan. In the event
that the borrower does not meet his or her payment obligations—that is, the borrower defaults—the lender has the
right to initiate foreclosure proceedings. In the United States, foreclosure procedures are determined by state law,
and differ in procedures, redemption rights, and deficiency judgments. The basic procedure in some states is a
judicial procedure. The lender files in court for a judgment against the borrower. In other states, the seizure and
sale of the property can take place without a court order if the terms of the mortgage include this right for the
lender. In all states, the borrower (mortgagor) has the right, called equitable right of redemption, to prevent a
foreclosure sale by making full payment (including penalties). In addition, some states provide for a statutory right
of redemption in which the mortgagor can regain the property after it has undergone foreclosure sale. This right
has a time limit, and the cost to redeem the property typically is the price paid plus interest and expenses.

Mortgage rates went lower and lower—with more flexible terms and looser qualification standards—during the
housing bubble of the early 2000s. When property values declined, variable mortgages were reset at higher rates
and foreclosures skyrocketed. (Graeme Robertson/Getty Images)
The decision to default is the choice of the mortgagor to exercise a “put” option—that is, the right to sell an asset
at a specified price, in this case the outstanding balance on the mortgage. Mortgagors typically default for two
reasons: a change in their ability to pay, and/or a decline in the value of the property. A household may
experience a decline in income, or may suffer from a family member’s illness or other unexpected difficulty or
expense. These changes may mean that the household is unable to meet its mortgage payments. A lender does
not automatically initiate the foreclosure process, but instead may decide to work with the household to modify the
payment schedule or other terms of the loan. A decline in the value of the property reduces the mortgagor’s
equity. If the decline in the property value is greater than the mortgagor’s equity, the property is worth less than
the amount of the loan, and the mortgagor, who is said to be “underwater,” may decide to default. The most
important variable in the default decision is the original loan-to-value ratio, but other variables, such as a change
in the unemployment rate, also are important. Some mortgages are nonrecourse loans, meaning that the lender
cannot go after the mortgagor’s private assets in the event of a default. Depending on state law, other mortgages
may be recourse loans, meaning that the lender can sue the mortgagor for the mortgage balance in the event of a
default.
Lenders guard against default risk by qualifying the borrower and evaluating the property. When qualifying a
borrower, most lenders compare a household’s monthly housing expenses—which include interest payments,
payments to reduce the principal of the loan, property taxes, and insurance—to its verifiable monthly income. The
ratio should not exceed some amount, usually estimated at 31 percent. Lenders also examine the borrower’s credit

rating, total assets, and other debts. In recent years, borrowers with low credit ratings often were approved for
subprime loans, which carried higher interest rates—and higher risk of default. Lenders also require an appraisal
of the property. The basic appraisal method is to find three comparable properties that are located near the
subject property and that were sold recently. The lender evaluates whether the property is really worth the amount
that the buyer and seller have agreed upon in order to determine the size of the loan that can be approved.
Mortgage default insurance provides protection for the holder of the mortgage. In the United States, the first public
mortgage insurance program was created by the Federal Housing Administration (FHA) in 1934. The borrower
pays an insurance premium that is used to build up the FHA insurance pool. The lender is insured for the full
amount of the loss incurred in the event of foreclosure. The FHA program permits borrowers to obtain loans that
are fully amortizing and that have low down payments and long maturities (thus reducing their monthly payments).
After World War II, the U.S. Veterans Administration (VA) created a mortgage insurance program for military
veterans in which the lender is insured for a portion of the property value, allowing the borrower to make a very
low down payment. Private mortgage insurance is supplied by private firms. The typical policy covers a portion of
the loan when the down payment is less than 20 percent of the price of the property.
 Secondary Mortgage Market
The secondary mortgage market is the market in which mortgages are bought and sold. The market consists of
two parts: a larger portion that is operated by two government-sponsored enterprises, and a smaller portion that is
operated by private firms.
The Federal National Mortgage Association (Fannie Mae) was created as a government agency in 1938 to
purchase FHA-insured mortgage loans and VA loans. In 1968, Fannie Mae became a private company, with the
implicit backing of the U.S. government. Its original purpose was to purchase loans at face value (even if the
current value was below face value) so that lenders could increase lending. Fannie Mae sold the mortgages to
investors when interest rates fell and their values increased. Its activities were financed by issuing bonds, and the
agency relied on the U.S. Treasury to cover its losses. Beginning in 1970, Fannie Mae was permitted to purchase
conventional mortgages as well as FHA and VA mortgages. In the 1970s, Fannie Mae held most of the mortgages
in its portfolio, and issued some mortgage-backed securities. During the 1980s, Fannie Mae issued more
mortgage-backed securities and began to purchase adjustable rate mortgages to reduce interest rate risk.
The Federal Home Loan Mortgage Corporation (Freddie Mac) was chartered in 1970 as a government-sponsored
enterprise with the mission of purchasing conventional, FHA, and VA loans. It specialized in the purchase of
conventional loans. Its initial capital came from the sale of stock to the Federal Home Loan Banks, the federally
chartered system of banks that issues bonds and lends to thrift institutions. Freddie Mac issues a variety of
mortgage-backed securities. Both Fannie Mae and Freddie Mac provide guarantees for the purchasers of their
mortgage-backed securities.
Fannie Mae and Freddie Mac expanded in the 1990s and 2000s, but began to experience large losses in 2007 as
a result of a sharp decline in housing prices that precipitated a massive wave of mortgage defaults throughout the
nation. At that time, the two agencies together owned or guaranteed $5 trillion in home mortgages—approximately
50 percent of all outstanding home loans. By summer 2008, it was clear that Fannie Mae and Freddie Mac soon
would be bankrupt, with negative consequences for the housing market and the entire financial system. In July
2008, federal legislation created the Federal Housing Finance Agency to oversee the two agencies; it placed
Fannie Mae and Freddie Mac into conservatorship in September of that year. This effectively made Fannie Mae
and Freddie Mac federal agencies. The government provides up to $100 billion to each firm to take care of any
shortfalls in capital. Subsequently, the Federal Reserve System initiated a program to purchase the mortgage-
backed securities issued by Fannie Mae and Freddie Mac.
Until 2008, the private portion of the mortgage-backed securities market was driven by major investment banks
such as Goldman Sachs, Morgan Stanley, Merrill Lynch, Lehman Brothers, and Bear Stearns. These financial
institutions were not depository institutions (with deposit insurance from the Federal Deposit Insurance

Corporation), but rather concentrated on underwriting financial instruments such as stocks and bonds for major
corporations and issuers of mortgage-backed securities.
The process of creating an MBS begins when an individual mortgage is funded by a lender, which then sells the
mortgage to one of these major investment banks, or to an intermediary that then sells mortgages to the major
institutions. The mortgages are accumulated and packaged as securities, which are offered to investors such as
pension funds, insurance companies, Federal Home Loan Banks, and other domestic and international financial
institutions.
Mortgage-backed securities can take a variety of forms. The simplest type is the pass-through security, in which
the investor receives a share of the cash flow from the pool of mortgages, which consists of interest payments,
principal payments, and prepaid loans. The mortgages are held by a trustee. A more complex pass-through
security is called a senior or subordinated pass-through. In this case, the senior pass-through has first priority on
cash flows, and the subordinated pass-through is held by the issuer as equity. Mortgage-backed bonds promise
semi-annual payments of principal and interest until maturity. Mortgage-backed bonds have maturity dates that are
considerably shorter than the terms of the underlying mortgages because most mortgages are not held to term.
Collateralized mortgage obligations are the most complex form of mortgage-backed securities. These instruments
restructure the cash flows from a pool of mortgages. The idea of the collateralized mortgage obligation is to
rearrange the cash flows into a number of different securities with different maturities. These different classes are
called tranches; a typical collateralized mortgage obligation has three or four tranches, but may have many more.
Because the cash flows are uncertain, the bottom tranche often is owned by the issuer as equity. An example of a
collateralized mortgage obligation structure is as follows:
 
Assets
Liabilities Maturity Coupon Rate Amount
Mortgages$70 Mil. Tranche A
5–7 yrs.
8.25%
$20 Mil.
Tranche B
7–9 yrs.
8.5%
$20 Mil.
Tranche C
7–10 yrs.
9%
$15 Mil.
Tranche Z
10–11 yrs. 9.5%
$10 Mil.
Equity
$5 Mil.
$70 Mil.
$70 Mil.
Payments to Tranche A have the highest priority and come from interest, principal, and prepayments. Payments in
excess of the coupon rate serve to retire Tranche A. If prepayments are made at a faster rate than expected, the
maturity of the tranche is shortened. Tranche B receives only interest payments until Tranche A has been retired.
Once Tranche A has been retired, Tranche B receives payments that go toward retiring this tranche. This pattern
is followed for the other tranches.
Other forms of collateralized mortgage obligations exist. For example, some tranches are based only on interest
payments, and others are based on principal payments. These tranches are known as interest-only and principal-
only strips.
The Government National Mortgage Association (Ginnie Mae), a U.S. government agency, specializes in providing
insurance for mortgage-backed securities (typically pass-through securities) that consist of FHA or VA loans.
Private institutions create these pass-through securities, known as Ginnie Mae securities, but the securities are
guaranteed by Ginnie Mae in the event of default.

The secondary mortgage market serves several purposes. As noted earlier, traditional mortgage originators such
as banks and thrift institutions face default and interest rate risk when they hold on to the mortgages in their
portfolios. Fannie Mae was created to provide these institutions with a secondary market for FHA loans so that
they could provide more of these insured loans to the public. The secondary market enables capital to flow more
readily from regions with a capital surplus to regions with a capital shortage. And the secondary market provides
investment vehicles for institutions such as pension funds and other institutions that wish to earn good returns in
the long term.
 Crisis in the Secondary Mortgage Market
It is now understood, however, that the secondary mortgage market system that expanded rapidly in the 2000s
was flawed as a result of “asymmetric information,” which can take two forms. Adverse selection occurs before a
transaction is consummated—in this case, it means that borrowers who are bad credit risks seek loans most
diligently. Moral hazard occurs after the transaction takes place, meaning that the borrower engages in behavior
that is undesirable from the lender’s point of view. In both cases, the lender has less information than the
borrower about the borrower’s qualifications and behavior.
Consider the following sequence involved in the creation of a mortgage-backed security: A mortgage broker
prepares an application for a borrower, but earns a fee only if the mortgage application is approved—and earns a
larger fee for higher loan amounts or higher interest rates. Thus, the broker has an incentive to exaggerate the
quality of both the applicant and the property and to push the applicant into a subprime loan with a higher interest
rate. Indeed, there is an incentive for fraud in representing both the qualifications of the borrower and the value of
the property. The mortgage broker has more information than the lender. If the mortgage is approved by the
lender, then the lender has an incentive to convince the issuer of a mortgage-backed security to buy the
mortgage. The mortgage-backed security issuer packages mortgages into a security and pays a rating agency to
give the security a rating of investment grade. The rating agencies face a conflict of interest. Higher ratings make
customers happier, and mean that more business will be forthcoming. Finally, the issuer wishes to sell the
tranches to investors. All of these examples involve asymmetric information with adverse selection. In addition,
once the loan has been granted, the borrower may decide not to make the required payments and to live in the
house until evicted—moral hazard.
These problems became particularly acute in the subprime mortgage market during the early 2000s. This market
provided mortgage loans to households that otherwise would not have qualified for standard mortgages because
of their low or unstable incomes and/or low credit ratings. Many of these mortgages were ARMs with low initial
interest rates that adjusted upward in two or three years. Indeed, a sizable number of these mortgages were
known as variable payment ARMs, in which the borrower could choose to make a payment that was less than the
interest on the loan. Furthermore, the bubble in housing prices that had emerged in 2003 began to deflate in mid-
2006. Housing prices in the United States had increased by 60 percent to 90 percent. Households that had
purchased homes near the end of boom in housing prices soon found that they were underwater (i.e., owed more
than the value of their home). Mortgage defaults increased rapidly in 2007 and 2008. In all, 2.3 percent of all
mortgage loans were delinquent or in foreclosure as of the second quarter of 2006; this percentage increased to
3.1 percent in the second quarter of 2007 and to 5.7 percent in the second quarter of 2008. During this same
time, the percentage of subprime loans that were delinquent or in foreclosure increased from 8.0 percent to 12.0
percent to 22.5 percent.
The sudden increase in mortgage defaults on subprime loans meant that many of the mortgage-backed securities
based on these loans lost value (some became completely worthless). Particularly hard hit were the financial
institutions that retained the equity tranche of the collateralized mortgage obligation. These institutions included
Fannie Mae, Freddie Mac, and the major investment banks, such as Bear Stearns, Lehman Brothers, Merrill
Lynch, and Morgan Stanley. In addition, insurance giant AIG (American International Group) had sold a form of
insurance called credit default swaps to the holders of collateralized mortgage obligations—but failed to hold
enough reserves to cover the losses.

All of these institutions faced bankruptcy in 2008. In September of that year, Bear Stearns was sold to JPMorgan
Chase (after the Federal Reserve provided a $30 billion loan to cover its losses), and Lehman Brothers went
bankrupt. Merrill Lynch was purchased by Bank of America, and Morgan Stanley narrowly escaped bankruptcy.
Fannie Mae and Freddie Mac were put under conservatorship by their federal oversight agency. All of the
problems had come home to roost, creating a massive dilemma for the financial system of the United States and,
indeed, most of the world.
John F. McDonald
 
See also:  Housing Booms and Busts;  Mortgage-Backed Securities;  Mortgage Lending
Standards;  Mortgage, Subprime;  Real-Estate Speculation. 
Further Reading
Barth, James. The Rise and Fall of the U.S. Mortgage and Credit Markets: A Comprehensive Analysis of the Market
Meltdown. Hoboken, NJ: John Wiley and Sons, 2009. 
Clauretie, Terrence M., and G. Stacy Sirmans. Real Estate Finance: Theory and Practice. Mason, OH: Cengage
Learning, 2010. 
Downs, Anthony. Real Estate and the Financial Crisis: How Turmoil in the Capital Markets Is Restructuring Real Estate
Finance. Washington, DC: Urban Land Institute, 2009. 
Mortgage, Reverse
 
A reverse mortgage—also called a lifetime mortgage, reverse equity mortgage, or reverse home mortgage—
enables U.S. homeowners age sixty-two or older who live in their homes for more than six months every year to
create the potential for tax-free income from the value of the homes without selling them, renting them out, or
taking on new monthly mortgage payments. In a reverse mortgage, the payment stream is “reversed”: the
homeowner receives regular monthly payments rather than making them. Reverse mortgages apply to single-
family homes, two-to four-unit properties, condominium units, townhouses, and even mobile homes if they are less
than thirty years old and the homeowner owns the land and pays real-estate taxes.
A reverse mortgage is not a gift, of course, but a low-interest loan against the equity in the home. If the owner
moves out, dies, or sells the property, the loan has to be repaid (the interest is tax deductible) in a single lump
sum one year later; the owner or his heirs may also convert the reverse mortgage into a traditional mortgage in
order to repay the former. If neither is possible, the home may be foreclosed. However, the homeowner will never
owe more than the property is worth, no matter how much its value decreases or how long the borrower lives.
Thus, the owner will never have to use assets other than the home itself to repay the reverse mortgage. (One
exception to this rule is that homeowners have to pay mortgage insurance premiums that reduce the lenders’ risks
in such cases.) Moreover, if the sale proceeds of the home are larger than the amount owed on the reverse
mortgage, the homeowners or their heirs will receive the difference.
The maximum sum homeowners can borrow under a reverse mortgage depends on several factors: their age (or
the age of the youngest spouse in a couple); the appraised home value; the selected program; current interest
rates (fixed or adjustable), usually pegged to U.S. Treasury bonds; the lending limit in the area; and whether or

not there is a mortgage on the property. The sum is larger if the homeowner is older, if the property is a more
valuable home, and if there is already a mortgage on it. If the property is already mortgaged, the owner can often
qualify for a reverse mortgage if he or she has paid off at least 40 percent of the loan or has the funds to do so.
Homeowners can also use the cash advance from the reverse mortgage to pay off the mortgage.
Homeowners can choose from a variety of plans to receive the funds from a reverse mortgage: (1) as a line of
credit—the most popular option, in which borrowers receive the proceeds as installments at times and in amounts
they choose, or as unscheduled payments; (2) as fixed monthly payments—whether for a designated time period
(term) or for as long as they live and occupy the home (tenure); (3) as a combination of 1 and 2, above; or (4) as
a lump sum (though this carries the highest interest fees). Generally, borrowers can change the payment plan at a
later time if they wish.
Homeowners themselves are free to decide how to spend the proceeds from a reverse mortgage—to make other
investments, cover their daily living expenses, repair or modify their home, pay off existing debts, take a vacation,
buy a car, or any other way they choose. As ever, of course, they must still cover all expenses associated with
owning the property, such as real-estate taxes, utilities, and routine maintenance. As long as they pay for these,
they cannot be evicted from the home.
Before applying for a reverse mortgage, homeowners typically go through a forty-five-minute face-to-face
interview or third-party telephone consultation with a counseling firm approved by the U.S. Department of Housing
and Urban Development (HUD) and receive a “certificate of counseling.” This requirement protects consumers and
their family members by ensuring that they understand the different types of reverse mortgages available to them,
the financial consequences of assuming one, and the effects on taxes and Medicaid and other need-based
government assistance payments. For example, these can be affected if the borrower withdraws too much from
the reverse mortgage line of credit—if the total liquid assets at the end of any month are greater than $2,000 for a
single person or $3,000 for a couple.
Many financial counselors advise against reverse mortgages, or at least argue that they should be used only as a
last resort to keep seniors in their homes. There are several reasons for this. Origination fees for reverse
mortgages can be steep, often double those for standard mortgages. If the value of the house—and hence the
owner’s equity—declines significantly, a reverse mortgage can cause their heirs to lose the right to own or live in
the property. (The original mortgage holder may not be evicted, but upon his or her death, the house could
become the property of the lender.) Finally, reverse mortgages might increase the borrower’s income to a level
that makes the borrower ineligible for Medicaid or other need-based social programs.
Tiia Vissak
 
See also:  Mortgage Equity;  Mortgage, Subprime;  Retirement Instruments. 
Further Reading
“Home Equity Conversion Mortgages. Handbook No. 4235.1.” Available at
www.hud.gov/offices/adm/hudclips/handbooks/hsgh/4235.1
“Reverse Mortgage Guides.” www.reversemortgageguides.org
Mortgage, Subprime

 
A subprime mortgage is a home mortgage offered to an individual with low income or with a minimal or weak
credit history—in other words, someone who would not be eligible for a standard, or prime, mortgage. Because
they involve higher risk, subprime mortgages usually come with higher interest rates than standard mortgages.
Lenders charge different interest rates for different subprime mortgages, depending on the income and
creditworthiness of the borrower, as well as the size of the loan and the loan-to-asset ratio (the size of the home
loan compared to the market value of the home being purchased). The degree of risk associated with a subprime
mortgage is rated by letter—A to D in ascending order of risk (descending order of quality).
To make it possible for low-income individuals to afford a mortgage, lenders offer different payment schemes on
subprime loans. In adjustable-rate mortgages (ARMs), the interest rate starts out low and then climbs (or falls)
depending on the index—such as the federal rate to members banks—to which it is linked. In interest-only
mortgages, the borrower defers paying back the principal until a later date. Both of these allow for lower initial
monthly payments before higher interest rates kick in or before a balloon payment against the principal comes
due. During the housing boom of 2003 to 2006, about 80 percent of subprime mortgages issued in the United
States were of the adjustable-rate variety.
Borrowers take out subprime loans either to purchase a home or to refinance one. In the latter case, several
factors may be considered in making the decision. When interest rates are falling, homeowners may decide that
they can reduce their monthly payments by taking out a new mortgage, especially one with an initial low interest
rate. Instead, homeowners may decide to refinance their homes in order to convert their equity into cash—so-
called cash-out refinancing—and use the money for any number of purposes.
Virtually nonexistent prior to the 1990s, subprime mortgages remained a small part of the overall mortgage market
through the end of the millennium. A variety of factors, including low interest rates, the increasing securitization of
mortgages and, most importantly, rising home prices (themselves set in motion, in part, by the proliferation of
subprime lending), encouraged lenders to offer more subprime mortgages during the housing boom that began
roughly in 2003. With the collapse in housing prices beginning in late 2006, however, and especially with the
tightening of credit during the recession and financial crisis of 2007–2009, the subprime mortgage market has
contracted significantly.
 Credit, Income, Ethnicity, and Neighborhood Factors
A number of factors, individually or in combination, may make a mortgagor a candidate for a subprime mortgage.
Most important is the person’s FICO score, a credit-risk rating established by the Fair Isaac Corporation.
Individuals with a FICO score below about 650, on a scale of 300 to 850, are usually required to take out a
subprime mortgage. Factors that go into setting a person’s FICO score include their history of paying back loans
on time (or not), the amount of outstanding debt they owe, the length of their credit history, the types of credit they
use, and their recent borrowing activity.
Even borrowers with a FICO score of above 650, however, may be required to take out a subprime mortgage if
their current debt service-to-income ratio is above 50 percent or if the mortgage itself will push the ratio above
that level. In addition, if the applicant has had a foreclosure, repossession, or judgment within the past two years,
or a bankruptcy within the past five years, they are likely to be required to take out a subprime mortgage. In
addition, even someone with a relatively high income and excellent credit history could be required to take out a
subprime mortgage if the value of the property—and hence the mortgage they are taking out—is so large that the
lender feels it may be an undue burden on the mortgagor’s finances. In general, subprime mortgages do not meet
the standards set for conforming loans by the Federal National Mortgage Association (FNMA, or Fannie Mae) and
the Federal Home Loan Mortgage Corporation (FHLMC, or Freddie Mac), the two government-sponsored
enterprises (GSEs) that insure the bulk of the country’s prime home mortgages.
Finally, such nonfinancial factors as race, gender, and the location of the property may also come into play in

deciding who is required to take out a subprime mortgage—even if lenders who take any of these factors into
consideration are in violation of federal antidiscrimination laws. Beginning in 2007, the National Association for the
Advancement of Colored People (NAACP) began filing lawsuits against about a dozen major financial institutions,
charging that they steered black and Hispanic homebuyers who might otherwise have been eligible for prime
mortgages into subprime mortgages. Legal and discriminatory issues aside, subprime borrowers tend to be poorer
or have sketchier credit histories than borrowers who qualify for prime loans. Thus, subprime lending tends to be
concentrated in marginal neighborhoods or in exurban areas where land costs, and hence housing prices, are
cheaper.
 History
Traditionally, mortgages in American history were hard to come by. Through the 1920s, most people taking them
out were required to put 50 percent down and expected to make a balloon payment on the rest of the principal
after a relatively short period of time. To encourage homeownership, the federal government began to move into
the business of insuring mortgages in the 1930s with the creation of Fannie Mae. However, the Depression and
World War II stifled the mortgage market until the late 1940s. As late as 1940, just 44 percent of American families
lived in homes they owned.
With government-sponsored enterprises such as Fannie Mae and Freddie Mac (after 1970) insuring mortgages,
banks and savings and loans began offering home loans that were more affordable for working-class and middle-
income households. These were generally fifteen-or thirty-year fixed-rate mortgages requiring an initial down
payment of 20 percent of the value of the property. This type of mortgage, along with low-cost loans through the
GI Bill for returning World War II veterans, expanded homeownership dramatically to about 65 percent in 1970. It
remained at that level through the mid-1990s, despite some dramatic fluctuations in housing prices and interest
rates. Meanwhile, the federal government prodded commercial financial institutions to lend to low-and moderate-
income neighborhoods through the Community Re-Investment Act of 1977, making such lending a factor in the
government’s approval of acquisitions, expansions, and mergers.
A more important factor in the origination and growth of the subprime market was, in effect, its legalization. Only
with the easing of usury laws in the early 1980s did it become possible for financial institutions to charge the
higher rates of interest that made subprime mortgages acceptable from a risk point of view. In addition, in 1982,
Congress passed the Alternative Mortgage Transaction Parity Act, which allowed for the variable interest rates and
balloon payments at the heart of most subprime mortgages. On the demand side, passage of the Tax Reform Act
of 1986 allowed for deductions on mortgage interest but not on consumer loan interest. This made even
mortgages with higher interest rates a relative bargain compared to other kinds of loans, and it prompted
homebuyers and homeowners to increase mortgage indebtedness relative to other forms of indebtedness.
Market factors also came into play. In the mid-1990s, interest rates began to rise, which undermined the prime
mortgage market. Lenders, particularly those specializing in mortgages, otherwise known as monoline lenders,
began to offer subprime mortgages as a way to increase business. Much of the capital for this lending was raised
through the sale of mortgage-backed securities (MBSs). But because subprime mortgages were a new kind of
business, many lenders miscalculated profit potential and risk. That, combined with the higher rates of borrowing
in the wake of the Asian financial crisis of the late 1990s, led to a shrinking and then a consolidation of the
business, though both came to be reversed during the housing boom of the following decade.
 Housing Boom
By the early years of the new millennium, the subprime market was on the rebound, especially as housing prices
took off. Between 2000 and 2006, at the height of the housing market boom, outstanding debt on subprime
mortgages more than tripled—from just under $200 billion to about $600 billion, or 20 percent of the overall new
mortgage market. A number of factors fed the growth in subprime lending. Most important was the jump in
housing prices, though here a case of chicken-and-egg causality comes into play. Rising house prices reassured

lenders that they could recoup losses on the inevitable foreclosures that would result from lending to higher-risk
borrowers, allowing them to increase the number of subprime loans. At the same time, by making more people
eligible to finance a home purchase, lenders increased the demand for homes, which contributed to the rise in
prices.
The increase in subprime lending helped push up new housing starts from about 1.4 million annually in the 1990s
to more than 2 million annually by 2005. In addition, the percentage of households living in owner-occupied
housing jumped from its 1970s–1990s average of 64 percent to nearly 70 percent. Meanwhile, housing prices rose
across the nation at an unprecedented pace. During the bull market in housing from 1994 to 1999, the median
home price in America rose from about $130,000 to $160,000 in non-inflation-adjusted dollars, an increase of
about 23 percent. By comparison, during the boom of 2001–2006, the median price shot from about $160,000 to
$250,000, an increase of about 56 percent. The increase was not equal in all areas of the country, of course. On
the West Coast, the urban Northeast, and Florida, home prices skyrocketed as much as 20 percent a year,
doubling or more in the five years from 2001 to 2006.
With home prices rising steadily, lenders felt reassured that they could sell homes financed with riskier subprime
loans should they go into foreclosure; lenders thus began to lower their standards, no longer even requiring
borrowers to provide documentation of their income or their assets, leading to so-called NINA (no-income, no-
assets) loans. There were even increases in the number of so-called NINJA loans, in which the borrower was not
asked for documentation regarding their income, assets, or job. What began to emerge was a kind of race to the
bottom among lending institutions. More conservative lenders found themselves losing business to more
aggressive ones, forcing the former to adjust or risk going out of business.
In addition, the increase in subprime lending was fueled by the securitization process, in which subprime loans
were bundled and sold to investors as financial instruments—mortgage-backed securities (MBSs). Securitization
meant that mortgage originators were no longer financially liable should the mortgages go into default, since the
risk had been transferred to investors. This process, known to economists as “moral hazard,” encouraged
mortgage originators to lend more recklessly. Thus, while securitization of mortgages had existed prior to the
housing boom of the new millennium’s first decade, it became commonplace during this period. In 2000, financial
institutions created an estimated $70 billion in mortgage-backed securities. By 2006, the figure had climbed to
more than $570 billion—an increase of more than 800 percent.
With the rapid expansion of the subprime market and the lowering of lending standards came the inevitable fraud.
This occurred at both ends of the transaction. Lenders claimed that they expected people to report their financial
status honestly even if they were not required to provide documentation—yet NINA and NINJA applications
encouraged deceit. But in the burgeoning subprime market, it was just as likely that borrowers would fall victim to
unscrupulous lenders. Increasingly complex mortgages often came with complicated documentation that hid the
high origination fees, the size of the bump in interest rates, and therefore the increase in monthly payments once
the upfront rate period was over. As long as home prices were rising and credit was flowing easily, this was not a
problem. Mortgagors could readily refinance with a new, lower ARM before the interest rates jumped, since the
rising equity in their homes lowered the default risk for lenders. Moreover, many people who might have been
eligible for prime mortgages were pushed into subprimes because these loans often carried higher generation fees
for the lender.
 Housing Bust
In the end, of course, housing prices could not climb at the unsustainable rate of the period between 2003 and
2006. That rate had risen steadily faster than the rate of inflation and had pushed median home prices to ever-
higher percentages of median income. By late 2006, there were signs that the housing bubble was beginning to
deflate, especially as interest rates climbed. The latter was a result of credit tightening credit by the Federal
Reserve, which was concerned about the size of the bubble and about inflation generally in a booming economy.
As home prices fell and credit markets tightened, mortgage originators began to cut back on lending and increase

documentation requirements. Along with tighter credit—partly a result of financial institutions’ concern about the
value of the MBSs on their balance sheets and therefore their own liquidity—falling home prices put a damper on
consumer spending, which helped push the economy into recession by late 2007. The unemployment rate
doubled from 5 percent in 1997 to more than 10 percent in late 2009.
Joblessness and the inability to refinance sent foreclosure rates skyrocketing across the country, from 240,000 in
the first quarter of 2007 to more than 930,000 in the third quarter of 2009. And subprime mortgages played an
important role in this increase. While subprime ARMs represented just 6.8 percent of all outstanding mortgages in
the United States as early as the third quarter of 2007, they accounted for 43 percent of the foreclosures being
initiated. All of these factors—the decline in home prices, rising joblessness, tightening credit, and securities
valuation questions—led to a near collapse in subprime lending and the market for MBSs, which fell from $570
billion in 2006 to less than $25 billion in 2008. Meanwhile, many housing market experts say that the foreclosure
situation was still a growing concern, as many ARMs would still be reset upward. Higher monthly payments would
be due from people who had either lost their jobs or whose income would not be sufficient, creating a new wave of
foreclosures and a further drag on home prices.
In addition, with house prices falling so dramatically from their 2006–2007 peak, many subprime borrowers found
themselves owing more than their houses were worth. Many homeowners in that situation—said to be “upside
down” or “underwater”—simply abandoned their homes rather than go through the lengthy and costly process of
foreclosure. This was more than a headache for lending institutions, who now found themselves with ever greater
numbers of unsold properties worth less than the loans on their books.
 Foreclosure Mitigation Efforts
To offset the rising level of foreclosures and loan abandonments, the Barack Obama administration initiated a
mortgage foreclosure mitigation plan in early 2009. Mortgagees (lenders) were offered government incentives to
maintain monthly payments at no more than 31 percent of the mortgagor’s income. In addition, the U.S. Treasury
Department increased the capitalization of Fannie Mae and Freddie Mac so that these GSEs could purchase
mortgages worth up to 105 percent of the value of a home.
By late that year, it had become clear that many mortgagees were failing to make the adjustments fast enough,
prompting the administration to announce in late November that it would be cracking down on slow-moving
lenders. Many experts, however, said that upward-shifting ARMs were no longer the main problem facing the
housing finance market. Instead, it was joblessness. According to that view, simply adjusting mortgage payments
to less than 31 percent of income offered little help to someone who had no job and hence no income. Only by
cutting into historically high jobless rates could the government make sure that a mortgage mitigation plan would
achieve its aims of keeping people in their homes.
Meanwhile, other economists began to ask deeper questions about whether homeownership was even a smart
and economically rational choice for lower-income families. This called into question decades of government
efforts to encourage homeownership across the economic spectrum—including the mortgage interest deduction on
personal income taxes.
James Ciment
 
See also:  Housing Booms and Busts;  Mortgage-Backed Securities;  Mortgage Lending
Standards;  Mortgage Markets and Mortgage Rates;  Recession and Financial Crisis (2007-). 
Further Reading
Bitner, Richard. Confessions of a Subprime Lender: An Insider’s Tale of Greed, Fraud, and Ignorance. Hoboken, NJ: John
Wiley and Sons, 2008. 

Chomsisengphet, Souphala, and Anthony Pennington-Cross.  “The Evolution of the Subprime Mortgage Market.” Federal
Reserve Bank of St. Louis Review 88:1 (January/February 2006): 31–56. 
Goodman, Laurie S., Shumin Li, Douglas J. Lucas, Thomas A. Zimmerman, and Frank J. Fabozzi. Subprime Mortgage
Credit Derivatives. Hoboken, NJ: John Wiley and Sons, 2008. 
Muolo, Paul, and Mathew Padilla. Chain of Blame: How Wall Street Caused the Mortgage and Credit Crisis. Hoboken,
NJ: John Wiley and Sons, 2008. 
Shiller, Robert J. The Subprime Solution: How Today’s Global Financial Crisis Happened and What to Do About
It. Princeton, NJ: Princeton University Press, 2008. 
Zandi, Mark. Financial Shock: A 360° Look at the Subprime Mortgage Implosion, and How to Avoid the Next Financial
Crisis. Upper Saddle River, NJ: FT Press, 2009. 
 
Myrdal, Gunnar (1898–1987)
 
Swedish economist and sociologist Gunnar Myrdal is best remembered for his critique of the United States’
“separate but equal” policy in his book An American Dilemma: The Negro Problem and Modern Democracy
(1944). A recipient, with Friedrich Hayek, of the 1974 Nobel Prize in Economic Sciences, Myrdal was also a
successful politician.

Swedish economist and public official Gunnar Myrdal served as a member of parliament, minister of commerce,
and secretary of a UN commission. He shared the 1974 Nobel Prize for “pioneering work in the theory of money
and economic fluctuations.” (The Granger Collection, New York)
Gunnar Myrdal was born on December 6, 1898, in Dalarna County, Sweden. He attended Stockholm University,
receiving a law degree in 1923 and a doctorate in economics in 1927. His thesis on the role of expectations in
price formation, published in 1927, influenced the development of the Stockholm school of economics. He
continued his studies in Germany and Great Britain and was a Rockefeller Fellow in the United States in 1929–
1930, during which time he published The Political Element in the Development of Economic Theory. Myrdal
taught for a year at the Graduate Institute of International Studies in Geneva, Switzerland, before being granted
the Lars Hierta Chair of Political Economy and Public Finance at the University of Stockholm.
In 1934, Myrdal was elected to the Swedish parliament as a Social Democrat. Then, in 1938, he was
commissioned by the Carnegie Foundation in New York to direct a study on race relations in the United States.
Published in 1944, the product of his study, An American Dilemma: The Negro Problem and Modern Democracy,
was one of the first works to expose the gap between the American “ideal” and the grim reality of racial
segregation and discrimination. The controversial publication appeared during an era when racial segregation still
was enforced in the United States under the doctrine of “separate but equal,” derived from the U.S. Supreme
Court’s 1896 decision in Plessy v. Ferguson. Myrdal’s book was said to have had a major influence on the
landmark Supreme Court decision in Brown v. Board of Education (1954), which prohibited racial segregation in
public schools.
Myrdal’s research—often informed by his experiences with Sweden’s welfare state—took into account a variety of
data (sociological, anthropological, and legal) in its examination of the crippling effects of President Franklin D.

Roosevelt’s New Deal policies on the African American population, particularly the minimum wage and restrictions
on agricultural production. Myrdal was among the first to suggest that the minimum wage tended to price the
marginal worker out of the market. He also contended that by giving farmers incentives to cut production in order
to raise prices, the Roosevelt administration actually had caused farmers, particularly in the South, to cut out their
black and white sharecroppers and cash and share tenants, adding to agricultural unemployment.
Myrdal resumed his work in the Swedish government in 1942, chairing the Post-War Planning Commission. A
year after the publication of An American Dilemma, even as the controversy surrounding it continued, Myrdal was
named Sweden’s minister of commerce, serving from 1945 to 1947. He joined the United Nations in 1947 as
executive secretary of the Commission on Europe, a post he held for ten years. In 1957, he directed a Twentieth
Century Fund study of economic trends and policies in South Asia that culminated in the 1968 publication of Asian
Drama: An Inquiry into the Poverty of Nations, which became required reading for would-be development
specialists. In that work, Myrdal contended that in order to solve its economic development problems, Southeast
Asia must control its population growth, institute land reform to broaden ownership, and commit resources to
education and health care.
Myrdal shared the Nobel Prize in Economic Sciences with Friedrich Hayek for “their pioneering work in the theory
of money and economic fluctuations and for their penetrating analysis of the interdependence of economic, social,
and institutional phenomena.” Myrdal died on May 17, 1987, in Danderyd, Sweden. His wife, Alva Reimer Myrdal,
who predeceased him in 1984, won the Nobel Peace Prize in 1982.
John Barnhill
 
See also:  New Deal;  Stockholm School. 
Further Reading
Jackson, Walter. Gunnar Myrdal and America’s Conscience: Social Engineering and Racial Liberalism, 1938–1987. Chapel
Hill: University of North Carolina Press, 1990. 
Lankester, Tim.  “‘Asian Drama’: The Pursuit of Modernization in India and Indonesia.” Asian Affairs 35:3 (November
2004): 291–304. 
Lindbeck, Assar, ed. Nobel Lectures: Economics, 1969–1980. Singapore: World Scientific, 1992. 
 
Nasdaq
 
Nasdaq is one of the largest stock markets in the world, second only to the New York Stock Exchange (NYSE) in
terms of the monetary volume of transactions and the market capitalization of listed firms. In fact, if measured by

the number of firms listed or the number of transactions, Nasdaq is the largest stock market worldwide. More than
3,200 firms, including 335 from 35 foreign countries, are listed on Nasdaq. Since its establishment in 1971, the
market has evolved from an electronic service listing price quotes into a bone fide stock exchange and the main
rival of the NYSE.
“Nasdaq” also refers to a variety of stock-price indices, the most important of which are the Nasdaq Composite, a
value-weighted index of all firms listed and traded on the Nasdaq stock market, and the Nasdaq-100, an index of
the 100 largest firms traded on the Nasdaq by market capitalization. Fluctuations in the former were quite
dramatic during the late 1990s and early 2000s as a result of the dot.com bubble and bust, and during the
financial crisis of 2008–2009. From a base value of 100 when it was launched on February 5, 1971, the Nasdaq
Composite grew to 1,000 by July 17, 1995, and reached a peak of 5,132.52 on March 10, 2000. On October 10,
2002, it fell to a low of 1,108.49. After rising again to over 2,800 in October 2007, then falling again below 1,300 in
March 2009, it had risen above 2,200 by early 2010.
Being listed on the NYSE “Big Board” is more prestigious and more costly than being listed on the Nasdaq. Many
small firms first listed with Nasdaq, then switched to the NYSE when they became large enough. As of 2010,
Nasdaq’s entry and listing fees were approximately $75,000 and $27,500 per year, respectively, while comparable
figures for the NYSE were $250,000 and up to $500,000 per year. In the past, Nasdaq-listed companies tended to
be disproportionately small, young technology companies, but now a wider variety of firms are listed, including
large companies such as Apple Computer, Intel, and Microsoft, which easily could choose to be listed on the
NYSE.
Historically, trades on the NYSE took place at a physical location in New York City, whereas Nasdaq was used by
securities dealers at various locations who were connected only electronically. This lack of a central physical
location slowed the market’s acceptance of Nasdaq as a financial exchange as real as those with trading floors in
historic buildings. In 2000, Nasdaq opened its MarketSite in Times Square. This provides a physical location for
Nasdaq’s public relations events and serves as a center for financial and business news. Its visually impressive
high-tech exterior contrasts markedly with the traditional look of its cross-town rival, the NYSE.
Nasdaq, established in 1971, is the second most active equities trading market in the United States after the New
York Stock Exchange. The world’s first exchange to conduct trading electronically, Nasdaq is relatively heavy in
technology stocks. (Bloomberg/Getty Images)

 Origins
Nasdaq was an outgrowth of the National Association of Securities Dealers (NASD), which was established in
response to the Great Depression. In the aftermath of trading abuses that contributed to the stock market crash of
1929, the Franklin D. Roosevelt administration wished to increase oversight of the financial markets. But effectively
regulating smaller, geographically dispersed markets posed an even greater challenge than centralized exchanges
such as the NYSE. Self-regulation was seen as a viable option, and in 1939, the NASD was founded in response
to amendments to the Securities Exchange Act of 1934.
The NASD was given the authority to regulate trading in equities, corporate bonds, securities futures, and options,
and to license individual members and write rules governing their activities. It also examined members and was
sanctioned by the U.S. Securities and Exchange Commission to discipline those that failed to comply with
securities laws or with the NASD’s rules and regulations.
On February 8, 1971, the NASD launched a method for viewing real-time quotes for “over-the-counter” stocks
transacted, called the National Association of Securities Dealers Automated Quotation (NASDAQ) system. This
was the first time dealers across the country could see real-time quotes for the over-the-counter securities that
they bought and sold. This was not really a stock market, but rather a sophisticated information resource.
Originally, only select “market makers” could use the system to trade securities, and until 1987, most NASDAQ
trading occurred by telephone. During the October 1987 stock market crash, market makers often refused to
answer their phones. This provided the impetus to expand access to screen-based trading and to develop the
Small Order Execution System, which provided a method for brokers and dealers to enter into contractually
binding trades. Over time, more and more securities dealers purchased dedicated terminals to engage in
transactions and to view electronically disseminated screen-based quotes. The development of this system
predated, and likely had a significant influence on, the development of the Internet.
The NASD’s ability to regulate the Nasdaq impartially was a concern from the start. In the early 1990s, prices still
were quoted in eighths. Academic researchers focused attention on the fact that very few prices were quoted in
“odd eighths.” Prices listed in fourths were much more common than 1/8, 3/8, and so on. This led to greater
scrutiny of the wide spreads between bid and asking prices, as well as other practices that seemed to benefit
dealers and brokers at the expense of their customers. The NASD’s response, which seemed to be directed at
protecting members rather than customers, highlighted the conflict of interest in having members regulate
themselves. By 2001, the NASDAQ quotation system had become the Nasdaq stock market. The NASD divested
its ownership interest in the Nasdaq but continued to act as the market’s regulator. In July 2007, the NASD
merged with the NYSE’s member regulation, enforcement, and arbitration functions to form the Financial Industry
Regulatory Authority, or FINRA, Nasdaq’s current regulator.
 First Electronic Stock Market
Since its inception, Nasdaq has been in direct competition with the NYSE. Over time, the two stock markets have
become more similar, but differences still remain. Nasdaq began as a “dealer market” in which market participants
buy and sell multiple orders to “market makers.” In contrast, the NYSE is described as an “auction market” in
which buyers and sellers typically transact with one another, with the highest bidding price matched to the lowest
asking price. On the NYSE, each security has one market maker who acts as an auctioneer and may or may not
act as a principal, buying or selling securities out of inventory. On the Nasdaq, each security has multiple market
makers, and, in theory, competition between them should lower transaction costs.
The NYSE has responded to the Nasdaq challenge by purchasing technologically advanced competitors and
automating transactions. Nasdaq, in turn, has responded by developing a physical presence and burnishing its
brand name, while touting its ability to trade in NYSE-listed stocks through its broker-dealer, Nasdaq Execution
Services.
 Demutualization, Acquisitions, and Future Direction

Three fundamental forces have had a great impact on stock markets: demutualization, diversification, and
globalization. Nasdaq (and the NYSE) are excellent examples of the impact of these forces. From its origins as an
over-the-counter market with transactions made and information disseminated between individual dealers over the
phone, Nasdaq evolved beyond a sophisticated electronic bulletin board to become the world’s most advanced
online trading system. It continually added to the information reported by its automated systems and increased the
speed at which transactions can be consummated, but still lacked an impressive physical location. In 1992,
Nasdaq joined with the London Stock Exchange to link securities markets across the continents for the first time.
In 2000, NASD members voted to demutualize, and Nasdaq became an investor-owned corporation.
There were several reasons for this decision. First, it would permit Nasdaq to merge and make cooperative
arrangements with other exchanges more readily. Second, the additional capital derived from a public offering
would allow the system’s infrastructure to be upgraded. Third, the change in ownership structure would eliminate
the conflict of interest between the long-term interests of the exchange (a faster, more direct, and technologically
advanced trading platform) and the short-term interests of its members (to continue a system that would require
use of brokers for transactions and allow them to maintain their informational advantage, with an ability to benefit
from the spread between bid and ask prices). Going public would allow member and owners to benefit from
changes that would increase the stock price of the Nasdaq but reduce their trading profits. Shares of Nasdaq
ownership began trading publicly on July 2, 2002.
In 2005, Nasdaq increased its ownership in the London Stock Exchange to just under 30 percent, but was
thwarted in its effort to obtain a controlling interest. In 2007, Nasdaq purchased old, established, but small regional
exchanges in Philadelphia and Boston, and in 2008, it purchased the financial company that controls seven Nordic
and Baltic stock exchanges, OMX, forming the NASDAQ-OMX Group. In this same year, Nasdaq formed a
strategic alliance with the large and dynamic Middle Eastern exchange, Bourse Dubai Ltd., and purchased a one-
third stake.
In 1998, the NASD merged Nasdaq with the American Stock Exchange (AMEX) to form the Nasdaq-Amex Market
Group. AMEX’s historically important trading location in Manhattan near Wall Street may have been a factor. But
AMEX remained an independent entity under the NASD parent company, and conflicts arose between the two. In
2004, NASD spun off AMEX, allowing its members to purchase independent ownership, thereby ending its
relationship with Nasdaq. (In October 2008, AMEX was purchased by NYSE Euronext, itself a merger of the
NYSE and a consortium of European exchanges.) Although mergers have reduced the number of exchanges,
competition seems to have intensified among those that remain, reducing fees and spurring advances in
technology and international linkages.
Bruce Brown
 
See also:  Dot.com Bubble (1990s-2000);  Information Technology;  Stock Markets, Global; 
Technological Innovation. 
Further Reading
“Battle of the Bourses.” The Economist, May 25, 2006. 
Burton, Maureen, Reynold Nesiba, and Bruce Brown. An Introduction to Financial Markets and Institutions.  2nd ed. Armonk,
NY: M.E. Sharpe, 2010. 
Ingebretsen, Mark. NASDAQ: A History of the Market That Changed the World. Roseville, CA: Forum, 2002. 
NASDAQ.  www.nasdaq.com.
Wright, Russell. Chronology of the Stock Market. Jefferson, NC: McFarland, 2002. 

National Bureau of Economic Research
 
The National Bureau of Economic Research (NBER) is a private, nonprofit organization that undertakes a broad
array of economic and financial analyses. It operates in a number of program areas, one of which focuses
specifically on economic fluctuations and growth. The NBER has had an important role in the dating and
understanding of business cycles through data monitoring and analysis for almost ninety years.
The NBER was founded in January 1920. It is located in Cambridge, Massachusetts, but also has branch offices
in New York City and Stanford, California. The NBER has seven officers, including, as of this writing, its chairman,
John Clarkeson, and president and chief executive officer James Poterba. In addition, it has 25 directors-at-large,
15 directors by university appointment, 11 directors by appointment of other organizations, and 10 directors emeriti.
The NBER is the leading private nonprofit research organization in the United States: its roster of contributing
experts includes 16 of the 31 American Nobel Prize winners in economics, including Paul Krugman who won the
Nobel in 2008, as well as earlier Nobel winners Joseph E. Stiglitz, Robert E. Lucas, Jr., Milton Friedman, and
Simon Kuznets. Six of the past chairmen of the president’s Council of Economic Advisers either have worked or
are currently working as research associates there. More than 1,000 university professors, most of them
specializing in business or economics, are also part-time NBER researchers at the same time.
Since its foundation, the NBER has followed five principles: (1) concentrate on facts important for dealing with
major economic problems and the connections between them; (2) focus on the collection and analysis of
quantitative data, but do not ignore qualitative data; (3) follow scientific principles; (4) stay impartial in presenting
results and inform the public of this impartiality; (5) and do not make policy recommendations. For these reasons,
and to avoid even a hint of partiality, the NBER has sought financial support from a wide range of sources—
including public and private institutions, firms, and individuals, all with diverse ideologies.
The NBER disseminates its economic research among businesspeople, academics, and policy makers. It has
studied various issues: national income and its distribution, savings, expenditures, prices (including stock prices
and exchange rates), interest rates, wages, employment, pensions, education, health, migration, production,
energy, credit, bank assets, financial instruments, business cycles and their measuring and forecasting methods,
and the economic and political environment. The NBER publishes time series and other macro, industry,
international trade, hospital, demographic, and patent data. (For instance, it has calculated the U.S. Industrial
Production Index with the earliest value from 1790 and U.S. foreign trade data from 1879.) It has published annual
reports, working papers, books and conference proceedings, the NBER Digest (summarizing four or more most
recent newsworthy NBER working papers), the NBER Reporter (providing reviews of the NBER’s research and
activity, including reviews of recent NBER conferences and a list of recent NBER working papers), and the NBER
Bulletin on Aging and Health.
The NBER has eighteen major programs, each of them involving at least twenty NBER research associates and
also some faculty research fellows: (1) aging, (2) asset pricing, (3) children, (4) corporate finance, (5) development
of the U.S. economy, (6) economics of education, (7) economic fluctuations and growth, (8) environmental and
energy economics, (9) health care, (10) health economics, (11) industrial organization, (12) international finance
and macroeconomics, (13) international trade and investment, (14) labor studies, (15) law and economics, (16)
monetary economics, (17) political economy, and (18) productivity and public economics. All of these programs
have a director and publish program working papers. Most of these programs include several primary projects with
ten to twelve economists and also some smaller projects focusing on more specialized research areas. An
economist may participate in several programs at the same time.

The NBER also has fifteen working groups: (1) Behavioral Finance, (2) Chinese Economy, (3) Cohort Studies, (4)
Economics of Crime, (5) Economics of National Security, (6) Entrepreneurship, (7) Higher Education, (8)
Insurance, (9) International Trade and Organization, (10) Market Design, (11) Market Microstructure, (12)
Organizational Economics, (13) Personnel Economics, (14) Risks of Financial Institutions, and (15) Urban
Economics. These groups meet regularly.
In doing research, the NBER cooperates with universities, private and public research organizations, independent
researchers, and governments. Eleven organizations (the Agricultural and Applied Economics Association, the
American Economic Association, the American Federation of Labor and Congress of Industrial Organizations, the
American Finance Association, the American Institute of Certified Public Accountants, the American Statistical
Association, the Canadian Economics Association, the Committee for Economic Development, the Conference
Board, the Economic History Association, and the National Association for Business Economics) have appointed
their own directors for the NBER’s board of directors, but the NBER also cooperates with other groups. Some
NBER working groups offer travel grants for graduate and doctoral students interested in attending professional
meetings or for postdoctoral students studying certain issues while not in residence at the NBER. Such calls for
proposals are available for the public at an NBER Web page,
www.nber.org/callforpapers/callpapers.html
Naturally, the NBER is not the only institution offering economic policy analyses or making economic forecasts.
The Congressional Budget Office, the U.S. Treasury Department, and the National Economic Council also perform
similar tasks.
Tiia Vissak
 
See also:  Congressional Budget Office;  National Economic Council;  Treasury, Department of
the. 
Further Reading
Fabricant, Solomon.  “Toward a Firmer Basis of Economic Policy: The Founding of the National Bureau of Economic
Research.” www.nber.org/nberhistory/sfabricantrev.pdf
National Bureau of Economic Research.  “NBER Information.” www.nber.org/info.html
National Economic Council
 
The National Economic Council (NEC) was founded in 1993 for the purposes of advising the U.S. president on
domestic and global economic policy. The NEC is a part of the Executive Office of the President, which includes
the Council of Economic Advisers, the Domestic Policy Council, the Office of Management and Budget, the Office
of the United States Trade Representative, as well as other agencies. In 2009, President Barack Obama appointed
former Treasury secretary Lawrence Summers as NEC director.
The NEC is a White House–led policy council charged with four main tasks: (1) to coordinate domestic and
international economic policy making, (2) to coordinate the offering of economic policy advice for the president, (3)
to ensure that U.S. policy decisions and programs coincide with the president’s economic goals, and (4) to
oversee the implementation of the president’s economic policy agenda. Basically, the NEC serves as an honest

broker (not an advocate) among viewpoints and agencies. At the NEC, the president’s top advisers can present,
test, and improve their ideas and seek support for them. Although the NEC was established in 1993, this does not
mean that the previous presidents did not have any structures for economic policy coordination, but this specific
form did not exist before. Nor did the NEC achieve instant success. However, President Bill Clinton still stated that
the NEC was the most significant organizational innovation made in the White House during his administration.
The NEC’s strong procedural norms were modeled after the ones of the National Security Council (NSC), but the
NEC remains smaller and slightly more informal than the latter. The NEC cooperates with the NSC as well as with
the Council of Economic Advisers (CEA), which is responsible for economic forecasting and general economic
analyses.
The director of the NEC and the two deputy directors cooperate actively with top officials and those departments
and agencies within the administration whose activities strongly impact the nation’s economy: the vice president,
the secretary of state, the secretary of the treasury, the secretary of agriculture, the secretary of commerce, the
secretary of labor, the secretary of housing and urban development, the secretary of transportation, and the
secretary of energy. Others who can be present at NEC meetings are the administrator of the Environmental
Protection Agency, the chair of the Council of Economic Advisers, the director of the Office of Management and
Budget, the U.S. trade representative, the assistant to the president for domestic policy, the assistant to the
president for national security, the assistant to the president for science and technology policy, and other senior
White House staff, because these officials and the heads of these departments and agencies are also considered
members of the NEC. Together, the director of the NEC and the officials of these agencies and departments aim
to implement the president’s economic policy objectives, including the ones related to financial markets, fiscal
policy, commerce, agriculture, labor, energy, health care, and Social Security. Advancing the American recovery
and reinvestment plan is also among their responsibilities.
Naturally, the NEC is not the only institution in the United States analyzing the state of the economy. The U.S.
Treasury Department, the Congressional Budget Office, and the National Bureau of Economic Research, among
others, are also active in this task.
Tiia Vissak
 
See also:  Congressional Budget Office;  National Bureau of Economic Research;  Treasury,
Department of the. 
Further Reading
Executive Office of the President:  www.whitehouse.gov/administration/eop
National Economic Council:  www.whitehouse.gov/administration/eop/nec
Rosen Wartell, Sarah.  “The White House: National Economic Council.” In Change for America: A Progressive Blueprint for
the 44th President, ed. Mark Green and Michele Jolin. Washington, DC: The Center for American Progress Action
Fund, 2008. 
Neoclassical Theories and Models
 
First developed toward the end of the nineteenth century, neoclassical economics is a set of theories and models

that applies the concept of marginality to the basic precepts of classical economics. Marginality represents the
additional benefit or cost that firms and individuals receive through the consumption or production of an additional
unit of goods or services.
 From Classical to Neoclassical Economics
According to the central paradigm of classical economics, established by thinkers such as Adam Smith, David
Ricardo, Thomas Robert Malthus, John Stuart Mill, and Karl Marx in the late eighteenth and mid-nineteenth
centuries, the value of a good or service is determined by the labor and material costs that go into making or
providing it. If demand for a good or service outstrips supply, then the cost of that good or service will go up. At a
certain point, rising costs depress demand, bringing it back into equilibrium with supply. The same process works
for wages. If there is a surplus of laborers, then wages decline, lowering the cost of production. As the cost of
production goes down, demand goes up, leading employers to hire more workers, thereby lifting wages. Once
again, an equilibrium between the demand for and supply of laborers is reached through market mechanisms. In
short, according to classical economics, the value of a good or service—or the value of labor—is inherent in that
good or service based on the inputs necessary to create or provide it. As for the value of labor, its value is based
on the skills, brains, or brawn of the worker—that is, how much value he brings to the production process.
The problem with the classical theory of value was that it focused on the object—the good or service or the
worker—while leaving the subject—the buyer of the good or service, the employer who hired the laborer, or even
the worker himself—out of the equation. This was evident in the fact that people often paid more (or less) for a
good than an objective measurement of the inputs would dictate. For example, a very hungry person will pay a
premium for her first potato, regardless of what it objectively cost to grow, harvest, process, distribute, and market
that potato. But as her appetite is satiated, she will be willing to pay less and less for each additional potato,
regardless of the fact that, objectively speaking, every potato cost the same to produce.
A similar process works for wages as well as prices. A firm will hire workers based not only on the value they
produce, but also on their marginal utility. That is, a firm will hire a new worker only so long as the value that the
new worker creates matches or exceeds the cost of employing that worker. From the worker’s perspective, he
sells his labor not only based on what the market says he is worth, but also on what he determines to be the
marginal utility of that wage. That is, the wage paid must exceed the disutility—the loss of leisure, the discomfort,
the absence from family—of the work itself. Finally, a firm will produce an additional good only if the revenue that
good provides exceeds the cost of producing that good. In this way, the firm is maximizing profit. The
development of this set of principles is known to economic historians as the “marginal revolution,” and it laid the
basis for the neoclassical economic paradigm to come.
Emphasizing the behavior of economic agents—as opposed to the objects of production—neoclassical economics
rests on three core concepts. First, people act rationally when making their preferences about different economic
outcomes. Second, individuals try to maximize utility and minimize disutility, just as firms try to maximize profit and
minimize cost. And for all this to work, it is assumed in neoclassical economics that people and firms make their
decisions independent of coercion and with full knowledge of all necessary and relevant information.
In short, economic value is determined by consumers, workers, and firms all attempting to maximize their utility or
profit within the constraints of the market. That is, a worker naturally would like to be paid far more than the
market may determine, and a firm would like to charge far more for its products than consumers may pay. But
both cannot, of course, because firms will not hire overpaid workers, and consumers will not pay too high a price.
Value, then, is determined by the conflict between desire and constraint, with the market serving as the arena for
this battle. When a price or wage is set, a truce is, in effect, called between these economic combatants, signaling
that desire and constraint are in balance, or equilibrium. But, of course, such truces are temporary, as constraints
and desires are ever changing. What the neoclassical economics model has in common with its classical
antecedent is the concept of an ever-adjusting equilibrium.

 Neoclassical Economics and the Business Cycle
With the work of the mid-twentieth-century American economist Robert Solow—who won a Nobel Prize for his
efforts—the neoclassical paradigm was applied to the business cycle. According to neoclassical thought, output is
produced by means of two inputs: labor and capital. Assuming that labor growth is a given, the main factor that
leads to growth comes through capital deepening—that is, the growth of capital in relation to labor. More capital
equipment per worker means more output per worker, but only up to a point. That is because the most productive
increases are made first. For example, a new railroad line will be constructed where demand is greatest. As new
lines are added, they tend to be located in less profitable markets. Thus, the marginal utility of increased capital
diminishes. At the same time, new capital equipment raises output per worker, assuming that the addition
outpaces labor growth, leading to an increase in wages as the marginal product of labor rises. But, at a certain
point, the return from capital deepening diminishes to the point at which it no longer makes sense to invest,
leading to diminished economic activity and then recession. Eventually, however, natural growth in labor and
consumer demand increases the marginal utility of capital deepening, leading to more investment and economic
recovery.
Neoclassical theory dominated the field of economics in the West during much of the twentieth century, for two
reasons. First, it is an all-encompassing set of ideas. One can apply the neoclassical critique to just about any
aspect of economic life, from employment to production to consumption, and even to such social phenomena as
getting married and having children, choosing the location of shelter (e.g., the marginal utility of housing costs
versus commuting time), and leisure (e.g., going to a ball game versus earning overtime pay for working over the
weekend).
The second reason for the dominance of neoclassical economics is that it lends itself to mathematical modeling.
For example, the foregoing description of capital deepening assumes an absence of technological change, as
improvements in the quality of capital equipment also would have as much—or perhaps more or less—of an
impact on marginal utility and profit as the quantity of capital equipment. Neoclassical economics does not ignore
such an obvious fact of modern life as technological change, but instead allows economists to factor it out—or in
—as the mathematical modeling of inputs requires. In short, neoclassical models and theories allow economics to
become more of a science than a social science, which is what the modern profession aspires to.
Not all modern economists agree with the neoclassical approach. Critics point out that by focusing on marginal
decisions, neoclassical economics misses important institutional and historical factors. Furthermore, the
mathematical rigor of neoclassical economics requires assumptions about individual rational decision-making that
blinded mainstream neoclassical economists to the irrational group behavior that caused the economic meltdown of
the late 2000s.
James Ciment
 
See also:  Classical Theories and Models. 
Further Reading
Arnold, Lutz G. Business Cycle Theory. New York: Oxford University Press, 2002. 
Hollis, Martin, and Edward J. Nell. Rational Economic Man: A Philosophical Critique of Neo-Classical Economics. New
York: Cambridge University Press, 1975. 
Milonakis, Dimitris, and Ben Fine. From Political Economy to Economics: Method, the Social and the Historical in the
Evolution of Economic Theory. New York: Routledge, 2008. 
Solow, Robert.  “A Contribution to the Theory of Economic Growth.” Quarterly Journal of Economics 70:1 (1956): 65–94. 
Yonay, Yuval P. The Struggle over the Soul of Economics: Institutionalist and Neoclassical Economists in America Between

the Wars. Princeton, NJ: Princeton University Press, 1998. 
Neo-Keynesian Theories and Models
 
Developed in the decades immediately following World War II, neo-Keynesian, or New Keynesian, economics is
the modern revival of Keynesian “neoclassical synthesis” economics. That is, it shares with neoclassical
economics the idea that, in the long run, economies gravitate toward a price equilibrium where there is low
unemployment and high output, but accepts John Maynard Keynes’s ideas that, in the short run, such an
equilibrium is often elusive. In other words, the main difference between neoclassical economists and the neo-
Keynesians rests on their respective views of the short run. Neo-Keynesian theory should not be confused with
Post Keynesian theory, the latter being more explicitly tied to Keynes’s original ideas.
Much of neo-Keynesian theory focuses on why the economy gets stuck in a position of less than full employment,
and on the possible policy responses to this situation. As such, the neo-Keynesian model can be labeled an
“imperfectionist” model because short-run disequilibrium positions are the result of some imperfection in how
markets operate in the real world, such as the inability of prices to adjust quickly. But if policy makers could
somehow remove this imperfection, the economy would slowly gravitate toward full employment on its own. In
other words, shocks that lead to economic disturbances can arise on either the demand or supply side. But
imperfections in market response create a situation in which the economy cannot adjust instantaneously to such
shocks—or may, indeed, amplify the shocks—and thus gets stuck in an equilibrium where there is high
unemployment and less than optimal output.
 Assumptions
The general neo-Keynesian model is based on three assumptions. The first is that, in the short run, money is
non-neutral—that is, the money supply affects both nominal variables such as prices and wages, and real
variables such as employment, inflation-adjusted gross domestic product (GDP), and inflation-adjusted
consumption The second is that markets may not work perfectly, and these imperfections in real-world markets
explain macroeconomic fluctuations in output, such as sticky prices and wages. The third is that aggregate
demand dominates in the short run while aggregate supply constraints exist in the long run.
The first assumption is related to the conduct of monetary policy. Breaking from mainstream economics, which
argues that central banks control the money supply, neo-Keynesians argue that central banks control the rate of
interest, setting it according to some policy objective, such as combating inflation. Non-neutrality means that
changes in the rate of interest will have an impact, in the short run, on output and economic fluctuations. In the
long run, however, neo-Keynesians agree with the neoclassical assumption of money neutrality—that is, that
money supply affects only nominal variables (e.g., prices and wages).
The second assumption says that if wages and prices (among other imperfections) do not adjust quickly enough to
changes in output (that is, they are “sticky”), then the economy will be slow to move to its new equilibrium
position. In other words, the price mechanism breaks down. This will translate into a disequilibrium position with
high unemployment that may persist for some time because nominal wages will fail to adjust downward in a timely
fashion. The third assumption simply states that, in the short run, changes in aggregate demand dominate:
positive changes in aggregate demand will have positive effects on output and employment. In the long run,
however, the economy is constrained by aggregate supply.

 New Consensus Model
Over the years, there have been many different versions of the neo-Keynesian model, although they all share the
three fundamental assumptions discussed above. Since the 1990s, a new model has emerged that has received
considerable attention from economists as well as central bankers and policy makers because of its potential
usefulness, especially to central bankers, who control monetary policy. The new model is called the “New
Consensus” model or the “Taylor Rule” model, because it is based on a monetary policy theory developed by
economist John Taylor. Indeed, the New Consensus model is currently the most widely cited iteration of the neo-
Keynesian model, in which the three assumptions from above are clearly implicit. And although there are many
variations of this model, the basic model can be summarized by three fundamental assumptions that are
expressed as equations.
The first equation is an aggregate demand equation, stipulating that changes in output (or rather the output gap,
defined as deviations in actual output from potential or long-run output) are caused largely by changes in the rate
of interest. In other words, whenever the central bank changes the rate of interest, output will change accordingly:
this is the basic principle of monetary non-neutrality. For example, an increase in the rate of interest will cause
output to fall. This result holds because of nominal wage rigidity, allowing monetary policy to have an impact in
the short run.
The second equation is a Phillips curve, a formula based on the work of New Zealand economist Alban William
Phillips that shows the inverse relationship between the unemployment rate and the rate of inflation. It stipulates
that inflation is largely explained by changes in aggregate demand. Whenever aggregate demand is greater than
potential output, or the theoretically highest level of GDP attainable by an economy at a given moment in time,
prices will tend to rise, and inflation rises above the target rate set by the central bank. The third equation
represents the monetary policy position of the central bank. According to this equation, the rate of interest is set
by the central bank according to its policy objectives.
The New Consensus model is representative of many countries’ current monetary policy stance. Indeed, in many
countries, the central bank targets a very specific level of inflation, or a corridor, and uses interest rates to reach
it. In many versions of the New Consensus model, control over inflation is the only—or, at least, the overreaching
—objective of the central bank.
In practical terms, the model works in the following matter. Assume that a central bank has an inflation target of 2
percent per annum and that actual inflation approaches or surpasses the target. In such a case, the central bank
will raise the interest rate. According to the first of the New Consensus model equations, the increase in the rate
of interest should therefore decrease output, since a higher rate of interest will deter investment and hiring. In turn,
this should lower inflation, according to the second equation. But if this fails to bring inflation down to the target
rate or below, the central bank will continue to raise the rate of interest until the inflation target is reached. Yet
because this adjustment is not instantaneous, the economy can spend some time in slumps with lower output and
higher unemployment. This is considered only temporary, however, as the economy will eventually, or should,
gravitate toward a low-unemployment, high-output equilibrium.
 Critique of the Neo-Keynesian Model
Post Keynesian economists have noted a number of possible weaknesses in the New Consensus approach. First,
and most notably, Post Keynesians point out that the New Consensus model is overly concerned with inflation.
They also believe that neo-Keynesians give too little consideration to unemployment or unequal income
distribution. Indeed, according to neo-Keynesians, only inflation is an economic scourge for policy makers.
Moreover, say the Post Keynesians, this bias is exacerbated by the neo-Keynesian tendency to focus on demand
shocks as the cause of inflation. For Post Keynesians, inflation is, rather, the result of excess costs and of conflicts
between workers and firms (and finance).
By focusing on a wrong interpretation of inflation, then, policy prescriptions will only tend to exacerbate the

problem, say these critics. In fact, if costs determine prices, then an increase in the rate of interest could lead to
an increase in prices in the short run, given the increased cost of borrowing credit. This is known as Gibson’s
Paradox. It suggests that interest rates and prices may not move in opposite directions. At the very least, their
precise correlation is unknown.
Second, Post Keynesians criticize the New Consensus model because its policies to fight inflation result in a
weaker economy. The New Consensus argues that unemployment beyond structural unemployment is necessary
in order to lower inflation. While in the long run, repeated increases in the rate of interest will eventually lower
inflation, doing so may cause a severe recession.
Third, Post Keynesians object to the New Consensus model’s conclusion that fiscal policy is ineffective, or at best
inflationary. In the New Consensus view, monetary policy is considered the only credible policy and the central
bank the only credible institution. Fiscal policy is not considered an effective tool to fight unemployment. Post
Keynesians, on the other hand, maintain that fiscal policy is needed to achieve policy goals that include full
employment and more equal income distribution.
For Post Keynesians, then, the emphasis on inflation and the exclusive use of monetary policy pose a definite
problem for macroeconomic stability. They propose shifting the focus away from inflation and monetary policy
dominance in favor of policies aimed at fighting unemployment. And while aggregate demand does play an
important role, the focus is rather on income distribution and fiscal policy.
 Neo-Keynesian Economics and the Recession of 2007–2009
Critique of the neo-Keynesian model has gained influence since the 2007–2009 recession undermined faith in the
theories and policy prescriptions of mainstream economists, who relied too heavily on the self-regulating ability of
the economy to gravitate instantly or slowly to a position of equilibrium, or rest. One thing has become clear: the
economy does not gravitate on its own toward a predetermined position of equilibrium. On the contrary, what this
severe crisis has taught us, say Post Keynesians, is that if left unconstrained, markets can be exuberant and
irrational, and prone to periodic excesses. Markets need help to generate employment and to grow and maintain
some sort of order.
In the end, numerous policy makers and economists alike have come to appreciate once again some of the
insights of John Maynard Keynes, especially his belief that markets need to be regulated to prevent them from
becoming too speculative and unstable. This suggests that new financial regulations are needed and that more
attention should be paid to maintaining steady growth and aggregate demand, both in the short run and in the
long run. In this sense, fiscal and regulatory polices are an integral component of well-functioning markets.
Louis-Philippe Rochon
 
See also:  Keynes, John Maynard;  Keynesian Business Model;  Neoclassical Theories and
Models;  Post Keynesian Theories and Models. 
Further Reading
Gordon, Robert J.  “What Is New-Keynesian Economics.” Journal of Economic Literature 28: 3 (September 1990): 1115–
1171. 
Mankiw, N. Gregory.  1990. “A Quick Refresher Course in Macroeconomics.” Journal of Economic Literature,  American
Economic Association 28:4 (December 1990): 1645–1660. 
Rochon, Louis-Philippe, and P. Nicholas Rowe.  “What Is Monetary Policy? Should Central Banks Be Targeting Inflation?”
In Introducing Macroeconomic Analysis: Issues, Questions, and Competing Views, ed. H. Bougrine and M.
Seccareccia. Toronto: Emond Montgomery, 2009. 

Taylor, John B., ed. Monetary Policy Rules. Chicago: University of Chicago Press, 2001. 
 
Netherlands, The
 
The Netherlands, also known as Holland, is a small country of about 16.5 million people, located in northwestern
Europe, between Germany and Belgium, on the North Sea coast. The nation is flat and low-lying, with about one-
quarter of its landmass situated below sea level, made habitable for about 60 percent of the population through
land reclamation and an extensive network of dikes.
The Netherlands has a long and illustrious economic history, having become a major center of finance and trade
in the seventeenth century and an innovator in the development of modern capitalist institutions. Although slow to
industrialize and historically eclipsed by larger neighbors, the Netherlands reemerged at the front ranks of
economic innovation and prosperity in post–World War II Europe. A member of the European Union and the
eurozone, contemporary Netherlands has a free-market economy based on manufacturing, services, trade, and
financial activities.

An economic power in the seventeenth century on the strength of maritime trade and financial innovations, The
Netherlands today boasts one of the most stable economies in Europe. Shipping is still a mainstay; the port of
Rotterdam is one of the world’s busiest. (Bloomberg/Getty Images)
 Economic History to World War II
Settled by Germanic tribes in the first millennium BCE, the Netherlands was conquered by the Romans in the first
century BCE. During the Middle Ages, the Low Countries (the Netherlands and Belgium) became a major
commercial center, where goods from the Mediterranean were exchanged with those from the Baltic region.
By the 1400s, the Dutch city of Antwerp had emerged as one of the largest marketplaces in Europe, where Italian
silks, marbles, and mirrors were exchanged for English woolens, German iron and copper, Spanish fruit, French
wines and dyes, and Baltic wheat, fur, and timber. At the beginning of the sixteenth century, the region came
under the dominion of Charles V, the Hapsburg emperor of Spain and other parts of Europe. The acquisition came
just as Spain was beginning its conquest of the Americas, an event that led to a flood of precious metals and
currency flowing into the Hapsburg dominions. Much of the money went to the Low Countries, where it paid for all
of the goods marketed there and turned Amsterdam into a leading financial center of Europe.
In the late sixteenth century, the seven provinces of what would become the Netherlands united and began their
nearly century-long struggle for independence from the Hapsburg Empire. Even as the conflict continued in the
1600s, the Dutch Republic, which had declared independence in 1581, emerged as the center of a global trading
system that ultimately stretched from the Americas to West Africa to the East Indies. Spearheading that network
was the Vereenigde Oost-Indische Compagnie (VOC, or Dutch East India Company), often called the world’s first
international corporation, established in 1602.
In the succeeding decades, armed VOC merchant ships pushed the Portuguese from Ceylon (now Sri Lanka) and
the East Indies (Indonesia), establishing Dutch control over the lucrative trade in spices and other tropical goods

from the east. With the founding of the Geoctroyeerde Westindische Compagnie (GWIC, or Dutch West India
Company) in 1621, merchants in the Netherlands helped steer some of the valuable trade in silver and other
products of Spanish America to Dutch ports. At the same time, Dutch merchants emerged as some of the most
aggressive slave traders, establishing trading posts along the West African coast.
The abundance of trade, combined with Amsterdam’s role as the financial center of Northern Europe, made the
Netherlands the richest country on the continent (on a per capita basis) and produced the so-called Golden Age of
Dutch history, including its outpouring of artistic masterpieces. A financial innovator as well, the Netherlands had
been identified by some economic historians as the first fully capitalist state in human history, where merchants
held sway over government and freed business from the often onerous restrictions set by royal authorities in such
nominally mercantile states as England. It was in Amsterdam that the first modern stock exchange was
established in the 1600s, while Dutch merchants created the modern insurance industry and pension system.
Holland was also home to the first great speculative bubble in modern capitalist history, tulipmania—a wild frenzy
over exotic tulip bulbs that created and destroyed fortunes overnight in the mid-1630s.
Despite the end of the Eighty Years’ War for independence in 1648, the days of Dutch commercial supremacy
were numbered. For with all their wealth and innovation, what the merchants could not do was make Holland
militarily competitive with rising powers such as Britain and France, both of which used their greater populations
and military resources to challenge Dutch maritime supremacy. By the eighteenth century, the Netherlands had
lost its place as a world power, though it still remained an important financial and trading center.
Under French dominion during late revolutionary and Napoleonic eras, the Netherlands emerged as a kingdom in
1814 and briefly united with Belgium, which would become independent in 1830. While Belgium would become
one of the leading centers of the industrial revolution in continental Europe during the nineteenth century, Holland
lagged behind, held back by the difficulty of building a modern industrial and transportation infrastructure in a
country laced with waterways and dependent on wind power.
 Post–World War II Boom
Occupied by Germany in World War II, and losing its valuable East Indian colonies just after the conflict, the
Netherlands emerged from the war eager to reestablish itself as a trading and financial center in a unified Europe.
Toward that end, it was a founding member of both the North Atlantic Treaty Organization (NATO) and the
European Coal and Steel Community, the predecessor of the European Union. Indeed, in the first decades after
the war Holland participated in the “economic miracle” of Western Europe, aided first by billions of dollars in
postwar U.S. funds under the Marshall Plan and then by rising demand from a growing middle class. In the 1950s
and 1960s, the country followed the Western European model of combining free markets with strong state
direction of major industries and a generous social welfare system.
For two decades, the policies worked well, raising Dutch standards of living to some of the highest levels in the
world. But with the oil crisis of the 1970s, which hit the Netherlands particularly hard—it had almost no indigenous
forms of energy, aside from that produced by its abundant windmills—the country entered a period of economic
stagnation. In response, the government embarked on one of the most aggressive sets of free-market reforms in
continental Europe during the 1980s, while retaining its extensive social welfare system.
By the 1990s and 2000s, the Netherlands was consistently posting some of the best economic numbers in the
European Union, with unemployment levels below those of other member countries and consistently higher annual
gross national product (GDP) growth. By 2008, the country was ranked by the World Bank as having the tenth-
highest GDP per capita in the world; overall the economy ranked sixteenth in size. The contemporary Dutch
economy rests on several pillars, including finance, shipping and transportation (the port of Rotterdam is the
busiest in Europe), and agriculture, including food processing.
With so much of its economy tied to finance and insurance, the country was hard hit by the global financial crisis of
2008–2009. The collapse of the Belgian financial giant Fortis Bank in September 2008—one of the largest bank

failures in Europe, with major effects in the Netherlands—forced the government to purchase the bank’s Dutch
banking and insurance divisions for more than $23 billion.
In addition, the global recession that followed the crisis had a major impact on the country’s port and shipping
sector. Together, the financial crisis and recession undermined economic performance. GDP growth had
consistently exceeded 3 percent annually through the early and middle 2000s, but the Dutch economy was
expected to shrink by about 0.75 percent in 2009. Unemployment was expected to climb above 6 percent,
relatively low for continental Europe but high by Dutch standards. With the gradual recovery of the European
economy in 2010, the Dutch GDP returned to growth, with a 1.7 percent increase, and the unemployment rate fell
to 5.5 percent. But the ongoing debt crisis surrounding Greece and other Eurozone members threatened to
undermine this gradual recovery in 2011.
James Ciment
 
See also:  Belgium;  Tulipmania (1636-1637). 
Further Reading
Arblaster, Paul. A History of the Low Countries. Basingstoke, UK: Palgrave Macmillan, 2005. 
CPB Netherlands Bureau for Economic Policy Analysis.  “The Credit Crisis and the Dutch Economy 2009–2010.”
Schama, Simon. The Embarrassment of Riches: An Interpretation of Dutch Culture in the Golden Age. New York: Alfred A.
Knopf, 1987. 
 
New Deal
 
The New Deal was the informal name for a set of programs initiated by the Franklin Roosevelt administration in
the 1930s to lift the U.S. economy out of the Great Depression and to provide social welfare benefits to lower-and
middle-income Americans. Historians actually refer to two “New Deals.” The first, dating from the years 1933–
1934, was aimed at reversing the worst economic downturn in U.S. history and providing immediate relief to hard-
hit individuals, families, farms, and businesses. The second, launched in 1935, established more long-term social
welfare and economic programs, many of which continue to the present day.
 Background
The background to the New Deal was, of course, the unprecedented economic catastrophe known as the Great
Depression. Between the stock market crash of October 1929—the triggering event for the downturn in the
public’s eye—and the inauguration of the Democrat Roosevelt in March 1933, the nation’s economy experienced a

contraction of epic proportions, as gross domestic product (GDP) fell by one-third and the official unemployment
rate soared to 25 percent.
After some hesitation, Roosevelt’s predecessor, Republican Herbert Hoover, had attempted to address both the
high unemployment and drop in economic output through limited public works programs, government loans to
major financial and industrial institutions, and appeals for private contributions to help the unemployed. But
Hoover’s efforts were limited by his philosophy that government relief encouraged dependency and by the
prevailing economic wisdom that government deficits contributed to recession by drying up the funds available for
private investment. Notably, both Hoover and Roosevelt subscribed to this conventional view, though the latter
would jettison it for much of his first term.
When Roosevelt took office—with large Democratic majorities in Congress to back him up—the most immediate
problem was the collapse of the nation’s financial system, as interbank lending dried up and thousands of smaller
institutions failed. To protect their savings, depositors had panicked, making runs on still solvent banks, forcing
many of them to fail because they did not have the liquid funds to meet all of the depositors’ withdrawals.
Roosevelt immediately declared a bank holiday, giving the Treasury Department time to certify solvent institutions
and reorganize insolvent ones, and reassuring customers that banks were now safe places to put their money.
Several months later, in June 1933, Congress passed the Glass-Steagall Act, which, among other things,
established the Federal Deposit Insurance Corporation (FDIC), guaranteeing deposits and separating commercial
and investment banking, the latter activity advancing the financial speculation that had contributed to the stock
market crash. These measures stabilized the banking system.
Farms were in equally bad shape, with crop prices having dropped below the cost of growing the products. The
Agricultural Adjustment Act encouraged farmers to limit their output, since overproduction was a major cause of
falling prices, and offered them payments that brought their income up to sustainable levels. To provide jobs, the
administration launched the Civilian Conservation Corps, which ultimately put some 2.5 million young people to
work restoring the environment; the Public Works Administration, which appropriated $3.3 billion (about $55 billion
in 2009 dollars) to hire unemployed adults on infrastructure projects; and the Tennessee Valley Authority, which
put thousands to work building dams and electrical power systems across a wide swath of the Appalachian South,
a particularly hard-hit region of the country.
But the most important program of the so-called First Hundred Days was the National Recovery Administration
(NRA), which attempted to limit the cutthroat competition in various industries that had driven down prices, profits,
wages, and employment. The NRA did this through committees of businessmen and labor and consumer
representatives that drafted codes to limit production and set prices. At the same time, the law guaranteed
workers the right to organize unions and bargain collectively.
While the various programs of the First New Deal helped stabilize the economy, they were only partially effective
against the most pernicious problem of the Depression—unemployment, which lingered above 17 percent into
1935. Meanwhile, Roosevelt’s New Deal policies faced growing criticism from both the Right and the Left. With the
economy somewhat recovered, business groups and Republicans spoke out against excessive government
interference. In 1935, they won a victory when the U.S. Supreme Court—in the case of Schecter v. United
States—overturned the enabling legislation for the NRA as an unconstitutional federal involvement in intrastate
trade. From the Left came demands for more radical measures to address social inequities. Among these was a
popular plan by California public-health advocate Francis Townsend for public pensions for old people and
demands from Louisiana politician Huey Long for a radical redistribution of the nation’s wealth.
 Second New Deal
Concerned that leftist opposition and lingering high unemployment might undermine his reelection chances in 1936
—and angry with what he felt was betrayal by the business interests he felt he had saved with First New Deal
legislation—Roosevelt launched a far broader panoply of social welfare and economic legislation in 1935. The first
piece was the Emergency Relief Appropriation Act, which set aside billions of dollars for a variety of programs.

Among these were the Resettlement Administration, which relocated destitute families into planned communities;
the Rural Electrification Administration, bringing electricity to underserved areas; and the National Youth
Administration, which provided jobs for young adults and students. However, the biggest program was the Works
Progress Administration (WPA), which ultimately hired some 8.5 million workers—including artists and performers
—to work on infrastructure and social welfare projects across the country.
First Lady Eleanor Roosevelt visits a construction site of the Works Progress Administration in 1936. The central
New Deal program to combat joblessness, the WPA provided jobs to some 8.5 million Americans—many on public
works projects—over eight years. (The Granger Collection, New York)
Then, in the summer of 1935, came the so-called Second Hundred Days, which introduced legislation with the
most lasting legacy for American society. Among the new laws was the National Labor Relations Act, which
further strengthened workers’ rights to organize and bargain collectively and launched the most far-reaching
unionization drive in American history. To steal the thunder of advocates for the elderly, Roosevelt pushed through
the Social Security Act, creating a public pension plan. The law also included a new federal-state partnership on
unemployment and a program—Aid to Dependent Children (later Aid to Families with Dependent Children, AFDC)
—that would lay the groundwork for federally subsidized welfare. The administration also introduced legislation to
redistribute wealth through a more progressive income tax through the Wealth Tax Act. However, compared to the
other key legislation of 1935, the Wealth Tax Act was a modest effort.
The initiatives proved so popular that, despite continued double-digit unemployment, Roosevelt won reelection in
1936 in the biggest landslide in American presidential history to that date, having forged what political experts
called a New Deal coalition of white Southerners (Southern blacks were largely debarred from voting) and urban
Northerners. But with the victory under his belt, Roosevelt made two costly errors. To fend off conservative
Supreme Court efforts to undermine New Deal legislation, he contrived a plan to increase the number of high
court justices—with his own appointees. While constitutionally legal, the move struck many Americans, including
Roosevelt supporters, as a power-grabbing effort not unlike those being made by fascist and Nazi governments in
Europe. Second, having never dropped his commitment to balance the federal budget, Roosevelt scaled back
many of the economic stimulus programs introduced during his first term. Meanwhile, fearing inflation in a
recovering economy, the Federal Reserve raised interest rates.
The two measures sent the economy into a new downward spiral in 1937 and 1938—the so-called “Roosevelt
Recession”—forcing the administration to return to deficit spending. While Roosevelt also introduced some new

legislation in 1937 and 1938—including measures to build housing for low-income families, another farm bill, and
legislation banning child labor and establishing a minimum wage and forty-hour workweek for many employees—
the energy of the New Deal had been largely spent, especially after a turn to the right by voters in the
congressional midterm elections of 1938 and as the administration shifted its focus to the growing threat of global
conflict.
 Impact and Legacy
Economists and historians in the decades since have debated the effectiveness and legacy of the New Deal. Most
agree it was effective, within limits. It did turn the economy around somewhat and prevented the kind of political
upheaval experienced in Europe. But many students of the era also say that it did not go far enough. While the
administration did adopt some of the countercyclical ideas of John Maynard Keynes—that is, deficit spending to
spur aggregate demand—Roosevelt was too concerned about government spending and thus too timid in the jobs
and social welfare programs he launched. That is why, critics say, unemployment remained in the double digits
into 1938. Only with the massive defense spending of the late 1930s and early 1940s—that is, government
spending on a scale urged by Keynes to address a downturn as steep as the Great Depression—did
unemployment fall to pre-Depression levels. Meanwhile, some conservative historians and economists in recent
years have resurrected contemporary arguments that New Deal programs actually prolonged the Depression by
absorbing capital that might have been used by the private sector and creating a sense of uncertainty in the
business community that stifled investment.
Few students of American history and economics, however, would deny the immense legacy of the New Deal,
with supporters arguing that it laid the foundation for the prosperity of the post–World War II era. Countercyclical
deficit spending became a standard tool in the recession-fighting arsenal of the federal government while the New
Deal’s social legislation laid the groundwork for the limited social welfare state of recent decades. And the New
Deal coalition assembled by Roosevelt dominated the country’s politics through the 1970s. Only with the economic
crises of that latter decade—and the conservative resurgence they triggered—did the New Deal coalition begin to
come apart and some of the New Deal policies begin to be reversed, though Social Security remains largely
untouched to this day.
James Ciment
 
See also:  Great Depression (1929-1933);  Keynes, John Maynard;  Keynesian Business Model; 
Public Works Policy. 
Further Reading
Barber, William J. Designs Within Disorder: Franklin Roosevelt, the Economists, and the Shaping of American Economic
Policy, 1933–1945. New York: Cambridge University Press, 1996. 
Bernstein, Michael A. The Great Depression: Delayed Recovery and Economic Change in America, 1929–
1939. Cambridge, UK: Cambridge University Press, 1989. 
Leuchtenburg, William. Franklin Roosevelt and the New Deal. New York: Harper & Row, 1963. 
Rosen, Elliot A. Roosevelt, the Great Depression, and the Economics of Recovery. Charlottesville: University of Virginia
Press, 2005. 

 
New York Stock Exchange
 
Measured by both the dollar value of its listed company securities and by the annual dollar total of shares traded,
the New York Stock Exchange (NYSE) is the world’s preeminent stock exchange. Founded in 1792, it has been
owned and operated since 2007 by NYSE Euronext, Inc., a holding company that controls or owns an interest in a
number of securities exchanges in Europe, the United States, and the Middle East.
 Operations
As a stock exchange, the NYSE offers facilities for the sale and purchase of various kinds of financial instruments,
most notably, corporate securities for companies that are listed on the exchange. The NYSE is located in two
buildings in the financial district of Lower Manhattan, with the main trading floor situated in a National Historic
Landmark building at 11 Wall Street.
The total capitalization of the roughly 2,700 companies listed on the exchange is about $10 trillion, and the value
of share trades on the exchange exceeds $20 trillion annually. To be listed on the NYSE, a company must have
at least 1 million shares valued at a minimum of $100 million. By comparison, the second-largest exchange, the
National Association of Securities Dealers Automated Quotations, or NASDAQ, has about half the annual share
trades, by value, of the NYSE; the largest non-U.S. exchange, the London Stock Exchange, does about $7.5
trillion in annual trades.
The value of stocks listed on the NYSE is indicated by various indices. The most widely watched index is the
Dow Jones Industrial Average, which comprises thirty of the largest and most representative companies listed on
the NYSE. The NYSE Composite is an index of all stocks traded on the exchange. With an original value of 50
points, based on the market closing at the end of 1965, the NYSE Composite stood at about 7,300 at the
beginning of 2010, down from its highest closing figure of 10,387 on October 31, 2007.
Corporate securities are bought and sold through the exchange during operating hours—9:30 a.m. to 4:30 p.m.,
Eastern Standard Time—by traders who work for investment banks and brokerage houses. To trade on the floor,
each of the several thousand traders must own a “seat,” which can cost upward of several million dollars and
requires that the trader pass certain competency and ethical tests. Activity on the floor can be frenzied, as traders
jockey to have their purchase or sale orders registered. However, the days of this kind of activity are probably
numbered, as the NYSE is expected to follow other exchanges, such as NASDAQ, in converting to all-electronic
trading. Moreover, many brokerage houses and investment banks have been reducing the number of workers
manning the trading posts on the floor, preferring to have them work at computer terminals in company-owned
trading floors and offices.
 History Through World War II
The history of the NYSE encapsulates the financial history of the United States itself. In 1790, the federal
government began refinancing the debt it and the thirteen states had accrued during the Revolutionary War,
offering the first major issues of publicly traded securities. Two years later, three of these government bonds, along

with shares in two banks, were traded in New York City. At the same time, to facilitate such trades, twenty-four
brokers and merchants signed the Buttonwood Agreement, named after a tree of that kind that grew on Wall
Street, establishing the basic rule that securities would be traded on a commission basis.
With the economy reviving in the wake of the War of 1812 and, along with it, the number of corporate securities,
members of the organization of brokers that had signed the Buttonwood Agreement drafted a constitution of rules
in 1817, adopting the name New York Stock & Exchange Board (the exchange’s name would be changed to its
current one in 1863) and renting their first offices in a room on Wall Street. The exchange would move several
times to larger headquarters over the years, finally establishing itself in its current Wall Street location in 1922.
Virtually all of the major economic endeavors of the early republic, including the Erie Canal and many of the first
railroad stocks, were financed through various forms of bonds and securities bought and sold on the New York
exchange. With the development of the telegraph in the 1840s, shares in distant companies could now be traded,
leading the exchange to establish more stringent requirements, including detailed financial statements, for a
company to be listed. The development of the stock ticker in 1867, which instantaneously transmitted share-price
information across the country and via undersea cable to Europe, also contributed to the NYSE’s growing
influence over U.S. financial markets.
To create more expedited exchanges, the NYSE adopted continuous trading in the 1870s, abandoning the old
practice of allowing trades at set times. To facilitate this new practice, brokers dealing in specific stocks—known
as specialists—manned set trading posts on the trading floor.
Despite setbacks, including several financial panics, the exchange continued to grow through the late nineteenth
and early twentieth centuries. In 1886, it posted its first million-share day, and in 1892, it organized the New York
Stock Exchange Clearing House to expedite trades between brokers. In 1896, the Wall Street Journal began to
publish its Dow Jones Industrial Average (DJIA), with an initial value of 40.74. The listing came on the eve of a
vast expansion in the value and volume of securities listed on the exchange, as the U.S. economy underwent a
wave of corporate mergers that crested in the early years of the twentieth century.
World War I represented a milestone in the history of the NYSE. With the United States emerging from the war as
the leading creditor nation in the world, New York supplanted London as the world’s financial center, with the
NYSE becoming the world’s largest exchange. By the late 1920s, the NYSE was the center of frenzied
speculation in corporate securities that drove up the DJIA to nearly 400. But the crash of 1929 and the Great
Depression that followed reduced that figure to just over 40 in 1932, about where it had been when the index was
created 36 years before. (The DJIA would not return to its 1929 high until 1954.) At the same time, the federal
government, through the new Securities and Exchange Commission, began to regulate the sale of corporate
securities and became more vigilant in preventing securities fraud.

A neoclassical building at 18 Broad Street in Lower Manhattan has been home to the New York Stock Exchange—
the world’s largest—since 1903. The trading floor is located on nearby Wall Street, where the exchange originated
in 1792. (Henny Ray Abrams/Stringer/AFP/Getty Images)
 History Since World War II
As trading on the NYSE expanded in the post–World War II period, the exchange instituted a number of reforms
and innovations. These included new recommendations of transparency for listed companies, asking them to bring
in outside directors and to stop transactions between the officers and directors. There were also internal changes.
In 1971, the exchange was reorganized as a not-for-profit corporation, with policy-making decisions shifted to the
twenty-one-member board of directors, including ten outside directors, the following year. Technological
innovation, including new data-processing computers and the development, in 1978, of the International Trading
System, providing electronic links with other exchanges around the world, facilitated the global securities trade. A
year later, the NYSE organized the New York Futures Exchange, offering trading in financial derivatives.
In the wake of the largest single-day percentage drop in the NYSE on October 19, 1987, the exchange instituted
what were known as “circuit breakers,” which would automatically halt trading in the event of huge price swings.
Many experts blamed computerized trading—in which sell orders on large blocks of shares held by institutions
automatically went through when prices hit a certain figure—for the massive sell-off in stocks that day. On
October 27, 1997, a 554-point drop in the DJIA triggered the circuit breaker for the first time.
Much of this innovation came under the leadership of Chief Executive Officer Richard Grasso, who had risen
through the NYSE’s ranks from clerk. Grasso was also widely credited with maintaining the NYSE’s reputation as

the world’s leading stock exchange. But when it was revealed in 2003 that he had received a compensation
package worth nearly $140 million, it created a firestorm, the revelation coming as it did in the wake of a series of
corporate pay scandals. In the end, Grasso was forced to resign but ultimately kept the compensation, after New
York State’s appeals court overturned a lower court ruling that he return much of the compensation.
Still, under Grasso’s leadership, trading on the exchange had continued to grow. In 1992, the NYSE had its first
billion-share day and the DJIA topped 10,000 for the first time in 1999. The exchange also modernized,
introducing expanded forms of electronic trading and going public as the now for-profit NYSE Group, Inc., with a
share offering of its own in 2006. A year later, the NYSE Group, Inc., merged with Paris-based Euronext, which
owned several major exchanges in Europe. Not only did the merger create what management called the “first
global stock exchange,” but it gave the NYSE access to Euronext’s expertise in electronic trading.
The financial crisis and recession of 2007–2009 had a major impact on the corporate securities markets. From its
peak of more than 14,000 in October 2007, the DJIA fell by more than half, to less than 6,500, in March 2009. At
the height of the crisis, in the late summer and early fall of 2008, the NYSE Composite and the DJIA experienced
some of the wildest fluctuations in their history. None of this, of course, dampened overall trading. More important
for the exchange’s future than temporary rises and falls in securities prices are two other factors—whether all
electronic trading will eliminate the need for a trading floor, and whether the shift in global economic power to East
Asia will eventually eclipse the NYSE’s position as the world’s leading stock exchange.
James Ciment
 
See also:  Dow Jones Industrial Average;  Stock Markets, Global. 
Further Reading
Geisst, Charles R. Wall Street, a History: From Its Beginnings to the Fall of Enron. New York: Oxford University
Press, 2004. 
New York Stock Exchange:  www.nyse.com
Sorkin, Andrew Ross. Too Big to Fail: The Inside Story of How Wall Street and Washington Fought to Save the Financial
System from Crisis—and Themselves. New York: Viking, 2009. 
New Zealand
 
New Zealand, an island nation located in the South Pacific, is home to about 4.3 million people. A relatively new
nation, its economic history is shorter than that of most countries around the world. Significant European
settlement only began in the 1840s, and for much of the country’s existence, the economy was based upon
agricultural exports. The collapse of the commodities markets around the world during the Great Depression had a
profound impact on New Zealand, leading to an insulated economy that stressed employment over growth. The
eventual lifting of restrictions led to considerable hardships for many New Zealanders, but also resulted in real
economic growth during the 1990s and the beginning of the twenty-first century.
The original inhabitants of New Zealand were the Maoris, whose tribal society saw little trade between different
groups. The first Europeans to settle in New Zealand came mostly from Great Britain. Their goals at first were to

exploit the natural resources of the two principal islands, which resulted in an export economy. Whale oil,
sealskins, and timber were among the earliest products and required little or no processing before being shipped
to foreign markets. During the 1850s, gold was discovered in several parts of New Zealand, attracting many
settlers. Within a decade or two, many of the readily exploitable resources had been depleted.
During the 1850s, however, sheep were introduced to New Zealand. Grasslands were created by partially clearing
some forest areas; this in conjunction with the suitable climate proved favorable for large-scale livestock
production. Wool from New Zealand found a ready market in British mills, thanks to free trade between the two
countries. Many immigrants were attracted to New Zealand, thanks to the relatively high living standards. British
capital allowed the New Zealand government to begin building an infrastructure of railroads and manufacturing in
the last half of the nineteenth century.
Between the mid-1870s and the mid-1890s, the economy of New Zealand stagnated. Flat prices for wool and the
need to pay off loans for the creation of infrastructure slowed New Zealand’s growth. The invention of refrigeration
in the 1890s, however, opened British markets to new products. Mutton, beef, cheese, and butter could be
preserved and shipped halfway around the world. The need to process New Zealand’s exports remained minimal.
Land prices increased, and most New Zealanders worked directly or indirectly in agriculture. The outbreak of
World War I in 1914 led to a boom for New Zealand. Great Britain remained its main trading partner, and food
products were in great demand. In return, manufactured products were imported in large amounts, and New
Zealand’s economy remained undiversified.
The end of the war resulted in a downturn in the commodity markets. Many New Zealand farmers had difficulty
paying back the loans they had taken to buy additional land. By 1931, farm income was negative in New Zealand
and unemployment was rising. As the rest of the world suffered through the Great Depression, New Zealand’s
exports fell dramatically. The demand for imports fell as well, since few people had money to spend. The
government forced the banks to reduce interest rates and devalued the New Zealand pound. It also began to exert
more direct control over the economy, including creating a central bank to stabilize the economy. Confidence
began to return, along with markets for commodities.
In 1938, a balance-of-payments crisis threatened to throw the New Zealand economy back into a recession. To
deal with the problem, the government introduced direct control of imports. The goal was to prevent
unemployment such as that suffered in the Depression. Known as “insulationism,” the policy was intended to
protect New Zealand’s developing industry and to provide full employment for workers. Domestic demand for
products was met by domestic production, even if the cost was greater than for imports. The government also
hoped to diversify New Zealand’s economy, to prevent dependence on the agricultural sector.
Other countries also limited imports during and immediately after World War II, but New Zealand was unique in
continuing the policy for decades. The commodity market collapsed after the Korean War, slowing the export of
New Zealand’s products. In addition, many countries, including the United States, subsidized their farmers to keep
food prices below those of New Zealand. Even Great Britain, whose market remained open to New Zealand’s
imports, could not absorb the islands’ production. In 1973, even that market was closed when Britain joined the
European Economic Community.
Although unemployment remained low during this time, New Zealand’s economy fell behind those of most other
developed countries. Real income failed to increase as much as expected, and consumers were deprived of goods
available in other countries. A crisis took place in 1973, when oil-producing countries agreed to raise the price of
oil. New Zealand, like all other oil-importing nations, was hard hit. Unemployment grew, as did inflation. The
government responded with a costly program known as “Think Big,” intended to make New Zealand more self-
sufficient. Government controls over wages and prices, as well as other parts of the economy, were instituted.
Large-scale investments were made in different industries, including chemical and oil refining. These policies were
unsuccessful and failed to reverse the decline of New Zealand’s economy.
In 1984, the Labour Party came to power with the goal of deregulating New Zealand’s economy. Import controls

and tariffs were lifted. Credit was made available, and many speculative investments were made in the late 1980s.
The stock market crash in October 1987 forced many companies into bankruptcy, and unemployment remained
high. Market forces, however, made their effect known in the 1990s. Inefficient industries were forced out of
business, and only those that could compete internationally remained. Throughout most of the 1990s, New
Zealand’s economy grew at a healthy rate. Despite a recession in 1998, this trend continued into the twenty-first
century. Unemployment fell and the economy became more diversified, with significant agricultural and industrial
sectors.
A growing level of external debt through the early and middle years of the 2000s—as local banks borrowed from
abroad to finance a housing and construction boom—exposed the New Zealand economy to the financial crisis of
2008–2009, leading to a severe recession that lasted from late 2008 through much of 2009, with the GDP
shrinking in 2009 by 1.3 percent. Many likened the country’s economic situation to that of Iceland, which nearly
went bankrupt when investors took their money out of local financial institutions during the credit crisis of 2008.
However, New Zealand was better situated economically, as many of its banks were owned by larger and better-
capitalized institutions in nearby Australia. The country posted modest growth in 2010, with a 2.5 percent increase
in GDP, but the country’s economy remained bogged down as external demand continued to drag down exports
and a limping world economy slowed tourism.
Tim J. Watts
 
See also:  Australia. 
Further Reading
Callaghan, Paul T. Wool to Weta: Transforming New Zealand’s Culture and Economy. Auckland, NZ: Auckland University
Press, 2009. 
Gould, Bryan. Rescuing the New Zealand Economy: Where We Went Wrong and How We Can Fix It. Nelson, NZ: Craig
Potton, 2008. 
Robinson, G.M., Robert Jude Loughran, and Paul J. Tranter. Australia and New Zealand: Economy, Society and
Environment. New York: Oxford University Press, 2000. 
Northern Rock
 
The British bank Northern Rock, once a major player in that nation’s mortgage market, went into government
ownership on February 22, 2008, several months after the Bank of England granted it an emergency loan to help
it achieve sufficient liquidity. Northern Rock was the first major British financial institution to collapse in the face of
the global financial crisis, and the first to go into government ownership in response. The bank’s condition
stabilized after its nationalization, and on January 1, 2010, it was restructured into two separate entities, both of
which remained in “temporary public ownership.”
The Northern Rock Building Society was formed in 1965 and became Northern Rock in 1997, when it made its
initial public offering on the London Stock Exchange. The bank eventually became one of the five-largest
mortgage lenders in the United Kingdom. Northern Rock’s somewhat high-risk investment model featured a strong
reliance on short-and medium-term wholesale funding. Beginning in 2006, Northern Rock also made subprime

mortgage loans. When the global capital and credit markets tightened in 2007, Northern Rock was hit with a
liquidity crisis. Although the bank had adequate assets, it could not access enough capital to honor maturing
money-market loans and other liabilities. A House of Commons Treasury Committee later found that Northern
Rock had not had adequate insurance to cover its holdings and that its investment strategies had been
unnecessarily risky.
In September 2007, the government-owned Bank of England, in its role as “lender of last resort,” granted an
emergency loan of £26.9 billion to Northern Rock. News of the loan sparked a run on the bank, and share prices
plummeted. In an effort to control public anxiety, Chancellor of the Exchequer Alistair Darling announced that the
government would guarantee all deposits held with Northern Rock; the British government thus took responsibility
for £29 billion in liabilities, in addition to the cost of the outright loan.
In the months after the Bank of England loan, several private companies bid unsuccessfully to take over Northern
Rock. Darling announced in November 2007 that, in order to protect both taxpayers and Northern Rock
depositors, any takeover bid offers would have to be approved by the UK government. By the February deadline
for bid submission, more than ten groups had made bids, including such major financial institutions as Olivant,
Cerebus, JC Flowers, Lloyds TSB, Lehman Brothers, and Bradford & Bingley. The largest of the private bids came
from a coalition consisting of Virgin Group, AIG, WL Ross, and First Eastern Investment.
Northern Rock, however, declined all bids, declaring them too far below the bank’s previous trading value. The UK
government agreed, and on February 17, 2008, Darling announced that Northern Rock would be taken into
temporary public ownership and that shareholders would be offered compensation for their shares. The next day,
trading in Northern Rock shares on the London Stock Exchange was suspended; the bank was formally
nationalized on February 22, 2008. Government-appointment chairman Ron Sandler took over its interim
leadership, later transitioning to a nonexecutive chairman role when Gary Hoffman (formerly of Barclays) became
chief executive in October 2008. The government set up the UK Financial Instruments Limited in November 2008
to manage the government’s investments in financial institutions, including Northern Rock.
In an attempt to cut costs and speed repayment of Northern Rock’s debts, the bank initiated the first of several
rounds of job cuts in July 2008, eliminating 800 positions. The bank planned to cut about one-third of all jobs—
about 2,000—by 2011. By March 2009, the bank had repaid two-thirds of the initial loan of £26.9 billion and
seemed likely be able to repay the government loan in full by the end of 2010. Other strategies to cut costs
included reducing the bank’s loan book by selling off its mortgage assets and not issuing new mortgages to
existing customers.
Northern Rock was restructured on January 1, 2010, into two separate entities: Northern Rock plc, which holds all
customer savings and about £10 billion of the Northern Rock mortgage book; and Northern Rock (Asset
Management) plc, which holds the remainder (about £50 billion) of the mortgage holdings, the remaining
government loan, and the firm’s riskier assets, including unsecured loans and subordinated debt. The asset-
management company does not accept deposits or make any new mortgage loans. Both of the new entities
remained in temporary government ownership as of early 2010, though an eventual return to the private sector
was still expected.
Northern Rock was the first major financial institution in the UK to experience problems severe enough to trigger a
government takeover. Since the takeover, however, the UK Treasury has taken on full or partial ownership of
several other major financial entities, including Bradford & Bingley, Royal Bank of Scotland, and Lloyds Banking
Group. The nationalization of Northern Rock generated some controversy in the UK, and critics have argued about
where the blame should fall for Northern Rock’s failure. In retrospect, it seemed that government intervention
helped bolster confidence in the bank as a safe place to deposit money, especially amid the turbulence of a
nationally struggling economy. Nonetheless, the Northern Rock experience remains relevant to the ongoing debate
in the United Kingdom as well as the United States about the proper role of government in financial markets.
Suzanne Julian

 
See also:  Banks, Commercial;  Recession and Financial Crisis (2007-);  United Kingdom. 
Further Reading
Brummer, Alex. The Crunch: The Scandal of Northern Rock and the Escalating Credit Crisis. London: Random House
Business, 2008. 
Walters, Brian. The Fall of Northern Rock: An Insider’s Story of Britain’s Biggest Banking Disaster. Petersfield,
UK: Harriman House, 2008. 
Norway
 
While Norway, like other European countries, felt the impact of the economic crisis of 2008–2009, it has fared
better than many other European Union (EU) countries. Its economic performance during this time is in part due to
its reliance on natural resources—especially oil and natural gas—for exports and the government’s prudence in
saving the revenues from these exports. Also, like Sweden, Norway’s situation illustrates the advantages of
remaining outside of the European Monetary Union and of the policies the EU Central Bank. Norway’s greater
freedom in promoting fiscal and monetary policies designed specifically for its economy has allowed it to avoid a
heavier impact from the global economic downturn.
Norway is a northern European country that includes over 50,000 islands. Along with Sweden, Denmark, Finland,
and Iceland, it is considered to be one of the Nordic countries. Norway has a population of approximately
4,730,000, with just over 10 percent of the population living in the capital of Oslo. The country is governed by a
constitutional monarchy with a unicameral parliament. Norway has a highly developed social welfare system, a
literacy rate of 100 percent, and a high standard of living for its citizens. Government expenditure in the economy
for the years 1999 to 2008 constituted an average of 43.96 percent of the nominal gross domestic product (GDP;
nominal GDP is the value of goods and services during a given year measured in current prices), compared to
42.54 percent for the euro-area countries.
Compared to most of its neighbors, the Norwegian economy is relatively small. However, in terms of GDP per
capita, Norway ranks second in the world, after Luxembourg, with a GDP per capita that is two-thirds greater than
that of the United States. Norway has the second-largest sovereign wealth fund after Abu Dhabi. For 2006,
Norway placed second among EU and European Free Trade Association (EFTA) countries for contributions to
official development assistance (ODA) to developing countries, exceeding the United Nations’ target of 0.7 percent
of donors’ gross national product (GNP).
 North Sea Riches
Norway has immense offshore oil and natural gas deposits. As of 2006, it was the third-largest net oil exporter in
the world and the second-largest supplier of natural gas to continental Europe. However, production has begun to
decline and plans are afoot for petroleum exploration in other regions. The revenues from oil and gas exports are
used to support state ownership of companies and to underwrite the social welfare networks. For domestic
consumption, Norway relies almost entirely on domestically generated hydropower as its source of electricity.
Norway is one of the world’s largest exporters of fish, and this sector employs many of the inhabitants of its
remote coastal regions. Norway has little arable land and has to import most of its food.

The country’s external debt was eliminated in the mid-1990s. Cognizant of the fact that its reserves will ultimately
be depleted, Norway prudently keeps a sizable amount of revenue from oil and gas export earnings. With the
increased demand and consequent price increases for petroleum, Norway’s economy flourished, particularly during
2004–2007. An indicator of Norway’s economic health is its unemployment rate, which was 2.6 percent in 2008,
the lowest among the Scandinavian countries and comparing favorably with both the euro-area average of 7.4
percent and the Organisation for Economic Co-operation and Development (OECD) average of 5.9 percent.
Norway has been a member of North Atlantic Treaty Organization (NATO) since its inception in 1949. Primarily
driven by possible threats to its sovereignty and control over the petroleum and fisheries industries in the region,
Norway has twice rejected EU membership in referenda, in 1972 and 1994. However, as a member of the EFTA’s
European Economic Area (along with Iceland and Liechtenstein), Norway participates in the EU market and
contributes to its funds and activities.
Between 1995 and 2000, real GDP grew at an average rate of 3.78 percent, which is high relative to the euro-
area and OECD averages of 2.68 percent and 3.23 percent, respectively, for the same period. (Real GDP is the
market value of all the goods and services produced within a country during a given year, measured in constant
prices, so that the value is not affected by changes in the prices of goods and services.) With the onset of the
recession in 2001, real GDP grew at an average rate of 1.50 percent over 2001–2003, returning to a healthier
3.03 percent over 2004–2006, concomitant with the construction boom related to private residence construction.
The rate of growth of real GDP increased to 3.7 percent in 2007 but started a downward trend in 2008 and 2009.
GDP fell by 1.4 percent in the latter year, though it squeaked into positive territory in 2010, with a hike of 0.4
percent. Continuing turmoil in Europe, associated with the sovereign debt crisis hitting various Eurozone countries,
was expected to dampen growth in 2011 and perhaps beyond.
Although, like any other resource-based economy, Norway’s oil revenues have fluctuated with the externally
determined price of oil, the government’s fiscal conservatism has somewhat insulated its economy from the world
financial turmoil that began in 2008. Nonetheless, toward the end of that year, a number of factors began to affect
the country’s economy: high inflation, high interest rates, accelerating decline in house prices, weakening demand
for exports, moderate increases in unemployment (with the manufacturing and construction sectors being hardest
hit), and a drop in consumer spending. In the third quarter of 2008, real house prices decreased by 6.8 percent, a
substantially greater change than the average decline of 1.8 percent for the euro area. By 2008, however, the
housing sector in Norway had recovered dramatically, with prices posting an impressive 8.4 percent gain in the
first quarter of 2011, though this came in the wake of a significant drop of roughly 10 percent in 2009.
Norway’s financial sector has not been as hard hit by the economic downturn as that of other northern countries
and Western European nations, partly due to the fact that the Norges Bank (Norway’s central bank) is independent
of the European Central Bank. The Norges Bank is taking action to ensure that this sector remains relatively
stable by reducing key interest rates. New banking funds were also set up, one to provide capital for banks and
another to buy company bonds.
Because of its foresight in saving funds from oil revenues and the proactive measures taken by its central bank, it
is unlikely that Norway will be affected to the same extent as its neighbors by the decade’s financial and economic
turbulence.
Marisa Scigliano
 
See also:  Denmark;  Finland;  Iceland;  Sweden. 
Further Reading
Central Intelligence Agency. The CIA World Factbook. New York. Skyhorse, 2009. 

Economist Intelligence Unit (EIU). Country Report—Norway.  London: EIU, 2009. 
Economist Intelligence Unit (EIU). ViewWire—Norway. London: EIU, 2009. 
Organisation for Economic Co-operation and Development (OECD). OECD Economic Outlook  no. 84. Paris: OECD, 2008. 
Statistical Office of the European Communities (Eurostat). Eurostat Yearbook 2009.  http://epp.eurostat.ec.europa.eu
 
Oil Industry
 
One of the largest and wealthiest industries in the world, the oil industry influences virtually all other sectors of the
economy, given how reliant modern business and civilization itself is on petroleum as an energy source. While
immensely powerful and profitable, the oil industry has been notoriously volatile, going back to its earliest days in
the Pennsylvania oil fields of Civil War–era America. Indeed, two of the hallmarks of its history have been the
sudden swings from shortage to glut and back to shortage. Through much of the history of the oil age, the product
has existed in abundance, leading to low prices and fierce competition, though since the oil shocks of the 1970s,
there have been periodic shortages that have led to much higher prices and profits. Throughout this history, oil
companies have tried to respond to this volatility—sometimes successfully, sometimes not—through consolidation
and other measures to limit what they consider destructive competition and overproduction.
Before considering the industry’s history, it is useful to understand what it comprises. The oil industry includes
three basic activities: (1) exploration, drilling, and pumping of crude oil from beneath the earth’s surface; (2) the
refining of that crude into useful petroleum products—everything from tar to home heating oil to gasoline to aviation
fuel; and (3) the distribution of those products to industry and consumers, through gasoline stations, home oil-
heating suppliers, and so forth. Major international oil companies may have operations in all three areas, though
the first is often done in cooperation with the national oil companies of petroleum-exporting countries while the
third is shared with many independent operators.
 Birth of an Industry
The oil industry did not spring from nothing—it was an event waiting to happen. It was an accepted fact in the
nineteenth century that anyone who discovered an abundant and cheap source of oil would “strike it rich,” as
kerosene was already the preferred source of lighting fuel. In 1859, the event happened, but not before the
Pennsylvania Rock Oil investors backing Edwin Drake, a retired railroad conductor, had given up hope on drilling
rather than digging for oil. Prior to Drake, most people in the nascent industry believed that oil could best be
obtained by digging into the ground, much in the way coal was exploited. Drake’s decision to drill led to the first
great oil strike in world history.
Immediately, the area around Titusville, Pennsylvania, became a boomtown, as a dollar invested in a producing
well could yield thousands of dollars in profits. But there was an inherent problem in the economic model of oil at
the outset—a problem the industry shared with much commodity and agricultural production. That is, revenue is
price multiplied by volume. Since there is nothing one can do about price, the secret of producing untold wealth
was to maximize production before the price fell or the oil field went dry. Drillers either made their fortunes or went
broke trying. Wells were drilled with wild abandon, pumping “full out,” and soon the market was flooded with
unwanted oil.

Maximizing revenue by maximizing volume works well when quantity demanded exceeds quantity supplied, but
when that maximizing strategy leads to overproduction and supply exceeds demand, prices drop, sometimes
precipitously. Indeed, oil prices plunged from $10 to 10¢ per barrel in less than a year, making the container more
valuable than the oil inside it. Pumping oil continued unabated as prices spiraled downward because individual
drillers could still maximize revenue by maximizing production as long as the price of crude oil exceeded the cost
of extraction. One driller showing restraint and slowing his rate of production only meant less revenue for him as
others pumped with all their might. Drillers collectively seemed unable to sense the repercussions of what
maximizing production today would do to price tomorrow; but even if they did, there was nothing they could do
about it. As boom went bust, overnight fortunes evaporated into a spate of bankruptcies, since money was entirely
reinvested in drilling rigs, which had lost all their value. Collapsing oil prices were not all that brought on the bad
times; too many wells operating full out were sucking oil fields dry in no time.
 Consolidation and Cartel
Ohio businessman John D. Rockefeller was the first to come up with a solution to the problem of overproduction.
Rockefeller recognized that it was impossible to control drillers, so he focused instead on the refining end of the
business. Drilling had a low barrier of entry whereas refining posed a higher barrier both financially and
technologically. Through a series of acquisitions, rarely ceded voluntarily, and by cutting deals with railroads to
transport his oil for less cost than his competitors, sometimes skirting law and regulations, Rockefeller in ten short
years was able to gather 90 percent of the refining industry under his corporate umbrella, the Standard Oil Trust.
With such a commanding control of the refining business, Rockefeller controlled the market for oil products and
the drillers. To his defenders, Rockefeller was a trust maker who brought an industry from disorder to order,
thereby eliminating wasteful booms and busts in the oil patch and guaranteeing to customers a plentiful supply of
standard products (products that the public could rely on) at a reasonable price. But much of the public and the
government saw him in a different light. In 1911, the U.S. Supreme Court forced the breakup of Standard Oil,
which meant that Rockefeller had to exchange his shares in Standard Oil for a group of companies. The
Rockefeller family fortune expanded after Rockefeller retired, as these companies set out on their individual paths
to develop new businesses that the Standard Oil Trust, as a single corporate entity, had been slow in doing. The
value of some of these companies, individually, was more in five years after the breakup than the entirety of
Standard Oil at the time of the breakup.
With the passing of the Standard Oil monopoly, stability in the oil patch was maintained on a global scale by the
oil power brokers of the day, including Walter Teagle, of Standard Oil of New Jersey (later Exxon) and Henri
Deterding, of Shell. Along with other oil magnates, they established a system of global pricing of oil at a social
affair held in a Scottish castle in 1928, calling for cooperation in production and the sharing of incremental demand
increases among a cartel of supposedly competing oil companies. Their system stabilized the price at a healthy
level for the oil companies as long as others joined, which they did. With a mechanism in place for allocating
incremental production to meet growing demand among the participating oil companies, the global oil business,
with the exception of Soviet oil, was under the control of a cartel of oil companies. Of course, the involvement of
U.S. oil companies in this arrangement to fix pricing and production was in direct violation of the Sherman Antitrust
Act. The Rockefeller dream of world control over oil, for the most part, had finally come true, although not with
domination vested in the hands of an individual, but in a small group of executives who, in the aggregate,
controlled most of the world’s oil. The success of this agreement hinged on all these individuals cooperating,
which was difficult to achieve except during times of falling oil prices.
 East Texas Oil Boom
In 1930, only two years after the system was set up, price stability was threatened by yet another mammoth oil
discovery of the kind that continued to plague the oil companies until the oil crisis in the 1970s. The East Texas
oil boom, coming at the time of the Great Depression, created a glut, and oil prices collapsed locally to 10¢ per
barrel. Teagle and Deterding were powerless to stop the flood of oil coming into the market because they did not

control the East Texas oil fields. But those involved in the boom sought a solution of their own by requesting
federal and state intervention. The state governments of Texas and Oklahoma obliged, declaring martial law on
the grounds that the independents were squandering a valuable natural resource, particularly at 10¢ per barrel.
Using conservation to justify their actions and the local militia to enforce their will, states succeeded in slowing oil
production significantly. Through the Texas Railroad Commission, a rationing system to control production was
established, and oil prices rose. Government action to protect and conserve a natural resource served the
interests of the global oil cartel. Thus, capitalism and conservation joined hands with a common objective, but
different goals. Deterding’s pooling arrangement among the oil cartel members and the Texas Railroad
Commission’s rationing of production stabilized the world price of oil. Both actions were valuable lessons for the
Organization of Petroleum Exporting Countries (OPEC) when it gained control over oil prices and production in the
1970s.
 Birth of OPEC
In 1960, Saudi Arabia, Iran, Iraq, Kuwait, and Venezuela created OPEC, not necessarily to raise oil prices but to
prevent further reductions in posted prices being forced on them by the major oil companies. The original unity of
purpose was gone by the second OPEC meeting in 1961, when a rough-and-tumble battle broke out among
OPEC members as each sought to garner a larger export volume at the expense of others. OPEC was behaving
no differently than the earliest oil drillers; it was every producer for itself.
By no measure could OPEC be considered a success prior to the oil crisis in 1973. There was little coordination
among the members, and politics kept getting in the way of negotiations. Meanwhile, new sources were coming on
stream, such as Nigeria, putting more pressure on OPEC’s approach of maximizing revenue by maximizing
production, another reminder of the oil industry’s early days. In 1965, OPEC failed at an attempt to gain control
over future increases in production just as it failed to gain control over current production. The major oil
companies, meanwhile, were trying to restrain production to prevent further declines in oil prices. The irony is that
in only ten years, OPEC would take over the oil companies’ role of restraining production to control prices. The
role reversal would not be complete, as the OPEC idea of what the market could and should pay for oil in the
1970s would be radically different than that of the oil companies in the 1960s.
The 1967 Six-Day War between Israel and Egypt sparked the first Arab boycott. The war was over before the
boycott had any effect, and the boycott was doomed anyway when Venezuela and Iran refused to join. Even the
formation of the Organization of Arab Petroleum Exporting Countries (OAPEC) within OPEC in 1968 did not
succeed in strengthening the resolve of OPEC to bring order to the oil market. Order, of course, meant maximizing
the respective production volume of each member to maximize revenue. Oil company attempts to rein in
production to maintain prices, which varied for each member of OPEC, irritated the oil producers, who now had to
contend with new oil production from Qatar, Dubai, Oman, and Abu Dhabi.
The 1973 oil crisis was not caused by a shortage of oil. In fact, the greatest worry right up to the eve of the crisis
was how to keep new production from flooding the market and further weakening oil prices. The producers were
worried about anything that would shrink their export volumes. But a series of crises—including the 1973 Arab-
Israeli War and subsequent boycott by Arab oil producers of perceived pro-Israeli Western countries, including the
United States, followed by the 1979 Iranian Revolution—led to cuts in production even as demand remained
strong. The result was spot shortages and a dramatic run-up in crude prices, from around $4 to $40, in non-
inflation-adjusted dollars, between 1973 and 1980.
 Oil Crises and Responses
From the birth of the automobile age in the early twentieth century, oil consumption has doubled about every
decade. Even the Great Depression did not dampen growth in oil consumption, but the age of oil did not begin in
earnest until after the Second World War. In 1960, OPEC was supplying 38 percent of world oil; this increased to
47 percent in 1965 and 56 percent in 1973, meaning that OPEC exports were growing faster than world oil

demand. During this time, the United States was emerging as a major world importer as its production began a
long-term decline.
With the rise of OPEC, the world no longer had to face a cartel of oil companies, but instead a cartel of oil-
producing states. The greatest transfer of wealth in history—from oil-consuming to oil-producing states—would
occur with the quadrupling of oil prices in the 1970s. But changes in the world of energy were at work that would
come back to haunt the oil producers. Among these was a worldwide economic decline that reduced overall
energy demand. High oil prices instigated a desperate search for alternative sources to oil, leading to a
resurgence of coal, an accelerated pace in building nuclear power plants, a greater reliance on natural gas and
anything else not called oil, including wood-burning electricity-generating plants.
There were also great gains in energy efficiency, whereby cooling a refrigerator, heating a home, and running an
automobile, truck, locomotive, marine, or jet engine could be achieved with significantly less energy. Conservation
of energy took the form of keeping indoor temperatures higher in summer and lower in winter, driving the family
car fewer miles, and recycling energy-intensive products such as glass, aluminum, and paper. Companies set up
energy managers to scrutinize every aspect of energy use in order to identify ways to reduce consumption.
In addition to slashing demand, high-priced oil caused an explosion in non-OPEC crude supplies, best exemplified
in the North Slope of Alaska and in the North Sea. The North Slope of Alaska is an inhospitable place to develop
and operate an oil field and necessitated the construction of an 800-mile-long pipeline to the port of Valdez in
southern Alaska over mountain ranges and tundra. North Slope production peaked at 2 million barrels per day
(bpd) a few years after the pipeline started operating in 1977. The North Sea was an even greater challenge, with
its hundred-knot gales and hundred-foot seas. Floating oil-drilling platforms explored for oil in waters a thousand
feet (304.8 meters) deep. “Oceanscrapers,” structures taller than the Empire State Building in New York City,
were built on land, floated out to sea, and flooded (carefully) to come to rest on the ocean bottom as production
platforms. North Sea oil started with 45,000 bpd of output in 1974 and grew to over 500,000 bpd in 1975, to 1
million bpd in 1977, to 2 million bpd in 1979, to 3 million bpd in 1983, eventually peaking at 6 million bpd in the
mid-1990s. Every barrel from the North Slope and North Sea was one barrel less from the Middle East OPEC
producers.
Oil exporters dictated prices after the 1973 oil crisis, but continually changing prices implied that OPEC could not
control the price as well as the oil companies had. When oil prices fluctuate widely, no one knows, including the
oil producers, what tomorrow’s price will be. This provides speculative opportunities for traders who try to outwit or
outguess oil producers. All they needed was a place where they could place their bets. Once the traders started
placing bets, buyers and sellers of oil had an opportunity to hedge their investments against adverse price
changes. In the early 1980s, the New York Mercantile Exchange (NYMEX) started trading futures in heating oil,
then gasoline, and finally crude oil. First attracting primarily speculators, soon oil companies as buyers and oil
producers as sellers started trading. The development of a cash and futures market, with contracts that could be
settled in monetary or physical terms, eventually eroded the oil producers’ control over price. Since the early
1980s, the primary determinant of oil prices has been the relationship between supply and demand. The oil
producers (OPEC) attempt to influence price by cutting back or expanding production, and in this indirect way to
affect the price of oil. But they no longer dictate price as they had in the years immediately following the 1973 oil
crisis.
 Collapsing Prices
With consumers doing everything they could to reduce oil consumption, and with every OPEC and non-OPEC
producer operating full out, taking advantage of the price bonanza to maximize revenue, it was becoming
increasingly difficult to maintain price. There had to be a swing producer to maintain a balance between supply
and demand in order to keep prices high, and that swing producer was Saudi Arabia.
Saudi Arabia’s production was initially boosted as replacement crude during the Iranian Revolution in 1978 and
1979 and during the early years of the Iran-Iraq war. After production in Iran and Iraq was restored, Saudi Arabia

had to cut back sharply to maintain price. With OPEC members producing full out, Saudi Arabia had to cut
production again to keep prices from eroding further. Saudi Arabia was now playing the same historical role
played by the United States when the Texas Railroad Commission had the authority to control oil production to
maintain oil prices. (The United States ceased being a swing producer in 1971 when the commission authorized
100 percent production for all wells under its jurisdiction.) This meant that it would not allow production to meet
demand but would allow producers to pump as much as they wanted, when they wanted.
Being a swing producer means that one has excess capacity that can be released onto the market if prices get
too high for consumers of a product or that can be cut back when prices fall too low for producers. While at first
glance it would seem in Saudi Arabia’s interest to maintain the highest possible prices it could get, there were
other factors that led to it wanting more stability. First, sky-high oil prices might encourage development of
alternative forms of energy that would ultimately hurt the oil industry or lead to exploration for new sources of oil,
which is indeed what happened after the oil shocks of the 1970s. There was also a financial component. Saudi
Arabia invested large chunks of its oil revenue in Western securities; if oil prices went so high as to cripple those
economies, Saudi finances would suffer.
In 1985, with cessation of exports just over the horizon, Saudi Arabia was at the end of its tenure as swing
producer. Something had to be done. Saudi Arabia again unsheathed the oil weapon, not against the consuming
nations but against its fellow OPEC members. Saudi Arabia opened the oil spigot and flooded the market with oil,
causing oil prices to collapse below $10 per barrel and threatening to financially wipe out OPEC. Saudi Arabia
then forced its fellow producers to sit around a table and come to an agreement on production quotas and a
mechanism for sharing production cutbacks whereby Saudi Arabia would cease to be the sole swing producer.
The cartel would now act as a cartel.
 Price Hikes of 2007–2008
The second period of high oil prices was from 2007 to late 2008, with the all-time record price of $147 per barrel
set in 2008. Economic growth, fueled by enormous personal debt acquisition by U.S. consumers, resulted in
higher crude oil growth in both the United States and Asia, particularly China, as manufacturer for the world.
Spare capacity for the OPEC producers fell to about 1–2 million barrels per day, a far cry from the late 1970s and
early 1980s, when Saudi Arabia could make up for the cessation of exports from Iran of nearly 6 million barrels
per day and still have capacity to spare. A low level of excess capacity is just another way of saying that quantity
demanded is getting too close to the quantity supplied, which can cause huge jumps in price as buyers start
bidding up to ensure supplies.
The spiking of prices in 2007 and 2008 was not the same as in 1973, when buyers and sellers struggled with
each other to control prices. The cause of the second era of high oil prices was simply a lack of spare capacity:
demand getting too close to supply. In 2009, oil prices were restored to the $60–80 per barrel range, propped up
by the continuing growth of oil demand in China and India despite the economic collapse blanketing the rest of the
world. From 2009 to 2011, the trend was generally upward, as oil prices reached roughly $100 a barrel in
November of the latter year. This reflected continuing high demand in Asian markets, as well as the after-effects of
political turmoil in the Middle East, including the uprising that overthrew Muammar Qaddafi’s regime in Libya in
late 2011.
Oil prices reflect a continuing tightness in the relationship between supply and demand. Any resurgence in
demand in the United States would cause a surge in oil prices again and potentially another round of boom and
bust. However, if the supply of oil is constricted from lack of discoveries below ground and governments above
ground prohibiting drilling, we might be entering an era of perpetual high prices. Oil production does not have to
peak for prices to act as though peaking is occurring. A lack of major discoveries to compensate for aging oil
fields and continuing growth in demand will keep spare capacity too anemic to induce a bust in the oil patch.
Roy Nersesian

 
See also:  Middle East and North Africa;  Oil Shocks (1973-1974, 1979-1980). 
Further Reading
Barsky, Robert B., and Lutz Kilian.  “Oil and the Macroeconomy Since the 1970s.” Journal of Economic Perspectives 18:4
(2004): 115–134. 
Feldman, David Lewis, ed. The Energy Crisis: Unresolved Issues and Enduring Legacies. Baltimore, MD: Johns Hopkins
University Press, 1996. 
Nersesian, Roy. Energy for the 21st Century. Armonk, NY: M.E. Sharpe, 2010. 
Verleger, Philip K. Adjusting to Volatile Energy Prices. Washington, DC: Institute for International Economics, 1993. 
Yergin, Daniel. The Prize: The Epic Quest for Oil, Money, and Power. New York: Touchstone, 1992. 
 
Oil Shocks (1973–1974, 1979–1980)
 
During the mid-and late 1970s, the U.S. and global economies were hit by two so-called oil shocks, in which
shipments of petroleum from the Middle East underwent dramatic curtailments because of war and political
upheaval, resulting in price spikes and supply shortages in many oil-importing countries.
The oil shocks had both immediate and long-term consequences for the United States and the world. The sudden
and dramatic rise in prices, particularly in the aftermath of the first oil shock in 1973–1974, accelerated a global
economic downturn that was marked by inflation and high unemployment, a situation that defied the traditional
understanding of and remedies for recessions. The inability of governments to deal with such a recession led to
dramatic political realignments in the United States and some industrialized countries, with liberal governments
giving way to conservative ones.
In addition, the two oil shocks undermined one of the pillars of the post–World War II global industrial boom:
cheap energy. At first, governments responded with measures to conserve energy and find alternatives to
petroleum. Ultimately, however, it was market forces that solved the “energy crisis” ushered in by the oil shocks,
at least temporarily. High petroleum prices encouraged new exploration and the development of new extraction
technologies, which flooded the market with cheap oil from the mid-1980s through the early 2000s. One effect of
this was to undermine conservation and alternative energy efforts, until more systemic shortages—a result of
stagnant production and soaring demand in emerging economies such as China and India—surfaced in the mid-
2000s.
Long used for lighting, petroleum emerged as a major source of energy in the early twentieth century, a result of
major new finds—in the United States, Russia, and the Middle East—and major new markets, most notably motor

vehicles and electric generating plants. By the early post–World War II era, petroleum vied with coal to become
the world’s most important source of energy. The dramatic rise in consumption was balanced by increases in
production, mostly in the Middle East, keeping prices low. In the early 1960s, for example, a gallon of gas in the
United States sold for roughly the same price, adjusted for inflation, that it had sold for in the 1920s.
Motorists line up for gas in Vienna, Austria, during the oil shortage of 1973–1974. Triggered by an OPEC
embargo in response to U.S. policy in the Middle East, the crisis had enduring economic consequences across
the industrialized world. (Rue des Archives/The Granger Collection, New York)
 1973–1974
Just as it is today, the Middle East—the source of much of the world’s oil exports—was a volatile region in the
1950s and 1960s, as rising Arab nationalism challenged both Western interests and the existence of Israel, a
nation founded in 1948 on land that many Arabs believed was rightfully theirs. Fearing an onslaught, Israel
launched a preemptive attack on three of its Arab neighbors—Jordan, Syria, and Egypt—in June 1967, quickly
defeating them and seizing parts of their territories. In response, three of the largest Arab oil exporters—Kuwait,
Libya, and Saudi Arabia—hastily imposed an oil embargo on the United States and other Western countries, both
as punishment for supporting Israel and as way to shift their foreign policy away from the Jewish state in the
future. The embargo not only failed in this regard, but also was largely a nonstarter, as a result of a lack of
coordination among the three governments and an oversupply of oil on the world market.
Three key changes took place between 1967 and the next Arab-Israeli War of 1973. The first, and perhaps more
important, change had to do with world oil supplies. By the late 1960s, global consumption of oil was beginning to
approach production capacity, leaving little slack in the event of a disruption in supply. Second, oil exporters had
enjoyed little of the gains that such a tight market should have produced. This was because the United States had
pulled out of the Bretton Woods Accord in August 1971, a World War II–era agreement that pegged the world’s
major currencies to the U.S. dollar and the U.S. dollar to the price of gold. Pulling out of Bretton Woods led to a
devaluation of the dollar, and because virtually all international oil purchases were made in dollars, this meant
less money in the coffers of oil-exporting states. The third change had to do with Arab politics. In 1968, several
conservative Arab oil exporters founded the Organization of Arab Petroleum Exporting Countries (OAPEC),
originally a kind of antiboycott group dedicated to politically leveraging their oil output in more moderate ways than
a boycott. But the inclusion of more Arab nationalist regimes, such as Algeria, Egypt, and Syria—as well newly

radicalized Libya—led to a radicalization of the organization.
Soon after the 1967 Arab defeat, leaders in Egypt and Syria began to plot their response to Israel’s seizure of
Arab territories, which they took in a coordinated surprise attack on October 6, 1973. In support of its fellow Arab
governments and to punish the West for its supposedly pro-Israeli policies, Saudi Arabia—the world’s greatest
petroleum exporter—along with fellow Islamic exporters Iran, Iraq, Kuwait, and the United Arab Emirates, posted a
unilateral price hike of 17 percent, to nearly $4 a barrel. Then, on October 16, they imposed a sales boycott on
the United States, to punish it for supporting Israel in the war. In early November, OAPEC announced a 25
percent cut in production, with a further 5 percent cut threatened. By early 1974, the boycott—as well as the
subsequent panic over oil supplies—had driven up prices by some 400 percent over prewar levels, to about $12
per barrel. Shortages began to be felt in the United States and other oil-importing countries, leading to long lines
at gasoline stations, a rationing system for the sale of gasoline in many countries, and unpopular conservation
measures in the United States, including a new national speed limit of 55 miles per hour (88.5 kilometers per
hour) and year-round daylight savings time.
The boycott was called off by all participants except Libya in March 1974, but the move had little effect on oil
prices, as the tightening of the ratio between supply and demand allowed the Organization of Petroleum Exporting
Countries (OPEC)—a larger group that included most of the noncommunist world’s largest producers at that time
—to continue to dictate high prices. Throughout much of the mid-1970s, prices remained at a new plateau of $10
to $20 per barrel, contributing to sluggish growth in the U.S. and global economies. Normally, economic weakness
translates into stable or even lower prices and wages. But not this time—the dramatic hike in oil prices rippled
through the economy in the form of higher prices for the many goods whose production and distribution depended
on significant inputs of energy. Thus, the standard Keynesian remedy used by Washington, D.C., and other
governments since the Great Depression—deficit spending to increase aggregate demand and thereby boost
investment, production, and employment—was largely off the table, as it would only increase already crippling
inflation rates.
 1979–1980
With the Iranian Revolution of 1979, which disrupted production for the world’s second-largest oil exporter, came
yet another blow to world oil supplies and the price of petroleum. Although this was not a concerted effort by oil
producers, OPEC took advantage of the situation by posting two price hikes that together yielded more than a
one-third increase in the price of a barrel of oil, to nearly $17. A panicked world market—responding to the
revolution and to Iraq’s invasion of Iran in 1980—sent prices to the stratospheric level of nearly $40 a barrel (about
$100 in 2009 dollars). Once again, Americans experienced lines at the gas station and rampant inflation. The
administration of President Jimmy Carter responded by deregulating the price of domestically produced oil—in the
hope that this would spur production and bring prices down—and by offering a plan to wean America from its
dependence on foreign oil by promoting conservation and the development of alternative energy sources.
The twin oil shocks of the 1970s also provided a windfall for the OPEC countries. Between 1972 and 1980, net
oil-export revenues for OPEC soared from less than $40 billion annually to nearly $300 billion in non-inflation-
adjusted dollars (or from about $100 billion to nearly $600 billion in 2009 dollars). Particularly in the Persian Gulf
states, the influx of money resulted in a sudden upswing in prosperity rarely seen before in world history. Massive
construction projects were soon under way, with unprecedented consumption on the part of ordinary citizens. The
rise in prices also led to flush economic times in such oil-producing American states as Texas and Oklahoma.
But the good times were not to last. While energy conservation helped drive down demand to a degree, increased
production—spurred by high prices—ultimately undid the same dramatic upswings. High prices prompted
aggressive exploration of new oil fields (e.g., in the North Sea off Great Britain and Norway) and the development
of new technologies that allowed for the more efficient extraction of oil from existing fields. All of the new supply
led to a dramatic decline in price. Between the mid-1980s and the early 2000s, the price of a barrel of oil hovered
between $10 and $30. Adjusted for inflation, oil prices hit a post–World War II low in 1998.

The dramatic decline helped ease the United States through a steep recession in the early 1980s and contributed
to an economic boom that would continue through the rest of the decade and, following another brief recession in
the early 1990s, into the early twenty-first century. At the same time, OPEC producers experienced a dramatic
decline in revenue until the end of the century, to between $100 billion and $150 billion annually, that severely
crippled their economies, as well as those of Texas and Oklahoma. The organization also lost much of its
geopolitical clout, as non-OPEC members, such as Angola (which joined in 2007), Mexico, Canada, Great Britain,
Norway, a newly free-market Russia, and the United States (with its vast new oil fields in Alaska), began to
outproduce OPEC member states.
But just as the market undermined OPEC’s efforts to drive up oil prices in the 1980s and 1990s, so market forces
contributed to a dramatic upswing in the middle to late 2000s. This time, however, the impetus came from
consumption rather than production, as developing countries—most notably China and India—industrialized rapidly.
Between early 2003 and July 2008, when the benchmark price on the New York Mercantile Exchange hit a record
high, the price of a barrel of crude oil soared from just under $30 to $147.30. Because the rise was so much
steeper than the increased demand seemed to warrant—even taking into account such shocks as Hurricane
Katrina’s impact on the oil industry in the Gulf of Mexico in 2005 and upheavals in the Middle East such as the
Israeli-Hezbollah War in 2006—there was much talk in the media and among experts about the influence of oil
speculators on crude prices. Whatever the case, by late 2008, the market once again was exerting its influence,
as a demand-reducing global recession—to which the spike in prices had contributed—brought prices back down
to $40 to $60 per barrel.
Aside from the impact of oil-price fluctuations on the global economy—and vice versa—the effects are more
lasting, because of two virtually incontrovertible facts about oil production and consumption. The first, often
referred to as “peak oil theory,” is that oil is a finite global resource and, more controversially, that the world is
now reaching a point at which reserves and production gradually will diminish. The second is that the burning of
oil and other carbon-based fuels leads to climate change and a host of cataclysmic effects, including massive
flooding in coastal areas and prolonged drought in more arid regions. Both of these forces impel humanity to find
alternative sources of energy, and with each upturn in prices, efforts are made to do so. Still, the fluctuating nature
of oil prices often undoes such efforts, undermining the long-term strategies necessary to build a global economy
that relies less on fossil fuels.
James Ciment and Nuno Luis Madureria
 
See also:  Middle East and North Africa;  Oil Industry;  Recession, Stagflation (1970s). 
Further Reading
Barsky, Robert B., and Lutz Kilian.  “Oil and the Macroeconomy Since the 1970s.” Journal of Economic Perspectives 18:4
(2004): 115–134. 
Bohi, Douglas R., and Michael A. Toman. The Economics of Energy Security. Norwell, MA: Kluwer, 1996. 
Feldman, David Lewis, ed. The Energy Crisis: Unresolved Issues and Enduring Legacies. Baltimore: Johns Hopkins
University Press, 1996. 
Verleger, Philip K. Adjusting to Volatile Energy Prices. Washington, DC: Institute for International Economics, 1993. 
Yergin, Daniel. The Prize: The Epic Quest for Oil, Money and Power. New York: Touchstone, 1992. 
Over-Savings and Over-Investment Theories of the Business

Cycle
 
Over-savings and over-investment theories of the business cycle are distinct theoretical models that explain the
cyclical behavior of the economy. While there are overlapping connections between these two theories, they
usually are treated as distinct from one another.
 Over-Savings Theories
In a macroeconomic context, over-savings means that households are saving a greater proportion of their income.
The aggregate effect of greater savings is a decrease in aggregate consumption, which leads to a decline in
aggregate expenditures, or demand. In essence, the decision by households to save more leads to a decrease in
aggregate demand with a corresponding decrease in national income. This fall in aggregate demand triggers a
business cycle downturn.
The question of over-savings, also referred to as underconsumption, has a long history in economics. In The
Wealth of Nations (1776), Adam Smith identified parsimony, or saving, as the source of capital, or investment by
entrepreneurs in business. One of Smith’s followers, Jean-Baptiste Say, established a fundamental law of markets:
supply creates its own demand. Say’s law asserted that it was impossible for over-production to occur in an
economy. There could never be underconsumption or over-savings, Say wrote in his Treatise on Political
Economy (1803).
 Debate over Say’s Law
One of the most famous debates about the over-savings/underconsumption theory of the business cycle took
place between Thomas R. Malthus and David Ricardo, both classical economists. Ricardo supported Say’s law,
arguing that all savings become investment expenditures—hence, there could be no over-savings. Malthus put
forth the over-savings doctrine that not all savings become investment; therefore, there could be an over-
production of goods and services. In the twentieth century, John Maynard Keynes, the most famous economist of
that century, said that it was unfortunate that Ricardo had won the debate. He believed that Malthus’s theory
helped explain the business cycle, which industrial economies had experienced periodically beginning early in the
era of industrial capitalism, starting around 1750.
Nearly two centuries later, Keynes constructed a more complete and comprehensive theory of the business cycle
that encompassed the potential of aggregate over-savings. His theory of the business cycle took into account the
disequilibrium between planned savings and actual savings, as well as a disturbance between planned business
investment and realized business investment. This was the first comprehensive, modern model of the business
cycle, and it had a role for underconsumption/over-savings.
 Over-Investment Theories
Over-investment theories of the business cycle explain the role of aggregate investment in the cyclical process of
the economy. There are several modern approaches to the over-investment theory of the business cycle. These
theories do not accept the assumption that financial markets are consistently efficient and tend toward stability.
These considerations are key aspects of modern financial economics. However, alternative approaches to the
business cycle identify a variety of factors that can destabilize financial markets. These include central bank
policies that inject additional liquidity into the economy; sudden increases in prices that lead business to a belief
that revenues are rising unexpectedly; innovations that stimulate significant business investment; and the
opportunity to create new types of financial securities. In any of these situations, businesses will increase their
level of investment beyond the previous equilibrium level of aggregate investment in the economy.
One theoretical explanation of the over-investment approach to the business cycle holds that a central-bank

expansion of the money supply will create a cycle of price movements. The money supply increase leads to price
increases, which stimulate businesses to increase investment. This is the core of the Austrian theory of the
business cycle. The economy experiences over-investment, which expands production capacity. As a result, there
is inadequate aggregate demand for the supply of goods and services, and price levels begin to decrease. The
deflation leads to a cyclical downturn in the economy, and the economy moves to a depressed state.
A second over-investment approach to the business cycle was put forth by Hyman Minsky, who argued that an
increase in financial liquidity in markets would spur a process of securitization. In essence, new financial securities
would be created. A speculative surge would grow in response to the new securities, and financial asset inflation
would begin. Eventually, a financial bubble would emerge in the broader securities markets. The bubble inevitably
would burst, causing a wave of uncertainty in these markets. This uncertainty would cause business investment to
fall, and the economy would begin a downturn in the business cycle.
The so-called Great Recession of 2007–2009 drew attention to heterodox theories of economic crises. In the mid-
2000s, Ben Bernanke, chair of the Federal Reserve Board of Governors, stated that excess liquidity in financial
markets resulted from a “glut” of global savings—a clear reference to the over-savings theory. However, the
economic problems that led to the financial crisis of the first decade of the twenty-first century were caused by
over-investment. The Post Keynesian theory of the business cycle associated with Minsky underscored the
increased securitization and creation of a speculative bubble. The growth of the derivatives markets from the mid-
1990s to the late 2000s had created a new set of financial securities that allowed excess liquidity to flow into the
housing markets in a highly speculative fashion—high-risk mortgages bundled as derivatives and sold as
creditworthy investments. By 2008, the derivatives market was valued at $660 trillion, at a time when the global
economy had a gross domestic product of approximately $60 trillion ($14 trillion in the United States). The
speculative fever broke, setting off worldwide panic in financial markets.
The response of many central banks was to place large amounts of monetary reserves and capital funds into the
private-sector financial system to prevent the collapse of major financial institutions. In the United States, the
Federal Reserve also provided guarantees against losses to encourage mergers of banks and brokerage firms. All
of this may form the foundation of an even greater wave of excess liquidity and create the potential for a new
round of securitization, which may create a future financial bubble.
William Ganley
 
See also:  Balance of Payments;  Savings and Investment. 
Further Reading
Clarke, Peter. The Keynesian Revolution in the Making: 1924–1936. Oxford: Clarendon, 1988. 
Colander, David C., and Harry Landreth. History of Economic Thought.  4th ed. Boston: Houghton Mifflin, 2002. 
Garretson, Roger.  “The Austrian Theory of the Business Cycle.” In Business Cycles and Depressions: An Encyclopedia, ed.
David Glasner. New York: Garland, 1997. 
Glasner, David, ed. Business Cycles and Depressions: An Encyclopedia. New York: Garland, 1997. 
Hayek, Friedrich A. Prices and Production.  2nd ed. New York: Augustus M. Kelley, 1935. 
Hunt, E.K. History of Economic Thought.  2nd ed. Armonk, NY: M.E. Sharpe, 2002. 
Kates, Steven. Say’s Law and the Keynesian Revolution. Cheltenham, UK: Edward Elgar, 2009. 
Keynes, John M. The General Theory of Employment, Interest, and Money. London: Macmillan, 1936. 
Malthus, Thomas R. Principles of Political Economy. Edited by John Pullen. Cambridge, UK and New York: Cambridge
University Press, 1989. 

Minsky, Hyman P. Can “It” Happen Again? Armonk, NY: M.E. Sharpe, 1982. 
Minsky, Hyman P. John Maynard Keynes. New York: Columbia University Press, 1975. 
Ricardo, David. Principles of Political Economy and Taxation.  1821. Cambridge, UK: Cambridge University Press, 1973. 
Overvaluation
 
Overvaluation occurs when the price of an asset does not reflect the asset’s intrinsic or fundamental value, or
when the selling price exceeds its “buy” value. The intrinsic or fundamental value of an asset would reflect all
information, which is complete and understood by all, including factors that have a direct effect on the expected
value of the income streams of the assets. Any asset subject to a financial valuation (such as a stock, bond, or
currency) can be subject to overvaluation—that is, it is trading at a price higher than its intrinsic value. An asset
can be overvalued for many reasons, including overconfidence of superior potential returns, market scarcity,
emotional attachment, and market hype. Overvaluation can also pressure companies to falsify or overstate current
earnings, and can lead to an economic meltdown if multiple assets are greatly overvalued and then, when doubts
about the overvaluation spread through society, are corrected all at once.
A full understanding of overvaluation must begin with the deeper concept of valuation. Valuation is the process of
evaluating the market value of an asset, usually obtained by assessing the current value as well as potential
returns through different financial models such as discounted cash flows. Market valuation may also include
intangibles. For example, when an inventor acquires a patent for a new device it will increase its value compared
to a similar product without patent protection; the patent confers upon the invention an added value: market
monopoly for a certain number of years. Also, two individuals making an evaluation can assign different values to
the same item because of different perceptions—correct or not—of the item’s utility. Market consensus is reached
when multiple individuals reach comparable valuations.
Hence, an asset will have two values: a buy value (the value agents in the market are willing to pay to acquire it)
and a sell value (the value for which the agents are ready to sell the asset). When the sell value is superior to the
buy value, one can infer that the good is overvalued. By the same reasoning, when a sell value is inferior to the
buy value, an asset is said to be undervalued.
To better understand this concept, it is useful to examine a basic financial asset such as a share of company
stock. Assume that an individual decides to purchase shares at a price of $10. Using a valuation model, the
individual estimates that the stock is worth $12 (based on expected market growth and dividends, for example),
meaning it is either undervalued by the market or overvalued by the buyer. A year later, the individual tries to sell
the stock for $14—making this the sell value—but the best buy value he can find is $13. Hence, one can say
either that he is overvaluing the stock or the market is undervaluing it.
A good can be overvalued for many different reasons. This can occur when a person overestimates the potential
returns of an asset; for example, he could overvalue the potential dividends that will be paid. It can also occur
when market scarcity is overestimated (the market believes the item is more rare than it actually is), or when
there is an emotional bond to the particular asset. For example, entrepreneurs will often overvalue their start-up
companies because of their own emotional involvement.
Another frequent source of overvaluation is market hype. In this case, the overestimation of potential returns is
shared by multiple individuals simultaneously, leading to a rapid inflation of the asset price. Market hype can be

generated by rumor, conjuncture, or even deceit. Such overvaluations last a relatively short period of time and are
subject to drastic market corrections.
If many financial assets are continually bought and sold at an overvalued price, the result can be an economic
bubble. This happens when an asset is continually traded at an upward value but the basics of its valuation
(present and future earnings) do not change. At some point, the buy and sell values are too extended and no one
is willing to purchase the asset at its sell price. This can lead to a rapid readjustment of the asset’s value, as
buyers bring the value back to a more rational level.
In addition, the constant overvaluation of company stock can exert undue internal pressure to meet the sell price,
sometimes with disastrous results. In some cases, overvaluation by the market has led companies to misstate
financial earnings and resort to accounting fraud. Recent decades have witnessed a number of cases in which a
major corporation padded or even faked earnings in order to match market expectations. Consequences for a
national economy can be equally drastic. If too many goods are overvalued, the competitiveness of the local
economy may be compromised. General consumption might then decline, and economic adjustments slow. Debt
burdens could also increase if the company uses debt to buy back its own stock to keep the stock trading at the
overvalued price.
A market correction occurs when multiple parties conclude that one or more assets are overvalued and that the
prices of the goods should be reduced to a more rational level. This is not a formal process (nobody declares a
market correction), but rather an informal group consensus in which the price of an asset is quickly adjusted. In a
conventional open market, where only a few goods are overvalued, the market is able to correct itself adequately
and at regular intervals. Although the impact can be catastrophic for a few companies, the overall effects can be
limited.
Where many goods are overvalued simultaneously, however, the danger to the overall economy is much greater.
In that situation, there is a greater risk of economic crash should multiple investors try to leave the market at the
same time. This can be especially dangerous when mass psychology prompts a wave of irrational selling, even
dragging down assets that are not overvalued. For example, if the market decides that many companies in the
real-estate sector are overvalued, there may be a mass movement to sell the stock of these companies, even if
some companies are fundamentally sounds and fairly valued. This has occurred time and again in economic
downturns, such as the subprime mortgage crisis of 2008–2009.
Jean-Francois Denault
 
See also:  Asset-Price Bubble. 
Further Reading
Damodaran, Aswath. The Dark Side of Valuation: Valuing Young, Distressed and Complex Businesses.  2nd ed. Upper
Saddle River, NJ: FT Press, 2010. 
Marciukaityte, Dalie, and Raj Varma.  “Consequences of Overvalued Equity: Evidence from Earnings Manipulation.” Journal
of Corporate Finance 14:4 (September 2008): 418–430. 
Pacific Rim

 
Pacific Rim is a term widely used to describe countries that border the Pacific Ocean. The term Pacific Rim was
used during the late 1980s by journalists in the United States to symbolize informally the common political and
economic interests of the countries in this region. Such countries include Brunei, China, Hong Kong, the Republic
of Korea (South Korea), Malaysia, the Philippines, Singapore, Taiwan (Chinese Taipei), and Thailand. Russia is
geographically a Pacific Rim country, but its traditional political and economic links have been with Europe. Other
countries that border the Pacific are Australia, Indonesia, Japan, New Zealand, Papua New Guinea, Canada, the
United States, and some countries in Central and South America. The description of the Pacific Rim region and
countries has been expanding geographically over time. Today, Pacific Rim means all the countries of Northeast
Asia, Southeast Asia, Oceania, North America, and Latin America that are geographically connected to the Pacific
Ocean. The Pacific Rim thus comprises a large number of countries with great linguistic, religious, historical,
cultural, economic, political, and other differences. It comprises advanced industrialized countries, newly
industrialized countries, and developing countries with diversified opportunities for trade, investment, and
movement of people.
 Economic Growth Experience
The “East Asian Miracle” has contributed substantially to the economic boom of the Pacific Rim. In Asia, while
about half the economies had high growth rates in the 1950s, 1960s, and 1970s, there were some “miracle”
economies that experienced not only high growth but also consistent growth for three continuous decades, from
the 1960s through the 1980s. These are the four newly industrialized economies (NIEs) —Hong Kong, South
Korea, Singapore, and Taiwan, also known as the “Asian Tigers.” Taiwan, in particular, had an overall (four-
decade) average annual growth rate of 5.8 percent. Studies have shown that in the second half of the twentieth
century there is no growth rate comparable to that of the four NIEs anywhere else in the world. Factors
contributing to such high growth include high rates of investment in physical and human capital, rapid growth of
agricultural productivity, export orientation, a decline in the fertility rate, and the role of government in promoting
the development of specific industries and efficient economic management. The economies of Indonesia, Malaysia,
and Thailand have also achieved reasonably high growth rates. China has witnessed phenomenal success as a
Pacific Rim region economy, experiencing sustained average growth of over 9.5 percent since 1981. Another
economic power in the Pacific Rim region, Japan, is one of the top three wealthiest nations in the world and is a
leading global exporter and importer. Since 1982, transpacific trade has exceeded transatlantic trade.
 Economic Organizations
The Pacific Rim region contains several regional, subregional, transpacific, and international organizations that
play critical roles in addressing the economic, political, geopolitical, security, and strategic interests of the
countries in this region, namely: ASEAN (Association of South East Asian Nations), APEC (Asia-Pacific Economic
Cooperation), East Asia Community (EAC), East Asia Summit (EAS), and the Asia-Europe Meeting (ASEM).
APEC and ASEAN are the two prominent arrangements for economic cooperation in the region.
ASEAN was established in 1967 in Bangkok by five countries (now known as the original members)—Indonesia,
Malaysia, Philippines, Singapore, and Thailand. Brunei Darussalam joined in 1984, Vietnam in 1995, the Lao
People’s Democratic Republic (Laos) and Myanmar in 1997, and Cambodia in 1999. In 2007, the ASEAN region
had a population of 560 million, a combined gross domestic product (GDP) of almost US$1.1 trillion, and a total
trade of about US$1.4 trillion. Export-oriented industrialization is the key driver of economic growth of ASEAN. The
organization has expanded to ASEAN+3, which includes the ten member states of ASEAN and the three East
Asian nations of Japan, China, and South Korea. ASEAN+3, with a total population of about 2 billion, has a
combined GDP of $9.09 trillion and foreign reserves of $3.6 trillion. The ASEAN+3 region not only represents one-
third of the world’s population and 16 percent of the world’s GDP, but also holds more than half of the world’s
financial reserves. Trade and investment relations between ASEAN countries and Japan, China, and South Korea
have facilitated economic growth in the region.

 APEC
Asia-Pacific Economic Cooperation (APEC) is a forum of twenty-one Pacific Rim countries described as “member
economies.” Unlike ASEAN, APEC stands as an example of an “open regionalism” forum. Generally, regional or
subregional trade agreements are made based on geographic proximity, such as the European Union, NAFTA,
and ASEAN. In that sense, APEC is a different kind of economic-cooperation arrangement. The process for the
formation of APEC began in 1989 with the involvement of twelve member economies (the other members joined
later), namely the six ASEAN countries (Brunei Darussalam, Indonesia, Malaysia, the Philippines, Singapore, and
Thailand), South Korea, and the five Pacific OECD (Organisation for Economic Co-operation and Development)
countries (Australia, Canada, Japan, New Zealand, and the United States). At a 1994 meeting held in Bogor,
Indonesia, APEC put forth the Bogor Goals of free and open trade and investment in the Asia Pacific. APEC’s 21
member economies, with a population of more than 2.7 billion (approximately 40 percent of the world’s
population), represent about 55 percent of world GDP and 49 percent of world trade. The term APEC is now
sometimes used synonymously with the Pacific Rim, and the organization is sometimes referred to as the Pacific
Rim group.
 Pacific Rim in the 1990s and Early Twenty-First Century
The 1990s saw economic boom and bust cycles in the Pacific Rim countries. Unprecedented growth in several
countries of the region was followed by the Japanese recession and the 1997–1998 Asian financial crisis. Some of
the countries affected by the Asian financial crisis, such as Indonesia, Malaysia, the Philippines, the Republic of
Korea, and Thailand, started reviving soon after. Between 1999 and 2006, average per capita income in these
countries grew by more than 8 percent. Japan, in spite of her sluggish economy, continues to rank as the world’s
second-largest economy. The economically dynamic China and rising India are the largest contributors to global
growth since 2000. China was responsible for 32 percent of world GDP growth in 2001–2004 and has seen a
nearly sevenfold increase in its trade; it is now the world’s third-largest trading economy. All economies in East
and North-East Asia, except China, saw a marginal slowdown in 2007, and China’s relatively better performance
is due to the generation of domestic consumer demand. Hong Kong, China, Taiwan, and South Korea saw higher
levels of consumption and investment during 2007. Slowing demand in the United States and the European Union,
major destinations for exports, is eventually reducing export revenues in export-oriented economies across the
region. It is predicted that overall economic growth in the Pacific Rim will slow down in line with the global
economic slump, and that the leading economies of India and China will continue to remain relatively strong as a
result of growing domestic demand and their demographic advantage. The recovery of the Asia Pacific region from
the global economic crisis of 2008–2009 has started. However, the region faces serious problems of
unemployment and government interventions, as well as a need for structural policies to stimulate sustainable
economic growth.
M.V. Lakshmi
 
See also:  Asian Financial Crisis (1997);  Australia;  Canada;  Central America;  Chile;  China; 
Colombia;  Indonesia;  Korea, South;  Latin America;  Mexico;  New Zealand;  Russia and the
Soviet Union;  Southeast Asia;  United States. 
Further Reading
Collinwood, Dean W. Japan and the Pacific Rim. New York: McGraw-Hill Higher Education, 2007. 
Houseman, Gerald L. America and the Pacific Rim: Coming to Terms with New Realities. Lanham, MD: Rowman &
Littlefield, 1995. 
McDougall, Derek. Asia Pacific in World Politics. New Delhi: Viva, 2008. 
Miller, Sally M., A.J.H. Latham, and Dennis O. Flynn, eds. Studies in the Economic History of the Pacific Rim.  Routledge

Studies in the Growth Economies of Asia. London and New York: Routledge, 1998. 
Rao, Bhanoji, East Asian Economies: The Miracle, a Crisis and the Future. Singapore: McGraw-Hill, 2001. 
Ravenhill, John, ed. APEC and the Construction of Pacific Rim Regionalism. Cambridge, UK: Cambridge University
Press, 2002. 
 
Panic of 1901
 
The panic of 1901 was a severe stock market crash. It was felt most strongly on the New York Stock Exchange
(NYSE). This was one of the first significant crashes on the NYSE and came at the end of a five-year period of
rapid expansion and increased interest in stock trading. During this period, the volume of activity had increased to
600 percent of what it had been in 1896. (That increase in trading required the exchange to relocate to its current
main building at 18 Broad Street, though the trading floor is at 11 Wall Street.)
 Background
The 1901 crisis was precipitated by a fight among bankers and tycoons over control of the Northern Pacific
Railroad. The Northern Pacific (NP) operated throughout northwest North America, particularly from Minnesota to
Washington, with tracks covering Manitoba and British Columbia in Canada. The railroad was chartered in 1864 to
connect the Great Lakes with the Puget Sound in Washington State, thus granting access to Pacific shipping to
the northern United States. It had largely been financed by Jay Cooke, an Ohio lawyer who was best known for
helping to finance the Union’s Civil War efforts. NP continued to expand throughout the late nineteenth century,
avoiding most of the serious financial hardships that befell many of the country’s other railroads— that is, until the
Panic of 1893. This was yet another recession caused by a burst railroad “bubble”; in this case, the crisis brought
about the NP’s bankruptcy. From the start, multiple parties fought for control of the company, and three individual
courts asserted their own claims to jurisdiction over the bankruptcy proceedings.
 Enter J.P. Morgan
Financier J.P. Morgan was given the task by the bankruptcy court of straightening out the NP. An adept banker,
Morgan became increasingly effective at merging and consolidating firms and business interests. He was one of
the first to recognize that railroads worked more effectively and were better investments when they were
consolidated into a larger financial unit, as economies of scale kicked in and competition was removed in various
markets. Morgan acquired and reorganized a number of regional rail lines and organized industry-wide
conferences to discuss the ramifications of legislation like the 1887 Interstate Commerce Act. He also arranged in
1892 the merger that created General Electric, and in 1901 consolidated various steel businesses with the
Carnegie Steel Company to form the United States Steel Corporation, thus enlarging even on the work of his
fellow nineteenth-century tycoons.

His practice of taking over distressed assets, reorganizing them, and making them profitable was widely known as
“Morganization,” even though the work was usually done by men he appointed to the task. With this track record
under his belt, the legendary financier was now called upon to Morganize the NP, and he began to buy up stock.
In a 1901 cartoon titled “Wall Street Bubble—Always the Same,” financier J.P Morgan is caricatured as a Wall
Street bull blowing bubbles of inflated stock—in this case, of Northern Pacific Railroad—for eager investors. The
result was a broad market collapse. (The Granger Collection, New York)
 Morgan, Hill, and NP
The reason for those stock purchases was that Morgan was responding to Edward Henry Harriman, the head of
the Union Pacific Railroad, who wanted to expand his line. Harriman, the NP, and the Great Northern railroad run
by James Hill, had all simultaneously sought access to Chicago, a major industrial city that had the potential to
serve as a hub for all of them, as well as a funnel of railroad traffic, and to which none of them yet had a route.
When the price of another railroad that did offer easy access to Chicago—the Chicago, Burlington, and Quincy
railway—was set at $200 a share, Hill and the NP swept in and bought it up, sharing the route between them.
Harriman, left completely out of the cold in this deal, responded by attempting to buy a controlling interest in the
NP, which in 1901 was still trading for a relatively low price because of its years in receivership. Once he
controlled the NP, Harriman intended to use that power as leverage for gaining Chicago access for his railroad—
the Union Pacific—at favorable terms.
 The Crisis
Harriman’s rapid stock purchases of NP, starting on May 3, 1901, caused James Hill to do the same, urged on by
Morgan, along with other Morgan-owned companies, all in order to keep the stock out of Harriman’s hands. As a
result of all this buying, the price of the stock inflated rapidly, trading for as much as $1,000 a share. But this price
had little to do with the worth of the company and far more to do with the extent to which Harriman wanted
Chicago for the Union Pacific and how far Hill and Morgan would go to stop him. Moreover, these mostly private
issues had little to do with the concerns of the everyday investors who owned NP shares or the speculators who
took advantage of the sudden upswing. So great was the rise that when the crash came, as a correction to
inflated stock prices that had spread from NP to other stocks as investors panicked and moved their money out of
all kinds of corporate securities, it came hard: railroad stocks, as well as mining stocks and U.S. Steel, crashed
severely.

Unable to gain control of NP, and thus of Chicago, Harriman organized a National Securities Company (NSC) as
a trust with which to buy up railroad stocks and managed to win Hill’s loyalty away from Morgan. However, the
NSC did not last long before it was shut down by government antitrust regulators. In the meantime, investors
returned to the market, lured by stock bargains, and the exchange rebounded, fueled by strong growth in the real
economy.
Bill Kte’pi
 
See also:  Panic of 1907;  Panics and Runs, Bank. 
Further Reading
Bruner, Robert F., and Sean D. Carr. The Panic of 1907: Lessons Learned from the Market’s Perfect Storm. New
York: John Wiley and Sons, 2009. 
Linsley, Judith Walker, Ellen Walker Rienstra, and Jo Ann Stiles. Giant Under the Hill: A History of the Spindletop Oil
Discovery at Beaumont, Texas. Denton: Texas State Historical Association, 2008. 
Morris, Charles R. The Tycoons: How Andrew Carnegie, John D. Rockefeller, Jay Gould, and J.P. Morgan Invented the
American Supereconomy. New York: Holt, 2006. 
Strouse, Jean. Morgan: American Financier. New York: Harper, 2000. 
Wolff, David. Industrializing the Rockies. Boulder: University Press of Colorado, 2003. 
 
Panic of 1907
 
A major financial crisis that precipitated a dramatic drop in U.S. stock prices and set off a mild recession, the
Panic of 1907 was triggered by a failed effort of speculators to corner part of the market in copper, which, in turn,
set off a liquidity crisis at key New York banks. The panic is best remembered for being the last time a U.S.
financial crisis was largely remedied by the efforts of private bankers, and was led by financier John Pierpont
Morgan. The panic renewed calls among economists and policy makers for the United States to follow major
European countries by setting up a central bank that could both prevent and respond to crises in the financial
markets by regulating bank credit and providing liquidity.

Commenting on the schemes and manipulation that lay behind the Panic of 1907, this contemporary cartoon
shows “common honesty” erupting from a volcano and people fleeing with “stocks,” “secret rate schedules,”
“rebates,” and “frenzied accounts.” (Library of Congress)
As in the case of most financial crises, the Panic of 1907 resulted from a number of contributing factors beyond
the immediate one. By the fall of 1907, when the cornering effort failed, the securities markets and the financial
sector had already taken a number of blows. Among these were the great San Francisco earthquake of 1906,
which led to market instability as capital flowed out of New York to the West Coast, and the passage of the
Hepburn Act that same year. The Hepburn Act gave the Interstate Commerce Commission increased powers,
including the authority to set maximum railroad rates, which sent railroad stock prices dramatically downward.
Finally, and perhaps most importantly, the powerful Bank of England raised its interest rates at the end of 1906,
drying up capital in the U.S. markets as British and other investors sought to take advantage of higher returns in
London. All of these factors contributed to the volatility of U.S. financial markets and a tightening of credit in 1907.
That same year, Otto Heinze, brother of United Copper Company founder Augustus Heinze, came up with an
elaborate scheme to gain control of the company and make a fortune in the process. Otto believed that a major
portion of the company’s publicly traded shares had been bought “short,” a financial practice whereby the
purchaser borrows money to buy shares in hopes that the share price will then go down. The borrower can then
sell the shares, repurchase them at a lower price, pay back the loan, and pocket the difference. Otto planned to
aggressively purchase shares so as to drive up the price. This would force the short buyers to dump their shares
in order to pay back their loans, at a much higher price. Otto would then sell his shares and reap a large profit.
Otto began his buying scheme in October, but he miscalculated on two fronts. He had underestimated the amount
of money he would need to effect the corner, and he overestimated the number of shares that had been bought
“short.” In other words, there were plenty of shares available for short sellers to buy in order to cover their
position. Otto was successful at first, driving up the price of United Copper stock from under $40 to over $60 a
share. Within days, however, short sellers dumped their holdings in the company, which caused a drop in the
share price to under $10. Otto and his brokerage firm, Gross and Kleeberg, were unable to meet their obligations
and were forced to declare bankruptcy. A number of banks and financial trusts with major holdings in United
Copper stock suddenly saw their assets shrink, made worse as panicky depositors withdrew their money. More
importantly, other banks and trusts became fearful, refusing to lend money to the troubled institutions. The credit
markets soon began to freeze up, triggering a recession that gripped the country for the rest of 1907 and much of
1908. The national unemployment rate rose from 3 percent to 8 percent, with industrial production falling by more
than 10 percent.

Even before the recession kicked in, however, the financial panic was being addressed by some of the richest and
most influential financiers on Wall Street. Their actions were coordinated by the most powerful banker of the age,
J.P. Morgan. Meeting in the library of his mansion on Manhattan’s Madison Avenue, the bankers came up with a
scheme to shore up the assets of key banks with tens of millions of dollars while at the same time liquidating
those of the most troubled institutions in order to pay depositors. Despite these actions, many New York banks
refused to make the short-term loans to each other that would lubricate the credit markets. The financial markets
responded with panic selling of securities, as stock prices plunged. This time Morgan assembled an even larger
group of bankers and got them to provide more than $20 million in short-term loans. Still, the markets continued
downward until the New York Clearing House, a consortium of city banks, came up with $100 million to shore up
the credit markets. Meanwhile, Morgan and other bankers persuaded the city’s major newspaper editors and
clergymen to urge calm.
In the end, the panic was quelled. While the value of shares traded in the various New York markets fell by 50
percent, they recovered quickly. Beyond the few institutions immediately affected by the collapse of United Copper
stock, there was no great wave of bankruptcies in the financial industry. The recession triggered by the panic
proved short-lived, representing a slight dip in an ongoing upward trend in economic growth and securities
valuation that lasted from the late 1890s through the far worse recession of 1921–1922.
And while Morgan was praised around the world for his deft handling of the crisis, many on Wall Street and
beyond became more convinced than ever that the United States needed a central bank to regulate credit and
lending and to provide liquidity during periods when private money dried up. The country had no such institution
since President Andrew Jackson vetoed the rechartering of the second Bank of the United States in the 1830s.
Six years after the Panic of 1907 came passage of the Federal Reserve Act and the founding of the Federal
Reserve Bank.
James Ciment
 
See also:  Panic of 1901;  Panics and Runs, Bank. 
Further Reading
Chernow, Ron. The Death of the Banker: The Decline and Fall of the Great Financial Dynasties and the Triumph of the
Small Investor. New York: Vintage, 1997. 
Strouse, Jean. Morgan: American Financier. New York: Random House, 1999. 
Panics and Runs, Bank
 
Bank runs are episodes in which depositors at a given financial institution lose confidence in that institution’s
ability to meet its obligations, specifically, its ability to produce the funds it holds on behalf of depositors. Many
such runs occurring simultaneously produce a bank panic. A common occurrence in the United States through the
early twentieth century, bank runs and panics have largely been eliminated since the creation in 1933 of the
Federal Deposit Insurance Corporation (FDIC), a federal agency that insures deposits up to a specified amount of
money.

An eerie echo of old-style bank runs occurred as a result of the subprime mortgage crisis of the late 2000s. In
June 2008, panicked clients withdrew more than $1.5 billion in deposits from the California-based bank IndyMac,
a financial institution highly exposed to subprime mortgages either by originating them or investing in subprime
mortgage-backed securities. But while scenes of depositors lining up outside the doors of IndyMac branches
evoked images from the early 1930s, the FDIC quickly moved in to secure deposits, making sure insured
depositors did not lose money and quieting the alarm.
 “Fractional Reserve Banking”
Virtually all banks maintain only a small amount of the assets entrusted to them by depositors in their vaults, a
system known as “fractional reserve” banking. This is done because much of the profit that banks generate comes
out of the “spread,” the difference between the lower interest they pay depositors and the higher interest they
collect on loans to businesses and individuals, or the return they earn on the securities they purchase with those
depositor assets.
Beyond keeping only a small amount of depositors’ funds on hand, fractional reserve banking presents another
risk, known as “asset-liability mismatch.” That is, a bank’s liabilities (the funds it technically owes depositors) are
usually more short-term in nature. Even if depositors do not regularly close their accounts, they draw upon them
frequently, taking some or most of their money out for any number of immediate needs on an ongoing basis. A
bank’s assets (loans and securities), however, are less liquid. A mortgage or a business loan, for example, is
usually paid back in installments over a long period of time while the securities a bank invests in often have fixed
maturity dates, such as government or corporate bonds. In short, a bank cannot readily meet sudden liabilities by
liquidating long-term assets.
While seemingly a risky practice, “fractional reserve” banking is, in fact, the way in which banks always operate,
allowing them to make loans. Usually, banks are able to function smoothly because of the “law of large numbers.”
That is, holding the funds of thousands or even millions of depositors assures a great degree of safety for a bank
since it is highly unlikely that, barring some emergency, all or even a large number of depositors would withdraw
their funds at once. And even should such an unlikely event occur, banks have several backstops to protect them:
they can borrow funds on a short-term basis from other banks through interbank lending, or, if they are members
of the Federal Reserve System (the Fed)—which all nationally chartered and most larger state-chartered banks
are—they can secure funds directly from the Federal Reserve Bank, the so-called “lender of last resort.” And, as
noted earlier, modern depositors enjoy a final protection, that of the FDIC, though FDIC intervention to protect
deposits usually entails placing the troubled institution into receivership, which can often lead to the sale of its
assets and the formal dissolution of the bank.
 Panics Through the Great Depression
A run may occur on an individual bank because of something specific to that institution, or a run can occur against
many banks at once. When the latter occurs, it is known as a “bank panic,” or simply “panic.” Panics were
common in nineteenth-century U.S. economic history, especially with the liberalization of state bank chartering
laws before the Civil War. During that period, banks not only lent out money against deposits, but also issued their
own currency in the form of bank notes. Lightly regulated, if at all, such banks often held little specie, or coinage
with an intrinsic value in the precious metal they contain, to back up the bank notes they issued. Indeed, the
assets of many banks, particularly in rural areas, were in the land deeds they held, a very illiquid asset indeed
and one that fluctuated wildly in value.
The notes banks issued against depositor assets varied in value as well, depending on the institution’s reputation
and location. Usually, bank notes depreciated in value the farther they circulated from the issuing bank, since the
reputation of that bank and the ability to submit the banks for specie became more attenuated with distance.
Unscrupulous individuals would often establish banks in very remote locations, making it almost impossible for
holders of notes to redeem them. Indeed, so far into the wilderness were they located that people called them

“wild cat” banks because they existed where wild cats roamed.
But even more reputable banks could be victims of bank runs, especially at times when real-estate bubbles burst
and the land deeds held by the bank became worth much less. Panicky depositors or holders of bank notes would
then descend on banks to withdraw their money or turn their notes into specie, creating a panic that brought the
nation’s financial system to its knees and triggered a general downturn in the economy. Such was the case in
1819, 1837 and 1857. By the late nineteenth and early twentieth centuries, bank panics—such as those of 1873,
1893, and 1907—were more likely to be triggered by a collapse in corporate securities valuations—often those of
railroads—an asset held by many banks at the time. In most of these cases, the panic began with a run on a
major bank in New York or a regional financial center, which usually produced a general lack of confidence in the
solvency of other banks. This pattern illustrates that both bank runs and bank panics tend to feed on themselves.
As more depositors withdraw their funds, the banks become less solvent, triggering fears in other depositors who
then demand their money.
To prevent such panics, in 1913, Congress established the Fed, which not only served as a lender of last resort
to member banks but set rules for the amount of assets a member bank had to keep on hand to pay depositors.
Still, this protection did not prevent the worst bank panic in American history, that of the early 1930s, from
occurring. As the economy reeled from the Great Depression, bank deposits dried up as people withdrew their
money to live on, to put into safer assets such as gold, or to stash away under the proverbial mattress. At the
same time, bank assets declined, either because individuals and businesses could no longer pay back their loans
or because the securities a bank held lost value. Many larger banks, for example, had invested in corporate
securities, whose values declined precipitously as a result of the Great Wall Street Crash of 1929.
By late 1932, some 9,000 banks had failed. As newly elected president Franklin Roosevelt waited to take office—
which, in those days, occurred in March—panic spread through the system, as depositors made runs on banks
across the country. The nation’s financial system was collapsing. As one of his first acts in office, Roosevelt
declared a “bank holiday,” ordering the closure of the nation’s banks for several days. While they were shut down,
Roosevelt signed the Emergency Banking Relief Bill, which allowed the Treasury Department to reopen banks that
were solvent and reorganize and manage those that were not. This move reassured depositors and ended the
panic. Later that year, Congress passed the Glass-Steagall Act, legislation that enabled the FDIC and prevented
commercial banks—that is, the ordinary banks that held deposits and issued loans—from engaging in such
investment bank activities as the underwriting of corporate securities and other high-risk investments.
The FDIC and Glass-Steagall effectively stabilized the U.S. banking system, kept bank runs to a minimum, and
ended traditional bank panics. Over the years, the amounts the FDIC insures have risen—from $2,500 to the
current, albeit temporary, limit of $250,000 per depositor—to keep pace with inflation and overall economic
growth. At the same time, however, Glass-Steagall’s firewall between commercial and investment banking was
torn down by the Gramm-Leach-Bliley Act of 1999, a reform that many economists say contributed to the financial
crisis of 2008–2009.
 Financial Crisis of the Late 2000s
Indeed, the late 2000s crisis has tested the safeguards established under Roosevelt’s New Deal legislation of the
1930s. As the housing boom took off in the early 2000s, many banks began to drop their lending standards,
allowing less qualified applicants—or subprime borrowers—to take out a mortgage. Rapidly rising home prices
reassured financial institutions that even should a borrower go into default, the bank would recoup its losses and
then some by selling off the repossessed home at a price well above the outstanding debt on the mortgage. In
addition, the securitization of mortgages appeared to spread the risk of default to many investors so that the bank
originating the mortgage was no longer 100 percent liable for the default.
But as the housing market collapsed and prices declined, a number of banks found themselves holding very large
quantities of nonperforming subprime loans. By June 2008, rumors began to spread that IndyMac Bank, a
California-based institution that had aggressively marketed subprime mortgages, might become insolvent. As

noted above, despite assurances from the FDIC, depositors lined up and demanded the right to remove their
money. While the images evoked the bank runs of the early 1930s, in fact, the system worked; the FDIC placed
the bank into receivership and ensured that depositors did not lose a dime of their money and did not face any
significant delays in gaining access to it. Three months later came the collapse of Washington State–based
Washington Mutual (WaMu), another victim of the subprime mortgage collapse. This time, however, the FDIC
moved more quickly and a bank run was avoided, with the FDIC putting the bank into receivership and then
orchestrating the purchase of its assets by JPMorgan Chase.
While such federal-government action served to foreshorten and avoid bank runs during the financial crisis of the
late 2000s, another problem looms, say some economists—that of banks whose assets are worth less than zero
but which remain propped up by the promise of support from the Fed. Such banks are popularly known as
“zombie banks”—the financial equivalent of the walking dead. Zombie banks are often the victim of “silent runs” in
which potential depositors avoid putting their money into the bank rather than existing depositors pulling theirs out.
To assure potential depositors, the zombie bank might offer higher interest rates, putting the squeeze on healthy
banks. This allows zombie banks to grow but not enough to return themselves to financial health. And because of
their straitened circumstances, they do not lend as much money, thereby curtailing investment and economic
activity. Moreover, their very existence undermines confidence in the banking system as a whole, since depositors
and other banks do not know which institutions are likely to become insolvent, thereby curtailing deposits and
interbank lending. Many economists cite the existence of such banks as a major factor behind the prolonged
slump in the Japanese economy following the real-estate collapse there in the early 1990s, and they worry that a
similar scenario might play out in the United States and other economies hard hit by the real-estate collapse of
the late 2000s.
James Ciment
 
See also:  Federal Deposit Insurance Corporation;  Great Depression (1929-1933);  IndyMac
Bancorp;  Panic of 1901;  Panic of 1907;  Savings and Loan Crises (1980s-1990s). 
Further Reading
Cecchetti, Stephen G. Money, Banking, and Financial Institutions. Boston: McGraw-Hill/Irwin, 2006. 
Chorafas, Dimitris N. Financial Boom and Gloom: The Credit and Banking Crisis of 2007–2009 and Beyond. New
York: Palgrave Macmillan, 2009. 
McGrane, Reginald. The Panic of 1837: Some Financial Problems of the Jacksonian Era. New York: Russell &
Russell, 1965. 
Wicker, Elmus. Banking Panics of the Gilded Age. New York: Cambridge University Press, 2000. 
Wicker, Elmus. The Banking Panics of the Great Depression.  2nd ed. New York: Cambridge University Press, 2000. 
 
Paulson, Henry (1946–)

 
Henry Paulson, an investment analyst who occupied senior management positions at the investment firm Goldman
Sachs for many years, was appointed U.S. secretary of the treasury in 2006 by President George W. Bush. The
appointment was the culmination of Paulson’s many years in public service. In his new capacity, Paulson worked
with Chairman of the Federal Reserve Ben Bernanke and Director of the Federal Reserve of New York Timothy
Geithner (who would later succeed Paulson as Treasury secretary) on managing the early stages of the global
financial crisis of 2008.
U.S. Treasury Secretary Henry Paulson, a former chairman and CEO of Goldman Sachs, played a key role in the
government’s response to the financial meltdown of 2008. He was the chief architect and promoter of the $700
billion TARP bailout. (Bloomberg/Getty Images)
Henry Merritt Paulson, Jr., was born March 28, 1946, in Palm Beach, Florida. He grew up in Barrington Hills,
Illinois (outside Chicago), as the son of a wholesale jeweler. He graduated from Dartmouth College in 1968, where
he majored in English and was a member of Phi Beta Kappa. While at Dartmouth, Paulson played varsity football
as an offensive lineman and was nicknamed “Hank the Hammer” for his determination and aggressive play; he
earned All Ivy and All East honors, as well as an honorable mention for All American. He continued his education
at the Harvard Business School, earning an MBA in 1970.
Moving to Washington, D.C., Paulson began his career as a staff assistant to the assistant secretary of defense at
the Pentagon from 1970 to 1972. He then moved on to the position of staff assistant to President Richard Nixon,

working under adviser John Ehrlichman from 1972 to 1973 as a member of the White House Domestic Council.
Paulson joined Goldman Sachs in 1974, rising rapidly through the ranks to become a partner in 1982. From 1983
to 1988, he headed Goldman’s Investment Banking Services for the Midwest Region, and in 1988 became the
managing partner in the Chicago office. Paulson’s meteoric rise in Goldman Sachs continued in the 1990s, as he
was appointed co-head of the Investment Banking Division (1990), president and chief operating officer (1994),
and co-senior partner (1998). With the firm’s initial public offering (IPO) in 1999, he was designated chairman and
chief executive officer of the Goldman Sachs Group, Incorporated.
Paulson had been dubbed “Mr. Risk,” in part because he was one of the first on Wall Street to recognize the profit
potential for investment banks that take leveraged positions with their own capital in addition to acting as
intermediaries. Under his leadership, Goldman Sachs’s strategy focused on identifying profitable risks, determining
how to control and monitor them, and avoiding catastrophic missteps in investing. During his thirty-two-year career
at the firm, Paulson accumulated an equity stake worth an estimated $700 million.
After an intense recruitment effort by the White House, Paulson left Goldman Sachs in early July 2006 to become
President Bush’s third Treasury secretary (after John Snow and Paul O’Neill). He had been reluctant to accept the
nomination but agreed after receiving assurances that he would be an active participant in economic-policy
formulation. Paulson was described by some as the ideal person to deal with the U.S. economic and fiscal
situation because of his understanding of debt and risk taking. As the U.S. economy had come to resemble a
giant venture-capital fund drawing money from global capital markets and investing it in high-risk, high-return
projects, the importance of understanding and managing risk, global imbalances, and capital flows had become
central to virtually every economic issue faced by the government.
As Treasury secretary, Paulson came face-to-face with the financial crisis that began to emerge in the early
months of 2008. Working closely with Bernanke and Geithner, he helped arrange emergency funding and other
measures to save such former financial giants as Bear Stearns, AIG, Merrill Lynch, and Citigroup. The
controversial decision to let Lehman Brothers fall into bankruptcy set off a wave of fear and uncertainty within the
U.S. and international financial markets that, many believe, catalyzed the global economic meltdown. Paulson was
also the architect and chief manager of the Troubled Assets Relief Program (TARP), a Bush administration plan
that provided some $700 billion in federal funds to ailing financial institutions.
In December 2008, reflecting on the lessons of the financial crisis and government intervention in the markets,
Paulson declared, “We need to get to a place in this country where no institution is too big or too interconnected
to fail... because regulation alone is never going to solve the problem. There’s no regulator that’s going to be so
good they’re going to be able to deal with or ferret out all of the problems. So it takes a balance between the right
regulatory system and market discipline.... Only if we have the freedom to fail, is there really going to be the
freedom to succeed.”
Following the election of Barack Obama in November 2008, Paulson worked with the incoming administration’s
economic team, including Geithner, Obama’s nominee to head the Treasury Department, to ease the transition at
a time of great economic peril. After leaving office in January 2009, Paulson accepted a teaching and research
position at the Johns Hopkins School of Advanced International Studies in Washington. An avid environmentalist,
he also remained active in various conservation causes.
Frank L. Winfrey
 
See also:  Stimulus Package, U.S. (2008);  Treasury, Department of the;  Troubled Asset Relief
Program (2008-). 
Further Reading
Altman, Alex.  “Henry M. Paulson, Jr.” Time, September 18, 2008. 

Bhushan, Amarendra.  “Henry M. Paulson, Jr.” CEOWORLD Magazine, September 25, 2008. 
“Mr. Risk Goes To Washington.” BusinessWeek, June 12, 2006. 
Paulson, Henry. On the Brink: Inside the Race to Stop the Collapse of the Global Financial System. New York: Business
Plus, 2009. 
Penn Central
 
The bankruptcy of the Penn Central railroad in 1970 was the largest bankruptcy in U.S. history to date. The
corporation’s failure was symptomatic of the decline of railroads after World War II, and illustrated how some
institutions had overextended themselves. The Penn Central crisis also highlighted the continuing disagreement in
the United States over whether poorly run corporations should be bailed out by the federal government, a
discussion that continues to the present.
Since its beginning in the nineteenth century, the American railroad industry has been plagued by overcapacity
and mismanagement. Between 1876 and 1970, railroad bankruptcies and receiverships totaled more than 1,100.
During the Great Depression, nearly one-third of the country’s railroads sought bankruptcy protection from
creditors. In 1933, Congress added Section 77 to the federal Bankruptcy Act, to be applied specifically to railroads.
Under this section, railroads were allowed to declare bankruptcy and then continue operations while being
protected from creditors. The section allowed rail traffic to continue, which was important to the economic life of
the United States, while the railroad was reorganized to pay off creditors.
Railroads joined most other industries in a financial resurgence during the 1940s. War production and the massive
movement of people across the country provided additional revenues. With the end of the war, however, railroads
began a slow decline, particularly in terms of passenger travel and freight being hauled over short distances. The
main competitors of the railroads were the automobile and the truck. Both provided greater flexibility than the train.
The development of interstate and multi-lane highway systems increased the advantages of motor vehicles over
trains. In 1956, the Federal-Aid Highway Act provided additional federal monies for constructing additional
highways.
Railroads that served the northeastern United States faced particular difficulties by the end of the 1950s. This
densely populated, but relatively small, region turned away from the railroads to a greater extent than other areas.
On November 1, 1957, two of the largest railroads in the region, and of the United States as a whole, announced
that they were studying a merger. Pennsylvania Railroad and New York Central both provided a combination of
freight and passenger service, and both suffered from declining revenues. Merger talks dragged on for years,
hampered by the suicide of New York Central’s chief executive officer. The Interstate Commerce Commission
approved the merger on April 27, 1966, but court hearings took another two years. On February 1, 1968, the two
railroads formally merged to become the Pennsylvania New York Central Transportation Company, better known
as the Penn Central.
The goal of the merger was to create a railroad more economically sound than its two predecessors. The planners
hoped that they could consolidate services and equipment, but their efforts were unsuccessful. Railroads were
highly regulated at the time by the Interstate Commerce Commission (ICC). The ICC prevented Penn Central from
ceasing operations on lines that were not economically viable, in order to prevent a loss of service to those who
lived along the lines. Rates charged by the railroads were also regulated by the ICC, so Penn Central could not
raise revenue that way. Efforts to save money by consolidating operations were unsuccessful because the

computer systems used by the railroads were not compatible. Operational savings were also hampered by
different signal systems and other equipment. Even labor costs could not be cut. Contracts with strong railroad
unions prevented Penn Central from eliminating unneeded positions or firing surplus personnel.
Realizing the railroad industry was in trouble, Penn Central’s managers tried to diversify the corporation’s
interests. Large investments were made in industries as different as pipeline networks, real estate, and land
development. These ventures, however, failed to bring in much additional income. They also drained much of the
railroad’s cash. Efforts to raise additional money to pay interest on short-term debts were unsuccessful, thanks to
a tight credit market in the late 1960s.
By June 1970, Penn Central was unable to pay its debts. Although the corporation had assets of nearly $7 billion,
most were pledged as security for $2.6 billion in loans. On June 21, Penn Central filed for bankruptcy under
Section 77. The announcement sent shock waves through the American financial system. An effort by the Nixon
administration to underwrite $200 million in loans came under fire from Democratic congressmen. The bailout was
viewed by most Americans as a favor for the administration’s friends in big business. Penn Central continued to
operate, although debts continued to accumulate. The corporation’s leadership was criticized for poor business
decisions and was fired.
Legislation followed to prop up the railroad industry. On May 5, 1971, Amtrak was created to operate rail
passenger service throughout the country under the federal government’s control. Because passenger service was
the least profitable railroad operation, most railroads were willing to turn operations over to Amtrak. Other railroads
faced bankruptcy, leading Congress to nationalize Penn Central on April 1, 1976. Five smaller railroads were
consolidated with Penn Central, resulting in the Consolidated Rail Corporation, better known as Conrail.
Penn Central’s collapse, followed by that of other railroads, caused Congress to deregulate the railroads in 1980.
Cost-cutting steps like closing down some unprofitable rail lines followed. Surviving railroads were able to
concentrate on the most profitable freight lines, and the industry became more prosperous in the 1990s.
Tim J. Watts
 
See also:  Debt. 
Further Reading
Daughen, Joseph R., and Peter Binzen. The Wreck of the Penn Central. Boston: Little, Brown, 1971. 
Salsbury, Stephen. No Way to Run a Railroad: The Untold Story of the Penn Central Crisis. New York: McGraw-Hill, 1982. 
Sobel, Robert. The Fallen Colossus. New York: Weybright and Talley, 1977. 
 
Philippines

 
Located off the coast of Southeast Asia, the Philippines consists of an archipelago of more than 7,000 small and
large islands between the Pacific Ocean and the South China Sea. While its more than 92 million people make it
the twelfth most populous country in the world, it ranks only forty-seventh in gross domestic product (GDP),
classifying it among middle-income nations.
Settled for tens of thousands of years, the islands were occupied by the Spanish in the sixteenth century and
remained a colony of Madrid until 1898, when they were seized by the United States in the Spanish American
War. Following occupation by the Japanese in World War II, the Philippines gained independence in 1946. Its
post-independence history was marked by political turmoil and dictatorship until the 1980s, when true democracy
was established.
A largely poor and agricultural country for most of its history, the Philippines emerged from American colonialism
as the second wealthiest nation in Asia, after Japan, but corruption and mismanagement reduced its fortunes
considerably until a rapid industrialization period began in the 1990s. Despite the vagaries of modern global
economics, including the Asian financial crisis of 1997–1998 and the global downturn of 2008–2009, the
Philippines has shown reasonably strong growth, even if it lags behind other East and Southeast Asian
economies.
In metropolitan Manila, the Philippines, high-rise construction was suspended and shantytowns spread during the
Asian financial crisis of 1997. Unequal distribution of wealth remains a chronic problem as the nation struggles for
economic stability. (Romeo Gacad/AFP/Getty Images)

 Economic History Before Marcos
The first humans are believed to have arrived in the archipelago around 40,000 BCE, though the dominant Malay
people did not begin settling there until about 6,000 BCE. By the first millennium CE, the islands were integrated
into a trade network that linked them with Southeast Asia and China. Because the country consists of thousands of
islands, it was never unified until the arrival of the Spanish, though various states emerged around Manila Bay as
early as the tenth century.
In 1521, Portuguese-born explorer Ferdinand Magellan, a naturalized Spaniard, arrived in the islands on his
historic voyage around the world. Magellan was killed in a battle there with indigenous peoples while attempting to
claim the islands for Spain. Over the course of the sixteenth century, Spain completed its conquest of the islands,
which were governed from Mexico, and converted most of the people to Catholicism. Thus, the Philippines
became the only majority Christian country in Asia, which it remains today.
While far from other colonial possessions, the Philippines became an important link in Spain’s global trade
network in the sixteenth and seventeenth centuries. Manila emerged as the major exchange port where Chinese
goods such as silks and artisan products were traded for American silver. With the decline of the Spanish Empire
in the late seventeenth and early eighteenth centuries, the Philippines languished economically until reforms in the
mid-1700s opened it up to world trade. This created a wealthy elite of mixed Spanish and Filipino heritage in the
nineteenth century who made their fortunes in the commercial production of tropical agricultural products. This
landed aristocracy would maintain a stranglehold over the Filipino economy through both the Spanish and U.S.
periods and into the post-independence era.
In 1898, the United States—a new global power—went to war with Spain and easily seized the Philippines from
the militarily weaker country. More difficult was the suppression of indigenous rebels under independence leader
Emilio Aguinaldo, who, angry that the United States would not grant them independence, fought the Americans in
a bitter insurgency that resulted in the deaths of hundreds of thousands of Filipinos between 1898 and 1901.
U.S. rule proved a mixed blessing for the Philippines. While the Americans created a modern educational system,
they also attempted to impose an alien culture. And while they helped build up the Filipino economy, it became
largely an appendage to the U.S. market, exporting tropical agricultural commodities in exchange for manufactured
goods. Still, under U.S. tutelage, the rudiments of a modern economy and industrial and transportation
infrastructures emerged. By independence in 1946—following a three-year occupation by Japanese forces in
World War II—the Philippines had the second-highest per capita GDP in all of East Asia, after Japan, though
there remained gross inequalities in wealth among classes and between urban and rural areas. The United States
had also attempted land reform but was largely unsuccessful in the effort.
A nominal democracy during the first quarter-century of independence, the Philippines was rocked by political
turmoil, including a long-running communist insurgency in the 1950s. The country also prospered economically
during the global boom of the 1950s and 1960s as a major exporter of agricultural products and as home to one of
the most advanced industrial infrastructures in Asia. With the rise to power of Ferdinand Marcos, who was first
elected president in 1965 and then seized dictatorial powers in the early 1970s, the economy languished, a victim
of misguided economic policies, corruption, and a crony capitalism in which family members and friends took over
most of the country’s industries and drove many of them into the ground, either through incompetence or plunder.
 Economy Under Marcos
Marcos, the president of the Philippines from 1966 to 1986, pushed through several initiatives that shaped the
Philippine economy into the 1980s, increasing agricultural productivity, building roads, developing alternative
sources of energy, and building hotels in the capital, Manila, to attract tourists. With the momentum of the
country’s ambitious undertakings, overseas lenders continued to grant loans to banks, businesses, and the
government.

In the latter part of the 1970s, economic indicators in the Philippines showed definite progress, as well as less
positive signals. The economy was expanding—the gross national product (GNP) grew by an average of 6 percent
throughout the 1970s—but the unemployment rate rose. Throughout the decade, the country imported far more
than it exported, leading to a rapidly expanding trade deficit. By the end of the decade, the foreign debt was
overwhelming economic activity. These negative indicators began to alert international investors that the country
was on the verge of severe economic problems.
In the early 1980s, several developments took place in the Philippines that would further erode the economy. Still
reeling from the oil shock of the 1970s and the global recession it unleashed, the country continued to rely heavily
on foreign borrowing and international trade. While the price of oil (and of oil imports) soared, the prices of the
Philippines’s major exports—sugar, lumber, copper concentrate, and coconut products—declined sharply. This
meant an accelerating trade deficit and reduced investments in the country by wary businesses and financial firms
in the West.
In January 1981, after being reelected to another six years as president through fraudulent balloting, Marcos
appeared to make new efforts to move the country toward economic stability. He ended martial law, which had
hampered domestic commerce and foreign investment in the country since 1972, and continued with his
development plans, including the construction of new plants for steel, phosphate, cement, diesel engines, and
petrochemicals. He also sought to reduce dependence on foreign oil by building nuclear and geothermal electric
power plants. But many of the projects proved to be overly ambitious, at once poorly planned and economically
unsupportable. The major source of funding was foreign borrowing, but misallocations, reckless spending, and the
unsound economic policies of the Marcos regime made foreign banks wary of lending to the Philippine
government. As foreign investments tightened up and interest rates skyrocketed, many of the projects were
abandoned, and the Philippines was mired in rising debt.
On top of the looming crisis, a major financial scandal erupted in 1981 that would further disrupt the plans for
progress. Textile magnate Dewey Dee fled the country after scamming several government and private banks,
leaving behind approximately $70 million in debt to various financial institutions. His disappearance provoked a
financial crisis that sent major investment houses and finance companies into turmoil.
Amid the mounting debts, fleeing investors, and declining popular support for the regime, political agitation
mounted. On August 21, 1983, Benigno Aquino, Marcos’s chief political rival, was assassinated upon returning
from political exile in the United States. The airport shooting triggered renewed political turbulence, which in turn
caused a crisis of confidence in the investment community. Capital fled the country at an alarming rate. By
October, the Central Bank of the Philippines was forced to notify creditors that it was unable to pay its debts. The
country was essentially bankrupt. President Marcos again turned to foreign lenders, but his dismal economic
performance in the past and the mounting political sentiment against him made overseas investors distrustful and
unwilling to lend.
In February 1986, after another fraud-riddled election, a popular uprising variously referred to as the People
Power Revolution and Yellow Revolution forced Marcos to flee the country and installed Corazon Aquino, widow of
Benigno, as president. After two decades of Marcos’s dictatorial rule, the Philippines was getting a fresh start
politically. The reforms that ensued brought an end to some of the worst corruption and cronyism of the era and
encouraged foreign investors, setting the stage for a period of renewed growth.
 Post-Marcos Economy
At first, foreign investors and lenders, including the United States and Japan, remained reluctant to put their
money into the Philippines. With the promising signs of reform, however, President Aquino was eventually able to
garner more than $1.2 billion in foreign loans and grants—over half of it coming from the United States. In moving
the economy forward, the Aquino administration focused on debt repayment as its top priority, economic growth
driven by exports, and less government regulation. As a result, the nation’s annual GDP rose from 4.3 percent in
1986 to 6.7 percent in 1998, with economic expansion continuing through most of the 1990s.

The 1997–1998 Asian financial crisis, which began in Thailand and spread throughout Southeast Asian and
Japan, had a major impact on the Philippines. Foreign investors, whose capital had been key to the country’s
export-led growth in the 1990s, began to pull out, which sent the value of the Philippine currency, the peso,
plummeting. After growing by more than 5 percent in 1997, the Filipino economy fell into negative growth the
following year, even as the government tried to bolster employment by raising tariffs on imported goods.
The late 1990s and early part of the next decade were a period of slow recovery, with the economy dragged
down by the aftermath of the crisis, a series of natural disasters, a major bank failure, and scandals that led to the
impeachment and ouster of President Joseph Estrada in 2001. By mid-decade, however, the Philippines was once
again enjoying robust growth, driven by strong prices for mineral exports and a burgeoning manufacturing sector,
supported by U.S., Japanese, and other firms building electronics and other assembly plants in the country to take
advantage of the Philippines’s relatively low labor costs. Between 2000 and 2007, per capita GDP, measured on a
purchasing-power parity basis, which allows for differences in the values of various currencies, rose from $3,600
to $5,000.
The 2008–2009 financial crisis did not affect the Filipino economy as badly as it did those of the United States
and much of the West. For one thing, the Philippines continued to lag behind many East Asian and Southeast
Asian countries as a destination for foreign capital. Moreover, much of the capital that had been invested in the
country went into building manufacturing and service infrastructure, not into securities or other financial instruments
that could be easily liquidated. In addition, the Asian financial crisis of the late 1990s had led the government to
impose higher capital requirements on banks, leaving them less leveraged and less vulnerable to the declines in
financial instruments that had caused so much damage elsewhere, such as mortgage-backed securities.
Still, the global recession that followed the financial crisis hit the Philippines on several fronts. First, the drop in
prices of raw materials hurt the nation’s mining sector, while the overall decline in global demand undermined
growth in manufactured exports. On the bright side, many multinational companies, eager to pare costs,
maintained their operations in the country because of its low labor costs, helping offset the drop in remittances
sent back by the millions of Filipino nationals working in economically ailing Persian Gulf and Western countries.
Thus, the country was expected to follow its East and Southeast Asian neighbors into a rapid recovery from what
turned out to be a relatively short-lived recession. Indeed, the country began to post impressive GDP growth rates
as the global economy slowly pulled out of recession, hitting 7.6 percent in 2010.
James Ciment and Berwyn Gonzalvo
 
See also:  Asian Financial Crisis (1997);  Indonesia;  Southeast Asia. 
Further Reading
“Avoiding Recession.” The Economist, August 28, 2009. 
Bresnan, John, ed. Crisis in the Philippines: The Marcos Era and Beyond. Princeton, NJ: Princeton University Press, 1986. 
Calit, Harry S., ed. The Philippines: Current Issues and Historical Background. New York: Nova, 2003. 
Karnow, Stanley. In Our Image: America’s Empire in the Philippines. New York: Random House, 1989. 
Nadeau, Kathleen. The History of the Philippines. Westport, CT: Greenwood, 2008. 
PNC Financial Services

 
PNC Financial Services is a Pittsburgh-based financial services corporation and regional banking franchise
operating in the mid-Atlantic and Midwest. It has become one of the largest banks in the United States, ranked
fifth by deposits and third by the number of off-site ATMs. The case of PNC offers an example of how a severe
economic crisis provides opportunities for fundamentally sound financial institutions.
PNC began as the Pittsburgh Trust and Savings Company in Pittsburgh, Pennsylvania, in 1852, and opened its
current corporate offices in 1858. Growing gradually but steadily over the course of the twentieth century, the
PTSC was renamed the Pittsburgh National Corporation in 1959. In 1982 it merged with the Provident National
Corporation—another descendant of a nineteenth-century Pennsylvania bank, this one in Philadelphia—and called
itself the PNC Financial Corporation. The new PNC quickly expanded its holdings through acquisitions of smaller
banks, until its coverage extended from New York City to Kentucky. By the start of 2008, acquisitions of the Riggs
National Corporation, Mercantile Bankshares, Yardville National Bancorp, and the Sterling Financial Corporation
had put it among the top ten American banks.
 Global Economic Crisis of 2008–2009
Like other reasonably healthy banks, PNC was in a position to benefit from the widespread bank failures of the
2007–2009 financial meltdown and subsequent crisis while at the same time using its financial health to help
stabilize the economy. In October 2008, it was chosen by federal regulators to acquire the failing National City
Bank, a Cleveland institution and one of the ten-largest banks in the country. Founded as the City Bank of
Cleveland in 1845, its history had largely paralleled that of PNC and other moderately successful nineteenth-
century banks. National City had gone on an acquisitive binge from 2004 to 2008, spending billions of dollars to
acquire the Provident Financial Group, Allegiant Bancorp, Fidelity Bankshares, Harbor Florida Bankshares, and
MAF Bancorp (the holding company of MidAmerica Bank).
The acquisitions proved too much to digest, and National City suffered as it expanded into the period when the
credit markets were about to freeze up and investments were beginning to fail. By the middle of 2008, federal
regulators put National City Bank on probation. The exact terms of the agreement reached with the Office of the
Comptroller of the Currency were confidential. National City did disclose to the public that the problems had
revolved around the bank’s overextension and its liberal risk-management practices—the two areas of concern for
so many of the banks that failed in the aftermath of the subprime mortgage crisis.
The PNC acquisition of National City was called a “takeunder” because the purchase price of $5.58 billion was
below National City’s nominal value. Moreover, taking over the bank allowed PNC to receive $7.7 billion of federal
money from the 2008 Emergency Economic Stabilization Act, which allowed PNC to make a significant profit. On
the other hand, the exact nature of that profit depended on the accuracy of value estimations of the bank’s
holdings—a less than sure thing in the fourth quarter of 2008, given how many banks had overestimated the
value of their investments and made other accounting errors.
Acquiring National City made PNC the fifth-largest bank by deposits, which it would remain even after selling 61
branches of the National City Bank as required by the Justice Department’s antitrust division. (The sales took
place in areas where the acquisitions gave PNC too much control over local banking, i.e., those places where
PNC was competing with National City).
Bill Kte'pi
 
See also:  Banks, Commercial;  Recession and Financial Crisis (2007-). 
Further Reading

Boselovic, Len.  “PNC Acquiring National City.” Pittsburgh Post-Gazette, October 24, 2008. 
O’Hara, Terence.  “Riggs, PNC Reach New Merger Agreement.” Washington Post, February 11, 2005. 
PNC Financial Services:  www.pnc.com
Political Theories and Models
 
The political theory of businesses cycles asserts that politicians cause upswings and downturns in the economy
for political reasons. According to this view, politicians and political trends can influence the business cycle under
specific conditions.
Ideally, politicians pursue economic policies that promote prosperity for all. Nor is it hard to imagine how such an
ideal situation might arise, at least hypothetically. If political candidates compete against each other openly and
honestly, if voters are fully informed about the policies and programs proposed by office seekers, and if elected
officials carry out the promises they made as candidates, the people are likely to get policies that best serve their
interests (or that they think will do so). Thus, under ideal political conditions, national leaders will do their best to
make the ups and downs of the business cycle less severe. In reality, however, there is great potential for—and a
long history of—abuse of government powers that cause large swings in economic performance. Elections are not
always open and candid, voters are often not well informed, and elected officials frequently alter their policies after
taking office. In the popular view, government leaders are notorious for misusing the public trust in this way, often
under pressure from special-interest groups.
 Elections and Economic Policy
One underlying cause of political business cycles is the timing of elections. Specifically, incumbent politicians may
be inclined to alter policy so as to exaggerate economic growth during election years, while accepting slower
growth rates, or even recessions, when not facing reelection. If this is indeed the case, political business cycle
theory underscores the destabilizing effects of governmental influence on the economy as a whole. Underlying the
possibility that the election cycle affects the business cycle are several facts of political and economic life. First,
voters strongly oppose higher taxes and inflation. Second, voters like increased government spending and more
jobs in their districts. Yet if elected officials try to keep taxes low and spending high, the government will face a
growing budget deficit, which can backfire as interest rates rise and choke off private spending. If the deficit is
accommodated by central bankers, then higher inflation will be the result.
Similarly, if politicians try to reduce inflation during an election year by raising interest rates, this can increase the
unemployment rate and cost them votes. Incumbent politicians may thus conclude that it is advantageous to run
larger deficits and accept higher inflation rates—policies that may trigger a boom—during election years,
particularly if a large part of the inflation (which lags output growth in the boom) does not emerge until after the
election. When the election is over, the officials can then implement policies to reduce deficits and inflation. This
means that politicians are more likely to cut spending and fight inflation in the early phases of their terms than in
the latter years. The result will tend to be economic slowdown in the period just after elections, followed by
economic acceleration as politicians rely on inflation and deficit spending to win votes in boom years.
The election cycle will affect the business cycle only to the extent that voters are shortsighted. An informed
electorate will not be swayed by an artificial election-year upswing if they know that it will be followed by a post-
election recession. Moreover, if voters remember that a politician fooled them with an artificial boom in the

previous election year, they will not fall prey to the same manipulation. According to several statistical studies,
however, economic conditions during an election year have a greater influence on voting outcomes than do
economic conditions for a presidential candidate’s full term in office.
 Historical Examples
History provides numerous examples of economic policy being used to political advantage. During the first term of
President Franklin D. Roosevelt (1933–1937), for example, political motivations were part and parcel of the New
Deal programs instituted to combat the Great Depression. Even though the South was the poorest region of the
United States, a disproportionate amount of New Deal spending went to Northeastern and Western states, where
incomes were 60 percent higher. Why did Roosevelt direct federal spending toward wealthier states rather than to
the poverty-stricken South? One explanation is that the Southern states could be relied on to vote Democratic no
matter their circumstances. Voters in the West and Northeast, meanwhile, were more open to voting Republican.
Because Roosevelt was a Democrat, it made political sense for his administration to spend more money in states
where they needed to win votes.
Federal spending by the Roosevelt administration totaled $4.5 billion per year in 1932 and again in 1933, jumping
to $6.5 billion in 1934 and the election year of 1935. In 1933, the U.S. gross national product (GNP) was $55.6
billion and the unemployment rate was 24.9 percent. In 1935, the GNP stood at $72.2 billion and the
unemployment rate at 20.1 percent. Roosevelt won reelection after a large increase in federal spending and a
modest improvement in economic conditions. Economic conditions deteriorated with the crash of 1937, but began
to improve during the election year of 1939. In short, the Roosevelt administration gives every appearance of
having structured and timed federal spending based on electoral considerations, which suggests that at least part
of the economic oscillation of the Great Depression was due to the election cycle.
During the postwar era, the election of 1972 presents an especially interesting case in point. In that year,
President Richard Nixon undertook a rather obvious effort to create a political business cycle, and it worked.
Blaming Republican election losses in 1954 and 1958, as well as his own defeat in 1960, on bad economic
conditions, Nixon pursued an anti-inflation policy in 1969, which led to a recession. As the 1972 election
approached, Nixon appointed Arthur Burns as Federal Reserve chairman. Burns injected an extra $51 billion into
the economy, and Nixon added an additional $63.6 billion through the Treasury Department. The result was an
election-year boom. The economy grew at an unsustainable rate of 7.3 percent during the twelve months leading
to the November 1972 balloting. At the same time, Nixon directed federal money to key voting groups in states he
needed to win, funding state aid projects, veterans’ benefits, and Social Security payments. Nixon’s landslide
victory—he won reelection with 60 percent of the vote—suggests that voters forgot about the high unemployment
during the early part of his first term and rewarded him for the election-year boom.
 Parties
The idea of a purely election-driven political business cycle suggests that the two major U.S. parties follow the
same policies and deliver the same results. While election-driven business cycles find considerable support in the
historical record, party affiliation is another political factor in the upswings and downturns of the economy. It has
been suggested, for example, that Republican politicians are relatively more concerned with preventing inflation,
and that Democratic politicians are generally more concerned with preventing unemployment. That being the case,
the ascendancy of one party or another may contribute to a change in the business cycle. If Democrats are in
power, they will tend to allow a higher inflation rate while they try to minimize unemployment; consumers thus
grow accustomed to higher inflation. Conversely, in a Republican administration, economic policy may shift toward
fighting inflation, and unemployment will therefore rise, at least temporarily; in this case, the public comes to
expect slower price increases.
The result is a tendency toward inflationary booms during Democratic administrations and recessions during the
early phases of Republican administrations. In the early years of a Democratic administration, after people have

come to expect low inflation (as a carryover from GOP policy), inflation may cause the economy to grow faster
and achieve lower unemployment, at least temporarily. Once these temporary gains are realized, however, there
will be little the Democratic administration can do to realize further reductions in unemployment. People will get
used to higher inflation, and the economy will return to “normal,” at least until an anti-inflation Republican
administration is elected.
While the proposition that political parties implement different policies for different purposes carries theoretical
plausibility and appears to be supported by statistical evidence, a closer look at history weakens the case for
partisan political business cycles. In support of the theory, economic statistics for the 1980s do indicate that
Republican president Ronald Reagan was more concerned with inflation than with unemployment. Moreover,
inflation rates did fall during the first term of the Reagan administration, and unemployment rates were high in
1981 and 1982. But the anti-inflation policies of the early Reagan years were implemented by Federal Reserve
chairman Paul Volcker, who was appointed by Reagan’s Democrat predecessor, Jimmy Carter, in August 1979.
Volcker had been preceded as Federal Reserve chairman by G. William Miller, appointed by Carter early in the
previous year. Miller, out of character for a Democratic administration, was far more concerned with inflation than
with unemployment. By 1979, as rising prices became a serious problem—both economically and politically—
Carter replaced Miller with the anti-inflation Volcker. Thus, by contrast with Nixon, who would fight inflation first
and switch to fighting unemployment as he approached reelection—which he won in a landslide—Carter took on
unemployment first and then fought inflation during the election year—and he lost. What these examples suggest,
therefore, is not that Democrats and Republicans pursue different policies that determine the economic cycle, but
rather that Nixon knew how to use the political business cycle to his advantage and Carter did not.
 Interest Groups
Still another explanation of political business cycles focuses on special interests. As a general principle, business
owners can earn more profits by keeping wages low, provided that workers maintain high productivity for less pay.
Because workers will find it easier to demand higher wages when there is full employment, businesses may find
advantage in economic instability. An occasional crisis might make workers so fearful of unemployment that they
will work hard even at relatively low wages. The special-interest-group version of political business cycle theory is
hard to substantiate. While individual interest groups certainly do have influence in politics, it is not clear that all
businesses can work together as a group against labor. On the business side, special-interest groups split up by
industry and cannot work together easily. Moreover, the labor movement continues to have a major influence on
politics even as union membership has declined over previous decades. The idea that business interests can
consistently beat out labor interests on the unemployment issue is far from proven, and the reverse could easily
be true. Labor unions might well be more influential when it comes to the business-cycle issue than are
businesses. Overall, the case for the interest-group theory of political business cycles is weaker than for the
election-cycle theory.
 Stability and Instability
Finally, the political stability or instability of a country may be a vital factor in the business cycle and the
expansion or contract of the economy. While the United States has been blessed by one of the most stable
political systems in world history—with the major exception of the Civil War, political conflict in America has
always been resolved through the electoral system—most other countries around the world have not been so
lucky. Economists generally assert that stability is good for the economy for a number of reasons. First, it provides
the kind of predictable political environment that the private sector needs to make investment decisions. Political
stability also tends to assure a relatively stable monetary policy, maintaining inflation at a level that encourages a
healthy level of consumption as well as a useful degree of savings. Finally, political stability reassures foreign
interests that a particular country is a good place to invest.
Some economists argue, however, that the relationship between political stability and economic prosperity is a
more complicated matter, and that, in some cases, instability can be more beneficial to the economy, at least in

the long-term. While politically destabilizing events such as revolution and war often interrupt economic activities in
the short run, they can also set the stage for more rapid growth in the medium-and long-term. This is especially
the case when revolutionary forces are set in motion by an existing government’s inability to deal with economic
issues. If a revolution overthrows such a system and installs one that is more capable of managing the economy,
the event can be seen as a boon to the economy.
The major example of political instability in U.S. history—the Civil War—offers a useful example. The waning
years of the antebellum era saw a political stalemate as the slaveholding states maintained a veto in the Senate
that blocked the interests of free states, despite the fact that the latter represented a large majority of the
population and economic output of the country. While this veto power was most notable in preventing antislavery
legislation, it also had an impact on other economic policy issues. For a variety of reasons, Southerners opposed
the tariffs that Northern industrial interests viewed as essential, they blocked homesteading legislation, and they
insisted that a transcontinental railroad run through their region of the country. Southern secession freed up the
legislative logjam, allowing a Northern-dominated Congress to pass an agenda that, say economic historians,
helped pave the way for the expansion of the industrial revolution in the latter half of the nineteenth century and
the emergence of the United States as the world’s great industrial economy.
D.W. MacKenzie, Bing Ran, and James Ciment
 
See also:  Fiscal Policy;  Monetary Policy;  Public Works Policy;  Tax Policy. 
Further Reading
Ake, Claude.  “A Definition of Political Stability.” Comparative Politics 7:2 (January 1975): 271–283. 
Alesina, Alberto, and Jeffrey Sachs.  “Political Parties and the Business Cycle in the United States, 1948–1984.” Journal of
Money, Credit and Banking 20:1 (February 1988): 63–82. 
Beck, Nathaniel.  “Elections and the Fed: Is There a Political Monetary Cycle?” American Journal of Political Science 31:1
(1987): 194–216. 
Golden, David G., and James M. Poterba.  “The Price of Popularity: The Political Business Cycle Reexamined.” American
Journal of Political Science 24:4 (1980): 131–143. 
MacRae, Duncan.  “A Political Model of the Business Cycle.” Journal of Political Economy 85:2 (April 1977): 239–263. 
Mevorach, Baruch.  “The Political Monetary Business Cycle: Political Reality and Economic Theory.” Political Behavior 11:2
(1989): 175–188. 
Nordhaus, William D.  “The Political Business Cycle.” Review of Economic Studies 42:2 (1975): 169–190. 
Olson, Mancur Lloyd, Jr. The Rise and Decline of Nations: Economic Growth, Stagflation, and Social Rigidities. New
Haven, CT: Yale University Press, 1982. 
Richards, Daniel J.  “Unanticipated Money and the Political Business Cycle.” Journal of Money, Credit and Banking 18:4
(1986): 447–457. 
 

Ponzi Scheme (1919–1920)
 
A Ponzi scheme is a financial scam in which existing investors are paid inordinately high returns, not from any
increase in the value of their investment but out of the funds provided by new investors. Named for Charles Ponzi,
an Italian American immigrant who conceived and ran one of the most notorious of such schemes in the early part
of the twentieth century, Ponzi schemes are fraudulent, illegal, and fated to collapse once the pool of new
investors dries up, usually resulting in widespread losses. Ponzi schemes have occurred periodically in U.S.
history, with the biggest to date being that run by New York financier Bernard Madoff from the early 1990s through
the mid-2000s, which bilked investors of tens of billions of dollars.
An impoverished immigrant from Sicily, Ponzi came to the United States in 1903 at the age of nineteen. After
working at various jobs in the United States and Canada—and running afoul of U.S. law for illegally smuggling
immigrants into the country—Ponzi in 1919 discovered a loophole in the international postal system that seemed
to guarantee substantial and assured profits for an investor. International postal-reply coupons (IPRCs) were a
form of prepaid postage in which a sender could pay for a recipient’s reply. IPRCs were popular in the Italian-
American community because they allowed relatively well-off immigrants to cover the cost of postage for poorer
relatives and friends back home.
Italian American immigrant Charles Ponzi bilked investors of some $5 million in a scam involving international
postal reply coupons in 1919–1920. Such illegal pyramid schemes—which took Ponzi’s name—reached a new

level with the Bernard Madoff case decades later. (The Granger Collection, New York)
The purchase price of an IPRC was the cost of a letter’s postage in the sender’s country. This created a
discrepancy in value if similar postage in the recipient’s country was worth more or less than that in the sender’s
country. For example, if sending a letter cost 5 cents in the sender’s country and 10 cents in the recipient’s
country, the IRPC was undervalued. The coupon could then be exchanged for stamps in the recipient’s country,
which in turn could be redeemed for cash, yielding a 5 cent, or 50 percent, profit. This was exactly the situation
between Italy and the United States in 1919. With Italy’s currency undergoing rapid devaluation against the U.S.
dollar after World War I, IPRCs could be bought relatively cheaply in Italy and redeemed for stamps in the United
States; these could then be sold for a hefty profit.
Based in Boston, Ponzi used his own and borrowed funds to make a test run. The results were less than
encouraging. Delays in the system and the large administrative expenses of buying and redeeming large numbers
of IPRCs ate up all of the profits and more. Yet Ponzi was not deterred. A talented salesman, he convinced
people—despite what he had learned—that the scheme was foolproof, promising investors major returns in ninety
days. So many flocked to his scheme that he was able to pay the original investors the same spectacular returns
in a mere forty-five days.
As the news spread, more and more people brought their money to Ponzi—many of them previous investors who
plowed their profits back into the enterprise. This not only allowed him to continue paying high returns, but to hire
a large staff to run the business and to live the life of a millionaire—at least for a time, in 1919 and early 1920.
Another term for a Ponzi scheme is a “pyramid scheme.” As the term suggests, a pyramid scheme requires an
exponentially larger base of new investors to pay off the growing number of earlier investors at the top. Sooner or
later, the pool will run out—sooner if the scheme is exposed, which is exactly what happened when the Boston
Post ran an exposé on July 26, 1920.
Thousands in Boston showed up at Ponzi’s offices demanding that he return their money. With investments still
pouring in from his operations in other cities, he was able to make good on their demands, reaffirming his
reputation and keeping the scheme alive. But time was running out. On August 10, auditors where he banked
announced that Ponzi did not have sufficient funds to redeem all of his obligations. This time the flood of investors
demanding money back overwhelmed his resources. Three days later, Ponzi was arrested by federal authorities
and charged with mail fraud. Tried and found guilty that November, he was sentenced to five years in federal
prison. It took almost a decade to untangle the finances of Ponzi’s scheme, with later investors recovering just 30
cents on the dollar. After serving several years in prison, Ponzi was eventually deported, dying in poverty in Brazil
in 1949.
Charles Ponzi did not invent the scheme that was named for him, nor did such scams cease occurring after he
had been exposed. In the decades since, law-enforcement authorities have exposed dozens of major Ponzi
schemes in the United States. The most spectacular was the one run through Bernard L. Madoff Investment
Securities, a New York City financial services firm run by a prominent Wall Street trader and philanthropist.
Facilitating over-the-counter, or nonexchange stock and bond trades directly between investors, Madoff and his
company for years gave investors consistently higher returns than other firms. By late 2008, however, with the
stock market in rapid decline and more and more investors wanting to withdraw their money, Madoff had trouble
meeting his obligations and keeping the scheme alive. With the firm already under federal investigation, Madoff
admitted publicly that his operation was a Ponzi scheme with liabilities of tens of billions of dollars. He was
arrested in December 2008, pled guilty to all charges in March 2009, and was sentenced to 150 years in prison in
June 2009.
As in Ponzi’s case, Madoff’s scheme required an operator capable of winning people’s confidence and a pool of
investors gullible enough to overlook the oldest adage of the marketplace: if a deal is too good to be true, it
probably is. The differences between Ponzi and Madoff were in the sophistication of the latter’s operation and the

scale and duration of the fraud. Whereas Ponzi’s roughly year-long scheme bilked investors of about $5 million,
Madoff’s scheme—which investigators say operated from the 1960s through the 2000s—cost them an estimated
$65 billion, making it the largest financial fraud in history.
James Ciment
 
See also:  Madoff, Bernard. 
Further Reading
Dunn, Donald. Ponzi: The Incredible True Story of the King of Financial Cons. New York: Broadway, 2004. 
Zuckoff, Mitchell. Ponzi’s Scheme: The True Story of a Financial Legend. New York: Random House, 2005. 
Portugal
 
A small nation situated on the Iberian Peninsula in southwestern Europe, Portugal, with a current population of
about 10.6 million, traditionally had one of the poorer and least developed economies in Western Europe.
Modernization in the post–World War II era, however, has brought it closer to European living standards and per
capital gross domestic product (GDP).
One of the first nation states to emerge in Western Europe in the Middle Ages, Portugal became a pioneer in
overseas exploration, conquest, and trade in the fifteenth and sixteenth centuries as mariners brought back spices
and other exotic goods from Asia while establishing colonies there, as well as in Africa and South America. But
the country was soon outpaced by larger and more economically advanced competitors and went into economic
decline from the seventeenth century through the early twentieth century.
As late as 1950, half of Portugal’s active population was still employed in agriculture and the GDP per capita was
similar to those of Greece, Bahrain, Colombia, and Namibia. Yet, as the decade of the 1950s unfolded, a strong
industrialization push—buoyed by technological transfers, by new projects melding private and public capital, and
by a controlled but progressive openness of the economy—sent the country toward the front ranks of growing
European economies, with 5.8 percent of annual growth in the GDP per capita for the period of 1950–1973.
Following a pattern later trod by successful NICs (newly industrialized countries) in the developing world, Portugal
added exports of light industrial goods to its traditional agricultural exports, which benefited from the comparative
advantage of cheap labor in the final value added. But just as plans for the development of heavy industry were
under way, the country was hit by a double shock: the international energy and economic crisis that spread from
oil upheaval in the Middle East and the military coup of April 25, 1974, known as the “Carnation Revolution,”
which ended forty-one years of authoritarianism and personal dictatorship without bloodshed.
The 1974 revolution unleashed a revolutionary process marked by new democratic freedoms, as well as street
confrontations between left and right. Among the most important changes brought on by the change in regimes
was the decision to dismantle Portugal’s overseas empire and end costly anti–national liberation wars in Angola,
Mozambique, and Guinea-Bissau. Decolonization saved the state a lot of resources but it also led to a wave of
returning Portuguese nationals from the overseas colonies. Around the same time, Portuguese emigration to other
European nations began to wane.

The revolution also wrought significant changes in economic policy, including a nationalization program that
brought the largest enterprises in banking, manufacturing, transportation, and services under government control.
This led to what economists call a dual economy—large, capital-intense sectors became public monopolies and
oligopolies, while small and medium-sized enterprises, as well as foreign-owned companies, remained in private
hands. At the same time, agrarian reform led to the redistribution of the large landed estates of the northern part
of the country but left the small land holdings in the center and south in the hands of existing owners.
This transformation led to much economic disruption. The large industrial enterprises underwent a swift
depreciation in capitalization and posted large losses. Price distortions, decreased competitiveness, and the
crowding out of private investment slowed economic growth. Moreover, Portugal’s troubled macroeconomic
situation, with public debt and inflation rising to crippling levels, combined with a general slowdown in the world
economy in the late 1970s and early 1980s, led to two International Monetary Fund interventions in 1978–1979.
Structural reforms eased the macroeconomic problems while a policy shift helped spur economic growth. Internal
consumption rose, and despite the austerity measures, a broader social welfare network was established.
By the mid-1980s, however, things were beginning to change. A strategic consensus of left and right led to
Portugal’s accession to the European Union (EU) in 1986. To harmonize its economic policies with those of the
EU, the government eased restrictions on the movement of capital in and out of the country and, under the
Center-Right government of President Aníbal Cavaco Silva, initiated a privatization agenda in 1998, all part of a
general trend toward increasing economic competitiveness.
The liberalization cycle, along with increased flows of foreign capital (balanced between direct foreign investment
and structural fund transfers from the EU), the largest privatization effort in the EU per capita, closer economic
ties with neighboring Spain, a strong revival in the all-important tourist sector, and an increase in construction, led
to dramatic economic growth. By the early 1990s, Portugal had pulled its GDP per capita up to about 75 percent
of the average for the EU.
At this point, many economists had come to believe that Portugal’s economic development was self-sustaining.
But in the late 1990s and early 2000s, new problems arose. Existing structural problems, weak productivity, a
poorly educated workforce, and new imbalances in public finances combined with the competitive pressures of
globalization led to a decade of anemic growth, political stalemate, and an inability to tackle structural problems.
This economic weakness left Portugal unprepared for the crisis that swept world financial markets in 2008. By
early 2010, public sector debt had risen so much—to more than 9 percent of GDP—that investors were beginning
to question the country’s very solvency. The Portuguese crisis—along with those in Greece and Spain—sent
global financial markets into panic and prompted talk of the EU guaranteeing the country’s debt.
By March 2011, all of the major credit rating agencies had downgraded Portugal’s bond ratings, making it more
expensive for the government to borrow money and forcing it to impose even tougher austerity measures. That
month, the government of José Sócrates collapsed, and the new government asked its European Union partners
for a bailout in April. The following month, the EU and the International Monetary Fund (IMF) agreed to a $116
bailout package. While the bailout and the new center-right government’s willingness to impose necessary
cutbacks in government spending stabilized the situation, many experts feared that the more dire debt crisis in
Greece would cause investors to panic and flee other troubled Eurozone economies, including Portugal, creating
the need for an even bigger bailout.
Nuno Luis Madureira and James Ciment
 
See also:  Greece;  Ireland;  Spain. 
Further Reading
Confraria, João.  “Portugal: Industrialization and Backwardness.” In European Industrial Policy, ed. James Foreman-Peck

and Giovanni Frederico, 268–294. Oxford: Oxford University Press, 1999. 
Corkill, David. The Portuguese Economy Since 1974. Edinburgh, UK: Edinburgh University Press, 1993. 
“Europe Watches as Portugal’s Economy Struggles.” New York Times, February 9, 2010. 
Syrett, Stephen. Contemporary Portugal: Dimensions of Economic and Political Change. Aldershot, UK: Ashgate, 2002. 
Poseidon Bubble (1969–1970)
 
The Poseidon bubble—a brief speculative episode involving Australian mining stocks in late 1969 and early 1970
—illustrates the severe economic fluctuations in the form of bubbles and busts that can occur in the absence of
regulatory controls over a country’s natural resources.
The Poseidon bubble was one of the biggest booms in the history of the Australian stock market, if only for a brief
time. Fueled by rampant speculation, shares in Poseidon NL (No Liability) rose from $0.80 in September 1969 to
$12.30 the following month, rocketing to $280 in February 1970 before finally crashing. Millionaire investors were
created overnight, only to be returned to the rank and file just as abruptly.
Poseidon NL was a mining company that discovered a potential new source of nickel in September 1969. The
price of nickel had been rising steadily because of the manufacturing demands of the Vietnam War and because
work stoppages at Inco, a major Canadian supplier of the metal, had sharply reduced the available supply.
Poseidon’s discovery of a major mining site in western Australia resulted in the first month’s leap in share price.
Mining in general, an inherently risky business, had been especially healthy and profitable during Australia’s “long
boom” since the end of World War II. Across the country, growth was strong, and unemployment and inflation
were low. The mining sector expanded rapidly, as venture capital was increasingly available to fund explorations
of the vast continent and its many natural resources.
From the 1950s to the time of Poseidon’s nickel find, fortunes had already been made from a succession of
explorations—the Mount Tom Price iron mine, the Bass Strait oil fields, the Weipa bauxite mine, and the Mary
Kathleen uranium mine. From 1958 to 1968, the ASX All Mining Index, tracking the performance of mining stocks
on the Australian Exchange, experienced an average growth of 25 percent per year. The price of nickel was
spiraling almost out of control at the time of the Poseidon find, nearly tripling from January 1968 to October 1969.
The company had been doing poorly in recent years, accounting for its low share price at the time of the
discovery. The mere announcement on September 29, 1969, that nickel had been found, with no indication of how
big the find was, was enough to more than double the price of the stock in one day. The big jump came after
October 1, 1969, when Poseidon’s directors announced that the find was a major one.
Poseidon’s previous struggles may have contributed to the speculative bubble that followed. A minor company
suddenly faced with a windfall after years of fruitless exploration made for a good story—and good stories capture
the public’s imagination and excite speculation. The ASX All Mining Index leaped 44 percent from October to
December on the strength of Poseidon’s performance and the general excitement over Australian mining; other
companies took out exploration leases near Poseidon’s. But hard information about the quantity and quality of the
nickel find was difficult to come by. Prices rose so quickly that Poseidon’s more detailed drilling report on
November 19, 1969, did not affect already inflated share prices. There was still no reasonable estimate of how
much or how good the ore was, other than it constituted “a major find.”
The Poseidon bubble, like many others, took on a life of its own. The rising cost of nickel propelled prices in the

early weeks, which only began declining back to more reasonable levels after the November peak. Poseidon
broke $100 per share, then $200. Even more remarkable was the rise in share prices for mining companies that
had not made any new finds. As long as they were in the nickel business and had a lease to drill near Windarra,
the location of Poseidon’s find, investors were interested. The value of the Poseidon find itself was still uncertain,
but speculators assumed the entire area was rich in nickel, with plenty for everyone. New companies found
buyers for their stock just by declaring an intent to drill near Windarra, even before any prospecting had been
done. Shares of the Tasminex Mining Company rose from $2.80 to $16.80 in two trading days, based on rumors
that it had found a nickel site. The stock price rose as high as $96 after the publication of an interview with the
company chairman about Tasminex’s high hopes. Tasminex never did find nickel, and the company chairman sold
his shares before the price fell.
Nevertheless, brokerage houses continued to recommend Poseidon stock, with a London-based broker suggesting
in early 1970 that as much as $382 per share would be a reasonable price for a stock that had sold for $0.80
barely three months earlier. Poseidon share prices peaked in February 1970, after which both it and the ASX All
Mining Index began to fall rapidly. Just as the skyrocketing prices had been only tenuously connected to any real
cause, there is no clear indication of what caused the fall. When some investors started selling, others followed.
Given the number and magnitude of assumptions inherent in investors’ behavior during the bubble, it was
inevitable that confidence would eventually fall. Continuing to buy Poseidon and other Australian mining stocks
required that an investor believe the price of nickel would remain high (despite two temporary events— that had
inflated the price—the Vietnam War and the strike in Canada), as well as that the Poseidon find was a major one
or that other companies would make significant finds, and that the ore was of high quality. Moreover, because
mining is not a fast-paced activity, these assumptions would have to be held for a long time while waiting for the
companies to turn profits. As it turned out, none of the assumptions proved correct.
The price of nickel drifted down to reasonable levels. Few other mining companies discovered nickel in Windarra.
And Poseidon’s own find did not actually produce any of the ore until 1974, five years after the bubble. Even at
that, the find turned out to be smaller than expected, yielding relatively low-quality nickel ore that cost more to
mine than the company had planned. The mine was soon taken over by Western Mining, and basically broke even
over the course of its lifetime. Poseidon, at least, had a viable mine; many of the other companies in the bubble
never even had mining leases.
After the bubble burst, enough investors had lost money that the Australian government took up an investigation.
It came to the conclusion that trading activity in the country’s stock exchanges had too little oversight and too little
regulation. The Poseidon bubble thus led to a reform of Australia’s securities trading regulations over the course of
the 1970s.
Bill Kte’pi
 
See also:  Asset-Price Bubble;  Australia;  Commodity Markets. 
Further Reading
Adamson, Graeme. Miners and Millionaires: The First One Hundred Years of the People, Markets and Companies of the
Stock Exchange in Perth, 1889–1989. Perth, Australia: Australian Stock Exchange, 1989. 
Chancellor, Edward. Devil Take the Hindmost: A History of Financial Speculation. New York: Farrar, Straus and
Giroux, 1999. 
Kindleberger, Charles P. Manias, Panics, and Crashes: A History of Financial Crises. Hoboken, NJ: John Wiley and
Sons, 2000. 

Post Keynesian Theories and Models
 
Post Keynesian theory explicitly develops concepts of British economist John Maynard Keynes’s “general theory”
that often were only implicitly or obliquely mentioned in Keynes’s revolutionary 1936 book, The General Theory of
Employment, Interest and Money. Moreover, Post Keynesians explain why early post–World War II economists
who labeled themselves as “Neoclassical Synthesis Keynesians” and their student progeny who call themselves
“New Keynesians” never understood Keynes’s analytical framework. Instead, these “Keynesians” merely adopted
classical theory, larded it with some Keynesian words, and attributed unemployment to the truculence of workers
who would not let the market wage decline sufficiently to generate full employment. Of course, this sticky wage
argument was the backbone of nineteenth-century classical economists’ explanation of the significant
unemployment observed to occur in the world in which we live.
Keynes’s paradigm-shifting economic text, The General Theory of Employment, Interest and Money, attempted to
overthrow orthodox classical theory, which said that rigid wage rates and government policies interfering with the
operation of a “free” market caused recessions and depressions. Keynes claimed that he provided a revolutionary
analytical way for economists to think about the economy.
Perhaps Keynes’s most radical assertion was that the three axioms underlying classical theory were not
applicable to a modern, money-using, entrepreneurial economic system. Once Keynes challenged the three
fundamental classical theory axioms, his resulting economic theory could explain the operation of a money-using,
market-oriented capitalist economy. However, these three classical axioms remain central to today’s mainstream
economic thinking. Only Post Keynesians have discarded them.
 Rejected Axioms of Classical Economics
An axiom is defined as a statement accepted as a universal truth, without proof, that is used as the basis for
argument. The classical axioms that Keynes rejected are (1) the ergodic axiom, which, in essence, assumes that
past history is a reliable basis for predicting future outcomes; (2) the gross substitution axiom, which asserts that
every item on the market is a good substitute for every other item on the market; and (3) the neutral money
axiom, which asserts that any increases in the quantity of money will always be inflationary. (More technical
definitions of these axioms are given below.)
Only if these three axioms are rejected can a model be developed that has the following characteristics: (1) money
matters in the long and short run; that is, money affects real decision-making, as the economic system moves
from an irrevocable past to an uncertain future; (2) economic decision makers recognize that they make important,
costly decisions in uncertain conditions where reliable rational calculations regarding the future are impossible; (3)
monetary contracts are a human institution developed to efficiently organize time-consuming production and
exchange processes in modern capitalist economies, with the money-wage contract being the most ubiquitous of
these contracts; and (4) unemployment, rather than full employment, and an arbitrary and inequitable distribution of
income, are outstanding faults in a modern, market-oriented, capitalist economy. These faults can be corrected
only when government policies, in cooperation with private initiatives, are aimed at eliminating these flaws in our
economic system.
The ergodic axiom postulates that all future events are actuarially certain—that is, the future can be accurately
forecasted from the analysis of existing past and current market data. Consequently, income earned today at any
employment level is entirely spent either on produced goods for today’s consumption or on buying investment
goods that will be used to produce goods for the (known) future consumption of today’s savers. In other words,
orthodox theory presumes if the future is knowable (i.e., ergodic) then all current income is always immediately

spent on producibles, so there is never a lack of effective demand for things that industry can produce at full
employment. The proportion of income that households save does not affect aggregate demand for producibles; it
only affects the composition of demand (and production) between consumption and investment goods. Thus,
savings creates jobs in the capital goods–producing industries just as much as consumption spending creates jobs
in the consumer goods–producing industries. Moreover, in this classical theory, savings is more desirable than
consumption since the resulting assumed investment projects have positive returns such that, when financed by
today’s savers, today’s investment will automatically increase tomorrow’s total income.
In Post Keynesian theory, however, people recognize that the future is uncertain (nonergodic) and cannot be
reliably predicted. Consequently, people decide on how much of their current income they will spend on consumer
goods and how much they will save. Current savings are then used to purchase various liquid assets. These liquid
assets are, in essence, time-machine vehicles that savers use to store and transport their saved purchasing power
to an indefinite future date. Unlike savers in the future, known (ergodic) classical system, real-world savers in a
world with an uncertain (nonergodic) future do not know exactly what they will buy or what contractual obligations
they will incur at any specific future date.
In an entrepreneurial economy, money is that thing that discharges all legal contractual obligations, and money
contracts are used to organize production and exchange activities. Accordingly, the possession of money—and
other liquid assets that have small carrying costs and can be easily resold for money in a well-organized and
orderly market—means that savers possess the liquidity to enter into money contracts to demand products
whenever they desire in the uncertain future and/or in order to meet a future contractual commitment that they
have not foreseen. Liquid assets are also savers’ security blankets, protecting them from the possibility of hard
times. For as Nobel Prize winner John Hicks stated, income recipients know that they “do not know just what will
happen in the future.”
Keynes argued that money (and all other liquid assets) have two essential properties: First, money (and all liquid
assets) does not grow on trees, and hence labor cannot be hired to harvest money when income earners reduce
consumption to save more and thereby increase their demand to hold money or other liquid assets. Accordingly,
the decision to consume versus to save income in the form of liquid assets, including money, is a choice between
an employment-inducing demand for producible goods and a non-employment-inducing demand for liquid assets,
including money. When savings increase at the expense of the demand for consumption producibles, sales and
employment decline in the consumption-production sector without any offsetting employment increases in the
money-(or liquidity-) providing sector.
Second, Keynes argued that liquid asset prices will increase as new savings increases the demand for such
assets relative to the supply of liquid assets. Because of high carrying and high resale costs, reproducible durables
are not gross substitutes for liquid assets as a means of storing contractual settlement power to be carried forward
into an uncertain future. Consequently, since producibles are not good substitutes for storing liquidity, higher liquid
asset prices do not divert this demand for liquidity in the form of nonproducibles into a demand for producibles. If,
however, the gross substitution axiom were applicable, then producibles would be gross substitutes for
nonproducible liquid assets. Accordingly, if savings increases the relative price of liquid, nonproducible assets,
then savers would be induced to substitute producibles, thus inducing entrepreneurs to hire more workers, and
there would never be a lack of effective demand for the products of industry.
In the real world, however, investment spending is constrained solely by entrepreneurs’ expectations of profits,
relative to their cost of obtaining financing and funding to pay for real investment in plants and equipment. If the
future is uncertain, these expectations of future profits cannot depend on a statistically reliable calculation of future
profit income. Instead, these expectations depend on the “animal spirits”—of a spontaneous (entrepreneurial)
desire for action rather than inaction, as Keynes wrote in The General Theory. In an economy where money is
created by banks, if entrepreneurs have high animal spirits and therefore borrow from banks to finance the
production of additional working-capital goods, then the resulting increases in the quantity of money as
entrepreneurs borrow to expand production will be associated with increasing employment and output. The

classical neutral money axiom, however, asserts that if the money supply increases in response to borrowing from
the banking system, this increase in the money quantity will not alter the level of employment or output. In other
words, the classical neutral money axiom asserts that if banks create additional money in response to any
increased demand for borrowing, the result must be that this increase in money will be more money chasing the
same level of output, thereby causing prices to inevitable increase—inflation.
Finally, in the political realm, classical economists especially associate this neutral money axiom with any
government borrowing (deficit spending). Thus, government deficit spending causes money to be “printed.” This
newly printed money means, given the neutral money axiom, that more money is chasing fewer goods.
Consequently, classical theory’s assertion that financing government deficits by increasing the money supply
always creates inflation is merely an assertion, and not a proven fact.
 Anti-Inflation Policy
Post Keynesians identify two types of inflation and therefore suggest two different policies to fight inflation. These
two types of inflation are (1) commodity inflation and (2) incomes inflation.
Commodity inflation is identified with rising prices of durable, standardized commodities such as agricultural
products, oil, and so forth, that are typically traded on well-organized public markets where the prices are reported
daily in the media. These markets tend to have prices associated with specific dates of delivery—either today
(spot market price) or on a specific date in the future. The markets for future delivery are typically limited to dates
only a few months in the future. Since most commodities take a significant amount of time to be produced, the
supply available for these near-future dates is relatively fixed by existing stocks plus semifinished products
expected to be available on a specific future date. If there is a sudden, unexpected increase in demand or fall in
supply for a future date, this change will inflate the market price for future delivery. The proper anti–commodity
inflation policy is for the government to maintain a buffer stock, that is, a commodity inventory that can be moved
into or out of the market to prevent the commodity market price from inflating (or deflating). For example, the U.S.
government maintains a Strategic Petroleum Reserve (SPR) in caves on the Gulf Coast. During “Desert Storm,”
the short Iraq war in 1991, in order to prevent disruption of oil supplies from causing commodity inflation, the U.S.
government made oil from the SPR available to the market. It is estimated that this prevented the price of
gasoline at the pump from increasing by 30 cents per gallon.
Incomes inflation is associated with rising money costs of production per unit of goods produced. These rising
money costs of production reflect increases in money income to owners of inputs used in the production process
that are not offset by increases in productivity. Post Keynesians advocate anti–incomes inflation policy that would
limit wage and other money income payments to increases in productivity. Such a policy is called an “incomes
policy,” and if effective, it will assure stable prices without depressing the economy and creating any
unemployment. For example, the Kennedy administration in the early 1960s instituted a wage-price guideline
policy that used publicity to limit union wage demands and/or industrial profit increases per unit of output.
On the other hand, the conventional wisdom that accepts the neutral money axiom believes inflation is caused by
government printing money. Consequently, it is argued that the central bank should institute a tight monetary
policy to fight incomes inflation. The central bank’s tight money policy can be successful only if the resulting rise in
interest rates and so forth reduces aggregate demand sufficiently to seriously threaten profits in the private sector
of the economy. The resulting slack in production and increased unemployment are expected to stiffen the
backbone of entrepreneurs sufficiently so that they will refuse to agree to any money wage and/or other production
costs increases. In other words, the conventional wisdom of anti-inflationary policy involves depressing the
economy sufficiently to constrain inflationary income demands. The cost is a large increase in unemployment and
loss in profits and output.
 Employment
In accepting Keynes’s ideas, Post Keynesians reject the classical neutral money axiom when they argue that

changes in the money supply due to borrowing from banks to finance the production of investment goods affects
the level of employment and output in both the short run and the long run.
In arguing that his “general theory” is more applicable to the world of experience than is classical economic theory,
Keynes wrote: “The classical theorists resemble Euclidean geometers in a non-Euclidean world who, discovering
that in experience straight lines apparently parallel often meet, rebuke the lines for not keeping straight—as the
only remedy for the unfortunate collisions which are occurring. Yet, in truth, there is no remedy except to throw
over the axiom of parallels and to work out a non-Euclidean geometry. Something similar is required today in
economics.”
In the above analogy, unemployment is the “unfortunate collision,” while “rebuking the lines” is classical economic
theory’s claim that unemployment is due to workers’ refusing to accept a market wage low enough to encourage
firms to hire all workers, that is, to create full employment. In other words, classical theory blames the unemployed
victims of unemployment for the problem.
At the time that Keynes wrote, not all of the three fundamental axioms of classical theory were explicitly
recognized by professional economists. Consequently, Keynes implicitly threw out these three classical axioms,
without knowing and therefore specifically spelling out which classical axioms were required to be overthrown to
provide the equivalent of a non-Euclidean economics for the world of experience in which the unemployed are not
responsible for the unemployment that occurs.
As a result, when, immediately after World War II, economists such as Paul Samuelson and his followers wanted
to explain Keynes’s economic analysis, Samuelson’s “Neoclassical Synthesis Keynesianism” was based on the
same three classical axioms that Keynes had implicitly thrown over. Accordingly, these economists who called
themselves “Keynesians” had no real theoretical connection to Keynes’s general theory. Instead, these Keynesians
explained unemployment to be the result of labor unions and workers refusing to accept a market wage that
assured full employment and/or government setting a minimum wage above the full-employment market wage
and/or monopolistic firms refusing to lower product prices to more competitive levels in order to sell all that a fully
employed labor force could produce.
The fundamental contribution of Post Keynesians, then, was to explicitly explain which axioms Keynes had to
overthrow to provide a general theory of employment, interest, and money applicable to the world we live in. In
this world or experience, money, money contracts, and the desire for liquidity are the major institutional forces that
make the real economic world go round. The classical theory that still dominates mainstream economic thought
and policy recommendations, whether labeled Neoclassical Synthesis Keynesian, “New Keynesian,” classical
theory, efficient market theory, rational expectations theory, or something else, on the other hand, is still
constructed on a foundation of the three classical axioms that Keynes overthrew. Consequently the mainstream
economic theory that has dominated economics analysis and policy decisions both domestically and internationally
in recent decades created economic conditions that resulted in the so-called “Great Recession” that began in
2007.
Why? If, for example, the future were knowable (ergodic), then all decision makers today would “know” their future
income stream and future contractual outlays. Consequently, in an ergodic world, no rational person today would
enter into a mortgage contract unless they knew they could meet all future mortgage contractual payments. Nor
would anyone have bought a house with a mortgage loan since today many a homeowner is “underwater”—that is,
the outstanding mortgage debt exceeds today’s market price for the house. Consequently, classical theory argues
that government should remove all restrictions and government protection of consumers on mortgage loans, credit
cards, banking lending policies, and so on. As a result, beginning in the 1970s, financial market deregulation was
adopted by most governments.
Today, most “experts” agree that a basic cause of the Great Recession was the large number of defaults in the
subprime mortgage market, and that the inability of the economy to recover is in large part due to many
homeowners still “under-water” and therefore spending less on goods and services. Yet, such an outcome would

not be possible in a classical world in which the future was known ergodically.
When Keynes labeled his analysis a general theory, he meant it was more general than classical theory because
it was based on fewer restrictive axioms. When the three classical axioms are added to Keynes’s general theory,
classical theory becomes a special case of Keynes’s more general theory. Keynes explicitly notes that “the
postulates [axioms] of classical theory are applicable to a special case only and not to the general case….
Moreover, the characteristics of this special case assumed by classical theory happen not to be those of the
economic society in which we actually live, with the result that its teaching is misleading and disastrous if we
attempt to apply it to the facts of experience.”
The policy prescriptions of Keynes’s general theory are quite different from the “special-case” conventional
classical theory wisdom of mainstream economists as far as policies to create a fully employed society and
encourage a prosperous global economic system without causing inflation.
Paul Davidson
 
See also:  Galbraith, John Kenneth;  Keynes, John Maynard;  Keynesian Business Model;  Neo-
Keynesian Theories and Models. 
Further Reading
Davidson, Paul. The Keynes Solution: The Path to Global Economic Prosperity. New York: Palgrave Macmillan, 2009. 
Hicks, John R. Economic Perspectives. Oxford: Clarendon Press, 1977. 
Keynes, John Maynard. The General Theory of Employment, Interest and Money. New York: Harcourt, Brace, 1936. 
 
Poverty
 
Poverty is generally defined as the condition of being unable to afford the basic necessities of life, including food,
potable water, clothing, and shelter. Beyond lacking the absolute minimum to keep body and soul together on a
daily basis, the term becomes trickier to define. Depending on the general standard of living and social values in a
particular country, the inability to obtain a primary education and basic health care for oneself and one’s family, for
example, might also be counted among the defining characteristics.
Poverty is also time and space specific. What constitutes poverty for one generation in a particular society might
not for another. Indoor plumbing was a luxury in the nineteenth century but is considered a necessity to even the
most impoverished household in the United States in the twenty-first. More importantly, poverty is defined very
differently in the developed and developing worlds. Someone living in a two-room shack with electricity, running
water, and enough to eat, but little else, might live well above the poverty line in Africa but would be considered

extremely impoverished in Europe or North America.
An inevitable consequence of downturns in the business cycle, poverty is a fixture on some U.S. landscapes. In
Detroit, the downtown Renaissance Center—headquarters of troubled General Motors—looms behind a homeless
shanty. (Bill Pugliano/Stringer/Getty Images)
 Measuring Poverty
For much of the developing world, the World Bank—the world’s leading multilateral development institution—sets
the income threshold not to be in poverty at $1.25 per person, per day. An income below that level makes it
almost impossible to provide the basic necessities of life, including 2,000 to 2,500 calories for an adult male per
day. By this standard, about 1.4 billion people—or one in four persons in the developing world—live below the
poverty level. At the same time, the U.S. government officially sets the poverty threshold for an individual at
$10,991 per person per year—or about $30 per day—and $22,025 for a family of four in 2008. While the U.S.
standard has been updated regularly to account for inflation since it was first conceived in the early 1960s, it
remains a controversial figure, since it is based on what many experts regard as an outmoded premise.
Specifically, the figure assumes that people spend about one-third of their income on food, which was the case
for poor people in the early 1960s. Today, however, with dramatically lower food prices in the United States, the
figure is closer to one-sixth. In other words, the government assumes that if the head of a household of four is
spending a $7,341.67 a year on food (one-third of $22,025), the household is living right on the poverty line. In
reality, however, if food represents one-sixth of the average poor person’s budget, then a person really needs six
times that amount, or $44,050 annually, to lift a family of four out of poverty. Moreover, with exceptions for costlier
Alaska and Hawaii, the government does not differentiate between locations, even though data and common
sense point to the fact that it is far more expensive to live in urban areas than in small towns. (Moreover, the
government measures income at pre-tax levels—even though the money going for taxes cannot be used to buy
food, shelter, and other necessities—and does not include various governments benefits, such as food stamps
and childcare subsidies, that help families obtain necessities.)
Because poverty is so difficult to define in absolute terms, most scholars argue for a relative definition. In 1995,
the National Academy of Sciences argued that poverty should be defined in income-relative terms. That is, a
family would be considered poor if its household income falls below 50 percent of the median household income.
This definition, of course, makes poverty a phenomenon independent of the overall business cycle. Thus, as
economies expand and prosperity increases—likewise the median income—the number of poor people will not

change significantly; nor would times of contraction, when median income falls, bring a change in the number
living in poverty.
In these relative terms, poverty is defined as inequality of income. To measure this, as opposed to absolute
deprivation, economists use a statistical tool known as the Gini coefficient. On the Gini scale of inequality, a zero
represents absolute equality (everyone in the measured group has exactly the same amount of income), while a
one represents absolute inequality (one person in the group has all of the income). Based on data from 2007–
2008, the United Nations determined that the country with the most equally distributed income was Denmark, with
a Gini coefficient of.247, while Namibia was the most unequal, with.743. The United States fell in roughly in the
middle of all countries, with a Gini coefficient of.408, though this placed it among the most income-unequal of
industrialized nations. Thus, measuring poverty in relative terms puts the United States at a high level of poverty
among industrialized nations.
In the course of U.S. history, long-term declines in poverty generally have coincided with periods of rising equality
—whether caused by rising levels of productivity in the private sector or by government initiative—regardless of
fluctuations in the business cycle. Thus, American inequality and poverty levels fell dramatically in the period
between 1929 and 1975, while poverty levels have gone up slightly between the mid-1970s and the late 2000s, a
period of greatly increased income inequality.
 Causes and Cures
Perhaps even more difficult than measuring poverty is determining its causes, which can differ widely among
individuals and nations. For impoverished nations, many factors play a critical role: a poor natural environment,
overpopulation, war, corruption, misguided government policies, or a colonial past. Whatever the combination of
causes, poverty is typically a vicious cycle for poor nations. Where people have little income, they save even less,
if at all. The lack of savings means a dearth of capital to invest in equipment and human resources (education and
health care). The lack of investment means that productivity remains low, which, in turn, means that people
cannot be paid more and thus cannot save, perpetuating the cycle.
Nevertheless, many countries in the post–World War II era have lifted themselves out of poverty, rising from
developing-world to developed-world status. The best-known examples are found in East Asia. In 1981, the region
was the world’s poorest, with some 80 percent of the population living below $1.25 a day (in inflation-adjusted
terms). By the late 2000s, the figure had fallen to well under 20 percent. Meanwhile, sub-Saharan Africa has
languished, with about 50 percent of its population living in poverty during the early 1980s and up to 2009.
While each nation has a unique history to explain its climb out of poverty, the key factor is breaking the vicious
cycle of low productivity, low savings, and low investment through large investments in education and basic health
care. By doing so, governments lift productivity and savings, which in turn increases investment. With rising levels
of productivity come rising profit margins, often because wages lag due to the pro-business, anti-union policies of
the government. Higher profit margins encourage foreign investment as well, contributing to incomes, savings, and
domestic investment.
The causes of poverty among cohorts of the population in relatively wealthy nations have many causes as well:
discrimination, high unemployment, inadequate education and health care, crime, substance abuse, and what
some sociologists refer to as a “culture of poverty,” whereby children growing up in poor families with a high
dependency on government relief tend to end up in the same situation when they become adults. In recent years,
there has been an effort in the United States and other industrialized countries to break that culture of poverty by
shifting from cash transfers in the form of welfare to in-kind vouchers for education and childcare, as well as the
old standby of food stamps. In addition, tougher work requirements have been imposed in the hopes that this
would create a home environment in which work, rather than idleness, becomes the cultural norm. An especially
popular program in the United States is the Earned Income Tax Credit, which provides refundable tax credits to
people whose wages alone are unable to lift them out of poverty, the so-called “working poor.”

James Ciment and Donna M. Anderson
 
See also:  Employment and Unemployment;  Income Distribution;  Wealth. 
Further Reading
Burtless, Gary.  “What Have We Learned About Poverty and Inequality? Evidence from Cross-National Analysis.”
Focus 25:1 (2007): 12–17. 
Danziger, Sheldon, and Robert Haveman, eds. Understanding Poverty. Cambridge, MA: Harvard University Press, 2001. 
Harrington, Michael. The Other America: Poverty in the United States. New York: Collier Books, 1994. 
Moffitt, Robert.  “Four Decades of Antipoverty Policy: Past Developments and Future Directions.” Focus 25:1 (2007): 39–
44. Madison, WI: Institute for Research on Poverty. 
Sachs, Jeffrey D. The End of Poverty: Economic Possibilities for Our Time. New York: Penguin, 2005. 
Schiller, Bradley R. The Economics of Poverty and Discrimination.  10th ed. Upper Saddle River, NJ: Pearson/Prentice
Hall, 2008. 
Price Stability
 
Price stability refers to the condition in which inflation is low and average prices remain relatively unchanged for a
given period of time even though some prices rise and others fall. Since the beginning of the 1990s, price stability
was the primary objective of monetary policy in many countries. Although central banks are not always explicit
about what exactly price stability means to them, most seem to have accepted that inflation is considered low
when it is kept at about a 2 percent year-over-year increase in the average price of a representative basket of
goods and services such as food, clothing, housing, and other items that typical consumers buy. For this purpose,
the most commonly used measure of inflation is the consumer price index (CPI). Other measures include the
producer price index (PPI), which gives the average change in prices received by producers for their products.
The GDP deflator, yet another widely used indicator, measures the average change in the price of all the goods
and services included in the gross domestic product (GDP), as opposed to the CPI, which omits some goods and
services included in the deflator while including others not in the deflator.
 Causes of Inflation
Inflation is the sustained increase in a price index, often the CPI, over a period of time. Therefore, economists do
not consider a one-time increase in prices when calculating inflation; increases must be continuous and must
affect the general or average price of items included in the basket—and not only a few items. For instance, the
inflation rate would be zero if increases in the prices of some commodities were offset by decreases in the prices
of other items in the basket. Thus, even if individual consumers experience inflation when the goods they happen
to purchase cost more, the overall inflation is low if the basket of goods includes items with small or no price
increases.
This discrepancy is only one of many limitations on the measure of inflation. All measures of inflation must
confront the limitation of changing goods and services over time. As new products are introduced, for example
DVD players, the inflation measure needs to include items that simply did not exist in a prior measurement period.

In addition, altered buying habits—either because of new tastes or because of higher or lower prices—mean that
the relative weight given to products needs to change. Economists take these complications into account, making
adjustments that are sometimes controversial. Even small adjustments, compounded over time, can alter
measurement of important economic statistics related to the business cycle. For example, disagreement about
whether or not the typical U.S. worker is better off today than in the past is based on which inflation measurement
in used.
Although there is general agreement among economists about what causes inflation in theory, there often are
debates about what has caused inflation in practice. Economists agree that inflation can occur when there is an
increase in the aggregate demand (or total spending) relative to the productive capacity of the economy. Increase
in demand can be caused by various factors, such as an increase in government spending, a fall in the rate of
interest, a cut in the income tax rate, or an improvement in business and consumer confidence. Demand side
inflation can also occur when, according to the common expression, “too much money is chasing too few goods.”
This may result from an increase in the money supply by the central bank, which led the Nobel Prize–winning
economist Milton Friedman to remark, “Inflation is always and everywhere a monetary phenomenon.”
Some economists point to the pricing methods of monopolistic firms as a source of inflation. A standard method
used in pricing products is known as “markup pricing,” in which firms add a profit margin to their cost of
production. If the cost of production rises, firms try to preserve their profit margins and pass on the increase to
customers. If markets were sufficiently competitive, no firm would dare attempt increasing its prices for fear of
losing customers or market share. Because firms do have power over the market, however, they often end up
increasing prices, and buyers generally accept living with inflation.
 Consequences of Inflation
Why is price stability desirable? Most economists and policy makers conclude that inflation has negative
consequences for an economy, including the arbitrary redistribution of assets and distorted decision-making. The
most obvious negative effect of high inflation is on people with fixed incomes that do not appreciate with rising
prices. Thus, some retirees (but not Social Security recipients), people depending on social welfare programs, and
workers without cost-of-living adjustments, lose to inflation. Overall economic growth can be slowed by inflation if it
creates uncertainty for consumers and investors, thereby making it difficult to plan for long-term projects. There is
debate about the impact of inflation on undesirable speculative activities. Some economists maintain that inflation
tends to encourage speculative activities at the expense of real investments that expand the productive capacity of
the economy. On the other hand, Nobel laureate James Tobin argues that inflation lowers the return on monetary
assets relative to real assets and that the reaction of investors is to hold more of their assets in real capital
projects. Finally, borrowers benefit from high inflation because the real rate of interest, the interest rate adjusted
for inflation, is lower than the nominal interest rate, the amount they actually pay. If the cost of borrowing is lower
(due to high inflation), consumers may buy more houses, cars, and other assets. Firms then have the incentive to
supply these items, which benefits the whole economy.
 Policy Options
Most central banks have made inflation targeting an essential element of their monetary policy. The inflation target
is usually set at an annual rate of 2 percent, achieved through a combination of monetary policies. If the
aggregate demand is considered too strong, the central bank will raise interest rates in order to depress demand
and keep inflation from rising. Similarly, when demand is weak and there is a risk of deflation, the central bank will
lower interest rates to stimulate the economy. In this way, many of the recessions in recent times appear to have
been engineered by policy makers.
Because it generally takes a long time for an economy to recover from a recession, some economists argue
against what they regard as an obsessive preoccupation with inflation at the expense of employment. They argue
that full employment should be the overriding objective of economic policy, and propose models that show that full

employment is compatible with price stability. In this view, relatively low levels of inflation, say below 4 percent, are
an acceptable trade-off for the more devastating impact of unemployment and slow economic growth. In addition,
some economists maintain that low levels of inflation are desirable because they give business firms flexibility in
setting wages and salaries. They point out that cutting an employee’s pay could lead to disgruntlement and lower
productivity. However, a bit of inflation allows employers to give all employees a nominal wage increase while
actually cutting the real pay of poorly performing workers by raising their pay less than the inflation rate.
 Galloping Inflation and Hyperinflation
When inflation rises to excessive rates, however, great damage can be done to an economy. Galloping inflation,
the informal term economists and policy makers use in referring to rates in the 20–1,000 percent per year range,
can significantly distort economic behavior. Galloping inflation occurred in a number of Latin American economies
from the 1970s to the 1990s, where individuals tended to hold onto minimal sums of the national currency, just
enough for daily purchases. The rest was sold for hard currencies; this capital flight caused local financial markets
to collapse, making it difficult for businesses to access capital for operations and expansion. During galloping
inflation, consumers also spend their money rapidly on larger purchases, and the increased demand fuels further
inflation. Contracts get written with prices hikes in mind, making it difficult to calculate the costs involved in
fulfilling contracts, thereby slowing business activity.
Even worse is hyperinflation, where the value of the currency virtually collapses over a very short period of time,
usually a few months or years. The most famous case of hyperinflation was in Germany in 1922 and 1923, where
inflation reached 10 billion percent. Unable to raise taxes enough to meet its war reparations commitments, the
German government opted instead to inflate its way out of the crisis by printing money in unsustainable amounts.
As the inevitable hyperinflation kicked in, consumers dumped their money as quickly as they could while wages
climbed precipitously. Price and wage instability created gross economic inequities and distortions, which left
workers in poverty and many businesses, particularly small ones, reduced to penury and bankruptcy. Many
historians cite this bout of hyperinflation as a prime factor in the rise of the authoritarian Nazi Party, which came to
power a decade later. A more recent example is Zimbabwe, where the government’s policy of printing quantities of
money far in excess of what the economy can support has led to a collapse in production and exports, as well as
a mass exodus of people to neighboring countries in search of work and economic stability.
Hassan Bougrine
 
See also:  Deflation;  Inflation;  Monetary Policy. 
Further Reading
Mosler, Warren. B.  “Full Employment and Price Stability.” Journal of Post Keynesian Economics 20:2 (1997–1998): 176–
182. 
Wray, L. Randall.  “The Social and Economic Importance of Full Employment” In Introducing Macroeconomic Analysis:
Issues, Questions and Competing Views, ed. Hassan Bougrine and Mario Seccareccia, 219–228. Toronto: Emond
Montgomery, 2009. 
Wray, L. Randall. Understanding Modern Money: The Key to Full Employment and Price Stability. Aldershot, UK: Edward
Elgar, 1998. 
Production Cycles

 
The production cycle for a firm is the period over which a product is created, or a service is provided, until
payment is received from the customer. For example, a custom-furniture manufacturer, when responding to a
customer order, will need to order materials and hire labor to manufacture the product. In this case, the production
cycle begins at the moment that these materials and labor arrive on the scene, and it doesn’t end until the product
is finished and paid for. For a service organization such as a restaurant, purchases of food and materials used to
provide meals, as well as the labor to prepare and serve the meals, all need to be in place prior to the
commencement of operations. The production cycle begins when they are put in place. The end of the cycle
occurs the moment the customers pay.
With the continued development of the Internet and fiber-optics communications infrastructure and related
techniques, such as just-in-time production methods and bar-coding control of warehousing operations, the
management of the operations cycle has become much more time efficient. The growing importance of service-
based industries might also imply that operation cycles are becoming more compressed as services are less
dependent on the flow from raw materials to final service. The modern science of operations management and
financial management are devoted to understanding the nature of this cycle, forecasting and optimizing product
and service flow, and minimizing costs and financial resources involved in the process.
 Long-Term Production Cycles
There are still many important industries in the economy that will continue to experience very long production
cycles. Organizations in the residential and nonresidential construction industry, for example, face very long
production periods due to the nature of the business. Construction firms need to purchase and assemble land,
seek zoning approvals from local governments, and install and connect to basic services like water, sewer, and
roads before building construction can begin. Accurately forecasting demand is extremely difficult, and the timing
of completion could unfortunately coincide with a downturn in the overall economy and slack demand.
Firms that manufacture large, complex capital goods such as aircraft and oceangoing ships also require
substantial periods of time to design and manufacture products. If an increase in capacity to manufacture is
required, a large-scale investment in plant and equipment will also add time to the production cycle. Firms that
provide utilities (infrastructure) such as fiber-optic cable service are assembling right-of-way agreements and
permits, and laying cable well in advance of any customer sales.
Such firms must live with the long time lags from project inception to receiving payments from customers. These
time lags create considerable risk for the firm. Will future economic conditions support sales at a time when
products or services are actually available? These risks tend to be attenuated or perhaps underappreciated upon
the introduction of a new technology, or the early part of a business expansion when the outlook for the future is
favorable and current sales are brisk. Decisions are made to proceed within an environment of optimism. Longer-
term negative implications are not as readily apparent. These could include the possibility of a downturn in the
economy, changes in technology that might negatively affect the competitive advantage of the firm’s product or
service, and the consideration of marketplace impacts of potential overcapacity relative to demand.
 Keynesian Economics and Production Cycles
The Keynesian aggregated-expenditures model captures this potential mismatch of planned activity versus results
upon market delivery as the cause of cycles of expansion or contraction of the overall economy. The model starts
with the plans of businesses (in the aggregate, over all businesses in the national economy) to produce at the
beginning of the period. A production plan ensues that is based upon the best forecast of sales by management
during the period. If this plan is realized by the end of the period, in that planned production equals sales, the
process is repeated and the economy is in equilibrium. If planned production is less than sales, unintended

inventory reduction occurs, and businesses plan to increase production the next period. If planned production is
greater than sales, unintended inventory increases, and production plans are reduced for the next period. During
the period, unanticipated changes in household income, business investment, government expenditure, and
changes in foreign trade (aggregate demand) or changes in credit (banking) conditions can affect the end of
period reconciliation of production and sales, and thus can change plans for the following period.
The uncertainty about the planning period, unanticipated changes in aggregate demand, and adjustments to
production plans set up the potential for an economy to experience periods of economic expansion and
contraction. Against this backdrop, government fiscal policy, monetary policy, credit and banking conditions, and
technological change all interact to amplify or attenuate the natural fluctuations of production cycles.
 Historical Examples of Production Cycles
Historically, the production cycle for capital goods and infrastructure, given its large size and impact on the
economy, has contributed to upward and downward swings in economic activity. Early large-scale infrastructure
projects such as the Erie Canal were commercially successful, but similar projects in the second wave of
expansion after the 1830s were not, due to the competition of more efficient railroads, a new technology whose
impact was unanticipated by those financing and building canals. State financial support was important during this
second phase as states sought canal investment to promote economic development. Along with the growth of rail
competition, excessive debt made the canal companies, and their state backers, subject to financial stress and
bankruptcy during downturns in economic activity such as the economic depression or panic of 1837.
Economic theorists and historians have extensively examined the effect of large-scale expansion of the railroad
network upon the long-term development of the U.S. economy. The network stretched approximately 30,000 miles
(48,300 kilometers) and was confined to east of the Mississippi prior to 1860. By 1910, 351,000 miles (565,110
kilometers) of track were in place within a transcontinental system. Business-cycle theorist Joseph Schumpeter
attributed a long upward swing in economic activity after 1875 to the railroads. Other economists, including Robert
Fogel and Albert Fishlow, have argued that the quantitative impact of the railroads on the overall economy and the
business cycle was relatively small (less than 5 percent of gross domestic product, or GDP). Additionally,
economist Jeffrey Williamson maintained that the indirect effects of railroads, in terms of promoting technical
change in industry and agriculture, induced higher rates of investment and productivity growth in the overall
economy. Overcapacity, high debt, and poor management made many railroads vulnerable to economic downturns
by the 1890s. Some 153 railroads filed for bankruptcy during the economic depression of 1893 alone.
Between 1899 and 1929, electric-power generation expanded by almost 300 percent. The new technology greatly
increased manufacturing production efficiency over steam power methods and introduced a whole new lifestyle on
the consumer side, based upon electric lighting and appliances in the home. Although the provision of electric
power required large-scale investment in plant and equipment and supportive financing, electric power utility
stocks were the most prized on Wall Street during the euphoria of the roaring 1920s economic expansion. These
companies suffered some of the worst financial outcomes after the stock market crash in 1929 and the onset of
the Great Depression from 1929 to 1932.
During the 1990s economic expansion, Internet technology companies and fiber-optic utilities that invested in the
infrastructure to support the Internet played the same role. The industry is broadly defined as the
telecommunications industry, which includes wholesale carriers of fiber-optic Internet traffic, such as AT&T and
Verizon, and equipment manufacturers such as Cisco Systems. Business investment in new equipment and
structures rose nearly one-third as a share of GDP from 1995 to 1999, as business had high and rising
expectations of profits for the decade ahead from the applications of new technology. A large share of the
investment increase appeared directly within the telecommunications industry. Its share of the economy doubled,
providing two-thirds of the new jobs and one-third of the new investment during the expansion. The expansion of
the network eventually outpaced the growth of Internet traffic. By 2002, the industry lost $2 trillion in market
capitalization, and 25 major companies in the industry were in bankruptcy. By 2007, the imbalance between

capacity and demand had in large part been attenuated. Falling prices for Internet service and productivity gains
by equipment suppliers have stimulated demand such that capacity utilization levels similar to 2000 have been
reattained.
 Housing Boom and Bust
The expansion of the U.S. economy after November 2001, deceleration of growth since 2006, and decline since
December 2007 have all in part been driven by the expansion and contraction of the production cycle for
residential housing. The contribution of housing to the 2007–2009 recession has been particularly spectacular and
unprecedented, with spillovers to the global economy.
The surge in demand and production actually started before the 2001 recession in the mid-1990s. The 2001
recession had just a minor impact on the strong upward trend in housing prices and production that ended in 2006
and 2007, respectively. The ensuing decline in prices and production from 2006–2007 was a major drag on the
economy, which eventually went into a recession in December 2007. From the first quarter of 2006 through the
first quarter of 2008, the decline in residential housing investment was the leading negative contributor to overall
economic performance. During the economic expansion from 2002 through 2003, the expansion of residential
housing investment was the second or third leading sector in contributing to growth, after household consumption
and nonresidential investment. Note that part of the increase in household consumption was fueled by
homeowners’ refinancing or taking home equity lines of credit that extract part of the inflated equity in their homes
due to the large increases in housing prices. The funds taken out could then be used for consumer purchases.
During 2005 and 2006, a number of events initiated an end to the long-term increase in housing construction and
house prices. By August 2006, housing starts were down by 20 percent from the 2005 peak. Rising home prices
reduced the number of households that could qualify for a mortgage to buy a home. Sales and inventories peaked
at 1.1 million units and 536,000 units during 2006. Both series entered a steep decline until May of 2009, with
sales and inventories falling to 342,000 and 292,000 respectively, levels comparable to the pre-expansion levels of
the early 1990s. Prices surged in real terms by 33 percent over the cycle, from 1996 to 2006, and then declined
by 15 percent to early 2009.
From 2006 to 2009, foreclosures on mortgages doubled, which made the reduction of inventories more difficult for
the home-building industry. Many homebuyers, at the margin in terms of ability to make mortgage payments, were
enticed by low initial interest-rate arrangements during the early 2000s that required refinancing at higher interest
rates within three to five years. Many of these households were not able to pay the higher payments or to
refinance because their homes had declined in value and they now owed more than what the homes were worth.
Many were forced into foreclosure. Consequently, the home-building industry cut back new construction to
extremely low levels. At the peak of the cycle during the first quarter of 2005, 448,000 units were started. By the
first quarter of 2009, housing starts were reduced to 78,000 units, a reduction of 83 percent.
The financial crisis and recession, beginning in late 2007 in the United States but in 2008 in many other countries,
are widely considered the worst since the Great Depression. The upward thrust of the business cycle during the
early 2000s and the eventual collapse in late 2007 have their origins in the production cycle of residential housing
and related factors and events.
A number of factors that were unique to the early 2000s contributed to the rapid growth in residential construction.
There was a large flow of foreign saving into the United States, and financial institutions were eager for new
customers, particularly in mortgage lending. Much of the lending was poorly managed in terms of evaluating client
risk. Domestically, the supply of savings to the mortgage market was no longer constrained by regulations, as
these had been removed by deregulation during the mid-1980s. Since that time, the housing-construction sector
has had a much stronger and sustained impact on the overall economy.
On the demand side, the surge of demand actually started in 1997 and continued through the 2001 recession to
peak in 2006. This has been linked to a strong upward shift in the long-term trend in labor productivity after 1997,

which continued until a deceleration in 2004. Researchers have discovered a strong link between growth in labor
productivity, growth in labor income, and a positive outlook by consumers, which spurred interest in investment in
housing. A number of other factors supercharged this demand, including interest by households in second homes
as an investment, and the availability of financing to lower-income households.
Innovations in financial markets also allowed for a freer flow of savings into mortgage markets. Since the 1990s,
securitization of mortgages allowed financial institutions to package and sell mortgages and receive new funds to
make more mortgages. Financial institutions no longer held mortgages to maturity. The negative side of this
process was that the quality of the mortgage-backed securities was not known, and many investors relied on
equally irrational ratings by services such as Moody’s, Standard & Poor’s, and Fitch Rating Services. Additionally,
some of the buyers of mortgage-backed securities bought them for their higher yield, by borrowing less expensive
funds in short-term financial markets, such as the commercial paper market. These negative aspects would
eventually become influential in the collapse of the housing market, the market for new housing, and impacts on
financial markets and the economy in general.
It was after September 2007, when home-price appreciation stopped, that events cascaded through financial
markets to start a broader downturn in the overall economy. This negatively affected public confidence in the
value of mortgage securities, which suddenly no longer had a viable market. Short-term sources of lending such
as the commercial paper market virtually disappeared as suppliers of capital became concerned that borrowers
holding mortgage-backed securities who were intending to refinance portfolios containing mortgage-backed
securities might not be able to repay loans. This triggered the failure and near-failure of many prominent financial
institutions. Credit markets froze and stock prices fell, initiating a national and global economic contraction. The
U.S. economy officially entered a long-lasting recession in December 2007.
Production cycles have played an important role in the history of business cycles in the United States. Production
cycles will continue to be important in the modern economy. With growing integration of the world economy,
production cycles could take on new meaning domestically as foreign customers and suppliers participate in the
cycle. International financial aspects such as exchange rates and interest rates will become involved in the
interaction between the production cycle of an industry and the overall business cycle.
Derek Bjonback
 
See also:  Inventory Investment;  Manufacturing. 
Further Reading
Bernanke, Ben.  “Four Questions About the Financial Crisis.” Board of Governors of the Federal Reserve System, April 14,
2009. 
Emmons, William A.  “Housing’s Great Fall: Putting Households’ Balance Sheets Together Again.” The Regional Economy, 
Federal Reserve Bank of St. Louis, October 2009. 
Hughes, Jonathan, and Louis P. Cain. American Economic History.  8th ed. Boston: Addison-Wesley, 2010. 
Kahn, James A.  “Productivity Swings and Housing Prices.” Current Issues in Economics and Finance 15:3 (July 2009): 1–
8. 
McConnell, Campbell R., and Stanley L. Brue. Economics: Principles, Problems, and Policies.  17th ed. New York: McGraw-
Hill/Irwin, 2006. 
Miles, William.  “Housing Investment and the U.S. Economy: How Have the Relationships Changed? Journal of Real Estate
Research 31:3 (2009): 329–350. 
Phelps, E.S.  “The Boom and the Slump: A Causal Account of the 1990s/2000s and the 1920s/1930s.” Policy Reform 7:1
(March): 3–19. 

Samuelson, Robert J.  “The Worrying Housing Bust.” Newsweek, October 16, 2006. 
Schwiekart, Larry, and Lynne Doti. American Entrepreneur. New York: Amacom Books, 2010. 
Stiglitz, Joseph E. The Roaring Nineties. New York: W.W. Norton, 2003. 
Productivity
 
Productivity is a measure of how efficiently an economy produces goods and services. Labor productivity equals
the national output divided by the number of workers needed to produce this output. Productivity growth, the
percentage change in productivity from one year to another, measures the additional output per worker that we
get this year compared to last year. A broader notion, total factor productivity, seeks to combine labor and capital
inputs in the denominator of the productivity measure. Because of the difficulties adding together such diverse
factors as labor and various kinds of capital, most economists focus on labor productivity.
National well-being mainly depends on productivity. The more each worker produces, the more a nation has to
consume; and the faster productivity grows, the faster average living standards rise. To take a simple example,
U.S. productivity grew around 3 percent per year from 1947 to 1973. Since then it has grown only 2 percent per
year. Had productivity growth not fallen, the average standard of living in the U.S. would have been nearly 50
percent greater in 2010 than it actually was. Productivity growth can also help tame inflation by offsetting higher
input costs with a need for fewer inputs by business firms.
There are three main economic approaches to analyzing the determinants of productivity and productivity growth—
the neoclassical, the Post Keynesian, and the behavioral-social.
 Neoclassical Approach
The neoclassical approach emphasizes the importance of new technology, capital investment, and economic
incentives. Capital investment takes place when firms expand, purchasing new plants and equipment. This
contributes to productivity growth because new machinery helps workers produce more efficiently. In addition,
when firms expand and invest, they usually adopt new technologies and therefore can produce goods more
efficiently. Robots on an automobile assembly line make automobile workers more productive; a computerized
inventory system that automatically orders goods when store shelves are low makes salespeople more productive.
According to the neoclassical view, savings are necessary for new investment to take place; they are what finance
business investment. The rewards for saving are the interest, dividends, and capital gains earned from income
that is saved rather than spent. For this reason, neoclassical economists support large incentives to save,
particularly low taxes on these forms of income.
Other economic incentives also affect productivity growth. Incentives spur people to work hard so that they can
become wealthier. If businesses and workers get to keep the gains from productivity growth, they will have greater
incentives to produce more efficiently. This means that income taxes and taxes on corporate profits must be kept
low.
Neoclassical economists also emphasize the need for labor discipline, to make sure people seek work rather than
seeking to increase their leisure time. If lack of effort leads to low income, and maybe even poverty, there is a
greater incentive to work hard. Consequently, government programs that help people survive with little or no
income from work (such as unemployment insurance and food stamps) provide disincentives to work hard and be

productive. In addition, these government programs must be financed, so they require a tax increase, further
hurting productivity.
Finally, the neoclassical approach to productivity demands that inflation be kept firmly under control. Prices
provide important information to the firm. They signal how to produce goods efficiently, and what resources are
cheaper and better to use in production. During inflationary times, when all prices are rising, firms usually have a
hard time figuring out the most efficient way to produce goods; as a result, productivity growth suffers. In addition,
in times of inflation, managers must focus on controlling inflation and reducing costs rather than producing goods
more efficiently.
 Post Keynesian Approach
The Post Keynesian approach emphasizes the importance of demand in promoting productivity growth. Three
mechanisms operate here. The first comes from Adam Smith, who noted that goods can be produced more
efficiently when firms can divide tasks, when individuals can specialize and develop expertise in narrow areas, and
when machinery can be employed to assist workers. This is possible only when sales are large enough to justify
capital investment and a restructuring of production. Robust sales let firms take advantage of economies of scale,
or the gains that come from specialization and investment in new machinery.
A second reason why productivity depends on demand stems from the characteristics of a service economy. The
productivity of a symphony orchestra does not depend on how fast musicians play a piece of music. Rather, its
productivity (the value of its output divided by the number of players) depends on ticket sales. When the economy
stagnates, people are reluctant to go to the symphony. The productivity of the orchestra thus falls. In a booming
economy, the concert hall is full and the productivity of the orchestra grows rapidly. The orchestra may also
produce CDs, a manufactured good. Again, demand determines productivity. The value of the output in this case
depends on how many CDs get sold. When demand is high and sales boom, productivity soars; when people are
not buying CDs, productivity declines.
What is true of the symphony orchestra is true of most service occupations and even industries that produce
physical goods (as the CD example shows). The productivity of the sales force in a store, of real-estate agents,
and of newspaper reporters all depend on sales. For example, when home sales fall due to poor macroeconomic
conditions, the productivity of realtors declines.
Finally, productivity is just an average of the productivity in different industries. It will change with changes in the
industrial composition of the nation. When demand shifts to goods and services produced by more productive
economic sectors, productivity will increase. In an expanding economy, growing sectors are likely to be more
productive and labor can shift there. Conversely, in a stagnant economy, people tend to stay put rather than move
to more productive sectors.
In the eighteenth century, François Quesnay pressed French policy makers to support agriculture, believing this
was most productive economic sector. He argued that greater demand for agricultural goods would move workers
from other sectors to agriculture, where they would be more productive, and let French farmers employ more
advanced production techniques to meet the additional demand. In the twentieth century, Nicholas Kaldor looked
at the manufacturing sector as the most productive sector, but his argument parallels Quesnay—some sectors are
more productive than other economic sectors, and the government should aid more productive economic sectors.
For these reasons, Post Keynesians look to government policies to promote full employment and support high-
productivity industries as a means of boosting productivity.
 Behavioral-Social Approach
The final approach to productivity growth focuses less on macroeconomic factors and more on microeconomic
ones such as relative pay and the treatment of workers. In this view, people matter; how they are treated and how

they work together affect productivity. In many jobs, individual effort is at the discretion of each employee.
Unhappy employees can reduce productivity to the detriment of all. They may even quit or have to be fired,
requiring both time and effort in order to find and train a replacement.
Behavioral and social economists see the excellent employee relations at Japanese firms as a major reason for
the large productivity growth in Japan after World War II. To improve productivity growth, they advocate following
many Japanese labor practices—increased employee participation in firm decision-making, rewarding all
employees for improvements in productivity, treating employees better, and putting greater effort toward improving
job satisfaction. In contrast, an adversarial relationship between management and workers, rather than a
cooperative one, hurts productivity. Attempts to control workers more, to prevent and break unions, and to
squeeze more out of each employee while reducing pay, benefits, and job security, will lead to lower productivity
growth.
Behavioral and social economists also point out that people care about relative pay as much as absolute pay. The
ultimatum game examines this issue. Two people have a fixed sum of money to divide. The first person can
propose any division she chooses; the second person can only accept or reject this division. If accepted, each
person receives the amount of money proposed by the first subject; if the division is rejected, each person
receives nothing. From a neoclassical perspective, assuming rational and self-interested individuals, dividers
should propose that they get almost all the money; the second subject should then accept this offer, rather than
receiving nothing. In many experimental settings, individuals have played this game for real stakes. Daniel
Kahneman found that dividers tend to make substantial offers and that most people reject unequal offers. These
results have been replicated many times, including in cases where people rejected offers equal to one month’s
pay because they felt the split was unfair.
According to the behavioral approach, the ultimatum game better approximates what goes on in the real world
than the neoclassical model of self-interested individuals. Large firms propose a division of the revenues they
receive from selling goods. Workers can ill afford to reject this offer outright, since they need a job and an income
to survive. But workers can quasi-reject a proposal they regard as unfair by working less hard, by sabotaging
production and firm efficiency, and by causing firm resources to be used in setting rules and monitoring workers
rather than in producing goods and services.
There is some empirical evidence that large pay differentials do hurt productivity when productivity depends on
team effort. This is true for sports teams, academic departments, and business organizations.
 Assessment of Theories
No consensus exists among economists regarding the most important determinants of productivity growth or the
reasons why U.S. productivity growth slowed beginning in the 1970s. In addition, the main theories face great
difficulty explaining the rise in productivity growth in 2009 during what has been called “the Great Recession.”
Since there was little new investment taking place and no new technology adopted into new production processes,
the neoclassical perspective cannot help explain this change, though some economists argue that the productivity
gains have something to do with employed workers carrying more of the load that had been performed by laid-off
workers. The increase of productivity growth during this time goes counter to the Post Keynesian prediction. And
since income inequality likely rose (as usually occurs during a recession), the behavioral perspective also fails to
explain the anomaly.
Overall, these three theories, plus the many other hypotheses that have been put forward, explain but a small part
of changes in productivity growth over time and across nations. Given the importance of productivity for national
well-being, this area of economics will surely receive greater attention and study in the near future.
Steven Pressman
 

See also:  Growth, Economic;  Wages. 
Further Reading
Altman, Morris.  “A High Wage Path to Economic Growth and Development.” Challenge 41(January/February 1998): 91–
104. 
Blinder, Alan, ed. Paying for Productivity: A Look at the Evidence. Washington, DC: Brookings Institution, 1990. 
Bloom, Matt.  “The Performance Effects of Pay Dispersion on Individuals and Organizations.” Academy of Management
Journal 42(February 1999): 25–40. 
Gordon, David. Fat and Mean. New York: Free Press, 1996. 
Levitan, Sar, and Diane Werneke. Productivity: Problems, Prospects and Policies. Baltimore, MD: Johns Hopkins University
Press, 1984. 
Pressman, Steven. Fifty Major Economists.  2nd ed. London and New York: Routledge, 2006. 
Pressman, Steven.  “The Efficiency of Nations: A Post Keynesian Approach to Productivity.” Journal of Post Keynesian
Economics,  forthcoming.
Profit
 
Profit is defined as the excess of revenues over costs. The term has roots in Latin words meaning to advance,
progress, grow, or increase. It is generally applied in two different ways, one for economics and one for
accounting.
 Accounting Profit
Accounting profit is the difference (excess) between revenues and costs incurred. The costs are incurred during
the process of bringing to market whatever goods and services are documented as a productive enterprise
(whether by harvest, extraction, manufacture, or purchase) and represent expenditures on the components of
delivered goods and/or services plus any operating or other expenses. These costs are typically divided into fixed
costs and variable costs. Fixed costs do not change in proportion to production levels or sales and include items
such as rent, salaries (of permanent, full-time workers), property taxes, and interest expense. Variable costs are
those that increase directly in proportion to the number of units produced or sold; they can include outlays for
goods sold, sales commissions, shipping charges, delivery charges, direct materials or supplies, and wages of
part-time or temporary employees.
The revenue portion of the profit calculation is relatively straightforward. Revenues represent the value, in dollars,
of the items sold. One potential ambiguity is the determination of which period the revenue should be recognized,
or “booked.” Costs, however, can be significantly more challenging to represent accurately, due to accounting
definitions and guidelines. Items such as depreciation, amortization, and overhead are often open to interpretation
and changing rules and regulations. Depreciation is defined as a deduction for the wearing down and/or
obsolescence of capital, such as vehicles, buildings, and machinery, used in the production of an enterprise’s
goods and services. Amortization is the consumption of the value of assets—usually intangible ones, such as a
patent or a copyright—over a period of time. Overhead expenses are production and nonproduction costs that are
not readily traceable to specific jobs or processes. Overhead expenses encompass three general areas: indirect

materials, indirect labor, and all other miscellaneous production expenses, such as taxes, insurance, depreciation,
supplies, utilities, and repairs.
Accounting profit can be expressed by a few different calculations, each defined by a different technical term:
1. Gross profit is the amount received from sales minus the costs of goods included in those sales, called “Cost
of Goods Sold.” On a unit basis, this represents the profit margin represented by each item sold. The profit
should be enough to pay for all other expenses associated with managing the business, such as salaries,
advertising, depreciation, and interest, among others.
2. Net profit is the amount received from sales, minus all the expenses incurred in operating the business,
including the opportunity cost of using all of the resources in their next best alternative. Thus, net profit
includes the foregone wages and interest that an owner/investor could have earned in the next best
alternative to this business. Net profit always represents a positive number. Net income is calculated by the
same method, but can be positive or negative. If it is negative, the term used is net loss.
3. Operating profit is the amount derived from core business activities and does not include revenue from other
sources, such as return on investments or interest on loans.
4. EBIT (earnings before interest and taxes) also represents core business earnings, but is calculated prior to
deductions for loan interest and tax payments.
 Economic Profit
Economic profit refers to the surplus funds generated by an enterprise over and above net profit. The calculation
of net profit includes all costs incurred in producing and selling the goods of the firm, including the implicit costs of
the owner’s foregone income to do this business. These costs also include any associated with capital investment,
including loan principal and interest. Economic profit can be viewed as a bonus or reward for the owner/investor,
in consideration of their willingness to take the risk of funding and/or operating the business enterprise. In this
context, net profit is sometimes referred to as “normal profit.” The calculation of normal profit includes (as a cost)
the rate of return required by a potential investor to make or continue their investment in the enterprise. Economic
profit occurs when the revenue level is sufficient to exceed all costs, including the cost of capital investment and
any other opportunity costs.
In a purely capitalistic society, profits and losses are the primary concern of the business owner, investors, and
management. Those who own firms (capitalists) undertake the task (personally or through their appointed
representatives) of organizing and performing production efforts so as to optimize their income and profitability.
The quest for profits is guided by the famous “invisible hand” of capitalism, as conceived by Adam Smith. When
profits are above the normal level, they attract additional investment, either by new firms or by existing firms
looking to maximize their return. The investment seeks only the highest reward and is not concerned with the
underlying vehicle or venue that produces the profits. Additional competitors generally drive prices (and then
profits) down. New investment continues to enter until profit levels no longer exceed the return available
elsewhere. This “free-market” system drives investment in areas where consumers provide demand. With
technological innovation, consumer demand shifts to new industries and investment follows. Over time, that market
becomes crowded, prices and profits fall, and there is demand for new innovation—repeating the cycle. In a
market economy, economic profits can exist for a long time only if the firms in the industry can keep other firms
from entering. If new firms can enter because the existing firms are earning a return over and above a “normal”
rate of return, the economic profit will be driven out by the increased competition.
 Social Profit
The social profit from a firm’s activities is the normal profit plus or minus any externalities that occur in its activity.

A firm may report relatively large economic profits, but by creating negative externalities, its social profit can be
rendered relatively small.
An externality is any effect not directly involved in the transaction. When this occurs, market prices do not reflect
the full costs or benefits in production or consumption of a product or service. A positive impact is called an
external benefit, while a negative impact is called an external cost. Producers and consumers in a given market
may either not bear all of the costs or not reap all of the benefits of the economic activity. For example, a
manufacturing or chemical company that causes air or water pollution imposes costs on the entire society, in the
form of cleanup efforts that will be needed later or in the form of taxes and regulatory restrictions on operations or
the consumption of products in that market. On the other hand, a homeowner who takes care to maintain his
house and property creates value not only for his own home, but also for the other homes and owners in the
neighborhood. For these neighbors, increased home values come without effort or expenditure on their part but
yield a higher price when they choose to sell their house.
In a competitive market, the existence of externalities causes either too much or too little of the good to be
produced or consumed in terms of overall costs and benefits to society. In the case of external costs, such as
environmental pollution, the goods often will be overproduced because the producer does not take into account
the external costs. If there are external benefits, as in the areas of education or safety, private markets may well
produce too little of the goods because the external benefits to others are not taken into account. Here, overall
cost and benefit to society is defined as the sum of the economic benefits and costs for all parties involved.
Early champions of capitalistic principles focused on economic profit, with little regard to social profit. As the
industrial revolution gained momentum, however, technological progress brought new challenges. Few, if any,
foresaw the cost of advances in technology, especially as American society was embracing a more materialistic,
consumption-oriented lifestyle. The quest to maximize profits rewarded short-term thinking and policies. The focus
on corporate earnings led to “creative” accounting practices, and ultimately outright fraudulent and criminal activity.
In the early years of the twenty-first century, the dubious practices of a number of major corporations were
brought to light. At Enron, Tyco, and WorldCom, to name just a few, senior managers were found to have
committed accounting fraud and sent to prison. The impact of their actions led to financial ruin for many
stockholders and employees, a general loss of confidence in the nation’s corporate infrastructure, and the
introduction of regulatory guidelines to prevent future occurrences. Laws such as the Sarbanes-Oxley Act—
officially the Public Company Accounting Reform and Investor Protection Act of 2002—were passed to establish
new or enhanced standards for the boards of directors, management, and accounting firms of all publicly traded
U.S. companies.
Despite these experiences—both the fraudulent practices and the efforts to enact remedies—the year 2008 found
the country facing similar profit-driven challenges. This time, the culprits were bad lending practices and
complicated investment strategies designed to maximize profits but understood by few investors, traders, or
lenders. The impact of these practices was found to be far more damaging and widespread than those earlier in
the decade, affecting companies across industries from banking to manufacturing and damaging national
economies around the globe. Given the depth and scope of the crisis, the world’s government leaders and
economic policy makers responded with Keynesian force, intervening with large bailouts and buyouts.
The global financial crisis and recession of the late 2000s gave impetus, at least in some quarters, to a new
paradigm for the conduct of business, particularly as it pertains to the definition and pursuit of profits. In the new
formula, social and environmental values were added to the traditional economic measures of corporate or
organizational success. Triple bottom line (TBL) accounting—measuring people, planet, profit—is a methodology
that quantifies the social and environmental impact of an organization’s activities relative to its economic
performance, for the purpose of targeting and achieving improvements. The phrase was coined by John Elkington
in his 1998 book Cannibals with Forks: The Triple Bottom Line of 21st Century Business and has been elaborated
by others, most notably Andrew Savitz in The Triple Bottom Line (2005).

With the ratification of new standards for urban and community accounting by the United Nations and ICLEI
(International Council for Local Environment Initiatives) in 2007, the triple bottom line became the dominant
international approach to public-sector, full-cost accounting. Similar UN standards apply to natural and human-
capital measurement to assist in measurements required by TBL, such as the ecoBudget standard for reporting
ecological footprints. In 2003, the ICLEI was officially renamed ICLEI–Local Governments for Sustainability. In the
private sector, a commitment to corporate social responsibility implies a commitment to some form of TBL
reporting.
Corporate social responsibility (CSR) is an obligation of the organization to act in ways that serve both the interest
of stockholders and those of its many stakeholders, both internal and external. It consists of three essential
components:
Doing business responsibly
Taking a leadership position in community investment and social issues relevant to the business
Transparency and public reporting of the social, environmental, and financial effects and performance of the
business.
Ideally, CSR policy would function as a built-in, self-regulating mechanism whereby businesses would monitor and
ensure their adherence to law, ethical standards, and international norms. Essentially, CSR is the deliberate
inclusion of public interest in corporate decision-making and respect for the triple bottom line.
The practice of CSR has been subject to considerable debate and criticism. Proponents argue that there is a
strong case to be made in pure business terms, in that corporations benefit from operating with a broader and
longer perspective than their own immediate, short-term profits. Critics argue that CSR distracts from the
fundamental economic role of businesses. Others argue that there is no proven link between CSR and financial
performance. And many question its relationship to the fundamental purpose and nature of the business enterprise
—inevitably entailing higher expenses and risking lower profits. Even the most ardent supporter of CSR will admit
that the financial advantages are hard to quantify and that the benefits are not always visible, at least not in the
short term.
 Nonprofits
A nonprofit organization (NPO, or not-for-profit) is any organization that does not aim to make a profit and is not a
public institution. Whereas for-profit corporations exist to earn and distribute taxable business earnings to
shareholders, nonprofit corporations exist solely to provide programs and services of public benefit. Often these
programs and services are not otherwise provided by local, state, or federal entities. While they are able to earn a
profit—called a surplus in the case of nonprofits—such earnings must be retained by the organization for the
future provision of programs and services. Earnings may not benefit individuals or stakeholders. Underlying many
effective nonprofit endeavors is a commitment to management. NPO leaders have learned that nonprofit
organizations need effective management just as much or even more than for-profit businesses do, precisely
because they lack the discipline of the bottom line.
In the United States, nonprofit organizations are formed by incorporating in the state in which they expect to do
business. The act of incorporating creates a legal entity that enables the organization to be treated as a
corporation under law and to enter into business dealings, form contracts, and own property as any other
individual or for-profit corporation may do. The two major types of nonprofit organization structure are membership
and board-only. A membership organization elects its board, holds regular meetings, and has power to amend its
bylaws. A board-only organization typically has a self-selected board and a membership whose powers are limited
to those delegated to it by the board. One major difference between a nonprofit and a for-profit corporation is that
the former does not issue stock or pay dividends, and may not enrich its directors.

Charles Richardson
 
See also:  Corporate Finance. 
Further Reading
Albrecht, William P. Economics. Englewood Cliffs, NJ: Prentice-Hall, 1983. 
Ammer, Christine, and Dean Ammer. Dictionary of Business and Economics. New York: Free Press, 1977. 
Drucker, Peter. What Business Can Learn from Nonprofits. Boston: Harvard Business Review Press, 1989. 
Hillstrom, Kevin, and Laurie Collier Hillstrom. Encyclopedia of Small Business. Farmington Hills, MI: Gale Cengage. 2002. 
Pyle, William W., and Kermit D. Larson. Fundamental Accounting Principles. Homewood, IL: Richard D. Irwin, 1981. 
 
Public Works Policy
 
A government’s public works policy derives from the fact that certain economic goods and services are deemed
“public goods.” Public goods provide economic benefit to society and cannot be obtained in efficient quantities
through the regular workings of the profit-based free market. That being the case, they are difficult to price
according to the law of supply and demand. The most common example of a public good is national defense.
Although defense is considered essential to any country, it is not always clear when defense is needed nor how
much people are willing to pay for it. What is clear is that if left to the provision of markets, the quantity of national
defense provided would be too small and not be socially optimal. This is because markets provide goods and
services as long as private benefits to the individual exceed private costs. However, with national defense, the
social benefits (total benefits to society) are much greater than the private benefits. Also, there is a “free-rider
problem” since you cannot exclude others from being protected. If one group voluntarily pays for national defense,
even those who do not pay cannot be excluded from being protected. Hence, everyone has an incentive to let
others pay. Thus, national defense like other public goods, is deemed to be best provided by government.
Public works—usually large infrastructure projects such as roads, bridges, dams, power systems, and the like—
are generally viewed by economists as public goods as well. Perhaps the best example of a public works project
in the United States is the Interstate Highway System built in the 1950s and 1960s. The congressional act
authorizing the federal, as opposed to local, funding of roads is known as the National Interstate and Defense
Highways Act of 1956. The highway system was deemed necessary for national defense, to facilitate troop and
armament movements throughout the country during the cold war. In addition, it was recognized, an interstate
highway system would also provide important commercial benefits to the private sector, as it would enable the
producers and suppliers of goods to reduce transportation costs. In economics, this is known as a positive
“spillover effect” from the provision of a public good.

The federally financed Interstate Highway System of the 1950s and 1960s was the largest public works project in
history. The nationwide network, featuring innovative design and engineering, was conceived for both civil defense
and economic development. (Topical Press Agency/Stringer/Hulton Archive/Getty Images)
Absent the profit motive of a market that determines the supply and demand of a good or service, governments
need to plan for and budget for the provision of public goods. How public goods policy is devised, implemented,
and paid for naturally varies by country and political system. In the United States, for example, public works
planning is largely decentralized, carried out by cities and states across the country; funding may be provided
directly or indirectly by the federal government, which may also play a role in regulatory oversight. In Japan, by
comparison, the national government views public works as a part of national industrial policy and plays a much
larger, direct role in planning and development.
 Public Works, Natural Monopolies, and Government Control
Another economic rationale for collective action, whether by government or voluntary self-organization, is that
some economic goods are deemed to be “natural monopolies.” A natural monopoly is a project that requires large
up-front investment in physical capital, such as an electrical plant. It does not make economic sense to build more
than one plant in a given geographical location that could be served by one plant. Some economists believe that
the concept of natural monopoly provides the economic rationale for government to make policy for public works,
which usually entail large costs and long periods of time to build. The most commonly known public works
projects, in addition to roads, are canals, water and sewer systems, shipping ports, dams, airports, bridges, and
mass transportation systems. In addition to the large upfront costs of public works projects, infrastructure can also
be expensive to maintain over long periods of time. For this reason, public works policy is also part of the fiscal
policy-making process of the government authority at any level.
Economists, planners, public administrators, and politicians do not agree on an exact definition of public works.

Many economists believe that, if a natural monopoly can provide a revenue stream, then it is best for profit-making
entities to provide the good or service because this will create an incentive for efficiency. Moreover, some
economists point out, governments may have an incentive to overprovide public goods because doing so tends to
enhance their power, increase their budgets, and help garner political favor through targeted spending. Other
economists contend that it is best for government to control public works to exert quality control and to ensure that
the public interest is fully served. In the United States, at least, public works policy is generally driven by both
economic reasoning and political benefit, usually combining both private and public interests.
 Public Works and Regulation Economics
Because of the natural monopoly and the public goods nature inherent in public works (the positive spillover
effect), many economists agree that there is a role for government in ensuring that the public good is provided at a
level that ensures the greatest net societal benefit. Net societal benefits are total societal benefits less total costs.
The economic analysis of natural-monopoly implementation is called “regulation economics.” The purpose of such
analysis is to ensure that providers of public goods and natural monopolies charge a fair and reasonable price for
the good or service, meet the public demand at that price, and that the government regulatory framework provides
the right incentives to allow this to happen. Regulation economics seeks to find the best policy for regulating
natural monopolies.
The chief argument for private provision of public goods is that it put less of a strain on public finances. For
example, many states in the United States have privatized parts of their road systems, usually those that link
multiple population centers and are used by a large number of people. This ensures that only those using the
roads are actually paying for them, thereby generating revenue and reducing congestion. The private provision of
public works is usually implemented through concessions, whereby private companies bid for the right to operate
and maintain the public roads and highways for a period of time in return for an upfront fee or a percentage of the
profits. Because of the large sums of monies involved in many of these projects, privatization of roads is a
politically charged issue.
 Public Works and Economic Development
Some economists recommend the private provision of public goods in underdeveloped nations because the tax
base is not large enough to publicly finance the building and maintenance of public works. Yet public works with
positive spillovers are important for economic growth. For example, the government might grant a private company
the right to build and operate a public work (such as a shipping port, road system, or hydroelectric dam) that will,
through spillover, encourage and facilitate economic activity. Ownership of the public work may either be
transferred to the private operator or remain with the political authority. Thus, the success or failure of privately
constructed public goods depends on the strength of the political institution and the way the bid process and
private-public contractual agreement are structured. The overriding goal should be to provide the greatest public
benefit at the least cost. In this sense, public works policy should be part of a larger economic development policy
for a given political entity.
Other economists believe that anything that generates revenue and can exclude “free riding” is neither a public
good nor a public work and should not be provided by government. Examples of public goods that are natural
monopolies include public utilities like gas and electricity. The provision of electricity for homes and businesses
and gas for heating, cooking, and manufacturing necessitates extensive outlays of capital and requires that gas
lines and electrical cables be built throughout a given area. The construction, maintenance, and repair of this
infrastructure can be disruptive to the day-to-day lives of citizens, so it is important that they are regulated and
managed by public officials. Although there are exceptions, in general, public works such as water and sewage
systems are publicly owned and operated so as to ensure public health and safety. Regulatory economics
attempts to answer the trade-off between efficiency and competition versus ensuring adequate provision of
services with the least disruption of public life.

 Public Works and the Business Cycle
Beyond the basic need for infrastructure or to pursue a common purpose such as national defense, public works
policy may be influenced by the business cycle. That is, governments will sometimes engage in public works
projects during periods of economic contraction as a means to increase employment, income, and aggregate
demand. Perhaps the most well-known public works program of this type in U.S. history was the Works Progress
Administration (WPA), a centerpiece of President Franklin Roosevelt’s New Deal to combat the Great Depression.
During the lifespan of the WPA (1935–1943), 25 percent of all American families had a member of their family
employed by the agency. In 1938 alone, more than 3 million people were employed by the program. During the
eight years of its existence, the WPA employed 8 million people, who had 30 million dependents. With a budget of
approximately 6 percent of national income, the WPA built or repaired some 120,000 bridges, 80,000 miles
(128,800 kilometers) of city streets, 540,000 miles (869,045 kilometers) of rural highways, 25,000 miles (40,233
kilometers) of sewers, 1,100 water-treatment plants, 18,000 parks and recreational buildings, 16,000 athletic fields,
500 airports, 36,000 schools, and 6,000 administrative buildings.
Historians and economists have debated whether the WPA was essentially a public works agency or a relief
program. It was certainly a way to reduce the suffering of mass unemployment (which averaged 17 percent in the
United States from 1930 to 1940). Political scientists have also shown that WPA projects tended to be built in
areas where it was expected that votes would do President Roosevelt the most good in gaining reelection. The
Roosevelt administration did attempt to institutionalize public works policy into the federal government, with a
proposed cabinet-level department, but Congress rejected the idea.
Keynesian economists believe that government can and should use fiscal stimulus to create demand in an
economy during times of recessionary crisis and that public works programs are a good way to do this. The WPA
is often cited as a successful example of this approach, though some economists question whether the WPA was
actually a stimulus or only provided temporary employment. Indeed, it was not until the United States joined World
War II and geared up military manufacturing that unemployment fell below 10 percent.
In summary, public works policy makes up part of the fiscal, development, natural resources, public health, and
transportation policy of a given political entity. Public works, by their nature being public goods and natural
monopolies, require a give-and-take between the private sector and between differing layers of government within
a nation. How a public works policy is made depends on the type of political system and institutions involved.
Nevertheless, public works are an important and indispensable part of an economy.
Cameron M. Weber
 
See also:  Employment and Unemployment;  Fiscal Policy;  New Deal. 
Further Reading
Buchanan, James M., and Gordon Tullock. The Calculus of Consent: Logical Foundation of Constitutional Democracy. Ann
Arbor: University of Michigan Press, 1965. 
Button, Kenneth, Jonathan Gifford, and John Petersen.  “Public Works Policy and Outcomes in Japan and the United
States.” Public Works Management and Policy 7:2 (2002): 124–137. 
Darby, Michael R.  “Three-and-a-Half Million U.S. Employees Have Been Mislaid: Or, an Explanation of Unemployment,
1934–1941.” Journal of Political Economy 84:1 (1976): 1–16. 
Howard, Donald S. The WPA and Federal Relief Policy. New York: Russell Sage Foundation, 1943. 
Jasay, Anthony de. Social Contract, Free Ride: A Study of the Public-Goods Problem, New York: Oxford University
Press, 1989. 

Real Business Cycle Models
 
Real business cycle models, developed by Nobel Prize–winning economists Finn Kydland and Edward Prescott
beginning in the early 1980s, suggest that fluctuations in economic activity are caused by changes in technology.
These models focus on supply: as firms become more or less productive, economic activity rises or falls. The
models are an outgrowth of the new classical business cycle models first developed by Robert E. Lucas, Jr., as an
alternative to Keynesian models.
There are three important features in a real business cycle model. First, the term “business cycle” is redefined as
a set of statistical regularities and not as the rise and fall of gross domestic product (GDP). Second, real business
cycle models are equilibrium market clearing models. Both of these two features are common with the earlier new
classical models. The distinguishing feature of real business cycle models is that fluctuations are caused by
technological change. Since their inception, real business cycle models have become influential in professional
macroeconomic research, although they have had limited impact on forecasting and policy discussions.
 Defining the Cycle
Most people think of the business cycle as a recurring series of booms and recessions. In this view, the most
important things to understand about the cycle are what causes a recession to begin and how to prevent the
recession from lasting too long or becoming too deep. Recessions are typically viewed as a negative reaction of
the economy to an adverse shock.
In real business cycle models, the focus changes away from the ups and downs of GDP movements. In the mid-
1970s, Lucas argued that it was better to think about the business cycle as a set of empirical regularities. For
example, Lucas identified the important features of the cycle as including regularities such as the fact that
production of durable goods fluctuates more than production of nondurable goods, and both prices and short-term
interest rates fluctuate in the same direction as output, but long-term interest rates do not fluctuate as much. The
goal of a real business cycle model, like the goal of the new classical models that preceded them, is to explain
these regularities. In particular, real business cycle models try to replicate the real world’s changes in GDP, and
the relationship between consumption, investment, and GDP, and other empirical regularities, such as changes in
the number of hours worked. As a result, there is very little emphasis in research using real business cycle
models on identifying what caused particular historical episodes to evolve the way they did. Instead, real business
cycle models ask whether the model matches historical data.
 Equilibrium Market Clearing Models
The second important feature of real business cycle models is that the economy is always in its long-run
equilibrium. By contrast, in Keynesian business cycle theory the economy could be above or below the long-term
potential, full-employment GDP. The role of Keynesian business cycle theory was to study GDP’s fluctuations
around this potential.
Real business cycle models seek to abolish the distinction between the long-run growth model and the business
cycle model. Its proponents argue that there is no need for two separate theories to explain the features of the
macroeconomy. The business cycle is perfectly well explained by simply assuming the economy is always on its
long-run growth path. While there are no deviations from this path, the path itself is changing, sometimes rising
faster than other times.

The importance of this feature shows up clearly in discussions about the labor market. Keynesian business cycle
theory posited a natural level of unemployment. A recession is a time in which unemployment rises above its
natural rate and large numbers of people are “involuntarily unemployed.” Such people would like to work at the
going wage rate, but are unable to find jobs at that wage. Yet, despite this large number of people who would like
to work, the wage rate does not immediately fall to bring the economy back into equilibrium.
The new classical models in general, and real business cycle models in particular, abolish this notion of
involuntary unemployment. In these models, the labor market is always in equilibrium; if there are people who do
not have a job, it is because they have chosen not to work at the wage rates that are offered to them. It is, after
all, optimal not to take the first job available if you believe that by searching slightly longer you can find a job
paying a higher wage rate. Employment will thus rise and fall over time, but at any given time, everyone who
wants to work at the current wage will have a job, and anyone who does not want to work for that wage will not
have a job. There will be fluctuations in this employment rate over time, but there is no reason to classify people
who have chosen not to work as being either voluntarily or involuntarily unemployed.
 Cycles: Real, Not Monetary
The feature that distinguishes the real business cycle models from the earlier new classical models is the cause of
GDP fluctuations. Lucas’s earlier models were monetary models in which changes in the money supply caused
the cycle. Real business cycles are thus distinguished from monetary business cycles; the driving forces are real
things and not nominal things.
Real business cycle models argue that fluctuations in the rate of technology innovation are the cause of business
cycle phenomena. In effect, real business cycle models ask us to imagine that instead of technology growing at a
steady 3 percent per year, there is some fluctuation in that growth rate. That fluctuating growth rate is sufficient to
explain the fluctuations in the economy.
One can get a sense of how a real business cycle model works by thinking through an example. In the absence of
a change in technology, workers must choose how much to work. Because they value not only their consumption
but also their leisure time, working more results in more consumption but less leisure. Workers in the model
optimally choose how much to work based on their preferences for consumption and leisure—a worker who enjoys
leisure a great deal will work fewer hours (sacrificing consumption) than a worker who greatly enjoys
consumption. If nothing changes, then the workers will settle down to a fixed work schedule.
If technological growth suddenly increases this year, what will happen? First, and most obviously, every worker
will become more productive this year, and output will increase even if there is no change in labor input. However,
labor input will not remain constant. Each worker is now faced with an additional trade-off: the worker can choose
either to work a constant number of hours this month and next month, or, if paid by the piece, take advantage of
the higher productivity this month by working more now and less next month when productivity will not be as high.
The other side of the same story occurs when there is a negative shock to technology. In this month a worker is
less productive, and so the worker will optimally choose to work fewer hours, enjoying more leisure, and return to
work when productivity rises again. This idea that workers are enjoying leisure when there is a negative
technological shock strikes many as an oddity. Part of the oddity is resolved when we realize that leisure time can
also be productive. Consider, for example, a worker faced with three options: working for pay at the going wage
rate, working at home on projects around the house, or enjoying leisure. The pay rate for wage employment will
vary over time as technology changes; the productivity of work at home is constant. So, at times when there is a
negative technology shock, it becomes optimal for a worker to forgo the explicit wages of a paying job and instead
engage in productive activity at home. Again, when the rate of technology improves, the worker will optimally
choose to leave the productive activities at home and return to the labor force.
The effects of changing productivity are amplified in more complex real business cycle models by increasing the

set of decisions facing people in the model. For example, if a positive shock to technology this year indicates a
high probability that technology will also be relatively high next year, then firms will increase investment in order to
have a larger productive capacity next year when productivity will still be unusually high. This investment effect will
amplify the effect of both positive and negative changes in productivity. But, in the end, the root cause of
economic fluctuations is exactly the same as the root cause of economic growth. Over time, economies become
more productive, causing growth, but since the rate of productivity growth is not constant, in some years the
economy will grow faster than in others.
 Implications for Government Policy
Perhaps the most controversial conclusion from real business cycle theory is the implication for government policy.
Traditional Keynesian models of the cycle have a large scope for governments to engage in activities to cure the
problems of the cycle. In a real business cycle model, however, policy makers have no such role to play because
there is nothing to cure.
Consider a situation conventionally labeled a “recession.” In the Keynesian cycle, recessions are times when the
economy is producing below its capacity and thus the government can do things to return the economy to potential
output. But, in real business cycle theory, the economy is always producing at potential GDP. There is never a
gap between the current state of the economy and some imaginary potential state of the economy. Given the level
of productivity at the time, people in the economy are always optimally choosing how much to work, produce, and
consume. Since the people in the economy have already made optimal choices, there is no scope for the
government to improve upon the choices that people freely made.
This does not mean, however, that governments have no effect on the economy in a real business cycle model.
Government policies have a large effect on economic performance though their effect on productivity. For
example, a tax policy that provides disincentives to work will lower GDP. Governments should enact legislation
that enhances rather than limits the growth in productivity. It is important to note, however, that this important role
for government has absolutely nothing to do with the business cycle as conventionally defined. A tax policy with
negative effects on productivity has the same undesirable effect when productivity levels are high as it does when
they are low; thus an argument to alter such a tax policy has the same force no matter what the current state of
the economy happens to be. The same holds true for government policies that encourage monopolies:
governments should encourage competition and free trade to prevent the inherent inefficiencies of monopolies
from having a detrimental effect on economic development.
As an example of the direction that real business cycle theory takes, consider the Great Depression. Output fell
by almost one-third in the first few years of the Depression, and it took over a decade for output to return to the
level of 1929. The story that emerges in real business cycle theory is that there was a dramatic fall in productivity
starting around 1929, and then the amount of labor stayed low throughout the mid-1930s, making the recovery
very slow. Seen in this light, there are two separate problems to investigate. Why did productivity fall? This
question is still unanswered. Why did the amount of labor stay low? The New Deal programs effectively allowed
firms and labor unions to restrict competition, with the result that firms and unions together agreed to hire fewer
workers and pay them higher wages. In effect, the New Deal lowered the productive capacity of the country,
resulting in a smaller labor force and thus lower levels of output. Note that in this explanation the economy is
always at the best possible point, given the state of productivity. The government could have done nothing to
offset a large drop in productivity in the early 1930s, but the New Deal policies were in effect a second,
permanent negative shock to productivity.
 Impact
Since its origin in 1982, real business cycle theory has been one of several competing theoretical frameworks to
study the macroeconomy. Its impact was clearly demonstrated when Kydland and Prescott won the Nobel Prize in
2004. Much macroeconomic research since the early 1980s has been a response to the original Kydland and

Prescott models, with the profession quite divided over whether the models are a useful way of thinking about the
economy or fundamentally flawed. Critics have pointed to problems with many aspects of the model, from the
definition of the cycle to the equilibrium market clearing assumption to the importance of technology and
productivity shocks.
While the work has been quite important in the professional literature, it has had very little influence outside the
confines of theoretical research. The major forecasting models still rely on fundamentally Keynesian explanations
of the macroeconomy. And public debate on government policy in the midst of a recession is still framed in the
same terms debates were in the 1960s and 1970s.
James E. Hartley
 
See also:  Neoclassical Theories and Models;  Technological Innovation. 
Further Reading
Hartley, James E., Kevin D. Hoover, and Kevin D. Salyer, eds. Real Business Cycles: A Reader. London: Routledge, 1998. 
Kehoe, Timothy J., and Edward C. Prescott, eds. Great Depressions of the Twentieth Century. Minneapolis, MN: Federal
Reserve Bank of Minneapolis, 2007. 
Miller, Preston J., ed. The Rational Expectations Revolution: Readings from the Front Line. Cambridge, MA: The MIT
Press, 1994. 
Parente, Stephen L., and Edward C. Prescott. Barriers to Riches. Cambridge, MA: The MIT Press, 2000. 
Prescott, Edward C.  “The Transformation of Macroeconomic Policy and Research.” Nobel Prize Lecture, 2004. 
http://nobelprize.org/nobel_prizes/economics/laureates/2004/prescott-lecture.html
Snowden, Brian, and Howard R. Vane. Modern Macroeconomics: Its Origins, Developments and Current
State. Cheltenham, UK: Edward Elgar, 2005. 
Real-Estate Speculation
 
Real-estate appraisal, also called property valuation, is the estimation of the market value of a parcel of land, the
buildings on it, and any other improvements. The market value is the price at which rational investors might (or
do) buy or sell the property at a particular time. A rational buyer or seller is presumed to have access to all
available information on the property and to be capable of comparing market alternatives. By basing their decision
on full knowledge of the property, the marketplace, and broader economic conditions, buyers and sellers are
assumed to make—or be able to make—a mutually acceptable deal for a fair price. That determines market value.
Speculation, in general, is a decision to buy, sell, or hold a given real or financial asset or commodity, such as
real estate, stocks, bonds, and so forth, based on an assumption that the future value of the asset will increase
and a profit will be made.
Speculation is based on the expectation of investors regarding the risk of losses versus the potential gains of a
prospective investment. In other words, the speculator weighs the potential monetary penalty, and the odds of
incurring it against the potential rewards, and the odds of realizing them. The decision is an educated guess

based on perceived risk and necessarily incomplete information about the future (one can never know for certain
what will happen next week, next month, or next year).
 Risk and Reward
Real-estate speculation is based on the predicted value of a piece of property. The speculator buys, sells, or
holds property consistent with a basic assumption about the future. That is, the speculator estimates the expected
value of the property and compares it with the expect values of other real and financial assets, including the
following types of financial instruments:
1.   Securities representing the ownership of a transferable and negotiable monetary value. Securities take the
form of debt securities (bonds, banknotes, and debentures, i.e., loan agreements) or equity securities
(common stocks and equity shares of stocks, i.e., partnerships in corporations and common stocks);
2. Deposits, or the money put into a bank account;
3. Derivatives, or agreements such as forwards, futures, options, and swaps and securitizations. A forward is a
written agreement or contract to buy or sell an asset on a particular date in the future at a price determined
today. The value of a derivative contract originates from the worth of an underlying asset on which the
contract is based. The underlying asset may be bonds, foreign exchange, commodities, stocks, or a pool of
mortgages;
4. Goods or fungible commodities (such as wheat and currency);
5. Collectibles (precious items for particular collectors, such as art and antiques).
The rationale of real or financial speculations is not to use an asset for individual direct consumption or to make
an income from it through rental, augmented dividend, or interest, but to profit from the variations in buying and
selling prices. This is the fundamental distinction between speculation and investment. Investments are made to
use them personally (or for the direct utilization of the firm if the buyer is a corporation) or to generate individual
earnings (whether for a person or corporation). On the other hand, speculations are made to generate profits—
usually in the short term and at a good profit margin—by realizing the difference between the price paid to obtain
an asset (cost) and the sales price (revenue). Time is an important factor, as the speculator typically incurs
special costs and higher risk in the expectation of a quick, hefty profit.
Real-estate speculators look for properties to buy and sell relatively quickly at a profit. In order to be successful,
they must identify and take advantage of a disparity between the price the seller is asking and the price a
prospective buyer will be willing to pay. This may not simply be a case of the seller misjudging the market value
of a property. A real-estate developer, for example, might buy a property for a low price in a substandard area of
a city knowing that the municipal government plans to redevelop the neighborhood. In a few years time, the
developer could then sell the property at many times the price paid for it and realize a hefty profit.
In this respect, speculation can be compared to arbitrage in the financial world. In both cases, profits are earned
by taking advantage of price differentials in the marketplace. At the same time, there are major dissimilarities
between these two concepts. Arbitrage refers to profiteering from price differences in multiple markets, rather than
buying an asset in one market and selling it in the same market for a higher price at a later date. In arbitrage,
buying and selling occur almost simultaneously across markets and thus the arbitrageur makes a riskless profit by
buying in the low-price market and reselling (instantaneously) in a high-price market. Thus, arbitrage is viable only
with securities and financial assets, which—unlike real estate—can be traded electronically.
Basic on these characteristics, successful real-estate speculators generally adhere to the following criteria:

1.   Buy a property to hold for a period of time—albeit brief for the real-estate market—and sell it later at a
higher price;
2. Do not expect to earn a safe, steady income from speculation, as a long-term investor does;
3. Good timing is critical; buying and selling should not happen simultaneously, as in the case of arbitrage, but
should occur at the lows and highs, respectively, of the marketplace;
4. The forecast of future prices should be fully informed, based on as much information as possible about the
property, its perceived value in the marketplace, and the broader economic climate;
5. The reason for buying a property is purely to make a profit from fluctuations in (or misjudgments of) market
value.
Market expectations play a critical role in real-estate speculation in a larger sense as well, beyond the future
valuation of a particular property. Market expectations also help determine broader property cycles—recurring
highs and lows in buying and selling. Such oscillations take the form of increases and declines in prices,
vacancies, rentals, supply, and demand. For some speculators, the ongoing cycles of the marketplace provide a
valuable tool for predicting future trends—a method called “adaptive expectation.” In general, cycles in the
commercial world affect the entire business cycle of a nation. The nature and direction of the commercial cycle
also have a direct influence on property cycles.
 Speculator Expectations
How people form expectations about market value is therefore important in understanding real-estate speculation.
The academic literature describes several models. In the myopic (short-sighted) model, speculators lose clear
vision in formulating their expectations, have no reliable sense of what the future may bring, and succeed or fail
for reasons beyond their own doing. At the other end of the spectrum, the perfect-foresight model assumes that
people have a clear, rational view of the future and make faultless predictions when it comes to their investments.
The rational-expectations model assumes that people have access to and use all information necessary to make
an optimal decision for the future. (Accordingly, the rational-expectations model is less optimistic than the perfect-
foresight model.) Finally, according to the adaptive-expectations model, people look back to past events and
patterns to predict upcoming events, as in the prediction on inflation based on historical data.
Speculative bubbles are caused in part by price expectations based on past increases, as investors tend to
speculate that high prices will continue. In the real-estate market, such speculation often causes cycles without
any actual changes or new trends in demand and supply. Thus, people’s expectations—based on whatever
criteria, with or without a rational basis, with whatever the financial outcome—may be a cause or symptom of
property cycles.
Ulku Yuksel
 
See also:  Florida Real-Estate Boom (1920s);  Housing;  Housing Booms and Busts;  Mortgage-
Backed Securities;  Mortgage Lending Standards;  Mortgage Markets and Mortgage Rates; 
Mortgage, Subprime;  Recession and Financial Crisis (2007-). 
Further Reading
Hardouvelis, G.A.  “Evidence on Stock Market Speculative Bubbles: Japan, the United States, and Great Britain.” Federal
Reserve Bank of New York Quarterly Review 13:2 (1988): 4–16. 
Kim, K.H., and S.H. Suh.  “Speculation and House Price Bubbles in the Korean and Japanese Real Estate Markets.” Journal
of Real Estate Finance and Economics 6:1 (1993): 73–88. 

Malpezzi, S., and S.M. Wachter.  “The Role of Speculation in Real Estate Cycles.” Journal of Real Estate Literature 13:2
(2005): 143–166. 
Renaud, B.  “The 1985–1994 Global Real Estate Cycle: An Overview.” Journal of Real Estate Literature 5:1 (1997): 13–44. 
Wheaton, W.C.  “Real Estate ‘Cycles’: Some Fundamentals.” Real Estate Economics 27:2 (1999): 209–230. 
 
Recession and Financial Crisis (2007–2009)
 
The international financial meltdown that began in 2007 is considered one of the worst global economic crises
since the Great Depression of the 1930s—if not the very worst. Like that earlier catastrophe, it was many years in
the making.
In early October 2007, the Dow Jones Industrial Average (DJIA) reached an all-time high of over 14,000. By
March 2009, it had fallen to under 7,000. Other stock market indices worldwide, from Western Europe to the high-
growth economies of India and China, saw similar or greater drops. The triggering event was the collapse of the
housing market in the United States, which saw nearly $6 trillion in home valuation disappear—a far greater
amount than was lost in the U.S. stock market and a loss that affected a far larger cohort of Americans. The
dramatic change in financial fortunes triggered a series of spectacular failures in the financial services and banking
sectors, beginning with the March 2008 collapse of Bear Sterns, one of five major U.S. investment banks and a
leader in subprime mortgage lending. The venerable financial institution was sold in a rescue merger with
JPMorgan Chase engineered by the U.S. Federal Reserve; the final price was $10 per share of Bear Stearns
stock, which had traded in excess of $170 per share a mere fourteen months earlier. The collapse and fire sale of
Bear Stearns was followed in September by the failure of another major U.S. investment bank and leader in
subprime mortgage financing, Lehman Brothers. This time, no federal bailout or emergency acquisition was
arranged. The bankruptcy of Lehman Brothers was the largest in U.S. history, valued at over $600 billion.
Next came the September 2008 demise of Washington Mutual, the savings bank holding company, in what
constituted the largest bank collapse in U.S. history. The federal government placed Washington Mutual into
receivership under the Federal Deposit Insurance Corporation (FDIC), which in turn sold the company’s banking
subsidiaries to JPMorgan Chase for $1.9 billion. Then, during the rest of 2008 and into 2009, the federal
government stepped in to rescue several of the nation’s leading financial institutions with billions of dollars in loans
and capital injections; among recipients were the insurance giant AIG, Citicorp, and Bank of America. European
governments had to undertake similar actions.
 Foundations: The Housing Bubble
Despite its global reach and historically severe impact, the meltdown of 2007–2008 reflects the characteristics of a
classic boom-and-bust cycle, fed by excess credit. There was the development of a housing bubble beginning in
2003, its peak in August 2005, a steady decline in housing prices, and the inevitable collapse of the mortgage and

housing markets in 2007 and 2008—all with dramatic economic and political consequences. Indeed, any
reasonable analysis of the bubble in terms of classic boom-and-bust cycles should have raised early warnings
about the housing market and its financial underpinnings. Yet even the few analysts and commentators who
predicted the bursting of the housing bubble failed to predict the scope and scale of its financial and economic
effects. The constriction of credit markets left financial institutions, major corporations, small companies, and
private investors throughout the world unable borrow the funds necessary to conduct business, resulting in a deep
global recession of indeterminate duration.
The root of the crisis thus lay in the securitized mortgage market and its role in the housing bubble. These
processes took place in the context of the ongoing globalization movement and key changes in U.S. banking and
security law—all of which came together in a kind of “perfect storm” with sudden, unexpected, and devastating
force.
 Structure and Evolution of the U.S. Mortgage Market
U.S. residential mortgages represent a multi-trillion-dollar market that expanded dramatically between 2002 and
2007. In June 2007, U.S. residential and nonprofit mortgages totaled $10.143 trillion, up from $5.833 trillion in
September 2002 and $2.3 trillion in 1989. In other words, the market took thirteen years (1989–2002) to increase
$2.5 trillion but only another five years (2002–2007) to increase by $4.3 trillion—nearly double again. (In both
cases, the rate of increase far outstripped the growth of the general population.)
At the same time, the number of firms and organizations participating in the market also proliferated. Until the
latter part of the twentieth century, home loans and mortgages in America typically were arranged between a
bank or savings and loan (S&L) and a local borrower, with the bank or S&L holding the mortgage until maturity,
the sale of the home, or refinancing. But beginning in the 1980s and expanding in the 1990s and beyond, all that
changed. Banks and S&Ls discovered the benefits of what is known as “securitization.” Rather than holding loans
in their portfolios as investments, banks and S&Ls began bundling individual mortgages into larger loan packages
and selling them to outside investors. In addition to the sales proceeds, the banks and S&Ls obtained fees for
helping service the loans. Because the banks and S&Ls no longer had to wait years or decades to be repaid for
the loan, mortgage securitization meant that they could rapidly “turn over” their balance sheets; they were paid
immediately by parties investing in the bundled loan packages. The process brought increased returns on capital
and earnings per share, both for common stockholders and corporate officers with stock options.
 Expansion of the System
The new system was tailor-made to expand nationally, and then internationally, as it both benefited lending
institutions and facilitated homeownership. The securitization of mortgages created a circular and self-perpetuating
flow of money from banks to global-securities investors to mortgage issuers of various kinds to families seeking to
purchase a home. In the new system, banks began selling mortgage-backed securities to investors all over the
world such as mutual funds, hedge funds, pension funds, corporate treasuries, insurance companies, and banks.
The money invested in these securities ultimately flowed back to the banks, which originated more mortgages—
and the process repeated itself. Meanwhile, because the banks themselves no longer relied on direct repayment
from the mortgage holder for their returns, eligibility standards for prospective homeowners grew increasingly lax.
Credit standards to qualify for a mortgage were lowered, and new financial instruments—such as the subprime,
adjustable rate mortgage—were devised to expand the pool of borrowers, many of whom would not have qualified
for a loan in the past.
Moreover, as the market expanded, economies of scale in specialization emerged at different points in the
mortgage financing and investment chain. To begin with, the development of the Internet created significant cost
benefits in sourcing and processing mortgage applications and approvals online. Just as homebuyers could
virtually tour several houses in an afternoon without leaving home, they could also compare mortgage rates from
several sources. Lenders, for their part, could quickly scan the credit scores of prospective buyers. Similarly,

advances in computing and telecommunications created economies of scale in servicing mortgages and investors.
Under the new system, it became increasingly possible to sidestep restrictions imposed by government
regulations, as it became easier to avoid doing business with a federally insured bank or S&L. Thus, for example,
a mortgage broker (the intermediary between lender and borrower) could find a financial institution such as GMAC
or GE Credit Services or Merrill Lynch instead of a traditional bank or S&L to grant the mortgage. These lenders
would then bundle the mortgages they owned into “pools.” In turn, either they or investment banks such as Bear
Stearns or Lehman Brothers (the number one and two underwriters of mortgage-backed securities, respectively)
would sell the bundles to other investors, specifically tailored to their individual requirements.
For example, long-term investors might want a commitment to final monthly payments, while shorter-term investors
might want only the first three years of interest. No investor owned an entire mortgage, and none were involved in
loan administration or handling security. Again, powerful computer systems serviced and supported the deals and
structures in all their complexity. This gave an advantage to firms that could source and service in volume,
spreading system costs over a large number of mortgages, customers, and investments.
 The Bursting of the Bubble and Its Aftermath
The financial viability of the mortgage securitization process rested on two simple but essential requirements: (1)
homeowners needed to continue making their monthly payments; and (2) homes needed to retain their market
value. An increasing number of residential-housing loans, including subprime mortgages, were adjustable-rate
mortgages (ARMs), in which interest rates rise after a specified period of time; how much they increase is based
on a designated financial index. Any rise in interest rates pushes up monthly payments, sometimes beyond the
means of low-income families or those who had taken out mortgages that were beyond their means. As long as
home values continued to rise and credit remained loose, homeowners could refinance at lower rates. But with the
collapse in home prices and the tightening of the credit markets, this was no longer possible, leading to a rush—
then a flood—of foreclosures. With the peaking of the U.S. housing market in August 2005, the second
requirement (that homes needed to retain their market value) was cast into doubt. And with the increase in
subprime lending during 2005 and 2006—including so-called NINJA loans (no declared income, job, or assets
required)—the first requirement likewise was in jeopardy. Once problems started to emerge, the size of the
mortgage-financing market, its rapid growth, and its increasing complexity combined to burst the bubble. Losses
by financial institutions and investors quickly reached the tens of billions, then spread throughout the national and
international financial system.
Financial calamity is typically followed by political finger-pointing, moral grandstanding, and criminal prosecution as
the collapse exposes illegal schemes and unregulated practices that had gone unnoticed or simply ignored while
all parties were making money. Individuals and institutions seek restitution of lost billions, policy makers discuss
ways to prevent future abuses, and law-enforcement authorities seek to punish wrongdoers. Not surprisingly,
litigation tends to focus on the handing off of loans by the bank or other lending institutions, such as mortgage
brokers, to the mortgage providers. The reason is that these transactions entail the clearest documentation of
warranties and responsibilities of the various parties.
These contractual obligations then become the basis for recovery. Even with such documentation, however,
problems in achieving recovery have abounded. This is because the complexity of the system—and indeed the
uncoordinated and disorderly way in which it was operated—made it unclear who was supposed to be in
ownership and control of the loans, mortgages, and payment streams. Moreover, the system made it difficult either
for the banks servicing the loans to renegotiate terms with borrowers or for the government to buy and restructure
the various “toxic” assets.
 Boom-and-Bust Scenario
The pattern of the U.S. housing market followed the classic boom-and-bust model. Indeed, predicting the bust
and its consequences might not have been as difficult as many have claimed. In his book Subprime Mortgages

(2007), for example, former Federal Reserve Board governor Edward Gramlich did that exactly.
In the classic boom-and-bust scenario, every mania or bubble begins with a displacement or disruption to the
economy that changes existing expectations, such as a major technical advance like the Internet or a large
injection of government-printed cash into the economy. In the case of the global financial meltdown of 2007–2008,
the disruption was a vast increase in liquidity and lower interest rates injected into the system by the Federal
Reserve in the aftermath of the dot.com bust of 2001. The effects of that disruption, moreover, were compounded
by two other factors: increasing government deregulation of the financial services industry and the proliferation of
global telecommunications during the Internet boom. Government deregulation vastly increased the number of
players in the mortgage-backed securities market, while the Internet boom created tremendous low-cost
computing and communications power that facilitated and accelerated the credit expansion.
With these catalysts, financial activity and the value of assets grew in value according to the classic boom-and-
bust model. The more money that was made, the greater the speculation and the more assets appreciated.
Increasingly, investments were made—and profits earned—on the basis of greed rather than productivity or sound
business. The bubble continued to expand until the leverage fueling it could no longer support the growth and
expansion of the housing market. Mortgage-backed securities— themselves financial instruments without a sound
foundation— finally collapsed under the pressure.
 Role of Banks and Subprime Mortgages
Overly aggressive bank lending was another critical element in the boom-and-bust cycle, as it fueled the
expansion of the housing bubble in the early years of the decade, then accelerated the collapse of the market by
restricting credit to the point of choking it off entirely. Exacerbating the situation was the advent of the largely
unregulated market for credit default swaps (CDS). CDSs constitute a kind of insurance, or hedge, whereby
investors speculate on whether an investment instrument—such as a mortgage-backed security—will go into
default. If it does, the buyer receives a payoff. The premiums for such protection, of course, accrue to the seller.
Devised in the 1990s as a hedge against default and to spread the risk around, CDSs came to constitute a
windfall for major investment banks whose financial exposure was estimated in the hundreds of trillions of dollars
by the mid-2000s.
With some investors leveraged at a ratio of more than 40 to 1 (i.e., homeowners and investors in mortgage-
backed securities borrowing $40 for every dollar they actually had), any glitch in the housing market could set off
a rapid downward spiral in sales and price—much as happened with stocks in 1929.
With housing prices already rising faster than people’s incomes, lenders at first sought to continue expanding the
pool of borrowers with low-interest “teaser” loans. In 2005, however, as the Federal Reserve began to tighten
interest rates, adjustable mortgages had to be reset at the higher rates. Foreclosures began to increase as a
result, and prospective new homebuyers began to be priced out of the market. These developments in turn
prompted lenders and investors to reassess risk and to reduce new money, leading to a drop in residential real-
estate values—one of the two key supports for the mortgage-backed securities market. Forced to reassess the
value of their investments, heavily leveraged mortgage holders tried to convert to cash (sell), setting the stage for
a wide-scale panic. The flood of mortgage-backed assets coming onto the market combined with increasing fear,
decreasing demand, and lack of available credit to bring on the crash.
Historically, financial busts are often followed by a spate of litigation and revelations of scandal. The housing
subprime mortgage meltdown of 2007–2008 was no exception. Indeed, evidence of mortgage fraud in the United
States was on the rise during the boom phase of the cycle, as prospective borrowers were lured into commitments
they simply could not keep. Between 1997 and 2005, suspicious activity reports related to mortgage fraud
increased over 1,000 percent between 1997 and 2005. And the trend continued from 2002 to 2007, as pending
mortgage-fraud investigations by the Federal Bureau of Investigation (FBI) rose from 436 to 1,210. The most
common frauds involved “property flipping” and other schemes to gain proceeds from mortgages or property sales
based on misleading appraisals or other false documentation.

Meanwhile, the U.S. Securities and Exchange Commission (SEC) was looking into insider trading at public
companies with increasingly “toxic” assets associated with the mortgage crisis. And thousands of investors with
Wall Street financier Bernard Madoff, a former chairman of the NASDAQ stock exchange, came to learn that they
had squandered a collective $50 billion in Madoff’s giant Ponzi scheme, said to be the single biggest financial
fraud in U.S. history. Many regarded the Madoff case as symbol of the rampant fraud, lack of regulation, and
sheer greed of the financial times. Madoff, at age 71, was sentenced to 150 years in prison after pleading guilty to
multiple counts of defrauding clients.
Much as in other times of financial distress, political pressure began to mount for measures that would prevent
future abuses. Such was the case after the stock market crash of 1929, with the formation of the SEC. In 1989,
Congressional responses to the savings and loan crisis and the junk-bond scandal substantially increased the
penalties for crimes affecting financial institutions and tightened capitalization standards for banks and S&Ls.
These measures did not prevent or even moderate the 2007–2008 crisis and continuing recession, however, as
the federal government was assigned a role many felt it did not carry out. Under the 1989 legislation, the Federal
Reserve was granted the authority to curb lending practices for home mortgages. Thus, the boom and bust of the
housing market in the first decade of the twentieth century was regarded in many circles as a failure of federal
regulatory control. Meanwhile, lawsuits have flown in the wake of the financial crisis and the growing revelations of
misfeasance and even malfeasance by those institutions that marketed the debt-backed exotic securities. Most of
these lawsuits involve charges that the issuers of the securities misled investors as to the risk of those securities;
in some cases, plaintiffs are even demanding that the issuers buy back at least some of the tainted securities.
 From Financial Crisis to Recession
The burst of the housing bubble and the subsequent crisis in the international financial system reverberated
outward into the rest of the economy by late 2007, triggering what most analysts say is the worst downturn in the
U.S. and global economy since the Great Depression.
The financial crisis of 2007–2009 became a global economic contagion, resulting in bank closures, stock market
declines, business failures, and burgeoning unemployment. The recession proved deep, long, and widespread.
(Mark Ralston/AFP/Getty Images)
Collapsing housing prices had a broad dampening effect on consumer demand, which generates roughly two-
thirds of all economic activity in the industrialized economies of the West. During the housing boom from 2003 to

2006, many homeowners borrowed against the rising equity in their homes, either by refinancing their mortgages
and taking out a percentage in cash or by obtaining low-interest home-equity loans. In addition, with interest rates
on mortgages dropping, homeowners could reduce their monthly payments by refinancing, thereby freeing up a
greater part of their income for discretionary spending. Rising home values also provided an indirect stimulus to
consumer spending, as many homeowners came to regard their ever-increasing equity as a form of retirement
savings, cut the amount of income they saved—the savings rate in the United States fell almost to zero at the
height of the boom in 2006—and spent the rest on discretionary purchases. Thus, for example, the sale of new
vehicles remained above $17 million annually from 2004 to 2006 (and fell to just over $13 million in 2008).
All of this reversed with the decline in housing values. Consumers cut back on spending and began saving more,
both because they feared for their jobs and because they began to recognize that the equity in their homes might
not see them through retirement. The savings rate in the United States climbed back into positive territory in 2009,
to an annualized rate of 5.7 percent in April. With declining consumer demand came falling corporate revenues,
expanding inventories of manufactured goods, and a slowdown in industrial output. The national unemployment
rate skyrocketed from the historically low 5.1 percent in 2005 to the historically high 10.2 percent in October 2009,
which further depressed consumer demand.
In addition, the bursting of the housing bubble tightened credit markets across the board, not only in mortgages.
Traditionally, banks tend to tighten credit as their own portfolios of outstanding loans—including home mortgages
—show weakness. Thus, the growth of increasingly complex and exotic financial instruments such as mortgage-
backed securities and credit default swaps heightened insecurity in the financial markets. So complex were these
instruments, in fact, that nobody knew exactly what they were worth, especially as the security of the mortgages
on which they were based became suspect.
Modern credit markets were already highly fluid, with banks constantly providing financing to each other in the
interbank lending market. The 1999 repeal of that part of the 1933 Glass-Steagall Act that separated investment
banks from commercial banks meant that major banks of all kinds now faced greater exposure to the collapse of
the complex new financial instruments. Not knowing the value of assets on each others’ books, banks all but
stopped lending to each other in the late 2008. While a total freeze on interbank lending—which would have been
catastrophic—was avoided by the injection of hundreds of billions of dollars by various central banks around the
world, including the U.S. Federal Reserve, insecurity in the financial system led to a radical tightening of credit.
The effects on business were devastating. Companies were forced to defer the hiring of new employees (or even
fire existing ones), as well as the purchase of materials and equipment, which increased unemployment,
depressed demand even further, and decreased production. Altogether, the recession had a major impact on the
gross domestic products (GDP) of countries around the world—causing declines at an annualized rate of 15.2
percent in the first quarter of 2009 in Japan, of 9.8 percent in the United Kingdom, and of 5.5 percent in the
United States.
To help economies escape this vicious recessionary cycle, governments around the world responded in a variety
of ways. In the United States, the first major policy initiatives were the 2008 Temporary Asset Relief Program
(TARP) and the American Recovery and Reinvestment Act of 2009, better known as the Economic Stimulus
Package. TARP provided $700 billion of federal money to purchase or insure various mortgage-related securities
of questionable value or security held by financial institutions, in the hope that this would create a greater sense of
security in the credit markets and thereby facilitate lending. The stimulus package injected $787 billion directly into
the economy in a number of ways—including tax credits and reductions, increases in unemployment
compensation, money to state and local governments, increased federal government purchases, infrastructure
development, and research—all with the Keynesian goal of increasing employment and, hence, consumer
demand. At the same time, however, this spending raised the federal budget deficit and the national debt to a
percentage of GDP not seen since the end of World War II.
 Lessons

As for the subprime mortgage crisis that triggered the global financial meltdown and subsequent recession,
experts began to recognize signs of the bubble’s development as early as 2005. It was then, many analysts came
to believe, that the Federal Reserve should have used its regulatory powers and authority as a bank examiner to
impose stricter credit standards against such lending. If it had, it was suggested, market mania might have been
avoided and its effects significantly moderated, including the collapse of global stock markets.
So what should be done to avoid future bubbles and the devastating consequences of their bursting? According to
many economists, overreliance on laissez-faire (unregulated), market-based policies resulted in large asset
bubbles in the late 1980s and in 2007–2008. Thus, analysts and policy makers assert, the Federal Reserve and
U.S. Treasury need to develop regulations and policy responses to extreme asset inflation. The study of past
booms and busts is essential to understanding when a rapid rise in asset prices is a bubble, when that bubble is
likely to burst, and what to do in response.
William Rapp
 
See also:  AIG;  Asset-Price Bubble;  Bear Stearns;  Collateralized Debt Obligations; 
Collateralized Mortgage Obligations;  Credit Default Swaps;  Debt Instruments;  Fannie Mae
and Freddie Mac;  Financial Markets;  Housing Booms and Busts;  Lehman Brothers; 
Mortgage-Backed Securities;  Securitization;  Stimulus Package, U.S. (2008);  Stimulus
Package, U.S. (2009);  Systemic Financial Crises;  “Too Big to Fail”;  Troubled Asset Relief
Program (2008-);  Washington Mutual. 
Further Reading
Barth, James R. The Rise and Fall of the U.S. Mortgage and Credit Markets: A Comprehensive Analysis of the Market
Meltdown. Hoboken, NJ: John Wiley and Sons, 2009. 
Fox, Justin. The Myth of the Rational Market: A History of Risk, Reward, and Delusion on Wall Street. New
York: HarperBusiness, 2009. 
Gramlich, Edward, Subprime Mortgage Crisis. Washington, DC: Urban Institute, 2007. 
Kansas, Dave. The Wall Street Journal Guide to the End of Wall Street as We Know It: What You Need to Know About the
Greatest Financial Crisis of Our Time—and How to Survive It. New York: HarperBusiness, 2009. 
Kindleberger, Charles, and Robert Aliber. Manias, Panics and Crashes, Hoboken, NJ: John Wiley and Sons, 2005. 
Krugman, Paul. The Return of Depression Economics and the Crisis of 2008. New York: W.W. Norton, 2009. 
Posner, Richard A. A Failure of Capitalism: The Crisis of’08 and the Descent into Depression. Cambridge, MA: Harvard
University Press, 2009. 
 
Recession, Reagan (1981–1982)

 
The steepest economic recession since the Great Depression, the Reagan recession of 1981–1982—so
designated because it occurred during the first term of President Ronald Reagan—produced America’s highest
unemployment rates since the Great Depression. While a number of factors contributed to the downturn, most
economists cite a dramatic tightening of credit by the Federal Reserve, which was attempting to lower double-digit
inflation, as the immediate cause. After bottoming out in the last quarter of 1982, the nation’s economy began a
slow recovery, aided by deficit spending and lower interest rates.
The U.S. recession of 1981–1982 brought the nation’s highest unemployment rate—10.8 percent in December
1982—since the Great Depression. GDP shrank by 2.9 percent for the period. (Keystone/Stringer/Hulton
Archive/Getty Images)
 Troubled Economy of the 1970s
Although U.S. economic growth following World War II was sometimes slowed by recessions, far more serious
downturns began during the 1970s and early 1980s. A number of factors contributed to the troubles, including
increased competition from Western Europe and Japan, stagnating productivity, a destabilization of the global
financial markets as the U.S. dollar was devalued, and, most importantly, shocks to world oil markets that sent
crude prices from about $3 a barrel in 1972 to more than $40 a barrel in 1980 (from $14 to $100 a barrel in 2008
dollars). All of these factors produced an economic phenomenon that came to be called “stagflation”—a
combination of stagnation and inflation—in which high unemployment and slow economic growth (or outright
contraction) coincided with high inflation. Economists and the media also devised a new economic measure, the
“misery index,” to refer to the combined impact—and total percentages—of unemployment and inflation. By 1980,

the misery index had reached 21.
The combination of stagnation and inflation baffled economic policy makers, who had relied on Keynesian
countercyclical measures to maintain steady growth and to smooth out the business cycle for most of the postwar
era. According to early-twentieth-century British economist John Maynard Keynes, governments could ease
economies out of recession through a combination of monetary policies (lowering interest rates and/or increasing
the money supply) and fiscal policies (tax cuts and/or government spending), thereby increasing aggregate
demand. But such measures failed to lift the American economy out of the doldrums in the early 1980s, as
tightening credit threw the economy further into recession, while increased government spending and looser credit
fueled inflation; the latter reached double digits in 1974 and the three years from 1979 to 1981.
 Reaganomics
The economic woes of 1979 and 1980—along with foreign policy setbacks such as the Iranian hostage crisis and
the Soviet invasion of Afghanistan—led to a dramatic realignment in American politics, as voters ousted moderate
Democratic president Jimmy Carter in favor of conservative Republican Ronald Reagan. In Congress, Democrats
saw their majority in the House shrink and their control of the Senate disappear. Reagan had based his
successful campaign on a few simple messages: tax cuts, reducing government regulation and the growth in
government spending, and a tougher foreign policy stance. Once in office, he immediately enacted his first plank,
offering the largest tax cut in history (as a proportion of gross domestic product, or GDP), equivalent to about 3
percent; much of it went to the upper-income brackets. He also moved on his third promise, dramatically
increasing the defense budget and ratcheting up anti-communist rhetoric. Finally, in unilaterally firing and replacing
striking air traffic controllers, he sent a strong message that he would rein in the power of unions and their ability
to demand wage hikes.
The Reagan administration offered a number of explanations for these moves. First, it argued, giving tax cuts to
the wealthy would spur investment and entrepreneurial activity that would benefit all workers with more jobs and
higher wages, an argument that critics derided as “trickle-down economics.” The administration further contended
that lower taxes would actually increase government revenue, as the economic expansion they triggered would
create more taxable income and capital gains. The increased revenue, it went on, would help pay for moderate
growth in domestic spending as well as a dramatic increase in defense spending. (As for the latter, Reagan was
determined to drive the Soviet Union into bankruptcy by forcing it to spend a far higher percentage of its smaller
GDP to keep up with the United States on defense.) The entire plan was based on principles of “supply-side
economics,” a conservative school of thought which argued that the best way to achieve economic growth was by
providing incentives—such as cuts in the marginal income tax rates and taxes on capital gains—for those
supplying goods and services. This ran counter to long-standing Keynesian principles, which called for government
measures to increase aggregate demand—through tax cuts for lower-income workers and increased spending on
public works—as the key to economic growth in times of recession.
 Monetarists and the Federal Reserve
Even as President Reagan was coming to power, conservative monetarists led by Nobel Prize laureate Milton
Friedman of the University of Chicago were gaining the upper hand in policy-making circles. Monetarists, who
believed in keeping the growth of the money supply roughly equal to overall economic expansion, argued that the
crippling inflation of the late 1970s and early 1980s had resulted from excessively loose credit policies at the
Federal Reserve (Fed). According to the monetarists, the Fed had pumped too much money into a system that
was not producing enough goods and services to justify the increase, thereby fueling inflation. Indeed, they
argued, inflation had wormed its way into the psyche of American workers and consumers. Workers were
demanding wage hikes to keep up with rising prices, even though productivity did not justify them. Consumers
were spending more of their money out of fear that prices would continue to rise, and that goods and services
would cost more in the future. This built-in inflation, said some economists, was dragging down the American
economy by distorting the natural workings of the marketplace, making it difficult for companies to invest—and

consumers to spend—rationally. The only way to end the crippling cycle of inflation and slow growth, it was
argued, was to contract the money supply dramatically. Fed chairman Paul Volcker, once a believer in Keynesian
economics, had been converted to monetarist thinking as he witnessed the failure of Keynesian measures to lift
the American economy out of the stagflation of the 1970s.
Upon taking office, Volcker raised the federal funds rate (the interest rate the Fed charges member banks to
borrow money) from an already high 11.2 percent in 1979 to a peak 20 percent in June 1981. The increase did
what it was intended to do, as banks raised the prime interest rate to a high of more than 21 percent. Interest
rates at this level made it difficult for both businesses and consumers to borrow money, thereby reducing demand,
investment, and employment. As workers began losing their jobs, demands for wage increases eased; and with
less money chasing goods and services, prices increases slowed. Likewise, hikes in mortgage rates cooled an
overheated housing market. All in all, between 1980 and 1983, the annual inflation rate fell from 13.5 percent to
just above 3 percent. Other than in 1990, it would never rise above 5 percent again; inflation had indeed been
wrung out of the system.
The success came at a steep cost, however, as the Reagan recession brought some of the nation’s worst
economic data of the postwar era. In the first quarter of 1982, the U.S. economy shrunk at an annualized rate of
nearly 7 percent, the most severe contraction since the Great Depression—and a figure not even matched by the
financial crisis and recession of 2007–2009. The year 1982 witnessed a 2 percent decline in GDP. Late that year,
the unemployment rate hit 10.8 percent, an all-time high for the post–World War II era.
 Recovery
The U.S. economy was growing again by 1983, as Volcker and the Fed, convinced that inflation had been
checked, lowered the federal funds rate to just over 8 percent, about where it would remain until being lowered to
3–6 percent during the much milder recession of the early 1990s. (By comparison, the Fed effectively lowered the
rate to zero during the deep recession of 2007–2009.) But there were other factors behind the recovery.
Keynesian economists cited the massive deficit spending of the Reagan years, as the president found it much
easier to lower tax rates than he did to stop the growth in government spending. (His massive defense build-up
did not help ease the deficit either.) Monetarists, on the other hand, argued that reining in inflation was critical
because it made all economic players—from businesses to workers to consumers—act more rationally, ensuring
that economic resources were allocated more efficiently. Entrepreneurially focused economists, such as those of
the Austrian school, emphasized the introduction of new technologies that bolstered demand and improved
productivity—most notably, the personal computer.
Still other economists argued that the recovery from the Reagan recession was not particularly robust by postwar
standards. Whereas GDP grew by an average of 5 to 6 percent in the 1950s and 1960s, growth from the recovery
year of 1983 until the beginning of the next recession in 1991 averaged only about 3 percent—not much better
than during the nonrecession years of the 1970s. Nevertheless, Reagan was able to convince enough Americans
that he had revived the country’s economy to win reelection in a landslide in 1984 and to perpetuate the era of
conservative economic hegemony.
James Ciment
 
See also:  Monetary Policy;  Recession, Stagflation (1970s);  Volcker, Paul. 
Further Reading
Heilbroner, Robert, and Peter Bernstein. The Debt and the Deficit: False Alarms/Real Possibilities. New York: W.W.
Norton, 1989. 
Hurewitz, J.C., ed. Oil, the Arab-Israeli Dispute, and the Industrial World: Horizons of Crisis. Boulder, CO: Westview, 1976. 

Means, Gardner, ed. The Roots of Inflation: The International Crisis. New York: B. Franklin, 1975. 
Mehtabdin, Khalid R. Reaganomics: Successes and Failures. Lewiston, ME: E. Mellen Press, 1986. 
Niskanen, William A. Reaganomics: An Insider’s Account of the Policies and the People. New York: Oxford University
Press, 1988. 
Sawyer, James E. Why Reaganomics and Keynesian Economics Failed. New York: St. Martin’s, 1987. 
Wilber, Charles K., and Kenneth P. Jameson. Beyond Reaganomics: A Further Inquiry into the Poverty of Economics. Notre
Dame, IN: University of Notre Dame Press, 1990. 
Recession, Roosevelt (1937–1939)
 
Often referred to as the Roosevelt recession—after Franklin Roosevelt, the president in office when it occurred—
the sharp economic contraction from 1937 to 1939 prolonged the Great Depression and undermined support for
the poverty-fighting, government interventionist New Deal economic agenda.
The Roosevelt recession can be understood only in the larger context of the Great Depression. Following the
stock market crash of 1929, the U.S. economy was plunged into the worst downturn in its history. Bankruptcies
soared, unemployment rose to 25 percent, and corporate profits declined by nearly 90 percent. All told, the gross
national product (GNP) fell by one-third between 1929 and the depths of the Depression in early 1933. Herbert
Hoover, the Republican president at the time, hewed to the economic orthodoxy of his day: cut back on
government spending to keep it in line with the decline in government revenue. Federal deficits, it was believed,
only prolonged economic contractions by absorbing capital that private enterprise needed for investment and job
creation. Moreover, the semi-independent Federal Reserve tightened credit by cutting the money supply and
raising interest rates, under the orthodox economic view that the crisis of the 1930s was one of too much
industrial capacity rather than too little consumer demand. Raising interest rates would, by this reasoning, force
businesses to cut production, thereby raising prices and profits.
Many later economists would cite these moves by both the Hoover administration and the Federal Reserve as
grave mistakes, given the seriousness of the economic contraction. And at the time, emergent demand-oriented
economists such as Britain’s John Maynard Keynes insisted that, in the absence of private-enterprise initiative,
the government must step in to bolster demand, either through monetary or fiscal policy—that is, either by
increasing the money supply and lowering interest rates, or through tax cuts and public works projects that would
pump money directly into the economy.
Ironically, in his first campaign for the presidency, against Hoover, Roosevelt stuck to economic orthodoxy,
emphasizing the need for a balanced federal budget. Upon coming to office, however, Roosevelt abandoned his
conservative economic beliefs to focus on three problems: fixing the financial system, which was on the verge of
collapse; aiding the ailing agricultural sector; and reviving industry. He promptly sponsored legislation to guarantee
bank deposits and thereby end the run on withdrawals (the Banking Act of 1933, which created the Federal
Deposit Insurance Corporation, or FDIC); to subsidize farmers (Agricultural Adjustment Act); and to establish
codes to regulate industrial output (National Recovery Act). (The latter two would be overturned by the Supreme
Court in the mid-1930s.)
With the exception of the banking measure, these policies—collectively referred to as the First New Deal—were
aimed at lowering production so as to bolster prices and profits. As for pumping money directly into the economy

to increase employment and hence demand—as Keynes was coming to urge—Roosevelt was reluctant, insisting
that direct government relief should always be the last option. Nevertheless, as the Depression persisted through
the mid-1930s, and as the president faced increasing criticism from the political Left, Roosevelt expanded and
federalized public works spending through the Works Progress Administration (WPA) as part of what came to be
called the Second New Deal. Still, the wages offered on many of these projects were well below the subsistence
level as determined by federal economists. In retrospect, however valuable for the bridges and artwork they
created, these programs were far too limited in size to have a major impact on the national economy.
Roosevelt’s efforts through his first term had mixed results. The immediate financial crisis had been fixed—
depositor runs had largely come to an end, banking institutions were no longer failing in large numbers, and
corporate profits were reviving. Overall, GNP had grown by a very healthy 10 percent annually—albeit from a
depressed base—getting back to where it had been just before the stock market crash in 1929. Even the
unemployment rate had fallen from its peak, if still at an abnormally high 14 percent. Nevertheless, many
Americans, including Roosevelt and many members of Congress, were convinced the Depression was over.
Upon returning to office in 1937, Roosevelt reverted to his old conservative beliefs that fiscal deficits and massive
relief programs hampered economic growth. In this he was not alone. Thus, as the president slashed the federal
budget, Congress dramatically reduced funding for the WPA and the Federal Reserve, and, worried that the
recovery might fuel inflation, raised interest rates. These moves, along with new Social Security withholding taxes,
undermined investment and consumer demand, sending the economy into a steep recession. The stock market
collapsed again, and 10 million new workers became unemployed.
Once again, Roosevelt changed direction and began pumping new expenditures into the economy. It was too late
to avoid dramatic Democratic losses in the 1938 mid-term elections, and a more conservative Congress scaled
back many of the Second New Deal programs of the mid-1930s. Still, in the end, Keynes’s argument that massive
government spending was the only way to revive an economy driven down by lack of aggregate demand—an idea
that had slowly gained credence among Roosevelt’s economic advisers—was put into action not long after the
Roosevelt recession: the government began a program of massive defense spending leading up to America’s
entry into World War II at the end of 1941.
James Ciment
 
See also:  Great Depression (1929-1933);  New Deal. 
Further Reading
Barber, William J. Designs Within Disorder: Franklin Roosevelt, the Economists, and the Shaping of American Economic
Policy, 1933–1945. New York: Cambridge University Press, 1996. 
Leuchtenburg, William. Franklin Roosevelt and the New Deal. New York: Harper & Row, 1963. 
Rosen, Elliot A. Roosevelt, the Great Depression, and the Economics of Recovery. Charlottesville: University of Virginia
Press, 2005. 

 
Recession, Stagflation (1970s)
 
After a quarter-century of unprecedented growth and prosperity, the United States entered a period of economic
uncertainty, stagnation, and contraction in the early 1970s, as a series of recessions slowed GDP growth and led
to a virtual halt in per capita income increases. Marked by both high unemployment and inflation, the combination
of which was dubbed “stagflation” by economists and the media, the economic slowdown of the 1970s baffled the
Keynesian economic orthodoxy of the day and led to a reappraisal of the measures the government should take to
lift the economy out of recession.
 Post–World War II Boom
Along with most of the noncommunist industrialized world, the United States experienced sustained growth from
the late 1940s through the early 1970s, fueled by enormous surges in aggregate demand, rapid population
growth, dramatic improvements in productivity, increased exports, a lack of foreign competition in its huge
domestic market, a stable global monetary system, cheap energy, and—say the Keynesian economists who
advocated and orchestrated them—countercyclical fiscal and monetary economic policies that helped smooth the
normal upturns and downturns in the economy. Between 1946 and 1970, the U.S. gross national product (GNP)
nearly quadrupled, from about $200 billion to just under $1 trillion (in inflation-adjusted dollars). Per capita annual
income also soared during that period, from about $1,500 to nearly $5,000. Unemployment remained in the 4–6
percent range, except during a few brief and mild recessions, such as those in 1957–1958 and 1969–1970.
Inflation remained largely in check through the mid-1960s.
The 1960s marked the culmination of the postwar boom, with the U.S. economy boasting some of the fastest and
greatest gains of the era. Indeed, some 60 percent of America’s postwar GNP growth through 1970 occurred in
that last decade alone. Yet much of the superheated growth was the result of deficit spending by the federal
government. Rather than increase taxes to pay for these antipoverty programs and the war in Vietnam—and risk
undermining support for his domestic and military agenda—President Lyndon Johnson maintained the large tax
cuts he had signed into law in early 1964. All of the additional money floating around the economy fueled inflation,
which rose to nearly 6 percent in 1970. There were other negative economic factors in play as well, including
increased competition from Western Europe and Japan, declining improvements in productivity, and instability in
global financial markets. From the end of World War II through 1971, the U.S. dollar—the measuring stick for most
other world currencies—had anchored those markets. But with rising U.S. inflation and a growing trade deficit, the
dollar began losing ground, and made convertibility increasingly difficult to maintain. In August 1971, President
Richard Nixon devalued the dollar, even as he declared a temporary freeze on wages and prices.
 Oil Shocks
These efforts eased the twin crises of inflation and financial instability, but only temporarily. By February 1973,
Nixon had again devalued the dollar, and then came an even greater shock to the U.S. economy—the energy
crisis. The United States and the rest of the industrialized world had grown rich in the postwar era on cheap oil,
much of it imported from the politically volatile Middle East. In response to the Arab-Israeli War of 1973, Arab oil
exporters hiked crude prices and then imposed an embargo on the United States, resulting in shortages across
the country. More long-lasting was the embargo’s effect on prices, which quadrupled in just a few months from
about $3 to $12 dollars a barrel (about $14 to $58 in 2008 dollars). As oil ran much of the American economy, the
impact of the price hike reverberated through industry after industry and ultimately to consumer pocketbooks. By
1974, the rate of inflation had reached a crippling 11 percent. Rising oil prices also damaged the vital U.S. car
industry, as buyers turned to more fuel-efficient foreign imports. This had a significant cumulative effect, as the

automobile industry was among the largest purchasers of steel, rubber, glass, and tool-and-die products.
Productivity increases throughout the U.S. economy also sagged, to about 1 percent in the early 1970s and near
zero in the latter half of the decade. Stagnating productivity also meant stagnating wages and lost jobs. As the
recession deepened in 1974 and 1975, the nation’s unemployment rate climbed from 5 percent to nearly 8.5
percent.
The oil shortage of 1973–1974 led to economic stagnation throughout the industrialized West, ending the post–
World War II boom. In the United States, unemployment and inflation both soared—peaking at 9 and 11 percent,
respectively, before mid-decade. (Bill Pierce/Time & Life Pictures/Getty Images)
Normally, stagnating wages and high unemployment reduce aggregate demand, which in turns brings inflation into
check. But that was not the case this time. Rising oil prices, though critical, were not the only factor behind the
phenomenon of stagflation. Conservatives blamed excessive government regulation, which they said stifled
innovation and directly hurt the bottom lines of many businesses. Unionists blamed “unfair” foreign competition and
demanded higher tariffs. Some economists blamed too much deficit spending by the federal government; others
blamed excessive consumer spending, fueled by the mass introduction of credit cards and an inflationary cycle
that prompted consumers to spend their money quickly so as to beat higher prices later. President Gerald Ford
tried cajoling businesses, workers, and consumers to curb their inflationary habits voluntarily, but his Whip Inflation
Now (WIN) program was largely ridiculed. More effective was the Federal Reserve’s decision to raise interest
rates for banks, tightening credit. While this helped lower inflation somewhat, it sent the economy deeper into
recession.
Then came more blows on the energy front, as the Iranian Revolution of 1979 and the Iran-Iraq War, which began
a year later, sent oil prices soaring again, to $40 a barrel by 1980 ($100 in 2008 dollars). (Iran and Iraq were the
world’s second-and third-largest oil exporters, respectively.) Stagflation returned. Economists, politicians, and the
media began referring to the “discomfort” or “misery” index, which added together unemployment and inflation.
The index, which had hit 17.5 percent at the depths of the 1974–1975 recession, climbed to nearly 21 percent in
1980. Political repercussions were inevitable, as voters replaced the incumbent Democratic president, Jimmy
Carter, with Republican Ronald Reagan in the 1980 election, giving huge gains to Republicans in Congress as
well. More importantly, the election brought to power a president and, to some degree, a Congress no longer
enamored with the Keynesian countercyclical nostrums, whereby government dramatically increased spending
and/or the money supply to overcome economic downturns. That ideology, the new Reagan administration
contended, had led to stagflation by increasing the amount of money in circulation without improving productivity.

 New Policies
Reagan had campaigned, among other things, on a vow to fix the economy by dramatically lowering taxes,
reducing government spending, and eliminating unnecessary regulation. He was able to keep his first promise and,
to some degree, the third, though deregulation took some time to affect economic growth. Reagan proved
unsuccessful at reducing government spending, however, as his argument was that reducing taxes would spur
enough growth so that tax revenues would actually increase. Things did not work out that way, and the Reagan
administration, which spent heavily on defense, racked up the largest federal deficits in the nation’s history. The
budget deficits, along with other factors, helped produce a modestly growing economy through the middle and late
1980s.
Instead, what really ended the “stagflation” of the 1970s and early 1980s, according to most economists, were the
policies of the Federal Reserve Board (Fed) under Chairman Paul Volcker. Increasingly influenced by monetary
theorist Milton Friedman, Volcker—once a Keynesian—had come to the conclusion that lax monetary policy during
the 1960s and 1970s was the major cause of the economic crises of the latter decade. To end the inflationary
cycle, Volcker in 1981 rapidly raised the interest rates the Fed charged member banks, dramatically tightening
credit. The result was the deepest recession and the highest unemployment rate—peaking at 10.8 percent in late
1982—since the Great Depression. But the Fed’s move, aided by falling oil prices, finally did squeeze inflation out
of the system; the annualized rate fell from above 13.5 percent in 1980 to about 6 percent in 1982 and just above
3 percent in 1983. Stagflation had finally been put to rest.
James Ciment
 
See also:  Inflation;  Oil Shocks (1973-1974, 1979-1980);  Recession, Reagan (1981-1982). 
Further Reading
Barnet, Richard. The Lean Years: Politics in the Age of Scarcity, New York: Simon & Schuster, 1980. 
Bluestone, Barry, and Bennett Harrison. The Deindustrialization of America: Plant Closings, Community Abandonment, and
the Dismantling of Basic Industry. New York: Basic Books, 1982. 
Heilbroner, Robert, and Peter Bernstein. The Debt and the Deficit: False Alarms/Real Possibilities. New York: W.W.
Norton, 1989. 
Hurewitz, J.C., ed. Oil, the Arab-Israeli Dispute, and the Industrial World: Horizons of Crisis. Boulder, CO: Westview, 1976. 
Means, Gardner, ed. The Roots of Inflation: The International Crisis. New York: B. Franklin, 1975. 
Niskanen, William A. Reaganomics: An Insider’s Account of the Policies and the People. New York: Oxford University
Press, 1988. 
Refinancing
 
Refinancing is the process of renegotiating the terms and conditions of existing credit through new loan
arrangements. Technically, then, it is the restructuring or refunding of a loan with a new debt or equity in which
the contractual party renegotiates the sum or terms of the credit. In a vast number of cases, the client will require
the bank (creditor) to grant new credit (most likely of higher value) to retire the existing credit debt, using the same

or related collateral. Refinancing should leave the debtor in a more advantageous position, obtaining either better
terms of credit or additional sums of money to invest in a profitable activity that will pay off the debt more quickly.
The latter option is known as cash-out refinancing, as the debtor will take some equity out of an asset.
Refinancing operations usually are undertaken when interest rates are falling, as market conditions make it
possible to negotiate a lower interest rate than the currently held loan has. The very decision as to whether or not
one should consider refinancing—restructuring one’s debt—thus should be based on a detailed deliberation of the
time and savings involved.
In a refinancing transaction, the debtor usually requires the creditor to reconsider the sum owed (real refinancing)
or the conditions of the credit without touching the approved sum (quasi-refinancing). In either situation, the debtor
should be put in more favorable position, and the overall deal should be cheaper on a monthly/annual basis
compared to the previous loan arrangements. However, the refinancing process itself may entail significant one-
time fees, which the borrower should take into account as an initial, sunk cost of the renegotiated credit/loan.
 Mortgage and Financial Market Refinancing
Refinancing is usually linked with mortgages, as debtors remain in constant search of better deals that are
plentiful in highly competitive markets—-and most mortgages are made with no prepayment penalties to the
borrower. Typically, debtors switch to another mortgage provider who offers a better rate or another type of
mortgage, which, at least in the short term, lessens the burden of the mortgage repayment. Conventional wisdom
in the developed mortgage market holds that a debtor should consider refinancing if the interest rate drops two
points below the current mortgage rate, the debtor has paid at least two years of mortgage payments and plans to
live in the property for more than two years in the future. If that is the case, refinancing should be available
without much difficulty. However, during the easy-credit housing boom of 2003 to 2006, qualifications for
mortgages were significantly relaxed due to loose regulation and oversight. Thus, even in many cases where
these conditions were not met, refinancing applications were approved.
In the case of refinancing of nonmortgage debt instruments such as bonds in financial markets, the issuer may
decide to retire the existing issue of securities and offer new ones to the market, if the original instruments—that
is, the market where basic factors, such as labor, capital or raw materials are bought and sold—allow the issuer to
do so. The new securities may carry better terms or have lower interest rates. The total cash outlay needed to
exercise the call provision includes payment to the holder of the security for any interest that has accrued to the
date of the call and the call price, including the premium. The call premium compensates the bondholder for the
risk of financial damage or disruption associated with recall.
It is also possible to build in the condition of deferred call, whereby the securities’ issuer cannot exercise the call
option until a stipulated period of time lapses (usually five or ten years). However, even if the deferral is built in,
the issuer is always free to acquire the securities in the open market and annul them through market acquisition.
Nevertheless, one should be fully aware that the securities markets (both government and commercial) are usually
very thin, with only a small percentage of securities being traded. Thus, if the issuer is considering refinancing, it
will most likely be necessary to resort to the call. The process of refinancing in the financial markets is also known
as re-funding.
Refinancing in the financial markets requires an issuer to exercise the call option, which, as a rule, must be known
from the outset. This provision gives the borrower the right to retire outstanding bonds at a stipulated price, most
likely at a premium over face value, but never less than face value. A company may also opt for an open
tendering procedure, which allows bidding for the securities, in order to acquire and retire them. While in the case
of repurchase and call for retiring debt the price of the securities may be preset, in the case of tendering, the open
competitive price will be achieved on the market, reflecting more or less successfully the quality of the market and
market infrastructure.
Refinancing may also be used in the case of project and property (real-estate) financing, as the conditions of the

existing credit are modified. The loan is usually extended for an additional period of time, to enable the developer
to complete the project and realize a profit in the marketplace. In the case of companies as commercial issuers of
debt, financial restructuring in the form of refinancing may be undertaken when the price of ordinary shares (i.e.,
common stock) is at the level that would cause the firm to replace its outstanding debt with equity (shares of
stock).
 Corporate and Government Refinancing
Corporate and/or government rescheduling of debt—loan restructuring or refinancing—attracts the attention of the
general public, which would always like to give more consideration to the issues of government and large company
private debt. When the decision to restructure debt is based on reliable risk-assessment procedures and input
data, it is possible to gauge the impact of the restructuring on the overall business and future decisions regarding
the corporate or government debt. In principle, the refinancing financial institution should conduct the necessary
due diligence and ensure that the input data are accurate and in line with the annual report.
Although more rigorous refinancing criteria are often applied, common sense often prevails in banking allocation
decisions, and many of the operations sanctioned by senior management are not in line with the proclaimed risk-
management practices. In the late 1990s and early the following decade, especially in the United States and the
United Kingdom, refinancing increased the overall risk in the system to a degree that few recognized, ultimately
triggering the global financial crisis of 2008–2009.
Making a refinancing decision is particularly difficult for the refinancing agent (financial institution), which may base
its approval or rejection on the past performance of an applicant (historical information) rather than on the
applicant’s current economic position and/or potential. As economic theory has amply documented, highly indebted
customers are ready to accept virtually any interest rate and any condition in order to have their obligations
rescheduled (the principle of moral hazard). Although systemic and regular refinancing may provide a boost to the
financial system in the short run, it can accrue a dangerously high unsubstantiated debt over the long term and
undermine the stability of the entire financial system—precisely what happened after the U.S. housing bubble
began to burst in late 2006.
Thus, financial experts have maintained that it is necessary to regulate refinancing operations much more
vigorously and for lending institutions to manage risk assessment for longer-term stability and growth rather than
short-term profits (and the corporate bonuses that go with them).
Željko Šević
 
See also:  Corporate Finance;  Debt;  Mortgage Equity;  Mortgage Markets and Mortgage
Rates. 
Further Reading
Agarwal, S., J.C. Driscoll, and D.I. Laibson. Optimal Mortgage Refinancing: A Closed Form Solution.  NBER Working Paper
No. W13487 (October 2007). Boston: NBER. 
Arsan, N., and E. Poindexter.  “Revisiting the Economics of Mortgage Refinance.” Journal of Retail Banking 15:4 (Winter
1993–1994): 45–48. 
Caplin, A., C. Freeman, and J. Tracy.  “Collateral Damage: Refinancing Constraints and Regional Recessions.” Journal of
Money, Credit and Banking 29:4 (1997): 496–516. 

Regulation, Financial
 
The financial crisis of 2008–2009 brought the issue of financial regulation to the forefront of discussions involving
business cycles and economic policy. In particular, the consensus among economists and business analysts was
that the lack of such regulations—or at least their insufficiency—especially in the United States, was a primary
reason for the worst economic crisis since the Great Depression.
Financial regulation is a form of governmental supervision relating to financial institutions and financial markets.
Regulations are designed and implemented to ensure the integrity of financial systems and economies through the
enforcement of laws and rules pertaining to financial organizations and instruments. Although there are several
different types of regulations, involving different regulatory structures, financial regulations are usually seen as
promoting the stability of financial systems through their ability to monitor and control excessive risk-taking
behavior. Sound regulations ensure banking stability and promote fair practices to ensure an orderly functioning of
financial markets.
 Ensuring Financial Stability
There is no easy definition of financial stability. The literature, however, suggests that the conditions describing
financial stability are not met when central banks remain the only institutions in which people put their confidence
—in other words, when commercial banks and other credit institutions can no longer be relied on to avoid
excessive risk taking. Such situations reveal a state of financial crisis—the very opposite of financial stability.
Financial regulations are designed to prevent such crises from arising, as financial stability is closely related to
banking stability.
At the beginning of the twenty-first century, the global economy is experiencing recurring financial disorders,
ranging from financial crisis in emerging countries such as Turkey in 2000 and Argentina in 2001, to the bursting
of the so-called dot.com and housing bubbles and the discovery of major accounting frauds at major corporations,
such as Enron, Tyco International, and WorldCom. To prevent disorders of these kinds, financial regulations have
been designed to enhance the quality of information, to develop reporting mechanisms, and to promote healthier
banking structures. The Sarbanes-Oxley Act of 2002, one of the most notable recent efforts to regulate business
practices—especially in the area of accounting—was passed after revelations of unethical behavior by U.S.
corporations. Rules pertaining to asset allocation, deposit guarantees, and the maintenance of certain financial
ratios help protect the well-being of investors by shielding banking institutions against inordinate or inappropriate
risks.
Financial regulations can be issued by public or private bodies. One international regulatory institution, the Basel
Committee on Banking Supervision, was created in 1974 by the central bank governors of the Group of Ten
industrialized nations after two major international bank failures (Bankhaus Herstatt in West Germany and Franklin
National Bank in the United States). The commission issues supervisory standards and statements of best
practices in banking supervision to “enhance understanding of key supervisory issues and improve the quality of
banking supervision worldwide” and to promote common international understanding and agreement about
financial regulations. In 2004, the committee published an ambitious reform known as Basel II, whose purpose was
to create international standards to regulate banking institutions through a set of principles known as pillars. The
first pillar ensures that banks are well capitalized and that the risks they face are correctly qualified and identified.
This pillar distinguishes three types of risks: credit risks, operational risks, and market risks. The second pillar
provides another framework designed to help regulators to deal with residual risks (including, but not limited to,
systemic risk, liquidity risk, legal risk, and reputation risk). The third pillar calls for more extensive and transparent
disclosure regimes, allowing market participants to access information that alerts them to risks incurred when
pricing or dealing with a given institution.

 Regulatory Authorities
The idea of financial stability highlights the role played by regulatory bodies or authorities in governing financial
institutions and markets. Every jurisdiction has its own authorities, and, despite efforts to homogenize regulations,
the differences may be extensive. The three examples that follow reflect the different types of structures involved
in financial regulation.
In the United Kingdom, basic forms of financial regulation can be found as far as back as the thirteenth century.
The founding of the Bank of England, in 1694, provided an early model for central banks in other countries. Three
objectives were assigned to the Bank of England: monetary control, the placement of government debt, and
prudential control, the last being a key function to preventing financial crises. The Banking Act of 1979, amended
in 1987, formalized the nation’s system of financial regulation and supervision; it remains the prudential regulation
to the present day. Paralleling the regulation of credit, lending, and deposit activities, banking supervision and
investment-services regulation were merged into the Securities and Investment Board (SIB) in 1985, after a
decision made by the chancellor of the exchequer. A series of scandals culminating in the 1995 collapse of the
Barings Bank, the oldest merchant bank in the United Kingdom, led to the Financial Services and Markets Act of
2000, which defined the duties and responsibilities of the Financial Services Authority (FSA, formerly the SIB).
Four statutory objectives were assigned to the FSA: “maintaining confidence in the financial system,” “promoting
public understanding of the financial system,” “securing the appropriate degree of protection for consumers,” and
“reducing the extent to which it is possible for a business to be used for a purpose connected with financial
crime.”
In the United States, financial regulations are devised and enforced by several government bodies, of which the
most important are the Federal Reserve (Fed) and the Securities and Exchange Commission (SEC). The Fed
promotes the stability of the nation’s financial system, and the SEC oversees market practices. The National Bank
Act (1863) defined the duties and regulations applicable to national banks, federally chartered by the office of the
comptroller of the currency. Fifty years later, in 1913, the Federal Reserve Act created a central bank to oversee
the banking system, supplying liquidity in the event of crises like that of October 1907, when public panic
threatened the existence of numerous banks and trust companies. As a consequence of the 1929 stock market
crash, new regulations separating commercial banking from investment banking were issued, namely the Glass-
Steagall Banking Act of 1933 and the Securities and Exchange Act of 1934. The latter led to the creation of the
Securities and Exchange Commission, whose mission is to “protect investors, maintain fair, orderly and efficient
markets, and facilitate capital formation.”
In France, the Banking Commission (Commission Bancaire), created in 1800, is a regulatory body chaired by the
governor of the French Central Bank (Banque de France). Its mission is to ensure that credit institutions and
investment firms comply with laws and regulations; it has “the power to impose administrative penalties or financial
sanctions to offenders,” to “protect depositors,” and “ensure the profitability and financial stability” of the French
financial system. Authorizations to operate a financial company are guaranteed by a dedicated committee (Credit
Institutions and Investment Firms Committee), while financial-market regulation and surveillance are enforced by
the Autorité des Marchés Financiers. Insurance companies are, in turn, overseen by the Autorité de Contrôle des
Assurances et des Mutuelles (ACAM).
 Fair, Efficient, Transparent Markets
Regulatory bodies exist to promote financial stability; they also enforce market rules and ensure that financial
practices do not operate outside the law. The underlying assumption governing the design, implementation, and
deployment of financial regulations is that market competitiveness cannot be met without a common set of rules
applicable to all participants. Without this, the exchange of financial instruments cannot occur in a stabilized
environment where both buyers and sellers meet with equal knowledge about the financial instruments they are
willing to exchange and the market within which such exchange takes place.

Financial transactions should be fair, efficient, and transparent—three conditions that provide the foundation of
properly operating (i.e., competitive) markets. These principles were reaffirmed in 1998 by the International
Organization of Securities Commissions (IOSCO), an international body created in 1983 that adopted a formal set
of objectives and principles aimed at regulating activities relating to securities (revised in 2003). The document
underlines the need for an effective regulation of practices in financial markets, protecting investors from
“misleading, manipulative or fraudulent practices, including insider trading, front running or trading ahead of
customers, and misuse of client assets.” In addition, mechanisms of approval and accreditation, whether directed
at market participants or market structures (exchange and trading systems), and the timely and widespread
dissemination of relevant price information are promoted as part of IOSCO’s effort toward market efficiency.
Once universal principles have been developed and announced, they need to be applied within organizations. To
help the translation of principles into regulatory mechanisms, dedicated control functions undertaken by trained
personnel within financial organizations have therefore been created. Consequently, financial institutions are now
populated with risk managers, internal auditors, permanent controllers, anti–money laundering officers, and
compliance officers, to name a few. These functions and the people in control of them ensure that appropriate
controls are put in place, issues relating to conflicting areas are well addressed, and rules complied with. Beyond
principles, codes of conduct, and standards of good practice, compliance manuals, procedures, and routines are
designed and applied.
The financial regulatory landscape is not static. Changes occur on a regular basis; each new financial crisis (the
1997–1998 Asian crisis, the dot.com bubble of 1998–2000, and the subprime mortgage crisis that began in 2006,
for example) leads to shifts in regulatory models. Thus, the twenty-first century has seen the return of regulatory
standards restricting the power of institutions to rule themselves after three decades of deregulation. And
measures relating to best price and best execution rules, to the transparency of pre-trade and post-trade
information, to disclosures about inducements (rebates, payments for order flow) have become mandatory for
market intermediaries.
Financial regulations play an important role in the shaping of markets themselves. They have a great impact on
the ways people transact business in financial markets. In times of crisis, regulations (or a lack thereof) are often
cited as the cause of economic contraction. Not surprisingly, the financial crisis and global recession of 2008–2009
brought a hue and cry—and a substantive campaign—for renewed regulatory controls worldwide. Finally, it is
important that there be global coordination of regulations among the major world economies. Regulations force
institutions into behaviors that they would not engage in on their own and that reduce their profit. On their own,
many institutions would take more risk, particularly in a world with deposit insurance or where there is thought to
be a “lender of last resort” to bail out failed institutions. If regulations are not global—that is, if there is not a level
playing field—then it is too easy for financial-market participants to move financial transactions to locales where
regulation is less stringent.
Marc Lenglet
 
See also:  Banks, Central;  Federal Deposit Insurance Corporation;  Federal Reserve System; 
Financial Markets;  Glass-Steagall Act (1933);  Liberalization, Financial;  Securities and
Exchange Commission. 
Further Reading
Alexander, Kern, Rahul Dhumale, and John Eatwell. Global Governance of Financial Systems: The International Regulation
of Systemic Risk. New York: Oxford University Press, 2006. 
Basel Committee on Banking Supervision:  www.bis.org/bcbs
Gray, Joanna, and Jenny Hamilton. Implementing Financial Regulation: Theory and Practice. Hoboken, NJ: John Wiley and
Sons, 2006. 

Heffernan, Shelagh. Modern Banking in Theory and Practice. Hoboken, NJ: John Wiley and Sons, 1996. 
International Organization of Securities Commissions:  www.iosco.org
May, Wilfried A.  “Financial Regulation Abroad: The Contrasts with American Technique.” Journal of Political Economy 47:4
(1939): 457–496. 
Mitchener, Kris J.  “Supervision, Regulation, and Financial Instability: The Political Economy of Banking During the Great
Depression.” Journal of Economic History 63:2 (2003): 525–532. 
Power, Michael. Organized Uncertainty: Designing a World of Risk Management. New York: Oxford University Press, 2007. 
Singh, Dalvinder. Banking Regulation of UK and US Financial Markets. Farnham, UK: Ashgate, 2007. 
Spencer, Peter D. The Structure and Regulation of Financial Markets. New York: Oxford University Press, 2000. 
Resource Allocation
 
Every society must allocate its resources of land, labor, capital, and entrepreneurial ability to produce a chosen
array of goods and services that satisfy the needs of the society. Economic systems organized according to
capitalist principles allow markets and individual choices to determine the allocation of resources and the rewards
from economic activities. Alternatively, in “command” economic systems, governments determine what is
produced, how it is produced, and who gets what.
Most societies have evolved to embrace a mix of market and command features, such that governments play a
significant direct role in certain economic sectors, like defense manufacturing and public transportation, as well as
income redistribution. The latter includes income support programs for the disabled and elderly, public education,
and a variety of other public services, all financed through taxation.
Economic theory has evolved to explain the process of resource allocation and the concept of economic efficiency
in resource allocation. The goal of economic activity regardless of the system or ideology of a particular society is
to meet the needs of individuals and to match the additional benefits of various goods and services with the
additional costs of providing them. At the outset, this process should embrace basic productive efficiency, so that
goods and services are produced at minimum cost, and the broader concept of allocative efficiency, so that an
efficient mix of goods and services is delivered to individuals.
 Command Economies
Command economic systems would appear to offer significant advantages over capitalist systems to ensure
allocative efficiency. From a central authority, production methods can be researched and applied in a controlled,
integrated manner to ensure the most up-to-date production methods. Moreover, in a national emergency, the
central planning authority can immediately assess the capacity of the economy to produce vital goods and
services to meet specific national needs. At the same time, however, command systems are limited by the
enormous information costs involved in centrally planned production and distribution. If a modern industrial society
utilizes central planning and a command system in order to attain allocative efficiency, central planners need
extensive, detailed information on consumer tastes and production methods for each region of the economy. A
market economy leaves these decisions to market participants.
The eighteenth-century British economist Adam Smith was one of the first to observe that a capitalist system has

built-in advantages to achieve allocative efficiency, although he did not use this term explicitly. Resources in a
competitive market economy, he noted, are guided by an “invisible hand” that pushes the business community to
produce products and services to serve consumer needs at the lowest possible cost. The self-interest of market
participants—workers and business owners—to gain income acts in a way to maximize the return to society.
Workers select lines of work in occupations of highest demand, and business owners are led by market forces to
produce the products and services in highest demand at the lowest cost in order to maximize their own profit.
Economic theory has also identified the limitations of market systems, or the factors that lead a capitalist system
away from allocative efficiency. But the most important aspect of the market system in achieving allocative
efficiency is competition. With the discipline of competition, market agents acting in self-interest lead to efficiency-
oriented behavior. Success, in terms of income, is measured by the degree to which the agent best meets the
most important of society’s needs at lowest cost. Without competition in products or services, the market serves
self-interest rather than the needs of the society. A monopolist in the product or service market acts in a way that
maximizes his own income at the expense of consumers by charging higher prices, limiting output, and producing
inferior-quality goods.
Government acts in two primary ways to maintain competition. First, it monitors merger and acquisition activities of
firms, disallowing monopolies or oligopolies (where a few large firms dominate an industry), which adversely affect
competition. Second, governments take regulatory action in industries that are natural monopolies, such as a
public water, natural gas, or electric utilities. Natural monopolies occur in situations where there are gains from
size such that one large firm can produce the entire market quantity at a lower per-unit cost than if there are
several firms. Regulatory action attempts to introduce competitive discipline by monitoring price levels, service
quality, and profits to ensure that outcomes more closely resemble those of a competitive marketplace.
A monopolist in the factor market, called monopsony, has similar incentives in the absence of competition to
reduce the quantity of the factor provided to increase resource return, and has incentives to restrict entry into the
line of work. The factor market is where basic economic inputs—labor, capital, raw materials and so forth—are
bought and sold. Examples of monopsonies include labor unions in labor markets, and OPEC nations in the
market for crude oil.
Economic theory has also addressed other factors limiting the performance of market systems to achieve
allocative efficiency. The technical nature of the provision of some goods and services—a notable example is the
health care field—limits the ability of markets to perform efficiently. Some goods and services cannot be provided
to the exclusion of any consumers; these are called public goods and services. Examples include fire protection,
police protection, national defense, and the like. Consumers who may be excluded from the private market by not
paying may hide their true preferences in the hope of having others pay for the goods on their behalf. The
existence of “free riders” leads to underallocation of resources to the provision of such goods and services.
Governments in these cases intervene to provide these goods and services, financing them through taxation.
 Externalities
Market systems also do not perform efficiently in the presence of external effects in the production or consumption
of goods and services. Perhaps the most familiar situation of external effects, also called externalities or spillovers,
involves pollution. If a firm has access to natural resources as inputs or outputs in production, and degrades this
resource through air and water pollution, for example, the process may adversely affect third parties who are not
part of the production process and do not even purchase the goods. In this case, the market will over-allocate
resources to the polluting industry, as it does not include all the resource costs within its production decision-
making. Simultaneously, market forces will underallocate resources to the industry adversely affected by pollution,
since resource costs are increased by the amount of the pollution. Government action in these cases attempts to
control the pollution either directly, through regulation, or through government-instituted market mechanisms such
as effluent fees or tradable pollution permits that limit pollution to environmentally acceptable levels.
In some cases, the provision and consumption of goods and services can also involve positive externalities. The

most frequently cited example is that of education. The benefits of education, for example, accrue to individuals
and families as well to the community at large. The external benefits to the community include a more skilled,
educated workforce, a more discerning electorate, and a more active, aware member of the community. Education
tends to be underprovided in a market economy, however, as the total benefits are not captured by the individual
or family unit. Thus, public funding of education through tax support is an attempt to create an optimum level of
expenditure that reflects external benefits.
A lack of information, the cost of gathering information, or information imbalances among market participants pose
obstacles to allocative efficiency. Most countries have legal systems that protect consumers, instituting laws and
regulations to shield the public from hazardous products, unsafe food and medicine, and the like. In a completely
unregulated environment, producers could develop products that are potentially harmful to users, the costs of
which would not be included in the product cost. Consequently, the market system would overallocate resources
to this activity.
 Access to Information
On another level, the cost to an individual consumer of gathering essential information on products can be
significant. If governments conduct such analysis, there would be an advantage in terms of economies of scale,
and research results would allow consumers to make more informed product choices, which would enhance
allocative efficiency.
Information problems also arise when information is withheld, manipulated, or gained through unethical means.
Legislation related to fair trading and lending is intended to allow consumers to make more informed decisions,
leading to an improvement in allocative efficiency. In financial markets, stringent rules on the gathering and use of
information have been developed to ensure that all market participants have equal access to information.
Individuals who have access to “inside information” on the future value of a financial security, and act on that
information, can damage the credibility of the market for the general public, which does not have such special
access. In the extreme, without securities laws to punish this behavior, the market cannot function and the
potential for firms to raise funds and consumers to invest are eliminated.
The resource allocation process is central to any society in organizing the means of production and meeting the
needs of the general populace. The two opposite means of carrying out this process are a command system in
which there is planning by a central authority, and a decentralized system driven by markets. Most societies today
are based on some variation, or combination, of the two extremes. The mixed economies of the Western world
have evolved policies, laws, regulations, and public services to address social issues and directly intervene to
address failures in resource allocation. The role of central planning in communist countries, meanwhile, has varied
greatly. The Chinese model is based on rigid central control of social and political life, with the marketplace
allowed to operate much more freely in the economic realm. In North Korea and Cuba, by contrast, strong central
planning rules every aspect of life and society.
Derek Bjonback
 
See also:  Classical Theories and Models;  Financial Markets. 
Further Reading
Blanchard, Olivier. Macroeconomics.  5th ed. Upper Saddle River NJ: Prentice Hall, 2009. 
Blaug, Mark. Economic Theory in Retrospect. London: Cambridge University Press, 1978. 
Samuelson, Paul A., and William D. Nordhaus. Economics. Boston: McGraw-Hill/Irwin, 2005. 

 
Retail and Wholesale Trade
 
Retail and wholesale trade are two components of an economic process in which goods are transferred from
manufacturers to consumers in exchange for money. The wholesale trade represents the intermediate step in
which goods are sold by manufacturers to persons or companies that are retailers of various kinds, usually in large
quantities and at below retail prices. In fact, there may be more than one step in wholesaling, as manufactured
goods are often sold to distributors, who then sell them to retailers. Retailers, then, are the individuals and
businesses that sell the goods to the ultimate consumers of those goods, be they individuals, businesses, or the
government. In certain cases, the wholesaler and/or retailer may be bypassed as manufacturers sell directly to
retailers and/or final consumers. Given that consumer demand is responsible for roughly two-thirds or more of all
economic activity in industrialized countries, such as the United States, the retail and wholesale trades can both
affect and be affected by the business cycle in a variety ways.
 Wholesale Trade
Goods produced by manufactures need to be brought to market and made available for consumption.
Wholesalers, such as the U.S. firm Sysco for food products, essentially move goods from producers to market.
Wholesaling usually implies the purchase of goods from producers or other suppliers and their storage in
warehouses, from which they are then made available for resale to companies that either intend to resell them
directly to consumers or use them for their own operations. Wholesale trade usually includes establishments that
sell products to retailers, contractors, and industrial, institutional, and other commercial users. Wholesalers,
because of the intermediate position they occupy in the production-distribution chain, are able to compete with
direct sales by manufacturers to retailers through economies of scale and scope in transport as well as in stock
holding.
Generally speaking, two main categories of wholesalers can be distinguished: merchant wholesalers on the one
hand, such as Michigan-based Apco for electronics or the Georgia-based S.P. Richards Company for furniture,
and wholesale electronic markets, agents, and brokers, such as Washington state–based importers.com for a
variety of industries on the other. The former purchase the merchandise they sell, taking full ownership of it.
Within the general category of merchant wholesalers, a further distinction can be drawn between those that
provide full service and those that provide only limited service. Full-service merchant wholesalers are differentiated
by the larger volume of sales and the broader array of services they can offer. Limited-service wholesalers handle
relatively smaller sales volumes and offer fewer services. Among the different kinds of limited-service merchant
wholesalers are cash-and-carry wholesalers, jobbers (also known as truck wholesalers), and drop shippers,
among others. Jobbers are wholesalers who transport and sell products, especially food, directly from their
vehicles. Drop shippers are wholesalers who do not handle the merchandise they sell, but remit orders directly to
producers. Their presence is common in bulk industries such as coal.
Unlike merchant wholesalers, wholesale electronic markets, agents, and brokers do not acquire ownership of the

products they sell. Using the Internet or other electronic means, wholesale electronic markets put purchasers and
sellers in touch with each other, usually for a commission. Brokers are marketing intermediaries common in
certain sectors, such as food, insurance, and real estate. They often represent different producers of noncompeting
goods and are paid on a commission basis. As for agents, three types can be differentiated: purchasing agents,
manufacturers’ agents, and sales agents. Purchasing agents generally perform the following functions: receipt,
storage, and shipment of products to buyers. Manufacturers’ agents are contractors who work on commission
basis and handle the sale of products for two or more producers. In fact, manufacturers’ agents frequently
represent different companies that offer compatible but noncompeting goods. This system offers the practical
advantage of limiting the costs of sale by spreading them across the different products. Manufacturers’ agents are
extensively employed by companies that lack the necessary financial resources to establish their own sales team.
In general, manufacturers’ agents can work quite independently, as they are not under the direct supervision of the
manufacturer. Unlike manufacturers’ agents, who do not handle the producer’s entire output, sales agents have
the contractual power to sell the entire output of a particular manufacturer. Working with relative autonomy, they
can set prices and determine conditions of sale.
Wholesaling can also be conducted by producers themselves without hiring the services of independent
wholesalers. This is made possible through manufacturers’ sales branches.
 Retail Trade
The direct sale of products to end users can be conducted by a wide variety of economic operators. Of the many
kinds of retailers that make products available for consumption and handle the final sale, independent retailers are
by far the largest group. They account for the most significant portion of the total volume of business done by
retail stores. An independent retailer is different from other types of retailers, such as chain stores, in several key
respects. Generally speaking, an independent retailer is a small enterprise owned and run by individual proprietors
or smaller partnerships. If not a family-run business, it tends to have the characteristics of one. Notable among
these is the capacity to establish close relations with customers. In small shops, for instance, the owner knows
many customers personally, remembers their tastes, and caters specifically to their needs. The close relationship
between retailer and customers is a great competitive advantage against larger, more impersonal businesses. For
many, in fact, it is crucial for the survival of the business at a time when local markets are increasingly dominated
by a few large retailer companies. Independent retailers are often located in or close to residential areas. Being
small-sized operators, they do not employ a specialized professional staff, with expertise in window display and
advertising, for example. Owner-operators typically handle such functions themselves.
At the other end of the retail spectrum are chain stores, such as Target (general merchandise), Staples (office
supplies), or Bed Bath & Beyond (housewares). A chain-store system is a group of at least four, but usually more,
stores with common ownership and central management. The chain store is a comparatively recent form of
retailing, emerging in the latter part of the nineteenth century and gaining prominence over the course of the
twentieth century.
Retailing is also conducted through department stores. The department store is a retail establishment that sells a
wide range of different products, such as clothes, furniture, kitchen and bath items, and appliances, among others.
In the early part of the twentieth century, such establishments were located in big cities, often in the central
downtown shopping area. In the decades following World War II, such stores were increasingly located in
suburban shopping areas, especially malls.
In the next evolutionary phase in retail distribution, the shopping mall appeared in the post–World War II era as a
confluence of social factors drew city dwellers to the suburbs. Shopping malls offer consumers the chance to shop
for a large variety of different products in the same place. In these establishments, in fact, there is a high
concentration of stores that sell a wide range of goods. Shopping malls soon became a consumer mainstay and
social attraction, opening in urban areas as well as the suburbs and exurbs.
Other common forms of retailing include discount stores (Costco), consumer cooperatives (often locally run, one-

off establishments), and franchise organizations (best known in the food industry). Discount stores are
establishments that sell merchandise at a discounted price, below the producer’s list price, limiting their operating
costs by offering fewer services and minimal customer assistance. Outlet malls, which have become increasingly
popular in recent years, are home to several or many stores. By and large, they are located away from large cities
and specialize in selling products—usually clothes and accessories—of well-known brands.
Consumer cooperatives (co-ops) are retail establishments created, owned, and operated by consumers for their
own benefit. Membership is open and profits are shared among the members in the form of refunds for their
purchases. Consumer cooperatives offer a convenient alternative to consumers who are not fully satisfied with the
prices and services offered by traditional retailers in the area where they live.
Franchise organizations have grown remarkably in recent decades and have become a common means of retail
distribution. A franchise organization is based on a specific type of relationship between a producer (or wholesaler
or service provider) and an independent entrepreneur, regulated by a special contract. Under the franchise
agreement, the producer (franchiser) and the independent entrepreneur (franchisee) stipulate terms by which the
latter purchases the right to own and run a number of units (retail outlets) in the franchise system. One of the
defining characteristics of franchise organizations is that they handle a unique product or service.
Another way of retailing products to consumers is through merchandising conglomerates, which derive from the
combination of different lines that share the same ownership. They have an integrated distribution and
management system.
With consumer sales down during times of recession, retailers find new ways to attract customers. Here, a woman
shops at a grocery outlet—also referred to as a “surplus” or “salvage” grocer—where overstocked and out-of-date
food is sold at a deep discount. (William Thomas Cain/Stringer/Getty Images)
Since the 1980s, retailing has become one of the fast-growing and most dynamic economic sectors in America,
boosted in no small measure by the opening of new channels on the Internet. Many retailers, new and old, have
successfully exploited the possibilities offered by electronic trade by operating Web sites where one can buy online
the same products as in a “real” shop—generally for much lower prices because the seller carries no store
overhead. The growth of electronic commerce and changes in the pattern of consumption in developed countries
have made online shopping a new, appealing, and profitable way of retailing.
With consumer spending accounting for more than two of every three dollars of economic activity in the United
States and other industrialized nations, the retail sector deeply affects the economic cycle. Retail and wholesale
sales are affected by both season (retail sales spike before Christmas while wholesale numbers spike well in

advance of the holidays) and by the business cycle and, in turn, have a major impact on the latter. During
economic downturns—marked by declines in aggregate demand—retailers see sales drop off, leading them to cut
orders to wholesalers and manufacturers. Drops in orders produce buildups in inventory, leading manufacturers to
cut back on production in order to reduce those inventories. Drops in manufacturing activity lead to higher
unemployment, less investment, and slower overall economic growth, or even decline. Such a slackening of
economic growth further reduces demand; if demand falls off enough, a recession results. Ultimately, when
inventories fall enough, businesses gear up production and hire workers, which stimulates demand and can lift the
economy out of recession and back into growth.
At the same time, wholesale and retail sales represent a key indicator of economic growth or contraction. Retail
and wholesale numbers are considered two of the most important economic indicators, and the monthly wholesale
and retail trade reports put out by the U.S. Census Bureau are two of the sets of numbers most closely watched
and analyzed by economists. If retail and wholesale numbers pick up, then it is likely that manufacturing will as
well, as inventories diminish and manufacturers move to take advantage of increased consumer demand.
 Financial Crisis and Recession of 2007–2009
The financial crisis and recession of 2007 to 2009 provides an example of how wholesale and retail sales are
affected by the larger economy and, at the same time, drive larger economic trends. In January 2007—about eight
months before the recession began in the United States—monthly retail sales stood at about $363 billion. (Note:
The U.S. Census Bureau lumps together retail and food service data.) By January 2008, as the recession was
just beginning to set in, the figure was $376 billion. While this represented a $13 billion increase in retail sales,
the 3.6 percent increase pales in comparison to more prosperous years, such as January 2005 to January 2006,
when the number climbed from about $330 billion to $358 billion, an 8.5 percent increase. By January 2009, at the
tail end of the recession, the retail figures were well into negative territory. Between January 2008 and January
2009, retail sales had fallen from $376 billion to $340 billion, a 6.9 percent drop, slightly larger than the annualized
gross domestic product contraction of 6.1 percent for the economy as a whole. By the second quarter of 2009,
however, government analysts were declaring the recession over, despite very high unemployment numbers. One
of the key factors behind this declaration was retail numbers. By January 2010, retail sales had climbed to $356
billion, an increase of 4.7 percent, though the figure was still below the number for January 2008. By October
2011, retail sales had climbed to $398 billion, a further hike of 12 percent, indicating gradually growing consumer
confidence.
While standard economic factors—such as slackening production and investment and rising unemployment—had
an impact on retail sales, economists also point to the financial crisis and housing-price crash as critical in
undermining retail demand. With house prices rising in the mid-2000s, many people began to spend more and
save less. The rising equity in their homes convinced many that they needed to save less of that equity for
retirement purposes. In addition, many people had taken out lines of credit against their rising home equity and
used the cash to pay for home improvements—a key retail sector—and other consumer goods. Many were able
to do this because interest rates were so low and lending standards had become lax. As housing prices began to
fall, the financial industry began to tighten credit, making it more difficult for people to borrow and spend.
Meanwhile, diminishing home equity forced many people to think again about their savings activity, sending
savings up and retail spending numbers down. And, as noted above, slackening consumer demand reinforced
weakening trends in the U.S. economy, helping to push the country into recession.
Antonella Viola and James Ciment
 
See also:  Circuit City Stores;  Consumption;  Inventory Investment;  Linens’n Things. 
Further Reading

Adcock, Dennis, Al Halborg, and Caroline Ross. Marketing Principles and Practice. Upper Saddle River, NJ: Pearson
Education, 2001. 
Alexander, Nicholas. International Retailing. New York: Oxford University Press, 2009. 
Darnay, Arsen J., and Joyce Piwowarski, eds. Wholesale and Retail Trade USA: Industry Analyses, Statistics, and Leading
Organizations. Detroit: Gale, 2000. 
Rosenbloom, Bert, ed. Wholesale Distribution Channels: New Insights and Perspectives. New York: Haworth, 1993. 
Sandhusen, Richard L. Marketing. New York: Barron’s Educational Series, 2008. 
Seth, Andrew, and Geoffrey Randall. The Grocers: The Rise and Rise of the Supermarket Chains. Dover, NH: Kogan
Page, 1999. 
Retirement Instruments
 
Between its inception in September 2007 and its low point in March 2009, the U.S. financial crisis cost an
estimated $3.8 trillion in lost retirement savings for the American people, reducing the average retirement account
by 43 percent. Although a portion of those losses was recouped in the market turnaround of succeeding months,
the episode underscored the rising vulnerability of America’s elderly in the current retirement system. Instead of
providing economic security, changes in the nation’s retirement and pension plans over time have increased the
exposure of the older population to financial volatility. In addition, defined-benefit plans, in which employers
guaranteed a specific monthly payment upon retirement, have been increasingly replaced in recent decades by
defined-contribution plans, in which employers and employees put set amounts into retirement accounts each
month. In other words, the retiree does not receive a fixed payment every month, but draws from a portfolio of
investments that could be subject to market volatility if it consists of corporate securities and other volatile financial
instruments.
 Public Pensions and Defined Benefit Plans
Currently there are three major plans that provide retirement income in the United States: Social Security,
employment-based plans (including defined-benefit and defined-contribution retirement plans), and non-
employment-based savings and investments (including home equity, though the latter has been diminished
somewhat by a slide in housing prices since 2007). Social Security was enacted in 1935 as part of President
Franklin D. Roosevelt’s New Deal policies to reverse and offset the effects of the Great Depression. Officially part
of the Old-Age, Survivors, and Disability Insurance (OASDI) program, Social Security was expanded in the
following decades to include the Medicare program and coverage for people with disabilities and their dependents.
Its main purpose was to fight poverty among the elderly. It was, and still is, based on the principle of taking in
contributions from active workers and paying out to those who are eligible to receive the retirement benefit. Since
1935, Social Security has been a major source of retirement income for the majority of Americans, constituting
about 36 percent of the aggregate income of persons over the age of 65. In the early twenty-first century there
has been an effort by some to change or privatize the Social Security system based on ideological opposition to
public programs or the argument that the fund is not solvent. Others, however, claim that it is solvent and
financially healthy enough to pay out to retirees until 2041 and to cover most payments beyond that date, as long
as the Treasury Department pays back its debt to the Social Security program. The ideological (conservative
versus liberal) controversy surrounding this issue continues into the second decade of the century.

Historically, employment-based retirement programs existed alongside the Social Security program. Until the late
1960s, defined-benefit pension plans were a prominent supplement to Social Security for many employees. Under
these plans, the employer guaranteed a certain retirement income calculated using a formula based on years of
service and a percentage of pay. The plans had to be fully funded even if the employee was not yet fully vested
in the plan—that is, before the employee had worked long enough to be eligible to collect from the plan. In
addition, the funds accumulated were not transferable—the employee could not take them along after changing
jobs. In December 1963, the collapse of the automaker Studebaker left thousands of workers without a pension.
The Studebaker event brought to public attention other episodes of poorly financed pension plans and complex
requirements regarding vesting. In response to the public outrage, Congress drafted and passed the Employee
Retirement Income Security Act (ERISA), which President Gerald Ford signed into law on September 2, 1974.
 Defined Contribution Plans
ERISA established a set of regulations concerning the operation of pension plans. Through Keogh plans, it also
expanded pension opportunities for the self-employed, who were not covered by Individual Retirement Accounts
(IRAs). Although ERISA was proposed with good intentions, it took away some of the flexibility enjoyed by
employers and employees in the pre-ERISA period. In addition, changes in tax codes regarding employee benefits
raised new questions about the funding of defined-retirement benefits. Not surprisingly, in the 1970s defined-
contribution plans started becoming more popular among employers, and they became a more common way for
employees to fund their retirement.
Defined-contribution plans allocate the employer contribution to individual-employee retirement accounts according
to a predetermined formula. There are a variety of such plans, each offering the benefit of tax deferment. That is,
the contribution the employee makes is deducted from his or her gross income at the time the payment is made,
with the tax paid upon retirement. The deferral of the tax payment is beneficial because it represents a smaller
amount of money due to inflation. Upon retirement, the employer collects the account contributions and the
returns to the account accumulated over time. In most cases the employer (called the sponsor) also contributes a
percentage of the employee’s salary to the retirement plan.
To expedite the management of the plans (and to distance itself from the entire retirement process) the sponsor
chooses an administrator—for example, a mutual fund or insurance company, or a brokerage firm. The
administrator offers a menu of assets the employee can choose from for investing. The menu consists of a
diversified list of mutual funds, stocks, and bonds, options, money market securities, and so on. A mutual fund is
a composite financial instrument where a large number of investors pool their contributions and invest in a mix of
stocks, bonds, money market funds, and other securities. The value of the investment pool fluctuates in the
market, and therefore there is no guaranteed return. A stock is a share of ownership in a company. If the
company does well, the price of the share in the market rises; the reverse is true if it falls. A bond is a financial
instrument that facilitates borrowing by companies with the promise that the principle and the predetermined
interest will be paid at the end of a fixed term. Like stocks, bonds are traded and their price fluctuates. Money
market securities are one of the more liquid and low-risk retirement instruments, and consequently have a
relatively low return. They are IOUs issued by corporations, financial institutions, and government agencies with a
short maturity.
The investment menu offered by mutual funds provides the opportunity to invest in equity funds, balanced funds,
bond funds, and money market funds. Equity funds are invested in corporate stocks. If the stocks are of large
corporations (large-cap), they have a relatively low risk. If the stocks are, on the other hand, of small and new
companies (small-cap), the risk is higher. Some of the fund investments may be value investments in companies
that are considered undervalued. Others may be growth investments in companies with a promise of future profit
growth. The return on investment in equity funds depends on the income from stocks and bonds and on the
growth of the value of stocks and bonds. A balanced fund is composed of stocks and bonds in variable
proportions to appeal to investors with different risk preferences. They are relatively low-risk. Bond funds are low-
to moderate-risk. The lowest-risk bonds are those backed by the federal government. Money market funds are

also low-risk and offer only interest income. These funds invest predominantly in short-term securities, such as
certificates of deposit (CDs) and U.S. Treasury bills.
Money-purchase pension plans are another financial retirement instrument in which the employer and employee
make predetermined contributions based on a percentage of the employee’s annual compensation. The sum is
invested into mutual or other funds and is subject to vesting requirements. Returns are not guaranteed; they
depend on market fluctuations.
Deferred profit-sharing plans are deferred plans (usually supplemental) whereby the employees get a share of the
company’s profits. These shares can be paid to the employees in the form of cash or stocks, or put into a
deferred plan. The rules for these plans regarding tax treatment, vesting, employee eligibility, and funding are
complex and are defined in ERISA in detail.
Employee stock ownership plans (ESOPs) are qualified defined-contribution retirement plans whereby the
employee receives shares in company stock, either through a stock-option plan or a company 401(k) plan. As a
result of stock ownership, the employee is vested in the company’s success. Thus, it is sometimes argued, these
kinds of plans have significant positive productivity effects. There are two kinds of stock-ownership plans. The
leveraged-ownership plan allows the company to raise its capital by borrowing from bank and nonbank financial
institutions to buy stocks. In the basic-ownership plan, on the other hand, the employer directly contributes tax-
free cash or stock shares. ESOPs are more suitable for relatively large companies, and can be costly for smaller
ones.
Stock-bonus plans are defined-contribution retirement plans under which the employer shares a portion of
company profits with the employee in the form of stock options. Contributions are discretionary, so the employer
may choose not to contribute in a given year. Employee performance is rewarded under such a plan, and giving
employees a stake in the company can encourage efficiency and productivity. This plan is ideal for newer
businesses with unstable profit patterns.
A simplified employee pension plan (SEP) is an easy way for an employer to help its employees begin saving for
retirement. The employee establishes a SEP IRA into which tax-deductible contributions are made by the
employer. The contribution limit is 25 percent of the participant’s total compensation. This type of plan does not
have the considerable amount of paperwork and compliance requirements of a regular retirement plan.
Individual retirement accounts (IRAs), as mentioned above, are tax-deferred retirement plans into which retirement
contributions are made by those who are not covered by any other plan.
 Shift to 401(k)s
In the course of recent decades, a number of large corporations (with the consent of the majority of employees)
have replaced their defined pension plans with 401(k)s. In the 1990s especially, when the stock market was
performing well, more and more defined-benefit plans were converted into defined-contribution plans. Since then,
the 1978 Revenue Act has been amended numerous times to regulate diversification, maximum compensation,
and defined and elective contribution limits, as well as to increase corporate responsibility and accountability
(Sarbanes-Oxley Act of 2002). More recently, Roth 401(k)s have become increasingly popular among investors.
Roth 401(k)s are different than other 401(k)s in that the contributions are taxed but the withdrawals are not,
allowing the contributions to grow tax-free. Another difference between the two types is that the Roth 401(k) has
no minimum withdrawal requirement at the age seventy-and-a-half. Since 401(k)s are tax-deferred, the Internal
Revenue Service (IRS) requires that at, age seventy-and-a-half, a retiree starts withdrawing from the retirement
savings according to a predetermined formula so that the IRS can start taxing the withdrawal and avoid the
situation of not receiving any tax payments on deferred retirement savings.
Tax-deferred 403(b)s are plans for the employees of nonprofit organizations, such as churches and hospitals, and
of public educational institutions. Similarly, 457s are for state and municipal employees, and thrift saving plans

(TSPs) are for federal employees (including members of armed forces, public health service, and other
government bodies).
IRAs, mentioned above, are available to individuals whose employers do not offer pension plans and whose
adjusted gross income is below a specified level. They are tax-deferred plans until withdrawal, at which point
proceeds are taxed as income. They also have contribution limits specified by law.
A different retirement instrument offered by insurance companies is the deferred annuity. Deferred annuities differ
from aforementioned instruments in that they provide a guaranteed income stream in retirement (lump sum or
incremental), the amount of which depends on the contributions one makes. Deferred annuities appeal to persons
who prefer a guaranteed income in retirement as well as those who have reached their contribution limits to IRAs.
Because of their high fees, however, they are expensive and carry restricted investment choices.
There are three major categories of annuities. Fixed annuities are relatively low-risk and have a minimum
guaranteed return; they also carry a guarantee against losses by the insurance company that offers the annuity. In
the case of variable annuity, contributions are not part of the insurance company’s assets and there is no
guaranteed return. The return and the risk of the annuity are determined by the market performance and
composition of the securities in the fund. A third category is the so-called equity-index annuity, which bases
returns on a specific market index, such as the Standard & Poor’s 500, with a guaranteed minimum interest. Thus,
one enjoys market gains but is protected against market losses.
In each of these deferred arrangements, the employer provides the employee with a list of different savings or
“investment” options, usually in the form of mutual funds. In some cases, the employer also contributes a
percentage of the employee’s salary to the fund. The implicit assumption in this process is that each employee is
a well-informed, rational decision maker regarding the maximization of the future retirement income stream. The
employee is also expected to assess the potential risks and benefits associated with various investment
opportunities, such as mutual funds based on different mixtures of domestic and international bonds, stocks, and
money market securities. Even though the selection process is facilitated by the mutual fund companies with
expert advice and retirement calculators, the wide variety of available funds presents a challenge for employees of
different ages, risk preferences, retirement incomes, and retirement age goals. Nevertheless, as long as the
markets perform well, defined-contribution plans are a rewarding source for retirement savings. Until 2007, they
indeed provided a significantly higher return than traditional savings instruments such as CDs and bank savings
accounts. As the markets started to melt down in mid-2007, however, people belatedly realized the extent of their
exposure to market volatility. In fact, even before the crisis set in, in spite of the market gains in the 1990s and
2003–2006, the average American worker without a defined-benefit retirement plan had not accumulated enough
savings to guarantee a decent retirement income. Half of the private-sector employees who contributed to a
401(k) had an average of only $25,000 in their account in 2006.
The complicated retirement financing system in the United States raises fundamental questions regarding
effectiveness. Many people consider the system both insufficient and insecure. Consequently, a new initiative has
been launched to create a mandatory, universal, secure retirement system, in addition to Social Security, under
the auspices of the Economic Policy Institute (EPI) think tank and the Service Employees International Union
(SEIU).
The recession and economic crisis of 2007–2009 highlighted the vulnerability of the retirement system in the
United States. The decline in securities valuations caused significant declines in the retirement portfolios of
millions of Americans. The impact was less severe for younger employees, who had smaller portfolios and a much
longer time period in which to recoup their losses. But for retirees and those nearing retirement age, the losses
can have a devastating effect, forcing many back to work—if they can find a job at all—to supplement depleted
reserves and forcing those approaching retirement to put off the day on which they can stop working.
Mehmet Odekon

 
See also:  Mortgage, Reverse;  Savings and Investment;  Tax Policy. 
Further Reading
Center for Economic and Policy Research. Slow-Motion Recession. Washington, DC: CEPR, 2008. 
Economic Policy Institute (EPI).  2009. Principles for a New Retirement System. Washington, DC: EPI, 2009. 
Employee Benefit Research Institute.  “How Much Have American Workers Saved For Retirement?” Fast Facts #19.
Washington, DC: EBRI, 2009. 
Risk and Uncertainty
 
Risk is defined in economic terms as the possibility of suffering financial loss, and uncertainty as the state of not
knowing whether one will experience gain or loss in the future. In economics, two general rules apply when it
comes to risk—most people are risk averse; and the greater the risk, the greater the return. Much economic
activity revolves around ways individuals and businesses seek to minimize risk for a given return, or maximize
return for a given level of risk. In general, activities that reduce risk for a given return enhance general economic
welfare.
Risk and uncertainty are, of course, inherent elements of life. We do not know, for example, what nature will send
our way. In California, an earthquake can bring a homeowner’s most important possession crashing to the ground;
in Florida, an untimely frost can destroy an orange grower’s carefully tended crop. In a sense, capitalist economics
mimics nature, as market forces often resemble in their complexity and unpredictability the workings of geology or
climate.
 Risk Aversion
By nature, most people are risk averse. That is to say, gains and losses being equal, most people experience a
greater degree of displeasure from loss than pleasure from gain. To take a simple example, say a person has
$5,000 safely deposited in a federally insured bank account. The risk of keeping it there is virtually zero. But let’s
say someone comes along and offers that person the chance of doubling their $5,000 on the flip of a coin.
Because there is an equal chance of doubling or losing one’s money, economists say the bet has an “expected
value” of zero. That being the case, such a bet can be considered fair. Nevertheless, most people would decline
the offer. That is because the loss of the $5,000 could put the person in the street, unable to pay her rent, while
the gain of $5,000 will just buy some luxuries or provide a degree of future security. Neither is as strongly positive
as imminent homelessness is negative. In a sense, then, risk aversion is a subset of that bedrock principle of
modern economic thought—the diminishing marginal utility of income. That is to say, the utility gained from the
extra amount of money one gets from winning the coin toss is less than the utility lost from losing it.
While humans may be risk averse, they still have to live with risk and uncertainty. Thus, most people work hard to
avoid or, at least, minimize risk. For instance, some people may choose career paths that have a higher
probability of reward than those that do not, even though the latter may be more personally satisfying. And people
tend to drive to work even though that is a riskier choice than walking. The marginal utility of driving is so much
greater in terms of speed that most people are willing to accept the dangers to life, limb, and fortune of driving a
car. Or at some point, most people will buy a home, even though they can never be sure if fire will destroy their

investment or a downturn in the market will diminish its value.
Businesses, too, must exist in a world of uncertainty and risk, but with a difference. While individuals generally do
not seek out economic risk, businesses must, if they expect to prosper or even survive. Retailers stock up for a
busy Christmas season knowing full well that a retail slump could leave them with unsold inventory. A mining
company invests huge amounts of capital in a new dig not knowing what the future holds. The mine may not bring
in enough ore to pay back the investment, or demand for the metal may slump, or political instability in the host
country may close down production.
 Risk Minimization and Avoidance
There are two ways individuals and businesses can help to reduce or minimize risk for a given return. One is
through information gathering and assimilation. Economic theory, of course, operates on the assumption that
people and businesses act rationally on all available information. At the same time, risk and uncertainty imply that
present information may be superseded by future, contradictory information.
Markets offer another way for people and businesses to avoid or minimize risk—risk sharing. The most common
example of this is insurance. Most homeowners and businesses, for example, take out fire insurance. This works
both for those taking out the policies and for those offering them. For the policyholder, it offers the peace of mind
and certainty that the economic consequences of an unpredictable event—a fire or flood, for example—is
minimized or eliminated. The marginal utility of protection from catastrophic loss outweighs the costs of monthly
premium payments. But while fire or flooding is unpredictable for an individual or business, it is much more
predictable for a given population. Thus, the insurance company can expect to make more money from the
thousands of premiums than it does from paying the costs of dozens of fires or floods—that is, if it has the right
information, in the form of actuarial tables, at its disposal.
On a larger scale, many governments work to reduce uncertainty and risk for individuals by insisting that their
citizens buy into social insurance schemes. By forcing people to pay into a retirement plan, such as America’s
Social Security system, governments ensure that people will have some income when they are too old to work.
On the other hand, if governments attempt to intercede to reduce market risks for businesses, they may in fact,
through a phenomenon known as moral hazard, have the opposite effect. For example, critics of the U.S.
government’s 2008 bailout of major financial institutions argued that the payouts would induce riskier behavior in
the future because the people making the investment decisions for those institutions may come to believe that
they will not suffer the full economic consequences should those decisions in the future turn out to be bad ones.
Insurance, however, is only the most obvious market mechanism for risk minimization and avoidance. Speculation
is another. At first glance, speculation appears to increase risk—one is buying something on the expectation that
its value will go up, a highly uncertain and risky act. But speculation can also offer certainty. A farmer, for
example, may sell his crop to a speculator for a particular price even before it is reaped, guaranteeing the farmer
a predictable income against volatile market forces. Speculation, then, differs from gambling. While the latter offers
no real social utility—other than the fleeting pleasure a gambler takes from the game—speculation does offer
social utility.
Risk minimization can also be achieved through diversity of investment. As all financial advisers suggest,
individuals and businesses should never place all of their capital in a single investment or even a single type of
investment. By diversifying a portfolio with corporate securities, government bonds, real estate, money market
accounts, and other investments, one minimizes the dangers of the boom-bust cycle, as many of these
investments tend to perform differently in given market conditions. Moreover, each offers different levels of return,
usually based on the level of risk. As noted earlier, with greater risk comes greater return.
The corporate structure and the joint venture provide other ways for individuals and businesses to share and, thus,
minimize risk. Corporations offer risk minimization in two ways—one legal and one economic. The corporate form
reduces the liability of the investor to the amount invested. Thus, if a corporation goes deeply into debt and

becomes insolvent, the investor is not in danger of losing his or her personal fortune. Economically, corporations
allow individuals and businesses to invest in a venture collectively, thereby spreading the risk. Similarly, joint
ventures between businesses allow them to share the costs of large projects. Of course, the same relationship of
risk and return applies in these cases as well. Two companies that choose to share the costs of a project on a
fifty-fifty basis, thus reducing potential losses by 50 percent, also choose to forego 50 percent of the returns.
 Recent Developments
While risk minimization is probably as old as economic activity itself, recent technological developments have
allowed for innovation in risk sharing unknown to previous generations. Through sophisticated computer modeling
and the communications revolution, financiers and financial institutions have been able develop and market a
nearly endless array of instruments—from mortgage-backed securities to financial derivatives to hedge fund
accounts—aimed at spreading risk and the returns that can derive from taking on risk. Such activities may have a
social utility. Just as insurance spreads the costs of fire losses over a large population base, so securitization of
debts spreads out the costs of default, making it possible for financial institutions to offer credit to more people at
lower cost.
However, as the recent financial crisis of the first decade of the 2000s has made clear, risk sharing and
minimization for the individual investor can increase risk to an economy as a whole. For example, by bundling
mortgages into securities and then selling them, the initiators of the mortgages reduced their risk of default to the
point where they minimized their own concern about the costs of those defaults, leading them to offer credit to
those who really did not deserve it. When, inevitably, home valuations declined and the economy went into
recession, those defaults piled up, diminishing the value of the mortgage-backed securities and increasing the risk
attached to them. However, the bundling process made it difficult to assess the value of those securities. And
because less information usually means greater risk, this inability to assess value greatly increased the risk of
lending to institutions that held the securities, which led to the freezing up of the credit markets that helped plunge
the world economy into the worst financial crisis and economic downturn since the Great Depression.
James Ciment
 
See also:  Behavioral Economics;  Classical Theories and Models;  Financial Markets;  Hedge
Funds;  Real-Estate Speculation. 
Further Reading
Banks, Erik. Risk and Financial Catastrophe. New York: Palgrave Macmillan, 2009. 
Rebonato, Riccardo. Plight of the Fortune Tellers: Why We Need to Manage Financial Risk Differently. Princeton,
NJ: Princeton University Press, 2007. 
van Gestel, Tony, and Bart Baesens. Credit Risk Management. Basic Concepts: Financial Risk Components, Rating
Analysis, Models, Economic and Regulatory Capital. New York: Oxford University Press, 2009. 
Von Neumann, John, and Oskar Morgenstern. Theory of Games and Economic Behavior.  3rd ed. Princeton, NJ: Princeton
University Press, 1953. 
Robbins, Lionel Charles (1898–1984)

 
British economist Lionel Charles Robbins was a prominent voice in debates over macroeconomic policy and
theory from the 1930s to his death in the 1980s. He helped integrate Austrian school economic theories into
British economic thought, especially in relation to the business cycle. In addition, he was largely responsible for
expanding the British university system.
Born on November 22, 1898, in Middlesex, England, Robbins was educated at Southall County School; University
College, London; and the London School of Economics, from which he received his undergraduate degree in
1923. After lecturing for a year at New College, Oxford, he returned to the London School of Economics in 1925
as a lecturer. In 1929, he was named a professor of political economics, a position he held until 1961; he
remained affiliated with the school on a part-time basis until 1980. During World War II, Robbins served as
director of the economics section of the Offices of the War Cabinet, and was a member of the team that
negotiated the 1945 Anglo-American Loan (from the United States to England to help the country get back on its
feet following the war). He served as president of the Royal Economic Society from 1954 to 1955, and was made
a life peer of Great Britain in 1959. From 1961 to 1970, he was chairman of the Financial Times. He also chaired,
from 1961 to 1964, the committee on higher education, and in 1963 published the Robbins Report, which
advocated the funding and expansion of higher education in Britain.
As a young economist, Robbins adopted the views of such Austrian school economists as Eugen von Böhm-
Bawerk, Ludwig von Mises, and Friedrich von Hayek, rather than following the Marshallian and Keynesian tradition
of British economics (associated with the work of Alfred Marshall and John Maynard Keynes). His best-known
book, An Essay on the Nature and Significance of Economic Science (1932), remains an extremely influential text,
regarded by some as one of most important works of twentieth-century economics. In it, Robbins defines
economics as “the science which studies human behaviour as a relationship between given ends and scarce
means which have alternative uses.” He posits that there is a clear separation between economics and such
disciplines as psychology and sociology, and he seeks to distinguish value (or subjective) judgments from those
aspects of economics that he believed were objective and “scientific” (hence his use of the term economic
science).
Robbins was an exponent of the Austrian theory of the trade cycle, using it to interpret the Great Depression of
the 1930s. The central feature of the Austrian theory was the belief that depressions are an inevitable
consequence of earlier expansions in a country’s money supply and an overexpansion of production capacity.
Robbins believed that the impact of World War I and subsequent economic problems in the early 1920s had led to
the overexpansion and overdevelopment of industries that produced capital goods. When banks were forced to
halt credit expansion, consumer expenditures declined and half-completed investment projects were abandoned.
Robbins’s views countered those of Keynes, who believed that the government should infuse a depressed
economy with money to encourage easier borrowing. Robbins thought that easy credit was among the factors that
had caused economic instability in the first place. He published his views in The Great Depression (1934), but
later, in his Autobiography of an Economist (1971), he confessed to being unhappy with his earlier work and said
he would rather see it forgotten. The change of attitude stemmed from Robbins’s contention that his ideas about
correcting economic depressions had been swayed by the elegance of the Austrian model. In other words, he
believed that he had become a hostage to a theoretical construction that was not appropriate to the economic
situation of the 1920s and 1930s.
Robbins came to regard his earlier rejection of Keynes as the greatest mistake of his professional career. In fact,
during the 1940s, his views on macroeconomic policy were similar to Keynes’s. Robbins spent his later years
writing about the history of economics, lecturing, and supporting education and the arts. He died in London on
May 15, 1984.
Christopher Godden
 

See also:  Austrian School;  Hayek, Friedrich August von;  Mises, Ludwig von. 
Further Reading
Howson, Susan.  “The Origins of Lionel Robbins’s Essay on the Nature and Significance of Economic Science.” History of
Political Economy 36:3 (2004): 413–443. 
Robbins, Lionel. An Essay on the Nature and Significance of Economic Science. London: Macmillan, 1932. 
Robbins, Lionel. Autobiography of an Economist. London: Macmillan, 1971. 
Robbins, Lionel. The Great Depression. London: Macmillan, 1934. 
Robertson, Dennis Holme (1890–1963)
 
Dennis Holme Robertson was a British economist who made important contributions to the study of money supply
and business cycles in the early part of the twentieth century. His work on business cycles, which emphasized the
role of technological innovation, is regarded by modern economists as being ahead of its time.
Robertson was born on May 23, 1890, in Lowestoft, England. He was educated at Eton and at Trinity College,
Cambridge, where he was elected a fellow in 1914. During World War I, he served as a transport officer and was
awarded the military cross for gallantry. He taught at Cambridge from 1930 to 1938, at which time he joined the
faculty of the University of London as the Sir Ernest Cassel professor of money and banking. After working in the
British Treasury during World War II, Robertson returned to Cambridge in 1944, where he taught political economy
until his retirement in 1957. He was knighted in 1953.
In his first major work, A Study of Industrial Fluctuation (1915), Robertson uses historical evidence to support his
argument that fluctuations in economic activity do not arise from psychological or monetary forces, but rather from
the impact on the economy of technological innovations and inventions. In this respect, Robertson viewed
business cycles as a consequence of the process of economic growth resulting from the introduction of new
technology. He suggests that innovations create upswings in the business cycle, leading to massive
overinvestment as entrepreneurs take economic advantage of the new technology. However, Robertson argues,
because investments are generally made without taking into account true economic conditions, such as actual
demand, overinvestment will eventually lead to a downturn of the cycle or even an economic contraction.
Robertson’s ideas about the role of innovations in business cycles and economic development had much in
common with those of the Austrian economist Joseph Schumpeter.
In his next major work, Banking Policy and the Price Level (1926), Robertson considers the impact of the
monetary system on the course of the business cycle. He distinguishes between “appropriate” or “justifiable”
changes in output on the one hand, and “inappropriate” or “actual” changes in output on the other hand. Within a
money-using economy, he argues, inappropriate fluctuations are likely to exceed appropriate fluctuations. And, he
maintains, inappropriate fluctuations can be minimized if monetary authorities regulate saving and investment
through control of the money supply, through such measures as adjusting interest rates. Although it was a highly
innovative work at the time, Banking Policy and the Price Level was considered one of the most difficult books in
the whole of economic literature, owing in part to Robertson’s use of rather complex language.
Throughout the 1920s, Robertson had a close working relationship with Cambridge economist John Maynard
Keynes, who had been one of his teachers before World War I. The relationship became strained in the 1930s

because of their opposing interpretations of the nature of saving, investment, and the rate of interest, and the
publication in 1936 of Keynes’s major work, the General Theory of Employment, Interest and Money. Although
Keynes died in 1946, his work continued to be influential at Cambridge. Robertson, who became an increasingly
isolated figure within the Cambridge economic community, died there on April 21, 1963. His was reappraised in
later years, and came to be regarded as one of Britain’s foremost economists of the early twentieth century.
Christopher Godden
 
See also:  Keynes, John Maynard;  Keynesian Business Model;  Schumpeter, Joseph. 
Further Reading
Fletcher, Gordon. Dennis Robertson. Basingstoke, UK: Palgrave Macmillan, 2008. 
Fletcher, Gordon. Dennis Robertson: Essays on His Life and Work. Basingstoke, UK: Palgrave Macmillan, 2007. 
Laider, David. Fabricating the Keynesian Revolution: Studies of the Inter-war Literature on Money, the Cycle, and
Unemployment. Cambridge, UK: Cambridge University Press, 1999. 
Robertson, Dennis H. Banking Policy and the Price Level: An Essay in the Theory of the Trade Cycle. London: P.S.
King, 1926. 
Robertson, Dennis H. A Study of Industrial Fluctuations: An Enquiry into the Character and Causes of the So-Called Cyclical
Movements of Trade. London: P.S. King, 1915. 
Robinson, Joan (1903–1983)
 
Associated for most of her career with Cambridge University, British economist Joan Robinson expounded on the
theories of John Maynard Keynes, particularly with regard to government involvement in a nation’s economy. As a
member of the Cambridge school (also known as the Cambridge Circus), Robinson became known for her
contributions in the 1930s to the Keynesian explanation for the rises and falls in the business cycle.
She was born Joan Violet Maurice on October 31, 1903, in Surrey, England. She attended Girton College,
Cambridge University, as an undergraduate student in economics, receiving a bachelor’s degree in 1925; that
same year she married economist Adam Robinson. She remained at Cambridge, taking positions at Newnham
College, where she was elected a fellow in 1962; Girton College, where she became a full professor and was
named a fellow in 1965; and King’s College, where she was named the first female fellow in 1979. The main
focus of her work was in the area of full employment and how and when it can occur in the business cycle, as
envisioned by Keynes. It was a topic she would further develop in subsequent years, importantly in The
Accumulation of Capital (1956). Robinson, along with economist Nicholas Kaldor, also expanded on discussions of
international trade and the growth of economic development in what was called the Cambridge growth theory. In
this work, she wrestled with such aspects of economic theory as the role of imperfect competition in business
cycles when monopolies or large firms control markets. Robinson proposed the Keynesian approach, according to
which imperfect competition is an important part of a real-world economy and must be incorporated into growth
models.
In the early 1960s, Robinson and Peiro Sraffa entered into a debate with economists Robert Solow and Paul

Samuelson in which Robinson and Sraffa contended that corporate profits grow from—and are determined by—
tensions among different societal classes. Solow and Samuelson argued that profits are nothing more or less than
economic benefits earned by the companies that most efficiently use their means of production. In other words,
the debate questioned whether profits occur because of purely economic factors or because of political (that is,
class) differences. The debate, conducted through written articles, continued for several years, with Robinson
ultimately claiming victory. Since that time, most commentators have generally agreed the result was a draw,
although some believe that, as a school of thought, neoclassical economics—which argues for pure competition
and no government regulation—suffered greatly as a result of the debate.
While Robinson described herself as a “philosophical Marxist,” economists have questioned whether she was truly
that. Either way, there is little doubt that the theories of Karl Marx had a significant influence on her and informed
much of her work. Although she frequently criticized Marxist economics, she was consistently critical of capitalism.
She acknowledged that the capitalist system appeared to be successful in the United States but viewed it as a
“very cruel system.” She also argued that the capitalist system—marked by imperfect competition arising from its
lack of planning and its acquisitiveness—leads to economic instability and downturns in the business cycle, and
results in great hardship for the working class, as occurred during the Great Depression of the 1930s. Robinson
died on August 5, 1983, in Cambridge, England.
Robert N. Stacy
 
See also:  Kaldor, Nicholas;  Keynes, John Maynard;  Samuelson, Paul;  Sraffa, Piero. 
Further Reading
Asimakopulos, A. Investment, Employment and Income Distribution. Boulder, CO: Westview, 1988. 
Feiwel, George R., ed. Joan Robinson and Modern Economic Theory. Hampshire, UK: Macmillan, 1989. 
Marcuzzo, Maria Cristina, Luigi L. Pasinetti, and Alessandro Roncaglia, eds. The Economics of Joan
Robinson. London: Routledge, 1996. 
Rima, Ingrid H., ed. The Joan Robinson Legacy. Armonk, NY: M.E. Sharpe, 1991. 
Robinson, Joan. Collected Economic Papers. Oxford, UK: Blackwell, 1980. 
Robinson, Joan. Further Contributions to Modern Economics. Oxford, UK: Blackwell, 1980. 
 
Romer, Christina (1958–)
 
Christina Romer is an American economist and economic historian whose area of specialty is the study of
recessions and depressions, especially the Great Depression of the 1930s. In addition to her academic career,

she has applied her study of business cycles to the formulation of public policy. In November 2008, Romer was
named head of the Council of Economic Advisers by newly elected President Barack Obama. Her primary initial
task in that position was to draft the administration’s recovery plan for the recession it inherited upon taking office
in early 2008.
A respected scholar and experienced economic policy maker, Cristina Romer advised candidate Barack Obama
during the 2008 presidential campaign. Upon taking office, Obama named her chair of the Council of Economic
Advisers. (Bloomberg/Getty Images)
Christina Duckworth Romer was born December 25, 1958, in Alton, Illinois, and received her bachelor’s degree
from the College of William and Mary (1981) and a Ph.D. in economics from the Massachusetts Institute of
Technology (1985). After beginning her academic career as an assistant professor at Princeton University, Romer
moved to the University of California at Berkeley in 1988, where she became the Garff B. Wilson professor of
economics. She became known as a leading expert on government intervention in the economy, especially on
matters of monetary policy, the impact of tax cuts and increases, and the effects of all these factors on inflation.
Romer’s focus on the relationships among various economic data has led her to take occasionally contrarian and
controversial positions. In the 1980s, for example, she argued that government economic estimates—based on
agriculture and manufacturing exclusively—exaggerated unemployment rates and the overall economic volatility of
the U.S. economy during the depression of the 1930s. Had the service sector been properly factored in, she
maintained, economic data for the period would have painted a somewhat better picture of the economy and
contributed to a greater sense of optimism. Romer’s conclusions questioned the conventional wisdom that it was
Keynesian-inspired government stimulative measures that finally stabilized the economy. In fact, she insisted, the
nation’s economy leading up to and during the Great Depression had never been as unstable as historians have
maintained.
In addition to her teaching, research, writing, and other academic pursuits—including a vice-presidency of the

American Economic Association—Romer has compiled an impressive record as an economic policy adviser and
strategist. She served as co-director of the Program in Monetary Economics at the National Bureau of Economic
Research and, until her appointment to the Obama administration, served on the bureau’s Business Cycle Dating
Committee (which determines whether or not the economy is in a recession).
While serving in these positions, Romer went on record commending the Federal Reserve for its efforts to stabilize
the U.S. financial system by purchasing equity stakes in major financial institutions and agreeing to insure
hundreds of billions of their troubled assets in late 2008, all part of a program known as the Troubled Assets
Relief Program (TARP). Based on her own previous research and the work of monetary theorist Milton Friedman,
Romer opined that had the Federal Reserve taken similar moves during the financial crisis of the early 1930s, the
Great Depression would not have been as deep or as lasting as it was.
President Obama’s decision to appoint Romer as head of the White House Council of Economic Advisers was not
unexpected, as she had advised candidate Obama through much of the 2008 campaign. The appointment was
widely hailed by fellow economists, who cited her work on government policy during the Great Depression and its
relevance to the current-day economic crisis. After taking office, Romer argued forcefully for selective tax cuts,
particularly for the middle class. President Herbert Hoover’s tax hikes of the early 1930s, imposed to help balance
the federal budget, had been a disastrous mistake, she maintained. Romer also remained a strong advocate of
TARP, holding that continued government infusions of capital in major financial institutions are critical to the
restoration of properly functioning credit markets, both at home and abroad. In September 2010, Romer resigned
as chair and joined the economics faculty at the University of California, Berkeley.
Robert N. Stacy and James Ciment
 
See also:  Council of Economic Advisers, U.S.;  Keynesian Business Model;  National Bureau of
Economic Research;  Troubled Asset Relief Program (2008-). 
Further Reading
Romer, Christina D. Changes in Business Cycles: Evidence and Explanations. Cambridge, MA: National Bureau of
Economic Research, 1999. 
Romer, Christina D.  “The Great Crash and the Onset of the Great Depression.” Quarterly Journal of Economics 105:3
(August 1990): 597–624. 
Romer, Christina D. Monetary Policy and the Well-Being of the Poor. Cambridge, MA: National Bureau of Economic
Research, 1998. 
Röpke, Wilhelm (1899–1966)
 
A German economist known for his strong support of free-market economics, Wilhelm Röpke was admired for his
humane views and strong beliefs in social and economic justice, which were founded on his religious beliefs and
conservative social values. Considered a member of the Austrian free-market school, Röpke was opposed to left-
wing socialism, communism, and any form of state-run economics.
Born on October 10, 1899, in Hannover, Germany, Röpke served in the German army during World War I, which

moved him to become profoundly antiwar and actively supportive of individual human rights. After the war, he
studied economics at the University of Marburg and received his doctorate in 1921. He went on to teach
economics in Jena, Graz, Marburg, and, having fled the Nazis in 1933, the University of Istanbul in Turkey and
then the Institute of International Studies in Geneva, Switzerland, where he remained until his death in 1966.
Röpke’s first attempt to shape government policy occurred in the early 1930s, when he proposed that Germany’s
Weimar government abandon its inflationary policies to combat the results of the economic collapse and instead
adopt the kinds of policies articulated by John Maynard Keynes in 1936. Later, sharply critical of the economics of
Benito Mussolini’s fascist Italy as well as the proposed economic policies of the Nazi Party, Röpke claimed that
fascism lacked intellectual freight and, as in the case of Italy, was more of a slogan than a well-thought-out
political philosophy.
Although he did not see the state as an appropriate authority to run a system as complex as an economy, he did
see a role for it. Here he broke with the Austrian school, for he believed that the state should make and enforce
rules that guaranteed fairness—particularly in competition, by means of antitrust legislation, for example—and
justice. He thought the state should support small businesses and provide temporary aid and assistance to people
whose livelihoods were disrupted by economic downturns and downtrends in the business cycle. Röpke drew a
line, however, at the creation of a welfare state, believing it would have too much influence over its citizens and
lead to social decay.
In his analysis of the causes of business cycles, Röpke noted the important role played by technology. He
observed that innovations such as railroads, steel manufacturing, automobiles, and electricity create spikes in
investment that, in turn, cause sudden rises in all economic forces reacting to the stimulus. Such disturbances can
only be overcome by a resulting depression. In linking the business cycle with technological developments, his
ideas clearly influenced the great economist Joseph Schumpeter and were the precursor to the latter’s theory of
the role of “creative destruction” in the business cycle.
Following World War II, Röpke served as an economic adviser to the West German government. He was opposed
to the Marshall Plan, believing foreign aid would not bring about the needed economic recovery. After the war, the
German zones occupied by Great Britain, France, and the United States retained aspects of Nazi economic policy,
such as wage-and-price controls and inflation, resulting in a halt to production and shortages of many goods. In
this environment, the black market thrived. The result was an economy in shambles.
Röpke recommended the abolishment of all controls, a halt to the printing of money, and the institution of a solid
currency. The immediate result was a great deal of hardship, but eventually the German economy recovered.
Robert N. Stacy
 
See also:  Austrian School;  Hayek, Friedrich August von;  Mises, Ludwig von. 
Further Reading
Hudson, M.  “German Economists and the Great Depression, 1929–33.” History of Political Economy 17:1 (1985): 35–50. 
Röpke, Wilhelm. The Crisis of European Economic Integration. Zurich: Swiss Credit Bank, 1963. 
Röpke, Wilhelm. Economics of the Free Society, trans. Patrick M. Boarman. Chicago: H. Regnery, 1963. 
Röpke, Wilhelm. A Humane Economy: The Social Framework of the Free Market. Chicago: H. Regnery, 1960. 
Röpke, Wilhelm. Welfare, Freedom, and Inflation. Tuscaloosa: University of Alabama Press, 1964. 
Zmirak, John. Wilhelm Röpke: Swiss Localist, Global Economist. Wilmington, DE: ISI Books, 2001. 

Rostow, Walt Whitman (1916–2003)
 
American economist Walt Whitman Rostow was an academic, political theorist, and high-ranking government
adviser best remembered for his role in advocating U.S. involvement in the Vietnam War. As an economic
historian, he first attracted notice for his writings on economic growth in developing nations from a democratic and
capitalist (anti-Marxist) perspective.
Rostow was born on October 7, 1916, in New York City. After receiving undergraduate and graduate degrees from
Yale University, he went on to teach briefly at Columbia University, then Cambridge University, and later at the
Massachusetts Institute of Technology (MIT). He served in the Office of Special Services (OSS, predecessor to
the Central Intelligence Agency) during World War II, then joined the State Department as an administrator for the
Marshall Plan (designed to help an economically devastated Europe). This experience likely contributed to his
strong opposition to communism. Later he worked for the United Nations Economic Commission for Europe under
Swedish economist Gunnar Myrdal.
During his years at MIT (1950–1961), Rostow continued U.S. government service, first as a speechwriter for
President Dwight Eisenhower and later as a deputy assistant for national security in the John F. Kennedy
administration. Following President Kennedy’s assassination, he served in the Johnson administration until 1968.
In 1959 Rostow published an article titled “The Stages of Economic Growth” in the Economic History Review,
followed the next year by a book of the same title, which was to be his best-known published work. In it, Rostow
described a model of economic growth (referred to as the Rostovian take-off model) centered on what he called a
biological view—similar to the human life cycle. This five-stage process begins with what Rostow defined as
“traditional society” and progresses to preconditions for economic take-off, followed by take-off, drive to maturity,
and, finally, mass consumption. Rostow’s model was seen as important to understanding the “take-off” of the
industrial revolution in the nineteenth century, first in England and then in the United States.
In his model, Rostow describes a point in an economy, called “beyond consumption,” at which people’s lives
would no longer be dominated by the pursuit of food, shelter, clothing, and durable goods. Instead, with these
needs satisfied, political considerations would come to dominate. Because of his ideological bent—and because it
was the cold war era—Rostow talked of these political factors in terms of capitalism versus communism. He
rigorously reinforced this point in a controversial book titled The Stages of Economic Growth: A Non-Communist
Manifesto (1960), in which he reinterpreted his earlier ideas on the stages of economic growth from as much a
political perspective as an economic one.
Some economists criticized the book, pointing to methodological problems with Rostow’s analysis. Others claimed
that Rostow’s ideas seemed to be firmly rooted in a nineteenth-century point of view, which took progress for
granted and assumed that all actors perform rationally according to self-interest. By the middle of the twentieth
century, following two world wars and political and intellectual revolutions, such assumptions were deemed no
longer valid. Critics also noted Rostow’s occasional inconsistencies, his predominantly Western perspective, and
examples said to be exceptional rather than typical.
In 1968, Rostow left government service and moved with his wife Elspeth to the University of Texas at Austin,
where he had a long career as an academic. He died on February 13, 2003.
Robert N. Stacy
 
See also:  Growth, Economic;  Myrdal, Gunnar. 

Further Reading
Milne, David. America’s Rasputin: Walt Rostow and the Vietnam War. New York: Hill and Wang, 2008. 
Rostow, W.W. Concept and Controversy: Sixty Years of Taking Ideas to Market. Austin: University of Texas Press, 2003. 
Rostow, W.W. The Economics of Take-Off into Sustained Growth. London: Macmillan, 1963. 
Rostow, W.W. The Process of Economic Growth. New York: W.W. Norton, 1952. 
Rostow, W.W. The Stages of Economic Growth: A Non-Communist Manifesto. Cambridge, UK: Cambridge University
Press, 1960. 
 
Rubin, Robert (1938–)
 
Financier and business executive Robert Rubin served as secretary of the treasury from 1995 to 1999 under
President Bill Clinton, who hailed him as the greatest treasury secretary since Alexander Hamilton. However,
Rubin’s enthusiasm for market deregulation later was seen by many as a contributing factor to the crisis in the
U.S. economy that began in the late 2000s.
Robert Edward Rubin was born on August 29, 1938, in New York City. His family relocated to Florida, and he
attended public high school in Miami. He received a bachelor’s degree in economics from Harvard University in
1960, and then briefly attended Harvard Law School and the London School of Economics. After earning a J.D.
degree from Yale Law School in 1964, Rubin joined the law firm of Cleary, Gottlieb, Steen & Hamilton before
taking a position at the investment firm Goldman Sachs. There, in 1971, he was named a general partner; in
1980, a member of the management committee; and in 1987, vice chair and a chief operating officer, a position
he held until 1990, when he became a chair and a senior partner.
Following Clinton’s election to the presidency, Rubin was named director of the National Economic Council, which
was established by the administration to coordinate all agency and departmental activities affecting the economy.
During Rubin’s tenure as director, the North American Free Trade Agreement was signed. Two years later, Rubin
was sworn in as secretary of the treasury. Considered highly effective in that position, Rubin—along with
Lawrence Summers, deputy secretary of the treasury, and Alan Greenspan, chair of the Board of Governors of the
Federal Reserve System—led efforts to stabilize developing financial crises in Mexico, Russia, Korea, Indonesia,
Thailand, and Latin America. Rubin’s conservative approach, particularly with regard to deficit reduction, garnered
support for Clinton from the business and financial sectors, and earned praise from Republicans and Democrats
alike. By the time Rubin stepped down from his post in 1999, the U.S. unemployment rate was at 4.3 percent and
the government had a budget surplus of $79 billion.
As head of the treasury, Rubin, an advocate of free-market economics, often supported deregulation. He

successfully lobbied for congressional repeal of the Glass-Steagall Act, legislation passed in 1933 that had made
it easier for banks to borrow from the Federal Reserve, separated investment banking from commercial banking
and insurance underwriting, and had given increased power to the Federal Reserve. Following deregulation,
Citicorp (a bank for which Rubin later would work) merged with Travelers Group (insurance underwriters) in a deal
valued at about $70 billion. With Greenspan’s support, Rubin successfully urged Congress to oppose the
regulation of trading in the credit derivatives of mortgage-backed securities. Regulation had been supported by,
among others, Brooksley Born, head of the Commodity Futures Trading Commission. Much later, large holdings
by financial institutions of these so-called toxic securities would lead to the demise of several prominent investment
firms, including AIG (American International Group), Bear Stearns, and Lehman Brothers, and contribute to the
financial meltdown of 2007–2008.
A former Goldman Sachs executive, Robert Rubin served as treasury secretary during the Bill Clinton
administration. Rubin was lionized for his role in the economic boom of the 1990s but later criticized for lax
oversight of the financial community. (Tim Sloan/Stringer/AFP/Getty Images)
After leaving the Treasury Department, Rubin accepted a position on the board of a community development
organization. He joined Citigroup as a member of the board of directors and an executive officer—overlapping and
potentially conflicting roles that raised concerns outside the company as well as among shareholders. In 2001, he
urged the Treasury Department not to downgrade the rating of the (later notorious) Enron Corporation because it
was a creditor to Citigroup. The request was refused, and the episode was investigated by a congressional
committee, which cleared Rubin of any wrongdoing. In 2007, Rubin served temporarily as chair of the board of
directors of Citigroup. But, facing increasing criticism as a result of mounting shareholder losses, as well as a 2008

shareholder lawsuit that accused Rubin and other firm executives of steering the company toward financial ruin,
Rubin announced his resignation from Citigroup in January 2009, where his earnings are estimated to have been
anywhere from tens to hundreds of millions of dollars.
In January 2009, a MarketWatch article named Rubin as one of the “10 Most Unethical People in Business,”
claiming that he was among those whose “flubs have tarnished the financial world.” Nevertheless, a number of
key figures who served under Rubin were appointed to positions in the administration of President Barack Obama,
including Treasury Secretary Timothy P. Geithner, Senior White House Economic Adviser Lawrence Summers,
and Budget Director Peter R. Orszag.
Robert N. Stacy
 
See also:  Citigroup;  Treasury, Department of the. 
Further Reading
Rubin, Robert E., and Jacob Weisberg. In an Uncertain World: Tough Choices from Wall Street to Washington. New
York: Random House, 2004. 
U.S. Senate Committee on Governmental Affairs, Permanent Subcommittee on Investigations. The Role of the Financial
Institutions in Enron’s Collapse: Hearing Before the Permanent Subcommittee of [sic] Investigations of the Committee on
Governmental Affairs, United States Senate, One Hundred Seventh Congress, Second Session, July 23 and 30,
2002. Washington, DC: U.S. Government Printing Office, 2002. 
 
Russia and the Soviet Union
 
A geographically vast nation of approximately 140 million people, Russia stretches across the northern half of the
Eurasian landmass from Eastern Europe in the west to the Pacific Ocean in the east. While the country has a
major heavy industrial infrastructure, most of its exports consist of raw materials, especially oil and gas.
Ruled until the early twentieth century by absolute monarchs known as czars, Russia lagged economically behind
the rest of Europe, its modernization stunted by a repressive social structure in which the peasantry had little
freedom. Rapid industrialization began in the late nineteenth and early twentieth centuries, a process contributing
to the social unrest that brought down the czarist state and replaced it with a communist one. Under successive
regimes, the renamed Union of Soviet Socialist Republics, or Soviet Union, made great industrial strides between
the 1920s and the 1960s, though its collectivized agriculture produced perennial food shortages.
Centralized planning and a repressive political environment also stunted economic innovation, however, leading to
stagnation from the 1970s on, and ultimately the collapse of the Communist regime and the Soviet state itself in

the early 1990s. Russia, which inherited most of the territory and population of the former Soviet Union,
experienced great economic dislocation through the 1990s, leading to near collapse in 1998.
Since then, the Russian economy has recovered somewhat as market forces, a more efficient bureaucracy, and
high market prices for natural resources led to substantial growth in the early years of the twenty-first century. The
wealth generated has been unevenly distributed, with well-connected elites, often former Communist Party officials,
reaping the lion’s share of income. The period of growth came to an end with the fall in energy prices and the
global financial crisis of 2008–2009.
 Economic History Before Communism
Modern Russia dates back to the creation by Viking invaders of the Kievan Rus in the ninth century CE, which,
within a few centuries, emerged as the most prosperous state in Europe. With trade networks that linked it to the
Black Sea, Scandinavia, and other parts of northern Europe, and with its capital in what is now the Ukrainian
capital of Kiev, Kievan Rus was more of a trading alliance than a unified state. It eventually succumbed to
repeated invasions by Turkic peoples and Mongols in the twelfth and thirteenth centuries.
By the fourteenth century, a new center of Russian power had emerged around Moscow, with an ever more
powerful czar controlling territories that by the end of the sixteenth century had come to incorporate much of what
is now western Russia and western Siberia. During the reign of Ivan IV, also known as Ivan the Terrible, in the
mid-sixteenth century, the social and economic order of the next several centuries of czarist Russian history was
established. Through violent means, Ivan reduced the power of the aristocrats and established serfdom in the
countryside, whereby peasants were turned into quasi-slaves of landholders who owed their elite status to the
czar in Moscow. These serfs were legally tied to the land, with almost all facets of their lives dictated by their
landholding masters.
Ivan established a similarly authoritarian order over towns, where artisans and traders were bound to their
occupations and localities so that they could be more efficiently taxed. Property rights, even of wealthy merchants,
were granted at the pleasure of the czar as well. After Ivan’s death in 1584, a period of anarchy ensued in
Russia, gradually giving way to new czarist authority in the early seventeenth century.
Under Peter the Great, who ruled Russia from 1682 to 1725, Russia built up the largest standing army in Europe
and proceeded to conquer new territories around the Baltic Sea. Peter established even more authority in the
central state, raising taxes and arbitrarily assigning serfs to work in emerging factories and mines. But he was
also a reformer, encouraging education, importing Western economic ideas, and building the port of St. Petersburg
on the Baltic, giving the country its “door” to Europe and the transatlantic world. Under Peter, Russia became a
major exporter of grains, furs, timber, and minerals to Europe.
Still, Russia remained a largely agricultural state and a relatively poor one, compared to Western Europe, through
the early nineteenth century. Russia’s loss to France and England in the Crimean War during the 1850s forced it
to begin an economic and social modernization process, starting with the abolition of serfdom in the 1860s. But
the collectivist agriculture that replaced the old order proved inefficient, and the farming sector continued to
underperform. More successful was the government’s effort to expand the vast country’s transportation network.
Between 1860 and 1900, the railroad network expanded from about 1,250 miles of track (2,000 kilometers) to
more than 15,000 miles (24,000 kilometers).
The new railroad network increased exports, bringing in the capital Russia needed to spur its belated
industrialization. By the turn of the twentieth century, numerous new factories had emerged around St. Petersburg,
Moscow, and other western Russian cities, along with a new urban proletariat.
 Communist Economics
Among the new working class, radical economic ideas of both the anarchist and communist varieties took hold.

This led to the revolution of 1905, which established a nominal constitutional monarchy, and then the Bolshevik
Revolution of 1917, spurred by the sufferings of the Russian people in World War I. Under Vladimir Lenin, the last
of the czars was murdered, and the Bolsheviks fought off counterrevolutionary forces in a brutal civil war that
lasted into the early 1920s.
The conflict left the country in ruins, with agricultural output off more than 50 percent from its pre–World War I
peaks and heavy industrial production down by as much as 95 percent. Initial efforts to collectivize agriculture and
light industry were abandoned, in favor of limited market freedoms for farmers, petty traders, and small
manufacturers; heavy industry remained nationalized. By the mid-1920s, the New Economic Policy (NEP) had
helped revive agricultural and industrial output to near or above prewar levels.
But with the emergence of dictator Joseph Stalin in the late 1920s, the NEP was abandoned in favor of more
centralized planning and control. In 1927, the Soviet government launched the first of its five-year plans, with the
emphasis on collectivized agriculture and state-directed heavy industrial development. Resistance among land-
owning peasants, known as Kulaks, was ruthlessly crushed, though the state reluctantly allowed them to grow
their own crops for sale on tiny plots.
To pay for the industrialization, Stalin imposed large, indirect taxes on peasants and workers, millions of whom
died in the effort to collectivize agriculture and rapidly build a heavy industry infrastructure. Such human costs
aside, the results were remarkable, as steel, coal, and other heavy industry output soared, even while consumer-
goods production lagged.
World War II struck a heavy blow to Soviet economic development as invading German armies and the struggle to
oust them decimated western Russia, the heartland of the country’s modern industrial economy, though efforts
were made to relocate some factories to east of the Ural Mountains. Victorious Soviet armies occupied much of
Eastern Europe after the war, helping to dismantle some of East Germany’s factories and move them to the Soviet
Union as compensation for the damage inflicted by Germany during the war. With Stalin’s death in 1953, the
country underwent a period of limited political liberalization, though Stalinist centralized planning and development
continued to be the hallmark of the Soviet economy. For a time, such planning worked, providing an ever-rising
standard of living for the Soviet people and a growing, albeit still limited, array of consumer goods.
Still, compared to the economic “miracle” in Western Europe, the Soviet Union continued to fall behind, even as it
maintained one of the most powerful military systems in the world. But, as many Western economists pointed out,
centralized planning simply could not keep up with modern consumer demands in the same way that free-market
economies could. By the 1970s, the country’s economy was stagnating, with consumers unable to obtain the
products they desired or else forced to wait in long lines to buy them. Meanwhile, a thriving black market in goods
and services emerged, despite being officially frowned upon by the state. Collectivized agriculture also proved a
disappointment, as the country was forced to turn to the West to fill its larders.
By the 1980s, the limitations of Soviet economic planning had become apparent even to the Communist Party
leadership, who appointed reformer Mikhail Gorbachev to run the country in 1985. Gorbachev immediately initiated
twin policies aimed at shaking up the sclerotic and repressive Soviet system. Glasnost, or openness, was intended
to allow competing and critical voices to emerge in media, government, and civil society. Perestroika, or
restructuring, permitted more market forces in the setting of prices, increased independence of industrial managers
from centralized planning, and the legalization of profit-seeking private cooperatives in the service sector.
Gorbachev hoped to reform and modernize the country’s socialist economics and politics, but his efforts had
unintended consequences. By ending Soviet control over Eastern Europe, Gorbachev helped catalyze a series of
largely nonviolent revolutions in that region, leading to the end of Communist rule there and the breakaway of the
three Baltic republics from the Soviet Union in 1989 and 1990. Following an unsuccessful coup by hardliners in
the Communist Party and military, the Soviet Union itself broke apart in 1991 as Communist Party rule gave way
to a limited multi-party democracy.

 Post-Soviet Economic Chaos
After the breakup of the Union of Soviet Socialist Republics in 1991, Russia attempted to transition from a
government-owned economy to a market economy with enterprises in the hands of private owners. To achieve
this, a massive privatization program was launched: factories, plants, stores, and offices were given out to people
virtually for free.
But in a country with no history of democratic institutions and no knowledge of how a market economy worked,
former government assets became concentrated in the hands of just a few federal and local elites. The new
owners had no entrepreneurial experience, often did not even believe in the market economy, and were not
interested in restructuring their new properties into effective enterprises. Instead, so-called stripping of assets
became prevalent. No investments were made in growing new businesses; rather, the machinery, tools,
technology, and even bricks from the building walls were sold for cash. The enterprises were falling apart as their
owners became richer. The new owners also often removed money from the economy by transferring it abroad—
opening bank accounts, purchasing real estate, or acquiring business in Western Europe and the United States.
Thus, privatization largely failed, as it did not create efficient owners who would (or could) grow their businesses.
With domestic production virtually nonexistent, so too were manufacturing jobs. This situation led most of the
population to seek employment in commerce, both domestic and foreign. Most of the consumer goods and food
items purchased by the people were imported, often going through a long chain of traders before reaching the end
consumer. Imports of foreign products effectively substituted for domestic production. However, enormous assets
inherited from the former Soviet Union allowed the country to exist quite well by simply selling them and
purchasing what the country needed from abroad. In addition, Russia had large natural reserves of oil, gas,
metals, and minerals. As long as oil prices remained high, the country was able to continue its somewhat stable
economic existence.
Meanwhile, the immediate post-Soviet problem of inflation eased as the government shifted to treasury bills to pay
for its debt. Russia did not have a balanced budget because it continuously spent more than it earned. To finance
the deficit, the government, following the instructions of International Monetary Fund (IMF), issued short-term
federal-debt obligations, known as GKOs—zero-coupon treasury bills issued by the Russian Finance Ministry.
With the real economy not growing, GKOs became a Ponzi scheme: the payments for matured obligations were
financed by the issuance of new obligations. The Russian government had to offer high return on this debt to
compensate for the risk; in the wake of the crisis, some government-issued bonds produced yields of almost 200
percent. The debt was becoming unsustainable.
The ruble exchange rate was stable because of an artificially fixed currency exchange rate. Attracted by large
returns, foreign investors began entering the Russian market. Deficits continued to grow even as stores were full
of goods from all over the world and at reasonable prices (thanks to an artificial exchange rate). International
brands started advertising in Russia as consumers there developed capitalist-style consumption habits. The
Russian economy seemed to be booming despite the fact that its deficits were growing and its gross domestic
product (GDP) was decreasing annually.

Oil exports, the mainstay of the Russian economy, brought good times in the mid-1990s and mid-2000s, when
prices were high, and hard times in the latter part of both decades, when prices dropped. An aging pipeline
system poses a long-term challenge. (AFP/Stringer/Getty Images)
 Crisis of 1998
World oil prices, however, started declining in 1998, falling to below $10 per barrel from over $20 per barrel just a
year before. And the prices of many other natural resources were also falling significantly. Because much of the
Russian economy was based on exporting these natural resources, the result was a sharp decline in revenues for
the country. In turn, this made it difficult to pay for the import of foreign products.
With the lack of revenues, the Russian government had to sell dollars from its reserves in order to support the
fixed ruble/dollar exchange rate. In an economy where debt was growing and pressures on the ruble were
increasing, the government had to sell more and more dollars. Because the dollar reserves were not unlimited,
they were eventually depleted.
By spring 1998, the Russian economy was in trouble, and Russian president Boris Yeltsin attempted to take
control of the situation. That March, he dismissed the entire government, including Prime Minister Viktor
Chernomyrdin. Yeltsin appointed Sergei Kiriyenko, a young liberal, as the acting prime minister, but the State
Duma, or parliament, twice rejected his appointment. Only on the third attempt, on April 24, 1998, facing the
threat of new parliamentary elections, was Kiriyenko approved.
As prime minister, Kiriyenko focused on negotiations with the IMF in attempts to secure additional loans to pay off
the country’s internal and external debt. Finally, in July, a new loan in the amount of over $22 billion was
approved. The new loan, however, was not able to cover all of the outstanding obligations. The government owed
over $12 billion in unpaid salaries to state employees alone. Kiriyenko also developed a comprehensive anticrisis
plan, but the State Duma rejected it. By the end of July, the situation was out of control.
On July 29, 1998, President Yeltsin interrupted his vacation to return to Moscow. Realizing that the economy
could not be saved, the money from the IMF loan was removed from the government’s accounts and
disappeared. Even today, it is unclear what happened to those funds. Despite expectations of another change of
government and prime minister, President Yeltsin made only one change—appointing as the head of the Federal
Securities Services an obscure public official, Vladimir Putin, who had completed his graduate degree less than a
year earlier.
On August 17, 1998—a day Russians would come to call Black Monday—the Russian economy collapsed

because of the decline in economic production, uncontrolled budget deficit, the plummeting prices of oil and other
natural resources, an artificial currency exchange rate, and the resulting undermining of investor confidence.
Russia defaulted on its debt obligations. The government and the central bank issued a statement saying that they
were suspending trading of GKOs, and they introduced compulsory restructuring of GKOs and other short-term
debt obligations into new long-term securities on very unfavorable terms for investors. Russian banks, which had
invested heavily in GKOs, lost almost half of their assets. Many of them went bankrupt and had to close down.
Russia also imposed a ninety-day moratorium on payments on the loans issued by nonresident lenders.
The fixed currency exchange rate was abandoned and the ruble was devalued by more than two-thirds. Inflation
started growing again. The cost of living increased substantially as Russia relied heavily on imported goods to
make up for the lack of its own production. Many foreign products became too expensive, quadrupling in price,
and imports declined. In anticipation of a complete collapse of the economy, people cleaned off the shelves of
stores in an attempt to stockpile basic goods. As stores stood empty, the shortage of even basic necessities
became inevitable. The Russian people took to the streets. The economic collapse was threatening political
turmoil.
Kiriyenko and his cabinet were dismissed. The new cabinet, appointed in violation of the Russian constitution, was
rejected by the Duma. President Yeltsin had to back down and appoint a prime minister who would be acceptable.
On September 11, 1998, Yevgeny Primakov became the new prime minister of the Russian Federation and the
Duma began working on President Yeltsin’s impeachment hearings.
 Post-1998 Economy and the Crisis of 2008–2009
For the remainder of 1998, the economy began to show signs of improvement. The impeachment of Yeltsin never
materialized, and political stability was restored. The conservative government of Yevgeny Primakov introduced a
more balanced budget and started working on improving fiscal policies in Russia. The devaluation of the ruble
made imported goods unattainable for the general public, which provided a stimulus to production of Russian
goods and services. For the first time since the end of the Soviet Union, Russian goods became available in
stores around the country.
The growth of domestic production also led to increased tax revenues and greater investment in domestic
production. Infusions of money into the economy allowed companies and government to start paying off arrears in
salaries to their workers, who in turn increased their consumption, creating the need for more domestic
production. The Russian Federation was also helped by the stabilization of oil prices.
Indeed, rising oil and commodity prices helped buoy the Russian economy in the early and middle 2000s, even as
President Vladimir Putin, in office since 2000, enhanced the powers of the government and continue paying out
pension, as well as salaries, that had been in arrears for years under Yeltsin. The new revenues allowed Russia
to service its foreign debt more effectively, repay international loans, and build up a surplus of foreign reserves in
the central bank, with part of the surplus going into a stabilization fund to help tide Russia over during future drops
in oil and natural gas prices.
As the country’s fiscal house was being put in order, its economy was also reviving. GDP growth rates between
2000 and 2008 averaged between 7 and 8 percent annually in most years, and industrial output increased by 75
percent. By 2008, the per capita GDP was about $11,000, measured in purchasing power parity. (That statistic
helps compensate for variations in currency values, putting Russia at the higher end of middle-income countries.)
Meanwhile, the more stable political environment and increasing consumer demand spurred domestic and foreign
investment, which, together, rose 125 percent in these years. Still, all of these gains were largely catch-up, as it
was only in 2008 that the country’s GDP surpassed the level it had reached in 1990, the last full year of the
Soviet Union’s existence. In other words, the impressive growth of the 2000s served only to recoup the losses
incurred in the economically disastrous 1990s.
A combination of factors in 2008 and 2009—including war with Georgia, which quelled foreign investment, falling

energy prices in the late 2008, and the global financial crisis and recession—ended the period of growth, creating
yet another economic crisis in Russia, though one not nearly as catastrophic as that of 1998. Russian securities
prices plummeted in late 2008 as foreign investors pulled out their assets, the ruble plunged in value, interest
rates skyrocketed, and bankruptcies spread through the financial system.
Unlike 1998, this time the Russian government had large capital reserves at its command to respond to the crisis.
Pledging he would do whatever was necessary to keep the financial system functioning, President Dmitry
Medvedev, a Putin protégé in power since 2008, injected nearly $200 billion dollars into the country’s struggling
financial institutions even as he offered some $50 billion in loans to major corporations suffering from the rapid
withdrawal of foreign capital in the wake of the global financial crisis. Over the longer term, the government
attempted to revive business by dropping corporate tax rates and lifting tariffs on imported capital goods needed
by business and industry.
Despite the various bailout and stimulus measures, which amounted to some 13 percent of GDP, the country’s
bonds continued to be downgraded by foreign rating services, unemployment rose to about 12 percent in 2009,
and growth was expected to go into negative territory for the first time since the crisis of 1998. By mid-2009,
however, many economists had come to believe that Russia, like quite a few other emerging economies, was
poised to recover ahead of many Western countries. It did, but its growth remained behind that of other emerging
economies, with GDP rising just 4 percent in 2010; higher oil prices were likely to increase the rate in 2011.
Meanwhile, the official unemployment rate had fallen significantly in 2010, to just 7.5 percent.
James Ciment and Alexander V. Laskin
 
See also:  BRIC (Brazil, Russia, India, China);  Eastern Europe;  Emerging Markets;  Marx, Karl; 
Transition Economies. 
Further Reading
Buchs, T.D.  “Financial Crisis in the Russian Federation: Are the Russians Learning to Tango?” Economics of Transition 7:3
(2003): 687–715. 
Longworth, Philip. Russia: The Once and Future Empire from Pre-History to Putin. New York: St. Martin’s, 2006. 
Malleret, T., N. Orlova, and V. Romanov.  “What Loaded and Triggered the Russian Crisis?” Post-Soviet Affairs 15:2
(1999): 107–129. 
Popov, A.  “Lessons of the Currency Crisis in Russia and in Other Countries.” Problems of Economic Transition 43:1
(2000): 45–73. 
Service, Robert. A History of Modern Russia: From Tsarism to the Twenty-first Century. Cambridge, MA: Harvard University
Press, 2009. 
S&P 500
 
The S&P 500 is a stock market index that measures the prices of 500 representative large companies and has
served as an important index of stock prices in the United States since 1957. It is also one of the most important
indicators of business cycle expansions and contractions. The Dow Jones Industrial Average (DJIA), another

popular index, contains only thirty companies and so is considered less representative of the U.S. market than the
S&P 500. The S&P 500 is among the best known of many indices and is owned and maintained by Standard &
Poor’s, a financial research firm whose parent company is McGraw-Hill.
S&P 500 refers not only to the index but also to the 500 companies that have their common stock included in the
index. The ticker symbol for the S&P 500 Index varies. Some examples of the symbol are ^GSPC,.INX, and
$SPX. The stocks included in the S&P 500 Index are also part of the broader S&P 1500 and S&P Global 1200
stock market indices. Other popular Standard & Poor’s indices include the S&P 600, an index of small companies
with a market capitalization between $300 million and $2 billion, and the S&P 400, an index of midsize companies
with market capitalization of $2 billion to $10 billion.
The S&P 500 indexes the prices of 500 American common stocks for large publicly held companies that trade on
either of the two largest American stock markets—the New York Stock Exchange (NYSE) and the National
Association of Securities Dealers Automated Quotations (NASDAQ). The companies are chosen from leading
industries within the U.S. economy, including utilities, construction, energy, health care, industrials, materials,
information technology, and telecom services. The index does include a handful of non-U.S. companies for
various reasons.
The S&P 500 is widely employed in the financial services industry as a measure of the general level of stock
prices because it includes both growth stocks and the generally less volatile value stocks. The index is one of the
most commonly used benchmarks for the overall U.S. stock market and is considered by many to be the very
definition of the market. It is included in the Index of Leading Economic Indicators (LEI), which predicts future
economic activity. The S&P 500 is often used as a baseline for comparisons in stock performance calculations
and charts. A chart will show the S&P 500 Index in addition to the price of the target stock.
The S&P 500 is maintained by the S&P Index Committee, a team of Standard & Poor’s economists and index
analysts who meet on a regular basis. The Index Committee’s goal is to ensure that the S&P 500 remains a
leading indicator of U.S. equities, accurately reflecting the risk and return characteristics of the U.S. market as a
whole on an ongoing basis. The Index Committee follows a set of established guidelines (available at
standardandpoors.com) that provide transparency and fairness needed to enable investors to replicate the index
and achieve the same performance as the S&P 500.
Standard & Poor’s introduced its first index in 1923 as the S&P 90, an index based on ninety stocks. By linking
this index to the S&P 500 Index, the latter can be extended back for comparison purposes to 1918. In 2000, the
index reached an all-time same-day high of 1,552.87 and then lost approximately half of its value in a two-year
bear market, reaching a low of 768.63 in 2002. In 2007, the S&P 500 closed at 1,530.23 to set its first all-time
closing high in more than seven years and a new same-day record of 1,555.10. In late 2007, difficulties stemming
from subprime mortgage lending began to spread to the wider financial sector, resulting in the second bear market
of the twenty-first century. The resulting crisis became acute in September 2008, commencing a period of unusual
volatility and a significant downturn. The index bottomed out in early March 2009, closing at 735.09—its lowest
close since late 1996. The loss in 2008 was the greatest since 1931. Beginning in March 2009, the index began
to recover, reaching 1,045.41 on November 3, 2009—still far below its previous peak in October 2007.
Maria Nathan
 
See also:  Dow Jones Industrial Average;  Nasdaq;  New York Stock Exchange. 
Further Reading
Sommer, Jeff.  “A Friday Rally Can’t Save the Week.” New York Times, November 23, 2008. 
Standard & Poor’s.  “Indices.” www.indices.standardandpoors.com
Standard & Poor’s 500 Guide.
Standard & Poor’s.
New York: McGraw-Hill, 2007.

 
 
 
 
 
 
Samuelson, Paul (1915–2009)
 
One of the towering figures in twentieth-century economics, the Nobel Prize laureate and neo-Keynesian Paul
Samuelson pioneered the study and application of international trade theory, business cycle analysis, and
equilibrium theory.
Paul Anthony Samuelson was born on May 15, 1915, in Gary, Indiana. He received his undergraduate degree
from the University of Chicago in 1935 and earned master’s and doctorate degrees from Harvard in 1936 and
1941, respectively. At Harvard, Samuelson studied with such Keynesian luminaries as Alvin Hansen and Joseph
Schumpeter, both of whom were involved in the study of business cycles. Samuelson’s own work went beyond
Keynesianism, combining principles from that school with others from neoclassical economics to form what
became known as the neoclassical synthesis. In awarding him the Nobel Prize for Economics in 1970, the Nobel
committee declared, “He has shown the fundamental unity... in economics, by a systematic application of the
methodology of maximization for a broad set of problems.” Indeed, Samuelson’s career has been characterized by
the broad reach and influence of his work, which encompasses many different subfields of the science.
Specifically, the Nobel was awarded “for the scientific work through which he has developed static and dynamic
economic theory and actively contributed to raising the level of analysis in economic science.”
In 1940, Samuelson joined the faculty at the Massachusetts Institute of Technology (MIT) as an assistant
professor of economics; he became an associate professor in 1944 and a full professor in 1947. During his tenure
at MIT, Samuelson also served as professor of international economic relations at the Fletcher School of Law and
Diplomacy at Tufts University; as a consultant to the Rand Corporation, a nonprofit think tank; and as an
economics adviser to the U.S. government. Beginning in 1966, he was an institute professor emeritus at MIT. In
addition to the Nobel Prize, Samuelson was awarded the John Bates Clark Medal by the American Economics
Association in 1947.
Samuelson’s first major work, Foundations of Economic Analysis (1947), grew out of his doctoral dissertation of
the same title. Its publication led to a renewed interest in neoclassical economics, exploring the theories underlying
such critical areas of study as equilibrium systems, maximizing behavior of agents, comparative statistics, cost and
production, consumer behavior, elasticities, cardinal utility, welfare economics, linear and nonlinear systems, and
dynamics—such as those associated with the business cycle. In Foundations, Samuelson emphasizes the
mathematical underpinnings of economics and the formal similarity of analyses regardless of the subject. He
draws a number of comparisons between economics and the mathematics of other sciences such as biology and
physics, particularly thermodynamics. Even his focus on equilibrium is the result of taking the mathematical models
from the field of physical thermodynamics and generalizing them for use in economics.
In 1948, Samuelson published Economics: An Introductory Analysis, which became the best-selling textbook in

the United States—in any subject—for nearly thirty years. Published in over forty languages and twenty English-
language editions, the text remains in wide use into the twenty-first century. (The twelfth edition, published in
1985, and subsequent editions were written with Yale economist William D. Nordhaus.)
A longtime MIT professor and government adviser, economist Paul A. Samuelson won the 1970 Nobel Prize and
wrote the best-selling college textbook in the field. He was cited by the Nobel committee for “raising the level of
analysis in economic science.” (Yale Joel/Time & Life Pictures/Getty Images)
More than a synthesizer, popularizer, and teacher, Samuelson was also an economic theorist of the first order.
Among the concepts for which he is known is the Samuelson condition, introduced in a 1954 article titled “The
Theory of Public Expenditure.” The construct and its underlying theory help explain why social utility will decrease
if a certain quantity of private good is substituted for public good. Another of his theories, the Balassa-Samuelson
hypothesis, is the causal model used to explain the Penn effect, a theory developed by economists at the
University of Pennsylvania. Specifically, the Balassa-Samuelson hypothesis predicts that consumer prices will be
higher in wealthy countries than in poor ones because productivity varies most in the traded-goods sector. The
Stolper-Samuelson theorem, originally derived by Samuelson and Harvard economist Wolfgang Stolper from the
Heckscher-Ohlin model of international trade, predicts that a rise in the relative price of goods will lead to a rise in
the return to the most-used factor in the production of those goods—a theory later derived using other models.
Samuelson continued to write and edit works on economics into his nineties. With William A. Barnett, he edited
Inside the Economist’s Mind: Conversations with Eminent Economists (2007). During the financial crisis and
economic recession of 2007–2009, he remained an active media commentator on public policy and prospects for
recovery. Samuelson died on December 6, 2009, at his home in Belmont, Massachusetts.

Bill Kte’pi
 
See also:  Friedman, Milton;  Hansen, Alvin Harvey;  Keynes, John Maynard;  Neoclassical
Theories and Models;  Schumpeter, Joseph. 
Further Reading
Samuelson, Paul. Foundations of Economic Analysis.  Enlarged ed. Cambridge, MA: Harvard University Press, 1983. 
Samuelson, Paul.  “The Pure Theory of Public Expenditure.” Review of Economics and Statistics 36(1954): 386–389. 
Soloki, E. Cary, and Robert M. Brown. Paul Samuelson and Modern Economic Theory. New York: McGraw-Hill, 1983. 
Szenberg, Michael, Lall Ramrattan, and Aron A. Gottesman, eds. Samuelsonian Economics and the Twenty-First
Century. New York: Oxford University Press, 2006. 
Wong, Stanley. Foundations of Paul Samuelson’s Revealed Preference Theory. New York: Routledge, 2009. 
Savings and Investment
 
In economics, “savings” is defined as the income that is not spent on consumption. “Investment” most often refers
to business investment—expenditures by businesses on capital goods such as equipment and tools, factories,
warehouses, and offices necessary to operate the business. “Total investment” also includes expenditures for new
housing construction. All of these investments contribute to gross domestic product (GDP) because new goods are
produced. “Financial investment” refers to something closer to the everyday use of the term “investment” and
includes the purchase of paper instruments such as stocks and bonds, which are not counted in GDP because
they produce no new goods or services. The interaction of savings and investment—and the decisions economic
players make concerning that interaction—is at the core of significant aspects of economic theory.
 Classical, Stockholm School, and Keynesian Views
Adam Smith, in The Wealth of Nations (1776), linked savings and investment in a theory of national economic
growth. According to Smith, economic growth took place as a result of the parsimonious behavior of entrepreneurs
(business owners) who saved out of their business profits to invest back into the businesses they owned.
However, as the market system evolved, it became clear that this picture of the process of capital accumulation
was too simplistic. Questions arose about the process by which savings of individuals and institutions neatly
flowed into business investment. Classical economists Lord Lauderdale, Thomas Malthus, and Jean Charles
Léonard Sismondi asked, what would happen if savings and investment were not exactly in balance with one
another? In this framework, would the interest rate move so as to adjust savings and investment into a balance, or
equilibrium? Suppose there were a greater amount of investment than there were savings to facilitate it? Then
interest rates would be increased due to the demand for investment funds. As interest rates began to go up,
savers would be attracted to the higher interest rates that could be earned on savings. On the other hand, if there
were a surplus of savings compared to investment, interest rates would decrease. As a consequence of lower
interest rates, more investment would be undertaken, and at lower interest rates savers would not save as much.
The change in the level of interest rates would cause an adjustment, bringing savings and investment into
equilibrium with one another.

This analysis was eventually labeled the loanable funds approach to savings and investment equilibrium. At the
start of the twentieth century, the founder of the Stockholm school, Knut Wicksell, raised a major theoretical
objection to the theory that the interest rate always brought savings and investment into balance. Wicksell’s
approach pointed out that investment by business firms was financed not just by savings but by bank credit
extended through loans to business. It is possible that credit will expand to finance investment beyond the level of
savings in the economy.
John Maynard Keynes built on the possibility of disengagement between investment and savings to explain the
depression of the 1930s and cyclical movement of economics in general. Keynes’s theory of the business cycle is
based on the inherent instability of investment decisions in the private business sector due to the uncertainty and
risk in investment expectations. The critical theoretical element in the Keynesian approach was that planned or
anticipated investment and savings would not necessarily be in balance with each other. Decisions about business
investment, said Keynes, are made by business owners or managers and are based on expectations about future
profits that depend on a set of rational calculations about individual business investment as well as the mood of
the business community. This mood could be affected by optimistic or pessimistic conditions in the stock market,
so that a major downturn in the stock market, or some other financial market, could create a mood of pessimism
in the business community, which might influence, or even offset, the rational calculations about expenditures on
business investment. On the other hand, households make savings decisions based on the level of income and
the propensity to save or to consume. Therefore, there is no particular reason why the consequences of these two
sets of decisions would coincide.
Keynes and others pointed out that after the fact, savings and investment would eventually adjust to a balance, or
equilibrium. However, this equilibrium could be at a level of national income or national output less than the full
employment of labor. The Great Depression of the 1930s was a clear example of a decade-long national income
equilibrium at less than full employment.
 Savings and Investment in a Globalized Economy
In the modern global economy, analysis of aggregate savings or aggregate investment is much less constrained
by national political or economic boundaries. In particular, there is now a much freer flow of savings and financial
investment across national boundaries through international financial markets. Therefore, the analyses of savings
and investment must be understood as looking at flows of global savings or global investment. For example, in the
middle of the first decade of the twenty-first century, the U.S. financial markets depended upon savings from the
rest of the world, especially Asia. Historically, this reliance on global factors has been true in earlier periods; a
good deal of the industrial expansion in the United States was financed by European savings, but now
investments move much more easily and much faster across national boundaries. The newly available global flow
of savings became a source for bubbles in global financial markets when the search for investment opportunities
led to overinvestment in speculative securities.
The flow of savings from those economies with high levels of aggregate savings is usually directed at relatively
safe financial markets. This is particularly true when there is a diversity of financial investment opportunities with
high rates of return. The U.S. economy provided these opportunities, and continues to do so, in what is considered
the safest and one of the most innovative financial markets. One of the innovations in the last quarter-century was
the creation of derivatives from bonds. These were new securities based on characteristics of bonds, such as
interest-only or principal-only derivatives. It was the creation of derivatives connected to home mortgage that
stimulated interest early in the twenty-first century. The derivatives were bundles of mortgages divided up into
high-, middle-, and low-risk categories that were then rated by private sector bond-rating agencies. Savings from
the United States and outside the United States flowed through a variety of financial institutions into these
derivatives at the start of the twenty-first century. This was the foundation of the financial bubble that burst in
2007–2008. The bursting of this bubble happened as a result of underlying weaknesses in the mortgage market
and the level of speculation in these derivatives. As a consequence, a worldwide financial panic ensued, and the
U.S. and global economies moved into the Great Recession of 2007–2009.

William Ganley
 
See also:  Banks, Commercial;  Banks, Investment;  Consumption;  Interest Rates; 
Investment, Financial;  Retirement Instruments. 
Further Reading
Akerlof, George A., and Robert J. Schiller. Animal Spirits. Princeton, NJ: Princeton University Press, 2009. 
Cooper, George. The Origins of Financial Crises: Central Banks, Credit Bubbles and the Efficient Market Fallacy. New
York: Vintage, 2008. 
Fox, Justin. The Myth of the Rational Markets: A History of Risk, Reward, and Delusion on Wall Street. New
York: HarperBusiness, 2009. 
Hunt, E.K. History of Economic Thought.  Updated 2nd ed. Armonk, NY: M.E. Sharpe, 2002. 
Kates, Steven. Say’s Law and the Keynesian Revolution. Cheltenham, UK: Edward Elgar, 1998. 
Keynes, John M. The General Theory of Employment, Interest and Money. New York: Macmillan, 1936. 
Minsky, Hyman. Can “It” Happen Again? Armonk, NY: M.E. Sharpe, 1982. 
Roncaglia, Alessandro. The Wealth of Ideas. New York: Cambridge University Press, 2005. 
Skidelsky, Robert. Keynes: The Return of the Master. New York: Public Affairs, 2009. 
Snowdon, Brian, and Howard R. Vane. Modern Macroeconomics. Cheltenham, UK: Edward Elgar, 2005. 
Stiglitz, Joseph. Free Fall: America, Free Markets and the Sinking of the World Economy. New York: W.W. Norton, 2010. 
Tily, Geoff. Keynes’s General Theory, the Rate of Interest and ‘Keynesian’ Economics. New York: Palgrave
Macmillan, 2007. 
Zamagni, Stefano, and Ernesto Screpanti. An Outline of the History of Economic Thought. New York: Oxford University
Press, 1993. 
 
Savings and Loan Crises (1980s–1990s)
 
Beginning in the mid-1980s and continuing into the early 1990s, the U.S. savings and loan industry experienced a
wave of bankruptcies that produced tens of billions of dollars in losses. Triggered by deregulation and excessive
real-estate speculation, the crises prompted a massive federal government bailout program in the late 1980s and
early 1990s. Some economists have argued that the bailout program prompted the much greater speculative real-
estate financing of the early and middle 2000s—a major cause of the financial crisis of the late 2000s—as the

financial community came to believe that, should such speculation lead to major losses, it would once again be
rescued by Washington.
 History of S&Ls
Savings and loan associations—also known as S&Ls or thrifts—emerged out of the mutual aid societies
established by working people and small business owners in the early and middle years of the nineteenth century.
These were cooperatives in which people pooled their money and then lent it to each other, at relatively low
interest, usually for extraordinary expenses, such as funerals, or to establish or expand businesses.
From the beginning, S&Ls differed from banks in a number of ways. They were smaller and not-for-profit; they
emphasized long-term savings accounts over short-term checking accounts; and their owners (the depositors) and
management identified not with the financial industry but with the various reform movements of the day. In other
words, the primary aim of S&Ls was to provide services and financial help to depositors rather than profits for
shareholders. Because they had no profits and offered limited services, S&Ls could often pay more in interest,
thereby attracting more depositors.
S&Ls also differed in the kinds of home mortgages they offered. While banks and insurance companies—a major
source of home financing through the early twentieth century—offered short-term mortgages (in which borrowers
paid interest for a few years before having to make a balloon payment to cover the principle at the end of the
mortgage), thrifts pioneered the amortizing mortgage (in which borrowers paid both principle and interest over a
longer term, with no balloon payment at the end). By the late 1920s, S&Ls had become an attractive alternative
for homebuyers, financing about one in five mortgages in America.
Being more conservative than banks in the kinds of investments they made, thrifts suffered less from the Great
Depression of the 1930s, though many struggled as foreclosures mounted. In the early part of that decade, the
federal government initiated several programs to aid the S&L industry, including the Federal Home Loan Bank to
offer loans to struggling thrifts; federal charters; and the Federal Savings and Loan Insurance Corporation
(FSLIC), which protected S&L depositors in the same way the Federal Deposit Insurance Corporation (FDIC) did
bank depositors.
During the post–World War II boom of the late 1940s through the early 1970s, the S&L industry thrived, now
highly regulated by the states and the federal government. Millions of suburbanizing Americans deposited their
money in S&Ls and bought homes with S&L-offered mortgages. By 1974, there were more than 5,000 S&Ls
nationally, holding assets worth nearly $300 billion.
 Deregulation and Crisis
High interest rates and slow growth in the recession-wracked middle and late 1970s hit the S&L industry hard.
Tightly regulated regarding the interest rates they could charge, S&Ls saw depositors pull out their money in
increasing volume and place it with financial institutions that could pay higher returns. Meanwhile, those same high
interest rates were cooling the housing market, cutting off the major source of income—mortgage interest—that
S&Ls needed in order to survive.
In the more conservative political climate of the early 1980s, the industry’s pleas for regulatory relief fell on
receptive ears, with Congress passing two laws allowing thrifts to offer different kinds of savings accounts and to
expand the kinds of loans they offered. The laws also increased the maximum amount of depositors’ money
insured by the FSLIC to $100,000 per account and eliminated restrictions on the minimum number of S&L
stockholders; the latter law concentrated decision-making power in a smaller number of people.
Collectively, these changes led S&Ls to engage in more lucrative but riskier lending practices. While continuing to
offer mortgages on single-family homes, many S&Ls branched out into financing commercial properties, including
shopping centers, resorts, and condominium complexes. To obtain the funds for these loans, the S&Ls began to

offer much higher interest rates, paid for by the returns on commercial property loans. Despite the higher risks
associated with this kind of speculative financing, depositors continued to put their money in S&L accounts. With
the new $100,000 limit on FSLIC protection, making deposits felt safer for those who could afford it, knowing that
the federal government would bail them out should the institution fail.
While such practices were perfectly legal in the new deregulated S&L industry, some thrift owners took their
lending practices beyond the law. Among these was Charles Keating, Jr., a real-estate developer and financier
who had purchased Lincoln Savings and Loan of California in 1984. When federal regulators began to look into
Lincoln’s lending practices, Keating turned to five U.S. senators—to whom he had made substantive campaign
contributions—to put pressure on the investigators to call off their probe. Ultimately, Lincoln failed, costing
taxpayers $2 billion, and the senators were either criticized or reprimanded by a Senate investigatory committee
for their participation in the scandal.
The 1989 failure of California-based Lincoln Savings and Loan, part of a wave of S&L bankruptcies, cost
taxpayers $2 billion. Financial contributions to five U.S. senators by Lincoln owner Charles Keating touched off a
political scandal. (Patrick Tehan/Time & Life Pictures/Getty Images)
By the late 1980s, so many S&Ls were going into bankruptcy that the industry itself was becoming tainted. Many
depositors pulled their money out of any thrifts even remotely suspected of being in trouble and thereby triggered
more failures. Not only was the number of S&Ls going bankrupt increasing, but so was the size of the institutions.
Indeed, by 1987, the FSLIC had become insolvent. Congress authorized more money, but it waived rules on
closing technically insolvent S&Ls in the hopes that they could get back on their feet and save the FSLIC money.
However, this only delayed and increased the size of the insolvencies in future years. Meanwhile, the rate of S&L
failures soared, from about 60 annually in the mid-1980s to 205 in 1988 and 327 in 1989; in 1989, the total

insured assets of failed S&Ls amounted to $135 billion.
 Government Response and Legacy
In response to the crisis, Congress passed the Financial Institutions Reform Recovery and Enforcement Act
(FIRREA) in 1989, which provided tens of billions of dollars in bailout money to S&Ls and created a new Office of
Thrift Supervision (OTS) to impose tighter regulations. Among these was a requirement that the thrifts maintain
higher asset-to-loan ratios. FIRREA also created another government-authorized institution, the Resolution Trust
Corporation, to dispose of the assets of the failed S&Ls in order to recoup some of the bailout money.
By the early 1990s, the federal initiatives seemed to be working, as the number of S&L failures returned to historic
norms. But $600 billion in lost assets in the period between the deregulation of the early 1980s and the end of the
crisis in the early 1990s represented the costliest financial collapse in the history of any nation to that time. In the
end, the S&L debacle cost America taxpayers some $500 billion, also setting a record as the greatest financial
bailout in history to that time.
While the newly regulated S&L industry largely avoided investments in the exotic, lucrative, and highly risky
derivatives markets of the early 2000s, the bailouts of the 1980s– 1990s era encouraged other financial
institutions to abandon more cautious lending practices. Those bailouts may have led many in the financial
services industry to believe that the government would bail them out in the event of any failures. Meanwhile, the
OTS, which oversaw regulation of many of the mortgage-backed securities at the heart of the late 2000s financial
crisis, had become so lax in its oversight that the Barack Obama administration called for its abolition, with its
duties consolidated into a new national bank supervisory agency as part of its 2009 reform package for the
financial services industry.
James Ciment and Bill Kte’pi
 
See also:  Community Reinvestment Act (1977);  Depository Institutions;  Mortgage,
Commercial/Industrial;  Mortgage Markets and Mortgage Rates;  Real-Estate Speculation; 
Regulation, Financial. 
Further Reading
Black, William K. The Best Way to Rob a Bank Is to Own One. Austin: University of Texas Press, 2005. 
Lowy, Michael. High Rollers: Inside the Savings and Loan Debacle. New York: Praeger, 1991. 
Mayer, Martin. The Greatest-Ever Bank Robbery: The Collapse of the Savings and Loan Industry. New
York: Scribner, 1992. 
Pizzo, Steven, Mary Fricker, and Paul Muolo. Inside Job: The Looting of America’s Savings and Loans. New York: McGraw-
Hill, 1989. 
Robinson, Michael A. Overdrawn: The Bailout of American Savings. New York: Dutton, 1990. 
White, Lawrence J. The S&L Debacle: Public Policy Lessons for Bank and Thrift Regulation. New York: Oxford University
Press, 1991. 

 
Schumpeter, Joseph (1883–1950)
 
Best known for his work on the relationship between technology, entrepreneurialism, and long-term economic
development—as well as for popularizing the phrase “creative destruction” to describe technological innovation’s
transformative effect on capitalist economies—Joseph Alois Schumpeter was a Czech-born, Austrian-educated
economist whose most important work dates from the first half of the twentieth century. Among his most influential
books are Business Cycles (1939), which examined the relationship between short-term and long-term economic
cycles, and Capitalism, Socialism, and Democracy (1942), in which he posited, in a variation on Karl Marx’s ideas,
that capitalism sowed the seeds of its own destruction and would be supplanted, albeit in evolutionary rather than
revolutionary fashion, by socialism.
Austrian-American economist Joseph Schumpeter maintained that fluctuations in the business cycle are

consequences of the growth process, which is driven by innovation, “creative destruction,” and new innovation in
products and production processes. (Imagno/Hulton Archive/Getty Images)
Born on February 8, 1883, into a well-off textile manufacturing family in what was then the Austro-Hungarian
Empire, Schumpeter studied law and economics at the University of Vienna, receiving his PhD in the latter, in
1906. After a short time abroad, where, among other things, he served as financial manager to an Egyptian
princess, Schumpeter returned to the empire. For the next eleven years, he alternated between positions in
academia and government. Shortly after World War I, he served as minister of finance. In 1920, he left to become
president of the Biedermann Bank, until its collapse—a result of excessive speculation—and his own personal
bankruptcy forced him to return to teaching. From 1925 to 1932, he served as a professor of economics at the
University of Bonn, Germany, until the rise of the Nazis forced him to emigrate. Moving to the United States, he
took a teaching position at Harvard University, where he remained until a year before his death in 1950.
Schumpeter’s first significant foray into the relationship between entrepreneurialism, technology, and economic
growth came in his 1911 book, Theory of Economic Development, in which he outlined what would become his
signature idea—that entrepreneurs are the major agents of economic change. He began the book by reexamining
late-nineteenth-and early-twentieth-century neoclassical economist Léon Walras’s general equilibrium theory in
which, simply put, supply and demand roughly balance one another, producing economic stasis or gradual
incremental growth. Schumpeter argued that entrepreneurs periodically upset this equilibrium. By introducing new
technologies and/or new business methods, they became the prime agents of economic change and development.
As for innovation itself, Schumpeter saw it as a creative activity, an act of genius that eludes economic
understanding and analysis. He also accorded a prominent role in this process to the financial sector, which
provides the credit necessary for the entrepreneur to realize his or her vision.
At the same time, Schumpeter identified a downside to this process. As more entrepreneurs seek to exploit
technological innovations, profit margins shrink and eventually disappear, leading to widespread bankruptcy, the
drying up of bank credit, unemployment, and ultimately, economic recession or depression. This contraction,
Schumpeter said, is a necessary evil, as it eliminates less efficient, less innovative business firms from the
system. Once that shaking-out process takes place, the cleverest entrepreneurs can again obtain the capital they
need to exploit new innovations, renewing the dynamic character of capitalism. Schumpeter would later borrow
and popularize the term “creative destruction”—originally coined by German economist Werner Sombart in 1913—
to describe this process.
Given his interest in innovation and economic change, it is no surprise that Schumpeter also had an interest in the
economics, econometrics, and historical properties of business cycles, a subject he turned to in earnest with his
1939 book Business Cycles. In this work, Schumpeter attempted to bring together cycles, or waves, of three
different lengths—each named after the economist who first described them—in a synthesis that would explain the
basic ebb and flow of capitalist economic growth, stagnation, and contraction: the Kitchin cycle (3-to-4-year cycles
revolving around the accumulation and reduction of business inventories, usually involving consumer goods); the
Juglar cycle (concerning business investment and capital equipment, lasting 8 to 11 years); and Kondratieff cycles
(long-term cycles of between 45 and 60 years, in which the adoption of major technological innovation plays a key
role).
In his last decade, Schumpeter broadened his perspective, incorporating a wider social, political, and historical
context to his economic analysis. In his widely read Capitalism, Socialism, and Democracy (1942), he moved
beyond the study of business cycles, combining sociology, politics, and economics to examine the future of
capitalism itself. Like Marx, he concluded that capitalism contains the seeds of its own destruction and will
eventually evolve into socialism—not because of its failures but because of its successes. Rather than being
destroyed by the social unrest of the exploited working class, the transformation would come about because of
capitalism’s efficiencies and enormous capacity for wealth creation. First, through the process of “creative
destruction” inherent within the business cycle, larger risk-averse firms with their heavy managerial bureaucracy
would come to replace innovative and risk-taking smaller firms. Second, the increased income generated by

capitalism would, he argued, swell the ranks of the educated middle class—a cohort that tended to be highly
critical of laissez-faire capitalism and that favored more government bureaucracy and regulation of the economy.
Both forces, then, would restrict the capacity of entrepreneurs to guide capitalism’s future, leading to economic
stasis and eventually to socialism.
Also occupying the last years of his life was Schumpeter’s study of the history of economic thought, published in
two well-received posthumous publications—Ten Great Economists (1951) and the magisterial, if unfinished,
History of Economic Analysis (1954).
In the years following his death, Schumpeter came to be recognized as one of the most influential, if somewhat
iconoclastic, figures in the history of economics. Indeed, scholars cite him as the originator of an entirely
independent school of economic thought—alongside the classical, Keynesian, and neoclassical schools—which
gives primacy to technological, institutional, organization, and social innovation in determining economic trends
and cycles and how capitalism itself operates. As the enormous forces of technological change, financial
innovation, and economic restructuring have continued to propel and buffet the economy in the late twentieth and
early twenty-first centuries, Schumpeter’s shadow has only grown longer. As Lawrence Summers, director of the
National Economic Council under President Obama, suggested in the early 2000s, “the economy of the future is
likely to be ‘Schumpeterian.’”
Christopher Godden and James Ciment
 
See also:  Austrian School;  Creative Destruction;  Technological Innovation. 
Further Reading
Allen, Robert Loring. Opening Doors: The Life and Work of Joseph Schumpeter.  2 vols. New Brunswick,
NJ: Transaction, 1991. 
Heertje, Arnold. Schumpeter on the Economics of Innovation and the Development of Capitalism. Cheltenham, UK: Edward
Elgar, 2006. 
McCraw, Thomas K. Prophet of Innovation: Joseph Schumpeter and Creative Destruction. Cambridge, MA: Harvard
University Press, 2007. 
Schumpeter, Joseph. Business Cycles: A Theoretical, Historical and Statistical Analysis of the Capitalist Process.  2 vols.
New York: McGraw-Hill, 1939. 
Schumpeter, Joseph. Capitalism, Socialism and Democracy. New York: Harper, 1942. 
Schumpeter, Joseph. History of Economic Analysis. London: George Allen & Unwin, 1954. 
Swedberg, Richard. Schumpeter: A Biography. Princeton, NJ: Princeton University Press, 1991. 
Schwartz, Anna (1915–)
 
An economist at the National Bureau of Economic Research (NBER) since 1941, Anna Jacobson Schwartz has
had a long and remarkable career as a monetary theorist. In 1963, she cowrote, with economist Milton Friedman,
A Monetary History of the United States, 1867–1960, which remains one of the most important works in economic

history and in the grand debate between Keynesians and monetarists on macroeconomics. A carefully researched
study of the role of monetary institutions in booms and busts in the U.S. economy, it is as relevant as ever in the
twenty-first century.
Anna Jacobson was born on November 11, 1915, in New York City, to Hillel and Pauline Shainmark Jacobson.
She earned a bachelor’s degree in 1934 from Barnard College and a master’s degree in 1935 from Columbia
University, where, in 1964, she also was awarded a doctorate in economics. In 1936, she married Isaac Schwartz,
whom she had met at a high school Hebrew camp; they would have four children.
After a brief stint at Columbia University’s Social Science Research Council, Schwartz began her lifelong career at
the NBER in 1941. In addition to her position at the bureau, she has been an adjunct faculty member at both the
City University of New York and New York University since 1964 and has served on the editorial boards of such
prominent journals as the American Economic Review, the Journal of Money, Credit, and Banking, and the Journal
of Monetary Economics.
During her distinguished career, Schwartz has authored or coauthored several books and dozens of articles in
leading economics journals. In addition to A Monetary History, her most important contributions to economics
include two collaborative projects with Friedman—Monetary Statistics of the United States (1970), and Monetary
Trends in the United States and the United Kingdom: Their Relation to Income, Prices, and Interest Rates, 1867–
1975 (1982)—as well as Growth and Fluctuations in the British Economy, 1790–1850: An Historical, Statistical,
and Theoretical Study of Britain’s Economic Development (1953), which she cowrote with Arthur Gayer and Walt
Whitman Rostow. Together these works demonstrate Schwartz’s skill as an economic historian and empiricist. Her
collaborations with Friedman were groundbreaking in their use and presentation of data to combat Keynesianism.
While Schwartz is best remembered for her work with Friedman, she also has made important contributions to
financial market regulations, monetary policy debates, and business-cycle theory. Her research, largely historical
in nature, led to a shift in monetary policy. Thanks to Schwartz and Friedman, monetary policy makers began to
concentrate more on price stability and less on the management of other macroeconomic variables.
Schwartz’s influence in the field of economics continues in the 2000s. A Monetary History is required reading in
many graduate macroeconomics and money and banking courses, and her writings on price stability and financial
institutions have influenced policy makers in the United States and around the world. During the financial
meltdown of 2007–2008, Schwartz took Federal Reserve (Fed) leaders—Chair Ben Bernanke, in particular—to
task for not doing their jobs to alleviate the crisis. In a 2008 Wall Street Journal article, she asserted that the
“credit market disturbance” was not the result of a lack of money to lend but rather a lack of “faith in the ability of
borrowers to repay their debts.” She faulted the Fed and the U.S. Treasury Department for “recapitalizing firms
that should be shut down,” asserting that “firms that made wrong decisions should fail.” In a New York Times op-
ed piece a year later, she urged President Barack Obama to choose an economist other than Bernanke as
Federal Reserve chair, accusing Bernanke of having “committed serious sins of commission and omission” in his
failure to convince the faltering markets that the Fed had a plan to help turn the ailing economy around.
Scott Beaulier and Joshua Hall
 
See also:  Friedman, Milton;  Monetary Policy;  Monetary Stability;  Monetary Theories and
Models. 
Further Reading
Carney, Brian M.  “Bernanke Is Fighting the Last War.” Wall Street Journal, October 18, 2008. 
Feldstein, Martin.  “Anna Schwartz at the National Bureau of Economic Research.” Journal of Financial Services
Research 18:2/3 (2000): 115–117. 

Friedman, Milton, and Anna Jacobson Schwartz. A Monetary History of the United States, 1867–1960. Princeton,
NJ: Princeton University Press, 1963. 
Friedman, Milton, and Anna Jacobson Schwartz. Monetary Statistics of the United States. New York: Columbia University
Press, 1970. 
Friedman, Milton, and Anna Jacobson Schwartz. Monetary Trends in the United States and the United Kingdom: Their
Relation to Income, Prices, and Interest Rates, 1867–1975. Chicago: University of Chicago Press, 1982. 
Schwartz, Anna Jacobson.  “Man Without a Plan.” New York Times, July 25, 2009. 
Seasonal Cycles
 
Seasonal cycles are short-term business cycles that occur within the course of a year and are determined by such
factors as weather, social phenomena, and cultural events. Weather affects annual swings in agricultural activities,
with peak levels of activity occurring at planting in the spring and harvesting in the fall. In certain parts of the
United States where favorable climate and availability of irrigation are the norm, including California and the
Southwest, agricultural cycles can be repeated a number of times during the year. Weather also has a major
impact on construction cycles in most regions of the country. Winter is a slow time when low temperatures
prevent the application and curing of concrete, and precipitation makes it difficult or impossible to undertake many
outdoor tasks involved in the erection of residential and commercial structures and in the construction of roads
and bridges. Weather is critical to outdoor recreation activities and their associated economic activities, such as
the accommodation and restaurant industries. Favorable winter weather (adequate snow pack for skiing) or sunny,
warm days of summer are important to the economic vitality of firms in these industries.
Social and cultural activities also drive seasonal production cycles. Christmas has a major effect on retail sales
and the vacation travel industry. Bookstores, for example, make the majority of their annual sales in November
and December. Thanksgiving creates a surge in retail sales related to the preparation of food to celebrate the
event. Super Bowl Sunday is a major boost to the food and beverage industries. Memorial Day weekend is a big
travel time for families to visit friends and relatives. All that traveling creates a surge in gasoline consumption,
accommodations, and restaurant activities.
 Economic Implications
Seasonal cycles matter a great deal to the economy for a variety of reasons. First, marketing and sales plans
must consider expected seasonal patterns in consumer purchases, and must be able to forecast these trends.
State and local governments need to understand the ebb and flow of seasonal cycles in order to plan for
government services such as police and emergency services, scheduling of highway maintenance, and forecasting
government revenues and expenditures, such as provision for unemployment insurance claims. The federal
government and the Federal Reserve Board have a keen interest in monitoring national economic trends during
the year in terms of overall employment growth or decline, and changes in price levels. These national agencies
must be able to distinguish between trends in employment and prices that are driven by seasonal factors, and the
shifts in longer-term fundamentals that indicate the economy is moving into a period of recession, with impending
dangers of excessive unemployment or inflation.
Seasonal cycles have an impact on levels of employment, sales, prices, and the costs of doing business. The
seasonal cycle in agriculture drives prices down at harvest time, while those prices rise as inventories are depleted

during the rest of the year. The farm community may also experience a shortage of workers that will increase
wages during harvest time. There may be a temporary, seasonal reduction in the wages of construction workers,
as well as the prices of building materials, as the weather deteriorates and the demand for workers declines. But
as good weather returns, wages and materials prices may increase as construction firms respond to an increased
demand for their services. Employment and wages in a region may decline due to purely seasonal factors. For
example, a region that has a vibrant tourist trade in the summertime might always experience a decline in
economic activity in the fall.
Information on how to understand the seasonal impacts on employment, sales, prices, and the costs of doing
business are important to decision-making in the private and public sectors. Business and governments are aware
of the seasonal cycles, but there is still a critical need to distinguish between seasonal effects and longer-term
fundamental trends. Is there a longer-term trend affecting the prices of crops produced by the farm community in
a region, or are there changes in labor market conditions (such as changes in immigration) that would attenuate or
diminish the effects of the seasonal cycle? This could have implications for crop planning and future production
decisions. Construction activity always improves in the spring, but the question for a contractor is, are the
improvements in the springtime over the past few years trending in a particular direction? A county government in
a summertime tourist region may have an ongoing concern for lower levels of employment and wages in the fall
and winter months, but the issue is whether this decline is getting less serious or more serious over time.
 Measuring the Impact of Seasonal Cycles
Many of the important measures of economic activity reported on a monthly or quarterly basis are presented on a
seasonally adjusted basis. The national unemployment rate, compiled by the U.S. Bureau of Labor Statistics
(BLS), is calculated on a seasonally adjusted basis; that is, the influence of seasonal effects is removed. For
example, the BLS’s press release indicated that the seasonally adjusted unemployment rate for February 2010
was 9.7 percent. The nonseasonally adjusted measured unemployment rate is actually 10.4 percent, the
difference being the expected seasonally higher unemployment rate for February, on average. Other important
monthly and quarterly statistical series released by the BLS on a seasonal basis include the consumer and
producer price indexes and the employment cost index. The U.S. Department of Commerce’s Bureau of Economic
Analysis (BEA) is responsible for reporting data on the national income and product accounts. Monthly and
quarterly data are provided on a seasonally adjusted basis for personal consumption expenditures (which
constitutes about 65 percent of national product), personal income, government receipts and expenditures,
business inventories, and residential and nonresidential construction.
These national economic indicators, which are seasonally adjusted, are a great help to private-and public-sector
planning. However, seasonal adjustment of time-series data is not available for all industry sectors or for all
regions. Seasonally adjusted series for unemployment are provided at the national, state, and major metropolitan
levels only, and are not available for localities such as nonmetropolitan counties. To meet their planning needs,
businesses and governments may decide to develop seasonal adjustments to important time-series data. That
information is now unavailable from government sources.
In short, seasonal cycles and data about seasonal cycles play an important, continuing role in economic
performance. The process of adjusting actual time-series data during the year to compensate for expected
seasonal swings can provide important information. This information is important for planning purposes to
distinguish between seasonal swings in economic data from more fundamental trends and events that affect the
economy.
Derek Bjonback
 
See also:  Agriculture;  Jevons, William Stanley;  Sunspot Theories. 

Further Reading
Hanke, John E., and Dean Wichern. Business Forecasting.  9th ed. Upper Saddle River, NJ: Pearson Prentice-Hall, 2005. 
Hooshmand, A. Reza. Business Forecasting: A Practical Approach.  2nd ed. New York: Routledge, 2009. 
Securities and Exchange Commission
 
The economic crisis of 2008–2009 has put the U.S. Securities and Exchange Commission (SEC) in an
unfavorable light due to the general consensus that the SEC was negligent as a supervisory agency just when it
was needed the most. The crisis thus underscores the role that the SEC is supposed to play in helping to prevent,
or at least mitigate, financial disasters that stem from fraudulent behavior on the part of public corporations and
those active in the trading of public securities.
The SEC is a U.S. government agency. On its Web site, the SEC states that its main mission is “to protect
investors, maintain fair, orderly, and efficient markets, and facilitate capital formation.” To achieve these goals, the
SEC oversees the federal securities laws, maintains the disclosure of financial information by publicly traded
companies, and brings enforcement actions against violators of the securities law. The SEC works in close
cooperation with several other U.S. government agencies, such as the Federal Reserve and the Treasury
Department.
Prior to 1933, the United States did not have comprehensive regulation of the securities markets on the federal
level. Instead, individual states were left to enact their own laws to protect their citizens against investment fraud.
These state laws were enacted in response to a growing number of fraudulent speculative schemes targeted at
the general population. The schemes were not backed up by any assets or reasonable business plans; such
fraudulent claims were thus said to have come “out of the blue sky.” Thus, these con artists were referred to as
blue-sky merchants, and the state laws protecting against these schemes came to be known as blue-sky laws.
Nevertheless, in the early 1930s, it became apparent that state laws alone could not combat securities fraud. The
development of communication and transportation networks made interstate securities trading easily accessible to
both the general public and con artists. State laws were virtually powerless and, in fact, did not even have legal
standing against interstate fraud. It soon became clear that the federal government needed to step in, and
Congress did so by passing the Securities Act of 1933, which required any original interstate sale or offer of
securities to be registered and to meet certain disclosure requirements. Subsequently, Congress passed the
Securities Exchange Act of 1934, aimed at regulating secondary market trading of securities. As part of these
federal regulations, the SEC was created.
 Structure of the SEC
The SEC is headed by five commissioners, who are appointed by the president of the United States for five-year
terms. The SEC must be a bipartisan body. To achieve this, no more than three commissioners can belong to the
same political party. One of the commissioners is designated by the president to be the chair of the commission.
Today the SEC employs nearly 3,500 people. The commission’s organizational chart consists of four divisions and
nineteen offices. The Division of Corporate Finance is directly charged with overseeing corporate disclosure
practices and making sure that all investors, from Wall Street financial analysts to retiring teachers in rural Iowa,
have equal access to corporate financial information. The division reviews required disclosure documents filed by

companies planning to sell their securities to the general public, as well as periodic disclosures by publicly traded
corporations. It encourages corporations to provide extensive and timely information, both positive and negative,
about the company’s business to ensure that investors can make an educated decision as to whether to buy, hold,
or sell securities of the company.
The Division of Trading and Markets provides oversight of securities trading. It controls the work of stock
exchanges, brokers, dealers, transfer agents, clearing agencies, credit rating agencies, and others. The goal of
this division is to ensure reliable and efficient operations of the securities trading markets.
The Division of Investment Management is in charge of mutual and pension funds operating in the United States.
A large portion of the money in these funds is collected from private investors who are saving for retirement,
college, a new house, or another purpose. Professional fund managers pool together money from millions of such
individuals and manage it on their behalf. This division ensures that fund managers act in the best interests of all
individual investors and provide full disclosure of fund activities to them.
The Division of Enforcement investigates securities law violations, obtains evidence of unlawful activities, and
prosecutes civil actions in court. It also collects complaints from private and professional investors and monitors
market activities daily to ensure the legality of all operations.
The nineteen offices of the commission ensure that the SEC can fulfill its functions in accordance with its mission.
For example, the Office of Compliance Inspections and Examinations makes sure that companies follow all the
compliance regulations. The Office of Investor Education and Advocacy helps individual investors with their
problems and concerns, promotes the issues important to such investors, and carries out educational efforts. The
role of the Office of Risk Assessment is to predict potential threats to the investment markets and to identify
fraudulent or illegal activities.
In addition, several offices serve to advise the commission on various issues. Among them, the Office of Economic
Analysis advises the SEC on the economic issues, the Office of the Chief Accountant on the accounting issues,
and the Office of the General Counsel on the legal issues.
The SEC also has eleven regional offices, in Atlanta, Boston, Chicago, Denver, Fort Worth, Los Angeles, Miami,
New York, Philadelphia, Salt Lake City, and San Francisco.
 Data Gathering and Distribution
The SEC also maintains the Electronic Data Gathering, Analysis and Retrieval (EDGAR) system. All publicly
traded companies are required to submit their financial information to EDGAR, and that information becomes
available to anybody who has a computer with Internet connection. EDGAR, however, is a noninteractive system
—the information is presented simply as text. In early 2009, the SEC introduced new regulations stating that,
starting from a fiscal period ending on or after June 15, 2009, all large companies must use the new system—
Interactive Data Electronic Application (IDEA). IDEA relies on the new interactive data format—extensible business
reporting language (XBRL). All other companies will start using IDEA by June 2011.
XBRL makes financial data not simply readily available to investors, but it also allows investors to download these
data directly into spreadsheets and perform data analysis. The data reporting will be standardized across
companies; thus, investors will be able to compare companies side by side. Finally, XBRL allows automated
searching within data. The key to XBRL is tagging—every variable coded in XBRL has a unique tag that identifies
the category the number belongs to. Thus, for example, if the tag for net profit is entered, the system will show
net profit for each company.
As the financial debacle of 2008–2009 grew, some economists and other experts blamed the SEC for contributing
to the crisis in three ways: (1) for failing to stay current with the ever-increasing complexity of the financial
markets and the variety of new instruments traded on these markets by many players; (2) for its laissez-faire
approach to regulations when it allowed the markets to basically regulate themselves and avoided interventions as

much as possible; and (3) for following the markets rather than leading them—in other words, for investigating
fraud once it happened and attempting to establish who was to blame for the fraud rather than trying to anticipate
and prevent the fraud from happening in the first place through changes in legislation and enhanced monitoring.
Probably the most notorious example of SEC failure to prevent fraud despite several warnings was that of Bernard
Madoff, who committed the largest investment fraud ever by a single person. Madoff had been under SEC
investigation sixteen times in the years while the fraud was being committed. Another high-profile case involves a
prominent financier from Texas, Robert Allen Stanford, who, as a CEO of Stanford Financial Group,
misappropriated billions of investors’ money.
Today the SEC joins other government agencies in dealing with the current financial crisis. On its Web site, the
SEC created a page dedicated to actions the agency is taking to mitigate the effects of the credit crisis. Among
these actions, the SEC lists aggressive fraud investigation, especially fraud connected with the subprime
mortgages, investigations of false rumors in the market, investigation of accounting fraud, and investigations of
illegal trading practices. The SEC also took actions to modify the regulations of the market by requiring disclosure
of hedge-fund positions in certain securities and updating regulations for banks and credit default swaps. Finally,
the SEC requested all companies to provide enhanced and full disclosure to supply investors with more relevant
and reliable information in a timely manner.
Alexander V. Laskin
 
See also:  Consumer and Investor Protection;  Glass-Steagall Act (1933);  New Deal; 
Regulation, Financial;  Stock Market Crash (1929). 
Further Reading
Alvares, R.I., and M.J. Astarita.  “Introduction to the Blue Sky Laws.” www.seclaw.com/bluesky.htm
“Part III: Securities and Exchange Commission.” Federal Register 74:26 (February 2009). 
Securities and Exchange Commission:  www.sec.gov
Securitization
 
Securitization is the process that financial institutions use to create financial securities from pools of financial
instruments. The securities created by this process are called either asset-backed securities (ABSs) or mortgage-
backed securities (MBSs). The classification of ABS or MBS depends on the financial instrument that backs, or
collateralizes, the security. If the financial instrument is a mortgage loan issued to finance residential real estate,
the securitization of a pool of these mortgage loans results in the issue of an MBS. If the mortgage loans are used
to finance commercial real estate, their securitization creates a commercial mortgage-backed security. Any asset
with a cash flow can be securitized if the cash flows are pooled together and sold off to investors. When the
financial instruments that are securitized are loans to fund the purchase of automobiles from dealers, then the
special purpose vehicle (SPV) issues ABSs backed by auto loans. Automobile dealers borrow to finance their
inventories of cars and trucks. These loans are called dealer floor-plan loans and are frequently securitized. In this
case, the SPV issues ABSs backed by dealer floor-plan loans. Other cash flows from financial instruments that
have been securitized include credit card balances, small business loans, student loans, aircraft and railroad car
leases, other equipment leases, home-equity lines of credit, retail automobile leases, accounts receivable, and

loans to finance manufactured housing.
The securitization process gained worldwide attention during the financial crisis of 2008–2009 because it allowed
credit to flow unsecured across many sectors of the global economy. This set the stage for the severe economic
contraction that occurred globally.
 Process
The process of securitization works basically as follows: (1) a financial institution originates loans; (2) the originator
finances the loans until the pool of loans on its balance sheet is large enough to securitize; (3) the originator sells
the loans to an SPV; (4) the SPV issues mortgage-or asset-backed securities to raise the funds to pay for the
financial assets it has purchased from the originator; (5) the funds obtained or raised from the securitization of the
original pool of mortgages are used by the originator to make new loans—which may lead to another securitization
transaction. In short, securitization effectively cycles funds from the broader capital and money markets back to
borrowers via the banking system. Securitization is a form of direct finance where the ultimate lender (purchaser of
the security) has a direct claim on the cash flow from the pool of assets rather than an indirect claim on a financial
intermediary.
Commercial banks, savings banks, and finance companies originate and securitize financial assets. While the
loans are on the balance sheet of the originator, they must be financed. Because bankers expect to finance the
loans only while the pool is being accumulated, perhaps over a few months or less, only short-term financing is
arranged. An SPV is set up to act as the purchaser and financier of the pool of loans. It is called an SPV
because, unlike a financial institution that can use its balance sheet to finance a broad array of ever-changing
assets, the SPV is constrained to buy and finance a specific pool of financial assets. Some financial assets are
revolving loans; so the amount financed by the SPV may change as the loan balances change.
The SPV cannot engage in any other activities except those that are directly related to financing the pool of assets
it buys from the originator. The SPV may issue multiple classes of ABS/MBSs or a single class. An example of a
single class would be an MBS that is structured as mortgage pass-through security. In most securitizations other
than those that issue mortgage pass-through securities, multiple classes of securities are issued by the SPV. Each
class of securities will have rights to a different stream of the cash flows that are generated by the securitized
asset pool. Some securities will be subordinate to others with respect to credit risk. Some securities will have the
same credit risk but different rights with respect to the timing of principal repayments. The ability to reallocate the
payments from a pool of assets has enabled MBS and ABS to be designed for a very broad spectrum of investors
all over the world. Some investors are more willing to take risks than others. For these investors, risky securities
are created. Because the total risk of a pool of assets cannot be changed, if a risky security is created, it means
that a safer security is also created. It is possible to create short-, medium-, and long-term securities by issuing
ABS/MBSs that amortize classes sequentially. Sequential amortization means that one class must be paid off
before the second class of investors begins to receive principal payments. Depending on how many securities are
in the sequence, the SPV may issue very short–, short-, medium-, and long-term securities to finance a single
pool of financial assets.
Subordinate securities are designed to finance a disproportionate level of losses caused by borrower defaults. For
example, a $1 million pool of auto loans is securitized. One subordinate class of securities is issued (class B)
having a principal value of $100,000 (10 percent of the pool value), and one senior class (class A) is issued with a
principal value of $900,000. Class B is structured to absorb all losses of the asset pool before class A. MBS and
ABS are generally rated by at least one rating agency. The ratings assigned to a security, including the various
classes of ABS and MBS, are an indication of how likely investors are to receive timely payment of all promised
interest and principal payments. A triple-A rating is the highest a security can receive and indicates very strong
credit.
Financial assets must be serviced whether they are securitized or financed on the originator’s balance sheet. If
the assets are securitized, the servicing function must be explicitly paid for out of the cash flows generated by the

assets. Typically, one-quarter to one-half of a percentage of the outstanding asset balance is the servicing fee per
year. Servicing may be retained by the originator or sold to another financial institution. The role of the servicer is
to collect payments from borrowers and to funnel the collections into the appropriate bank accounts, where they
will be used by the paying agent to pay investors. The servicer can advance funds when borrowers are delinquent,
with the servicer receiving compensation for making the advance. The servicer is also responsible for organizing
collection efforts and foreclosure proceedings if borrowers are delinquent or have defaulted.
 Separation of Loan Origination, Financing, and Servicing
Prior to the widespread adoption of securitization, banks and finance companies traditionally originated, financed,
and serviced financial assets such as mortgages, consumer loans, car loans, equipment leases, and business
loans. Origination, financing, and servicing were bundled together and could not be effectively separated.
Securitization enables bankers to separate the three formally connected banking activities—the origination,
servicing, and financing of financial assets. A bank can originate mortgage loans and securitize the loans. Even
though securitization involves selling the loans, the bank will earn a fee for the origination of the mortgage as well
as fees for servicing the mortgage. The risk to ABS/MBS investors is that the asset pool will perform worse than
expected or that guarantees of the performance of the ABS/MBS cannot fulfill their obligations.
A financial receivable is short-term extension of credit. It is an asset of the company that has extended the credit.
For example, if a manufacturer of television sets allows Wal-Mart to have sixty days to pay for televisions once
they are delivered, the supplier has extended credit to Wal-Mart. The supplier of the televisions now has what is
called an account receivable on its balance sheet.
Because securitization is a dominant form of financing for financial institutions, it is a vital conduit for funds and
must operate effectively if credit is going to flow smoothly among the universe of borrowers. However, the
securitization process played a pivotal role in the financial meltdown of 2007–2008, in part because it was the
instrument by which highly risky (or even insufficiently collateralized) mortgages became integrally linked to a vast
network of unsecured financing throughout the U.S. and world economies. Many of the mortgage-backed
securities that were subprime, and consequently had low credit ratings, were packaged together into instruments
that received top credit ratings and were sold into the global marketplace. Consequently, when the less-than-
investment-grade mortgages failed, the top-rated securities were compromised, and the U.S. and global financial
structure shattered. The default rates on credit card receivables, automobile loans, home-equity loans, and
mortgages were much higher than investors in ABSs and MBSs had expected when they purchased these
securities. The high default rates severely depressed prices of MBSs and ABSs and constrained new issues of the
securities.
Charles A. Stone
 
See also:  Collateralized Debt Obligations;  Collateralized Mortgage Obligations;  Credit
Default Swaps;  Debt Instruments;  Innovation, Financial;  Liberalization, Financial. 
Further Reading
Fabozzi, Frank J., ed. Accessing Capital Markets Through Securitization. Hoboken, NJ: John Wiley and Sons, 2001. 
Fabozzi, Frank J., and Vinod Kothari. Introduction to Securitization. Hoboken, NJ: John Wiley and Sons, 2008. 
Kendall, Leon T., and Michael J. Fishman, eds. A Primer on Securitization. Cambridge, MA: MIT Press, 2000. 
Obay, Lamia. Financial Innovation in the Banking Industry: The Case of Asset Securitization. London: Taylor &
Francis, 2001. 
Tavakoli, Janet M. Structured Finance and Collateralized Debt Obligations: New Developments in Cash and Synthetic

Securitization. Hoboken, NJ: John Wiley and Sons, 2008. 
Shackle, George (1903–1992)
 
Economist and professor George Shackle is remembered for his work on uncertainty, specifically regarding crucial
choices whose outcomes may define, for better or worse, the chooser’s future possibilities. His work frequently is
cited by economists and management theorists in the areas of uncertainty, information asymmetry, game theory,
competitive options, and knowledge management.
George Lennox Sharman Shackle was born on July 14, 1903, in Cambridge, England. He attended the Perse
School and then worked his way through undergraduate school as a bank clerk and a teacher; he earned a
bachelor’s degree in 1931 from the University of London. He obtained his doctorate from the London School of
Economics in 1937; his dissertation on business-cycle theory was based on John Maynard Keynes’s General
Theory of Employment, Interest and Money (1936). Shackle’s focus was on expectations and uncertainty in the
analysis of economic behavior, and the related theme of the question of time. The work was published in 1938
under the title Expectations, Investment and Income. In 1939, Shackle served in British Prime Minister Winston
Churchill’s office as an economist. After World War II, he held posts in the Cabinet Office and at the University of
Leeds. He became a professor of economics at the University of Liverpool in 1951, where he remained until his
retirement in 1969.
Most of Shackle’s academic writings expanded on the topic of his dissertation, in particular, the influence of
entrepreneurs’ and consumers’ expectations on the business cycle and, hence, on the rate of employment. In
1958, he published Time in Economics; in 1961, Decision, Order and Time in Human Affairs; and in 1965, A
Scheme of Economic Theory.
The study of expectations in the area of macroeconomics led to new discoveries about the causes of business
cycles. One study focused on the effects of the changing nature of human expectations (in particular, those of
entrepreneurs), which impart the dynamic impulses to the economic system that generate cycles. The second
study focused on the distinction between initial planning expectations and the assessment of final outcomes, both
of which are captured in what are known as ex ante and ex post (before and after) multiplier effects, respectively.
A multiplier effect expands or intensifies as an activity is repeated or spreads from person to person, rippling
throughout the economy. For example, the multiplier effect of an initial increase in the aggregate flow of a net
investment in facilities will be unexpected by the investor; it will improve the profit outlook and lead to a further
acceleration in investment and a further multiplier effect. Eventually, such multiplier effects will be expected, at
which stage net investment flow will have reached its maximum, as there will be no more unexpected increases in
aggregate income to further stimulate investment. The failure of net investment to accelerate further will deprive
investors of the multiplier effect, which they have come to expect. Investors will be disappointed by the anticipated
but unachieved “growth,” and as a result, they will reduce their rate of investment. The cycle’s downswing and
reversal are mirror images of its upswing and subsequent downswing. Thus, the entire cycle is a result of changing
expectations, which are continuously influenced by the effects of previous changes.
Shackle considered his greatest work to be “A Student’s Pilgrimage” (1983), in which he addressed the difference
between uncertainty and risk. By uncertainty, he was referring to the inability to predict with certainty what is going
to happen in the future (e.g., with interest rates, prices, political situations). In such cases, probability cannot be
scientifically or mathematically calculated and applied—thus, the decision maker, when choosing a particular
course of action, is faced with uncertainty and can only anticipate a range of outcomes.

In addition to writing about his own theories, Shackle wrote classic texts on the history of economic thought,
including The Years of High Theory: Invention and Tradition in Economic Thought, 1926–1939 (1967) and
Epistemics and Economics (1972). He died on March 3, 1992.
Carol M. Connell
 
See also:  Risk and Uncertainty. 
Further Reading
Ford, J.L. G.L.S. Shackle: The Dissenting Economist’s Economist. Northampton, MA: Edward Elgar, 1994. 
Ford, J.L.  “G.L.S. Shackle, 1903–1992: A Life with Uncertainty.” Economic Journal 103:418 (1993): 683–697. 
Shackle, George L.S. Epistemics and Economics: A Critique of Economic Doctrines. Cambridge, UK: Cambridge University
Press, 1972. 
Shackle, George L.S. Expectations, Investment and Income. Oxford, UK: Oxford University Press, 1938. 
Shackle, George L.S. Keynesian Kaleidics: The Evolution of a General Political Economy. Edinburgh, UK: Edinburgh
University Press, 1974. 
Shadow Banking System
 
The shadow banking system is an umbrella name for a range of highly leveraged financial intermediaries such as
hedge funds, private equity funds, money market mutual funds, monoline insurers, conduits, structured investment
vehicles (SIVs), special-purpose vehicles (SPVs), and other off-balance-sheet vehicles that are centered around
the credit markets. The shadow banking system has come to play a key role in financial intermediation both in the
United States and in other developed nations as businesses and consumers have increasingly shifted away from
commercial banks to the markets for their borrowing and lending needs.
 Lack of Regulation
The institutions that populate the shadow banking system effectively function as commercial banks in supplying
credit and even accepting deposits. Unlike commercial banks, however, they are not granted access to the lender
of last resort (the central bank) or to the institution that insures bank deposits (the Federal Deposit Insurance
Corporation—FDIC—in the United States). Due to the lack of access to the lender of last resort and deposit
insurance, they are not subject to regulations, such as effective capital requirements, leverage limits, and other
restrictions imposed on commercial banks. Nor do they come under the supervision of the central bank.
The only type of regulation, if any, that these institutions are subject to in the United States is capital market
regulation provided by the Securities and Exchange Commission (SEC), which is designed to provide full
disclosure to potential investors in the securities but not to provide prudential regulation nor to function as a lender
of last resort. Unlike regulated and protected commercial banks, many shadow banking institutions, such as SIVs,
have few or no reporting obligations or governance standards (again, those registered with the SEC do). In the
past, even when regulations did exist, they were often suspended; for example, collateralized SPVs were granted
registration and reporting exemptions, and SIVs were granted reporting consolidation exemptions.

In sum, despite the fact that some of these institutions are regulated by the SEC and function as banks, they are
not subject to prudential regulations, which are basically aimed at insuring the liquidity of assets. Rather, they are
regulated as capital market institutions, the aim of which is to ensure that the mark-to-market value of assets of
the entity will be sufficient to liquidate assets at all times. This absence of proper regulation for these financial
intermediaries has allowed the shadow banks to employ a high degree of leverage, the expansive use of which
was one of the main causes of the global financial crisis that started in 2007.
Traditionally, commercial banks have been the main suppliers of credit in economies, but in the past three
decades, market-based institutions have taken over a large part of this business. A commercial bank creates
liquidity by ensuring that its liabilities—largely, its deposits—have a higher liquidity premium than its assets, mostly
its loans outstanding. It issues short-term highly liquid deposits, which can always be easily converted into the
currency and are the closest substitutes for cash that there is.
The asset side of bank balance sheets was traditionally dominated by commercial and industrial loans supported
by income flows they helped generate, although in the past couple of decades most large banks shifted out of
shorter-term industrial loans and into longer-term mortgages. Even though the possibility always exists that banks
might not have enough liquidity at any point in time to meet redemptions of their liabilities, the lender of last resort
and the deposit insurer are put in place to increase confidence in the banking system and to mitigate the liquidity
problems a solvent bank might face.
Money market mutual funds are very similar to banks in their liability structure and in the manner in which they
create liquidity. They issue short-term liabilities similar to bank deposits with a promised equivalent redemption
value, and use the funds to buy short-term, highly liquid credit market instruments, such as commercial paper,
essentially funding corporate borrowing. Investment banks, for instance, create liquidity by acting as broker-dealers
in the securities market and hence facilitating the transformation of longer-term illiquid higher-risk assets into
shorter-term liquid lower-risk assets.
 Role in Housing Bubble
Despite the existence of capital market–based institutions functioning as banks, it was not until the mass
securitization drive of the past decade that the U.S. shadow banking system exploded in size, overtaking the
commercial banking sector in terms of asset size. In the second quarter of 2007, near the peak of the housing
bubble, market-based institutions (such as government-sponsored enterprises, or GSEs, GSE pools, asset-backed
security issuers, broker dealers, and finance companies) involved in securitization had assets valued at $16 trillion,
equal to 120 percent of assets of depository institutions (commercial banks, savings and loans, and credit unions).
This rising importance of the shadow banks was especially pronounced in the mortgage market, with market-
based institutions holding about two-thirds of the $11 trillion dollars of home mortgages.
In the case of securitization, liquidity is created by transforming longer-term, higher-risk illiquid assets into short-
term, low-risk, and highly liquid assets through the balance sheet of an SPV. The SPV set up by banks and other
financial institutions issues asset-backed commercial paper (that is, rolled over or redeemed at par) to finance its
position in a securitized asset, thus transforming illiquid mortgage loans into highly liquid short-term credit market
instruments. The next step forward in the innovative world of shadow banks involved structured securitizations
such as auction rate notes and collateralized debt obligations, which also create liquidity, albeit in a different
manner. The liquidity of these depends on the proper functioning of the securities market, and they have no
explicit price guarantees except for insurance of principal provided by credit default swaps or other credit
enhancement products sold by third-party monoline insurers.
The funding of shadow banking institutions is short term and usually comes in the form of secured or unsecured
borrowing in the commercial paper market, as well as reverse repo transactions. Although some shadow banking
institutions, such as SPVs sponsored by commercial banks, can be backed by credit lines from the sponsoring
bank (and hence have indirect access to the lender of last resort), most of them depend on the normal functioning

of short-term funding markets. Once these dry up, the shadow banks have no lender of last resort to turn to.
The lack of regulation and the employment of high leverage and risk distribution through securitization have
allowed the shadow banks to meet borrowers’ needs for financial intermediation more cost-effectively, resulting in
an explosive increase of their share in the financial sector assets. While the main source of income for traditional
banks was the interest rate spread between long-term assets and short-term liabilities, the majority of shadow
bank income came from loan origination and servicing fees. Due to regulatory arbitrage and lack of supervision,
the shadow banking system took over the main function of commercial banks, facilitating borrowing and lending,
including maturity intermediation. Positions in longer-term, higher-risk, lower-liquidity assets were financed by
issuing shorter-term, lower-risk, and higher-liquidity assets. In this scenario, the only way for commercial banks to
maintain their profitability was by setting up off-balance-sheet entities and acquiring affiliates that were a part of
the shadow banking system to be able to utilize high leverage and participate in the activities that commercial
banks were forbidden to engage in.
Since the shadow banks were major participants in the liquidity creation that fueled the historic U.S. housing
bubble of 2003–2007, it is not surprising that the bursting of the bubble and the financial crisis essentially began
with a run on the shadow banks—that is, withdrawal of short-term financing from the securitization market due to
declining asset-backed security prices. As investors became concerned about the state of the housing market,
short-term funding markets dried up, cutting short the life support of these institutions. The life span of the shadow
banking system was limited to the willingness of institutional investors to invest in short-term credit market
instruments.
The public safety nets put in place to safeguard against a run on the commercial banking system were useless for
preventing a run on the shadow banks, hence all the extraordinary measures taken by the Federal Reserve to
prevent a total collapse of the system. Once an asset is securitized through an off-balance-sheet entity, the loan
disappears from the bank’s balance sheet and so do government guarantees (some institutions did have recursive
arrangements with their sponsors, mostly commercial banks, and so got indirect support from the government).
Under current institutional arrangements, when these securities go bad, the government does not stand ready (or
is not supposed to) to rescue the holders of their liabilities, unlike the case of commercial banks.
The shadow banking system was a major contributor to the global financial fiasco. If these institutions are to
engage in financial intermediation, advocates of more government oversight say, they need to be as tightly
regulated and supervised as commercial banks are, and government guarantees available to the latter need to be
extended to them as well. This will eliminate the cost advantage of shadow banks relative to commercial banks,
and will effectively limit the size of the shadow banking system, preventing another financial debacle, for now.
Yeva Nersisyan
 
See also:  Countrywide Financial;  Luminent Mortgage Capital;  Money Store, The;  Regulation,
Financial;  Venture Capital. 
Further Reading
Adrian, T., and H.S. Shin.  “The Shadow Banking System: Implications for Financial Regulation.” Staff Report no. 382, 
Federal Reserve Bank of New York Staff Reports, July 2009. 
Kregel, J.  “No Going Back: Why We Cannot Restore Glass-Steagall’s Segregation of Banking and Finance.” Public Policy
Brief no. 107.  Levy Economics Institute, 2010. 

Shock-Based Theories
 
Single-shock theories of the business cycle combine the concept of a business cycle with that of general
equilibrium theory, treating the events of the cycle as a “shock” that disrupts the equilibrium of the economy.
 General Equilibrium Theory
Equilibrium is a persistent and self-sustaining state of balance and stability. Examples in the physical world are
easy to identify and useful for basic understanding. The planets’ orbiting around the sun represents equilibrium in
that their movement through space—which should propel them away from the sun—is balanced by the
gravitational force of the sun. They will remain in their elliptical orbits until an exogenous force, such as the gravity
of another star, or an endogenous one, the sun’s eventual collapse, upsets the equilibrium. Moreover, as this
example shows, a state of equilibrium does not imply lack of motion. An economic market in which prices and
profits remain the same, without excess supply or unmet demand, is another example of equilibrium.
In economics, equilibrium theory is generally microeconomic in focus. That is, it is concerned with prices, supply
and demand, and consumer decisions rather than with the inflation, unemployment, and economic policy decisions
studied in macroeconomics. The equilibrium price of a good is the one that will satisfy supply and demand without
surpluses or shortages. It is often viewed as the price achieved naturally after some transitional volatility, bouncing
back and forth like a tennis ball in response to corrective forces (overproduction, underproduction, and the like).
Thus, for example, if one were to introduce a new type of automobile into a market, one could expect to sell more
automobiles in the first year than in subsequent years. Once everyone owns the car, new customers will be
created slowly (as children grow up and reach driving age, or individuals’ incomes increase to enable them to
afford a car), and most sales will be replacement vehicles rather than first-time purchases. Equilibrium is achieved
when the number of cars sold each year becomes stable and predictable.
This example represents a case of partial equilibrium analysis—it considers only one good and assumes that all
other prices remain constant or irrelevant. Modern economists focus on the problem of general equilibrium, a
model that takes everything into account. General equilibrium is implicitly macroeconomic in scope in that it
attempts to postulate and model a state of equilibrium for the entire economy.
 Sudden and General Disturbances
Although general equilibrium became a special concern of economists beginning in the 1920s, it had been the
subject of previous study. In the late nineteenth century, the French neoclassical economist Léon Walras was the
first to attempt to model equilibrium prices for a whole economy. His Elements of Pure Economics, published in
1874, describes the effects of what we now call the business cycle: “Just as a lake is at times stirred to its very
depths by a storm, so also the market is sometimes thrown into violent confusion by crises, which are sudden and
general disturbances of equilibrium.” However, Walras and the economists who followed him were unable to
adequately explain these disturbances. They could only describe a model of equilibrium that would persist for as
long as such disturbances could be avoided.
After the banking panics of the late nineteenth and early twentieth centuries and the Great Depression, as
economists delved deeper into the phenomenon of booms and busts, the business cycle came to be seen as less
and less compatible with general equilibrium theory. In the 1930s, however, Norwegian economist Ragnar Frisch
and Ukrainian economist Eugen Slutsky were the first to overcome this view. Frisch and Slutsky both argued in
favor of analyzing the effects of “sudden and general disturbances”—or economic shocks—without worrying about
their cause. Or, to extend Walras’s lake analogy, one does not need to know whether a rock or a tennis ball has
been thrown into the lake in order to know how the water will be affected. Shocks to the equilibrium of the
economy are common, even if individual types of shocks are rare, and they are randomly but normally distributed.

By extension, a roughly equal number of shocks are positive (booms) and negative (busts), and most of them are
small. A single large shock to the equilibrium accounts for a major economic downswing, such as that experienced
during the Great Depression.
The Frisch-Slutsky marriage of equilibrium theory with the business cycle paints the cycle as a series of random
variations to the trend of general equilibrium. The business cycle, in this view, is a series of shocks—those that
arise from the interaction between predictable, nonrandom phenomena and random phenomena. However, the
shocks—and the mechanics of the equilibrium—are not so simple that a negative shock can be corrected by a
positive shock of equal value. For example, there may be no government policy that will simply flip one switch
back on when a shock to the equilibrium switches it off. The forces of stability are strong enough that there is a
lag time, the effect of which is that further shocks can prolong the negative effect rather than cancel it out. By way
of analogy, consider the difficulty of trying to cancel out the ripples in Walras’s lake caused by a thrown stone.
What can one throw into the water to reverse the ripples? In short, this approach to business cycles denies their
cyclicality. The shocks occur not with a periodicity that can be described, but stochastically (randomly).
 Other Shock Theories
The Polish economist Michal Kalecki incorporated the concept of exogenous shocks (those that originate outside
the system, such as a rock thrown into the lake) in his 1954 book Theory of Economic Dynamics: An Essay on
Cyclical and Long-Run Changes in Capitalist Economy. According to this model, continual shocks to the economy
are responsible for oscillations in the business cycle.
The Frisch-Slutsky theory of shocks was revived in the 1980s and incorporated into “real business cycle” or
“stochastic growth” theory, which blends neoclassical models of production, spending levels, and consumer
preference with Frisch-Slutsky-type shocks.
The implications of the real business cycle theories are profound for economic policy makers. If, as argue some
economists, booms and busts are a natural process as the market economy efficiently responds to endogenously
triggered increases and falls in demand, then governments should not attempt to respond to shocks with short-
term fiscal and monetary policies. In particular, governments should not attempt to bolster employment and
demand during times of recession. Instead, they should focus on long-term policy, both structural and monetary,
creating the infrastructure needed by a growing economy. In addition, they should not adjust monetary policy to
meet the short-term ups and downs of the economy but instead emphasize stable growth of the money supply to
keep up with overall economic growth.
On the other hand, some economists respond that exogenous shocks are critical to the economic cycle, and
markets cannot necessarily adjust to them on their own. Consequently, the government does have a role in
implementing short-term fiscal and monetary policy to respond to the shocks. The recent financial crisis has
bolstered the position of those who argue that governments do have a role to play in adjusting to shocks to the
economy.
Bill Kte’pi
 
See also:  Catastrophe Theory;  Frisch, Ragnar;  Kalecki, Michal. 
Further Reading
Schumpeter, Joseph A. History of Economic Analysis. New York: Oxford University Press, 1996. 
Strøm, Steinar. Econometrics and Economic Theory in the 20th Century: The Ragnar Frisch Centennial Symposium. New
York: Cambridge University Press, 1999. 
Tangian, Adranik, and Josef Gruber, eds. Constructing and Applying Objective Functions: Proceedings of the Fourth

International Conference on Econometric Decision Models. New York: Springer, 2001. 
Van Overtveldt, Johan. The Chicago School: How the University of Chicago Assembled the Thinkers Who Revolutionized
Economics and Business. Chicago: Agate, 2007. 
Slow-Growth Recovery
 
As the name implies, a slow-growth recovery is a postrecession economic rebound characterized by anemic
economic growth and persistent high levels of unemployment. Given the importance of the latter to the overall
health of the economy, a newly popular term for the phenomenon has arisen—“jobless recovery.” What constitutes
a slow recovery is debated by economists, but most agree it is marked by growth rates of between 1 and 2
percent rather than the 4 or 5 percent needed to make a dent in high unemployment levels.
In addition, most economists agree that the U.S. economy has experienced a slow-growth recovery since the
official end of the “Great Recession” of 2007–2009, though there was debate as the country emerged from the
downturn as to whether the rebound would become more robust, remain slow, or fail altogether, leading to what is
known as a “double dip recession”—that is, two recessions separated by a brief period of modest growth.
The U.S. economy emerged very slowly from the Great Depression of the 1930s, with gross domestic product
(GDP) reaching only its precrash levels of 1929—as measured in inflation-adjusted dollars—in 1936. Meanwhile,
unemployment remained quite high. After peaking at over 25 percent in 1933, the rate remained in the double
digits until at least 1941, even though the economy was growing from 1933 onward, aside from a brief but sharp
recession in 1937–1938.
In the post–World War II era, however, recoveries have tended to be more rapid. While the U.S. economy was
buffeted by several recessions in the 1970s and early 1980s, business activity rebounded robustly after each of
them, and employers quickly began rehiring. Indeed, some economists even came to argue that the sharp
recoveries were structured into the U.S. economy of the postwar period.
The recession of 2007–2009, however, was different. First, of course, it was of much greater duration and severity
than any other economic downturn of the post–World War II era. Between the fourth quarter of 2007, when the
recession officially began, and its conclusion in the second quarter of 2009, the U.S. economy shed about 8.3
million jobs, with the unemployment rate peaking at 10.1 percent in the third quarter of 2009. Meanwhile, overall
GDP contracted between 6 and 8 percent, depending on how it is measured, over this same time period.
At first glance, the U.S. economy appeared to be recovering nicely after the end of the recession. The annualized
growth rate for the fourth quarter of 2009 was a robust 5.9 percent, more than adequate, if sustained, to
invigorate the kind of hiring necessary to make up for those 8 million-plus lost jobs. Meanwhile, the Dow Jones
Industrial Average of major corporate securities rose more than 60 percent from its low point of around 6,500 in
March 2009.
But by other measurements, the rebound was occurring slowly and painfully. Consumer confidence remained low,
aggregate demand was weak, and unemployment remained stubbornly high, at 9.7 percent into January and
February 2010, with the economy still shedding jobs, though at a much reduced rate.
Economists—at least, those who insist this recovery will be much more anemic than other postwar ones—cite a
wide variety of explanations for why the U.S. economy is experiencing a slow-growth recovery following the great

recession of 2007–2009. High levels of accumulated debt and a depressed housing market are causing
consumers to save more and spend less, driving down demand. High unemployment rates are driving down
consumer confidence, which is further undermining demand. Fearful that demand will remain weak, businesses
are less willing to expand and hire.
Perhaps the most widely cited reason behind the slow-growth recovery of 2009–2010 is the credit crisis. After
relaxing their lending standards for years—particularly in the mortgage sector—lenders in the financial sector
experienced unprecedented losses, as many of the loans they made proved to be bad ones, with mortgagors
defaulting on their loans in record numbers. To shore up their balance sheets and avoid further losses, banks and
other lenders raised credit standards and tightened the amount of money they were willing to loan.
Tightening credit standards mean that businesses cannot get the loans they need to invest and expand, thus
reducing the amount of new hiring they can do. Without access to capital, businesses tend to make do with what
they have, even as demand rises, which usually translates into more productivity per hour worked.
Meanwhile, homebuyers find it much more difficult to obtain mortgages, which continues to keep the housing
market depressed. With home equity having fallen by a historic degree and remaining far below where it was at
the peak of the housing boom in 2006, homeowners—even those with jobs—feel poorer and are less likely to
engage in discretionary spending. This means that the economy is unlikely to get a boost from consumer
spending and, as this spending accounts for about 70 percent of all economic activity, the tightening credit market
contributes to very weak overall demand.
In short, say many economists, earlier recessions that were caused either by weakening demand—such as those
of the 1990s—or by the Federal Reserve hiking interest rates—as was the case with the recession of the early
1980s—tend to produce sharp recoveries. But those recessions that are caused by credit contractions, such as
that of the early 1990s and that of 2007–2009, are followed by very slow and weak recoveries.
James Ciment
 
See also:  Employment and Unemployment;  Great Depression (1929-1933);  Growth,
Economic;  Recession and Financial Crisis (2007-). 
Further Reading
Aronowitz, Stanley. Just Around the Corner: The Paradox of the Jobless Recovery. Philadelphia: Temple University
Press, 2005. 
Reddy, Sudeep.  “The Economic Recovery: Fast, Slow or Neither?” Wall Street Journal, August 18, 2009. 
Roubini, Nouriel.  “A ‘Jobless’ and ‘Wageless’ Recovery?” Forbes.com, August 13, 2009. 
 
Smith, Adam (1723–1790)

 
One of the most influential economic thinkers in history, Adam Smith was one of the architects of classical
economics and is generally considered the father of modern economics. His 1776 work, The Wealth of Nations,
laid out the basic principles of the field, even as it helped persuade British policy makers, albeit decades later, to
gradually move their country toward a laissez-faire, free trade economy.
Scottish-born political philosopher and economist Adam Smith argued in one work that selflessness makes a
person moral and self-aware. But in The Wealth of Nations, he used the term “invisible hand” to explain how
collective self-interest promotes social good. (The Granger Collection, New York)
Born in Kirkcaldy, Scotland, in 1723, Smith was raised by his mother; his father died several months before his
birth. A child prodigy, Smith entered the University of Glasgow at the age of fourteen and then studied for a time
at Oxford University, though he did not receive a degree, being forced to leave when his scholarship ran out. In
1751, he became a professor of moral philosophy at Glasgow University, where he taught logic, ethics, rhetoric,
law, and economics. In 1778, he was appointed a commissioner of customs, a post that provided him with a
secure source of income but also required him to crack down on smuggling even though his own writings had
justified such clandestine activities in the face of “unnatural” legislation that tried to restrict trade.
Smith began his intellectual career as a philosopher, and his first book, The Theory of Moral Sentiments (1759),
reflects his interests, being an examination of the influence of social relationships on individual conscience. In it,
Smith argues that selflessness and sympathy for others has a positive effect on the individual, making him or her
more moral and more self-aware. The book stands in stark contrast to his most celebrated work, The Wealth of
Nations, which makes the argument that the collective self-interest of individuals affects social and economic

progress. Recent scholarship on Smith, however, has tried to reconcile the two contradictory messages, saying
that Smith’s two major works illustrate how people must and do act differently in their various roles in life.
Widely regarded as the foundational text of classical economics, An Inquiry Into the Nature and Causes of the
Wealth of Nations, as its full title reads, was in fact a five-book series that attempted to decipher the causes and
nature of a nation’s prosperity. Using the now-famous example of pin makers, Smith argued that increasing labor
specialization was the key to economic growth. That is, he wrote, “ten workers could produce 48,000 pins per day
if each of eighteen specialized tasks was assigned to particular workers. Average productivity: 4,800 pins per
worker per day. But absent the division of labor, a worker would be lucky to produce even one pin per day.”
More broadly, Smith compared and contrasted the political and economic systems of Britain and France to reach
his conclusions about what made nations prosperous. In doing so, he offered three basic arguments. First, he
said, prices for goods were not arbitrary but reflected underlying value, largely the labor that went into making
them. He then argued that society and political systems could influence the kinds of goods being manufactured
and that certain policies can ensure that the right kinds of goods are being made and in the most economically
efficient manner. Finally, he put forth the basics of supply-and-demand theory, arguing that high prices are a “self-
curing disease,” since an increase in price will ensure an increase in production, thereby lowering costs.
Smith offered up his timeless “invisible hand” metaphor to illustrate how collective self-interest promotes social
good. “By directing that industry in such a manner as its produce may be of greatest value,” Smith wrote in The
Wealth of Nations, “he [the individual economic actor] intends only his own gain, and he is in this, as in many
other cases, led by an invisible hand to promote an end which was no part of his intention.”
Although the “invisible hand” is referred to only once in The Wealth of Nations, it is Smith’s best-remembered
metaphor for the smooth operation of the free market. Smith declared that consumers choose a product or service
for the lowest price and that entrepreneurs choose to sell said good or service for the highest rate of profit. He
asserted that by thus making their choices or needs (demand) known through market prices, consumers “directed”
entrepreneurs’ investment money to the most profitable industry. For example, if an entrepreneur is making a large
profit by selling a particular product, other entrepreneurs will enter the market because profit opportunities are
available.
Typically, he went on, when additional entrepreneurs enter a market, the price offered to purchase the product is
lower, so the new entrepreneurs can attract customers. This undercutting process forces established entrepreneurs
to find ways to become more efficient and/or less expensive so that they, in turn, can charge less for the same
product. Entrepreneurs will continue to enter the market until a barrier to entry is created or profits for that product
are no longer attainable. When goods are highly valued by consumers, profits increase; profits attract competition,
and so the general economic well-being of both the individual and the nation increase.
Because of his emphasis on free markets, economic conservatives (economic “liberals” in British and European
political parlance) have long embraced Smith to bolster their argument that the government has a minimal role to
play in directing the economy and that when it does interfere, the effects are usually deleterious in that they distort
the natural workings of the “invisible hand.” But, say many Smith scholars, this oversimplifies and misunderstands
the economist’s thinking. Not only did Smith argue that governments should enforce contracts and issue patents
and copyrights to encourage invention—positions that even most modern conservatives embrace—he also
contended that governments should actively prevent collusion and price-fixing by businesses and that
governments should invest in useful public works projects that private interests would not and could not undertake
because said projects might not produce direct and immediate profit. Nor was Smith a pure free trade theorist,
contending that retaliatory tariffs were a legitimate weapon of economic policy makers.
Unsatisfied with his later writings, Smith asked friends to burn nearly all of his work after The Wealth of Nations,
though a series of essays on physics, metaphysics, and astronomy was published after his death in 1790.
Michael Rawdan

 
See also:  Classical Theories and Models;  Law, John;  Marx, Karl;  Mill, John Stuart. 
Further Reading
Coase, R.H.  “Adam Smith’s View of Man.” Journal of Law and Economics 19:3 (1976): 529–546. 
Ross, Ian Simpson. The Life of Adam Smith. New York: Oxford University Press, 1995. 
Smith, Adam. An Inquiry Into the Nature and Causes of the Wealth of Nations. Chicago: University of Chicago Press, 1977. 
Smith, Adam. The Theory of Moral Sentiments, ed. Knud Haakonssen. Cambridge, UK: Cambridge University Press, 2002. 
Souk al-Manakh (Kuwait) Stock Market Crash (1982)
 
The largest financial crisis to emerge out of the oil price boom of the 1970s and early 1980s, the Souk al-Manakh
stock market crash of 1982 involved a dramatic collapse in share prices on an over-the-counter, or direct trader-
to-trader, exchange established in Kuwait in the early 1980s. The crash, which led to the closing down of the
exchange, also contributed to the economic downturn that afflicted the oil-rich Persian Gulf nations through the
1980s.
The origins of the Souk al-Manakh crash date back to the Arab oil embargo of the early 1970s and the run-up in
crude oil prices that followed it. From 1973 to 1974, oil prices roughly quadrupled, from about $3 to $12 a barrel
(about $14 to $58 in 2008 dollars). The price hikes flooded oil exporting countries with cash—none more so than
Kuwait, with about 10 percent of the world’s proven petroleum reserves. With the nation pumping approximately 2
million barrels per day, its oil revenues jumped from $6 million per day to $24 million per day, or from about $2.2
billion to $8.7 billion annually—this with a population of less than 1 million people. The dizzying increase in
revenues brought legitimate investment in the nation’s infrastructure as well as conspicuous consumption and
speculative financing on the official Kuwait Stock Exchange (KSE). In 1977, the KSE crashed, though not nearly
on the scale that the Souk would five years later. The government responded to the 1977 decline in two ways:
first, it imposed much stricter regulation on the KSE, making it difficult to speculate in securities traded there;
second, it bailed out investors hit hard by the drop in prices.
The two responses would do much to encourage the development of the highly speculative Souk in the early
1980s. Tighter regulation made the KSE a more staid venue, focused on the sale of large blocs of stock to a
handful of very wealthy and interconnected families. This left few opportunities for other investors. And because
prices moved relatively slowly on the KSE, those who were interested in riskier investments had to find another
forum for their activities.
By 1981, there were plenty of such investors. Encouraged to overlook risk by recalling the government’s bailout of
investors in 1977, Kuwaiti investors were once again flush with oil money, as the Iranian Revolution of 1979 and
the onset of the Iran-Iraq War a year later had pumped up prices to $40 a barrel (about $100 in 2008 dollars).
Although the war drove down Kuwaiti oil exports to about 1.5 million barrels per day, revenues were still about $22
billion annually—this for a country that, despite high birth rates, still had a population of only 1.4 million.
With all that cash in circulation, with so many investors believing that risks were minimal, and with an official stock
exchange allowing no real outlet for investments, it was all but inevitable that a new, more speculative forum for
investment would emerge. (U.S. and European stock exchanges offered little opportunity at the time, as both were

suffering severe downturns amid the deepest economic recession since the Great Depression.) Housed in an air-
conditioned parking garage in Kuwait City, the unregulated, highly speculative, over-the-counter stock market
known as the Souk al-Manakh fit the bill perfectly. And the money poured in. At one point, in the summer of 1982,
the Souk had the third-highest market capitalization in the world, after the New York Stock Exchange and the
Nikkei Index of Japan. Brokers and bankers added to the investment fervor by accepting postdated checks for the
purchase of shares, flooding the market with additional credit.
It was a classic financial bubble, drawing investors from around the oil-rich Persian Gulf, and it burst as soon as
confidence in the system was compromised. Investor checks began to bounce in August 1982, and the exchange
collapsed within a month. The Kuwaiti Ministry of Finance insisted that all postdated checks be cleared by banks
and officially shut down the exchange. The government established a clearinghouse company that tried to
untangle the commitments of investors and brokers and set up an arbitration panel to settle disputes between
traders or to enforce deals made voluntarily by the affected traders themselves. In addition, the government set up
a $1.7 billion trust fund to compensate some of the less speculative investors. This was actually far less generous
than the compensation package that followed the 1977 collapse, as an official investigation found that some 6,000
investors had passed nearly $100 billion in bad checks at the height of the 1980s boom.
In the end, the Souk al-Manakh collapse triggered a major crisis in the Kuwaiti economy and, to a lesser extent,
the economies of other Persian Gulf countries. As in much of the Arab world, Kuwait’s economy was based less
on individuals than on families, many of which were financially crippled by the actions of a single member who
used family credit to fund speculation. Meanwhile, all the unpaid debts drove every Kuwaiti bank into insolvency
except for the very largest, the National Bank of Kuwait. By weakening the national economy and ruining the
finances of many Kuwait families, the Souk al-Manakh crash left the country even more vulnerable to the dramatic
decline in oil prices later in the 1980s—a drop that pushed much of the Persian Gulf region into recession.
James Ciment and Marie Gould
 
See also:  Asset-Price Bubble;  Middle East and North Africa;  Oil Shocks (1973-1974, 1979-
1980);  Stock Markets, Global. 
Further Reading
Al-Yahya, Mohammed A. Kuwait: Fall and Rebirth. New York: Kegan Paul International, 1993. 
Dawiche, Fida. The Gulf Stock Exchange Crash: The Rise and Fall of the Souq Al-Manakh. Dover, NH: Croom Helm, 1986. 
South Africa
 
Situated at the southern tip of the African continent, the Republic of South Africa is an ethnically diverse country
with a population of nearly 49 million. It has the largest and most developed economy on the continent, anchored
by commercial agriculture, manufacturing, tourism, and most important, mining. Still, the country faces daunting
economic challenges—partly a legacy of its racist past—including widespread poverty, high levels of
unemployment, and some of the most egregious income and wealth inequalities in the world. With its
macroeconomic fundamentals in good shape, South Africa seemed well positioned to avoid the worst of the
financial crisis of 2008–2009, but the subsequent recession undermined demand and prices for key commodity

exports, leading to a nearly 2 percent contraction in gross domestic product (GDP) in 2009.
 Economic History to the Apartheid Era
Archaeological finds indicate that South Africa has been inhabited by protohumans and modern humans longer
than any other part of the world, barring East Africa. Around the middle of the first millennium CE, the native
hunters and gatherers of the region were joined by farming Bantu peoples from the north, who soon displaced
many of the former to deserts and other outlying regions.
South Africa’s strategic location and the resources under its soil played a key role in introducing the region to the
larger world economy in the modern era. Situated on the key shipping route between Europe and the East Indies,
South Africa’s second-largest city, Cape Town, was first settled by the Dutch in the mid-seventeenth century,
serving as a provisioning port. Gradually spreading out into the hinterlands, the Dutch were joined by British
settlers in the eighteenth century, and the colony fell into Great Britain’s permanent possession in the early
nineteenth. Angered by Britain’s decision to ban slavery and to promote more equal treatment of native Africans,
the descendants of the Dutch settlers, known as Afrikaners or Boers, moved further inland beginning in the 1830s,
meeting stiff resistance from the native peoples, particularly the Xhosa and the Zulu.
When first diamonds, in the 1860s, and then gold, in the 1880s, were discovered in the interior of the country, the
finds set off a rush of prospectors and businesses that further antagonized relations between the Afrikaners and
the British. This provoked two hard-fought wars, the second of which, from 1899 to 1902, was particularly brutal
and led to a half century of subjugation of the Boers. In 1910, the British gave South Africa limited independence
as a dominion within the empire, with English-speaking whites in control of the government.
Meanwhile, the diamond mines of Kimberley and the gold mines of the Transvaal, around the boomtown of
Johannesburg, South Africa’s largest city, drew hundreds of thousands of people, both immigrants and indentured
servants from overseas and native Africans from rural areas. The influx of capital into the region helped the
country establish a manufacturing base and a large commercial agriculture sector. At the same time, the country’s
urbanization—which inevitably led to racial mixing—prompted authorities to implement segregation measures,
which triggered the beginning of an African nationalist movement and the founding of the South African Native
National Congress in 1912 (known as the African National Congress, or ANC, from 1923 on).
With its economy highly dependent on extractive industries and the export of commodities such as minerals and
agricultural goods, the country was hard hit by the global depression of the 1930s, though exports to war-torn
Britain and Europe led to renewed prosperity—at least for the minority white population—in the 1940s.
 Apartheid Era
With the triumph of the Afrikaner-led National Party in the election of 1948—a poll largely restricted to white
voters—the South African government introduced its policy of apartheid, or strict separation of the four main ethnic
groups in the country. These were, in descending order of official status, (1) persons of European descent; (2)
persons of Asian descent, largely Indians brought into the country by the British as indentured servants to work
mines and plantations in the nineteenth century; (3) “colored” or mixed-race persons, largely situated in Cape
Town and the environs; and (4) native Africans, who made up the vast majority of the country’s population. The
last group was divided further into ten Bantu nations, or Bantustans, each of which was assigned a rural
homeland. Today the country is roughly divided along the following lines: black African (79 percent), white (9.6),
“colored” (8.9), and Asian (2.5).
The apartheid system was about more than simple separation of the races; it also was about economic power.
The most lucrative occupations were reserved for whites, as was the best agricultural land, forcing rural native
Africans to either scratch out a living in the overcrowded Bantustans or migrate to the cities and mines in search
of work. Strict laws regulated their movement, so many had to leave their families behind. In urban areas, they
were relegated to overcrowded and underserviced townships on the peripheries of the cities. Meanwhile, Indians

and “colored” persons also were relegated to low-paying occupations and specific neighborhoods, though many of
the former became petty merchants. Public investments in infrastructure, health, and education were skewed to
favor the white population.
Meanwhile, these measures were deeply opposed by a growing antiapartheid movement among blacks,
“coloreds,” and Asians. They also were condemned by the international community, especially after a protest in
the Johannesburg-area township of Sharpeville led to the massacre of sixty-nine unarmed demonstrators by
security forces.
Despite the political troubles, the South African economy prospered in the 1950s and 1960s, its mineral exports
much in demand by a booming global economy. South Africa’s finance sector became the wealthiest and most
sophisticated in Africa, while domestic and foreign capital helped lay the foundation for large manufacturing and
commercial agricultural sectors, the most highly developed in Africa and comparable, if smaller in scale, to those
in the developed world. A modern and extensive transportation and communications infrastructure also was
developed.
After a relatively quiescent decade, the anti-apartheid movement was revived by street protests in the 1970s,
leading to crackdowns at home and calls abroad for economic sanctions against the country. At first, foreign
corporations maintained their presence in South Africa—it was too lucrative a market to disinvest, and foreign
executives argued that they were the only economic entities in the country offering opportunities to blacks—but a
growing solidarity movement in the West eventually forced most companies to leave by the 1980s.
South African authorities remained defiant, viciously cracking down on protests, declaring Bantustans independent
states (technically making most native Africans living in urban areas foreigners in their own country, a move that
was disregarded by the international community), embarking on an aggressive actions against pro-ANC regimes in
neighboring black-run states, and developing an autarkic, or self-reliant, economy so as not to be subject to
international pressures.
But the costs were too high. South Africa, especially its prosperous white community, was simply too small to
provide a sufficient market for the country’s manufactured goods. Without access to foreign expertise and capital,
domestic industry became uncompetitive. Meanwhile, the costs of maintaining a major defense and security
system resulted in crippling deficits and high interest rates. The sagging economy and the country’s growing
isolation internationally, combined with mass protests at home, finally convinced the country’s Afrikaner leadership
that it must abandon apartheid and white minority rule.
Dramatic reforms ensued over the course of the late 1980s and early 1990s, as apartheid laws were eased and
finally ended. ANC officials—including the revered leader Nelson Mandela—were released from prison, and full
democracy was restored. In 1994, an election was held in which all adult South Africans participated. That election
brought the ANC to power and Mandela to the presidency. The end of apartheid and the onset of majority rule
provided an immediate fillip to the economy, as governments around the world lifted their embargoes on South
Africa, foreign investors began to put their money into the economy, and international corporations began to do
business again in South Africa.
 Postapartheid Era
While the ANC long had been affiliated with the South African Communist Party and its leaders had espoused
leftist economic rhetoric, the Mandela government—and those of presidents Thabo Mbeki and Jacob Zuma to
follow—generally have pursued free-market policies and maintained what economists call solid macroeconomic
fundamentals, keeping government spending in line with revenues, pursuing conservative monetary policies, and
upholding private property rights. At the same time, the government has tried to undo the gross inequities that
were created by apartheid and more than 100 years of white domination of the economy. The ANC has embarked
on an ambitious plan to provide decent housing, more utilities, better health care, and higher-quality education to
black Africans, who constitute about 80 percent of the country’s population.

Most economists give high marks to successive ANC governments on both counts—maintaining solid economic
growth and providing more basic services to poorer South Africans. On the first front, the country has enjoyed
impressive GDP growth in the years since the ANC’s triumph in 1994, while bringing inflation down from its
double-digit highs near the end of the apartheid era. By the middle of the 2000s, many were speaking of South
Africa as one of the economic leaders of the developing world—a smaller version of China, India, or Brazil.
Still, the country faced major problems: unemployment, particularly among the black majority, remained stubbornly
high; income and wealth inequality—not just between blacks and whites, but now also between a minority of
successful blacks and the vast majority of still impoverished blacks—remained at some of the highest levels in the
world; and crime exploded, with the country’s urban areas suffering from some of the highest rates of violent crime
in the world.
Still, with its solid GDP growth and well-maintained macroeconomic fundamentals—along with the fact that South
Africa’s relatively conservative financial sector largely had avoided investing in the exotic debt-backed securities
that crippled financial institutions in the United States and some European countries—many economists expected
the country to weather the financial crisis and global recession of the late 2000s. But much of South Africa’s solid,
postapartheid economic performance relied—as that performance always had—on commodity exports. This had
served the country well in the years between 1995 and 2007, as a boom in demand—fueled in part by burgeoning
economies in Asia—had led to sustained high prices for the minerals and agricultural products the country
produced in abundance. But the global recession hurt demand and caused a dramatic fall in prices, leading to a
growing government deficit and a 2 percent contraction of GDP in 2009.
However, with demand reviving—driven by fast-growing Asian economies in 2010—South Africa resumed modest
economic growth in 2010, with GDP rising by 2.8 percent. Still, the country’s long-term economic problem, say
experts, remains what it has been throughout the postapartheid era: how to maintain the solid macroeconomic
fundamentals that assure foreign investment and steady growth while addressing the overwhelming problems of
poverty and inequality from the country’s racist past.
James Ciment
 
See also:  Africa, Sub-Saharan;  Emerging Markets;  Transition Economies. 
Further Reading
Allen, Michael H. Globalization, Negotiation, and the Failure of Transformation in South Africa: Revolution at a
Bargain? New York: Palgrave Macmillan, 2006. 
Aron, Janine, Brian Kahn, and Geeta Kingdon, eds. South African Economic Policy Under Democracy. New York: Oxford
University Press, 2009. 
Feinstein, Charles H. An Economic History of South Africa: Conquest, Discrimination, and Development. New
York: Cambridge University Press, 2005. 
Hamilton, Carolyn, Bernard Mbenga, and Robert Ross, eds. The Cambridge History of South Africa. New York: Cambridge
University Press, 2009. 
Jones, Stuart, ed. The Decline of the South African Economy. Northampton, MA: Edward Elgar, 2002. 

 
South Sea Bubble (1720)
 
The best known example of speculative excess in the emerging capitalist marketplaces of early modern Europe,
the South Sea Bubble of 1720 occurred when investors rushed to put their money in the South Sea Company,
which had been granted a British royal monopoly on trade with Latin America in exchange for assuming a large
national debt incurred by the British government during the War of the Spanish Succession.
The origins of the bubble go back nearly a decade earlier, to the decision of the British government to grant a
monopoly on trade with Latin America to Robert Harley, a well-connected politician of the day who served as Lord
Treasurer in the early 1710s. (In those days, all the waters surrounding Latin America were referred to as the
“South Seas.”) The granting of the charter was part of a complex deal Harley and the government had come up
with to retire about £10 million in short-term debt it had incurred fighting the War of the Spanish Succession, a
conflict that had begun in 1701 to prevent the merger of the French and Spanish crowns, and thereby upset the
balance of power in Europe.
Harley was looking to develop a means to sell the debt, or government bonds, through private channels. Setting
up a joint-stock bank was the most straightforward means to this end. The bank, whose shares would be owned
by private investors, would then invest in the government bonds. Under British law, however, the Bank of England
was the only joint-stock bank permitted to do business in the country.
So Harley came up with a scheme. He would create a company whose official mandate was trade but would, in
fact, really function as a bank. (Underlining the secondary role of trade in the company’s bottom line, the Treaty of
Utrecht, which ended the War of the Spanish Succession in 1713, granted the company the right to send just two
ships a year to Spanish colonies in Latin America, one of them a slave ship.) To get investors to buy into the firm,
the government offered the South Sea Company a perpetual annuity. In other words, the bank would own the £10
million in debt forever, with a guaranteed return of 6 percent, or a total payment of over £500,000 each year.
In 1717 and 1719, the company took on even more debt. Indeed, in the latter year it assumed roughly half the
British national debt, or about £31 million, with guarantees of slightly lower returns of 5 percent through 1727 and
4 percent thereafter. The idea was that the government would convert high-interest debt to low-interest debt,
thereby saving it money, while investors would be offered a guaranteed return forever.
The returns were solid but not spectacular and failed to attract many investors. In 1719, the company changed its
marketing strategy, playing up the trading possibilities of its monopoly with Latin America, even though such
possibilities were meager. The strategy worked, producing a frenzy of investment. By the early eighteenth century,
the British economy had emerged as the most powerful and expansive in Europe, having displaced the Dutch as
the continent’s great trading nation. Many of the merchants and others who had done well by this trade had
excess capital to invest.
Shares in the company took off in value, from £128 in January 1720 to more than £500 by May, an increase of
more than 300 percent. To maintain government support for the scheme, politicians were essentially loaned
shares and then allowed to sell them when the price went up, thereby pocketing profits on money they had never

actually invested. The company then used the names of these prestigious individuals to sell more shares in the
company to less well connected investors.
Meanwhile, other financiers tried to jump on the bandwagon by developing similar joint-stock trading companies in
Britain, as well as in France and the Netherlands. To stop such activity, at least in Britain, the government then
passed the Royal Exchange and London Assurance Corporation Act in June 1720, requiring all joint-stock
companies to get a royal charter. This stipulation effectively ended any competition to the South Sea Company,
which had a charter, sending its shares even higher to about £1,000 at the peak of the bubble in August 1720.
By this point, thousands of ordinary people were buying shares or partial shares in the company, often with loans
given to them by the company expressly to make the purchases. By August, however, some of the loans were
coming due. When investors could not come up with the money to pay them back, they began unloading shares.
These sales exerted downward pressure on prices, which brought share values back to about £150 by
September. Contributing to the downward push were the simultaneous collapses of other joint-stock schemes
elsewhere in Europe, most notably, France’s Compagnie des Indes, or Company of the Indies. Indeed, some
historians refer to 1720 as the “bubble year.”
The collapse in the value of South Sea Company shares reverberated through the British financial system as
banks that had accepted stock in the company or had made loans against those stocks began to fail. Thousands
of individual investors also failed financially, including some of the most illustrious names in Britain.
A period engraving satirizes the South Sea Bubble of 1720 with symbols of greed and suffering, including
clergymen gambling (bottom left) and investors being taken for a ride—literally—on a merry-go-round. (Hulton
Archive/Getty Images)
Responding to widespread outrage, Parliament launched an investigation that not only uncovered fraudulent
activities among the company’s directors but corruption and bribery that went to the highest level of government,
including members of the cabinet, some of whom were impeached. Led by Robert Walpole, the newly appointed
lord treasurer, Parliament confiscated the estates of the company’s directors and used the proceeds to partially
compensate defrauded investors. Meanwhile, remaining stock in the company—essentially the national debt it held
—was turned over to the Bank of England and the East India Company.

James Ciment
 
See also:  Asset-Price Bubble;  Mississippi Bubble (1717-1720);  Tulipmania (1636-1637); 
United Kingdom. 
Further Reading
Balen, Malcolm. The King, the Crook, and the Gambler: The True Story of the South Sea Bubble and the Greatest Financial
Scandal in History. New York: Fourth Estate, 2004. 
Dale, Richard. The First Crash: Lessons from the South Sea Bubble. Princeton, NJ: Princeton University Press, 2004. 
Garber, Peter M. Famous First Bubbles: The Fundamentals of Early Manias. Cambridge, MA: MIT Press, 2000. 
 
Southeast Asia
 
Southeast Asia, a region bounded by India to the west and China to the north, consists of twelve nations with very
different demographics, histories, and economies. These include Brunei, Cambodia, Indonesia, Laos, Malaysia,
Myanmar (Burma), Papua New Guinea, Philippines, Singapore, Thailand, Timor-Leste (East Timor), and Vietnam.
Demographically, the region is dominated by giant Indonesia, Philippines, Vietnam, Thailand, and Myanmar.
Altogether, the region has a population of about 600 million.
The culture of the region has long represented a cross of indigenous, Indian, and Chinese influences. The major
religions of the region are Islam (Brunei, Indonesia, and Malaysia), Buddhism (Cambodia, Laos, Myanmar,
Singapore, Thailand, and Vietnam), Hinduism (parts of Indonesia), Christianity (Philippines and many Vietnamese),
and animist faiths (Papua New Guinea).
Home to some of the most vibrant civilizations in Asian history prior to the sixteenth century CE, much of the
region—aside from Thailand—fell under the sway of European colonizers between the sixteenth century and the
first half of the twentieth. After being occupied by the Japanese during World War II, most of the countries in the
region won their independence peacefully, though Indonesia and Vietnam were forced to fight protracted wars of
independence in the 1940s and 1950s. In the years since, three of the countries—Cambodia, Laos, and Vietnam
—became communist states, though all three now combine a communist-dominated political system with a
transitional market economy. For the most part, the countries in the region are democracies—with the significant
exceptions of Vietnam and Myanmar—though most have been under authoritarian regimes in the recent past.
Economically, the region offers dramatic contrasts. Tiny Cambodia, still recovering from the genocide of the 1970s,
and Myanmar, ruled by a brutal junta since the early 1960s, are among the poorest nations in Asia, while oil-rich
Brunei and the city-state of Singapore are among the richest.

Many of the countries in the region have seen significant export-driven economic expansion in recent decades,
despite the major setback of the Asian financial crisis of 1997–1998. This has especially been the case in the
middle-income countries of Indonesia, Malaysia, and Thailand, along with Vietnam.
 Economies and Population of Southeast Asian Countries, 2009 (estimated) 
Country
Population (in
millions)
GDP (in millions of
dollars)
GDP per capita (in
dollars)
Brunei
0.4
14,700
36,700
Cambodia
14.8
10,900
800
Indonesia
240.3
514,900
2,200
Laos
6.3
5,721
900
Malaysia
28.3
191,400
6,800
Myanmar (Burma) 50.0
26,820
500
Papua New
Guinea
6.7
8,200
1,200
Philippines
92.0
158,700
1,700
Singapore
5.0
177,100
35,500
Thailand
67.8
263,500
3,900
Timor-Leste (East
Timor)
1.1
599
500
Vietnam
88.1
97,120
1,100
Total
600.8
1,470,060
2,447
Source: CIA World Factbook.
 History Through Independence
Even before European penetration of the region, Southeast Asia was a major center of regional trade, offering
spices and tropical goods, such as exotic animals and rare hardwoods, to China. Situated at the crossroads
between the great civilizations of the Indian subcontinent and East Asia, it was populated—particularly the regions
that would become Malaysia and Indonesia—by merchants from both, who also brought their culture with them.
Next came Arab merchants from across the Indian Ocean beginning in the eighth century CE. As with the
Europeans who would follow, the Arabs came in search of spices, which were used in their own territories or
traded to Europe. And as with the Indians and Chinese before them, Arabs helped culturally influence the ever
more cosmopolitan littoral of the region, bringing advances in the sciences, along with the Islamic faith.
The first Europeans to arrive in the region were the Spanish and the Portuguese in the sixteenth century, the
former conquering parts of the Philippine archipelago and the latter ousting Arab traders from strategic ports in
what are now Indonesia and Malaysia. The Portuguese, in particular, developed a lucrative trade, offering silver
mined in the Americas, along with European manufactured goods, for the spices grown in the region, which
included nutmeg, cinnamon, cloves, and, most important, pepper.
But the Portuguese were soon pushed from the region by the more entrepreneurial and better-armed merchants
of the Dutch East India Company in the early seventeenth century. The Dutch would eventually establish

settlements on Java and other parts of Indonesia, which, as the Dutch East Indies, would remain part of the
overseas empire of the Netherlands into the middle of the twentieth century. The Spanish established colonial
settlements in the Philippines, but their economic exploitation of the archipelago was not as extensive as that of
the Dutch in the East Indies. However, they did eventually convert much of the country to Catholicism, and the
Philippines became the only Christian-majority country in Asia.
For the most part, mainland Southeast Asia remained independent of European colonizers into the nineteenth
century. Between the 1850s and the 1880s, however, the French military extended its sway over Indochina—
present-day Cambodia, Laos, and Vietnam. The British, meanwhile, began to occupy the Malay Peninsula in the
late eighteenth century and established the critical trading port of Singapore in the early nineteenth. Myanmar
(then Burma) also came under British control in the nineteenth century. Only Thailand remained as an
independent kingdom, serving as a buffer territory between French Indochina and British possessions in Burma
and the Malay Peninsula.
The French and British more effectively exploited the region economically than had earlier Dutch, Spanish, and
Portuguese colonizers. Rather than merely trading for spices and other indigenous commodities and manufactured
goods, both the French and British established large plantations and developed extensive mining concerns. The
British, for example, established a near hegemony over the world rubber trade by the early twentieth century, and
French Indochina became a major supplier of tin.
European economic and political control of the region was finally broken not by indigenous independence
movements but by another outside force, the Japanese in the 1940s. Having adopted European technology,
Japan became militarily powerful and imperialistic in the first half of the twentieth century. In 1941, it launched a
multipronged invasion of Southeast Asia that overwhelmed the small European garrisons in the region. The
Japanese also conquered the Philippines, which had become an American colony as a result of the Spanish-
American War of 1898.
The Japanese talked of “Asia for the Asians” and incorporated Southeast Asia into their Greater Asian Co-
Prosperity Sphere. However, the sphere was really an empire by another name. The region was meant to serve
as a supplier of raw materials, including crucial petroleum, which had been drilled for in Indonesia by the Royal
Dutch Shell Company since the early twentieth century, and a captive market for Japanese manufactured goods.
Never popular, the Japanese faced tough resistance from local independence movements, which were aided by
the Americans and the British. At the end of World War II, the Japanese were driven from the region.
 Early Independence: 1940s Through 1970s
While the Japanese occupation was brief, it had far-reaching consequences for Southeast Asia, as independence
movements became more emboldened in militarily confronting European colonizers. The Dutch were challenged in
Indonesia, which they surrendered in 1949; the British faced armed resistance in Malaysia, eventually pulling out
in 1957. The toughest fighting, however, occurred in Vietnam, where the French attempted to set up a puppet
regime under their control until being defeated, after a brutal independence war, in 1954.
While each was different in its own way, all of these nationalist struggles were inspired by a mixture of nationalism
and left-wing economics and politics. Each country went its own way after independence. In Indonesia, a socialist
regime headed by independence leader Sukarno (Indonesians often go by a single name) was overthrown in a
bloody coup in the 1960s, leading to years of dictatorial corruption and economic stagnation under Suharto.
Ethnically mixed Malaysia went a more capitalist route, but with a government policy of economically favoring the
native Malay population over the more entrepreneurial Chinese and Indian populations.
Vietnam split into two warring halves, a communist north and a capitalist south, eventually drawing in American
forces in the 1960s and finally being reunited under communist rule in 1975. That struggle would be mirrored by
similar civil wars between pro-Western governments and communist-inspired guerrilla movements in Cambodia
and Laos before both fell to the latter in the mid-1970s. Cambodia’s fate proved catastrophic as the new Khmer

Rouge regime attempted to return the country to what it called “year zero,” a kind of precapitalist utopia, murdering
millions of people in the process.
Chinese-dominated Singapore broke from Malaysia in 1965 and soon established itself as a critical trading and
financial hub, becoming the second richest nation in the region, after oil-rich Brunei. The Philippines, the most
economically advanced country in the region upon winning its independence from the United States in 1946, also
fell under a corrupt dictatorship, that of Ferdinand Marcos, and stagnated economically. Thailand, always
independent, also went a more pro-Western, capitalist route.
 Rapid Development and Crisis: Since the 1980s
The economic and political history of the various countries in Southeast Asia began to converge again in the
1980s and 1990s. To varying degrees, all of the larger countries in the region—with the notable exception of
authoritarian Myanmar—began to industrialize rapidly, first developing low-tech manufacturing, such as textiles and
shoes, and then moving on to more value-added products, such as electronics. In many cases, much of the
foreign capital came from abroad, with American, European, and Japanese companies establishing branch plants
in the region to take advantage of lower labor costs.
Still a relatively poor country, Vietnam has enjoyed one of the world’s fastest-growing economies since the early
1990s on the strength of market reforms. A billboard advertises private villas in the city of Da Nang, site of a
major airbase during the Vietnam War. (Hoang Dinh Nam/AFP/Getty Images)
This general economic convergence was evidenced institutionally in the growth of the regional policy coordination
organization, the Association of Southeast Asian Nations (ASEAN). First founded by Indonesia, Malaysia, the
Philippines, Singapore, and Thailand in 1967, ASEAN had expanded to include all the countries of the region—
even isolationist Myanmar—by the mid-1990s. It had also become more of a coherent organization, with member
countries—along with important outsiders such as China—using the frequent summits to coordinate economic,
security, and environmental policies, among other things.
During this same period, two of the largest countries in the region—Indonesia and the Philippines—made the
transition from dictatorship to democracy, while Malaysia, Thailand, and Singapore saw a more modest transition
to semi-democratic rule. In Myanmar, the military junta continued its authoritarian rule, while Vietnam, following
the path pioneered by China, combined one-party communist political rule with an emerging free-market economy.
Southeast Asia’s emergence as an economic powerhouse did not come without its setbacks. Its reliance on large

infusions of foreign investment for rapid growth led to speculation in local securities and real estate in the 1980s
and 1990s. In addition, many of the countries in the region failed to modernize their financial sectors, with
governments providing little regulatory oversight of banks. The result was financial bubbles that eventually popped,
as foreign investors began to pull capital out of these markets, first in Thailand in 1997 and then across much of
the region. The Asian financial crisis of 1997–1998 set back the growth of much of the region through the early
years of the twenty-first century, though the impact varied between hard-hit Thailand and the less-affected
Vietnam.
A booming China and a growing world economy from 2002 to 2007 helped revived the fortunes of the region, as
virtually all of the countries experienced strong growth rates. When an even greater financial crisis rocked much of
the industrialized world in 2008, many experts expected the region to be badly hit, as foreign investors pulled out
and exports to recessionary Europe and the United States shrank. However, while growth rates in the major
Southeast Asian economies did slow—and even shrank slightly in Malaysia and the Philippines in 2009—overall
the region’s economy proved more resilient, buoyed by strong internal demand and pulled along by a still surging
Chinese economy. Economists even began to speak of Southeast Asia—as well as other rapidly developing
regions of the world—becoming “decoupled” from the more advanced but sluggish Western and Japanese
economies.
James Ciment
 
See also:  Asian Financial Crisis (1997);  China;  India;  Indonesia;  Philippines. 
Further Reading
 Association of Southeast Asian Nations (ASEAN):  www.aseansec.org
 Borthwick, Mark. Pacific Century: The Emergence of Modern Pacific Asia.  3rd ed. Boulder, CO: Westview, 2007. 
 McGregor, Andrew.  Southeast Asian Development.  New York:  Routledge,  2008. 
 McLeod, Ross, and Ross Garnaut, eds.  East Asia in Crisis: From Being a Miracle to Needing One?  London:  Routledge, 
1998. 
 Owen, Norman G., ed.  The Emergence of Modern Southeast Asia: A New History.  Honolulu:  University of Hawai’i Press, 
2005. 
 Rigg, Jonathan. Southeast Asia: The Human Landscape of Modernization and Development.  2nd ed. New
York: Routledge, 2003. 
 SarDesai, D.R. Southeast Asia: Past and Present.  6th ed. Boulder, CO: Westview, 2010. 
 Stiglitz, Joseph E., and Shahid Yusef, eds.  Rethinking the East Asian Miracle.  New York:  Oxford University Press,  2001. 
Spain
 
A medium-sized country of about 47 million people located on the Iberian Peninsula in southwestern Europe,
Spain has a long and varied economic and political history. Inhabited since at least 35,000 BCE, Spain is home to
several distinct ethnic groups, including the dominant Castilian speakers of the middle and southern parts of the
country, the Basques of the north, the Galicians of the northwest, and the Catalans in the northeast. Spain has

been populated by Celts and Iberian peoples since the third century BCE, when it was conquered by Rome. By
the early eighth century, some 300 years after the fall of the Roman Empire, much of the country was occupied by
Moorish invaders.
For the next 700 years, the Christian kingdoms of the north fought a successful battle to drive the Moors out. That
goal finally was achieved in 1492, the same year Christopher Columbus, an Italian explorer in the employ of the
Spanish monarchy, sailed to the Americas. The voyage inaugurated a period of conquest during which Spain
seized much of the Americas and other overseas possessions. During the fifteenth and sixteenth centuries, Spain
was the dominant power in Europe, before falling behind Great Britain and France. It languished economically and
politically through much of eighteenth, nineteenth, and twentieth centuries, emerging as a dictatorship following a
civil war in the 1930s.
Since the fall of the dictatorship in the 1970s, Spain has emerged as an economically and politically vibrant
democracy, joining the European Community (later the European Union) in 1986. Several decades of high growth
came to an abrupt end, however, with the financial crisis of 2008–2009, when a housing bubble and construction
boom that had buoyed the economy burst. As a result, the country faced various domestic economic problems
and the effects of global recession.
 Economic History Through the Conquest of the Americas
Home to the caves of Altamira, with paintings dating to roughly 15,000 BCE, Spain was inhabited by Celts,
Iberians, and Basques at the beginning of the second millennium BCE. The region was incorporated into various
Phoenician and Greek trading networks by the early part of the first millennium, becoming an important source of
gold and silver. Both established trading colonies along the country’s Mediterranean coast, with the Carthaginians,
the North African–based heirs of the Phoenicians, taking control of the region around 300 BCE.
Rome seized Spain from the Carthaginians during the Punic Wars of the third century BCE. Under the Romans,
the culture of much of Spain was Latinized, and local leaders became part of the Roman aristocracy. The province
of Hispania, as it was called, was incorporated into the Roman economy, exporting gold, silver, mercury, olive oil,
and wine.
With the fall of the Roman Empire in the fifth century C.E., Spain came under the rule of various Christianized
Visigoth leaders, until much of the Iberian Peninsula was conquered by invading Muslim armies in the early eighth
century. For the next 700 years, the country was divided between an ever-expanding northern Christian section
and a slowly contracting Moorish Muslim south.
Despite sporadic warfare with the north, Moorish Spain flourished in the Middle Ages as a center of learning,
commerce, and art. Not only a source of great scientific, literary, artistic, and philosophical achievements, Moorish
Spain also served as a conduit through which the learning of the ancient and Islamic worlds was passed on to
Christian Europe, helping to bring about the Renaissance. The cities of Moorish Spain also were major centers of
trade, with a large merchant class that exported the region’s products to both the Islamic and Christian worlds.
The north prospered as well, gradually being integrated into the Mediterranean economy of Christian Europe, with
pilgrims traveling to the shrine of Compostela providing a major source of foreign capital.
The 700-year reconquista, or reconquest, of the Iberian Peninsula created a militarized state in the north, in which
successful conquistadores, or conquering warriors, were rewarded with extensive land grants that they often
turned into large-scale wool-producing sheep and cattle ranches. The pattern of giving huge land grants and
feudal control over local peasants would be replicated when Spain conquered much of the Americas in the
sixteenth century.
In 1469, the crowns of Aragon and Castile, the two main Christian kingdoms of the Iberian Peninsula, were united,
leading to the conquest of Granada, in Andalusia, the last Moorish outpost in western Europe, in 1492. That year
proved momentous in Spanish history. Not only did it mark Spain’s first encounter with the Americas, but it also

saw the government issue an order expelling the Jews, who, as merchants, traders, and artisans, had been
integral to the commercial success of Moorish Spain.
The conquest of the Americas, with its Aztec and Inca riches and lucrative silver and gold mines, brought a flood
of money into the country. However, this newfound wealth proved to be a mixed blessing for the Spanish
economy. The influx of precious metal devalued the currency, creating runaway inflation and making Spanish-
made goods less competitive in foreign markets. In addition, the newly enriched Hapsburg monarchy in Spain
used its wealth to launch a series of wars, both to expand its holdings in Europe and to fight the wars of the
Counter-Reformation in an effort to halt and roll back the Protestant revolution in northern Europe.
 Wars, Economic Decline, and Dictatorship: 1600s Through 1975
Together, the ongoing wars, the loss of much of the merchant class through the expulsion of the Jews and Moors,
and the inflation set off by the influx of American gold and silver eventually undermined the Spanish economy and
power in Europe, leading to a long-term decline during the seventeenth and early eighteenth centuries. Spain lost
most of its European possessions in the aftermath of the long-term independence struggle of the Netherlands in
the late sixteenth and early seventeenth centuries and the War of the Spanish Succession in the early eighteenth
century.
The War of the Spanish Succession put into power a new dynasty, the French Bourbons, who moved to establish
a more centralized government, ending the loose alliance of provinces. Under the Bourbons, the Spanish
government adopted administrative reforms to make its tax collection more efficient and imposed tighter controls
over trade with its possessions in the Americas, both of which led to some economic improvement.
The Napoleonic Wars of the early nineteenth century undid much of that progress as armies battled across Spain,
wrecking the economy. Unable to control Atlantic sea routes, Spain could not stop an independence movement
that saw much of mainland South and Central America fall from its grip by the 1820s. Throughout the rest of the
nineteenth century, Spain remained one of the most economically backward countries in western Europe,
predominantly agricultural—and inefficient agriculture at that—and a stunted manufacturing center. Outside the
Basque and Catalan regions of the north, where metalworking and textile industries emerged, the country largely
was bypassed by the economic dynamism of the industrial revolution that swept much of northern Europe in this
period.
The decline culminated in what Spaniards call the “disaster of’98,” the country’s abject defeat at the hands of the
rising economic and military power of the United States in the Spanish-American War of 1898. The United States
either seized Spain’s colonies in the Americas and the Philippines for itself or, in the case of Cuba, granted it
independence.
Well into the early twentieth century, Spain lacked effective irrigation to make its lands more productive; its land
tenure system did not encourage agricultural innovation; its banking sector offered little credit for businesses and
industry; and road building, education, and other state services remained well behind those in much of the rest of
western Europe. Napoleon’s famous quip that Europe ended at the Pyrenees, the mountains separating Spain
from France, still seemed to hold true.
Modest reforms to help manufacturing in the 1920s, including state planning and tariffs, largely were undone by
the Great Depression and a catastrophic three-year-long civil war in the late 1930s, which left much of the country
destroyed, wiped out the country’s gold and foreign currency holdings, and brought to power the fascist
dictatorship of Francisco Franco, who would rule the country until his death in 1975. Not until the 1950s would
manufacturing and agricultural output reach their pre–civil war levels.
Under Franco, Spain was saddled with a large and unresponsive bureaucracy, which led to cronyism, corruption,
and stagnant growth through the early 1960s, when free-market reforms were imposed. The government also
introduced measures at the time to bring its own fiscal house in order and to reduce inflation. Together, these

reforms, along with an increasing tourist sector and remittances from Spanish workers who were drawn to the
booming economies of western Europe, led to substantive growth in the 1960s and early 1970s. Spain benefited
from the economic boom in the rest of western Europe and built a transportation and industrial infrastructure, even
as it modernized its agricultural sector, which became a major source of citrus and other subtropical products for
the rest of Europe.
 Economic History After Franco
The death of Franco ushered in a democratic government in the mid-1970s, but the oil shocks of that period
undermined growth, as they did in much of the industrialized world. The stagnant economy and desire for change
led to the victory of the Socialist Party in the early 1980s, less than a decade after the end of a regime that
essentially had outlawed socialist politics. But the Spanish socialists were no economic radicals. They applied
fiscal discipline to the government, closed inefficient state enterprises, and improved labor market flexibility.
Between the late 1980s and mid-2000s, Spain—which in 1986 joined the European Community, the predecessor
of the European Union—had one of the fastest-growing economies in Europe, though it continued to be plagued
by one of the continent’s highest unemployment rates. The country’s rapid growth was the result of several
factors: relatively low wages, compared to much of the rest of the European Union, which made it competitive;
successive governments that maintained solid macroeconomic fundamentals to keep inflation in check; and large
subsidies from the European Union.
Never before had the Spanish economy seen such a long period of continued high growth. For the twelve years
from 1995 to 2007, gross domestic product (GDP) grew at an average annual rate of 3.5 percent. This impressive
performance was fueled mainly by a construction boom that was unparalleled in western Europe. Half of all new
houses in Europe during this time were built in Spain and sold in advance to both investors and ordinary citizens
mainly from European countries (United Kingdom, Germany, and, of course, Spain).
However, the construction boom began to run out of steam by the middle of 2007 and was dampened further by
the financial crisis that dried up most of the international funding that financed the country’s huge infrastructure
investments in 2008. In turn, this sparked higher unemployment—more than 14 percent by early 2009—and
dragged down investment and consumer spending. Fortunately, Spain avoided excessive exposure to the
subprime mortgage crisis that wreaked havoc in the United States because of the Bank of Spain’s strict
regulations on commercial banks.
Spain suffers from a particularly specific economic crisis, with causes different from those of the international
financial crisis of the 2000s. It was not simply a construction boom that went wrong in the country. Rather, the
economy was overheated beyond capacity, causing a severe current account deficit, as imports were sucked in to
satisfy the voracious demand fueled by the jump in construction activity and the wealth effect of steadily rising
property prices. Apart from this, it became clear by the mid-to late 2000s that Spain’s economy was derailing, a
situation made worse by the external debt accumulated by the inflow of foreign funds and the large number of
migrant workers who were sucked into the boom. As of late 2009, there were 5 million immigrants working in
Spain, making up about 12 percent of the population—the highest proportion of first-generation immigrants in the
European Union. In an effort to free up more jobs for Spaniards, the government offered legal immigrants a
monetary incentive for returning to their home countries. For those who agreed to leave Spain for at least three
years, the government paid the unemployment benefit they were entitled to receive in a lump sum—40 percent
upon leaving and 60 percent upon arrival back home—with average payments running approximately $14,000.
All the ingredients were in place for the country’s downturn during the global economic collapse of 2008–2009.
Spain’s property market dropped fast, declining as much as 60 percent in some places. Up to 1.5 million unsold
new homes stood empty in 2009, equivalent to five years of sales at the depressed rates. The prices of existing
homes fell by 10 percent or more well into 2009, and more than 1,000 Spanish property and building firms filed for
bankruptcy in 2008. The same number followed suit in 2009 as they struggled to repay more than €447 billion
($625 billion) in debt. Meanwhile, the collapse of Spain’s decade—long housing boom, according to economists,

increased nonperforming loans from 4 percent in 2009 to some 9 percent by early 2010, threatening the solvency
of savings banks, which hold more than half of all property debt.
In an effort to bolster the ailing economy, the Spanish government in November 2008 instituted a €38 billion ($52
billion) stimulus package of spending and tax measures, which received a mixed reception from economists and
business groups. One of the plan’s chief measures, cash handouts to households totaling around €20 billion ($28
billion), did little to support the economy. In addition to the controversial cash benefits, the supplementary budget
allocated €10 billion ($14 billion) to revitalizing local economies and supporting small companies. Bills linked to the
supplementary budget also included a measure to raise the limit on injecting public funds into ailing banks.
Although the new plan intended to boost investment in public infrastructure by €25 billion ($35 billion), channeled
through regional and local governments, Spain’s budget deficit remained below the European Union threshold of 3
percent of GDP in 2009. At the end of the year, the public deficit stood at 8 percent, and the International
Monetary Fund predicted that it would reach double that amount in 2010.
Under Prime Minister José Luis Rodríguez Zapatero, the Spanish government remained steps behind other
European countries in the global economic crisis. The nation’s 19 percent unemployment rate ranked second only
to Latvia’s in the European Union. The government’s series of emergency welfare measures aimed at saving jobs,
increasing consumption, and reviving the stagnant housing market did little to bolster Spain’s economy, and
economists predicted that the country would experience a level of economic decline unprecedented in the last half
century of its history.
As a result of the collapse in construction and the housing market, along with the global economic slowdown,
Spain saw GDP growth slow to just 0.9 percent in 2008 and shrink by 3.9 percent in 2009, one of the poorest
performances in western Europe. In 2010, it remained essentially flat, at -0.1 percent. Moreover, by early 2010, in
the aftermath of financial difficulties in Greece, there were fears in international financial markets that Spain’s large
public deficit—estimated at 11.4 percent of GDP in 2009, well above the European Union limit of 3 percent—
threatened to send foreign investors in Spanish government securities fleeing, making it harder and costlier for the
country to borrow to finance its debt. The government did impose austerity measures in 2010, which lowered the
deficit to 9.4 percent of GDP but failed to satisfy investors who continued to demand much higher interest rates on
Spanish bonds, further exacerbating the fiscal situation. Moreover, the ongoing crisis in Greece cast a pall over
other trouble Eurozone countries and, as of late 2011, many experts feared that the sovereign debt crisis hitting
Greece and Portugal would spread to the much larger economies of Spain and Italy.
James Ciment and Jesus M. Zaratiegui
 
See also:  Greece;  Ireland;  Portugal. 
Further Reading
Barton, Simon. A History of Spain. New York: Palgrave Macmillan, 2003. 
Carr, Raymond. Spain: A History. New York: Oxford University Press, 2000. 
“Europe: Unsustainable; Spain’s Economic Troubles.” The Economist, November 28, 2009. 
Harrison, Joseph, and David Corkill. Spain: A Modern European Economy. Burlington, VT: Ashgate, 2004. 
“Spain’s Happy-Go-Lucky Government: When Good Politics Is Bad Economics.” The Economist, July 30, 2009. 
“Spanish Banks: The Mess in La Mancha.” The Economist, April 2, 2009. 

Spiethoff, Arthur (1873–1957)
 
Arthur Spiethoff was a member of the German historical school of economics. His research on business cycles, on
which he had his greatest impact, was based on Mikhail Tugan-Baranovsky’s overinvestment theory. The impulse
to overinvest, he suggested, was sparked by such innovations as technological inventions and by the discovery of
new markets.
Arthur August Kaspar Spiethoff was born on May 13, 1873, in Dusseldorf, Germany. He became a leading
economist of the “younger generation” of the German historical school, which disagreed with many of the central
constructs of the Austrian school of economics, such as the nature of business cycles and the methodology of
economics in general.
Spiethoff was heavily influenced by the economist Gustav Schmoller and later by the works of Ukrainian
economist Mikhail Tugan-Baranovsky, who themselves had influenced the work of Nikolai Kondratieff, a major
figure in business-cycle theory. In general, these economists believed that overinvestment—which occurred for
any number of reasons, including a misreading of the available market—was the cause of downturns in the
business cycle. They argued that overinvestment led to overproduction, which resulted in a surplus of a particular
good, thus forcing down its price. When this became widespread, it would lead to a recession. Spiethoff first
outlined this idea in “Preamble to the Theory of Overproduction,” published in 1902.
As the influence of the Austrian school’s economic theories about business cycles grew, Spiethoff became one of
the school’s most vigorous opponents. Whereas Spiethoff and the economists of the German historical school
focused on endogenous (internal) economic forces—especially investment patterns—to explain business cycles,
the Austrian school pointed to exogenous (external) psychological factors, or the role of the individual and
individual choice in economic processes. Spiethoff—one of the few German school economists to make much
headway against the Austrian school and the economic mainstream it represented—rejected theory in favor of
empirical research. Using empirical data to illustrate historical patterns in the numerous booms and busts of
nineteenth-century Europe, he argued that before the mid-1800s, most economic downturns and periods of great
prosperity had correlated to periods of war, and good or bad harvests. He pointed out that in the nineteenth
century, this changed, as countries’ economies became more and more closely linked, especially on the European
continent, where large-scale mass production was taking place. According to Spiethoff’s theory of business cycles,
published in 1923, companies during periods of continuous boom initially use their profits to pay shareholders and
owners. Next, companies begin to invest in new factories that can produce more goods to satisfy increasing
demand. Companies pay for much of this expansion with borrowed money. As a result, during economic
downturns, the economy is hit twice—first with a credit shortage, rising interest rates, and growing debt when
companies cannot repay their loans, and again when consumers’ wages diminish (or are expected to diminish),
and they buy less.
Ironically, Spiethoff, an empiricist and antitheorist, devoted much of his later research and writing to examining the
relationship between empirical and theoretical economics. After World War II, his major works included “The
Historical Character of Economic Theories,” published in the Journal of Economic History (1952); and “Pure
Theory and Economic Gestalt Theory,” published in Enterprise and Secular Change (1953). Spiethoff died on April
4, 1957, in Tübingen, Germany.
Justin Corfield
 
See also:  German Historical School. 

Further Reading
Blaug, Mark. Great Economists Before Keynes. An Introduction to the Lives and Works of One Hundred Great Economists
of the Past. Atlantic Highlands, NJ: Humanities, 1986. 
Spiethoff, A.  “Crises.” International Economic Papers 3([1925] 1953): 75–171. 
Spillover Effect
 
Spillovers are the exchange of ideas among individuals or groups, especially when businesses expand into new
locations and begin operations. Once operations have gotten under way and firms have conducted business for a
period of time, spillover begins to occur. Since the nineteenth century, knowledge and technology spillovers are
believed to have been major drivers of productivity growth and economic expansion in regional and national
economies throughout the Western world. As such, knowledge and technology spillovers are important factors in
the expansion phase of business cycles.
In this context, there are two types of spillovers: knowledge spillovers and technology spillovers. (Technically
speaking, there are other, less positive types of spillovers, or externalities, as economists sometimes call them.
Examples include the noise produced by an airport or carbon released into the atmosphere by utilities companies.
This article, however, focuses on the narrower definition and more salutary understanding outlined above.)
Knowledge spillovers can be defined as the exchange of information developed and shared from within the same
industry or between industries. For example, a green company develops new technology to produce solar cells
that will save energy and reduce emissions—an innovative product that requires unique materials and inputs.
Unless the company is entirely vertically integrated, it will need to buy materials from outside businesses in order
to build the solar cells. Thus, very quickly, some of the knowledge and expertise to produce the new solar cells
will be shared outside the company. Moreover, employees of the green company may get hired by a vendor or
competing firm. The institutional knowledge and expertise required to produce the new solar cells is thereby
transferred to outside interests and is no longer secret. This can be considered knowledge spillover, as the
essential know-how from the original firm has spilled over to other firms. Often in a market economy, the external
company will use its newfound knowledge to compete with the originating firm. Also, once an industry gets to a
certain size in an area, local community colleges might start training programs to supply workers with the skills
needed in the industry. When the industry is small, no such programs would be started.
Knowledge spillovers generally occur within the same industry, but they can also occur between companies in
unrelated or marginally related industries or sectors. Spillover between directly competing companies is
exemplified by the following hypothetical: Hewlett-Packard (HP) develops a new computer code that requires less
hardware and saves significant energy. One of the HP engineers who helped develop the code then decides to
leave the company and work for Dell, a competing computer manufacturer. The engineer then transfers the
knowledge developed at HP to his new employer, and knowledge spillover has taken place simply by virtue of the
employee job change. Depending on the extent and nature of the information, the knowledge spillover may have a
significant impact on marketplace competition.
Spillovers also occur within companies belonging to unrelated industries. In the same hypothetical situation, HP
develops a new computer code. The engineer who helped develop it leaves the company and goes to work for a
solar cell manufacturer. If this manufacturer is able to use the computer code to help improve one of its own
programs or processes, a true knowledge spillover has occurred between industries. In today’s world, firms are

increasingly relying on contract hires as opposed to full-time permanent employees because of cost benefits and
the lack of long-term commitments to regular employees. Contract hires are brought on board for specific projects
and move much more easily from one employer to the next. In such a world, the spillover of knowledge from firm
to firm and industry to industry is enhanced.
The intangible nature of knowledge spillover and the lack of empirical evidence regarding its effects—or even its
specific occurrence—have given rise to considerable debate within the economics community. According to Nobel
Prize–winning economist Paul Krugman, “Knowledge flows are invisible; they leave no paper trail by which they
may be measured.” In other words, when any type of spillover occurs, it is difficult, if not impossible, to quantify
the amount of spillover or its effects.
Technology spillovers are similar in definition and pattern to knowledge spillovers but entail the transfer of tangible
goods. In other words, technology spillovers are the exchange, development, and improvement of technologies
among individuals or groups. Like knowledge spillovers, they can occur both within and between industries.
As early as the mid-nineteenth century, knowledge and technology spillovers have been closely associated with
the spread of industry and advanced technology. New England gave rise to the textile and machine-tool clusters
along the rivers of Massachusetts and Connecticut; upper New York State spawned the Niagara-region cluster
based on cheap hydroelectric power; and more recently, the Silicon Valley in northern California became a
national center of computer and electronics development. In all of these instances, knowledge and technology
spillovers played a vital role in establishing and expanding highly competitive but closely interlinked business
communities—all of which were focal points of national economic growth during expansive periods in the business
cycle. By the same token, the close intellectual and economic relationships formed by knowledge and technology
spillovers between companies in a creative cluster can also have a damaging domino effect, with the failure of
one company damaging another company during times of economic retraction. This phenomenon was much in
evidence in the Silicon Valley during the economic crisis of 2007–2009.
Michael Rawdan
 
See also:  Technological Innovation. 
Further Reading
Carlino, Gerald A.  “Knowledge Spillovers: Cities’ Role in the New Economy.” Business Review, Federal Reserve Bank of
Philadelphia (4th quarter, 2001): 17. 
Kokko, A.  “Technology, Market Characteristics, and Spillovers.” Journal of Development Economics 43:2 (1994): 279–293. 
Krugman, Paul. Geography and Trade. Cambridge, MA: MIT Press, 1991. 
Sprague, Oliver (1873–1953)
 
Oliver Mitchell Wentworth Sprague was a Harvard economist and an adviser to the U.S. government and other
nations in the years prior to World War II. An expert on banking and banking crises, he believed that increased
taxation, price regulation, and competition were necessary to minimize the impact of economic crises, such as the
Great Depression.

Sprague was born on April 22, 1873, in Somerville, Massachusetts. He attended St. Johnsbury Academy, in
Vermont, and Harvard University, from which he received a bachelor’s degree in 1894, a master’s in 1895, and a
doctorate in economics in 1897; he then joined the faculty as an assistant professor in economics. From 1905 to
1908, on sabbatical from Harvard, Sprague taught economics at the Imperial University in Tokyo, Japan. Upon his
return to Harvard, he helped establish the university’s Graduate School of Business Administration, where he
became an assistant professor in banking and finance before being named Edmund Cogswell Converse professor
of banking and finance in 1913. Sprague’s first major work, History of Crises Under the National Banking System,
was published in 1910; Banking Reform in the United States appeared the following year. Sprague held the
Edmund Cogswell Converse chair at Harvard until his retirement in 1941.
In addition to his academic responsibilities, Sprague worked as an adviser to the U.S. government from the late
1910s to the early 1930s. In this capacity, he was able to apply his economic research to the formulation of public
policy at the federal and international levels. During World War I, he recommended funding the escalating war
expenditures through increased taxation rather than through government borrowing. This fundamentally
conservative view put him at odds with many politicians who believed it was easier to borrow money in order to
keep the government and the banking system “healthy”—a view Sprague roundly criticized in his third major work,
Theory and History of Banking (1929).
With the onset of the Great Depression, Sprague went to London at the behest of the British government, serving
as an adviser to the Bank of England from 1930 to 1933. Soon his expertise was being sought—and received—by
governments and financial institutions around the world, including Germany’s Reichsbank, the Bank of France, the
Bank for International Settlements, and the League of Nations.
In 1933, Sprague returned to the United States to participate in the establishment of the New Deal by the new
Franklin D. Roosevelt administration. Sprague’s work as a financial and executive assistant to Secretary of the
Treasury Henry Morgenthau led to his highly controversial 1934 booklet Recovery and Common Sense, which
caused a major split within the administration over the best way to achieve economic recovery. Sprague
recommended raising taxes to pay for New Deal projects, arguing that lower prices and increased competition
would stimulate demand for consumer goods as well as for capital.
Resigning from government service in 1933 over Roosevelt’s decision to end the gold standard and devalue the
dollar, Sprague devoted more time to academic research, receiving a doctor of letters degree from Columbia
University in 1938. After his retirement from Harvard in 1941, he served on the board of directors of the National
Shawmut Bank of Boston, on the advisory board of the Massachusetts Investors Trust, and as an adviser on
foreign exchange issues for the General Motors Corporation. Sprague died in Boston on May 24, 1953.
Justin Corfield
 
See also:  Great Depression (1929-1933);  New Deal;  Regulation, Financial. 
Further Reading
Cole, A.H., R.L. Masson, and J.H. Williams.  “Memorial O.M.W. Sprague, 1873–1953.” American Economic
Review 44(1954): 131–132. 
Schlesinger, Arthur M. The Coming of the New Deal. London: Heinemann, 1960. 
Sprague, O.M.W. History of Crises Under the National Banking System. Washington, DC: Government Printing
Office, 1910. 
Sprague, O.M.W. Theory and History of Banking. New York: Putnam, 1929. 

Sraffa, Piero (1898–1983)
 
Italian economist Piero Sraffa is regarded as one of the twentieth-century giants in the field. His book Production
of Commodities by Means of Commodities: Prelude to a Critique of Economic Theory (1960) is credited with
starting the neo-Ricardian school of economic thought, which combined the fundamental ideas of early-nineteenth-
century economics with twentieth-century mathematical and theoretical advances.
Sraffa was born on August 5, 1898, in Turin, Italy, to Angelo and Irma Sraffa. He attended schools in Parma and
Milan and graduated from the University of Turin. After serving in the Italian army during World War I, he returned
to Turin and earned a doctorate in 1920; his thesis was titled “Inflation in Italy During and After the War.” From
1921 to 1922, he studied at the London School of Economics. Back in Italy, he served as director of the provincial
labor department in Milan and then as a professor of political economy at universities in Perugia and Sardinia.
Sraffa’s experience in World War I made him a lifetime pacifist, and he strongly opposed the rise to power in Italy
of fascist Benito Mussolini. His doctoral thesis had earned him a reputation as a “monetarist” interested in the role
of the money supply in economic systems. This, in addition to his friendship with Italian Communist Party leader
Antonio Gramsci, led him into conflict with Mussolini’s government. The situation worsened following the
publication of several articles in the Economic Journal and the Manchester Guardian in which Sraffa exposed the
problems that had led to Italy’s banking crisis.
British economist John Maynard Keynes came to Sraffa’s rescue, inviting him to the University of Cambridge.
There, Sraffa and Keynes—both book collectors as well as economists—became close friends. In 1925, Sraffa
translated into Italian Keynes’s A Tract on Monetary Reform, and both men argued against the Austrian school’s
theory of the business cycle as a product of overproduction and deficient demand. Sraffa also opposed Alfred
Marshall’s neoclassical economic theories, which stressed—and indeed relied on—the belief that, when making
economic decisions, people behave completely rationally in order to optimize their material well-being. With
Keynes’s help, Sraffa was appointed a lecturer in the faculty of economics at the University of Cambridge.
Keynes’s ideas were greatly influenced by Sraffa, and the two argued publicly against the neoclassical economists
regarding the forces behind business cycles.
Painfully shy, Sraffa despised lecturing. He resigned his post at Cambridge in 1930 and, with Keynes’s support,
was appointed Marshall librarian at King’s College, Cambridge, before being named assistant director of research
there. In 1931 Sraffa started on a monumental, twenty-year project editing the complete writings of nineteenth-
century economist David Ricardo, whom he greatly admired. Published between 1951 and 1971, the eleven-
volume Works and Correspondence of David Ricardo was noted for its clear interpretation of classical and
neoclassical economic theory, particularly surplus theory, the labor theory of value, and the basis of Karl Marx’s
critical analysis of capitalist production.
Sraffa was named a fellow of Trinity College, Cambridge, in 1939 and a reader in economics in 1963. In 1961, he
was awarded the prize of the Stockholm Academy of Science, the highest award for economics at the time. (The
Nobel Prize for Economics was not awarded until 1969). After editing Ricardo’s works, Sraffa’s major theoretical
work, Production of Commodities by Means of Commodities (1960), reinterpreted Ricardo’s theory for the
twentieth century and led to the founding of the so-called neo-Ricardian school and the classical revival at
Cambridge. Sraffa’s work influenced not only Keynes but also, among others, philosopher Ludwig Wittgenstein.
Following Sraffa’s death on September 3, 1983, economist Paul Samuelson remarked that he doubted whether
any scholar who had written so little had contributed so much to economic science.
Justin Corfield

 
See also:  Keynes, John Maynard. 
Further Reading
Schefold, B.  “Piero Sraffa 1898–1983.” Economic Journal 106: 438 (1996): 1314–1325. 
Sraffa, Piero. Production of Commodities by Means of Commodities: Prelude to a Critique of Economic Theory. New York
and Cambridge, UK: Cambridge University Press, 1960. 
Stability and Stabilization, Economic
 
A stable economy is one in which there is sustained growth along with low inflation and low unemployment. It is
an economy in which there is general equilibrium between aggregate supply and aggregate demand. Another sign
of stability is that the economy is neither experiencing an unsustainable boom or bubble, which is often
accompanied by high inflation, nor is it experiencing little or negative growth, which is usually accompanied by
high unemployment. Stability, then, is not the same as stasis. An economy that is not growing and changing is, in
fact, an economy that is malfunctioning.
Since economic stability is a desirable condition, governments employ two basic kinds of policies to achieve,
recover, or maintain it: fiscal policies (taxing and spending) and monetary policies (adjusting the money supply).
Usually, governments employ some of both to affect economic stability, although sometimes the policies work
against one another. For example, expansionary fiscal policy may be accompanied by contractionary monetary
policy, and vice versa. Collectively, such efforts are known as economic stabilization policies. For the most part,
these policies focus on the demand side of the equation.
 Fiscal and Monetary Policy
When an economy begins to slow or contract—that is, when it is entering or already in a recessionary period—
economic output begins to fall significantly below the economy’s potential, leaving both workers and capital
equipment idle. To lift aggregate demand, the government can employ fiscal policy—that is, taxing and spending
policies—attempting to spur investment by lowering taxes on businesses and to spur consumption by lowering
taxes on households. Alternatively, or at the same time, the government can increase spending on infrastructure,
stimulating demand for goods from the private sector and giving jobs to idle workers, or it can increase transfer
payments that are then spent on consumption, which increases demand and employment. Similarly, governments
can impose higher taxes or cut spending to cool an economy if it is overheating and threatening to trigger inflation.
Of course, by spending money or cutting taxes during recessionary periods, which usually coincide with lowered
revenues, governments run budget deficits, which can have negative effects on future growth and inflation if they
became too large.
To stabilize an economy, a government can also use monetary policy, essentially altering interest rates and the
growth rate of the money supply. To do so, the government, through its central bank, has several tools at its
disposal. In the United States, the Federal Reserve Bank can raise or lower the interest rate it charges member
banks for loans, thereby forcing commercial banks to raise or lower their interest rates to businesses and
households. Making money more expensive leads to less borrowing and thus shrinks, or slows, the growth of
money in circulation. This essentially makes money more expensive, thereby cooling inflation. By lowering interest

rates, central banks make it cheaper to borrow, thereby increasing the money in circulation and helping to
stimulate demand and employment. The central bank can also purchase or sell government securities. Purchasing
securities puts more money in circulation; selling them shrinks the money in circulation. Finally, a central bank
such as the Federal Reserve can increase or lower liquidity requirements for commercial banks. By increasing
these—that is, by requiring banks to hold more money against their outstanding loans—they in effect lower the
amount of money in circulation. Conversely, by decreasing liquidity requirements, they increase the amount of
money available for borrowing by businesses and households, thereby increasing aggregate demand. The
preponderance of economic stabilization by the Federal Reserve is done through the second of the above-noted
methods—that is, the buying and selling of government securities to pump loanable funds into the economy or to
pull them out.
 Bureaucracy and Politics
Theoretically, both fiscal and monetary policy can be effectively used to alter aggregate demand and therefore
achieve economic stability. The real world, however, presents more complexities. In democratic countries such as
the United States, fiscal policy is initiated by elected representatives. This immediately presents two problems in
its goal to achieve economic stability. The first concerns timeliness. Legislatures often find it difficult to move
quickly in response to rapidly changing economic situations. For example, while the recession of the late 2000s
began in the fourth quarter of 2007, it took the U.S. Congress until after the November 2008 elections to pass a
major stimulus package. In addition, legislators naturally find it easier to cut taxes than to raise them. In other
words, it is politically easier to stimulate aggregate demand to fight a sluggish economy than it is to slow it down.
Finally, fiscal policy often fails to work even when employed in a timely fashion. There are several reasons for this.
First, most people view changes in the tax code as temporary and fail to adjust their spending patterns in
response. In addition, even when businesses and consumers respond to a tax cut or rebate, the effect lasts only
as long as the tax cut and rebate are in effect. Thus, for example, the 2009 Cash for Clunkers program, in which
the federal government offered a rebate to consumers exchanging an old car for a new, fuel-efficient one,
produced a flurry of demand in the automobile business but only for a short time period and not enough,
according to most economists, to lift demand for that troubled sector over the long term. Also, a long time line is
often required in order for a spending increase to work its way through the economy. For example, it could take a
year or two for the government to get an infrastructure project going. By that time, the economy may have turned
on its own, and the impact of the stimulus might hit just at the time when the economy needs to slow down.
By contrast, monetary policy is much more indirect. Rather than injecting money directly into the economy, it tends
to pull the strings behind the economy, triggering the private financial sector to change its lending patterns and
hence stimulate or curb demand. Moreover, monetary policy in most democracies is under the control of
nonelected central bankers. In the United States, the chairman and governors of the Federal Reserve are
appointed by the president with approval of the Senate to fourteen-year terms, somewhat insulating them from
political pressure. These officials are not entirely insulated from the will of the people, but they nevertheless enjoy
great independence from the political process. This ensures that monetary policy is not unduly influenced by the
election cycle, which otherwise could hamper economic stability. Elected central bankers might well choose to
stimulate the economy to create an illusion of prosperity before an election even if the state of the real economy
does not call for such stimulus. This is more critical since inflation, which is the cost of any overly stimulative
policy, occurs with a lag and might not begin until after the election. Moreover, as institutions, central banks are
less unwieldy than legislatures, allowing them to respond more quickly to changes in the economy.
 Shift from Fiscal to Monetary Policy
Because of all of these factors, both economists and policy makers have shifted the emphasis in their thinking and
activities over the course of the late twentieth and early twenty-first centuries from fiscal to monetary policy as the
most effective means of altering demand and achieving economic stability. They have tended to utilize the former
only during times of especial economic distress, such as the “stagflation” period of the 1970s and early 1980s

(where the response was the 1981 Reagan tax cuts) and the recession of the late 2000s (responding with the
economic stimulus package of 2009).
Yet while most economists agree that monetary policy is more effective than fiscal policy in responding to
changes in the economy and securing economic stability, the debate remains heated over how much impact such
policies have and over what time period. Studies have found that increasing the money supply through the various
means at the disposal of central banks tends to more effectively stimulate output in the short term, in the first one
to three years. Wages and prices, however, tend to respond more sluggishly, with most of the inflation caused by
increasing the money supply felt three to five years out. After five years or so, most of the effect of the increase in
the money supply comes in the form of higher prices and wages. This fact must be accounted for in the decision-
making of monetary authorities if they are not to trigger too much inflation as a result of their efforts to lift
aggregate demand.
Still, while monetary policy is considered more effective in lifting overall demand, fiscal policy can be used to great
effect by shifting the composition of economic output. As the Cash for Clunkers program made clear, tax policy
can be used to stimulate a specific economic sector. More generally, it can be used to increase investment over
savings during times of recession and savings over investment during periods of inflation. In addition, fiscal
policies can alter the effectiveness of monetary policies, and vice versa. Thus, most policy makers recognize the
importance of coordinating the two policies in their efforts to achieve economic stability. In the real world, this often
has not happened, as large government deficits limit the increase of deficits in recessions and put the onus on the
Federal Reserve to fight inflation in situations where politicians do not want to either increase taxes or cut
government spending.
Finally, there is an international dimension to economic stabilization policies. Monetary policy in particular has to
be considered in the light of international capital flows, especially for countries that are highly dependent on them
for their growth, such as many of those in the developing world. For example, expanding the money supply by
lowering interest rates may stimulate growth, but it can also trigger capital flight, as international investors seek
better returns on their capital elsewhere, thereby curbing investment and undermining any increase in aggregate
demand triggered by the initial effort to increase the money supply. Thus, in the pursuit of economic stability, a
government can trigger the opposite.
James Ciment
 
See also:  Employment and Unemployment;  Fiscal Policy;  Growth, Economic;  Inflation; 
Monetary Policy;  Tax Policy. 
Further Reading
Lewis, William W. The Power of Productivity: Wealth, Poverty, and the Threat to Global Stability. Chicago: University of
Chicago Press, 2004. 
Nardini, Franco. Technical Progress and Economic Growth: Business Cycles and Stabilization. New York: Springer, 2001. 
Norton, Hugh S. The Quest for Economic Stability: Roosevelt to Bush.  2nd ed. Columbia: University of South Carolina
Press, 1991. 
Perkins, Martin Y., ed. TARP and the Restoration of U.S. Financial Stability. Hauppauge, NY: Nova Science, 2009. 
Røste, Ole Bjørn. Monetary policy and Macroeconomic Stabilization: The Roles of Optimum Currency Areas, Sacrifice
Ratios, and Labor Market Adjustment. New Brunswick, NJ: Transaction, 2008. 
Wray, L. Randall. Understanding Modern Money: The Key to Full Employment and Price Stability. Northampton,
MA: Edward Elgar, 2003. 

Steindl, Josef (1912–1993)
 
Although he was trained in the Austrian school of economics, Josef Steindl developed his theories primarily in
England and was especially influenced by the Polish economist Michal Kalecki. Steindl’s books Small and Big
Business: Economic Problems of the Size of Firms (1945) and Maturity and Stagnation in American Capitals
(1952) are considered classics of the postwar economics literature.
Born in Vienna, Austria, on April 14, 1912, Steindl was educated at the Hochschule für Welthandel (now the
Vienna University of Economics and Business Administration) in that city. He received his doctorate in 1935, after
which he worked for three years at the Austrian Institute of Economic Research, founded by Ludwig von Mises,
where he was introduced to the work of John Maynard Keynes. When the Nazis occupied Austria in 1938, Steindl
lost his job and was forced to flee the country. With the help of Friedrich von Hayek, von Mises, and Gottfried von
Haberler, he received a post at Balliol College, Oxford, and then at the Oxford Institute of Statistics, which had
become home to a group of European intellectuals fleeing fascism.
At the institute, Steindl met Kalecki, whom he later acknowledged was his greatest intellectual influence. Kalecki’s
emphasis on imperfect competition and his denial of neoclassical labor theory (or the marginal productivity of labor
demand theory) formed the basis of Steindl’s own economic theory. The denial of perfect competition—along with
Keynesian theory on the policy side and a Marxist historical perspective—combined with Steindl’s formidable
mathematical talent to give his thinking a unique heterodox shape.
Steindl’s first book, Small and Big Businesses: Economic Problems of the Firm, was a microeconomic
investigation into the effects of business size on profitability, cost structures, investment decisions, and capacity.
Steindl contended that the economy has a tendency toward large-scale production, owing to the fact that, as a
result of economics of scale, cost factors favor large enterprises. Although not denying the existence of small
enterprises, Steindl believed that the representative firm is the oligopolistic firm. Thus, any investigation into
general economic developments must use this as a starting point for contemplation of the larger macroeconomic
problems of unemployment, slow growth, and unused (both capital and labor) capacity. This refutation of Alfred
Marshall’s idea of the cyclical rebirth of small firms through a perfectly competitive environment endogenized the
elements that were said to lead to economic instability.
Maturity and Stagnation in American Capitalism was a macroeconomic continuation of the microeconomic study in
Small and Big Business. Written at the institute in Oxford, the book is an investigation of the Great Depression.
While the Depression was difficult to explain in terms of orthodox economic doctrine, Steindl’s thesis states that
the crisis had its roots in the growing monopolization of industrial economies. He maintains that when output in
specific industries becomes concentrated in the hands of a few firms, competition fails, and with it, investment.
In a perfectly competitive model, the interaction of profits, prices, investment, and capacity utilization combine to
maintain a macroeconomic equilibrium (through the entry and exit of small firms). In an oligopolistic market,
however, there are no small firms to eliminate, and therefore, excess capacity cannot be reduced through
competition. In the standard model, any temporary economic contraction gives way to expansion; but in an
oligopolistic economy, declining investments continue undetected and lead to stagnation.
Steindl’s thesis placed the origins of the Great Depression in the growing concentration of industry, which was
itself a result of competition. Competition led to revenues, and winners accumulated business earnings that
needed to be invested. In Steindl’s model, the investment was entered in the market for smaller firms, thus
squaring the circle: competition led to concentration, which led to the Depression, and the result was endogenous

capitalist development.
Unfortunately for Steindl, the publication of Maturity and Stagnation, which coincided with the economic boom of
the 1950s, received little attention. The economic stagnation of the 1970s brought renewed interest in the book,
however, and Steindl, continuing to work on his theories, adding to his original model the increased role of
government and trade liberalization.
In 1950, Steindl returned to Austria and joined the Austrian Institute, where he continued to explore the problems
that occupied his intellectual mentors, Kalecki, Keynes, and Marx. There, he published works on saving,
investment, income distribution, stagnation, and growth. Steindl was made an honorary professor at Vienna
University of Economics and Business Administration in 1970 and was a visiting professor at Stanford University
in California in 1974–1975. When he died on March 7, 1993, he left behind a body of work that was a testament
to a mind searching for an economic view originating with theory and moving into issues of economic policy and
social justice.
Robert Koehn
 
See also:  Austrian School;  Great Depression (1929-1933);  Kalecki, Michal;  Keynes, John
Maynard. 
Further Reading
Shapiro, N.  “Market Structure and Economic Growth: Steindl’s Contribution.” Social Concept 4(1988): 72–83. 
Steindl, Josef.  “Capitalist Enterprise and Risk.” Oxford Economic Papers,  no. 7 (1945): 21–45. 
Steindl, Josef. Economic Papers, 1941–88. London: Macmillan, 1990. 
Steindl, Josef. Maturity and Stagnation in American Capitalism. Oxford, UK: Basil Blackwell, 1952. 
Steindl, Josef. Random Processes and the Growth of Firms. London: Hafner, 1965. 
Steindl, Josef. Small and Big Business: Economic Problems of the Size of Firms. Oxford, UK: Basil Blackwell, 1945. 
Steindl, Josef.  “Stagnation: Theory and Policy.” Cambridge Journal of Economics 3(1980): 1–14. 
Steindl, Josef.  “Technical Progress and Evolution.” In Research, Development and Technological Innovation, ed. Devendra
Sahal. Lanham, MD: Lexington, 1982. 
 
Stimulus Package, U.S. (2008)
 
The Emergency Economic Stabilization Act of 2008, popularly known as the fiscal stimulus package of 2008, was
a $170 billion federal program aimed at easing the effects of a recession that had begun in the last quarter of the

previous year. Heavy on tax cuts and rebates, the package failed to prevent a deepening of the economic
downturn, leading Congress and the new Barack Obama administration to pass an even costlier and more far-
reaching stimulus package in 2009.
 Deepening Recession
By late 2007, there were a host of signs that the U.S. economy was entering a period of slow or negative growth.
Beginning in late summer 2006 came a decline in home prices across much of the country. For some five years
prior to that, U.S. home prices had undergone unprecedented increases, a result not so much of overall prosperity
but of government policies—most notably efforts by the Federal Reserve Board (Fed) to lower interest rates,
reducing the rates paid by homebuyers for mortgages—and the financial industry’s development of new types of
more affordable mortgages. As home equity values rose and mortgage rates fell, homeowners came to believe
they could spend more on general consumer purchases, thereby creating greater economic activity and growth.
The fact that many of the new mortgages were based on adjustable rates created a dire situation when rates rose.
Many homeowners were forced to sell their homes or go into foreclosure, which contributed to a rapid decline in
home prices. As existing homeowners saw their equity continue to shrink, in many cases owing more than the
value of the property, they cut back significantly on overall spending. Retailers began to see the change during
the Christmas season of 2007, as sales dropped off sharply from the previous year. Meanwhile, the consumer
confidence index had declined steadily through much of 2007, even as unemployment rate remained relatively
low. The jobless rate remained below 5 percent in February 2008—when President George W. Bush signed the
stimulus package into law—but many policy makers, looking ahead to the 2008 elections nine months away,
feared it would rise as economic activity slowed.
The collapse in the housing market created problems in the financial industry as well. Many banks and other
financial institutions found themselves highly exposed to nonperforming mortgages, particularly of the subprime
variety that allowed people with low income or bad credit to obtain mortgages, usually of the adjustable-rate
variety. Many of these mortgages had been packaged into securities and sold to other financial institutions,
increasing their exposure to the continuing decline in housing prices. Further destabilizing the financial industry
were collateralized debt obligations, securitized assets that amounted to a kind of insurance policy taken out by
financial institutions against mortgage-backed securities. In short, rising foreclosure rates were creating a ripple
effect throughout the financial industry, both in the United States and abroad.
As recession loomed in early 2008 (in fact, later numbers would show that it had begun in the fourth quarter of
2007), both President Bush and Congress decided to take action. Congress was urged on by Fed chairman Ben
Bernanke’s testimony that prompt, strong measures were needed to prevent an economic downturn. In January
2008, Bush proposed a stimulus package of just under $150 billion, almost all of it devoted to tax cuts and
rebates. While there was a consensus in Congress that a federal stimulus was needed to prevent a recession, the
exact form the stimulus should take was hotly debated.
 What Kind of Package?
Governments generally have three options for stimulating an economy: tax cuts or rebates, increased spending, or
a combination of the two. In general, Democrats tend to favor more government spending, either in the form of
increased payments for unemployment, aid to states, and food stamps or in infrastructure projects. When they do
push for tax cuts or rebates, they tend to want them targeted at lower-and middle-income taxpayers. More often,
Democrats argue, increased federal benefits pump more money into the economy in the short term than do tax
cuts. Tax refunds are often saved rather than spent, and tax cuts may take a while to show up in consumer and
business spending patterns. While the economic impact of infrastructure projects may also take time to be felt,
they provide a more permanent stimulus. Nevertheless, in the debate over the 2008 stimulus package, Democrats
also called for special tax incentives to support investment in green technologies.
Republicans, to the contrary, tend to prefer tax cuts and rebates for all income groups and businesses. An

exception to this trend is the earned income tax credit, a refundable credit aimed at lower-income workers, which
is more typically supported by Democrats and opposed by Republicans. The reasons they give are both pragmatic
and ideological. First, Republicans argue that it is more in keeping with American values to allow taxpayers to
keep more of their own money and let them choose how they want to use it. Second, they argue, putting more
money in the hands of consumers and businesses allows more efficient market forces to determine how that
money will be spent. In addition, they say, tax rebates put money into the economy much more quickly than
infrastructure projects. In the debate over the 2008 stimulus package, Republican legislators also pushed for an
accelerated depreciation allowance for small businesses. (Depreciation is the amount of money a business can
write off against an asset as it loses value over time, such as a computer that becomes increasingly outdated;
accelerating the allowance offers a more immediate tax break for businesses, thereby encouraging capital
equipment purchases.)
Despite the extent and nature of the debate, seasoned congressional observers were impressed at the alacrity
with which Congress moved to pass a stimulus package. Historically, Congress has acted to stimulate the
economy only when a consensus develops that market forces—usually taking the form of lower prices and wages,
which stimulate demand and hiring—will take too long to reduce unemployment or increase production. By early
2008, such a consensus had emerged.
The package proposed by President Bush in January 2008 totaled $145 billion, but by the time the bill wended its
way through Congress, the projected cost of the program was nearly $170 billion. In late January and early
February, the House passed its version of the stimulus by a margin of 385–35 (with 11 congressmen present but
not voting), and the Senate voted 81–16 in favor of a slightly different bill (with 3 present but not voting). The
House immediately approved the Senate bill, which Bush signed into law on February 13.
Even though Democrats had seized control of Congress in the 2006 midterm elections, the stimulus package as
approved by legislators and signed by the president emphasized tax cuts over federal spending. Holding slim
majorities in both houses of Congress and facing a conservative Republican in the White House, the Democrats
felt it was necessary to take the traditional GOP approach to stimulus in order to avert congressional filibuster or
presidential veto.
Under the stimulus plan, married couples with a taxable income of under $150,000 who filed joint returns in the
spring of 2008 received $1,200 in rebates, plus an additional $300 per child. Individual taxpayers with a taxable
income of less than $75,000 received a rebate of $600, plus $300 per child. Those with incomes of more than
$150,000 were ineligible. In addition, the package included an accelerating depreciation allowance for newly
purchased equipment costing up to $800,000 and an increase in the size of mortgages that could be backed by
government-sponsored enterprises, such as Fannie Mae and Freddie Mac, thereby lowering interest payments.

With congressional leaders and cabinet members looking on, President George W. Bush signs the Emergency
Economic Stabilization Act of 2008. The measure granted tax rebates of $300 to $1,200 to American households,
totaling $170 billion. (Alex Wong/Getty Images)
 Effects
As the recession continued to deepen through 2008 and 2009—becoming America’s worst economic downturn
since the Great Depression—most economists came to the conclusion that the stimulus act of 2008 had failed to
turn the economy around. They cited several reasons for the lack of effect, most of them associated with the
observation that the tax rebates did little to encourage long-term changes in consumer behavior.
Foremost, critics pointed out that the stimulus was simply too small for an economy facing such a severe
economic crisis. In addition, the stimulus lagged because of a well-known factor economists call the “irreversibility
effect,” a phenomenon first theorized by the French-born physicist-turned-economist Claude Henry, based on the
work of English economist John Maynard Keynes in the 1930s. In times of economic uncertainty, Henry noted,
people are reluctant to spend money or to lend money to someone else so that they can spend because the
purchase of durable goods (such as a car or appliance) or a long-term loan to a borrower represents a serious
commitment that may be difficult to back out of when earnings fall. Decisions taken today about the purchases of
durable goods, especially when financed through credit, may be hard to reverse at a later date. In short, these
decisions are “irreversible.” Moreover, in times of economic distress, it is naturally better to wait for conditions to
improve before making a major expenditure. While consumers collectively might recognize that the economy would
be better off if everyone continued to borrow, lend, and spend at previous levels, self-interest dictates that
individual consumers will wait to see what happens with the economy. It was precisely this strong relationship
between effective demand and the irreversibility effect, according to many economists, that limited the
effectiveness of the 2008 stimulus package.

According to a survey by the National Retail Federation (NRF) in May 2008, just as the first rebate checks were
being mailed out, fewer than 40 percent of consumers planned to spend their rebate money—a figure seconded
by a Congressional Budget Office (CBO) analysis; the rest would save the money in case of future financial
setbacks, such as a job layoff. Indeed, later estimates indicated that the NRF survey and CBO analysis were
overly optimistic and that only 20 percent of tax refunds were put into the economy (spent) within six months.
As for the accelerated depreciation allowance, evidence has emerged that it may have cost the government up to
four times the amount in lost revenue that it generated in new business spending. This is because purchases of
new equipment and other forms of business investment may have been perceived as inopportune, despite the tax
advantages, for a variety of reasons—such as lack of confidence in future economic growth, heavy inventory, and
idle capacity.
Overall, assessing the impact of the 2008 stimulus package was complicated by the fact that, between May and
July 2008, as the rebate checks were distributed, real-estate credit became tighter, gasoline prices rose sharply,
and consumer confidence fell still further below the already low levels of late 2007. All three of these factors
countered the expansionary effect of tax cuts. In addition, after falling dramatically between October 2007 and
June 2008, investment spending in the third quarter of 2008 increased by only 0.4 percent over the second
quarter of that year and then dropped 12.3 percent in the fourth quarter. The difficulties faced by private and
federally supported financial businesses, such as Fannie Mae and Freddie Mac, especially since the global
financial crisis of late 2008, all contributed to lower consumer confidence, depressed investment spending, and
increasing unemployment.
With the general consensus among economists and policy makers that the 2008 stimulus package had failed to
halt a skidding economy, the candidates in the presidential campaign of that year began to debate the possibility
of a second stimulus. Some argued that the Bush stimulus was too small; others contended that it
overemphasized tax cuts and rebates and underemphasized government spending. Fiscal conservatives insisted
that the failure of the package to boost the economy indicated that no further stimulus measures should be taken.
By early 2009, with Democrat Barack Obama in the White House and a much larger Democratic majority in
Congress, the impetus for a new and much larger stimulus package—one emphasizing government spending
rather than tax cuts—gained momentum, ultimately leading to passage of the $787 billion American Recovery and
Reinvestment Act of 2009, popularly known as the economic stimulus package of 2009.
James Ciment
 
See also:  Fiscal Policy;  Recession and Financial Crisis (2007-);  Stimulus Package, U.S.
(2008);  Tax Policy;  Troubled Asset Relief Program (2008-). 
Further Reading
Bernanke, Benjamin.  “Irreversibility, Uncertainty, and Cyclical Investment.” Quarterly Journal of Economics 98:1 (February
1983): 85–106. 
Congressional Budget Office.  “Did the 2008 Tax Rebates Stimulate Short-Term Growth?” Economic and Budget Issue
Brief, June 10, 2009. 
Keynes, John Maynard. The General Theory of Employment, Interest and Money. New York: Harcourt, Brace, and
World, 1964. 
Rivlin, Alice.  “The Need for a Stimulus Package Now.” Testimony to House Budget Committee, January 29, 2008. 
Zandi, Mark M.  Written Testimony Before the House Committee on Small Business, Hearing on “Economic Stimulus For
Small Business: A Look Back and Assessing Need For Additional Relief.” July 24, 2008. 

 
Stimulus Package, U.S. (2009)
 
The U.S. financial crisis of 2008–2009, which resulted from the collapse of the nation’s housing bubble and
followed the declaration of bankruptcy by the investment banking firm Lehman Brothers in September 2008, sent
stock prices tumbling, unemployment soaring, and banks into a deep freeze during which they were lending no
money. Without lending by banks, firms could not borrow and remain in business, and consumers could not
borrow money to purchase goods such as homes and cars and faced reduced credit card limits.
As John Maynard Keynes and others explained during the Great Depression, two policy actions are needed to
deal with this kind of situation. First, the central bank must lower interest rates. In 2008, heeding that advice, the
Federal Reserve pushed its interest rates down toward zero. This move was replicated by many central banks
around the world, as the financial crisis and recession that resulted from the burst housing bubble in the United
States spread to other countries. Second, said Keynes, the government must cut taxes and increase its spending,
using fiscal policy to get the economy growing again. Because the George W. Bush administration was in its last
months, with presidential and congressional elections scheduled for November, the United States was at loss for
major fiscal policy changes in late 2008. It would have to await the next administration.
 Background
Upon his election in November 2008, Barack Obama with his economic advisers began devising an economic
stimulus package. Their original plan was for a large spending program with a few tax cuts thrown in. Over time,
in order to appeal to Republicans and conservative Democrats, the emphasis was more on tax cuts and less on
government spending. To appease those in Congress concerned about the deficit, the cost of the bill was reduced
from $1 trillion to less than $800 billion.
On January 26, 2009, David Obey (D-WI) introduced the American Recovery and Reinvestment Act of 2009 in the
House of Representatives; the bill passed two days later. The Senate approved a similar measure on February
10. Once the few differences were reconciled, the House passed a new bill on February 12 followed by the
Senate on February 13, with votes essentially following party lines. No Republicans in the House voted for the
measure, and only three Republican senators voted for it—Susan Collins and Olympia Snowe from Maine, and
Arlen Specter of Pennsylvania, who soon thereafter changed his party affiliation.

President Barack Obama seeks public support for his economic stimulus plan in February 2009. The $787-billion
package, passed by Congress later that month, included tax cuts, job-creation incentives, and social-welfare
benefits in the face of the ongoing recession. (Getty Images)
With much fanfare, President Obama signed the law on February 17, 2009, at an economic forum he was hosting
in Denver. Christina Romer and Jared Bernstein, the chief economic advisers to President Obama and Vice
President Joe Biden, respectively, estimated that the stimulus bill would create or save 3.5 million jobs over two
years, with more than 90 percent of them in the private sector. Its main features were tax cuts, an expansion of
unemployment insurance and other social programs to aid those hurt by the recession, and increased government
spending for education, health care, and infrastructure.
The Congressional Budget Office, an independent arm of Congress, estimated that the bill would cost $787 billion.
This figure included the interest costs of having to borrow money to provide fiscal stimulus to the economy and
subtracted the increased taxes and lower government spending resulting from the positive economic impact of the
stimulus.
A bit more than one-third of the stimulus package, or $286 billion, was devoted to tax cuts. The other $501 billion
involved additional government spending, including the future interest costs of borrowing money to stimulate the
economy.
 Tax Breaks
Most of the tax breaks in the stimulus bill went to individuals—about $116 billion of the total $288 billion in the bill
—with the rest going to businesses. A payroll tax credit of $400 per worker and $800 per couple added $13 per

week in 2009 to the average paycheck and $7.70 per week in 2010. Since this credit was refundable, people
owing no income tax also received it. The credit was phased out for individuals earning more than $75,000 and
couples making more than $150,000. The payroll tax credit for individuals cost a total of $116 billion.
The second-largest tax break dealt with the alternative minimum tax (AMT). Enacted in 1969 to keep wealthy
individuals from escaping taxation through large deductions, the AMT over time has applied to more and more
taxpayers because it was not indexed to inflation. Every year Congress has provided a temporary fix so the tax
would not hit middle-class households, but it has refused to make permanent changes, which would acknowledge
greater budget deficits in the future. The ATM patch for 2009 and 2010 cost $70 billion. This provision offered no
real stimulus, since it would have taken place anyway and did not give people more money to spend.
There were a number of smaller tax breaks for individuals as well. Some received little public attention, others a
great deal of publicity. In the former category were $4.7 billion to expand the Earned Income Tax Credit and $4.3
billion in tax credits to homeowners who made their homes more energy efficient by installing new energy-efficient
windows, doors, and air conditioners. In addition, at a cost of $5 billion, the first $2,400 of unemployment
insurance was made exempt from taxation, and college students or their parents received a tax credit of up to
$2,500 for tuition and related expenses in 2009 and 2010 (cost, $14 billion).
Better publicized was the $8,000 tax credit for new homebuyers on properties purchased by November 1, 2009;
the total estimated cost of this benefit was $14 billion. Its effects were noticeable in the summer and fall of 2009,
when median U.S. home prices rose by a small percentage for the first time since 2006 and home sales stabilized
after falling sharply for several years. The program was so successful that Congress expanded eligibility to June
30, 2010, to include existing homeowners who buy a new house.
To help the auto industry, persons buying new cars, trucks, and SUVs (costing up to $49,500) between February
18 and December 31, 2009, could deduct all sales taxes from their federal income tax. Taxpayers who do not
itemize their deductions were eligible for this benefit, but not wealthy taxpayers. The initial cost of the program
was $2 billion. When this program failed to spur auto sales, the government provided additional aid in June 2009
with its Cash for Clunkers program, which granted vouchers of $3,500 to $4,500 for purchasing a new fuel-
efficient car.
Numerous tax breaks, estimated to cost $51 billion, were directed at business firms. Firms could use current
losses to offset profits for the previous five years ($15 billion); they were given credits for renewable energy
production ($13 billion), and they were allowed to depreciate equipment such as computers more quickly for tax
purposes ($5 billion).
 Government Spending
The stimulus bill included two forms of government spending: funds spent directly by the federal government and
funds spent through aid to state and local governments. Unlike the federal government, which can incur an
ongoing deficit by increasing the money supply or borrowing from foreigners, state and local governments must
balance their annual budgets except for capital projects, such as building new schools. This becomes a problem
during times of recession, when tax revenues fall and state governments must spend more on programs to
support the needy (such as unemployment insurance and Medicaid). To keep their budgets in balance, state and
local governments must therefore raise taxes, cut spending, or both. These actions counter any stimulus from the
federal government, resulting in a smaller total impact from government actions.
One way around this problem is for the federal government to provide revenue assistance to state and local
governments. For this reason, the federal government increased its contributions to state Medicaid spending by
$87 billion and to state spending on education (public schools plus public colleges and universities) by $67 billion,
increased unemployment insurance benefits ($36 billion), gave additional benefits to the hungry and poor ($21
billion), and increased funds for teaching children with special needs ($12 billion). The federal government also
spent $4 billion so that state and local governments could hire additional police officers and expand programs to

prevent drug-related crimes, violence against women, and Internet crimes against children.
Most economists regard these provisions as the most effective part of the stimulus bill; they helped some state
and local governments avoid or limit tax increases and the laying off of public employees, which would have
significantly worsened the recession. Another success was government aid so that people could keep their health
insurance after being laid off. At a cost of $25 billion, the federal government subsidized nearly two-thirds of
health insurance premiums for the newly unemployed for a period of up to nine months.
Initially, the Obama administration stressed direct spending on “shovel-ready” projects, where spending could take
place immediately and jobs be created quickly. But this was just a small part of the final bill that Congress passed.
The stimulus package allocated $30.5 billion for building and repairing bridges and highways, improving public
lands and parks, and developing high-speed rail lines between major cities. A total of $11 billion went to make the
energy grid more efficient, and $6 billion went to water treatment projects. Several billion dollars went to other
infrastructure projects, such as dam repair and flood control, increasing energy efficiency in public buildings and
military facilities, and expanding broadband access in rural areas. In practice, it proved difficult to start these
projects quickly. As a result, few additional jobs were created in 2009 from infrastructure spending.
Finally, the stimulus bill gave a great deal of money directly to people. Interest expenses on the stimulus were
estimated at about $50 billion. The maximum Pell Grant to college students increased by $500 (to $5,350), a total
cost of $15.6 billion. A one-time payment of $250 was made to all Social Security and Supplemental Security
Income (SSI) recipients, plus everyone who was receiving veteran’s benefits. The cost of this provision was $14.4
billion.
 Assessment
From the moment it was proposed, the stimulus package generated a great deal of controversy. A full-page ad in
the New York Times funded by the Cato Institute, a right-wing think tank, criticized the large amounts of
government spending and feared the consequences of the large deficits. Among the 200 economists who
endorsed the ad with their signatures were Nobel laureates James Buchanan, Robert Lucas, and Vernon Smith.
Another petition signed by nearly 200 economists and sponsored by the Center for American Progress, a middle-
of-the-road think tank, supported the stimulus package as essential for dealing with the nation’s rising
unemployment rate. This petition was also signed by several Nobel Prize–winning economists, including Kenneth
Arrow, Lawrence Klein, Paul Samuelson, and Robert Solow.
From the left of the political spectrum, Nobel laureate Paul Krugman criticized the stimulus package as insufficient
to deal with the economic problems facing the United States. He called for a larger stimulus, focused more on
government spending and less on tax cuts, and argued that it would be hard politically for President Obama to ask
Congress for more spending after the first stimulus proved ineffective.
Other critics complained that $134 billion of the stimulus would not be spent until fiscal year 2011, and another
$90 billion not until fiscal years 2012 through 2015. This, they maintained, would do nothing to help the economy
until the end of 2010 and thereafter, when the economic crisis would likely be history and a stimulus unnecessary.
Moreover, several provisions of the stimulus bill, such as $18 billion to support scientific research and
development, would have positive long-term benefits but would not create many new jobs.
By late 2009, some economists were claiming “the proof is in the pudding.” Alan Blinder, for one, maintained that
the stimulus package, while hardly perfect, had basically done what was necessary and what Keynes had
prescribed. It provided large tax cuts to households and business firms and sharply increased government
spending. As such, it kept the U.S. economy from falling into another Great Depression, which many had seen as
a very real possibility only months earlier.
Steven Pressman

 
See also:  Fiscal Policy;  Recession and Financial Crisis (2007-);  Stimulus Package, U.S.
(2008);  Tax Policy;  Troubled Asset Relief Program (2008-). 
Further Reading
Blinder, Alan.  “Comedy Aside, an Obama Report Card.” New York Times, October 18, 2009. 
Burtless, Gary.  “The ‘Great Recession’ and Redistribution: Federal Antipoverty Policies.” Fast Focus, December 2009. 
Krugman, Paul. The Return of Depression Economics and the Crisis of 2008. New York: W.W. Norton, 2009. 
Pressman, Steven. Fifty Major Economists.  2nd ed. New York: Routledge, 2006. 
Stochastic Models
 
Stochastic business cycle models are mathematical representations that attempt to make predictions about the
economy’s ups and downs. Unlike traditional or deterministic macroeconomic models, stochastic business cycle
models incorporate the fact that the state of the economy in a country can be affected by random, unexpected
shocks. In the case of the 2007–2009 recession in the United States, for example, the random shock was the
unanticipated collapse of the financial market and economic system as a result of shock waves caused by
excessive subprime lending, liquidity issues in banking institutions, foreclosures, Ponzi schemes, declines in
investor and consumer confidence, regulatory failure, and a record high unemployment rate.
 Nature of Business Cycles
Business cycles, also referred to as trade or economic cycles, are time periods with identifiable patterns of
fluctuation in business and economic activity measured primarily by national income or real gross domestic
product (GDP). In the United States, these indicators are determined by the National Bureau of Economic
Research (NBER). Identification of a recession by the NBER is based on a notable decline in economic activity
across most industries lasting for two quarters in succession. The stage of the cycle is measured by the condition
of the country’s industrial productivity, employment status, consumption, and other key economic indicators. The
highest point in the expansion period is called the peak, and the lowest point in the contraction period is called
the trough. A full business cycle period includes an expansion period or boom (upward pattern from a trough to
peak) and a contraction period or bust (downward pattern from a peak to trough).
Although a typical cycle lasts about five years, it can vary from as little as a year to as long as a decade. In
between the expansion and contraction phases are progressive periods called growth and recession. The
amplitude of a business cycle—the difference between the extreme points of the cycle, from peak to trough—also
can vary significantly. Governments attempt to mitigate the amplitudes by implementing fiscal (tax-and-spend)
policies or monetary (changes in money supply) policies to ease volatility.
 Stochastic Process and Outcomes
The stochastic model attempts to describe the business cycle by including random or varied behavior due to
incomplete information and uncertain or unpredictable input variables as part of the independent or exogenous
factors. These stochastic processes usually involve time-related data series, subject to unpredictable random

variations. They are described by a probability distribution, that is, they are measured by the likelihood of an event
occurring. For example, suppose there is a 50 percent chance the market will begin to recover or grow again in
two years. Often the stochastic processes will have similar functions or mathematical ranges or have strong
correlations with each other, thereby making them more easily represented in the model.
 Time and Geographic Effects
Stochastic models are especially relevant and indispensable as new random factors are taken into consideration in
a fast-changing, volatile, and global economic environment. One example is a comparison of the 1930s and
2008–2009 financial crises. By looking at the different government policies and remedial rescue efforts in a
stochastic business cycle model theory, it is possible to better understand what caused each crisis and what
worked or could have worked to resolve them.
Another application of stochastic business cycle models is the examination of distinctive regional variations in
economic cycles. For example, the economic impact of the subprime mortgage crisis was more acutely felt in
markets, such as Florida and the West, where the housing bubble had inflated the most, even though these
regions followed general regulatory guidelines similar to the rest of the country’s. The stochastic model can help us
understand the different outcomes in business cycle stage between states that were triggered by varied levels of
real-estate business, the economic conditions prior to the burst of the bubble, and regions’ varying responses to
the problem.
 Contributing Factors
Research using stochastic models reaches a variety of findings. According to studies by economists Chang-Jin
Kim and Jeremy Piger, shifts in stochastic trends in an economy have permanent effects on the level of output,
though findings in a study by Whelan Karl suggest that shocks such as changes in technology are not dominant
forces driving the business cycle. In the international market context, studies have found that world real interest
rate shocks (adjusted for inflation) can explain up to one-third of the output fluctuations and more than half of the
fluctuations in net exports (a component of a nation’s GDP). The world real interest rate therefore can be an
important transmission mechanism in driving the business cycle phases in a small open economy.
In addition, stochastic models show that the prices of a nation’s exports and imports are an important source of
business fluctuations, or cycles, in a small and developing open economy. For instance, if the terms of trade (the
relative prices of exports and imports) deteriorates, the country may experience a downward business cycles.
Moreover, Stephen DeLoach and Robert Rasche find that, even for a large economy such as that of the United
States, permanent changes in exchange rates adjusted for inflation can have a large impact on business cycles.
Finally, M. Ayhan Kose has found that world commodity price shocks have a significant role in driving business
cycles in the developing economy.
Beryl Y. Chang
 
See also:  Shock-Based Theories. 
Further Reading
Canova, Fabio, and Angel Ubide.  “International Business Cycles, Financial Markets and Household Production.” Journal of
Economic Dynamics and Control 22:4 (1998): 545–572. 
DeLoach, Stephen B., and Robert H. Rasche.  “Stochastic Trends and Economic Fluctuations in a Large Open Economy.”
Journal of International Money and Finance 17:4 (1998): 565–596. 
Kim, Chang-Jin, and Jeremy Piger.  “Common Stochastic Trends, Common Cycles, and Asymmetry in Economic
Fluctuations.” Journal of Monetary Economics 49:6 (2002): 1189–1211. 

Kose, M. Ayhan.  “Explaining Business Cycles in Small Open Economies: ‘How Much Do World Prices Matter?’” Journal of
International Economics 56:2 (2002): 299–327. 
Mullineux, A.W. The Business Cycle After Keynes: A Contemporary Analysis. Brighton, UK: Wheatsheaf, 1984. 
Stock and Bond Capitalization
 
In microeconomics, stock and bond capitalization represents the ways that a business enterprise can commence,
continue, or grow its operations. The way a company gets the funds needed to operate is called its means of
capitalization. These means fall into three general categories: self-financing, debt financing, and equity financing.
As the name implies, self-financing means that the firm’s proprietor or proprietors provide all of the financing
themselves. Typical with start-up companies—which are small, require little capital, and lack a business record
that would allow them to secure financing from other individuals and institutions—self-financing has the advantage
of giving the proprietor complete control of the firm’s management and operations. The disadvantages come in the
relatively limited supply of capital available and the great risk to the proprietor of being solely responsible for the
company’s financial success. Self-financing means that the proprietor must finance the company with his or her
own funds or through the profits the enterprise makes.
A proprietor who seeks to expand operations but still retain full control of the company can use debt financing,
which can take two forms. One way is simply to borrow money from a financial institution, such as a bank or credit
union. In the case of sole proprietorship, the individual doing the borrowing is then liable for the debt; thus, even a
proprietor’s personal assets can be taken by the lender in the case of the company’s bankruptcy. For that reason,
many proprietors incorporate. This makes the corporation alone liable for the debts, leaving the personal assets of
the corporation’s owner or owners protected. In most cases, small corporations retain all of their shares internally
—that is, the shares are owned by the proprietor or proprietors. However, in most cases, even when the firm is
incorporated, banks require personal guarantees from the owners to lend funds to the firm. Incorporation does
prevent the owner’s personal assets from being at risk from a lawsuit against the firm.
A company may also secure capitalization through the issuance of bonds. In essence, a bond is a financial
instrument that requires the issuer to pay back the principal of the bond, plus interest, over a given period of time.
The terms of the bond, the record of the company, and the guarantees for paying back the bond determine the
interest rate. In general, a bond’s interest rate goes up with the perception of risk of nonrepayment attached to it.
Bonds tend to vary in their rates more than bank loans, where government regulations and bank policy determine
rates.
A third means of capitalization is equity financing. Rather than borrowing money from a bank or through the
issuance of bonds, equity financing involves selling partial or full ownership rights—in the form of stock shares—to
outsiders. Selling shares in a company turns it into a publicly owned firm. This dilutes the authority of the
proprietor, who must answer to the shareholders. Moreover, publicly owned corporations must abide by
government regulations and laws that require them to operate in a more transparent fashion than is the case with
a sole proprietorship, which only requires the owner to divulge financial information to tax authorities and to
holders of company debt.
In general, both financing through bonds and equity sales involve large enterprises and are usually conducted
through institutions that specialize in the business of capitalization, such as investment banks. Not only do

investment banks market the bonds and shares and provide expertise in the tax and legal complexities of bond
and equity financing, but they often underwrite the sale as well, by directly purchasing the newly issued stocks and
bonds and then marketing them. In this respect, the underwriter bears the risk that the instruments will not sell for
as high a price as expected.
Most large companies finance their operations in various ways simultaneously—equity and bond sales, and loans.
A company that does so is said to have both debt and equity capitalization. The relative amount of debt and equity
capitalization is known as the debt-to-equity ratio, which often determines the financial viability of the company. An
inordinately low debt-to-equity ratio may mean that the company is not adequately leveraging its assets and could
be underperforming, which can affect its share value. An extremely high ratio can put a strain on a company’s
capital flow, meaning that too much of its revenue is obligated to servicing its debts.
In the case of very large corporations, management and ownership are separated. The owners of the company
hire managers or executives who run the day-to-day operations and even conduct long-term strategy, though the
latter is usually conducted in coordination with the owners, who have the ultimate decision-making power. (Of
course, managers themselves may own shares, making them partial owners.) Moreover, in large corporations,
ownership may be so dispersed that the shareholders elect a board of directors to work with management in
fashioning long-term company strategy.
Such structures of ownership and management allow for the smooth operation of vast enterprises with widely
dispersed ownership, as is the case with most large corporations. During periods of economic expansion, there is
usually little tension between shareholders, boards of directors, and management. Executives receive bonuses—
often in the form of shares or share options, which allow them to buy shares in the future at a specified price—
while both the boards of directors and shareholders receive positive returns on their investment. When a company
starts to perform poorly, tensions can arise as shareholders, who are losing money on their equity stakes, begin to
chafe at the bonuses earned by managers—bonuses that are often agreed upon by boards of directors in
consultation with the managers themselves.
James Ciment
 
See also:  Capital Market;  Corporate Finance;  Debt;  Savings and Investment. 
Further Reading
Baskin, Jonathan Barron. A History of Corporate Finance. New York: Cambridge University Press, 1997. 
Haas, Jeffrey J. Corporate Finance in a Nutshell. St. Paul, MN: Thomson/West, 2004. 
 
Stock Market Crash (1929)

 
The largest decline in publicly traded securities prices in modern history, the stock market crash of late 1929
wiped out billions of dollars in paper fortunes before triggering a larger economic downturn that would plunge the
United States and much of the industrialized world into the worst depression of the twentieth century. The crash
followed an unprecedented run-up in stock valuation in the late 1920s—valuation that ran far ahead of the
underlying worth of the securities being traded. Economic historians cite this securities bubble, fueled by loose
credit, as the major cause of the crash. At the same, experts argue that it was deeper problems in the economy
that translated a downturn in securities prices into a major economic collapse. While the U.S. economy would
gradually recover through New Deal stimulus spending, the massive defense expenditures of World War II, and
pent-up demand after the war, the Dow Jones Industrial Average (DJIA), a key index of thirty major industrial
stocks, did not reach its pre-1929 crash level until 1954.
 1920s Economic Growth
By many but not all measures, the U.S. economy in the 1920s was strong and stable. Significant gains in
productivity—a result of breakthroughs in communication, electrification, and transportation—were coupled with
strong consumer demand, as Americans bought a host of newly available, mass-produced items such as radios
and automobiles. This combination of circumstances produced an economy marked by low unemployment and
inflation but solid growth. However, there were still significant weaknesses in the U.S. economy, mainly in
agriculture and declining industrial sectors such as coal (hit by competition from petroleum) and railroads (facing
competition from automobiles, buses, and trucks). The greatest weakness, according to some economists, was the
growing disparity in income and wealth. Much of the economic gain of the decade accrued to the top 10 percent
of the population, with the lion’s share going to the wealthiest 1 percent. The latter saw its share of national
income climb from 12 to 34 percent between 1919 and 1929, the fastest gain in American history. And even as
their income increased, their taxes went down. Thus, by 1929, the top 1 percent of the population owned more
than 44 percent of nation’s financial wealth, while the bottom 87 percent owned just 8 percent.
The wealth accruing to the top income earners had to go somewhere. A portion of it went toward conspicuous
consumption, contributing to the ebullient culture of the Roaring Twenties. But most of it was invested, much of it
in speculative pursuits. Through the early years of the decade, a significant portion of this money went into real
estate, driving up land, housing, and commercial building values to record highs. Suburbs blossomed across the
country as the automobile-owning middle class sought refuge from cities overcrowded with immigrants. In certain
areas of the country, such as Southern California and South Florida, the real-estate boom led to bubble
economies that, in the case of Florida, came crashing down by the middle years of the decade. However, even in
less expansive regions of the country, real-estate values began to stagnate and even decline after 1927.
Contributing to the real-estate boom were the monetary policies of the U.S. Federal Reserve (Fed). To help purge
the economy of post–World War I inflation, the Fed dramatically hiked the interest rate it charged on loans to
member banks, helping to trigger a deep recession in 1921–1922. As the economy came out of that recession,
however, economists noted that solid growth was not accompanied by inflation, permitting the Fed to drop interest
rates, thus making credit cheaper and more available to real-estate buyers and speculators. After real-estate gains
stalled in the mid-1920s, the Fed loosened the spigots even more, dropping interest rates further.
 Stock Market Bubble
Virtually all economists agree that the low-interest, expansive money policies of the Fed helped fuel the dramatic
run-up in securities prices in the late 1920s. The solid economic growth of the general economy contributed as
well. The gains in productivity and a wave of corporate mergers helped push the Dow Jones Industrial Average
(DJIA), an indicator measuring the collective performance of thirty major stocks traded on the New York Stock
Exchange, dramatically upward, as it doubled from a low of just above 60 at the tail end of the 1921–1922
recession to a high of about 125 at the beginning of 1925, surpassing the previous high of nearly 120 in 1920. By
the beginning of 1927, the Dow had climbed to about 160, a rise of 33 percent in two years. Nevertheless, these

gains would pale in comparison to what happened over the following two years and eight months, as the DJIA
climbed to 200 by the end of 1927, 300 by the end of 1928, and 381.17 at its peak close on September 3, 1929.
Fueling the run-up was a boom in stock trading, as middle-and even working-class investors tried to get their
share of rising returns on securities, especially as the press began to play up stories about the huge profits being
made on Wall Street and offering advice on how average citizens could make their own fortunes. By the height of
the boom in 1929, about 1 million Americans, an unprecedented number to that time, had done exactly that.
Making things easier for these investors was the spread of the stock market ticker, which allowed for the
instantaneous transmission of stock data throughout the country and the practice of margin buying. With interest
rates low and credit easy to come by, brokerage houses began to allow customers to buy stocks with a down
payment rather than the full price, sometimes as low as 10 percent. To make up the other 90 percent of the
purchase price, the brokers lent money, which they themselves had borrowed from banks, to stock purchasers,
who secured the loans using the collateral in the rising equity of the stocks themselves. These practices are
illegal now—many made so by regulations and laws passed in response to the 1929 crash—but it was not illegal
in the 1920s. Indeed, there was almost no regulation of the securities industry; even commercial banks were
allowed to invest assets—that is, depositors’ money—in securities, even of the riskiest sort. Among the most
popular of these stocks were high-profile “blue chips,” which were perceived as being on the economic edge.
RCA, the highest flying of the stocks, saw its share price rise from $20 in 1927 to nearly $120 on the eve of the
great crash in October 1929.
Pulitzer Prize–winning cartoonist Rollin Kirby was one of the few to perceive that the stock market was out of
control in 1929. His depiction of a runaway bear pulling along a helpless investor was prescient. The cartoon
appeared three weeks before the crash. (The Granger Collection, New York)

 Downturn and Crash
It all worked smoothly as long as share prices continued to rise. By 1929, however, share valuations for many
companies were far in excess of underlying worth, as measured by price-to-earnings (P/E) figures, the most
widely used measure of the ratio between the stock price and corporate net income, or profit. While the historical
average for the S&P 500, a much broader index than the DJIA, was about 16:1 for the years 1925–1975, the P/E
ratio stood at 32:1 at the height of the 1929 boom. Not only were investors paying too much for share prices in
1929, they were also ignoring underlying problems in the economy.
By 1927, economic growth was beginning to slow, as consumers began to spend less, bogged down by
increasing debt, stagnant income, and a dearth of new products to capture their interest. As demand slackened,
retail and wholesale inventories grew, reducing manufacturers’ orders, sending unemployment creeping higher,
further depressing demand. Already saddled with a number of sagging sectors, the economy became further
burdened by a slowdown in construction, a major engine of growth in the early and middle 1920s.
By early September 1929, these weaknesses were beginning to be felt on Wall Street, as the DJIA stalled and
then began to fall. After a slight recovery in early October, stock prices began to go down again, with the biggest
losses hitting what had been the highest-flying stocks—General Electric, Westinghouse, and Montgomery Ward
among them. The media, always looking for a new angle, began to play up the new “bear market,” contributing to
the growing panic among investors. Then came the crash. Over a series of “black” days in late October, the DJIA
began to plunge by dozens of points. On October 23 alone, known as Black Wednesday, the DJIA lost 7.5
percent of its value, falling from 415 to 384 in one trading session. In October, total U.S. stock market valuation
plummeted from $87 billion to $55 billion (or about $1.8 trillion to $685 billion in 2008 dollars). Since rising stock
equity was the key to margin buying, the loss of equity sent investors and brokers scrambling to get loans paid
back, forcing people to dump stocks as fast as they could, sending prices down even faster and pushing many
brokers and even some banks into insolvency.
The largest investment banks made efforts to shore up stock prices with highly publicized block purchases, but the
bear market was too fierce. Subsequent months did bring rallies as investors thought the worst was over—one in
early 1930 sent the DJIA from just under 200 back to near 300—but the gains inevitably were undermined by
growing investor pessimism. Moreover, by this point, the underlying weaknesses in the economy were beginning
to be felt. Bankruptcies increased rapidly, as did unemployment. By the depths of the Great Depression in early
1933, the DJIA had plunged nearly 90 percent, from just over 381 to just over 41. By that time, U.S. stock
valuation had plunged to less than $10 billion. Not until November 23, 1954, more than a quarter-century later,
would the DJIA climb above where it had closed at its peak in September 1929.
The more than 22 percent drop in the DJIA on Black Monday, October 19, 1987, was nearly twice the biggest
one-day loss during the 1929 crash—that of October 28, when the index fell by just under 13 points—but nothing
in the history of Wall Street would ever again equal the cumulative losses of the great crash of 1929 and the early
1930s. Nor would the impact of subsequent crashes on Wall Street ever have such a wide and long-lasting impact
on the general economy of the United States and the world.
James Ciment
 
See also:  Asset-Price Bubble;  Boom, Economic (1920s);  Great Depression (1929-1933);  New
York Stock Exchange. 
Further Reading
Galbraith, John Kenneth. The Great Crash, 1929. Boston: Houghton Mifflin, 1997. 

Klein, Maury. Rainbow’s End: The Crash of 1929. New York: Oxford University Press, 2001. 
Klingaman, William K. 1929: The Year of the Great Crash. New York: Harper & Row, 1989. 
Thomas, Gordon, and Max Morgan-Witts. The Day the Bubble Burst: A Social History of the Wall Street Crash of 1929. New
York: Penguin, 1980. 
Wigmore, Barrie A. The Crash and Its Aftermath: A History of Securities Markets in the United States, 1929–
1933. Westport, CT: Greenwood, 1985. 
 
Stock Market Crash (1987)
 
On Monday, October 19, 1987, the Dow Jones Industrial Average (DJIA), a leading index of U.S. stock market
prices, fell by 508 points and lost 22.6 percent of its total value. It was the largest one-day percentage drop in
U.S. stock market history. The events of “Black Monday,” as it came to be called, caused panic on Wall Street and
in stock markets around the world as images of the Great Depression of the 1930s were played up by the media,
leading to greater fear of investing in stocks. The market crash precipitated major declines in foreign markets. By
the end of October, stock markets in Hong Kong had fallen 45.8 percent, Australia 41.8 percent, the United
Kingdom 26.4 percent, and Canada 22.5 percent. In the United States, the overall net loss in market capitalization
of all stocks has been estimated at half a trillion dollars.
The stock market crash of Monday, October 19, 1987, was a global event, starting in Hong Kong and spreading

like a seismic wave through Europe to North America. The Dow Jones Industrial Average lost nearly a quarter of
its value that day—a record. (Hulton Archive/Getty Images)
 Black Monday Crisis
Before trading began in New York on October 19, stock markets in Tokyo and Europe declined when, for reasons
still being debated among economists and financial analysts, U.S. investors began selling prior to the opening of
the market in New York. However, the high level of stock market volatility and the unprecedented magnitude of
the 508-point drop in the Dow on Black Monday were generally unexpected.
During the crisis of October 19, 1987, the New York Stock Exchange (NYSE) considered taking the unusual step
of halting trading but determined that such an action could increase investor panic and produce greater selling
activity. The severity of the crisis was revealed later that day when stocks stopped trading because of the
overwhelming amount of sell orders. The halt in trading occurred because certain specialist firms on the exchange
—which, because of their great liquidity, generally can maintain orderly markets by buying and selling in specific
stocks during volatile conditions even when there is no other market for a given stock—were now simply
overwhelmed and found themselves without adequate capital resources to continue to maintain a market. Stock
options and futures trading also deteriorated as the underlying securities tied to the options and futures had ceased
to trade.
On Tuesday, October 20, the NYSE opened in hopes that the panic would subside, but the trading crisis
continued. By noon, many stocks had stopped trading due to a lack of buy orders. The drop in the price of stocks
caused banks to stop extending credit to securities dealers. Unlike the 1929 crash, however, some stability took
hold later in the afternoon, and buying outweighed selling for the remainder of the session. By the end of the day,
even though some indices still were down, the DJIA closed up an impressive 102 points (5.88 percent) and posted
a gain of 186 points on Thursday, October 22.
 Government Intervention
To avoid continued losses and any further disintegration of the stock market, the Federal Reserve Bank of New
York (the leading branch of the U.S. central bank) took steps to provide credit and liquidity to the securities
dealers through an infusion of cash. This was accomplished by taking the necessary actions to lower short-term
interest rates on government securities. As a result, cheaper credit became available to securities dealers, easing
their concerns about the market and helping to spur increased buying activity. This, in turn, boosted the Dow
Jones Industrial Index and, in effect, saved the world’s financial markets from further disaster.
 Causes of the Crash
In late October 1987, Nicholas Brady, a former Wall Street investment banker and New Jersey senator, was
appointed chair of the Presidential Task Force on Market Mechanisms, an investigative commission formed to
examine and report on the causes of the crash and suggest regulatory safeguards as necessary. The Brady
Commission report of January 1988 concluded that the chief cause of Black Monday was the poor performance of
financial specialists. Some brokers on the Big Board had helped fuel the crash by not having accurate, up-to-date
information on the state of the market. Their picture of market conditions, which was far worse than was actually
the case, caused them to erroneously sell more stock than they bought.
Specifically, the Brady report faulted two groups of Wall Street specialists, portfolio insurers and speculators, who
acted at cross-purposes and in ways that accelerated the crash. Portfolio insurers protect the prices of specified
stocks when prices fall below a certain point by selling them as futures to speculators. Speculators buy those
stocks as futures in the hope that their price will rebound and they can then sell them back at a profit. Portfolio
insurers, by facilitating this process, can help stabilize the price of stocks—unless they sell off too many stock
futures in a short amount of time.

The Brady Commission concluded that “a few portfolio insurers” sold futures equivalent to just under $400 million
in stocks in the first half hour of futures trading on Black Monday. The unprecedented intensity of futures activity
generated fear in the market and led to widespread sell-off of many stocks, which, in turn, caused stock prices to
decline precipitously.
Finally, the report also faulted the lack of safeguards in place for new trading technologies, including computer-
driven, automatic trading. Many larger investment institutions had created computer programs designed to sell off
stock in large batches automatically when certain marketplace conditions, such as those on Black Monday, are in
place. Much of the supposed emotional “panic selling” of the stock market crash was in fact done by cold,
calculating machines.
Not all economists and investors subscribed to the Brady report or at least not completely. These skeptics cited
longer-term causes, putting the origins of the crash at the beginning of the 1980s bull market, when the Dow rose
from 776 points in August 1982 to a high of 2,722 points in August 1987. The stock market was overvalued and
has been described by some analysts as having been an accident waiting to happen.
In response to the crash, the New York Stock Exchange undertook a number of reforms, such as banning esoteric
trading strategies and instituting careful monitoring of electronic trading. Under the new rules, if the DJIA fell by
more than 250 points in a day, program trading was prohibited for a time, thus allowing brokers time to contact
each other, regroup, and reevaluate the market. In the wake of Black Monday, many computer programs added
built-in stopping points. Limits to program trading were removed on November 2, 2007. However, circuit breakers
that halted all market trading remained in force. Circuit breakers were originally triggered by a given point fall in
the DJIA. As the DJIA increased dramatically, a given point reduction was recognized as sufficient because such
a reduction, to be meaningful, was dependent on the overall level of the market. After several iterations of the
system, a percentage decline process was adopted under which circuit-breaker halts in trading would be
established. Currently, there are circuit breakers in place that halt all market trading on any day under the
following circumstances: if the DJIA falls 10 percent, trading on the NYSE is halted for one hour; if the DJIA falls
20 percent, trading is halted for two hours; and if the DJIA falls 30 percent, trading is halted for the rest of the
day. The actual point drops are revised every quarter based upon the DJIA.
Although many economists feared the crash would trigger a recession, the fallout from the crash was relatively
small, in part due to the efforts of the Federal Reserve. It took only two years for the Dow to recover completely,
and by September 1989 the market had regained all of the value it had lost in the 1987 crash. This is in stark
contrast to how the crisis in mortgage and mortgage-backed securities and collateralized debt obligations spread
to the global economy in 2008 and 2009 to cause the severest downturn in economic activity since the Great
Depression.
Teresa A. Koncick
 
See also:  Automated Trading Systems;  New York Stock Exchange. 
Further Reading
Brady, Nicholas F., James C. Cotting, Robert G. Kirby, John R. Opel, and Howard M. Stein. Report of the Presidential Task
Force on Market Mechanisms. Washington, DC: GPO, January 1988. 
Carlson, Mark. A Brief History of the 1987 Stock Market Crash with a Discussion of the Federal Reserve
Response. Washington, DC: Board of Governors of the Federal Reserve, 2006. 
Metz, Tim. Black Monday: The Stock Market Catastrophe of October 19, 1987. Washington, DC: Beard Books, 1988. 
Stewart, James B., and Daniel Hertzberg.  “Terrible Tuesday.” Wall Street Journal, November 20, 1987. 

United States Securities and Exchange Commission (Division of Market Regulation). The October 1987 Market
Break. Washington, DC: GPO, February 1988. 
Stock Markets, Global
 
Global stock markets are organized exchanges that facilitate the trading of equity shares in corporations globally.
Equity shares, or stocks, represent claims to profits of corporations. Shares of stock in corporations also represent
voting rights concerning corporate leadership. Global stock markets therefore play a central role in coordinating
international production and capital investment.
Capital investment funds the purchase of machinery or other equipment used to make consumer goods. The task
of planning global capital investment requires some institutional means of coordinating industrial development
among nations. There are two difficult problems with planning industrial development in the modern global
economy. First, capital must be divided between nations. Second, the use of capital must be organized or planned
within each nation. These two problems are interrelated because investment plans within nations make sense only
in the context of rational division of capital between nations. For example, nations such as the United States and
Japan have invested heavily in steel production. The specific plans for investment in the United States and
Japanese steel industries make economic sense only if other countries are unable to produce steel more efficiently
than the United States or Japan.
 Increasing Integration
There has been a trend toward the integration of global stock trading. Perhaps the single most important event in
the globalization of stock markets was the fall of the Soviet Union in December 1991. That event allowed for the
development of Eastern and Central European stock markets. There has been greater development of stock
markets outside of the former Soviet bloc nations as well. The older, more traditional stock exchanges in Western
Europe, the United States, and Japan have extended trading beyond their borders. Established stock exchanges
often list foreign companies and sell stock to foreign investors.
Also, stock exchanges themselves are becoming more integrated. The NASDAQ exchange (the National
Association of Securities Dealers Automated Quotations) and the American Stock Exchange (AMEX) merged in
1998. This merger was undone in 2004. In recent years the Frankfurt Stock Exchange (owned by Deutsche
Börse) and NASDAQ both attempted to acquire the London Stock Exchange. While the NASDAQ and Frankfurt
mergers failed, the New York Stock Exchange did manage to merge with the Paris-based Euronext exchange in
2006. Modern technology has also made it easier for global investors to buy and sell globally in stock exchanges.
While stock exchanges are nowhere near full integration, international financial markets have grown and spread to
reach into all but the most remote corners of the world.
 Role of Stock Markets
The globalization of investment has been important to economic development. Stock markets play an
indispensable role in directing global capital investment. Corporations can raise funds by selling new shares of
stock on a stock market or by borrowing money from creditors (that is, bank loans or corporate bonds). Much
investment is funded by credit rather than by selling new shares of stock, but stock markets affect credit-financed
investment. Stock markets price capital goods in terms of how these goods are put to use by corporations. Since
stock earnings and capital gains represent the profits to corporate owners and shareholders, these earnings reflect

the success or failure of corporate strategies. Corporations that invest capital efficiently will earn profits and realize
capital gains. Stock prices and dividend payments therefore reflect the value of capital as used by any corporation.
Stock markets provide information on the relative value of capital as used by different corporations. Data on the
relative value of capital is clearly important to decisions to fund corporate investment through credit.
Competition in stock markets can also promote the efficient management of corporations. Stockholders will likely
retain executives who deliver high dividends and capital gains. Shareholders will often remove executives who
deliver low dividends and capital losses. Also, decreases in stock prices make it easier for new investors to buy up
shares of the stock and replace incumbent executives. Stock markets can therefore redirect the use of capital
either by replacing failed executives or by redirecting funds.
If global financial markets operated with perfect efficiency, investment funds would always flow to the most
productive and lucrative industrial projects. Competition in stock and other financial markets should equalize the
productivity of additional capital investment between nations. What this means is that if an additional increment of
capital investment delivers more products in Spain than it does in Portugal, then efficient stock markets will
indicate higher returns on investment in Spanish industry, and Portugal will lose capital to Spain.
 Inefficiencies in Capital Allocation
Historical experience shows that capital does not always flow to its most productive uses. For example, the
productivity of additional capital investment in India is fifty-eight times higher than the productivity of additional
capital investment in the United States. What this means is that there is so much capital investment in the United
States that the returns on additional capital investment are small. Since people in the United States have already
invested heavily to develop areas of potential high productivity, additional investment will not increase the
production of goods much more. The relative lack of capital in India, as well as in other less developed nations,
means that they have yet to develop many high-productivity areas of their economies. Perfectly efficient global
financial markets would redirect some capital from nations with more advanced economies, such as the United
States, to less-developed nations, such as India. But capital remains concentrated in relatively few advanced
industrial nations.
One possible explanation for the lack of investment in many nations is that stock markets work imperfectly. One of
the more common criticisms of stock markets is that they are subject to speculative booms and crashes. While it
is obvious that stock markets have had bullish and bearish periods, this does not necessarily explain chronic
problems with global investment. Booms and busts exist in both advanced and less developed nations. It is not at
all clear that such cycles in stock exchanges should prevent the flow of capital investment to less developed
nations. After all, less developed nations lack capital investment during both booms and busts. It is also not clear
that booms and busts are inherent to stock exchanges. Of course, there are many examples of stock market
booms and crashes. Stock market booms typically take place during credit expansions by central banks. Central
banks, like the U.S. Federal Reserve, do not seem to have a direct influence on stock market activity. However,
they do exert indirect influence through their influence over interest rates and bond prices. Private investors
compare bond rates and returns on stocks. For example, the Federal Reserve expanded the money supply 5.9
percent per year during the economically expansive 1920s and kept interest rates low. Low interest rates on
bonds made corporate stocks more attractive to investors, fueling the run-up of stock prices. The Federal Reserve
also expanded the money supply and kept interest rates low during the more recent dot.com boom of the late
1990s and the housing boom of the early and mid 2000s, again making stocks more attractive and contributing to
inflated prices.
Some economists point to imperfect information and “irrational exuberance” among investors as sources of stock
market instability. There is some truth to these claims. However, economists do not expect stock markets to attain
perfect results. There are also more plausible explanations for the skewed distribution of global capital. Countries
with high taxes, heavy-handed regulation, and corruption tend to have low levels of foreign investment. What this
suggests is that global stock markets are working effectively to help investors avoid high taxes and other

unnecessary burdens. Efficient stock markets should help investors avoid high-risk investment, and the existence
of restrictive regulations and corruption pose real risks to investors. Consequently, the skewed distribution of global
capital investment might be due in large part to the efficiency of global stock markets. In other words, the actual
source of inefficiency in many less developed nations might be excessive taxes and regulation, and corruption in
these nations.
 History
To fully understand how stock markets work and the role they play in economic development, it is important to
look at their history. Stock markets in cities such as London, Tokyo, Amsterdam, New York, and Frankfurt began
as informal local institutions in the seventeenth and eighteenth centuries. Trading of financial securities often
began in coffee houses and private clubs. With the passage of time, these early stock markets developed into
complex institutions with detailed rules and regulations. Stock exchanges in major cities came to direct capital
investment. For example, there is much historical evidence indicating that German stock markets, especially the
Frankfurt exchange, contributed greatly to German industrialization. For its part, Belgian industry developed rapidly
during the nineteenth century, and Belgium’s independence in 1830 was key to its financial and industrial
development. Liberalization of the Belgian stock market in 1867 accelerated financial and economic development,
including that of the Brussels Stock Exchange, and statistics indicate that this in turn drove the country’s industrial
development.
The German and Belgian examples are not unique. Many statistical studies show that stock market development
facilitates industrial development and long-run economic growth. The spread of stock markets globally has
increased capital investment and raised productivity throughout the world. In recent decades many developing
nations have formed more advanced financial markets, including stock markets. One study of nine African nations
indicates that the development of African stock markets has improved economic development. A study of twenty-
one developing nations shows that stock market development increases private investment and contributes to
economic growth. Such industrial development has brought about gradual increases in living standards for many
poor workers around the world.
 Government Investment Alternative
The overall record of privately financed and directed investment indicates that stock and other financial markets
play an important role in economic development. Of course, there is an alternative to private financing of capital
investment. Governments can fund capital investment though taxes and public borrowing. The overall record of
government-funded investment is mixed. Many government projects and programs entail waste and corruption.
Government investment often benefits politically connected special-interest groups rather than the general
population. Many Western nations have provided direct foreign aid to the developing world. Direct foreign aid
projects have tended to deliver poor results. Direct aid by foreign governments often benefits political elites within
the recipient nation rather than the general population.
Many government investment projects have funded “prestige projects.” One example of a government-funded
prestige project is the Apollo project to land a man on the moon. Another example was the development of the
Concord, the world’s fastest jet airliner. Such projects are a source of national pride and attract much attention,
but do little to improve the lives of ordinary people. The past success of private investment in the developed
countries, combined with the recent but limited success of private investment in developing nations, indicates that
stock exchanges are the most effective means of promoting economic development. Thus, stock exchanges
appear to be very important to improving economic conditions in developing nations.
 Financial Crisis of Late 2000s
Global stock exchanges were deeply affected by the financial crisis of 2008–2009. There was a sharp decline in
stock indices with the onset of the recession in late 2007. Such declines in stock prices are not unusual during

recessions. The decline in stock indices worsened as the severity of this financial crisis became apparent. Of
course, the biggest losses of equity value were sustained by corporations such as Bear Sterns, Lehman Brothers,
Citigroup, AIG, and General Motors, which either experienced losses that threatened their solvency or went
bankrupt. One could say that share prices did not reflect the true value of these corporations prior to the crisis.
Such inaccuracy in stock price is a sign of stock market inefficiency. However, stock prices were generally inflated
during the housing boom, and the stocks of some companies, especially AIG, were weak even before the crisis.
The recent crash is in some sense a correction—a return to more accurate values on stock markets around the
world. It does not appear that stock trading itself drove the 2008–2009 crisis. It is rather the case that various
government policies and private sector miscalculations in derivative markets caused the global crisis. Of course,
there has been a great loss of wealth globally, and it is likely that many stocks have fallen “too far.” Given time,
stock markets will recover and should continue to regulate global investment and production. (Recovery of stock
markets in many countries is well under way in late 2009, at the time of this writing.)
Global stock markets emerged with globalization of industry. The emergence of global organization of production
was in fact facilitated by the development of stock markets within nations. In modern times stock markets have
themselves become increasingly global. While some people see stock markets as centers for greed and financial
manipulation, these markets have contributed greatly to economic development and rising living standards around
the world. Stock markets were vitally important to early industrialization in developed countries and have more
recently contributed to Eastern European and developing-world industrial development. Economists say that the
trend toward globalization of stock and other financial markets is likely to continue, with many concluding that this
is a good thing. They argue that the public should welcome the trend toward global stock markets and global
finance in general, as financial institutions play an indispensable role in promoting economic efficiency and
prosperity.
D.W. MacKenzie
 
See also:  Nasdaq;  New York Stock Exchange;  Souk al-Manakh (Kuwait) Stock Market Crash
(1982). 
Further Reading
Atje, R., and B. Jovanovic. 1993.  “ Stock Markets and Development.”European Economic Review 37:632–640. 
Caporale, G.M., P. Howells, and A. Soliman.  “Stock Market Development and Economic Growth: The Causal Linkage.”
Journal of Economic Development 29:1 (June 2004): 33–50. 
Demirguch-Kunt, A., and R. Levine.  “Stock Market Development and Financial Intermediaries: Stylized Facts.” World Bank
Economic Review 10:2 (1996): 291–321. 
El-Erian, M.A., and M. Kumar.  “Emerging Equity Markets in the Middle Eastern Countries.” IMF Staff Papers 42:2
(1995): 313–343. 
Greenwood, J., and B. Smith.  “Financial Markets in Development and the Development of Financial Markets.” Journal of
Economic Dynamics and Control 21:1 (January 1997): 145–181. 
Levine, R.  “Stock Markets, Growth, and Tax Policy.” Journal of Finance 46:4 (1991): 1445–1465. 
Levine, R., and S. Zervos.  “Stock Markets, Banks, and Economic Growth” American Economic Review 88:3 (1998): 537–
558. 
Stringham, Edward.  “The Emergence of the London Stock Exchange as a Self-Policing Club.” Journal of Private
Enterprise 17:2 (Spring 2002): 1–20. 

Stockholm School
 
The Stockholm school (Stockholmsskolan, or Swedish school of economics) refers to a group of Swedish
economists who, during the 1920s and 1930s, made important contributions to the development of dynamic
macroeconomic analysis. The chief contributors to this particular “school” were Erik Lindahl (1891–1960), Gunnar
Myrdal (1898–1987), Bertil Ohlin (1899–1979), Erik Lundberg (1907–1989), and Dag Hammarskjöld (1905–1961).
The “Stockholm school” label was not applied to this group of men until 1937. The group was a rather loosely
organized group of young economists, leading some other economists to argue about whether or not it constituted
a school of thought based around a set of ideas and specific research objectives. If a common theme may be
discerned through the Stockholm school economists’ work, it was their attention to the interaction between
economic variables over different time periods (a process referred to as the theory of dynamic processes). It is
possible to identify some similarities between the macroeconomic theories of the Stockholm school and those
developed by their contemporary, the British economist John Maynard Keynes. However, the extent to which the
economists of the Stockholm school anticipated the central tenets of Keynesian analysis remains a contentious
subject.
A key contribution of the Stockholm school was the view that decisions by economic agents (households,
businesses, governments, organizations) regarding aggregate savings (S) and investment (I) are inherently
forward-looking. The recognition of this fact led them to concentrate on the economic importance of future plans or
expectations, and ultimately on the distinction between ex ante calculations and ex post results.
Ex ante calculations (basically, calculations that are made before some specified or relevant time period) refer to
the situation whereby economic agents will formulate investment and saving decisions on the basis of expected
future incomes. Ex post results (those that are known after the specified or relevant time period) are also
important, as they form the basis on which subsequent ex ante calculations will be made.
On first examination, the ex ante–ex post distinction appears to be of little consequence. The crucial importance of
these terms emerges, however, once it is understood that, in the real world, households and businesses do not
possess perfect foresight and so, in all probability, will be mistaken in their expectations. For example, even
though households and businesses may expect ex ante savings to equal ex ante investment, there is absolutely
no certainty that this will be the case. The fundamental difference between subjective forecasts (expectations) and
real-world outcomes (realizations) therefore has important economic implications. Indeed, the Stockholm school
regarded anticipations and expectations as the fundamental forces that drove the dynamic process forward. This
idea can be explained in the following example.
First, let us assume that there is an ex ante imbalance between savings and investment (in other words, that the
ex ante amount of aggregate savings is not equal to the amount of planned investment). This inequality would set
in motion important dynamic processes as actual investment, actual savings, and actual income would clearly
differ from expected investment, expected savings, and expected income.
Let us now take the scenario in which there is an excess of ex ante investment over savings (ex ante I > S).
Assuming that there are sufficient unemployed factors of production, this would generate an expansion of the
economy, associated with the upswing of the cycle. This upswing, however, would be expected to bring about
additional savings through increases in profits and incomes. The result would be that ex post savings would equal
investment. Let us now take the opposite scenario, where there is an excess of ex ante savings over investment
(ex ante S > I). Here the economy would experience a contraction, as businesses and households would find
themselves with lower-than-expected profits and incomes. This would clearly be associated with the downswing of

the cycle. Once again, however, we must recognize the other forces at play here. As businesses would be unable
to sell what they had already produced, they would be forced to reduce their investments. The consequence would
be that ex post savings would again equal investment. It can therefore be seen that, in either scenario, a disparity
between ex ante savings and investment would set into motion processes that would develop into an ex post
equality between the two.
Beyond these general points, it becomes difficult to identify any clear central message within the work of the
Stockholm school. Its members never provided a well-defined theory of the movement of the business cycle. One
of the key reasons for this was that while the members of Stockholm school sought to highlight the importance of
dynamic macroeconomic processes, the economic theories that they constructed proved extremely difficult to
analyze. This meant that instead of presenting a detailed examination of the complete movement of the cycle, they
were forced to concentrate on a number of separate examples that provided only possible interpretations of the
expansion and contraction phases. Having said this, the members of the Stockholm school expressed an interest
in policy matters and advocated the use of both fiscal and monetary policies as a means of stabilizing the
fluctuation of the cycle. Several members were involved with the Committee on Unemployment, a Swedish
government–appointed committee that lasted from 1927 to 1935. Yet due to their inability to fully analyze their own
economic system, the Stockholm school possessed no rigid policy prescription regarding the use of stabilizing
policies. This led them to argue that the use of different policy measures depended very much on the economic
conditions prevailing at a given moment in time.
It is a curious situation that the obvious strengths associated with its ideas of dynamic processes ultimately served
to undermine the overall success of the Stockholm school. Put simply, the members of the school did not possess
the necessary analytical methods that would enable them to fully examine the interconnections between the
various parts of the economic system that they had constructed. Perhaps as a consequence their ideas never
formed the basis, in Sweden or elsewhere, for further research into either business cycle theory or stabilization
policy.
The Stockholm school did not survive very much beyond the late 1930s, as the young members of the group
moved on to other intellectual pursuits. For example, Ohlin served as leader of the Swedish Liberal Party from
1944 to 1967; Myrdal held various economic and political appointments before winning the Nobel Prize in
economics in 1974, while Hammarskjöld was elected secretary general of the United Nations in 1952 (and was
posthumously awarded the Nobel Peace Prize in 1961). Lindahl was the only member of the school to pursue an
academic career.
Christopher Godden
 
See also:  Keynes, John Maynard;  Lundberg, Erik Filip;  Myrdal, Gunnar;  Sweden. 
Further Reading
Hansson, Berg. The Stockholm School and the Development of Dynamic Method. London: Croom Helm, 1982. 
Laidler, David. Fabricating the Keynesian Revolution: Studies of the Inter-war Literature on Money, the Cycle, and
Unemployment. Cambridge, UK: Cambridge University Press, 1999. 
Patinkin, Don.  “On the Relation between Keynesian Economics and the Stockholm School.” Scandinavian Journal of
Economics (1978): 135–143. 
Siven, Claes-Henric.  “The End of the Stockholm School.” Scandinavian Journal of Economics 87:4 (1985): 577–593. 

Subsidies
 
Subsidies are government payments to firms or households for the purposes of the lowering the cost of production
or encouraging the consumption of goods. Subsides come in two basic types: direct and indirect. Direct subsides
include transfers of money to producers and consumers. Farm, research, and export subsidies go to the former
and food stamps, rent support, and scholarships to the latter. Direct subsidies may come in the form of cash
payments or loans at below-market interest rates. Indirect subsidies, which typically are much more valuable
overall than direct subsidies but do not involve transfers of money from governments to households and firms,
include things such as copyright and patent protections, tariffs and other trade barriers, and most important, tax
deductions, deferments, and rebates.
Governments offer different kinds of subsidies for different reasons. Subsidies to households are usually, but not
always, offered to persons near or below the poverty line. They are usually made for reasons of social equity; the
argument is that wealthy industrial societies have the resources to make sure that all of citizens have food,
shelter, and the other basic necessities of life. In the United States, there are both direct (rent support, welfare,
food stamps) and indirect subsidies to poorer households. One of the most significant indirect subsidies is the
earned income tax credit, a refund on payroll taxes for workers who are supporting children and other dependent
minors. However, it is more likely to be middle-and upper-income households who take advantage of perhaps the
largest indirect subsidy in the U.S. economy, the deduction on mortgage payments on primary residences. Even
in poorer countries, governments often offer subsidies to bring down the cost of basic necessities. This is done for
reasons of social equity but also social peace. In a place where a majority of the people live at the poverty level,
cutting off such subsidies can lead to great political instability.
Governments give direct and indirect subsidies to firms for various reasons but usually to encourage desirable
economic activity. Copyright and patent protection, for instance, provide incentives for innovation, as do research
subsidies. Depreciation allowances, which allow companies to write off the cost of equipment, are designed in part
to encourage new investment, as is the lower tax rate on capital gains versus earned income. Farm supports are
meant to assure a secure supply of domestically grown food and to keep rural economies afloat.
 Economic Impact
Subsidies have a direct impact—sometimes intended and sometimes not—on the functioning of the marketplace.
Subsidies to producers increase supply and, all other things being equal, lower the cost of a given good.
Subsidies to households increase demand and, again, if all things are equal, can lead to higher prices. In both
cases, subsidies create a new price equilibrium of supply and demand that is different from where it would have
been if market forces had been left to operate on their own. Free-market-oriented economists usually consider
subsidies to be a bad thing, as they distort the smooth functioning of the marketplace through the creation of
nonmarket incentives and penalties. Moreover, export subsidies and tariffs also disrupt normal trading patterns,
whereby goods are made where production is most efficient.
Subsidies can also have an effect on a country’s macroeconomy. By offering generous subsidies, particularly in
times of economic crisis, governments are usually forced to borrow money. This can lead to higher interest rates,
which make it more difficult for private industry to obtain the funds needed to operate and expand. Deficits can
also endanger a government’s ability to borrow abroad, which is critical for development in poorer countries.
Consequently, multilateral lenders, such as the World Bank and the International Monetary Fund, usually insist on
subsidy reduction or elimination for basic necessities before extending new loans to heavily indebted countries.
More Keynesian-oriented economists argue that market forces alone do not always produce healthy equilibriums of
supply and demand. Indeed, for a variety of reasons involving the fact that wages and prices do not always adjust
smoothly to changes in supply and demand, an equilibrium may be reached at a low supply/low demand level,

accompanied by high unemployment and less than maximum utilization of production facilities. To lift that
equilibrium to a level at which unemployment falls and productive capacity gets utilized, governments use fiscal
policy—direct and indirect subsidies in the form of tax cuts and spending—to bolster supply and demand.
These differing views on subsidies are reflected in the contentious political debate that surrounds them. Various
countries take different views on subsidies; in general, European countries tend to be more generous in the direct
subsidies they offer to households, while the United States usually emphasizes indirect subsidies in the form of
lower taxes. Even within countries, there is much debate over subsidies, with liberals generally favoring more
generous household subsidies and conservatives opposing them. In most industrialized countries, producer
subsidies generally enjoy more bipartisan support, first because of the argument that they create jobs and second
because they often have powerful interest groups fighting for them. That is, once a subsidy is created, an interest
group develops around it. That interest group will fight for the subsidy vigorously, while opposition to the subsidy
remains more diffuse.
 History
Subsidies are as old as government itself. The government of ancient Rome is said to have maintained peace and
order in an inequitable society through “bread and circuses,” that is, subsidies on food and the provision of free or
low-cost diversions. Since medieval times in the West, governments have offered direct subsidies to individuals—
albeit parsimonious ones—through the poorhouse. Countries also offered producer subsidies. Among the best
known was the land grants the U.S. federal government offered to railroads in the nineteenth century to get them
to build lines in the sparsely populated territories and states of the West.
By the late nineteenth and early twentieth centuries, however, there was a growing consensus that government
subsidies to the poor were the right thing to do not only to ensure social peace but also to ensure social equity.
From its beginning in 1916, the U.S. income tax was meant to tax the well-off more than the poor, partly in order
to provide services to the latter. The Great Depression and the manifest suffering it created provided the great
impetus for more direct subsidies to individuals and households; among them were jobs programs and welfare
payments, though the welfare program did not really become significant until after World War II.
The Great Depression also saw the introduction of federal farm subsidies on a large scale. However, it was World
War II that truly provided the impetus for producer subsidies, largely in the defense sector. In addition, by the
early post–World War II era, most governments, including that of the United States, had accepted the Keynesian
logic of using subsidies to prevent economies from sinking into recession, where the supply and demand
equilibrium resulted in low production and high unemployment. In short, subsidies had become a major tool in the
government’s effort to smooth out the business cycle.
By the 1980s, however, a new, conservative paradigm arose, particularly in the United States and the United
Kingdom, that argued that direct subsidies to households, particularly low-income ones, had a negative effect on
society, discouraging people from participating in the marketplace by offering them alternatives to work, and
thereby encouraging a “culture of dependence.” At the same time, conservatives argued, indirect subsidies (largely
in the form of lower taxes) to firms and to the wealthy individuals who provided much of the investment capital in
the United States would create accelerated economic growth that would benefit all of society—the so-called
supply side”economics argument.
More recent events have prompted a great debate over subsidies. The U.S. war in Iraq, which many believe was
fought in part to secure access to critical Middle Eastern energy supplies, demonstrated to many that oil enjoyed a
massive subsidy in the form of defense outlays and that these outlays distorted the market, making oil appear
cheaper than alternative energy sources. An increased awareness of climate change has also led many to believe
that industries responsible for the release of large amounts of carbon dioxide, the chief component of the
greenhouse gases that are raising global temperatures, are not paying their fair share for the damage that
atmospheric carbon dioxide causes. In other words, they are receiving an indirect subsidy, leading many policy
makers to advocate various schemes to address that problem, including fines and taxes, or the more market-

oriented cap-and-trade policy.
The recession and financial crisis of 2007–2009 have highlighted the costs of subsidies to the housing and
financial sector. Most obvious were the bailouts of financial institutions orchestrated by the United States and other
industrialized countries in the wake of the financial market meltdown of September 2008. While most economists
agree that the infusion of massive amounts of money—nearly a trillion dollars in the United States alone—was
necessary to provide the liquidity banks needed to stay afloat, start lending again, and keep economies from
collapsing, they worry about the effects of bailout in terms of moral hazard. That is, they are concerned that aiding
banks in this crisis will lead them to act recklessly in the future since they will come to believe that, no matter how
risky their behavior, they will be bailed out again.
There are also less obvious and more indirect subsidies that contributed to the crisis, particularly in the housing
sector, where the financial meltdown began. One was the above-mentioned mortgage interest tax deduction,
which provided a nonmarket incentive for people to buy homes as opposed to renting since they would get a large
rebate on their taxes for buying. This helped drive up both demand and prices. In addition, Washington, in effect,
backstopped the two government-sponsored enterprises—the Federal National Mortgage Association (Fannie
Mae) and the Federal Home Loan Mortgage Corporation (Freddie Mac)—allowing them to insure subprime
mortgages to less creditworthy individuals. This helped lead to increasingly reckless lending by financial institutions
that left them vulnerable to a drop in housing prices or a rise in unemployment. When both of those occurred,
foreclosures mounted, and banks found themselves with lots of nonproducing assets that jeopardized their
solvency.
To remedy the crisis, the Barack Obama administration introduced a number of new subsides of its own. In March
2010, for instance, it put out a plan to refinance mortgages when foreclosure was likely, particularly for those
borrowers whose homes were worth less than the amount initially borrowed to pay for them.
James Ciment
 
See also:  Fiscal Policy;  Political Theories and Models;  Tax Policy. 
Further Reading
Anderson, Kym. Distortions to Agricultural Incentives: A Global Perspective, 1955–2007. New York: Palgrave
Macmillan, 2009. 
Barth, James R. The Rise and Fall of the U.S. Mortgage and Credit Markets: A Comprehensive Analysis of the Market
Meltdown. Hoboken, NJ: John Wiley and Sons, 2009. 
Laffont, Jean-Jacques. Incentives and Political Economy. New York: Oxford University Press, 2000. 
Sawyer, James E. Why Reaganomics and Keynesian Economics Failed. New York: St. Martin’s, 1987. 
Van Beers, Ceesm and André de Moor. Public Subsidies and Policy Failures: How Subsidies Distort the Natural
Environment, Equity and Trade, and How to Reform Them. Northampton, MA: Edward Elgar, 2001. 
Zahariadis, Nikolaos. State Subsidies in the Global Economy. New York: Palgrave Macmillan, 2008. 

 
Summers, Lawrence (1954–)
 
One of the most influential U.S. economic policy makers of the late-twentieth and early-twenty-first centuries,
Lawrence Summers served as both deputy secretary and secretary of the treasury in the Bill Clinton
administration and director of the National Economic Council, a White House advisory group, in the Barack
Obama administration. A controversial figure, Summers has been a strong advocate of financial deregulation and
free markets, which has led to charges that he bears a significant share of the blame for the financial crisis that
enveloped the global economy beginning in 2007.
Lawrence Summers, a former chief economist at the World Bank, treasury secretary under President Bill Clinton,
and president of Harvard University, returned to Washington as part of Barack Obama’s economic team but was
criticized for his ties to Wall Street. (Mark Wilson/Getty Images)

Born in Connecticut in 1954, Summers comes from one of the most illustrious lineages in modern American
academics; his father, Robert, was a celebrated economic theorist at the University of Pennsylvania, and his
mother, Anita, chaired the Department of Public Policy and Management Economics. In addition, two of his uncles
—Paul Samuelson and Kenneth Arrow—are Nobel Prize–winning economists.
Summers himself was a mathematical prodigy, being accepted at the Massachusetts Institute of Technology at
age sixteen; he earned a PhD in economics at Harvard in 1982 and then, a year later, became one of the
youngest tenured professors in Harvard University’s history. Much of Summers’s research has been on the role of
taxes in economic growth, where he has argued that corporate and capital gains taxes are both inefficient sources
of revenue and poor economic policy and thus should be scaled back.
After a brief stint in the early 1980s as a member of President Ronald Reagan’s Council of Economic Advisers
and as an economic adviser to the unsuccessful Democratic presidential candidate Michael Dukakis in 1988,
Summers left Harvard to serve as chief economist for the World Bank. There, in one of a number of controversial
statements that has marked his public career, Summers suggested in an internal memo that it made economic
sense to ship toxic wastes to underdeveloped countries, though he later claimed the remark was meant to be
sardonic.
In 1993, incoming President Bill Clinton appointed Summers deputy secretary of the treasury under the latter’s
longtime mentor, Robert Rubin. Given a portfolio that emphasized international economic issues, Summers was
an advocate of privatization and liberalization of economies that had long been heavily state directed, such as
those of Russia and Mexico.
With his appointment to succeed Rubin as Treasury secretary in 1999, Summers pushed for deregulation in a
number of industries and hailed the passage of the Financial Services Modernization Act of 1999, which
overturned New Deal–era legislation—specifically, the Glass-Steagall Act of 1933, which prevented commercial
banks from dealing in insurance and investment banking.
With the inauguration of George W. Bush in January 2001, Summers left government service to take up the post
of president of Harvard University. His five years there were marked by a series of controversies, including his
public criticism of noted African-American scholar Cornel West—Summers suggested that the professor’s rap
album put the university in a bad light—and his implication that women may be underrepresented in the sciences
because of “issues of intrinsic aptitude.” Widely criticized by faculty, Summers was all but forced to resign in
2006.
His appointment as Obama’s director of the National Economic Council also prompted some controversy,
particularly on the political left. Many felt that Summers’s strong advocacy of financial deregulation in the Clinton
administration, including his support for the overturning of much of Glass-Steagall, made him one of the principal
agents of the financial meltdown of 2007–2008. By his critics’ reasoning, the deregulation that Summers
advocated exposed major commercial banks to the kinds of risks normally associated with the less regulated
investment banking and insurance industries. Specifically, it allowed commercial banks to invest in risky financial
derivatives, including mortgage-backed securities. Thus, when those derivatives rapidly lost value beginning in
2007, it left major commercial banks with vast quantities of “toxic” assets on their books, requiring the $700 billion
financial bailout orchestrated and implemented by the Bush and Obama administrations.
Critics of the appointment, who included Nobel Prize–winning economists Joseph Stiglitz and Paul Krugman,
argued that Obama was now going to be advised on how to get the United States out of the economic mess of
financial deregulation by the very man who had vigorously advocated policies that created the mess in the first
place. Indeed, Summers’s advocacy of tax cuts as the best way to stimulate the economy has run counter to
liberal economists’ arguments that public spending on infrastructure would be a more effective means to that end.
Summers resigned from the National Economic Council at the end of 2010 and returned to Harvard, where he took
up a professorship at the John F. Kennedy School of Government.

James Ciment
 
See also:  Liberalization, Financial;  National Economic Council;  Regulation, Financial; 
Treasury, Department of the. 
Further Reading
“Lawrence H. Summers.” New York Times, June 9, 2009. 
Lizza, Ryan.  “Inside the Crisis: Larry Summers and the White House Economic Team.” The New Yorker, October 12, 2009. 
Scheiber, Noam.  “Obama’s Choice: The Next Larry Summers... or Larry Summers.” The New Republic, November 5, 2008. 
Sunspot Theories
 
In economics, the term “sunspots” refers to extrinsic random variables—that is, noneconomic factors originating
outside the economic system under discussion—that affect the economy. Although it is most often used in this
general, figurative sense, the term is a specific reference to nineteenth-century work by British economist William
Stanley Jevons (1835–1882), who attempted to construct a model linking the eleven-year cycle of sunspot activity
observed by astronomers with the business cycle as measured by economists. No crackpot or fringe thinker,
Jevons was one of the fathers of modern macroeconomic theory. Literal sunspots do exist, of course, and the
term refers to fluctuating regions of lower surface temperature and high magnetic activity. What astronomers call
“space weather”—the environmental conditions in space, such as radiation, ambient plasma, and the activity of
magnetic fields—is caused by sunspot activity that follows an eleven-year period called the solar cycle or sunspot
cycle.
There are legitimate correlations between the sunspot cycle and certain phenomena on Earth. For example,
sunspot peaks tend to precede outbreaks of the flu, as they did before the 1918–1919 Spanish flu pandemic.
There appear to be at least two reasons for this connection. First, the increase in solar radiation increases the
frequency of mutation, allowing the influenza virus to become a more dangerous type of disease that can break
through whatever immunity the human population possesses; and second, the increase in solar radiation
adversely affects the human immune system, thereby making humans more susceptible to the flu virus. (In this
light, it is worth nothing that the swine flu pandemic of 2009 succeeded the minimum, or trough, of the sunspot
cycle by less than one year.)
So the idea that sunspots can have a tangible effect on human economic cycles is not as far-fetched as it might
seem, particularly if the relationship is indirect—with sunspot activity affecting agricultural yields, for instance.
Jevons introduced his sunspot theory in 1875 and presented it in papers to professional associations of scientists
and economists three years later. In the afterglow of his major work, Principles of Science (1874), Jevons
announced that he had found a correlation between the periodicity (timing and duration) of the business cycle and
the periodicity of the solar cycle. His major explanation for this related to agricultural effects of solar activity. Thus,
he found first a cycle of European harvest yields and price crises, and later one of Indian harvest yields and
import price crises. Based on these observations, his sunspot theory posited that solar-cycle minimums—such as
that experienced amid the global financial meltdown in late 2008—correspond to stability and steady economic
growth.

In the end, the data simply did not support Jevons’s conclusions, at least not in any way he could demonstrate,
but his attempts were notable because they marked the first time the business cycle had even been examined in
any systematic way. Although economists had been well aware of fluctuations in economic activity and even the
way extrinsic events can have economic repercussions through very indirect means, the idea of an actual cycle—a
periodicity of economic booms and busts—was both compelling and important.
The figurative use of sunspots was popularized by a 1983 paper titled “Do Sunspots Matter?” by economists
David Cass and Karl Shell. The paper referred specifically to random events or conditions—such as actual
sunspots—that can have economic effects if and when people think they matter. Such beliefs affect consumer and
business confidence whether or not the event would otherwise affect the economy. Anticipating the twenty-first-
century Keynesian revival, Cass and Shell referred to John Maynard Keynes’s “animal spirits” and his rejection of
the idea that all economic activity can be explained by rational behavior. Quite to the contrary, in Keynes’s view
and in that suggested in the Cass and Shell paper, a complete model of the business cycle must account for the
irrational, for self-fulfilling prophecies, for manic booms and self-destructive panics, and for the importance of
human anticipation, perception, and expectation.
Bill Kte’pi
 
See also:  Jevons, William Stanley;  Seasonal Cycles. 
Further Reading
Benhabib, Jess, Stephanie Schmitt-Grohe, and Martin Uribe.  “The Perils of Taylor Rules.” Journal of Economic
Theory 96:1–2 (2001): 40–69. 
Cass, David, and Karl Shell.  “Do Sunspots Matter?” Journal of Political Economy 91: 21 (1983): 193–227. 
Duffy, John, and Eric O. Fisher.  “Sunspots in the Laboratory.” American Economic Review 95:3 (2005): 510–529. 
Sweden
 
The third-largest country in the European Union by territory, with a population of just over 9 million people,
Sweden is located in the center of Scandinavia in northern Europe. It has a free-market economy, though one in
which the state offers a generous package of social welfare programs, paid for by one of the highest tax rates in
the world.
Home to the marauding, trade-oriented Vikings in medieval times, the modern state of Sweden emerged as a
unified kingdom in the sixteenth century. Within a century, it had become one of the great powers of Europe,
extending its control over most of Scandinavia and the Baltic region. But with its relatively small population, it was
unable to maintain its hold over these far-flung lands. By the time of the Napoleonic Wars in the early nineteenth
century, it had shrunk back to its present size plus Norway, which won its freedom in 1905.
As the leading power in the Baltic region, Sweden became wealthy through the export of fur, timber, and grain (as
Russia had done in the early modern era), and began to industrialize and urbanize in the late nineteenth century.
By the early twentieth century, Sweden had built one of the most modern financial systems in the world and had
emerged as one of the wealthiest countries in Europe.

The Great Depression, which sent Sweden’s economy reeling and produced major social unrest among the hard-
hit working classes, led to the 1932 victory of the Social Democratic Party, which began to build the modern social
welfare system for which the country is now famous.
Neutral in both world wars, Sweden avoided the widespread destruction visited upon much of the rest of the
continent and even prospered by selling resources to both sides. (The Swedes leaned politically toward the Allies
and offered political refuge to many people fleeing Nazi oppression.)
As a major industrialized country and one with a rich resource base in timber and iron, Sweden took advantage of
the postwar economic boom in Western Europe to establish itself as a leading economic force, with much
government direction of the economy, including ownership of certain major industries. At the same time, it
continued to build on one of the most generous social safety networks in the world, offering full health care, free
education through university, and substantial unemployment benefits for its citizens. These programs all came at
the cost of one of the highest tax rates in Europe, with the government’s share of the gross domestic product
(GDP) approaching 50 percent.
The oil shocks and economic stagnation that gripped much of the industrialized world in the 1970s hit Sweden
especially hard, as manufacturing declined and tight regulation of the economy, including strict price controls,
hampered efforts toward economic reform. Widespread deregulation and privatization under a center-right
coalition of parties in the late 1970s and early 1980s helped revive the economy but, according to many
economists, in ways that proved damaging in the long run.
Liberalization of lending laws led to bubbles in the housing and financial sectors that burst during the global
recession of the early 1990s. With GDP in serious decline, there was a run on the krona, Sweden’s national
currency, which forced the Swedish Central Bank to dramatically hike interest rates. Unemployment rose
significantly, and GDP fell by some 5 percent in 1992 and 1993. By 1994, the government’s budget deficit had
reach 15 percent of GDP, a level unmatched since the Great Depression.
In response, Stockholm took a number of measures. It scaled back some of the social welfare benefits its citizens
enjoyed and privatized a host of industrial concerns. To integrate its economy with that of the rest of the continent,
Sweden joined the European Union in 1995, though it did not adopt the euro as its national currency when other
countries did in 2002.
The Swedish government also assumed ownership of about 25 percent of the assets of the country’s banks during
the crisis, attempting to isolate and then liquidate the bad assets that had caused the nation’s credit system to
freeze up. In retrospect, according to many economists, the so-called Stockholm solution was the key to Sweden’s
rapid economic recovery in the late 1990s and early 2000s. Indeed, economic policy makers in the United States
and elsewhere cited Sweden’s policies in the early 1990s as justification for similar measures to confront the more
widespread financial panic of 2008–2009.
Meanwhile, many conservatives in Europe pointed to the reduction of Sweden’s social welfare system as evidence
that, in a modern global economy, such government generosity was unsustainable. Indeed, they said, it had been
made possible only by the fact that Sweden had emerged out of World War II with little industrial competition from
a devastated Europe. Conversely, defenders of Sweden’s social welfare policies argue that the diminution in
services was minor and, compared with those in most other industrialized states, Swedish services remained
generous. In fact, they maintained, Sweden provides a model for how to sustain social welfare benefits in a global
economy, as its unemployment rate remained low and GDP growth strong for an industrialized country.
Because of the banking reforms enacted in the wake of the national financial crisis of the early 1990s, including
tighter regulation of lending, liquidity, and leveraging assets, Sweden’s financial sector weathered the global
financial crisis of 2008–2009 better than many other countries. Indeed, the four largest Swedish banks, which
together accounted for 80 percent of banking activity, had not invested in U.S. subprime securities and remained
profitable into 2009 (though subsidiaries had actively invested in domestic and commercial activities in the

neighboring Baltic states of Estonia, Latvia, and Lithuania, which underwent significant economic downturns).
Being outside the euro zone gave the Swedish Central Bank more flexibility in responding both to the crisis and to
the recession that followed it. In late 2008 and early 2009, it lowered interest rates four times.
Despite these advantages, Sweden was not able to escape the global recession entirely. During the first quarter of
2009, the country’s GDP took its biggest hit in history, dropping 6.5 on an annualized basis, while the
manufacturing sector suffered a 24 percent decline in production and exports fell by more than 16 percent. Among
the casualties was the automobile manufacturer Saab, which was forced into bankruptcy as a result of the
economic woes of its parent company, U.S. auto giant General Motors.
Still, because Sweden avoided the worst of the financial crisis and because the Swedish government had put its
fiscal house in order following the national crisis of the early 1990s, most economists were predicting that Sweden
would weather the global recession better than many other European economies and might emerge from it more
quickly. And, indeed, its economy returned to robust growth in 2010, with GDP rising by 5.5 percent, among the
highest rates in the developed world.
James Ciment and Marisa Scigliano
 
See also:  Denmark;  Finland;  Norway;  Stockholm School. 
Further Reading
“Bank Bail-outs: Stockholm Syndrome.” Economist, November 27, 2008. 
Economist Intelligence Unit (EIU). Country Report—Sweden London: EIU, 2009. 
Kent, Neil. A Concise History of Sweden. New York: Cambridge University Press, 2008. 
Nordstrom, Byron J. The History of Sweden. Westport, CT: Greenwood, 2002. 
Organisation for Economic Co-operation and Development (OECD). OECD Economic Outlook,  no. 84. Paris: OECD, 2008. 
 
Switzerland
 
Apart from its chocolates, cheeses, and watches, Switzerland is well known for its banks. The small, mountainous
country is one of the most important financial centers in the world, and its citizens count among the world’s
wealthiest. As a major financial center, Switzerland is often deeply affected by global booms and busts.
There are several reasons for Switzerland’s success in establishing a thriving financial sector. First of all, it lies in
the center of Europe, surrounded by Germany, France, Italy, Liechtenstein, and Austria. As commerce revived in
the middle of the last millennium, Switzerland became a conduit for much of Western Europe’s internal trade. The

Swiss earned money through imposing tariffs and guiding foreign businesspeople across the rough and hostile
Alps. The trade flows encouraged the development of currency-exchange offices—the precursor of banks.
Furthermore, for centuries Switzerland made a significant amount of money through the hiring out of its soldiers;
these Swiss mercenaries fought for different kingdoms all around Europe from the Middle Ages up to the
seventeenth century. Not only did it profit from the money that was paid in exchange for the soldiers’ service, but
it also took advantage of the connections that were created thanks to the various trade streams. Despite the fact
that many Swiss men fought as mercenaries in different wars around Europe, Switzerland itself has not been
involved in any war since 1515. Because of its neutrality and its relatively liberal government, Switzerland
remained a neutral country and safe harbor surrounded by countries with often dictatorial laws. Among the famous
refugees who came to Switzerland over the course of the centuries are the Huguenots, who brought the watch
industry with them, and Henri Nestlé, the founder of the famous food company. Switzerland has typically profited
from its immigrants.
Along with England, Switzerland was one of the earliest industrialized countries in Europe. The biggest sector was
the textile industry. Thanks to its early industrialization in the eighteenth century, Switzerland was among the
wealthiest countries in Europe. In 1770, as a more efficient way of producing cloth in England brought down textile
prices, the Swiss textile sector was severely affected. This led to an overall crisis in the Swiss economy that
forced the Swiss to also increase the mechanization of the textile sector. Weaving machines replaced traditional
looms. These allowed for less labor-intensive production and increased the Swiss per capita income. The industry
profited from the abundance of water, taking advantage of hydraulic power. Furthermore, the pharmaceutical and
consumption industry emerged and flourished. Likewise, the machine and watch industry grew more important and
increased the need for a financial sector to make loans available.
In 1848, Switzerland went from being a loose confederation of cantons (similar to states in the United States) to a
federal state. As a consequence, the economic area widened and became unified. At the same time, immigrants
exceeded emigrants, leading to a growing population. This was a boom time for Switzerland: The bank and
insurance sector flourished; the construction of a nationwide railway system led to employment and later to
increased mobility. The federal technical university was introduced and attracted scholars from around Europe.
Switzerland escaped the destruction of the two world wars thanks to its neutrality. The Swiss franc was a highly
demanded currency for the neighboring countries involved in the war. World War I and the instability in the
financial markets of the war-torn countries turned Switzerland into a significant international financial center; its
strong currency, relatively low taxes, and political stability attracted, and still attract, money from all around the
world. The introduction of the bank secrecy laws in 1934 increased the attraction of the financial center even
more. These laws gave anonymity and thus strict confidentiality to all holders of bank accounts.
Switzerland was also hit by the world economic crisis, which took its greatest toll in the United States through the
Great Depression. The Depression and World War II temporarily slowed down the Swiss economy. Just as it had
profited from World War I, Switzerland also profited from World War II, again because of its neutrality. During the
war, the Swiss franc was the only freely exchangeable currency. This is why Switzerland was able to carry out
significant financial transactions—one of the more shameful ones being the purchase of gold that the Nazi regime
had stolen from Jewish citizens and the central banks of the occupied countries.
Despite the opposition from the Allied powers, Switzerland managed to maintain its bank secrecy laws in the
post–World War II period. This strengthened the reputation of Switzerland as a refuge for capital from around the
world, which led to a steady increase in the Swiss gross domestic product (GDP). As a matter of fact, the inflow
of capital to Switzerland was so high that the Swiss authorities tried to regulate it.
Along with countries that were dependent on oil, Switzerland suffered a blow during the oil crisis in the 1970s. In
addition, the Swiss franc became very strong, which made Swiss exports more expensive for foreigners, thus
drastically decreasing exports. At the same time, the watch industry in Switzerland suffered due to the invention of
the quartz watch in Japan. Although the watch industry has recovered since—the Swiss brand Swatch was hugely
successful—the crisis in the 1970s permanently slowed down the previously fast-growing Swiss economy. Ever

since, Switzerland’s GDP growth rate has fallen behind that of its neighboring countries. However, because it
started out at a high economic level, the country continued to do well economically relative to its neighbors.
For much of the twentieth century, Switzerland’s GDP per capita was always higher than that of its European
neighbors. In the postwar setting, it passed the U.S. GDP as well. Yet the energy crisis in the 1970s, which hit
Switzerland harder than many other countries due to its lack of energy resources and its high energy consumption,
caused Switzerland to fall behind the United States. The 1980s and 1990s were also marked by slow economic
growth, and Switzerland was hard hit by the global recessions of the early 1990s and early 2000s. Nevertheless,
the slow growth came on top of a very high GDP per capita base. Indeed, on the eve of the late 2000s recession,
Switzerland had the second highest GDP per capita of European countries with a population above 1 million,
second only behind oil-rich Norway.
UBS, Switzerland’s largest bank, received $60 billion in government bailout funds in 2008. Meanwhile, the United
States was suing UBS for using Swiss banking secrecy laws to help American depositors evade taxes; UBS later
agreed to pay a fine of $780 million. (AFP/Stringer/Getty Images)
The financial meltdown of 2007–2009 hit Switzerland hard. Many of the nation’s banks had invested in subprime-
backed mortgage securities, which led to huge losses. UBS (Union Bank of Switzerland), one of the largest banks
in the world, announced losses of approximately $17 billion in 2008. Because the financial sector accounts for
more than 13 percent of the overall Swiss economy, it is feared that the crisis in the financial sector will lead to a
major slowing of economic growth in the rest of the Swiss economy. It is expected that Switzerland will be in a
recession through 2010. As demand from the newly wealthy in emerging markets decreases, 2009 emerged as
the worst year for the Swiss watch industry since the 1970s. The Swiss government is trying to decrease the
damage of the financial crisis by helping out the banks, and especially UBS, with 68 billion Swiss francs (more
than $80 billion). This might seem like a small sum in comparison with the roughly $1.5 trillion in financial bailout
and stimulus money that the U.S. government is spending in an attempt to save its economy in 2008–2009.
However, the Swiss rescue package is huge when put into perspective, for it accounts for almost 23 percent of

Switzerland’s GDP, whereas the U.S. rescue package is barely 10 percent of the income generated within the
U.S. borders. This is not the first crisis Switzerland has faced, but it certainly is the most severe.
Christina Zenker
 
See also:  Banks, Commercial;  UBS. 
Further Reading
Bauer, Hans, and Warren J. Blackman. Swiss Banking: An Analytical History. New York: St. Martin’s, 1998. 
Mottet, Louis. Geschichte der Schweizer Banken  (History of the Swiss Banks). Zurich: Buchclub Ex Libris, 1987. 
Vincent, Isabel. Hitler’s Silent Partners: Swiss Banks, Nazi Gold, and the Pursuit of Justice. New York: William
Morrow, 1997. 
Systemic Financial Crises
 
A systemic financial crisis is the occurrence of significant loss in asset values or failure of financial institutions on
a broad scale, triggered by a sudden, unforeseen shock. The triggering event can be the failure of a major
financial institution or of government policy, a stock market crash, a foreign investor assault on a national
currency, or an external event such as a natural disaster or an act of war that may shut down the financial
payment system. The negative shock spreads through declining stock prices, tightening credit lending, frozen
liquidity, changes in interest rates, and cascading or chain-reaction effects.
Systemic financial crises have market-wide, sometimes global effects that are extremely costly. Recoveries are
typically slow and painful. Affected nations or regions usually adopt significant changes in financial practices and
regulatory reforms after such a crisis. There have been many systemic financial crises in history. Major examples
over the last century include the Great Wall Street Crash of 1929, the currency crisis in Asia in 1997–1998, and
the global financial crisis of 2008–2009.
 Stock Market Crash of 1929 and Great Depression
Technological innovations and increased manufacturing productivity throughout the 1920s created the Roaring
Twenties, an era in which the United States prospered tremendously. But the wealth was unequally distributed,
creating an unstable economy. Excessive speculation in stock markets also generated an artificial boom in the
late 1920s, adding more instability to society. When the stock market began to crash in October 1929, it was the
triggering event that sent the U.S. and global economies into the Great Depression of the 1930s.
The Wall Street crash led to bank runs that forced widespread bank closures. During the early 1930s, over 9,000
banks failed. Since bank deposits were not insured at that time, people lost all their savings as a result. Surviving
banks were unwilling to lend. A reduction in spending by individuals and businesses led to decreased production
and rising unemployment. Rising unemployment further reduced spending, and the cycle continued.
According to some economists, some government policy responses may have helped to deepen the Depression.
For example, throughout the 1920s, President Herbert Hoover advocated a high-wage policy to prevent incomes
from falling. Later, in 1933, President Franklin D. Roosevelt passed the National Recovery Administration (NRA),

which aimed at reducing production and raising wages and prices. Both policies increased the real labor cost and,
according to critics, resulted in more business failures and unemployment.
More widely accepted by economists as a contributing factor in the global downturn, the U.S. Smoot-Hawley Tariff
Act of 1930 was another example of poorly thought through government policy. The act significantly raised U.S.
tariffs on over 20,000 imported goods to record levels, causing retaliations from America’s trading partners.
Countries tried to protect their own industries by raising tariffs and taxes on imports. International trade declined
sharply. This led to a collapse of the U.S. farming industry, which depended upon exports of items such as wheat,
corn, and other crops to overseas markets.
Furthermore, government tightened monetary and fiscal policies during the Depression and caused the economy
to contract further. During 1931–1932, in order to maintain the gold standard, the U.S. Federal Reserve (Fed)
increased interest rates at the height of the Depression. Many more banks and businesses failed afterward. In
1932, in an attempt to fill the budget deficit, Congress approved a large increase in taxes. Reduction in household
income reduced purchases and economic expansion. These contractionary policies caused even greater damage
to an already fragile U.S. economy.
By 1932, the U.S. economy had declined by half. Twenty-five percent of the workforce was unemployed. Ninety
percent of stock market values had been wiped out. The United States went through slow and agonizing economic
hardship throughout the entire 1930s, and the economy did not fully recover, say most economists, until the World
War II, when massive military spending eliminated unemployment and boosted growth.
The Great Depression was one of the most widespread systemic crises. It affected almost all nations in the world.
Many countries experienced bank runs and stock market crashes at first. For some countries, such as Britain,
France, Canada, the Netherlands, and the Nordic countries, the Depression was less severe and ended by 1931.
But in many other countries it lasted until the late 1930s or early 1940s. Poor economic conditions speeded the
militarization in Germany, Italy, and Japan, the Axis that plunged the world into World War II.
 Asian Currency Crisis and Russian Sovereign Default
Many Southeast Asian countries achieved high growth rates in the early and mid-1990s. Thailand, Malaysia,
Indonesia, Singapore, and South Korea, for example, experienced gross domestic product (GDP) growth rates of 8
to 12 percent. Fast growth attracted large capital inflows. Almost half of the total global capital inflows to
developing countries went to developing Asia during that time.
However, what some had referred to as an Asian “economic miracle” had been in part the result of increasing
capital investments developed into bubbles fueled by “hot money”—short-term capital flows that are expensive and
often conditioned for quick profits. Further, Asian economies were overly leveraged; that is, extensive borrowing in
both the public and private sectors was not supported by sufficient underlying assets. Many Asian governments
had adopted a policy to maintain fixed exchange rates in order to protect their exports. The implicit government
guarantees of the fixed exchange rate encouraged foreign borrowing and led to excessive exposure to foreign
exchange rate risks. For example, Thailand had acquired a large amount of foreign currency–denominated loans
that made the country effectively bankrupt even before the crisis. Corporate sectors of South Korea were highly
leveraged with foreign debt. Although varying widely from nation to nation, bank credits extended to unhealthy
levels in countries such as Thailand, South Korea, Malaysia, and Indonesia. There were other problems
underneath the veil of prosperity that destabilized the Asian economies. In Indonesia, for example, there was the
additional complication of “crony capitalism,” in which development money went only to those individuals who were
close to the centers of power.
In the mid-1990s, rising U.S. interest rates reduced capital flows into the Southeast Asian region. Interest rate
hikes drove up the value of the U.S. dollar and caused the appreciation of many Asian currencies that were
pegged to the dollar. Currency appreciations made exports costly and less competitive in global markets, and
Southeast Asia’s export growth slowed dramatically starting in the spring of 1996.

The Asian currency crisis started with a drastic devaluation of Thai baht on July 2, 1997, quickly followed by
devaluations of the Philippine peso, the Malaysian ringgit, the Indonesian rupiah, and the Singaporean dollar. After
the first round of currency attacks took place between July and October of 1997, the second round started in
October of 1998, when the Hong Kong stock market lost 40 percent of value and ended with the South Korean
won’s dramatic devaluation in less than two months.
After frantic attempts to protect their currencies, many countries gave up and let their currencies go into a free fall.
Only Hong Kong managed to maintain the peg, but at a cost of more than US$1 billion to defend its currency.
Steep devaluations of local currencies increased debt burdens in Asia, since most existing borrowings were made
in foreign currencies. As a result, there were massive numbers of bankruptcies.
The International Monetary Fund (IMF) created a series of rescue packages for the most affected countries.
However, the aid was conditioned on reforms that called for crisis nations to cut government spending, close
troubled banks, and aggressively raise interest rates. Many of these reforms were contractionary and pushed crisis
countries further into the recession. The IMF’s responses were widely criticized during and after the crisis. As an
example, Malaysia refused the IMF’s bailout. Instead, it imposed temporary capital controls to deal with the crisis.
Indonesia, Malaysia, South Korea, Thailand, and the Philippines were the countries most affected by the crisis.
They suffered permanent currency devaluations, real-estate busts, high unemployment, and social unrest. Nominal
U.S. dollar GDP per capita fell 42.3 percent in Indonesia in 1997, 21.2 percent in Thailand, 19 percent in
Malaysia, 18.5 percent in South Korea, and 12.5 percent in the Philippines. The crisis had ripple effects
throughout the globe, with major impact felt as far and wide as Argentina, Mexico, Chile, Brazil, Russia, and even
the United States.
Many viewed the Asian crisis as the trigger of the Russian sovereign default in 1998, as investors fled most
developing markets. Recession in Asia had driven down the price of oil. This was devastating to Russia as an oil
exporter. The weakening of the Russian economy put pressure on the ruble, resulting in its disastrous decline. By
mid-1998 Russia was in need of the IMF’s help to maintain its exchange rate. Near the end of that year, Russia’s
interest payments on its sovereign debt—national debt denominated in foreign currencies—well exceeded its tax
revenue. Even though the government hiked interest rates to 150 percent, investors still fled Russia. In late 1998,
Russia defaulted on its sovereign debt, sending another shock wave around the globe.
 Subprime Mortgage Meltdown and Global Financial Crisis of 2008–2009
Beginning in early 2001, the Fed began to lower interest rates and kept them low through 2004. Large capital
inflows from Asia and other emerging markets also pushed down U.S. interest rates. Excessively low borrowing
costs attracted real-estate investments and created a housing bubble. Many mortgages were originated without
the proper risk assessments. Approximately 80 percent of mortgages issued during this boom period were at
subprime and/or adjustable rates. These mortgages were then repackaged and sold widely as complex derivative
securities with high credit ratings. Many mega-sized financial conglomerates took excessive risks by using
borrowed money to invest in mortgage-backed securities.
In mid-2006, U.S. house prices peaked and then started a steep decline. Meanwhile, the Fed had raised
benchmark interest rates. Refinancing of home loans became difficult, and mortgage delinquencies soared. With
large holdings of assets backed by subprime mortgages, many financial institutions found that their assets lost
value significantly. Without adequate capital to cushion against losses, these companies were on the brink of
default. Since financial conglomerates were highly interlinked and interdependent, when the Lehman Brothers
financial services firm filed for bankruptcy on September 15, 2008, it quickly set off a chain reaction that adversely
impacted the entire financial system in the United States.
Investors then became excessively risk-averse, since they could not properly evaluate complex financial products
derived from subprime mortgages. As a result, few were willing to lend or invest. Liquidity dried up. Credit markets
were tightened, first in the financial sector and then in the nonfinancial sectors. Many companies were unable to

finance their daily operations when money markets froze in September 2008. The mortgage market meltdown led
to a sharp decline in business activity and rising unemployment nationwide. A financial crisis quickly became an
economic recession in the United States.
Between June 2007 and November 2008, Americans lost more than a quarter of their net worth. Total financial
losses, including home equity stocks, retirement investments, and savings and investment assets, add up to an
astonishing $8.3 trillion. The financial crisis in the United States quickly spread to the rest of the world, resulting in
a number of European bank failures and declines in various stock indices. For example, in 2007 there was a
consumer run on the British bank Northern Rock, which led to additional financial cracks on the Bank of England.
And, adjusted for size, Iceland’s banking collapse was the largest sustained by any nation in economic history.
The financial crisis also spread through international trade linkages and severely reduced the imports from many
Asian and Latin American countries.
Responses by the U.S. Fed, the European Central Bank, and other central banks around the globe were
dramatic. In the United States, to prevent a further spread of the systemic crisis, the Fed facilitated the acquisition
of several large investment banks. The Treasury Department placed Fannie Mae and Freddie Mac into
conservatorship and assumed control of insurance giant AIG in September 2008. To stimulate economic growth,
the Fed eased monetary policy by lowering the federal funds rate to historic lows, ranging from 0 percent to 0.25
percent. Many other central banks around the globe followed suit.
In 2008, the U.S. government passed the Emergency Economic Stabilization Act. It authorized $700 billion to the
Troubled Asset Relief Program (TARP) for lending funds to banks. On February 17, 2009, President Barack
Obama signed the American Recovery and Reinvestment Act of 2009, a $787 billion package that included
spending, homeowner assistance, and tax cuts to help stimulate the U.S. economy. The European Union and
many individual nations, such as the United Kingdom, Canada, China, India, Russia, Sweden, Mexico, and Brazil,
released similar packages to stimulate economic growth. By the end of 2009, governments worldwide had spent or
committed trillions of dollars in loans, asset purchases, guarantees, and direct spending in an attempt to revive
their economies from the ongoing crisis.
 Common Characteristics
Every crisis is unique. There are, however, some commonalities and trends that can be summarized. Systemic
financial crises are usually initiated by unforeseen events, but the real causes are not the triggers but systemwide
fundamental weaknesses such as asset bubbles, excessive borrowing, fraudulent financial and corporate
practices, and regulatory failures.
Many events may trigger a systemic crisis. The failure of a large financial company is more likely to trigger a
systemic crisis than the failure of a nonfinancial company. Due to the nature of their businesses, financial
institutions are more interlinked and highly leveraged, which makes it more likely for them to transmit and magnify
a negative shock. Financial companies, especially banks, finance their long-term assets with short-term debt,
causing asset maturity mismatches. This makes them more vulnerable to interest-rate hikes and liquidity freezes
that are common before and during a financial crisis.
Due to financial and technological innovations and increasing globalization, the impact of a systemic financial crisis
has become stronger and faster than ever. Incomplete information can help spread a crisis. Recent financial
innovations such as financial products derived from subprime mortgages are often complex, opaque, and difficult to
value during a crisis. When there is a lack of transparency, investors are more likely to become excessively risk-
averse and to engage in herd mentality or contagious behavior that helps drive down asset values steeply.
Technological innovations have increased the speed and efficiency of financial transactions, but they leave little
time for market participants to digest proper information and to recover losses. Therefore, a crisis becomes more
severe as panic selling becomes common.
Globalization increases the linkage of financial institutions and the interdependency of financial markets, which in

turn helps spread a crisis to a wider scope and a larger scale. Financial globalization helps savers to earn higher
returns and borrowers to obtain cheaper capital worldwide. Easy access to capital helps economic growth as well
as attracting financial speculation, and helps build up asset bubbles. Huge amounts of overseas capital can be
quickly withdrawn when a bubble bursts. Such capital flight can leave the domestic economy in shock.
Globalization also increases international trade among countries. Troubles in one country’s financial system often
spread to another country’s economic sectors through trade linkages during a crisis.
To reduce their vulnerability to a systemic financial crisis, financial institutions have traditionally tried to participate
in clearinghouses or trade through financial exchanges. However, since private companies do not bear all the
costs of their own failure, say economists, government has a role to play in preventing and managing a crisis.
The failure of the British banking firm Overend & Gurney in 1866 led to a key change in the role of central banks
in managing financial crises. Overend & Gurney was a major London financial institution whose failure caused
widespread bankruptcy of many smaller banks. In order to prevent this spillover effect, the Bank of England
provided liquidity to the entire British financial system. Since then, it is common for central banks to act as the
“lender of last resort” during a systemic financial crisis.
The Great Depression greatly transformed the role of government in the economy. During the Depression, the
size and expenditure of the U.S. federal government increased dramatically, and it gained huge power in
managing the economy. After the Depression, the government established extensive regulations to prevent future
crises from happening; that is, it established the Securities and Exchange Commission and the Federal Deposit
Insurance Corporation and separated commercial and retail banking through the Glass-Steagall Act. Many of
these rules and regulations helped shape the financial systems in the United States and many other countries
today.
The magnitude of government intervention has become stronger through each crisis. Central governments and
international organizations, such as the IMF, have played increasingly important roles in preventing and managing
crises. International cooperation has also became critical in protecting and reviving the global economy.
 Future Issues
It is difficult to predict when, where, how, and why the next systemic financial crisis will occur. Economists and
policy makers have studied each past crisis extensively. Many of the lessons learned from the past could help to
prevent a future crisis.
Proper responses in managing and recovering from a crisis are equally important. Two related issues deserve
attention. First, how should governments manage these too-big-to-fail or too-connected-to-fail companies?
Failures of such firms would likely impose severe losses on other firms or markets. A government’s bailing out of
large, systemically important firms may prevent further spread of a crisis. However, expectations of bailouts by big
companies may cause moral hazard and give these companies incentives to take more risks than they otherwise
would. Excessive risk taking could thus become one of the causes of another potential crisis.
Second, to what extent and with what financial resources should government intervene in the market to manage a
crisis? In the first decade of the twenty-first century, many economists expressed concerns over the enormous
U.S. federal deficit and the growing national debt due to various bailout and stimulus packages. Other economists
maintained that increased debt was a necessary response to the unprecedented crisis, without which the
economic downturn would have been much worse.
It remains to be seen if regulations will be put in place to prevent future excessive risk, and if there will be
sufficient government borrowing capacity should another systemic financial crisis occur.
Priscilla Liang
 

See also:  Recession and Financial Crisis (2007-);  Regulation, Financial;  “Too Big to Fail”; 
Troubled Asset Relief Program (2008-). 
Further Reading
Allen, Franklin, and Douglas Gale. Understanding Financial Crises. New York: Oxford University Press, 2009. 
Bernstein, Michael A. The Great Depression: Delayed Recovery and Economic Change in America, 1929–
1939. Cambridge, UK: Cambridge University Press, 1989. 
Foster, John B., and Fred Magdoff. The Great Financial Crisis: Causes and Consequences. New York: Monthly
Review, 2009. 
Goldstein, Morris. The Asian Financial Crisis: Causes, Cures, and Systemic Implications. Washington, DC: Institute for
International Economics, 1998. 
Honohan, Patrick, and Luc Laeven, eds. Systemic Financial Crises: Containment and Resolution. New York: Cambridge
University Press, 2005. 
International Monetary Fund. Global Financial Stability Report, April 2009: Responding to the Financial Crisis and
Measuring Systemic Risks. Washington, DC: International Monetary Fund, 2009. 
Shiller, Robert J. The Subprime Solution: How Today’s Global Financial Crisis Happened, and What to Do About
It. Princeton, NJ: Princeton University Press, 2008. 
Tirole, Jean. Financial Crises, Liquidity, and the International Monetary System. Princeton, NJ: Princeton University
Press, 2002. 
Tax Policy
 
Taxes are monetary levies imposed by governments on individuals, households, businesses, and other
institutions. While taxes come in a bewildering variety—with levies on income, property, consumption, importation
and exportation, capital gains, inheritance, and so on—they basically fall under two broad organizing headings:
benefits received and ability to pay. Taxes under the first heading might include highway tolls—the more one
benefits from driving on the highway, the more one pays. If highways, however, were funded by an income tax in
which the rich pay a higher share than the poor, then the taxation used to pay for the highways would fall under
the ability-to-pay heading.
 Principles
Two even larger, philosophical principles are also at work in determining tax policy—horizontal and vertical equity.
The former principle—nearly universally accepted—says that people or institutions doing the same thing should be
treated the same. For example, all other things being equal, the property tax rate on a store selling toys and a
store selling stationery should be the same. More controversial but still widely accepted is the principle of vertical
equity: that wealthier people should pay higher rates of taxes, particularly on income, than poorer people. When
the tax rate is the same for all persons or institutions, it is said to be proportional; when it more heavily taxes the
rich, it is called progressive. When it more heavily taxes the poor, it is said to be regressive. An example of the
latter is the sales tax. The less wealthy you are, the less you save and the more you spend, as a percentage of
your income, on consumption, and hence the greater the percentage of your income you pay in sales taxes. If you

earn $1,000 per month, for example, you are likely to spend all of it on consumption, and if the sales tax rate is 8
percent, you would pay 8 percent of your income in sales taxes. However, if you earn $30,000 per month, you
might spend only half of it on consumption and save the other half. Thus, you would pay $1,200 (8 percent x
$15,000) in sales taxes. Since $1,200 is 4 percent of $30,000, the person with the higher earnings would be
paying only 4 percent of her income in sales taxes. This might be partially offset, as in many jurisdictions, by
exempting from taxation the items that poorer people spend a higher percentage of their money on, notably
nonprepared food.
At the other end of the equation, taxes serve two general purposes. First and foremost, taxes pay for what the
government does—national defense, domestic security, transportation, education, health care, social welfare, and
any number of other public functions, services, and goods. Second, taxes are a tool of economic and social policy,
as governments attempt to maintain growing economies and smoothly running societies by imposing or not
imposing taxes on a particular economic good or activity, or by adjusting the level of taxation on a good or activity.
Tax policy, then, attempts to determine all of these things—what kind of taxation is fair and equitable; what serves
societal goals; what best promotes sustained and healthy economic growth; and what level provides the
government with the revenues it needs to performs its duties, functions, and responsibilities. Looming over all of
this is the macroeconomic function of taxation—the shifting of private economic resources to public ends. That is,
when government imposes a tax, it is removing the decision-making power over the economic resources in
question from the individual or privately owned institution and transferring it to the government itself, which is
supposed to represent the public good and, in a democracy, is ultimately controlled by the public. So, to cite a
simple example, if the government had a blanket tax of 10 percent on all property and economic activity, then it
would be assuming control of 10 percent of a nation’s resources.
When it comes to tax policy, the landscape is always shifting and is always subject to political considerations.
Populations grow and move about, technology changes economic and social activity, and public sentiment about
what is an acceptable level of taxation and what taxes should be used to pay for mutates. Thus, tax policy is
always changing as well. Indeed, one of the overriding decisions in elections—where citizens determine the broad
outlines of how they want government to operate in the future—concerns tax policy.
 Business Cycle
Tax policy also plays a major role in the business cycle. Excessive taxation or the wrong kinds of taxation can
contribute to the intensity of the contraction phase of the cycle. High taxes on individuals can lower aggregate
demand while various kinds of taxes on business can dampen investment and hiring. Taxes can also distort
economic behavior, causing individuals and businesses to make economic decisions based on avoiding or
minimizing tax liability rather than as a response to market conditions. This can, then, impede the smooth running
of markets and contribute to slower or negative economic growth.
The more important role of tax policy vis-à-vis the business cycle is as a remedy or a response. Taxes represent
one of three basic tools at a government’s disposal to respond to economic slowdowns and contractions. The
others are monetary policy and spending—the latter, with taxation, falling under the rubric of fiscal policy. During
these periods of slowdowns and contractions, governments may reduce taxes on business activities in order to
leave more money in the private sector for investment and hiring. To this end, the government can use all kinds
of targeted tax breaks, tax cuts, and tax rebates. It can also reduce the taxes on individuals or rebate taxes
already paid in order to increase aggregate demand.
While lowering taxes seems like an obvious solution to an economic downturn, this policy presents problems of its
own because during economic downturns, tax revenues are already declining. Since so much of taxation is based
on economic activity—people’s income, business profits, consumption—when that activity declines so do tax
revenues, as occurred quite dramatically in the recession of 2007 to 2009. At the same time, economic
contractions represent a time of increased government expenditures, particularly on things like unemployment
compensation and social welfare programs. The combination of lower tax revenue and higher expenditures can

lead to ballooning deficits, as was also evidenced in the 2007–2009 recession at the municipal, state, and federal
levels in the United States, as well as in many other countries. When the government has to borrow large sums of
money, it can make borrowing more expensive for private industry and individuals, dampening consumption,
investment, and hiring.
Economists and policy makers vigorously debate the effectiveness of the various tools at the government’s
disposal in responding to economic downturns, with those on the right emphasizing tax cuts and appropriate
monetary policy and those on the left arguing for more government spending. This breakdown, of course,
oversimplifies things. Even liberal Post Keynesian economists agree that properly targeted tax cuts and an
expansive monetary policy are necessary to combat recessions, while all but the most conservative neoclassical
and monetarist economists agree that the government has to spend more on things like unemployment
compensation during such times.
The question, then, is the balance, as the debate over the $787 billion economic stimulus package promoted by
the Barack Obama administration in early 2009 revealed. The plan included both spending and tax cuts but was
largely opposed by Republicans because it emphasized the former, while Democrats supported it for the same
reason. In the wake of the stimulus plan, Republicans argued that the emphasis on spending increases over tax
cuts resulted in rising unemployment rates while the Obama administration insisted that, absent that spending, the
rate would have been much higher. In any case, as unemployment remained in or near double digits, the
administration shifted to targeted tax cuts or rebates for business in early 2010, with an emphasis on those, such
as cutting the amount small businesses needed to contribute to Social Security taxes for newly hired employees
and offering businesses a tax credit on new hires.
James Ciment
 
See also:  Fiscal Policy;  Monetary Policy. 
Further Reading
Brownlee, W. Elliot. Federal Taxation in America: A Short History.  2nd ed. New York: Cambridge University Press, 2004. 
Slemrod, Joel, and Jon Bakija. Taxing Ourselves.  4th ed. Cambridge, MA: MIT Press, 2008. 
Steuerle, C. Eugene. Contemporary U.S. Tax Policy.  2nd ed. Washington, DC: Urban Institute, 2008. 
 
Technological Innovation
 
Technological innovation is the development of new ways to attain the goals of humankind. It can come in two
forms: process and product. The former allows firms to make more of a product than before with the same inputs

of labor, natural resources, and capital, to or to make the same amount of a product as before with less labor,
resources, or capital input. More apparent to persons outside a particular industry is product innovation—that is,
the introduction of new or improved products for individual or institutional consumers.
Technological innovation has been a key factor behind the steadily improving living standards of humanity,
particularly since the advent of the industrial revolution 250 years ago. At the same time, technological innovation
is also disruptive and destabilizing. That is, technological innovation as adapted by entrepreneurs leads to the
development of new processes and products but also the destruction of old ones and the economic collateral
damage of bankruptcy and unemployment that results from it. In addition, the adaptation of new technology by
entrepreneurs, while first creating an economic expansion, can then lead to economic contraction if too many
entrepreneurs invest in it, saturating the market and leading to ever lower profit margins and eventual bankruptcy
for less efficient firms.
Technological innovation has been identified as a primary cause of upswings in the business cycle throughout
history. Yet mechanized farming was a mixed blessing for the American farmer in the 1920s, contributing to large
crop surpluses and falling prices. (Buyenlarge/Hulton Archive/Getty Images)
 Impact on Business Cycle
Several aspects of technological innovation have significant bearing on economic cycles. Foremost is the fact that
technological innovation occurs unevenly through time. If not for this empirical reality, it would be much less likely
that innovation could generate booms and busts. One key reason for the temporal irregularity is that rare “big”
innovations spawn a series of small innovations that improve on or extend the big ones. The former are often
referred to as “general purpose technologies” (GPTs). Examples include the steam engine and the computer. In
both cases, the original device was much improved through time and found application in a wider and wider range
of economic activities. Thus, a GPT innovation creates an environment in which rapid innovation can occur. The
personal computer, for example, led to a host of software and hardware innovations that have both changed the
way virtually all commerce gets done and led to the introduction of a vast array of new consumer products and
services.
A second critical characteristic of technological innovation is that it generally, though not always, requires some
investment before it can be applied to commerce and industry. That is, new technology often requires the purchase

of new machinery and even the construction of new buildings. For example, the assembly line—a process-type
innovation—required entrepreneurs to erect factories suitable for its application, while the microchip required the
construction of facilities with sterilized “clean rooms.” Even when new machines or buildings are not necessary,
some upfront investment in the training of workers is generally required.
Thus, technological innovation—along with factors such as the cost of borrowing and the expectations of future
growth—is a critical contributor to a firm’s decision to increase investment in capital goods. In short, all other
factors being equal, investment increases during periods of technological innovation and adaptation and decreases
during periods of technological stagnation. With regard to expectations, it is noteworthy that a firm’s expectations
for the future will depend a great deal on its technological predictions. If it foresees being able to introduce new
products or improve the cost or quality of existing products, it will be confident of the future. Notably, the business
press focuses on how innovation might affect economic prospects in the short and medium term.
A third and related characteristic is that technological innovation might also affect consumption decisions. People
are more likely to spend rather than save if some new product has just been introduced into the marketplace. This
effect is most likely in the case of consumer durables—goods that consumers buy only every several years.
Historically, the most important example may be the automobile, which became a mass-market good with the
development of assembly-line production. Many economists have suspected that some of the boom-bust cycle of
the 1920s and 1930s in the United States and several other countries reflected the fact that the middle class
bought automobiles in the late 1920s and did not see any need to buy additional or replacement vehicles in the
early 1930s. Automobile manufacturers struggled in response to convince consumers through annual model
changes that new cars were superior to the old but had little success in the absence of technological innovation in
automobile quality.
Fourth, different types of technological innovation—that is, process versus product innovation—have different
effects on investment, consumption, and ultimately employment. Both types of innovation will usually encourage
investment. They have quite different effects on consumption, however. As suggested above, new goods or
services (or improved quality in existing goods and services) will encourage increased consumption expenditure. A
drop in the price of a good already regularly purchased will usually lead to a drop in consumption expenditure.
There may be exceptions here, if the drop in price encourages a more-than-proportional increase in the quantity
demanded. The above example of assembly lines and automobiles is such an exception. Yet, in general,
expenditures fall as prices fall.
 Innovation and Employment
The effects on employment of the two types of innovation differ greatly. To produce a new product, firms will
generally need to hire workers. If, however, a firm is able to decrease the cost of producing existing goods (and
the quantity demanded does not rise by more than the cost falls), the effect will generally be a decrease in
employment. In both cases, these within-firm employment decisions may be offset elsewhere in the economy.
Consumers spending more, or less, money on the output of the innovating firm may spend less, or more,
elsewhere in the economy, and thus, employment may move in the opposite direction in other firms, though
adjustments generally do not happen instantaneously.
An economy characterized by a great deal of process innovation and very little product innovation may thus
experience increased unemployment. This was exactly the situation in the United States and other developed
countries during the interwar period. The only major new product introduced in the decade after 1925 was the
electric refrigerator (hundreds of thousands were sold during the depth of the Depression). On the other hand, the
interwar period witnessed three of the most important major process innovations of the last century: the assembly
line, electrification, and continuous processing (where homogenous outputs like chemicals were produced
continuously rather than in batches). As workers were let go by firms introducing these new processes, there were
no firms looking to hire workers to make new products. The problem, then, is not process innovation itself.
Process innovation is, after all, the main driver of the productivity advance on which economic growth depends.

Yet unless accompanied by product innovation, process innovation may yield unemployment, at least in the short
and medium term.
While product and process innovation both encourage investment, there is considerable difference across
particular innovations with respect to the amount of investment required. The assembly line, electrification, and
continuous processing all required a great deal of investment during the 1920s. The most important process
innovation of the 1930s was the development of new tungsten carbide cutting tools: these could generally be fitted
to existing machines at little cost but allowed cutting operations to be performed several times faster. There was
also a great deal of organizational innovation during the 1930s as firms figured out how best to manage the
technologies put in place in the 1920s. This also tended to require little investment but produced major advances
in worker productivity.
 Real Business Cycle Theory
As the above cases make clear, technological innovation plays a role in longer-term boom-bust cycles of capitalist
economies. Less obvious—and less studied until the last few decades—has been the role of technological
innovation on shorter-term cycles. Real business cycle theory—developed by Norwegian economist Finn Kydland
and his American counterpart Edward Prescott in the 1980s (the two would win the 2004 Nobel Prize in
Economics for their work)—attempted to address this question. Real business cycle theory posited a link between
productivity growth and unemployment that depended on workers choosing not to work for a while if they expected
real wages to rise in future. That is, if workers anticipate future process innovation (the theory neglected product
innovation), they might decide to put off getting a job until wages rose to reflect the fact that workers could
produce more. Not surprisingly, it proved difficult to empirically support the idea that unemployment rates
responded to annual changes of a couple of percent in productivity. Moreover, the theory ignored the possibility
that firms might hire fewer workers after introducing a process innovation.
 Government Policy
Government also plays a significant role in terms of technological innovation, financing both scientific and
technological research, encouraging or discouraging innovation through a variety of regulations, and providing
patent protection to many sorts of innovation. But given the importance of different types of technological
innovation to the business cycle, economists and policy makers debate whether governments should adjust their
technology policies to emphasize steadier, long-term growth, which is generally the economic outcome most
governments seek to promote.
Governments, say some economists, might indeed be able to encourage both more innovation and a better
balance between product and process innovation. The latter could prove difficult in practice because it is difficult to
predict the likely result of any research process or how popular any sort of innovation will prove in the
marketplace.
The more innovation there is in a society, the more likely that there will be growing firms looking to hire workers
displaced by firms that are shedding workers (and vice versa). Expectations of the future are more likely to be
positive when lots of innovation is occurring. Governments might thus find that policies that encourage innovation
in general not only encourage growth but reduce the severity of booms and busts.
Finally, governments may find, in turn, that policies that reduce the intensity of booms and busts also serve to
advance the rate of technological innovation. While certain types of innovation may be encouraged during boom or
bust periods, a more likely effect is that severe booms and busts discourage innovative activity. Funding for
research tends to decline during busts. This reflects both lower firm profitability and lowered expectations of future
demand. Firms that are more confident about the future will be more likely to invest in research.
Rick Szostak

 
See also:  Dot.com Bubble (1990s-2000);  Information Technology;  Real Business Cycle
Models;  Spillover Effect. 
Further Reading
Freeman, Christopher, and Francisco Louçã. As Time Goes By: From the Industrial Revolutions to the Information
Revolution. New York: Oxford University Press, 2001. 
Kleinknecht, Alfred. Innovation Patterns in Crisis and Prosperity: Schumpeter’s Long Cycle
Reconsidered. London: Macmillan, 1987. 
Szostak, Rick. Technological Innovation and the Great Depression. Boulder, CO: Westview, 1995. 
Vivarelli, Marco. The Economics of Technology and Unemployment. Cheltenham, UK: Edward Elgar, 1995. 
Woirol, Gregory R. The Technological Unemployment and Structural Unemployment Debates. Westport,
CT: Greenwood, 1996. 
Tequila Effect
 
The “tequila effect” (efecto tequila) is a term that refers to the negative effect of the Mexican peso crisis on
various Latin American economies in 1995. More broadly, it refers to the diffusion of an economic crash from one
Latin American country to other countries in the region. In December 1994, the drastic and sudden devaluation of
the Mexican national currency, the peso, affected not only Mexico but several other Latin American countries as
well. After the collapse of the peso, many foreign speculators lost confidence in the stability of Latin American
economies; this loss of confidence triggered a massive flight of capital from the whole region.
During the administration of President Carlos Salinas de Gortari (1988–1994), the Mexican government tried to
attract foreign direct investment (FDI). With his administration opening markets and increasing economic links with
industrialized countries, especially the United States, foreign investors brought technological know-how, capital,
and jobs to the nation. Prior to President Salinas’s administration, almost all investments into Mexico were tied to
oil-producing activity. After 1988, investments targeted new industries, and Mexico was able to build a large
source of international money reserves.
Beginning in the mid-1980s, Mexico had an apparently reliable economic framework, with relatively low inflation
ensured by exchange-rate commitments and a public-sector surplus. Economic reforms had increased the
country’s productivity and created more jobs and generated more exports, especially to the United States and
other members of the Organisation for Economic Cooperation and Development (OECD). Into the 1990s, the peso
seemed strong, and Mexico seemed a good place to invest. The signing of the North American Free Trade
Agreement (NAFTA) in 1992 helped ensure an apparently bright economic future.
But the Mexican economy proved less stable than many had anticipated, relying on the steady influx of foreign
direct investment. As a result of the heavy FDI, Mexico began racking up deficits in the balance of trade. To
counterbalance its losses and finance the growing deficit, the Mexican government decided to seek even more
outside capital. This led to an even stronger dependency on foreign investments. In the last years of President
Salinas’s administration, all indications pointed toward a mounting economic crisis, though neither the president
nor his successor, Ernesto Zedillo Ponce de León, saw the signs. The main strategy of the Salinas administration

was to keep inflation low, even though this inevitably slowed down the economy and increased unemployment.
Other economic experts argued that a depreciation of the national currency would enhance economic growth. A
devaluation of the peso, they reasoned, could help increase exports, as Mexican products would become cheaper
and more competitive on the international market. At the same time, the price of imported goods would increase
for Mexicans, resulting in a shift of domestic demand toward local products.
After being elected in August 1994, Zedillo was reluctant to devaluate the peso, but a conflict in the southern state
of Chiapas forced him to act. Because of the still-strong peso, many people in Chiapas lost their jobs and were no
longer willing to accept promises and excuses by the government. It seemed to many that the country as a whole
was getting wealthier but that the poor were not getting their share. When foreign investors heard the news about
insurgencies in Chiapas, they began to withdraw their capital. Mexico lost some $16 billion in just ten months,
from February to December 1994.
At this point, President Zedillo decided to break his campaign promise of keeping the peso stable and instituted a
devaluation in December 1994. The unforeseen change in policy made international investors question whether
they could trust the new government and the stability of the Mexican economy. The government, for its part,
believed that a decrease in FDI was a normal sign of adjustment and did not anticipate the extent of the coming
crisis.
Two days after the investor reaction, the Zedillo administration decided to let the peso float freely against the U.S.
dollar. Foreign investors owned about $29 billion in Mexican tesobonos, short-term bonds that must be paid back
in U.S. dollars. Afraid of losing all their money, they cashed them immediately. Within just a few days, large sums
of FDI left the country, and the peso became virtually worthless.
The United States reacted rapidly to the crisis, buying up large amounts of Mexican pesos to stem further
depreciation, but the move was only marginally successful. U.S. president Bill Clinton then granted a $20 billion
loan to Mexico via the Exchange Stabilization Fund; the United States and Canada offered short-term currency
swaps; the Bank for International Settlements offered a line of credit; and the International Monetary Fund (IMF)
approved a standby credit agreement that would remain in effect for eighteen months. In total, Mexico received
about $50 billion in loans, with other Latin American countries, such as Argentina and Brazil, contributing several
million more. As a result of these efforts, the peso remained stable for the next few years, until it was destabilized
again by the Asian crisis of 1997–1998.
Because Mexico was required to meet its commitments toward NAFTA and the IMF, it had to introduce tight fiscal
policies, keep its market open to free trade, and maintain a floating exchange rate. Thanks to its membership in
NAFTA and a weak peso, exports from Mexico increased and kept the country from falling into a long-term
recession. By 1996, the country’s gross domestic product was showing positive growth, but millions of private
households, which took out loans and mortgages at tremendously high interest rates during the economic crisis,
had to struggle for several years until they were able to repay their debts.
The following year, 1997, brought international contagion in a number of Latin American countries. In previous
years, capital inflows had helped Latin American countries to improve their economies and overcome a recession.
But many investors interpreted the Mexican crisis as indicating increased risk in investing in the entire region.
Argentina and Brazil were among the countries worst hit by the tequila effect because their economic stability and
rapid growth also relied on FDI. As nervous speculators withdrew their funds in the wake of the Mexican crisis,
the Argentine stock market plunged and remained bearish for several months; domestic spending declined and an
increase in interest rates in March 1995 led to a credit crunch. Although the effects were not as severe as in
Mexico, it took Argentina a long time to return to its former interest rate. Even after interest rates reached the
precrisis level, output and investment there continued to decline.
Carmen De Michele
 

See also:  Debt;  Latin America;  Mexico. 
Further Reading
Humphrey, Brett M.  “The Post-Nafta Mexican Peso Crisis: Bailout or Aid? Isolationism or Globalization?” Hinckley Journal
of Politics 2:1 (2000): 33–40. 
Lustig, Nora. Mexico: Remaking of an Economy.  2nd ed. Washington, DC: Brookings Institution, 1998. 
Pastor, Manuel, Jr.  “Pesos, Policies, and Predictions.” In The Post-NAFTA Political Economy, ed. Carol Wise. University
Park: Penn State University Press, 1998. 
Roett, Riordan. The Mexican Peso Crisis. Boulder, CO: Lynne Rienner, 1996. 
Thorp, Willard Long (1899–1992)
 
The American Economist Willard Thorp spent much of his career in academia, government, and business
researching business cycles in the United States and around the world. As director of economic research at Dun &
Bradstreet, he became a vocal critic of American business’s failure to plan effectively to avoid a repetition of the
Great Depression, especially as the economy began to improve.
Thorp was born on May 24, 1899, in Oswego, New York, and raised in Chelsea, Massachusetts, and Duluth,
Minnesota. He served in the U.S. Army during World War I, after which he earned a bachelor’s degree from
Amherst College in 1920, a master’s degree in economics from the University of Michigan in 1921, and a
doctorate from Columbia University in 1924.
In 1923, he joined the new National Bureau of Economic Research, where he compiled centralized economic data
on seventeen countries dating to 1890, resulting in the 1926 publication of Business Annals, with an introduction
by economist Wesley C. Mitchell. A historian and an outspoken advocate for the use of statistics in predicting and
evaluating economic events, Thorp, who had become head of economic research at Dun & Bradstreet in 1935,
wrote an article titled “Wanted—Industrial Statistics,” in which he stated that the use of statistics and analysis had
declined since the 1920s, a trend he saw as deplorable. Also in 1935, Thorp became editor of Dun’s Review.
Researching business cycles in the 1930s, Thorp noted that recovery is the most difficult phase of a business
cycle to identify. He stated that recessions are easily defined and are often signaled by “spectacular events.”
Recoveries, on the other hand, are not marked by spectacular events; additionally, industry and government
economies recover at different times and rates. Thorp also criticized the era’s lack of sophistication in data
gathering and market and industry analysis.
In the 1920s, Thorp had written about the economic growth that occurs during wartime as a result of increasing
demand for goods by the government, the maximum use of the workforce, and the elimination of competition as
some companies adjust their businesses to support the war effort. Thorp used historical evidence to show that
such prosperity is typically marked by inflation and rising prices and followed by severe recession. These views
led to various U.S. government appointments from the 1930s to the 1960s.
In 1938, Thorp became an adviser to the U.S. secretary of commerce and an analyst for the National Economic
Committee on Monopolies. Beginning in 1945, he served under four U.S. secretaries of state and represented the
United States at the General Agreement Tariffs and Trade (GATT) talks in Geneva, Switzerland, and at the Paris

Peace Conference. Serving as assistant secretary of state in the administration of President Harry S. Truman,
Thorp supported what became known as the Marshall Plan, believing it was the responsibility of the United States
to help rebuild Western Europe and to provide aid to noncommunist countries to stop the spread of Soviet
communism.
In the 1950s, Thorp returned to academia, teaching primarily at Amherst, from which he retired in 1965. In 1960,
he headed a United Nations mission to Cyprus, and in 1961, at the request of President John F. Kennedy, he
headed an economic mission to Bolivia.
Thorp spent his later years lecturing, writing, and consulting. Writing in the New York Times shortly before his
death, he suggested that the U.S. government should adopt a massive infrastructure improvement program to
assist the U.S. economy, much as it had during Roosevelt’s New Deal. He died in Pelham, Massachusetts, on
May 10, 1992.
Robert N. Stacy
 
See also:  Mills, Frederick Cecil;  Mitchell, Wesley Clair;  National Bureau of Economic
Research. 
Further Reading
Thorp, Willard Long. Business Annals. New York: NBER, 1926. 
Thorp, Willard Long. Economic Institutions. New York: Macmillan, 1928. 
Thorp, Willard Long. The New Inflation. New York: McGraw-Hill, 1959. 
Thorp, Willard Long.  “Wanted—Industrial Statistics.” Journal of the American Statistical Association 1(March 1936): 193. 
Three-Curve Barometer
 
After World War I, the study of business cycles emerged as a new field in economic statistics. In 1919, the
Harvard University Committee for Economic Research began publishing a periodic business indicator in its
general Review of Economic Statistics (later Review of Economics and Statistics). The three-curve barometer, as
the new indicator was called, was developed by Harvard economist Warren Milton Persons (1878–1937) as a
means of tracking turns in the business cycle. The three curves measured speculation, business, and money and
credit. Because Persons was a member of the Harvard Committee for Economic Research, chaired by Charles
Jesse Bullock, his tracking system was also known as the Harvard barometer.
After the turn of the twentieth century, statistical information on economic activity became increasingly available to
American businessmen as large corporations, banks, and the government all began to realize that it was to their
advantage to have smaller players aware of market conditions. The goal was to minimize disruptions and shocks.
Research departments began issuing circulars to maximize investment stability, and before long, specialized
private organizations were gathering, processing, and publishing information for the general public. The first
business indicators appeared in the years leading up to World War I, generated by such forecasting agencies as
the Brookmire Economic Service and Babson Statistical Organization. Because these early services used
relatively simple methods to generate their barometers, their analyses were limited.

Roger Babson, a pioneer in the use of statistical charts to forecast business cycles and the founder of the Babson
Statistical Organization in Wellesley, Massachusetts, devised a composite called the Babson chart, which graphed
weighted indices as an X-Y line equalizing booms and busts. Because Babson’s organization buttressed the data
with frequent sampling of the opinions of leading businessmen, the Babson chart provided a useful measure of
business confidence. Another early business indicator, the Brookmire barometer, devised by James H. Brookmire
of St. Louis, forecast changes in the business cycle based on the assumption that banking led the way in
significant rises and falls, with stocks following several months later and general business some months after that.
The Harvard three-curve barometer built on the general concept of the Brookmire system. Persons, however, was
confident that he could build a better barometer by interpreting and analyzing data rather than just gathering and
publishing it. In January 1919, Persons and the Harvard committee began adjusting monthly data since 1903 for
seasonal variation and long-term trends. Each corrected series was charted, and the charts of various series were
compared. Based on patterns developed from this analysis, the charts were grouped by similar variations in cycle.
At this point, the committee developed a composite for each group and brought the composites together on a
single chart. The result was three curves, each related to a specific type of economic activity: speculation (A),
business (B), and money and credit (C). Persons and his team immediately recognized that the three curves
seemed to maintain similar relations to each other through each type of activity. Nevertheless, to counter the
perception that the C curve was less important than the A curve, the Harvard barometer, beginning in 1920, used
both A and C to forecast B, with a decline in A and a simultaneous rise in C indicating a serious problem in B.
The three-curve barometer predicted the economic crisis of 1920–1921, putting forecasts by the Harvard group
and others in high demand. Businesses relied on Persons’s barometer in ordering goods and making other key
investment decisions. Even the failure of the barometers to forecast trends of 1923 was excused, as the Federal
Reserve sought to modify economic trends by injecting or removing money from the economy.
The Harvard barometer became internationally known, and other institutions adopted its methods. AT&T used it as
the basis for its own barometer, and General Motors developed a barometer it was willing to put against
Harvard’s. Together, Persons’s three-curve barometer and the statistical indicators that evolved from it
transformed the business report from a tool of individual economic evaluation to a “scientific” document suitable for
regulating the entire market. By the end of the 1920s, central banks and governments relied on such measures
for their interventions in national economic policy. This was consistent with the growth at the same time of large
organizations or divisions devoted to data collection and analysis in order to forecast economic trends and foretell,
if not forestall, coming economic crises. Great Britain, France, Russia, Germany, and other European nations
based their key economic indicators on the three-curve barometer. Sweden and Italy proceeded with more
caution; the League of Nations implemented a committee on business cycle analysis in 1926.
Critics challenged the reliability of cyclic barometers. The Italian statistician and demographer Corrado Gini
(originator of the Gini coefficient) noted that they carried the potential for self-fulfilling-prophecy, with
businesspeople who relied on the various indicators exaggerating and destabilizing the economic cycle. The
Austrian economist Oskar Morgenstern pointed out that the Harvard barometer was not based on true probability
—impossible given the nature of the data—and that therefore it should not be used in economic decision-making.
The debate culminated in an inconclusive discussion by the International Statistical Institute in 1929. That October,
the Harvard Committee first failed to predict the stock market crash and then failed to explain it. The barometer
remained flat, indicating no downturn, let alone a depression. Those who depended on the barometers reacted to
the initial stock downturn in a manner consistent with their belief that it was only a minor blip before the market
stabilized. Nevertheless, the three-curve barometer retained its hold on the American business community during
the early 1930s, leading to a number of erroneous decisions. The ongoing calamity of the Great Depression finally
rendered the Harvard indicator and others moot, particularly as they continued to forecast rapid recovery.
Publication of the three-curve barometer ceased in 1935.
John Barnhill

 
See also:  Asset-Price Bubble;  Credit Cycle;  Great Depression (1929-1933);  Stock Market
Crash (1929). 
Further Reading
Glasner, David, and Thomas F. Cooley. Business Cycles and Depressions. New York: Taylor and Francis, 1997. 
Graf, Hans Georg. Economic Forecasting for Management: Possibilities and Limitations. Westport, CT: Quorum, 2002. 
Schumpeter, Joseph A. Ten Great Economists. New York: Routledge, 1997. 
Thrift Supervision, Office of
 
Established amid the savings and loan crisis of the late 1980s and early 1990s, the Office of Thrift Supervision
(OTS) is the primary federal agency charged with overseeing and regulating nationally chartered savings
associations (thrifts). Criticized for its lack of supervision of the mortgage practices of savings and loans (S&Ls)
during the housing boom of the early and middle 2000s, the OTS has been slated for termination under President
Barack Obama’s proposed reform package for the regulatory system overseeing the nation’s financial system.
The origins of the OTS go back to the Great Depression, when a collapse in the housing market sent many thrifts
into bankruptcy. Among the reforms enacted under President Franklin Roosevelt’s New Deal was the Federal
Home Loan Bank Board, which issued federal charters for thrifts and created a home ownership–promoting
regulatory system for the savings and loan industry. Also created at the time was the Federal Savings and Loan
Insurance Corporation, a separate agency that provided government guarantees to protect deposits at S&Ls.
The system worked well in the first decades following the end of World War II. While S&Ls were tightly regulated,
with high deposit-to-loan ratio requirements, mortgages became increasingly affordable and homeownership
expanded dramatically. But when interest rates soared in the 1970s, the restrictions on the interest rates that
savings and loans could offer on deposits led to capital flight to banks and into investment options. As a wave of
insolvencies threatened the S&L industry, regulations were eased, allowing S&Ls to make more loans on higher-
risk investments. By the 1980s, this deregulation led to a crisis, as borrowers proved unable to repay their loans
and a wave of bankruptcies hit the S&L industry, ultimately causing Congress to pass a bailout bill that, as of
1999, was estimated by the Federal Deposit Insurance Corporation (FDIC) to have cost taxpayers about $124
billion.
In 1989, Congress passed the Financial Institutions Reform, Recovery and Enforcement Act, a new set of
regulatory reforms that moved the S&L deposit insurance to the FDIC (which traditionally provided such insurance
to banks) and created the OTS to charter, oversee, and regulate the thrift industry. Unique among the federal
financial institution regulatory agencies, the OTS oversees both S&Ls and financial holding companies—that is,
companies, including nonfinancial businesses such as General Electric, that own significant stock in financial
institutions.
This created a problem. With the financial industry overseen by several agencies—each with its own history and
reputation for tight or lax regulation—financial institutions and holding companies were often able to shop around
for the regulatory agency that they deemed most likely to allow them to conduct business as they saw fit. For

many companies, the OTS fit the bill.
During the housing boom of the early and middle 2000s, the OTS, according to many financial industry experts,
took a more lax approach to regulation, allowing financial institutions to maintain ever-lower liquid asset-to-loan
ratios. At the same time, the OTS encouraged the development of a variety of higher-risk mortgage options,
including adjustable rate mortgages (ARMs) that offered low upfront interest-only payments to borrowers, followed
by an adjustment to higher principal-and-interest payments. Potentially, these higher payments could send
borrowers into foreclosure. But ever-rising home prices—and ever-rising home equity—allowed borrowers to
refinance before the adjustment to a higher monthly payment kicked in.
Still, other federal regulators, including the Office of the Comptroller of the Currency (OCC), issued warnings as
early as the mid-2000s that ARMs presented great risk to borrowers, and exposure to such ARMs presented a
great risk to the financial institutions that offered excessive amounts of them. However, say experts, the OTS
refused to issue new regulations or even warnings to the financial institutions it oversaw.
As the housing bubble burst in 2007 and 2008, many of these financial institutions were hit by a wave of
foreclosures and became saddled with bad loans. In 2008, several major thrifts and holding companies overseen
by the OTS became insolvent, including the California-based thrift IndyMac Bank and the Washington state–based
holding company Washington Mutual. The latter represented the largest failure of a financial institution in American
history.
These failures led to renewed scrutiny of the OTS’s operations by Congress. But the legislators’ concerns went
beyond the OTS’s history of lax regulation to a questioning of the whole federal financial institution regulatory
structure. Many critics argued that overlapping agencies made the system too unwieldy and too prone to
manipulation and evasion by financial institutions seeking ways to avoid oversight and regulation.
The incoming Obama administration made reforming and streamlining the federal financial regulatory system a top
priority. But while many advocates of reform have applauded the administration’s call to fold the OTS into the
Office of the Comptroller of the Currency (OCC), they say the plans do not go far enough since that is the only
one of the several federal regulatory agencies involved in financial industry regulation and oversight slated for
termination.
James Ciment
 
See also:  Regulation, Financial;  Savings and Loan Crises (1980s-1990s). 
Further Reading
Immergluck, Daniel. Foreclosed: High-Risk Lending, Deregulation, and the Undermining of America’s Mortgage
Market. Ithaca, NY: Cornell University Press, 2009. 
Office of Thrift Supervision:  www.ots.treas.gov
White, Lawrence J. The S&L Debacle: Public Policy Lessons for Bank and Thrift Regulation. New York: Oxford University
Press, 1991. 
Tinbergen, Jan (1903–1994)

 
Dutch economist Jan Tinbergen was a recipient (with Ragnar Frisch of Norway) of the first Nobel Prize in
Economic Sciences in 1969 for “having developed and applied dynamic models for the analysis of economic
processes.” Tinbergen was a founder of the field of econometrics, which he applied to the study of the dynamics
of business cycles. (His brother, Nikolaas Tinbergen, a pioneer in the field of ethology, was a co-recipient of the
1973 Nobel Prize for Physiology or Medicine.)
Jan Tinbergen was born on April 12, 1903, in The Hague, Netherlands, to Dirk Cornelis Tinbergen and Jeannette
Van Eek. He studied mathematics and physics at the University of Leiden, but redirected his studies and earned a
doctorate in economics in 1929. Like many of his generation of economists, Tinbergen worked in both the
academic and public-service sectors. While teaching at the Netherlands School of Economics from 1933 to 1973,
he also served as a consultant to the League of Nations (1936–1938) and as director of the Netherlands Central
Planning Bureau (1945–1955).
Tinbergen’s particular area of interest was the movement and mechanics of business cycles. He began
constructing economic models early in his career. Working concurrently with Frisch, he developed the first
econometric model of a national economy in the decade before World War II—these efforts mark the beginning of
econometrics as a practical and organized field of study. Together with Frisch, Irving Fisher, and thirteen others,
Tinbergen founded the Econometric Society in Cleveland, Ohio, in 1930. It was to the development of the science
of econometrics, particularly in the study and analysis of business cycles, that Tinbergen made what may be
considered his greatest contributions to economics.
Also in 1930, Tinbergen used econometrics to develop a model known as the “cobweb theory,” which showed
how past and present business-cycle behavior is closely interlinked and can be used to predict future cycles. He
built on it the following year with a study of the shipbuilding industry and its cycles. By 1937, Tinbergen was
developing a model of the Dutch economy and published An Econometric Approach to Business Cycle Problems.
As the decade progressed, Tinbergen developed models of increasing size and complexity. As result of his work
for the League of Nations, he created an econometric model of the U.S. economy, published in two volumes (both
in 1939), A Method and Its Application to Investment Activity and Business Cycles in the United States of America,
1919–1932. In A Method, he used econometric studies to provide a realistic basis for testing the various theories
that attempted to explain the causes and characteristics of the stages of the business cycle. In Business Cycles,
he showed the results of his large-scale model, which used forty-eight different equations. The work was a
landmark both in methodology and in result.
Following World War II, Tinbergen held numerous academic and private-sector positions. In 1965, he was
appointed chair of the United Nations Committee for Development Planning. His knowledge about and effective
use of economics to shape public policy, especially in the 1960s and 1970s, drew comparisons to John Maynard
Keynes. Increasingly, Tinbergen turned his attention to global economic concerns, including the future availability
of natural resources, world security, and Third World development. He died on June 9, 1994, in Amsterdam.
Robert N. Stacy
 
See also:  Fisher, Irving;  Frisch, Ragnar. 
Further Reading
Hansen, Bent.  “Jan Tinbergen: An Appraisal of His Contributions to Economics.” Swedish Journal of Economics 71:4
(December 1969): 325–326. 
Magnus, Jan R., and Mary S. Morgan.  “The ET Interview: Professor J. Tinbergen.” Econometric Theory 3:1 (February
1987): 117–142. 

Tinbergen, Jan. Econometrics. New York: Routledge, 2005. 
Tinbergen, Jan. Entering the Third Millennium: Some Suggestions. Rotterdam: Fijan, 1991. 
Tinbergen, Jan. World Security and Equity. Brookfield, VT: Gower, 1990. 
 
Tobin, James (1918–2002)
 
An admired Yale University economist and Nobel laureate, James Tobin combined a distinguished academic
career with public service. His work on the theoretical formulation of investment behavior provided important
insights into financial markets and helped earn him the Nobel Prize in Economics in 1981. He was specifically
cited by the nominating committee for “his analysis of financial markets and their relations to expenditure
decisions, employment, production, and prices.”

Nobelist James Tobin argued that economic growth is not ensured purely by market forces and that the economy
cannot be managed solely by adjusting the money supply. The government should take steps to control inflation,
unemployment, and other factors. (Hulton Archive/Getty Images)
Tobin was born on March 5, 1918, in Champaign, Illinois. His father, Michael, was a journalist and his mother,
Margaret, a social worker. Coming of age during the Great Depression, and later his introduction to the theories of
John Maynard Keynes, had a profound influence on and informed much of Tobin’s thinking throughout his career.
He attended Harvard University, receiving a bachelor’s degree in 1939, and worked as an economist with the
Office of Price Administration in 1941–1942. After serving in the U.S. Navy during World War II, he returned to
Harvard and earned a doctorate in economics in 1947. Joining the faculty of Yale University in 1950, he was
named Sterling Professor of Economics in 1957 and served multiple terms as director of the Cowles Foundation
for Research at Yale (1955–1961, 1964–1965). During the John F. Kennedy administration, he served on the
Council of Economic Advisers (1961–1962), in addition to several terms on the Board of Governors of the Federal
Reserve System.
Tobin’s major work involved analysis of financial markets, determining optimal investment strategies and predicting
consumer-purchasing patterns. In developing his ideas, Tobin created three major components of financial
management theory: portfolio management theory, probit analysis, and the Tobin q value.
Tobin’s portfolio management theory was critical to the foundations of modern portfolio theory. In 1958, Tobin
wrote a seminal article, “Liquidity Preference as Behaviour Towards Risk,” in which he proved that in a world with
one safe asset and a large number of risky assets, portfolio choice by any risk-averse individual consists of a
choice between a risk-free safe asset and the same portfolio of risky assets. The degree of risk aversion is the

only determinant of the shares in the total portfolio accounted for by the safe asset and by the common portfolio
of risky assets. When asked to explain his research, Tobin commented in his Nobel Prize acceptance speech that
his work could be summarized by the phrase, “Don’t put all your eggs in one basket.”
Tobin’s analysis of household consumption patterns, titled “Liquidity Preference as Behaviour Towards Risk,” was
published in the Review of Economic Studies in 1958. His work used data on individual household income and
expenditures that had been collected since the late 1940s but never been put to any meaningful use. Using
sophisticated statistical models, Tobin adopted so-called probit analysis to determine the probability of an event
occurring. This enabled analysts to estimate not only the likelihood of purchases but also the likely amount of
related expenditures.
One of Tobin’s innovations, called Tobin’s q, can be used to decide whether a firm should invest in an additional
unit of capital. Tobin’s q is the ratio of the market value of the capital relative to the replacement cost. If q is high,
then the firm should invest in more capital because the capital’s market value would be greater than its
replacement costs. If q is less than 1, the market value of the additional capital is less than its replacement cost,
and hence, the firm should not invest. The beauty of Tobin’s q is that it provides a simple solution to a complex
problem: increase investment only if q is greater than 1.
In addition to his Nobel Prize–winning work, Tobin spent much of his life refining the original Keynesian model,
including the necessity of government intervention in economic growth. In the mid-1960s, in a lecture titled
“Economic Growth as an Objective of Government Policy,” Tobin stated that government policies to promote
economic growth represent a collective decision that concerns future generations. Thus, government should take
measures to ensure growth because it cannot be done exclusively by private markets.
Tobin studied and wrote about a wide range of topics, including household finances and behavior,
macroeconomics (which he maintained had a strong orientation toward public policy), and financial markets. He
was the model for Tobit, a minor character in Herman Wouk’s novel The Caine Mutiny. Tobin died on March 11,
2002, in New Haven, Connecticut.
Robert N. Stacy
 
See also:  Consumption;  Keynesian Business Model. 
Further Reading
Baumol, William J. Growth, Industrial Organization and Economic Generalities. Cheltenham, UK: Edward Elgar, 2003. 
Shiller, Robert J.  “The ET Interview: Professor James Tobin.” Econometric Theory 15:6 (December 1999): 867–900. 
Tobin, James. Essays in Economics: Macroeconomics. Cambridge, MA: MIT Press, 1987. 
Tobin, James. Full Employment and Growth: Further Keynesian Essays on Policy. Cheltenham, UK: Edward Elgar, 1996. 
Tobin, James.  “Liquidity Preference as Behaviour Towards Risk.” Review of Economic Studies 25:2 (February 1958): 65–
86. 
Tobin, James. Measuring Prices in a Dynamic Economy: Re-Examining the CPI. New York: Study Group on the Consumer
Price Index, Conference Board, 1999. 
Tobin, James. Money, Credit, and Capital. Boston: Irwin/McGraw-Hill, 1998. 

 
“Too Big to Fail”
 
“Too big to fail” is a term that was employed by financial pundits and the press during the financial crisis and the
subsequent federal bailout of major financial institutions in 2008–2009. It refers to private financial institutions that
are so large, so diverse, and so interconnected with other businesses that their failure could create panic in the
global financial markets.
 History
While the phrase “too big to fail” gained notoriety in recent years, the concept behind it is nothing new. Many of
the periodic financial panics of the late nineteenth and early twentieth centuries were triggered by the failure of
major banks and financial institutions, often as a result of financial speculation, which caused a cascade of
bankruptcies in the financial markets as credit flows froze. In 1907, financier J.P. Morgan orchestrated a $100
million bailout by a consortium of major banks of financial institutions that had been caught up in speculation in the
copper market. Though successful, the effort convinced many in the financial community and government that only
Washington would have the resources to act as a lender of last resort in the event of an even greater crisis in the
future. The result of this thinking was the creation of the Federal Reserve System (Fed) in 1913.
But the Fed proved unable to prevent the avalanche of bank failures in the early 1930s that threatened the U.S.
financial system. In response to that unprecedented crisis, the Franklin D. Roosevelt administration created the
Federal Deposit Insurance Corporation (FDIC) to insure depositors against losses up to a certain amount, thereby
assuring them that their savings were secure and thus preventing depositor runs that could lead to bank failures.
Other reforms implemented in the 1930s included new regulations on financial institutions to prevent such failures
in the first place, including a law that banned commercial banks from engaging in more speculative investment
banking activities.
In the event of a bank failure, the FDIC had three options: close and liquidate the institution’s assets, purchase
the institution and then sell off its assets to another institution, or provide loans to shore up the institution (this last
option was enabled by the Federal Deposit Insurance Act of 1950). The system appeared to work until the early
1980s, when it finally was tested by the potential failure of the Continental Illinois National Bank in 1984. Exposed
to liabilities created by collapsing oil prices, America’s seventh-largest bank threatened to create a major financial
panic if it proved unable to meet its obligations. Normally, federal regulators would have orchestrated its purchase
by other institutions, but with the nation in its deepest recession since the 1930s, this proved impossible, forcing
the Fed to declare that it would meet any and all of Continental Illinois’s liquidity needs.
Despite the Continental Illinois crisis, the 1980s and 1990s saw massive deregulation in the financial industry, a
process backed by both Republican and Democratic administrations. Financial industry experts argued that
excessive regulations limited the profit-making potential of financial institutions and prevented them from creating
new and innovative products that could provide more readily available credit and make financial markets more
efficient. Moreover, it was argued that bigger institutions could operate more effectively in increasingly global
financial markets.

The result of this deregulatory trend was a wave of consolidation in the financial industry, the rise of multistate
banking (banks operating across state lines), and the end of the Depression-era firewall between commercial and
investment banking activity. The last move was pushed in 1999 by Citigroup, America’s largest financial institution,
which sought to operate brokerage, insurance, investment banking, and commercial banking businesses under
one roof.
The formation of Citigroup and the creation and expansion of other financial giants led to a perception in the
financial community that some banks had become “too big to fail.” In other words, if any of these large national
banks faced a liquidity crisis or potential insolvency, this would create both unsustainable liquidity demands on
other financial institutions as well as a general perception that no institution was safe. Moreover, a failure of such
a large institution would cause credit markets to freeze up, making it difficult for financial institutions and
businesses to secure the short-term credit they needed to run their daily operations, thereby triggering a financial
crisis and recession. With financial markets increasingly integrated across national borders, such a crisis could
become global in scope.
During the financial boom years of the late 1990s and early 2000s, few people seemed concerned about such a
scenario. Indeed, larger financial institutions benefited from the “too big to fail” notion, as large commercial
depositors—unprotected by the FDIC because of the sheer size of their accounts—moved their assets to those
institutions, believing that if a crisis came, the federal government would step in to protect the largest banks. At
the same time, the managers of the largest banks understood that these perceptions permitted the same
managers to offer lower interest rates, thereby increasing the profitability of the biggest banks and allowing them
to grow bigger still. In addition, there was the matter of “moral hazard.” Simply put, managers of the largest banks,
knowing that their institutions would not be allowed to fail, had an incentive to engage in risky—and potentially
more profitable—investment activities. Moral hazard, then, created distortions in the financial marketplace
competition.
 Financial Crisis of 2008–2009
The financial crisis triggered by the collapse of the subprime mortgage market in 2008 set in motion the “too big to
fail” scenario. Late in that year, the George W. Bush administration and the Fed convinced Congress to authorize
a $700 billion federal bailout plan, known officially as the Troubled Asset Relief Program (TARP). On October 3,
2008, Congress passed and the president signed the Emergency Economic Stabilization Act. The act authorized
the U.S. Department of the Treasury to purchase up to $700 billion of “toxic” mortgage-backed and other securities
in order to provide funds to intermediaries so that they could resume lending and prevent further deterioration of
the economy. On October 14, 2008, the TARP plan was revised by the Treasury Department, so that the initial
$250 billion would be used to purchase preferred stock in American banks under the Capital Purchase Program.
The Treasury decided that it would be better to inject capital directly into banks rather than purchase the toxic
assets. By the end of October, nine of the largest American banks had applied for and received $125 billion in
bailout funds.
The thinking among policy makers was that by shoring up the capital of banks, those institutions would begin
lending again, and the financial system would recover. In November 2008, the Treasury authorized the use of $40
billion in TARP funds to purchase preferred stock in the insolvent insurance giant AIG (American International
Group), which had been taken over by the federal government two months earlier.
In the same month, three large insurance companies announced plans to purchase depository institutions in order
to give them access to TARP funds. Finally, Treasury Secretary Henry Paulson officially announced that the TARP
funds would not be used at that time to purchase “toxic” assets, but rather would be used to support the financial
system in other ways. In December, the Treasury authorized the use of TARP funds to bail out General Motors
and Chrysler.
In December 2008, TARP money was used to buy preferred stock in large and small banks. By early the following
January, about $305 billion of the bailout funds had been spent, with approximately $200 billion used to buy

preferred stock in banks, $40 billion used to bail out AIG, an additional $45 billion invested in Citigroup and Bank
of America (both of which had participated in the Capital Purchase Program), and $20 billion invested with the
automakers and their financing subsidiaries. Some analysts charged that the bailout funds were not used for their
original purposes and merely supported large institutions that had created the problems in the first place. The
argument made at the time was that the failure of any of these institutions would create such a stress on the
financial markets that they simply would shut down, triggering an economic downturn rivaling that of the 1930s.
In addition, the government orchestrated the liquidation and/or repurchase of such major financial failures as
IndyMac Bank of California and Washington Mutual of Washington State. The bailout came after the federal
government refused to rescue Lehman Brothers, a large investment bank, in September 2008, a decision that
many financial experts say contributed to the financial crisis.
Ironically, according to some financial experts, the crisis and the bailout contributed to the “too big to fail”
phenomenon by reinforcing the gains that large institutions achieved through moral hazard. In other words, large
commercial depositors and managers of large financial institutions no longer had to act on faith that the
government would step in, knowing that they would be protected and thereby giving these institutions a leg up on
their smaller rivals. Moreover, the government, through the bailout and other actions, encouraged larger
institutions to buy out smaller and weaker competitors, increasing the size of the former.
In a 2009 news conference on “lessons learned” a year after federal intervention in the U.S. financial markets,
House Republicans call for an end to the “Too Big to Fail” policy under which giant institutions are rescued to
avoid panic or other dire consequences. (Scott J. Ferrell/Congressional Quarterly/Getty Images)
While some economists, particularly on the left side of the political spectrum, have argued that these
developments necessitate the breakup of the biggest financial institutions, the Barack Obama administration began

pushing another option—granting new regulatory powers to the Fed to oversee these “too big to fail” institutions
and potentially to intervene should they engage in the kinds of risky behavior that might lead to their failure. In
other words, President Obama and his advisers argued that the federal government needs to have a kind of
“systemic risk agency” that would prevent a repeat of the financial crisis that unfolded in 2008–2009 and nearly
plunged the nation and the world into another Great Depression.
James Ciment
 
See also:  Moral Hazard;  Systemic Financial Crises;  Troubled Asset Relief Program (2008-). 
Further Reading
Sorkin, Andrew Ross. Too Big to Fail: The Inside Story of How Wall Street and Washington Fought to Save the Financial
System from Crisis—and Themselves. New York: Viking, 2009. 
Stern, Gary F., and Ron J. Feldman. Too Big to Fail: The Hazards of Bank Bailouts. Washington, DC: Brookings
Institution, 2009. 
Transition Economies
 
A transition economy is an emerging market economy that is in the process of changing from a centrally planned
to a free-market economy. Economists usually use the term to describe economies that were once part of the
communist bloc, such as those in Eastern Europe and the former Soviet Union. More loosely, the term is applied
to any emerging market economy that was once dominated by state planning and direction, such as India and
Iraq.
The transition process in such countries is about more than mere economic change; it is about the society as a
whole. According to economists Oleh Havrylyshyn, Thomas Wolf, and Jan Svejnar, the transition encompasses the
following changes: limiting the central bank to monetary policy and allowing new private banks to assume
commercial banking operations; liberalization of economic activities, including removing barriers to the creation of
new firms; ending most government price setting, though government price controls may continue for certain
crucial goods and services, such as housing, energy, and medicine; cutting supports for state-owned enterprises
so that resources can be allocated by the market; achieving macroeconomic stabilization through market-oriented
means, as opposed to direct policy decisions; increasing efficiency and competitiveness through the privatization
of state-owned firms; implementing tight budgetary constraints to avoid large public debts; securing property rights;
and applying transparent, market-entry regulations for both domestic and foreign-owned firms.
 Differences Among Transition Economies
Various authorities and institutions define transition economies somewhat differently and so list different countries
as transition economies. The United Nations Statistics Division, for example, identifies eighteen countries from
Europe and Asia as transition economies: Albania, Armenia, Azerbaijan, Belarus, Bosnia and Herzegovina,
Croatia, Georgia, Kazakhstan, Kyrgyzstan, the Former Yugoslav Republic (FYR) of Macedonia, Moldova,
Montenegro, Russia, Serbia, Tajikistan, Turkmenistan, Ukraine, and Uzbekistan. According to the European Bank
for Reconstruction and Development (EBRD), a total of thirty countries are classified as transitional: Albania,

Armenia, Azerbaijan, Belarus, Bosnia and Herzegovina, Bulgaria, Croatia, Czech Republic, Estonia, Georgia,
Hungary, Kazakhstan, Kyrgyzstan, Latvia, Lithuania, FYR of Macedonia, Moldova, Mongolia, Montenegro, Poland,
Romania, Russia, Serbia, Slovak Republic, Slovenia, Tajikistan, Turkey, Turkmenistan, Ukraine, and Uzbekistan.
Other countries sometimes classified as transitional include East Germany, Iraq, and China.
Transition economies are historically, politically, geographically, demographically, and economically diverse
countries. Their populations range from 678,000 (Montenegro) to 141 million (Russia), and population densities
(people per square kilometer) range from 2 (Mongolia) to 134 (Czech Republic). Some transition countries are oil-
rich (Azerbaijan and Turkmenistan), while others have no oil reserves at all (Estonia). Some have a relatively
warm climate (Turkey and Slovenia), others a cold climate (Lithuania), and still others have highly diverse climates
and terrains, including flatlands, steppes, taigas, and deserts (Russia and Kazakhstan). Some countries are
located on the ocean or sea (Russia, Croatia, and Latvia), while others are landlocked (Hungary and Mongolia).
Such demographic, geographical, and geological differences have had a direct impact on economic development,
systems, and structures, among which are the absence of large multinational corporations (which are not easy to
establish in a very small country), modes of transport, and the role of agriculture and mining in the economy.
In addition to the above-mentioned differences, the countries also have major historical differences and
institutional legacies. A total of fifteen transition economies today belonged to the Soviet Union until 1991:
Armenia, Azerbaijan, Belarus, Estonia, Georgia, Kazakhstan, Kyrgyzstan, Latvia, Lithuania, Moldova, Russia,
Tajikistan, Turkmenistan, Ukraine, and Uzbekistan. Some of these were generally willing to maintain strong
economic and political ties with Moscow, while others (such as Yugoslavia and Hungary) were more autonomous.
So, when the Soviet Union dissolved and the Council for Mutual Economic Assistance—the trading bloc within the
Soviet-dominated communist world—was abolished, some countries were affected more strongly than others.
Firms in the closer satellite countries, generally state-run, were more apt to lose their markets and shut down.
Those that survived and the new ones that were created had little knowledge of Western production systems,
lacked contacts with outside distributors, and had few sources of financing for new technology, developing new
products, and entering new markets. Thus, most of these economies experienced large declines in total output.
Others were less affected because of their weaker connections to—and reliance on—the Soviet Union and their
strong ties to capitalist economies of the West.
 Economic Progress
The EBRD tracks the economic progress of transition countries according to eight indicators: (1) large-scale and
small-scale privatization—the percentage of privately owned firms; (2) governance and enterprise restructuring—
the effectiveness of corporate control, tightness of credit and subsidy policy, and enforcement of bankruptcy
legislation; (3) price liberalization—the share of price control outside housing, transport, and national monopolies;
(4) trade and foreign exchange—the lifting of export and import restrictions and the existence of a fully transparent
foreign exchange regime; (5) competition policy—effective enforcement by special institutions; (6) banking reform
and interest rate liberalization—the harmonization of banking laws and regulations with international standards, the
provision of competitive banking services, the removal of interest rate ceilings; (7) securities markets and nonbank
financial institutions—meeting international standards in securities regulation and having well-functioning nonbank
financial institutions; and (8) infrastructure reform—the decentralization, commercialization, and effective regulation
of electric power, railways, roads, telecommunications, water, and wastewater. According to these indicators,
Estonia, Hungary, and Poland have transitioned the quickest while Belarus, Turkmenistan, and Uzbekistan have
lagged significantly behind.
Moreover, the EBRD has noted a significant connection between the pace of transition and degree of economic
growth, with quicker transitions resulting in faster growth. That is, those countries that have completed basic
structural reforms have developed faster than those that have not. At the same time, there is a direct correlation
between per capita gross domestic product (GDP) and transition pace, though not a direct cause-and-effect
relationship. That is, there is a correlation between higher-income countries and faster paces of transition, but
which triggers which is open to debate.

To that end, it is important to note that not all of these countries started on the transition path from the same level
of development. Countries that were lagging even before the transition began have generally had more uneven
growth than more advanced countries. In addition, which transition measures countries have undertaken has
determined their pace of economic growth. Economists have found that countries that encouraged the entry and
growth of new firms—as well as encouraging them to innovate and grow faster—did better than countries that did
not. Moreover, countries that imposed budgetary constraints on former state-owned enterprises—including denying
them access to credit—created economic chaos.
Moreover, say economists at the EBRD and elsewhere, the speed of transition depended on the form of
privatization; countries that succeeded in finding successful strategic investors, often from abroad, have developed
faster than those that privatized companies to ineffective owners (often, employees or former managers) or
continued with state ownership. Also important was the development of new commercial/legal systems and the
institutions necessary for the effective functioning of a free-market economy, including the defining and enforcing
of private property rights and the means to transfer property. In some transition economies, policy makers were
disappointed to find that the free market could not simply take care of itself but needed effective laws and
regulatory institutions. In many of these countries, insiders opposed such changes because they were profiting off
the privatization and transition processes and wanted a free hand to make their illicit gains. In addition, if existing
firms were given special benefits, those who were making money off of them tended to oppose ending such
market-distorting subsidies. Finally, those countries that invested more heavily in the social welfare, education, and
health systems tended to fare better economically than those that did not.
 Impact of the Financial Crisis and Recession Since 2007
The financial crisis and recession of 2007–2009 hit many of the transition economies hard. Ironically, those
countries, such as Estonia, that had integrated themselves more closely into the global financial markets (a factor
that benefited them during the boom years of their early transition period) were the hardest hit, primarily because
they had become heavily dependent on foreign investment, which often produced unsustainable growth rates and
speculative bubbles in the local real-estate market. When access to that capital disappeared, these economies
went into a tailspin. Estonia, for example, saw its annual growth rate plummet from 7.2 percent in 2007 to a
depression-level –13.7 percent in 2009. Moreover, as capital fled these countries, private and public debt levels
grew, making it more difficult for firms and governments to borrow from abroad to address the social problems
exacerbated by the economic downturn.
Most economists agree that for transition economies to emerge from the crisis in better shape than they entered
it, they must do the following: improve the governance and structure of their financial sectors and provide them
with more liquidity; continue to invest in small and medium-sized firms; fix fiscal imbalances so that future
fluctuations in capital flows do not have such a heavy impact on government finances; strengthen competition by
removing barriers that obstruct trade and the entry of new firms into the marketplace; support innovation; improve
the quality of education; and help firms both diversify and move up the supply chain so that they can produce
higher value-added products. At the same time, the private sectors in these economies, say development experts,
need to improve their fundamentals as well by cutting costs, developing new and better products, modernizing
their marketing strategies, and even restructuring.
Tiia Vissak and James Ciment
 
See also:  China;  Eastern Europe;  Emerging Markets;  India;  Latin America;  Russia and the
Soviet Union;  Southeast Asia. 
Further Reading
Ichimura, Shinichi, Tsuneaki Sato, and William James. Transition from Socialist to Market Economies: Comparison of

European and Asian Experiences. New York: Palgrave Macmillan, 2009. 
McGee, Robert W., ed. Corporate Governance in Transition Economies. New York: Springer, 2008. 
Svejnar, Jan.  “Transition Economies: Performance and Challenges.” Journal of Economic Perspectives 16: 1 (2002): 3–28. 
Transition Report 2008: Growth in Transition. Norwich, UK: EBRD, 2009. 
Transition: The First Ten Years. Analysis and Lessons for Eastern Europe and the Former Soviet Union. Washington,
DC: The World Bank, 2002. 
 
Treasury Bills
 
Treasury securities include Treasury bills, Treasury notes, Treasury bonds, and Treasury Inflation-Protected
Securities (TIPS); they are issued (sold) by the U.S. Department of the Treasury to investors who are willing to
lend money to the federal government. These investors can include other government agencies, the Federal
Reserve (Fed), professional investment firms, banks, foreign governments, and individuals. Among these
securities, Treasury bills have the shortest term, with a maximum maturity of one year from the time of issue.
The interest that investors usually earn from bonds comes from their coupon payments, which are generally paid
semiannually. For example, if a bond promises a 5 percent coupon rate and the face (par) value of the bond is
$1,000, then the annual coupon payment is 5 percent x $1,000 = $50 (coupon rate times face value), and the
investor receives $25 every six months. Treasury bills, commonly referred to as “T-bills,” are zero-coupon bonds,
which means they do not make coupon payments and the investor’s return is simply the difference between the
purchase price and the face (par) value received when the bill matures. Because of this, T-bills are said to be
priced and sold at “a discount to par value,” or an amount less than their face value, and are quoted on a discount
rate basis. Dealers who deal in Treasury securities buy and sell T-bills from investors. An abbreviated T-bill listing
from a typical day, February 27, 2009, appears below:
 Treasury Bill Prices, February 27, 2009 
Maturity
Days to Maturity
Bid
Asked
Chq
Asked Yield
2009 Mar 05
6
0.120
0.095
–0.010
0.096
2009 Mar 12
13
0.130
0.125
Unch.
0.127
2009 Mar 19
20
0.170
0.140
–0.005
0.142
2009 Apr 02
34
0.180
0.160
–0.025
0.162
2009 Apr 09
41
0.190
0.160
–0.020
0.162
2009 May 28
90
0.260
0.250
–0.010
0.254

2009 Jun 04
97
0.285
0.275
Unch.
0.279
2009 Jun 11
104
0.300
0.290
Unch.
0.294
2009 Jun 18
111
0.300
0.290
–0.020
0.294
2009 Jun 25
118
0.320
0.305
–0.010
0.310
2009 Aug 20
174
0.450
0.415
+0.005
0.422
2009 Aug 27
181
0.450
0.440
–0.015
0.447
2009 Sep 03
188
0.485
0.475
Unch.
0.483
2009 Dec 17
293
0.660
0.653
–0.015
0.664
2010 Jan 14
321
0.653
0.643
–0.007
0.654
2010 Feb 11
349
0.690
0.685
–0.013
0.698
Source: Thomson Reuters.
The longest term T-bill outstanding on this day had 349 days to maturity on February 11, 2010. “Bid” and “ask”
quotes represent the prices an investor pays to purchase the security (ask) from a securities dealer or the amount
an investor receives when selling the security (bid) to a broker. In other words, the bid price is the price at which
a dealer will buy T-bills from an investor, and the ask price is the price at which a securities dealer will sell the
security to an investor. Bids and asks are usually expressed as prices or percentages of par value, depending on
the security being traded; for T-bills, these numbers are discount interest rates. The 0.685 percent “asked”
discount rate for the 349-day T-bill corresponds to a price of $993.359. An investor who paid that amount for one
of the T-bills receives $1,000 upon maturity on February 11, 2010, which equates to a return over that time period
of ($1,000 – $993.359)/$ 993.359 = 0.6685 percent. The asked yield, 0.698 percent, comes from annualizing the
0.6685 percent. Note that for simplicity, in the example a $1,000 T-bill is used. In reality, T-bills are sold in $100
increments with a $100 minimum.
 How Treasury Bills Are Sold and Purchased
Investors purchase T-bills by participating in auctions conducted by the Department of the Treasury. Auctions of 4-
week, 13-week, and 26-week bills are held every week; 52-week bills are auctioned every 4 weeks. Auction
announcements are published in newspapers, issued via press releases or e-mails, and can be found at the
Treasury Direct Web site. An auction announcement reveals the important dates and deadlines related to the
auction and how much money the government is seeking to borrow. Investors can participate in these auctions
either as competitive bidders or noncompetitive bidders. Professional investors tend to be the competitive bidders,
and they specify the discount rate they would like to receive along with the dollar amount they are interested in
lending. Individuals, who tend to be the noncompetitive bidders, submit only the dollar amount they are willing to
lend (subject to the maximum of $5 million).
Once the auction’s “winning” discount rate is determined, noncompetitive bidders receive the full amount of their
bid, while competitive bidders receive all, a portion, or none of their bid dollar amount, depending on how the
discount rate they submitted with their bids compares with the winning discount rate. For example, a competitive
bidder who submitted a discount rate higher than the winning discount rate may receive nothing because the
government wants to minimize borrowing costs and will allocate the T-bills in the auction to investors who bid at
lower discount rates first. The results of the auction are made public shortly after the auction closes at 1:00 p.m.
Eastern Time. Individuals can participate directly in Treasury auctions at the Treasury Department Web site.
 Impact of the 2007–2009 Recession and Financial Crisis
One of the tools available to the Fed in conducting monetary policy is its ability to target the fed funds rate,

defined as the overnight lending rate at which depository institutions borrow and lend reserves from each other to
meet their reserve requirements. The fed funds rate is a market determined interest rate. However, the Fed can
influence this rate by buying and selling Treasury securities to determine the amount of reserves in the banking
system. If the Fed purchases government securities from dealers, it increases the supply of reserves available to
the banking system. If the Fed sells government securities to government securities dealers, the supply of
reserves decreases. When the supply of reserves increases, the fed funds rate will fall, since with reserves being
more plentiful in the system, there is no need to borrow reserves and there is more to lend among depository
institutions. Likewise, when the supply of reserves decreases, this puts upward pressure on the fed funds rate
because more depository institutions want to borrow reserves and fewer want to lend them. When the Fed lowers
the interest target for the fed funds rate, other interest rates also go down because the Fed is pumping more
reserves into the banking system which will increase lending. As an overnight rate, the fed funds rate is short-
term, so T-bills are the Treasury securities that would be expected to most closely reflect changes in it since they
are the shortest term-to-maturity among treasuries.
The figure below plots the fed funds rate against the median/average discount rate from T-bill auctions for the
period from January 2000 to December 2008. The two lines mirror each other. The two periods that feature steep
declines in these rates correspond to the bear market of September 2000 to September 2002 and the financial
crisis that began in late 2007. The latter event, and the economic recession it provoked, created great uncertainty
in the financial markets. In such climates, investors seek out safe investments like Treasury securities. Investor
demand for these securities, like that for any good or service, tends to push prices upward, which corresponds to
the decline in T-bill rates depicted in the graph above (since bond prices and rates are inversely related).
Fed Funds Rate vs. T-Bill Auction Rates: 2000–2008 
The reaction of the U.S. federal government included fiscal policy measures such as TARP (Troubled Asset Relief
Program), TALF (Term Asset-Backed Securities Loan Facility), and a $787 billion stimulus package signed by
President Barack Obama in February 2009. Without significant tax increases, all of these measures require the
government to borrow more money to help pay for them. The additional borrowing was already observable in the
increase in T-bill auction dollar amounts from January 2000 to February 2009. The upward trend in borrowing
(from the sale of T-bills alone) accelerated at the end of 2008 and into 2009. In February 2009, investors loaned
$509 billion to the government via T-bill auctions. In June 2008, the fifty-two-week bill was reintroduced for
auction after not having been available since 2001.
John J. Neumann

 
See also:  Debt Instruments;  Federal Reserve System;  Interest Rates;  Monetary Policy; 
Monetary Stability;  Treasury, Department of the. 
Further Reading
 Burghardt, Galen D., Terrence M. Belton, Morton Lane, and John Papa. The Treasury Bond Basis: An In-Depth Analysis
for Hedgers, Speculators, and Arbitrageurs.  3rd ed. New York: McGraw-Hill, 2005. 
 Treasury Direct:  www.treasurydirect.gov
 United States Government Accountability Office (U.S. GAO).  Report to the Secretary of the Treasury: November 2008
Financial Audit. Bureau of the Public Debt’s Fiscal Years 2008 and 2007 Schedules of Federal Debt.  Washington, DC, 
November 2008. 
Treasury, Department of the
 
The Federal Reserve and the U.S. Department of the Treasury are without question the two government agencies
that have the greatest impact on monitoring, measuring, and affecting the course of U.S. business cycles. The
Treasury Department possesses a dizzying array of responsibilities and powers that directly impinge on U.S.
economic policy and expansion, both domestic and international. These vital responsibilities and powers have
emerged over the past two centuries. Most recently, the department has been charged with overseeing the 2008
Troubled Assets Relief Program (TARP), dispersing some $700 billion in taxpayer funds to financial institutions
saddled with nonperforming assets of questionable value, such as mortgage-backed securities.
 History and Main Functions
The Treasury Department was established in September 1789 by an act of Congress for managing and improving
government revenue (before that, such functions were carried out by other institutions). In the late eighteenth and
early nineteenth centuries, the department was rather small, consisting of the secretary of the treasury, a
comptroller, an auditor, a treasurer, a register, and an assistant to the secretary. Currently, the secretary of the
treasury manages more than 100,000 employees. On January 26, 2009, Timothy F. Geithner became the seventy-
fifth secretary of the treasury.
Over the years, the functions of the Treasury have been extended. Currently, it manages federal finances;
advises the president and others on economic and financial issues (including domestic and international financial,
monetary, economic, trade, and tax, or fiscal policy); collects taxes, duties, and monies paid to and due to the
United States; pays all the country’s bills; manages government accounts and the public debt (if necessary,
borrowing funds for running the federal government); supervises national banks and credit institutions; enforces
federal finance and tax laws, investigating and prosecuting tax evaders, counterfeiters, and forgers; and
implements economic sanctions against foreign threats to the United States. It is responsible for the production of
currency and coinage. Thus, this agency maintains very important systems of the United States and cooperates
with other federal agencies, international financial institutions, and foreign governments to advance global
economic development and to predict and prevent economic and financial crises. Finally, the Treasury is
responsible for ensuring the financial security of the United States, promoting its economic prosperity, and
encouraging sustainable economic growth.

 Operating Bureaus and Departmental Offices
The Department of the Treasury has twelve operating bureaus carrying out the department’s specific operations
(about 98 percent of the Treasury’s employees work in these bureaus) and nine departmental offices that
formulate the Treasury’s policy and manage it. The operating bureaus are as follows:
1. Alcohol and Tobacco Tax and Trade Bureau: regulates the production, use, and distribution of alcohol and
tobacco products and collects excise taxes for firearms and ammunition;
2. Bureau of Engraving and Printing: designs and manufactures U.S. official certificates and awards, including
currency and securities;
3. Bureau of the Public Debt: borrows the funds needed to operate the federal government and also issues and
services U.S. Treasury securities;
4. Community Development Financial Institution Fund: provides capital and financial services to distressed
communities;
5. Financial Crimes Enforcement Network: cooperates globally to fight against domestic and international
financial crimes and also analyzes domestic and worldwide trends and patterns;
6. Financial Management Service: maintains government accounts, receives and disburses all public monies,
and makes reports on government finances;
7. Department of the Treasury’s Office of Inspector General: provides objective reviews of the department’s
operations;
8. Treasury Inspector General for Tax Administration: is responsible for the administration of the internal
revenue laws and minimizing fraud and abuse;
9. Internal Revenue Service: determines, assesses, and collects U.S. internal revenue;
10. Office of the Comptroller of the Currency: regulates the U.S. banking system;
11. Office of Thrift Supervision: regulates federal-and state-chartered thrift institutions such as savings banks and
savings and loan associations;
12. United States Mint: designs and manufactures coins, commemorative medals, and other numismatic items;
distributes U.S. coins to the Federal Reserve banks; and protects U.S. gold and silver assets.
The departmental offices include the following:
1. Office of Domestic Finance: is responsible for developing policies and advising banks and other financial
institutions on federal debt financing, financial regulations, and capital markets;
2. Office of Economic Policy: reviews and analyzes current and prospective, domestic, and international
economic and financial developments and helps to determine appropriate economic policies;
3. Office of General Counsel: coordinates the activities of the Treasury Legal Division and offers advice to the
secretary and other departmental staff;
4. Treasury’s Office of International Affairs: formulates and executes U.S. international economic and financial
policy; for example, financial, trade and development programs;

5. Assistant Secretary for Management and Chief Financial Officer: manages the department; deals with its
budget, personnel, information technology, and offers administrative services to departmental offices;
6. Office of Public Affairs: is responsible for the department’s communications strategy;
7. Office of Tax Policy: develops and implements tax policies, programs, regulations, and treaties and analyzes
the consequences of tax policy decisions; for instance, for the president’s budget;
8. Office of Terrorism and Financial Intelligence: combats domestic and international terrorist financing, money
laundering, and other financial crimes;
9. Office of the U.S. Treasurer: offers advice on financial education, coinage, and currency of the United States.
 Strategic Plan for 2007–2012
According to its strategic plan for fiscal years 2007–2012, the U.S. Treasury Department has to concentrate on
four strategic priorities. First, it has to manage the government’s finances effectively and ensure that sufficient
financial resources are available for operating the government. Every year, the Treasury issues more than 960
million payments on behalf of the federal government, collects over $2 trillion, and manages over $8 trillion in
debt. An important goal is to reduce the tax gap (the difference between the taxes taxpayers should pay and
those they actually pay) by increasing voluntary compliance with tax laws—for instance, through tax simplification
—and by reducing evasion opportunities, thus reducing the country’s need to borrow. Second, it is responsible for
securing the United States’ economic and financial future and raising standards of living. The department supports
foreign trade liberalization and develops policies for fostering innovation that supports economic growth. It also has
to ensure that the U.S. currency is trusted worldwide, that the country is economically competitive, and that
financial and economic crises are prevented or mitigated. Moreover, it supports some emerging countries, as this
policy increases trade and investment opportunities and ensures regional stability. Third, the Treasury has to
strengthen national security. In cooperation with other national and international agencies and governments, but
also with private financial institutions, it tries to stop the financers of terrorist groups, drug traffickers, money
launderers, and other criminals and rogue regimes that threaten the United States and other free and open
economies. Fourth, it has to produce effective management results and guarantee that its programs and activities
perform efficiently, transparently, and cost-effectively.
Meanwhile, in 2007–2008, the department, under then secretary Henry Paulson, was faced with the worst
financial crisis since the Great Depression. With financial institutions reeling from the collapse in housing prices
and rising foreclosure rates, Paulson sent a request to Congress for hundreds of billions of dollars that could be
used to help banks and thrifts that were collapsing under the weight of bad mortgages and mortgage-backed
securities.
Initially, Congress balked not just at the size of the rescue plan but at its lack of detail and at the enormous,
unchecked latitude it gave the Treasury secretary in deciding what to do with the funds. But as the crisis
deepened, Congress was forced to act, passing the Emergency Stabilization Act of 2008 in early October; the act
provided $700 billion to the Treasury department to bail out troubled and potentially troubled financial institutions,
though with more oversight of the Treasury secretary’s actions.
Tiia Vissak
 
See also:  Federal Reserve System;  Fiscal Policy;  Monetary Policy;  National Economic
Council;  Tax Policy. 
Further Reading
 

United States Department of the Treasury:  www.treasury.gov
Tribune Company
 
The Tribune Company is a diversified media corporation with a history stretching back over one-and-a-half
centuries. Beginning with the Chicago Tribune newspaper, the company’s leaders recognized the power and
potential of both radio and television when those media first appeared. Aggressive acquisition in large and mid-
level markets made the Tribune Company a national force. However, changes in the traditional media markets
and questionable business decisions resulted in a serious cash flow problem that caused the Tribune Company to
file for bankruptcy protection in December 2008.
The Tribune Company originated on June 10, 1847, with the publication of the first issue of the Chicago Daily
Tribune. The newspaper struggled through its early years until being acquired in 1855 by Joseph Medill. Medill
recognized that Chicago could become a thriving metropolis and that his newspaper might play an important role
in that future. He affiliated the paper with the Republican Party and backed Abraham Lincoln’s presidential
campaign in 1860. During the Civil War, Chicago Tribune reporters provided excellent coverage of Union
operations across the country. After the war, Medill became a booster of Chicago, calling for improvements such
as greater fire protection. Although the Tribune building was destroyed by the great fire of October 1871, the
newspaper reappeared only two days later, with a prediction that Chicago would rebuild better than ever. Medill
was elected mayor soon afterward.
Medill’s two grandsons eventually succeeded him at the Chicago Tribune. They expanded beyond Chicago,
purchasing a newspaper in New York and founding a national literary magazine. They also recognized that radio
could reach new audiences. In 1924, the newspaper purchased a Chicago station whose call letters were changed
to WGN. The initials stood for the Chicago Tribune’s motto, “World’s Greatest Newspaper.” Innovative
programming on WGN included coverage of the 1925 World Series, the Indianapolis 500, and the Kentucky
Derby. Other innovations included live microphones in the courtroom of the so-called Scopes monkey trial in
Tennessee and a regular comedy series that came to be nationally broadcast as Amos’n’ Andy.
After World War II, the Tribune Company expanded into television. In 1948, both WGN-TV in Chicago and WPIX-
TV in New York were launched. WGN started to reach a nationwide audience in 1978, when most cable television
systems around the country began carrying it. As a “superstation,” WGN could attract advertisers who wanted a
national audience. It also carried Chicago Cubs baseball games, giving that team a national following.
Between 1963 and 1985, the Tribune Company continued to expand nationally. Additional newspapers were
purchased, including ones in Florida and California. Additional television and radio stations were purchased in
other states. The company was reorganized into two divisions, with one concentrating on publishing and the other
on broadcasting. Significant resources went into the production of television programs for the cable systems
beginning in the 1980s. Shows such as the Geraldo Rivera Show and Gene Roddenberry’s Andromeda were
created and sold to broadcasters across the nation. In 1981, the Tribune Company also purchased the Chicago
Cubs, building on a relationship that had existed for decades. In 1983, the company went from private to public
ownership, with one of the largest stock offerings in history.
In June 2000, the Tribune Company purchased the Times Mirror Company, publisher of the Los Angeles Times,
for $8.3 billion. The acquisitions included several other major newspapers, including a Spanish-language one in
New York. The deal made the Tribune Company the third-largest newspaper publisher in the United States.

Expansion into Internet sites and more interactive media also took place.
On April 2, 2007, Chicago-based investor Sam Zell announced his plans to purchase the Tribune Company and
make it privately owned. Zell put up $315 million of his own money and financed $8.2 billion from various lenders
to purchase the outstanding Tribune Company stock. Experts warned against the deal because it would load the
company with a heavy debt when revenues were not increasing. Despite the warning, 97 percent of the
stockholders approved the deal on August 21, 2007. Under Zell’s leadership, the company purchased its own
stock over the next few months. December 20, 2007, was the last day on which Tribune Company stock was
traded publicly.
The company soon faced financial difficulties. Although some properties were sold off to raise cash, they were not
enough. During the first nine months of 2008, revenue decreased 7.9 percent. Publishing revenue, which provided
most of the income, declined 11.6 percent. Part of the reason was competition from the Internet and the public’s
changing interest in media. Traditional print media, including newspapers, have declined in circulation as more
people rely upon the Internet for their news. As circulation declined, advertisers were less interested in using the
Tribune Company’s newspapers. The recession that began in 2007 only accelerated a trend that began in the
1990s. Broadcasting revenues for the company also failed to increase during this time period. Total revenue for
the company was only $3.1 billion, with total liabilities of $13 billion.
On December 8, 2008, the Tribune Company filed for bankruptcy to protect its remaining assets. Under
bankruptcy law, the company was allowed to continue operating while a plan to pay back its creditors was worked
out. Because some of the original lenders had sold their loans to third parties, many people believed the Tribune
Company would be broken up and the parts sold off. In October 2009, for example, the bankruptcy court allowed
Thomas Ricketts to buy 95 percent of the Cubs, along with their stadium and broadcasting rights. Zell was
criticized for not allowing the sale earlier, when the price might have been better.
In December 2009, the corporate leadership was changed. Zell remained as chairman of the board, while Randy
Michaels became chief executive officer. In May 2011, Michaels was replaced as CEO by Los Angeles Times
publisher Eddy Hartenstein.
Tim J. Watts
 
See also:  Information Technology;  Technological Innovation. 
Further Reading
Madigan, Charles M. 30: The Collapse of the Great American Newspaper. Chicago: Ivan R. Dee, 2007. 
Smith, Richard Norton. The Colonel: The Life and Legend of Robert R. McCormick, 1880–1955. Evanston, IL: Northwestern
University Press, 2003. 
Wendt, Lloyd. Chicago Tribune: The Rise of a Great American Newspaper. Chicago: Rand McNally, 1979. 
Tropicana Entertainment
 
A privately owned Las Vegas–based corporation specializing in gambling, hotels, and resorts, Tropicana
Entertainment was one of the fastest growing businesses in its sector until the financial crisis and recession that

began in 2007 forced the company into bankruptcy. The case of Tropicana, say many industry observers, shows
the vulnerability of entertainment-based enterprises during severe economic downturns. This is especially true of
companies that have overextended their resources through expansion.
Founded in 1957, during the post–World War II heyday of Las Vegas, the Tropicana began as a hotel and casino
complex on the south end of the famed Las Vegas Strip (that hotel became the flagship of a nationwide, eleven-
strong resort and casino empire in the mid-2000s). Over the subsequent decades, new additions to the resort
were added, including a golf course, new towers, a theater, and other leisure-themed amenities.
In 1979, the national hotel chain Ramada bought the Tropicana Hotel. Two years later, the company opened
another Tropicana Casino and Resort in the newly created gambling haven of Atlantic City, New Jersey. In 1989,
Ramada spun off a new publicly traded company known as the Aztar Corporation, whose assets included the two
hotels. To take advantage of the expansion of legalized gambling across the country, Aztar acquired casinos in
several states in the 1990s.
In 2006, Columbia Sussex, a hotel and casino group founded in 1972, created a subsidiary company known as
Tropicana Entertainment to run the hotel and casino properties it had acquired when it purchased Aztar for $2.1
billion. Within a year, Tropicana Entertainment had acquired other properties, creating a chain of hotels and
casinos in Atlantic City, Las Vegas, other cities in Nevada, and legalized gambling meccas across the South.
Meanwhile, the new company announced plans for a massive expansion of its flagship property in Las Vegas.
Initially conceived at the tale end of the Las Vegas boom of the 1990s and early 2000s, which saw dozens of new
hotel and casino complexes open on the Strip, the plans were quickly shelved as the recession began to take a
major bite out of the city’s gambling and tourist trade. A further blow to the company’s finances came in 2007,
when the New Jersey Casino Control Commission refused to renew the gambling license of the Tropicana
property in Atlantic City. After hearing complaints about severe pay cuts and unsanitary conditions at the hotel, the
commission decided that neither Columbia Sussex nor Tropicana Entertainment had adequate financial resources
to operate the property, also citing the “lack of business ability, good character, honesty, and integrity” of the two
companies.
Within months of losing its New Jersey license, Tropicana Entertainment filed for Chapter 11 bankruptcy protection
on May 5, 2008, and the president of Columbia Sussex resigned. The Tropicana Atlantic City casino was not
included in the filing nor was the Amelia Belle, a riverboat casino operation in Louisiana. Exactly a year after the
filing, the Delaware Bankruptcy Court granted Tropicana Entertainment permission, on May 5, 2009, to emerge
from bankruptcy as two companies, both of them spun off from Columbia Sussex. One would be the Vegas-based
Tropicana Resort and Casino; the other would be a holding company for the remaining casinos. The
reorganization plan had to await the approval of the gaming regulatory bodies in the states affected.
Bill Kte’pi and James Ciment
 
See also:  Recession and Financial Crisis (2007-). 
Further Reading
“History of the Tropicana.” www.tropicanamediasite.com/history-tropicana.ia
Krauss, Clifford.  “Economic Troubles Affect the Vegas Strip.” New York Times, May 6, 2008. 
Troubled Asset Relief Program (2008–)

 
The Troubled Asset Relief Program (TARP) was the central component of the federal government’s efforts to
alleviate the crisis that gripped U.S. and overseas financial markets in late 2008 and to avoid what many
economists predicted could be a global slide into a second Great Depression. The enabling legislation—the
Emergency Economic Stabilization Act of 2008, signed into law by President George W. Bush on October 3—was
a U.S. program initially aimed at buying assets of questionable value and liquidity on the books of major financial
institutions. It provided $700 billion to Secretary of the Treasury Henry Paulson and gave him broad, ill-defined
powers to act as he saw fit in relieving key financial institutions of various “troubled assets,” most notably
mortgage-backed securities and collateralized debt obligations (CDOs).
As the financial crisis and resulting recession deepened in the fall of 2008, Paulson shifted his emphasis from
purchasing financial institutions’ “troubled” or “toxic” assets to taking ownership through equity stakes in collapsing
financial institutions themselves, including investment banks, commercial banks, and insurance companies. In
addition, the program was expanded by presidential executive order to the troubled U.S. automobile industry.
Much criticized by both the political Left and Right at the time and since, TARP has generally been considered a
success by most mainstream economists, who say that the vast amount of bailout money stabilized and restored
confidence in the international credit markets, preventing a freezing up in short-term lending that might have
brought the global economy grinding to a halt. Still, there has been much criticism as well, focusing on whether
the money could have been better spent elsewhere and whether the banks used it to do what the federal
government wanted them to—that is, provide more lending.
 Causes of the Crisis
TARP’s scale was unprecedented. Never before had the U.S. government injected so much capital—or intervened
so forcefully—in the financial markets. But, say defenders of the program, not since the Great Depression had the
United States faced an equivalently dire financial crisis, the origins of which were many years in the making and
rooted in various causes, including: the deregulation of the financial industry; loose monetary policy by the Federal
Reserve Board (Fed); the development of new and complicated financial instruments such as mortgage-backed
securities and derivatives like collateralized debt obligations; new forms of executive compensation in the financial
industry that encouraged risk taking; the rise of hedge funds, which often used derivatives and short selling to
offset exposure to ordinary securities investment; and most directly, an unprecedented bubble in housing prices.
Beginning in the late 1970s and accelerating in the 1980s and 1990s, the financial regulatory structure established
in the United States during the first half of the twentieth century was dismantled, a trend backed by almost all
Republicans and many Democrats in the White House and Congress. Among the most important acts of
deregulation was 1999 legislation that overturned a Depression-era law, the Glass-Steagall Act of 1933, forbidding
commercial banks from engaging in the investment banking and insurance businesses. Even as regulations were
eased, there remained a plethora of competing regulatory agencies, including the Office of Thrift Supervision,
created in the wake of the savings and loan crisis of the late 1980s and early 1990s. With regulatory duties
spread out among so many different agencies, financial institutions were able to “shop around,” in effect, to find
the agency that would conduct the least oversight of and allow the greatest flexibility in ever-riskier investment
strategies.
Advocates of the deregulatory effort argued that the technological and communications revolutions of the late
twentieth century had made information so widely available that markets were capable of regulating themselves
far more efficiently than any government agency could. Part of the self-regulation came in the form of new
financial instruments that spread risk over ever-greater numbers of investors, thereby smoothing the ups and
downs of the financial cycle. Mortgage-backed securities, for example, spread the risk of mortgage default beyond
the originator of the mortgage, while derivatives offered a kind of insurance policy against financial losses. Many of

these new and “exotic” securities were barely regulated at all. Meanwhile, new forms of compensation—often
crafted to avoid tax exposure for the company or the individual being compensated—were too closely tied to the
immediate performance of the financial company’s stock, encouraging executives to put more of their company’s
assets in these new, high-performing but high-risk investments.
All of these factors contributed to the unprecedented run-up in U.S. home prices that was at the heart of the
financial crisis of 2008–2009. By spreading the risk of mortgage default over a wide investor base—and then
insuring the remaining risk through derivatives like CDOs—mortgage-backed securities removed the incentive for
mortgage originators to make sure that the people getting the mortgages had the income and credit history to
justify lending them tens or hundreds of thousands of dollars. Inevitably, standards declined to the point that many
people were receiving mortgages without having to provide evidence of income or assets. Such mortgagees
comprised a growing market, known as the subprime market. At the same time, adjustable rate mortgages
(ARMs) were being marketed both in the prime and subprime markets. These offered low initial rates against the
interest alone, with upward adjustments—sometimes dramatic—to cover principal and higher interest payments
after a given period of time.
To some economists during the housing price run-up of the early and mid-2000s, ARMs represented a ticking
time bomb, except that the fuse kept being lengthened. With interest rates at historically low levels—a result of
Fed policy—more and more people could afford to take on larger and larger mortgages, thus inflating housing
prices. With housing prices rising, creating more home equity, homeowners with ARMs could simply refinance
before the adjustable rate went up. Eventually, however, the Fed was forced to raise rates to cool the overheated
market—home prices in some parts of the country were increasing by 20 percent or more a year—and avoid the
inflation that the market might trigger in other sectors of the economy. (Many people were taking out home-equity
loans to finance other consumer purchases.)
 To the Brink
By late 2006, home prices began to decline, effectively destroying the equity that allowed mortgagors to refinance.
As ARMs adjusted upward, many homeowners found themselves unable to meet their monthly mortgage
payments, and many were forced into foreclosure. The wave of foreclosures sent home prices down ever farther,
finally causing the housing bubble to burst. Mortgage defaults and foreclosure rates shot far above historic norms.
This, in turn, undermined confidence in both mortgage-backed securities and the derivatives that insured them.
Financial institutions with these securities and derivatives on their books found their assets declining at an
accelerating rate, leading to precipitous drops in share prices. Indeed, some of these institutions were unable to
obtain credit from other financial institutions, who questioned their solvency. The first to face such a crisis was
Bear Stearns, a Wall Street investment bank that found itself on the verge of collapse in March 2008 until the Fed
arranged an emergency takeover of the company by the investment bank JPMorgan Chase.
The government and financial community hoped that Bear Stearns was an extreme case—that its exposure to the
subprime mortgage market, in the form of mortgage-backed securities and CDOs, was unusually high. By late
summer, however, it was becoming increasingly clear that “toxic” or “troubled” assets littered the books of many
financial institutions, including major brokerage houses, investment banks, hedge fund companies, and even
commercial banks. Events seemed to spin out of control, as worries also arose about the solvency of Fannie Mae
and Freddie Mac, two federal government–sponsored private enterprises that insured more than half of the
nation’s home mortgages. In early September, Secretary Paulson announced a government takeover of the two
mortgage insurance giants.
A week later came the collapse of the investment bank Lehman Brothers. Having faced much criticism for its Bear
Stearns bailout—and worried that further bailouts would create an expectation in the markets that excessive risk
taking would go unpunished and that the government would come to the rescue of over-aggressive players—the
Fed opted to let Lehman Brothers fail in what was the largest bankruptcy filing in U.S. history, valued at more than
$600 billion. But even as Fed chairman Ben Bernanke was letting Lehman fail, he was moving to shore up the

insurance giant AIG with $85 billion, in exchange for a nearly 80 percent government stake in the company.
Bernanke and others argued that the unprecedented move to rescue the world’s largest insurer of financial
instruments was critical, as its failure would create mass panic in the credit markets. Not only was the sum huge,
but technically, AIG, as an insurance company, was outside the Fed’s purview. In addition, the fact that AIG
executives treated themselves to a lengthy retreat at an expensive California resort a mere week after they
received government money did not help ease the growing political backlash over the bailout.
Meanwhile, the Fed’s decision to let Lehman fail proved disastrous in the short term as panic began to grip the
financial markets, freezing up the short-term interbank lending that kept many institutions afloat. In the wake of the
Lehman collapse came news that one of the nation’s largest bank holding companies, Washington Mutual
(WaMu), was teetering. WaMu was a creation of the 1999 deregulatory law that allowed commercial banks to
reform as bank holding companies and to engage in investment banking and other financial services. In the end,
the risks assumed by the investment banking division put the rest of the company at risk, and WaMu’s commercial
banking activities were placed under the receivership of the Federal Deposit Insurance Corporation (FDIC) and
many of its assets sold to JPMorgan Chase.
 Paulson’s Plan
By mid-September 2008, it was becoming clear to both government officials and the financial community that the
nation’s financial system was experiencing a crisis of major proportions, akin, some said, to that faced in the
depths of the Great Depression. On September 20, Secretary Paulson offered the first bailout plan, whereby the
Treasury Department would use $700 billion to buy up troubled assets—largely mortgage-backed securities and
collateralized debt obligations—on the books of major American financial institutions. Short on details, including
how the figure of $700 billion was arrived at, the plan gave Secretary Paulson almost unlimited power to spend
the money as he saw fit, with little congressional oversight.
The initial rescue plan raised hackles among both liberals and conservatives in Congress. The former saw it as a
giveaway to big banks, the latter as unacceptable government interference in the financial markets. On September
29, the House of Representatives voted down Paulson’s plan, sending the stock market reeling as financial stocks
collapsed. Fears arose in the markets that the House’s rejection of the Paulson plan would freeze up U.S. and
global credit markets, making it impossible for companies to obtain the short-term loans they needed to meet
payroll. Talk spread in the media of a complete breakdown of the financial markets and a “new Great Depression.”
Such talk, along with collapsing stock prices and Paulson’s efforts to flesh out more details of a bailout plan that
would give Congress nominally more oversight, persuaded a still reluctant Congress to pass the Emergency
Economic Stabilization Act, the enabling legislation for TARP. The legislation signed into law by President Bush on
October 3 called for the release of half the $700 billion immediately and the other half to be authorized by
Congress the following January, once the Treasury Department explained more fully how it would be used.
Receiving that report, Congress authorized the release of the second $350 billion on January 15, 2009.
 Impact and Criticism
TARP’s basic goal was simple. No one really knew the value of the troubled assets on the books of the nation’s
financial institutions nor even exactly what they were. Because mortgages had been bundled and resold so many
times, financial institutions themselves did not know what they owned. This made investors reluctant to keep their
money in financial institutions and made the institutions themselves reluctant to lend money to each other. After
all, they did not know if the institution to which they were lending would soon become insolvent. Without this
modicum of trust and the free flow of credit, the world’s financial system could freeze up and bring down the
entire global economy. TARP, by taking troubled assets off the books of key financial institutions such as Citigroup
(which received $50 billion in TARP money) and Bank of America ($45 billion), would reassure the markets and
get credit flowing again. Then, once the markets had stabilized, the government could sell the assets to recoup
some of the costs assumed by taxpayers with the bailout.

Secretary Paulson soon came to realize, however, that this approach would not work fast enough to reassure the
markets and get credit flowing again. Banks were still reluctant to lend to each other or to nonbanking institutions
and individuals. Taking a cue from British Prime Minister Gordon Brown, Paulson quickly shifted gears and took a
different approach to financial relief. Rather than use TARP funds to buy up troubled bank assets, he would use
the money to buy equity stakes directly in major financial institutions. By November 2008, the immediate crisis
seemed to be passing. While most banks remained reluctant to lend for longer-term projects, short-term credit
between financial institutions and to nonfinancial institutions was flowing again.
TARP underwent further alteration again in December by President Bush, who used his executive power to expand
the program to include major U.S. automobile manufacturers, two of which—General Motors and Chrysler—were
on the brink of bankruptcy. Ultimately, GM received some $14 billion in bailout money and Chrysler $4 billion, in
exchange for equity stakes. In neither case, however, were the funds enough for the firms to avoid bankruptcy in
2009.
Politically, TARP proved highly controversial, especially after it was revealed that some of the financial institutions
being bailed out with taxpayer money were paying large bonuses to top executives and other employees.
Economically, the results were mixed. According to most economists, the injection of such a vast amount of
capital into the financial marketplace prevented a complete meltdown of the international financial system.
Other criticisms of the plan take three basic forms. Many argued that the TARP money would have been better
spent in any number of other ways, including helping distressed homeowners pay their mortgages. These were, it
was said, the very people whose financial difficulties were the original source of the crisis. Another group of critics
maintained that Secretary Paulson misread the basic problem faced by financial institutions. The problem was not
one of liquidity, they maintained, but one of financial irresponsibility. Thus, TARP only encouraged more
irresponsibility by rescuing bankers from their own folly. Finally, still other critics of TARP argued that banks
inevitably misused the money. Rather than lend the money to businesses and individuals and help lift the
economy as a whole, it was said, the banks used the funds to pay down their own debt and buy up weaker
institutions. If one factor of the crisis was the fact that some institutions had grown “too big to fail”—meaning that
their collapse posed a systemic risk to financial markets—TARP only made them bigger.
Meanwhile, the government’s entire financial rescue plan—of which TARP was the largest part—had contributed
to further growth in the already sizable federal debt, which climbed above $15 trillion by the end of 2011. Such an
increase, many feared, was likely to trigger an inflationary spiral in the future and further weaken the economy. On
the positive side, it was estimated by the nonpartisan Congressional Budget Office that just $432 billion of the
total $700 billion bailout package was actually dispersed and that the ultimate cost to taxpayers would be under
$20 billion; the low figure was partly the result of interest payments charged to financial institutions for the money.
James Ciment
 
See also:  Banks, Commercial;  Banks, Investment;  Federal Reserve System;  Financial
Markets;  Moral Hazard;  Recession and Financial Crisis (2007-);  “Too Big to Fail”;  Treasury,
Department of the. 
Further Reading
Acosto, Jarod R., ed. Assessing Treasury’s Strategy: Six Months of TARP. Hauppauge, NY: Nova Science, 2009. 
Board of Governors of the Federal Reserve System.  “Troubled Asset Relief Program (TARP) Information.” Available at
www.federalreserve.gov/bankinforeg/tarpinfo.htm.
Elliott, Douglas J. Measuring the Cost of TARP. Washington, DC: Brookings Initiative on Business and Public Policy Fixing
Finance Series, January 2009. 
Lefebvre, Adelaide D., ed. Government Bailout: Troubled Asset Relief Program (TARP). Hauppauge, NY: Nova

Science, 2009. 
Perkins, Martin Y., ed. TARP and the Restoration of U.S. Financial Stability. Hauppauge, NY: Nova Science, 2009. 
Tugan-Baranovsky, Mikhail Ivanovich (1865–1919)
 
The Ukrainian economist Mikhail Ivanovich Tugan-Baranovsky was an early and leading proponent of the idea that
economic crises are an unavoidable—and, indeed, intrinsic—aspect of the capitalist system and its surge toward
industrialization. His was among the first purely economic theories of business cycles, and his theories of capitalist
crises later challenged those of Karl Marx. He was also one of the founders of the National Academy of Science of
Ukraine.
Tugan-Baranovsky was born in 1865 in the countryside of Kharkov, Ukraine. After completing his undergraduate
studies in Kharkov, he spent some months at the British Museum Library researching the history of the British
economy. He received a master’s degree in 1894 from the University of Moscow, for which he produced his
masterpiece, Industrial Crises in England. He earned a doctorate in 1898 and published his thesis, titled The
Russian Factory in the 19th Century. From 1895 on, he taught economics in various institutions in St. Petersburg,
remaining deeply involved in debates on the Russian economy. He made substantial contributions to economic
theory and actively participated in the nation’s cooperative movement. His renown among Western economists
largely rests, however, on his contributions to the theory of industrial crises.
Economic booms and busts were long thought to be accidental, provoked by exogenous factors, including wars,
crop failures, and events such as the discovery of gold. Tugan-Baranovsky acknowledged that while this may
have been the case before the era of industrialization, it was no longer true in nineteenth-century Great Britain. In
Industrial Crises in England, he uses historical evidence to support the theory that crises recur with some
similarities and striking regularity (every seven to eleven years). He formulated one of the first endogenous
theories of economic crises, in which he posited that they are endemic to modern economies. He expanded the
notion of a capitalist cycle composed of three phases: expansion, industrial crisis, and stagnation. By emphasizing
industrial crises, he discarded widespread theories that disruptions in the monetary or credit systems cause crises.
Tugan-Baranovsky did not deny monetary and credit crises, but he viewed them as symptoms that appear during
industrial crises.
According to Tugan-Baranovsky, industrial crises emerge from the antagonistic nature of a capitalist economy.
Production rules consumption in a capitalist system, whereas in a socialist system, consumption is the aim of
production. In capitalist economies, the means of production are mainly intended to create new means of
production. Thus, an accumulation of capital takes place, and production strives for infinite and disordered growth.
In a capitalist system, the nation’s production is disorganized, and the resulting anarchy disrupts growth. The lack
of a plan to regulate production between the alternative sectors in the economy (in other words, the
disproportionality between production and consumption) is the major cause of modern industrial crises, whereas
defective organization in monetary and credit institutions only intensifies them.
Tugan-Baranovsky used the cyclical fluctuations of free available capital to explain the predicted regularity of
industrial crises. In a boom period, when prices are high and speculation rises, savings (free capital) is often
productively invested. During bust periods, prices are low, and free capital accumulates in banks. Tugan-
Baranovsky used the metaphor of the steam engine to illustrate the recurrence of the capitalist cycle. Free capital
plays the role of the steam in a steam engine: it accumulates until the evacuation of the pressure becomes

unavoidable.
Industrial Crises was translated into several languages, and Tugan-Baranovsky’s arguments about the
disproportionality in production and the investment-saving fluctuations influenced many economists and authors
outside Russia—first German, then French, and eventually British. But he believed his theory was of vital
importance to Russia, as well. Vigorous industrialization policies were carried out in Russia in the 1890s that
raised questions about the balance between the agricultural and industrial sectors. In The Russian Factory,
Tugan-Baranovsky shows that economic fluctuations were becoming more and more relevant to the industrial
development of prerevolutionary Russia. He died in 1919 near Odessa, Ukraine.
François Allisson
 
See also:  Classical Theories and Models;  Over-Savings and Over-Investment Theories of the
Business Cycle. 
Further Reading
Barnett, Vincent.  “Tugan-Baranovsky as a Pioneer of Trade Cycle Analysis.” Journal of the History of Economic
Thought 23:4 (December 2001): 443–466. 
Tugan-Baranovsky, M. The Russian Factory in the 19th Century.  Trans. from the 3rd Russian ed. by Arthur Levin and
Claora S. Levin. Homewood, IL: R.D. Irwin, 1970. 
 
Tulipmania (1636–1637)
 
The seventeenth-century Dutch phenomenon referred to as “tulipmania” is considered the first important financial
“bubble” in European history. As the nickname implies, the episode was characterized by rampant speculation in
tulip bulbs. More generally, tulipmania has become a metaphor in the economics profession for a highly
speculative financial market in which prices for a product or commodity soar irrationally and then, suddenly and
often unexpectedly, crash. Despite the numerous boom-and-bust incidents that have occurred since that time,
tulipmania remains the event against which all speculative market excesses have been compared. As the noted
economic historian Charles P. Kindleberger wrote, tulipmania represented “probably the high watermark in
bubbles.”

A Dutch engraving of 1637, titled “The Fool’s Cap,” lampoons the speculative frenzy in tulips—and the investors
who paid fortunes for them—in The Netherlands that year. The run-up in tulip prices is the first known speculative
bubble in history. (The Granger Collection, New York)
Both the peculiar biology of tulips and the unusual economic circumstances of seventeenth-century Amsterdam
contributed to the mania. Tulips can be grown from either seeds or bulbs, the latter representing the faster
method. Although growing them from seeds can take up to a dozen years, it is through the budding and seed
process that the mosaic virus—which may create spectacular and highly coveted color patterns—propagates itself.
Once the particular strain of tulip develops, it naturally clones itself through the bulb. In other words, a highly
desired variety is difficult to propagate—and thus is more valuable—but once it is propagated, it is long lasting. In
this regard, tulips are akin to gold—rare and beautiful but also durable.
Tulips at the time also had a novelty factor, having been brought to Europe from the Ottoman Empire in the
sixteenth century and first propagated in Holland around the turn of the seventeenth. This was just at the time that
Holland and its chief city, Amsterdam, were entering their “golden age.” A vital center of finance and trade,
Amsterdam was arguably Europe’s richest city in the first half of the seventeenth century, boasting numerous
upper-class merchants and the largest middle class on the continent, both with discretionary income to invest and
spend.
By the early 1630s, excess money was washing around the Dutch economy, as the country’s trading activities
drew in coin and precious metals from around Europe and across the Atlantic. Between January 1636 and
January 1637, the height of the tulip boom, deposits in the Bank of Amsterdam rose by more than 40 percent.
The rapid run-up in the money supply fostered an atmosphere that was ripe for irrational speculation. Into this
feverish climate came the tulip bulb.
Although tulips were popular in the first three decades of the seventeenth century, trade in the bulbs was largely
limited to professional growers until the early 1630s. By the middle years of that decade, the bulbs were
increasingly being traded among nonprofessionals. A host of new varieties—given grandiose names to enhance
their value—were introduced in 1634, which brought prices down for the more common varieties, making them a
popular investment for middle-class purchasers. Meanwhile, the upper classes were speculating on the truly rare
and spectacular varieties, a popular diversion from the mania for art collection that had gripped wealthy Dutch
merchants.
The tulip market was essentially a futures market from September to June each year. Beginning in the summer of

1636, the trading of tulip futures took place in all sorts of public places in Amsterdam, such as taverns and coffee
houses. Groups of traders, called “colleges,” created rules that restricted the bidding and fees associated with
trading. Only a small fraction of the purchase price of any bulb was required for the down payment, which was
known as “wine money.”
Upon the arrival of the contract settlement date, buyers typically did not have the required cash to settle the trade,
but the sellers did not have the bulbs to deliver either, for they were still in the ground. Thus, the trade was
settled with only a payment of the difference between the contract price and the expected settlement price. Such
margin buying further exacerbated the speculative fever. By February 1637, when prices hit their peak, tulip bulbs
and tulip bulb futures were trading for extraordinary sums. As British journalist Charles Mackay, the first to
seriously chronicle the phenomenon, wrote in 1841, a single Viceroy bulb traded for 8,000 pounds of wheat,
16,000 pounds of rye, 4 fat oxen, 8 fat swine, 12 fat sheep, 126 gallons of wine, over a thousand gallons of beer,
more than 500 gallons of butter, 1,000 pounds of cheese, a complete bed, a suit of clothes, and a silver drinking
cup.
Like all speculative bubbles, tulipmania came to a sudden and spectacular end in the winter of 1637, when new
investors could no longer be found. With little intrinsic value, tulip bulb prices plummeted. In January 1637, the
common Witte Croonen bulb, which had risen in value by 2,600 percent, fell to one-twentieth of its peak price in a
single week. Investors soon tried to dump their bulb futures on the market but found few takers. They then sought
help from the government, which allowed investors to get out of their contract by paying 10 percent of the
contract’s face value. When the sellers sued, the courts offered them little solace, declaring the debts a result of
gambling and therefore unenforceable.
The legacy of tulipmania for the Dutch economy has been debated ever since. Focusing on the small group of
relatively well-off investors caught up in the speculation, Mackay insisted that the effects were devastating, citing
“[s]ubstantial merchants reduced almost to beggary and many a representative of a noble line saw the fortunes of
his house ruined beyond redemption.” According to modern historians, however, Mackay was guilty of hyperbole.
The mania, as noted above, affected only a small portion of the Dutch population, and its impact on the nation’s
economy was minimal. Holland remained one of the wealthiest countries in Europe long after the mania
dissipated, and its relative decline versus other powers had more to do with geopolitical and other economic
factors, such as war, colonial expansion, and the rise of financial and trade centers elsewhere on the continent.
Douglas French and James Ciment
 
See also:  Asset-Price Bubble;  Mississippi Bubble (1717-1720);  Netherlands, The;  South Sea
Bubble (1720). 
Further Reading
Dash, Mike. Tulipomania: The Story of the World’s Most Coveted Flower and the Extraordinary Passions It Aroused. New
York: Crown, 2001. 
Garber, Peter M. Famous First Bubbles: The Fundamentals of Early Manias. Cambridge, MA: MIT Press, 2000. 
Kindleberger, Charles P. A Financial History of Western Europe. London: George Allen and Unwin. 1984. 
Kindleberger, Charles P. Manias, Panics, and Crashes: A History of Financial Crises. New York: John Wiley and
Sons, 1978. 
Mackay, Charles. Memoirs of Extraordinary Popular Delusions and the Madness of Crowds.  1841. Reprint edition. New
York: Cosimo Classics, 2008. 

Turkey
 
Located largely on the Anatolian Peninsula in southwestern Asia, with a tiny but demographically and
economically important section in southeastern Europe, Turkey—with a majority Islamic population of about 74
million—is a major developing country and a political mediator between the West and the Middle East.
 Origins of the Turkish Republic
Turkey’s economy has experienced a series of highs and lows since the creation of the modern republic in 1923.
At that time, the country had an overwhelmingly agricultural economy, and the new government was determined to
support more domestic industry. Conservative policies helped Turkey to weather the Great Depression relatively
well, but World War II caused a general economic decline. A cycle of rapid expansions and economic downturns
followed the war, resulting in several interventions by the army to stabilize the country. Economic reforms in the
1980s improved Turkey’s overall economy, but high rates of inflation continued to prevent stability. Modest
industrialization, combined with income from services and commodities, helped the Turkish economy to expand
during much of the 1990s and early twenty-first century. The worldwide recession beginning in 2007 caused
Turkey’s economy to contract, with an uncertain outlook for the future.
The multinational Ottoman Empire, of which Turkey was the core, declined militarily and economically during the
nineteenth century. Its government borrowed heavily from European lenders, and much of its industry, including
railroads, was under foreign control. Beginning in 1912, a decade of warfare devastated the Ottoman economy. By
the time the Republic of Turkey was established in 1923, the empire’s population had declined by 25 percent.
Control over Iraq, Syria, and Palestine was lost, while most Greeks and Armenian inhabitants had been expelled
or killed. Agriculture had suffered from the conscription of workers and draught animals. Exports had been
curtailed by blockade as well.
During the 1920s, however, an economic recovery took place. Agricultural production returned to prewar levels. In
1924, the medieval practice of using tax farmers to confiscate one-tenth of agricultural production was
discontinued, providing significant relief to growers. The government of Turkey’s founder, Mustafa Kemal Ataturk,
encouraged the development of a Turkish middle class by privatizing various state monopolies and offering
incentives and subsidies to industry. State-controlled railroads were constructed to help tie all parts of the country
into a unified market.
Beginning in 1929, the Great Depression led to a worldwide economic downturn. Commodity prices declined
sharply, harming Turkish growers who depended on the export market. To prop up the Turkish economy, the state
assumed more direct control over foreign trade. The amount of goods that could be imported was limited, and
tariffs were raised on imports of food products and consumer goods. The government also ceased to make
payments on foreign debts and demanded a conference on restructuring the payments. As a result, during most of
the 1930s, Turkey had a trade surplus.
 Advent of Statist Economics
In 1932, the Turkish government adopted a new economic strategy known as statism. Under statism, the state
became a major producer and investor in the urban industrial economic sector, developing an alternative to an
export-driven agricultural economy. A five-year plan was set up with assistance from Soviet advisers. Many
monopolies that had been privatized during the 1920s, such as transportation and finance, were taken back under
state control. Controls over prices and markets were established. Limits on wages and labor activities were also
enforced. While state funds were concentrated on heavy industry and transportation, private investment was still

encouraged and subsidized. Statism was generally considered a success. Unlike many other economies of the
1930s, the Turkish economy continued to grow slowly. Turkey’s statism was adopted after World War II by other
countries in the Middle East.
Agriculture also expanded during the Depression. Because commodity prices remained low, Turkish farmers grew
more products to achieve the same income. Improvements in transportation gave them a national market. As the
world economy improved, agricultural exports also increased, encouraging more growth.
World War II caused stagnation in Turkey’s economy. Although the country remained neutral throughout the war,
the government ordered a full mobilization to discourage foreign attack. The lack of manpower for farms and
industry reduced output. Exports also fell because of blockades and dangers to shipping. State income fell, so
expenditures were reduced. The government abandoned statism as an economic policy, and the general standard
of living decreased. General dissatisfaction with single-party rule in Turkey led to the adoption of a multiparty
system by 1950.
After 1950, Turkey’s economy underwent a cycle of boom and bust, with a crisis occurring about every decade.
Inflation became a serious problem. During the 1950s, Turkey’s economy continued to depend upon exporting
agricultural products, particularly wheat. When international prices declined after the Korean War, the Turkish
government subsidized wheat farming. Rapid inflation resulted, made worse by the overvaluation of the Turkish
lira. Devaluation in 1958 helped bring on a recession. In turn, economic problems led to a military coup in 1960.
During the 1960s, state-owned industries assumed a larger part of the Turkish economy. To promote domestic
industrialization, imports of consumer goods were restricted and Turkish industries encouraged. The economy
prospered, aided by the many workers who found jobs in other European countries. Inflation again became a
problem. During the 1970s, private producers were encouraged to borrow from foreign lenders, with guarantees by
the government. The recession caused by increases in oil prices depleted Turkish reserves of foreign currency.
Short-term loans were necessary for day-to-day expenses. Inflation reached triple digits by 1979.
 Free-Market Reforms
In the 1980s, economic reforms promoted by Turgut Ozal shifted Turkey’s economy toward a more export-
oriented outlook. Changes included devaluation of the lira, control over money supply and credit, eliminating most
subsidies, and allowing prices charged by state industries to reach their market level. The balance of payments
improved, and government spending was reduced. A customs arrangement with the European Economic
Community, predecessor to the European Union, helped increase exports. Additional income came from charges
for pipeline services carrying oil from Iraq to the Mediterranean and from the growth of tourism. Although the
reforms caused many hardships at first, the Turkish economy rebounded. As the economy heated up,
unemployment and inflation became major problems by the end of the 1980s.
During the 1990s, excessive government borrowing and an overvalued lira led to an economic crisis that peaked
in the mid-1990s. Devaluation in 1994 led to renewed growth and foreign investment. By the early 2000s,
Turkey’s economy grew at an average annual rate of 5 percent. Key industrial sectors included clothing and
textiles, automotive, and electronics. The completion of a pipeline carrying oil from Baku to world markets in May
2006 also provided a major source of income. The Turkish economy was rocked in 2007 and 2008, with the world
recession causing a decline in exports. Domestic turmoil also led to uncertainty, as secular and moderate Islamic
political parties faced a challenge from Muslim fundamentalists. Aggressive steps by the government to encourage
foreign investment and reduce unemployment helped the economy recover in 2010, with GDP growing by an
impressive 8.9 percent.
Tim J. Watts
 
See also:  Emerging Markets;  Middle East and North Africa. 

Further Reading
Aydin, Zulkuf. The Political Economy of Turkey. London: Pluto, 2005. 
Metz, Helen Chapin. Turkey, a Country Study. Washington, DC: Federal Research Division, Library of Congress, 1996. 
Morris, Chris. The New Turkey: The Quiet Revolution on the Edge of Europe. London: Granta, 2005. 
UBS
 
UBS is one of the largest investment banks in the world, with branches and clients in many locations outside its
headquarters in Switzerland. Since its formation in 1998, UBS has been rocked by numerous scandals, and it
suffered great financial losses in the global recession that began in 2007. Thanks to restructuring and reductions
in the number of employees, UBS had returned to a measure of profitability by the second half of 2009.
UBS was created by the merger of two rival Swiss banks. The first was the Swiss Bank Corporation (SBC), which
was founded in 1854 by six private bankers in Basel. Known as the Bankverein, the entity was formed to provide
funds for increasing demands for credit from Swiss railroad and manufacturing companies. The bank became a
joint stock company known as the Basler Bankverein in 1872. The next year, the bank suffered significant financial
losses. The Viennese stock exchange collapsed, and borrowers defaulted on a number of large loans. As a result,
the Bankverein’s leadership refused to issue dividends to investors and adopted a conservative fiscal approach.
This caution in business served the bank well, and it prospered over the next quarter-century. Smaller Swiss
banks were acquired, and by 1897, the entity became the Schweizerischer Bankverein, known in English as the
SBC.
While World War I caused some difficulties, the end of the war offered new opportunities for SBC. Loans were
made to countries that were trying to rebuild from the war. Many wealthy foreigners also deposited funds with SBC
to protect their wealth from inflation. During the Great Depression, SBC helped support weaker financial
institutions. Foreign offices were opened, including one in New York in 1939, giving additional opportunities for
international investments. As World War II threatened, many Europeans placed their funds in the bank for safety.
Following the war, SBC concentrated on business with multinational corporations. Additional foreign offices were
opened in Europe, North America, and Asia. A less conservative approach to business, including asset
management, securities, and investment banking, was adopted during the 1990s, leading to greater profits but
also additional risks.
The other partner in the merger that created UBS was the Union Bank of Switzerland. Its roots lay in the Bank of
Winterthur, founded in 1872. Unlike SBC, the Union Bank of Switzerland concentrated its efforts on domestic
banking operations, including commercial and personal loans, mortgages, and leasing. Its first foreign office was
not opened until 1967, and international operations continued at a slow rate after that. During the 1990s,
stockholders became unhappy with the bank’s conservative leadership and low returns. Battles over the bank’s
direction resulted in a general weakening of the company.
In 1998, SBC and the Union Bank of Switzerland merged. Although the name originally was to be United Bank of
Switzerland, the corporation’s official name quickly became UBS. A logo of three keys, standing for confidence,
security, and discretion, was taken over from SBC. Although the original Union Bank of Switzerland had greater
assets, most leadership positions went to executives from SBC. When the bank was created, it was the second-

largest commercial bank in the world, with assets around $600 billion.
Despite its financial strength, the Union Bank of Switzerland suffered both financial and public relations disasters
since its creation. One of the most embarrassing and costly episodes had to do with the Holocaust. During the
1930s, after the Nazis came to power in Germany, many wealthy Jewish families deposited their funds in Swiss
banks, including the Union Bank of Switzerland, to keep them safe. While some individuals were able to escape
and reclaim their wealth, many more perished in the Nazi death camps. Following World War II, the Union Bank of
Switzerland, along with other Swiss banks, made no effort to contact the owners of dormant accounts. During the
1990s, Jewish groups accused the Union Bank of Switzerland of cooperating with the Nazis and profiting at the
expense of Holocaust victims. The Union Bank of Switzerland refused to make public the account owners’ names,
justifying this action on the basis of the traditional secrecy surrounding Swiss banks. Eventually, public opinion
convinced the Union Bank of Switzerland leadership to release the names of people who had opened accounts
before 1945—the assets totaled more than $41 million. Bank president Robert Studer further inflamed public
opinion by describing the amount as “peanuts.” The ensuing firestorm resulted in Studer’s ouster when the Union
Bank of Switzerland and SBC merged the following year.
The Union Bank of Switzerland also was embarrassed in 1997, when a security guard found employees shredding
files related to the bank’s dealings with Nazis. Such activities were forbidden, and criminal proceedings were
launched against the bank’s archivist. The security guard also was charged with violating bank secrecy. Both
prosecutions were dismissed in September 1997, but the public remained suspicious of the Union Bank of
Switzerland’s activities.
On June 22, 2008, the U.S. Federal Bureau of Investigation announced that it was investigating a tax evasion
case involving UBS. Up to 20,000 American citizens were accused of hiding funds with the bank to avoid paying
taxes. UBS was accused of marketing tax-evasion schemes to these citizens, with up to $20 billion being
deposited. On February 18, 2009, UBS agreed to pay a $780 million fine to the U.S. government and to reveal the
names of certain American depositors. The next day, the U.S. government filed suit to force UBS to reveal the
names of 52,000 depositors. Under a settlement reached in August 2009, U.S. depositors were granted a grace
period in which to report their activities and pay any back taxes and fines without facing criminal prosecution.
UBS, like many banks, was affected by the global recession that began in 2007. Bad loans and mismanagement
led to large losses that shook public confidence in the bank. Nearly 10,000 jobs were cut in all divisions worldwide.
In 2008, the bank was forced to write off $49 billion in bad investments, with a loss of $15.7 billion for the year. In
October 2008, UBS had to accept a bailout from the Swiss government to remain solvent. Losses continued
through 2009, as Europe and the world suffered through an economic downturn. In response, UBS put greater
emphasis on its traditional wealth management activities and turned away from risky investments—thus returning
to mild profitability in late 2009.
Tim J. Watts
 
See also:  Banks, Investment;  Switzerland. 
Further Reading
Bauer, Hans, and Warren J. Blackman. Swiss Banking: An Analytical History. New York: St. Martin’s, 1998. 
Schutz, Dirk. The Fall of UBS: The Forces That Brought Down Switzerland’s Biggest Bank. New York: Pyramid Media
Group, 2000. 
Vincent, Isabel. Hitler’s Silent Partners: Swiss Banks, Nazi Gold, and the Pursuit of Justice. New York: William
Morrow, 1997. 

Unemployment, Natural Rate of
 
The natural rate of unemployment occurs when production of goods and services reflect the full (normal) utilization
of an economy’s resources. Ultimately there is only one stable rate of unemployment, the natural rate. Deviations
from the stable rate come about when there are disturbances in the economy, as during recessions or inflationary
booms.
Even under the most balanced supply and demand “normal” conditions, the unemployment rate will not be zero.
Labor markets always have unemployment for several reasons. First, labor markets work imperfectly; it takes time
for unemployed workers to find new jobs and while they are moving from job to job, they are unemployed. This
type of unemployment is not bad but rather reflects the dynamic nature of economy. In addition, certain labor
market restrictions create some unemployment. Minimum wage laws raise the wages of some workers, but higher
minimum wages also discourage employers from hiring some workers. Minimum wage laws usually result in higher
unemployment for teenagers, and high teenage unemployment rates increase the average unemployment rate.
Also, some workers become dislocated when their skills become outmoded, such as happened to blacksmiths
when the automobile replaced horseback as the preferred way of transportation. Thus, unemployment caused by
imperfections and impediments in labor markets—when supply and demand are otherwise well balanced—add up
to the natural (stable) rate of unemployment.
The most important studies in the field of the natural rates of unemployment involve the relationship between
unemployment and inflation in an economy. This work shows that, all other factors being equal, inflation can cause
an economy to deviate from its natural rate of unemployment. In exploring these deviations, economists and
governments have attempted to devise policies by which governments can help mitigate the high (unnatural)
unemployment rates that occur during economic recessions.
 Phillips Curve
The understanding of how inflation can reduce unemployment below its natural rate began in 1958, when Alban
William Phillips published his study of the British economy over the previous century, “The Relationship between
Unemployment and the Rate of Change of Money Wages in the United Kingdom, 1861–1957.” According to this
study, there is a negative relationship between the rate of inflation and unemployment. That is to say, as the
prices of goods rise, unemployment falls. This relationship is summarized in what later became known as the
Phillips curve. Note that the original study related the unemployment rate and changes in wages. Because there is
a strong correlation between changes in wages and changes in prices (inflation), economists quickly substituted
inflation for changes in wages.
 Samuelson-Solow Theory
While Phillips’s work focused only on Britain in the second half of the nineteenth and first half of the twentieth
centuries, American economists Paul Samuelson and Robert Solow—both of whom won the Nobel Prize (in 1970
and 1987, respectively)—maintained that the Phillips study was not a special case but that it revealed a general
theoretical principle. In other words, Samuelson and Solow believed that there is always a tradeoff between
inflation and unemployment.
If they are correct, then governments can use inflation to control unemployment, that is, to reduce unemployment
below its natural rate. Central banks can increase the rate of inflation simply by putting more money in circulation.
As the government creates more money, the demand for goods rises and all prices increase. As the price of

consumer goods rises, real (i.e., inflation-adjusted) wages fall. This means that the money paid to workers will buy
less. Since inflation effectively cuts the wages of workers, employers will want to hire more employees. As
employers hire more employees, unemployment rates fall. Hence, higher inflation rates push unemployment below
its natural rate.
In the 1970s, economist Arthur Okun of the Brookings Institution refined the theory of the Phillips curve by shifting
the focus of decision-making from the employer to the worker. Okun pointed out that workers will accept job offers
more quickly if wages are increasing. Thus, workers search for jobs with a particular money wage in mind. If
inflation is driving up prices and wages, workers will accept job offers more quickly, also lowering unemployment
to below its natural rate.
 Phelps-Friedman Theory
Both the Samuelson-Solow and Okun explanations of the Phillips curve rely on the idea that workers do not notice
inflation. Milton Friedman and Edmund Phelps (Nobelists in 1976 and 2006, respectively) agreed that inflation
causes the unemployment rate to fall below its natural rate, but objected to the idea that workers are so easily
fooled. This proved to be a critical distinction with regard to policy options for influencing unemployment rates.
According to Phelps and Friedman, workers will not notice inflation immediately. Thus, workers can be fooled by
inflation, but only for a short period of time; in the long run, they will take note of inflation. Once they do so, they
will push for wage increases to keep up with rising prices or reduce the amount of labor they are willing to supply.
Likewise, if employers pay out inflation-adjusted wages, they will hire fewer employees. Thus, inflation-adjusted
wages raise the unemployment rate toward its natural level. What Phelps and Friedman argued is that workers
adapt to inflation later on, when its negative effects become obvious. Consequently, the only way government can
keep unemployment below its natural rate is by increasing inflation over and over again and at an increasing rate.
Thus, once workers get used to 5 percent inflation, policy makers would have to take actions to increase inflation
by 10 percent to reduce the unemployment rate below its natural rate. This policy would lead to massive inflation
in the economy. In fact, in the United States during the 1970s, unemployment rates rose at the same time that
inflation rates rose. Many economists see this data as proof that Phelps and Friedman were right about inflation
and unemployment.
 Lucas Theory
Phelps and Friedman argued that the Philips study revealed only a temporary tradeoff between inflation and
unemployment. Robert Lucas, a student of Milton Friedman, took his ideas about the natural rate of
unemployment a step further, arguing that workers learn to anticipate inflation. In the Friedman-Phelps theory of
natural unemployment, workers adapt to inflation only after it has been around for a while. Government authorities
can therefore use inflation to achieve temporary reductions in unemployment below its natural rate. But Lucas
argued that workers will learn to demand wage increases for inflation before it actually happens. If that is the
case, then the government cannot use inflation to gain even temporary reductions in unemployment. What Lucas’s
theory implies is that the natural rate of unemployment prevails not only over the long term, but in the short run as
well.
Lucas does not maintain that unemployment is always at its natural rate, but his theory does imply that the
government cannot systematically reduce unemployment with inflation. Unemployment will rise above its natural
state and fall below its natural state with random events in the economy, but the government cannot be relied
upon to use inflation to counteract this problem. Nearly all economists accept either the Lucas theory or the
Phelps-Friedman theory.
D.W. MacKenzie
 
See also:  Employment and Unemployment;  Samuelson, Paul;  Wages. 

Further Reading
Fisher, Irving.  “I Discovered the Phillips Curve: ‘A Statistical Relation between Unemployment and Price Changes.’” Journal
of Political Economy 81:2 (1973): 496–502.  Reprinted from 1926 edition of International Labour Review.
Friedman, Milton.  “The Role of Monetary Policy.” American Economic Review 68:1 (1968): 1–17. 
Frisch, Helmut. Theories of Inflation. New York: Cambridge University Press, 1983. 
Lucas. R.E.  “Adjustment Costs and the Theory of Supply.” Journal of Political Economy 75:4 (1967): 321–334. 
Okun, Arthur. Prices and Quantities: A Macroeconomic Analysis. Washington, DC: Brookings Institution, 1981. 
Phelps, E.S.  “Phillips Curves, Expectations of Inflation and Optimal Unemployment Over Time.” Economica 34(1967): 254–
281. 
Phillips, A.W.  “The Relationship between Unemployment and the Rate of Change of Money Wages in the United Kingdom,
1861–1957.” Economica 25:100 (1958): 283–299. 
Samuelson, P.A., and R.M. Solow.  “Analytical Aspects of Anti-Inflation Policy.” American Economic Review 50:2
(1960): 177–194. 
Tobin, James.  “Inflation and Unemployment.” American Economic Review 62(1972): 1–18. 
 
United Kingdom
 
Birthplace of the industrial revolution in the late eighteenth and early nineteenth centuries, the United Kingdom
(UK) is a medium-sized island nation of about 60 million people occupying most of the British Isles in
northwestern Europe. Once part of the Roman Empire and then conquered by the Normans of France in the
eleventh century CE, the modern United Kingdom was created in the early eighteenth century with the political
union of England (including Wales) and Scotland, and the addition of Ireland about a century later.
Already a rising power, Great Britain built the most extensive sea-based empire in history by the late eighteenth
century, a period in which it also pioneered the industrial revolution. By the nineteenth century, the country had
the largest economy in the world. But the rise of rivals, such as Germany and the United States, as well as two
world wars, undermined Britain’s dominant position and, by the early post–World War II period, it had shed most
of its empire and had fallen behind economically, held back by an aging infrastructure.
In the 1980s, the country embarked on major free-market reforms, including privatizing much of the state-owned
industrial infrastructure, reducing the power of unions, and deregulating finance. The result was an economic
resurgence, albeit marked by increasing inequities in wealth. As one of the world’s leading financial centers and
with its own housing bubble, Great Britain was hard hit by the financial crisis of 2008–2009 and the accompanying
recession.

A proponent of retaining the British pound writes “No EMU” (European Monetary Union) during the “Give a Pound
to Save Our Pound” campaign in 1998. As a member of the European Union, Great Britain could join the euro but
opted against doing so. (Sinead Lynch/AFP/Getty Images).
 Economic History Through the Industrial Revolution
Settled as far back as 35,000 BCE, Britain was home to various iron-smelting cultures by the first millennium BCE,
at the end of which most of the area was conquered by Rome. With the fall of the Roman Empire in the fifth
century CE, the British Isles were subject to invasions by various seafaring peoples, and finally conquered by the
Normans of France in 1066. By the sixteenth century, most of what is now England had been united under the
Tudor monarchy, which began tentative empire building outside Europe and removed England from the Catholic
Church.
Torn by civil war and ruled briefly as a republic in the mid-seventeenth century, England began to assert its
political hegemony over Scotland in the north and Ireland across the Irish Sea, officially uniting with the former in
1707 and the latter in 1801. It was also during this time that Britain began to dramatically increase the size of its
empire, despite losing its thirteen colonies in North America. By the end of the nineteenth century, Britain ruled
lands on every continent, with sovereignty over roughly one in four of the world’s people.
World War I and World War II sapped Britain’s capacity to rule its far-flung empire. That burden, along with rising
nationalist ambitions in colonies, led to the dismemberment of the empire. This process had already begun in the
late nineteenth and early twentieth centuries, with independence granted to the settler colonies of Australia,

Canada, and South Africa. At the end of World War II, Britain granted independence to its prize colony, India, and,
from the late 1950s through the 1970s, saw virtually all of its colonies in the Americas, Africa, and Asia become
independent. That process culminated with the return of Hong Kong to China in 1997.
Britain’s rise and fall from a land of feuding tribes to unified nation to great empire to former imperial power is
evidenced in its economic history. The British Isles were first incorporated into a larger economy with their
conquest by Rome, becoming a major exporter of grain and woolen cloth, the latter a product that would remain
an important export into the early modern era. With the collapse of the empire in the fifth century, the British Isles
reverted to a local barter economy, with the little remaining manufacturing activity centered in monasteries.
With the revival of trade in Europe at the beginning of the second millennium CE, Britain once again became a
major exporter of wool and woolen cloth to the continent. By the thirteenth century, a number of English towns
from York in the north to Exeter in the south became cloth-producing centers, with London and other port cities
thriving on the wool trade.
While causing great immediate suffering and economic dislocation, the Black Death, which reduced the population
of the British Isles by as much as one-third in the fourteenth century, had a positive long-term effect on England’s
economy, according to many historians. By reducing the number of laborers, it gave those who survived higher
wages and more freedom, creating greater balances in power and wealth among the different classes. The Black
Death thus contributed to the fall of the feudal order, in which semi-enslaved peasants, or serfs, were tied to the
land and subjected to the total control of local lords; the end of feudalism allowed more geographic and social
mobility.
A second revolution in the English economy occurred in the late seventeenth and early eighteenth centuries with
the so-called enclosure movement, in which traditional forms of landholding and farming practices were replaced
with more commercial and efficient ones. The process led to more food production even as it displaced many
peasants from the land, sending them to towns and cities. Together, these events created the workforce for a new
economy based on manufacturing even as they allowed that new workforce to be fed relatively cheaply.
The expansion of empire in the seventeenth and eighteenth centuries also allowed for more overseas trade, which
the British government tried to control through mercantilist policies that ensured its colonies would remain captive
markets for the mother country’s manufactured goods. This trade—as well as less expensive agricultural products
—increased the prosperity of the English people, creating greater demand for manufacturing goods such as cloth
and shoes. This, in turn, created the impetus for more efficient forms of manufacturing, leading to the factory
system of production and the introduction of labor-saving machinery, some of it powered by the newly invented
steam engine. By the early nineteenth century, the steam engine had been harnessed for transportation, creating
the beginnings of a railroad network that would make for a more integrated national market.
By the mid-nineteenth century, the United Kingdom had become manufacturer to much of the world, producing
great capital reserves that were then invested both at home and abroad. By the end of the century, Britain had
also emerged as the world’s largest foreign investor as well.
 War, Loss of Empire, and Economic Decline
Even as the British Empire and the British economy were reaching their zenith, new competitors were emerging,
most notably Germany and the United States. With larger internal markets (especially true of the United States)
and more modern equipment, these two countries gradually came to surpass Britain as manufacturers—the United
States by century’s end and Germany just before World War I.
The Great War had a profound effect on the British economy, costing Britain many of its most important markets
in continental Europe. In addition, it found itself in great debt, much of it owed to the United States, which
prevented it from investing in new capital equipment. As its industry became less productive than that of other
countries, such as the United States, it lost out in other markets around the world. While there was increasing

social equality in Britain after World War I, there was also much unemployment, even as the state began to offer
new services, such as housing and medical subsidies, old-age pensions, and unemployment benefits.
Such measures helped Britain escape the worst effects of the Great Depression. While unemployment jumped to
18 percent by 1932, the nation’s overall economy recovered faster than that of the United States and France, with
production some 20 percent higher in 1937 than it had been on the eve of the Wall Street crash of 1929.
While World War II brought great destruction to Britain’s industrial infrastructure and the immediate postwar era
saw a weakened mother country give in to nationalist movements around the world and grant independence to the
majority of its colonies in Africa, the Americas, and Asia by the 1970s, Britain prospered in the 1950s and early
1960s. Wartime rationing generally ended by the mid-1950s as the economy expanded alongside that of its
continental neighbors, as part of the overall boom in Western Europe. Years of shortages had created much pent-
up demand even as the government began to spend more on health care, infrastructure, education, and other
public services.
For all the gains, there were fundamental flaws in the British economy that began to put a drag on growth by the
latter half of the 1960s. The industrial infrastructure was aging, which reduced productivity compared to other
industrial powers. The loss of colonies meant a reduced external market for its exports. Many state-run
enterprises were inefficient, and unions exerted great sway over the economic decision-making of both
government agencies and businesses, making it difficult to close down money-losing factories and mines. The oil
shocks of the 1970s only added to the economic malaise, leading voters to abandon the Labour Party, which had
built the social welfare state of the postwar era, for the more market-oriented Conservative Party, led by Margaret
Thatcher, in 1979.
 Thatcher and Free-Market Reforms
Over the next decade, Thatcher’s government succeeded in changing the face of the UK. Privatization, battles
with the trade unions, and economic policy based on monetarism became the new hallmarks of British domestic
affairs. Many economists agreed that the UK needed a reality check and that its traditional industries,
manufacturing and mining, were no longer competitive. Instead, there was an increased emphasis on the service
industries, in particular financial services. The financial services industry was deregulated; loans and mortgages
became easier to obtain, and a round of tax cuts led to economic boom times.
The success of the Thatcher government in bringing down inflation and unemployment while generating economic
growth came to a grinding halt in the late 1980s. In his efforts to conquer inflation, Chancellor of the Exchequer
Nigel Lawson had pinned his faith on linking the UK pound with the German deutschmark. By shadowing the
deutschmark, Lawson hoped to impose some monetary discipline on British businesses. If the prospect of
currency depreciation were removed, he reasoned, then firms would be forced to become more competitive by
controlling costs and increasing productivity.
The logical conclusion to this policy was for the UK to join the Exchange Rate Mechanism (ERM), which it did in
October 1990. The pound was allowed to float within a range of £1 = DM2.83 and DM3.13. At the time of entry,
UK inflation was running at 10.9 percent; wages were rising at 10 percent; and unemployment, after 44 months of
successive falls, started to rise again to over 1.6 million. Interest rates stood at 14 percent, having been cut by 1
percent in October 1990.
In the mid-to late 1980s, a strong housing market had led many people to take the risk of buying near the peak of
the market. However, the need to maintain high interest rates finally began to impact borrowing and investment.
Britain’s gross domestic product (GDP) fell by 0.77 percent between 1989 and 1990, and then by 0.6 percent
between 1990 and 1991.
The number of house repossessions rose dramatically: 247,000 from 1990 to 1993. Unemployment rose to 2.6
million by the end of 1991, and inflation stood at 4.6 percent. In the face of the recession, the government had

only limited room to maneuver given its membership in the ERM. Any major cut in interest rates to stimulate the
economy would have put pressure on the sterling and risked it moving out of its range to the deutschmark. The
government cut interest rates to 10.5 percent by September 1991, and the sterling ended the year near its lowest
level.
The year 1992 brought a continuation of the recession. Economic growth was still negative, and unemployment
reached 2.87 million, with 1,200 businesses closing every week. Inflation fell to 3 percent, helped by the cuts in
interest rates. In September, the sterling began to be sold heavily and fell below its ERM floor; the government
attempted to prop up the value of the pound by increasing interest rates, which it raised as high as 15 percent. On
the evening of September 16, 1992, dubbed Black Wednesday, Chancellor of the Exchequer Norman Lamont
announced to the press that the UK was withdrawing from the ERM. Speculators seemed to know that the
government simply did not have the funds to support the pound indefinitely, and keeping interest rates at such a
high level would have exacerbated the recession.
The withdrawal of the UK from the ERM marked a major change in economic policy. The recession had reduced
inflationary pressures, but the cost in terms of high unemployment and business failures was significant. At the
end of 1992, the total number of failed businesses had reached 61,767 for the year. Without the constraints of the
ERM, Lamont cut interest rates to 7 percent, which heralded the start of a slow recovery. In 1993, GDP rose by
1.75 percent, but unemployment, ever the lagging indicator, peaked at over 3 million. Inflation also fell rapidly,
dropping to 1.2 percent by the end of the year.
The government faced additional problems as a result of the recession. Tax revenue fell as unemployment
continued to rise, business profits fell, and consumer spending slowed markedly. At the same time, spending on
public benefits increased. Government borrowing rose to £50 billion. To address this problem, the government
increased the value-added tax (VAT) on fuel and power, and then made changes to mortgage tax relief and
personal allowances that had the effect of adding to the income-tax burden. Interest rates, however, were reduced
to 5.5 percent by the end of 1993, providing some relief for homeowners and encouraging borrowing by both
businesses and individuals.
The recovery continued to gather pace in 1994, with GDP growth at 4 percent; inflation remained manageable at
just over 2.0 percent, and unemployment fell to under 2.5 million. The tax increases also started to have their
effect on public borrowing, which fell to £40 billion. Throughout 1994 and 1995, while the key economic variables
seemed to be moving in the right direction, the problem for the government was that the feel-good factor seemed
to be missing in the minds of many of the population. In 1995, GDP growth was 2.5 percent, unemployment was
falling, and retail sales were flat, but there were signs that consumer credit was starting to rise.
The same trends continued into 1996. House prices rose at an annual rate of about 5 percent, and unemployment
fell to below 7 percent of the workforce. The inflation rate of 2.8 percent exceeded the government’s target of 2.5
percent, but it was still a significant improvement from the levels of the late 1980s and early 1990s. Consumer
confidence finally seemed to be picking up, but there were concerns that much of the spending was being
financed by credit.
 Labour Government and Economic Boom
The general election of May 1997 returned the Labour Party to power. One explanation for the cycles of boom
and bust in the United Kingdom over the previous thirty years was the fact that the power to change interest rates
lay in the hands of the government. There was always a temptation to manipulate interest rates for political gain,
as opposed to pure economic reasons. Reducing interest rates at the wrong time, however, could have a direct
effect on inflation and economic growth, leading to knee-jerk reactions rather than considered policy. The
temptation to reduce rates before an election—whatever the state of the economy—was, it was argued, too great.
The inevitable consequence was that government would have to raise taxes or interest rates—or both—in the
aftermath of an election. This, in turn, led to the lurches in economic activity that characterize boom-and-bust
economies.

The answer, it was determined, was to put decision-making power regarding interest rates in the hands of the
Bank of England. In the first week of the new government, the Bank was given operational authority over
monetary policy. The chancellor of the exchequer set a target for inflation, and it was the responsibility of the
Bank of England to adjust interest rates to help meet this target. A meeting of the Monetary Policy Committee
(MPC), made up of members of the bank itself and external members (nine in all), would be held every month to
review the prospects for inflation and make the decision about interest rates. If inflation rose above its target level
by more than 1 percent, the governor of the Bank of England was required to write an open letter to the chancellor
giving an explanation and an outline of the strategy to bring inflation back under control.
One of the main benefits of this system would be to influence inflationary expectations. The rational expectations
model had gained some currency in economic theory. If businesses and individuals knew that the Bank of England
would keep a firm hold on inflation—raising interest rates in the face of rising inflationary expectations and
lowering them to keep inflation at target level in times of economic slowdown—decision-making would be affected.
No more would interest rate changes be at the whim of a chancellor seeking electoral popularity.
The initial test for the bank came when inflation rose above the target level of 2.5 to 3.7 percent. In response, the
MPC raised interest rates to a seven-year high of 7.25 percent by the end of 1997—by which time unemployment
had fallen to around 1.4 million and growth stood at 3.1 percent. In many respects, then, the improving economic
climate inherited by the new exchequer, Gordon Brown, was largely due to the policies of his Conservative
predecessors. Brown’s challenge was to maintain that momentum. In his first budget, in 1997, he announced that
the government would set in place a number of fiscal rules to guide policy. The “Golden Rule” stated that there
would be a commitment that over the period of the economic cycle, government borrowing would be made only to
finance investment and not to fund current spending. This meant that the burden of current spending would be
shouldered by those who would benefit from it—current taxpayers rather than future taxpayers.
The other major fiscal rule Brown introduced was the so-called sustainable investment rule, according to which
public sector debt as a proportion of GDP would be held at a prudent level, or below 40 percent. In addition,
Brown cut corporation taxes, the tax on business profits, and the VAT on gas and electricity and imposed a
windfall tax on privatized utilities that would raise £5.2 billion over two years from twenty-eight companies.
Over the next ten years, Brown presided over a period of unprecedented economic stability. Unemployment fell to
below 1 million, and inflation remained within the target range redefined in 2003 at around 2 percent. The Bank of
England varied interest rates by only a quarter point on the vast majority of occasions it felt the need to change
them. Economic growth remained positive throughout the first ten years of the Labour government.
 Increased Speculation and Financial Crisis
Improved public finances meant there was room to cut income taxes and increase public spending, including
heavy increases for health and education. Consumer confidence was high and the housing market, in particular,
was booming. The effects of financial deregulation back in the 1980s meant that banks and building societies
competed to provide ever more attractive mortgage products. Money was being lent under ever more generous
terms, and the housing market continued to strengthen. Although some cautioned that the housing market was
bound to crash, their worries appeared ill founded as home prices continued to rise.
The use of credit to finance purchases also continued to expand. The initial signs that consumers were using the
increased range of credit facilities being offered by banks and financial institutions began in the mid-1990s under
the Conservative government. By 2008, UK consumers had built up credit debt of £1.4 trillion. One of the reasons
put forward for the buildup was that homeowners were able to use the equity in their properties as security for
additional borrowing. The availability of credit and debit cards also made it much easier to spend now and worry
later. All in all, public finances had improved, inflation seemed well under control, and years of continued economic
growth seemed to breed the expectation that the good times could only continue.

The global financial meltdown of 2007–2008 showed all too clearly that this was not the case. The United
Kingdom, whose financial-services sector had helped devise and market many of the more exotic credit swap
derivatives and mortgage-backed securities that were at the heart of the crisis, was especially hard hit. At first, the
British government, now led by Prime Minister Gordon Brown, seemed to take the lead in confronting the crisis,
rescuing a collapsing Northern Rock Bank, one of the country’s largest financial institutions, and offering huge
amounts of government capital to shore up the financial system.
Soon, however, the crisis proved too much for even these aggressive measures. The underlying problems were
simply too great. One of the problems had to do with housing. During the boom years of the 1990s and especially
during the early and middle 2000s, the United Kingdom had seen a dramatic increase in housing prices, which
severely deflated again with the contraction of the credit markets in the wake of the crisis.
Indeed, the credit crunch went beyond the housing sector. Household debt had grown dramatically in the 1990s
and 2000s, as Britons tried to maintain their standard of living despite falling exports and a growing trade
imbalance with the rest of the world. Moreover, the British economy had been boosted by significant inflows of
capital as London came to rival New York as the world’s financial capital. But this, too, shrank in the wake of the
financial crisis and the freezing up of the credit markets beginning in late 2007 and accelerating in 2008.
Large amounts of household debt, rising foreclosure rates, increasing unemployment, and the decline in the critical
financial sector of the economy all sent the United Kingdom into its first recession in more than a decade. In the
second quarter of 2008, the nation’s economy began to contract for the first time since the recession of the early
1990s. By the first quarter of 2009, the contraction had reached 2.5 percent, though it eased to just 0.5 percent in
the second quarter. The worst appeared to have passed, as the economy emerged from recession in the fourth
quarter of 2009, albeit with an anemic growth rate of just 0.3 percent.
In May 2010, British voters ousted the Labour Party and replaced it with a coalition of Conservatives and Liberal
Democrats headed by David Cameron. Cameron’s government began quickly sweeping austerity measures into
place. Critics blamed these measures for the country’s anemic growth rate of 1.3 percent in 2010, though
supporters said it was necessary to restore Britain’s fiscal balance. On December 9, 2011, the Cameron
government cast the only negative vote on a proposed revision of the European Union treaty that would have
imposed tighter budgetary discipline on individual members states and thereby help alleviate the sovereign debt
crisis spreading across the Eurozone. Although Cameron felt that domestic anti-Euro sentiment barred him from
supporting the measure, the political opposition warned that the decision would isolate and marginalize Britain
from the EU, to its own detriment.
Andrew Ashwin and James Ciment
 
See also:  France;  Germany;  Italy;  Northern Rock. 
Further Reading
Alford, B.W.E. Britain in the World Economy Since 1880. New York: Longman, 1996. 
“Banking Crisis Timeline: How the Credit Crunch Has Led to Dramatic, Unprecedented Events in the City, on Wall Street
and Around the World.” The Guardian, October 30, 2008. 
Chapman, Stanley. Merchant Enterprise in Britain: From the Industrial Revolution to World War I. New York: Cambridge
University Press, 1992. 
Dellheim, Charles. The Disenchanted Isle: Mrs. Thatcher’s Capitalist Revolution. New York: W.W. Norton, 1995. 
Lawson, Nigel. The View from No. 11: Britain’s Longest-Serving Cabinet Member Recalls the Triumphs and
Disappointments of the Thatcher Era. New York: Doubleday, 1993. 
Romano, Flavio. Clinton and Blair: The Political Economy of the Third Way. New York: Routledge, 2006. 

Roy, Subroto, and John Clarke, eds. Margaret Thatcher’s Revolution: How It Happened and What It Meant. New
York: Continuum, 2005. 
Trentmann, Frank. Free Trade Nation: Commerce, Consumption, and Civil Society in Modern Britain. New York: Oxford
University Press, 2008. 
 
United States
 
With the world’s largest gross domestic product (GDP) and most diversified economy, the United States—
population just over 300 million—boasts a major manufacturing infrastructure, the world’s most productive
agricultural system, a well-developed service sector, and a globally influential financial sector. In the years
following World War II, the United States emerged as the world’s largest importer and exporter and the dollar
became the world’s key currency.
The origins of the modern United States date back to the establishment of European colonial settlements in
eastern North America in the early seventeenth century. After achieving independence from Great Britain in the
late eighteenth century, the United States embarked on a vast geographic expansion that ultimately would span
the North American continent.
In the nineteenth century, the nation became one of the pioneers of the industrial revolution, creating a major
manufacturing and transportation infrastructure that, by century’s end, would render the country the world’s largest
economy. Victory in two world wars in the first half of the twentieth century helped turned the United States into a
military and geopolitical superpower with strategic interests around the globe.
With its entrepreneurial dynamism and high social mobility, the United States has enjoyed buoyant economic
growth through much of its history but at the price of great economic volatility, with booms and busts marking the
period from the early nineteenth through the early twenty-first centuries. Most recently, the United States—
particularly its housing and financial sectors—has been at the epicenter of the crisis that brought down the world’s
financial markets in 2008.
For much of its history, the United States has adhered to a more laissez-faire approach to economic policy than
many other industrialized countries, leaving the free market to allocate economic resources and eschewing the
elaborate welfare system erected in many European countries in the twentieth century. Still, it has erected a
regulatory infrastructure—enhanced since the Depression of the 1930s by a slew of social welfare programs—
designed to smooth out and ease the impact of the boom-and-bust capitalist cycle.
 Colonial Era, Revolution, and Constitution
What is now the United States of America has been home to indigenous peoples since at least 15,000 BCE, and
by the time of first contact with Europeans in the sixteenth century, a number of different kinds of cultures had

emerged, with some tribes practicing agriculture and others existing as hunters and gatherers. In many areas,
elaborate trading networks had been established. In the Southwest, for example, native peoples, some of whom
had built small-scale urban settlements, traded as far away as the Aztec Empire of central Mexico.
Aside from a few Spanish settlements in the Southwest and Florida, the first European colonizers came from
Northern Europe in the early seventeenth century—primarily from England, though the Dutch settled what would
later become New York. The colonies they founded varied significantly, from the religious settlements of New
England, to the commercial agriculture and trading colonies of New Netherland (later New York) and
Pennsylvania, to the plantation agriculture–based colonies of Virginia and the Carolinas.
By the eighteenth century, the thirteen British colonies had thriving and diversified economies (the Dutch were
ousted from New York by the 1670s). New England was a center of fishing, whaling, and trade, while the mid-
Atlantic colonies boasted the major trading ports of New York, Philadelphia, and Baltimore, where grains and other
agricultural products of the hinterland were exchanged for manufactured goods from Europe. Most lucrative of all
were the southern colonies. Utilizing slave labor imported from Africa and the Caribbean, Virginia became a major
exporter of tobacco, while South Carolina and Georgia shipped out ever growing quantities of rice and cotton.
Under British mercantilist policy, the colonies were meant to be suppliers of raw materials for the mother country
and to serve as a captive market for English manufactured goods. But the policy was honored more in the breach
through the mid-eighteenth century when, to pay the costs of imperial defense, the British began imposing higher
taxes and tighter restrictions on colonial trade. The impositions rankled many colonists, particularly influential
merchants in the North and planters in the South.
Leading a coalition that would eventually encompass most of the white colonial population, patriotic leaders first
petitioned the British government to change its policies and allow colonial representation in Parliament. Then,
upon being turned down, they declared independence from Britain while launching an armed insurrection. After six
years of often bitter fighting, the thirteen colonies defeated the much more powerful British military, signing a
peace treaty in 1783 that gave the new United States of America control not just of the Atlantic seaboard but of
most of territory south of the Great Lakes and east of the Mississippi River.
While the new republic had much in its favor economically—vast and rich farmlands in the North, a lucrative
plantation system in the South, thriving ports, a well-developed artisan manufacturing system, and an
entrepreneurially minded mercantile community—it was plagued with problematic public finances. The war had
impoverished the country, ruined its currency, and saddled both the central and individual colonial governments
with enormous debts. Adding to the new nation’s woes, in the opinion of many of the republic’s early leaders, was
the weakness of the central government, which, because it had virtually no ability to raise revenues, could not
deal with these many economic problems. In 1787, proponents of a more powerful central government fashioned
a Constitution that granted far greater authority to the federal government, though still leaving a significant amount
of policy-making power to the individual states.
A great debate then ensued about what kind of national economy should be promoted. Southern planters, led by
Secretary of State Thomas Jefferson, argued for an economy based on small-scale farming, limited manufacturing,
a minimal financial sector, and a small, low-taxing, low-debt government, especially at the federal level. Arrayed
against Jefferson and his allies were the merchants of the North, led by Treasury Secretary Alexander Hamilton, a
key confidant of President George Washington.
Interpreting the Constitution loosely—meaning that its mandate to “promote the general welfare” should be
understood as allowing government to expand its role in setting economic policy—Hamilton argued for a plan that
would allow the central government to assume and pay off state debts, thereby putting it on sound financial
footing, and establish a central bank to regulate the nation’s currency and financial system. Hamilton ultimately
won the day, though his plan for using the federal government to encourage manufacturing was resisted by those
who believed America’s economic future lay with agriculture and trade.

 Booms and Busts of the Early Republic and Antebellum Eras
Despite the setbacks to trade caused by the Napoleonic Wars in Europe—which triggered a brief but devastating
trade embargo on the part of the Jefferson administration in 1807—the U.S. economy prospered during the first
two decades of the nineteenth century. With improvements in transportation, particularly in the form of canals,
more farmers turned to raising commercial crops, selling them on the open market for cash to buy manufactured
goods and foodstuffs, such as sugar or coffee, which they were unable to grow themselves. There was also an
expansion in domestic manufacturing, particularly in New England, where the first water-powered, machine-driven
textile mills began to open as early as the 1790s. Increasingly, the United States was developing a national
marketplace for goods, a process aided by a series of Supreme Court decisions in the early nineteenth century
upholding contracts and limiting the power of state governments to regulate trade.
With the end of European and transatlantic war in 1815, the economy took off; trade increased and manufacturing
capacity expanded significantly. As wages and prices rose, a speculative real-estate frenzy swept many parts of
the country, particularly in the West, fueled by easy credit as state banks issued far more bank notes—a form of
currency—than their assets suggested they should. When manufacturing and trade began to fall off in 1818 and
prices dropped, panic set in as speculators tried to get rid of their real-estate holdings in order to meet debt
obligations. The result was the Panic of 1819, the first national financial panic in U.S. history and one that sent
unemployment and insolvency proceedings soaring. (Under the Constitution, only Congress could pass bankruptcy
legislation; after a brief experiment in 1800, it declined to do so until 1841.)
By the mid-1820s, the national economy had revived, spurred by continued improvements in transportation,
increasing factory output, and westward expansion. A key development was the completion of the Erie Canal in
1825, linking the Great Lakes to the Atlantic Ocean and turning New York City, where the Hudson River/Erie
Canal waterway met the ocean, into the nation’s preeminent port and financial center. This period also saw an
increase in the number of corporations, as investors began to pool their money in order to undertake projects
whose capital requirements were too great for any single entrepreneur to meet. Further, corporations offered the
protection of limited liability, so that, should they fail, the stockholders were liable only for the money they had
invested, leaving their personal fortunes intact. Because corporations offered this protection and because they
possessed great financial power, they were limited at first, with investors required to petition state legislatures for
charters that often had time limits and other restrictions. Meanwhile, laws against imprisonment for debt were
overturned, lifting the fear of jail time from the minds of risk-taking entrepreneurs.
As business was expanding, so, too, was democracy. By the 1820s and 1830s, virtually all property restrictions
on voting had been removed, granting nearly all white adult males the franchise. This development led to
increasingly populist-tinged politics, culminating in the 1828 victory of the war hero Andrew Jackson, who
campaigned as a man of the people who would fight the economic elites of the major eastern cities. Among his
pledges was to do away with Hamilton’s creation, the Bank of the United States. (Actually, it was the Second
Bank of the United States, the re-chartered heir to Hamilton’s original institution.) In 1832, Jackson vetoed the bill
that would re-charter the institution—though the bank would not close until 1836—thus ending federal regulation of
the nation’s money supply and financial system for the rest of the century.
The bank’s closing could not have come at a worse time. During much of the 1830s, yet another speculative real-
estate bubble had been inflating as state banks, many of them poorly capitalized, offered easy credit to purchase
lands in the ever-expanding western states and territories. Prices skyrocketed for rural land and especially for
plots in future cities, many of them nothing more than plans on paper. But with the closing of the Bank of the
United States, credit suddenly dried up, prices collapsed, bankruptcies multiplied, and unemployment soared in a
new panic—beginning in 1837—that was even more economically devastating and long lasting than the one of
1819. Indeed, its effect would continue to be felt into the early 1840s. In response to the Panic of 1837, many
states decided that the solution lay in more competition. Legislatures passed new bank chartering laws that
allowed anyone who met certain capital requirements and followed certain rules regarding how much they loaned
against assets to open a bank. Previously, persons interested in starting a bank had to petition the legislature for
a special charter, a process that often restricted the granting of this lucrative opportunity to political insiders.

Again, the economy expanded dramatically in the late 1840s and 1850s, as manufacturing centers began to
emerge outside New England, linked to national and international markets by a rapidly expanding railroad system,
itself a source of much economic investment. Indeed, despite the ever gloomier political climate—as sectional
rivalries over slavery threatened secession—the economy largely boomed right up to the opening salvoes at Fort
Sumter in 1861, the first battle of the Civil War. There had been a sharp panic and economic downturn in 1857,
triggered by an investor loss of confidence in the financial system, the failure of a major Ohio bank, and the loss
to shipwreck of a major gold shipment from California. Although short in duration, the Panic of 1857 did point out
one fact: the country’s reliance on foreign, and especially British, capital to fund its economic expansion. The
withdrawal of British capital from U.S. banks is what most historians cite as the panic’s single most important
cause.
The California gold rush of the late 1840s and early 1850s gave rise to boomtowns such as San Francisco and
Sacramento, to an influx of wealth seekers from across America and overseas, and to the reality of riches for
some. (The Granger Collection, New York)
 From the Civil War to the 1920s
While the Civil War was primarily about slavery and race, other economic factors were involved. Many
Southerners, particularly the planters who led the charge to war, were angry that while they produced the cotton
and other commercial crops that earned the foreign capital that helped make the nation’s dramatic economic
expansion possible, most of the profits—and the economic development made possible by those profits—accrued
to merchants, traders, bankers, and other middlemen of Northern cities such as New York, Boston, and
Philadelphia. By becoming independent, the South hoped to be able to exert more control over its economic

destiny.
The Civil War had a more profound effect on the economy than any other single event in nineteenth-century
American history, disrupting internal trade and finance, closing off regional markets, and halting the flow of
commodities. Southern producers of agricultural goods such as cotton, sugar, and tobacco were hit particularly
hard and would never fully recover. Some Northern businesses, particularly textile mills that relied on cotton from
the South, also suffered. Merchants and banks were hurt as well, since they could not recover their debts from
Southerners.
Soon, however, much of the economy had recovered from the initial shock, spurred by unprecedented government
spending on war supplies. The war revived the fortunes of the financial industry, as the government issued vast
quantities of bonds through New York financiers. Free from the resistance of Southern legislators, Congress
passed a series of laws favoring Northern businesses and farmers, including acts to build a transcontinental
railroad and to offer free western lands to homesteading farmers. Indeed, agriculture also got a short-term fillip
from the war, as Northern farmers not drafted into or volunteering for the army prospered as food prices climbed
due to shortages and demand. A lack of farm labor also encouraged the use of labor-saving agricultural
machinery, increasing output and creating demand among manufacturers of this equipment.
The war also accelerated economic divisions. Because the government needed to ramp up production quickly, it
sought out manufacturers capable of meeting the demand, contributing to the growth of the kinds of large-scale
businesses that would dominate the economy through the rest of the nineteenth century. At the same time, rising
prices led to demand for higher wages and encouraged the growth of a labor movement that would come to blows
with big business time and again during the late nineteenth and early twentieth centuries.
With the Union victorious in 1865, the country was poised for another period of rapid expansion. Between 1865
and 1913, the United States underwent the largest economic expansion in human history to that time, as the
gross national product (GNP) climbed more than 700 percent, from less than $7 billion in the former year to about
$50 billion in the latter. By 1900, the United States had come to surpass Great Britain as the largest economy in
the world.
The economic growth was fueled by several factors. One was heavy manufacturing, as a second wave of
industrialization threw up vast steelmaking works in the Midwest and a ready-to-wear clothing industry in the
Northeast, a food processing industry in the Midwest, and manufacturing plants of various types throughout the
country. Even the economically backward South participated, as textile manufacturers moved to the region to take
advantage of its cheaper labor. The country also became increasingly urbanized (the number of people in towns
would come to surpass those living in rural areas by the 1920 census), creating a population of people whose
consumer demand fueled economic growth. The national population also skyrocketed, both because of high birth
rates (though they were actually falling slightly), improved mortality rates, and massive waves of immigrations. A
larger population meant a larger domestic market, contributing further to economies of scale.
But the economic boom of the period was not without its problems and setbacks. First, there were the growing
inequalities in wealth between those who owned the means of production and those who came to work for them.
Such disparities often led to widespread and sometimes violent labor unrest, particularly during economic
downturns when companies tried to cut their costs by lowering wages and accelerating production.
Indeed, the late-nineteenth-century economy of the United States was an especially volatile one. Two major
panics—and several smaller ones—triggered long-term economic recessions in every decade between the 1870s
and 1890s. These panics were usually triggered by speculative excess. But unlike the panics of the antebellum
era, which were fueled by real-estate crises, the panics of the late nineteenth century were caused by
unsustainable run-ups in the price of railroad stocks. As the speculative bubbles burst and share prices collapsed,
overextended financial institutions suddenly found themselves unable to meet their financial obligations, leading to
depositor runs on bank assets, which further contributed to the panic. The result was a dramatic contraction of
credit that undermined investment and hiring, leading to wage cuts and higher unemployment. This, in turn, led to

deflationary pressure that produced a further loss in confidence and continued reductions in investment.
Many people, particularly in the South and West, came to believe that the heart of the problem lay in the nation’s
money supply. With the dollar backed by gold exclusively—and gold in limited supply until major discoveries were
made in the mid-1890s in the Yukon and South Africa—money became increasingly valuable. That is, there was
a limited amount of money in an ever-expanding economy, which reduced prices and wages. The solution,
opponents of the gold standard argued, was to monetize silver, which, because it was relatively plentiful, would
increase the money supply, raise prices, and make it easier for borrowers—especially farmers—to pay their debts.
Opposed by major financiers and the pro-business Republican establishment that dominated national politics in
the late nineteenth and early twentieth centuries, the silver solution was largely ignored by policy makers.
With the expansion of the gold supply in the 1890s, however, the issue became less economically pressing, even
if the politics surrounding it reached their crescendo in the heated presidential campaign of 1896. Still, while the
money question began to fade in importance and the Populists who thrived on it lost their political footing, a new
economic issue arose—the power of the huge corporations that had emerged in manufacturing and transportation
in the years since the Civil War.
While the social welfare of the poor and working classes—many of them laboring for big business—was at the
heart of local and state reform efforts of the early twentieth century (federal involvement in such issues was still
decades in the future), the Progressive movement at the national level focused largely on how to control the
excessive power of big business to affect political decision-making and hinder economic competition.
Progressives, such as President Theodore Roosevelt, achieved this either by breaking monopolistic corporations
into smaller competing companies, as in the case of Standard Oil in 1909–1911, or by passing regulations to make
sure that corporations behaved more responsibly vis-à-vis consumers, workers, and the environment.
In short, Progressives believed that government played an important role in shaping economic behavior and
steering economic activity in socially beneficial directions. This included reducing the excessive financial
speculation that triggered panics and produced economic downturns. In the wake of the short but sharp recession
of 1907, many business and government leaders came to the conclusion that it was time to create a central bank,
like those in many European countries. The Federal Reserve System, which came into being in 1913, was created
to ease volatility by maintaining control over the money supply, raising interest rates to cut back on speculative
excess, and lowering them to spur economic growth during downturns.
But in its first two decades, the Fed, as it came to be called, was less than effective. During the booming 1920s, it
maintained an artificially low interest rate that spurred speculative excess, first in the real-estate market and then,
when prices there fell back to Earth in the middle years of the decade, in the stock market, which saw
unprecedented run-ups in the price of corporate securities. In fact, the 1920s were a time of great economic
prosperity for some and economic hardship for others, as wealth and income became ever more inequitably
distributed, both between classes and between rural and urban areas.
 Crash, Depression, and War
The great Wall Street crash of 1929 was grounded in both aspects of economic life in the 1920s—the run-up in
share prices and the growing inequality in wealth. As for the former, much of the skyrocketing price of corporate
securities could be traced to too much easy credit, as brokerages borrowed money from banks to lend to
investors, who would buy stock on margin (pay a small portion of the stock price and use the full value as
collateral against the loan). This worked as long as stock prices were climbing. But when they began to fall in late
1929, the house of cards collapsed, as brokerages called in their margin loans to pay back panicky banks.
The crash might have been confined to Wall Street investors, still a tiny minority of the population in 1929, if it had
not been for the underlying weaknesses of the national economy. As credit dried up, so did investment, sending
unemployment rising and bankruptcies soaring. Because most consumers lived on the financial edge—having
failed to be included in the prosperity of the 1920s—demand for the goods being produced by manufacturers

plummeted.
The result was the worst economic downturn in American history—the Great Depression of the 1930s. Making
things worse were the actions of government, in particular the Fed. Indeed, some economic historians blame the
severity of the economic downturn of the early 1930s—when overall economic output fell by some 40 percent and
unemployment topped 25 percent—on the tight money policy of the Fed. Believing that overcapacity was the
major problem facing the country, the Fed decided to lower interest rates slowly beginning in early 1930, a pace
that many economists have argued contributed to the severity of the Depression. This policy not only contributed
to the fall in economic output, but sent many banks to the edge financially, creating panic among depositors, who
tried to withdraw their money. This, in turn, caused a wave of bankruptcies in the financial sector.
The landslide victory of Democrat Franklin Roosevelt in the 1932 presidential election ushered in an era of
enhanced government involvement in the economy. The New Deal, as Roosevelt’s domestic agenda was called,
consisted of two major parts. The first focused on business and included such programs and institutions as the
Federal Deposit Insurance Corporation, to protect bank depositors and prevent runs on banks; the Securities and
Exchange Commission, to regulate Wall Street and prevent the kind of excesses that had led to the stock market
crash of 1929; and the National Recovery Administration, an ambitious program to prevent “destructive
competition” by setting wage and price standards that was ultimately ruled unconstitutional by the Supreme Court.
The “second New Deal,” as it came to be known, was not inaugurated until late in Roosevelt’s first term. It
consisted of laws and programs designed to bolster the demand side of the economic equation, including the
National Labor Relations Act, which made unionization easier, and the Social Security Act, which set a national,
government-run pension system. Meanwhile, the federal government tried to ease unemployment by hiring
thousands of citizens to work on infrastructure and other projects through the Works Progress Administration and
other programs.
While Roosevelt did not explicitly invoke the name of John Maynard Keynes, the ideas of that pioneering English
economist were increasingly at the heart of what his administration was trying to achieve. Keynes, focusing on the
demand side of the economic equation, argued that government spending, even if it causes deficits, is critical in
lifting an economy out of a slump. Prior to Keynes, most economists held to the idea that deficit spending dried up
capital necessary for private investment. And indeed, with the economy recovering by the mid-1930s, Roosevelt
took such conventional advice and cut back on government spending. That, most economic historians agree, sent
the nation’s economy into the steep recession of 1937–1938.
But Keynes’s ideas were invoked by Roosevelt, if obliquely, at decade’s end, as the country mobilized for war.
Economists continue to debate the role of World War II in lifting the economy out of the Depression. Some argue
that massive defense spending was not exactly what Keynes recommended—he preferred more socially
productive spending, on things such as infrastructure—but it did the trick, boosting industrial output and
dramatically slashing unemployment. Others, however, contend that employment was lowered by the fact that
more than 16 million Americans were removed from the labor force by entering the military, while output was
artificially sustained by defense spending.
 Boom and Recession in the Postwar Era
There is no argument about the reality of the prolonged economic boom that followed the war. Aside from
occasional recessions and bouts of inflation, the U.S. economy enjoyed unprecedented expansion between the
late 1940s and the early 1970s—an acceleration of growth fueled by pent-up demand after years of depression
and war; large government outlays for housing, education, and defense; and limited foreign competition, as other
major industrial powers focused on rebuilding after the devastation caused by World War II. The postwar boom
was also one in which the U.S. government explicitly pursued Keynesian counter-cyclical economic policy,
reducing taxes and increasing spending during downturns. Meanwhile, the United States emerged from World War
II as the dominant global economic player—or, at least, the dominant player outside the Communist bloc. When
the U.S. economy boomed, it tended to lift other economies with it; when it contracted it usually created

recessionary conditions in much of the rest of the world.
In the mid-1970s, however, the country entered a period in which recessions became deeper and more frequent.
In addition, they were accompanied not by falling prices, as was usually the case during downturns, but by
galloping inflation. This pattern of “stagflation” (a combination of “stagnation” and “inflation”) was caused by
multiple oil price hikes by OPEC and a wage-price spiral (cost-push inflation instead of demand-pull inflation);
expansion of the money supply by the Fed served to accelerate inflation rather than ease unemployment and
reverse slow or negative growth. Post Keynesian economists then suggested that taxes and other government
measures be imposed on large corporations to prevent them from hiking wages and prices. But such an approach
was not implemented, other than a 90-day freeze on wages and prices instituted by President Richard Nixon in
1971.
While all economists agree that the oil shocks of the 1970s, which sent crude prices spiking, contributed to the
“malaise,” other factors are highly debated. In particular, economists point to a still not well-explained decline in
productivity. On the left, economists pointed to the end of a period of labor peace in which capitalists had granted
higher wages in return for more productivity. However, more influential politically was the claim by conservatives
that excessive workplace, environmental, and other forms of regulation were undermining productivity, limiting
investment, and restricting the country’s competitiveness. With millions of Americans hurting from unemployment,
high interest rates, and inflation, the message hit home, propelling conservative Republican Ronald Reagan into
the White House in the 1980 election.
The Reagan administration’s “supply-side” solution to the country’s economic problems was premised on the idea
that lowering tax rates on corporations and wealthy individuals would spur investment, which would eventually
“trickle down” to benefit all Americans in the form of higher employment and wages. In addition, it was argued—
counter to previous conservative economic thinking—the deficits produced by lower revenues would be only
temporary, as a booming economy would soon produce even more revenues. And, indeed, Reagan dramatically
lowered tax rates across the board, which meant that most of the savings accrued to those who proportionately
paid the most in taxes—that is, the business sector and wealthy individuals. The Reagan administration also
initiated a policy of laxer enforcement of government regulations, refused to give in to an air traffic controllers’
strike—thereby sending a signal to unions to rein in their wage demands—and eased up on antitrust enforcement.
At the same time, Fed chairman Paul Volcker began to dramatically hike interest rates—thereby contracting the
growth in the money supply—in an effort to wring inflation out of the system. The immediate result was the worst
economic downturn since the Great Depression, with national unemployment topping 10 percent for the first time
in more than four decades. The recession proved relatively short-lived, however, and did wring inflation out of the
system. Meanwhile, the economy began to boom by the mid-1980s, though at the cost of growing inequalities in
wealth.
In addition, the anti-regulatory policies of the 1980s also contributed to one of the greatest financial crises in
American history, as much of a once-staid savings and loan (S&L) industry—freed from requirements about what it
could invest in and how much money institutions must maintain against their loans—nearly collapsed, requiring a
massive government bailout and liquidation of distressed assets. While the S&L debacle of the late 1980s and
early 1990s did result in specific reforms to prevent a recurrence, the overall trend toward deregulation of the
financial industry accelerated during the 1990s.
After a brief recession in the early part of the decade—triggered in part by dramatically lower defense spending
as the cold war wound down—the economy entered the longest period of continuous sustained growth in U.S.
history, propelled by the great productivity gains made possible by the spread of computers, more advanced
telecommunications technology, and better management techniques, including just-in-time inventory and improved
quality control.
With moderate Democrat Bill Clinton in the White House and conservative Republicans in charge of Congress, the
country continued on a generally conservative economic path through the end of the century. The national welfare

system was significantly restructured, with a new emphasis on pushing recipients into the workforce (1996), the
telecommunications industry was deregulated (1996), the federal deficit was turned into a surplus (beginning in
1998), and legislation from the New Deal era barring commercial banks from engaging in brokerage and insurance
activity was repealed (1999).
 Booms and Busts of the 1990s and 2000s
Meanwhile, the boom of the 1990s was not without its flaws. Most pronounced was a price bubble in the high-
tech industry. By the late 1990s, it was becoming increasingly apparent that the Internet was going to
revolutionize the way business was conducted. Hundreds of billions of dollars were invested in companies set up
to take advantage of the new technology, both privately and through initial public offerings on securities
exchanges, most notably, the National Association of Securities Dealers Automated Quotations (NASDAQ).
Valuations soared, padding the paper fortunes of investors. But many of the dot.com firms lacked a realistic, profit-
making business model. By 2000, these weaknesses became increasingly glaring, leading to a dramatic collapse
in valuations that, along with the terrorist attacks of September 11, 2001, contributed to recession in 2001 and
2002.
To counteract the downturn, the Fed began to dramatically lower the interest rates it charged member banks—to
historic lows of 3 percent or less—from late 2002 to mid-2005. These low rates, passed on to consumers,
encouraged millions of Americans to buy homes or refinance their mortgages, fueling a boom in the construction
industry and housing prices that peaked in late 2006 and early 2007. As with the high-tech boom of the 1990s,
however, there were underlying problems that few noticed in the midst of what many thought was a sustained
expansion of the housing sector. First, many financial institutions—facing less oversight and regulation by the
government—began to lower their lending standards, offering mortgages to prospective home-buyers who
ordinarily would not be qualified. Many of these so-called subprime mortgages—as well as many ordinary
mortgages—had graduated payment clauses in which they started off low and then jumped after a set period of
time, or were set up as adjustable rate mortgages with higher payments due if interest rates increased. As long as
housing prices rose, mortgagors could simply refinance with a new adjustable rate, using the rising equity in their
homes to cover the closing costs.
Abetting this phenomenon was the securitization of mortgages. That is, mortgages were increasingly being
bundled into packages and sold to investors, both in the United States and abroad. The idea was that, by bundling
mortgages, the losses caused by foreclosures were spread around, thereby limiting the risk for those holding the
mortgages. Reducing risk even further—or so investors thought—were credit default swaps, essentially insurance
policies taken out against mortgage-backed securities. But there was a basic flaw in the system. For in selling the
mortgages to other investors, the initiators of the mortgage—whether brokers or lending institutions—no longer
had to worry about the creditworthiness of those taking out the mortgages, contributing to the lowering of lending
standards.
For a while, no one in the industry or among government regulators seemed overly concerned, as housing prices
continued to soar. As in the case of all financial bubbles, however, confidence in ever-rising prices eventually
evaporated and real-estate valuations began to fall. With home equity disappearing, marginal homeowners could
not refinance, which led to accelerated foreclosures and the so-called subprime mortgage crisis, which began in
2007.
The contagion inevitably spread to the financial institutions holding the mortgage-backed securities and collateral
debt obligations, leading to the collapse of several major investment banks on Wall Street and precipitating the
financial crisis of late 2008. The crisis—which prompted a $700 billion bailout of major financial institutions by the
George W. Bush and Barack Obama administrations—contributed, along with collapsing home prices, to the
longest and deepest recession, as measured by negative GDP growth, since the Great Depression. Nor was the
crisis confined to the United States, since so many of the troubled, mortgage-related assets had been purchased
by financial institutions around the world.

While the crisis and recession led to great immediate suffering by individuals and businesses alike—including the
bankruptcy of two of America’s three leading carmakers—it also prompted longer-term changes in government
economic policy. Never before in U.S. history had the government taken such large equity stakes in financial
institutions and other corporations, such as General Motors, in order to keep them afloat. Nor had the federal
government ever spent more on an economic stimulus package than the one Obama pushed through Congress in
early 2009, which amounted to $787 billion, prompting economists and the media to talk of a revival of Keynesian
economics at the federal level. And with Democrats in control of both the White House and Congress after the
2008 elections, there appeared to be a new consensus in Washington that tighter regulation of the financial
industry, including more consumer protections and stricter limits on executive compensation, was needed if the
country was to avoid a repeat of the kind of financial crisis that nearly pushed the country and the world into a
new Great Depression.
But a conservative backlash against the rising government debt—and a widespread reform of the nation’s health
insurance system—led to significant Republican gains in the 2010 midterm congressional elections. Backed by the
right-wing populist Tea Party, deficit and tax hawks in Congress refused to consider new stimulus measures to
ease persistent high unemployment rates or new tax revenues on the rich to pay for them or to reduce the debt.
Instead, they insisted on more spending cuts. In the summer of 2011, they even refused to raise the government’s
debt ceiling, normally a routine matter, which put the government’s credit rating in jeopardy. Although a deal was
eventually reached, it did not stave off a decision by Standard & Poor’s to downgrade the nation’s credit rating
from AAA to AA+, the first time this had happened in 70 years. Still, conservatives had a point—America’s
growing public debt—$15 trillion and rising by the end of 2011—represented a major threat to the long-term health
of the economy. Meanwhile, the same conservative wave that swept Republicans into control of the House of
Representatives in 2010 also led to large gains at the state level. Faced with growing debts, conservative
governors and state legislators attempted austerity measures of their own, firing large numbers of public
employees and calling for dramatic reforms of public pension programs. There were also successful efforts to rein
in the power of public employee unions. As a result of the layoffs—and a continuing anemic hiring in the private
sector—unemployment remained stubbornly high through 2010 and 2011, hovering around nine percent. Many
experts said that if those who had given up looking for work or were working part-time were included, the rate
was more like 15 percent. All of this added up to a situation in which the economy was no longer shrinking—and,
hence, no longer technically in recession—but not growing dramatically either. GDP growth through 2010 and
2011 remained around two percent, barely keeping up with the growth in population.
While rising public debt and the need for austerity measures dominated the political discourse through 2010 and
early 2011, a new issue was gaining attention by late 2011—economic inequality between the richest one percent
of the population and the rest of the country. A new movement, known as Occupy Wall Street, soon spread
across the country, demanding that measures be taken to rein in the wealth and power of America’s richest
individuals and corporations. Such sentiments were reflected in the polls, which showed a majority of Americans
believed that the rich should be taxed more and the money spent to address the country’s myriad economic and
social problems. Moreover, a report by the nonpartisan Congressional Budget Office confirmed the growing
inequality in income, noting that the wealthiest one percent of Americans had seen their inflation-adjusted income
increase by 275 percent between 1979 and 2007 while the middle 60 percent saw theirs go up by just 40 percent.
These two visions of the problems America faced—a conservative view that government had grown too big and a
liberal view that said wealth and power were too unequally distributed—led to political stalemate at the federal
level, with many on both sides hoping they would win a mandate in the 2012 elections to fix an economy that all
agreed was facing significant short- and long-term problems.
James Ciment
 
See also:  Boom, Economic (1920s);  Boom, Economic (1960s);  Canada;  Dot.com Bubble
(1990s-2000);  Great Depression (1929-1933);  New Deal;  Panic of 1901;  Panic of 1907; 
Recession and Financial Crisis (2007-);  Recession, Reagan (1981-1982);  Recession,

Roosevelt (1937-1939);  Recession, Stagflation (1970s);  Savings and Loan Crises (1980s-
1990s). 
Further Reading
Andreano, Ralph L., ed. The Economic Impact of the American Civil War. Cambridge, MA: Schenkman, 1962. 
Badger, Anthony J. The New Deal: The Depression Years, 1933–1940. Chicago: Ivan R. Dee, 2002. 
Bluestone, Barry, and Bennett Harrison. The Deindustrialization of America: Plant Closings, Community Abandonment, and
the Dismantling of Basic Industry. New York: Basic Books, 1982. 
Cassidy, John. Dot.con: The Greatest Story Ever Sold. New York: HarperCollins, 2002. 
Chandler, Alfred D. The Visible Hand: The Managerial Revolution in American Business. New York: Arno, 1977. 
Fishlow, Albert. American Railroads and the Transformation of the Ante-Bellum Economy. Cambridge, MA: Harvard
University Press, 1965. 
Freidman, Benjamin. Day of Reckoning: The Consequences of American Economic Policy Under Reagan and After. New
York: Random House, 1988. 
Galbraith, John Kenneth. The Affluent Society,  40th anniversary ed. Boston: Houghton Mifflin, 1998. 
Galbraith, John Kenneth. The Great Crash: 1929. Boston: Houghton Mifflin, 1997. 
Henretta, James A. The Origins of American Capitalism. Boston: Northeastern University Press, 1991. 
Keller, Morton. Regulating a New Economy: Public Policy and Economic Change in America, 1900–1933. Cambridge,
MA: Harvard University Press, 1990. 
Nettels, Curtis. The Emergence of a National Economy, 1775–1815. Armonk, NY: M.E. Sharpe, 1989. 
Perkins, Edwin J. The Economy of Colonial America. New York: Columbia University Press, 1988. 
Solomon, Robert. Money on the Move: The Revolution in International Finance Since 1980. Princeton, NJ: Princeton
University Press, 1999. 
Tedlow, Richard. The Rise of the American Business Corporation. Philadelphia: Harwood, 1991. 
 
Veblen, Thorstein (1857–1929)
 
Thorstein Veblen, a Norwegian-American social critic and economist, was best known for his 1899 book The
Theory of the Leisure Class. In that work, he coined the term “conspicuous consumption” to describe the
acquisition of consumer goods and services for the purpose of attaining and reflecting social status.
He was born Tosten Bunde Veblen on July 30, 1857, in Cato, Wisconsin, and raised in Minnesota. He received a

bachelor’s degree in economics from Carleton College in 1880, where he studied under John Bates Clark, a
leading American neoclassical economist. Later he studied philosophy and political economy at Johns Hopkins
University and at Yale, from which he received a PhD in 1884.
Veblen was greatly influenced by the work of British philosopher of biological and social evolution Herbert
Spencer and British naturalist Charles Darwin; Veblen later developed the field of evolutionary economics based
on Darwin’s principles as well as on the then new thinking in the areas of psychology, anthropology, and
sociology. After recovering from a long illness, Veblen studied economics at Cornell University from 1891 to 1892,
leaving there to take a position as a professor of political economy at the University of Chicago and as managing
editor of the Journal of Political Economy. In 1906 he moved to Stanford University; then, from 1911 to 1918, he
joined the economics department at the University of Missouri. Finally, in 1918, he moved to New York. There, he
became an editor of The Dial and, in 1919, co-founded the New School for Social Research with Charles Beard,
John Dewey, and James Robinson.
Veblen’s academic career was marked by turmoil and rumors of scandal. Although his published work brought
great success, he was not highly regarded as a teacher and was viewed as something of an outsider by the
academic establishment. This, combined with his so-called sociological or “nonscientific” approach to economics,
resulted in Veblen’s less than spectacular rise through the ranks of academia.
After helping found the New School for Social Research in 1919, Veblen became part of Howard Scott’s Technical
Alliance (later Technocracy Incorporated), one of the first think tanks in the United States. He remained at the
New School until 1927. He returned to his home in Palo Alto, California, where he died on August 3, 1929.
In The Theory of the Leisure Class, Veblen defines and analyzes conspicuous consumption. Contrary to the newly
developed neoclassical economics of the period, which viewed the consumer as a rational being whose behavior
was driven by utility and self-interest, Veblen suggested the revolutionary theory that consumers’ buying behavior
—and, by extension, consumerism and economic expansions and contractions—is driven by social institutions,
tradition, and a desire for social status. He argued that in a modern society’s leisure class, wealth is determined
by the possession of “useless” things— or the ability to spend money on things that do not provide any real utility
to the consumer.

Known for his concept of “conspicuous consumption”—that consumers care more about the status conferred by
material goods than their utility—Thorstein Veblen argued that public institutions must ensure the proper
distribution of resources because people waste them. (The Granger Collection, New York)
Thus, social context is prominent in Veblen’s consumption theory. People, he argued, consume to emulate and
impress others; so consumption is socially determined, not decided by individuals on a rational basis. This notion
became the foundation of his twentieth-century evolutionary economics. According to Veblen, when consumers
purchase useless products that they cannot afford instead of essentials, or necessities of high utility, problems
arise in the economy. He questioned how the conspicuous consumer can justify, for example, living in a big house
to impress neighbors and friends when the loan repayments are prohibitive and, in times of financial crises, the
consumer is faced with possible unemployment, rising interest rates, and the threat of losing the family home.
Veblen’s critique went even further. He claimed that the production of such conspicuous goods for nonutility
consumption wastes valuable economic resources. The state, he argued, must ensure the availability of public
goods; it must tax the nonutility goods to redistribute economic resources. This insight made Veblen a prominent
representative of the “institutionalist” school of economics. According to this view, because individual incentives for
consumption waste resources, public institutions must—in modern society—make sure that resources are properly
distributed for optimal economic use.
Sabine H. Hoffmann
 
See also:  Consumption;  Institutional Economics. 
Further Reading

Dorfman, Joseph. Thorstein Veblen and His America. New York: A.M. Kelley, 1972. 
Tilman, Rick, ed. The Legacy of Thorstein Veblen. Northampton, MA: Edward Elgar, 2003. 
Veblen, Thorstein. The Theory of Business Enterprise. New York: A.M. Kelley, 1975.  First published 1904.
Veblen, Thorstein. The Theory of the Leisure Class. New York: Dover, 1994.  First published 1899.
Veblen, Thorstein.  “Why Is Economics Not an Evolutionary Science?” Cambridge Journal of Economics 22:4 ([1898]
1998): 403–414. 
Venture Capital
 
Venture capital (VC) is a form of financial capital typically provided to high-risk, early-stage, high-growth-potential
companies dealing with high technology. This type of capital is usually invested in enterprises that are too risky for
standard capital markets or for bank loans. A venture capital firm is a private governmental or semigovernmental
organization that provides early-stage or growth-equity capital and/or loan capital, seeking returns significantly
higher than accepted market return rates.
Venture capital activity has been increasingly associated with technological growth and economic growth in
developed countries. By the same token, economic recessions tend to have a devastating effect on the level of
venture capital investments, resulting in a slowdown in the pace of innovation and, in turn, the rate of economic
growth.
 Origins and Evolution
In the first half of the twentieth century, capital investment (originally known as “development capital”) was not a
regular source of funding. Private equity was mostly provided by wealthy individuals and families. In 1938, for
example, Laurance S. Rockefeller helped finance the creation of both Eastern Air Lines and Douglas Aircraft.
Entrepreneurs at the time who had no wealthy friends or family had little opportunity to fund their ventures.
The modern VC industry was established in 1946, when a U.S. general named George Doriot returned from World
War II to Harvard University and founded the American Research and Development Corporation (ARD), a venture
capital firm created to raise private-sector investments in businesses launched by returning soldiers. Doriot was
followed by the industrialist and philanthropist John Hay Whitney, who established the J.H. Whitney & Company
venture capital firm later that same year.
The development of the venture capital industry in the United States was encouraged by passage of the Small
Business Investment Act in 1958, which established the Small Business Administration (SBA) “to aid, counsel,
assist and protect... the interests of small business concerns.” The legislation allowed the SBA to license small
business investment companies (SBICs) to help the financing and management of small entrepreneurial
businesses in the United States. During the 1960s and 1970s, venture capital firms focused their investment
activity primarily on high-tech ventures, mostly in computer-related, data-processing, electronic, and medical
companies.
The public successes of the venture capital industry in the 1970s encouraged the proliferation of venture capital
investment firms across the country and in diverse sectors of the economy. During the 1980s, the number of
venture capital firms operating in the United States surged to over 650; the capital they managed increased to

more than $31 billion. During the late 1980s and early 1990s, however, the industry’s growth was hampered by
sharply declining profits.
With the emergence and proliferation of the global information technology (IT) industry, the mid-1990s was a
boom time for the venture capital industry. Initial public offerings (IPOs)—by which firms become public by selling
stock—on the NASDAQ stock exchange for technology and other growth companies were thriving, with venture-
backed firms enjoying large windfalls. The good times ended abruptly in 2001, however, with the bursting of the
so-called dot.com bubble.
The recovery arrived in 2004, and the industry grew to about $31 billion within three years. In 2008, however, VC
investments in the United States dropped to $28 billion due to the world financial crisis that started in the last
quarter of the year.
 Corporate Venture Capital
Following the emergence and growth of VC firms in the post–World War II period, corporate venture capital (CVC)
programs—in which corporations invest directly in other firms—began appearing in the mid-1960s. The successful
private VCs were believed to be the drivers of this first CVC wave. The next wave occurred during the first half of
the 1980s, driven by new technological opportunities and favorable changes in legislation. The third wave came
with the Internet boom in the mid-to late 1990s, when 400 CVC programs were operated by such major players
as Intel and Siemens, among others.
VC funding is most attractive for new companies with limited operating history. It is a vital tool of economic
development in market economies and plays a key role in facilitating access to finance for small and medium
enterprises (SMEs). It also plays a vital role in the creation and growth of public corporations. In addition to
providing capital at critical stages of development, VC firms add value to the process of going public through
screening, monitoring, and decision-support functions. Since VC firms usually specialize by industry and stage of
development, their knowledge, experience, and contacts assist entrepreneurs in strategic, financial, and
operational planning. These activities, in addition to the capital invested, are critical to ensuring that a steady
stream of well-prepared firms goes public. Research results also indicate that VC involvement improves the
survival profile of firms. A VC-backed enterprise will usually survive longer than a non-VC-backed operation.
Venture capital activity also contributes directly to productivity growth. VC firms have an indirect impact on
productivity growth by improving the output and outcomes of research and development activities. Increased VC
participation makes it easier for a firm to absorb the knowledge generated by universities and firms, thereby
improving a country’s economic performance.
 Structure of VC Firms
The investors in VC firms are called “limited partners” because of their limited legal responsibility. Their financial
resources usually come from large institutional players such as pension funds and insurance companies. The
limited partners pay the general partners—those who manage the VC firm—an annual management fee of 1 to 2
percent and a percentage of profits, typically up to 20 percent; this is referred to as the “two and 20 arrangement.”
The remaining 80 percent of profits are paid to the fund’s investors. The life span of each fund raised for
investment by the VC firm is usually six to eight years. Because a fund may run out of capital before the end of its
life, larger venture capital firms try to have several overlapping funds at the same time.
Each fund typically invests in a number of new ventures. The fund managers select two to five out of hundreds of
opportunities presented to them through a process of screening and due diligence (checking out the favorability of
a possible investment). Selection criteria include a solid business plan, a product or service with clear competitive
advantage, capable and highly motivated entrepreneurs and management team, and of course, the likelihood of
profits and growth.

Unlike with public companies, information regarding entrepreneurial business plans is typically confidential and
proprietary. As part of the due diligence process, most venture capitalists require significant detail with respect to
the proposed business plan. Entrepreneurs must remain vigilant about sharing information with venture capitalists
who are potential investors or current shareholders in their competitors. Venture capitalists are also expected to
nurture the companies in which they invest in order to increase the likelihood of a successful exit.
 VC Investment Cycle
Venture capital investments are illiquid—they cannot be readily turned into cash—and generally require three to
seven years to harvest, or receive a return on investment. Corresponding to the life cycle of a firm, there are six
different stages of financing offered by venture capital:
Seed money: low-level financing needed to prove a new idea
Early stage: funding for expenses mainly needed for market research and product development
First round: early sales and manufacturing funds
Second round: working capital for early-stage companies that are selling product but not yet turning a profit
Third round (a stage also called mezzanine financing): expansion money for a newly profitable company
Fourth round: referred to as “bridge” financing as it leads to the initial public offering (IPO)
Many VC firms will consider funding only after a firm passes the early stage, when the company can prove at
least some of its claims about the technology and/or market potential for its product or services. Therefore, many
entrepreneurs who are just starting out seek initial funding from “angel investors” or “technology incubators” prior
to VC funding.
Angel investors provide an informal source of capital from wealthy businesspeople, doctors, lawyers, and others
who are willing to take an equity, or ownership, stake in a fledgling company in return for their funding. In recent
years, more and more angel investors have been previous successful high-tech entrepreneurs themselves.
Technology incubators are usually government-driven sources of early-stage financing at the federal, state, or
local level. Their main contribution, in addition to funding, is marketing, legal, and administrative support for the
new venture. Initially, in order to prove at least some of its claims about the technology and/or market potential for
its product or services, many start-ups seek self-finance, a practice called “bootstrapping.”
 Activities of Venture Capitalists
The activities of venture capitalists in a start-up enterprise include investing and monitoring, aimed at “exiting” on
their investment. The exit, or harvest, is the Holy Grail for venture capitalists, achieved by selling their shares in
the business and realizing their profits. The shares are usually sold to a large corporation or offered to the public
after a successful IPO.
In most cases, venture capitalists make investments of cash in exchange for shares in the invested company.
Since venture capitalists assume high risk while investing in smaller and less mature companies, they generally
retain a measure of control over company decisions through board membership, in addition to owning a significant
portion of the company’s shares. Many of the most successful American high-tech companies today, such as
Microsoft, Intel, and Apple Inc., were backed at their early stage by venture capital funds, such as Benchmark,
Sequoia, and others.
The return rates on VC investment are generally between 36 percent and 45 percent for early-stage investments
and 26 and 30 percent for expansion and late-stage investments. The average rate of return varies according to

the anticipated time of exit and the maturity level (age) of the venture. In individual cases, of course, the return
varies much more significantly, based on the success of the product or service, market conditions, and
management of the company.
Banks and large corporations also have capital investment units; notable early players included General Electric,
Paine Webber, and Chemical Bank. Companies that make strategic investments seek to identify and exploit the
complementary relationship that exists between themselves and a new venture. For example, a corporation
looking at a financial investment seeks to do as well as or better than private VC investors because of what it
sees as its superior knowledge of markets and technologies, its strong balance sheet, and its ability to be a patient
investor. The endorsement of a major corporate brand is regarded as a sign of quality for the start-up, helping
bring in other investors and potential customers.
 Global VC Industry
American firms have traditionally been the largest participants in venture deals. In 1996 the U.S. venture capital
pool was about three times larger than the total venture capital pool in twenty-one other countries where it
existed. Moreover, about 70 percent of the venture capital in the rest of the world was concentrated in three
countries with strong ties to the U.S. economy: Israel, Canada, and the Netherlands.
It may be argued that in countries with strong banking systems, such as Germany and Japan, there is less need
for venture capital. In recent years, however, non-U.S. venture investment has been growing, and the number and
size of non-U.S. venture capitalists has been expanding. The European venture capital industry has followed the
U.S. model of VC investment, as have the industries in such Asian countries as Singapore, Taiwan, and China.
Policy makers in these areas also believe that venture capital should be encouraged, as it is a boost to high-tech
growth, general entrepreneurship, and overall economic growth.
The global competitiveness report of 2006–2007 of the World Economic Forum ranked the United States first and
Israel second for venture capital availability. In 2008 there were about fifty venture capital funds operating in Israel.
Since the mid-1990s, the total capital raised was about $10 billion, with investments made in more than 1,000
Israeli start-up companies. The average size of the leading Israeli venture capital fund jumped from $20 million in
1993 to more than $250 million in 2007. As elsewhere, investments have focused on technological innovation with
global application in the areas of communication, computer software and IT, semiconductors, medical equipment
and biotechnology, and homeland security.
In 2008, American VC firms invested about $28.3 billion in 3,908 deals, down a bit from $30.9 billion in 3,952
deals during 2007. According to the European Private Equity and Venture Capital association, €3.01 billion of VC
cash was invested by European VCs in 2007, an increase of 80 percent from 2006. However, following the
worldwide economic recession that started in late 2008, returns deteriorated for most VC firms, and the industry
encountered one of the most radical shakeouts in its history.
Eli Gimmon
 
See also:  Dot.com Bubble (1990s-2000);  Information Technology;  Technological Innovation. 
Further Reading
Chesbrough, H.W.  “Making Sense of Corporate Venture Capital.” Harvard Business Review 80(2002): 90–99. 
Dushnitsky, G.  “Corporate Venture Capital: Past Evidence and Future Directions.” The Oxford Handbook of
Entrepreneurship. New York: Oxford University Press, 2006. 
Metrick, Andrew. Venture Capital and the Finance of Innovation. Hoboken, NJ: John Wiley and Sons, 2007. 
National Venture Capital Association:  www.nvca.org

Timmons, J., and S. Olin. New Venture Creation: Entrepreneurship for the 21st Century. Boston: McGraw-Hill, 2004. 
VeraSun Energy
 
Once the largest producers of the corn-based renewable fuel ethanol in the United States, South Dakota–based
VeraSun Energy was founded in the early 2000s to take advantage of the growing interest in and market for
alternative fuels that would allow the country to cut its dependence on foreign oil. Despite high fuel prices and
passage of federal legislation requiring oil refiners in the United States to use billions of gallons of renewable fuel,
VeraSun has struggled financially, filing for bankruptcy protection in late 2008, a victim of the notoriously volatile
energy and agricultural industries and the global financial crisis.
Ethanol, also known as grain alcohol, is a flammable hydrocarbon liquid produced from a variety of crops, mainly
sugarcane and corn. From the dawn of the automobile age, it has been technically possible to run motor vehicles
on a flex fuel basis, that is, either on gasoline alone or on a mix of gasoline and renewable fuels, such as ethanol.
Over the years, however, gasoline came to predominate and renewable fuels fell by the wayside. But with the oil
shocks of the 1970s came new interest in such fuels to counter growing U.S. dependence on sources of oil in
politically volatile parts of the world.
In recent years, many manufacturers have produced cars that run on what is known E85, a mixture of 15 percent
gasoline and 85 percent ethanol, though regular cars can run on a mix of 10 percent ethanol and 90 percent
gasoline, known as gasohol. (There is also a 70 percent ethanol mix for use in cold temperatures.) By the late
2000s, most new automobiles, vans, SUVs, and pick-up trucks had been made available in flexible-fuel versions,
allowing them to use either gasoline or E85.
VeraSun was founded in Sioux Falls, South Dakota, in 2001 to produce ethanol. The goal of the company, beyond
making a profit, was to provide the United States with a source of domestic energy that would at the same time
boost rural economies, both by buying up corn from farmers and by employing people at local refineries where
that corn was turned into ethanol.
In 2003, VeraSun opened its first ethanol production facility in Aurora, South Dakota. The first company to surpass
the 100-million-gallon annual production milestone, VeraSun also created the first branded E85 fuel, which it
trademarked as V85. Over the next several years, the company opened several new plants, and in 2006 it went
public with a listing on the New York Stock Exchange. Soon, VeraSun was acquiring smaller producers, and by
2008 had seventeen production facilities across the corn-growing Upper Midwest and Great Lakes region.
Meanwhile, the company benefited from passage of the Energy Policy Act of 2005, which required that oil refiners
use 4 billion gallons of ethanol by 2006, 6.1 billion gallons by 2009, and 7.5 billion gallons by 2012.
But such legislation and the record high fuel prices of 2008, when crude hit nearly $150 a barrel, could not prevent
VeraSun from experiencing financial troubles; in the end, VeraSun was a victim of its own industry’s success.
The demand for ethanol forced up the price of corn from about $4 a bushel to $7 a bushel over the spring and
summer of 2008, as farmers could not keep up with the demand for ethanol, a result of the 2005 legislation. To
protect itself against what it expected to be continuing rises in the price of corn, VeraSun locked itself into
contracts to buy large amounts of the commodity at those inflated prices. But when corn prices came down in the
fall of 2008, VeraSun found itself in an uncompetitive position, especially when fuel prices began to fall in the late
summer and autumn of 2008. The locked-in corn contracts added to the company’s debt load of hundreds of

millions of dollars. Then came the financial crisis of September 2008. With credit markets freezing up, VeraSun
found itself unable to service its debts, and over the course of 2008, the company’s share price fell from nearly
$18 to under $1. On October 31, VeraSun filed for protection under Chapter 11 of the U.S. bankruptcy code,
allowing it to stay in business as it reorganized and found ways to meets its obligation to creditors. In early 2009,
Texas-based Valero, the largest oil refiner in North America, purchased VeraSun for just under half a billion
dollars.
James Ciment and Bill Kte’pi
 
See also:  Agriculture;  Oil Shocks (1973-1974, 1979-1980). 
Further Reading
Asplund, Richard W. Profiting from Clean Energy: A Complete Guide to Trading Green in Solar, Wind, Ethanol, Fuel Cell,
Carbon Credit Industries, and More. New York: John Wiley and Sons, 2008. 
Rubino, John. Clean Money: Picking Winners in the Green Tech Boom. New York: John Wiley and Sons, 2008. 
“VeraSun Seeks Bankruptcy Protection.” Wall Street Journal, November 2, 2008. 
Viner, Jacob (1892–1970)
 
Known in the economics community as “the outstanding all-rounder of his generation,” as one colleague put it,
Jacob Viner is especially noted for his work on the history of economic thought and international trade theory.
Viner’s other important contributions to the field include an analysis of the proper role of the government during
economic recessions.
Viner was born on May 3, 1892, in Montreal, Canada. He received a bachelor’s degree in 1914 from McGill
University and a doctorate in 1922 from Harvard University, where he studied under the noted economist Frank
Taussig. In 1916 he became an instructor at the University of Chicago and was named a full professor in 1925.
Although he was never a member of the Chicago school of economic thought, he did teach at Chicago until 1946,
also editing, along with Frank Knight, the Journal of Political Economy. After leaving Chicago, he moved to
Princeton University—where he contributed greatly to the school’s intellectual life and where he was known as a
tough but generous teacher—and remained there until his retirement in 1960.
In addition to his academic work, Viner held a number of government positions. During World War I, he served on
the U.S. Tariff Commission and the Shipping Board. In the 1930s, he contributed to the original plans for the
Social Security program at the Treasury Department, where he served again during World War II. Later he was a
consultant to the State Department and to the Board of Governors of the Federal Reserve System. Perhaps
because of his government service, Viner believed that while research and theoretical studies are useful, the
functional and practical help that economists, trained as scholars, can offer outweigh the benefits of pure
research.
An anti-Keynesian (although he acknowledged the significant contributions of Keynes’s work), Viner favored
government intervention and inflation as a means to successfully fight the Great Depression. He believed that the
Depression’s corresponding deflation resulted directly from the prices of outputs falling faster than the fall in the

costs of outputs. In contrast to Keynesian theory, he opposed monetary expansion, although he favored deficit
spending. In 1931, Viner and other economists on the faculty at the University of Chicago issued a memorandum
advocating heavy deficit spending to increase jobs as a way to help improve the depressed economy. In the mid-
1930s, Viner attacked Keynesian economics as being effective in the short run but ineffective as a long-run
solution, stating that a short-term solution would be “a structure built on shifting sands.” This emphasis on the
difference between short-term versus ultimately better long-term solutions is a consistent theme throughout his
writing.
Viner’s early publications dealt chiefly with international economics. His first book, Dumping: A Problem in
International Trade (1924), was followed the next year by Canada’s Balance of International Indebtedness.
Although he pursued other interests, Viner continued his research in the area of international trade. In the late
1930s, he published what many considered his most important work on the subject, Studies in the Theory of
International Trade, which was acknowledged by some economists as the main source of historical knowledge in
the field.
In 1962 Viner received the Francis A. Walker Medal from the American Economic Association for his contributions
to economics. He taught for a year at Harvard as the Taussig research professor and continued to write and
lecture. He was a member of the Institute for Advanced Study in Princeton and an honorary fellow of the London
School of Economics. Viner died on September 12, 1970.
Robert N. Stacy
 
See also:  Fiscal Policy;  Great Depression (1929-1933);  Keynes, John Maynard. 
Further Reading
Bloomfield, Arthur I.  “On the Centenary of Jacob Viner’s Birth: A Retrospective View of the Man and His Work.” Journal of
Economic Literature 30:4 (December 1992): 2052–2085. 
Robbins, Lionel. Jacob Viner: A Tribute. Princeton, NJ: Princeton University Press, 1970. 
Rotwein, Eugene.  “Jacob Viner and the Chicago Tradition.” History of Political Economy 15:2 (1983): 265–280. 
Viner, Jacob. Balanced Deflation, Inflation, or More Depression. Minneapolis: University of Minnesota Press, 1933. 
Viner, Jacob. The Long View and the Short: Studies in Economic Theory and Policy. New York: Free Press, 1958. 
Viner, Jacob. Problems of Monetary Control. Princeton, NJ: Department of Economics, Princeton University, 1964. 
Winch, Donald.  “Jacob Viner.” American Scholar 50:4 (January 1981): 519–525. 
 
Volcker, Paul (1927–)

 
American economist Paul Volcker served from 1979 to 1987 as chairman of the Federal Reserve System. He is
credited with reducing the runaway inflation of that period with historic increases in interest rates. His influence on
U.S. economic policy continued into the early twenty-first century, when he served as head of the Economic
Recovery Board under President Barack Obama, charged with helping navigate the U.S. economy through the
global financial crisis that began in 2007.
Former Federal Reserve Board chairman Paul Volcker, credited with taming inflation in the early 1980s by raising
interest rates to above 20 percent, returned to government in 2009 as head of President Barack Obama’s
Economic Recovery Advisory Board. (Win McNamee/Getty Images)
Born on September 5, 1927, in Cape May, New Jersey, Volcker grew up in Teaneck, New Jersey. He graduated
in 1949 from Princeton University and, in 1951, earned a master’s degree in political economy and government
from Harvard University’s graduate schools of Arts and Sciences and Public Administration. He attended the
London School of Economics from 1951 to 1952.
Volcker started his career in 1952 as an economist with the Federal Reserve Bank of New York. He joined Chase
Manhattan Bank in 1957 (returning in 1965 as a director of planning) and in 1962 joined the U.S. Treasury
Department as a director of financial analysis, becoming undersecretary for monetary affairs in 1963.
Volcker next served in the Department of the Treasury as undersecretary for international monetary affairs from
1969 to 1974, where he was involved in deliberations surrounding the U.S. decision to devalue the dollar that
resulted in the collapse of the Bretton Woods system (which pegged foreign currencies to the dollar, which in turn

was pegged to gold). After leaving the Treasury, he became president of the Federal Reserve Bank of New York
from 1975 to 1979.
In The Rediscovery of the Business Cycle (1978), Volcker stated his view that business cycles last for periods of
ten to twenty years, considerably shorter than the Kondratieff cycles of just over fifty years. The shorter cycles,
according to Volcker, do not occur in any predictable pattern or length. He also noted that since the end of World
War II, recessions generally have been milder than those before the war, largely because of government
interventions. One of the greatest challenges of the time, he believed, was restoring price stability without
adversely affecting economic stability. Volcker’s belief that the control of the money supply is crucial because of
the cause-and-effect relationship between inflation and money growth became a point of contention in the early
1980s. Volcker believed that there were limits on the extent to which the curtailment of the money supply could be
accomplished without having a negative impact on business.
As chairman of the Federal Reserve, Volcker took an activist approach to eliminating inflation. Record-high
interest rates in the late 1970s and early 1980s resulted in a dramatic economic downturn and made it
increasingly difficult for consumers and businesses to obtain credit. In 1982, the number of business failures in the
United States was almost 50 percent greater than in any year since 1945; two years later, the number had
doubled. In addition, unemployment rates were higher than at any time since the end of World War II. Volcker was
criticized by both Republicans and Democrats, many of whom urged a lowering of interest rates; some sought his
removal. Opponents of his activist approach argued that the Federal Reserve should not adjust interest rates to
affect business cycles.
By the mid-1980s, however, Volcker’s policy, which had continued under two administrations—that of Democrat
Jimmy Carter and that of Republican Ronald Reagan—began to show signs of success. The economic recovery
that had begun in 1983 continued to 1990. By then, inflation was so low that it was no longer a concern. Volcker
returned to the service of the White House in February 2009 as an economic adviser to President Barack Obama.
Volcker was named chairman of the President’s Economic Recovery Advisory Board, a panel of academics,
businessmen, and other private-sector experts charged with reporting to the president on the economic crisis and
policies to reverse it. Volcker resigned from the position in January 2011.
Robert N. Stacy
 
See also:  Federal Reserve System;  Inflation;  Recession and Financial Crisis (2007-); 
Recession, Reagan (1981-1982). 
Further Reading
Neikirk, William. Volcker: Portrait of the Money Man. New York: Congdon & Weed, 1987. 
Samuelson, Robert J. The Great Inflation and Its Aftermath: The Past and Future of American Affluence New York: Random
House, 2008. 
Treaster, Joseph B. Paul Volcker: The Making of a Financial Legend. Hoboken, NJ: John Wiley and Sons, 2004. 
Volcker, Paul A. The Rediscovery of the Business Cycle. New York: Free Press, 1978. 
Volcker, Paul A. The Triumph of Central Banking? Washington, DC: Per Jacobsson Foundation, 1990. 
Von Neumann, John (1903–1957)

 
Regarded as one of the most brilliant and influential mathematicians of the twentieth century, John von Neumann
made notable contributions in such diverse fields as quantum mechanics, computer science (he was part of the
team that constructed the ENIAC computer at the University of Pennsylvania), and statistics. He was also involved
in the Manhattan Project and the building of the first nuclear bomb. In economics, he is known for his work in
game theory and the modeling of supply-demand equilibrium.
He was born Neumann János Lajos on December 28, 1903, in Budapest, Hungary, and raised in a privileged
household. His father, Max Neumann, a successful banker in Budapest, purchased the honorific “von,” which
János later applied to his name. In 1911, von Neumann entered Budapest’s Lutheran gymnasium, where his
remarkable skills in mathematics were quickly recognized. After studying at the University of Berlin and earning a
degree in chemical engineering from the Technical Institute in Zürich, he went on to obtain a PhD in mathematics
from Pázmány Péter University (Budapest University) in 1926 with a thesis on the method of inner models in set
theory. Along the way, he published in 1923 what became the lasting definition of ordinal numbers.
After a year of postdoctoral study as a Rockefeller fellow at the University of Göttingen, von Neumann taught
mathematics at the University of Berlin (1926–1929) and the University Hamburg (1929–1930). He went to the
United States as a visiting professor at Princeton University in 1930 and was appointed to a professorship in
mathematical physics the following year. He was invited to join the prestigious Institute for Advanced Study at
Princeton in 1933 and taught mathematics there until his death some twenty-two years later. In the meantime, he
became a U.S. citizen in 1937.
Von Neumann had already become interested in the application of mathematical analysis to economic problems,
especially after extensive conversations with economist Nicholas Kaldor. In the 1930s, von Neumann published
two papers that had a great impact on the study of economics. The first was on game theory, or the use of
complex simulations to solve difficult quantitative problems. While the paper did not deal with economics directly,
it formed the basis of his later collaboration with Oskar Morgenstern, Theory of Games and Economic Behavior
(1944). In what would become a classic in the field, the book built on von Neumann’s work in game theory. Von
Neumann’s original studies explored the mathematics of two-person, zero-sum card games in which the players
know the cards. One area of particular focus was the strategies other players should follow to minimize their
maximum possible losses (“minimax”). Game theory emerged as a compelling new means of exploring
nonmathematical disciplines such as economics and politics.
Von Neumann’s second important paper on economic theory was originally presented in 1932, published five
years later, and translated into English in 1945 as “A Model of General Economic Equilibrium” (also known as the
“Expanding Economic Model”). In it, von Neumann presented a highly mathematical approach to the study of
market equilibrium, in which supply and demand combine to achieve maximum price stability—a sophisticated
model that helped explain the dynamics of the business cycle. This idealized model was based on production,
profitability, and wages in a hypothetical economic environment without external influences. While it was generally
agreed that the model of economic equilibrium was brilliantly conceived, it was also criticized for being too
abstract and removed from economic reality. The approach proved valuable, however, in that it provided a new
method, based on mathematical models, to examine questions of economic growth and equilibrium between inputs
and outputs.
During and after World War II, von Neumann served as a consultant to the Allied armed forces. He helped
develop the implosion method for bringing nuclear fuel to explosion and played a key role in the development of
both the atomic and hydrogen bombs. In 1940, he became a member of the Scientific Advisory Committee at the
Ballistic Research Laboratories in Aberdeen, Maryland. He served on the Navy Bureau of Ordnance from 1941 to
1955 and as a consultant to the Los Alamos Scientific Laboratory from 1943 to 1955. From 1950 to 1955, he was
also a member of the Armed Forces Special Weapons Project in Washington, DC. In 1955, President Dwight
Eisenhower appointed him to the Atomic Energy Commission. Von Neumann died in Washington, DC, on

February 8, 1957.
Robert N. Stacy
 
See also:  Kaldor, Nicholas;  Morgenstern, Oskar. 
Further Reading
Aspray, William. John von Neumann and the Origins of Modern Computing. Cambridge, MA: MIT Press, 1990. 
Dore, Mohammed, Sukhamoy Chakravarty, and Richard Goodwin, eds. John von Neumann and Modern
Economics. Oxford, UK: Oxford University Press, 1989. 
Macrae, Norman. John von Neumann. New York: Pantheon, 1992. 
Pierre, Andrew J., ed. Unemployment and Growth in the Western Economies. New York: Council on Foreign
Relations, 1984. 
Von Neumann, John. John von Neumann: Selected Letters, ed. Mikl´os R´edei. Providence, RI: American Mathematical
Society, 2005. 
Von Neumann, John. Theory of Games and Economic Behavior. Princeton, NJ: Princeton University Press, 2004. 
Wachovia
 
The Wachovia Corporation, created from the 2001 merger of two major North Carolina–based banks, was a wide-
ranging financial services company with branches in twenty-one states coast to coast. With assets in the hundreds
of billions of dollars by the mid-2000s, the Wachovia Corporation was one of the largest and most prominent
victims of the financial crisis of 2008–2009. Having expanded aggressively in the early and mid-2000s, Wachovia
was forced, in the face of mounting losses and a collapsing share price, to sell itself to Wells Fargo for a mere $15
billion.
Before the buyout, Wachovia offered a variety of financial services beyond commercial banking, including
mortgage banking, home equity lending, investment advisory services, insurance, securities brokering, investment
banking, and asset-based lending and leasing through its various subsidiaries.
The origins of the bank go back to 1866, when a banker named William Lemly founded the First National Bank in
Salem, North Carolina. Thirteen years later, the company moved to nearby Winston and was renamed Wachovia
National Bank, after the original German land grant in the region. In 1893, textile and railroad entrepreneur Francis
Fires opened a separate company known as Wachovia Loan and Trust Company, North Carolina’s first trust, in
what would soon become the merged city of Winston-Salem. In 1911, the two Wachovias also merged, becoming
the Wachovia Bank and Trust Company. The new company derived a large portion of its deposits from the wealth
generated by the state’s growing textile and tobacco industries.
For much of the twentieth century, the business was known for its conservative banking style, specializing in low-
risk lending. So sterling was the company’s reputation that at the beginning of the 1990s, it was one of very few
banks in the United States to earn a triple-A credit rating, having avoided some of the more speculative real-
estate lending that triggered the savings and loan crisis of the late 1980s and early 1990s.

Even after the U.S. Supreme Court’s 1985 Northeast Bancorp, Inc. v. Governors, Federal Reserve System
decision opened the door to interstate banking, Wachovia proceeded cautiously in its acquisitions, buying up a
few banks around the South between the mid-1980s and mid-1990s. By the mid-1990s, that strategy had
changed, however, as the company began to buy up not just other banks, but a variety of financial services
companies around the country.
Its future partner, First Union, had nearly as long a history. Founded as the Union National Bank of Charlotte in
1908, it merged fifty years later with the First National Bank and Trust Company of Asheville to form the First
Union National Bank of North Carolina. In 1964, First Union acquired the Raleigh-based national mortgage and
insurance firm Cameron-Brown Company. By the 1990s, First Union was also expanding rapidly, and by 2000 had
banking operations in East Coast states and the District of Columbia. First Union was also the home of the sixth-
largest securities broker-dealer network in the country, operating in forty-seven states. But First Union’s
expansion also created strains as it earned a reputation for poor service and experienced shaky share prices on
Wall Street.
In September 2001, First Union merged with Wachovia, keeping the more highly esteemed name of the latter, as
part of a wave of mergers in the financial industry made possible by changes in regulatory law and a belief among
players in the industry that to survive in the globalized financial marketplace, companies had to get big. To do
that, the newly merged company acquired other banks, including the Golden West Financial Corporation of
California in 2006.
It was this last purchase that left Wachovia vulnerable, as Golden West was heavily exposed to the subprime and
adjustable rate mortgages that were at the heart of the financial crisis that hit in late 2007. In the second quarter
of 2008, Wachovia posted nearly $9 billion in losses, a record for the company and an amount far greater than
analysts had predicted. New management was brought in, but the company’s share price continued to plummet in
September as a result of the failure of rival Washington Mutual.
Rumors began to spread that Wachovia was in serious trouble, leading depositors to reduce their holdings in the
bank to the $100,000 limit the Federal Deposit Insurance Corporation (FDIC) will protect. (Later during the crisis,
the insurance limit was increased to $250,000.) The depositor run made Wachovia’s collapse all but inevitable.
Because of its size, however, and the drain its failure would exact on the FDIC’s available funds, federal regulators
declared Wachovia “systematically important.” In other words, the federal government decided for the first time
that it would seek a buyer for a collapsing bank rather than a mere liquidation of its assets. On September 29,
2008, the FDIC announced that Citigroup would purchase Wachovia and that the federal government would
absorb up to $42 billion in losses Citigroup would suffer from its purchase.
But Wachovia’s management and shareholders balked at the deal, complaining that Citigroup was offering too low
a price for Wachovia stock. Just four days after the FDIC announced the Citigroup deal, Wachovia itself
announced that it was merging with Wells Fargo, a transaction that was completed by the end of 2008.
James Ciment and Frank Winfrey
 
See also:  Banks, Commercial;  Banks, Investment;  Recession and Financial Crisis (2007-). 
Further Reading
Brubaker, Harold.  “Wachovia Latest to Seek Merger Rescue.” Philadelphia Inquirer, September 18, 2008. 
Dash, Eric.  “Wachovia Reports $23.9 Billion Loss for Third Quarter.” New York Times, October 23, 2008. 
de la Merced, Michael J.  “Regulators Approve Wells Fargo Takeover of Wachovia.” New York Times, October 10, 2008. 
“Outclassed.” The Economist, April 21, 2001. 

Singletary, Michelle.  “Wachovia: Taking a Great Leap Northward.” 
 October 20, 1997. 
Wages
 
Jobs and wages are vital indicators of business cycles. Both closely shadow and are heavily influenced by the
economic forces at play during recessions and periods of expansion. Wages are defined as money paid or
received for work or services, usually determined and reported on an hourly, weekly, or monthly basis.
 Determining Factors
Wages vary significantly across occupations, which, according to economic theory and considerable research, are
determined largely by two factors: the supply of workers who are willing and able to do a particular job and
employer demand for workers in that job.
In general, the more people with the education, training, and desire to perform a job, the lower the wage rate; a
larger supply depresses the price of labor. This explains why, for example, low-skilled jobs—such as entry-level
positions in the fast-food industry—pay less than jobs requiring an advanced level of skill and education—such as
that of engineer or doctor. The advanced skill and educational requirements of a job tend to limit the number of
people who are able to do the job, which pushes up the wage rate.
In addition to the wage level, the desire of a worker to do a job for a particular employer may be determined by
working conditions and other aspects of the job, such as benefits, hours, location, and other factors. In general,
dangerous or otherwise undesirable working conditions or a geographically undesirable work location tend to limit
the supply of workers willing to work for certain employers, putting upward pressure on wages. By the same
token, all else being equal, workers are more likely to supply their labor to an employer who offers more generous
benefits, such as health insurance and a retirement plan, which tends to depress the wage rate. In general, there
is often an inverse relationship between wage levels and the availability of favorable benefits.
Wage levels are also affected by the local demand for workers, or lack thereof. In general, the greater the demand
by employers for a particular job skill, the higher the wage. If a job within the organization is deemed essential,
the business is willing to pay a higher salary to the employee in that job than to an employee in a less essential
role. For example, a professional baseball team is willing to offer a player a high salary if that player attracts fans
to the stadium, which raises total revenue for the franchise. The player is deemed much more essential to the
organization than a clerical worker in the front office or a peanut vendor, for example, and will be compensated
more generously—much more.
The productivity of labor is also positively related to employer demand. Labor productivity is defined as the number
of units of output per unit of labor input, usually measured in hours of labor. Employers value and compensate
productive labor, which is directly related to education, training, and experience; the use of equipment—such as
computers—that allows workers to produce more per hour; improvements in production technologies and
management organizational techniques; and public policies and societal attitudes that allow workers to make the
best use of their skills and talents. Labor productivity and, thus, wages, are generally pro-cyclical, meaning that
they decrease during recessions and increase during economic upturns. This is because companies are reluctant
to lay off workers during recessions and hesitant to hire when the economy begins to recover.
The demand for workers is a derived demand, meaning that it is driven by market demand for the product or
Washington Post,

service being provided. For example, the demand for autoworkers—and hence their wages—is driven by
consumer demand for new cars and trucks. The relatively high wages paid in the health care professions today
are derived from the increasing demand for health care services caused in part by the aging of the population.
Similarly, the relatively high wages in computer-science occupations are derived by the increasing demand for
computers both in businesses and at home.
Unions strive to raise the wages of their members through collective bargaining with employers. However, union
membership in the United States has been declining since the mid-1950s, when 33 percent of nonagricultural
workers were members; by 2008, the figure had fallen to 12.4 percent.
Researchers also observe that wages vary based on worker characteristics unrelated to job productivity, including
age, gender, and ethnicity. For example, in 2008, the median weekly earnings of females working full time were
only 80.5 percent of the median weekly earnings of males working full time; the black-white wage differential was
79.3 percent, and the Hispanic-white wage differential was 71.3 percent. Such gaps are a function of a variety of
factors, and not entirely of employer discrimination. The wage differentials may be explained in part by differences
among the groups in type and level of schooling, training, and experience, and other social factors.
Wages also play a key role in the economic cycle. According to classical economics, wages should decline during
economic downturns when unemployment increases. Theoretically, a greater number of workers looking for jobs
should lower wages, both for new hires and existing employees. Ultimately, according to this model, wages should
fall to a level that entices employers to hire, eliminating involuntary unemployment and lifting wages. However, as
the early-twentieth-century British economist John Maynard Keynes and others have pointed out, wages are
sticky, responding very slowly to economic downturns. Falling wages, then, do not clear the labor market of
excess idle workers, and this stickiness leads to persistent unemployment during economic downturns and often
slow hiring during recoveries. Moreover, some believe that falling wages, rather than clearing the labor market,
would make the situation worse. This is because debts such as mortgages are denominated in dollars, and when
wages fall, the real value of household debt increases—forcing many households into bankruptcy. Also, falling
wages and the crisis in confidence that they may engender can cause households to further reduce spending,
leading to a reduction in demand and further layoffs.
Among the chief reasons why wages respond slowly to economic fluctuations is that wages are not determined by
what economists call an “auction market,” but by an “administered market.” In the former, buyers and sellers of
goods or services bargain with each other freely, with little constraint. In the case of employers and employees,
however, most companies have fixed pay scales that do not adapt immediately to changing economic
circumstances. Employers fear that constantly lowering wages might sap employee morale. In unionized industries,
wages are also affected by “outside” parties representing workers. Finally, labor laws of various kinds, including
minimum wage rules and, in some localities, “living wage” laws, also reduce wage flexibility. In short, employee
wages constitute an administered, rather than an auction, market.
 Labor Laws
Laws enacted to ensure fair labor and wage practices in response to discriminatory and other unfair practices on
the part of employers include the Fair Labor Standards Act (FLSA), passed in 1938 and administered by the U.S.
Department of Labor. The legislation established minimum wage, overtime pay, and child labor standards for most
private and government employees. Workers employed as executive, administrative, professional, and outside
sales employees who are paid on a salary basis are exempt from minimum wage and overtime laws. In the many
states that have their own minimum wage requirements, employers are required to pay the higher of the two (state
or federal). As of July 24, 2009, the U.S. federal minimum wage was $7.25 per hour. Employers can pay workers
less than the minimum wage if the employee receives tips, but the total compensation must equal or exceed the
hourly minimum. Employers may legally pay less than the federal minimum wage to workers younger than twenty
years of age for the first ninety days of employment and only if their work does not replace other workers. Other
programs that allow for payment of less than the full federal minimum wage apply to workers with a disability, full-

time students, and vocational education students.
Federal legislation prohibiting wage discrimination based on worker characteristics unrelated to productivity,
enforced by the Equal Employment Opportunity Commission (EEOC), include the Equal Pay Act of 1963, Title VII
of the Civil Rights Act of 1964, the Age Discrimination in Employment Act of 1967, Title I of the Americans with
Disabilities Act of 1990, and the Lilly Ledbetter Fair Pay Act of 2009.
Donna Anderson
 
See also:  Employment and Unemployment;  Income Distribution;  Productivity. 
Further Reading
McConnell, Campbell R., Stanley L. Brue, and David A. Macpherson. Contemporary Labor Economics.  8th ed. New
York: McGraw-Hill/Irwin, 2009. 
U.S. Bureau of Labor Statistics. Usual Weekly Earnings of Wage and Salary Workers: Fourth Quarter 2008. Washington,
DC: U.S. Department of Labor, 2009. 
U.S. Department of Labor.  “Wages.” www.dol.gov/dol/topic/wages
Washington Mutual
 
Washington Mutual, often abbreviated as WaMu, was a primary player in the financial meltdown of 2008 and a
telling example of how the failure of one major bank can have unexpected consequences for a major economy
and help to spur on economic recession.
WaMu was the sixth-largest bank in the United States, and the largest savings and loan, until its 2008 failure—
the largest bank failure in American history. “WaMu” refers either to Washington Mutual Bank, which went into
receivership and had its assets sold to JPMorgan Chase, or Washington Mutual Inc., the holding company that
formerly owned the bank and has since filed for Chapter 11 bankruptcy.
 Prefailure History
Originally the Washington National Building Loan and Investment Association, incorporated in Seattle in 1889,
WaMu changed its name to the Washington Mutual Savings Bank in 1917. Long a successful regional bank, its
major growth came after it demutualized in 1983. Demutualization is a process by which a mutual company (such
as a bank jointly owned by its depositors) converts to a publicly traded company owned by shareholders.
Demutualization was especially common in the 1980s in the wake of bank deregulation, as financial institutions
were eager to take advantage of the greater opportunities offered by the new legal landscape.
WaMu first acquired Murphey Favre, a Washington brokerage firm, in 1983, and by so doing, doubled its assets
between 1983 and 1989. More acquisitions rapidly followed—fifteen in all between 1990 and 2006, including
savings and loans and banks—partly in response to the Interstate Banking and Branching Efficiency Act of 1994,
which allowed bank holding companies to acquire branches outside of the state holding their charter.
These acquisitions changed not only the scale of WaMu’s operations but also their character, as the company

became the ninth-largest credit card issuer and third-largest mortgage lender in the country. Chief Executive
Office (CEO) Kerry Killinger in 2003 explicitly compared the company’s mission with Starbucks, Wal-Mart, and
Costco, companies that had gone from regional presences to well-known national corporate icons. “Five years from
now,” he said, “you’re not going to call us a bank.” The company’s heavy interest in high-risk mortgages to poor
borrowers and subsequent securitization and sale of those loans amid the escalating subprime mortgage market
resulted in Killinger’s prediction being correct, albeit in ways he clearly did not intend.
 Demise
At the end of 2007, in the midst of a subprime mortgage crisis the scale of which was not yet evident, WaMu
made drastic cuts to its home loan division, eliminating a quarter of its staff and half of its offices. Layoffs in the
holding company followed, with further closures of offices and loan processing centers. Killinger rejected a secret
deal offered by JPMorgan Chase to buy out the company, considering the offer too low. Killinger himself stepped
down as chairman in June 2008 and was dismissed as CEO in early September. Stock prices were falling, and
shareholders were perturbed at the capital influx by outside investors funded by TPG Capital, which diluted the
ownership stake of existing shares.
The JPMorgan Chase offer had been for $8 a share. In 2007, WaMu stock had traded for $45 a share, but by the
middle of September 2008 it had plummeted to close to $2 and WaMu was quietly courting potential buyers, with
no success. The obvious struggles of the company led to a massive ten-day run on the bank as customers
panicked, with nearly $17 billion taken out in deposits, primarily through electronic means. The Treasury
Department insisted WaMu find a buyer, and the Federal Deposit Insurance Corporation (FDIC) took over the
search. On the night of September 25, 2008, the 119th anniversary of WaMu’s founding, the Treasury
Department’s Office of Thrift Supervision (OTS) put Washington Mutual Bank (but not the holding company) into
receivership and sold its assets to JPMorgan Chase for $1.9 billion.
The new owners acquired WaMu’s assets, but not its equity obligations, which meant existing WaMu stock was
nearly worthless, dropping to $0.16 a share and prompting the bankruptcy filing of the holding company and an
as-yet unresolved protest by existing shareholders (first quarter of 2010), who have been exploring the possibility
of a lawsuit over what they declare an illegal takeover by the Treasury. The takeover required the OTS to declare
that WaMu was unstable and unable to meet its obligations; the contention of the potential lawsuit is that while the
bank was troubled, it had not lost enough liquidity to require receivership.
Account holders formerly with WaMu have been unaffected by the change; deposits have been transferred to
JPMorgan Chase, and WaMu customers can bank with Chase branches and use Chase ATMs without the usual
fees for noncustomers. The conversion of WaMu branches to Chase branches was completed by the first quarter
of 2010. Washington Mutual Inc. was delisted from the New York Stock Exchange; its remaining subsidiary, WMI
Investment Corporation, is a partner in various capital funds, which it has been selling off as part of WaMu’s
Chapter 11 proceedings.
Bill Kte’pi
 
See also:  Banks, Commercial;  JPMorgan Chase;  Recession and Financial Crisis (2007-). 
Further Reading
Krugman, Paul. The Return of Depression Economics and the Crisis of 2008. New York: W.W. Norton, 2009. 
Shiller, Robert J. The Subprime Solution: How Today’s Global Financial Crisis Happened, and What to Do About
It. Princeton, NJ: Princeton University Press, 2008. 
Soros, George. The New Paradigm for Financial Markets: The Credit Crisis of 2008 and What It Means. New
York: PublicAffairs, 2008. 

Wealth
 
Wealth is defined in economic terms as the amount of tangible and financial assets, minus liabilities, owned by a
person, household, enterprise, community, or nation. Thus, in economic terms, everyone and every country has
wealth, even if they are not “wealthy” in the popular sense of the word, meaning affluent. Moreover, wealth is
time-sensitive: it is the stock of what a person or collective possesses at a given point in time. Wealth is different
from income, which measures the flow of money or assets earned over a given period of time.
 Types of Wealth
Wealth can take either of two basic forms: tangible assets and financial assets. Tangible assets include real
property, or land, structures, precious metals, capital equipment, paid-for inventory, and personal property, such
as jewelry, vehicles, furniture, and so on. Financial assets include cash, bank deposits, stocks, bonds, and other
financial instruments. Certain types of property—particularly tangible property, such as houses, cars, and jewelry—
are more likely to be owned by private individuals and families, while other types of assets, such as capital
equipment, are more likely to be owned by businesses. For the economy as a whole, the net amount of financial
assets is equal to the amount of domestic and foreign financial assets owned less the amount of domestic and
foreign financial assets owed. In a closed economy with no foreign trade or capital flows, net financial assets
would be equal to zero, since every financial asset is a financial liability for someone else. Thus it is only the
excess of financial assets owned by economics actors in a country over the amount of financial liabilities owed by
the same actors that add to national wealth.
In capitalist countries, most of the wealth is privately owned, whether by individuals and households, business
owners, or shareholders of publicly held corporations. In the United States, for example, some 80 percent of all
wealth is held by the private sector, with about 80 percent of this owned by private organizations and corporations
(both profit and nonprofit) and 20 percent by private persons. The other 20 percent of all wealth is held by various
governments—local, state, and federal.
Aside from tangible and financial assets, wealth can be divided into three basic types. Some wealth generates
income directly. Among financial assets, there are instruments that pay annuities, or regular payments to share
owners. Among tangible assets, there are assets such as apartment buildings that bring the owner rent income
that exceeds the total cost of the building’s mortgage, maintenance, and depreciation. Other forms of wealth do
not produce income but tend to increase in value over time, such as corporate securities, land, artwork, and so
on. Finally, there are forms of wealth that lose value. A computer is a form of wealth, but one that sheds its value,
or depreciates, relatively rapidly. In short, wealth accrues in two basic ways, through savings and through asset
appreciation.
On the other hand, the ordinary depreciation of a tangible asset is not the only way that wealth disappears.
Assets can also lose market value, of course. In the housing crash of the late 2000s, for example, most Americans
saw the value of their homes decline appreciably. In stock market crashes, securities lose value rapidly as well. In
all of these cases, wealth simply disappears. But other forces can work to redistribute wealth as well. For example,
governments that tax wealth through inheritance and property taxes, as well as other levies, often redistribute a
portion of that wealth as income to the poor.
Inflation also works to redistribute wealth. A critical type of asset—and one that is owned by many corporations

and financial institutions—are loans and bonds of various types. For example, a bank that has loaned $10,000 to
an individual to buy a car can list that $10,000 loan as an asset on its books. But if inflation eats away half the
value of the money over the life of the loan, then, all other things being equal, $5,000 in wealth has shifted from
the bank to the borrower. Inflation can also erode the value of tangible items. If, for example, someone were to
purchase a home worth $100,000 and, for whatever reasons particular to the local market, the home went up in
value only $20,000, or 20 percent, over ten years, it would lose value if the cumulative inflation over the same
time period were 40 percent.
 Wealth Distribution
Finally, wealth is never evenly distributed. Even in the most egalitarian societies, some individuals and households
enjoy greater wealth than others. Economists use a mathematical formula known as the Gini coefficient to
measure wealth (as well as income) distribution on a zero-to-one scale, with zero representing perfectly equal
distribution of wealth (everyone owns exactly the same amount) and one representing perfect inequality (one
individual owns all of the wealth).
Wealth distribution varies greatly across regions and through history. Economically developed regions of the world,
of course, claim a far greater portion of global wealth than developing regions. Europe, for example, is home to
just under 10 percent of the world’s population but owns more than 25 percent of the world’s wealth. Africa, by
contrast, is home to just over 10 percent of the world’s people but owns just 1.5 percent of the world’s wealth.
Generally speaking, then, the more advanced the nation is economically, the more evenly distributed its wealth.
Thus, industrialized economies tend to have lower Gini coefficients than do developing world countries. There are
a number of reasons for this. Developed nations usually have much larger middle classes than developing
nations. Workers in developed nations are more highly skilled, meaning they can command more of the income
their labor produces; and by commanding more income, they can save more or invest in assets that appreciate in
value, thereby increasing their wealth. Finally, developed nations tend to have more advanced social welfare
networks and tax systems to even out income and wealth distribution. Yet even among developed nations, there
are differences in wealth distribution. Countries with more generous social welfare systems—and redistributive tax
schemes to pay for them—tend to have greater wealth equality. Scandinavian countries, for example, have
relatively low Gini coefficient numbers.
Social factors within countries can have a major impact on wealth distribution as well. For example, in the United
States, white households have about ten times the wealth of African-American households, even though African-
American incomes were about 60 percent those of whites. There are two basic reasons for this. Lower income
means less opportunity for savings and investment, and a history of discrimination has meant that African-
Americans have lacked the opportunity to build up inherited wealth that they can pass on to their children.
Wealth distribution also changes over time. There have been periods in U.S. history, for example, where wealth
has been more equally distributed and periods when it has been less equally distributed. Industrialization and the
rise of large corporations in the late nineteenth and early twentieth centuries led to growing wealth inequality, while
the Great Depression, which destroyed the value of many assets, and the post–World War II boom, which saw a
rapid expansion of the middle class, saw greater wealth equality, at least through the 1970s. Since then, however,
there has been a trend toward greater wealth inequality. Economists point to a variety of factors: a weakened
labor movement, which depresses wages; tax cuts for the wealthy; and globalization, which increases corporate
profits, much of which accrue to the wealthiest portions of society.
Another great shift has been in the composition of household wealth. Whereas in the 1920s, less than one in ten
American households owned corporate securities, by the early 2000s, the figure had climbed to 50 percent, with a
good portion of that increase accruing since the 1980s. Whereas stocks represented just 6 percent of household
wealth in the late 1980s, by the early 2000s it had climbed to 15 percent. Note, however, that the bulk of
household stock market wealth is not held by stock ownership directly, but rather through ownership of a mutual
fund, a retirement account, or a pension plan that invests in the stock market.

Finally, the importance of wealth distribution to overall economic performance is an issue hotly debated among
economists. Some, particularly those on the right of the political spectrum, tend to see wealth inequality as a less
important factor in economic performance than those on the left. Indeed, they sometimes argue, more wealth
accumulating at the top creates more wealth available for investment, which spurs economic growth to the benefit
of all. Economists on the left side of the political spectrum argue that a more equal distribution of wealth assures
more sustained growth since it provides security to households, allowing them to spend money on consumer
goods—a crucial factor given that consumer spending generates some 70 percent of all economic activity in the
United States. In addition, it is argued, greater distribution of wealth allows households to spend more on
education, which assures a productive workforce in the future.
James Ciment
 
See also:  Consumption;  Income Distribution;  Poverty. 
Further Reading
Clark, John Bates. The Distribution of Wealth: A Theory of Wages, Interest, and Profits. New York: Cosimo Classics, 2005. 
Collins, Robert M. More: The Politics of Economic Growth in Postwar America. New York: Oxford University Press, 2000. 
Dowd, Douglas. U.S. Capitalist Development Since 1776: Of, By, and For Which People? Armonk, NY: M.E. Sharpe, 1993. 
Ferguson, Niall. The Ascent of Money: A Financial History of the World. New York: Penguin, 2008. 
Oliver, Melvin L., and Thomas M. Shapiro. Black Wealth, White Wealth: A New Perspective on Racial Inequality. New
York: Routledge, 2006. 
Ornstein, Allan. Class Counts: Education, Inequality, and the Shrinking Middle Class. Lanham, MD: Rowman &
Littlefield, 2007. 
Reynolds, Alan. Income and Wealth. Westport, CT: Greenwood, 2006. 
 
World Bank
 
The World Bank provides fiscal and technical support to developing countries and development programs (such as
for building schools, roads, and bridges), with the objective of reducing poverty. The role of the World Bank is to
provide aid in various forms to countries stuck in economic stagnation, with the purpose of helping them gain
stability and build long-term economic growth in the context of globalization. As part of its mission, the World Bank
conducts ongoing research on the causes and profiles of booms and busts during the business cycles of both
developed and developing countries.

The World Bank was one of two institutions (with the International Monetary Fund, or IMF) created in July 1944 at
the United Nations Monetary and Financial Conference under the Bretton Woods Agreement, with the primary
goal of rebuilding Europe after World War II. In the years and decades that followed, the goals and efforts of both
organizations expanded to rebuilding the infrastructure of Europe’s former colonies.
The World Bank today is headquartered in Washington, D.C., with 186 member nations as of year-end 2009 and
more than 100 offices worldwide. Its declared mission is “to fight poverty with passion and professionalism for
lasting results and to help people help themselves and their environment by providing resources, sharing
knowledge, building capacity and forging partnerships in the public and private sectors.” The institution—
sometimes referred to as the World Bank Group—is composed of the International Bank for Reconstruction and
Development (IBRD) and the International Development Association (IDA), as well as three lesser-known
agencies: the International Finance Corporation (IFC), the Multilateral Investment Guarantee Agency (MIGA), and
the International Centre for Settlement of Investment Disputes (ICSID). The specific duties of the World Bank are
overseen by twenty-four executive directors, including one each from the five largest shareholders—France,
Germany, Japan, Great Britain, and the United States—and the nineteen others selected from the remaining
member nations.
World Bank president Robert Zoellick attends the opening of a new office in Berlin, Germany, in 2007. A
specialized agency of the UN, the World Bank provides financial and technical assistance to middle-income and
poor nations. (Andreas Rentz/Getty Images)
The World Bank is active in a number of economic development areas, and includes the following departmental
programs: Agriculture and Rural Development; Conflict Prevention and Reconstruction; Education; Energy;
Environment; Gender Issues; Governance; Health, Nutrition, and Population; Industry; Information, Computing, and
Telecommunications; International Economics and Trade; Labor and Social Protections; Law and Justice;
Macroeconomic and Economic Growth; Mining; Poverty Reduction; Private Sector, Public Sector Governance;
Rural Development; Social Development; Social Protection; Trade; Transport; Urban Development; Water
Resources; and Water Supply and Sanitation.
 IBRD and IDA
The International Bank for Reconstruction and Development (IBRD) primarily works with middle-income and

creditworthy poorer countries to promote equitable and sustainable job growth, while simultaneously alleviating
poverty and addressing issues of regional and global importance. The agency is structured like a cooperative,
delivering flexible, timely, and tailored financial products, knowledge and technical services, and strategic advice.
Clients of the IBRD also have access to financial capital in larger volumes and more favorable terms with longer
maturities through the World Bank Treasury.
Specifically, the IBRD preserves borrowers’ financial strength by providing support during bust periods, when
poorer individuals are likely to be most adversely affected; helping meet long-term human and social development
needs that the world’s private financial markets do not support; using the leverage of financing to promote key
policy and institutional reforms (such as social safety net programs and anticorruption efforts); and providing
financial support (in the form of grants) in areas that are critical to the well-being of poor people.
While IBRD serves middle-income countries, the International Development Association (IDA) is the single largest
donor for social services in the world’s seventy-nine poorest countries, about half of which are located in Africa.
The agency was established in 1960 with the aim of reducing poverty and financial inequality, boosting economic
growth, and improving people’s living conditions. The money is lent on concessional terms with no interest
charges and repayments stretched over 35 to 40 years, including a 10-year grace period. Since its inception, the
IDA grants and credits have well exceeded $200 billion, reaching an average of some $12 billion annually in the
early twenty-first century. About half of the funding goes to countries in Africa.
 Research on Booms and Busts
Research by the World Bank on economic expansions and contractions worldwide shows that the first global real-
estate boom was reached around 1990 in most countries associated with the Organisation for Economic and Co-
operation and Development (OECD). Asset inflation—the condition in which the prices of assets such as real
estate have increased to far higher values than can be justified by underlying economic conditions—was
extraordinarily high at the time. In some European markets, real-estate values rose by almost 400 percent in just
one decade, followed by a sharp decline of almost 50 percent from peak values in the following five years. Such
volatility has been shown to be costly and destructive, with negative effects on the banking system, households,
and the economy in general. Given the irreversible globalization of the financial markets, World Bank research also
indicated that, to help avoid future financial and economic crises, governments should limit incentives that lead to
overvalued assets and should more tightly regulate the banking and finance industry to limit the flow of cash into
overly risky projects.
 Mortgage Recommendations
The World Bank also has issued recommendations for lending in emerging markets based on the subprime
mortgage boom and bust that occurred in the United States in 2008–2009. That crisis raised the question of
extending mortgage loans to low-and moderate-income households. In most emerging markets, mortgage
financing is still considered a luxury, restricted to upper-income individuals and households. As policy makers in
these markets seek to regulate lending practices, the World Bank has recommended that they should adopt more
flexible policies that incorporate a wider variety of financing methods in order to steer many accounts out of the
subprime mortgage trap. Beyond that, the World Bank has recommended rules and regulations that force lenders
to more closely link rental or purchase agreements to the real and long-term financial capacity of the household.
 Criticisms
According to some critics, the World Bank has failed in its mission to promote development in the Third World by
failing to take into account the particular economic circumstances of a given country; instead, the bank imposes a
one-size-fits-all approach based on Western models. Moreover, the bank has sometimes been seen as promoting
export-oriented development over the needs of the local economy. Because of this emphasis on developing
exports—often low-priced ones intended for rich-country markets—some critics of the institution, particularly on

the political left, have accused it of primarily serving the economic and even political needs of the developed
world.
Critics charge that through all of its practices, the bank has unfairly placed a burden on the world’s poorer, ill-
equipped nations to compete against the more developed countries. As a result, the critics claim, the World Bank
has actually increased poverty in the poorer countries and has been detrimental to the environment, public health,
and cultural diversity.
Even the organizational structure of the World Bank has come under criticism, since the president of the institution
is always a U.S. citizen nominated by the president of the United States (subject to the approval of the other
member countries). It is also alleged that the decision-making structure of the World Bank is undemocratic, since
the United States has veto power on some constitutional decisions despite having only 16 percent of shares in the
bank. Reflecting that concern, seven Latin American nations have formed the Bank of the South to minimize U.S.
influence in the region. The issue of U.S. dominance aside, at least in part, critics charge that the internal
governance of the World Bank lacks transparency regarding the politics of how it conducts business.
Abhijit Roy
 
See also:  International Development Banks;  International Monetary Fund. 
Further Reading
Bret, Benjamin. Invested Interests: Capital, Culture, and the World Bank. Minneapolis: University of Minnesota Press, 2007. 
Gilbert, Christopher, and David Vines. The World Bank: Structure and Policies. New York: Cambridge University
Press, 2000. 
Goldman, Michael. Imperial Nature: The World Bank and Struggles for Social Justice in the Age of Globalization. New
Haven, CT: Yale University Press, 2005. 
Kulshreshtha, Praveen.  “Public Sector Governance Reform: The World Bank’s Framework.” International Journal of Public
Sector Management 21:5 (2008): 556–567. 
Mallaby, Sebastian. The World’s Banker: A Story of Failed States, Financial Crises, and the Wealth and Poverty of
Nations. New York: Penguin, 2004. 
World Bank:  www.worldbank.org
WorldCom
 
WorldCom, Inc., provided a sobering example to the global business community of a very large company brought
down by overextension, gross mismanagement, and unethical corporate behavior in the unregulated markets of
the late twentieth and early twenty-first centuries—the years leading up to the financial meltdown of 2007–2009.
WorldCom was the second-largest long-distance telecommunications provider in the United States, second only to
AT&T, in the late 1980s—when the importance of long-distance telephone service had not yet been greatly
diminished by the expansion of cellular telephone service. The company’s merger with MCI Communications in
1997 was the largest in American history to that time—as was its Chapter 11 bankruptcy filing in 2002. After

emerging from bankruptcy, WorldCom was acquired by Verizon Communications in February 2005, a month
before former chief executive officer (CEO) Bernard Ebbers was found guilty of fifteen counts of fraud and
conspiracy in yet another notorious landmark for the storied company—the largest accounting scandal in history.
WorldCom was founded as Long Distance Discount Services (LDDS) in 1983, amid the divestiture of AT&T into
seven regional operating companies (the “Baby Bells”). Ebbers, a motel chain operator, was soon hired as CEO,
and LDDS prospered. It aggressively acquired other small telecommunications companies, picking up sixty before
changing the firm’s name to WorldCom in 1995. The acquisitions included not only long-distance companies, but
also Internet companies such as MFS Communications, which owned UUNET Technologies, one of the largest
Internet service providers in the world.
In 1997, seemingly out of the blue, Ebbers and WorldCom announced an intention to acquire MCI
Communications. Founded as Microwave Communications, Inc., in 1963, MCI was originally devoted to building
microwave relay stations to transmit the signals from two-way radios used by the transportation industry. The
company went public in 1972 to raise money for infrastructure, using what it learned along the way to move into
other forms of telecommunications. It established a vast fiber-optic network during the 1980s and became the
second-largest telecommunications company by the end of the decade. MCI’s influence was instrumental in
spurring the antitrust case against AT&T that opened up the industry to other competitors.
WorldCom’s move to acquire MCI came on the heels of buyout offers from GTE and British Telecom, both of
which Ebbers successfully outbid. After a stock-swap deal of $34.7 billion, the restructured MCI WorldCom began
business on September 15, 1998. A further merger, with the Sprint Corporation, was immediately considered, and
a $129 billion deal worked out by the following fall—sufficient to catapult the resulting company past AT&T.
Although no official action was taken against the company, the merger was terminated at the urging of the
European Union and the U.S. Justice Department on antitrust grounds. WorldCom’s stock began declining, losing
the gains it had attained in anticipation of the Sprint merger, as well as suffering from the general slump of the
telecommunications industry.
Much of Ebbers’s personal wealth came from his MCI WorldCom stock, whose declining prices eroded his fortune.
To continue financing his other businesses, Ebbers secured hundreds of millions of dollars of loans from the
WorldCom board of directors. The board worried that without such loans, Ebbers would begin selling his stock,
further reducing the price. But the loans proved insufficient for Ebbers, who before long was forced to resign as
CEO. His debts to the company at the time totaled $408.2 million.
Meanwhile, internal auditors working after hours had discovered widespread fraud and financial misreporting on
the part of corporate management and notified the board of directors. Ebbers, Chief Financial Officer Scott
Sullivan, Controller David Myers, and Director of General Accounting Buddy Yates had artificially inflated the price
of WorldCom stock by overreporting revenues and underreporting costs, among other cases of fraudulent
accountancy. Sullivan was immediately fired, and the Securities and Exchange Commission (SEC) began an
investigation. After eighteen months, the SEC established that the fraud had exaggerated WorldCom’s assets by
$11 billion. Ebbers was eventually convicted of multiple felonies and sentenced to twenty-five years in prison;
WorldCom investors brought a class-action lawsuit against him and the other defrauders, which was settled out of
court for just over $6 billion. Sullivan, Myers, Yates, and Accounting Managers Betty Vinson and Troy Normand all
were convicted of related felonies.
A month after the SEC investigation began, on July 21, 2002, WorldCom filed for Chapter 11 bankruptcy. The
proceedings were held during the same months as those for Enron’s bankruptcy; the filings by the companies
constituted the two largest corporate bankruptcies in U.S. history. In the bankruptcy, MCI WorldCom shed the
WorldCom from its name and began doing business as MCI, Inc.—although most of its business consisted of
paying off its debts. Just months after MCI emerged from bankruptcy in 2004, Verizon Communications acquired
the company for $7.6 billion.
Bill Kte’pi

 
See also:  Corporate Corruption;  Technological Innovation. 
Further Reading
Cooper, Cynthia. Extraordinary Circumstances: The Journey of a Corporate Whistleblower. Hoboken, NJ: John Wiley and
Sons, 2008. 
Jeter, Lynne W. Disconnected: Deceit and Betrayal at WorldCom. Hoboken, NJ: John Wiley and Sons, 2003. 
Zarnowitz, Victor (1919–2009)
 
Victor Zarnowitz was a leading scholar in the fields of economic forecasting, business cycles, and economic
indicators whose influence extended well beyond academia. He was a senior fellow and economic counselor for
the Conference Board, an economist at the National Bureau of Economic Research (NBER), and a professor of
economics and finance at the University of Chicago’s graduate school of business.
Zarnowitz was born on November 3, 1919, in Lancut, Poland. He studied at the University of Krakow from 1937 to
1939, when he and his brother were forced to flee Poland during the Nazi invasion. The brothers were captured
by Soviet Russians and imprisoned in a Siberian labor camp, where Zarnowitz’s brother died. Zarnowitz escaped
and was reunited with his family; he later wrote about the experience in Fleeing the Nazis, Surviving the Gulag
and Arriving in the Free World (2008).
Zarnowitz earned a doctorate in economics in 1951 from the University of Heidelberg in West Germany, for which
he produced a thesis titled “Theory of Income Distribution.” While a student at Heidelberg, Zarnowitz also tutored
in economics at the university and at the Graduate School of Business in Mannheim, Germany.
Zarnowitz moved with his family to the United States in 1952 and took a position as a research economist at the
NBER in New York City. From 1956 to 1959, he was a lecturer and visiting professor at Columbia University. He
was appointed associate professor of economics and finance at the University of Chicago’s Graduate School of
Business, becoming a full professor in 1965 and professor emeritus following his retirement in 1989.
In his research, Zarnowitz concluded that economic forecasters were rarely accurate in predicting the turning
points of business cycles, since their data tended to be faulty or incomplete. He attempted to measure the length
of business cycles himself in order to help predict the durations of economic booms and busts. He advocated the
use of forecast averages instead of individual forecasts, and his approach later became standard practice in the
preparation of government budgets and predicting revenue streams from taxation. Zarnowitz’s method has also
been used by businesses to anticipate recruitment needs and expansion plans.
Zarnowitz was active in helping guide U.S. economic policy, serving as director of the Study of Cyclical Indicators
at the U.S. Department of Commerce’s Bureau of Economic Analysis from 1972 to 1975. He was a fellow of the
National Association of Business Economists, the American Statistical Association, an honorary fellow of the
International Institute of Forecasters, and an honorary member of the Centre for International Research on
Economic Tendency Surveys.
A prolific author, Zarnowitz wrote a number of books that integrated theoretical and practical understandings of
business cycles and their behavior, including Orders, Production, and Investment: A Cyclical and Structural

Analysis (1973); Business Cycles: Theory, History, Indicators, and Forecasting (1992), and What Is a Business
Cycle? (1992). He also co-authored and edited The Business Cycle Today (1972) and contributed numerous
academic papers, including “Recent Work on Business Cycles in Historical Perspective” in 1985; “Has the
Business Cycle Been Abolished?” in 1998; “Theory and History Behind Business Cycles” in 1999, and “The Old
and the New in U.S. Economic Expansion” in 2000.
As a member of the NBER’s Business Cycle Dating Committee, Zarnowitz was one of the seven economists who
officially identified the recession that began in late 2007. With a career that spanned six decades, he witnessed
firsthand every U.S. recession since the Great Depression. Zarnowitz died on February 21, 2009, in New York
City.
Justin Corfield
 
See also:  Confidence, Consumer and Business. 
Further Reading
Zarnowitz, Victor. Business Cycles: Theory, History, Indicators, and Forecasting. Chicago: University of Chicago
Press, 1992. 
Zarnowitz, Victor. Fleeing the Nazis, Surviving the Gulag and Arriving in the Free World. Wesport, CT: Praeger, 2008. 
Zarnowitz, Victor. Orders, Production, and Investment: A Cyclical and Structural Analysis. New York: Columbia University
Press, 1973. 
Zarnowitz, Victor.  “Recent Work on Business Cycles in Historical Perspective: A Review of Theories and Evidence.” Journal
of Economic Literature 23:2 (1985), 523–580. 
Zarnowitz, Victor, and G.H. Moore.  “Major Changes in Cyclical Behavior.” In The American Business Cycle Today:
Continuity and Change, ed. R.J. Gordon. Chicago: University of Chicago Press, 1986. 
Chronology
 
 
1630s 
Speculators send the price of exotic tulip bulbs soaring in newly wealthy Amsterdam, the
Netherlands, before market forces cause them to collapse; the episode, known as
“tulipmania,” is widely viewed by historians as the first great speculative bubble in the
history of capitalism.
1668 
The Sveriges Riksbank (Bank of the Swedish Realm) opens as the world’s first central
bank.
1694 
The Bank of England, which serves as the model for most other central banks, is founded in
London.
1716–
1720 
After winning a royal charter that gives it a monopoloy on trade with French North America,
Paris-based Compagnie d’Occident (Company of the West) draws thousands of investors;
its soaring stock price turns many investors into “millionaires,” a newly coined term, before
the bubble bursts, wiping out many fortunes.
1720 
In an episode known as the South Sea Bubble, British investors bid up the share price of

the South Sea Company, a joint-stock company that had been given a royal monopoly on
trade with Latin America, by nearly 1,000 percent before lack of credit causes it to tumble;
the collapse wiped out the fortunes of thousands of middle-and upper-class investors.
1776 
Scottish economist Adam Smith publishes The Wealth of Nations, which laid the foundation
for the classical economic tradition.
1791 
Congress establishes the First Bank of the United States, the country’s first central bank; its
charter is allowed to lapse twenty years later.
1792 
A group of merchants and financiers found the New York Stock Exchange.
1798 
British economist and demographer Thomas Malthus publishes his influential Essay on the
Principle of Population, on demographic cycles, in which he theorizes that population growth
inevitably outstrips agricultural production, leading to famines.
1812 
City Bank of New York, the predecessor institution of Citigroup, is founded.
1815–
1819 
The end of the Napoleonic Wars in Europe triggers a four-year economic boom in the
United States.
1816 
Congress establishes the Second Bank of the United States; its proposed rechartering is
vetoed by President Andrew Jackson twenty years later; Jackson saw the bank as an
institution that served the interests of wealthy eastern merchants and financiers rather than
those of small businessmen and farmers in the West and South.
1819 
The United States experiences its first major nationwide bank panic and recession.
1833–
1837 
Expanding credit creates a four-year economic boom and an unsustainable bubble in real-
estate prices.
1837–
1843 
A currency crisis and bank panic triggers the worst economic depression in U.S. history to
date, lasting six years.
1844 
The Bank of England, Britain’s central bank, is granted a monopoly on the issuance of
banknotes, or paper money.
1857 
The collapse of an Ohio bank triggers a nationwide panic that plunges the country into a
brief but sharp recession.
French economist Clément Juglar publishes an article laying out one of the first theories on
the cyclical nature of capitalist economies.
1858 
German immigrant Mayer Lehman moves to Montgomery, Alabama, and joins his brother
Henry Lehman in founding the partnership that would eventually become the investment
bank Lehman Brothers.
1860 
Henry Varnum Poor publishes his History of Railroads and Canals in the United States,
which includes the first major credit rating of corporate securities.
1862 
British economist William Stanley Jevons publishes a paper entitled “On the Study of
Periodic Commercial Fluctuations,” one of the first major analyses of business cycles.
1867 
German economist Karl Marx publishes Das Kapital (Capital), a critique of capitalist
economics that helps lay the foundation for the various communist revolutions of the
twentieth century.
1869 
German immigrant businessman Marcus Goldman founds the investment bank that became
Goldman Sachs in 1882, when his son-in-law Samuel Sachs became a partner.
1871 
Carl Menger, founder of the Austrian school of economics, publishes The Principles of
Economics, the first major text to explore the concept of marginal utility, a key component of
modern economic thinking.
Financiers Andrew Drexel and J.P. Morgan found Drexel, Morgan & Company, predecessor
of the investment bank portion of the financial services company JP Morgan Chase.
1873 
A major bank panic sets off a recession that grips the U.S. economy through 1877; it
creates high unemployment and lower wages, and sets off major labor unrest.
1874 
British economist William Stanley Jevons publishes Principles of Science, in which he lays
out his theory connecting fluctuations in the business cycle to sunspot activity.
1886 
Charles Dow, founder of the Wall Street Journal, publishes an index of eleven leading

companies, the forerunner of the Dow Jones Industrial Average.
1890 
British economist Alfred Marshall publishes Principles of Economics, a seminal text of
neoclassical economics.
1893–
1897 
One of the worst economic downturns in history grips the U.S. economic and much of the
industrialized world.
1899 
American sociologist Thorstein Veblen publishes The Theory of the Leisure Class, in which
his contention that consumer behavior is often driven by irrational impulses challenges
neoclassical assumptions that consumers are rational agents.
1901 
A struggle over control of the Northern Pacific Railroad triggers a major panic on Wall
Street.
1904 
What is now the Bank of America is founded in San Francisco, as the Bank of Italy, by
Italian immigrant Amadeo Giannini.
1907 
A failed effort to corner the market in copper leads to a major financial panic, prompting
financier J.P. Morgan to arrange a $100 million infusion of capital to free up credit markets;
the panic prompts economists and investors to call for a U.S. central bank to provide
stability in the credit market, which leads six years later to creation of the Federal Reserve
System.
1908 
Horse-carriage manufacturer William Durant founds General Motors; the company was the
world’s largest automobile producer through most of the twentieth century.
1913 
The Federal Reserve Bank (Fed), America’s modern central bank, is founded; in a nod to
America’s federalist heritage and regional interests, the bank consists of twelve regional
banks, though the New York branch becomes the pacesetter of Fed policy.
1914 
Financier Charles E. Merrill founds his eponymous company, predecessor of the brokerage
firm and investment bank Merrill Lynch.
1919 
American Insurance Group (AIG) is founded in China by American businessman Cornelius
Vander Starr.
The Harvard University Committee for Economic Research begins publishing what comes to
be known as the three-curve barometer, one of the first econometric tools for measuring the
financial system.
1919–
1920 
Italian immigrant Charles Ponzi sets up an investment scheme involving international
postage, in which he pays existing clients with funds from new investors; the illegal pyramid
scheme quickly collapses, costing investors millions and putting Ponzi behind bars and
attaching his name to all similar pyramid schemes in the future.
1921–
1922 
A major recession hits the United States, partly as a result of over-rapid demobilization
efforts following World War I.
1922–
1925 
The Florida land boom sends real-estate prices in South Florida dramatically upward; the
boom ends as financial analysts begin to question whether land and home prices are too
high for their underlying value.
1923 
Germany undergoes a bout of hyperinflation in which the mark, the national currency, is
rendered virtually worthless, creating economic havoc and contributing to the political
instability that would give rise to the Nazi Party.
1926 
Soviet economist Nikolai Kondratieff publishes an article stating that the major capitalist
economies of the West experience long-term economic cycles, based on the interplay of
entrepreneurial activity and technological innovation; the article proved influential, and the
cycles of roughly sixty years in length were eventually named in his honor.
1927–
1929 
A stock market boom sends prices soaring on Wall Street; the Down Jones Industrial
Average rises from about 150 at the beginning of 1927 to more than 380 at its peak in
September 1929.
1929 
Over several weeks in October and November, securities listed on the New York Stock
Exchange plummet in value, resulting in billions of dollars of lost investor assets and
triggering an economic panic that led to the Great Depression.
1929–
Following a crash in U.S. corporate securities prices, the United States and much of the

1933 
rest of the world is plunged into the Great Depression, the worst economic downturn in
history.
1930 
Following the great Wall Street stock market crash of 1929, corporate securities prices
revive significantly (later economists described the revival as an “echo bubble”) before
plunging to even greater losses from 1931 to 1933.
1931 
In one of the first major works on monetary theory, Prices and Production, Austrian school
economist Friedrich von Hayek argues that monetary stability is key to avoiding the
excesses of the boom-bust cycle in capitalist economics.
1933 
In the wake of widespread bank failures, the Franklin Roosevelt administration creates the
Federal Deposit Insurance Corporation (FDIC) to insure deposits at commercial banks and
prevent bank panics; the FDIC is one of a variety of agencies, programs, and policies
initiated by the Roosevelt administration to counter the Great Depression through regulatory
reform and countercyclical spending.
American economist Irving Fisher publishes his article “The Debt Deflation Theory of Great
Depression”; it describes how high debt during boom periods can lead to a vicious cycle of
debt and deflation during economic downturns.
In the wake of revelations about the speculative excesses of banks and the role they played
in the Great Stock Market Crash of 1929, Congress passes the Banking Act, better known
as the Glass-Steagall Act, which, among other things, prevents banks from engaging in
such investment bank activities as underwriting corporate securities.
Investment bank Morgan Stanley is created as a result of the Glass-Steagall Act requiring
J.P. Morgan and Company to divest itself of its commercial banking activities.
1934 
Hungarian-born economic Nicholas Kaldor first formulates his “cobweb theory,” which
challenged the conventional economic thinking that disruptions to agricultural and other
markets did not always correct themselves automatically.
Congress and the Roosevelt administration create the Federal Housing Administration to
insure mortgages offered by commercial institutions, thereby making those mortgages more
affordable.
In the wake of the 1929 stock market crash and revelations of securities fraud on Wall
Street, Congress passes the Securities Exchange Act, establishing the Securities and
Exchange Commission to oversee and regulate the securities trading industry.
1936 
British economist John Maynard Keynes publishes The General Theory of Employment,
Interest and Money; it critiques the equilibrium paradigm of classical and neoclassical
economic thinking and emphasizes the importance of aggregate demand to economic
growth and the role of government fiscal and monetary policies in addressing downturns in
the economic cycle.
1937–
1939 
Responding to fears that the federal deficit will dry up funds needed for private investment,
President Franklin Roosevelt cuts back on stimulus spending even as the Federal Reserve,
fearing renewed inflation, hikes interest rates; the result is a two-year economic downturn
known as the Roosevelt recession.
1938 
The federal government creates the Federal National Mortgage Association (Fannie Mae),
an agency that buys and securitizes mortgages in order to make them more affordable; it
would be turned into a private shareholder-owned company in 1968.
1939 
The National Association of Security Dealers, predecessor of the National Association of
Securities Dealers Automated Quotation, now officially known as Nasdaq, is founded.
1942 
In his book Capitalism, Socialism and Democracy, Austrian school economist Joseph
Schumpeter popularizes the term “creative destruction” to describe how innovation drives
capitalist growth.
1944 
Amid the waning days of World War II, Allied powers meet at Bretton Woods, New
Hampshire, where they hammer out plans for the international economic order, including
rules on currency exchange and trade; they also lay the foundations for the International
Monetary Fund (IMF) and the World Bank to provide capital for investment in war-damaged
and underdeveloped parts of the globe.

1947 
Twenty-three major economies agree to establish a global free-trading regime known as the
General Agreement on Tariffs and Trade (GATT), the predecessor of the World Trade
Organization (WTO).
1948 
Australian-born American economist Alfred Jones establishes the first hedge fund; a
flattering 1966 article on the fund in the business magazine Fortune helps launch the hedge
fund industry.
1952 
American economist George Katona begins publishing the quarterly Survey of Consumer
Attitudes, the first major measurement of consumer confidence.
1957 
Standard & Poor’s, a credit rating agency, creates the widely watched S&P 500 Index of
representative large companies.
1960 
American financier Bernard Madoff founds a broker-dealer firm that soon becomes a Ponzi
scheme, paying dividends to existing clients with the investment funds of new clients; losses
to investors had come to $65 billion when the scheme was exposed in 2008.
1963 
American economists Milton Friedman and Anna Schwartz publish A Monetary History of
the United States, 1867–1960, a book that helped establish monetarism as a major school
of economic thinking.
1964 
President Lyndon Johnson signs into law the biggest tax cut as a percentage of gross
domestic product (GDP) in U.S. history; the cut, along with heavy defense spending on the
Vietnam War, contributes to the economic boom of the late 1960s.
1966 
An article in the business magazine Fortune highlighting the success of a hedge fund
spreads the popularity of this investment strategy.
1967 
Entrepreneur Alan Turtletaub founds The Money Store, a monoline lender that helps
pioneer the subprime mortgage.
1969 
Countrywide Financial, which will become one of the largest originators of subprime
mortgages, is founded in California.
1969–
1970 
Following discoveries of nickel deposits by Poseidon NL, and Australian mining firm,
investors run up prices on Australian mining shares; the so-called Poseidon bubble
collapses when returns on investment prove lower than expected.
1970 
Congress creates the Federal Home Loan Mortgage Corporation (Freddie Mac) to buy
home mortgages and to provide competition for the newly privatized Fannie Mae.
In the largest bankruptcy in American history to the time, railroad giant Penn Central
collapses, a victim of changes in transportation patterns and the decline in the American
railroad industry.
1971 
As an inflation-fighting measure, President Richard Nixon imposes a ninety-day freeze on
wages and prices; Nixon cancels the direct convertibility of U.S. dollars into gold, allowing
the dollar to float against other currencies and ending the international financial order set in
place at the Bretton Woods Conference in 1944.
The National Association of Security Dealers, the predecessor of Nasdaq, sets up the first
electronic securities exchange.
1973–
1974 
A sudden hike in oil prices helps set off a global recession and begins a period marked by
“stagflation,” in which slow or negative growth is accompanied by inflation.
1973–
1981 
Oil-price hikes create vast fortunes for a number of oil-exporting countries, particularly in the
Middle East, which they plow into investments in the United States and other Western
countries.
1977 
Congress passes the Community Reinvestment Act, encouraging commercial banks and
savings and loans to make credit and mortgages more available to low-income borrowers.
1978 
To enhance competition in the industry, the U.S. Congress passes the Airline Deregulation
Act, which lowers prices and expands services but also contributes to the eventual
bankruptcy of several major carriers.
Chinese Communist Party chairman Deng Xiaoping pushes for the introduction of market
forces, launching China on its path to economic modernization and liberalization.
1979 
Deeply in debt and on the verge of bankruptcy, Chrysler, a major American automobile

manufacturer, receives billions of dollars in federal bailout money, helping to return the
company to profitability.
The Iranian Revolution pushes up oil prices; this second “oil shock” contributes to slow and
negative economic growth in much of the industrialized world in the late 1970s and early
1980s.
1981–
1982 
A dramatic hike in interest rates designed to wring inflation out of the economy contributes
to the worst economic downturn in American history since the Great Depression; the
recession is often referred to as the Reagan recession, after the sitting U.S. president
Ronald Reagan.
1982 
Congress passes the Garn– St. Germain Depository Institutions Act, freeing savings and
loans to move beyond their traditional role of financing home mortgages to invest in riskier
commercial real estate and business financing, which would contribute to the savings and
loan crisis of the late 1980s and early 1990s.
Congress passes the Alternative Mortgage Transaction Parity Act, which allows for the
adjustable interest rates and balloon payments at the heart of subprime mortgages.
Having borrowed heavily in foreign markets on the back of rising prices for its oil exports,
Mexico is pushed to the verge of sovereign default as oil prices plunge, forcing the
government to devalue the peso; with the plunge of the Mexican economy into recession,
foreign investment in Mexico and other Latin American markets is frozen.
The alternative and unregulated Souk al-Manakh stock exchange in Kuwait crashes, wiping
out billions of dollars in assets; the crash comes after several years in which newly oil-rich
Middle East investors plowed funds into securities listed on the exchange.
1983 
The first collateralized debt obligation, a financial instrument that pools debt securities, is
offered to investors.
1985 
Natural gas pipeline company Internorth and Houston Natural Gas merge to form a
company that is soon called Enron.
1986 
American economist Hyman Minsky publishes the first of his studies on what would become
known as Minsky’s financial instability hypothesis, in which he argues that, contrary to
mainstream economic thinking, financial markets are prone to instability as opposed to
equilibrium.
1987 
In the worst single-day percentage loss in its history, the Dow Jones Industrial Average falls
508 points, or 22.6 percent, on October 19, known as Black Monday, triggering a cascade
of stock market crashes around the world.
1989 
Having aggressively moved into speculative commercial real-estate lending—newly
permitted under 1982 deregulation legislation—some 327 savings and loans, with assets of
$135 billion, go bankrupt in what would come to be called the savings and loan crisis.
Congress then passes the Financial Institutions Reform Recovery and Enforcement Act,
providing tens of billions of dollars in bailout money to savings and loans and setting up the
Office of Thrift Supervision to oversee the industry.
1989–
1991 
Revolutions in Eastern Europe and the collapse of the Soviet Union cause the collapse of
command-style communist economies in countries throughout the region and their
replacement by market-oriented ones.
1990 
After a dizzying rise in real-estate and securities prices, the Japanese economy enters a
period of deflation and nearly flat economic growth that lasts through the rest of the century,
a period that would come to be known as the “lost decade.”
1992 
The U.S. Congress establishes the Office of Federal Housing Enterprise Oversight to
oversee the quasi-governmental mortgage insurers Fannie Mae and Freddie Mac.
In the wake of a speculative real-estate boom gone bust, Sweden’s banking system
experiences near collapse; the government responds by forcing the bank shareholders to
take major losses and then takes major equity stakes in leading commercial banks.
1994 
A series of political shocks shake foreign investor confidence in Mexico, causing capital
outflows; with the government on the verge of defaulting on its foreign loans, the U.S.
government, the International Monetary Fund, and other countries and institutions put

together a $50 billion bailout package.
1995 
An initial public offering of stock in Netscape, an early Internet browser, rakes in billions,
and helps set off a boom in high-tech and Internet stocks, the so-called dot.com boom of
the late 1990s.
Mexico’s dramatic devaluation of the peso causes foreign investors to pull out of bonds and
securities not only in that country but throughout Latin America; the phenomenon of
Mexico’s financial troubles infecting other Latin American markets comes to be called the
“Tequila effect.”
1996 
Federal Reserve chairman Alan Greenspan utters his now-famous “irrational exuberance”
remark about how overly optimistic investors were driving securities prices to unsustainable
levels.
1997 
Investment bank J.P. Morgan & Co. issues the credit default swap, a contract that transfers
the risk of default from the purchaser of a financial security to a guarantor.
1997–
1998 
Fearing a collapse in the baht, Thailand’s national currency, foreign investors begin to pull
capital out of securities markets in Thailand and other Asian countries, setting off the Asian
financial crisis and plunging much of Southeast Asia, South Korea, and other Asian
economies into a steep recession.
1998 
The European Union establishes a common central bank, the European Central Bank,
followed by the introduction of an electronically traded common currency, the euro; printed
euros appeared in 2002 and replaced most European national currencies.
To prevent what it fears could become a global financial crisis, the Federal Reserve
coordinates a multi-billion-dollar bailout by major U.S. banks of the hedge fund giant Long-
Term Capital Management.
Falling oil and natural resource prices damage Russian finances, leading to a $22 billion
loan from the International Monetary Fund, but it is not enough to prevent Moscow from
defaulting on its foreign debt obligations and imposing a ninety-day moratorium on loan
payments to nonresident lenders.
1999 
The U.S. Congress passes the Financial Services Modernization Act, or Gramm-Leach-
Bliley, overturning Depression-era restrictions on commercial banks engaging in investment
banking activities, as laid out in the Glass-Steagall Act of 1933.
2000 
As investors grow increasingly leery of poorly run, overhyped high-tech and Internet stocks,
securities prices plunge, putting many start-up companies out of business and signaling the
end of the so-called dot.com boom of the late 1990s.
In response to the recession triggered in part by the collapse of the dot.com boom, the Fed
reduces the prime interest rate, lowering the price of credit and helping to fuel the housing
boom of 2003–2006.
2001 
American manufacturer Bethlehem Steel declares bankruptcy, marking a major milestone in
the deindustrialization of the United States.
As accounting and other scandals expose its faulty finances, Enron, a Houston-based
energy provider, declares bankruptcy.
2002 
In response to several high-profile corporate corruption incidents, Congress passes the
Sarbanes-Oxley Act, requiring more transparency in corporate accounting.
In the wake of the collapse in high-tech stocks and revelations of corporate scandals, the
telecommunications giant WorldCom declares bankruptcy, becoming the largest firm in U.S.
history to do so up to that date.
2003–
2006 
Low interest, easy credit, and loose lending standards fuel a dramatic run-up in housing
prices, particularly in the urban Northeast, the Southwest, and Florida, with the national
median price peaking in July 2006.
2007 
March:  After months of declining U.S. home prices and waves of subprime mortgage
defaults, Fed chairman Ben Bernanke attempts to reassure international credit markets that
he believes the growing crisis will not spread beyond the subprime mortgage market.
August–September:  A series of high-profile subprime lenders—including Ameriquest
Financial and Luminent Mortgage Capital—declare bankruptcy.

September:  To stimulate the economy, the Federal Reserve begins cutting interest rates
for the first time in four years; the cuts, which continued through 2009, brought the effective
rate to near zero.
October:  The Dow Jones Industrial Average reaches its all-time peak of 14,164.53 on
October 9.
October–December:  A contracting economy ushers in the beginning of a U.S. recession
—the worst of the post–World War II era—that will continue through the second quarter of
2009.
2008 
January:  Bank of America buys out the struggling Countrywide Financial, the nation’s
largest mortgage lender; home sales fall to their lowest level in twenty-five years.
February 13:  As a recession-fighting measure, President George W. Bush signs a $170
billion stimulus package, largely consisting of tax cuts, credits, and rebates.
February 28:  The British bank Northern Rock, a major but now struggling player in its
country’s mortgage market, is taken over by the government, an indication that the bursting
housing bubble is not confined to the United States.
March:  Having grown dramatically in the credit boom of the early 2000s, Icelandic banks
begin to fail, causing a collapse in the nation’s currency and a rescue from the International
Monetary Fund, a rarity for a developed-world country.
May:  In an effort to prevent panic in international credit markets, Treasury Secretary Henry
Paulson coordinates the sale of investment bank Bear Stearns to fellow investment bank
JPMorgan and provides a $29 billion federal loan to facilitate the deal.
July:  Following a run by bank depositors, the Federal Deposit Insurance Corporation
(FDIC) seizes California’s IndyMac Bank.
September 7:  The federal government announces it is assuming control of Fannie Mae
and Freddie Mac, two government-sponsored but privately held entities that insured or
owned roughly half the mortgages in the country.
September 14:  Under federal government prodding, Bank of America agrees to buy
Merrill Lynch, a major investment bank on the verge of collapse.
September 15:  Lehman Brothers, the oldest investment bank in the United States, fails
after the federal government declines to bail it out.
September 18:  The federal government provides $85 billion in capital to American
International Group (AIG), the world’s largest insurance company and one heavily invested
in credit default swaps; the Federal Reserve and other major central banks pump $180
billion into the global financial system as a means of preventing a freezing up of
international credit markets.
September 19:  To prevent a collapse in the prices of financial stocks, the Securities and
Exchange Commission bans short selling of such stocks.
September 21:  The Federal Reserve approves the decision of Goldman Sachs and
Morgan Stanley, the last two standing investment banks, to convert themselves into bank
holding companies; this move gives them better access to Fed lending but also subjects
them to more regulatory scrutiny.
September 24:  Runs on international money market funds raise fears that interbank
lending, a key component of the international financial system, will freeze up.
September 25:  A depositor run forces the FDIC to put Washington Mutual, the nation’s
largest savings and loan, into receivership.
September 26:  The Federal Reserve and other central banks conclude two days of
injecting billions more into financial markets around the world.
September 29:  Citing a lack of specifics and oversight, the House of Representatives
rejects Treasury Secretary Henry Paulson’s $700 billion bailout package for the financial
system; the vote sends the Dow Jones Industrial Average plummeting 777 points—the
largest point drop in its history—on fears of a collapse of global financial markets.
October 3:  Chastened by collapse in securities prices and reassured that it will be given

more oversight, Congress passes the Emergency Economic Stabilization Act, which
provides $700 billion in bailout money to financial institutions as part of the Troubled Assets
Relief Program (TARP); global financial markets begin to stabilize.
December:  Failing automakers Chrysler and General Motors receive roughly $25 billion
of TARP money, but with the condition that they reorganize their operations.
2009 
February 4:  Responding to public outrage over large compensation packages, President
Barack Obama caps executive pay at firms receiving federal bailout money at $500,000 a
year.
February 17:  In a recession-fighting move, President Obama signs the American
Recovery and Reinvestment Act; it provides roughly $787 billion in stimulus money,
approximately one-third consisting of tax cuts and two-thirds in government spending, about
half of the latter in grants to financially strapped states and local governments.
February 18:  President Obama announces the Homeowner Affordability and Stability
Plan, which provides $75 billion to lenders to modify mortgage terms to aid homeowners
threatened with default.
March 9:  The Dow Jones Industrial Average bottoms out at 6547.05, down more than 53
percent from its peak in October 2007.
April 2:  Leaders of the G-20 group of the world’s largest economies meet in London and
pledge a collective $1.1 trillion to help emerging markets fight the global recession.
May 7:  The Federal Reserve releases the results of its Supervisory Capital Assessment
Program, popularly known as the “stress test,” which tested 19 major banks’ ability to
withstand a severe economic downturn; of the 19, 9 are deemed to have adequate capital
while the rest are told they would need to add $185 billion in capital to bring them up to the
standards set by the program.
June–September:  The U.S. economy experiences an annualized 3.5 percent growth
rate, marking the end of the 2007–2009 recession.
October:  The seasonally adjusted unemployment rate in the United States peaks at 10.1
percent, the highest level since the early 1980s.
October 14:  The Dow Jones Industrial Average climbs above 10,000 for the first time in
more than a year.
December 1:  AIG begins to pay back the bailout money it received from the federal
government in 2008.
December 2:  Bank of America announces that it will begin paying back the $45 billion it
received from TARP.
2010 
February:  On the verge of sovereign default, Greece is offered a major bailout by the
European Central Bank, which fears that the eurozone member’s default would create panic
in financial markets and undermine investor faith in the twelve-year-old currency.
March 26:  The Obama administration announces a new $75 billion initiative to help the
unemployed and also to help those who owe more than their homes are worth to stay in
their homes; the money is to come from unused and repaid TARP funds.
April 2:  The Department of Labor announces that the U.S. economy created 162,000 jobs
in March, the biggest gain in three years, but with thousands of unemployed once again
seeking work, the unemployment rate remains at the same level as in February, 9.7 percent.
April 20:  The Securities and Exchange Commission votes to charge the investment bank
Goldman Sachs with fraud for its involvement in the sale of mortgage-backed securities.
April 27:  The Standard & Poor’s credit agency downgrades Greece’s credit rating to
“junk” status.
May 2:  The European Union and the International Monetary Fund announce a $146 billion
rescue package for Greece.
June 26:  Disagreements arise at the G20 Summit in Toronto as European leaders,
spearheaded by Germany’s Chancellor Angela Merkel and France’s President Nicolas
Sarkozy, advocate renewed austerity to deal with the continuing lackluster economic

performance of the developed world economy, while U.S. President Obama pushes for
more stimulus measures.
July 21:  President Obama signs the Dodd-Frank Wall Street Reform and Consumer
Protection Act, establishing, among other things, an independent Consumer Financial
Protection Bureau within the Federal Reserve Board.
November 28:  The EU and IMF agree on a $114 billion rescue package for the trouble
economy of Ireland. 
2011 
March 11:  A massive earthquake and tsunami hit Japan, undermining growth in that
nation’s economy and causing disruptions in supply chains around the world, contributing to
continuing anemic growth in developed world economies.
March 18:  The U.S. Federal Reserve announces the results of its Comprehensive Capital
Analysis and Review, popularly known as the “stress test,” designed to test the adequacy of
the nation’s largest financial institutions in the face of various negative economic scenarios.
May 5:  The EU and IMF agree to provide $116 billion to aid the Portuguese economy,
ailing from the spread of the sovereign debt crisis of the Eurozone countries.
July 21:  With the Greek economy continuing to flounder, EU and IMF officials announce a
second bailout, this one worth $156 billion.
July 31:  On the eve of a government default, Republicans and Democrats in Congress
agree on a deal to raise the government’s debt ceiling; the deal fails to prevent Standard &
Poor’s from downgrading America’s debt rating.
August 7:   To halt the spreading crisis of confidence in the bonds issued by troubled
Eurozone member economies, the European Central Bank (ECB) announces a plan to buy
up Spanish and Italian government debt.
September 21:  In yet another attempt to revive the anemic U.S. economy, the Fed sells
$400 billion in short-term treasuries in exchange for longer-term bonds; the move is known
as “Operation Twist.”
October 27:  Eurozone leaders, headed by Merkel of Germany and Sarkozy of France,
agree to write off 50 percent of the Greece government’s bond debt but continue to insist on
tough austerity measures for the country.
November 1–3:  Facing the growing unrest of the Greek population, Prime Minister
George Papandreou announces a public referendum on austerity measures imposed as part
of the EU and IMF bailout plans; the announcement sends global financial markets into
turmoil, forcing Papandreou to call off the referendum.
November 30:  In an attempt to alleviate the effects of Europe’s sovereign debt and
budget crises, the world’s major central banks announce joint action to provide cheap loans
of U.S. dollars to banks in Europe and elsewhere.
December 9:  Great Britain is the only European Union member to vote against changes
to the EU treaty that would have imposed stricter budgetary discipline on member nations
and help alleviate the sovereign debt crisis spreading across the Eurozone; the British vote
effectively vetoes the measure.
Glossary
 A

AAA.  
The highest rating offered on a corporate bond by most credit-rating agencies.
ABS.
See asset-backed security.
acquisition.    
The purchase of one company by another.
adjustable rate mortgage.    
A mortgage in which a low initial interest rate is followed, after a set period of time, by an interest rate pegged to
an index.
aggregate demand.    
Total spending in an economy at a given time: consumption, investment, government, and net exports (amount
exports exceed imports).
aggregate supply.    
Total amount of goods and services businesses would like to produce over a given period of time.
algorithm.    
A step-by-step procedure for solving a mathematical problem. In finance, a mathematical formula or computer
program for estimating the future performance of securities or markets.
Alt-A.    
A mortgage offered to someone with good credit but an inability to document his or her income.
alternative trading system.    
A government-approved, nonexchange venue for the trading of securities.
amortization.    
The steady reduction in the principal of a loan over the term of the loan so that the balance is fully repaid by
maturity.
animal spirits.    
John Maynard Keynes’s term for optimistic expectations by investors and business managers.
annuity.    
An insurance contract that pays a given stream of income for a given period time or for the life of the beneficiary.
arbitrage.    
The purchase of a good or asset in one market and its immediate sale in another, with the purchaser earning a
profit based on a discrepancy in prices between the two markets.
ARM.    
See adjustable rate mortgage.
asset.    
A tangible or intangible item of value; buildings, financial assets, and brand names are all assets.
asset-backed security.    
A security that is collateralized by assets such as credit card borrowings, auto loans, school loans, or home
mortgages.
audit.    

An examination and verification of a company’s or individual’s financial records.
automated trading system.    
A system in which computers and computer programs determine the buying and selling of securities.
 B
bailout.    
Money provided to a firm or individual when that firm or individual is threatened with default; the term is usually
used when the money comes from the government.
balance of payments.    
A statement measuring the monetary transactions or flows between residents of one country and those of another.
bank cycle.    
See credit cycle.
bank holding company.    
A bank that controls one or more commercial banks or other holding companies by owning 25 percent or more of
the equity in each.
bankruptcy.    
The legal action of resolving unpaid liabilities and dispersing assets to creditors.
barrier to trade.    
A policy designed to limit imports.
bear market.    
A securities market where sellers outnumber buyers, driving prices down.
black swan.    
An event that is highly improbable but whose consequences, if it does occur, dwarf the consequences of more
probable outcomes.
bond.    
A financial security for which a borrower (issuer of the bond) agrees to pay the lender (purchaser of the bond)
interest payments based on the principal of the bond and coupon rate (usually in semiannual payments), as well
as the face value of the bond at maturity.
boom.    
A period of rapid economic growth and rising expectations often initiated by new economic developments. In the
first phase, investor expectation usually corresponds with economic realities. In the second phrase, investor
euphoria results in speculative investment leading to a turning point, where precipitous selling causes asset values
and economic activities to decline rapidly.
brokerage house.    
A f irm that buys and sells securities for clients.
bubble.    
The rapid and unsustainable inflation of asset values.
building cycle.    
A business cycle related to construction.
bull market.    

A securities market where buyers outnumber sellers, driving prices up.
bullion.    
Precious metal in noncoin form.
business cycle.    
Fluctuations in the output of national economies, usually marked by the period from high output to low output and
back to high output.
bust.    
A precipitous fall in economic activity brought on by the sudden realization by investors that the preceding boom is
unsustainable. A financial bust is quickly followed by business contraction, bankruptcies, and unemployment.
buying long.    
The conventional form of purchasing a security (the expectation is that it will increase in value); buying long is the
opposite of selling short.
 C
call.    
In economics, a demand for immediate payment on a debt, usually before the debt is due.
capital.    
In economics, all assets, aside from land and labor, utilized in production; produced goods used to produce other
goods.
capital account.    
The statement measuring the inflow and outflow of financial capital from a given country.
capital flight.    
The sudden withdrawal of financial capital from a given market (a term usually used in reference to a particular
national market).
capital flow.    
The flow of money across international borders.
capital goods.    
Produced goods used to produce other goods, including factories, equipment, tools, and so forth.
capital inflow.    
The flow of funds into a given market; the term is usually used in reference to a national market.
capital market.    
The market for equity and debt securities with an original maturity greater than one year.
capital outflow.    
The flow of funds out of a given market; usually used in reference to a national market.
capital-intensive.    
Referring to economic activity that requires large amounts of capital.
capitalization.    
The process in which companies get the funds they need to operate and expand.
cartel.    

A group of firms or countries producing a similar good or commodity that work together to set quantity supplied
and/or to determine price.
CDO.    
See collateralized debt obligation.
CDS.    
See credit default swap.
central bank.    
A government-operated bank that sets a nation’s monetary policy.
CMO.    
See collateralized mortgage obligation.
collateral.    
The assets offered to secure a loan.
collateralization.    
The bundling and selling of collateral or debt obligations, usually mortgages, as security.
collateralized debt obligation.    
A security created from the bundling together of a pool of financial assets where the payments made on the
underlying financial assets are passed on to the investors in the security.
collateralized mortgage obligation.    
A security created from the bundling together of a pool of mortgages where the mortgage payments are passed on
to the investors in the security.
command economy.    
An economy where resources are allocated primarily by government dictate.
commercial bank.    
A bank that takes deposits and makes loans. It provides checking, savings, and money market accounts (as
distinct from an investment bank).
commercial paper.    
Short-term debt instruments issued by financial and nonfinancial institutions.
commodity.    
A good for which there is a demand.
common stock.    
Equity claims representing ownership of the net income and assets of a corporation; common stock holders are
last in line to receive any payments from the corporation after all other lenders and creditors have been paid.
confidence.    
In economics, the belief by individuals or businesses that economic conditions will improve.
consumption.    
In economics, the total amount of spending on consumer goods.
cooperative.    
A jointly owned enterprise that produces or sells goods or services for the benefit of its owners.

cornering.    
The act of trying to control the supply of a given commodity in order to set prices and derive large profits.
corporation.    
A legal entity separate and distinct from its owners, usually established to conduct business and earn profit.
cost-push inflation.    
General inflation triggered by the upsurge in the price of a critical good or commodity, such as oil, or else caused
by a wage-price spiral in which wages rise at a faster rate than productivity in a critical sector of the economy,
such as auto manufacturing; the result in either case is a rise in prices.
countercyclical.    
Something that runs counter to the direction of the economy at a given point in time.
creative destruction.    
In capitalism, the process by which innovation creates new products, firms, and markets while destroying old ones;
the term was coined by Joseph Schumpeter.
credit.    
The use of someone else’s capital, with the promise of repayment, usually plus interest, at a future date.
credit cycle.    
Period of expanding and contracting credit.
credit default swap.    
A contract that, for a fee, transfers the default risk on a given security from the purchaser of that security to the
guarantor.
credit rating.    
A mutually agreed-upon measure of the creditworthiness of an issuer of debt or of a debt instrument, usually set
by a credit-rating agency.
credit union.    
Cooperatively owned depository institution whose members usually have a common affiliation, such as a common
employer.
crowd behavior.    
In booms and busts, the tendency of investors to buy when others are buying and sell when others are selling.
current account.    
The statement measuring the value of exports and the value of imports of goods and services, as well as transfer
payments, of a given country.
cyclical unemployment.    
Unemployment that results from contractions in the business cycle.
 D
debt deflation.    
A phenomenon in which excessive debt leads to deflation of prices, causing the real value of the debt to rise.
debt instrument.    
A financial instrument representing debt rather than equity of the issuer.
debt-to-equity ratio.    

The ratio of debt obligations to financial and real assets held.
decoupling.    
In international economics, the process by which developing-world economies become less affected by
fluctuations in developed-world economies.
default.    
The failure of a borrower to make an agreed-upon payment of interest and/or principal when due.
deficit.    
In finance, the gap by which expenditure exceeds income.
deficit finance.    
Deficit spending by a government in an economic downturn in order to offset a decline in private sector demand.
The concept was originated by John Maynard Keynes.
deficit spending.    
Spending by government that exceeds government revenues.
defined benefit plan.    
A retirement or pension plan in which an individual receives a set amount per a given time period once he/she
has retired from work.
defined contribution plan.    
A retirement or pension plan in which an individual (or the corporation in the name of the individual) puts aside a
portion of her/his income at each pay period.
deflation.    
A broad decrease in prices over time.
deindustrialization.    
The reduction or removal of manufacturing capacity and the economic change wrought by that process.
delinquency.    
In economics, the falling behind in payments on the interest and/or principal of a loan.
demand-pull inflation.    
A  classic form of inflation that is caused when aggregate demand outpaces an economy’s productive capacity.
demographic cycle.    
Fluctuations in human populations over time and the impact of those fluctuations on economies.
deposit insurance.    
Government guarantee to make good on deposits at banks and other depository institutions up to a certain level
should those institutions become insolvent.
depository institution.    
A financial institution that issues checkable deposits and uses them to make loans. The institution earns a profit on
the spread, the difference between what the institution earns on its assets and pays for its liabilities.
depreciation.    
The falling value of a good, asset, or currency.
deregulation.    
The removal of government regulations on economic activities.

derivative.    
A financial instrument that derives its value from other assets or securities.
devaluation.    
A reduction by monetary authorities of a currency’s value relative to other currencies.
development bank.    
A multilateral financial institution that receives money from developed-world countries and distributes it in the form
of loans or grants to developing-world countries.
discount rate.    
The interest rate a central bank charges on loans to commercial banks.
diversification.    
For the purposes of risk management, the putting together of a portfolio of investments whose returns are
relatively uncorrelated.
dividend.    
A payment made to stockholders from a company’s after-tax profits.
 E
earnings-to-price ratio.    
The ratio of dividends paid on a share of stock plus retained earnings to the share price.
echo bubble.    
A smaller asset price bubble that follows a larger asset price bubble after a short period of time.
econometrics.    
The application of mathematics and statistics to the study of economic behavior.
economic contraction.    
A decrease in the output of goods and services in an economic system.
economic cycle.    
See business cycle.
economic growth.    
An increase in the output of goods and services in an economic system.
economic indicators.    
Measures of aspects of the economy that help determine future performance of the economy as a whole.
economic policy.    
The fiscal and monetary policy of a government, designed for the purposes of assuring sustainable economic
growth, full employment, and stable prices.
effective demand.    
In microeconomics, the ability of an individual or firm to pay combined with the desire to buy; in macroeconomics,
a synonym for aggregate demand.
elasticity.    
In economics, the degree to which prices and wages react to market forces; the less they react, the more inelastic
they are.

emerging market.    
The financial market in a developing country.
endogenous.    
Generated from within an economic system, model, or theory, as opposed to being determined outside the system,
model, or theory.
entrepreneurialism.    
The act of starting a business and assuming the financial risks associated with that business in the hopes of
gleaning profit.
equilibrium.    
The state in which various economic forces balance one another so that there is no tendency to change; for a
business, the point at which the firm is maximizing profit and therefore has no incentive to change; for consumers,
the point at which utility is maximized; in macroeconomics, the point at which aggregate quantity demanded and
aggregate quantity supplied are in balance.
equities.    
Shareholder stakes (shares of stock) in a company.
equity.    
The difference between the market value of an asset and what is owed on that asset; in finance, a shareholder
stake in a company.
ergodic axiom.    
The argument that past economic history is a reliable basis for predicting future economic activity.
exchange rate.    
The value of a national currency in relation to the value of other national currencies.
exogenous.    
Generated from outside an economic system, model, or theory as opposed to being determined within the system,
model, or theory.
exposure.    
The total amount of credit committed to a borrower; also, the potential for gains or losses due to market
fluctuations.
extensive growth.    
Growth in aggregate gross domestic product.
externalities.    
See spillover effect.
externality.    
An activity that affects others for good or ill, without those others paying for or being compensated for the activity.
 F
face value.    
See nominal value.
factors of production.    
The resources—land, labor, capital (buildings, equipment, tools, and so forth)—utilized to produce goods and

services.
FICO score.    
A person’s credit-risk rating as established by the Fair Isaac Corporation and used as a basis to determine if
credit will be extended and, if so, at what interest rate.
financial crisis.    
A period when credit becomes tight, or less available, usually because of widespread fears of default by
borrowers.
financial deepening.    
See financial development.
financial development.    
The growth in the quantity of financial assets relative to the growth of gross domestic product; also, the increase
in the variety of assets available to savers and investors.
financial fragility.    
The degree to which a financial system is vulnerable to collapse.
financial friction.    
Occurs when a nonmarket force or thing hampers business, trade, or exchange.
financial innovation.    
The development of new financial operations, instruments, and institutions in response to regulatory and market
challenges and opportunities, and to changes in technology.
financial integration.    
The integration of one country’s financial markets with that of another country or countries.
financial intermediation.    
Borrowing for the purpose of re-lending, where the profit is the difference between what the intermediary earns on
its assets and what it pays for its liabilities.
financial market.    
A figurative place where various forms of securities are bought and sold.
financial modeling.    
The use of financial data to determine future expansions or contractions in the economy.
financial regulation.    
Government oversight of the financial markets and government enforcement of the rules governing financial
markets.
financial services institution.    
A company that provides a host of financial services, which may include commercial and investment banking,
insurance, brokerage, underwriting, and others services.
fiscal balance.    
The state at which a government’s revenues and expenditures are equal, producing neither a deficit nor a surplus.
fiscal policy.    
The taxing and spending policy of a government.
fixed business investment.    

The amount of money businesses invest in capital assets, primarily buildings and equipment with a lifespan of one
year or more.
flexible.    
In economics, the ability or willingness to respond to market forces.
flipping.    
Slang for the rapid buying and reselling of real property, ideally at a profit.
foreclosure.    
The legal process by which a lender seizes the collateral of a borrower, usually a home, after the latter defaults
on the loan.
foreign direct investment.    
Investment by foreigners in the productive assets of a given country.
foreign exchange.    
The currency of a foreign country.
free market economy.    
An economy in which resources are allocated primarily by market forces.
fungible.    
In economics, referring to the interchangeability of an asset or a commodity with a similar item of value; for
example, stocks and bonds are fungible in that one can be exchanged for the other.
furlough.    
The temporary laying off of a worker or the shortening of the hours an employee works for the purposes of saving
on labor costs.
futures agreement.    
In economics, an agreement to buy or sell a standardized quantity of a commodity or financial asset, at a price
determined today, on a standardized date in the future.
 G
galloping inflation.    
A rapid increase in prices, usually in an annual range of 20 to 1,000 percent.
game theory.    
A theory about competition based on gains and losses of opposing players and their strategic behavior.
GDP.    
See gross domestic product.
Gini coefficient.    
A measure of inequality, usually of income or wealth, that ranges between zero and one. A score approaching
zero indicates greater equality; a score approaching one indicates greater inequality.
globalization.    
The integration of markets around the world, implying a freer flow of goods and services, physical capital, people,
and financial capital.
government-sponsored enterprise (GSO).    
A privately owned company established under government aegis and with the explicit or implicit guarantee that the

government will assume the enterprise’s liabilities should that enterprise become insolvent. Fannie Mae and
Freddie Mac, which make mortgage loans, are examples.
gross domestic product (GDP).    
The total market value of the goods and services produced in a nation in the course of a year.
gross substitution axiom.    
The idea that every item in the market is a good substitute for every other item.
GSE.    
See government-sponsored enterprise.
 H
hedge.    
In finance, an investment made to limit the risk of other investments.
hedge fund.    
An investment fund in which high-net-worth investors pool their funds to purchase a basket of high-risk
investments designed, collectively, to cushion market fluctuations.
hoarding.    
The acquisition and holding of resources in expectation of future demand or future lack of supply.
holding company.    
A legal entity created to hold a controlling interest in other companies.
home equity loan.    
A loan secured by the equity in a home, usually secondary to a mortgage.
hyperinflation.    
An extremely high rate of inflation, usually measured in thousands or millions of percent per annum and the result
of excessive printing of money by the government.
 I
import substitution.    
A government policy for creating industries to supply goods and services that were previously imported.
income.    
The flow of wages, dividends, interest payments, and other monies during a given period of time.
index.    
A composite of values that measures the changes in a given market or economy.
inflation.    
A broad increase in overall prices over time.
information asymmetry.    
An economic exchange in which one individual or firm (usually the seller or borrower) has more information than
the other about the exchange (usually the buyer or lender).
initial public offering (IPO).    
The first offering of a company’s shares to the investing public.

innovation.    
The introduction of a new or improved product, production technique, or market.
insider trading.    
The illegal trading of securities by persons who have knowledge unavailable to the investing public.
insolvency.    
The inability to meet financial obligations.
intensive growth.    
Aggregate economic growth driven by increased productivity (higher output per unit than input), often a result of
technological advances.
interest.    
The price paid to borrow money at a given point in time, usually set as a percentage of the total borrowed. Also
the return on money lent.
international development bank.    
A multilateral institution that collects money from developed countries in order to make loans to developing
economies.
inventory.    
Goods kept on hand by retailers and wholesalers to meet future demand.
inventory cycle.    
A business cycle related to the building up and drawing down of business inventories.
investment.    
The money firms spend on newly produced tangible and intangible goods and services for the purposes of
earning more revenues; also the money spent by households on newly constructed housing or on purchases of
real estate with the intent of reselling at a higher price; also government spending on durable projects such as
roads and schools.
investment bank.    
A bank that specializes in financial market activities rather than lending money to or holding money for customers.
investment-grade bond.    
A corporate bond receiving a rating of BBB/Baa or above from a credit-rating agency, signifying that the bond is
safe and has a very low probability of default.
invisible hand.    
An expression coined by Adam Smith referring to the presumed self-regulating character of the market.
irrational exuberance.    
A term used by Alan Greenspan to describe extreme investor optimism divorced from market realities.
irreversibility effect.    
The reluctance to make large purchases or to loan money for others to make large purchases for fear that the
commitment of money cannot be reversed should earnings fall in the future.
 J
junk bond.    
A very risky bond that pays a high interest rate to compensate for the risk.

 L
labor discipline.    
A body of policies designed to ensure that people will seek work rather than more leisure time.
labor-Intensive.    
Referring to e conomic activity that requires large inputs of labor.
lag.    
The delaying of the settlement of a debt in international trade.
lagging indicator.    
An economic indicator that lags behind the performance of the economy as a whole.
laissez-faire.    
The notion that economic performance is best achieved when the government interferes as little as possible in the
workings of markets.
lead.    
The expediting of the settlement of a debt in international trade.
leading indicator.    
An economic indicator that anticipates the performance of the economy as a whole.
leveraging.    
In economics, borrowing money to make an investment.
liability.    
A debt or other financial obligation owed to another firm or individual.
liberalization.    
In economics, the process of freeing an economy from government regulation and control.
liquidation.    
The selling of assets by a bankrupt firm to pay off creditors.
liquidity.    
Holding cash or near-cash, such as government securities; the ability of having ready access to invested money;
the ability to sell an asset for cash.
liquidity preference.    
In a slump, the preference of households, businesses, and banks to hold money or near-money, such as
government securities, as the safest way to preserve assets otherwise available for spending or investment.
liquidity trap.    
A situation in which the expansion of the money supply fails to stimulate the economy because the demand for
money has become perfectly inelastic; that is, the demand for money remains flat no matter how far interest rates
are lowered.
loan-to-value ratio.    
The ratio of a loan amount to the value of the asset being purchased with money from the loan.
long wave theory.    
The theory that economies undergo long-term fluctuations in the rate of growth.

 M
macroeconomics.    
The study of the behavior of the economy as a whole.
marginal utility.    
The amount of satisfaction received from consuming an additional unit of a good or service.
mark to market.    
Pricing an asset at its current market value.
market correction.    
A drop in the value of a traded asset or security or of an index when investors decide it has been overvalued.
market reform.    
The introduction of market forces into an economy.
maturity.    
In economics, the length of time between a security’s issuance and the date on which it can be redeemed at face
value.
MBS.    
See mortgage-backed security.
mercantilism.    
An archaic economic theory based on the idea that national prosperity results from a positive balance of
payments.
merger.    
The fusion of two or more companies.
microeconomics.    
The study of the behavior of individual units such as firms and households within an economy.
mixed economy.    
An economy dominated by private enterprise but where the government exerts significant influence on and control
of economic activity.
monetary policy.    
The activities of a central bank in determining the money supply, interest rates, and credit conditions in order to
affect the overall level of economic activity and prices.
monetary stability.    
A goal of monetary policy, in which the value of a currency remains relatively stable over time.
money illusion.    
The process by which individuals mistake the nominal value of money for the real value of money.
money market.    
Financial market for short-term financial instruments; that is, those with an original maturity of one year or less.
money supply.    
Narrowly defined as M1 (the amount of currency in circulation plus the amount of checkable deposits) or M2
(broadly defined as M1 plus liquid assets and quasi monies, such as savings deposits, money market funds, and
so on).

monoline institution.    
An institution specializing in one form of financing.
monopoly.    
A market in which there is only one seller.
monopsony.    
A market in which there is only one buyer.
moral hazard.    
A situation that leads individual investors or firms to take excessive risks because they believe that possible
losses will be absorbed in part or in full by others, particularly by government or insurance companies.
mortgage.    
A loan taken out for the purchase of property, which is secured by the property.
mortgage-backed security.    
A security that is collateralized by a pool of mortgages.
multiplier mechanism.    
The process by which changes in investment or government spending trigger successive rounds of spending that
lead to subsequent and expanding changes in income and output.
mutual company.    
A company owned by its customers.
mutual fund.    
An open-ended investment trust that pools investors’ capital to buy a portfolio of securities.
 N
nationalization.    
The process by which privately owned companies are acquired by the government of the country where they are
located, with or without compensation.
natural rate of unemployment.    
The unemployment level associated with an economy utilizing all of its productive resources.
neutrality of money.    
An economic concept that states that changes in the money supply only affect prices in an economy and not the
output of goods and services.
NINA.    
Abbreviation for “no income, no assets”; a mortgage obtained by a borrower who does not have to document
income or assets.
NINJA.    
Abbreviation for “no income, no job or assets”; a mortgage obtained by a borrower who does not have to
document income, assets, or employment.
nominal value.    
Non-inflation-adjusted value, also known as face value.
nonperforming loan.    

A loan in which neither the interest is being paid nor the principal is being paid down.
 O
oligopoly.    
A market in which there are only a few sellers or where a few sellers dominate.
opportunity cost.    
The value that a person or firm places on a commodity or investment compared with the value that a person or
firm places on alternative commodities or investments that are declined because the one chosen is expected to
result in greater satisfaction or higher returns.
option.    
An agreement giving an investor the right—as opposed to the obligation—to buy a financial asset or commodity at
a given point in the future at a price determined today.
overinvestment.    
A situation in which businesses increase the level of investment beyond the equilibrium level of aggregate
investment in an economy.
overproduction.    
Excessive production that causes a lowering of profits and a slowing or contraction of the economy.
oversavings.    
A situation in which households are saving so much of their income that it lowers aggregate demand.
over-the-counter.    
Referring to the trade of a security directly between buyer and seller, outside an established stock or other
exchange.
overvaluation.    
A situation in which an asset’s price exceeds its intrinsic value.
 P
Phillips curve.    
That element of an economics graph that shows the trade-off between employment and inflation, indicating that
when unemployment goes down, wage inflation goes up.
political cycle.    
In economics, a business cycle that is determined by political events.
Ponzi scheme.    
An illegal financial arrangement in which current investors are paid profits or interest out of the capital invested by
new investors.
portfolio investment.    
The purchase by foreigners of stocks, bonds, and other financial instruments of a given country, which does not
result in foreign management, ownership, or control.
poverty level.    
An official level of income below which individuals or households cannot afford the basic necessities of life as
defined by a given society.

preferred stock.    
Stock whose holders have priority in the payment of a fixed dividend if the corporation earns a profit.
price equilibrium.    
The point where quantity demanded and quantity supplied meet, determining the price and quantity of a given
good or service.
price stability.    
A situation in which prices remain relatively unchanged over a given period of time.
principal.    
The amount of money borrowed.
private placement.    
A stock or bond issue offered directly to investors.
privatization.    
The process by which government-owned assets and firms are sold off to private investors.
production.    
The process whereby economic inputs are turned into economic outputs.
production cycle.    
The period of time required by a firm to provide a good or service and receive compensation for that good or
service.
productivity.    
The ratio of output to input, usually used in reference to labor.
profit.    
Total revenue minus total costs.
public debt.    
The amount owed by a government to bondholders.
public goods.    
Goods that provide benefits to large sectors of society or society as a whole that cannot be profitably created in
optimum amounts by private industry.
public works.    
The creation of public goods by the government.
publicly traded company.    
A company that has received legal permission to sell its shares to the public.
purchasing power parity.    
A comparison showing how much a given amount of money buys in various national economies.
pyramid scheme.    
See Ponzi scheme.
 Q
quant.    
Slang term for a person who uses training in the hard sciences or mathematics to calculate risk, uncertainty, and

other financial investment variables.
quantitative analysis.    
The use of mathematics to determine investment strategy in securities.
 R
random walk.    
The process by which the price of an asset varies in unpredictable ways as a result of new information about the
value of that asset.
real business cycle.    
Fluctuations in economic activity triggered by changes in technology.
real economy.    
The nonfinancial sector of the economy, where real goods and services are produced and sold.
real-estate investment trust (REIT).    
An open-ended investment trust that pools investors’ capital to buy a portfolio of real-estate properties.
real value.    
Inflation-adjusted value.
recession.    
A period of economic contraction, usually lasting two successive quarters or longer.
redlining.    
The process, usually illegal, in which banks refuse to offer loans or offer them at higher interest rates to minority
or low-income neighborhoods.
refinancing.    
Borrowing money to pay back a loan, usually on different terms.
REIT.    
See real-estate investment trust.
resource allocation.    
The means by which a society distributes its factors of production.
retirement instrument.    
A tax-deferred private financial instrument or plan that provides income for an individual once he or she retires
from work or is rendered disabled.
revenue.    
Income derived from the normal operations of a business; also, the funds obtained by a government through
taxes, fees, and other means.
reverse mortgage.    
A type of mortgage designed for senior citizens who want to convert the equity in their homes to monthly cash
payments from a lender.
risk.    
In economics, the possibility that an investment will experience a loss or a less-than-expected return in the future.
risk-based pricing.    

Determining loan terms using the credit history of the borrower.
 S
S&L.    
See savings and loan bank.
savings.    
Income not spent on consumption.
savings and loan bank (S&L).    
A bank that accepts savings deposits and primarily makes mortgage loans.
seasonal cycle.    
A business cycle determined by weather or other seasonal variables.
secured loan.    
A loan secured by assets.
securitization.    
The process by which various types of loans are bundled and sold to investors as securities.
security.    
An investment instrument issued by a government, corporation, or other organization.
share.    
A share of ownership in a company that entitles the owner to receive a share of profits.
shock.    
In economics, an exogenous event that has a dramatic effect on the performance of an economy or disturbs an
economic equilibrium.
short sale.    
The sale of the collateral used to obtain a loan, usually a home, for less than the value of the loan; a short sale
allows a mortgagor to avoid foreclosure.
short-selling (also known as “shorting”).    
Borrowing financial securities and selling them in anticipation of a drop in the price of the securities. In such a
case a profit can be made by repaying the loan of the securities with the securities purchased at a price lower
than the price at which the short seller originally borrowed the securities.
sinusoidal.    
In economic terms, wavelike deviations from long-term trends.
social responsibility.    
A belief that economic agents, including businesses, should look beyond profits and act in ways that enhance
social goals.
sovereign default.    
The failure of a government to make timely payments on the principal or interest of a loan or bond.
sovereign risk.    
The degree to which investors feel that a nation will be unable to meet the financial obligations of its sovereign
debt.

special-purpose vehicle.    
A limited liability company or other legal entity created for a specific economic activity.
speculation.    
Based on the prediction of future performance, the act of making an investment in the hope of receiving large
rewards.
speculative-grade bond.    
See junk bond.
spillover effect.    
Also known as externalities. Positive or negative external effects created by the activities of firms or individuals.
stabilization policies.    
Efforts by a government to achieve economic stability in the wake of endogenous or exogenous shocks.
stagflation.    
An economic phenomenon in which inflation coincides with slow or negative growth in the economy.
sticky.    
In economics, the inability or unwillingness of workers, employers, and consumers to respond to market forces
when setting or accepting wages and prices.
stimulus.    
Government spending or tax reductions designed to revive or spur demand.
stochastic model.    
A business-cycle model that takes into account random, unexpected shocks.
stock.    
An ownership share in a company.
stock exchange.    
A real (physical location) or virtual (electronic) auction market for the buying and selling of stocks.
stock market.    
A market for buying and selling stocks, whether in a physical stock exchange, through a network of dealers, or
more recently, electronically over the Internet.
stock market crash.    
The sudden collapse in the valuation of a broad array of stocks on a given exchange.
structured investment vehicle (SIV).    
An arrangement in which an investment firm borrows money by selling short-term securities at low interest rates
and then buys long-term securities that pay higher interest rates, making a profit for the investment firm.
stylized fact.    
In economics, the simplified presentation of empirical data.
subprime mortgage.    
A mortgage loan requiring little or no down payment made to a borrower with a poor credit history, usually at a
teaser (low) interest rate that is adjusted sharply upward after a few years.
subsidiary.    
A company in which controlling interest is held by another company.

subsidy.    
A government payment to firms or households for the purposes of lowering the cost of production or encouraging
the consumption of goods.
sunspot theory.    
The theory that extrinsic random variables (variables outside the system, such as sunspots) affect economic
activity.
surplus.    
In finance, the gap by which income exceeds expenditure.
surplus value.    
The value created by the production of goods by workers that is not returned to workers as compensation.
synthetic CDO.    
A complex financial derivative in which the underlying assets of the collateralized debt obligation (CDO) are not
owned by the creator (such as an investment firm); a synthetic CDO “references” a group of assets.
systemic financial crisis.    
A crisis that affects a broad sector of a national financial system or of the international financial system as a
whole.
 T
tariff.    
A tax on imported goods.
tax.    
A levy charged by government on products, activities, assets, or income for the purposes of raising revenues for
the government and influencing economic and social behavior.
T-bill.    
See treasury bill.
thrift.    
See savings and loan bank.
tiger.    
Slang for a fast-growing economy in the developing world.
too big to fail.    
An expression for a firm whose financial collapse would so destabilize financial markets that the government
becomes obligated to ensure its solvency.
toxic asset.    
Slang for an asset of questionable value whose presence on a firm’s balance sheet leads others to question the
solvency of that firm.
trade cycle.    
See business cycle.
tranche.    
A portion or allocation of the returns on an investment, often based on risk.

transition economy.    
A national economy undergoing transition from a command-style economy to a free-market economy.
transparency.    
In business, the act of opening up the operations of a firm to the public to ensure that the firm is operating in a
fair and legal manner.
treasury bill.    
A government debt security with a maturity date of one year or less.
treasury bond.    
A government debt security with a maturity date of at least ten years in the future.
troubled asset.    
See toxic asset.
trust.    
In economic history, a combination of companies under one board of directors created for the purpose of
controlling an industry and dictating prices to consumers.
 U
uncertainty.    
In economics, the possibility of profit or loss on an investment in the future.
underconsumption.    
A lack of consumer demand that causes a slowing or contraction of the economy.
undervaluation.    
A situation in which an asset’s intrinsic value exceeds its price.
underwater mortgage.    
Slang for a mortgage with a face value that is greater than the value of the house it has financed.
underwriting.    
A form of insurance in which an individual or institution agrees to take a fee for guaranteeing the purchase of a
specific quantity of a new security issue should public demand be insufficient.
unemployment.    
The state of being available—and looking—for work but unable to find it.
unsecured loan.    
A loan whose repayment is not secured by real or financial assets.
upside-down mortgage.    
See underwater mortgage.
 V
velocity.    
In economics, the speed at which money circulates in an economy.
venture capital.    
Private equity financial capital directly invested in the early stages of a new firm with high growth or profit
potential.

volume-weighted average price.    
The ratio of the combined value of all stocks traded (price of each stock times the number of shares traded) over
the course of a trading session divided by the total number of shares traded.
 W
wages.    
Money received by labor for work performed or service rendered.
wealth.    
The amount of tangible and financial assets, minus financial liabilities, owned by an individual, household, firm, or
nation.
windfall profit.    
Large, sudden, and/or unexpected profit.
Master Bibliography
 Further Reading
Abelshauser, Werner. The Dynamics of German Industry: Germany’s Path Towards the New Economy and the American
Challenge. Oxford, UK: Berghahn, 2005.
Acharya, Viral, and Matthew Richardson, eds. Restoring Financial Stability: How to Repair a Failed System. Hoboken, NJ:
John Wiley and Sons, 2009.
Acosto, Jarod R., ed. Assessing Treasury’s Strategy: Six Months of TARP. Hauppauge, NY: Nova Science, 2009.
Agarwal, Monty. The Future of Hedge Fund Investing: A Regulatory and Structural Solution for a Fallen Industry. Hoboken,
NJ: John Wiley and Sons, 2009.
Aghion, Philippe, and Peter Howitt. Endogenous Growth Theory. Cambridge, MA: MIT Press, 1998.
Ahamed, Liaquat. Lords of Finance: The Bankers Who Broke the World. New York: Penguin, 2009.
Akerlof, George A., and Robert J. Shiller. Animal Spirits: How Human Psychology Drives the Economy, and Why It Matters
for Global Capitalism. Princeton, NJ: Princeton University Press, 2009.
Akerman, Johan. Economic Progress and Economic Crises. Philadelphia: Porcupine, 1979.
Akerman, Johan. Theory of Industrialism: Causal Analysis and Economic Plans. Philadelphia: Porcupine, 1980.
Albrecht, William P. Economics. Englewood Cliffs, NJ: Prentice-Hall, 1983.
Aldcroft, Derek H. The European Economy, 1914–2000. London: Routledge, 2001.
Alexander, Kern, Rahul Dhumale, and John Eatwell. Global Governance of Financial Systems: The International Regulation
of Systemic Risk. New York: Oxford University Press, 2006.
Alexander, Nicholas. International Retailing. New York: Oxford University Press, 2009.
Alford, B.W.E. Britain in the World Economy Since 1880. New York: Longman, 1996.
Aligica, Paul Dragos, and Anthony J. Evans. The Neoliberal Revolution in Eastern Europe: Economic Ideas in the Transition

from Communism. New York: Edward Elgar, 2009.
Allen, Franklin, and Douglas Gale. Financial Innovation and Risk Sharing. Cambridge, MA: MIT Press, 1994.
Allen, Franklin, and Douglas Gale. Understanding Financial Crises. New York: Oxford University Press, 2009.
Allen, Robert Loring. Opening Doors: The Life and Work of Joseph Schumpeter. 2 vols. New Brunswick, NJ: Transaction,
1991.
Altman, Morris. Handbook of Contemporary Behavioral Economics: Foundations and Developments. Armonk, NY: M.E.
Sharpe, 2006.
Altucher, James. SuperCash: The New Hedge Fund Capitalism. Hoboken, NJ: John Wiley and Sons, 2006.
Al-Yahya, Mohammed A. Kuwait: Fall and Rebirth. New York: Kegan Paul International, 1993.
Ammer, Christine, and Dean Ammer. Dictionary of Business and Economics. New York: Free Press, 1977.
Anderson, Kym. Distortions to Agricultural Incentives: A Global Perspective, 1955–2007. New York: Palgrave Macmillan,
2009.
Antczak, Stephen, Douglas Lucas, and Frank Fabozzi. Leveraged Finance: Concepts, Methods, and Trading of High-Yield
Bonds, Loans, and Derivatives. Hoboken, NJ: John Wiley and Sons, 2009.
Ardagh, John. France in the New Century: Portrait of a Changing Society. London: Penguin, 2000.
Ariff, Mohamed, and Ahmed M. Khalid. Liberalization, Growth and the Asian Financial Crisis: Lessons for Developing and
Transitional Economies in Asia. Northampton, MA: Edward Elgar, 2000.
Arndt, H.W., and Hal Hill, eds. Southeast Asia’s Economic Crisis: Origins, Lessons and the Way Forward. St. Leonards,
Australia: Allen and Unwin, 1999.
Arnold, Lutz G. Business Cycle Theory. New York: Oxford University Press, 2002.
Aron, Janine, Brian Kahn, and Geeta Kingdon, eds. South African Economic Policy Under Democracy. New York: Oxford
University Press, 2009.
Aronowitz, Stanley. Just Around the Corner: The Paradox of the Jobless Recovery. Philadelphia: Temple University Press,
2005.
Arvedlund, Erin. Too Good to Be True: The Rise and Fall of Bernie Madoff. New York: Portfolio, 2009.
Asimakopulos, A. Investment, Employment and Income Distribution. Boulder, CO: Westview, 1988.
Aspray, William. John von Neumann and the Origins of Modern Computing. Cambridge, MA: MIT Press, 1990.
Auletta, Ken. Greed and Glory on Wall Street: The Fall of the House of Lehman. New York: Random House, 1986.
Aydin, Zulkuf. The Political Economy of Turkey. Ann Arbor, MI: Pluto, 2005.
Badger, Anthony J. The New Deal: The Depression Years, 1933–1940. Chicago: Ivan R. Dee, 2002.
Baer, Werner. The Brazilian Economy: Growth and Development. 4th ed. Westport, CT: Praeger, 1995.
Balen, Malcolm. The King, the Crook, and the Gambler: The True Story of the South Sea Bubble and the Greatest Financial
Scandal in History. New York: Fourth Estate, 2004.
Bamber, Bill, and Andrew Spencer. Bear-Trap: The Fall of Bear Stearns and the Panic of 2008. New York: Brick Tower,
2008.
Banks, Erik. Risk and Financial Catastrophe. New York: Palgrave Macmillan, 2009.
Barber, William J. Designs Within Disorder: Franklin Roosevelt, the Economists, and the Shaping of American Economic
Policy, 1933–1945. New York: Cambridge University Press, 1996.
Barlett, Bruce R. The New American Economy: The Failure of Reaganomics and a New Way Forward. New York: Palgrave
Macmillan, 2009.

Barnett, Vincent L. Kondratiev and the Dynamics of Economic Development: Long Cycles and Industrial Growth in Historical
Context. New York: St. Martin’s, 1998.
Barth, James. The Rise and Fall of the U.S. Mortgage and Credit Markets: A Comprehensive Analysis of the Market
Meltdown. Hoboken, NJ: John Wiley and Sons, 2009.
Baskin, Jonathan Barron. A History of Corporate Finance. New York: Cambridge University Press, 1997.
Bauer, Hans, and Warren J. Blackman. Swiss Banking: An Analytical History. New York: St. Martin’s, 1998.
Beatty, Jack, ed. Colossus: How the Corporation Changed America. New York: Broadway, 2001.
Bellofiore, Riccardo, and Piero Ferri. The Economic Legacy of Hyman Minsky. Vol. 1, Financial Keynesianism and Market
Instability. Northampton, MA: Edward Elgar, 2001.
Bellofiore, Riccardo, and Piero Ferri. The Economic Legacy of Hyman Minsky. Vol. 2, Financial Fragility and Investment in
the Capitalist Economy. Northampton, MA: Edward Elgar, 2001.
Belobaba, Peter, Amedeo Odoni, and Cynthia Barnhart. The Global Airline Industry. Chichester, UK: John Wiley and Sons,
2009.
Ben-Bassat, Avi, ed. The Israeli Economy, 1985–1998: From Government Intervention to Market Economics.  Cambridge,
MA: MIT Press, 2002.
Benston, George J. The Separation of Commercial and Investment Banking: The Glass Steagall Act Revisited and
Reconsidered. New York: Oxford University Press, 1999.
Bergsman, Steve. Maverick Real Estate Financing. Hoboken, NJ: John Wiley and Sons, 2006.
Berlin, Isaiah. Karl Marx: His Life and Environment. Oxford, UK: Oxford University Press, 1978.
Bernanke, Ben. Essays on the Great Depression. Princeton, NJ: Princeton University Press, 2000.
Bernstein, Irving. The Lean Years: A History of the American Worker, 1920–1933. Boston: Houghton Mifflin, 1960.
Bernstein, Michael A. The Great Depression: Delayed Recovery and Economic Change in America, 1929–1939.
Cambridge, UK: Cambridge University Press, 1989.
Bernstein, Peter L. A Primer on Money, Banking, and Gold. Hoboken, NJ: John Wiley and Sons, 2008.
Blanchard, Olivier. Macroeconomics. 5th ed. Upper Saddle River NJ: Prentice Hall, 2009.
Blaug, Mark. Economic Theory in Retrospect. London: Cambridge University Press, 1978.
Blaug, Mark. Great Economists Before Keynes. An Introduction to the Lives and Works of One Hundred Great Economists
of the Past. Atlantic Highlands, NJ: Humanities, 1986.
Bodie, Z., A. Kane, and A. Marcus. Essentials of Investments. Boston: McGraw-Hill/Irwin, 2008.
Bonner, William, and Addison Wiggin. Empire of Debt: The Rise and Fall of an Epic Financial Crisis. Hoboken, NJ: John
Wiley and Sons, 2006.
Bookstaber, Richard. A Demon of Our Own Design: Markets, Hedge Funds, and the Perils of Financial Innovation.
Hoboken, NJ: John Wiley and Sons, 2007.
Borcherding, Thomas E., ed. Budgets and Bureaucrats: The Sources of Government Growth. Durham, NC: Duke University
Press, 1977.
Borjas, George. Heaven’s Door: Immigration Policy and the American Economy. Princeton, NJ: Princeton University Press,
1999.
Borthwick, Mark. Pacific Century: The Emergence of Modern Pacific Asia. 3rd ed. Boulder, CO: Westview, 2007.
Boughton, James M., and Domenico Lombardi, eds. Finance, Development, and the IMF. New York: Oxford University
Press, 2009.
Boyes, Roger. Meltdown Iceland: How the Global Financial Crisis Bankupted an Entire Country. New York: Bloomsbury,

2009.
Braun, Hans-Joachim. The German Economy in the Twentieth Century: The German Reich and the Federal Republic. New
York: Routledge, 1990.
Bresnan, John, ed. Crisis in the Philippines: The Marcos Era and Beyond. Princeton, NJ: Princeton University Press, 1986.
Bret, Benjamin. Invested Interests: Capital, Culture, and the World Bank. Minneapolis: University of Minnesota Press, 2007.
Brian, Harvey W., and Deborah L. Parry. The Law of Consumer Protection and Fair Trading. London: Butterworths, 2000.
Brigham, Eugene F., and Michael C. Ehrhardt. Financial Management: Theory and Practice. Mason, OH: South-Western,
2008.
Bronner, Stephen Eric. Rosa Luxemburg: A Revolutionary for Our Times. University Park: Pennsylvania State University
Press, 1997.
Brown, Lester R., Gary Gardner, and Brian Halweil. Beyond Malthus: Sixteen Dimensions of the Population Problem.
Washington, DC: Worldwatch Institute, 1998.
Brownlee, W. Elliot. Federal Taxation in America: A Short History. 2nd ed. New York: Cambridge University Press, 2004.
Bruner, Robert F., and Sean D. Carr. The Panic of 1907: Lessons Learned from the Market’s Perfect Storm. New York:
John Wiley and Sons, 2009.
Brunner, Karl. Money and the Economy: Issues in Monetary Analysis. New York: Cambridge University Press, 1993.
Bullock, Charles Jesse. Economic Essays. Freeport, NY: Books for Libraries Press, 1968 (reprint ed.).
Bullock, Charles Jesse. Essays on the Monetary History of the United States. New York: Greenwood, 1969 (reprint ed.).
Burchardt, F.A., et al. The Economics of Full Employment: Six Studies in Applied Economics. Oxford, UK: Basil Blackwell,
1944.
Burdekin, Richard C.K., and Pierre L. Siklos, eds. Deflation: Current and Historical Perspectives. New York: Cambridge
University Press, 2004.
Burns, Arthur F. Wesley Clair Mitchell: The Economic Scientist. New York: National Bureau of Economic Research, 1952.
Burns, Arthur F., and Wesley C. Mitchell. Measuring Business Cycles. New York: National Bureau of Economic Research,
1946.
Burton, Dawn. Credit and Consumer Society. New York: Routledge, 2007.
Burton, Katherine. Hedge Hunters: How Hedge Fund Masters Survived. New York: Bloomberg, 2010.
Burton, Maureen, and Bruce Brown. The Financial System and the Economy. 5th ed. Armonk, NY: M.E. Sharpe, 2009.
Burton, Maureen, Reynold Nesiba, and Bruce Brown. An Introduction to Financial Markets and Institutions. 2nd ed. Armonk,
NY: M.E. Sharpe, 2010.
Butler, Eamonn. Milton Friedman: A Guide to His Economic Thought. New York: Universe, 1985.
Buzo, Adrian. The Making of Modern Korea. New York: Routledge, 2007.
Bye, Raymond T. An Appraisal of Frederick C. Mills’ The Behavior of Prices. New York: Social Science Research Council,
1940.
Calder, Lendol. Financing the American Dream: A Cultural History of Consumer Debt. Princeton, NJ: Princeton University
Press, 1999.
Caldwell, Bruce. Hayek’s Challenge: An Intellectual Biography of F.A. Hayek. Chicago: University of Chicago Press, 2004.
Calit, Harry S., ed. The Philippines: Current Issues and Historical Background. New York: Nova, 2003.
Callaghan, Paul T. Wool to Weta: Transforming New Zealand’s Culture and Economy. Auckland, NZ: Auckland University
Press, 2009.
Carlson, Mark. A Brief History of the 1987 Stock Market Crash with a Discussion of the Federal Reserve Response.

Washington, DC: Board of Governors of the Federal Reserve, 2006.
Cassidy, John. Dot.con: The Greatest Story Ever Sold. New York: HarperCollins, 2002.
Cassidy, John. How Markets Fail: The Logic of Economic Calamities. New York: Farrar, Straus, and Giroux, 2009.
Cate, Thomas, ed. An Encyclopedia of Keynesian Economics. Cheltenham, UK: Edward Elgar, 1997.
Cecchetti, Stephen G. Money, Banking and Financial Markets. Boston: McGraw-Hill/Irwin, 2006.
Center for Economic and Policy Research. Slow-Motion Recession. Washington, DC: CEPR, 2008.
Cento, Alessandro. The Airline Industry: Challenges in the 21st Century. New York: Springer Heidelberg, 2008.
Chancellor, Edward. Devil Take the Hindmost: A History of Financial Speculation. New York: Farrar, Straus and Giroux,
1999.
Chapman, Stanley. Merchant Enterprise in Britain: From the Industrial Revolution to World War I. New York: Cambridge
University Press, 1992.
Chernow, Ron. The Death of the Banker: The Decline and Fall of the Great Financial Dynasties and the Triumph of the
Small Investor. New York: Vintage, 1997.
Chernow, Ron. The House of Morgan: An American Banking Dynasty and the Rise of Modern Finance. New York: Simon &
Schuster, 2010.
Chipman, John S. The Theory of International Trade. Cheltenham, UK: Edward Elgar, 2008.
Chorafas, Dimitris N. Financial Boom and Gloom: The Credit and Banking Crisis of 2007–2009 and Beyond. New York:
Palgrave Macmillan, 2009.
Clapham, Sir John. The Bank of England: A History. London: Cambridge University Press, 1970.
Clark, John Bates. The Distribution of Wealth: A Theory of Wages, Interest, and Profits. New York: Cosimo Classics, 2005.
Clarke, Peter. The Keynesian Revolution and Its Economic Consequences. Cheltenham, UK: Edward Elgar, 1998.
Clarke, Peter. The Keynesian Revolution in the Making, 1924–1936. New York: Oxford University Press, 1988.
Clauretie, Terrence M., and G. Stacy Sirmans. Real Estate Finance: Theory and Practice. Mason, OH: Cengage Learning,
2010.
Coatsworth, John H., and Alan M. Taylor. Latin America and the World Economy Since 1800. Cambridge, MA: Harvard
University, 1998.
Cohan, William D. House of Cards: A Tale of Hubris and Wretched Excess on Wall Street. New York: Doubleday, 2009.
Colander, David C., and Harry Landreth. History of Economic Thought. 4th ed. Boston: Houghton Mifflin, 2002.
Collins, Robert M. More: The Politics of Economic Growth in Postwar America. New York: Oxford University Press, 2000.
Commons, John R. Institutional Economics: Its Place in Political Economy. New York: Macmillan, 1934.
Cooper, George. The Origin of Financial Crises: Central Banks, Credit Bubbles and the Efficient Market Fallacy. New York:
Vintage, 2008.
Corkill, David. The Portuguese Economy Since 1974. Edinburgh, UK: Edinburgh University Press, 1993.
Cottrell, Allin F., Michael S. Lawlor, and John H. Wood, eds. The Causes and Costs of Depository Institution Failures. New
York: Springer, 1995.
Cukierman, Alex. Central Bank Strategy, Credibility, and Independence: Theory and Evidence. Cambridge, MA: MIT Press,
1992.
Cumings, Bruce. Korea’s Place in the Sun: A Modern History. New York: W.W. Norton, 2005.
Dale, Richard. The First Crash: Lessons from the South Sea Bubble. Princeton, NJ: Princeton University Press, 2004.
Dash, Mike. Tulipomania: The Story of the World’s Most Coveted Flower and the Extraordinary Passions It Aroused. New

York: Crown, 1999.
Davidson, Paul. The Keynes Solution: The Path to Global Economic Prosperity. New York: Palgrave Macmillan, 2009.
Dawiche, Fida. The Gulf Stock Exchange Crash: The Rise and Fall of the Souq Al-Manakh. Dover, NH: Croom Helm, 1986.
De Goede, Marieke. Virtue, Fortune and Faith: A Genealogy of Finance. Minneapolis: University of Minnesota Press, 2005.
Deacon, John. Global Securitisation and CDOs. Chichester, UK: John Wiley and Sons, 2004.
Dellheim, Charles. The Disenchanted Isle: Mrs. Thatcher’s Capitalist Revolution. New York: W.W. Norton, 1995.
Demeny, Paul, and Geoffrey McNicoll. The Political Economy of Global Population Change, 1950–2050. New York:
Population Council, 2006.
Desruelle, Dominique, and Alfred Schipke, eds. Economic Growth and Integration in Central America. Washington, DC:
International Monetary Fund, 2007.
Dimand, Robert W., and John Geanakopolos, eds. Celebrating Irving Fisher: The Legacy of a Great Economist. Oxford, UK:
Blackwell, 2005.
Dimitrakopoulos, Dionyssis G., and Argyris G. Passas. Greece in the European Union. New York: Routledge, 2004.
Doganis, Rigas. The Airline Business. New York: Routledge, 2005.
Dore, Mohammed, Sukhamoy Chakravarty, and Richard Goodwin, eds. John von Neumann and Modern Economics.
Oxford, UK: Oxford University Press, 1989.
Dorfman, Joseph. Thorstein Veblen and His America. New York: A.M. Kelley, 1972.
Dormois, Jean-Pierre. The French Economy in the Twentieth Century. New York: Cambridge University Press, 2004.
Dowd, Douglas. U.S. Capitalist Development Since 1776: Of, By, and For Which People? Armonk, NY: M.E. Sharpe, 1993.
Downs, Anthony. Real Estate and the Financial Crisis: How Turmoil in the Capital Markets Is Restructuring Real Estate
Finance. Washington, DC: Urban Land Institute, 2009.
Drucker, Peter. What Business Can Learn from Nonprofits. Boston: Harvard Business Review, 1989.
Duesenberry, James S. Income, Saving, and the Theory of Consumer Behavior. Cambridge, MA: Harvard University Press,
1962.
Dunn, Donald. Ponzi: The Incredible True Story of the King of Financial Cons. New York: Broadway, 2004.
Durlauf, Steven N., and Lawrence E. Blume. Behavioural and Experimental Economics. Hampshire, UK: Palgrave
Macmillan, 2010.
Dutzler, Barbara. The European System of Central Banks: An Autonomous Actor? The Quest for an Institutional Balance in
EMU. New York: Springer, 2003.
Dymski, Gary, and Robert Pollin, eds. Monetary Macroeconomics: Explorations in the Tradition of Hyman P. Minsky. Ann
Arbor: University of Michigan Press, 1994.
Eastman, Lloyd. Family, Fields, and Ancestors: Constancy and Change in China’s Social and Economic History. New York:
Oxford University Press, 1988.
Ebenstein, Lanny. Milton Friedman: A Biography. New York: Palgrave Macmillan, 2007.
Eckstein, Otto. The Great Recession. Amsterdam, NY: North-Holland, 1978.
Economic Commission for Latin America and the Caribbean. Economic Survey of Latin America and the Caribbean: 2008–
2009. Santiago: ECLAC, 2009.
Economist Intelligence Unit (EIU). Country Report—Denmark.  London: EIU, 2009.
Economist Intelligence Unit (EIU). Country Report—Finland. London: EIU, 2009.
Economist Intelligence Unit (EIU). Country Report—Norway.  London: EIU, 2009.

Economist Intelligence Unit (EIU). Country Report—Sweden London: EIU, 2009.
Edwards, George. The Evolution of Finance Capitalism. New York: Augustus M. Kelley, 1938. Reprint, 1967.
Edwards, John. Australia’s Economic Revolution. Sydney: University of New South Wales Press, 2000.
Eichengreen, Barry. The European Economy Since 1945. Princeton, NJ: Princeton University Press, 2007.
Eichengreen, Barry. Golden Fetters: The Gold Standard and the Great Depression, 1919–1939. New York: Oxford
University Press, 1992.
Elias, Victor J. Sources of Growth: A Study of Seven Latin American Economies. San Francisco: ICS Press, 1989.
Elliott, Douglas J. Measuring the Cost of TARP. Washington, DC: Brookings Initiative on Business and Public Policy Fixing
Finance Series, January 2009.
Elliott, Larry. The Gods That Failed: How Blind Faith in Markets Has Cost Us Our Future. New York: Nation, 2009.
Ellis, Charles D. The Partnership: The Making of Goldman Sachs. New York: Penguin, 2008.
Enderwick, Peter. Understanding Emerging Markets: China and India. New York: Routledge, 2007.
Endlich, Lisa. Goldman Sachs: The Culture of Success. New York: A.A. Knopf, 1999.
Fabozzi, Frank J., ed. Accessing Capital Markets Through Securitization. Hoboken, NJ: John Wiley and Sons, 2001.
Fabozzi, Frank J., ed. The Handbook of Mortgage-Backed Securities. New York: McGraw-Hill, 2006.
Fabozzi, Frank J., and Vinod Kothari. Introduction to Securitization. Hoboken, NJ: John Wiley and Sons, 2008.
Fabozzi, Frank J., and Franco Modigliani. Capital Markets: Institutions and Instruments. 4th ed. Upper Saddle River, NJ:
Prentice Hall, 2008.
Fabozzi, Frank J., Franco Modigliani, and Frank J. Jones. Foundations of Financial Markets and Institutions. 4th ed. Upper
Saddle River, NJ: Prentice Hall, 2009.
Farber, David. Sloan Rules: Alfred P. Sloan and the Triumph of General Motors. Chicago: University of Chicago Press,
2002.
Feinstein, Charles H. An Economic History of South Africa: Conquest, Discrimination, and Development. New York:
Cambridge University Press, 2005.
Feinstein, Charles H., Peter Temin, and Gianni Toniolo. The European Economy Between the Wars. Oxford, UK: Oxford
University Press, 1997.
Feiwel, George R., ed. Joan Robinson and Modern Economic Theory. Hampshire, UK: Macmillan, 1989.
Feldman, David Lewis, ed. The Energy Crisis: Unresolved Issues and Enduring Legacies. Baltimore, MD: Johns Hopkins
University Press, 1996.
Felix, David. Biography of an Idea: John Maynard Keynes and the General Theory of Employment, Interest and Money.
New Brunswick, NJ: Transaction, 1995.
Felix, David. Keynes: A Critical Life. Westport, CT: Greenwood, 1999.
Fellner, William J. Competition Among the Few: Oligopoly and Similar Market Structures. New York: Alfred A. Knopf, 1949.
Fellner, William J. Towards a Reconstruction of Macroeconomics: Problems of Theory and Policy. Washington, DC:
American Enterprise Institute for Public Policy Research, 1976.
Fenton-O’Creevy, Mark, Nigel Nicholson, Emma Soane, and Paul Willman. Traders: Risks, Decisions, and Management in
Financial Markets. Oxford, UK: Oxford University Press, 2005.
Ferguson, Niall. The Ascent of Money: A Financial History of the World. New York: Penguin, 2008.
Feser, Edward. The Cambridge Companion to Hayek. New York: Cambridge University Press, 2006.
Fisher, Irving. Booms and Depressions: Some First Principles. New York: Adelphi, 1932.

Fiske, Frank S., ed. The Mississippi Bubble: A Memoir of John Law; to Which Are Added Authentic Accounts of the Darien
Expedition, and the South Sea Scheme. New York: Greenwood, 1969.
Flaschel, Peter, and Michael Landesmann, eds. Mathematical Economics and the Dynamics of Capitalism: Research in
Honor of Richard M. Goodwin. New York: Routledge, 2008.
Fleckenstein, William A., and Frederick Sheehan. Greenspan’s Bubbles: The Age of Ignorance at the Federal Reserve. New
York: McGraw-Hill, 2008.
Fletcher, Gordon. Dennis Robertson. Basingstoke, UK: Palgrave Macmillan, 2008.
Ford, J.L. G.L.S. Shackle: The Dissenting Economist’s Economist. Northampton, MA: Edward Elgar, 1994.
Foster, John B., and Fred Magdoff. The Great Financial Crisis: Causes and Consequences. New York: Monthly Review,
2009.
Foster, Richard, and Sarah Kaplan. Creative Destruction: Turning Built-to-Last into Built-to-Perform. New York: Currency,
2001.
Fox, Justin. The Myth of the Rational Market: A History of Risk, Reward, and Delusion on Wall Street. New York:
HarperBusiness, 2009.
Fox, Loren. Enron: The Rise and Fall. Hoboken, NJ: John Wiley and Sons, 2003.
Frank, Robert H. Falling Behind: How Rising Inequality Harms the Middle Class. Berkeley: University of California Press,
2007.
Freeman, Christopher, and Francisco Louçã. As Time Goes By: From the Industrial Revolutions to the Information
Revolution. New York: Oxford University Press, 2001.
Freeman, Richard B. America Works: Critical Thoughts on the Exceptional U.S. Labor Market. New York: Russell Sage
Foundation, 2007.
Freidman, Benjamin. Day of Reckoning: The Consequences of American Economic Policy Under Reagan and After. New
York: Random House, 1988.
Friedman, Milton. A Theory of the Consumption Function. Princeton, NJ: Princeton University Press, 1957.
Friedman, Milton, and Anna Jacobson Schwartz. The Great Contraction, 1929–1933. Princeton, NJ: Princeton University
Press, 2009.
Friedman, Milton, and Anna Jacobson Schwartz. A Monetary History of the United States, 1867–1960. Princeton, NJ:
Princeton University Press, 1963.
Friedman, Walter A. The Seer of Wellesley Hills: Roger Babson and the Babson Statistical Organization. Boston: Harvard
Business School, 2008.
Frisch, Helmut: Theories of Inflation. Cambridge, UK, and New York: Cambridge University Press, 1983.
Furtado, Celso. The Economic Growth of Brazil: A Survey from Colonial Times to Modern Times. Berkeley: University of
California Press, 1963.
Galbraith, John Kenneth. The Affluent Society. 40th anniversary ed. Boston: Houghton Mifflin, 1998.
Galbraith, John Kenneth. The Great Crash: 1929. Boston: Houghton Mifflin, 1997.
Galbraith, John Kenneth. The New Industrial State. Boston: Houghton Mifflin, 1967.
Galbraith, John Kenneth. A Short History of Financial Euphoria. New York: Whittle Books, in association with Viking, 1993.
Gali, Jordi. Monetary Policy, Inflation, and the Business Cycle: An Introduction to the New Keynesian Framework. Princeton,
NJ: Princeton University Press, 2008.
Garber, Peter M. Famous First Bubbles: The Fundamentals of Early Manias. Cambridge MA: MIT Press, 2000.
Garrison, Roger. Time and Money: The Macroeconomics of Capital Structure. New York: Routledge, 1999.

Geanuracos, John, and Bill Millar. The Power of Financial Innovation. New York: HarperCollins Business, 1991.
Geisst, Charles R. Wall Street, a History: From Its Beginnings to the Fall of Enron. New York: Oxford University Press,
2004.
Gilbert, Christopher, and Vines, David. The World Bank: Structure and Policies. New York: Cambridge University Press,
2000.
Gilpin, Robert. Global Political Economy: Understanding the International Economic Order. Princeton, NJ: Princeton
University Press, 2001.
Glade, William P. The Latin American Economies: A Study of Their Institutional Evolution. New York: American Book, 1969.
Glasner, David, ed. Business Cycles and Depressions: An Encyclopedia. New York: Garland, 1997.
Gold, Gerry, and Paul Feldman. A House of Cards: From Fantasy Finance to Global Crash. London: Lupus, 2007.
Goldman, Michael. Imperial Nature: The World Bank and Struggles for Social Justice in the Age of Globalization. New
Haven, CT: Yale University Press, 2005.
Goldstein, Morris. The Asian Financial Crisis: Causes, Cures, and Systemic Implications. Washington, DC: Institute for
International Economic, 1998.
Goodhart, C.A.E., and Boris Hofmann. House Prices and the Macroeconomy: Implications for Banking and Price Stability.
New York: Oxford University Press, 2007.
Goodman, Laurie S., Shumin Li, Douglas J. Lucas, Thomas A. Zimmerman, and Frank J. Fabozzi. Subprime Mortgage
Credit Derivatives. Hoboken, NJ: John Wiley and Sons, 2008.
Goodwin, Richard M. Chaotic Economic Dynamics. New York: Oxford University Press, 1990.
Gordon, John Steele. The Great Game: The Emergence of Wall Street as a World Power, 1653–2000. New York: Simon &
Schuster, 1999.
Gould, Bryan. Rescuing the New Zealand Economy: Where We Went Wrong and How We Can Fix It. Nelson, NZ: Craig
Potton, 2008.
Grabowski, Richard, Sharmistha Self, and Michael P. Shields. Economic Development: A Regional, Institutional, and
Historical Approach. Armonk, NY: M.E Sharpe, 2006.
Graf, Hans Georg. Economic Forecasting for Management: Possibilities and Limitations. Westport, CT: Quorum, 2002.
Gramlich, Edward M. Subprime Mortgages: America’s Latest Boom and Bust. Washington, DC: Urban Institute, 2007.
Gray, Joanna, and Jenny Hamilton. Implementing Financial Regulation: Theory and Practice. Hoboken, NJ: John Wiley&
Sons, 2006.
Greenspan, Alan. The Age of Turbulence: Adventures in a New World. New York: Penguin, 2007.
Greider, William. Secrets of the Temple: How the Federal Reserve Runs the Country. New York: Simon & Schuster, 1989.
Groenewagen, Peter. A Soaring Eagle: Alfred Marshall, 1842–1924. Cheltenham, UK: Edward Elgar, 1995.
Gros, Daniel, and Alfred Steinherr. Economic Transition in Central and Eastern Europe: Planting the Seeds. 2nd ed. New
York: Cambridge University Press, 2004.
Gross, Daniel. Pop! Why Bubbles Are Great for the Economy. New York: Collins Business, 2007.
Gup, Benton E., ed. Too Big to Fail: Policies and Practices in Government Bailouts. Westport, CT: Praeger, 2004.
Haas, Jeffrey J. Corporate Finance in a Nutshell. St. Paul, MN: Thomson/West, 2004.
Haas, Peter M. Knowledge, Power, and International Policy Coordination. Columbia: University of South Carolina Press,
1997.
Haberler, Gottfried. Prosperity and Depression: A Theoretical Analysis of Cyclical Movements. 5th ed. New York:
Atheneum, 1963.

Haberler, Gottfried. The Theory of International Trade: With Its Applications to Commercial Policy. Trans. Alfred Stonier and
Frederick Benham. New York: Augustus M. Kelley, 1968.
Hall, Peter A., and David Soskice, eds. Varieties of Capitalism: The Institutional Foundations of Comparative Advantage.
New York: Oxford University Press, 2001.
Hamilton, Carolyn, Bernard Mbenga, and Robert Ross, eds. The Cambridge History of South Africa. New York: Cambridge
University Press, 2009.
Hamouda, O.F. John R. Hicks: The Economist’s Economist. Oxford, UK: Blackwell, 1993.
Hanke, John E., and Dean Wichern. Business Forecasting. 9th ed. Upper Saddle River, NJ: Pearson Prentice-Hall, 2005.
Hansen, Alvin Harvey. Guide to Keynes. New York: McGraw-Hill, 1953.
Hansson, Berg. The Stockholm School and the Development of Dynamic Method. London: Croom Helm, 1982.
Harris, Ethan S. Ben Bernanke’s Fed: The Federal Reserve After Greenspan. Boston: Harvard Business, 2008.
Harrison, Joseph, and David Corkill. Spain: A Modern European Economy. Burlington, VT: Ashgate, 2004.
Hartley, James E., Kevin D. Hoover, and Kevin D. Salyer, eds. Real Business Cycles: A Reader. London: Routledge, 1998.
Hayek, Friedrich A. Monetary Theory and the Trade Cycle. New York: Harcourt, Brace, 1933.
Hayek, Friedrich A. The Pure Theory of Capital. London: Macmillan, 1941.
Head, Simon. The New Ruthless Economy: Work and Power in the Digital Age. New York: Oxford University Press, 2005.
Hearder, Harry, and Jonathan Morris. Italy: A Short History. 2nd ed. New York: Cambridge University Press, 2001.
Heertje, Arnold. Schumpeter on the Economics of Innovation and the Development of Capitalism. Cheltenham, UK: Edward
Elgar, 2006.
Heffernan, Shelagh. Modern Banking in Theory and Practice. Hoboken, NY: John Wiley and Sons, 1996.
Heilbroner, Robert, and Peter Bernstein. The Debt and the Deficit: False Alarms/Real Possibilities. New York: W.W. Norton,
1989.
Henn, R., and O. Moeschlin, eds. Mathematical Economics and Game Theory: Essays in Honor of Oskar Morgenstern. New
York: Springer, 1977.
Henretta, James A. The Origins of American Capitalism. Boston: Northeastern University Press, 1991.
Henriques, Diana B. The White Sharks of Wall Street: Thomas Mellon and the Original Corporate Raiders. New York:
Scribner, 2000.
Henwood, Doug. After the New Economy. New York: New Press, 2003.
Hetzel, Robert L. The Monetary Policy of the Federal Reserve. New York: Cambridge University Press, 2008.
Hicks, John Richard. Collected Essays on Economic Theory. 3 vols. Cambridge: Harvard University Press, 1981–1983.
Hill, Charles. International Business: Competing in the Global Marketplace. 7th ed. New York: McGraw-Hill/Irwin, 2009.
Hirschmeier, Johannes, and Tsunehiko Yui. The Development of Japanese Business, 1600–1980. Boston: G. Allen &
Unwin, 1981.
Hixson, William F. Triumphs of the Bankers: Money and Banking in the Eighteenth and Nineteenth Centuries. Westport, CT:
Praeger, 1993.
Hollander, Samuel. The Economics of John Stuart Mill. Toronto: University of Toronto Press, 1985.
Hollander, Samuel. The Economics of Karl Marx: Analysis and Application. New York: Cambridge University Press, 2008.
Hollander, Samuel. The Economics of Thomas Robert Malthus. Toronto: University of Toronto Press, 1997.
Hollis, Martin, and Edward J. Nell. Rational Economic Man: A Philosophical Critique of Neo-Classical Economics. New
York: Cambridge University Press, 1975.

Homer, Sidney, and Richard Sylla. A History of Interest Rates. Hoboken, NJ: John Wiley and Sons, 2005.
Honohan, Patrick, and Luc Laeven, eds. Systemic Financial Crises: Containment and Resolution. New York: Cambridge
University Press, 2005.
Hooshmand, A. Reza. Business Forecasting: A Practical Approach. 2nd ed. New York: Routledge, 2009.
Horowitz, Daniel. The Anxieties of Affluence: Critiques of American Consumer Culture, 1939–1979. Amherst: University of
Massachusetts Press, 2004.
Horwich, George, and Paul Samuelson, eds. Trade, Stability, and Macroeconomics: Essays in Honor of Lloyd A. Metzler.
New York: Academic, 1974.
Hough, J.R. The French Economy. New York: Holmes & Meier, 1982.
Houseman, Gerald L. America and the Pacific Rim: Coming to Terms with New Realities. Lanham, MD: Rowman &
Littlefield, 1995.
Hudis, Peter, and Kevin B. Anderson, eds. The Rosa Luxemburg Reader. New York: Monthly Review, 2004.
Hughes, Jonathan, and Louis P. Cain. American Economic History. 8th ed. Boston: Addison-Wesley, 2010.
Humphreys, Norman K. Historical Dictionary of the International Monetary Fund. Lanham, MD: Scarecrow, 1999.
Hunt, E.K. History of Economic Thought. 2nd ed. Armonk, NY: M.E. Sharpe, 2002.
Hurewitz, J.C., ed. Oil, the Arab-Israeli Dispute, and the Industrial World: Horizons of Crisis. Boulder, CO: Westview, 1976.
Hutchinson, T.W. The Politics and Philosophy of Economics: Marxians, Keynesians, and Austrians. New York: New York
University Press, 1981.
Ichimura, Shinichi, Tsuneaki Sato, and William James. Transition from Socialist to Market Economies: Comparison of
European and Asian Experiences. New York: Palgrave Macmillan, 2009.
Immergluck, Daniel. Foreclosed: High-Risk Lending, Deregulation, and the Undermining of America’s Mortgage Market.
Ithaca, NY: Cornell University Press, 2009.
Ingebretsen, Mark. NASDAQ: A History of the Market That Changed the World. Roseville, CA: Forum, 2002.
International Monetary Fund (IMF). Balance of Payments and International Investment Position Manual. 6th ed. Washington,
DC: International Monetary Fund, 2008.
International Monetary Fund (IMF). Balance of Payments Statistics Yearbook. Washington, DC: International Monetary
Fund, 2008.
International Monetary Fund (IMF). Global Financial Stability Report. Washington, DC: International Monetary Fund, 2009.
International Monetary Fund (IMF). World Economic Outlook: Housing and the Business Cycle. Washington, DC:
International Monetary Fund, 2008.
Issing, Otmar, Vitor Gaspar, and Oreste Tristani. Monetary Policy in the Euro Area: Strategy and Decision Making at the
European Central Bank. New York: Cambridge University Press, 2001.
Jackson, Walter. Gunnar Myrdal and America’s Conscience: Social Engineering and Racial Liberalism, 1938–1987. Chapel
Hill: University of North Carolina Press, 1990.
Jevons, William Stanley. Investigations in Currency and Finance. London: Macmillan, 1909.
Johnson, Chalmers, ed. The Industrial Policy Debate. San Francisco: ICS Press, 1984.
Johnson, Harry G. The Canadian Quandary: Economic Problems and Policies. Montreal: McGill-Queen’s University Press,
2005.
Johnson, Moira. Roller Coaster: The Bank of America and the Future of American Banking. New York: Ticknor & Fields,
1990.
Jones, Charles I. Macroeconomics. New York: W.W. Norton, 2008.

Jones, Stuart, ed. The Decline of the South African Economy. Northampton, MA: Edward Elgar, 2002.
Juglar, Clement. A Brief History of Panics: And Their Periodical Occurrence in the United States. 3rd ed. New York:
Forgotten Books, 2008.
Kalaitzidis, Akis. Europe’s Greece: A Giant in the Making. New York: Palgrave Macmillan, 2010.
Kaldor, Nicholas. Essays on Economic Stability and Growth. 2nd ed. New York: Holmes & Meier, 1980.
Kansas, Dave. The Wall Street Journal Guide to the End of Wall Street As We Know It: What You Need to Know About the
Greatest Financial Crisis of Our Time—and How to Survive It. New York: HarperBusiness, 2009.
Karier, Thomas. Great Experiments in American Economic Policy: From Kennedy to Reagan. Westport, CT: Praeger, 1997.
Kates, Steven. Say’s Law and the Keynesian Revolution. Cheltenham, UK: Edward Elgar, 2009.
Kaufman, Henry. The Road to Financial Reformation: Warnings, Consequences, Reforms. Hoboken, NJ: John Wiley and
Sons, 2009.
Kaufman, Perry J. New Trading Systems and Methods. Hoboken, NJ: John Wiley and Sons, 2005.
Kautsky, John H. Karl Kautsky: Marxism, Revolution & Democracy. New Brunswick, NJ: Transaction, 1994.
Keaney, Michael, ed. Economist with a Public Purpose: Essays in Honor of John Kenneth Galbraith. New York: Routledge,
2001.
Kehoe, Timothy J., and Edward C. Prescott, eds. Great Depressions of the Twentieth Century. Minneapolis, MN: Federal
Reserve Bank of Minneapolis, 2007.
Keller, Morton. Regulating a New Economy: Public Policy and Economic Change in America, 1900–1933. Cambridge, MA:
Harvard University Press, 1990.
Kemp, Murray C. International Trade Theory. London: Routledge, 2008.
Kendall, Leon T., and Michael J. Fishman, eds. A Primer on Securitization. Cambridge, MA: MIT Press, 2000.
Kent, Neil. A Concise History of Sweden. New York: Cambridge University Press, 2008.
Kenton, Lawrence V., ed. Manufacturing Output, Productivity, and Employment Indications. New York: Novinka/Nova
Science, 2005.
Keynes, John Maynard. The General Theory of Employment, Interest and Money. New York: Harcourt, Brace, 1936.
Keynes, John Maynard. A Treatise on Money. New York: Harcourt, Brace, 1930.
Keynes, John Maynard. A Treatise on Probability. Vol. 8,The Collected Writings of John Maynard Keynes, ed. D.
Moggeridge and E. Johnson. London: Macmillan, 1973, 1921.
Killingsworth, Mark R. Labor Supply. New York: Cambridge University Press, 1983.
Kindleberger, Charles P. A Financial History of Western Europe. London: George Allen and Unwin, 1984.
Kindleberger, Charles P. The World in Depression, 1929–1939. Berkeley: University of California Press, 1973.
Kindleberger, Charles, and Robert Z. Aliber. Manias, Panics, and Crashes: A History of Financial Crises. 6th ed.
Basingstoke, UK: Palgrave Macmillan, 2010.
King, John Edward. Nicholas Kaldor. Basingstoke, UK: Palgrave Macmillan, 2008.
Kirtzman, Andrew. Betrayal: The Life and Lies of Bernie Madoff. New York: HarperCollins, 2009.
Kirzner, Israel M. Ludwig von Mises: The Man and His Economics. Wilmington, DE: ISI Books, 2001.
Kirzner, Israel M. The Meaning of Market Process: Essays in the Development of Modern Austrian Economics. New York:
Routledge, 1992.
Klein, Maury. Rainbow’s End: The Crash of 1929. New York: Oxford University Press, 2001.
Kleinknecht, Alfred. Innovation Patterns in Crisis and Prosperity: Schumpeter’s Long Cycle Reconsidered. London:

Macmillan, 1987.
Klingaman, William K. 1929: The Year of the Great Crash. New York: Harper & Row, 1989.
Knoop, Todd. Recessions and Depressions: Understanding Business Cycles. Westport, CT: Praeger, 2004.
Koo, Richard C. The Holy Grail of Macroeconmics: Lessons from Japan’s Great Recession. Hoboken, NJ: John Wiley and
Sons, 2008.
Koopmans, Tjalling. Three Essays on the State of Economic Theory. New York: McGraw-Hill, 1957.
Kose, M., C. Otrok, and E. Prasad. Global Business Cycles: Convergence or Decoupling? Washington, DC: International
Monetary Fund, 2008.
Kriesler, Peter. Kalecki’s Microanalysis: The Development of Kalecki’s Analysis of Pricing and Distribution. New York:
Cambridge University Press, 1987.
Krugman, Paul. Geography and Trade. Cambridge, MA: MIT Press, 1991.
Krugman, Paul. The Return of Depression Economics and the Crisis of 2008. New York: W.W. Norton, 2009.
Krugman, Paul R., and Maurice Obstfeld. International Economics: Theory and Policy. Boston: Pearson Addison-Wesley,
2009.
Kuenne, Robert E. Eugen von Böhm-Bawerk. New York: Columbia University Press, 1971.
Kuznets, Simon S. Capital in the American Economy: Its Formation and Financing. Princeton, NJ: Princeton University
Press, 1961.
Kuznets, Simon S. Economic Change: Selected Essays in Business Cycles, National Income, and Economic Growth. New
York: W.W. Norton, 1953.
Lachmann, Ludwig Maurits. Capital and Its Structure. London: Bell and Sons, 1956.
Laffont, Jean-Jacques. Incentives and Political Economy. New York: Oxford University Press, 2000.
Laider, David. Fabricating the Keynesian Revolution: Studies of the Inter-war Literature on Money, the Cycle, and
Unemployment. Cambridge, UK: Cambridge University Press, 1999.
Langdana, Farrokh K. Macroeconomic Policy: Demystifying Monetary and Fiscal Policy. New York: Springer, 2009.
Langley, Lester D. The Americas in the Modern Age. New Haven, CT: Yale University Press, 2003.
Lawson, Tony, J. Gabriel Palma, and John Sender, eds. Kaldor’s Political Economy. London: Academic, 1989.
Lefebvre, Adelaide D., ed. Government Bailout: Troubled Asset Relief Program (TARP). Hauppauge, NY: Nova Science,
2009.
Lerner, Abba. Selected Writings of Abba Lerner, ed. David Colander. New York: New York University Press, 1983.
Leuchtenburg, William. Franklin Roosevelt and the New Deal. New York: Harper & Row, 1963.
Leuchtenburg, William. The Perils of Prosperity, 1914–1932. Chicago: University of Chicago Press, 1958.
Lewis, Michael. The Big Short: Inside the Doomsday Machine. New York: W.W. Norton, 2010.
Lewis, Michael. Boomerang: Travels in the New Third World. New York: W.W. Norton, 2011.
Lewis, William W. The Power of Productivity: Wealth, Poverty, and the Threat to Global Stability. Chicago: University of
Chicago Press, 2004.
Lindbeck, Assar, ed. Nobel Lectures: Economics, 1969–1980. Singapore: World Scientific, 1992.
Lowe, Adolph. Essays in Political Economics: Public Control in a Democratic Society. New York: New York University Press,
1987.
Lowe, Adolph. On Economic Knowledge: Toward a Science of Political Economics. Armonk, NY: M.E. Sharpe, 1977.
Lowenstein, Roger. When Genius Failed: The Rise and Fall of Long-Term Capital Management. New York: Random

House, 2000.
Lowenstein, Roger. The End of Wall Street. New York: Penguin, 2010.
Lowy, Michael. High Rollers: Inside the Savings and Loan Debacle. New York: Praeger, 1991.
Lucas, Douglas J., Laurie S. Goodman, and Frank J. Fabozzi. Collateralized Debt Obligations: Structures and Analysis. 2nd
ed. Hoboken, NJ: John Wiley and Sons, 2006.
Lucas, Douglas J., Laurie S. Goodman, Frank J. Fabozzi, and Rebecca Manning. Developments in Collateralized Debt
Obligations: New Products and Insights. Hoboken, NJ: John Wiley and Sons, 2007.
Lundberg, Erik Filip. The Development of Swedish and Keynesian Macroeconomic Theory and Its Impact on Economic
Policy. Cambridge, UK: Cambridge University Press, 1996.
Lundberg, Erik Filip. Instability and Economic Growth. New Haven, CT: Yale University Press, 1968.
Lustig, Nora. Mexico: The Remaking of an Economy. Washington, DC: Brookings Institution, 1998.
Lys, Thomas, ed. Economic Analysis and Political Ideology: Selected Essays of Karl Brunner. Cheltenham, UK: Edward
Elgar, 1996.
Mackay, Charles. Memoirs of Extraordinary Popular Delusions and the Madness of Crowds. New York: Cosimo Classics,
2008. First published 1841.
Macrae, Norman. John von Neumann. New York: Pantheon, 1992.
Maddison, Angus. Monitoring the World Economy 1820–1992. Paris: OECD, 1995.
Maddison, Angus. The World Economy: A Millennial Perspective. Paris: OECD, 2001.
Madura, Jeff. Financial Markets and Institutions. Mason, OH: South-Western, 2008.
Magdoff, Fred, and Michael Yates. The ABCs of the Financial Crisis. New York: Monthly Review, 2009.
Maksakovsky, Pavel V. The Capitalist Cycle, trans. Richard B. Day. Chicago: Haymarket, 2009.
Mallaby, Sebastian. The World’s Banker: A Story of Failed States, Financial Crises, and the Wealth and Poverty of Nations.
New York: Penguin, 2004.
Malthus, Thomas Robert. An Essay on the Principle of Population, ed. Philip Appleman. New York: W.W. Norton, 2004.
Malthus, Thomas Robert. Principles of Political Economy, ed. John Pullen. Cambridge, UK, and New York: Cambridge
University Press, 1989.
Mandel, Ernest. Long Waves of Capitalist Development: A Marxist Interpretation. New York: Verso, 1995.
Mankiw, N. Gregory. Principles of Macroeconomics. 5th ed. Mason, OH: South-Western, 2009.
Manzetti, Luigi. Neoliberalism, Accountability, and Reform Failures in Emerging Markets: Eastern Europe, Russia,
Argentina, and Chile in Comparative Perspective. University Park: Pennsylvania State University Press, 2009.
Marcuzzo, Maria Cristina, Luigi L. Pasinetti, and Alessandro Roncaglia, eds. The Economics of Joan Robinson. London:
Routledge, 1996.
Markham, Jerry W. A Financial History of Modern U.S. Corporate Scandals: From Enron to Reform. Armonk, NY: M.E.
Sharpe, 2005.
Markham, Jerry W. A Financial History of the United States. 3 vols. Armonk, NY: M.E. Sharpe, 2002.
Markham, Jerry W. A Financial History of the United States: From Enron-Era Scandals to the Subprime Crisis (2004–2006);
From the Subprime Crisis to the Great Recesion (2006–2009). Armonk, NY: M.E. Sharpe, 2010.
Marshall, Alfred. Industry and Trade. London: Macmillan, 1919.
Marshall, Alfred. Money, Credit and Commerce. London: Macmillan, 1923.
Marshall, Alfred. Principles of Economics. Amherst, NY: Prometheus, 1997. First published 1890.

Marx, Karl. Capital: A Critique of Political Economy, trans. Ben Fowkes. New York: Vintage, 1976–1981.
Marx, Karl. Das Kapital. 3 vols. London: Penguin, 2004.
Matusow, Allen J. Nixon’s Economy: Booms, Busts, Dollars, and Votes. Lawrence: University Press of Kansas, 1998.
Mavrotas, George, and Anthony Shorrocks, eds. Advancing Development: Core Themes in Global Economics. New York:
Palgrave Macmillan, 2007.
Mayer, Martin. The Greatest-Ever Bank Robbery: The Collapse of the Savings and Loan Industry. New York: Scribner,
1990.
Mayer, Thomas. Monetary Policy and the Great Inflation in the United States: The Federal Reserve and the Failure of
Macroeconomic Policy, 1965–1979. Northampton, MA: Edward Elgar, 1999.
Mayer, Thomas, James S. Duesenberry, and Robert Z. Aliber. Money, Banking, and the Economy. New York: W.W. Norton,
1996.
McCarthy, Dennis. International Economic Integration in Historical Perspective. New York: Routledge, 2006.
McConnell, Campbell R., and Stanley L. Brue. Economics: Principles, Problems, and Policies. 17th ed. New York: McGraw-
Hill/Irwin, 2006.
McConnell, Campbell R., Stanley L. Brue, and David A. Macpherson. Contemporary Labor Economics. 8th ed. New York:
McGraw-Hill/Irwin, 2009.
McCraw, Thomas K. Prophet of Innovation: Joseph Schumpeter and Creative Destruction. Cambridge, MA: Harvard
University Press, 2007.
McDonald, Lawrence G., with Patrick Robinson. A Colossal Failure of Common Sense: The Inside Story of the Collapse of
Lehman Brothers. New York: Crown Business, 2009.
McDougall, Derek. Asia Pacific in World Politics. New Delhi: Viva, 2008.
McGee, Suzanne. Chasing Goldman Sachs: How the Masters of the Universe Melted Wall Street Down— and Why They’ll
Take Us to the Brink Again. New York: Crown, 2010.
McGrane, Reginald. The Panic of 1837: Some Financial Problems of the Jacksonian Era. New York: Russell & Russell,
1965.
McGregor, Andrew. Southeast Asian Development. New York: Routledge, 2008.
McLellen, David. Karl Marx: His Life and Thought. London: Macmillan, 1973.
McLeod, Ross, and Ross Garnaut, eds. East Asia in Crisis: From Being a Miracle to Needing One? London: Routledge,
1998.
Means, Gardner, ed. The Roots of Inflation: The International Crisis. New York: B. Franklin, 1975.
Meijer, Gerrit. New Perspectives on Austrian Economics. New York: Routledge, 1995.
Mengkui, Wang, ed. China in the Wake of Asia’s Financial Crisis. New York: Routledge, 2009.
Metrick, Andrew. Venture Capital and the Finance of Innovation. Hoboken, NJ: John Wiley and Sons, 2007.
Metz, Tim. Black Monday: The Catastrophe of October 19, 1987, and Beyond. New York: William Morrow, 1988.
Metzler, Lloyd A. Collected Papers. Cambridge, MA: Harvard University Press, 1973.
Metzler, Lloyd A. Income, Employment and Public Policy: Essays in Honor or Alvin H. Hansen. New York: W.W. Norton,
1948.
Michaelson, Adam. The Foreclosure of America: The Inside Story of the Rise and Fall of Countrywide Home Loans, the
Mortgage Crisis, and the Default of the American Dream. New York: Berkley, 2009.
Milgate, Murray and Shannon C. Stimson. After Adam Smith: A Century of Transformation in Politics and Political Economy.
Princeton, NJ: Princeton University Press, 2009.

Mill, John Stuart. The Collected Works of John Stuart Mill, ed. J.M. Robinson. Toronto: University of Toronto Press, 1963–
1991.
Mill, John Stuart. Principles of Political Economy. New York: Oxford University Press, 1998.
Miller, Sally M., A.J.H. Latham, and Dennis O. Flynn, eds. Studies in the Economic History of the Pacific Rim. Routledge
Studies in the Growth Economies of Asia. London and New York: Routledge, 1998.
Mills, Frederick C. The Behavior of Prices. New York: Arno, 1975.
Mills, Frederick C. Statistical Methods Applied to Economics and Business. 3rd ed. New York: Holt, 1955.
Milonakis, Dimitris, and Ben Fine. From Political Economy to Economics: Method, the Social and the Historical in the
Evolution of Economic Theory. New York: Routledge, 2008.
Minsky, Hyman P. Can “It” Happen Again? Essays on Instability and Finance. Armonk, NY: M.E. Sharpe, 1982.
Minsky, Hyman P. John Maynard Keynes. New York: Columbia University Press, 1975.
Minsky, Hyman P. Stabilizing an Unstable Economy. New Haven, CT: Yale University Press, 1986.
Minton, Robert. John Law, The Father of Paper Money. New York: Association, 1975.
Mises, Ludwig von. The Theory of Money and Credit. London: J. Cape, 1934.
Mitchell, Wesley Clair. A History of the Greenbacks: With Special Reference to the Economic Consequences of Their Issue,
1862–65. Chicago: University of Chicago Press, 1960.
Mitchell, Wesley Clair. What Happens During Business Cycles, a Progress Report. New York: National Bureau of Economic
Research, 1951.
Moffett, Michael, Arthur I. Stonehill, and David Eiteman. Fundamentals of Multinational Finance. Boston: Pearson Prentice
Hall, 2009.
Mommen, Andre. The Belgian Economy in the Twentieth Century. London: Routledge, 1994.
Moreno-Brid, Juan Carlos, and Jaime Ros. Development and Growth in the Mexican Economy: A Historical Perspective.
New York: Oxford University Press, 2009.
Morris, Charles R. The Trillion Dollar Meltdown: Easy Money, High Rollers, and the Great Credit Crash. New York:
PublicAffairs, 2008.
Morris, Charles R. The Tycoons: How Andrew Carnegie, John D. Rockefeller, Jay Gould, and J.P. Morgan Invented the
American Supereconomy. New York: Holt, 2006.
Morris, Chris. The New Turkey: The Quiet Revolution on the Edge of Europe. London: Granta, 2005.
Mosk, Carl. Japanese Industrial History: Technology, Urbanization, and Economic Growth. Armonk, NY: M.E. Sharpe, 2001.
Motamen-Samadian, Sima, ed. Capital Flows and Foreign Direct Investments in Emerging Markets. New York: Palgrave
Macmillan, 2005.
Mullineux, A.W. The Business Cycle After Keynes: A Contemporary Analysis. Brighton, UK: Wheatsheaf, 1984.
Muolo, Paul, and Mathew Padilla. Chain of Blame: How Wall Street Caused the Mortgage and Credit Crisis. Hoboken, NJ:
John Wiley and Sons, 2008.
Murphy, Antoin E. John Law: Economic Theorist and Policy-Maker. New York: Oxford University Press, 1997.
Nadeau, Kathleen. The History of the Philippines. Westport, CT: Greenwood, 2008.
Nardini, Franco. Technical Progress and Economic Growth: Business Cycles and Stabilization. New York: Springer, 2001.
Naughton, Barry. The Chinese Economy: Transitions and Growth. Cambridge, MA: MIT Press, 2007.
Naylor, R.T. Canada in the European Age, 1453–1919 . 2nd ed. Montreal: McGill-Queen’s University Press, 2006.
Neikirk, William. Volcker: Portrait of the Money Man. New York: Congdon & Weed, 1987.

Nell, Edward J., and Willi Semmler, eds. Nicholas Kaldor and Mainstream Economics: Confrontation or Convergence? New
York: St. Martin’s, 1991.
Nettels, Curtis. The Emergence of a National Economy, 1775–1815. Armonk, NY: M.E. Sharpe, 1989.
Nettl, J.P. Rosa Luxemburg. London: Oxford University Press, 1966.
Niskanen, William A. Reaganomics: An Insider’s Account of the Policies and the People. New York: Oxford University Press,
1988.
Nitzan, Jonathan, and Shimshon Bichler. The Global Political Economy of Israel. Sterling, VA: Pluto, 2002.
Norton, Hugh S. The Quest for Economic Stability: Roosevelt to Bush. 2nd ed. Columbia: University of South Carolina,
1991.
Obay, Lamia. Financial Innovation in the Banking Industry: The Case of Asset Securitization. London: Taylor & Francis,
2001.
OECD. Economic Survey of Iceland 2006: Policy Challenges in Sustaining Improved Economic Performance. Paris:
Organisation for Economic Co-operation and Development, 2006.
Okun, Arthur M. Equality and Efficiency: The Big Tradeoff. Washington, DC: Brookings Institution, 1975.
Olson, Mancur Lloyd, Jr. The Rise and Decline of Nations: Economic Growth, Stagflation, and Social Rigidities. New
Haven, CT: Yale University Press, 1982.
Overton, Rachel H. China’s Trade with the United States and the World. Hauppage, NY: Nova Science, 2009.
Owen, Norman G., ed. The Emergence of Modern Southeast Asia: A New History. Honolulu: University of Hawai’i Press,
2005.
Page, Alan C., and R.B. Ferguson. Investor Protection. London: Orion, 1992.
Panayiotopoulos, Podromos. Immigrant Enterprise in Europe and the USA. New York: Routledge, 2006.
Parker, Richard. John Kenneth Galbraith: His Life, His Politics, His Economics. New York: Farrar, Straus and Giroux, 2005.
Parthasarathi, P. The Transition to a Colonial Economy: Weavers, Merchants, and Kings in South India 1720–1800. New
York: Cambridge University Press, 2001.
Paulson, Henry. On the Brink: Inside the Race to Stop the Collapse of the Global Financial System. New York: Business
Plus, 2009.
Pauly, Louis W. Who Elected the Bankers? Surveillance and Control in the World Economy. Ithaca, NY: Cornell University
Press, 1997.
Peart, Sandra. The Economics of W.S. Jevons. London and New York: Routledge, 1996.
Peláez, Carlos M., and Carlos A. Peláez. Regulation of Banks and Finance: Theory and Policy After the Credit Crisis. New
York: Palgrave Macmillan, 2009.
Perkins, Edwin J. The Economy of Colonial America. New York: Columbia University Press, 1988.
Perkins, Martin Y., ed. TARP and the Restoration of U.S. Financial Stability. Hauppauge, NY: Nova Science, 2009.
Philion, Stephen E. Workers’ Democracy in China’s Transition from State Socialism. New York: Routledge, 2009.
Pierre, Andrew J., ed. Unemployment and Growth in the Western Economies. New York: Council on Foreign Relations,
1984.
Pizzo, Steven, Mary Fricker, and Paul Muolo. Inside Job: The Looting of America’s Savings and Loans. New York: McGraw-
Hill, 1989.
Posner, Richard A. A Failure of Capitalism: The Crisis of’08 and the Descent into Depression’. Cambridge, MA: Harvard
University Press, 2009.
Power, Michael. Organized Uncertainty: Designing a World of Risk Management. New York: Oxford University Press, 2007.

Prebisch, Raúl. The Economic Development of Latin America and Its Principal Problems. Lake Success, NY: United Nations
Department of Social Affairs, 1950.
Pressman, Steven. Fifty Major Economists. 2nd ed. London and New York: Routledge, 2006.
Quiggin, John. Zombie Economics: How Dead Ideas Still Walk Among Us. Princeton, NJ: Princeton University Press, 2011.
Rajan, Raghuram G. Fault Lines: How Hidden Fractures Still Threaten the World Economy. Princeton, NJ: Princeton
University Press, 2010.
Rao, Bhanoji, East Asian Economies: The Miracle, a Crisis and the Future. Singapore: McGraw-Hill, 2001.
Ravenhill, John, ed. APEC and the Construction of Pacific Rim Regionalism. Cambridge, UK: Cambridge University Press,
2002.
Rebonato, Riccardo. Plight of the Fortune Tellers: Why We Need to Manage Financial Risk Differently. Princeton, NJ:
Princeton University Press, 2007.
Reekie, W. Duncan. Markets, Entrepreneurs, and Liberty: An Austrian View of Capitalism. New York: St. Martin’s, 1984.
Reich, Robert B. The Next American Frontier. New York: New York Times Books, 1983.
Reich, Robert B. Aftershock: The Next Economy and America’s Future. New York: Vintage, 2010.
Reinhart, Carmen M., and Kenneth S. Rogoff. This Time Is Different: Eight Centuries of Financial Folly. Princeton, NJ:
Princton University Press, 2009.
Reinis, August, ed. Standards of Investment Protection. New York: Oxford University Press, 2008.
Reisman, David. The Economics of Alfred Marshall. London: Macmillan, 1986.
Ricklefs, M.C. A History of Modern Indonesia Since c. 1200. Palo Alto, CA: Stanford University Press, 2008.
Rigg, Jonathan. Southeast Asia: The Human Landscape of Modernization and Development. 2nd ed. New York: Routledge,
2003.
Rima, Ingrid H., ed. The Joan Robinson Legacy. Armonk, NY: M.E. Sharpe, 1991.
Robbins, Lionel. The Great Depression. London: Macmillan, 1934.
Robbins, Lionel. Jacob Viner: A Tribute. Princeton, NJ: Princeton University Press, 1970.
Robertson, Dennis H. A Study of Industrial Fluctuations: An Enquiry into the Character and Causes of the So-Called Cyclical
Movements of Trade. London: P.S. King, 1915.
Robinson, Joan. Collected Economic Papers. Oxford, UK: Blackwell, 1980.
Robinson, Joan. Further Contributions to Modern Economics. Oxford, UK: Blackwell, 1980.
Robinson, Michael A. Overdrawn: The Bailout of American Savings. New York: Dutton, 1990.
Robinson, William. Transnational Conflicts: Central America, Social Change, and Globalization. New York: Verso, 2003.
Roett, Riordan. The Mexican Peso Crisis. Boulder, CO: Lynne Rienner, 1996.
Romano, Flavio. Clinton and Blair: The Political Economy of the Third Way . New York: Routledge, 2006.
Romer, Christina D. Changes in Business Cycles: Evidence and Explanations. Cambridge, MA: National Bureau of
Economic Research, 1999.
Romer, Christina D. Monetary Policy and the Well-Being of the Poor. Cambridge, MA: National Bureau of Economic
Research, 1998.
Roncaglia, Alessandro. The Wealth of Ideas. New York: Cambridge University Press, 2005.
Röpke, Wilhelm. The Crisis of European Economic Integration. Zurich: Swiss Credit Bank, 1963.
Röpke, Wilhelm. A Humane Economy: The Social Framework of the Free Market. Chicago: H. Regnery, 1960.
Rose, Peter S., and Sylvia C. Hudgins. Bank Management and Financial Services. New York: McGraw-Hill, 2008.

Rosen, Elliot A. Roosevelt, the Great Depression, and the Economics of Recovery. Charlottesville: University of Virginia
Press, 2005.
Rosenof, Theodore. Economics in the Long Run: New Deal Theorists and Their Legacies, 1933–1993. Chapel Hill:
University of North Carolina Press, 1997.
Ross, Ian Simpson. The Life of Adam Smith. New York: Oxford University Press, 1995.
Ross, Stephen A., Randolph W. Westerfield, and Bradford Jordan. Fundamentals of Corporate Finance. 9th ed. New York:
McGraw-Hill, 2009.
Røste, Ole Bjørn. Monetary Policy and Macroeconomic Stabilization: The Roles of Optimum Currency Areas, Sacrifice
Ratios, and Labor Market Adjustment. New Brunswick, NJ: Transaction, 2008.
Rostow, W.W. Concept and Controversy: Sixty Years of Taking Ideas to Market. Austin: University of Texas Press, 2003.
Rostow, W.W. The Process of Economic Growth. New York: W.W. Norton, 1952.
Rowley, C., and Y. Paik, eds. The Changing Face of Korean Management. London: Routledge, 2009.
Roy, Subroto, and John Clarke, eds. Margaret Thatcher’s Revolution: How It Happened and What It Meant. New York:
Continuum, 2005.
Rubin, Robert E., and Jacob Weisberg. In an Uncertain World: Tough Choices from Wall Street to Washington. New York:
Random House, 2004.
Ryscavage, Paul. Rethinking the Income Gap. New Brunswick, NJ: Transaction, 2009.
Sachs, Jeffrey D. The End of Poverty: Economic Possibilities for Our Time. New York: Penguin, 2005.
Sadowski, ZdzisŁaw, and Adam Szeworski, eds. Kalecki’s Economics Today. New York: Routledge, 2004.
Safford, Frank, and Marco Palacios. Colombia: Fragmented Land, Divided Society. New York: Oxford University Press,
2002.
Salvadori, Massimo L. Karl Kautsky and the Socialist Revolution, 1880–1938. London: NLB, 1979.
Samii, Massood, and Gerald Karush, eds. International Business and Information Technology: Interaction and
Transformation in the Global Economy. New York: Routledge, 2004.
Samuelson, Paul. Foundations of Economic Analysis. Enlarged ed. Cambridge, MA: Harvard University Press, 1983.
Samuelson, Paul A., and William D. Nordhaus. Economics. Boston: McGraw-Hill/Irwin, 2009.
Samuelson, Robert J. The Great Inflation and Its Aftermath: The Past and Future of American Affluence New York: Random
House, 2008.
SarDesai, D.R. Southeast Asia: Past and Present. 6th ed. Boulder, CO: Westview, 2010.
Saunders, Anthony, and Marcia Millon Cornett. Financial Markets and Institutions: An Introduction to the Risk Management
Approach. New York: McGraw-Hill, 2007.
Sawyer, James E. Why Reaganomics and Keynesian Economics Failed. New York: St. Martin’s, 1987.
Sawyer, Malcolm C., ed. The Legacy of MichaŁ Kalecki. Cheltenham, UK: Edward Elgar, 1999.
Schama, Simon. The Embarrassment of Riches: An Interpretation of Dutch Culture in the Golden Age. New York: Alfred A.
Knopf, 1987.
Schedvin, C.B. Australia and the Great Depression. Sydney: Sydney University Press, 1970.
Schiere, Richard. China’s Development Challenges: Public Sector Reform and Vulnerability to Poverty. New York:
Routledge, 2010.
Schiller, Bradley R. The Economics of Poverty and Discrimination. 10th ed. Upper Saddle River, NJ: Pearson/Prentice Hall,
2008.
Schioppa, Fiorella P. Italy: Structural Problems in the Italian Economy. New York: Oxford University Press, 1993.

Schlefer, Jonathan. Palace Politics: How the Ruling Party Brought Crisis to Mexico. Austin: University of Texas Press, 2008.
Schlesinger, Arthur M. The Coming of the New Deal. London: Heinemann, 1960.
Schneider, Ben Ross. Business Politics and the State in Twentieth-Century Latin America. New York: Cambridge University
Press, 2004.
Schofield, Neil C. Commodity Derivatives: Markets and Applications. Hoboken, NJ: John Wiley and Sons, 2007.
Schumpeter, Joseph. Business Cycles: A Theoretical, Historical and Statistical Analysis of the Capitalist Process. 2 vols.
New York: McGraw-Hill, 1939.
Schumpeter, Joseph. The Theory of Economic Development. New Brunswick, NJ: Transaction, 1983.
Seidman, L. William. Full Faith and Credit: The Great S&L Debacle and Other Washington Sagas. New York: Times Books,
1993.
Sena, Vania. Credit and Collateral. New York: Routledge, 2007.
Senor, Dan, and Saul Singer. Start-Up Nation: The Story of Israel’s Economic Miracle. New York: Twelve, 2009.
Shackle, George L.S. Epistemics and Economics: A Critique of Economic Doctrines. Cambridge, UK: Cambridge University
Press, 1972.
Shackle, George L.S. Keynesian Kaleidics: The Evolution of a General Political Economy. Edinburgh, UK: Edinburgh
University Press, 1974.
Sharpe, Myron E. John Kenneth Galbraith and the Lower Economics. 2nd ed. White Plains, NY: International Arts and
Sciences, 1974.
Shefrin, Hersh. Beyond Greed and Fear: Understanding Behavioral Finance and the Psychology of Investing. New York:
Oxford University Press, 2007.
Shiller, Robert J. Irrational Exuberance. 2nd ed. New York: Currency/Doubleday, 2005.
Shiller, Robert J. The Subprime Solution: How Today’s Global Financial Crisis Happened and What To Do About It.
Princeton, NJ: Princeton University Press, 2008.
Shlaes, Amity. The Forgotten Man: A New History of the Great Depression. New York: Harper Perennial, 2008.
Sicilia, David, and Jeffrey Cruikshank. The Greenspan Effect. New York: McGraw-Hill, 2000.
Siebert, Horst. The German Economy: Beyond the Social Market. Princeton, NJ: Princeton University Press, 2005.
Siklos, Pierre L., ed. The Economics of Deflation. Northampton, MA: Edward Elgar, 2006.
Simon, Julian L., ed. The Economics of Population: Classic Writings. New Brunswick, NJ: Transaction, 1998.
Singh, Dalvinder. Banking Regulation of UK and US Financial Markets. Farnham, UK: Ashgate, 2007.
Skidelsky, Robert. John Maynard Keynes: A Biography. London: Macmillan, 1983.
Skidelsky, Robert. Keynes: The Return of the Master. New York: Public Affairs, 2009.
Skott, Peter. Kaldor’s Growth and Distribution Theory. New York: Peter Lang, 1989.
Smith, Adam. An Inquiry into the Nature and Causes of the Wealth of Nations. Chicago: University of Chicago Press, 1977.
Smith, Adam. The Theory of Moral Sentiments, ed. Knud Haakonssen. Cambridge, UK: Cambridge University Press, 2002.
Smith, John, ed. The Rescue and Repair of Fannie Mae and Freddie Mac. Hauppauge, NY: Nova Science, 2009.
Snider, David, and Chris Howard. Money Makers: Inside the New World of Finance and Business. New York: Palgrave
Macmillan, 2010.
Soloki, E. Cary, and Robert M. Brown. Paul Samuelson and Modern Economic Theory. New York: McGraw-Hill, 1983.
Solomon, Robert. Money on the Move: The Revolution in International Finance Since 1980. Princeton, NJ: Princeton
University Press, 1999.

Solomou, Solomos. Phases of Economic Growth, 1850–1973: Kondratieff Waves and Kuznets Swings.  New York:
Cambridge University Press, 1988.
Sorkin, Andrew Ross. Too Big to Fail: The Inside Story of How Wall Street and Washington Fought to Save the Financial
System from Crisis—and Themselves. New York: Viking, 2009.
Soros, George. The New Paradigm for Financial Markets: The Credit Crisis of 2008 and What It Means. Jackson, TN:
PublicAffairs, 2008.
Sowell, Thomas. On Classical Economics. New Haven, CT: Yale University Press, 2006.
Spence, Jonathan. The Search for Modern China. New York: W.W. Norton, 1999.
Spencer, Peter D. The Structure and Regulation of Financial Markets. New York: Oxford University Press, 2000.
Spencer, Roger W., and John H. Huston. The Federal Reserve and the Bull Markets: From Benjamin Strong to Alan
Greenspan. Lewiston, NY: Edwin Mellen, 2006.
Spotgeste, Milton R., ed. Securitization of Subprime Mortgages. Hauppauge, NY: Nova Science, 2009.
Spotton Visano, Brenda. Financial Crises: Socio-economic Causes and Institutional Context. New York: Routledge, 2006.
Sprague, O.M.W. History of Crises Under the National Banking System. Washington, DC: Government Printing Office, 1910.
Sprague, O.M.W. Theory and History of Banking. New York: Putnam, 1929.
Spulber, Nicolas. Managing the American Economy, from Roosevelt to Reagan. Bloomington: Indiana University Press,
1989.
Steindl, Josef. Economic Papers, 1941–88. London: Macmillan, 1990.
Steindl, Josef. Maturity and Stagnation in American Capitalism. Oxford, UK: Basil Blackwell, 1952.
Stern, Gary F., and Ron J. Feldman. Too Big to Fail: The Hazards of Bank Bailouts. Washington, DC: Brookings Institution,
2009.
Stiglitz, Joseph E. Free Fall: America, Free Markets and the Sinking of the World Economy. New York: W.W. Norton, 2010.
Stiglitz, Joseph E. Globalization and Its Discontents. London: Penguin, 2002.
Stiglitz, Joseph E., and Shahid Yusef, eds. Rethinking the East Asian Miracle. New York: Oxford University Press, 2001.
Strasser, Susan, Charles McGovern, and Matthias Judt. Getting and Spending: European and American Consumer
Societies in the Twentieth Century. New York: Cambridge University Press, 1998.
Stridsman, Thomas. Trading Systems That Work: Building and Evaluating Effective Trading Systems. New York: McGraw-
Hill, 2000.
Strober, Deborah H. Catastrophe: The Story of Bernard L. Madoff, the Man Who Swindled the World. Beverly Hills, CA:
Phoenix, 2009.
Strouse, Jean. Morgan: American Financier. New York: Harper, 2000.
Swedberg, Richard. Schumpeter: A Biography . Princeton, NJ: Princeton University Press, 1991.
Syrett, Stephen. Contemporary Portugal: Dimensions of Economic and Political Change. Aldershot, UK: Ashgate, 2002.
Szenberg, Michael, Lall Ramrattan, and Aron A. Gottesman, eds. Samuelsonian Economics and the Twenty-First Century.
New York: Oxford University Press, 2006.
Szostak, Rick. The Causes of Economic Growth: Interdisciplinary Perspectives. Berlin: Springer, 2009.
Tavakoli, Janet M. Structured Finance and Collateralized Debt Obligations: New Developments in Cash and Synthetic
Securitization. Hoboken, NJ: John Wiley and Sons, 2008.
Taylor, John B., ed. Monetary Policy Rules. Chicago: University of Chicago Press, 2001.
Tedlow, Richard. The Rise of the American Business Corporation. Philadelphia: Harwood Academic, 1991.

Teichman, Judith A. Policymaking in Mexico: From Boom to Crisis. New York: Routledge, 1988.
Temin, Peter. Lessons from the Great Depression. Cambridge, MA: MIT Press, 1989.
Tett, Gillian. Fool’s Gold: The Inside Story of J.P. Morgan and How Wall Street Greed Corrupted Its Bold Dream and
Created a Financial Catastrophe. New York: Free Press, 2010.
Thomas, Gordon, and Max Morgan-Witts. The Day the Bubble Burst: A Social History of the Wall Street Crash of 1929. New
York: Penguin, 1980.
Thorp, Rosemary. Progress, Poverty, and Exclusion: An Economic History of Latin America in the 20th Century.
Washington, DC: Inter-American Development Bank, 1998.
Thorp, Willard Long. Economic Institutions. New York: Macmillan, 1928.
Thorp, Willard Long. The New Inflation. New York: McGraw-Hill, 1959.
Tibman, Joseph. The Murder of Lehman Brothers: An Insider’s Look at the Global Meltdown. New York: Brick Tower, 2009.
Tilman, Rick, ed. The Legacy of Thorstein Veblen. Northampton, MA: Edward Elgar, 2003.
Tily, Geoff. Keynes’s General Theory, the Rate of Interest and ‘Keynesian’ Economics. New York: Palgrave Macmillan,
2007.
Timmons, J., and S. Olin. New Venture Creation: Entrepreneurship for the 21st Century. Boston: McGraw-Hill, 2004.
Tinbergen, Jan. Econometrics. New York: Routledge, 2005.
Tirole, Jean. Financial Crises, Liquidity, and the International Monetary System. Princeton, NJ: Princeton University Press,
2002.
Tobin, James. Essays in Economics: Macroeconomics. Cambridge, MA: MIT Press, 1987.
Tobin, James. Full Employment and Growth: Further Keynesian Essays on Policy. Cheltenham, UK: Edward Elgar, 1996.
Topik, Steven, Carlos Marichal, and Zephyr Frank, eds. From Silver to Cocaine: Latin America Commodity Chains and the
Building of the World Economy, 1500–2000. Durham, NC: Duke University Press, 2006.
Treaster, Joseph B. Paul Volcker: The Making of a Financial Legend. Hoboken, NJ: John Wiley and Sons, 2004.
Trentmann, Frank. Free Trade Nation: Commerce, Consumption, and Civil Society in Modern Britain. New York: Oxford
University Press, 2008.
Tuch, Hans N. Arthur Burns and the Successor Generation: Selected Writings of and About Arthur Burns. Lanham, MD:
University Press of America, 1999.
Turner, Graham. The Credit Crunch: Housing Bubbles, Globalisation and the Worldwide Economic Crisis. London: Pluto,
2008.
Turner, Marjorie S. Nicholas Kaldor and the Real World. Armonk, NY: M.E. Sharpe, 1993.
Turner, Michael, ed. Malthus and His Time. Houndmills, UK: Macmillan, 1986.
Tvede, Lars. Business Cycles: History, Theory, and Investment Reality. New York: John Wiley and Sons, 2006.
Van Overtveldt, Johan. The Chicago School: How the University of Chicago Assembled the Thinkers Who Revolutionized
Economics and Business. Chicago: Agate, 2007.
Veblen, Thorstein. The Theory of Business Enterprise. New York: A.M. Kelley, 1975. First published in 1904.
Veblen, Thorstein. The Theory of the Leisure Class. New York: Dover, 1994. First published in 1899.
Viner, Jacob. Balanced Deflation, Inflation, or More Depression. Minneapolis: University of Minnesota Press, 1933.
Viner, Jacob. The Long View and the Short: Studies in Economic Theory and Policy. New York: Free Press, 1958.
Visano, Brenda Spotton. Financial Crises: Socio-economic Causes and Institutional Context. New York and London:
Routledge, 2006.

Volcker, Paul A. The Rediscovery of the Business Cycle. New York: Free Press, 1978.
Volcker, Paul A. The Triumph of Central Banking? Washington, DC: Per Jacobsson Foundation, 1990.
Von Neumann, John. John von Neumann, Selected Letters, ed. Mikl´os R´edei. Providence, RI: American Mathematical
Society, 2005.
Von Neumann, John, and Oskar Morgenstern. Theory of Games and Economic Behavior. Princeton, NJ: Princeton
University Press, 2007.
Wagener, Hans-Jürgen. Economic Thought in Communist and Post-Communist Europe. New York: Routledge, 1998.
Weil, David N. Economic Growth. Boston: Addison-Wesley, 2005.
Weintraub, E. Roy. How Economics Became a Mathematical Science. Durham, NC: Duke University Press, 2002.
Wells, Wyatt. Economist in an Uncertain World: Arthur F. Burns and the Federal Reserve, 1970–1978. New York: Columbia
University Press, 1994.
Wessel, David. In Fed We Trust: Ben Bernanke’s War on the Great Panic. New York: Crown, 2010.
White, Lawrence H. A Theory of Monetary Institutions. New York: Wiley-Blackwell, 1999.
White, Lawrence J. The S&L Debacle: Public Policy Lessons for Bank and Thrift Regulation. New York: Oxford University
Press, 1991.
Wicker, Elmus. Banking Panics of the Gilded Age. New York: Cambridge University Press, 2000.
Wicker, Elmus. The Banking Panics of the Great Depression. 2nd ed. New York: Cambridge University Press, 2000.
Wigmore, Barrie A. The Crash and Its Aftermath: A History of Securities Markets in the United States, 1929–1933.
Westport, CT: Greenwood, 1985.
Williams, Andrea D., ed. The Essential Galbraith. Boston: Houghton Mifflin, 2001.
Wilson, Scott. Remade in China: Foreign Investors and Institutional Change in China. New York: Oxford University Press,
2009.
Wolfson, Martin H. Financial Crisis: Understanding the Postwar U.S. Experience. Armonk, NY: M.E. Sharpe, 1986.
Wong, Stanley. Foundations of Paul Samuelson’s Revealed Preference Theory. New York: Routledge, 2009.
Wood, Christopher. The Bubble Economy: Japan’s Extraordinary Boom of the’80s and the Dramatic Bust of the’90s. New
York: Atlantic Monthly, 1992.
Woodward, Bob. Maestro: Greenspan’s Fed and the American Boom. New York: Simon & Schuster, 2001.
Wray, L. Randall. Understanding Modern Money: The Key to Full Employment and Price Stability. Northampton, MA:
Edward Elgar, 2003.
Wright, Russell. Chronology of the Stock Market. Jefferson, NC: McFarland, 2002.
Yarbrough, Beth V., and Robert M. Yarbrough. The World Economy: Trade and Finance. Mason, OH: South-Western, 2006.
Yonay, Yuval P. The Struggle over the Soul of Economics: Institutionalist and Neoclassical Economists in America Between
the Wars. Princeton, NJ: Princeton University Press, 1998.
Young, Warren. Harrod and His Trade Cycle Group. New York: New York University Press, 1989.
Zahariadis, Nikolaos. State Subsidies in the Global Economy. New York: Palgrave Macmillan, 2008.
Zamagni, Stefano, and Ernesto Screpanti. An Outline of the History of Economic Thought. New York: Oxford University
Press, 1993.
Zandi, Mark. Financial Shock: A 360° Look at the Subprime Mortgage Implosion, and How to Avoid the Next Financial
Crisis. Upper Saddle River, NJ: FT Press, 2009.
Zarnowitz, Victor. Business Cycles: Theory, History, Indicators, and Forecasting. Chicago: University of Chicago Press,
1992.

Zmirak, John. Wilhelm Röpke: Swiss Localist, Global Economist. Wilmington, DE: ISI Books, 2001.
Zuckerman, Gregory. The Greatest Trade Ever: The Behind-the-Scenes Story of How John Paulson Defied Wall Street and
Made Financial History. New York: Broadway Books, 2009.
Master Bibliography
 Web Sites
Association of Southeast Asian Nations (ASEAN). http://www.aseansec.org
Basel Committee on Banking Supervision. http://www.bis.org/bcbs
Conference Board. http://www.conference-board.org
Congressional Budget Office. http://www.cbo.gov
Consumers International. http://www.consumersinternational.org
Council of Economic Advisers. http://www.whitehouse.gov/administration/eop/cea
Department of Housing and Urban Development. http://www.hud.gov
Dow Jones Indices. http://www.djaverages.com
Emerging Markets Monitor. http://www.emergingmarketsmonitor.com
European Central Bank. http://www.ecb.int
Executive Office of the President. http://www.whitehouse.gov/administration/eop
Fannie Mae. http://www.fanniemae.com
FDIC. http://www.fdic.gov
Federal Housing Administration. http://www.fha.com
Federal Housing Finance Agency. http://www.fhfa.gov
Federal Housing Finance Board. http://www.fhfb.gov
Federal Reserve System. http://www.federalreserve.gov
Freddie Mac. http://www.freddiemac.com
Government Accountability Office. http://www.gao.gov
Government National Mortgage Association (GinnieMae). http://www.ginniemae.gov
International Monetary Fund. http://www.imf.org

International Organization of Securities Commissions. http://www.iosco.org
Morgan Stanley. http://www.morganstanley.com
Mortgage Bankers Association. http://www.mortgagebankers.org
NASDAQ. http://www.nasdaq.com
National Venture Capital Association. http://www.nvca.org
New York Stock Exchange. http://www.nyse.com
Office of Federal Housing Enterprise Oversight. http://www.ofheo.gov
Office of Thrift Supervision. http://www.ots.treas.gov
Securities and Exchange Commission. http://www.sec.gov
Securities Industry and Financial Markets Association. http://www.sifma.org
Standard & Poor’s. http://www.standardandpoors.com
Statistics Canada. http://www.statcan.gc.gov
Treasury Direct. http://www.treasurydirect.gov
U.S. Bureau of Labor Statistics. http://www.bls.gov
U.S. Department of Labor. “Wages”. http://www.dol.gov/dol/topic/wages
U.S. Department of the Treasury. http://www.treasury.gov
World Bank. http://www.worldbank.org

