Journal of Machine Learning Research 23 (2022) 1-71
Submitted 1/21; Revised 10/21; Published 1/22
Score Matched Neural Exponential Families
for Likelihood-Free Inference
Lorenzo Pacchiardi
lorenzo.pacchiardi@stats.ox.ac.uk
Department of Statistics
University of Oxford
Oxford, OX1 3LB
United Kingdom
Ritabrata Dutta
ritabrata.dutta@warwick.ac.uk
Department of Statistics
University of Warwick
Coventry, CV4 7AL
United Kingdom
Editor: Mohammad Emtiyaz Khan
Abstract
Bayesian Likelihood-Free Inference (LFI) approaches allow to obtain posterior distribu-
tions for stochastic models with intractable likelihood, by relying on model simulations. In
Approximate Bayesian Computation (ABC), a popular LFI method, summary statistics
are used to reduce data dimensionality. ABC algorithms adaptively tailor simulations to
the observation in order to sample from an approximate posterior, whose form depends
on the chosen statistics. In this work, we introduce a new way to learn ABC statistics:
we ï¬rst generate parameter-simulation pairs from the model independently on the ob-
servation; then, we use Score Matching to train a neural conditional exponential family to
approximate the likelihood. The exponential family is the largest class of distributions with
ï¬xed-size suï¬ƒcient statistics; thus, we use them in ABC, which is intuitively appealing and
has state-of-the-art performance. In parallel, we insert our likelihood approximation in an
MCMC for doubly intractable distributions to draw posterior samples. We can repeat that
for any number of observations with no additional model simulations, with performance
comparable to related approaches. We validate our methods on toy models with known
likelihood and a large-dimensional time-series model.
Code for reproducing the experiments is available at https://github.com/LoryPack/
SM-ExpFam-LFI.
Keywords:
Likelihood-Free Inference, Score Matching, Approximate Bayesian Compu-
tation, MCMC for doubly intractable distributions, Exponential Family.
1. Introduction
Stochastic simulator models are used to simulate realizations of physical phenomena; usu-
ally, a set of parameters Î¸ govern the simulation output. As the model is stochastic, repeated
simulations with ï¬xed Î¸ yield diï¬€erent outputs; their distribution is the modelâ€™s likelihood.
Simple models provide an analytic expression of the likelihood, which is however unavailable
for more complex ones.
câƒ2022 Lorenzo Pacchiardi and Ritabrata Dutta.
License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
at http://jmlr.org/papers/v23/21-0061.html.

Pacchiardi and Dutta
Upon observing a real-world realization of the phenomenon the model is describing,
researchers may want to obtain a posterior distribution over parameters. If the likelihood
is known, standard Bayesian inference tools (such as MCMC or variational inference) allow
to get posterior samples; if otherwise the likelihood is missing, Likelihood-Free Inference
(LFI) techniques are the only viable solution. LFI has been applied in several domains,
including genomics (TavarÂ´e et al., 1997), biological science (Dutta et al., 2018), meteorology
(Hakkarainen et al., 2012), geological science (Pacchiardi et al., 2020), genomics (TavarÂ´e
et al., 1997; Toni et al., 2009; Marttinen et al., 2015), and epidemiology (McKinley et al.,
2018; Minter and Retkute, 2019; Dutta et al., 2021a).
In some LFI techniques (Wood, 2010; Thomas et al., 2020; Price et al., 2018), the
missing likelihood is replaced with an explicit approximation built from model simulations
at each parameter value. Alternatively, Approximate Bayesian Computation (ABC) (Marin
et al., 2012; Lintusaari et al., 2017) circumvents the unavailability of the likelihood by
comparing model simulations with the observation according to some notion of distance.
To reduce data dimensionality, ABC usually relies on summary statistics, whose choice is
not straightforward. Recently, the expressive capabilities of Neural Networks (NNs) have
been leveraged to learn ABC statistics (Jiang et al., 2017; Wiqvist et al., 2019; Pacchiardi
et al., 2020; ËšAkesson et al., 2020). These techniques train NNs parametrizing the statistics
by minimizing a suitable loss on a training set of parameter-simulation pairs generated from
the model.
In the present work, we propose a new way to learn ABC statistics with NNs. Most pre-
vious works (Jiang et al., 2017; Wiqvist et al., 2019; ËšAkesson et al., 2020) trained a single NN
with the regression loss introduced in Fearnhead and Prangle (2012). Instead, we consider
an exponential family with two NNs parametrizing respectively the suï¬ƒcient statistics and
natural parameters and ï¬t this to the model. As the exponential family is the most general
class of distributions with suï¬ƒcient statistics of ï¬xed size (Appendix A), it makes intuitive
sense to use the learned suï¬ƒcient statistics in ABC. Indeed, this approach empirically yields
superior or equivalent performance with respect to state-of-the-art approaches.
As in previous approaches, we consider a training set of parameter-simulation pairs;
however, we train the NNs with Score Matching (SM) or its Sliced approximation (SSM),
which do not require evaluating the normalizing constant of the exponential family. We
extend the SM and SSM objectives to the setting of conditional densities, thus ï¬tting the
likelihood approximation for all values of data and parameters.
In contrast to related approaches (Jiang et al., 2017; Wiqvist et al., 2019; Pacchiardi
et al., 2020; ËšAkesson et al., 2020), our method provides a full likelihood approximation. We
test therefore direct sampling from the corresponding posterior (in place of ABC) with an
MCMC algorithm for doubly intractable distributions. This approach achieves satisfactory
performance with no additional model simulations and is therefore a valid alternative to
standard LFI schemes for expensive simulator models. The computational gain is even larger
when inference is performed for several observations, as the same likelihood approximation
can be used.
The rest of our paper is organized as follows. In Section 2, we brieï¬‚y review some LFI
methods. Next, in Section 3 we introduce the neural conditional exponential family and
show how to ï¬t it with SM or SSM. In Section 4 we discuss how to exploit the exponential
family to extract ABC statistics or for direct sampling. We extensively validate our proposed
2

Score Matched Neural Exponential Families for LFI
approaches in Section 5. Finally, we discuss related works in Section 6 and conclude in
Section 7, where we also highlight directions for future research.
1.1 Notation
We set here notation for the rest of our manuscript. We will denote respectively by X âŠ†Rd
and Î˜ âŠ†Rp the data and parameter space. Upper case letters will denote random variables
while lower case ones will denote observed (ï¬xed) values. Subscripts will denote vector
components and superscript in brackets sample indices, while âˆ¥Â· âˆ¥will denote the â„“2 norm.
2. Likelihood-Free Inference
Let us consider a model which allows to generate a simulation x âˆˆX at any parameter
value Î¸ âˆˆÎ˜, but for which it is not possible to evaluate the likelihood p0(x|Î¸). Given an
observation x0 and a prior on the parameters Ï€(Î¸), Bayesian inference obtains the posterior
distribution Ï€0(Î¸|x0) = Ï€(Î¸)p0(x0|Î¸)
p0(x0)
. However, obtaining that explicitly (or even sampling
from it with Markov Chain Monte Carlo, MCMC) is impossible without having access to
the likelihood, at least up to a normalizing constant that is independent on x.
Likelihood-Free Inference techniques yield approximations of the posterior distribution
by replacing likelihood evaluations with model simulations. Broadly, they can be split into
two kinds of approaches: Surrogate Likelihood, which explicitly builds a likelihood function,
and Approximate Bayesian Computation (ABC), which instead uses discrepancy between
simulated and observed data. We detail those two approaches in the following.
2.1 Approximate Bayesian Computation (ABC)
Approximate Bayesian Computation (ABC, Marin et al. 2012; Lintusaari et al. 2017) algo-
rithms sample from an approximate posterior distribution by ï¬nding parameter values that
yield simulated data resembling the observations to a suï¬ƒcient degree. Usually, ABC meth-
ods rely on statistics function to reduce the dimensionality of simulated and observed data
and thus improve the computational eï¬ƒciency of the algorithm. Therefore, the similarity
between simulated and observed data is normally deï¬ned via a distance function between
the corresponding statistics. ABC algorithms sample from:
Ï€Ïµ(Î¸|s(x0)) âˆÏ€(Î¸)
Z
X
1[d(s(x0), s(xsim)) â‰¤Ïµ] p0(xsim|Î¸)dxsim,
where s and d denote respectively the statistics function and the discrepancy measure, while
1[Â·] is an indicator function and Ïµ is an acceptance threshold.1
It is easy to sample from Ï€Ïµ(Î¸|s(x0)) with rejection, by generating (Î¸(j), x(j)) âˆ¼Ï€(Î¸)p0(x|Î¸)
and accepting Î¸(j) if d(s(x0), s(x(j))) â‰¤Ïµ. This is ineï¬ƒcient when Ïµ is small, as most simula-
tions end up being rejected; more eï¬ƒcient ABC algorithms have been developed, based for
instance on Markov Chain Monte Carlo (Marjoram et al., 2003), Population Monte Carlo
1. In general, 1(d(s(x0), s(xsim))) can be replaced with KÏµ(d(s(x0), s(xsim))), where KÏµ is a rejection kernel
which concentrates around 0 and goes to 0 for argument going to Â±âˆ, and whose width decreases when
Ïµ decreases. We restrict here on 1, which is the most common choice.
3

Pacchiardi and Dutta
(Beaumont, 2010) or Sequential Monte Carlo (Del Moral et al., 2012), which reach lower
values of Ïµ with the same computational budget.
If d is a distance measure and the statistic s is suï¬ƒcient for p(x|Î¸), then the ABC poste-
rior Ï€Ïµ(Î¸|s(x0)) converges to the true one as Ïµ â†’0 (Lintusaari et al. 2017; see Appendix A
for deï¬nition of suï¬ƒcient statistics).
In general, however, suï¬ƒcient statistics are unavailable so that ABC can at best give
an approximation of the exact posterior even with Ïµ â†’0. Hand-picking the best summary
statistics for a given model limits the applicability of ABC. Therefore, some approaches
to automatically build them have been developed (Fearnhead and Prangle, 2012; Nunes
and Balding, 2010; Joyce and Marjoram, 2008; Aeschbacher et al., 2012; Blum et al., 2013;
Pacchiardi et al., 2020). We describe here the approach that is most commonly used.
ABC with semi-automatic statistics.
The summary statistic sâ‹†(x) = E[Î¸|x] yields an
approximate posterior Ï€Ïµ(Î¸|sâ‹†(x0)) whose mean minimizes the expected squared error loss
from the true parameter value, as Ïµ â†’0 (Fearnhead and Prangle, 2012). Of course, E[Î¸|x] is
unknown; therefore, Fearnhead and Prangle (2012) suggested to consider a set of functions
sÎ²(x) and ï¬nd the one that best approximates E[Î¸|x].
Empirically, this corresponds to
ï¬nding:
Ë†Î² = arg min
Î²
1
N
N
X
j=1
sÎ²(x(j)) âˆ’Î¸(j)
2
2 ,
(1)
where (Î¸(j), x(j))N
j=1, Î¸(j) âˆ¼Ï€(Î¸), x(j) âˆ¼p0(Â·|Î¸(j)) are N data-parameter pairs generated
before performing ABC.
In practice, Fearnhead and Prangle (2012) considered sÎ²(x) to be a linear function of
the weights Î², such that solving the minimization problem in Eq. (1) amounts to linear
regression. Jiang et al. (2017); Wiqvist et al. (2019) and ËšAkesson et al. (2020) instead used
Neural Networks to parametrize sÎ² and train them to minimize the loss in Eq. (1).
2.2 Surrogate Likelihood
Surrogate likelihood approaches exploit simulations to build an explicit likelihood approxi-
mation, which is then inserted in standard likelihood-based sampling schemes (say, MCMC;
Wood 2010; Thomas et al. 2020; Fasiolo et al. 2018; An et al. 2019, 2020; Price et al. 2018).
Here, we discuss two methods that fall under this framework.
Synthetic Likelihood.
Synthetic Likelihood (SL, Wood 2010) replaces the exact likeli-
hood with a normal distribution for the summary statistics s = s(x); speciï¬cally, it assumes
S|Î¸ âˆ¼N(ÂµÎ¸, Î£Î¸), where the mean vector ÂµÎ¸ and covariance matrix Î£Î¸ depend on Î¸. For
each Î¸, an estimate of ÂµÎ¸ and Î£Î¸ is built with model simulations, and the likelihood of the
observed statistics s(x0) is evaluated.
Using the statistics likelihood to deï¬ne a posterior distribution yields a Bayesian SL
(BSL). In Price et al. (2018), MCMC is used to sample from the BSL posterior; as in
ABC, this approach requires generating simulations for each considered Î¸. However, one
single simulation per parameter value is usually suï¬ƒcient in ABC, while estimating ÂµÎ¸, Î£Î¸
in BSL requires multiple simulations. Nevertheless, a parametric likelihood approximation
seemingly allows scaling to larger dimensional statistics space (Price et al., 2018).
4

Score Matched Neural Exponential Families for LFI
Ratio Estimation.
Thomas et al. (2020) tackle LFI by estimating the ratio between
likelihood and data marginal: r(x, Î¸) = p(x|Î¸)/p(x). As r(x, Î¸) âˆp(x|Î¸) with respect to
Î¸, this can be inserted in a likelihood-based sampling scheme in order to sample from the
posterior, similarly to BSL. This method is termed Ratio Estimation (RE).
In practice, for each value of Î¸, an estimate Ë†r(x, Î¸) is built by generating simulations
from the model p0(x|Î¸) and from the marginal2 p(x), and then ï¬tting a logistic regression
discriminating between the two sets of samples. In fact, logistic regression attempts to ï¬nd
a function Ë†h(x; Î¸) for which eË†h(x;Î¸) â‰ˆr(x; Î¸).
In Thomas et al. (2020), Ë†h is chosen in the class of functions hÎ²(x; Î¸) = Pk
i=1 Î²i(Î¸)Ïˆi(x) =
Î²(Î¸)T Ïˆ(x), where Ïˆ is a vector of summary statistics and Î²(Î¸) are coeï¬ƒcients determined
independently for each Î¸; this therefore boils down to a linear logistic regression. Viewed
diï¬€erently, this approach approximates the likelihood with an exponential family, as in fact
it assumes p(x|Î¸) âˆË†r(x, Î¸) = exp(Ë†Î²(Î¸)T Ïˆ(x)), for some coeï¬ƒcients Ë†Î² determined by data;
it is thus a more general likelihood assumption than SL (as the normal distribution belongs
to the exponential family).
Remark 1 (Reducing the number of simulations.) Several approaches have attempted
to reduce the number of simulations required in SL, RE and ABC. Speciï¬cally, Gaussian
Processes can be exploited to replace evaluations of simulation-based quantities with emu-
lated values, while at the same time providing an uncertainty quantiï¬cation used to guide
the next model simulations; that has been done for both ABC (Meeds and Welling, 2014;
Wilkinson, 2014; Gutmann et al., 2016; JÂ¨arvenpÂ¨aÂ¨a et al., 2019; Jarvenpaa et al., 2020)
and SL (Meeds and Welling, 2014; Wilkinson, 2014; Moores et al., 2015; JÂ¨arvenpÂ¨aÂ¨a et al.,
2021). These approaches all consider a ï¬xed observation x and emulate over parameter
values.
In Hermans et al. (2020), a classiï¬er more powerful than logistic regression is used to
learn the likelihood ratio estimate for all (x, Î¸) using parameter-simulation pairs (similarly
to what we propose in Section. 3.3), amortizing therefore RE with respect to the observation.
3. Likelihood approximation with the exponential family
In this Section, we ï¬rst introduce the parametric family which we use to approximate the
likelihood.
Then, we describe Score Matching (SM) and Sliced Score Matching (SSM),
which allow us to ï¬t distributions with unknown normalizing constants to data. Finally,
we discuss how to extend SM and SSM to the setting of conditional densities, to obtain a
likelihood approximation valid for all data and parameter values.
3.1 Conditional exponential family
A probability distribution belongs to the exponential family if it has a density of the fol-
lowing form:3
2. Simulating from the marginal can be done by drawing Î¸j âˆ¼p(Î¸), xj âˆ¼p(x|Î¸j) and discarding Î¸j.
3. As we are concerned with continuous random variables, across the work we use the Lebesgue measure
as a base measure, without explicitly referring to it.
5

Pacchiardi and Dutta
p(x|Î¸) = eÎ·(Î¸)T f(x)h(x)
Z(Î¸)
,
(2)
where x âˆˆX, Î¸ âˆˆÎ˜.
Here, f : X â†’Rds is a function of the data (suï¬ƒcient statis-
tics), Î· : Î˜ â†’Rds is a function of the parameters (natural parameters) and h(x) :
X â†’R is a scalar function of data (base measure). The normalizing constant Z(Î¸) =
R
X exp{Î·(Î¸)T f(x)}h(x)dx is intractable and assumed to be ï¬nite âˆ€Î¸ âˆˆÎ˜; we will discuss
later (Sec. 3.3) why this assumption is not an issue.
Across this work, we refer to Eq. (2) as conditional exponential family to stress that we
learn an approximation valid for all xâ€™s and Î¸â€™s by selecting functions f, Î· and h.
In the following, we rewrite Eq. (2) as: p(x|Î¸) = exp(Â¯Î·(Î¸)T Â¯f(x))/Z(Î¸), where Â¯Î·(Î¸) =
(Î·(Î¸), 1) âˆˆRds+1 and Â¯f(x) = (f(x), log h(x)) âˆˆRds+1. For simplicity, we will drop the bar
notation, using as convention that Î·(Î¸) contains the natural parameters of the exponential
family plus an additional constant term, and that the last component of f(x) is log h(x).
Neural conditional exponential family.
Let now fw and Î·w denote two Neural Net-
works (NNs) with collated weights w (in practice the two NNs do not share parameters);
then, the neural conditional exponential family is deï¬ned as:
pw(x|Î¸) = eÎ·w(Î¸)T fw(x)
Zw(Î¸)
.
(3)
Remark 2 Ratio Estimation (Sec. 2.2) parametrizes the likelihood as an exponential family
with user-speciï¬ed statistics and natural parameters Î² independently learned for each Î¸ âˆˆÎ˜.
In contrast, here we learn both the fw(x) and the Î·w(Î¸) transformations over all X and Î˜
at once by selecting the best w.
Remark 3 (Identiï¬ability) For a family of distributions indexed by a parameter Ï†, iden-
tiï¬ability means that pÏ†(x|Î¸) = pÏ†â€²(x|Î¸) âˆ€x, Î¸ =â‡’Ï† = Ï†â€². The weights w in the neural
conditional exponential family are not identiï¬able for two reasons: ï¬rst, NNs have many
intrinsic symmetries. Secondly, replacing fw in Eq. (3) with A Â· fw and Î·w with (AT )âˆ’1 Â· Î·w
does not change the probability density. In Khemakhem et al. (2020), two suitable con-
cepts of function identiï¬ability up to some linear transformations are deï¬ned. fw and Î·w in
Eq. (3) are identiï¬able according to those deï¬nitions, under strict conditions on the archi-
tectures of the Neural Networks parametrizing them (Khemakhem et al., 2020). Moreover,
they empirically verify that NNs not satisfying the above assumptions result in approximately
identiï¬able fw and Î·w, according to their deï¬nition. More details are given in Appendix B.1.
Remark 4 (Universal approximation) Using larger ds in Eq. (2) increases the expres-
sive power of the approximating family.
Khemakhem et al. (2020) proved that, by con-
sidering a freely varying ds and generic f and Î·, the conditional exponential family has
universal approximation capabilities for the set of conditional probability densities; we give
more details in Appendix B.2. This result does not consider the practicality of ï¬tting the
approximating family to data, which arguably becomes harder when ds increases.
6

Score Matched Neural Exponential Families for LFI
3.2 Score Matching
In order to ï¬t a parametric density pw to data, the standard approach is ï¬nding the Maxi-
mum Likelihood Estimator (MLE) for w; in the limit of inï¬nite data, that corresponds to
minimizing the Kullback-Leibler divergence from the data distribution. MLE requires how-
ever the normalizing constant of pw to be known, which is not the case for our approximating
family (Eq. 3).
Score Matching (SM, HyvÂ¨arinen 2005) is a possible way to bypass the intractability of
the normalizing constant. In this Subsection, we review SM for unconditional densities,
discuss a faster version and provide an extension for distributions with bounded domain.
The original Score Matching (SM)
Let us discard now the conditional dependency
on Î¸ and, following HyvÂ¨arinen (2005), consider a random variable X distributed according
to p0(x). We want to use samples from p0 to ï¬t a generic model pw(x) = Ëœpw(x)/Zw, where
Ëœpw is unnormalized and Zw is intractable.
Deï¬nition 5 Score Matching (SM) corresponds to ï¬nding:
arg min
w
DF (p0âˆ¥pw),
where the Fisher Divergence DF is deï¬ned as:
DF (p0âˆ¥pw) = 1
2
Z
X
p0(x)âˆ¥âˆ‡x log p0(x) âˆ’âˆ‡x log pw(x)âˆ¥2dx.
(4)
The Fisher divergence depends only on the the logarithmic derivatives âˆ‡x log p0(x) and
âˆ‡x log pw(x), which are termed scores4; computing DF does not require therefore knowing
the normalizing constant, as in fact:
âˆ‡x log pw(x) = âˆ‡x(log Ëœpw(x) âˆ’log Zw) = âˆ‡x log Ëœpw(x).
Nevertheless, a Monte Carlo estimate of DF in Eq. (4) using samples from p0 would re-
quire knowing the logarithmic gradient of p0; for this reason, Eq. (4) is usually termed
implicit Fisher divergence. If X = Rd and under mild conditions, Theorem 1 in HyvÂ¨arinen
(2005) obtains an equivalent form for DF in which p0 only appears as the distribution over
which expectation needs to be computed. We give here a similar result holding for more
general X = Nd
i=1(ai, bi), where ai, bi âˆˆR âˆª{Â±âˆ}; speciï¬cally, we consider the following
assumptions:
A1 p0(x) âˆ‚
âˆ‚xi log pw(x) â†’0 when xi â†˜ai and xi â†—bi, âˆ€w, i,
A2 Ep0[âˆ¥âˆ‡x log p0(X)âˆ¥2] < âˆ, Ep0[âˆ¥âˆ‡x log pw(X)âˆ¥2] < âˆ, âˆ€w,
A3 Ep0

âˆ‚2
âˆ‚xiâˆ‚xj log pw(X)
 < âˆ, âˆ€w, âˆ€i, j = 1, . . . , d.5
4. In most of the statistics literature, score usually refer to the derivative of the log-likelihood with respect
to the parameter; here, the nomenclature is slightly diï¬€erent.
5. We remark that this assumption was not present in HyvÂ¨arinen (2005), but it is necessary to apply
Fubini-Tonelli theorem in the proof (see Appendix C.1), as already discussed in Yu et al. (2019).
7

Pacchiardi and Dutta
With the above, we can state the following:
Theorem 6 Let X = Nd
i=1(ai, bi), where ai, bi âˆˆR âˆª{Â±âˆ}. If p0(x) is diï¬€erentiable and
pw(x) doubly diï¬€erentiable over X, then, under assumptions A1, A2, A3, the objective in
Eq. (4) can be rewritten as:
DF (p0âˆ¥pw) =
Z
X
p0(x)
d
X
i=1
"
1
2
âˆ‚log pw(x)
âˆ‚xi
2
+
âˆ‚2 log pw(x)
âˆ‚x2
i
#
dx + C.
(5)
Here, C is a constant w.r.t. pw and xi is the i-th coordinate of x.
A proof is given in Appendix C.1. We refer to Eq. (5) as the explicit Fisher Divergence, as
it is now immediate to build an unbiased Monte Carlo estimate using samples from p0.
From Eq. (4), it is clear that DF (p0âˆ¥pw) â‰¥0 âˆ€w, and that choosing pw = p0 implies
DF (p0âˆ¥pw) = 0; however, the converse can not be said in general unless p0 is supported
over all X, as stated in the following theorem:
Theorem 7 (Theorem 2 in HyvÂ¨arinen, 2005) Assume p0(x) > 0 âˆ€x âˆˆX. Then:
DF (p0âˆ¥pw) = 0 â‡â‡’p0(x) = pw(x) âˆ€x âˆˆX.
We prove Theorem 7 in Appendix C.2. If p0(x) is zero for some x âˆˆX, there could be a
distribution pw such that DF (p0âˆ¥pw) = 0 even if pw Ì¸= p0 (as discussed in Appendix C.6).
Sliced Score Matching (SSM)
In the explicit Fisher Divergence (Eq. 5), the second
derivatives of the log-density with respect to all components of x are required. When using
the neural conditional exponential family (Eq. 3), this amounts to evaluating the second
derivatives of fw with respect to its input (see Appendix C.10). Practically, automatic
diï¬€erentiation libraries to obtain the derivatives eï¬€ortlessly; however, the computational
cost of the second derivatives is substantial, as it requires a number of forward and backward
passes proportional to the dimension of the data x (see Appendix E.1)6.
Some approaches to reducing the computational burden have been proposed; we review
some in Section 6. Here, we consider Sliced Score Matching (SSM, Song et al. 2020), which
considers projections of the scores on random directions; matching the projections on all
random directions ensures the two distributions are identical (under the same conditions as
the original SM). More precisely, let v âˆˆV âŠ†Rd be a noise vector with a distribution q;
SSM is deï¬ned as follows:
Deï¬nition 8 Sliced Score Matching (SSM) corresponds to ï¬nding:
arg min
w
DFS(p0âˆ¥pw),
where the Sliced Fisher Divergence is deï¬ned as:
DFS(p0âˆ¥pw) = 1
2
Z
V
q(v)
Z
X
p0(x)(vT âˆ‡x log p0(x) âˆ’vT âˆ‡x log pw(x))2dxdv.
(6)
6. Computing the derivatives during the forward pass oï¬€ers some speedup, as automatic diï¬€erentiation re-
peats some computations several times. However, this approach requires custom NN implementation (see
Appendix E.1.1) and is thus not scalable to complex architectures. More importantly, the computational
gain is limited with respect to what is achieved by the Sliced SM version introduced next.
8

Score Matched Neural Exponential Families for LFI
We will require the noise distribution q(v) to satisfy the following Assumption:
A4 For the random vector V âˆ¼q, the covariance matrix E[V V T ] â‰»0 is positive deï¬nite
and Eâˆ¥V âˆ¥2
2 < âˆ.
As for SM, we can obtain an explicit formulation from the implicit one in Eq. (6). This
is done in the following Theorem, which we prove for convenience in Appendix C.3:
Theorem 9 (Theorem 1 in Song et al. (2020)) Let X = Nd
i=1(ai, bi), where ai, bi âˆˆ
R âˆª{Â±âˆ}. If p0(x) is diï¬€erentiable and pw(x) doubly diï¬€erentiable over X, then, under
assumptions A1, A2, A3, A4, the objective in Eq. (6) can be rewritten as:
DFS(p0âˆ¥pw) =
Z
V
q(v)
Z
X
p0(x)

vT (Hx log pw(x))v + 1
2
 vT âˆ‡x log pw(x)
2
dxdv + C, (7)
where Hx log pw(x) denotes the Hessian matrix of log pw(x) with respect to components of x
and C is a constant w.r.t. pw.
Assumption A4 is satisï¬ed by, among others, Gaussian and Rademacher random vari-
ables (Song et al., 2020). Additionally, these two distributions allow to computes explicitly
the expectation with respect to v of
 vT âˆ‡x log p0(x)
2 in Eq. (7), which leads to:
DFS(p0âˆ¥pw) =
Z
X
p0(x)
Z
V
q(v)

vT (Hx log pw(x))v

dv + 1
2 âˆ¥âˆ‡x log pw(x)âˆ¥2
2

dx+C; (8)
in Song et al. (2020), the Monte Carlo estimate of the latter expression is found to perform
better than the one for Eq. (7); across this work, we will therefore consider Eq. (8) with the
Rademacher noise when using SSM.
Analogously to SM, a non-negative p0 ensures that the sliced Fisher divergence is zero
if and only if pw = p0:
Theorem 10 (Lemma 1 in Song et al. (2020)) Assume Assumption A4 holds and that
p0(x) > 0 âˆ€x âˆˆX. Then:
DFS(p0âˆ¥pw) = 0 â‡â‡’p0(x) = pw(x) âˆ€x âˆˆX.
The above result is proven in Appendix C.4.
SSM has a reduced computational cost with respect to SM when automatic diï¬€eren-
tiation is used. In fact, computing the second derivatives in Eq. (5) requires d backward
propagations, while the quadratic form involving the Hessian in Eq. (8) only requires two
backward propagations independently on the dimension of x (see Appendix E.1).
SM and SSM over transformed domain.
If the domain X is unbounded (ai = âˆ’âˆ,
bi = +âˆâˆ€i), A1 is usually satisï¬ed. Instead, it is easy for that assumption to be violated
if X is bounded: for instance, if p0(x) converges to a constant at the boundary of X, A1
requires âˆ‡x log pw(x) to go to 0. Further, if p0(x) diverges, âˆ‡x log pw(x) has to converge to
0 faster than some rate, which is in general a strong requirement.
To apply SM to distributions with bounded domain X under looser conditions, we then
transform X to Y = Rd with a suitable bijection mapping t; this creates a new random
variable Y = t(X), with distributions pY
0 (y) and pY
w(y) induced by p0 and pw on X.
9

Pacchiardi and Dutta
Deï¬nition 11 We deï¬ne respectively as Transformed Score Matching (TranSM) and Trans-
formed Sliced Score Matching (TranSSM) the procedures:
arg min
w
DF (pY
0 âˆ¥pY
w)
and
arg min
w
DFS(pY
0 âˆ¥pY
w).
TranSM and TranSSM enjoy similar properties as SM and SSM, as stated in the following
Theorem, which mirrors Theorem 7 and Theorem 10:
Theorem 12 Let Y = t(X) âˆˆY for a bijection t. Assume Assumption A4 is satisï¬ed,
p0(x) > 0 âˆ€x âˆˆX, and let D denote either DF or DFS; then:
D(pY
0 âˆ¥pY
w) = 0 â‡â‡’pw(x) = p0(x) âˆ€x âˆˆX.
We prove this Theorem (in Appendix C.5) by relying on the equivalence between distribu-
tions for the random variables Y and X.
Motivated by Theorem 12, across this work we will apply TranSM and TranSSM when-
ever X is bounded; precisely, we adopt the same bijections as in the Stan package for
statistical modeling (Appendix C.8, Carpenter et al. 2017).
Remark 13 Another way to extend SM to distributions with bounded support involves mul-
tiplying the scores in the implicit Fisher divergence by a correction factor that allows to
obtain an explicit for under looser assumptions. This Corrected SM (CorrSM) approach
was ï¬rst proposed for non-negative random variables in HyvÂ¨arinen (2007), which also re-
marked how CorrSM and TranSM are equivalent (Appendix C.7). TranSM is however more
practically viable, as a single SM implementation is needed, while CorrSM requires separate
implementations for diï¬€erent kinds of domains. Additionally, the transformations can be
straightforwardly applied to SSM, while (to the best of our knowledge) a correction approach
for SSM has not yet been proposed.
Remark 14 Across this work we restrict to domains X deï¬ned by independent constraints
across the coordinates, i.e. X = Nd
i=1(ai, bi). However, TranSM, TranSSM and CorrSM
can be potentially applied to distributions with more general supports. We brieï¬‚y review this
and other extensions in Appendix C.9.
3.3 Score Matching for conditional densities
We now go back to conditional densities p0(Â·|Î¸) and deï¬ne an expected Fisher divergence
(Arbel and Gretton, 2018) by considering Î¸ âˆ¼Ï€(Î¸):
DE
F (p0âˆ¥pw) =
Z
Î˜
Ï€(Î¸)DF (p0(Â·|Î¸)âˆ¥pw(Â·|Î¸))dÎ¸
= 1
2
Z
Î˜
Z
X
p0(x|Î¸)Ï€(Î¸)âˆ¥âˆ‡x log p0(x|Î¸) âˆ’âˆ‡x log pw(x|Î¸)âˆ¥2dxdÎ¸.
(9)
Analogously, we deï¬ne an expected sliced Fisher divergence DE
FS(p0âˆ¥pw) from DFS.
Note that DE
F (p0âˆ¥pw) â‰¥0; moreover, under the assumption that p0(x|Î¸) is supported
on the whole domain X âˆ€Î¸, the above objective is equal to 0 if and only if p0(Â·|Î¸) = pw(Â·|Î¸)
10

Score Matched Neural Exponential Families for LFI
Ï€(Î¸)-almost everywhere (Arbel and Gretton, 2018). In fact, DF (p0(Â·|Î¸)âˆ¥pw(Â·|Î¸)) â‰¥0 âˆ€Î¸,
with equality holding if and only if the two conditional distributions are the same, as long
as they are both supported on the whole X (by Theorem 7). Exploiting Theorem 10, a
similar result can be obtained for DE
FS.
Requiring p0(x|Î¸) > 0 âˆ€x âˆˆX, âˆ€Î¸ âˆˆÎ˜ means that the model is capable of generating
all possible values of x âˆˆX with non-zero probability for all Î¸ âˆˆÎ˜; otherwise, there may
be distributions pw Ì¸= p0 minimizing the objective (see Appendix C.6).
Analogously to Eqs. (5) and (7), we can obtain explicit formulations7of DE
F and DE
FS:
DE
F (p0âˆ¥pw) =
Z
Î˜
Z
X
p0(x|Î¸)Ï€(Î¸)
d
X
i=1
"
1
2
âˆ‚log pw(x|Î¸)
âˆ‚xi
2
+
âˆ‚2 log pw(x|Î¸)
âˆ‚x2
i
#
dxdÎ¸
|
{z
}
J(w)
+C,
DE
FS(p0âˆ¥pw) =
Z
V
Z
Î˜
Z
X
q(v)p0(x|Î¸)Ï€(Î¸)

vT (Hx log pw(x|Î¸))v + 1
2 âˆ¥âˆ‡x log pw(x|Î¸)âˆ¥2
2

dxdÎ¸dv
|
{z
}
JS(w)
+C,
(10)
where C denotes constants with respect to w. For these two expressions, it is immediate to
obtain Monte Carlo estimates using samples (Î¸(j), xj)N
j=1, Î¸(j) âˆ¼Ï€ and x(j) âˆ¼p(Â·|Î¸(j)), and
draws from the noise model {v(j,k)}1â‰¤jâ‰¤N,1â‰¤kâ‰¤M :
Ë†J(w) = 1
N
N
X
j=1
ï£®
ï£°
d
X
i=1
ï£«
ï£­1
2
 
âˆ‚log pw(x(j)|Î¸(j))
âˆ‚xi
!2
+
 
âˆ‚2 log pw(x(j)|Î¸(j))
âˆ‚x2
i
!ï£¶
ï£¸
ï£¹
ï£»,
Ë†JS(w) =
1
NM
N
X
j=1
M
X
k=1

v(j,k),T (Hx log pw(x(j)|Î¸(j)))v(j,k) + 1
2
âˆ‡x log pw(x(j)|Î¸(j))

2
2

.
(11)
As we discussed before, computing the square bracket term in Ë†JS(w) only requires two
backward propagations, while the square bracket term in Ë†J(w) requires d. However, Ë†JS(w)
sums over M Â· N terms, while Ë†J(w) sums over N. Nevertheless, when using Ë†JS(w) to train
a neural network, good results can be obtained by using a single diï¬€erent noise realization
for each training sample (Î¸(j), xj) at each epoch, thus leading to smaller computational cost
with respect to Ë†J(w) (Song et al., 2020).
7. Requiring p0(x|Î¸) to be diï¬€erentiable and pw(x|Î¸) doubly diï¬€erentiable over X for all Î¸ and:
A1. p0(x|Î¸)
âˆ‚
âˆ‚xi log pw(x|Î¸) â†’0 for xi â†˜ai and xi â†—bi, âˆ€w, i, Î¸,
A2. Ep0[âˆ¥âˆ‡x log p0(X|Î¸)âˆ¥2] < âˆ, Ep0[âˆ¥âˆ‡x log pw(X|Î¸)âˆ¥2] < âˆ, âˆ€w, Î¸,
A3. Ep0

âˆ‚2
âˆ‚xiâˆ‚xj log pw(X|Î¸)
 < âˆ, âˆ€w, Î¸, âˆ€i, j = 1, . . . , d.
.
11

Pacchiardi and Dutta
Score Matching for conditional exponential family.
Both the implicit and explicit
versions DE
F (p0âˆ¥pw) and DE
FS(p0âˆ¥pw) are well deï¬ned only if pw is a proper density, which
requires Zw(Î¸) < âˆ. In practice, we are interested in:
Ë†w = arg min Ë†J(w)
or
Ë†w = arg min Ë†JS(w).
(12)
When we compute Ë†J(w) or Ë†JS(w), we only need to evaluate fw in a ï¬nite set of points
{x(j)}N
j=1 which belong to a bounded subset of X, say A âŠ‚X. We can thus redeï¬ne the
approximating family as follows:
pâ€²
w(x|Î¸) = Ëœpâ€²
w(x|Î¸)
Zâ€²w(Î¸) ,
Ëœpâ€²
w(x|Î¸) =
(
exp(Î·w(Î¸)T fw(x))
if x âˆˆA,
gw(x, Î¸)
otherwise,
where gw is such that Zâ€²
w(Î¸) < âˆ, and can always be chosen such that Ëœpâ€²
w(x|Î¸) is a smooth
and continuous function of x.
Replacing pw in Eq. (11) with pâ€²
w does not change the value of Ë†J(w) and Ë†JS(w); however,
as pâ€²
w is normalized, Eqs. (9) and (10) are now well deï¬ned for all w. Additionally, we do
not need to specify A or gw explicitly as we never evaluate the normalizing constant Zâ€²
w(Î¸)
(which depends on them). In the following we will thus use interchangeably pw and pâ€²
w.
Remark 15 (Notation) For notational simplicity, in the rest of the work we drop the hat
in Ë†w, denoting by pw the approximation obtained by one of the strategies in Eq. (12), and
by fw and Î·w the corresponding suï¬ƒcient statistics and natural parameters networks.
4. Inference using the likelihood approximation
By ï¬tting the neural conditional exponential family (Eq. 3) with SM or SSM to parameter-
simulation pairs generated from the model, we obtain an approximation of the likelihood up
to the normalization constant Zw(Î¸): for each ï¬xed Î¸, the function x 7â†’exp(Î·w(Î¸)T fw(x))
is approximately proportional to p0(x|Î¸). We exploit pw in two ways: ï¬rst, by using fw
as summaries in ABC; secondly, using the full approximation to draw samples from the
posterior with MCMC for doubly intractable distributions. Both approaches are illustrated
in Figure 1 and discussed next.
4.1 ABC with neural conditional exponential family statistics
The exponential family is the most general set of distributions with suï¬ƒcient statistics of
a given size (see Appendix A). Using fw as summaries in ABC is therefore intuitively ap-
pealing: fw represents in fact the suï¬ƒcient statistics of the best exponential family approx-
imation to p0, according to the (sliced) Fisher divergence. If p0 belongs to the exponential
family, DE
F (p0âˆ¥pw) = DE
FS(p0âˆ¥pw) = 0 if fw and Î·w are suï¬ƒcient statistics and natural
parameters for p0.
To use fw in ABC, some practicalities are needed: ï¬rst, we discard the last component
of fw, which represents the base measure hw(x). Secondly, as discussed in Section 3.1, fw is
identiï¬able only up to a scale parameter, so that the magnitude of the diï¬€erent components
of fw can vary signiï¬cantly. Before using ABC, then, we rescale the diï¬€erent components
12

Score Matched Neural Exponential Families for LFI
Model 
ABC
ExchangeMCMC
(Sliced) Score
Matching
Neural exponential 
family
Simulate from the model
Simulate
from the
model
ABC posterior
samples
ExchangeMCMC 
posterior samples
Training samples
Training samples
Figure 1: Diagram showing the roles of the diï¬€erent components in the inference
routines. SSM or SM are used to train the neural exponential family on model simulations.
Then, the suï¬ƒcient statistics fw are used in ABC or, alternatively, the neural exponential
family is inserted in ExchangeMCMC. Notice how additional model simulation are needed
for ABC but not for ExchangeMCMC.
by their standard deviation on new samples generated from the model, to prevent the ones
with larger magnitude from dominating the ABC distance.
In the following, we will call the approach described here ABC with Score Matching
statistics, for short ABC-SM, or ABC-SSM in the Sliced Score Matching case.
4.2 ExchangeMCMC with neural conditional exponential family
In contrast to other statistics learning methods (Jiang et al., 2017; Wiqvist et al., 2019;
Pacchiardi et al., 2020; ËšAkesson et al., 2020), our technique provides a full likelihood ap-
proximation; it is therefore tempting to sample from the corresponding posterior directly,
bypassing in this way ABC (and the additional model simulations required) altogether.
Unfortunately, pw is known only up to the normalizing constant Zw(Î¸); therefore, stan-
dard MCMC cannot be directly exploited , and methods for doubly-intractable distributions
are required (see Park and Haran, 2018 for a review). Here, we use ExchangeMCMC (Mur-
ray et al. (2012), Algorithm 1 in Appendix E.4), an MCMC where, for each proposed Î¸â€²,
a simulation from the distribution pw(x|Î¸â€²) is used within a Metropolis-Hastings rejection
step.
In our case, we cannot generate samples from pw(Â·|Î¸â€²) directly; to circumvent such issue,
Murray et al. (2012) suggested to run an MCMC targeting pw(Â·|Î¸â€²) for each ExchangeMCMC
step; if the chain is long enough, the last step can be considered as a draw from pw. Empirical
results (Caimo and Friel, 2011; Alquier et al., 2016; Everitt, 2012; Liang, 2010) show that
a relatively small number of inner MCMC steps are enough for good performance and that
initializing the inner chain at the observation improves convergence; we employ therefore
this strategy in our work.
Further, a variant of ExchangeMCMC employs Annealed Importance Sampling to im-
prove the mixing of the chain (Murray et al., 2012). Speciï¬cally, a sequence of K auxiliary
variables are drawn from Metropolis-Hastings kernels targeting some intermediate distri-
butions, and the ExchangeMCMC acceptance rate is modiï¬ed accordingly. This approach,
termed bridging, decreases the number of rejections in ExchangeMCMC due to poor auxil-
iary variables. See Appendix E.4 for more details.
13

Pacchiardi and Dutta
In the following, we will refer to using ExchangeMCMC with our likelihood approxi-
mation as Exc-SM or Exc-SSM, according to whether SM or SSM are used to obtain the
likelihood approximation. Exc-SM and Exc-SSM avoid additional model simulations (be-
yond the ones required to train pw) at the cost of running a nested MCMC. However, the
number of steps in the inner MCMC required to obtain good performance increases with
the dimension of X. Exc-SM and Exc-SSM are therefore ideally applied to models with
moderate dimension x (up to a few hundred), for which simulation is expensive. The same
likelihood approximation can be used to perform inference on several observations, which
makes the computational gain even greater in this case.
Remark 16 (Model misspeciï¬cation) The neural exponential family approximation is
only valid close to where training data was distributed. Speciï¬cally, if x0 is far from that
region, p Ë†w(x0|Î¸) may be assigned a large value rather than a (correct) small one. This could
happen when the model p0 is unable to generate data resembling the observation for any
parameter value, such that standard Bayesian inference in presence of the likelihood would
also perform poorly. To get better inference in such scenarios, we could use the generalized
posterior introduced in Matsubara et al. (2021), which is robust to outliers and suitable for
doubly-intractable distributions (as the exponential family).
In Exc-SM and Exc-SSM, we may wonder whether running MCMC over x targeting
pw(x|Î¸) for a ï¬xed Î¸ can fail due to what we discussed above. We believe this is not the
case: if the MCMC is initialized close to training data and pw(Â·|Î¸) is a good representation
of p0(Â·|Î¸) in that region, pw(Â·|Î¸) is small for values of x close to the boundary of the region
where training data was distributed. Then, the MCMC is â€œtrappedâ€ inside that region and
has no way of reaching regions of X where pw may behave irregularly.
5. Simulation Studies
We perform simulation studies with our proposed approaches (Exc-SM, Exc-SSM, ABC-SM
and ABC-SSM) and we compare with:
â€¢ ABC with statistics learned with the rejection approach discussed in Section 2.1
(Fearnhead and Prangle, 2012; Jiang et al., 2017), which we term ABC-FP.
â€¢ Population Monte Carlo (PMC, CappÂ´e et al. 2004) with Ratio Estimation (PMC-RE).
â€¢ PMC with Synthetic Likelihood, using the robust covariance matrix estimator devel-
oped in Ledoit and Wolf (2004) to estimate Î£Î¸. We will denote this as PMC-SL.
Speciï¬cally, we consider three exponential family models and two time-series models
(AR(2) and MA(2)) for which the exact likelihood is available, as well as a large-dimensional
model with unknown likelihood (the Lorenz96 model, Lorenz 1996; Wilks 2005).
Exc-SM and Exc-SSM do not require additional simulations and run an MCMC, in con-
trast to sequential algorithms for the other methods, which we run with parallel computing.
Comparing the computational cost is therefore not easy; in Appendix G.3, we report the
number of simulations needed by the diï¬€erent methods to reach the same performance
achieved by Exc-SM for the models with known likelihood.
14

Score Matched Neural Exponential Families for LFI
Choice of neural network architecture
In our exponential family approximation, we
ï¬x ds to the number of parameters in each model. We added a Batch Normalization layer
(see Appendix E.2) on top of the neural network representing Î·w to reduce the unidentiï¬abil-
ity introduced by the dot product between fw(x) and Î·w(Î¸) (as discussed in Appendix C.10).
For all techniques, we use 104 training samples. All NNs use SoftPlus nonlinearity (NNs
using the more common ReLU nonlinearity cannot be used with SM and SSM as they have
null second derivative with respect to the input).
For all models, Î·w is represented by fully connected NNs. For the exponential family
models, fully connected NNs are also used for fw and sÎ². For the time series and Lorenz96
models, we parametrize fw and sÎ² with Partially Exchangeable Networks (PENs, Wiqvist
et al. 2019). The output of an r-PEN is invariant to input permutations which do not
change the probability density of data distributed according to a Markovian model of order
r (see Appendix E.3). As AR(2) is a 2-Markovian model, we use a 2-PEN; similarly, a
1-PEN is used for the Lorenz96 model, which is 1-Markovian. Finally, we use a 10-PEN
for the MA(2) model; albeit not being Markovian, Wiqvist et al. (2019) argued that MA(2)
can be considered as â€œalmostâ€ Markovian so that the loss of information in imposing a PEN
structure of high enough order is negligible. Further details are given in Appendix F.
Choice of inferential parameters.
For ABC, we employ Simulated annealing ABC
(SABC, Albert et al. 2015), which considers a set of particles and updates their position
in parameter space across several iterations.
We use 100 iterations and 1000 particles
(posterior samples), corresponding to 1000 model simulations per iteration.
In Exc-SM and Exc-SSM, we run Exchange MCMC for 20000 steps, of which the ï¬rst
10000 are burned-in. During burn-in, at intervals of 100 outer steps, we tune the proposal
sizes according to the acceptance rate in the previous 100 steps. Our implementation of
the Exchange algorithm is detailed in Appendix E.4.1.
For the exponential family and
time-series models, we test diï¬€erent numbers of inner MCMC steps, and eventually use 30
for the former and 100 for the latter, above which there was no noticeable performance
improvement (more details in Appendix G.1.3 and Appendix G.2).
In PMC-SL and PMC-RE, we run the PMC algorithm with 10 iterations, 1000 posterior
samples and respectively 100 (with SL) and 1000 (with RE) simulations per parameter
value in order to estimate the approximate likelihood; such a large number of simulations
(respectively 105 and 106 for each iteration) is required for the likelihood estimate to be
numerically stable. For the exponential family models, we use the true suï¬ƒcient statistics;
for AR(2) and MA(2), we instead use autocovariances with lag 1 and 2 (as for instance in
Marin et al. 2012). For PMC-RE, the cross-product of the statistics is also added to the
list of statistics, as PMC-SL implicitly uses it.
5.1 Exponential family models
First, we consider three models for which a sample is deï¬ned as a 10-dimensional inde-
pendent and identical distributed (i.i.d.) draw from either a Gaussian, Gamma or Beta
distribution (all belonging to the exponential family). We put uniform priors on the param-
eters, with bounds given in Table 1. These models have diï¬€erent data ranges: unbounded
for Gaussian, and respectively lower bounded by 0 and bounded in [0, 1] for Gamma and
15

Pacchiardi and Dutta
Model
Beta
Gamma8
Gaussian
AR(2)
MA(2)
Lorenz96
Parameter
Î±
Î²
k
Î¸
Âµ
Ïƒ
Î¸1
Î¸2
Î¸1
Î¸2
b0
b1
Ïƒe
Ï†
Lower bound
1
1
1
1
âˆ’10
1
âˆ’1
âˆ’1
âˆ’1
0
1.4
0
1.5
0
Upper bound
3
3
3
3
10
10
1
0
1
1
2.2
1
2.5
1
Table 1: Bounds of uniform priors for the considered models.
Beta. Therefore, we directly apply SM and SSM to the Gaussian model, while TranSM and
TranSSM are applied to Gamma and Beta.
Inferred suï¬ƒcient statistics and natural parametrization.
With these models, our
exponential family approximation is well speciï¬ed. Thus, we compare the learned fw and
Î·w with the exact suï¬ƒcient statistics and natural parameters using the Mean Correlation
Coeï¬ƒcient (MCC, Appendix B.1.1) metric, which ranges in [0, 1] and measures how well
a multivariate function is recovered. We report results obtained with SM in Table 2; the
values are close to 1, indicating that the suï¬ƒcient statistics and natural parameters are
recovered quite well by our method. In Appendix G.1, we report values corresponding to
SSM (Table 9), as well as comparisons of exact and learned embeddings in Figures 9 and
10.
Model
MCC weak in
MCC weak out
MCC strong in
MCC strong out
Beta (statistics)
0.964
0.958
0.723
0.723
Beta (nat. par.)
0.990
0.991
0.807
0.812
Gamma (statistics)
0.911
0.924
0.894
0.883
Gamma (nat. par.)
0.967
0.967
0.872
0.873
Gaussian (statistics)
0.944
0.937
0.808
0.824
Gaussian (nat. par.)
0.974
0.974
0.970
0.972
Table 2: MCC for exponential family models between exact embeddings and
those learned with SM. We show weak and strong MCC values; MCC is between 0 and
1 and measures how well an embedding is recovered up to permutation and rescaling of its
components (strong) or linear transformation (weak); the larger, the better. â€œinâ€ denotes
MCC on training data used to ï¬nd the best transformation, while â€œoutâ€ denote MCC on
test data. We used 500 samples in both training and test data sets.
Inferred posterior distribution.
Figure 2 shows posteriors obtained with the proposed
methods, for a possible observation for each model; we see that all approximate posteriors
are remarkably close to the exact one; moreover, the results with SM and SSM are indis-
tinguishable. For all methods, we also estimate the Wasserstein distance between true and
approximate posterior and compute the Root Mean Squared Error (RMSE) between mean
of the true and approximate posterior; this is repeated for 100 simulated observations, with
8. The scale parameter of the Gamma distribution is usually called Î¸; in contrast, in all the rest of this
work we use Î¸ to denote all parameters. Please beware of the diï¬€erence.
16

Score Matched Neural Exponential Families for LFI
1.0
1.5
2.0
2.5
3.0
1.0
1.5
2.0
2.5
3.0
True posterior
Posterior mean
True value
1.0
1.5
2.0
2.5
3.0
1.0
1.5
2.0
2.5
3.0
ABC-SM
Posterior mean
True value
1.0
1.5
2.0
2.5
3.0
1.0
1.5
2.0
2.5
3.0
ABC-SSM
Posterior mean
True value
1.0
1.5
2.0
2.5
3.0
1.0
1.5
2.0
2.5
3.0
Exc-SM
Posterior mean
True value
1.0
1.5
2.0
2.5
3.0
1.0
1.5
2.0
2.5
3.0
Exc-SSM
Posterior mean
True value
(a) Beta
1.0
1.5
2.0
2.5
3.0
k
1.0
1.5
2.0
2.5
3.0
True posterior
Posterior mean
True value
1.0
1.5
2.0
2.5
3.0
k
1.0
1.5
2.0
2.5
3.0
ABC-SM
Posterior mean
True value
1.0
1.5
2.0
2.5
3.0
k
1.0
1.5
2.0
2.5
3.0
ABC-SSM
Posterior mean
True value
1.0
1.5
2.0
2.5
3.0
k
1.0
1.5
2.0
2.5
3.0
Exc-SM
Posterior mean
True value
1.0
1.5
2.0
2.5
3.0
k
1.0
1.5
2.0
2.5
3.0
Exc-SSM
Posterior mean
True value
(b) Gamma
10
5
0
5
10
1
4
7
10
True posterior
Posterior mean
True value
10
5
0
5
10
1
4
7
10
ABC-SM
Posterior mean
True value
10
5
0
5
10
1
4
7
10
ABC-SSM
Posterior mean
True value
10
5
0
5
10
1
4
7
10
Exc-SM
Posterior mean
True value
10
5
0
5
10
1
4
7
10
Exc-SSM
Posterior mean
True value
(c) Gaussian
Figure 2: True and approximate posteriors for exponential family models, for a
single observation per model. Dashed line represents posterior mean, while green solid line
represents the exact parameter value.
results reported in Figure 3. ABC-FP is here the worst method; additionally, ABC-SM and
ABC-SSM perform similarly to PMC-SL and PMC-RE. Finally, Exc-SM and Exc-SSM are
marginally worse.
5.2 Time series models
The Moving Average model of order 2, or MA(2), and the AutoRegressive model of order
2, or AR(2), are special cases of the ARMA time-series model. The MA(2) model is deï¬ned
by:
X1 = Î¾1,
X2 = Î¾2 + Î¸1Î¾1,
Xj = Î¾j + Î¸1Î¾jâˆ’1 + Î¸2Î¾jâˆ’2,
j = 3, . . . , t,
while the AR(2) is deï¬ned as:
X1 = Î¾1,
X2 = Î¾2 + Î¸1X1,
Xj = Î¾j + Î¸1Xjâˆ’1 + Î¸2Xjâˆ’2,
j = 3, . . . , t;
in both, Î¾jâ€™s are i.i.d. standard normal error terms (unobserved). We take here t = 100
and we put uniform priors on the parameters of the two models, with bounds given in
17

Pacchiardi and Dutta
PMC-SL
PMC-RE
ABC-FP
ABC-SM
ABC-SSM
Exc-SM
Exc-SSM
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Wasserstein distance
Beta
0.0
0.2
0.4
0.6
0.8
1.0
RMSE posterior mean
PMC-SL
PMC-RE
ABC-FP
ABC-SM
ABC-SSM
Exc-SM
Exc-SSM
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Wasserstein distance
Gamma
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
RMSE posterior mean
PMC-SL
PMC-RE
ABC-FP
ABC-SM
ABC-SSM
Exc-SM
Exc-SSM
0
2
4
6
8
10
Wasserstein distance
Gaussian
0
2
4
6
8
10
RMSE posterior mean
Figure 3: Performance of the diï¬€erent techniques for exponential family models.
Wasserstein distance from the exact posterior and RMSE between exact and approximate
posterior means are reported for 100 observations using boxplots. Boxes span from 1st
to 3rd quartile, whiskers span 95% probability density region and horizontal line denotes
median. The numerical values are not comparable across examples, as they depend on the
range of parameters. Here, SL and RE used the true suï¬ƒcient statistics.
Table 1. For these models, the true likelihood can be evaluated, but they do not belong to
the exponential family9.
Inferred posterior distribution.
Figure 4 shows the posterior obtained with our pro-
posed methods, for a possible observation for each model; again, our approximations are
close to the exact posterior, with Exc-SM and Exc-SSM leading to slightly broader poste-
riors. Again, we assess performance with the Wasserstein distance from the true posterior
and the RMSE between the means of true and approximate posterior for all methods, over
100 fresh observations. The results are reported in Figure 5; here, ABC-FP is the best
method, with ABC-SM and ABC-SSM marginally worse. Exc-SM and Exc-SSM follow and
perform better or comparably to PMC-RE and PMC-SL.
5.3 Lorenz96 meteorological model
The Lorenz96 model (Lorenz, 1996) is a toy model of chaotic atmospheric behaviour, in-
cluding interacting slow and fast variables. We use here a modiï¬ed version (Wilks, 2005)
where the eï¬€ect of the fast variables on the slow ones is replaced by a stochastic eï¬€ective
term depending on a set of parameters. Speciï¬cally, this model is deï¬ned by the following
coupled Diï¬€erential Equations (DEs):
dyk
dt = âˆ’ykâˆ’1(ykâˆ’2 âˆ’yk+1) âˆ’yk + 10 âˆ’g(yk, t; Î¸);
k = 1, . . . , K,
where yk(t) is the value at time t of the k-th variable and indices are cyclic (index K + 1
corresponds to 1, and so on). The eï¬€ective term g depends on Î¸ = (b0, b1, Ïƒe, Ï†), and is
9. More precisely, they cannot be written as an exponential family with embedding dimension ï¬xed with
data size; in fact, MA(2) can be written as a Gaussian distribution with t Ã— t covariance matrix, which
is an exponential family whose embedding dimension increases with data size.
18

Score Matched Neural Exponential Families for LFI
1.0
0.5
0.0
0.5
1.0
1
1.0
0.8
0.6
0.4
0.2
0.0
2
True posterior
Posterior mean
True value
1.0
0.5
0.0
0.5
1.0
1
1.0
0.8
0.6
0.4
0.2
0.0
2
ABC-SM
Posterior mean
True value
1.0
0.5
0.0
0.5
1.0
1
1.0
0.8
0.6
0.4
0.2
0.0
2
ABC-SSM
Posterior mean
True value
1.0
0.5
0.0
0.5
1.0
1
1.0
0.8
0.6
0.4
0.2
0.0
2
Exc-SM
Posterior mean
True value
1.0
0.5
0.0
0.5
1.0
1
1.0
0.8
0.6
0.4
0.2
0.0
2
Exc-SSM
Posterior mean
True value
(a) AR(2)
1.0
0.5
0.0
0.5
1.0
1
0.0
0.2
0.4
0.6
0.8
1.0
2
True posterior
Posterior mean
True value
1.0
0.5
0.0
0.5
1.0
1
0.0
0.2
0.4
0.6
0.8
1.0
2
ABC-SM
Posterior mean
True value
1.0
0.5
0.0
0.5
1.0
1
0.0
0.2
0.4
0.6
0.8
1.0
2
ABC-SSM
Posterior mean
True value
1.0
0.5
0.0
0.5
1.0
1
0.0
0.2
0.4
0.6
0.8
1.0
2
Exc-SM
Posterior mean
True value
1.0
0.5
0.0
0.5
1.0
1
0.0
0.2
0.4
0.6
0.8
1.0
2
Exc-SSM
Posterior mean
True value
(b) MA(2)
Figure 4: True and approximate posteriors for AR(2) and MA(2) models, for a
single observation per model. Dashed line represents posterior mean, while green solid line
represents the exact parameter value.
PMC-SL
PMC-RE
ABC-FP
ABC-SM
ABC-SSM
Exc-SM
Exc-SSM
0.0
0.2
0.4
0.6
0.8
Wasserstein distance
AR(2)
0.0
0.2
0.4
0.6
0.8
RMSE posterior mean
PMC-SL
PMC-RE
ABC-FP
ABC-SM
ABC-SSM
Exc-SM
Exc-SSM
0.0
0.2
0.4
0.6
0.8
Wasserstein distance
MA(2)
0.0
0.2
0.4
0.6
0.8
RMSE posterior mean
Figure 5: Performance of the diï¬€erent techniques for AR(2) and MA(2) models.
Wasserstein distance from the exact posterior and RMSE between exact and approximate
posterior means are reported for 100 observations using boxplots. Boxes span from 1st
to 3rd quartile, whiskers span 95% probability density region and horizontal line denotes
median.
deï¬ned upon discretizing the DEs with a timestep âˆ†t:
g(y, t; Î¸) =
linear deterministic term
z
}|
{
b0 + b1y
+
zero mean AR(1)
z
}|
{
Ï† Â· r(t âˆ’âˆ†t) + Ïƒe(1 âˆ’Ï†2)1/2Î·(t),
where Î·(t) âˆ¼N(0, 1). We numerically integrate the model using 4th order Runge-Kutta
method with âˆ†t = 1/30 in an interval [0, T]; further, we ï¬x the initial condition to a value
y(0) which is independent on parameters. Here, the true likelihood is unaccessible, so that
sampling from the exact posterior is impossible.
19

Pacchiardi and Dutta
We consider the model in a small and large conï¬guration: in the small one, we take
K = 8 and T = 1.5, which lead to 45 timesteps and a data dimension of d = 45 Â· 8 = 360.
In the large one, we take instead K = 40 and T = 4, corresponding to 120 timesteps and a
data dimension of d = 120 Â· 40 = 4800.
Inferred posterior distribution.
For both conï¬gurations, we perform inference with
ABC-SSM and ABC-FP; additionally, we use Exc-SSM for the small conï¬guration (as
running the exchange algorithm in the large one is too costly). In ExchangeMCMC, we
used 500 inner MCMC steps and 200 bridging steps for each outer step. Figure 6 reports
posteriors for a single observation in both setups. The ABC-FP posterior is narrower than
the one for ABC-SSM, but concentrated around similar parameter values. The posterior for
Exc-SSM looks slightly diï¬€erent: it concentrates on similar parameter values as the other
two for Î¸1 and Î¸2, but is broader for Ïƒe and Ï†.
Posterior predictive validation.
We assess the performance of the posterior predictive
distribution p(x|x0) =
R
p(x|Î¸)Ï€(Î¸|x0)dÎ¸, in which Ï€(Î¸|y) is the posterior obtained with one
of the considered techniques. Speciï¬cally, we use the Kernel and Energy Scoring Rules (SRs)
to evaluate how well the predictive p(x|x0) predicts the observation x0. A SR (Gneiting
and Raftery, 2007) is a function of a distribution and an observation, and assesses the
mismatch between them (the smaller, the better). More details on Scoring Rules are given
in Appendix D, together with the deï¬nition of those used here.
As the Lorenz96 model returns a (multivariate) time-series, we compare predictive dis-
tribution and observation at each timestep independently. We repeat this validation with
100 diï¬€erent observations and corresponding posteriors, for all techniques and both model
conï¬gurations. We obtain cumulative scores by summing the scores over timesteps; median
and some quantile values over the 100 observations are shown in Fig. 7. In both model
conï¬gurations, the posterior predictive generated by ABC-SSM and Exc-SSM marginally
outperform ABC-FP, according to both SRs. Score values at each timestep, together with
more details on the validation technique, are reported in Appendix G.4.
6. Related works
ABC statistics with NN.
Using NNs for learning statistics for ABC has been previously
suggested, ï¬rst with the regression approach discussed in Sec. 2.1 (used in Jiang et al. 2017;
Wiqvist et al. 2019; ËšAkesson et al. 2020). In Pacchiardi et al. (2020), a NN is trained so
that the distance between pairs of simulated statistics best reproduces the distance between
the corresponding parameter values. Chen et al. (2021) instead ï¬t a NN by maximizing an
estimate of the mutual information between statistics and parameters.
ABC statistics from auxiliary models.
Some similarities with our work can be found
in Gleim and Pigorsch (2013) and Ruli et al. (2016), which consider respectively an auxiliary
model and a composite likelihood alongside the simulator model and obtain summary statis-
tics from them; similarly, our approach can be seen as building an auxiliary exponential
family model with easily accessible summary statistics.
LFI with NNs.
The idea of sampling directly from the approximate posterior deï¬ned by
pw is related to a suite of LFI methods using NNs. A large part of these works use normaliz-
20

Score Matched Neural Exponential Families for LFI
1
2
e
1
2
e
ABC-FP
1
2
e
1
2
e
ABC-SSM
1
2
e
1
2
e
Exc-SSM
(a) Lorenz96 - Small conï¬guration
1
2
e
1
2
e
ABC-FP
1
2
e
1
2
e
ABC-SSM
(b) Lorenz96 - Large conï¬guration
Figure 6: Example of posterior inference for a single Lorenz96 observation with
diï¬€erent approaches, for both small (ï¬rst row) and large (second row) conï¬gurations.
In each panel, the diagonal plots represent the univariate marginal distributions, while the
oï¬€-diagonal ones are bivariate density contour plots. Moreover, the green and red lines
represent respectively exact parameter value and posterior mean. All axes span full prior
range (see Table 1).
ing ï¬‚ows (NNs implementing invertible transformations, suitable for eï¬ƒciently representing
probability densities; see Papamakarios et al., 2021 for a review); speciï¬cally, Papamakarios
et al. (2019); Lueckmann et al. (2019) use them to learn an approximation of the likeli-
hood, while Papamakarios and Murray (2016); Lueckmann et al. (2017); Greenberg et al.
(2019); Radev et al. (2020) learn the posterior. Most of the above approaches focus on
inference for a single observation, tailoring the simulations to better approximate the pos-
terior for the relevant parameter values at lower computational cost; instead, Radev et al.
(2020) and Lueckmann et al. (2019) propose to amortize across observations, similarly to
our approach. Besides normalizing ï¬‚ows, Klein et al. (2020) casts the LFI problem as a
distributional regression one. Finally, Tabak et al. (2020) does not employ NNs but rather
21

Pacchiardi and Dutta
ABC-FP
ABC-SSM
Exc-SSM
8000
9000
10000
11000
Energy Score
Small Lorenz96
100
110
120
130
140
150
160
170
Kernel Score
ABC-FP
ABC-SSM
160000
180000
200000
220000
Energy Score
Large Lorenz96
1000
1500
2000
2500
3000
Kernel Score
Figure 7: Predictive performance of the diï¬€erent methods according to the Ker-
nel and Energy Scoring Rules. Each boxplot represents cumulative (i.e., summed over
the time index) scoring rule value for a given method for the small (left) and large (right)
Lorenz96 conï¬guration. Boxes span from 1st to 3rd quartile, whiskers span 95% probability
density region and horizontal line denotes median.
solves a Wasserstein barycenter problem to model conditional maps which allow sampling
from the distribution of an observation conditional on some covariates.
Fitting unnormalized models.
Several techniques besides Score Matching have been
proposed for ï¬tting unnormalized models: MCMC-MLE (Geyer, 1991) exploits MCMC
to estimate the normalizing constant for diï¬€erent values of the parameter and uses that in
MLE. Contrastive Divergence (CD, Hinton 2002) instead uses MCMC to obtain a stochastic
approximation of the gradient of the log-likelihood; this requires a smaller number of MCMC
steps with respect to MCMC-MLE, but the stochastic gradient estimate is biased. Minimum
Probability Flow (MPF, Sohl-Dickstein et al. 2011) considers a dynamics from data to model
distribution, and minimizes the Kullback Leibler divergence between the data distribution
and the one obtained by running the dynamics for a short time; however, the eï¬ƒcacy
of MPF depends signiï¬cantly on the considered dynamics. Noise Contrastive Estimation
(NCE, Gutmann and HyvÂ¨arinen 2012) converts the parameter estimation problem to ratio
estimation between data distribution and a suitable noise distribution; in practice, NCE uses
logistic regression to discriminate the observed data and data generated from the noise; in
the loss, the normalizing constant appears explicitly and can be estimated independently. To
be eï¬€ective, NCE requires the noise distribution to overlap well with the data density while
being easy to sample from and to evaluate, which is not easy to get. Finally, some works
(Dai et al., 2019a,b) use the dual formulation of MLE to avoid estimating the normalizing
constant at the price of introducing dual variables to be jointly estimated.
Fitting unnormalized conditional models.
The approaches above running dynamics
to estimate quantities (CD, MCMC-MLE, MPF) cannot easily be applied to the conditional
setting. NCE has been instead used in Ton et al. (2021) with a single noise distribution for all
values of the conditioning variable Î¸; it however requires an independent NN to parametrize
22

Score Matched Neural Exponential Families for LFI
the normalizing constant Z(Î¸). SM can be instead easily applied to the conditional setting,
as previously done in Arbel and Gretton (2018) and further demonstrated in our work. To
the best of our knowledge, SSM was not applied to a conditional setting before, although
the extension straightforwardly follows what is done for SM.
Fast approximations of SM.
Besides SSM, some approximations to SM have been in-
vestigated, all of which only require ï¬rst derivatives. For instance, Denoising Score Matching
(Vincent, 2011) computes the Fisher divergence between the model and a Kernel Density
Estimate of the data distribution, which is equal to a quantity independent on the second
derivative. Alternatively, Kernel Stein Discrepancy (Liu and Wang, 2016) intrinsically de-
pends on ï¬rst derivatives only. For both techniques, however, several samples for each Î¸
would be required in the conditional setting. Finally, Wang et al. (2020) exploits a connec-
tion between the Fisher divergence and gradient ï¬‚ows in the 2-Wasserstein space to develop
an approximation that only relies on ï¬rst-order derivatives.
Kernel Conditional Exponential Families (KCEFs).
In KCEFs (Arbel and Gret-
ton, 2018), the summary statistics and natural parameters are functions in a Reproducing
Kernel Hilbert Space, whose properties allow to evaluate the density although an inï¬nite-
dimensional embedding space is used (using the kernel trick). In Arbel and Gretton (2018),
SM was used to ï¬t KCEFs instead of our neural conditional exponential families. They
build on Sriperumbudur et al. (2017), which ï¬rst used SM to perform density estimation
with (non-conditional) Kernel Exponential Families (KEFs). In Wenliang et al. (2019) NNs
are used to parametrize the kernel in a KEF and trained with the SM loss. However, as
KCEFs do not have ï¬nite-dimensional suï¬ƒcient statistics, they are unsuitable for learning
ABC statistics. Additionally, KCEFs have a worse complexity in terms of training dataset
size with respect to NN-based methods. KCEFs have also been used with a dual MLE
objective in Dai et al. (2019a).
7. Conclusions and extensions
We proposed a technique to approximate the likelihood using a neural conditional expo-
nential family, trained via (Sliced) Score Matching to handle the intractable normalizing
constant.
We tested this approximation in two setups: ï¬rst, by using the exponential family
suï¬ƒcient statistics as ABC statistics, which is intuitively appealing as the exponential
family is the largest class of distributions with ï¬xed-size suï¬ƒcient statistics. We empirically
showed this to be comparable or outperform ABC with summaries built via the state-of-
the-art regression approach (Fearnhead and Prangle, 2012; Jiang et al., 2017).
Secondly, we used MCMC for doubly-intractable distributions to sample from the pos-
terior corresponding to the likelihood approximation, which we found to have performance
comparable to the other approaches. This can be repeated for any new observation without
additional model simulations, making it advantageous for expensive simulator models.
Our proposed direct sampling approach based on exponential family likelihood approx-
imation and ExchangeMCMC could be improved as follows:
â€¢ we used ExchangeMCMC (Murray et al., 2012) to handle double intractability, but
other algorithms could be more eï¬ƒcient, (for instance the one in Liang et al. (2016),
23

Pacchiardi and Dutta
which makes use of parallel computing). Alternatively, we could exploit the general-
ized posterior introduced in Matsubara et al. (2021), which allows standard MCMC
to be used for double intractable distributions and is robust to outliers.
â€¢ To infer the posterior for a single observation, approximating the likelihood for all xâ€™s
and Î¸â€™s as we do now is suboptimal. Similar performance may in fact be obtained
with fewer simulations tailored to the observation. Sequential schemes implementing
such ideas have been introduced for LFI using normalizing ï¬‚ows (see for instance
Papamakarios et al. 2019) and could be extended to our setup.
â€¢ The motivation for the current work was learning ABC statistics, hence the exponen-
tial family formulation. However, the dot-product structure between fw and Î·w is not
beneï¬cial for the direct sampling approach. An energy-based model, which employs
a single NN with input (x, Î¸) in the exponent, may be more expressive and easier to
train.
â€¢ An energy-based posterior approximation Ï€w(Î¸|x) could be trained by minimizing the
expectation over the data marginal p(x) of the (sliced) Fisher divergence with respect
to the true posterior. This is complementary to the strategy employed in this work
to ï¬t pw(x|Î¸). Interestingly, Ï€w(Î¸|x) would be known up to a normalizing constant
depending on x only, making use of standard MCMC possible.
Acknowledgments
We acknowledge fruitful discussion with prof. Christian Robert, prof. Antonietta Mira
and prof. Geoï¬€Nicholls. LP is supported by the EPSRC and MRC through the OxWaSP
CDT programme (EP/L016710/1), which also funds the computational resources used to
perform this work. RD is funded by EPSRC (grant nos. EP/V025899/1, EP/T017112/1)
and NERC (grant no. NE/T00973X/1).
All summary statistics learning strategies used in this manuscript (the regression ap-
proach and the exponential family with SM and SSM) have been implemented in the Python
library ABCpy (Dutta et al., 2021b), which exploits Pytorch (Paszke et al., 2019) to train
the NNs. ABCpy was also used to run PMC-SL, PMC-RE and the ABC experiments.
24

Score Matched Neural Exponential Families for LFI
Appendix A. Suï¬ƒcient statistics
Consider a conditional probabilistic model p(x|Î¸); moreover, abusing notation, we will also
denote as p(x|t) the density pX|T (X = x|T = t), as well as p(x|t; Î¸) the density pX|T,Î˜(X =
x|T = t; Î˜ = Î¸), where here Î˜ denotes the random variable which takes values Î¸. Finally,
we use Ï€ to denote distributions over the parameter values Î¸; speciï¬cally, Ï€(Î¸|x) denotes the
standard posterior, and Ï€(Î¸|t) is an abuse of notation for the density Ï€Î˜|T (Î˜ = Î¸|T = t).
Deï¬nition 17 A statistic t = t(x) is suï¬ƒcient if p(x|t; Î¸) = p(x|t), where Î¸ is a parameter
of the distribution. Alternatively, we have, in the Bayesian setting:
Ï€(Î¸|x) = Ï€(Î¸|t(x)),
for any (non-degenerate) choice of prior distribution Ï€(Î¸).
The existence of suï¬ƒcient statistics implies a precise form of the distribution:
Theorem 18 (Fisher-Neyman factorization theorem): A statistic is suï¬ƒcient â‡â‡’
p(x|Î¸) = h(x)g(t(x)|Î¸), where h and g are non-negative functions.
It is clear from the above theorem that f(x) in the exponential family is suï¬ƒcient.
A stronger result regarding exponential family also exists, which goes under the name
of Pitmanâ€“Koopmanâ€“Darmois theorem. This theorem says that the exponential family is
the most general family of distributions for which there is a suï¬ƒcient statistics whose size is
ï¬xed with the number of samples, provided that the domain of the probability distribution
does not vary with the parameter Î¸ (Koopman, 1936).
Appendix B. Properties of the conditional exponential family
B.1 Linear identiï¬ability
Let us consider the exponential family model pw(x|Î¸) = exp(Î·w(Î¸)T fw(x))/Zw(Î¸), where
here Î·w and fw are not restricted to be Neural Networks. Khemakhem et al. (2020) studies
the identiï¬ability properties of the above family; speciï¬cally, they consider identiï¬ability
properties of the feature extractors Î·w and fw. Identiï¬ability of representations is useful
as it means that two diï¬€erent models from the above family learn similar representations
when trained with diï¬€erent initialization on the same data set. In the framework of our
likelihood-free inference task, it also means that if the true model belongs to the family we
are considering, it is theoretically possible to recover the natural parameters and the true
suï¬ƒcient statistics.
If we consider now Î·w and fw to be Neural Networks, this causes problems as they are
not identiï¬able in the standard sense. In fact, many diï¬€erent parameter conï¬gurations lead
to the same function (as there are many symmetries in how the transformations in a Neural
Network layer are deï¬ned).
Therefore, Khemakhem et al. (2020) introduces, following
previous works, two more general notions of identiï¬able representations. Subsequently, let
W denote the space of the possible Neural Network weights w.
25

Pacchiardi and Dutta
Deï¬nition 19 Weak identiï¬ability (Section 2.2 in Khemakhem et al., 2020). Let
âˆ¼f
w and âˆ¼Î·
w be equivalence relations on W deï¬ned as:
w âˆ¼f
w wâ€² â‡â‡’fw(x) = Afwâ€²(x) + c
w âˆ¼Î·
w wâ€² â‡â‡’Î·w(Î¸) = BÎ·wâ€²(Î¸) + e
where A and B are (ds Ã—ds)-matrices of rank at least min(ds, d) and min(ds, p) respectively,
and c and e are vectors.
Deï¬nition 20 Strong identiï¬ability (Section 2.2 in Khemakhem et al., 2020).
Let âˆ¼f
s and âˆ¼Î·
s be equivalence relations on W deï¬ned as:
w âˆ¼f
s wâ€² â‡â‡’âˆ€i, fi,w(x) = aifÏƒ(i),wâ€²(x) + ci
w âˆ¼Î·
s wâ€² â‡â‡’âˆ€i, Î·i,w(Î¸) = biÎ·Î³(i),wâ€²(Î¸) + ei
where Ïƒ and Î³ are permutations of [[1, n]], ai and bi are non-zero scalars and ci and ei are
scalars.
Weak identiï¬ability means that two parameters are equivalent if the corresponding fea-
ture extractors are the same up to linear transformation. Strong identiï¬ability is a speciï¬c
case of the weak one, in which the linear transformation is restricted to be a scaled permu-
tation.
After introducing these concepts, Khemakhem et al. (2020) provides two theorems (The-
orem 1 and 2) in which weak or strong identiï¬ability of the representations are implied if
diï¬€erent parameter values lead to same distributions, i.e. pw(x|Î¸) = pwâ€²(x|Î¸)âˆ€x, Î¸
=â‡’
w âˆ¼f
w wâ€² or pw(x|Î¸) = pwâ€²(x|Î¸)âˆ€x, Î¸ =â‡’w âˆ¼f
s wâ€² (and similar for âˆ¼Î·
w and âˆ¼Î·
s). These two
results hold under some strict conditions on the functional form of the feature extractors f
and Î· (concerning diï¬€erentiability, rank of the Jacobian and other properties).
Then, they verify explicitly these conditions for a simple fully connected Neural Network
architecture, in which they restrict the activation functions ot be LeakyReLUs and the layer
widths to be monotonically increasing or decreasing. Of course, this architecture is quite
restrictive; it is in fact impossible to study theoretically the properties of more complex ar-
chitectures and to show that they satisfy the necessary conditions for identiï¬ability to hold.
However, they show empirically that the conditional structure, even with more complex
architectures, is helpful in increasing identiï¬ability of the representations (as computed by
the MCC, see Appendix B.1.1).
Therefore, even if the architectures which are used throughout this work do not satisfy
the assumptions needed to explicitly show identiï¬ability of the representations, we argue
that the presence of such results is an hint towards the fact that identiï¬ability (according
to the above deï¬nitions) is actually achievable.
B.1.1 Mean Correlation Coefficient (MCC)
In order to evaluate empirically whether the above identiï¬ability results are satisï¬ed, we
need a way to measure similarity between two embeddings of the same set of data. By
following Khemakhem et al. (2020), we describe here the Mean Correlation Coeï¬ƒcient
26

Score Matched Neural Exponential Families for LFI
(MCC), which is a simple measure to do that. In the main text, we used this technique to
assess how well our approximating family recovered the exact suï¬ƒcient statistics and natural
parameters, in the case where the data generating model p0 belongs to the exponential
family.
In the following, we describe two versions of MCC, which are directly linked to the weak
and strong identiï¬ability deï¬nitions in Appendix B.1.
Strong MCC.
Let us consider two sets of embeddings {yi âˆˆRds}n
i=1 and {zi âˆˆRds}n
i=1,
which can be thought of as samples from two multivariate random variables Y = t1(X) and
Z = t2(X), for two functions t1, t2 and a random variable X; in general, the order of the
components of these two vectors is arbitrary, so that we cannot say, for instance, that the
1st component of Y corresponds to the 1st of Z. Then, MCC computes all the correlation
coeï¬ƒcients between each pair of components of Y and Z.
Next, it solves a linear sum
assignment problem which identiï¬es each component of Y with a component of Z aiming
to maximize the sum of the absolute value of the corresponding correlation coeï¬ƒcient. In
this way, it tries to couple the embeddings which are most linear one to the other. Finally,
the MCC is computed as the mean of the absolute correlation coeï¬ƒcients after the right
permutation of elements of the vector.
MCC is therefore a metric between 0 and 1 which measures how well each component
of the original embedding (say, Y ) has been recovered independently by the other one (say,
Z). Moreover, as it relies on the correlation coeï¬ƒcient, it is not sensitive to rescaling or
translation of each of the embeddings (in fact, the correlation coeï¬ƒcient of two univariate
random variables is Â±1 when a deterministic linear relationship between the two exists,
unless the relationship is is perfectly horizontal or vertical).
Finally, in order to get a better estimate of the MCC, we can split the set of embeddings
in two: one which will be used to determine the permutation and which will give an in-
sample estimate of the MCC, and the other one to which the previous permutation will be
applied and will give an out-of-sample estimate of MCC.
Weak MCC.
The above deï¬nition of MCC aims to estimate how well each single element
of the embedding is recovered, independently on the other; we call it, following Khemakhem
et al. (2020), strong MCC. However, there may be cases where the recovered embedding
is equal to the correct one up to a more generic linear transformation A. In that case,
we would like to have a way to measure, up to a linear transformation, how far are two
embeddings. Following Khemakhem et al. (2020), we therefore apply Canonical Correlation
Analysis (CCA) Hotelling (1936) to learn A, and after that compute MCC, which we will
call weak MCC. Speciï¬cally, CCA is a way to compute linear transformation A so that the
correlations between corresponding components of AÂ·Y and Z is maximized. The so-deï¬ned
weak MCC is therefore a number between 0 and 1 which measures how close is Y to Z after
the best linear transformation is applied to Y. Similarly as before, a part of data is used
to learn the best embedding; we can therefore use fresh samples to get an out-of-sample
estimate along the in-sample one.
27

Pacchiardi and Dutta
B.2 Universal approximation capability
Khemakhem et al. (2020) provide a result (Theorem 3) in which they prove universal ap-
proximation capability of the conditional exponential family. Speciï¬cally, they show that,
considering the dimension of the representations ds as an additional parameter, it is possi-
ble to ï¬nd an arbitrarily good approximation of any conditional probability density p0(x|Î¸),
provided that X and Î˜ are compact Hausdorï¬€spaces. In practice, good approximation may
be achieved with a value of ds larger than d or p. Moreover, as remarked in the main text,
this result is not concerned with the way the approximating family is ï¬t; indeed, we expect
that this task becomes (both statistically and computationally) more challenging when ds
increases.
Appendix C. Some properties of Score Matching
C.1 Proof of Theorem 6
Our Theorem 6 extends Theorem 1 in HyvÂ¨arinen (2005), that considers the case X = Rd,
and which is recovered in the case ai = âˆ’âˆand bi = +âˆâˆ€i. Our proof follows HyvÂ¨arinen
(2005), which however explicitly stated only Assumptions A1 and A2. Following Yu et al.
(2019), we add the additional Assumption A3 which is required for Fubini-Tonelli theorem
to apply.
In order to prove Theorem 6, Assumption A3 can be weakened to Ep0
 âˆ‚2
âˆ‚x2
i log pw(X)
 <
âˆ, âˆ€w, âˆ€i = 1, . . . , d.
We state the more general one in the main text as that allows
Theorem 9 for SSM to be proved as well.
Proof Let s0(x) = âˆ‡x log p0(x) denote the score of the data distribution, and analogously
s(x; w) = âˆ‡x log pw(x). Then, Eq. (4) can be rewritten as:
DF (p0âˆ¥pw) = 1
2
Z
X
p0(x)âˆ¥s0(x) âˆ’s(x; w)âˆ¥2dx
= 1
2
Z
X
p0(x)

âˆ¥s0(x)âˆ¥2 + âˆ¥s(x; w)âˆ¥2 âˆ’2s0(x)T s(x; w)

dx
= 1
2
Z
X
p0(x)âˆ¥s0(x)âˆ¥2dx
|
{z
}
C
+ 1
2
Z
X
p0(x)âˆ¥s(x; w)âˆ¥2dx
|
{z
}
A
âˆ’
Z
X
p0(x)s0(x)T s(x; w)dx
|
{z
}
B
Note that we can split the integral into the three parts as the ï¬rst two are assumed to be
ï¬nite in A2; as a consequence, B is also ï¬nite thanks to |2ab| â‰¤a2 + b2.
The ï¬rst element does not depend on w, so that we can safely ignore that. The second
one appears as it is in the ï¬nal Eq. (5). Therefore, we will focus on the last term, which we
can write as:
B = âˆ’
d
X
i=1
Z
X
p0(x)s0,i(x)si(x; w)dx;
(13)
Letâ€™s now consider a single i, which we can rewrite as:
âˆ’
Z
X
p0(x)âˆ‚log p0(x)
âˆ‚xi
si(x; w)dx = âˆ’
Z
X
p0(x)
p0(x)
âˆ‚p0(x)
âˆ‚xi
si(x; w)dx = âˆ’
Z
X
âˆ‚p0(x)
âˆ‚xi
si(x; w)dx.
28

Score Matched Neural Exponential Families for LFI
Now, letâ€™s consider the integral over xi ï¬rst, and apply partial integration to it. Doing this
relies on the Fubini-Tonelliâ€™s theorem, which can be safely applied due to B being ï¬nite as
discussed above. We now apply partial integration to the integral over xi:
âˆ’
Z b1
a1
Z b2
a2
. . .
Z bd
ad
âˆ‚p0(x)
âˆ‚xi
si(x; w)dx = âˆ’
Z b1
a1
. . .
Z biâˆ’1
aiâˆ’1
Z bi+1
ai+1
. . .
Z bd
ad
"
p0(x)si(x; w)|xiâ†—bi
xiâ†˜ai
âˆ’
Z bi
ai
p0(x)âˆ‚si(x; w)
âˆ‚xi
dxi
#
dx1 . . . dxiâˆ’1dxi+1 . . . dxd
=
Z b1
a1
. . .
Z biâˆ’1
aiâˆ’1
Z bi+1
ai+1
. . .
Z bd
ad
" Z bi
ai
p0(x)âˆ‚si(x; w)
âˆ‚xi
dxi
#
dx1 . . . dxiâˆ’1dxi+1 . . . dxd
=
Z
X
p0(x)âˆ‚si(x; w)
âˆ‚xi
dx.
where the second equality holds thanks to Assumption A1. For stating the last equality
rigorously, we need to invoke Fubini-Tonelliâ€™s theorem again; this relies on the assumption:
Z
X
p0(x)âˆ‚si(x; w)
âˆ‚xi
 dx = Ep0

âˆ‚si(X; w)
âˆ‚xi
 < âˆ,
which is equivalent to Assumption A3.
By repeating this argument for all terms in the sum in Eq. (13), we obtain that:
B =
Z
X
p0(x)
d
X
i=1
âˆ‚si(x; w)
âˆ‚xi
dx =
Z
X
p0(x)
d
X
i=1
âˆ‚2 log pw(x)
âˆ‚x2
i
dx,
which concludes our proof.
C.2 Proof of Theorem 7
We give here an extended version of Theorem 7, which we then prove following HyvÂ¨arinen
(2005).
Theorem 21 Assume âˆƒwâ‹†: p0(Â·) = pwâ‹†(Â·). Then:
w = wâ‹†=â‡’DF (p0âˆ¥pw) = 0.
Further, if p0(x) > 0 âˆ€x âˆˆX, you also have:
DF (p0âˆ¥pw) = 0 =â‡’pw(Â·) = p0(Â·).
Finally, if no other value w Ì¸= wâ‹†gives a pdf pw that is equal to pwâ‹†:
DF (p0âˆ¥pw) = 0 =â‡’w = wâ‹†.
29

Pacchiardi and Dutta
Proof The ï¬rst statement is straightforward. For the second one, if DF (p0âˆ¥pw) = 0 and
p0(x) > 0 âˆ€x âˆˆX, then âˆ‡x log pw(x) = âˆ‡x log p0(x); this in turn implies that log pw(x) =
log p0(x) + c âˆ€x âˆˆX for a constant c, which however is 0 as both p0 and pw are pdfâ€™s.
The third statement follows from the second as, for the additional assumption, wâ‹†is
the only choice of w which gives pw = p0.
C.3 Proof of Theorem 9
Similarly to the SM case, our Theorem 9 extends Theorem 1 in Song et al. (2020), that
considers the case X = Rd, and which is recovered in the case ai = âˆ’âˆand bi = +âˆâˆ€i.
Our proof follows Song et al. (2020), which however explicitly stated only Assumptions A1,
A2 and A4. Following Yu et al. (2019), we add the additional Assumption A3 which is
required for Fubini-Tonelli theorem to apply. The strategy of the proof is very similar to
Theorem 6.
Proof
As before, let s0(x) = âˆ‡x log p0(x) denote the score of the data distribution, and
analogously s(x; w) = âˆ‡x log pw(x). Then, Eq. (6) can be rewritten as:
DFS(p0âˆ¥pw) = 1
2
Z
V
Z
X
q(v)p0(x)[vT s0(x) âˆ’vT s(x; w)]2dxdv
= 1
2
Z
V
Z
X
q(v)p0(x)

(vT s0(x))2 + (vT s(x; w))2 âˆ’2(vT s0(x))(vT s(x; w))

dxdv
=
Z
V
Z
X
q(v)p0(x)
1
2(vT s(x; w))2 âˆ’(vT s0(x))(vT s(x; w))

dxdv + C
Note that we can split the integral into the three parts thanks to Assumptions A2 and A4,
which ensure that the expectation of each term is bounded. Additionally, we have absorbed
in the constant C the term which does not depend on w.
In the last row of the above Equation, the ï¬rst term in the square brackets appears in
Eq. (7), which is what we want to prove. We focus therefore on the second term:
âˆ’
Z
V
Z
X
q(v)p0(x)

(vT s0(x))(vT s(x; w))

dxdv
= âˆ’
Z
V
Z
X
q(v)p0(x)

(vT âˆ‡x log p0(x))(vT âˆ‡x log pw(x))

dxdv
= âˆ’
Z
V
Z
X
q(v)

(vT âˆ‡xp0(x))(vT âˆ‡x log pw(x))

dxdv
= âˆ’
d
X
i=1
Z
V
Z
X
q(v)

vi
âˆ‚p0(x)
âˆ‚xi
(vT âˆ‡x log pw(x))

dxdv
(14)
We will now consider one single element of the sum for a chosen i, and consider the integral
over xi ï¬rst. Swapping the order of integration relies on Fubini-Tonelliâ€™s theorem, which
can be safely applied due to the above quantity being ï¬nite as discussed above. We then
30

Score Matched Neural Exponential Families for LFI
apply partial integration to the integral over xi:
âˆ’
Z
V
Z b1
a1
Z b2
a2
. . .
Z bd
ad
q(v)

vi
âˆ‚p0(x)
âˆ‚xi
(vT âˆ‡x log pw(x))

dxdv
= âˆ’
Z
V
Z b1
a1
. . .
Z biâˆ’1
aiâˆ’1
Z bi+1
ai+1
. . .
Z bd
ad
q(v)
"
vip0(x)(vT âˆ‡x log pw(x))
xiâ†—bi
xiâ†˜ai
âˆ’
Z bi
ai
vip0(x)

vT âˆ‚
âˆ‚xi
âˆ‡x log pw(x)

dxi
#
dxâˆ’i,
where dxâˆ’i = dx1 . . . dxiâˆ’1dxi+1 . . . dxd. The ï¬rst element in the square bracket goes to 0
thanks to Assumption A1. We are left therefore with:
Z
V
Z b1
a1
. . .
Z biâˆ’1
aiâˆ’1
Z bi+1
ai+1
. . .
Z bd
ad
" Z bi
ai
q(v)p0(x)vi

vT âˆ‚
âˆ‚xi
âˆ‡x log pw(x)

dxi
#
dxâˆ’i
=
Z
V
Z
X
q(v)p0(x)vi

vT âˆ‚
âˆ‚xi
âˆ‡x log pw(x)

dxdv.
The last equality again exploits Fubini-Tonelli theorem, which in this case requires:
Z
V
Z
X
q(v)p0(x)
vi

vT âˆ‚
âˆ‚xi
âˆ‡x log pw(x)
 dxdv = EV âˆ¼qEXâˆ¼p0
Vi

V T âˆ‚
âˆ‚xi
âˆ‡x log pw(X)
 < âˆ.
This is satisï¬ed by combining Assumptions A3 and A4, as in fact:
EV âˆ¼qEXâˆ¼p0
Vi

V T âˆ‚
âˆ‚xi
âˆ‡x log pw(X)
 = EV âˆ¼qEXâˆ¼p0

Vi
d
X
j=1
Vj
âˆ‚2
âˆ‚xiâˆ‚xj
log pw(X)

â‰¤
d
X
j=1
EV âˆ¼qEXâˆ¼p0
ViVj
âˆ‚2
âˆ‚xiâˆ‚xj
log pw(X)
 =
d
X
j=1
EV âˆ¼q |ViVj| Â· EXâˆ¼p0

âˆ‚2
âˆ‚xiâˆ‚xj
log pw(X)

â‰¤
d
X
j=1
q
EV âˆ¼qV 2
i Â· EV âˆ¼qV 2
j Â· EXâˆ¼p0

âˆ‚2
âˆ‚xiâˆ‚xj
log pw(X)
 ,
where the last inequality holds thanks to Cauchy-Schwarz inequality; Assumptions A3 and
A4 ensure the last row is < âˆ.
Back to Eq. (14), we have therefore:
âˆ’
d
X
i=1
Z
V
Z
X
q(v)

vi
âˆ‚p0(x)
âˆ‚xi
(vT âˆ‡x log pw(x))

dxdv
=
Z
V
Z
X
d
X
i=1
q(v)p0(x)vi

vT âˆ‚
âˆ‚xi
âˆ‡x log pw(x)

dxdv
=
Z
V
Z
X
d
X
i=1
d
X
j=1
q(v)p0(x)vivj
âˆ‚2
âˆ‚xiâˆ‚xj
log pw(x)dxdv
=
Z
V
Z
X
q(v)p0(x)

vT (Hx log pw(x)) v

dxdv.
31

Pacchiardi and Dutta
C.4 Proof of Theorem 10
We give here an extended version of Theorem 10. This is a version of Lemma 1 in Song
et al. (2020), whose proof we adapt.
Theorem 22 Assume âˆƒwâ‹†: p0(Â·) = pwâ‹†(Â·). Then:
w = wâ‹†=â‡’DFS(p0âˆ¥pw) = 0.
Further, if p0(x) > 0 âˆ€x âˆˆX, you also have:
DFS(p0âˆ¥pw) = 0 =â‡’pw(Â·) = p0(Â·).
Finally, if no other value w Ì¸= wâ‹†gives a pdf pw that is equal to pwâ‹†:
DFS(p0âˆ¥pw) = 0 =â‡’w = wâ‹†.
Proof The ï¬rst statement is straightforward.
For the second one, if DF (p0âˆ¥pw) = 0 and p0(x) > 0 âˆ€x âˆˆX, then:
Z
V
q(v)(vT (âˆ‡x log p0(x) âˆ’âˆ‡x log pw(x)))2dv = 0
â‡â‡’
Z
V
q(v)vT (âˆ‡x log p0(x) âˆ’âˆ‡x log pw(x))(âˆ‡x log p0(x) âˆ’âˆ‡x log pw(x))T vdv = 0
â‡â‡’(âˆ‡x log p0(x) âˆ’âˆ‡x log pw(x))T E[V V T ](âˆ‡x log p0(x) âˆ’âˆ‡x log pw(x)) = 0
(â‹†)
â‡â‡’âˆ‡x log p0(x) âˆ’âˆ‡x log pw(x) = 0
â‡â‡’log pw(x) = log p0(x) + c âˆ€x âˆˆX,
where in the third line above V is a random variable distributed according to q, for which
therefore E[V V T ] is positive deï¬nite by Assumption A4, which ensures equivalence (â‹†)
holds. Additionally, as both p0 and pw are pdfâ€™s (and therefore normalized), the constant
c = 0.
The third statement follows from the second as, for the additional assumption, wâ‹†is
the only choice of w which gives pw = p0.
C.5 Proof of Theorem 12
We give here an extended version of Theorem 12, which we then prove.
Theorem 23 Let Y = t(X) âˆˆY for a bijection t, and denote by pY
0 and pY
w the distributions
on Y induced by the distributions p0 and pw on X. Assume âˆƒwâ‹†: p0(Â·) = pwâ‹†(Â·), and let D
denote either DF or DFS. Then:
w = wâ‹†=â‡’D(pY
0 âˆ¥pY
w) = 0.
32

Score Matched Neural Exponential Families for LFI
Further, if p0(x) > 0 âˆ€x âˆˆX and Assumption A4 holds, you also have:
D(pY
0 âˆ¥pY
w) = 0 =â‡’pw(Â·) = p0(Â·).
Finally, if no other value w Ì¸= wâ‹†gives a pdf pw that is equal to pwâ‹†:
D(pY
0 âˆ¥pY
w) = 0 =â‡’w = wâ‹†.
Proof The proof relies on the equivalence between distributions for the random variables
Y and X; in fact, by ï¬xing y = t(x) and denoting by |Jt(x)| the determinant of the Jaco-
bian matrix of t evaluated in x, we have that pY
0 (y) =
p0(x)
|Jt(x)| and pY
w(y) =
pw(x)
|Jt(x)|, so that
p0(Â·) = pw(Â·) â‡â‡’pY
0 (Â·) = pY
w(Â·). The ï¬rst and third statements follow directly from this
fact by applying Theorem 21 (if D is chosen to be DF ) or Theorem 22 (if D = DFS) to
D(pY
0 âˆ¥pY
w); for the second, notice also that p0(x) > 0 âˆ€x âˆˆX =â‡’pY
0 (y) âˆ€y âˆˆY > 0 as t is
a bijection; then, Theorem 21 or Theorem 22 imply that pY
0 (Â·) = pY
w(Â·) =â‡’p0(Â·) = pw(Â·).
C.6 Discussion on the positivity condition for SM
We follow here the discussion in Appendix D of Arbel and Gretton (2018).
Consider again the Fisher divergence in Eq. (4); we want to understand the conditions
under which this is a divergence between probability measures, which essentially means it
is 0 if and only if the probability distributions are the same.
Besides the fact that both p0 and pw need to be continuous (otherwise the gradient
would be a delta function), it turns out that a necessary condition is that p0 is positive
on the whole space to which the random variable belong (letâ€™s say X), if we donâ€™t put any
restrictions on pw. Otherwise, the following scenario may happen (see Appendix D in Arbel
and Gretton, 2018): consider the case in which p0(x) is a mixture of two densities supported
on disjoint sets: p0(x) = Î±ApA(x) + Î±BpB(x), with non-negative weights Î±A, Î±B, such that
pA(x) > 0 â‡â‡’x âˆˆXA,
pB(x) > 0 â‡â‡’x âˆˆXB,
XA, XB âˆˆX;
XA âˆ©XB = âˆ….
Note that this implies âˆƒx âˆˆX : p0(x) = 0. In this setting, any pw of the form pw(x) =
Î²ApA(x) + Î²BpB(x) will give DF (p0âˆ¥pw) = 0. This can be seen by computing the Fisher
divergence directly:
33

Pacchiardi and Dutta
DF (p0âˆ¥pw) = 1
2
Z
X
p0(x)âˆ¥âˆ‡x log p0(x) âˆ’âˆ‡x log pw(x)âˆ¥2dx
= 1
2
Z
X
p0(x)âˆ¥âˆ‡x log(Î±ApA(x) + Î±BpB(x)) âˆ’âˆ‡x log(Î²ApA(x) + Î²BpB(x))âˆ¥2dx
= 1
2
Z
XA
p0(x)âˆ¥âˆ‡x log(Î±ApA(x)) âˆ’âˆ‡x log(Î²ApA(x))âˆ¥2dx+
1
2
Z
XB
p0(x)âˆ¥âˆ‡x log(Î±BpB(x)) âˆ’âˆ‡x log(Î²BpB(x))âˆ¥2dx
= 1
2
Z
XA
p0(x) âˆ¥âˆ‡x log pA(x) âˆ’âˆ‡x log pA(x)âˆ¥2
|
{z
}
=0
dx+
1
2
Z
XB
p0(x) âˆ¥âˆ‡x log pB(x) âˆ’âˆ‡x log pB(x)âˆ¥2
|
{z
}
=0
dx
= 0,
where the third equality relies on splitting the integration domain over the two subsets on
which pA and pB are supported (and the other is 0) and the fourth equality relies on the
presence of the logarithmic derivatives, that removes the importance of the mixture weights.
Due to the above, in the case of the conditional Fisher divergence in Eq. (9), p0(x|Î¸)
needs to be supported on the whole X for each Î¸ in order for DE
F (p0|pw) = 0 â‡â‡’p0(Â·|Î¸) =
pw(Â·|Î¸) Ï€(Î¸)-almost everywhere.
This can be seen by considering the case of univariate
parameter Î¸ and by choosing p0(x|Î¸) = pA(x)H(Î¸) + (1 âˆ’H(Î¸))pB(x), where pA and pB
are as above and H(Â·) represents the Heaviside function. In this case, choosing pw(x|Î¸) =
q(x), where q(x) = Î²ApA(x) + Î²BpB(x) will give DE
F (p0|pw) = 0, as for each ï¬xed Î¸,
DF (p0(Â·|Î¸)|pw(Â·|Î¸)) falls in the case considered above.
C.7 Equivalence of Correction Factor and Transformed Score Matching
As discussed in the main text (Section 3.2), the ï¬rst extension of score matching to non-
negative random variables was given in HyvÂ¨arinen (2007):
D+
F (p0âˆ¥pw) = 1
2
Z
Rd
+
p0(x)âˆ¥âˆ‡x log p0(x) âŠ™x âˆ’âˆ‡x log pw(x) âŠ™xâˆ¥2dx,
(15)
where âŠ™denotes element wise product between vectors and Rd
+ is the positive octant of Rd.
The correction factor x relaxes assumption A1 to p0(x)x2
i
âˆ‚
âˆ‚xi log pw(x) â†’0, so that it is
possible to get an explicit form of the above with looser assumptions. This is an example
of Corrected Score Matching (CorrSM, 3.2), in which the issue arising due to distribution
having a compact support is ï¬xed by introducing a correction factor in the formulation of
the objective.
Yu et al. (2019) proposed a more general score matching for non-negative random vari-
ables by allowing freedom of choice in the factor that is used in the integrand to correct for
the integration by parts step (Appendix C.1), leading to the following objective:
D+
F (p0âˆ¥pw) = 1
2
Z
Rn
+
p0(x)âˆ¥(âˆ‡x log p0(x)) âŠ™
p
h(x) âˆ’(âˆ‡x log pw(x)) âŠ™
p
h(x)|âˆ¥2dx,
(16)
34

Score Matched Neural Exponential Families for LFI
where h(x) has the same dimension as x, and has positive elements almost surely.
The explicit formulation associated to Eq. (16) can be obtained under the following
assumptions:
A1b p0(x)hj(xj)âˆ‚j log pw(x) â†’0 for xi â†˜0 and xi â†—âˆ, âˆ€w, i,
A2b Ep0âˆ¥âˆ‡x log p0(X) âŠ™h1/2(X)âˆ¥2
2 < âˆ, Ep0âˆ¥âˆ‡x log pw(X) âŠ™h1/2(X)âˆ¥2
2 < âˆâˆ€w,
A3b Ep0âˆ¥(âˆ‡x log pw(X)âŠ™h(X)))â€²âˆ¥1 < âˆâˆ€w, where the prime symbol denotes element-wise
diï¬€erentiation.
Under the above assumption, Eq. (16) is equal to:
D+
F (p0âˆ¥pw) =
Z
Rn
+
p0(x)
d
X
i=1
"
1
2hi(x)
âˆ‚log pw(x)
âˆ‚xi
2
+
hi(x)
âˆ‚2 log pw(x)
âˆ‚x2
i

+ hâ€²
i(x)âˆ‚log pw(x)
âˆ‚xi
#
dx + C,
where C is a constant with respect to pw.
In Proposition 2 in Yu et al. (2019), they give a result similar to our Theorems 7
and 12 guaranteeing that minimization of D+
F (p0âˆ¥pw) is a valid procedure for estimating a
probabilistic model. When considering the ï¬nite-sample estimate of C.7, diï¬€erent choices
of h(x) may allow to focus more on smaller/larger values of x, which may in practice have
better properties than the original form for non-negative data in HyvÂ¨arinen (2007), which
is recovered for h(x) = x2 (where the square is applied element-wise).
This formulation in Eq. (16), albeit originally considered for non-negative random vari-
ables only, can be extended to random variables with any bounded domain, by choosing a
suitable function h and modifying A1b to hold for xi going to the limits of the domain.
In the next Sections, we therefore compare this approach with TranSM without specifying
the domain; we will show that both the implicit and explicit formulation are the same
with both TranSM and CorrSM, implying that the two are equivalent (we will show this in
the speciï¬c case in which the transformation and the function h act independently on the
diï¬€erent coordinates, but we believe this to be the case more in general; see Appendix C.9).
C.7.1 Equivalence of the implicit form
As mentioned in the main text (Section 3.2), another approach to apply Fisher divergence to
distributions with bounded domain (on one side or both) is to transform the data space to
the real line and then apply standard Fisher divergence; we called this Transformation Score
Matching (TranSM). Let us denote t such a transformation, which we assume to be bijective.
Then, starting from p0(x) and pw(x) we get p0(y) = p0(x)
|Jt(x)| and pw(y) = pw(x)
|Jt(x)| for y = t(x),
where Jt is the Jacobian matrix of t and | Â· | denotes here the determinant; here, diï¬€erently
from the main text, we use a lighter notation where p0(y) and p0(x) are two diï¬€erent
densities associated to the diï¬€erent name of the argument (same for pw). We investigate
what is the Fisher divergence between the densities of the transformed distributions. Recall
that p0(y)dy = p0(x)dx, for y = t(x). Moreover, we also have that:
35

Pacchiardi and Dutta
âˆ‡yg(y) = Jtâˆ’1(t(x))âˆ‡xg(t(x)) = (Jt(x))âˆ’1âˆ‡xg(t(x)),
where the second equality comes from the fact that Jtâˆ’1(t(x)) = (Jt(x))âˆ’1 due to the inverse
function theorem. Then, we can compute the Fisher Divergence between p0(y) and pw(y)
(corresponding to the TranSM objective):
DF (p0(y)âˆ¥pw(y)) = 1
2
Z
p0(y)âˆ¥âˆ‡y log p0(y) âˆ’âˆ‡y log pw(y)âˆ¥2dy
= 1
2
Z
p0(x)âˆ¥(Jt(x))âˆ’1[âˆ‡x log p0(x) âˆ’âˆ‡x log |Jt(x)| âˆ’âˆ‡x log pw(x) + âˆ‡x log |Jt(x)|]âˆ¥2dx
= 1
2
Z
p0(x)âˆ¥(Jt(x))âˆ’1[âˆ‡x log p0(x) âˆ’âˆ‡x log pw(x)]âˆ¥2dx.
In the rather common case in which the transformation t acts on each component inde-
pendently, the Jacobian matrix is diagonal; in this case, the latter is equivalent to Eq. (16)
upon deï¬ning h(x) to be a vector containing the squares of the diagonal elements of the
Jacobian, i.e. putting
p
hi(x) = (Jt(x))âˆ’1
ii .
C.7.2 Equivalence of the explicit form
For both TranSM and CorrSM it is possible to get an explicit form of the objective (Eqs. 5
and C.7), in which the integrand does not depend on the data distribution p0. In case in
which the transformation t acts on the diï¬€erent components independently, the original
explicit divergence for the transformed variable Y = t(X) is equivalent to the corrected
explicit divergence for the original X, analogously to the implicit Fisher divergence form.
In fact, by applying the deï¬nition of explicit Fisher divergence (Eq. 5) to the transformed
Y , you get:
DF (p0(y)âˆ¥pw(y)) =
Z
p0(y)
d
X
i=1
"
1
2
âˆ‚log pw(y)
âˆ‚yi
2
+
âˆ‚2 log pw(y)
âˆ‚y2
i
#
|
{z
}
â‹†
dy + C
where C is a constant with respect to pw. Considering only the term in square brackets,
denoting âˆ‚i =
âˆ‚
âˆ‚xi and setting
p
hi(x) = (Jt(x))âˆ’1
ii , we get:
â‹†= 1
2hi(x)

(âˆ‚i log pw(x))2 + 1
4 (âˆ‚i log hi(x))2 + âˆ‚i log pw(x) Â· âˆ‚i log hi(x)

+ 1
2hâ€²
i(x) Â· âˆ‚i log pw(x) + 1
4hâ€²
i(x)âˆ‚i log hi(x) + hi(x)âˆ‚2
i log pw(x) + 1
2hi(x)âˆ‚2
i log hi(x)
= 1
2hi(x) (âˆ‚i log pw(x))2 + 1
8hi(x) (âˆ‚i log hi(x))2
+ hâ€²
i(x) Â· âˆ‚i log pw(x) + 1
4hâ€²
i(x)âˆ‚i log hi(x) + hi(x)âˆ‚2
i log pw(x) + 1
2hi(x)âˆ‚2
i log hi(x).
The blue terms are the same that appear in the CorrSM explicit formulation (Eq. C.7),
while all other terms are constants with respect to pw.
We have shown therefore that CorrSM and TranSM are equivalent in both the explicit
and implicit formulation if the transformation is applied independently on the elements of
36

Score Matched Neural Exponential Families for LFI
x. Therefore, the two approaches are completely equivalent when it comes to minimizing
them.
C.8 Speciï¬c formulation of TranSM
We discuss here the transformations we apply in this work in TranSM; speciï¬cally, we only
consider the case in which the support for the multivariate x is deï¬ned by an intersection of
element-wise inequalities, i.e. x âˆˆNd
i=1(ai, bi), where ai, bi can take on the values Â±âˆas
well. In this case, then, a transformation can be applied independently on each element of
x. We consider here the following transformations (which are also used in the Stan package
Carpenter et al., 2017):
â€¢ When X âˆˆ[0, âˆ)d, the transformation we use is yi = log(xi) âˆˆRd. This corresponds
to diagonal Jacobian with elements (Jt(x))âˆ’1
ii
= xi, so that the above expression
becomes the same as the original Fisher divergence for non-negative random variables
discussed in Eq. (15).
â€¢ More generally, if xi âˆˆ[ai, +âˆ) for |ai| < âˆ, we can transform the data as yi =
log(xi âˆ’ai) âˆˆR, while if xi âˆˆ(âˆ’âˆ, bi] for |bi| < âˆ, we simply reverse the trans-
formation: yi = log(bi âˆ’xi) âˆˆR.
These correspond to (Jt(x))âˆ’1
ii
= xi âˆ’ai and
(Jt(x))âˆ’1
ii = bi âˆ’xi.
â€¢ Finally, if xi âˆˆ(ai, bi) for |ai|, |bi| < âˆ, we can use the transformation deï¬ned as:
yi = t(xi) = logit

xiâˆ’ai
biâˆ’ai

with inverse transformation xi = tâˆ’1(yi) = a+(bâˆ’a)
eyi
eyi+1.
This corresponds to (Jt(x))âˆ’1
ii = (xiâˆ’ai)(biâˆ’xi)
biâˆ’ai
.
C.9 Score matching for distributions with more general domain
As highlighted in the main text, across this work we are concerned with applying score
matching to distributions whose support is deï¬ned by independent constraints on the dif-
ferent coordinates, as for instance X = Nd
i=1(ai, bi). That is arguably the most common
case in the literature. However, there have been some recent works which applied SM to
more general cases. For instance, Mardia et al. (2016) devised a way to apply it to a di-
rectional distribution deï¬ned on an oriented Riemannian manifold (for instance, a sphere).
It is interesting how their derivation of the explicit form from the implicit one relies on
the classical divergence theorem (also known as Stokesâ€™ theorem), of which the partial in-
tegration trick used in Theorem 6 is a speciï¬c case. Liu and Kanamori (2019) introduced
instead a way to apply score matching for a distribution on Euclidean space with complex
truncation boundaries; their approach boils down to introducing a smart correction factor
which goes to 0 at the boundary (thus allowing partial integration) but still being tractable;
again, they need a more general version of Theorem 6 to obtain an objective for which the
integrand does not depend on the data distribution.
In Appendix C.7, we established that CorrSM and TranSM are equivalent if the trans-
formation is applied independently on the elements of x, which requires the domain to be
deï¬ned by independent constraints on the coordinates. In the more general case of a ir-
regular subset of Euclidean space (as in the setup of Liu and Kanamori, 2019), it is not
37

Pacchiardi and Dutta
clear whether it is always possible to associate a correction factor to a transformation which
maps the space to Rd. That seems to be plausible if the domain satisï¬es some regularity
conditions which may be related to convexity (for instance think of a triangle in R2, which
can be easily stretched to cover the full space). We are not aware however of any work
investigating this.
C.10 Score matching with exponential family
We consider here the exponential family:
pw(x|Î¸) = exp(Î·w(Î¸)T fw(x))/Zw(Î¸),
and want to ï¬nd the value of w minimizing either DE
F (p0âˆ¥pw) or DE
FS(p0âˆ¥pw), which are
deï¬ned in Eq. (10).
Under the conditions discussed in Section 3.3, if Ï€(Î¸) > 0 âˆ€Î¸, then DE
F (p0âˆ¥pw) = 0 and
DE
FS(p0âˆ¥pw) = 0 â‡â‡’pw(x|Î¸) = p0(x|Î¸) Ï€(Î¸)-almost everywhere. In this case, if fw and
Î·w satisfy the conditions required for the theorems mentioned in Appendix B.1 to hold,
then fw and Î·w are respectively suï¬ƒcient statistics and natural parameters of p0.
In order to ï¬nd the value of the empirical estimate of the explicit for of both DE
F (p0âˆ¥pw)
and DE
FS(p0âˆ¥pw), we insert the deï¬nition of the exponential family with in Eq. 11, which
leads to:
Ë†J(w) = 1
N
N
X
j=1
" d
X
i=1
 
1
2

Î·w(Î¸(j))T âˆ‚
âˆ‚xi
fw(x(j))
2
+ Î·w(Î¸(j))T âˆ‚2
âˆ‚x2
i
fw(x)
!#
,
Ë†JS(w) =
1
NM
N
X
j=1
M
X
k=1
"
v(j,k),T Hx(Î·w(Î¸(j))T fw(x(j)))v(j,k) + 1
2
d
X
i=1

Î·w(Î¸(j))T âˆ‚
âˆ‚xi
fw(x(j))
2#
.
Note that the objective does not change if you set fw(x) to c + fw(x), for a constant
vector c; in fact, this constant gets absorbed into the normalizing constant in pw(x|Î¸).
Similarly, Î·w(Î¸)T fw(x) = (1/c Â· Î·w(Î¸))T (c Â· fw(x)) for some constant c Ì¸= 0. Therefore,
statistics and corresponding parameters are only deï¬ned up to a scale with respect to
one another; if you use two Neural Networks to learn both of them, diï¬€erent network
initializations may lead to diï¬€erent learned statistics and natural parameters, but their
product should be ï¬xed (up to translation of fw(x)).
However, this degeneracy may make training the approximate likelihood pw with the
score matching approach harder. In order to improve training, we usually add a Batch
Normalization layer on top of the Î·w network. Basically, Batch Normalization ï¬xes the
scale of the output of Î·w over a training batch, therefore removing this additional degree of
freedom and making training easier. We discuss in more detail this in Appendix E.2.
Appendix D. Scoring Rules
A Scoring Rule (SR) S (Dawid and Musio, 2014; Gneiting and Raftery, 2007) is a function
of a probability distribution over X and of an observation in X.
In the framework of
38

Score Matched Neural Exponential Families for LFI
probabilistic forecasting, S(P, y) represents the penalty which you incur when stating a
forecast P for an observation y.10
If the observation y is a realization of a random variable Y with distribution Q, the
expected Scoring Rule can be deï¬ned as:
S(P, Q) := EY âˆ¼QS(P, Y ),
where we overload notation in the second argument of S. The Scoring Rule S is said to be
proper relative to a set of distributions P(X) over X if
S(Q, Q) â‰¤S(P, Q) âˆ€P, Q âˆˆP(X),
i.e., if the expected Scoring Rule is minimized in P when P = Q. Moreover, S is strictly
proper relative to P(X) if P = Q is the unique minimum:
S(Q, Q) < S(P, Q) âˆ€P, Q âˆˆP(X) s.t. P Ì¸= Q;
when minimizing an expected strictly proper Scoring Rule, a forecaster would provide their
true belief (Gneiting and Raftery, 2007).
By following Dawid and Musio (2014), we deï¬ne the divergence related to a proper
Scoring Rule as D(P, Q) := S(P, Q) âˆ’S(Q, Q) â‰¥0. Notice that P = Q =â‡’D(P, Q) = 0,
but there may be P Ì¸= Q such that D(P, Q) = 0. However, if S is strictly proper, D(P, Q) =
0 â‡â‡’P = Q, which is the commonly used condition to deï¬ne a statistical divergence
(as for instance the Kullback-Leibler, or KL, divergence). Therefore, each strictly proper
Scoring Rule corresponds to a statistical divergence between probability distributions
In the following, we detail the two Scoring Rules which we consider in the main text
(Section 5.3).
Energy score
The energy score is given by:
S(Î²)
E (P, y) = 2 Â· E
h
âˆ¥X âˆ’yâˆ¥Î²
2
i
âˆ’E
h
âˆ¥X âˆ’Xâ€²âˆ¥Î²
2
i
,
X âŠ¥âŠ¥Xâ€² âˆ¼P,
where Î² âˆˆ(0, 2) and âŠ¥âŠ¥denotes independence between random variables. This is a strictly
proper Scoring Rule for the class PÎ²(X) of probability measures P such that EXâˆ¼P âˆ¥Xâˆ¥Î² <
âˆ(Gneiting and Raftery, 2007). The related divergence is the square of the energy distance,
which is a metric between probability distributions (Rizzo and SzÂ´ekely 2016)11:
DE(P, Q) = 2 Â· E
h
âˆ¥X âˆ’Y âˆ¥Î²
2
i
âˆ’E
h
âˆ¥X âˆ’Xâ€²âˆ¥Î²
2
i
âˆ’E
h
âˆ¥Y âˆ’Y â€²âˆ¥Î²
2
i
,
for X âŠ¥âŠ¥Xâ€² âˆ¼P and Y âŠ¥âŠ¥Y â€² âˆ¼Q.
10. Some authors (Gneiting and Raftery, 2007) use the convention of S(P, y) representing a reward rather
than a penalty, which is equivalent up to change of sign.
11. The probabilistic forecasting literature (Gneiting and Raftery, 2007) use a diï¬€erent convention of the
energy score and the subsequent kernel score, which amounts to multiplying our deï¬nitions by 1/2. We
follow here the convention used in the statistical inference literature (Rizzo and SzÂ´ekely, 2016; ChÂ´erief-
Abdellatif and Alquier, 2020; Nguyen et al., 2020)
39

Pacchiardi and Dutta
In our case of interest (Section 5.3, Appendix G.4), we are unable to evaluate exactly
S(Î²)
E
as we do not have a closed form for P. Thus, we obtain samples {xj}m
j=1, xj âˆ¼P, and
unbiasedly estimate the Energy Score with:
Ë†S(Î²)
E ({xj}m
j=1, y) = 2
m
m
X
j=1
âˆ¥xj âˆ’yâˆ¥Î²
2 âˆ’
1
m(m âˆ’1)
m
X
j,k=1
kÌ¸=j
âˆ¥xj âˆ’xkâˆ¥Î²
2 .
In the main text (Section 5.3), we use Î² = 1, in which case we simplify notation S(1)
E
= SE.
Kernel score
For a positive deï¬nite kernel k(Â·, Â·) (see Gretton et al. 2012), the kernel
Scoring Rule for k is deï¬ned as (Gneiting and Raftery, 2007):
Sk(P, y) = E[k(X, Xâ€²)] âˆ’2 Â· E[k(X, y)],
X âŠ¥âŠ¥Xâ€² âˆ¼P.
The corresponding divergence is the squared Maximum Mean Discrepancy (MMD, Gretton
et al., 2012) relative to the kernel k:
Dk(P, Q) = E[k(X, Xâ€²)] + E[k(Y, Y â€²)] âˆ’2 Â· E[k(X, Y )],
for X âŠ¥âŠ¥Xâ€² âˆ¼P and Y âŠ¥âŠ¥Y â€² âˆ¼Q.
The Kernel Score is proper for the class of probability distributions for which E[k(X, Xâ€²)]
is ï¬nite (by Theorem 4 in Gneiting and Raftery, 2007). Additionally, it is strictly proper
under conditions which ensure that the MMD is a metric for probability distributions on
X (see for instance Gretton et al., 2012). These conditions are satisï¬ed, among others, by
the Gaussian kernel (which we use in this work):
k(x, y) = exp

âˆ’âˆ¥x âˆ’yâˆ¥2
2
2Î³2

;
(17)
there, Î³ is a scalar bandwidth, which is tuned as described in Appendix G.4.1. As for the
Energy Score, when the exact form of P is inaccessible and therefore Sk is impossible to
be evaluated exactly, we use samples {xj}m
j=1, xj âˆ¼P, and unbiasedly estimate the Kernel
Score Sk(P, y) with:
Ë†Sk({xj}m
j=1, y) =
1
m(m âˆ’1)
m
X
j,k=1
kÌ¸=j
k(xj, xk) âˆ’2
m
m
X
j=1
k(xj, y).
Appendix E. Computational practicalities
E.1 Computational cost of SM and SSM
For SM, as discussed in Section 3.2, exploiting automatic diï¬€erentiation libraries to compute
the second derivatives of the log density requires d times more backward derivative compu-
tations with respect to the ï¬rst derivatives. In fact, automatic diï¬€erentiation libraries are
able to compute derivatives of a scalar with respect to several variables at once. One single
call is therefore suï¬ƒcient to obtain âˆ‡x log pw(x). However, d additional calls are required to
40

Score Matched Neural Exponential Families for LFI
obtain the second derivatives
âˆ‚2
âˆ‚x2
i log pw(x), i = 1, . . . , d, which are the diagonal elements of
the Hessian matrix of log pw(x); each additional call computes the gradient of
âˆ‚
âˆ‚xk log pw(x)
with respect to all components of x, for some k âˆˆ[1, 2, . . . , d]. Algorithm 2 in Song et al.
(2020) gives a pseudocode implementation of this approach. Computational improvement
can be obtained by implementing custom code which performs the gradient computation
in the forward pass (i.e., along the computation of the neural network output for a given
input x). This avoids repeating some computations multiple times, which is done when
performing the backward step repeatedly; however, the implementation is tricky and needs
custom code for each diï¬€erent neural network type (we discuss how it can be done for a fully
connected neural network in Appendix E.1.1). Additionally, the computational speed-up
achievable in this way is limited with respect to what is oï¬€ered by, for instance, SSM.
SSM instead requires only two backward propagation steps independently on the input
size of the network. This is possible by exploiting the vector-Hessian product structure and
computing the linear products with v (which is independent on the input x, thus can be
swapped with gradient computation) after the gradient has been computed once, so that
you only ever require the gradient of a scalar quantity. See Algorithm 1 in Song et al. (2020)
for a precise description of how that can be done.
E.1.1 Forward computation of derivatives
In the standard score matching approach, the ï¬rst and second derivatives of Neural Network
outputs with respect to the inputs are required, namely:
âˆ‚fw(x)
âˆ‚xi
and
âˆ‚2fw(x)
âˆ‚x2
i
.
Neural network training is possible thanks to the use of autodiï¬€erentiation libraries, which
allow to keep trace of the diï¬€erent operations and to automatically compute the gradients
required for training. These libraries can be used to obtain the above derivatives.
However, as discussed previously, it is more eï¬ƒcient to compute the required derivatives
during the forward pass of training (i.e.
when the output of the Neural Network for a
given input is computed).
This requires additional coding eï¬€ort speciï¬c to the chosen
Neural Network architecture. For instance, Avrutskiy (2017) provide formulas to compute
derivatives of any order recursively for fully connected Neural Networks. For more complex
NN architectures, this approach is not viable as the coding eï¬€ort becomes substantial.
Additionally, with large d the improvement obtained by forward derivatives computation is
much smaller than what oï¬€ered, for instance, by SSM.
In the current work, the forward derivatives approach has been implemented for fully
connected Neural Networks and Partially Exchangeable Networks (Appendix E.3). The
computational advantage is evident for the computation of the second derivatives, as shown
below. This approach allowed us to apply SM to relatively high dimensional data spaces
(up to 100 dimensional for the MA(2) and AR(2) case), but not to the larger Lorenz96
model.
Forward computation of derivatives of fully connected NNs
We revisit here the
work in Avrutskiy (2017). Let us consider a fully connected Neural Networks with L layers,
where the weights and biases of the l-th layer are denoted by Wl and bl, l = 1, . . . , L. Let
41

Pacchiardi and Dutta
us denote by x the input of the Neural Network, and by zl the hidden values after the
l-th layer, before the activation function (denoted by Ïƒ) is applied. Speciï¬cally, the Neural
Network hidden values are determined by:
z1 = W1 Â· x + b1,
zl = Wl Â· Ïƒ(zlâˆ’1) + bl,
l = 2, . . . , L,
where the activation function is applied element wise. Similar recursive expressions can be
given for the ï¬rst derivatives:
âˆ‚z1
âˆ‚xi
= (W1)i,Â·,
âˆ‚zl
âˆ‚xi
= Wl Â·

Ïƒâ€²(zlâˆ’1) âŠ™âˆ‚zlâˆ’1
âˆ‚xi

,
l = 2, . . . , L,
where (W1)i,Â· denotes the i-th column of W1, and âŠ™denotes element wise multiplication.
Expressions for the second derivatives are instead:
âˆ‚2z1
âˆ‚x2
i
= 0,
âˆ‚2zl
âˆ‚x2
i
= Wl Â·
"
Ïƒâ€²â€²(zlâˆ’1) âŠ™
âˆ‚zlâˆ’1
âˆ‚xi
2
+ Ïƒâ€²(zlâˆ’1) âŠ™âˆ‚2zlâˆ’1
âˆ‚x2
i
#
,
l = 2, . . . , L,
where here 0 denotes a 0 vector with the same size as z1.
When instead we are interested in cross terms of the form
âˆ‚zl
âˆ‚xiâˆ‚xj , we can apply the
following:
âˆ‚z1
âˆ‚xiâˆ‚xj
= 0,
âˆ‚zl
âˆ‚xiâˆ‚xj
= WlÂ·

Ïƒâ€²â€²(zlâˆ’1) âŠ™âˆ‚zlâˆ’1
âˆ‚xi
âŠ™âˆ‚zlâˆ’1
âˆ‚xj
+ Ïƒâ€²(zlâˆ’1) âŠ™âˆ‚2zlâˆ’1
âˆ‚xiâˆ‚xj

, l = 2, . . . , L.
With respect to naively using auto-diï¬€erentiation libraries, computing the derivatives
in the forward step is much cheaper; speciï¬cally, for fully connected NNs, we found empir-
ically the ï¬rst to scale quadratically with the output size, while the second scales linearly
(Figure 8).
E.2 Batch normalization
The exponential family form used as pw depends on Î·w(Î¸)T fw(x); if you multiply fw(x)
with an invertible matrix A and multiply Î· with (AT )âˆ’1, the product does not change. In
order to remove this additional degeneracy, we use a Batch Normalization (BatchNorm)
layer (Ioï¬€e and Szegedy, 2015) to normalize the outputs of Î·w(Î¸). Essentially, BatchNorm
rescales the diï¬€erent features to have always the same range across diï¬€erent batches. More
speciï¬cally, BatchNorm performs the following operation on y:
Ëœy =
y âˆ’E[Y ]
p
V[Y ] + Ïµ
âˆ—Î³ + b,
where Ïµ is a small constant used for numerical stability, and Î³ and b are two (optionally
learnable) sets of constants with dimension equal to y (set to 1 and 0 respectively by default).
During training, the expectation E and variance V are estimated over the batch of training
samples fed to the Neural Network. In testing mode, BatchNorm rescales the features by
using a ï¬xed estimate of E and V; this estimate is obtained as a running mean over the
batches; let s(Y ) represent either the population expectation or variance. When the t-th
42

Score Matched Neural Exponential Families for LFI
5
10
15
20
Output size (ds)
0
100
200
300
400
500
600
Time (s)
Naive autodif
5
10
15
20
Output size (ds)
0.65
0.70
0.75
0.80
0.85
0.90
0.95
Time (s)
Forward computation
Figure 8: Computational complexity for second order derivatives of Neural Net-
work outputs versus size of the output; we compare the forward computation of derivatives
with naive autodiï¬€erentiation. Here, input size is ï¬xed to 100, and one single batch of 5000
samples is fed to the network. Computations are done on a CPU machine with 8 cores.
batch is fed through the network in training mode, the running mean estimate is updated
as follows:
Ë†snew(Y ) = (1 âˆ’p) Â· Ë†sold(Y ) + p Â· st(Y ),
where st represent the estimate on the current batch, Ë†sold and Ë†snew respectively the old and
updated running mean and p is a momentum constant which determines how quickly the
running mean changes (the smaller it is, the slower the change of the estimate).
For instance, if the training of a Neural Network is quite unstable, the running estimate
may not be a correct estimate of E and V, so that the test loss across training epochs may
be very spiky, until network training stabilized. To solve this issue, you can either increase
p (so that the running estimate â€œforgetsâ€ past information faster) or, alternatively, do a
forward pass of the training data set (without computing gradients) before evaluating the
test loss, so that the running estimate is more precisely estimated.
Across this work, we apply BatchNorm to y = Î·w(Î¸). Moreover, we do not learn the
translation parameters Î³, b, and rather we ï¬x them to be a vector of 1s and 0s.
E.3 Partially Exchangeable Networks
Partially Exchangeable Networks (PENs) were introduced in Wiqvist et al. (2019) as a
Neural Network architecture that satisï¬es the probabilistic invariance of Markovian models.
Speciï¬cally, let consider the case in which x = (x1, . . . , xd) comes from a Markovian
model of order r, i.e.:
p(x|Î¸) = p(x1|Î¸)p(x2|x1; Î¸)p(x3|x2, x1; Î¸)
d
Y
i=4
p(xi|x1, . . . , xiâˆ’1; Î¸)
= p(x1|Î¸)
d
Y
i=2
p(xi|xiâˆ’r, . . . , xiâˆ’1; Î¸).
43

Pacchiardi and Dutta
This deï¬nition is an extension of the standard Markovianity assumption (of order 1),
which corresponds to p(x|Î¸) = p(x1|Î¸) Qd
i=2 p(xi|xiâˆ’1; Î¸), and it means that each element of
x only depends on the last r elements. When r = 0, this corresponds to i.i.d. assumption.
When a model is r-Markovian, the probability density of an observation x is invariant
to r-block-switch transformation, which is deï¬ned as follows:
Deï¬nition 24 r-block-switch transformation (Wiqvist et al., 2019) Let xi:j and
xk:l be two non-overlapping blocks with xi:(i+r) = xk:(k+r) and x(jâˆ’r):j = x(lâˆ’r):l. Then,
denoting b = (i, j, k, l), with j âˆ’i â‰¥r and l âˆ’k â‰¥r:
x = x1:iâˆ’1 xi:j x(j+1):(kâˆ’1) xk:l x(l+1):M
T (r)
b
(x) = x1:iâˆ’1 xk:l x(j+1):(kâˆ’1) xi:j x(l+1):M.
Otherwise, if xi:(i+r) Ì¸= xk:(k+d) or x(jâˆ’r):j Ì¸= x(lâˆ’r):l, then T (r)
b
(x) = x.
For instance, let us consider the case in which the data space is X = {0, 1 . . . 9}16, and
r = 2. An example of the above transformation is the following:
x = 1 7 2 3 6 4 5 8 1 7 7 2 9 5 8 1
T (2)
(2,8,11,15)(x) = 1 7 2 9 5 8 1 7 7 2 3 6 4 5 8 1.
The authors of Wiqvist et al. (2019) provide a simple Neural Network structure which
is invariant to the r-block-switch transformation, motivated by the following theorem:
Theorem 25 ((Wiqvist et al., 2019)) Let f : X M â†’A r-block-switch invariant. If X
is countable, âˆƒÏ† : X r+1 â†’R and Ï : X r Ã— R â†’A such that:
âˆ€x âˆˆX M, f(x) = Ï
 
x1:r,
Mâˆ’r
X
i=1
Ï†(xi:(i+r))
!
.
(18)
In practice, Ï† and Ï are two independent Neural Networks (which we take to be fully
connected in our case), giving rise to a PEN of order r.
In Wiqvist et al. (2019), the authors show that the posterior mean of a Markovian
variable of order r needs to be invariant to the r-block-switch transformation. Therefore, this
motivates using a PEN for learning a summary statistics as in the approach by Fearnhead
and Prangle (2012). Here, we use PEN for parametrizing the statistics in the approximating
exponential family as well; this choice implies that the approximating family has the same
Markovianity property as the true distribution, as we discuss in the following.
E.3.1 Results for the exponential family
A deeper connection between r-Markovian probability models and r-block-switch transfor-
mation exists. We can in fact state the following result:
Lemma 26 A probability model p(x|Î¸) is r-Markovian
â‡â‡’
the function x 7â†’p(x|Î¸) is
r-block-switch invariant, i.e. p(x|Î¸) = p(T (r)
b
(x)|Î¸) âˆ€T (r)
b
.
44

Score Matched Neural Exponential Families for LFI
Proof The forward direction is straightforward and can be seen by considering the decom-
position of an r-Markovian model.
The converse direction can be shown by contradiction; assume in fact that x 7â†’p(x|Î¸) is
r-block-switch invariant but not Markovian. As it is not Markovian, âˆƒx = (xi, x2, . . . , xn)
for which xi:(i+r) = xk:(k+r) and x(jâˆ’r):j = x(lâˆ’r):l such that, deï¬ning b = (i, j, k, l),
p(x|Î¸) Ì¸= p(T (r)
b
(x)|Î¸).
This is however in contradiction with r-block-switch invariance,
which leads to our result.
In the case where the model we consider has a suï¬ƒcient statistic, we can write p(x|Î¸) =
h(x)g(t(x)|Î¸). We get therefore the following corollary, which can be seen by applying the
above result:
Corollary 27 Consider a distribution p(x|Î¸) = h(x)g(t(x)|Î¸); if the function h(x) and t(x)
are r-block-switch invariant, then p(x|Î¸) is r-Markovian.
Without any further assumptions, this result is not enough to say that the suï¬ƒcient
statistics t(x) for a Markovian model is r-block-switch invariant; in fact, the choice t(x) = x
always constitutes a suï¬ƒcient statistic; moreover, in the decomponsition p(x|Î¸) = h(x)g(t(x)|Î¸),
it may be that the function t(x) is not r-block-switch invariant but g(t(x)|Î¸) is, or otherwise
that the product h(x)g(t(x)|Î¸) is r-block-switch invariant even if the individual terms are
not. We can however get the following result:
Lemma 28 Consider a r-Markovian distribution p(x|Î¸) = h(x)g(t(x)|Î¸); if x 7â†’t(x) is not
an injection mapping, then x 7â†’h(x) is r-block-switch invariant.
Proof If x 7â†’t(x) is not an injection, âˆƒx, xâ€² such that t(x) = t(xâ€²). If the density is not
degenerate, moreover, âˆƒÎ¸ : p(x|Î¸), p(xâ€²|Î¸) > 0. Therefore, we consider the following ratio:
p(x|Î¸)
p(xâ€²|Î¸) = h(x)
h(xâ€²) Â· g(t(x)|Î¸)
g(t(xâ€²)|Î¸) = h(x)
h(xâ€²).
Now, the left hand side is r-block-switch invariant with respect to both x and xâ€² indepen-
dently, implying that h(x) is as well.
We remark that it does not seem possible in general to say anything about t(x), as in
fact it may be that the function t(x) is not r-block-switch invariant but g(t(x)|Î¸) is. In the
speciï¬c case of an exponential family, however, a more speciï¬c result can be obtained:
Lemma 29 Consider an exponential family distribution p(x|Î¸) = h(x) exp(Î·(Î¸)T f(x))/Z(Î¸)
which is r-Markovian distribution; if x 7â†’f(x) is not an injection mapping, then x 7â†’f(x)
is r-block-switch invariant.
Proof Without loss of generality, we consider the case in which all elements of Î·(Î¸) are not
constant with respect to Î¸; if this is not the case, in fact, you can redeï¬ne the exponential
family by incorporating the elements of f(x) corresponding to the constant ones of Î· in the
h(x) factor.
45

Pacchiardi and Dutta
Now, h(x) is r-block-switch invariant thanks to Lemma 28. Consider now the following
decomposition:
log p(x|Î¸) = log h(x) âˆ’log Z(Î¸) +
X
i
fi(x)Î·i(Î¸);
as that needs to be r-block-switch invariant for any Î¸, this can happen only if each of the
fi(x) elements are r-block-switch invariant.
Overall, these results imply that an exponential family in which f is parametrized with
a PEN of order r is r-Markovian. Moreover, provided that f is not an injection mapping, all
r-Markovian exponential families have f which satisfy the r-block-switch invariant property,
which is imposed by using a PEN network of order r.
E.3.2 Forward computation of derivatives for PENs
We give here the derivation for the forward computation of derivatives with PENs. If we
pick here Ï† and Ï to be fully connected Neural Networks, we can moreover apply the forward
computation of derivatives for them and we are able to compute the derivatives for PENs
at a much lower cost with respect to using automatic diï¬€erentiation libraries.
In Eq. (18), let us denote for brevity z = PMâˆ’r
i=1 Ï†(xi:(i+r)). We are interested now in
computing the derivative:
âˆ‚f
âˆ‚xj
=
âˆ‚
âˆ‚xj
Ï(x1:r, z),
where note that z depends in general on xi. Therefore, in computing the above, we need to
compute the derivative with respect to both arguments; let us denote by
âˆ‚â€²
âˆ‚xj the derivative
with respect to the ï¬rst argument. Then, we have:
âˆ‚f
âˆ‚xj
= âˆ‚â€²
âˆ‚xj
Ï(x1:r, z) Â· 1[j â‰¤r] + âˆ‚
âˆ‚z Ï(x1:r, z) Â· âˆ‚z
âˆ‚xj
,
where the second term is (note that z and Ï are multivariate, so that
âˆ‚â€²
âˆ‚xj Ï(x1:r, z) is a
Jacobian matrix):
âˆ‚z
âˆ‚xj
=
âˆ‚
âˆ‚xj
Mâˆ’r
X
i=1
Ï†(xi:(i+r)) =
j
X
i=jâˆ’r
iâ‰¥1
âˆ‚
âˆ‚xj
Ï†(xi:(i+r)),
where all other terms of the sum disappear as they do not contain xj.
Now, we are interested in obtaining the second derivative terms:
âˆ‚2f
âˆ‚x2
j
=
âˆ‚
âˆ‚xj
âˆ‚f
âˆ‚xj
=
âˆ‚
âˆ‚xj
"
âˆ‚â€²
âˆ‚xj
Ï(x1:r, z) Â· 1[j â‰¤r] +
X
k
âˆ‚
âˆ‚zk
Ï(x1:r, z)âˆ‚zk
âˆ‚xj
#
= âˆ‚â€²2
âˆ‚x2
j
Ï(x1:r, z) Â· 1[j â‰¤r] + 2
X
k
âˆ‚â€²
âˆ‚xj
âˆ‚
âˆ‚zk
Ï(x1:r, z) Â· âˆ‚zk
âˆ‚xj
Â· 1[j â‰¤r]
+
X
k,kâ€²
âˆ‚2
âˆ‚zkâˆ‚zkâ€² Ï(x1:r, z)âˆ‚zk
âˆ‚xj
âˆ‚zkâ€²
âˆ‚xj
+
X
k
âˆ‚
âˆ‚zk
Ï(x1:r, z)âˆ‚2zk
âˆ‚x2
j
;
46

Score Matched Neural Exponential Families for LFI
in the above expression, P
k runs over the elements of z and âˆ‚â€²2
âˆ‚x2
j denotes second derivative
with respect to the ï¬rst element.
Note that all terms appearing in the above formulas
contain ï¬rst and second derivatives of the Neural Networks Ï† and Ï with respect to one
single input, except for the terms highlighted in red. In order to compute that, obtaining
the full hessian matrix of Ï is required. We remark that the latter can be very large in case
the input dimension is large, therefore leading to memory overï¬‚ow issues.
E.4 Exchange MCMC
For convenience, we describe here the ExchangeMCMC algorithm by Murray et al. (2012).
We consider the task of sampling from a posterior distribution Ï€(Î¸|x). We can evaluate
an unnormalized version of the likelihood Ëœp(x|Î¸), and we denote the normalized version as
p(x|Î¸) = Ëœp(x|Î¸)/Z(Î¸), Z(Î¸) being an intractable normalizing constant. We want to build an
MCMC chain by using a proposal distribution q(Â·|Î¸; x) (which optionally depends on the
considered x as well). Usually, the standard Metropolis acceptance threshold for a proposal
Î¸â€² is deï¬ned as:
Î± = Ï€(Î¸â€²|x)q(Î¸|Î¸â€²; x)
Ï€(Î¸|x)q(Î¸â€²|Î¸; x) = Ëœp(x|Î¸â€²)q(Î¸|Î¸â€²; x)Ï€(Î¸â€²)
Ëœp(x|Î¸)q(Î¸â€²|Î¸; x)Ï€(Î¸) Â· Z(Î¸)
Z(Î¸â€²),
where the last factor cannot be evaluated, as we do not have access to the normalizing
constant.
The ExchangeMCMC algorithm proposed by Murray et al. (2012) bypasses this issue
by drawing an auxiliary observation xâ€² âˆ¼p(Â·|Î¸â€²) and deï¬ning the acceptance probability as:
Î± = p(x|Î¸â€²)q(Î¸|Î¸â€²; x)Ï€(Î¸â€²)
p(x|Î¸)q(Î¸â€²|Î¸; x)Ï€(Î¸) Â· p(xâ€²|Î¸)
p(xâ€²|Î¸â€²) = Ëœp(x|Î¸â€²)Ëœp(xâ€²|Î¸)q(Î¸|Î¸â€²; x)Ï€(Î¸â€²)
Ëœp(x|Î¸)Ëœp(xâ€²|Î¸â€²)q(Î¸â€²|Î¸; x)Ï€(Î¸) Â· 
Z(Î¸)

Z(Î¸â€²) Â· 
Z(Î¸â€²)

Z(Î¸) .
(19)
Here, all the normalizing constants cancel out, so that the acceptance threshold can
be evaluated explicitly, at the expense of drawing a simulation from the likelihood for each
MCMC step. Murray et al. (2012) showed that an MCMC chain using the above acceptance
rate targets the correct posterior Ï€(Î¸|x). The resulting algorithm is given in Algorithm 1:
Algorithm 1 Original exchangeMCMC algorithm (Murray et al., 2012).
Require: Initial Î¸, number of iterations T, proposal distribution q, observation x.
1: for i = 1 to T do
2:
Propose Î¸â€² âˆ¼q(Î¸â€²|Î¸; x)
3:
Generate auxiliary observation xâ€² âˆ¼p(Â·|Î¸â€²)
4:
Compute acceptance threshold Î± as in Eq. (19)
5:
With probability Î±, set Î¸ â†Î¸â€²
6: end for
Bridging.
When considering more closely the acceptance rate in Eq. (19), it can be
seen that it depends on two ratios:
p(x|Î¸â€²)
p(x|Î¸) represents how well the proposed parameter
value explains the observation with respect to the previous parameter value, while instead
p(xâ€²|Î¸)
p(xâ€²|Î¸â€²) measures how well the auxiliary variable (generated using Î¸â€²) can be explained with
47

Pacchiardi and Dutta
parameter Î¸. Therefore, even if the former is large and Î¸ would be a suitable parameter
value, Î± can still be small if the auxiliary random variable is not explained well by the
previous parameter value. This can lead to slow mixing of the chain; to improve on this,
Murray et al. (2012) proposed to sample a set of auxiliary variables (xâ€²
0, xâ€²
1, . . . , xâ€²
K) from
intermediate distributions in the following way12: consider a set of densities
Ëœpk(x|Î¸, Î¸â€²) = Ëœp(x|Î¸â€²)Î²k Ëœp(x|Î¸)1âˆ’Î²k,
Î²k = K âˆ’k + 1
K + 1
;
xâ€²
0 is generated from p(Â·|Î¸â€²) as before, and then each xâ€²
k is generated from R(Â·|xâ€²
kâˆ’1; Î¸, Î¸â€²),
which denotes a Metropolis-Hastings transition kernel starting from xâ€²
kâˆ’1 with stationary
density Ëœpk(Â·|Î¸, Î¸â€²). Then, the acceptance rate is modiï¬ed as follows:
Î± = Ëœp(x|Î¸â€²)q(Î¸|Î¸â€²; x)Ï€(Î¸â€²)
Ëœp(x|Î¸)q(Î¸â€²|Î¸; x)Ï€(Î¸) Â·
K
Y
k=0
Ëœpk+1(xâ€²
k|Î¸, Î¸â€²)
Ëœpk(xâ€²
k|Î¸, Î¸â€²) .
(20)
The overall algorithm is given in Algorithm 2. Note that K = 0 recovers the original
ExchangeMCMC. This procedure generally improves the acceptance rate as it basically
introduces a sequence of intermediate updates to the auxiliary data which by smoothening
out the diï¬€erence between the two distributions.
Algorithm 2 ExchangeMCMC algorithm with bridging (Murray et al., 2012).
Require: Initial Î¸, number of iterations T, proposal distribution q, number of bridging
steps K, observation x.
1: for i = 1 to T do
2:
Propose Î¸â€² âˆ¼q(Î¸â€²|Î¸; x)
3:
Generate auxiliary observation xâ€²
0 âˆ¼p(Â·|Î¸â€²)
4:
for k = 1 to K do
â–·Bridging steps
5:
Generate xâ€²
k âˆ¼R(Â·|xâ€²
kâˆ’1; Î¸, Î¸â€²)
6:
end for
7:
Compute acceptance threshold Î± as in Eq. (20)
8:
With probability Î±, set Î¸ â†Î¸â€²
9: end for
ExchangeMCMC without perfect simulations.
If, as in the setup considered across
this work, we are not able to sample from p(Â·|Î¸â€²) as it is required in the ExchangeMCMC
algorithm (line 3 in Alg. 1), Murray et al. (2012) suggested to run Tin steps of an MCMC
chain on x targeting p(Â·|Î¸â€²) at each step of ExchangeMCMC; if Tin is large enough, the
last sample can be considered as (approximately) drawn from p(Â·|Î¸â€²) itself and used in
place of the unavailable perfect simulation.
In practice, however, this only leads to an
approximate ExchangeMCMC algorithm, as at each iteration of the inner chain a ï¬nite
Tin is used, so that the inner chain would not perfectly converge to its target; for this
reason, even an inï¬nitely long outer chain would not target the right posterior for any ï¬nite
12. Diï¬€erently from the rest of the work, here subscripts do not denote vector components, but rather
diï¬€erent auxiliary variables.
48

Score Matched Neural Exponential Families for LFI
Tin. Nonetheless, this approach was shown empirically to work satisfactorily in Caimo and
Friel (2011); Everitt (2012); Liang (2010). Some theoretical guarantees (albeit under strong
conditions), are given in in Appendix B by Everitt (2012), which bounds the total variation
distance between target of approximate ExchangeMCMC with ï¬nite Tin and the target of
the exact one, and shows that they become equal when Tin â†’âˆ.
In Liang (2010), they argue that starting the inner chain from the observation value
improves convergence; we adapt this approach in our implementation (Algorithm 3).
Algorithm 3 ExchangeMCMC algorithm (Murray et al., 2012) with inner MCMC.
Require: Initial Î¸, number of iterations T and Tin, proposal distributions q and qx, obser-
vation x.
1: for i = 1 to T do
â–·Outer chain
2:
Propose Î¸â€² âˆ¼q(Î¸â€²|Î¸; x)
3:
Set xâ€² = x
â–·Start inner chain from the observation
4:
for j = 1 to Tin do
â–·Inner chain
5:
Propose xâ€²â€² âˆ¼qx(xâ€²â€²|xâ€²)
6:
With probability pw(xâ€²â€²|Î¸â€²)qx(xâ€²|xâ€²â€²)
pw(xâ€²|Î¸â€²)qx(xâ€²â€²|xâ€²) , set xâ€² = xâ€²â€²
7:
end for
8:
Compute acceptance threshold Î± as in Eq. (19)
â–·This uses last point of inner
MCMC
9:
With probability Î±, set Î¸ â†Î¸â€²
10: end for
Note that it is still possible to run bridging steps after the inner MCMC to sample from
p(Â·|Î¸â€²); the algorithm combining bridging and inner MCMC, which is used across this work,
is given in Algorithm 4).
Related algorithms.
Algorithms for sampling from doubly-intractable targets which are
suitable for parallel computing exist, for instance Caimo and Friel (2011) propose a parallel-
chain MCMC algorithm, while Everitt et al. (2017) build instead an SMC-type algorithm
which is also capable of recycling information from past simulations. However, in this work
we stick to using ExchangeMCMC, which turned out to be relatively cheap to use and easy
to implement.
Finally, we remark that Liang et al. (2016) proposed an algorithm which is inspired
from ExchangeMCMC and, in the case of impossible perfect sampling, still targets the right
invariant distribution. It works by considering a set of parallel chains targeting p(Â·|Î¸(i)) for
a ï¬xed set of {Î¸(i)} and iteratively updating those and the main chain over Î¸. The algorithm
relies on some assumptions which are probably satisï¬ed in practice (as discussed in Park
and Haran, 2018). It also requires some hand-tuning and needs to keep in memory a large
amount of data, which may hinder its applicability. For this reason, we do not investigate
using that here.
E.4.1 Implementation details
Acceptance rate tuning.
In our implementation of ExchangeMCMC, we discarded some
burn-in steps to make sure the chain forgets its initial state. During burn-in, moreover, after
49

Pacchiardi and Dutta
Algorithm 4 ExchangeMCMC algorithm (Murray et al., 2012) with inner MCMC and
bridging.
Require: Initial Î¸, number of iterations T and Tin, number of bridging steps K, proposal
distributions q and qx, observation x.
1: for i = 1 to T do
â–·Outer chain
2:
Propose Î¸â€² âˆ¼q(Î¸â€²|Î¸; x)
3:
Set xâ€² = x
â–·Start inner chain from the observation
4:
for j = 1 to Tin do
â–·Inner chain
5:
Propose xâ€²â€² âˆ¼qx(xâ€²â€²|xâ€²)
6:
With probability pw(xâ€²â€²|Î¸â€²)qx(xâ€²|xâ€²â€²)
pw(xâ€²|Î¸â€²)qx(xâ€²â€²|xâ€²) , set xâ€² = xâ€²â€²
7:
end for
8:
Set xâ€²
0 = xâ€²
9:
for k = 1 to K do
â–·Bridging steps
10:
Generate xâ€²
k âˆ¼R(Â·|xâ€²
kâˆ’1; Î¸, Î¸â€²)
11:
end for
12:
Compute acceptance threshold Î± as in Eq. (20)
13:
With probability Î±, set Î¸ â†Î¸â€²
14: end for
each window of 100 outer steps, we tuned the proposal sizes considering the acceptance rate
in the window, and increasing (respectively decreasing) if that is too large (small) with
respect to a chosen interval (see below). Notice that we applied this strategy independently
to the proposal size for the outer chain, the inner chain and the bridging step, when the
latter is used. When tuning the proposal size for the inner chain or bridging steps, we
considered the overall acceptance rate over each window of 100 outer steps. We remark how
this was only done during burn-in, so that the convergence properties of the chain were not
aï¬€ected.
For inner and outer steps as well as bridging, we found that a target acceptance rate
in the interval [0.2, 0.5] lead to good performance; this is consistent with the recommended
range for Metropolis-Hastings MCMC (Roberts et al., 1997).
MCMC on bounded space.
When the inner (or outer) MCMC chain is run on a
bounded domain, we apply the transformations discussed in Appendix C.8 to map it to
an unbounded domain, and therefore run the MCMC on that space. Notice that the Jaco-
bian factor arising from the transformations has therefore to be taken into account when
computing the acceptance rate.
Appendix F. Details on Neural Networks training
For all experiments, we used the Pytorch library (Paszke et al., 2019) to train NNs. In
Tables 3, 4, 5, 6 and 7 we report the Neural Network architectures used in the diï¬€erent
experiments. FC(n, m) denotes a fully connected layer with n inputs and m outputs. For the
time-series and Lorenz96 experiments, Ï†w and Ïw represent the two Neural Networks used
to build the PEN network fw as described in Eq. (18), and similarly Ï†Î² and ÏÎ² represent
the ones used in building sÎ². Finally, BN(p) represents a BatchNorm layer with momentum
50

Score Matched Neural Exponential Families for LFI
p (as described in Appendix E.2), with Î³, b, ï¬xed respectively to be vectors of 1s and 0s.
We remark that the momentum value does not impact on the training of the network, but it
modiï¬es the evaluation of the test loss, which we use for early stopping, as discussed below.
Network
fw
Î·w
sÎ²
Structure
FC(10,30)
FC(30,50)
FC(50,50)
FC(50,20)
FC(20,3)
FC(2,15)
FC(15,30)
FC(30,30)
FC(30,15)
FC(15,2)
BN(0.9)
FC(10,30)
FC(30,50)
FC(50,50)
FC(50,20)
FC(20,2)
Table 3: Architectures used for the exponential family models (Gaussian, Gamma
and Beta).
Network
fw
Î·w
sÎ²
Ï†w
Ïw
Ï†Î²
ÏÎ²
Structure
FC(3,50)
FC(50,50)
FC(50,50)
FC(30,20)
FC(22,50)
FC(50,50)
FC(50,3)
FC(2,15)
FC(15,30)
FC(30,30)
FC(30,15)
FC(15,2)
BN(0.9)
FC(3,50)
FC(50,50)
FC(50,30)
FC(30,20)
FC(22,50)
FC(50,50)
FC(50,2)
Table 4: Architectures used for the AR(2) model.
Network
fw
Î·w
sÎ²
Ï†w
Ïw
Ï†Î²
ÏÎ²
Structure
FC(11,50)
FC(50,50)
FC(50,30)
FC(30,20)
FC(30,50)
FC(50,50)
FC(50,3)
FC(2,15)
FC(15,30)
FC(30,30)
FC(30,15)
FC(15,2)
BN(0.9)
FC(11,50)
FC(50,50)
FC(50,30)
FC(30,20)
FC(30,50)
FC(50,50)
FC(50,2)
Table 5: Architectures used for the MA(2) model.
In all experiments, stochastic gradient descent with a batch size of 1000 samples is used
with Adam optimizer (Kingma and Ba, 2015), whose parameters are left to the default
values as implemented in Pytorch. Finally, we evaluate the test loss on a test set with same
size as the training set at intervals of Tcheck epochs and early-stop training if the test loss
increased with respect to the last evaluation. In order to have a better running estimate
51

Pacchiardi and Dutta
Network
fw
Î·w
sÎ²
Ï†w
Ïw
Ï†Î²
ÏÎ²
Structure
FC(16,50)
FC(50,100)
FC(100,50)
FC(50,20)
FC(28,40)
FC(40,90)
FC(90,35)
FC(35,5)
FC(4,30)
FC(30,50)
FC(50,50)
FC(50,30)
FC(30,4)
BN(0.9)
FC(16,50)
FC(50,100)
FC(100,50)
FC(50,20)
FC(28,40)
FC(40,90)
FC(90,35)
FC(35,4)
Table 6: Architectures used for the Lorenz96 model in the small setup.
Network
fw
Î·w
sÎ²
Ï†w
Ïw
Ï†Î²
ÏÎ²
Structure
FC(80,120)
FC(120,160)
FC(160,120)
FC(120,20)
FC(60,80)
FC(80,100)
FC(100,80)
FC(80,5)
FC(4,30)
FC(30,50)
FC(50,50)
FC(50,30)
FC(30,4)
BN(0.9)
FC(80,120)
FC(120,160)
FC(160,120)
FC(120,20)
FC(60,80)
FC(80,100)
FC(100,80)
FC(80,4)
Table 7: Architectures used for the Lorenz96 model in the large setup.
of the quantities of interest for the BatchNorm layer, before each test epoch we perform a
forward pass of the whole training data set, without computing gradients, as discussed in
Appendix E.2. We do not perform early stopping before epoch Tstart. The nets are trained
for a maximum of T training epochs with Ntrain training samples. In some experiments,
we used an exponential learning rate scheduler, which decreases progressively the learning
rate by multiplying it by a factor Î¶ < 1 at each epoch. We ï¬xed Î¶ = 0.99. The values of the
parameters are reported in Table 8 for all the diï¬€erent models and setups, together with
the values of the learning rates (lr) used and whether the scheduler was used or not (Sch).
We remark that, in order to make the comparison fair, the learning rates for the FP
experiments were chosen by cross validation with several learning rates choices. Similarly,
we hand-picked the best learning rate values for training the Neural Networks with SM
and SSM, even if in that case it was not possible, due to computational constraints, to
perform a full search on a large set of values of learning rates for all experiments. In fact,
the computational cost of training the exponential family approximation with SM or SSM
is larger than the cost of learning the summary statistics with the FP approach. Moreover,
in this scenario we have two independent learning rates values to tune, as we train two
networks simultaneously. For this reason, we did not spend too much time trying diï¬€erent
lr values for models for which we already got satisfactory results.
52

Score Matched Neural Exponential Families for LFI
Model
Setup
Learning rates
Ntrain
T
Tstart
Tcheck
Sch
Gaussian
SM
lr(fw) = 0.0003, lr(Î·w) = 0.003
104
500
150
10
Yes
SSM
lr(fw) = 0.001, lr(Î·w) = 0.001
104
500
200
10
Yes
FP
lr(sÎ²) = 0.01
104
1000
300
25
No
Gamma
SM
lr(fw) = 0.001, lr(Î·w) = 0.001
104
500
200
10
Yes
SSM
lr(fw) = 0.001, lr(Î·w) = 0.001
104
500
200
10
Yes
FP
lr(sÎ²) = 0.001
104
1000
300
25
Yes
Beta
SM
lr(fw) = 0.001, lr(Î·w) = 0.001
104
500
200
10
Yes
SSM
lr(fw) = 0.001, lr(Î·w) = 0.001
104
500
200
10
Yes
FP
lr(sÎ²) = 0.01
104
1000
250
50
Yes
AR(2)
SM
lr(fw) = 0.001, lr(Î·w) = 0.001
104
500
100
25
Yes
SSM
lr(fw) = 0.001, lr(Î·w) = 0.001
104
500
100
25
Yes
FP
lr(sÎ²) = 0.001
104
1000
500
25
Yes
MA(2)
SM
lr(fw) = 0.001, lr(Î·w) = 0.001
104
500
100
25
Yes
SSM
lr(fw) = 0.001, lr(Î·w) = 0.001
104
500
100
25
Yes
FP
lr(sÎ²) = 0.001
104
1000
500
25
Yes
Lorenz96
Small
SSM
lr(fw) = 0.001, lr(Î·w) = 0.001
104
1000
500
50
Yes
FP
lr(sÎ²) = 0.001
104
1000
200
25
Yes
Lorenz96
Large
SSM
lr(fw) = 0.001, lr(Î·w) = 0.001
104
1000
500
50
Yes
FP
lr(sÎ²) = 0.001
104
1000
200
25
Yes
Table 8: Hyperparameter values for NN training: â€œFPâ€ denotes the least squares
regression by Fearnhead and Prangle (2012); Jiang et al. (2017) used for the ABC-FP
experiment, while â€œSMâ€ and â€œSSMâ€ denote respectively exponential family trained with
Score Matching and Sliced Score Matching.
53

Pacchiardi and Dutta
Appendix G. Additional experimental results
G.1 Exponential family models
G.1.1 Mean Correlation Coefficients for Neural Networks trained with
SSM
We report in Table 9 the weak and strong Mean Correlation Coeï¬ƒcient (MCC, Appendix B.1.1)
for Neural Networks trained with SSM, for the exponential family models. MCC is a metric
in [0, 1], with 1 denoting perfect recovery up to a linear transformation (weak) or permu-
tation (strong). As it can be seen in Table 9, our method leads to values quite close to 1,
particularly for the weak MCC, implying that our method is able to recover the embeddings
up to a linear transformation, as expected.
Model
MCC weak in
MCC weak out
MCC strong in
MCC strong out
Beta (statistics)
0.990
0.986
0.982
0.979
Beta (nat. par.)
0.987
0.989
0.983
0.985
Gamma (statistics)
0.939
0.928
0.723
0.709
Gamma (nat. par.)
0.977
0.977
0.792
0.794
Gaussian (statistics)
0.874
0.844
0.623
0.638
Gaussian (nat. par.)
0.861
0.862
0.581
0.543
Table 9: MCC for exponential family models between exact embeddings and
those learned with SSM. We show weak and strong MCC values; MCC is between 0 and
1 and measures how well an embedding is recovered up to permutation and rescaling of its
components (strong) or linear transformation (weak); the larger, the better. â€œinâ€ denotes
MCC on training data used to ï¬nd the best transformation, while â€œoutâ€ denote MCC on
test data. We used 500 samples in both training and test data sets.!
G.1.2 Learned and exact embeddings for the exponential family models
We compare here the exact and learned suï¬ƒcient statistics and natural parameters of the
exponential family models; precisely, we draw samples (x(j), Î¸(j)) and then plot the learned
statistics fw(x(j)) versus the exact one, and similarly for the natural parameters. Figure 9
reports the results for Neural Networks trained with SM, while Figure 10 reports the results
for Neural Networks trained with SSM.
54

Score Matched Neural Exponential Families for LFI
980
960
940
920
900
First learned statistics
10
0
10
First true statistics
600
580
560
540
520
500
480
Second learned statistics
0
100
200
300
400
Second true statistics
(a) Statistics Gaussian
35.0
32.5
30.0
27.5
25.0
22.5
First learned statistics
1
0
1
2
First true statistics
110
105
100
Second learned statistics
2.5
5.0
7.5
10.0
Second true statistics
(b) Statistics Gamma
162.5
160.0
157.5
155.0
152.5
150.0
First learned statistics
2
1
First true statistics
160
155
150
145
140
Second learned statistics
2
1
Second true statistics
(c) Statistics Beta
0
1
2
3
First learned parameters
5
0
5
First true parameters
4
2
0
2
4
Second learned parameters
0.5
0.4
0.3
0.2
0.1
Second true parameters
(d) Parameters Gaussian
2
1
0
1
2
First learned parameters
0.0
0.5
1.0
1.5
2.0
First true parameters
2
1
0
1
2
Second learned parameters
1.0
0.8
0.6
0.4
Second true parameters
(e) Parameters Gamma
2
1
0
1
2
First learned parameters
1.0
1.5
2.0
2.5
3.0
First true parameters
2
1
0
1
2
Second learned parameters
1.0
1.5
2.0
2.5
3.0
Second true parameters
(f) Parameters Beta
Figure 9: Learned and exact embeddings for exponential family models obtained
with SM. Each point represents a diï¬€erent x (for the statistics) or Î¸ (for the natural
parameters); 1000 of each were used here.
55

Pacchiardi and Dutta
1540
1520
1500
1480
1460
1440
First learned statistics
10
0
10
First true statistics
240
260
280
300
320
Second learned statistics
0
100
200
300
400
Second true statistics
(a) Statistics Gaussian
254
252
250
248
246
First learned statistics
1
0
1
2
First true statistics
68
66
64
62
60
58
56
54
Second learned statistics
2
4
6
8
10
Second true statistics
(b) Statistics Gamma
298
296
294
292
290
288
286
284
First learned statistics
2.5
2.0
1.5
1.0
0.5
First true statistics
42
44
46
48
50
52
54
Second learned statistics
2.5
2.0
1.5
1.0
0.5
Second true statistics
(c) Statistics Beta
1.0
0.5
0.0
0.5
1.0
1.5
2.0
2.5
3.0
First learned parameters
7.5
5.0
2.5
0.0
2.5
5.0
7.5
First true parameters
3.0
2.5
2.0
1.5
1.0
0.5
0.0
0.5
1.0
Second learned parameters
0.5
0.4
0.3
0.2
0.1
Second true parameters
(d) Parameters Gaussian
2.0
1.5
1.0
0.5
0.0
0.5
1.0
1.5
2.0
First learned parameters
0.0
0.5
1.0
1.5
2.0
First true parameters
2
1
0
1
2
Second learned parameters
1.0
0.8
0.6
0.4
Second true parameters
(e) Parameters Gamma
2.0
1.5
1.0
0.5
0.0
0.5
1.0
1.5
2.0
First learned parameters
1.0
1.5
2.0
2.5
3.0
First true parameters
1.5
1.0
0.5
0.0
0.5
1.0
1.5
2.0
Second learned parameters
1.0
1.5
2.0
2.5
3.0
Second true parameters
(f) Parameters Beta
Figure 10: Learned and exact embeddings for exponential family models obtained
with SSM. Each point represents a diï¬€erent x (for the statistics) or Î¸ (for the natural
parameters); 1000 of each were used here.
56

Score Matched Neural Exponential Families for LFI
10
30
100
200
Inner MCMC steps
0.0
0.2
0.4
0.6
0.8
Wasserstein distance
Beta
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
RMSE post mean
10
30
100
200
Inner MCMC steps
0.0
0.2
0.4
0.6
0.8
Wasserstein distance
Gamma
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
RMSE post mean
10
30
100
200
Inner MCMC steps
0
2
4
6
8
10
Wasserstein distance
Gaussian
0
2
4
6
8
10
RMSE post mean
(a) Exc-SM
10
30
100
200
Inner MCMC steps
0.0
0.2
0.4
0.6
0.8
Wasserstein distance
Beta
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
RMSE post mean
10
30
100
200
Inner MCMC steps
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Wasserstein distance
Gamma
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
RMSE post mean
10
30
100
200
Inner MCMC steps
0
1
2
3
4
5
6
7
Wasserstein distance
Gaussian
0
1
2
3
4
5
RMSE post mean
(b) Exc-SSM
Figure 11: Performance of Exc-SM with diï¬€erent number of inner MCMC steps,
for exponential family models.
Wasserstein distance from the exact posterior and
RMSE between exact and approximate posterior means are reported for 100 observations
using boxplots. Boxes span from 1st to 3rd quartile, whiskers span 95% probability density
region and horizontal line denotes median. The numerical values are not comparable across
examples, as they depend on the range of parameters.
G.1.3 Performance of Exc-SM and Exc-SSM
We study here the performance of Exc-SM and Exc-SSM with diï¬€erent numbers of inner
steps in the ExchangeMCMC algorithm (Algorithm 4) for the Exponential family models.
Speciï¬cally, we run the inference with 10, 30, 100 and 200 inner MCMC steps, and we
evaluate the performance in these 4 cases (Figure 11); considering the diï¬€erent models, we
observe that the performance with 30 steps is almost equivalent to the one with 100 and
200, albeit being faster (see the computational time in Table 10). In the main text, we
therefore present results using 30 inner MCMC steps.
Inner MCMC steps
10
30
100
200
Time (minutes)
â‰ˆ2
â‰ˆ4
â‰ˆ16
â‰ˆ28
Table 10: Approximate computational time of Exc-SM with diï¬€erent number of
inner MCMC steps for the exponential family models. These values were obtained
by running on a single core.
57

Pacchiardi and Dutta
Inner MCMC steps
10
30
100
200
Time (minutes)
â‰ˆ2
â‰ˆ5
â‰ˆ17
â‰ˆ29
Table 11: Approximate computational time of Exc-SM with diï¬€erent number of
inner MCMC steps, for the AR(2) and MA(2) models. These values were obtained
by running on a single core.
10
30
100
200
Inner MCMC steps
0.0
0.1
0.2
0.3
0.4
0.5
Wasserstein distance
AR(2)
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
RMSE post mean
10
30
100
200
Inner MCMC steps
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Wasserstein distance
MA(2)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
RMSE post mean
(a) Exc-SM
10
30
100
200
Inner MCMC steps
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Wasserstein distance
AR(2)
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
RMSE post mean
10
30
100
200
Inner MCMC steps
0.0
0.1
0.2
0.3
0.4
0.5
Wasserstein distance
MA(2)
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
RMSE post mean
(b) Exc-SSM
Figure 12: Performance of Exc-SM with diï¬€erent number of inner MCMC steps,
for AR(2) and MA(2) models.
Wasserstein distance from the exact posterior and
RMSE between exact and approximate posterior means are reported for 100 observations
using boxplots. Boxes span from 1st to 3rd quartile, whiskers span 95% probability density
region and horizontal line denotes median.
G.2 AR(2) and MA(2) models
We study here the performance of Exc-SM and Exc-SSM with diï¬€erent numbers of inner
steps in the ExchangeMCMC algorithm (Algorithm 4) for the AR(2) and MA(2) models.
Speciï¬cally, we run the inference with 10, 30, 100 and 200 inner MCMC steps, and we
evaluate the performance in these 4 cases (Figure 12); considering the diï¬€erent models, we
observe that the performance with 30 steps is almost equivalent to the one with 100 and
200, albeit being faster (see the computational time in Table 11). In the main text, we
therefore present results using 30 inner MCMC steps.
G.3 Simulation cost to reach equivalent performance as Exc-SM
Above, we have showed that Exc-SM and Exc-SSM are competitive with the other ap-
proaches, even if they require no additional model simulations.
Here, we quantify how
many model simulations are needed for the other techniques to reach the same performance
of Exc-SM in the exponential family and time-series models. The same analysis could be
done with respect to Exc-SSM but, as its performance is generally close to the one achieved
by Exc-SM, we avoid repeating it.
We compute therefore the performance of ABC-SM, ABC-SSM, ABC-FP, PMC-SL,
PMC-RE at each iteration for all 100 observations and ï¬nd when their median performance
58

Score Matched Neural Exponential Families for LFI
(as quantiï¬es by the Wasserstein distance with respect to the true posterior) becomes com-
parable or better than the median one achieved by Exc-SM. The number of required sim-
ulations are reported in Table 12. Notice that some techniques are not able to outperform
Exc-SM for some models; we highlight that by a dash in the Table. SL and RE reach
similar performance to Exc-SM with one single iteration, when they are able to do so; in
fact, we found empirically that the performance of them does not signiï¬cantly improve with
iterations. Still, we remark that one single iteration of SL and RE requires a very large
number of model simulations.
In Table 12, we also give the number of simulations required for the preliminary NN
training in the methods which use one; further, we compute the overall cost of inference in
terms of model simulations (taking into account both NN training and inference) for diï¬€erent
number of observations; we remark that, as discussed previously, our method requires no
additional model simulations to perform inference after the NNs have been trained. From
Table 12, it can be seen that, for all models, ABC-SM, ABC-FP, PMC-RE and PMC-SL
require a number of simulations larger than the one needed to train the NNs in Exc-SM to
reach the performance achieved by Exc-SM, which makes the latter an interesting option
for models in which simulations are very expensive.
In Figures 13 and 14 we represent the performance attained by ABC-FP, ABC-SM,
ABC-SSM, PMC-SL and PMC-RE at each iteration of the iterative algorithm. On the
horizontal axis of all plots, we report the number of model simulations corresponding to the
iteration of the algorithm.
G.4 Validation with Scoring Rules for Lorenz96 model
Recall that the Lorenz96 model (Section 5.3) is a multivariate time-series model. Therefore,
we use the Scoring Rules (on which more details are given in Appendix D) to evaluate the
predictive performance at each timestep separately.
First, let us recall the deï¬nition of the posterior predictive density:
p(x|x0) =
Z
p(x|Î¸)Ï€(Î¸|x0),
whose corresponding posterior predictive distribution we denote as P(Â·|x0). In the above
expression, Ï€(Î¸|x0) may represent the posterior obtained with any of the considered methods
(Exc-SM, Exc-SSM, ABC-FP, ABC-SM, ABC-SSM).
Let P (t)(Â·|x0) denote the posterior predictive distribution at time t conditioned on an
observation x0, and let x0,(t) denote the t-th timestep of the observation. Then, we are
interested in:
SE(P (t)(Â·|x0), x0,(t))
and
Sk(P (t)(Â·|x0), x0,(t));
we estimate these using samples from P (t)(Â·|x0) with the unbiased estimators discussed
in Appendix D. In the Kernel Score, the bandwidth of the Gaussian kernel is set from
simulations as described in Appendix G.4.1.
In this way, we obtain a score for an observation x0 at each timestep of the model. This
procedure is repeated for 100 diï¬€erent observations; the scores at each timestep are reported
in Figure 15, while the summed scores over timesteps were reported in Figure 7 in the main
text. In both Figures, we report the median value and various quantiles over the considered
59

Pacchiardi and Dutta
Beta
Exc-SM
ABC-SM
ABC-SSM
ABC-FP
PMC-SL
PMC-RE
NN training
2 Â· 104
2 Â· 104
2 Â· 104
2 Â· 104
-
-
Inference
0
2.5 Â· 104
1.8 Â· 104
-
1 Â· 105
1 Â· 106
Total 1 obs
2 Â· 104
4.5 Â· 104
3.8 Â· 104
-
1 Â· 105
1 Â· 106
Total 3 obs
2 Â· 104
9.5 Â· 104
7.4 Â· 104
-
3 Â· 105
3 Â· 106
Total 100 obs
2 Â· 104
2.52 Â· 106
1.82 Â· 106
-
1 Â· 107
1 Â· 108
Gamma
Exc-SM
ABC-SM
ABC-SSM
ABC-FP
PMC-SL
PMC-RE
NN training
2 Â· 104
2 Â· 104
2 Â· 104
2 Â· 104
-
-
Inference
0
4.7 Â· 104
2.9 Â· 104
-
1 Â· 105
1 Â· 106
Total 1 obs
2 Â· 104
6.7 Â· 104
4.9 Â· 104
-
1 Â· 105
1 Â· 106
Total 3 obs
2 Â· 104
1.61 Â· 105
1.07 Â· 105
-
3 Â· 105
3 Â· 106
Total 100 obs
2 Â· 104
4.72 Â· 106
2.92 Â· 106
-
1 Â· 107
1 Â· 108
Gaussian
Exc-SM
ABC-SM
ABC-SSM
ABC-FP
PMC-SL
PMC-RE
NN training
2 Â· 104
2 Â· 104
2 Â· 104
2 Â· 104
-
-
Inference
0
2.7 Â· 104
2.6 Â· 104
-
-
1 Â· 106
Total 1 obs
2 Â· 104
4.7 Â· 104
4.6 Â· 104
-
-
1 Â· 106
Total 3 obs
2 Â· 104
1.01 Â· 105
9.8 Â· 104
-
-
3 Â· 106
Total 100 obs
2 Â· 104
2.72 Â· 106
2.62 Â· 106
-
-
1 Â· 108
AR2
Exc-SM
ABC-SM
ABC-SSM
ABC-FP
PMC-SL
PMC-RE
NN training
2 Â· 104
2 Â· 104
2 Â· 104
2 Â· 104
-
-
Inference
0
3.2 Â· 104
3.1 Â· 104
2.3 Â· 104
1 Â· 105
-
Total 1 obs
2 Â· 104
5.2 Â· 104
5.1 Â· 104
4.3 Â· 104
1 Â· 105
-
Total 3 obs
2 Â· 104
1.16 Â· 105
1.13 Â· 105
8.9 Â· 104
3 Â· 105
-
Total 100 obs
2 Â· 104
3.22 Â· 106
3.12 Â· 106
2.32 Â· 106
1 Â· 107
-
MA2
Exc-SM
ABC-SM
ABC-SSM
ABC-FP
PMC-SL
PMC-RE
NN training
2 Â· 104
2 Â· 104
2 Â· 104
2 Â· 104
-
-
Inference
0
3.1 Â· 104
2.1 Â· 104
2.0 Â· 104
1 Â· 105
-
Total 1 obs
2 Â· 104
5.1 Â· 104
4.1 Â· 104
4.0 Â· 104
1 Â· 105
-
Total 3 obs
2 Â· 104
1.13 Â· 105
8.3 Â· 104
8.0 Â· 104
3 Â· 105
-
Total 100 obs
2 Â· 104
3.12 Â· 106
2.12 Â· 106
2.02 Â· 106
1 Â· 107
-
Table 12: Model simulations needed for the diï¬€erent techniques, for both NN train-
ing and inference; for ABC-FP, ABC-SM, ABC-SSM, PMC-SL and PMC-RE, we report
simulations needed to obtain performance at least as good as Exc-SM; in case the approach
does not reach the same performance as Exc-SM, we denote that by a dash. Notice that
PMC-SL and PMC-RE do not require NN training before performing inference. We also
show the total number of simulations needed to apply the diï¬€erent approaches on 1, 3 and
100 observations, by taking into account NN training and inference steps.
60

Score Matched Neural Exponential Families for LFI
20
40
60
80
100
Number of simulations (Ã—1000)
0.0
0.2
0.4
0.6
0.8
1.0
Wasserstein distance
Beta
ABC-FP
ABC-SM
ABC-SSM
Exc-SM
20
40
60
80
100
Number of simulations (Ã—1000)
0.0
0.2
0.4
0.6
0.8
1.0
Wasserstein distance
Gamma
ABC-FP
ABC-SM
ABC-SSM
Exc-SM
20
40
60
80
100
Number of simulations (Ã—1000)
0
2
4
6
8
10
Wasserstein distance
Gaussian
ABC-FP
ABC-SM
ABC-SSM
Exc-SM
200
400
600
800
1000
Number of simulations (Ã—1000)
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Wasserstein distance
Beta
PMC-SL
Exc-SM
200
400
600
800
1000
Number of simulations (Ã—1000)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Wasserstein distance
Gamma
PMC-SL
Exc-SM
200
400
600
800
1000
Number of simulations (Ã—1000)
0
1
2
3
4
5
Wasserstein distance
Gaussian
PMC-SL
Exc-SM
2000
4000
6000
8000
10000
Number of simulations (Ã—1000)
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Wasserstein distance
Beta
PMC-RE
Exc-SM
2000
4000
6000
8000
10000
Number of simulations (Ã—1000)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Wasserstein distance
Gamma
PMC-RE
Exc-SM
2000
4000
6000
8000
10000
Number of simulations (Ã—1000)
0.0
0.5
1.0
1.5
2.0
2.5
3.0
Wasserstein distance
Gaussian
PMC-RE
Exc-SM
Figure 13: Wasserstein distance between approximate and exact posterior at dif-
ferent iterations of the sequential algorithms for the exponential family models,
for 100 diï¬€erent observations. The solid line denotes median, while colored regions denote
95% probability density region; an horizontal line denoting the value obtained with Exc-SM
is also represented, 95% probability density region denoted by dotted horizontal lines. The
horizontal axis reports the number of model simulations corresponding to the iteration of
the diï¬€erent algorithms.
61

Pacchiardi and Dutta
20
40
60
80
100
Number of simulations (Ã—1000)
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Wasserstein distance
AR(2)
ABC-FP
ABC-SM
ABC-SSM
Exc-SM
20
40
60
80
100
Number of simulations (Ã—1000)
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Wasserstein distance
MA(2)
ABC-FP
ABC-SM
ABC-SSM
Exc-SM
200
400
600
800
1000
Number of simulations (Ã—1000)
0.00
0.05
0.10
0.15
0.20
0.25
Wasserstein distance
AR(2)
PMC-SL
Exc-SM
200
400
600
800
1000
Number of simulations (Ã—1000)
0.0
0.1
0.2
0.3
0.4
0.5
Wasserstein distance
MA(2)
PMC-SL
Exc-SM
2000
4000
6000
8000
10000
Number of simulations (Ã—1000)
0.0
0.2
0.4
0.6
0.8
Wasserstein distance
AR(2)
PMC-RE
Exc-SM
2000
4000
6000
8000
10000
Number of simulations (Ã—1000)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Wasserstein distance
MA(2)
PMC-RE
Exc-SM
Figure 14: Wasserstein distance between approximate and exact posterior at dif-
ferent iterations of the sequential algorithms for the AR(2) and MA(2) models,
for 100 diï¬€erent observations. The solid line denotes median, while colored regions denote
95% probability density region; an horizontal line denoting the value obtained with Exc-SM
is also represented, 95% probability density region denoted by dotted horizontal lines. The
horizontal axis reports the number of model simulations corresponding to the iteration of
the diï¬€erent algorithms.
62

Score Matched Neural Exponential Families for LFI
100 observations. It can be seen that ABC-FP is slightly outperformed by our proposed
methods for both the large and small Lorenz96 conï¬guration. Additionally, notice how, in
the small conï¬guration, the Energy score has an increasing trend over t, while the Kernel
one instead decreases.
0
0.5
1
1.5
t
100
200
300
400
500
Energy Score
Small Lorenz96
ABC-FP
ABC-SSM
Exc-SSM
0
1
2
3
4
t
1000
1250
1500
1750
2000
2250
2500
2750
Energy Score
Large Lorenz96
ABC-FP
ABC-SSM
0
0.5
1
1.5
t
0
1
2
3
4
5
6
7
8
Kernel Score
Small Lorenz96
ABC-FP
ABC-SSM
Exc-SSM
0
1
2
3
4
t
0
5
10
15
20
25
30
35
40
Kernel Score
Large Lorenz96
ABC-FP
ABC-SSM
Figure 15: Posterior predictive performance of the diï¬€erent methods at each
timestep according to the Kernel and Energy Scores; the smaller, the better. Sam-
ples from the posterior predictive were obtained for 100 observations, and both Scoring
Rules estimated. The solid lines denote medians over the 100 observations, while colored
regions denote 95% probability density region.
G.4.1 Setting Î³ in the kernel Scoring Rule
In the kernel score Sk, the Gaussian kernel in Eq. (17) is used across this work. There,
the kernel bandwidth Î³ is a free parameter. In order to ensure comparability between the
Scoring Rule values for diï¬€erent observations and inference methods, the value of Î³ needs
to be ï¬xed independently on both.
Inspired by Park et al. (2016), we exploit an empirical procedure to set Î³ for the speciï¬c
case of multivariate time-series models (of which our Lorenz96 model is an instance, see
Section 5.3). Speciï¬cally, we use the following procedure:
1. Draw a set of parameter values Î¸j âˆ¼Ï€(Î¸) and simulations xj âˆ¼p(Â·|Î¸j), for j =
1, . . . , m.
2. Estimate the median of {||x(t)
j
âˆ’x(t)
k ||2}m
jk and call it Ë†Î³(t), for all values of t.
3. Set the estimate for Î³ as the median of Ë†Î³(t) over all considered timesteps t.
63

Pacchiardi and Dutta
Empirically, we use m = 1000. Note that the above strategy uses medians rather than
means as those are more robust to outliers in the estimates. With this method, we obtain
Î³ â‰ˆ1.54 for the small version of the Lorenz96 model and Î³ = 6.38 for the large one.
References
Simon Aeschbacher, Mark A Beaumont, and Andreas Futschik.
A novel approach for
choosing summary statistics in approximate Bayesian computation. Genetics, 192(3):
1027â€“1047, 2012.
Mattias ËšAkesson, Prashant Singh, Fredrik Wrede, and Andreas Hellander. Convolutional
neural networks as summary statistics for approximate Bayesian computation.
arXiv
preprint arXiv:2001.11760, 2020.
Carlo Albert, R. KÂ¨unsch Hans, and Andreas Scheidegger. A simulated annealing approach
to approximate Bayesian computations. Statistics and Computing, 25:1217â€“1232, 2015.
Pierre Alquier, Nial Friel, Richard Everitt, and Aidan Boland. Noisy Monte Carlo: Conver-
gence of Markov chains with approximate transition kernels. Statistics and Computing,
26(1-2):29â€“47, 2016.
Ziwen An, Leah F South, David J Nott, and Christopher C Drovandi. Accelerating Bayesian
synthetic likelihood with the graphical lasso. Journal of Computational and Graphical
Statistics, 28(2):471â€“475, 2019.
Ziwen An, David J Nott, and Christopher Drovandi. Robust Bayesian synthetic likelihood
via a semi-parametric approach. Statistics and Computing, 30(3):543â€“557, 2020.
Michael Arbel and Arthur Gretton. Kernel conditional exponential family. In International
Conference on Artiï¬cial Intelligence and Statistics, pages 1337â€“1346. PMLR, 2018.
V.I. Avrutskiy.
Backpropagation generalized for output derivatives.
arXiv preprint
arXiv:1712.04185, 2017.
Mark A Beaumont. Approximate Bayesian computation in evolution and ecology. Annual
review of ecology, evolution, and systematics, 41:379â€“406, 2010.
Michael GB Blum, Maria Antonieta Nunes, Dennis Prangle, and Scott A Sisson. A com-
parative review of dimension reduction methods in approximate Bayesian computation.
Statistical Science, 28(2):189â€“208, 2013.
Alberto Caimo and Nial Friel. Bayesian inference for exponential random graph models.
Social Networks, 33(1):41â€“55, 2011.
Olivier CappÂ´e, Arnaud Guillin, Jean-Michel Marin, and Christian P Robert. Population
Monte Carlo. Journal of Computational and Graphical Statistics, 13(4):907â€“929, 2004.
Bob Carpenter, Andrew Gelman, Matthew D Hoï¬€man, Daniel Lee, Ben Goodrich, Michael
Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. Stan: A prob-
abilistic programming language. Journal of statistical software, 76(1), 2017.
64

Score Matched Neural Exponential Families for LFI
Yanzhi Chen, Dinghuai Zhang, Michael U Gutmann, Aaron Courville, and Zhanxing Zhu.
Neural approximate suï¬ƒcient statistics for implicit models. In Ninth International Con-
ference on Learning Representations 2021, 2021.
Badr-Eddine ChÂ´erief-Abdellatif and Pierre Alquier. MMD-Bayes: Robust Bayesian esti-
mation via maximum mean discrepancy.
In Symposium on Advances in Approximate
Bayesian Inference, pages 1â€“21. PMLR, 2020.
Bo Dai, Hanjun Dai, Arthur Gretton, Le Song, Dale Schuurmans, and Niao He. Kernel
exponential family estimation via doubly dual embedding. In The 22nd International
Conference on Artiï¬cial Intelligence and Statistics, pages 2321â€“2330. PMLR, 2019a.
Bo Dai, Zhen Liu, Hanjun Dai, Niao He, Arthur Gretton, Le Song, and Dale Schuurmans.
Exponential family estimation via adversarial dynamics embedding. In Advances in Neu-
ral Information Processing Systems, pages 10979â€“10990, 2019b.
Alexander Philip Dawid and Monica Musio. Theory and applications of proper scoring
rules. Metron, 72(2):169â€“183, 2014.
Pierre Del Moral, Arnaud Doucet, and Ajay Jasra. An adaptive sequential Monte Carlo
method for approximate Bayesian computation. Statistics and Computing, 22(5):1009â€“
1020, 2012.
Ritabrata Dutta, Bastien Chopard, Jonas LÂ¨att, Frank Dubois, Karim Zouaoui Boudjel-
tia, and Antonietta Mira. Parameter estimation of platelets deposition: Approximate
Bayesian computation with high performance computing.
Frontiers in physiology, 9,
2018.
Ritabrata Dutta, Susana N Gomes, Dante Kalise, and Lorenzo Pacchiardi. Using mobility
data in the design of optimal lockdown strategies for the covid-19 pandemic.
PLoS
Computational Biology, 17(8):e1009236, 2021a.
Ritabrata Dutta, Marcel Schoengens, Lorenzo Pacchiardi, Avinash Ummadisingu, Nicole
Widmer, Pierre KÂ¨unzli, Jukka-Pekka Onnela, and Antonietta Mira. ABCpy: A high-
performance computing perspective to approximate bayesian computation. Journal of
Statistical Software, 100(7):1â€“38, 2021b. doi: 10.18637/jss.v100.i07. URL https://www.
jstatsoft.org/index.php/jss/article/view/v100i07.
Richard G Everitt. Bayesian parameter estimation for latent Markov random ï¬elds and
social networks. Journal of Computational and graphical Statistics, 21(4):940â€“960, 2012.
Richard G Everitt, Dennis Prangle, Philip Maybank, and Mark Bell. Marginal sequential
Monte Carlo for doubly intractable models. arXiv preprint arXiv:1710.04382, 2017.
Matteo Fasiolo, Simon N Wood, Florian Hartig, and Mark V Bravington. An extended
empirical saddlepoint approximation for intractable likelihoods. Electronic Journal of
Statistics, 12(1):1544â€“1578, 2018.
65

Pacchiardi and Dutta
Paul Fearnhead and Dennis Prangle.
Constructing summary statistics for approximate
Bayesian computation: semi-automatic approximate Bayesian computation [with Discus-
sion]. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 74(3):
419â€“474, 2012. ISSN 1369-7412.
Charles J Geyer. Markov chain Monte Carlo maximum likelihood. In Computing science
and statistics: Proceedings of 23rd Symposium on the Interface Interface Foundation,
Fairfax Station, 1991, pages 156â€“163, 1991.
Alexander Gleim and Christian Pigorsch.
Approximate Bayesian computation with
indirect
summary
statistics.
Draft
paper:
http://ect-pigorsch.
mee.
uni-bonn.
de/data/research/papers, 2013.
Tilmann Gneiting and Adrian E Raftery.
Strictly proper scoring rules, prediction, and
estimation. Journal of the American statistical Association, 102(477):359â€“378, 2007.
David Greenberg, Marcel Nonnenmacher, and Jakob Macke. Automatic posterior transfor-
mation for likelihood-free inference. In Kamalika Chaudhuri and Ruslan Salakhutdinov,
editors, Proceedings of the 36th International Conference on Machine Learning, volume 97
of Proceedings of Machine Learning Research, pages 2404â€“2414. PMLR, 09â€“15 Jun 2019.
URL http://proceedings.mlr.press/v97/greenberg19a.html.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard SchÂ¨olkopf, and Alexander
Smola. A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723â€“
773, 2012.
Michael U Gutmann and Aapo HyvÂ¨arinen. Noise-contrastive estimation of unnormalized
statistical models, with applications to natural image statistics.
Journal of Machine
Learning Research, 13(Feb):307â€“361, 2012.
Michael U Gutmann, Jukka Corander, et al.
Bayesian optimization for likelihood-free
inference of simulator-based statistical models. Journal of Machine Learning Research,
2016.
J Hakkarainen, A Ilin, A Solonen, M Laine, H Haario, J Tamminen, E Oja, and H JÂ¨arvinen.
On closure parameter estimation in chaotic systems. Nonlinear processes in Geophysics,
19(1):127â€“143, 2012.
Joeri Hermans, Volodimir Begy, and Gilles Louppe. Likelihood-free MCMC with amortized
approximate ratio estimators. In International Conference on Machine Learning, pages
4239â€“4248. PMLR, 2020.
Geoï¬€rey E Hinton.
Training products of experts by minimizing contrastive divergence.
Neural computation, 14(8):1771â€“1800, 2002.
Harold Hotelling. Relations between two sets of variates. Biometrika, 28(3/4):321â€“377,
1936.
Aapo HyvÂ¨arinen. Estimation of non-normalized statistical models by score matching. Jour-
nal of Machine Learning Research, 6(Apr):695â€“709, 2005.
66

Score Matched Neural Exponential Families for LFI
Aapo HyvÂ¨arinen.
Some extensions of score matching.
Computational statistics & data
analysis, 51(5):2499â€“2512, 2007.
Sergey Ioï¬€e and Christian Szegedy. Batch Normalization: Accelerating deep network train-
ing by reducing internal covariate shift. In Francis Bach and David Blei, editors, Proceed-
ings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings
of Machine Learning Research, pages 448â€“456, Lille, France, 07â€“09 Jul 2015. PMLR. URL
http://proceedings.mlr.press/v37/ioffe15.html.
Marko JÂ¨arvenpÂ¨aÂ¨a, Michael U Gutmann, Arijus Pleska, Aki Vehtari, and Pekka Marttinen.
Eï¬ƒcient acquisition rules for model-based approximate Bayesian computation. Bayesian
Analysis, 14(2):595â€“622, 2019.
Marko Jarvenpaa, Aki Vehtari, and Pekka Marttinen. Batch simulations and uncertainty
quantiï¬cation in gaussian process surrogate approximate Bayesian computation. In Con-
ference on Uncertainty in Artiï¬cial Intelligence, pages 779â€“788. PMLR, 2020.
Marko JÂ¨arvenpÂ¨aÂ¨a, Michael U Gutmann, Aki Vehtari, and Pekka Marttinen. Parallel Gaus-
sian process surrogate Bayesian inference with noisy likelihood evaluations. Bayesian
Analysis, 16(1):147â€“178, 2021.
Bai Jiang, Tung-yu Wu, Charles Zheng, and Wing H Wong. Learning summary statistic
for approximate Bayesian computation via deep neural network. Statistica Sinica, pages
1595â€“1618, 2017.
Paul Joyce and Paul Marjoram. Approximately suï¬ƒcient statistics and Bayesian computa-
tion. Statistical applications in genetics and molecular biology, 7(1), 2008.
Ilyes Khemakhem, Ricardo Pio Monti, Diederik P. Kingma, and Aapo HyvÂ¨arinen. ICE-
BeeM: Identiï¬able conditional energy-based deep models. CoRR, abs/2002.11537, 2020.
URL https://arxiv.org/abs/2002.11537.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representa-
tions, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings,
2015. URL http://arxiv.org/abs/1412.6980.
Nadja Klein, David J Nott, and Michael Stanley Smith. Marginally calibrated deep dis-
tributional regression. Journal of Computational and Graphical Statistics, pages 1â€“17,
2020.
Bernard Osgood Koopman. On distributions admitting a suï¬ƒcient statistic. Transactions
of the American Mathematical society, 39(3):399â€“409, 1936.
Olivier Ledoit and Michael Wolf. A well-conditioned estimator for large-dimensional covari-
ance matrices. Journal of multivariate analysis, 88(2):365â€“411, 2004.
Faming Liang. A double Metropolisâ€“Hastings sampler for spatial models with intractable
normalizing constants. Journal of Statistical Computation and Simulation, 80(9):1007â€“
1022, 2010.
67

Pacchiardi and Dutta
Faming Liang, Ick Hoon Jin, Qifan Song, and Jun S Liu. An adaptive exchange algorithm
for sampling from distributions with intractable normalizing constants. Journal of the
American Statistical Association, 111(513):377â€“393, 2016.
Jarno Lintusaari, Michael U. Gutmann, Ritabrata Dutta, Samuel Kaski, and Jukka Coran-
der. Fundamentals and recent developments in approximate Bayesian computation. Sys-
tematic Biology, 66(1):e66â€“e82, 2017. ISSN 1076836X. doi: 10.1093/sysbio/syw077. URL
https://doi.org/10.1093/sysbio/syw077.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose Bayesian
inference algorithm. In Advances in neural information processing systems, pages 2378â€“
2386, 2016.
Song Liu and Takafumi Kanamori. Estimating density models with complex truncation
boundaries. arXiv preprint arXiv:1910.03834, 2019.
Edward N Lorenz. Predictability: A problem partly solved. In Proc. Seminar on predictabil-
ity, volume 1, 1996.
Jan-Matthis Lueckmann, Pedro J Goncalves, Giacomo Bassetto, Kaan Â¨Ocal, Marcel Non-
nenmacher, and Jakob H Macke. Flexible statistical inference for mechanistic models of
neural dynamics. In Advances in Neural Information Processing Systems, pages 1289â€“
1299, 2017.
Jan-Matthis Lueckmann, Giacomo Bassetto, Theofanis Karaletsos, and Jakob H Macke.
Likelihood-free inference with emulator networks. In Symposium on Advances in Approx-
imate Bayesian Inference, pages 32â€“53. PMLR, 2019.
Kanti V Mardia, John T Kent, and Arnab K Laha. Score matching estimators for directional
distributions. arXiv preprint arXiv:1604.08470, 2016.
Jean-Michel Marin, Pierre Pudlo, Christian P Robert, and Robin J Ryder. Approximate
Bayesian computational methods. Statistics and Computing, 22(6):1167â€“1180, 2012.
Paul Marjoram, John Molitor, Vincent Plagnol, and Simon TavarÂ´e. Markov chain Monte
Carlo without likelihoods. Proceedings of the National Academy of Sciences, 100(26):
15324â€“15328, 2003.
Pekka Marttinen, Nicholas J Croucher, Michael U Gutmann, Jukka Corander, and
William P Hanage. Recombination produces coherent bacterial species clusters in both
core and accessory genomes. Microbial Genomics, 1(5), 2015.
Takuo Matsubara,
Jeremias Knoblauch,
FranÂ¸cois-Xavier Briol,
Chris Oates,
et al.
Robust generalised Bayesian inference for intractable likelihoods.
arXiv preprint
arXiv:2104.07359, 2021.
Trevelyan J McKinley, Ian Vernon, Ioannis Andrianakis, Nicky McCreesh, Jeremy E Oakley,
Rebecca N Nsubuga, Michael Goldstein, Richard G White, et al. Approximate Bayesian
computation and simulation-based inference for complex stochastic epidemic models. Sta-
tistical science, 33(1):4â€“18, 2018.
68

Score Matched Neural Exponential Families for LFI
Edward Meeds and Max Welling.
GPS-ABC: Gaussian process surrogate approximate
Bayesian computation.
In Proceedings of the Thirtieth Conference on Uncertainty in
Artiï¬cial Intelligence, pages 593â€“602, 2014.
Amanda Minter and Renata Retkute. Approximate Bayesian computation for infectious
disease modelling. Epidemics, 29:100368, 2019.
Matthew T Moores, Christopher C Drovandi, Kerrie Mengersen, and Christian P Robert.
Pre-processing for approximate Bayesian computation in image analysis. Statistics and
Computing, 25(1):23â€“33, 2015.
Iain Murray, Zoubin Ghahramani, and David MacKay. MCMC for doubly-intractable dis-
tributions. arXiv preprint arXiv:1206.6848, 2012.
Hien Duy Nguyen, Julyan Arbel, Hongliang LÂ¨u, and Florence Forbes. Approximate Bayesian
computation via the energy statistic. IEEE Access, 8:131683â€“131698, 2020.
Matthew A Nunes and David J Balding. On optimal selection of summary statistics for
approximate Bayesian computation. Statistical applications in genetics and molecular
biology, 9(1), 2010.
Lorenzo Pacchiardi, Pierre KÂ¨unzli, Marcel SchÂ¨ongens, Bastien Chopard, and Ritabrata
Dutta.
Distance-learning for approximate Bayesian computation to model a volcanic
eruption. Sankhya B, Jan 2020. ISSN 0976-8394. doi: 10.1007/s13571-019-00208-8. URL
https://doi.org/10.1007/s13571-019-00208-8.
George Papamakarios and Iain Murray. Fast Îµ-free inference of simulation models with
Bayesian conditional density estimation. In Advances in Neural Information Processing
Systems, pages 1028â€“1036, 2016.
George Papamakarios, David Sterratt, and Iain Murray. Sequential neural likelihood: Fast
likelihood-free inference with autoregressive ï¬‚ows. In Kamalika Chaudhuri and Masashi
Sugiyama, editors, Proceedings of Machine Learning Research, volume 89 of Proceedings
of Machine Learning Research, pages 837â€“848. PMLR, 16â€“18 Apr 2019.
URL http:
//proceedings.mlr.press/v89/papamakarios19a.html.
George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Bal-
aji Lakshminarayanan. Normalizing ï¬‚ows for probabilistic modeling and inference. Jour-
nal of Machine Learning Research, 22(57):1â€“64, 2021. URL http://jmlr.org/papers/
v22/19-1028.html.
Jaewoo Park and Murali Haran.
Bayesian inference in the presence of intractable nor-
malizing functions. Journal of the American Statistical Association, 113(523):1372â€“1390,
2018.
Mijung Park, Wittawat Jitkrittum, and Dino Sejdinovic. K2-ABC: Approximate Bayesian
computation with kernel embeddings. In Artiï¬cial Intelligence and Statistics, 2016.
69

Pacchiardi and Dutta
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, An-
dreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank
Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An
imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. dâ€™AlchÂ´e Buc, E. Fox, and R. Garnett, editors, Advances in Neural
Information Processing Systems 32, pages 8024â€“8035. Curran Associates, Inc., 2019.
Leah F Price, Christopher C Drovandi, Anthony Lee, and David J Nott. Bayesian synthetic
likelihood. Journal of Computational and Graphical Statistics, 27(1):1â€“11, 2018.
Stefan T Radev, Ulf K Mertens, Andreas Voss, Lynton Ardizzone, and Ullrich KÂ¨othe.
BayesFlow: Learning complex stochastic models with invertible neural networks. IEEE
Transactions on Neural Networks and Learning Systems, 2020.
Maria L Rizzo and GÂ´abor J SzÂ´ekely.
Energy distance.
Wiley interdisciplinary reviews:
Computational statistics, 8(1):27â€“38, 2016.
Gareth O Roberts, Andrew Gelman, Walter R Gilks, et al. Weak convergence and optimal
scaling of random walk Metropolis algorithms. The annals of applied probability, 7(1):
110â€“120, 1997.
Erlis Ruli, Nicola Sartori, and Laura Ventura. Approximate Bayesian computation with
composite score functions. Statistics and Computing, 26(3):679â€“692, 2016.
Jascha Sohl-Dickstein, Peter B Battaglino, and Michael R DeWeese.
New method for
parameter estimation in probabilistic models: minimum probability ï¬‚ow. Physical review
letters, 107(22):220601, 2011.
Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable
approach to density and score estimation. In Ryan P. Adams and Vibhav Gogate, editors,
Proceedings of The 35th Uncertainty in Artiï¬cial Intelligence Conference, volume 115 of
Proceedings of Machine Learning Research, pages 574â€“584, Tel Aviv, Israel, 22â€“25 Jul
2020. PMLR. URL http://proceedings.mlr.press/v115/song20a.html.
Bharath Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Aapo HyvÂ¨arinen, and Revant
Kumar. Density estimation in inï¬nite dimensional exponential families. The Journal of
Machine Learning Research, 18(1):1830â€“1888, 2017.
Esteban G Tabak, Giulio Trigila, and Wenjun Zhao. Conditional density estimation and
simulation through optimal transport. Machine Learning, pages 1â€“24, 2020.
Simon TavarÂ´e, David J Balding, Robert C Griï¬ƒths, and Peter Donnelly. Inferring coales-
cence times from DNA sequence data. Genetics, 145(2):505â€“518, 1997.
Owen Thomas, Ritabrata Dutta, Jukka Corander, Samuel Kaski, Michael U Gutmann,
et al. Likelihood-free inference by ratio estimation. Bayesian Analysis, 2020.
70

Score Matched Neural Exponential Families for LFI
Jean-Francois Ton, CHAN Lucian, Yee Whye Teh, and Dino Sejdinovic. Noise contrastive
meta-learning for conditional density estimation using kernel mean embeddings. In In-
ternational Conference on Artiï¬cial Intelligence and Statistics, pages 1099â€“1107. PMLR,
2021.
Tina Toni, David Welch, Natalja Strelkowa, Andreas Ipsen, and Michael PH Stumpf. Ap-
proximate Bayesian computation scheme for parameter inference and model selection in
dynamical systems. Journal of the Royal Society Interface, 6(31):187â€“202, 2009.
Pascal Vincent. A connection between score matching and denoising autoencoders. Neural
computation, 23(7):1661â€“1674, 2011.
Ziyu Wang, Shuyu Cheng, Li Yueru, Jun Zhu, and Bo Zhang. A Wasserstein minimum
velocity approach to learning unnormalized models. In Silvia Chiappa and Roberto Ca-
landra, editors, Proceedings of the Twenty Third International Conference on Artiï¬cial
Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research,
pages 3728â€“3738. PMLR, 26â€“28 Aug 2020.
URL https://proceedings.mlr.press/
v108/wang20j.html.
Li Wenliang, Dougal Sutherland, Heiko Strathmann, and Arthur Gretton. Learning deep
kernels for exponential family densities. In International Conference on Machine Learn-
ing, pages 6737â€“6746. PMLR, 2019.
Richard Wilkinson.
Accelerating ABC methods using Gaussian processes.
In Artiï¬cial
Intelligence and Statistics, pages 1015â€“1023. PMLR, 2014.
Daniel S Wilks. Eï¬€ects of stochastic parametrizations in the Lorenzâ€™96 system. Quarterly
Journal of the Royal Meteorological Society, 131(606):389â€“407, 2005.
Samuel Wiqvist, Pierre-Alexandre Mattei, Umberto Picchini, and Jes Frellsen. Partially
exchangeable networks and architectures for learning summary statistics in approximate
Bayesian computation. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Pro-
ceedings of the 36th International Conference on Machine Learning, volume 97 of Pro-
ceedings of Machine Learning Research, pages 6798â€“6807. PMLR, 09â€“15 Jun 2019. URL
http://proceedings.mlr.press/v97/wiqvist19a.html.
Simon N Wood. Statistical inference for noisy nonlinear ecological dynamic systems. Nature,
466(7310):1102, 2010.
Shiqing Yu, Mathias Drton, and Ali Shojaie. Generalized score matching for non-negative
data. Journal of Machine Learning Research, 20(76):1â€“70, 2019. URL http://jmlr.
org/papers/v20/18-278.html.
71

