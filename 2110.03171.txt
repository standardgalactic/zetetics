Proceedings of Machine Learning Research vol 178:1–35, 2022
35th Annual Conference on Learning Theory
Assemblies of Neurons Learn to Classify Well-Separated Distributions
Max Dabagia
MAXDABAGIA@GATECH.EDU
School of Computer Science, Georgia Tech
Christos H. Papadimitriou
CHRISTOS@COLUMBIA.EDU
Department of Computer Science, Columbia University
Santosh S. Vempala
VEMPALA@GATECH.EDU
School of Computer Science, Georgia Tech
Editors: Po-Ling Loh and Maxim Raginsky
Abstract
An assembly is a large population of neurons whose synchronous ﬁring is hypothesized to repre-
sent a memory, concept, word, and other cognitive categories. Assemblies are believed to provide a
bridge between high-level cognitive phenomena and low-level neural activity. Recently, a compu-
tational system called the Assembly Calculus (AC), with a repertoire of biologically plausible oper-
ations on assemblies, has been shown capable of simulating arbitrary space-bounded computation,
but also of simulating complex cognitive phenomena such as language, reasoning, and planning.
However, the mechanism whereby assemblies can mediate learning has not been known. Here we
present such a mechanism, and prove rigorously that, for simple classiﬁcation problems deﬁned on
distributions of labeled assemblies, a new assembly representing each class can be reliably formed
in response to a few stimuli from the class; this assembly is henceforth reliably recalled in response
to new stimuli from the same class. Furthermore, such class assemblies will be distinguishable
as long as the respective classes are reasonably separated — for example, when they are clusters
of similar assemblies, or more generally separable with margin by a linear threshold function. To
prove these results, we draw on random graph theory with dynamic edge weights to estimate se-
quences of activated vertices, yielding strong generalizations of previous calculations and theorems
in this ﬁeld over the past ﬁve years. These theorems are backed up by experiments demonstrating
the successful formation of assemblies which represent concept classes on synthetic data drawn
from such distributions, and also on MNIST, which lends itself to classiﬁcation through one as-
sembly per digit. Seen as a learning algorithm, this mechanism is entirely online, generalizes from
very few samples, and requires only mild supervision — all key attributes of learning in a model
of the brain. We argue that this learning mechanism, supported by separate sensory pre-processing
mechanisms for extracting attributes, such as edges or phonemes, from real world data, can be the
basis of biological learning in cortex.
Keywords: List of keywords
1. Introduction
The brain has been a productive source of inspiration for AI, from the perceptron and the neocog-
nitron to deep neural nets. Machine learning has since advanced to dizzying heights of analytical
understanding and practical success, but the study of the brain has lagged behind in one important
dimension: After half a century of intensive effort by neuroscientists (both computational and ex-
perimental), and despite great advances in our understanding of the brain at the level of neurons,
synapses, and neural circuits, we still have no plausible mechanism for explaining intelligence, that
© 2022 M. Dabagia, C.H. Papadimitriou & S.S. Vempala.
arXiv:2110.03171v3  [cs.NE]  3 Jul 2022

DABAGIA PAPADIMITRIOU VEMPALA
is, the brain’s performance in planning, decision-making, language, etc. As Nobel laureate Richard
Axel put it, “we have no logic for translating neural activity into thought and action” (Axel, 2018).
Recently, a high-level computational framework was developed with the explicit goal to ﬁll this
gap: the Assembly Calculus (AC) (Papadimitriou et al., 2020), a computational model whose basic
data type is the assembly of neurons. Assemblies, called “the alphabet of the brain” (Buzs´aki, 2019),
are large sets of neurons whose simultaneous excitation is tantamount to the subject’s thinking of an
object, idea, episode, or word (see Piantadosi et al. (2016)). Dating back to the birth of neuroscience,
the “million-fold democracy” by which groups of neurons act collectively without central control
was ﬁrst proposed by Sherrington (1906) and was the empirical phenomenon that Hebb attempted
to explain with his theory of plasticity (Hebb, 1949). Assemblies are initially created to record
memories of external stimuli (Quiroga, 2016), and are believed to be subsequently recalled, copied,
altered, and manipulated in the non-sensory brain (Piantadosi et al., 2012; Buzs´aki, 2010). The
Assembly Calculus provides a repertoire of operations for such manipulation, namely project,
reciprocal-project, associate, pattern-complete, and merge encompassing
a complete computational system. Since the Assembly Calculus is, to our knowledge, the only ex-
tant computational system whose purpose is to bridge the gap identiﬁed by Axel in the above quote
(Axel, 2018), it is of great interest to establish that complex cognitive functions can be plausibly
expressed in it. Indeed, signiﬁcance progress has been made over the past year, see for example
Mitropolsky et al. (2021) for a parser of English and d’Amore et al. (2021) for a program mediating
planning in the blocks world, both written in the AC programming system. Yet despite these recent
advances, one fundamental question is left unanswered: If the Assembly Calculus is a meaningful
abstraction of cognition and intelligence, why does it not have a learn command? How can the
brain learn through assembly representations?
This is the question addressed and answered in this paper. As assembly operations are a new
learning framework and device, one has to start from the most basic questions: Can this model clas-
sify assembly-encoded stimuli that are separated through clustering, or by half spaces? Recall that
learning linear thresholds is a theoretical cornerstone of supervised learning, leading to a legion of
fundamental algorithms: Perceptron, Winnow, multiplicative weights, isotron, kernels and SVMs,
and many variants of gradient descent.
Following Papadimitriou et al. (2020), we model the brain as a directed graph of excitatory
neurons with dynamic edge weights (due to plasticity). The brain is subdivided into areas, for
simplicity each containing n neurons connected through a Gn,p random directed graph (Erd˝os and
R´enyi, 1960). Certain ordered pairs of areas are also connected, through random bipartite graphs.
We assume that neurons ﬁre in discrete time steps. At each time step, each neuron in a brain area
will ﬁre if its synaptic input from the ﬁrings of the previous step is among the top k highest out
of the n neurons in its brain area. This selection process is called k-cap, and is an abstraction
of the process of inhibition in the brain, in which a separate population of inhibitory neurons is
induced to ﬁre by the ﬁring of excitatory neurons in the area, and through negatively-weighted
connections prevents all but the most stimulated excitatory neurons from ﬁring. Synaptic weights
are altered via Hebbian plasticity and homeostasis (see Section 2 for a full description). In this
stylized mathematical model, reasonably consistent with what is known about the brain, it has been
shown that the operations of the Assembly Calculus converge and work as speciﬁed (with high
probability relative to the underlying random graphs). These results have also been replicated by
simulations in the model above, and also in more biologically realistic networks of spiking neurons
(see Legenstein et al. (2018); Papadimitriou et al. (2020)). In this paper we develop, in the same
2

LEARNING WITH ASSEMBLIES OF NEURONS
model, mechanisms for learning to classify well-separated classes of stimuli, including clustered
distributions and linear threshold functions with margin. Moreover, considering that the ability to
learn from few examples, and with mild supervision, are crucial characteristics of any brain-like
learning algorithm, we show that learning with assemblies does both quite naturally.
2. A mathematical model of the brain
Here we outline the basics of the model in Papadimitriou et al. (2020). There are a ﬁnite number a
of brain areas, denoted X, Y, . . . (but in this paper, we will only need one brain area where learning
happens, plus another area where the stimuli are presented). Each area is a random directed graph
with n nodes called neurons with each directed edge present independently with probability p (for
simplicity, we take n and p to be the same across areas). Some ordered pairs of brain areas are also
connected by random bipartite graphs, with the same connection probability p. Importantly, each
area may be inhibited, which means that its neurons cannot ﬁre; the status of the areas is determined
by explicit inhibit/disinhibit commands of the AC. 1 This deﬁnes a large random graph
G = (N, E) with |N| = an nodes and a number |E| of directed edges which is in expectation
(a + b)pn2 −apn, where b is the number of pairs of areas that are connected. Each edge (i, j) ∈E
in this graph, called a synapse, has a dynamic non-negative weight wij(t), initially 1.
This framework gives rise to a discrete-time dynamical system, as follows: The state of the
system at any time step t consists of (a) a bit for each area X, inh(X, t), initially 0, denoting
whether the area is inhibited; (b) a bit for each neuron i, ﬁres(i, t), denoting whether i spikes at time
t (zero if the area X of i has inh(X, t) = 1); and (c) the weights of all synapses wij(t), initially one.
The state transition of the dynamical system is as follows: For each neuron i in area A with
inh(X, t + 1) = 0 (see the next paragraph for how inh(X, t + 1) is determined), deﬁne its synaptic
input at time t + 1,
SI(i, t + 1) =
X
(j,i)∈E
ﬁres(j, t)wji(t).
For each i in area X with inh(X, t + 1) = 0, we set ﬁres(i, t) = 1 iff i is among the k neurons in
its area that have highest SI(i, t + 1) (breaking ties arbitrarily). This is the k-cap operation, a basic
ingredient of the AC framework, modeling the inhibitory/excitatory balance of a brain area.2 As for
the synaptic weights,
wji(t + 1) = wji(t)(1 + β · ﬁres(j, t)ﬁres(i, t + 1)).
That is, if j ﬁres at time t and i ﬁres at time t + 1, Hebbian plasticity dictates that wji be increased
by a factor of 1 + β at time t + 1. So that the weights do not grow unlimited, a homeostasis process
renormalizes, at a slower time scale, the sum of weights along the incoming synapses of each neuron
(see Davis (2006) and Turrigiano (2011) for reviews of this mechanism in the brain).
Finally, the AC is a computational system driving the dynamical system by executing commands
at each time step t (like a programming language driving the physical system that is the computer’s
hardware). The AC commands (dis)inhibit(X) change the inhibition status of an area at time
1. The brain’s neuromodulatory systems (Jones, 2003; Harris and Thiele, 2011) are plausible candidates to implement
these mechanisms.
2. Binas et al. (2014) showed rigorously how a k-cap dynamic could be emerge in a network of excitatory and inhibitory
neurons.
3

DABAGIA PAPADIMITRIOU VEMPALA
t; and the command fire(x), where x is the name of an assembly (deﬁned next) in a disinhibited
area, overrides the selection by k-cap, and causes the k neurons of assembly x to ﬁre at time t.
An assembly is a highly interconnected (in terms of both number of synapses and their weights)
set of k neurons in an area encoding a real world entity. Initially, assembly-like representations exist
only in a special sensory area, as representations of perceived real-world entities such as a heard
(or read) word. Assemblies in the remaining, non-sensory areas are an emergent behavior of the
system, copied and re-copied, merged, associated, etc., through further commands of the AC. This
is how the model is able to simulate arbitrary n
k space bounded computations (Papadimitriou et al.,
2020). The most basic such command is project(x, Y, y), which, starting from an assembly x
in area X, creates in area Y (where there is connectivity from X to Y ) a new assembly y, which
has strong synaptic connectivity from x and which will henceforth ﬁre every time x ﬁres in the
previous step, and Y is not inhibited. This command entails disinhibiting the areas X, Y , and then
firing (the neurons in) assembly x for the next T time steps. It was shown by Papadimitriou and
Vempala (2019) that, with high probability, after a small number of steps, a stable assembly y in
Y will emerge, which is densely intraconnected and has high connectivity from x. The mechanism
achieving this convergence involves synaptic input from x, which creates an initial set y1 of ﬁring
neurons in Y , which then evolves to yt, t = 2, . . . through sustained synaptic input from x and
recurrent input from yt−1, while these two effects are further enhanced by plasticity.
Incidentally, this convergence proof (see Legenstein (2018); Papadimitriou and Vempala (2019);
Papadimitriou et al. (2020) for a sequence of sharpened versions of this proof over the past years)
is the most mathematically sophisticated contribution of this theory to date. The theorems of the
present paper can be seen as substantial generalizations of that result: Whereas in previous work
an assembly is formed as a copy of one stimulus ﬁring repeatedly (memorization), so that this new
assembly will henceforth ﬁre whenever the same stimulus us presented again, in this paper we show
rigorously that an assembly will be formed in response to the sequential ﬁring of many stimuli, all
drown from the same distribution (generalization), and the formed assembly will ﬁre reliably every
time another stimulus from the same distribution is presented.
The key parameters of our model are n, k, p, and β. Intended values for the brain are n =
107, k = 104, p = 10−3, β = 0.1, but in our simulations we have also had success on a much
smaller scale, with n = 103, k = 102, p = 10−1, β = 0.1. β is an important parameter, in
that adequately large values of β guarantee the convergence of the AC operations. For a publicly
available simulator of the Assembly Calculus (in which the Learning System below can be readily
implemented) see http://brain.cc.gatech.edu.
The learning mechanism.
For the purpose of demonstrating learning within the framework of
AC, we consider the speciﬁc setting described below. First, there is a special area, called the sensory
area, in which training and testing data are encoded as assembly-like representations called stimuli.
There is only one other brain area (besides the sensory area), and that is where learning happens,
through the formation of assemblies in response to sequences of stimuli.
A stimulus is a set of about k neurons ﬁring simultaneously (“presented”) in the sensory area.
Note that, exceptionally in the sensory area, a number of neurons that is a little different from k may
ﬁre at a step. A stimulus class A is a distribution over stimuli, deﬁned by three parameters: two
scalars r, q ∈[0, 1], r > q, and a set of k neurons SA in the sensory area. To generate a stimulus
x ∈{0, 1}n in the class A, each neuron i ∈SA is chosen with probability r, while for each i ̸∈SA,
the probability of choosing neuron i is qk/n. It follows immediately that, in expectation, an r
4

LEARNING WITH ASSEMBLIES OF NEURONS
Figure 1: A mathematical model of learning in the brain. Our model (left) has a sensory area
(column) connected to a brain area (square), both made up of spiking neurons. Two
different stimuli classes (with their core sets in blue/green on the left) project from the
sensory area via synaptic connections (arrows). Assemblies in the brain area (with core
sets in the corresponding colors) form in response to these stimuli classes, each of which
consistently ﬁres when a constant fraction of the associated stimuli class’s core set does.
Our learning algorithm (right) consists of presenting a stream of stimuli from each class.
fraction of the neurons in the stimulus core are set to 1 and the number of neurons outside the core
that are set to 1 is also O(k).
The presentation of a sequence of stimuli from a class A in the sensory area evokes in the
learning system a response R, a distribution over assemblies in the brain area. We show that, as a
consequence of plasticity and k-cap, this distribution R will be highly concentrated, in the following
sense: Consider the set SR of all assemblies x that have positive probability in R. Then the numbers
of neurons in both the intersection R∗= T
x∈SR x, called the core of R and the union ¯R = S
x∈SR x
are close to k, in particular k −o(k) and k + o(k) respectively.3 In other words, neurons in R∗ﬁre
far more often on average than neurons in ¯R \ R∗.
Finally, our learning protocol is this: Beginning with the brain area at rest, stimuli are repeatedly
sampled from the class, and made to ﬁre. After a small number of training samples, the brain area
returns to rest, and then the same procedure is repeated for the next stimulus class, and so on. Then
testing stimuli are presented in random order to test the extent of learning. (see Algorithm 1 in an
AC-derived programming language.)
That is, we sample T stimuli x from each class, ﬁre each x to cause synaptic input in the brain
area, and after the Tth sample has ﬁred we record the assembly which has been formed in the brain
area. This is the representation for this class.
Related work
There are numerous learning models in the neuroscience literature. In a variation of the model
we consider here, Rangamani and Gandhi (2020) have considered supervised learning of Boolean
functions using assemblies of neurons, by setting up separate brain areas for each label value.
3. The larger the plasticity, the closer these two values are (see Papadimitriou and Vempala (2019), Fig. 2).
5

DABAGIA PAPADIMITRIOU VEMPALA
Algorithm 1: The learning mechanism. (B denotes the brain area.)
Input: a set of stimulus classes A1, . . . , Ac; T ≥1
Output: A set of assemblies y1, . . . , yc in the brain area encoding these classes
foreach stimulus class i do
inh(B) ←0
foreach time step 1 ≤t ≤T do
Sample x ∼Ai and ﬁre x
end
yi ←read(B)
inh(B) ←1
end
Amongst other systems with rigorous guarantees, assemblies are superﬁcially similar to the “items”
of Valiant’s neuroidal model (Valiant, 1994), in which supervised learning experiments have been
conducted (Valiant, 2000; Feldman and Valiant, 2009), where an output neuron is clamped to the
correct label value, while the network weights are updated under the model. The neuroidal model is
considerably more powerful than ours, allowing for arbitrary state changes of neurons and synapses;
in contrast, our assemblies rely on only two biologically sound mechanisms, plasticity and inhibi-
tion.
Hopﬁeld nets (Hopﬁeld, 1982) are recurrent networks of neurons with symmetric connection
weights which will converge to a memorized state from a sufﬁciently similar one, when properly
trained using a local and incremental update rule. In contrast, the memorized states our model pro-
duces (which we call assemblies) emerge through plasticity and randomization from the structure of
a random directed network, whose weights are asymmetric and nonnegative, and in which inhibition
— not the sign of total input — selects which neurons will ﬁre.
Stronger learning mechanisms have recently been proposed. Inspired by the success of deep
learning, a large body of work has shown that cleverly laid-out microcircuits of neurons can ap-
proximate backpropagation to perform gradient descent (Lillicrap et al., 2016; Sacramento et al.,
2017; Guerguiev et al., 2017; Sacramento et al., 2018; Whittington and Bogacz, 2019; Lillicrap
et al., 2020). These models rely crucially on novel types of neural circuits which, although bio-
logically possible, are not presently known or hypothesized in neurobiology, nor are they proposed
as a theory of the way the brain works. These models are capable of matching the performance of
deep networks on many tasks, which are more complex than the simple, classical learning problems
we consider here. The difference between this work and ours is, again, that here we are showing
that learning arises naturally from well-understood mechanisms in the brain, in the context of the
assembly calculus.
3. Results
Very few stimuli sampled from an input distribution are activated sequentially at the sensory area.
The only form of supervision required is that all training samples from a given class are presented
consecutively. Plasticity and inhibition alone ensure that, in response to this activation, an assembly
will be formed for each class, and that this same assembly will be recalled at testing upon presen-
tation of other samples from the same distribution. In other words, learning happens. And in fact,
6

LEARNING WITH ASSEMBLIES OF NEURONS
despite all these limitations, we show that the device is an efﬁcient learner of interesting concept
classes.
Our ﬁrst theorem is about the creation of an assembly in response to inputs from a stimulus
class. This is a generalization of a theorem from Papadimitriou and Vempala (2019), where the
input stimulus was held constant; here the input is a stream of random samples from the same
stimulus class. Like all our results, it is a statement holding with high probability (WHP), where the
underlying random event is the random graph and the random samples. When sampled stimuli ﬁre,
the assembly in the brain area changes. The neurons participating in the current assembly (those
whose synaptic input from the previous step is among the k highest) are called the current winners.
A ﬁrst-time winner is a current winner that participated in no previous assembly (for the current
stimulus class).
Theorem 1 (Creation) Consider a stimulus class A projected to a brain area. Assume that
β ≥β0 = 1
r2
 √
2 −r2 q
2 ln
  n
k

+
√
6
√kp +
q
2 ln
  n
k

Then WHP no ﬁrst-time winners will enter the cap after O(log k) rounds, and moreover the total
number of winners ¯A can be bounded as
| ¯A| ≤
k
1 −exp(−( β
β0 )2)
≤k + O
 log n
r3pβ2

Remark 2 The theorem implies that for a small constant c, it sufﬁces to have plasticity parameter
β ≥1
r2
c
p
kp/(2 ln(n/k)) + 1
.
Our second theorem is about recall for a single assembly, when a new stimulus from the same class
is presented. We assume that examples from an an assembly class A have been presented, and a
response assembly A∗encoding this class has been created, by the previous theorem.
Theorem 3 (Recall) WHP over the stimulus class, the set C1 ﬁring in response to a test assembly
from the class A will overlap A∗by a fraction of at least 1 −e−kpr, i.e.
|C1 \ A∗|
k
≤e−kpr
The proof entails showing that the average weight of incoming connections to a neuron in A∗from
neurons in SA is at least
1 + 1
√r
√
2 +
r 2
kpr ln
n
k

+ 2

Our third theorem is about the creation of a second assembly corresponding to a second stimulus
class. This can easily be extended to many classes and assemblies. As in the previous theorem,
we assume that O(log k) examples from assembly class A have been presented, and ¯A has been
created. Then we introduce B, a second stimulus class, with |SA ∩SB| = αk, and present O(log k)
samples to induce a series of caps, B1, B2, . . ., with B∗as their union.
7

DABAGIA PAPADIMITRIOU VEMPALA
Theorem 4 (Multiple Assemblies) The total support of B∗can be bounded WHP as
|B∗| ≤
k
1 −exp(−( β
β0 )2)
≤k + O
 log n
r3pβ2

Moreover, WHP, the overlap in the core sets A∗and B∗will preserve the overlap of the stimulus
classes, so that |A∗∩B∗| ≤αk.
This time the proof relies on the fact that the average weight of incoming connections to a neuron
in A∗is upper-bounded by
γ ≤1 +
q
2 ln
  n
k

−
p
2 ln((1 + r)/rα)
αr√kp
Our fourth theorem is about classiﬁcation after the creation of multiple assemblies, and shows that
random stimuli from any class are mapped to their corresponding assembly. We state it here for
two stimuli classes, but again it is extended to several. We assume that stimulus classes A and B
overlap in their core sets by a fraction of α, and that they have been projected to form a distribution
of assemblies A∗and B∗, respectively.
Theorem 5 (Classiﬁcation) If a random stimulus chosen from a particular class (WLOG, say B)
ﬁres to cause a set C1 of learning area neurons to ﬁre, then WHP over the stimulus class the fraction
of neurons in the cap C1 and in B∗will be at least
|C1 ∩B∗|
k
≥1 −2 exp

−1
2(γα −1)2kpr

where γ is a lower bound on the average weight of incoming connections to a neuron in A∗(resp.
B∗) from neurons in SA (resp. SB).
Taken together, the above results guarantee that this mechanism can learn to classify well-separated
distributions, where each distribution has a constant fraction of its nonzero coordinates in a subset
of k input coordinates. The process is naturally interpretable: an assembly is created for each distri-
bution, so that random stimuli are mapped to their corresponding assemblies, and the assemblies for
different distributions overlap in no more than the core subsets of their corresponding distributions.
Finally, we consider the setting where the labeling function is a linear threshold function, pa-
rameterized by an arbitrary nonnegative vector v and margin ∆. We will create a single assembly to
represent examples on one side of the threshold, i.e. those for which v · X ≥∥v∥1k/n. We deﬁne
D+ denote the distribution of these examples, where each coordinate is an independent Bernoulli
variable with mean E(Xi) = k/n+∆vi, and deﬁne D−to be the distribution of negative examples,
where each coordinate is again an independent Bernoulli variable yet now all identically distributed
with mean k/n. (Note that the support of the positive and negative distributions is the same; there
is a small probability of drawing a positive example from the negative distribution, or vice versa.)
To serve as a classiﬁer, a fraction 1 −ϵ+ of neurons in the assembly must be guaranteed to ﬁre for
a positive example, and a fraction ϵ−< 1 −ϵ+ guaranteed not to ﬁre for a negative one. A test
example is then classiﬁed as positive if at least a 1 −ϵ fraction of neurons in the assembly ﬁre (for
ϵ ∈[ϵ−, 1 −ϵ+]), and negative otherwise. The last theorem shows that this can in fact be done
8

LEARNING WITH ASSEMBLIES OF NEURONS
with high probability, as long as the normal vector v of the linear threshold is neither too dense nor
too sparse. Additionally, we assume synapses are subject to homeostasis in between training and
evaluation; that is, all of the incoming weights to a neuron are normalized to sum to 1.
Theorem 6 (Learning Linear Thresholds) Let v be a nonnegative vector normalized to be of unit
Euclidean length (∥v∥2 = 1). Assume that Ω(k) = ∥v∥1 ≤√n/2 and
∆2β ≥
s
2k
p (
p
2 ln(n/k) + 2) + 1).
Then, sequentially presenting Ω(log k) samples drawn at random from D+ forms an assembly A∗
that correctly separates D+ from D−: with probability 1 −o(1) a randomly drawn example from
D+ will result in a cap which overlaps at least 3k/4 neurons in A∗, and an example from D−will
create a cap which overlaps no more than k/4 neurons in A∗.
Remark 7 The bound on ∆2β leads to two regimes of particular interest: In the ﬁrst,
β ≥
p
2 ln(n/k) + 2 + 1
√kp
and ∆≥
√
k, which is similar to the plasticity parameter required for a ﬁxed stimulus (Papadim-
itriou and Vempala, 2019) or stimulus classes; in the second, β is a constant, and
∆≥
 2k
β2p
1/4 p
2 ln(n/k) + 2 + 1
1/2
.
Remark 8 We can ensure that the number of neurons outside of A∗for a positive example or in A∗
for a negative example are both o(k) with small overhead4, so that plasticity can be active during
the classiﬁcation phase.
Since our focus in this paper is on highlighting the brain-like aspects of this learning mechanism,
we emphasize stimulus classes as a case of particular interest, as they are a probabilistic general-
ization of the single stimuli considered in Papadimitriou and Vempala (2019). Linear threshold
functions are an equally natural way to generalize a single k-sparse stimulus, say v; all the 0/1
points on the positive side of the threshold v⊤x ≥αk have at least an α fraction of the k neurons
of the stimulus active.
Finally, reading the output of the device by the Assembly Calculus is simple: Add a readout
area to the two areas so far (stimulus and learning), and project to this area one of the assemblies
formed in the learning area for each stimulus class. The assembly in the learning area that ﬁres in
response to a test sample will cause the assembly in the readout area corresponding to the class to
ﬁre, and this can be sensed through the readout operation of the AC.
4. i.e. increasing the plasticity constant β by a factor of 1 + o(1)
9

DABAGIA PAPADIMITRIOU VEMPALA
Proof overview.
The proofs of all ﬁve theorems can be found in the Appendix. The proofs hinge
on showing that large numbers of certain neurons of interest will be included in the cap on a partic-
ular round — or excluded from it. More speciﬁcally:
• To create an assembly, the sequence of caps should converge to the assembly’s core set. In
other words, WHP an increasing fraction of the neurons selected by the cap in a particular
step will also be selected at the next one.
• For recall, a large fraction of the assembly should ﬁre (i.e. be included in the cap) when
presented with an example from the class.
• To differentiate stimuli (i.e. classify), we need to ensure that a large fraction of the correct
assembly will ﬁre, while no more than a small fraction of the other assemblies do.
Following Papadimitriou and Vempala (2019), we observe that if the probability of a neuron having
input at least t is no more than ϵ, then no more than an ϵ fraction of the cohort of neurons will
have input exceeding t (with constant probability). By approximating the total input to a neuron as
Gaussian and using well-known bounds on Gaussian tail probabilities, we can solve for t, which
gives an explicit input threshold neurons must surpass to make a particular cap. Then, we argue
that the advantage conferred by plasticity, combined with the similarity of examples from the same
class, gives the neurons of interest enough of an advantage that the input to all but a small constant
fraction will exceed the threshold.
4. Experiments
The learning algorithm has been run on both synthetic and real-world datasets, as illustrated in
the ﬁgures below. Code for experiments is available at https://github.com/mdabagia/
learning-with-assemblies.
Beyond the basic method of presenting a few examples from the same class and allowing plas-
ticity to alter synaptic weights, the training procedure is slightly different for each of the concept
classes (stimulus classes, linearly-separated, and MNIST digits). In each case, we renormalize the
incoming weights of each neuron to sum to one after concluding the presentation of each class, and
classiﬁcation is performed on top of the learned assemblies by predicting the class corresponding to
the assembly with the most neurons on.
• For stimulus classes, we estimate the assembly for each class as composed of the neurons
which ﬁred in response to the last training example, which in practice are the same as those
most likely to ﬁre for a random test example.
• For a linear threshold, we only present positive examples, and thus only form an assembly for
one class. As with stimulus classes, the neurons in the assembly can be estimated by the last
training cap or by averaging over test examples. We classify by comparing against a ﬁxed
threshold, generally half the cap size.
Additionally, it is important to set the plasticity parameter (β) large enough that assemblies are
reliably formed. We had success with β = 0.1 for stimulus classes and β = 1.0 for linear thresholds.
In Figure 2 (a) & (b), we demonstrate learning of two stimulus classes, while in Figure 2 (c)
& (d), we demonstrate the result of learning a well-separated linear threshold function with assem-
blies. Both had perfect accuracy. Additionally, assemblies readily generalize to a larger number of
10

LEARNING WITH ASSEMBLIES OF NEURONS
Figure 2: Assemblies learned for various concept classes. On the top two lines, we show assemblies
learned for stimulus classes, and on the bottom two lines, for a linear threshold with
margin. In (a) & (c) we exhibit the distribution of ﬁring probabilities over neurons of
the learning area. In (b) & (d) we show the average overlap of different input samples
(red square) and the overlaps of the corresponding representations in the assemblies (blue
square). Using a simple sum readout over assembly neurons, both stimulus classes and
linear thresholds are classiﬁed with 100% accuracy. Here, n = 103, k = 102, p =
0.1, r = 0.9, q = 0.1, ∆= 1.0, with 5 samples per class, and β = 0.01 (stimulus classes)
and β = 1.0 (linear threshold).
classes (see Figure 6 in the appendix). We also recorded sharp threshold transitions in classiﬁcation
performance as the key parameters of the model are varied (see Figures 3 & 4).
There are a number of possible extensions to the simplest strategy, where within a single brain
region we learn an assembly for each concept class and classify based on which assembly is most
activated in response to an example. We compared the performance of various classiﬁcation models
on MNIST as the number of features increases. The high-level model is to extract a certain number
of features using one of the ﬁve different methods, and then ﬁnd the best linear classiﬁer (of the
training data) on these features to measure performance (on the test data). The ﬁve different feature
extractors are:
• Linear features. Each feature’s weights are sampled i.i.d. from a Gaussian with standard
deviation 0.1.
• Nonlinear features. Each feature is a binary neuron: it has 784 i.i.d. Bernoulli(0.2) weights,
and ‘ﬁres’ (has output 1, otherwise 0) if its total input exceeds the expected input (70 × 0.2).
11

DABAGIA PAPADIMITRIOU VEMPALA
Figure 3: Mean (dark line) and range (shaded area) of classiﬁcation accuracy for two stimulus
classes (left) and a ﬁxed linear threshold (right) over 20 trials, as the classes become more
separable. For stimulus classes, we vary the ﬁring probability of neurons in the stimulus
core while ﬁxing the probability for the rest at k/n, while for the linear threshold, we
vary the margin. For both we used 5 training examples with n = 1000, k = 100, p = 0.1,
and β = 0.1 (stimulus classes), β = 1.0 (linear threshold).
Figure 4: Mean (dark line) and range (shaded area) of classiﬁcation accuracy of two stimulus
classes for various values of the number of neurons (n, left) and the cap size (k, right).
For variable n, we let k = n/10; for variable k, we ﬁx n = 1000. Other parameters are
ﬁxed, as p = 0.1, r = 0.9, q = k/n, and β = 0.1.
• Large area assembly features. In a single brain area of size m with cap size m/10, we attempt
to form an assembly for each class. The area sees a sequence of 5 examples from each
class, with homeostasis applied after each class. Weights are updated according to Hebbian
plasticity with β = 1.0. Additionally, we apply a negative bias: A neuron which has ﬁred for
a given class is heavily penalized against ﬁring for subsequent classes.
12

LEARNING WITH ASSEMBLIES OF NEURONS
• ’Random’ assembly features. For a total of m features, we create m/100 different areas of
100 neurons each, with cap size 10. We then repeat the large area training procedure above in
each area, with the order of the presentation of classes randomized for each area.
• ’Split’ assembly features: For a total of m features, we create 10 different areas of m/10
neurons each, with cap size m/100. Area i sees a sequence of 5 examples from class i.
Weights are updated according to Hebbian plasticity, and homeostasis is applied after training.
After extracting features, we train the linear classiﬁcation layer to minimize cross-entropy loss on
the standard MNIST training set (60000 images) and ﬁnally test on the full test set (10000 images).
The results as the total number of features ranges from 1000 to 10000 is shown in Fig. 4. ’Split’
assembly features are ultimately the best of the ﬁve, with ’split’ features achieving 96% accuracy
with 10000 features. However, nonlinear features outperform ’split’ and large-area features and
match ’random’ assembly features when the number of features is less than 8000. For reference, the
linear classiﬁer gets to 89%, while a two-layer neural network with width 800 trained end-to-end
gets to 98.4%.
Going further, one could even create a hierarchy of brain areas, so that the areas in the ﬁrst
“layer” all project to a higher-level area, in hopes of forming assemblies for each digit in the higher-
level area which are more robust. In this paper, our goal was to highlight the potential to form useful
representations of a classiﬁcation dataset using assemblies, and so we concentrated on a single layer
of brain areas with a very simple classiﬁcation layer on top. It will be interesting to explore what is
possible with more complex architectures.
Figure 5: MNIST test accuracy as the number of features increases, for various classiﬁcation mod-
els. ’Split’ assembly features, which forms an assembly for class i in area i, achieves the
highest accuracy with the largest number of features.
13

DABAGIA PAPADIMITRIOU VEMPALA
5. Discussion
Assemblies are widely believed to be involved in cognitive phenomena, and the AC provides evi-
dence of their computational aptitude. Here we have made the ﬁrst steps towards understanding how
learning can happen in assemblies. Normally, an assembly is associated with a stimulus, such as
Grandma. We have shown that this can be extended to a distribution over stimuli. Furthermore, for
a wide range of model parameters, distinct assemblies can be formed for multiple stimulus classes
in a single brain area, so long as the classes are reasonably differentiated.
A model of the brain at this level of abstraction should allow for the kind of classiﬁcation that
the brain does effortlessly — e.g., the mechanism that enables us to understand that individual
frames in a video of an object depict the same object. With this in mind, the learning algorithm
we present is remarkably parsimonious: it generalizes from a handful of examples which are seen
only once, and requires no outside control or supervision other than ensuring multiple samples from
the same concept class are presented in succession (and this latter requirement could be relaxed in
a more complex architecture which channels stimuli from different classes). Finally, even though
our results are framed within the Assembly Calculus and the underlying brain model, we note
that they have implications far beyond this realm. In particular, they suggest that any recurrent
neural network, equipped with the mechanisms of plasticity and inhibition, will naturally form an
assembly-like group of neurons to represent similar patterns of stimuli.
But of course, many questions remain. In this ﬁrst step we considered a single brain area —
whereas it is known that assemblies draw their computational power from the interaction, through
the AC, among many areas. We believe that a more general architecture encompassing a hierarchy
of interconnected brain areas, where the assemblies in one area act like stimulus classes for others,
can succeed in learning more complex tasks — and even within a single brain area improvements
can result from optimizing the various parameters, something that we have not tried yet.
In another direction, here we only considered Hebbian plasticity, the simplest and most well-
understood mechanism for synaptic changes. Evidence is mounting in experimental neuroscience
that the range of plasticity mechanisms is far more diverse (Magee and Grienberger, 2020), and in
fact it has been demonstrated recently (Payeur et al., 2021) that more complex rules are sufﬁcient to
learn harder tasks. Which plasticity rules make learning by assemblies more powerful?
We showed that assemblies can learn nonnegative linear threshold functions with sufﬁciently
large margins. Experimental results suggest that the requirement of nonnegativity is a limitation
of our proof technique, as empirically assemblies readily learn arbitrary linear threshold functions
(with margin). What other concept classes can assemblies provably learn? We know from support
vector machines that linear threshold functions can be the basis of far more sophisticated learn-
ing when their input is pre-processed in speciﬁc ways, while the celebrated results of Rahimi and
Recht (2007) demonstrated that certain families of random nonlinear features can approximate so-
phisticated kernels quite well. What would constitute a kernel in the context of assemblies? The
sensory areas of the cortex (of which the visual cortex is the best studied example) do pre-process
sensory inputs extracting features such as edges, colors, and motions. Presumably learning by the
non-sensory brain — which is our focus here — operates on the output of such pre-processing.
We believe that studying the implementation of kernels in cortex is a very promising direction for
discovering powerful learning mechanisms in the brain based on assemblies.
14

LEARNING WITH ASSEMBLIES OF NEURONS
Acknowledgments
We thank Shivam Garg, Chris Jung, and Mirabel Reid for helpful discussions. MD is supported
by an NSF Graduate Research Fellowship. SV is supported in part by NSF awards CCF-1909756,
CCF-2007443 and CCF-2134105. CP is supported by NSF Awards CCF-1763970 and CCF-1910700,
and by a research contract with Softbank.
References
Richard Axel. Q & A. Neuron, 99:1110–1112, 2018.
Jonathan Binas, Ueli Rutishauser, Giacomo Indiveri, and Michael Pfeiffer. Learning and stabiliza-
tion of winner-take-all dynamics through interacting excitatory and inhibitory plasticity. Frontiers
in computational neuroscience, 8:68, 2014.
Gy¨orgy Buzs´aki. Neural syntax: cell assemblies, synapsembles, and readers. Neuron, 68(3), 2010.
Gy¨orgy Buzs´aki. The Brain from Inside Out. Oxford University Press, 2019.
Francesco d’Amore, Daniel Mitropolsky, Pierluigi Crescenzi, Emanuele Natale, and Christos H
Papadimitriou. Planning with biological neurons and synapses. arXiv preprint arXiv:2112.08186,
2021.
Graeme W Davis. Homeostatic control of neural activity: from phenomenology to molecular design.
Annu. Rev. Neurosci., 29:307–323, 2006.
Paul Erd˝os and Alfr´ed R´enyi. On the evolution of random graphs. Publ. Math. Inst. Hung. Acad.
Sci, 5(1):17–60, 1960.
Vitaly Feldman and Leslie G. Valiant. Experience-induced neural circuits that achieve high capacity.
Neural Computation, 21(10):2715–2754, 2009. doi: 10.1162/neco.2009.08-08-851.
Jordan Guerguiev, Timothy P Lillicrap, and Blake A Richards. Towards deep learning with segre-
gated dendrites. ELife, 6:e22901, 2017.
Kenneth D Harris and Alexander Thiele. Cortical state and attention. Nature reviews neuroscience,
12(9):509–523, 2011.
Donald Olding Hebb. The organization of behavior: A neuropsychological theory. Wiley, New
York, 1949.
John J Hopﬁeld. Neural networks and physical systems with emergent collective computational
abilities. Proceedings of the national academy of sciences, 79(8):2554–2558, 1982.
Barbara E Jones. Arousal systems. Front Biosci, 8(5):438–51, 2003.
R. Legenstein, W. Maass, C. H. Papadimitriou, and S. S. Vempala. Long-term memory and the
densest k-subgraph problem. In Proc. of 9th Innovations in Theoretical Computer Science (ITCS)
conference, Cambridge, USA, Jan 11-14. 2018, 2018.
15

DABAGIA PAPADIMITRIOU VEMPALA
Robert A Legenstein. Long term memory and the densest k-subgraph problem. In 9th Innovations
in Theoretical Computer Science Conference, 2018.
Timothy P. Lillicrap, Daniel Cownden, Douglas Blair Tweed, and Colin J. Akerman.
Random
synaptic feedback weights support error backpropagation for deep learning. In Nature communi-
cations, 2016.
Timothy P Lillicrap, Adam Santoro, Luke Marris, Colin J Akerman, and Geoffrey Hinton. Back-
propagation and the brain. Nature Reviews Neuroscience, pages 1–12, 2020.
Jeffrey C Magee and Christine Grienberger. Synaptic plasticity forms and functions. Annual review
of neuroscience, 43:95–117, 2020.
Daniel Mitropolsky, Michael J Collins, and Christos H Papadimitriou. A biologically plausible
parser. To appear in TACL, 2021.
Christos H Papadimitriou and Santosh S Vempala. Random projection in the brain and computation
with assemblies of neurons. In 10th Innovations in Theoretical Computer Science Conference,
2019.
Christos H Papadimitriou, Santosh S Vempala, Daniel Mitropolsky, Michael Collins, and Wolfgang
Maass. Brain computation by assemblies of neurons. Proceedings of the National Academy of
Sciences, 117(25):14464–14472, 2020.
Alexandre Payeur, Jordan Guerguiev, Friedemann Zenke, Blake A Richards, and Richard Naud.
Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits. Nature neu-
roscience, pages 1–10, 2021.
Steven T Piantadosi, Joshua B Tenenbaum, and Noah D Goodman. Bootstrapping in a language of
thought: A formal model of numerical concept learning. Cognition, 123(2):199–217, 2012.
Steven T Piantadosi, Joshua B Tenenbaum, and Noah D Goodman.
The logical primitives of
thought: Empirical foundations for compositional cognitive models. Psychological review, 123
(4):392, 2016.
Rodrigo Quian Quiroga. Neuronal codes for visual perception and memory. Neuropsychologia, 83:
227–241, 2016.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. Advances in
neural information processing systems, 20, 2007.
Akshay Rangamani and A Gandhi. Supervised learning with brain assemblies. Preprint, private
communication, 2020.
Joao Sacramento, Rui Ponte Costa, Yoshua Bengio, and Walter Senn. Dendritic error backpropaga-
tion in deep cortical microcircuits. arXiv preprint arXiv:1801.00062, 2017.
Jo˜ao Sacramento, Rui Ponte Costa, Yoshua Bengio, and Walter Senn. Dendritic cortical microcir-
cuits approximate the backpropagation algorithm. Advances in Neural Information Processing
Systems, 31:8721–8732, 2018.
16

LEARNING WITH ASSEMBLIES OF NEURONS
Charles Scott Sherrington. The Integrative Action of the Nervous System, volume 2. Yale University
Press, 1906.
Gina Turrigiano. Too many cooks? intrinsic and synaptic homeostatic mechanisms in cortical circuit
reﬁnement. Annual review of neuroscience, 34:89–103, 2011.
Leslie G. Valiant. Circuits of the mind. Oxford University Press, 1994. ISBN 978-0-19-508926-4.
Leslie G. Valiant. A neuroidal architecture for cognitive computation. J. ACM, 47(5):854–882,
2000. doi: 10.1145/355483.355486.
James CR Whittington and Rafal Bogacz. Theories of error back-propagation in the brain. Trends
in cognitive sciences, 23(3):235–250, 2019.
17

DABAGIA PAPADIMITRIOU VEMPALA
Appendix: Further experimental results
Figure 6: Assemblies learned for four stimulus classes. On the left, the distributions of ﬁring prob-
abilities over neurons; on the right, the average overlap of the assemblies. Each additional
class overlaps with previous ones, yet a simple readout over assembly neurons allows for
perfect classiﬁcation accuracy. Here, n = 103, k = 102, p = 0.1, r = 0.9, q = 0.1, β =
0.1, with 5 samples per class.
Figure 7: Assemblies formed during training. Each row is the response of the neural population to
examples from the respective class. At each step, a new example from the appropriate
class is presented. Due to plasticity, a core set emerges for the class after only a few
rounds. (Here, ﬁve rounds are shown.) Inputs are drawn from two stimulus classes, with
n = 103, k = 102, p = 0.1, r = 0.9, q = 0.1 and β = 0.1.
18

LEARNING WITH ASSEMBLIES OF NEURONS
Appendix: Proofs
Preliminaries
We will need a few lemmas. The ﬁrst is the well-known Berry-Esseen theorem:
Lemma 9
Let X1, . . . , Xn be independent random variables with E[Xi] = 0, E[X2
i ] = σ2
i and
E[|Xi|3] = ρi < ∞. Let
S =
 n
X
i=1
σ2
i
!−1/2  n
X
i=1
Xi
!
Denote by FS the CDF of S, and Φ the standard normal CDF. There exists some constant C such
that
sup
x∈R
|F(x) −Φ(x)| ≤C
 n
X
i=1
σ2
i
!−1/2
max
i
ρi
σ2
i
This implies the following:
Lemma 10
Let X1, . . . , X2n denote the weights of the edges incoming to a neuron in the brain
area from its neighbors (i.e. Xi = wi w.p. p, and 0 otherwise). Denote their sum as S = P
i Xi,
and consider the normal random variable
Y ∼N(E[S], Var[S])
Then for p = o(1), and wi = o(√np),
sup
x∈R
| Pr(S < t) −Pr(Y < t)| = o(1)
Proof Let ˜Xi = Xi −E[Xi]. We have
E[ ˜X2
i ] = p(1 −p)2w2
i + (1 −p)p2w2
i = p(1 −p)w2
i
and
E[| ˜Xi|3] = p(1 −p)3w3
i + (1 −p)p3w3
i ≤p(1 −p)w3
i
Let
˜S = S −E[S]
p
Var[S]
=
 2n
X
i=1
E[ ˜X2
i ]
!−1/2  2n
X
i=1
˜Xi
!
Then by Lemma 9, there exists some constant C so that
sup
x∈R
|F ˜S(x) −Φ(x)| ≤C
 2n
X
i=1
E[ ˜X2
i ]
!−1/2
max
i
E[| ˜Xi|3]
E[ ˜X2
i ]
As 1 ≤wi = o(√np), it follows that
2n
X
i=1
E[ ˜X2
i ] = p(1 −p)
2n
X
i=1
w2
i ≥2p(1 −p)n
19

DABAGIA PAPADIMITRIOU VEMPALA
and
E[| ˜Xi|3]
E[ ˜X2
i ]
≤wi = o(√np)
Hence,
sup
x∈R
|F ˜S(x) −Φ(x)| ≤C
wi
p
2p(1 −p)n
= o

1
√1 −p

= o(1)
for p = o(1). Noting that
Pr(S < t) = Pr
 
˜S < t −E[S]
p
Var[S
!
= F ˜S
 
t −E[S]
p
Var[S]
!
and using the afﬁne property of normal random variables completes the proof.
In particular, we may substitute appropriate Gaussian tail bounds (such as the one provided in the
following lemma) for tail bounds on sums of independent weighted Bernoullis throughout.
Lemma 11 For X ∼N(0, 1) and t > 0,
Pr(X > t) ≤
1
√
2πte−t2/2
For X ∼N(µ, σ2) and P(X > t) = p, we have
t = µ + σ
p
2 ln(1/p) + ln(2 ln(1/p)) + o(1)
Proof Recall that
Pr(X > t) =
Z ∞
t
1
√
2πe−τ 2/2dτ
Then observing that for τ ≥t,
e−τ 2/2 ≤τ
t e−τ 2/2
we have
Pr(X > t) ≤
1
√
2πt
Z ∞
t
τe−τ 2/2 =
1
√
2πte−t2/2
For the second part, simply solve for t.
Next, we will use the distribution of a normal random variable, conditioned on its sum with another
normal random variable.
Lemma 12 For X ∼N(µx, σ2
x), Y ∼N(µy, σ2
y), and Z = X + Y , then conditioning X on Z
gives
X|(Z = z) ∼N
 
σ2
x
σ2x + σ2y
z + σ2
yµx −σ2
xµy
σ2x + σ2y
,
σ2
xσ2
y
σ2x + σ2y
!
20

LEARNING WITH ASSEMBLIES OF NEURONS
Proof Bayes’ theorem provides
fX|Z=z(x, z) = fZ|X=x(z, x)fX(x)
fZ(z)
where fW is the probability density function for variable W. From the deﬁnition, fZ|X=x(z, x) =
fY (z −x). Then substituting the Gaussian probability density function and simplifying, we have:
fX|Z=z(x, z) =
1
√
2πσ2y exp

−(z−x−µy)2
2σ2y

1
√
2πσ2x exp

−(x−µx)2
2σ2x

1
√
2π(σ2x+σ2y) exp

−(z−(µx+µy))2
2(σ2x+σ2y)

=
1
r
2π
σ2xσ2y
σ2x+σ2y
exp

−
1
2
σ2xσ2y
σ2x+σ2y
 
x −
 
σ2
x
σ2x + σ2y
z + σ2
yµx −σ2
xµy
σ2x + σ2y
!!2

The following is the distribution of a binomial variable X, given that we know the value of another
binomial variable Y which uses X as its number of trials.
Lemma 13
Denote by B(n, p) the binomial distribution over n trials with probability of success
p. Let X P B(n, p) and Y |X ∼B(X, q). Then
X|Y ∼Y + B(n −Y, p(1−q)
1−pq )
Proof Via Bayes’ rule,
Pr(X = x|Y = y) = Pr(Y = y|X = x) Pr(X = x)
Pr(Y = y)
It is well-known that Y ∼B(n, pq). Hence, using the formulae for the distributions and simplifying,
Pr(X = x|Y = y) =
 x
y

qy(1 −q)x−y n
x

px(1 −p)n−x
 n
y

(pq)y(1 −pq)n−y
=
n −y
x −y
(p(1 −q))x−y(1 −p)n−x
(1 −pq)n−y
=
n −y
x −y
 p(1 −q)
1 −pq
x−y  1 −p
1 −pq
n−x
=
n −y
x −y
 p(1 −q)
1 −pq
x−y 
1 −p(1 −q)
1 −pq
n−x
Note that for Z ∼B(n −y, p(1−q)
1−pq ), we have
Pr(X = x|Y = y) = Pr(Z = x −y)
and so X|Y ∼Y + Z.
The next observation is useful: Exponentiating a random variable by a base close to one will increase
its concentration.
21

DABAGIA PAPADIMITRIOU VEMPALA
Lemma 14 Let X ∼N(µx, σ2
x) be a normal variable, and let Y = (1+β)X. Then Y is lognormal
with
E(Y ) = (1 + β)µx(1 + β)ln(1+β)σ2
x/2
Var Y = ((1 + β)ln(1+β)σ2
x −1)(1 + β)2µx(1 + β)ln(1+β)σ2
x
In particular, for ln(1 + β)σ2
x close to 0, Y is highly concentrated at (1 + β)µx.
Proof Observe that Y = eln(1+β)X, so it is clearly lognormal. So, deﬁne ˜X = ln(1 + β)X ∼
N(ln(1 + β)µx, ln(1 + β)2σ2
x). Then we have
E(Y ) = exp

E( ˜X) + 1
2 Var ˜X

= exp

ln(1 + β)µx + 1
2 ln(1 + β)2σ2
x

= (1 + β)µx(1 + β)ln(1+β)σ2
x/2
and
Var Y =

exp

Var ˜X

−1

exp

2E( ˜X) + Var ˜X

=
 exp
 ln(1 + β)2σ2
x

−1

exp
 2 ln(1 + β)µx + ln(1 + β)2σ2
x

= ((1 + β)ln(1+β)σ2
x −1)(1 + β)2µx(1 + β)ln(1+β)σ2
x
Furthermore, if ln(1 + β)σ2
x ≈0, then (1 + β)ln(1+β)σ2
x ≈1 and we obtain the concentration.
For learning a linear threshold function with an assembly (Theorem 6), we will require an additional
lemma.
Lemma 15
Let X1, . . . , Xn and Y1, . . . , Yn be independent Bernoulli variables, and let Z =
Pn
j=1 XiYi. Then for any t ≥0,
E(Yi|Z ≥E(Z) + t) ≥E(Yi)
Proof Bayes’ rule gives
E(Yi|Z ≥E(Z) + t) = Pr(Yi = 1|Z ≥E(Z) + t) = Pr(Z ≥E(Z) + t|Yi = 1) Pr(Yi = 1)
Pr(Z ≥E(Z) + t)
Then observe that the events Z ≥EZ + t and Yi = 1 are positively correlated, and so
Pr(Z ≥EZ + t|Yi = 1) ≥Pr(Z ≥EZ + t)
Then substituting gives
E(Yi|Z ≥E(Z) + t) ≥Pr (Z ≥E(Z) + t) Pr(Yi = 1)
Pr(Z ≥E(Z) + t)
= E(Yi)
as required.
Lastly, the following lemma allows us to translate a bound on the weight between certain synapses
into a bound on the number of rounds (or samples) required.
22

LEARNING WITH ASSEMBLIES OF NEURONS
Lemma 16
Consider a neuron i, connected by a synapse to a neuron j with weight initially 1,
and equipped with a plasticity parameter β. Assume that j ﬁres with probability p and i ﬁres with
probability q on each round, and that there are at least T rounds, with
T ≥1
pq
ln γ
ln(1 + β)
Then the synapse will have weight at least γ in expectation.
We are now equipped to prove the theorems.
Proof of Theorem 1
Let µt be the fraction of ﬁrst-timers in the cap on round t. The process stabilizes when µt < 1/k,
as then no new neurons have entered the cap.
For a given neuron i, let X(t) and Y (t) denote the input from connections to the k neurons in
SA and the n −k neurons outside of SA, respectively, on round t. For a neuron which has never
ﬁred before, they are distributed approximately as
X(t) ∼N(kpr, kpr)
Y (t) ∼N(kpq, kpq)
which follows from Lemma 10, for a total input of X(t) + Y (t) ∼N(kpr + kpq, kpr + kpq).
(Note that we ignore small second-order terms in the variance.) To determine which neurons will
make the cap on the ﬁrst round, we need a threshold that roughly k of n draws from X(1) + Y (1)
will exceed, with constant probability. In other words, we need the probability that X(1) + Y (1)
exceeds this threshold to be about k/n. Taking L = 2 ln(n/k) and using the tail bound in Lemma
11, we ﬁnd the threshold for the ﬁrst cap to be at least
C1 = kp(r + q) +
p
kp(r + q)L
On subsequent rounds, there is additional input from connections to the previous cap, distributed
as N(kp, kp(1 −p)). Using µt as the fraction of ﬁrst-timers, a ﬁrst-time neuron must be in the top
µtk of the n −k ∼n neurons left out of the previous cap. The activation threshold is thus
Ct = kp(1 + r) + kpq +
p
kp(1 + r + q)(L + 2 ln(1/µt))
Now consider a neuron i which ﬁred on the ﬁrst round. We know that X(1) + Y (1) ≥C1, so
using Lemma 12,
X(1)|(X(1) + Y (1) = C1) ∼N

r
r + qC1, kp rq
r + q

If X(1) = x, Lemma 13 indicates that the true number of connections with stimulus neurons is
distributed roughly as ˜X|(X(1) = x) ∼N(x + (k −x)p(1 −r), (k −x)p(1 −r)). Conditioning
on X(1) + Y (1) = C1, ignoring second-order terms, and bounding the variance as kp, we have
˜X|(X(1) + Y (1) = C1) ∼N

kp(1 −r) +
r
r + qC1, kp

23

DABAGIA PAPADIMITRIOU VEMPALA
On the second round, the synapses between neuron i and stimulus neurons which ﬁred have
had their weights increased by a factor of 1 + β, and these stimulus neurons will ﬁre on the second
round with probability r. An additional ˜X −X(1) stimulus neurons have a chance to ﬁre for the
ﬁrst time. Neuron i also receives recurrent input from the k other neurons which ﬁred the previous
round, which it is connected to with probability p. So, the total input to neuron i is roughly
N

(1 + β) r2
r + qC1 + kpr(1 −r), kp

1 +
rq
r + q

+ N(kp(1 + q), kp(1 + q))
In order for i to make the second cap, we need that its input exceeds the threshold for ﬁrst-timers,
i.e.
(1 + β) r2
r + qC1 + kp(1 + r(1 −r) + q) + Z ≥C2
where Z ∼N (0, kp(1 + r + q)). Taking µ = µ2, we have the following:
Pr(i ∈C2|i ∈C1) = 1 −µ
≥Pr

Z ≥C2 −(1 + β) r2
r + qC1 −kp(1 + r(1 −r) + q)

≥Pr

Z ≥−βkpr2 −(1 + β)
r2
√r + q
p
kpL +
p
kp(1 + r + q)(L + 2 ln(1/µ))

Now, normalizing Z to N(0, 1) we have (again by the tail bound)
1 −µ ≥1 −exp


−

β√kpr2 + (1 + β)
r2
√r+q
√
L +
p
(1 + r + q)(L + 2 ln(1/µ))
2
2(1 + r + q)



More clearly, this means
p
2(1 + r + q) ln(1/µ) ≤β
p
kpr2 + (1 + β)
r2
√r + q
√
L +
p
(1 + r + q)(L + 2 ln(1/µ))
Then taking
β ≥β0 =
√r + q
r2
√1 + r + q −
r2
√r+q
 √
L +
p
2(1 + r + q)
√kp +
√
L
gives µ ≤1/e, i.e. the overlap between the ﬁrst two caps is at least a 1 −1/e fraction.
Now, we seek to show that the probability of a neuron leaving the cap drops off exponentially
the more rounds it makes it in. Suppose that neuron i makes it into the ﬁrst cap and stays for t
consecutive caps. Each of its connections with stimulus neurons will be strengthened by the number
of times that stimulus neuron ﬁred, roughly N(tr, tr(1 −r)) times. Using Lemma 14, the weight
of the connection with a stimulus neuron is highly concentrated around (1 + β)tr. Furthermore we
know that i has at least ˜X|(X(1) + Y (1) = C1) ∼N

kp(1 −r) +
r
r+qC1, kp

such connections,
of which N

kpr(1 −r) +
r2
r+qC1, kpr

will ﬁre. So, the input to neuron i will be at least
(1 + β)tr(kpr(1 −r) +
r2
r + qC1) + kp(1 + q) + Z
24

LEARNING WITH ASSEMBLIES OF NEURONS
where Z ∼N(0, kp(1 + (1 + β)2trr + q))
To stay in the (t + 1)th cap, it sufﬁces that this input is greater than Ct+1, the threshold for
ﬁrst-timers. Using µ = µt+1 and reasoning as before:
Pr(i ∈Ct+1|i ∈C1 ∩. . . ∩Ct) = 1 −µ
≥Pr

Z > Ct+1 −(1 + β)tr

kpr(1 −r) +
r2
r + qC1

−kp(1 + q)

= Pr

Z > −tβkpr −(1 + trβ)
r2
√r + q
p
kpL +
p
kp(1 + r + q)(L + 2 ln(1/µ))

≥1 −exp


−

trβ√kp + (1 + trβ)
r2
√r+q
√
L −
p
(1 + r + q)(L + 2 ln(1/µ))
2
2(1 + r + q)



where in the last step we approximately normalized Z to N(0, 1). Then
β ≥1
tr2
p
(1 + r + q)(L + 2t2) −r2)
√
L + t
√
2
√kp +
√
L
will ensure µ ≤e−t2, which is no more than β0.
Now, let neuron i be a ﬁrst time winner on round t. Let X ∼N(kpr, kpr) denote the input
from stimulus neurons, Y ∼N(kp, kp) the input from recurrent connections to neurons in the
previous cap, and Z ∼N(kpq, kpq) the input from nonstimulus neurons. Then conditioned on
X + Y + Z = Ct, the second lemma indicates that
X|(X + Y + Z = Ct) ∼N

r
1 + r + qCt, kp r(1 + q)
1 + r + q

Y |(X + Y + Z = Ct) ∼N

1
1 + r + qCt, kp
r + q
1 + r + q

So, the input on round t + 1 is at least
(1 + β)1 −µt + r2
1 + r + q Ct + kpr(1 −r) + kpµt + kpq + Z
where Z ∼N

0, kp

(1 + β)2 r2(1+q)+(1−µt)(r+q)
1+r+q
+ r(1 −r) + q

. By the usual argument we
have
Pr(i ∈Ct+1|i ∈Ct) = 1 −µt+1
≥Pr

Z ≥Ct+1 −(1 + β)1 −µt + r2
1 + r + q Ct −kpr(1 −r) −kpµt −kpq

So, we will have µt+1 < e−1µt when
β ≥
1
1 −µt + r2
r(1−r)+q+µt
√1+r+q
p
L + 2 ln(1/µt) +
p
2 ln(1/µt)
√kp +
q
L+2 ln(1/µt)
1+r+q
25

DABAGIA PAPADIMITRIOU VEMPALA
which is smaller than β0. Assuming r + q ∼1, we may simplify β0, so that
β0 = 1
r2
 √
2 −r2 √
L +
√
6
√kp +
√
L
So, if β ≥β0, the probability of leaving the cap once in the cap t times drops off exponentially.
We can conclude that no more than ln(k) rounds will be required for convergence. Additionally,
assuming that a neuron enters the cap at time t, let 1 −pτ denote the probability it leaves after τ
rounds. Then its probability of staying in the cap on all subsequent rounds is
Y
τ≥1
pτ ≥
Y
τ≥1
 
1 −exp
 
−τ 2
 β
β0
2!!
≥1 −exp
 
−
 β
β0
2!
Thus, every neuron that makes it into the cap has a probability at least 1 −exp(−(β/β0)2)) of
making every subsequent cap, so the total support of all caps together is no more than k/(1 −
exp(−(β/β0)2))) in expectation.
■
Proof of Theorem 3
Let µ denote the fraction of newcomers in the cap. A neuron in A∗can expect an input of
Xa = γkpr + kpq + Za
where Za ∼N(0, γ2kpr + kpq), while neurons outside of A∗can expect an input of
X = kpr + kpq + Z
where Z ∼N(0, kpr + kpq). Then the threshold is roughly
C1 = kpr + kpq +
p
kp(r + q)(L + 2 ln(1/µ))
For a neuron i in A∗to make the cap, it needs to exceed this threshold. We have
Pr(i ∈C1|i ∈A∗) = 1 −µ
≥Pr(Xa ≥C1)
= Pr

Za ≥−(γ −1)kpr +
p
kp(r + q)(L + 2 ln(1/µ))

Applying the tail bound gives
p
2 ln(1/µ) ≤(γ −1)
r
√r + q
p
kp −
p
L + 2 ln(1/µ)
so for r + q ∼1 and
γ ≥1 + 1
√r
√
2 +
p
L/kpr + 2

we will have µ ≤e−kpr.
■
26

LEARNING WITH ASSEMBLIES OF NEURONS
Proof of Theorem 4
Let νt be the fraction of neurons in A∗included in the cap on round t, and let µt be the fraction of
true ﬁrst-timers. The input on the ﬁrst round for ﬁrst-timers will be N(kpr, kpr) + N(kpq, kpq),
while for neurons in A∗will be
N(γαkpr, γ2αkpr) + N((1 −α)kpr, (1 −α)kpr) + N(kpq, kpq)
For a neuron not in A∗to make the cap, it needs to be in the top (1 −ν1)k of n −k ∼n draws.
Thus, the threshold is at least
C1 = kp(r + q) +
p
kp(r + q)(L −2 ln(1 −ν1))
For a neuron in A∗to make the cap, it needs to exceed this threshold. Thus, we have
Pr(i ∈C1|i ∈A∗) = ν1
≤Pr(Z > C1 −γαkpr −(1 −α)kpr −kpq)
= Pr

Z > −(γ −1)αkpr +
p
kp(r + q)(L −2 ln(1 −ν1))

≤exp

−(
p
(r + q)(L −2 ln(1 −ν1)) −(γ −1)αr
p
kp)2/2

Rearranging we have
p
2 ln(1/ν1) ≤
p
(r + q)(L −2 ln(1 −ν1)) −(γ −1)αr
p
kp
Thus, so as long as
γ ≤1 +
p
(r + q)L −
p
2 ln((1 + r)/rα)
αr√kp
we will have ν1 ≤rα/(1 + r). On any round t after the ﬁrst, a neuron in SA \ Ct−1 will receive
(1 −α)kpr + γαkpr + γνt−1kp + (1 −νt−1)kp + kpq
while the threshold for ﬁrst-timers is at least
kp(1 + r + q) +
p
kp(1 + r + q)(L + 2 ln(1/µt))
where µt ≤e−t2β/β0, as in the proof of Theorem 1. The neurons in SA which make the cap on round
t + 1 consist of those that made the previous cap and this one, and those that are ﬁrst-timers. From
Theorem 1, we know Pr(i ∈Ct+1|i ∈Ct) ≥1 −e−t2, so take Pr(i ∈Ct+1|i ∈SA ∩Ct) ∼νt.
We need only to ﬁnd Pr(i ∈Ct+1|i ∈SA \ Ct) = ν′
t. On the second round, we have ν1 =
rα
1+r. So,
letting Z ∼N(0, kp(γ2 −1)α(1 + r) + kpq), it follows that:
Pr(i ∈C2|i ∈SA \ C1) = ν′
2
≤Pr

Z > −(γ −1)α(2 + r)
1 + r
)kpr +
p
kp(1 + r + q)(L)

≤exp
 
−1
2
p
(1 + r + q)(L) −αr(2 + r)
1 + r
)
p
kp
2!
27

DABAGIA PAPADIMITRIOU VEMPALA
and so if
γ ≤1 +
p
(1 + r + q)L −
p
2 ln((1 + r)/α)
α r2+2r
1+r
√kp
will ensure that ν′
2 ≤α/(1 + r), which is very nearly the bound for the ﬁrst cap. Thus, no more
than an α fraction of neurons in the second cap are in SA.
Now, we seek to show that if νt ≤α, then we will have νt+1 ≤α + 1/k, since this will ensure
that no new neurons from SA have entered the cap. Reasoning as before, let Pr(i ∈Ct+1|i ∈
SA \ Ct) = ν′
t+1, and then we have
Pr(i ∈Ct+1|i ∈SA \ Ct) = ν′
t+1
≤Pr

Z > −2(γ −1)αkpr +
p
kp(1 + r + q)(L + 2 ln(1/µt))

≤exp

−
p
(1 + r + q)(L + 2 ln(1/µt)) −2(γ −1)αr
p
kp
2
/2

Then solving for γ, we ﬁnd that we will have ν′
t ≤1/k as long as
γ ≤1 +
p
(1 + r + q)(L + 2t2) −
p
2 ln(k)
2αr√kp
which is the least upper bound so far. Thus, taking r + q ∼1, so long as
γ ≤1 +
√
L −
p
2 ln((1 + r)/rα)
αr√kp
the overlap of any cap with A∗will never exceed αk neurons. By Theorem 1, an assembly B will
form with high probability after ln(k) examples, and we conclude that |A∗∩B∗| ≤αk.
■
Proof of Theorem 5
Let µ be the fraction of ﬁrst-timers included in the cap C1, and ν be the fraction of neurons in A∗
in the cap. A neuron in B∗receives
Xb = γkpr + kpq + Zb
where Zb ∼N(0, γ2kpr + kpq) while a neuron in A∗receives
Xa = γαkpr + γ(1 −α)kp
qk
n

+ kpq + Za
= γαkpr + kpq + Za + O(1)
where Za ∼N(0, γ2αkpr + kpq). The threshold to be in the top νk of |A∗| ∼k draws from this
distribution is
C1 = γαkpr + kpq +
p
2kp(γ2αr + q) ln(1/ν)
28

LEARNING WITH ASSEMBLIES OF NEURONS
Neurons in B∗will make the ﬁrst cap if they exceed this threshold. So, we have
Pr(i ∈C1|i ∈B∗) = 1 −ν
≥Pr(Xb ≥C1)
= Pr

Zb ≥−γ(1 −α)kpr +
p
2kp(γ2αr + q) ln(1/ν)

≥1 −exp


−1
2

γ(1 −α)r√kp −
p
2(γ2αr + q) ln(1/ν)
2
γ2r + q



where the last step follows from Lemma 11. Solving for ν gives
ν ≤exp

−(1 −α)2kpr
2(1 + α)

Similarly, neurons in neither of the two assembly cores must also exceed the threshold to make the
cap, which means
Pr(i ∈C1|i ̸∈A∗∪B∗) = µ
≤Pr(X > C1)
= Pr

Z > (γα −1)kpr +
p
2kp(γ2αr + q) ln(1/ν)

≤exp

−
1
2(r + q)

(γα −1)r
p
kp +
p
2(γ2αr + q) ln(1/ν)
2
≤exp

−1
2(γα −1)2kpr

· νγ2α
Then the fraction of neurons in C1 \ B∗will be
ν + µ ≤exp

−(1 −α)2kpr
2(1 + α)

+ exp

−1
2(γα −1)2kpr

■
Proof of Theorem 6
For each round 1, . . . , t, . . ., deﬁne µt to be the fraction of neurons ﬁring for the ﬁrst time on that
round, and let X(t) be an example sampled from D+. For each neuron i, let W i
j(t) represent the
weight of the synapses between neuron i and input neuron j on round t. Since each synapse is
present independently with probability p, the number of synapses (i.e. the norm ∥W i∥0) between
the input region and neuron i is a binomial random variable, with expectation np. Furthermore, the
Chernoff bound shows that the number of synapses is sharply concentrated about its mean as
Pr(|∥W i∥0 −np| ≥n1−ϵp) ≤2 exp

−n1−2ϵp
3

for any ϵ > 0. Thus, once normalized, the weight of a particular synapse W i
j(0) will be between,
say,
1
np−n1/3p and
1
np+n1/3p with high probability. Since both of the quantities approach 1
np for large
enough n, we will regard the weight of every synapse at the outset as 1
np, so that EW i
j(0) =
p
np = 1
n.
29

DABAGIA PAPADIMITRIOU VEMPALA
If i has not yet ﬁred, then the round t input W i(t) · X(t) is approximately normal, with mean
E(W i(t) · X(t)) =
n
X
j=1
E(W i
j(t))E(Xj(t)) ≥
n
X
j=1
1
n
k
n + ∆vj

= k
n + ∆
n ∥v∥1
and variance approximately
k
n2p. So, using Lemma 11, a neuron will be in the top µtk of the
approximately n neurons which have never ﬁred before if its input from X(1) exceeds
C1 = k
n + ∆
n ∥v∥1 +
√kpL
np
where L = 2 ln(n/k), and including input from the previous cap N(k/n, k/n2p) on subsequent
rounds,
Ct = 2k
n + ∆
n ∥v∥1 + 1
np
p
2kp(L + 2 ln(1/µt))
Now, let i be a neuron which made the ﬁrst cap. Then W i(1) · X(1) ≥C1 and each nonzero
component in W i(1) will be increased by a factor of 1+β if the corresponding component of X(1)
was nonzero as well. By Lemma 15, the conditional expectation of W i
j(2) will be
E(W i
j(2)|W i(1) · X(1) ≥C1) ≥(1 + β Pr(X(1) = 1)) E(W i
j(1))
≥1
n + β 1
n
k
n + ∆vj

Then we have
E(W i(2) · X(2)|i ∈C1) =
n
X
j=1
E(W i
j(2)|i ∈C1)E(Xj(2))
≥
n
X
j=1
1
n
k
n + ∆vj

+ β 1
n
k
n + ∆vj
2
≥k
n + ∆
n ∥v∥1 + β∆2
n
Then letting Y ∼N(k/n, k/n2p) denote the input from the previous cap, we have
Pr(i ∈C2|i ∈C1) = 1 −µ2
≥Pr(W i(2) · X(2) + Y ≥C2)
Taking Z ∼N(0, k/n2p) to be an underestimate of the variance,
1 −µ2 ≥Pr

Z ≥1
np
p
2kp(L + 2 ln(1/µ2)) −β∆2
n

≥1 −exp

−1
2kp

β∆2p −
p
2kp(L + 2 ln(1/µ2))
2
30

LEARNING WITH ASSEMBLIES OF NEURONS
following from the tail bound in Lemma 11. Then so as long as
∆2β ≥
p
2k(L + 2) +
√
2k
√p
we will have µ2 ≤1/e.
Now, suppose i ﬁred on all of the ﬁrst t rounds. Input neuron j is expected to ﬁre at least
( k
n + ∆vi)t times, so by Lemma 14 the weight of an extant synapse W i
j(t + 1) will be concentrated
about its mean, which is at least
1
np(1 + β)( k
n+∆vi)t. The expected input W i(t + 1) · X(t + 1) is
thus
E(W i(t + 1) · X(t + 1)|i ∈C1 ∩. . . ∩Ct) =
n
X
j=1
E(W i
j(t + 1)|i ∈C1 ∩. . . ∩Ct)ˆE(Xj(t + 1))
≥
n
X
j=1
1
n
k
n + ∆vj

+ βt 1
n
k
n + ∆vj
2
≥k
n + ∆
n ∥v∥1 + ∆2
n βt
Then including recurrent input Y ∼N(kp, kp), we have
Pr(i ∈Ct+1|i ∈C1 ∩. . . ∩Ct) = 1 −µt+1
≥Pr(W i(t + 1) · X(t + 1) + Y ≥Ct+1)
Again taking Z ∼N(0, k/n2p) to be the variance, we have
1 −µt+1 ≥Pr

Z ≥1
np
p
2kp(L + 2 ln(1/µt+1)) −∆2
n βt

≥1 −exp

−1
2kp

∆2βtp −
p
2kp(L + 2 ln(1/µt+1))
2
again following from the tail bound. Then we will have µt+1 ≤e−t as long as
∆2β ≥
p
2k(L + 2t) +
√
2kt
t√p
which is certainly no more than β0.
So, if β ≥β0, the probability of a neuron that made the cap t times missing the next one drops
off exponentially. It follows that after ln(k) rounds the process will have converged, and each neuron
in a particular cap has a probability of at least 1 −exp(−(β/β0)2)) of making every subsequent
cap. So, the total support of all caps together is no more than k/(1 −exp(−(β/β0)2))) = k + o(k)
in expectation.
Now, suppose the training process continues until the cap converges to some set A∗of size
k + o(k), which takes roughly ln(k) rounds with high probability. For neuron i ∈A∗, the weight
of an extant synapse W i
j(ln(k)) will be, in expectation,
W i
j(ln(k)) = 1
np(1 + β)( k
n+∆vj) ln(k)
31

DABAGIA PAPADIMITRIOU VEMPALA
and on average over the neurons in A∗, we will have
n
X
j=1
W i
j ≥
n
X
j=1
p
np(1 + β)( k
n+∆vj) ln(k) ≥1 + β ln(k)
n
∆∥v∥1
The neurons are then allowed to return to rest, and each neuron renormalizes the weights of its
incoming synapses to sum to 1, so that
W i
j = (1 + β)( k
n+∆vj) ln(k)
np + βp ln(k)∆∥v∥1
+ o(n−1)
Choose examples X+ and X−at random, so that
v⊤X+ ≥k
n∥v∥1 + ∆
v⊤X−≤k
n∥v∥1 −∆
Letting C+, C−denote the caps formed after presenting the respective example once, deﬁne ϵ+ to
be the fraction of neurons in C+ formed in response to X+ which are not in A∗, and ϵ−to be the
fraction of neurons in C−which are in A∗, i.e.
ϵ+ = |C+ \ A∗|
k
ϵ−= |C−∩A∗|
k
As before, the input for a neuron i outside of A∗will be nearly normal with expectation
E(W i · X+) =
n
X
j=1
E(W i
j)E(X+
j )
≥
n
X
j=1
1
n
k
n + ∆vj

= k
n + ∆
n ∥v∥1
and variance nearly Z ∼N(0, k/n2p), so that the threshold to make the top ϵ+k neurons will be at
least
C+ = k
n + ∆
n ∥v∥1 + 1
np
p
kp(L + 2 ln(1/ϵ+))
Now, consider i ∈A∗. Its input in expectation will be
E(W i · X+|i ∈A∗) =
n
X
j=1
E(W i
j)E(X+
j )
≥
n
X
j=1
k
n + ∆vj
np + βp ln(k)∆∥v∥1
+ β ln(k)
  k
n + ∆vj
2
np + βp ln(k)∆∥v∥1
≥k + ∆∥v∥1 + β ln(k)∆2
np + βp ln(k)∆∥v∥1
32

LEARNING WITH ASSEMBLIES OF NEURONS
while its variance will be
Var(W i · X+|i ∈A∗) = E((W i · X+)2|i ∈A∗) −E(W i · X+|i ∈A∗)2
≥
n
X
j=1
p
k
n + ∆vj

(1 + β)( k
n+∆vj)2 ln(k)
(np + β ln(k)p∆∥v∥1)2
≥
k
(n + β ln(k)∆∥v∥1)2p
Then letting Z ∼N(0,
k
(n+β ln(k)∆∥v∥1)2p) denote the uncertainty, we have
Pr(i ∈C+|i ∈A∗) = 1 −ϵ+
≥Pr(W i · X+ ≥C+)
≥Pr
 
Z ≥β ln(k)
∆∥v∥1
n
(k + ∆∥v∥1) −∆2
n + β ln(k)∆∥v∥1
+ 1
np
p
kp(L + 2 ln(1/ϵ+))
!
Then using Lemma 11,
ϵ+ ≤exp

−1
2kp

pβ ln(k)

∆2 −∆∥v∥1(k+∆∥v∥1)
n

−n+β ln(k)∆∥v∥2
1
n
p
kp(L + 2 ln(1/ϵ+))
2
So, for ﬁxed ϵ+ we need
∆2 −∆∥v∥1
n
(k + ∆∥v∥1) ≥

1 + β ln(k)∆∥v∥1
n
 p
kp(L + 2 ln(1/ϵ+)) +
p
2kp ln(1/ϵ+)
pβ ln(k)
Now, note that if ∆≥2k
√n, recalling that ∥v∥1 ≤√n/2, we have must have ∆≥
k∥v∥1
n/2−∥v∥2
1 , and so
∆2 −∆∥v∥1
n
(k + ∆∥v∥1) = ∆2 −

∆
k∥v∥1
n/2 −∥v∥2
1
n/2 −∥v∥2
1
n
+ ∆2∥v∥2
1
n

≥∆2 −
∆2
2 −∆2∥v∥2
1
n
+ ∆2∥v∥2
1
n

= ∆2
2
Additionally, if ∆≥2
q
k
np
p
L + 2 ln(1/ϵ+) (a much milder bound compared to the previous one,
for p = Ω(k−1) and ϵ+ ﬁxed) then it sufﬁces that
∆2β ≥
4
√
k
ln(k)√p
p
L + 2 ln(1/ϵ+) +
p
2 ln(1/ϵ+)

33

DABAGIA PAPADIMITRIOU VEMPALA
Now, consider the example X−. The input to a neuron i outside of A∗will be nearly normal
with expectation
ˆE(W i · X−) =
n
X
j=1
E(W i
j)E(X−
j )
=
n
X
j=1
k
n2
= k
n
For i ∈A∗,
E(W i · X−|i ∈A∗) =
n
X
j=1
ˆE(W i
j)E(X−
j )
≥
n
X
j=1
(1 + β)( k
n+∆vj) ln(k)
P(1 + β)( k
n+∆vl) ln(k)
k
n
= k
n
with variance no larger than
k
n2p. It follows that the threshold for a neuron in A∗to make the top
ϵ−k of the k neurons in A∗will be no more than
C−= k
n +
p
2kp ln(1/ϵ−)
np
So, for Z ∼N(0, k/n2p) we have
Pr(i ∈C−|i ̸∈A∗) = (1 −ϵ−)k
n
≥Pr
 W i · X−≥C−)

≥Pr
 
Z ≥
p
2kp ln(1/ϵ−)
np
!
= ϵ−
Rearranging shows that
ϵ−≤
k
n
1 + k
n
≤k
n
34

LEARNING WITH ASSEMBLIES OF NEURONS
Now, comparing all of above bounds on ∆when ϵ+ ≤1/e:
∆≥2k
√n
(1)
∆≥2
s
k
np
√
L + 2
(2)
∆2β ≥
4
√
k
β ln(k)√p
√
L + 2 +
√
2

(3)
∆2β ≥
p
2k(L + 2) +
√
2k
√p
(4)
Since k ≤n, and assuming β is no larger than a constant, the bound in (4) is the strongest. Thus,
taking
∆2β ≥
p
2k(L + 2) +
√
2k
√p
and ∆≥
√
L + 2 is sufﬁcient to ensure that the assembly creation process converges within ln(k)
steps, and ϵ+, ϵ−≤1/e. It follows that if more than half (resp. less than half) of the neurons in A∗
ﬁre in response to any given example, it is a positive (resp. negative) one with high probability.
■
35

