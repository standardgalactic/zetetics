arXiv:2103.00915v1  [math.OC]  1 Mar 2021
TSSOS: A JULIA LIBRARY TO EXPLOIT SPARSITY FOR
LARGE-SCALE POLYNOMIAL OPTIMIZATION
VICTOR MAGRON AND JIE WANG
Abstract. The Julia library TSSOS aims at helping polynomial optimizers to
solve large-scale problems with sparse input data. The underlying algorithmic
framework is based on exploiting correlative and term sparsity to obtain a new
moment-SOS hierarchy involving potentially much smaller positive semideﬁ-
nite matrices. TSSOS can be applied to numerous problems ranging from power
networks to eigenvalue and trace optimization of noncommutative polynomi-
als, involving up to tens of thousands of variables and constraints.
1. Introduction
The TSSOS library intends to address large-scale polynomial optimization prob-
lems, where the polynomials in the problem’s description involve only a small num-
ber of terms compared to the dense ones.
The library is available at https://github.com/wangjie212/TSSOS. The ultimate
goal is to provide semideﬁnite relaxations that are computationally much cheaper
than those of the standard SOS-based hierarchy [11] or its sparse version [12, 21]
based on correlative sparsity by taking into account the so-called term sparsity.
Existing algorithmic frameworks based on correlative sparsity have been imple-
mented in the SparsePOP solver [22] and many applications of interest have been
successfully handled, for instance certiﬁed roundoﬀerror bounds [13, 14], optimal
powerﬂow problems [9], volume computation [19], dynamical systems [18], non-
commutative POPs [10], Lipschitz constant estimation of deep networks [5, 6] and
sparse positive deﬁnite functions [17].
Throughout the paper, we consider the following formulation of the polynomial
optimization problem (POP):
(1.1)
(Q) :
ρ∗= inf
x { f(x) : x ∈K },
where the objective function f is assumed to be a polynomial in n variables x =
(x1, . . . , xn) and the feasible region K ⊆Rn is assumed to be deﬁned by a ﬁnite
conjunction of m polynomial inequalities and t polynomial equalities, namely
(1.2)
K := {x ∈Rn : g1(x) ≥0, . . . , gm(x) ≥0, h1(x) = 0, . . . , ht(x) = 0},
for some polynomials g1, . . . , gm, h1, . . . , ht in x.
For the sake of simplicity, we
assume in what follows that there are only inequalities in (1.2), i.e., t = 0.
A
nowadays well-established scheme to handle (Q) is the moment-SOS hierarchy [11],
where SOS is the abbreviation of sum of squares.
The moment-SOS hierarchy
Date: March 2, 2021.
2010 Mathematics Subject Classiﬁcation. Primary, 14P10,90C25; Secondary, 12D15,12Y05.
Key words and phrases. moment-SOS hierarchy, correlative sparsity, term sparsity, TSSOS,
large-scale (non)-commutative polynomial optimization, optimal power ﬂow.
1

2
V. MAGRON AND J. WANG
provides a sequence of semideﬁnite programming (SDP) relaxations, whose optimal
values are non-decreasing lower bounds of the global optimal value ρ∗of (Q). Under
certain mild conditions, the sequence of lower bounds is guaranteed to converge to
ρ∗generically in ﬁnite many steps. Despite of this beautiful theoretically property,
the bottleneck of the scheme is that the size of the SDP relaxations become quickly
intractable as n increases. Hence in order to improve the scalability, it is crucial
to fully exploit the structure of POP (1.1) to reduce the size of these relaxations.
A commonly present structure in large-scale polynomial optimization is sparsity.
In view of this, TSSOS implements the sparsity-adapted moment-SOS hierarchies.
Here the terminology “sparsity” is referred to the well-known correlative sparsity,
or the newly proposed term sparsity [26, 27], or the combined both [28]. The idea
of exploitation of sparsity behind TSSOS is not restricted to POPs, but can be
also applied to other SOS optimization problems, e.g., the computation of joint
spectral radii [24] or learning of linear dynamical systems [29, 30]. TSSOS can be
also combined with eﬃcient ﬁrst-order methods to speed up the computation of the
SDP relaxations themselves [15, 16].
2. Algorithmic background and overall description
Let dj = ⌈deg(gj)/2⌉, j = 1, . . . , m and dmin = max{⌈deg(f)/2⌉, d1, . . . , dm}.
The moment hierarchy indexed by the integer d ≥dmin for POP (1.1) is deﬁned
by:
(2.1)
(Qd) :









inf
Ly(f)
s.t.
Md(y) ⪰0,
Md−dj(gjy) ⪰0,
j ∈[m],
y0 = 1.
Here Md(y) is the d-th order moment matrix, Md−dj(gjy) is the (d −dj)-th order
localizing matrix (see [11] for more details) and [m] := {1, . . . , m} for a positive
integer m. The index d is called the relaxation order of the hierarchy. Note that if
d < dmin, then (Qd) is infeasible.
2.1. Correlative sparsity. By exploiting correlative sparsity, we decompose the
set of variables into subsets and then construct moment (localizing) matrices for
each subset. Fix a relaxation order d ≥dmin. Let J′ := {j ∈[m] | dj = d}. For
a polynomial h = P
α hαxα (xα := xα1
1 · · · xαn
n ), the support of h is deﬁned by
supp(h) := {α ∈Nn | hα ̸= 0}. For α = (αi) ∈Nn, let supp(α) := {i ∈[n] |
αi ̸= 0}. The correlative sparsity pattern (csp) graph associated with POP (1.1) is
deﬁned to be the graph Gcsp with nodes V = [n] and edges E satisfying {i, j} ∈E
if one of followings holds:
(i) there exists α ∈supp(f) ∪S
j∈J′ supp(gj) such that {i, j} ⊆supp(α);
(ii) there exists k ∈[m]\J′ such that {i, j} ⊆S
α∈supp(gk) supp(α).
Let G
csp be a chordal extension of Gcsp and {Il}l∈[p] be the list of maximal
cliques of G
csp. The polynomials gj, j ∈[m]\J′ can be then partitioned into groups
{gj | j ∈Jl}, l ∈[p] with respect to variable subsets {x(Il)}l∈[p], where x(Il) :=
{xi | i ∈Il}. Consequently, we obtain the following moment hierarchy indexed by

TSSOS: A JULIA LIBRARY TO EXPLOIT SPARSITY FOR LARGE-SCALE POPS
3
d based on correlative sparsity:
(Qcs
d ) :















inf
Ly(f)
s.t.
Md(y, Il) ⪰0,
l ∈[p],
Md−dj(gjy, Il) ⪰0,
j ∈Jl, l ∈[p],
Ly(gj) ≥0,
j ∈J′,
y0 = 1.
Here Md(y, Il) (Md−dj(gjy)) is the moment (localizing) matrix constructed with
respect to the variables x(Il).
2.2. Term sparsity. By exploiting term sparsity, we are able to construct moment
(localizing) matrices with a block structure in an iterative manner. For d ∈N, let
Nn
d := {α = (αi) ∈Nn | Pn
i=1 αi ≤d}. Fix a relaxation order d ≥dmin. Set d0 := 0
and g0 := 1. Let S0 = supp(f) ∪Sm
j=1 supp(gj) ∪(2Nn
d) be the initial support. For
each step k ≥1, let F (k)
d,j be the graph with V (F (k)
d,j ) = Nn
d−dj and
(2.2)
E(F (k)
d,j ) = {{β, γ} | (β + γ + supp(gj)) ∩Sk−1 ̸= ∅}
for j ∈{0}∪[m] and let G(k)
d,j be a chordal extension of F (k)
d,j . The extended support
at the k-th step is deﬁned as
Sk :=
m
[
i=0
{α + β + γ | α ∈supp(gj), {β, γ} ∈E(G(k−1)
d,j
) or β = γ}.
Example 2.1. Consider the polynomial f = 1 + x2
1 + x1x2 + x2
2 + x2
1x2 + x2
1x2
2 +
x2x3 +x2
3 +x2
2x3 +x2x2
3 +x2
2x2
3. To minimize f over R3, we can take the monomial
basis {1, x1, x2, x3, x1x2, x2x3}.
Figure 1 shows the graph F (1) (without dashed
edges) and its chordal extension G(1) (with dashed edges) for f, where we omit the
subscripts d, j since there is no constraint.
Figure 1. The graph F (1) and its chordal extension G(1) for f
x1
x2
x3
x1x2
1
x2x3
Let {C(k)
d,j,i}sd,j
i=1 be the list of maximal cliques of G(k)
d,j for j = 0, . . . , m. Then the
moment hierarchy based on term sparsity for POP (1.1) is deﬁned as:
(2.3)
(Qts
d,k) :











inf
Ly(f)
s.t.
[Md(y)]C(k)
d,0,i ⪰0,
i ∈[sd,0],
[Md−dj(gjy)]C(k)
d,j,i ⪰0,
i ∈[sd,j], j ∈[m],
y0 = 1.

4
V. MAGRON AND J. WANG
Here we denote by AC the submatrix of A ∈Rr×r with rows and columns indexed
by C ⊆[r].
The above hierarchy (called the TSSOS hierarchy) is indexed by two parameters:
the relaxation order d and the sparse order k.
For a ﬁxed d, the sequence of
optimums of (Qts
d,k) is non-decreasing and stabilizes in ﬁnitely many steps. There
are two particular choices for the chordal extension G(k)
d,j of F (k)
d,j : approximately
smallest chordal extensions [26]1 and the maximal chordal extension [27]2. This
oﬀers a trade-oﬀbetween the computational cost and the quality of obtained lower
bounds. Typically, the choice of approximately smallest chordal extensions leads to
positive semideﬁnite (PSD) blocks of smaller sizes but may provide a looser lower
bound whereas the choice of the maximal chordal extension leads to PSD blocks
of larger sizes but may provide a tighter lower bound. It was proved in [27] that if
the maximal chordal extension is chosen, then the sequence of optimums of (Qts
d,k)
converges to the optimum of the corresponding dense relaxation as k increases for
a ﬁxed relaxation order d.
2.3. Correlative-term sparsity. We can further exploit correlative sparsity and
term sparsity simultaneously. Namely, ﬁrst partition the set of variables into subsets
{Il}l∈[p] as done in Sec. 2.1 and then apply the iterative procedure for exploiting
term sparsity in Sec. 2.2 to each subsystem involving variables x(Il). Let G(k)
d,l,j be
the graphs associated with each subsystem and {C(k)
d,l,j,i}sd,l,j
i=1
be the list of maximal
cliques of G(k)
d,l,j for j ∈{0} ∪Jl, l ∈[p]. Then the moment hierarchy based on
correlative-term sparsity for POP (1.1) is deﬁned as:
(2.4)
(Qcs-ts
d,k ) :

















inf
Ly(f)
s.t.
[Md(y, Il)]C(k)
d,l,0,i ⪰0,
i ∈[sd,l,0], l ∈[p],
[Md−dj(gjy, Il)]C(k)
d,l,j,i ⪰0,
i ∈[sd,l,j], j ∈Jl, l ∈[p],
Ly(gj) ≥0,
j ∈J′,
y0 = 1.
The above hierarchy (called the CS-TSSOS hierarchy) is also indexed by two
parameters: the relaxation order d and the sparse order k.
For a ﬁxed d, the
sequence of optimums of (Qcs-ts
d,k ) is non-decreasing and stabilizes in ﬁnitely many
steps. As discussed in Sec. 2.2, the choice of chordal extensions G(k)
d,l,j oﬀers a
trade-oﬀbetween the computational cost and the quality of obtained lower bounds.
It was proved in [28] that if the maximal chordal extension is chosen, then the
sequence of optimums of (Qcs-ts
d,k ) converges to the optimum of (Qcs
d ) as k increases
for a ﬁxed relaxation order d.
1A smallest chordal extension is a chordal extension with the smallest clique number. Comput-
ing a smallest chordal extension is an NP-hard problem. Fortunately, there are eﬃcient heuristic
algorithms to produce a good approximation of smallest chordal extensions.
2By the maximal chordal extension, we refer to the chordal extension that completes each
connected component of the graph.

TSSOS: A JULIA LIBRARY TO EXPLOIT SPARSITY FOR LARGE-SCALE POPS
5
3. The implementation of TSSOS
TSSOS, which implements the sparsity-adapted moment-SOS hierarchies (i.e.
(Qcs
d ), (Qts
d,k) and (Qcs-ts
d,k )), is developed as a Julia library, aiming to solve large-
scale polynomial optimization problems by fully exploiting sparsity as well as other
techniques. TSSOS provides an easy way to deﬁne a POP and to solve it by SDP
relaxations. The tunable parameters (e.g. d, k, the types of chordal extensions)
allow the user to ﬁnd the best compromise between the computational cost and the
solving precision. The following script is a simple example to illustrate the usage
of TSSOS.
using TSSOS
using DynamicPolynomials
@polyvar x[1:6]
f = x[1]^4 + x[2]^4 - 2x[1]^2*x[2] - 2x[1] + 2x[2]*x[3] - 2x[1]^2*x[3]
- 2x[2]^2*x[3] - 2x[2]^2*x[4] - 2x[2] + 2x[1]^2 + 2.5x[1]*x[2] - 2x[4]
+ 2x[1]*x[4] + 3x[2]^2 + 2x[2]*x[5] + 2x[3]^2 + 2x[3]*x[4] + 2x[4]^2 +
x[5]^2 - 2x[5] + 2 # define the objective function
g = 1 - sum(x[1:2].^2) # define the inequality constraint
h = 1 - sum(x[3:5].^2) # define the equality constraint
d = 2 # define the relaxation order
numeq = 1 # define the number of equality constraints
To solve the ﬁrst step of the TSSOS hierarchy with approximately smallest chordal
extensions (option TS="MD"), run
opt,sol,data = tssos first([f;g;h], x, d, numeq=numeq, TS="MD")
We obtain
optimum = 0.20967292920706904
To solve higher steps of the TSSOS hierarchy, repeatedly run
opt,sol,data = tssos higher!(data, TS="MD")
For instance, for the second step of the TSSOS hierarchy we obtain
optimum = 0.21230011405774876
To solve the ﬁrst step of the CS-TSSOS hierarchy, run
opt,sol,data = cs tssos first([f;g;h], x, d, numeq=numeq, TS="MD")
We obtain
optimum = 0.20929635879961658
To solve higher steps of the CS-TSSOS hierarchy, repeatedly run
opt,sol,data = cs tssos higher!(data, TS="MD")
For instance, for the second step of the CS-TSSOS hierarchy we obtain
optimum = 0.20974835386107363
3.1. Dependencies. TSSOS depends on the following Julia packages:
• MultivariatePolynomials to manipulate multivariate polynomials;
• JuMP [7] to model the SDP problem;
• LightGraphs [4] to handle graphs;
• MetaGraphs to handle weighted graphs;
• ChordalGraph [23] to generate approximately smallest chordal extensions;

6
V. MAGRON AND J. WANG
• SemialgebraicSets to compute Gr¨obner bases.
Besides, TSSOS requires an SDP solver, which can be MOSEK [1], SDPT3 [20], or COSMO
[8]. Once one of the SDP solvers has been installed, the installation of TSSOS is
straightforward:
Pkg.add("https://github.com/wangjie212/TSSOS")
3.2. Binary variables. TSSOS supports binary variables. By setting nb = s, one
can specify that the ﬁrst s variables are binary variables x1, . . . , xs which satisfy
the equation x2
i = 1, i ∈[s]. The speciﬁcation is helpful to reduce the number of
decision variables of SDP relaxations since one can identify xr with xr (mod 2) for a
binary variable x.
3.3. Equality constraints. If there are equality constraints in the description of
POP (1.1), then one can reduce the number of decision variables of SDP relaxations
by working in the quotient ring R[x]/(h1, . . . , ht), where {h1 = 0, . . . , ht = 0} is
the set of equality constraints. To conduct the elimination, we need to compute
a Gr¨obner basis GB of the ideal (h1, . . . , ht).
Then any monomial xα can be
replaced by its normal form NF(xα, GB) with respect to the Gr¨obner basis GB
when constructing SDP relaxations. This reduction is conducted by default for the
TSSOS hierarchy in TSSOS.
3.4. Adding extra ﬁrst-order moment matrices. When POP (1.1) is a quadrat-
ically constrained quadratic program, the ﬁrst-order moment-SOS relaxation is also
known as Shor’s relaxation. In this case, (Q1), (Qcs
1 ) and (Qts
1,1) yield the same op-
timum. To ensure that any higher order sparse relaxation (i.e. (Qcs-ts
d,k ) with d > 1)
provides a tighter lower bound compared to the one given by Shor’s relaxation, we
may add an extra ﬁrst-order moment matrix for each variable clique in (Qcs-ts
d,k ):
(3.1)
(Qcs-ts
d,k )′ :





















inf
Ly(f)
s.t.
[Md(y, Il)]C(k)
d,l,0,i ⪰0,
i ∈[sd,l,0], l ∈[p],
[M1(y, Il)] ⪰0,
l ∈[p],
[Md−dj(gjy, Il)]C(k)
d,l,j,i ⪰0,
i ∈[sd,l,j], j ∈Jl, l ∈[p],
Ly(gj) ≥0,
j ∈J′,
y0 = 1.
In TSSOS, this is accomplished by setting MomentOne = true.
3.5. Chordal extensions. For correlative sparsity, TSSOS uses approximately small-
est chordal extensions. For term sparsity, TSSOS supports two types of chordal ex-
tensions: the maximal chordal extension (option TS = “block”) and approximately
smallest chordal extensions. TSSOS generates approximately smallest chordal ex-
tensions via two heuristics: the Minimum Degree heuristic (option “TS = MD”) and
the Minimum Fillin heuristic (option “TS = MF”). See [3] for a full description of
these two heuristics. The Minimum Degree heuristic is slightly faster in practice,
but the Minimum Fillin heuristic yields on average slightly smaller clique numbers.
Hence for correlative sparsity, the Minimum Degree heuristic is recommended and
for term sparsity, the Minimum Fillin heuristic is recommended.

TSSOS: A JULIA LIBRARY TO EXPLOIT SPARSITY FOR LARGE-SCALE POPS
7
Figure 2. Merge two 4 × 4 blocks into a single 5 × 5 block


•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•


−→


•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•


3.6. Merging PSD blocks. In case that two PSD blocks have a large portion of
overlaps, it might be beneﬁcial to merge these two blocks into a single block for
eﬃciency. See Figure 2 for such an example. TSSOS supports PSD block merging
inspired by the strategy proposed in [8]. To activate the merging process, one just
needs to set the option Merge = True. The parameter md = 3 can be used to tune
the merging strength.
3.7. Representing polynomials in terms of supports and coeﬃcients. The
Julia package DynamicPolynomials provides an eﬃcient way to deﬁne polynomials
symbolically. But for large-scale polynomial optimization (say, n > 500), it is more
eﬃcient to represent polynomials by their supports and coeﬃcients. For instance,
we can represent f = x4
1 + x4
2 + x4
3 + x1x2x3 in terms of its support and coeﬃcients
as follows:
supp = [[1; 1; 1; 1], [2; 2; 2 ;2], [3; 3; 3; 3], [1; 2; 3]] # define
the support array of f
coe = [1; 1; 1; 1] # define the coefficient vector of f
The above representation of polynomials is natively supported by TSSOS. Hence
the user can deﬁne the polynomial optimization problem directly by the support
data and the coeﬃcient data to speed up the modeling process.
3.8. Extension to noncommutative polynomial optimization. The whole
framework of exploiting sparsity for (commutative) polynomial optimization can be
extended to handle noncommutative polynomial optimization [25], including eigen-
value and trace optimization, which leads to the submodule NCTSSOS in TSSOS.
Table 1 displays the numerical results for the eigenvalue minimization of the non-
commutative Broyden banded function. Here “mb” stands for the maximal block
size of the matrices involved in the SDP relaxation. It is evident that the sparse
approach scales much better than the dense one.
4. Numerical experiments
In this section, we present numerical results for the alternating current optimal
power ﬂow (AC-OPF) problem – a famous industrial problem in power system,
which can be cast as a POP involving up to tens of thousands of variables and
constraints. To tackle an AC-OPF instance, we ﬁrst compute a locally optimal
solution with a local solver and then rely on the sparsity-adapted moment-SOS
hierarchy to certify the global optimality. Suppose that the optimal value reported
by the local solver is AC and the optimal value of the SDP relaxation is opt. Then
the optimality gap between the locally optimal solution and the SDP relaxation is

8
V. MAGRON AND J. WANG
Table 1. The eigenvalue minimization of the noncommutative
Broyden banded function: exploiting sparsity versus without ex-
ploiting sparsity. n: the number of variables; mb: the maixmal
size of PSD blocks; opt: the optimum returned by the SDP solver;
time: running time in seconds; “-” indicates an out of memory
error.
n
sparse
dense
mb
opt
time
mb
opt
time
20
15
0
0.18
61
0
1.39
40
15
0
0.72
121
0
66.1
60
15
0
1.05
181
0
505
80
15
0
1.24
-
-
-
100
15
0
1.41
-
-
-
200
15
0
3.25
-
-
-
400
15
−0.0001
6.70
-
-
-
600
15
−0.0002
13.2
-
-
-
deﬁned by
gap := AC −opt
AC
× 100%.
When the optimality gap is less than 1%, we accept the locally optimal solution
as globally optimal. The test cases in this section are selected from the AC-OPF
library PGLiB [2]. All numerical experiments were computed on an Intel Core i5-
8265U@1.60GHz CPU with 8GB RAM memory. The SDP solver is Mosek 9.0 with
default parameters.
Table 2. The results for AC-OPF instances. n: the number of
variables; m: the number of constraints; AC: the local optimum;
mc: the maximal size of variable cliques; opt: the optimum re-
turned by the SDP solver; time: running time in seconds; gap: the
optimality gap.
case
n
m
AC
mc
opt
time
gap
14 ieee api
38
147
5.9994e3
6
5.9994e3
0.54
0.00%
30 ieee
72
297
8.2085e3
8
8.2085e3
0.99
0.00%
39 epri sad
98
361
1.4834e5
8
1.4831e5
1.45
0.02%
118 ieee
344
1325
9.7214e4
10
9.7214e4
7.71
0.00%
179 goc api
416
1827
1.9320e6
10
1.9226e6
9.69
0.48%
300 ieee
738
2983
5.6522e5
14
5.6522e5
25.2
0.00%
793 goc
1780
7019
2.6020e5
18
2.5932e5
66.1
0.34%
1354 pegase sad
3228
13901
1.2588e6
26
1.2582e6
387
0.05%
1951 rte api
4634
18921
2.4108e6
26
2.4029e6
596
0.32%
2000 goc api
4476
23009
1.4686e6
42
1.4610e6
1094
0.51%
2312 goc
5076
21753
4.4133e5
68
4.3858e5
997
0.62%
3022 goc sad
6698
29283
6.0143e5
50
5.9859e5
1340
0.47%

TSSOS: A JULIA LIBRARY TO EXPLOIT SPARSITY FOR LARGE-SCALE POPS
9
From Table 2, we can see that for all test cases, TSSOS successfully reduces the
optimality gap to less than 1%, namely, certiﬁes the global optimality. The largest
instance 3022 goc sad has 6698 variables and 29283 constraints.
Acknowledgements. This work was supported by the Tremplin ERC Stg Grant
ANR-18-ERC2-0004-01 (T-COPS project). The ﬁrts author was supported by the
FMJH Program PGMO (EPICS project) and EDF, Thales, Orange et Criteo. This
work has beneﬁted from the European Union’s Horizon 2020 research and innova-
tion program under the Marie Sklodowska-Curie Actions, grant agreement 813211
(POEMA) as well as from the AI Interdisciplinary Institute ANITI funding, through
the French “Investing for the Future PIA3” program under the Grant agreement
n◦ANR-19-PI3A-0004.
References
[1] MOSEK ApS. The MOSEK optimization toolbox. Version 8.1., 2017.
[2] Sogol Babaeinejadsarookolaee, Adam Birchﬁeld, Richard D Christie, Carleton Coﬀrin,
Christopher DeMarco, Ruisheng Diao, Michael Ferris, Stephane Fliscounakis, Scott Greene,
Renke Huang, et al. The power grid library for benchmarking ac optimal power ﬂow algo-
rithms. arXiv preprint arXiv:1908.02788, 2019.
[3] Hans L Bodlaender and Arie MCA Koster. Treewidth computations i. upper bounds. Infor-
mation and Computation, 208(3):259–275, 2010.
[4] Seth Bromberger, James Fairbanks, and other contributors. Juliagraphs/lightgraphs.jl: an
optimized graphs package for the julia programming language, 2017.
[5] Tong Chen, Jean-Bernard Lasserre, Victor Magron, and Edouard Pauwels. Semialgebraic
optimization for lipschitz constants of relu networks. arXiv e-prints, pages arXiv–2002, 2020.
[6] Tong Chen, Jean-Bernard Lasserre, Victor Magron, and Edouard Pauwels. A sublevel
moment-sos hierarchy for polynomial optimization. arXiv preprint arXiv:2101.05167, 2021.
[7] Iain Dunning, Joey Huchette, and Miles Lubin. Jump: A modeling language for mathematical
optimization. SIAM Review, 59(2):295–320, 2017.
[8] Michael Garstka, Mark Cannon, and Paul Goulart. Cosmo: A conic operator splitting method
for large convex problems. In 2019 18th European Control Conference (ECC), pages 1951–
1956. IEEE, 2019.
[9] C´edric Josz and Daniel K Molzahn. Lasserre hierarchy for large scale polynomial optimization
in real and complex variables. SIAM Journal on Optimization, 28(2):1017–1048, 2018.
[10] Igor Klep, Victor Magron, and Janez Povh. Sparse noncommutative polynomial optimization.
Mathematical Programming, pages 1–41, 2021.
[11] J.-B. Lasserre. Global Optimization with Polynomials and the Problem of Moments. SIAM
Journal on Optimization, 11(3):796–817, 2001.
[12] J.-B. Lasserre. Convergent sdp-relaxations in polynomial optimization with sparsity. SIAM
Journal on Optimization, 17(3):822–843, 2006.
[13] Victor Magron. Interval enclosures of upper bounds of roundoﬀerrors using semideﬁnite
programming. ACM Transactions on Mathematical Software (TOMS), 44(4):1–18, 2018.
[14] Victor Magron, George Constantinides, and Alastair Donaldson. Certiﬁed roundoﬀer-
ror bounds using semideﬁnite programming. ACM Transactions on Mathematical Software
(TOMS), 43(4):1–31, 2017.
[15] Ngoc Hoang Anh Mai, Abhishek Bhardwaj, and Victor Magron. The constant trace property
in noncommutative optimization. arXiv preprint arXiv:2102.02162, 2021.
[16] Ngoc Hoang Anh Mai, Jean-Bernard Lasserre, Victor Magron, and Jie Wang. Exploiting con-
stant trace property in large-scale polynomial optimization. arXiv preprint arXiv:2012.08873,
2020.
[17] Ngoc Hoang Anh Mai, Victor Magron, and J-B Lasserre. A sparse version of reznick’s posi-
tivstellensatz. arXiv preprint arXiv:2002.05101, 2020.
[18] Corbinian Schlosser and Milan Korda. Sparse moment-sum-of-squares relaxations for non-
linear dynamical systems with guaranteed convergence. arXiv preprint arXiv:2012.05572,
2020.

10
V. MAGRON AND J. WANG
[19] Matteo Tacchi, Tillmann Weisser, Jean-Bernard Lasserre, and Didier Henrion. Exploiting
sparsity for semi-algebraic set volume computation. arXiv preprint arXiv:1902.02976, 2019.
[20] Kim-Chuan Toh, Michael J Todd, and Reha H T¨ut¨unc¨u. Sdpt3—a matlab software package
for semideﬁnite programming, version 1.3. Optimization methods and software, 11(1-4):545–
581, 1999.
[21] H. Waki, S. Kim, M. Kojima, and M. Muramatsu. Sums of Squares and Semideﬁnite Program-
ming Relaxations for Polynomial Optimization Problems with Structured Sparsity. SIAM
Journal on Optimization, 17(1):218–242, 2006.
[22] Hayato Waki, Sunyoung Kim, Masakazu Kojima, Masakazu Muramatsu, and Hiroshi Sugi-
moto. Algorithm 883: Sparsepop—a sparse semideﬁnite programming relaxation of polyno-
mial optimization problems. ACM Transactions on Mathematical Software (TOMS), 35(2):1–
13, 2008.
[23] Jie Wang. ChordalGraph: A Julia Package to Handle Chordal Graphs. 2020.
[24] Jie Wang, Martina Maggio, and Victor Magron. SparseJSR: A Fast Algorithm to Compute
Joint Spectral Radius via Sparse SOS Decompositions. arXiv preprint arXiv:2008.11441,
2020.
[25] Jie Wang and Victor Magron. Exploiting term sparsity in noncommutative polynomial opti-
mization. arXiv preprint arXiv:2010.06956, 2020.
[26] Jie Wang, Victor Magron, and Jean-Bernard Lasserre. Chordal-TSSOS: a moment-SOS hi-
erarchy that exploits term sparsity with chordal extension. SIAM Journal on Optimization,
2020. Accepted for publication.
[27] Jie Wang, Victor Magron, and Jean-Bernard Lasserre. TSSOS: A Moment-SOS hierarchy
that exploits term sparsity. SIAM Journal on Optimization, 2020. Accepted for publication.
[28] Jie Wang, Victor Magron, Jean-Bernard Lasserre, and Ngoc Hoang Anh Mai. CS-TSSOS:
Correlative and term sparsity for large-scale polynomial optimization. arXiv:2005.02828,
2020.
[29] Quan Zhou and Jakub Marecek. Proper learning of linear dynamical systems as a non-
commutative polynomial optimisation problem. arXiv preprint arXiv:2002.01444, 2020.
[30] Quan Zhou, Jakub Marecek, and Robert N Shorten. Fairness in forecasting and learning
linear dynamical systems. arXiv preprint arXiv:2006.07315, 2020.

